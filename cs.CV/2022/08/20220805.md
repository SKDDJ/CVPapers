# Arxiv Papers in cs.CV on 2022-08-05
### Applied monocular reconstruction of parametric faces with domain engineering
- **Arxiv ID**: http://arxiv.org/abs/2208.02935v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02935v1)
- **Published**: 2022-08-05 00:04:18+00:00
- **Updated**: 2022-08-05 00:04:18+00:00
- **Authors**: Igor Borovikov, Karine Levonyan, Jon Rein, Pawel Wrotek, Nitish Victor
- **Comment**: 16 pages; SIPP 2022, 10th International Conference on Signal, Image
  Processing and Pattern Recognition (London, United Kingdom)
- **Journal**: None
- **Summary**: Many modern online 3D applications and videogames rely on parametric models of human faces for creating believable avatars. However, manual reproduction of someone's facial likeness with a parametric model is difficult and time-consuming. Machine Learning solution for that task is highly desirable but is also challenging. The paper proposes a novel approach to the so-called Face-to-Parameters problem (F2P for short), aiming to reconstruct a parametric face from a single image. The proposed method utilizes synthetic data, domain decomposition, and domain adaptation for addressing multifaceted challenges in solving the F2P. The open-sourced codebase illustrates our key observations and provides means for quantitative evaluation. The presented approach proves practical in an industrial application; it improves accuracy and allows for more efficient models training. The techniques have the potential to extend to other types of parametric models.



### Learning to Generate 3D Shapes from a Single Example
- **Arxiv ID**: http://arxiv.org/abs/2208.02946v2
- **DOI**: 10.1145/3550454.3555480
- **Categories**: **cs.GR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.02946v2)
- **Published**: 2022-08-05 01:05:32+00:00
- **Updated**: 2022-09-22 01:06:48+00:00
- **Authors**: Rundi Wu, Changxi Zheng
- **Comment**: SIGGRAPH Asia 2022; 19 pages (including 6 pages appendix), 17
  figures. Project page: http://www.cs.columbia.edu/cg/SingleShapeGen/
- **Journal**: None
- **Summary**: Existing generative models for 3D shapes are typically trained on a large 3D dataset, often of a specific object category. In this paper, we investigate the deep generative model that learns from only a single reference 3D shape. Specifically, we present a multi-scale GAN-based model designed to capture the input shape's geometric features across a range of spatial scales. To avoid large memory and computational cost induced by operating on the 3D volume, we build our generator atop the tri-plane hybrid representation, which requires only 2D convolutions. We train our generative model on a voxel pyramid of the reference shape, without the need of any external supervision or manual annotation. Once trained, our model can generate diverse and high-quality 3D shapes possibly of different sizes and aspect ratios. The resulting shapes present variations across different scales, and at the same time retain the global structure of the reference shape. Through extensive evaluation, both qualitative and quantitative, we demonstrate that our model can generate 3D shapes of various types.



### Joint Attention-Driven Domain Fusion and Noise-Tolerant Learning for Multi-Source Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2208.02947v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02947v2)
- **Published**: 2022-08-05 01:08:41+00:00
- **Updated**: 2022-10-06 06:53:11+00:00
- **Authors**: Tong Xu, Lin Wang, Wu Ning, Chunyan Lyu, Kejun Wang, Chenhui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: As a study on the efficient usage of data, Multi-source Unsupervised Domain Adaptation transfers knowledge from multiple source domains with labeled data to an unlabeled target domain. However, the distribution discrepancy between different domains and the noisy pseudo-labels in the target domain both lead to performance bottlenecks of the Multi-source Unsupervised Domain Adaptation methods. In light of this, we propose an approach that integrates Attention-driven Domain fusion and Noise-Tolerant learning (ADNT) to address the two issues mentioned above. Firstly, we establish a contrary attention structure to perform message passing between features and to induce domain movement. Through this approach, the discriminability of the features can also be significantly improved while the domain discrepancy is reduced. Secondly, based on the characteristics of the unsupervised domain adaptation training, we design an Adaptive Reverse Cross Entropy loss, which can directly impose constraints on the generation of pseudo-labels. Finally, combining these two approaches, experimental results on several benchmarks further validate the effectiveness of our proposed ADNT and demonstrate superior performance over the state-of-the-art methods.



### A Novel Enhanced Convolution Neural Network with Extreme Learning Machine: Facial Emotional Recognition in Psychology Practices
- **Arxiv ID**: http://arxiv.org/abs/2208.02953v1
- **DOI**: 10.1007/s11042-022-13567-8
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2208.02953v1)
- **Published**: 2022-08-05 02:21:34+00:00
- **Updated**: 2022-08-05 02:21:34+00:00
- **Authors**: Nitesh Banskota, Abeer Alsadoon, P. W. C. Prasad, Ahmed Dawoud, Tarik A. Rashid, Omar Hisham Alsadoon
- **Comment**: 19 pages
- **Journal**: Multimedia Tools and Applications, 2022
- **Summary**: Facial emotional recognition is one of the essential tools used by recognition psychology to diagnose patients. Face and facial emotional recognition are areas where machine learning is excelling. Facial Emotion Recognition in an unconstrained environment is an open challenge for digital image processing due to different environments, such as lighting conditions, pose variation, yaw motion, and occlusions. Deep learning approaches have shown significant improvements in image recognition. However, accuracy and time still need improvements. This research aims to improve facial emotion recognition accuracy during the training session and reduce processing time using a modified Convolution Neural Network Enhanced with Extreme Learning Machine (CNNEELM). The system entails (CNNEELM) improving the accuracy in image registration during the training session. Furthermore, the system recognizes six facial emotions happy, sad, disgust, fear, surprise, and neutral with the proposed CNNEELM model. The study shows that the overall facial emotion recognition accuracy is improved by 2% than the state of art solutions with a modified Stochastic Gradient Descent (SGD) technique. With the Extreme Learning Machine (ELM) classifier, the processing time is brought down to 65ms from 113ms, which can smoothly classify each frame from a video clip at 20fps. With the pre-trained InceptionV3 model, the proposed CNNEELM model is trained with JAFFE, CK+, and FER2013 expression datasets. The simulation results show significant improvements in accuracy and processing time, making the model suitable for the video analysis process. Besides, the study solves the issue of the large processing time required to process the facial images.



### DeepWSD: Projecting Degradations in Perceptual Space to Wasserstein Distance in Deep Feature Space
- **Arxiv ID**: http://arxiv.org/abs/2208.03323v1
- **DOI**: 10.1145/3503161.3548193
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.03323v1)
- **Published**: 2022-08-05 02:46:12+00:00
- **Updated**: 2022-08-05 02:46:12+00:00
- **Authors**: Xingran Liao, Baoliang Chen, Hanwei Zhu, Shiqi Wang, Mingliang Zhou, Sam Kwong
- **Comment**: ACM Multimedia 2022 accepted thesis
- **Journal**: None
- **Summary**: Existing deep learning-based full-reference IQA (FR-IQA) models usually predict the image quality in a deterministic way by explicitly comparing the features, gauging how severely distorted an image is by how far the corresponding feature lies from the space of the reference images. Herein, we look at this problem from a different viewpoint and propose to model the quality degradation in perceptual space from a statistical distribution perspective. As such, the quality is measured based upon the Wasserstein distance in the deep feature domain. More specifically, the 1DWasserstein distance at each stage of the pre-trained VGG network is measured, based on which the final quality score is performed. The deep Wasserstein distance (DeepWSD) performed on features from neural networks enjoys better interpretability of the quality contamination caused by various types of distortions and presents an advanced quality prediction capability. Extensive experiments and theoretical analysis show the superiority of the proposed DeepWSD in terms of both quality prediction and optimization.



### Memory-Guided Collaborative Attention for Nighttime Thermal Infrared Image Colorization
- **Arxiv ID**: http://arxiv.org/abs/2208.02960v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02960v1)
- **Published**: 2022-08-05 03:04:04+00:00
- **Updated**: 2022-08-05 03:04:04+00:00
- **Authors**: Fu-Ya Luo, Yi-Jun Cao, Kai-Fu Yang, Yong-Jie Li
- **Comment**: 16 pages, 13 figures
- **Journal**: None
- **Summary**: Nighttime thermal infrared (NTIR) image colorization, also known as translation of NTIR images into daytime color images (NTIR2DC), is a promising research direction to facilitate nighttime scene perception for humans and intelligent systems under unfavorable conditions (e.g., complete darkness). However, previously developed methods have poor colorization performance for small sample classes. Moreover, reducing the high confidence noise in pseudo-labels and addressing the problem of image gradient disappearance during translation are still under-explored, and keeping edges from being distorted during translation is also challenging. To address the aforementioned issues, we propose a novel learning framework called Memory-guided cOllaboRative atteNtion Generative Adversarial Network (MornGAN), which is inspired by the analogical reasoning mechanisms of humans. Specifically, a memory-guided sample selection strategy and adaptive collaborative attention loss are devised to enhance the semantic preservation of small sample categories. In addition, we propose an online semantic distillation module to mine and refine the pseudo-labels of NTIR images. Further, conditional gradient repair loss is introduced for reducing edge distortion during translation. Extensive experiments on the NTIR2DC task show that the proposed MornGAN significantly outperforms other image-to-image translation methods in terms of semantic preservation and edge consistency, which helps improve the object detection accuracy remarkably.



### An Efficient Person Clustering Algorithm for Open Checkout-free Groceries
- **Arxiv ID**: http://arxiv.org/abs/2208.02973v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02973v3)
- **Published**: 2022-08-05 03:48:07+00:00
- **Updated**: 2022-09-18 04:39:21+00:00
- **Authors**: Junde Wu, Yu Zhang, Rao Fu, Yuanpei Liu, Jing Gao
- **Comment**: European Conference on Computer Vision (ECCV) 2022
- **Journal**: None
- **Summary**: Open checkout-free grocery is the grocery store where the customers never have to wait in line to check out. Developing a system like this is not trivial since it faces challenges of recognizing the dynamic and massive flow of people. In particular, a clustering method that can efficiently assign each snapshot to the corresponding customer is essential for the system. In order to address the unique challenges in the open checkout-free grocery, we propose an efficient and effective person clustering method. Specifically, we first propose a Crowded Sub-Graph (CSG) to localize the relationship among massive and continuous data streams. CSG is constructed by the proposed Pick-Link-Weight (PLW) strategy, which \textbf{picks} the nodes based on time-space information, \textbf{links} the nodes via trajectory information, and \textbf{weighs} the links by the proposed von Mises-Fisher (vMF) similarity metric. Then, to ensure that the method adapts to the dynamic and unseen person flow, we propose Graph Convolutional Network (GCN) with a simple Nearest Neighbor (NN) strategy to accurately cluster the instances of CSG. GCN is adopted to project the features into low-dimensional separable space, and NN is able to quickly produce a result in this space upon dynamic person flow. The experimental results show that the proposed method outperforms other alternative algorithms in this scenario. In practice, the whole system has been implemented and deployed in several real-world open checkout-free groceries.



### Analyzing the Impact of Shape & Context on the Face Recognition Performance of Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.02991v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02991v1)
- **Published**: 2022-08-05 05:32:07+00:00
- **Updated**: 2022-08-05 05:32:07+00:00
- **Authors**: Sandipan Banerjee, Walter Scheirer, Kevin Bowyer, Patrick Flynn
- **Comment**: None
- **Journal**: None
- **Summary**: In this article, we analyze how changing the underlying 3D shape of the base identity in face images can distort their overall appearance, especially from the perspective of deep face recognition. As done in popular training data augmentation schemes, we graphically render real and synthetic face images with randomly chosen or best-fitting 3D face models to generate novel views of the base identity. We compare deep features generated from these images to assess the perturbation these renderings introduce into the original identity. We perform this analysis at various degrees of facial yaw with the base identities varying in gender and ethnicity. Additionally, we investigate if adding some form of context and background pixels in these rendered images, when used as training data, further improves the downstream performance of a face recognition model. Our experiments demonstrate the significance of facial shape in accurate face matching and underpin the importance of contextual data for network training.



### Perception-Distortion Balanced ADMM Optimization for Single-Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2208.03324v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.03324v2)
- **Published**: 2022-08-05 05:37:55+00:00
- **Updated**: 2022-08-16 05:16:23+00:00
- **Authors**: Yuehan Zhang, Bo Ji, Jia Hao, Angela Yao
- **Comment**: None
- **Journal**: None
- **Summary**: In image super-resolution, both pixel-wise accuracy and perceptual fidelity are desirable. However, most deep learning methods only achieve high performance in one aspect due to the perception-distortion trade-off, and works that successfully balance the trade-off rely on fusing results from separately trained models with ad-hoc post-processing. In this paper, we propose a novel super-resolution model with a low-frequency constraint (LFc-SR), which balances the objective and perceptual quality through a single model and yields super-resolved images with high PSNR and perceptual scores. We further introduce an ADMM-based alternating optimization method for the non-trivial learning of the constrained model. Experiments showed that our method, without cumbersome post-processing procedures, achieved the state-of-the-art performance. The code is available at https://github.com/Yuehan717/PDASR.



### Localized Sparse Incomplete Multi-view Clustering
- **Arxiv ID**: http://arxiv.org/abs/2208.02998v3
- **DOI**: 10.1109/TMM.2022.3194332
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.02998v3)
- **Published**: 2022-08-05 05:48:28+00:00
- **Updated**: 2023-03-13 13:25:27+00:00
- **Authors**: Chengliang Liu, Zhihao Wu, Jie Wen, Chao Huang, Yong Xu
- **Comment**: Published in IEEE Transactions on Multimedia (TMM). The code is
  available at Github https://github.com/justsmart/LSIMVC
- **Journal**: None
- **Summary**: Incomplete multi-view clustering, which aims to solve the clustering problem on the incomplete multi-view data with partial view missing, has received more and more attention in recent years. Although numerous methods have been developed, most of the methods either cannot flexibly handle the incomplete multi-view data with arbitrary missing views or do not consider the negative factor of information imbalance among views. Moreover, some methods do not fully explore the local structure of all incomplete views. To tackle these problems, this paper proposes a simple but effective method, named localized sparse incomplete multi-view clustering (LSIMVC). Different from the existing methods, LSIMVC intends to learn a sparse and structured consensus latent representation from the incomplete multi-view data by optimizing a sparse regularized and novel graph embedded multi-view matrix factorization model. Specifically, in such a novel model based on the matrix factorization, a l1 norm based sparse constraint is introduced to obtain the sparse low-dimensional individual representations and the sparse consensus representation. Moreover, a novel local graph embedding term is introduced to learn the structured consensus representation. Different from the existing works, our local graph embedding term aggregates the graph embedding task and consensus representation learning task into a concise term. Furthermore, to reduce the imbalance factor of incomplete multi-view learning, an adaptive weighted learning scheme is introduced to LSIMVC. Finally, an efficient optimization strategy is given to solve the optimization problem of our proposed model. Comprehensive experimental results performed on six incomplete multi-view databases verify that the performance of our LSIMVC is superior to the state-of-the-art IMC approaches. The code is available in https://github.com/justsmart/LSIMVC.



### Task-Balanced Distillation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.03006v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03006v1)
- **Published**: 2022-08-05 06:43:40+00:00
- **Updated**: 2022-08-05 06:43:40+00:00
- **Authors**: Ruining Tang, Zhenyu Liu, Yangguang Li, Yiguo Song, Hui Liu, Qide Wang, Jing Shao, Guifang Duan, Jianrong Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Mainstream object detectors are commonly constituted of two sub-tasks, including classification and regression tasks, implemented by two parallel heads. This classic design paradigm inevitably leads to inconsistent spatial distributions between classification score and localization quality (IOU). Therefore, this paper alleviates this misalignment in the view of knowledge distillation. First, we observe that the massive teacher achieves a higher proportion of harmonious predictions than the lightweight student. Based on this intriguing observation, a novel Harmony Score (HS) is devised to estimate the alignment of classification and regression qualities. HS models the relationship between two sub-tasks and is seen as prior knowledge to promote harmonious predictions for the student. Second, this spatial misalignment will result in inharmonious region selection when distilling features. To alleviate this problem, a novel Task-decoupled Feature Distillation (TFD) is proposed by flexibly balancing the contributions of classification and regression tasks. Eventually, HD and TFD constitute the proposed method, named Task-Balanced Distillation (TBD). Extensive experiments demonstrate the considerable potential and generalization of the proposed method. Specifically, when equipped with TBD, RetinaNet with ResNet-50 achieves 41.0 mAP under the COCO benchmark, outperforming the recent FGD and FRS.



### TransMatting: Enhancing Transparent Objects Matting with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2208.03007v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03007v3)
- **Published**: 2022-08-05 06:44:14+00:00
- **Updated**: 2022-09-04 15:21:49+00:00
- **Authors**: Huanqia Cai, Fanglei Xue, Lele Xu, Lili Guo
- **Comment**: Accepted by ECCV 2022. Project page:
  https://github.com/AceCHQ/TransMatting
- **Journal**: None
- **Summary**: Image matting refers to predicting the alpha values of unknown foreground areas from natural images. Prior methods have focused on propagating alpha values from known to unknown regions. However, not all natural images have a specifically known foreground. Images of transparent objects, like glass, smoke, web, etc., have less or no known foreground. In this paper, we propose a Transformer-based network, TransMatting, to model transparent objects with a big receptive field. Specifically, we redesign the trimap as three learnable tri-tokens for introducing advanced semantic features into the self-attention mechanism. A small convolutional network is proposed to utilize the global feature and non-background mask to guide the multi-scale feature propagation from encoder to decoder for maintaining the contexture of transparent objects. In addition, we create a high-resolution matting dataset of transparent objects with small known foreground areas. Experiments on several matting benchmarks demonstrate the superiority of our proposed method over the current state-of-the-art methods.



### Rethinking Degradation: Radiograph Super-Resolution via AID-SRGAN
- **Arxiv ID**: http://arxiv.org/abs/2208.03008v1
- **DOI**: 10.1007/978-3-031-21014-3_5
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.03008v1)
- **Published**: 2022-08-05 06:54:44+00:00
- **Updated**: 2022-08-05 06:54:44+00:00
- **Authors**: Yongsong Huang, Qingzhong Wang, Shinichiro Omachi
- **Comment**: Accepted to MICCAI 2022 Workshop. Code:
  https://github.com/yongsongH/AIDSRGAN-MICCAI2022
- **Journal**: None
- **Summary**: In this paper, we present a medical AttentIon Denoising Super Resolution Generative Adversarial Network (AID-SRGAN) for diographic image super-resolution. First, we present a medical practical degradation model that considers various degradation factors beyond downsampling. To the best of our knowledge, this is the first composite degradation model proposed for radiographic images. Furthermore, we propose AID-SRGAN, which can simultaneously denoise and generate high-resolution (HR) radiographs. In this model, we introduce an attention mechanism into the denoising module to make it more robust to complicated degradation. Finally, the SR module reconstructs the HR radiographs using the "clean" low-resolution (LR) radiographs. In addition, we propose a separate-joint training approach to train the model, and extensive experiments are conducted to show that the proposed method is superior to its counterparts. e.g., our proposed method achieves $31.90$ of PSNR with a scale factor of $4 \times$, which is $7.05 \%$ higher than that obtained by recent work, SPSR [16]. Our dataset and code will be made available at: https://github.com/yongsongH/AIDSRGAN-MICCAI2022.



### Learning Spatiotemporal Frequency-Transformer for Compressed Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2208.03012v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03012v1)
- **Published**: 2022-08-05 07:02:30+00:00
- **Updated**: 2022-08-05 07:02:30+00:00
- **Authors**: Zhongwei Qiu, Huan Yang, Jianlong Fu, Dongmei Fu
- **Comment**: ECCV2022
- **Journal**: None
- **Summary**: Compressed video super-resolution (VSR) aims to restore high-resolution frames from compressed low-resolution counterparts. Most recent VSR approaches often enhance an input frame by borrowing relevant textures from neighboring video frames. Although some progress has been made, there are grand challenges to effectively extract and transfer high-quality textures from compressed videos where most frames are usually highly degraded. In this paper, we propose a novel Frequency-Transformer for compressed video super-resolution (FTVSR) that conducts self-attention over a joint space-time-frequency domain. First, we divide a video frame into patches, and transform each patch into DCT spectral maps in which each channel represents a frequency band. Such a design enables a fine-grained level self-attention on each frequency band, so that real visual texture can be distinguished from artifacts, and further utilized for video frame restoration. Second, we study different self-attention schemes, and discover that a divided attention which conducts a joint space-frequency attention before applying temporal attention on each frequency band, leads to the best video enhancement quality. Experimental results on two widely-used video super-resolution benchmarks show that FTVSR outperforms state-of-the-art approaches on both uncompressed and compressed videos with clear visual margins. Code is available at https://github.com/researchmm/FTVSR.



### Calibrate the inter-observer segmentation uncertainty via diagnosis-first principle
- **Arxiv ID**: http://arxiv.org/abs/2208.03016v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.03016v1)
- **Published**: 2022-08-05 07:12:24+00:00
- **Updated**: 2022-08-05 07:12:24+00:00
- **Authors**: Junde Wu, Huihui Fang, Hoayi Xiong, Lixin Duan, Mingkui Tan, Weihua Yang, Huiying Liu, Yanwu Xu
- **Comment**: arXiv admin note: text overlap with arXiv:2202.06505
- **Journal**: None
- **Summary**: On the medical images, many of the tissues/lesions may be ambiguous. That is why the medical segmentation is typically annotated by a group of clinical experts to mitigate the personal bias. However, this clinical routine also brings new challenges to the application of machine learning algorithms. Without a definite ground-truth, it will be difficult to train and evaluate the deep learning models. When the annotations are collected from different graders, a common choice is majority vote. However such a strategy ignores the difference between the grader expertness. In this paper, we consider the task of predicting the segmentation with the calibrated inter-observer uncertainty. We note that in clinical practice, the medical image segmentation is usually used to assist the disease diagnosis. Inspired by this observation, we propose diagnosis-first principle, which is to take disease diagnosis as the criterion to calibrate the inter-observer segmentation uncertainty. Following this idea, a framework named Diagnosis First segmentation Framework (DiFF) is proposed to estimate diagnosis-first segmentation from the raw images.Specifically, DiFF will first learn to fuse the multi-rater segmentation labels to a single ground-truth which could maximize the disease diagnosis performance. We dubbed the fused ground-truth as Diagnosis First Ground-truth (DF-GT).Then, we further propose Take and Give Modelto segment DF-GT from the raw image. We verify the effectiveness of DiFF on three different medical segmentation tasks: OD/OC segmentation on fundus images, thyroid nodule segmentation on ultrasound images, and skin lesion segmentation on dermoscopic images. Experimental results show that the proposed DiFF is able to significantly facilitate the corresponding disease diagnosis, which outperforms previous state-of-the-art multi-rater learning methods.



### Deep Bayesian Active-Learning-to-Rank for Endoscopic Image Data
- **Arxiv ID**: http://arxiv.org/abs/2208.03020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03020v1)
- **Published**: 2022-08-05 07:22:08+00:00
- **Updated**: 2022-08-05 07:22:08+00:00
- **Authors**: Takeaki Kadota, Hideaki Hayashi, Ryoma Bise, Kiyohito Tanaka, Seiichi Uchida
- **Comment**: 14 pages, 8 figures, accepted at MIUA 2022
- **Journal**: None
- **Summary**: Automatic image-based disease severity estimation generally uses discrete (i.e., quantized) severity labels. Annotating discrete labels is often difficult due to the images with ambiguous severity. An easier alternative is to use relative annotation, which compares the severity level between image pairs. By using a learning-to-rank framework with relative annotation, we can train a neural network that estimates rank scores that are relative to severity levels. However, the relative annotation for all possible pairs is prohibitive, and therefore, appropriate sample pair selection is mandatory. This paper proposes a deep Bayesian active-learning-to-rank, which trains a Bayesian convolutional neural network while automatically selecting appropriate pairs for relative annotation. We confirmed the efficiency of the proposed method through experiments on endoscopic images of ulcerative colitis. In addition, we confirmed that our method is useful even with the severe class imbalance because of its ability to select samples from minor classes automatically.



### Multimodal Brain Disease Classification with Functional Interaction Learning from Single fMRI Volume
- **Arxiv ID**: http://arxiv.org/abs/2208.03028v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.03028v3)
- **Published**: 2022-08-05 07:54:10+00:00
- **Updated**: 2023-03-01 06:54:58+00:00
- **Authors**: Wei Dai, Ziyao Zhang, Lixia Tian, Shengyuan Yu, Shuhui Wang, Zhao Dong, Hairong Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: In neuroimaging analysis, fMRI can well assess the function changes for brain diseases with no obvious structural lesions. To date, most deep-learning-based fMRI studies have employed functional connectivity (FC) as the basic feature for disease classification. However, FC is calculated on time series of predefined regions of interest and neglects detailed information contained in each voxel. Another drawback of using FC is the limited sample size for the training of deep models. The low representation ability of FC leads to poor performance in clinical practice, especially when dealing with multimodal medical data involving multiple types of visual signals and textual records for brain diseases. To overcome this bottleneck problem in the fMRI feature modality, we propose BrainFormer, an end-to-end functional interaction learning method for brain disease classification with single fMRI volume. Unlike traditional deep learning methods that construct convolution and transformers on FC, BrainFormer learns the functional interaction from fMRI signals, by modeling the local cues within each voxel with 3D convolutions and capturing the global correlations among distant regions with specially designed global attention mechanisms from shallow layers to deep layers. Meanwhile, BrainFormer can deal with multimodal medical data including fMRI volume, structural MRI, FC features and phenotypic data to achieve more comprehensive brain disease diagnosis. We evaluate BrainFormer on five independent multi-site datasets on autism, Alzheimer's disease, depression, attention deficit hyperactivity disorder and headache disorders. The results demonstrate its effectiveness and generalizability for multiple brain diseases diagnosis with multimodal features. BrainFormer may promote precision of neuroimaging-based diagnosis in clinical practice and motivate future studies on fMRI analysis.



### ChiQA: A Large Scale Image-based Real-World Question Answering Dataset for Multi-Modal Understanding
- **Arxiv ID**: http://arxiv.org/abs/2208.03030v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.03030v1)
- **Published**: 2022-08-05 07:55:28+00:00
- **Updated**: 2022-08-05 07:55:28+00:00
- **Authors**: Bingning Wang, Feiyang Lv, Ting Yao, Yiming Yuan, Jin Ma, Yu Luo, Haijin Liang
- **Comment**: CIKM2022 camera ready version
- **Journal**: None
- **Summary**: Visual question answering is an important task in both natural language and vision understanding. However, in most of the public visual question answering datasets such as VQA, CLEVR, the questions are human generated that specific to the given image, such as `What color are her eyes?'. The human generated crowdsourcing questions are relatively simple and sometimes have the bias toward certain entities or attributes. In this paper, we introduce a new question answering dataset based on image-ChiQA. It contains the real-world queries issued by internet users, combined with several related open-domain images. The system should determine whether the image could answer the question or not. Different from previous VQA datasets, the questions are real-world image-independent queries that are more various and unbiased. Compared with previous image-retrieval or image-caption datasets, the ChiQA not only measures the relatedness but also measures the answerability, which demands more fine-grained vision and language reasoning. ChiQA contains more than 40K questions and more than 200K question-images pairs. A three-level 2/1/0 label is assigned to each pair indicating perfect answer, partially answer and irrelevant. Data analysis shows ChiQA requires a deep understanding of both language and vision, including grounding, comparisons, and reading. We evaluate several state-of-the-art visual-language models such as ALBEF, demonstrating that there is still a large room for improvements on ChiQA.



### Blockwise Temporal-Spatial Pathway Network
- **Arxiv ID**: http://arxiv.org/abs/2208.03040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03040v1)
- **Published**: 2022-08-05 08:43:30+00:00
- **Updated**: 2022-08-05 08:43:30+00:00
- **Authors**: SeulGi Hong, Min-Kook Choi
- **Comment**: ICIP 2021
- **Journal**: None
- **Summary**: Algorithms for video action recognition should consider not only spatial information but also temporal relations, which remains challenging. We propose a 3D-CNN-based action recognition model, called the blockwise temporal-spatial path-way network (BTSNet), which can adjust the temporal and spatial receptive fields by multiple pathways. We designed a novel model inspired by an adaptive kernel selection-based model, which is an architecture for effective feature encoding that adaptively chooses spatial receptive fields for image recognition. Expanding this approach to the temporal domain, our model extracts temporal and channel-wise attention and fuses information on various candidate operations. For evaluation, we tested our proposed model on UCF-101, HMDB-51, SVW, and Epic-Kitchen datasets and showed that it generalized well without pretraining. BTSNet also provides interpretable visualization based on spatiotemporal channel-wise attention. We confirm that the blockwise temporal-spatial pathway supports a better representation for 3D convolutional blocks based on this visualization.



### Low-Light Hyperspectral Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2208.03042v1
- **DOI**: 10.1109/TGRS.2022.3201206
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.03042v1)
- **Published**: 2022-08-05 08:45:52+00:00
- **Updated**: 2022-08-05 08:45:52+00:00
- **Authors**: Xuelong Li, Guanlin Li, Bin Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Due to inadequate energy captured by the hyperspectral camera sensor in poor illumination conditions, low-light hyperspectral images (HSIs) usually suffer from low visibility, spectral distortion, and various noises. A range of HSI restoration methods have been developed, yet their effectiveness in enhancing low-light HSIs is constrained. This work focuses on the low-light HSI enhancement task, which aims to reveal the spatial-spectral information hidden in darkened areas. To facilitate the development of low-light HSI processing, we collect a low-light HSI (LHSI) dataset of both indoor and outdoor scenes. Based on Laplacian pyramid decomposition and reconstruction, we developed an end-to-end data-driven low-light HSI enhancement (HSIE) approach trained on the LHSI dataset. With the observation that illumination is related to the low-frequency component of HSI, while textural details are closely correlated to the high-frequency component, the proposed HSIE is designed to have two branches. The illumination enhancement branch is adopted to enlighten the low-frequency component with reduced resolution. The high-frequency refinement branch is utilized for refining the high-frequency component via a predicted mask. In addition, to improve information flow and boost performance, we introduce an effective channel attention block (CAB) with residual dense connection, which served as the basic block of the illumination enhancement branch. The effectiveness and efficiency of HSIE both in quantitative assessment measures and visual effects are demonstrated by experimental results on the LHSI dataset. According to the classification performance on the remote sensing Indian Pines dataset, downstream tasks benefit from the enhanced HSI. Datasets and codes are available: \href{https://github.com/guanguanboy/HSIE}{https://github.com/guanguanboy/HSIE}.



### Hybrid Multimodal Feature Extraction, Mining and Fusion for Sentiment Analysis
- **Arxiv ID**: http://arxiv.org/abs/2208.03051v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.03051v2)
- **Published**: 2022-08-05 09:07:58+00:00
- **Updated**: 2022-08-12 09:58:06+00:00
- **Authors**: Jia Li, Ziyang Zhang, Junjie Lang, Yueqi Jiang, Liuwei An, Peng Zou, Yangyang Xu, Sheng Gao, Jie Lin, Chunxiao Fan, Xiao Sun, Meng Wang
- **Comment**: 8 pages, 2 figures, to appear in MuSe 2022 (ACM MM2022 co-located
  workshop)
- **Journal**: None
- **Summary**: In this paper, we present our solutions for the Multimodal Sentiment Analysis Challenge (MuSe) 2022, which includes MuSe-Humor, MuSe-Reaction and MuSe-Stress Sub-challenges. The MuSe 2022 focuses on humor detection, emotional reactions and multimodal emotional stress utilizing different modalities and data sets. In our work, different kinds of multimodal features are extracted, including acoustic, visual, text and biological features. These features are fused by TEMMA and GRU with self-attention mechanism frameworks. In this paper, 1) several new audio features, facial expression features and paragraph-level text embeddings are extracted for accuracy improvement. 2) we substantially improve the accuracy and reliability of multimodal sentiment prediction by mining and blending the multimodal features. 3) effective data augmentation strategies are applied in model training to alleviate the problem of sample imbalance and prevent the model from learning biased subject characters. For the MuSe-Humor sub-challenge, our model obtains the AUC score of 0.8932. For the MuSe-Reaction sub-challenge, the Pearson's Correlations Coefficient of our approach on the test set is 0.3879, which outperforms all other participants. For the MuSe-Stress sub-challenge, our approach outperforms the baseline in both arousal and valence on the test dataset, reaching a final combined result of 0.5151.



### Sinusoidal Sensitivity Calculation for Line Segment Geometries
- **Arxiv ID**: http://arxiv.org/abs/2208.03059v2
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.03059v2)
- **Published**: 2022-08-05 09:30:55+00:00
- **Updated**: 2022-08-17 17:17:51+00:00
- **Authors**: Luciano Vinas, Atchar Sudyadhom
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Provide a closed-form solution to the sinusoidal coil sensitivity model proposed by Kern et al. Solution allows for the precise computations of varied, simulated bias fields which can be directly applied onto raw intensity datasets.   Methods: Fourier distribution theory and standard integration techniques were used to calculate the Fourier transform of measured magnetic field produced from line segment sources.   Results: A $L^1_{\rm loc}(\mathbb{R}^3)$ function is derived in full generality for arbitrary line segment geometries. Sampling criteria and equivalence to the original sinusoidal model are discussed. Lastly a CUDA accelerated implementation $\texttt{biasgen}$ is provided for on-demand sensitivity and bias generation.   Conclusion: Given the modeling flexibility of the simulated procedure, practitioners will now have access to a more diverse ecosystem of simulated datasets which may be used to quantitatively compare prospective debiasing methods.



### Exploring Resolution and Degradation Clues as Self-supervised Signal for Low Quality Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.03062v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03062v1)
- **Published**: 2022-08-05 09:36:13+00:00
- **Updated**: 2022-08-05 09:36:13+00:00
- **Authors**: Ziteng Cui, Yingying Zhu, Lin Gu, Guo-Jun Qi, Xiaoxiao Li, Renrui Zhang, Zenghui Zhang, Tatsuya Harada
- **Comment**: Accepted by ECCV 2022. arXiv admin note: substantial text overlap
  with arXiv:2201.02314
- **Journal**: None
- **Summary**: Image restoration algorithms such as super resolution (SR) are indispensable pre-processing modules for object detection in low quality images. Most of these algorithms assume the degradation is fixed and known a priori. However, in practical, either the real degradation or optimal up-sampling ratio rate is unknown or differs from assumption, leading to a deteriorating performance for both the pre-processing module and the consequent high-level task such as object detection. Here, we propose a novel self-supervised framework to detect objects in degraded low resolution images. We utilizes the downsampling degradation as a kind of transformation for self-supervised signals to explore the equivariant representation against various resolutions and other degradation conditions. The Auto Encoding Resolution in Self-supervision (AERIS) framework could further take the advantage of advanced SR architectures with an arbitrary resolution restoring decoder to reconstruct the original correspondence from the degraded input image. Both the representation learning and object detection are optimized jointly in an end-to-end training fashion. The generic AERIS framework could be implemented on various mainstream object detection architectures with different backbones. The extensive experiments show that our methods has achieved superior performance compared with existing methods when facing variant degradation situations. Code would be released at https://github.com/cuiziteng/ECCV_AERIS.



### Instance As Identity: A Generic Online Paradigm for Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.03079v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03079v2)
- **Published**: 2022-08-05 10:29:30+00:00
- **Updated**: 2022-08-16 00:47:09+00:00
- **Authors**: Feng Zhu, Zongxin Yang, Xin Yu, Yi Yang, Yunchao Wei
- **Comment**: Accepted to ECCV2022
- **Journal**: None
- **Summary**: Modeling temporal information for both detection and tracking in a unified framework has been proved a promising solution to video instance segmentation (VIS). However, how to effectively incorporate the temporal information into an online model remains an open problem. In this work, we propose a new online VIS paradigm named Instance As Identity (IAI), which models temporal information for both detection and tracking in an efficient way. In detail, IAI employs a novel identification module to predict identification number for tracking instances explicitly. For passing temporal information cross frame, IAI utilizes an association module which combines current features and past embeddings. Notably, IAI can be integrated with different image models. We conduct extensive experiments on three VIS benchmarks. IAI outperforms all the online competitors on YouTube-VIS-2019 (ResNet-101 43.7 mAP) and YouTube-VIS-2021 (ResNet-50 38.0 mAP). Surprisingly, on the more challenging OVIS, IAI achieves SOTA performance (20.6 mAP). Code is available at https://github.com/zfonemore/IAI



### MorDeephy: Face Morphing Detection Via Fused Classification
- **Arxiv ID**: http://arxiv.org/abs/2208.03110v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03110v1)
- **Published**: 2022-08-05 11:39:22+00:00
- **Updated**: 2022-08-05 11:39:22+00:00
- **Authors**: Iurii Medvedev, Farhad Shadmand, Nuno Gonçalves
- **Comment**: 10 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Face morphing attack detection (MAD) is one of the most challenging tasks in the field of face recognition nowadays. In this work, we introduce a novel deep learning strategy for a single image face morphing detection, which implies the discrimination of morphed face images along with a sophisticated face recognition task in a complex classification scheme. It is directed onto learning the deep facial features, which carry information about the authenticity of these features. Our work also introduces several additional contributions: the public and easy-to-use face morphing detection benchmark and the results of our wild datasets filtering strategy. Our method, which we call MorDeephy, achieved the state of the art performance and demonstrated a prominent ability for generalising the task of morphing detection to unseen scenarios.



### A Lightweight Machine Learning Pipeline for LiDAR-simulation
- **Arxiv ID**: http://arxiv.org/abs/2208.03130v1
- **DOI**: 10.5220/0011309100003277
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.03130v1)
- **Published**: 2022-08-05 12:45:53+00:00
- **Updated**: 2022-08-05 12:45:53+00:00
- **Authors**: Richard Marcus, Niklas Knoop, Bernhard Egger, Marc Stamminger
- **Comment**: Conference: DeLTA 22; ISBN 978-989-758-584-5; ISSN 2184-9277;
  publisher: SciTePress, organization: INSTICC
- **Journal**: Proceedings of the 3rd International Conference on Deep Learning
  Theory and Applications - DeLTA, 2022, pages 176-183
- **Summary**: Virtual testing is a crucial task to ensure safety in autonomous driving, and sensor simulation is an important task in this domain. Most current LiDAR simulations are very simplistic and are mainly used to perform initial tests, while the majority of insights are gathered on the road. In this paper, we propose a lightweight approach for more realistic LiDAR simulation that learns a real sensor's behavior from test drive data and transforms this to the virtual domain. The central idea is to cast the simulation into an image-to-image translation problem. We train our pix2pix based architecture on two real world data sets, namely the popular KITTI data set and the Audi Autonomous Driving Dataset which provide both, RGB and LiDAR images. We apply this network on synthetic renderings and show that it generalizes sufficiently from real images to simulated images. This strategy enables to skip the sensor-specific, expensive and complex LiDAR physics simulation in our synthetic world and avoids oversimplification and a large domain-gap through the clean synthetic environment.



### BoxShrink: From Bounding Boxes to Segmentation Masks
- **Arxiv ID**: http://arxiv.org/abs/2208.03142v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.03142v1)
- **Published**: 2022-08-05 13:07:51+00:00
- **Updated**: 2022-08-05 13:07:51+00:00
- **Authors**: Michael Gröger, Vadim Borisov, Gjergji Kasneci
- **Comment**: None
- **Journal**: None
- **Summary**: One of the core challenges facing the medical image computing community is fast and efficient data sample labeling. Obtaining fine-grained labels for segmentation is particularly demanding since it is expensive, time-consuming, and requires sophisticated tools. On the contrary, applying bounding boxes is fast and takes significantly less time than fine-grained labeling, but does not produce detailed results. In response, we propose a novel framework for weakly-supervised tasks with the rapid and robust transformation of bounding boxes into segmentation masks without training any machine learning model, coined BoxShrink. The proposed framework comes in two variants - rapid-BoxShrink for fast label transformations, and robust-BoxShrink for more precise label transformations. An average of four percent improvement in IoU is found across several models when being trained using BoxShrink in a weakly-supervised setting, compared to using only bounding box annotations as inputs on a colonoscopy image data set. We open-sourced the code for the proposed framework and published it online.



### Adversarial Robustness of MR Image Reconstruction under Realistic Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2208.03161v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.03161v1)
- **Published**: 2022-08-05 13:39:40+00:00
- **Updated**: 2022-08-05 13:39:40+00:00
- **Authors**: Jan Nikolas Morshuis, Sergios Gatidis, Matthias Hein, Christian F. Baumgartner
- **Comment**: Accepted at the MICCAI-2022 workshop: Machine Learning for Medical
  Image Reconstruction
- **Journal**: None
- **Summary**: Deep Learning (DL) methods have shown promising results for solving ill-posed inverse problems such as MR image reconstruction from undersampled $k$-space data. However, these approaches currently have no guarantees for reconstruction quality and the reliability of such algorithms is only poorly understood. Adversarial attacks offer a valuable tool to understand possible failure modes and worst case performance of DL-based reconstruction algorithms. In this paper we describe adversarial attacks on multi-coil $k$-space measurements and evaluate them on the recently proposed E2E-VarNet and a simpler UNet-based model. In contrast to prior work, the attacks are targeted to specifically alter diagnostically relevant regions. Using two realistic attack models (adversarial $k$-space noise and adversarial rotations) we are able to show that current state-of-the-art DL-based reconstruction algorithms are indeed sensitive to such perturbations to a degree where relevant diagnostic information may be lost. Surprisingly, in our experiments the UNet and the more sophisticated E2E-VarNet were similarly sensitive to such attacks. Our findings add further to the evidence that caution must be exercised as DL-based methods move closer to clinical practice.



### Discover the Mysteries of the Maya: Selected Contributions from the Machine Learning Challenge & The Discovery Challenge Workshop at ECML PKDD 2021
- **Arxiv ID**: http://arxiv.org/abs/2208.03163v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.03163v2)
- **Published**: 2022-08-05 13:41:31+00:00
- **Updated**: 2022-08-09 12:54:34+00:00
- **Authors**: Dragi Kocev, Nikola Simidjievski, Ana Kostovska, Ivica Dimitrovski, Žiga Kokalj
- **Comment**: Chapter authors. Chapter 1: Matthew Painter and Iris Kramer; Chapter
  2: J\"urgen Landauer, Burkhard Hoppenstedt, and Johannes Allgaier; Chapter 3:
  Thorben Hellweg, Stefan Oehmcke, Ankit Kariryaa, Fabian Gieseke, and
  Christian Igel; Chapter 4: Christian Ayala, Carlos Aranda, and Mikel Galar
- **Journal**: None
- **Summary**: The volume contains selected contributions from the Machine Learning Challenge "Discover the Mysteries of the Maya", presented at the Discovery Challenge Track of The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2021).   Remote sensing has greatly accelerated traditional archaeological landscape surveys in the forested regions of the ancient Maya. Typical exploration and discovery attempts, beside focusing on whole ancient cities, focus also on individual buildings and structures. Recently, there have been several successful attempts of utilizing machine learning for identifying ancient Maya settlements. These attempts, while relevant, focus on narrow areas and rely on high-quality aerial laser scanning (ALS) data which covers only a fraction of the region where ancient Maya were once settled. Satellite image data, on the other hand, produced by the European Space Agency's (ESA) Sentinel missions, is abundant and, more importantly, publicly available. The "Discover the Mysteries of the Maya" challenge aimed at locating and identifying ancient Maya architectures (buildings, aguadas, and platforms) by performing integrated image segmentation of different types of satellite imagery (from Sentinel-1 and Sentinel-2) data and ALS (lidar) data.



### Disentangling 3D Attributes from a Single 2D Image: Human Pose, Shape and Garment
- **Arxiv ID**: http://arxiv.org/abs/2208.03167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03167v1)
- **Published**: 2022-08-05 13:48:43+00:00
- **Updated**: 2022-08-05 13:48:43+00:00
- **Authors**: Xue Hu, Xinghui Li, Benjamin Busam, Yiren Zhou, Ales Leonardis, Shanxin Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: For visual manipulation tasks, we aim to represent image content with semantically meaningful features. However, learning implicit representations from images often lacks interpretability, especially when attributes are intertwined. We focus on the challenging task of extracting disentangled 3D attributes only from 2D image data. Specifically, we focus on human appearance and learn implicit pose, shape and garment representations of dressed humans from RGB images. Our method learns an embedding with disentangled latent representations of these three image properties and enables meaningful re-assembling of features and property control through a 2D-to-3D encoder-decoder structure. The 3D model is inferred solely from the feature map in the learned embedding space. To the best of our knowledge, our method is the first to achieve cross-domain disentanglement for this highly under-constrained problem. We qualitatively and quantitatively demonstrate our framework's ability to transfer pose, shape, and garments in 3D reconstruction on virtual data and show how an implicit shape loss can benefit the model's ability to recover fine-grained reconstruction details.



### Brain Lesion Synthesis via Progressive Adversarial Variational Auto-Encoder
- **Arxiv ID**: http://arxiv.org/abs/2208.03203v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.03203v1)
- **Published**: 2022-08-05 14:39:06+00:00
- **Updated**: 2022-08-05 14:39:06+00:00
- **Authors**: Jiayu Huo, Vejay Vakharia, Chengyuan Wu, Ashwini Sharan, Andrew Ko, Sebastien Ourselin, Rachel Sparks
- **Comment**: 11 pages, 4 figures, accepted by International Workshop on Simulation
  and Synthesis in Medical Imaging (SASHIMI 2022)
- **Journal**: None
- **Summary**: Laser interstitial thermal therapy (LITT) is a novel minimally invasive treatment that is used to ablate intracranial structures to treat mesial temporal lobe epilepsy (MTLE). Region of interest (ROI) segmentation before and after LITT would enable automated lesion quantification to objectively assess treatment efficacy. Deep learning techniques, such as convolutional neural networks (CNNs) are state-of-the-art solutions for ROI segmentation, but require large amounts of annotated data during the training. However, collecting large datasets from emerging treatments such as LITT is impractical. In this paper, we propose a progressive brain lesion synthesis framework (PAVAE) to expand both the quantity and diversity of the training dataset. Concretely, our framework consists of two sequential networks: a mask synthesis network and a mask-guided lesion synthesis network. To better employ extrinsic information to provide additional supervision during network training, we design a condition embedding block (CEB) and a mask embedding block (MEB) to encode inherent conditions of masks to the feature space. Finally, a segmentation network is trained using raw and synthetic lesion images to evaluate the effectiveness of the proposed framework. Experimental results show that our method can achieve realistic synthetic results and boost the performance of down-stream segmentation tasks above traditional data augmentation techniques.



### Task-agnostic Continual Hippocampus Segmentation for Smooth Population Shifts
- **Arxiv ID**: http://arxiv.org/abs/2208.03206v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.03206v1)
- **Published**: 2022-08-05 14:46:00+00:00
- **Updated**: 2022-08-05 14:46:00+00:00
- **Authors**: Camila Gonzalez, Amin Ranem, Ahmed Othman, Anirban Mukhopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Most continual learning methods are validated in settings where task boundaries are clearly defined and task identity information is available during training and testing. We explore how such methods perform in a task-agnostic setting that more closely resembles dynamic clinical environments with gradual population shifts. We propose ODEx, a holistic solution that combines out-of-distribution detection with continual learning techniques. Validation on two scenarios of hippocampus segmentation shows that our proposed method reliably maintains performance on earlier tasks without losing plasticity.



### Neighborhood Collective Estimation for Noisy Label Identification and Correction
- **Arxiv ID**: http://arxiv.org/abs/2208.03207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03207v1)
- **Published**: 2022-08-05 14:47:22+00:00
- **Updated**: 2022-08-05 14:47:22+00:00
- **Authors**: Jichang Li, Guanbin Li, Feng Liu, Yizhou Yu
- **Comment**: Accepted to European Conference on Computer Vision (ECCV), 2022
- **Journal**: None
- **Summary**: Learning with noisy labels (LNL) aims at designing strategies to improve model performance and generalization by mitigating the effects of model overfitting to noisy labels. The key success of LNL lies in identifying as many clean samples as possible from massive noisy data, while rectifying the wrongly assigned noisy labels. Recent advances employ the predicted label distributions of individual samples to perform noise verification and noisy label correction, easily giving rise to confirmation bias. To mitigate this issue, we propose Neighborhood Collective Estimation, in which the predictive reliability of a candidate sample is re-estimated by contrasting it against its feature-space nearest neighbors. Specifically, our method is divided into two steps: 1) Neighborhood Collective Noise Verification to separate all training samples into a clean or noisy subset, 2) Neighborhood Collective Label Correction to relabel noisy samples, and then auxiliary techniques are used to assist further model optimization. Extensive experiments on four commonly used benchmark datasets, i.e., CIFAR-10, CIFAR-100, Clothing-1M and Webvision-1.0, demonstrate that our proposed method considerably outperforms state-of-the-art methods.



### Bias and Fairness in Computer Vision Applications of the Criminal Justice System
- **Arxiv ID**: http://arxiv.org/abs/2208.03209v1
- **DOI**: 10.1109/SSCI50451.2021.9660177
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03209v1)
- **Published**: 2022-08-05 14:48:52+00:00
- **Updated**: 2022-08-05 14:48:52+00:00
- **Authors**: Sophie Noiret, Jennifer Lumetzberger, Martin Kampel
- **Comment**: \c{opyright} 2021 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: Discriminatory practices involving AI-driven police work have been the subject of much controversies in the past few years, with algorithms such as COMPAS, PredPol and ShotSpotter being accused of unfairly impacting minority groups. At the same time, the issues of fairness in machine learning, and in particular in computer vision, have been the subject of a growing number of academic works. In this paper, we examine how these area intersect. We provide information on how these practices have come to exist and the difficulties in alleviating them. We then examine three applications currently in development to understand what risks they pose to fairness and how those risks can be mitigated.



### Distance-based detection of out-of-distribution silent failures for Covid-19 lung lesion segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.03217v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.03217v1)
- **Published**: 2022-08-05 15:05:23+00:00
- **Updated**: 2022-08-05 15:05:23+00:00
- **Authors**: Camila Gonzalez, Karol Gotkowski, Moritz Fuchs, Andreas Bucher, Armin Dadras, Ricarda Fischbach, Isabel Kaltenborn, Anirban Mukhopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic segmentation of ground glass opacities and consolidations in chest computer tomography (CT) scans can potentially ease the burden of radiologists during times of high resource utilisation. However, deep learning models are not trusted in the clinical routine due to failing silently on out-of-distribution (OOD) data. We propose a lightweight OOD detection method that leverages the Mahalanobis distance in the feature space and seamlessly integrates into state-of-the-art segmentation pipelines. The simple approach can even augment pre-trained models with clinically relevant uncertainty quantification. We validate our method across four chest CT distribution shifts and two magnetic resonance imaging applications, namely segmentation of the hippocampus and the prostate. Our results show that the proposed method effectively detects far- and near-OOD samples across all explored scenarios.



### RadTex: Learning Efficient Radiograph Representations from Text Reports
- **Arxiv ID**: http://arxiv.org/abs/2208.03218v2
- **DOI**: 10.1007/978-3-031-16876-5_3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03218v2)
- **Published**: 2022-08-05 15:06:26+00:00
- **Updated**: 2023-04-07 13:50:21+00:00
- **Authors**: Keegan Quigley, Miriam Cha, Ruizhi Liao, Geeticka Chauhan, Steven Horng, Seth Berkowitz, Polina Golland
- **Comment**: Awarded Best Paper at Resource Efficient Medical Image Analysis
  (REMIA) Workshop, MICCAI 2022
- **Journal**: None
- **Summary**: Automated analysis of chest radiography using deep learning has tremendous potential to enhance the clinical diagnosis of diseases in patients. However, deep learning models typically require large amounts of annotated data to achieve high performance -- often an obstacle to medical domain adaptation. In this paper, we build a data-efficient learning framework that utilizes radiology reports to improve medical image classification performance with limited labeled data (fewer than 1000 examples). Specifically, we examine image-captioning pretraining to learn high-quality medical image representations that train on fewer examples. Following joint pretraining of a convolutional encoder and transformer decoder, we transfer the learned encoder to various classification tasks. Averaged over 9 pathologies, we find that our model achieves higher classification performance than ImageNet-supervised and in-domain supervised pretraining when labeled training data is limited.



### Driving Points Prediction For Abdominal Probabilistic Registration
- **Arxiv ID**: http://arxiv.org/abs/2208.03232v1
- **DOI**: 10.1007/978-3-031-21014-3_30
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03232v1)
- **Published**: 2022-08-05 15:28:01+00:00
- **Updated**: 2022-08-05 15:28:01+00:00
- **Authors**: Samuel Joutard, Reuben Dorent, Sebastien Ourselin, Tom Vercauteren, Marc Modat
- **Comment**: None
- **Journal**: None
- **Summary**: Inter-patient abdominal registration has various applications, from pharmakinematic studies to anatomy modeling. Yet, it remains a challenging application due to the morphological heterogeneity and variability of the human abdomen. Among the various registration methods proposed for this task, probabilistic displacement registration models estimate displacement distribution for a subset of points by comparing feature vectors of points from the two images. These probabilistic models are informative and robust while allowing large displacements by design. As the displacement distributions are typically estimated on a subset of points (which we refer to as driving points), due to computational requirements, we propose in this work to learn a driving points predictor. Compared to previously proposed methods, the driving points predictor is optimized in an end-to-end fashion to infer driving points tailored for a specific registration pipeline. We evaluate the impact of our contribution on two different datasets corresponding to different modalities. Specifically, we compared the performances of 6 different probabilistic displacement registration models when using a driving points predictor or one of 2 other standard driving points selection methods. The proposed method improved performances in 11 out of 12 experiments.



### Real-time Gesture Animation Generation from Speech for Virtual Human Interaction
- **Arxiv ID**: http://arxiv.org/abs/2208.03244v1
- **DOI**: 10.1145/3411763.3451554
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03244v1)
- **Published**: 2022-08-05 15:56:34+00:00
- **Updated**: 2022-08-05 15:56:34+00:00
- **Authors**: Manuel Rebol, Christian Gütl, Krzysztof Pietroszek
- **Comment**: CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human
  Factors in Computing Systems. arXiv admin note: text overlap with
  arXiv:2107.00712
- **Journal**: In CHI EA 2021. ACM, New York, NY, USA, Article 197, 1-4
- **Summary**: We propose a real-time system for synthesizing gestures directly from speech. Our data-driven approach is based on Generative Adversarial Neural Networks to model the speech-gesture relationship. We utilize the large amount of speaker video data available online to train our 3D gesture model. Our model generates speaker-specific gestures by taking consecutive audio input chunks of two seconds in length. We animate the predicted gestures on a virtual avatar. We achieve a delay below three seconds between the time of audio input and gesture animation. Code and videos are available at https://github.com/mrebol/Gestures-From-Speech



### 3D Pose Based Feedback for Physical Exercises
- **Arxiv ID**: http://arxiv.org/abs/2208.03257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03257v1)
- **Published**: 2022-08-05 16:15:02+00:00
- **Updated**: 2022-08-05 16:15:02+00:00
- **Authors**: Ziyi Zhao, Sena Kiciroglu, Hugues Vinzant, Yuan Cheng, Isinsu Katircioglu, Mathieu Salzmann, Pascal Fua
- **Comment**: Video: https://youtu.be/W3kyyeHe0SI
- **Journal**: None
- **Summary**: Unsupervised self-rehabilitation exercises and physical training can cause serious injuries if performed incorrectly. We introduce a learning-based framework that identifies the mistakes made by a user and proposes corrective measures for easier and safer individual training. Our framework does not rely on hard-coded, heuristic rules. Instead, it learns them from data, which facilitates its adaptation to specific user needs. To this end, we use a Graph Convolutional Network (GCN) architecture acting on the user's pose sequence to model the relationship between the body joints trajectories. To evaluate our approach, we introduce a dataset with 3 different physical exercises. Our approach yields 90.9% mistake identification accuracy and successfully corrects 94.2% of the mistakes.



### Convolutional Ensembling based Few-Shot Defect Detection Technique
- **Arxiv ID**: http://arxiv.org/abs/2208.03288v3
- **DOI**: 10.1145/3571600.3571607
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.03288v3)
- **Published**: 2022-08-05 17:29:14+00:00
- **Updated**: 2022-11-23 12:06:50+00:00
- **Authors**: Soumyajit Karmakar, Abeer Banerjee, Prashant Sadashiv Gidde, Sumeet Saurav, Sanjay Singh
- **Comment**: 7 pages, 7 images
- **Journal**: None
- **Summary**: Over the past few years, there has been a significant improvement in the domain of few-shot learning. This learning paradigm has shown promising results for the challenging problem of anomaly detection, where the general task is to deal with heavy class imbalance. Our paper presents a new approach to few-shot classification, where we employ the knowledge-base of multiple pre-trained convolutional models that act as the backbone for our proposed few-shot framework. Our framework uses a novel ensembling technique for boosting the accuracy while drastically decreasing the total parameter count, thus paving the way for real-time implementation. We perform an extensive hyperparameter search using a power-line defect detection dataset and obtain an accuracy of 92.30% for the 5-way 5-shot task. Without further tuning, we evaluate our model on competing standards with the existing state-of-the-art methods and outperform them.



### Deep Learning-based Segmentation of Pleural Effusion From Ultrasound Using Coordinate Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2208.03305v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.03305v1)
- **Published**: 2022-08-05 17:45:48+00:00
- **Updated**: 2022-08-05 17:45:48+00:00
- **Authors**: Germain Morilhat, Naomi Kifle, Sandra FinesilverSmith, Bram Ruijsink, Vittoria Vergani, Habtamu Tegegne Desita, Zerubabel Tegegne Desita, Esther Puyol-Anton, Aaron Carass, Andrew P. King
- **Comment**: This paper has been accepted for publication at the MICCAI FAIR
  workshop
- **Journal**: None
- **Summary**: In many low-to-middle income (LMIC) countries, ultrasound is used for assessment of pleural effusion. Typically, the extent of the effusion is manually measured by a sonographer, leading to significant intra-/inter-observer variability. In this work, we investigate the use of deep learning (DL) to automate the process of pleural effusion segmentation from ultrasound images. On two datasets acquired in a LMIC setting, we achieve median Dice Similarity Coefficients (DSCs) of 0.82 and 0.74 respectively using the nnU-net DL model. We also investigate the use of coordinate convolutions in the DL model and find that this results in a statistically significant improvement in the median DSC on the first dataset to 0.85, with no significant change on the second dataset. This work showcases, for the first time, the potential of DL in automating the process of effusion assessment from ultrasound in LMIC settings where there is often a lack of experienced radiologists to perform such tasks.



### Lethal Dose Conjecture on Data Poisoning
- **Arxiv ID**: http://arxiv.org/abs/2208.03309v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2208.03309v3)
- **Published**: 2022-08-05 17:53:59+00:00
- **Updated**: 2022-10-18 19:41:22+00:00
- **Authors**: Wenxiao Wang, Alexander Levine, Soheil Feizi
- **Comment**: 36th Conference on Neural Information Processing Systems (NeurIPS
  2022)
- **Journal**: None
- **Summary**: Data poisoning considers an adversary that distorts the training set of machine learning algorithms for malicious purposes. In this work, we bring to light one conjecture regarding the fundamentals of data poisoning, which we call the Lethal Dose Conjecture. The conjecture states: If $n$ clean training samples are needed for accurate predictions, then in a size-$N$ training set, only $\Theta(N/n)$ poisoned samples can be tolerated while ensuring accuracy. Theoretically, we verify this conjecture in multiple cases. We also offer a more general perspective of this conjecture through distribution discrimination. Deep Partition Aggregation (DPA) and its extension, Finite Aggregation (FA) are recent approaches for provable defenses against data poisoning, where they predict through the majority vote of many base models trained from different subsets of training set using a given learner. The conjecture implies that both DPA and FA are (asymptotically) optimal -- if we have the most data-efficient learner, they can turn it into one of the most robust defenses against data poisoning. This outlines a practical approach to developing stronger defenses against poisoning via finding data-efficient learners. Empirically, as a proof of concept, we show that by simply using different data augmentations for base learners, we can respectively double and triple the certified robustness of DPA on CIFAR-10 and GTSRB without sacrificing accuracy.



### Deep Learning Neural Network for Lung Cancer Classification: Enhanced Optimization Function
- **Arxiv ID**: http://arxiv.org/abs/2208.06353v1
- **DOI**: 10.1007/s11042-022-13566-9
- **Categories**: **eess.IV**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2208.06353v1)
- **Published**: 2022-08-05 18:41:17+00:00
- **Updated**: 2022-08-05 18:41:17+00:00
- **Authors**: Bhoj Raj Pandit, Abeer Alsadoon, P. W. C. Prasad, Sarmad Al Aloussi, Tarik A. Rashid, Omar Hisham Alsadoon, Oday D. Jerew
- **Comment**: 22pages
- **Journal**: Multimedia Tools and Applications, 2022
- **Summary**: Background and Purpose: Convolutional neural network is widely used for image recognition in the medical area at nowadays. However, overall accuracy in predicting lung tumor is low and the processing time is high as the error occurred while reconstructing the CT image. The aim of this work is to increase the overall prediction accuracy along with reducing processing time by using multispace image in pooling layer of convolution neural network. Methodology: The proposed method has the autoencoder system to improve the overall accuracy, and to predict lung cancer by using multispace image in pooling layer of convolution neural network and Adam Algorithm for optimization. First, the CT images were pre-processed by feeding image to the convolution filter and down sampled by using max pooling. Then, features are extracted using the autoencoder model based on convolutional neural network and multispace image reconstruction technique is used to reduce error while reconstructing the image which then results improved accuracy to predict lung nodule. Finally, the reconstructed images are taken as input for SoftMax classifier to classify the CT images. Results: The state-of-art and proposed solutions were processed in Python Tensor Flow and It provides significant increase in accuracy in classification of lung cancer to 99.5 from 98.9 and decrease in processing time from 10 frames/second to 12 seconds/second. Conclusion: The proposed solution provides high classification accuracy along with less processing time compared to the state of art. For future research, large dataset can be implemented, and low pixel image can be processed to evaluate the classification



### A Sketch Is Worth a Thousand Words: Image Retrieval with Text and Sketch
- **Arxiv ID**: http://arxiv.org/abs/2208.03354v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.03354v1)
- **Published**: 2022-08-05 18:43:37+00:00
- **Updated**: 2022-08-05 18:43:37+00:00
- **Authors**: Patsorn Sangkloy, Wittawat Jitkrittum, Diyi Yang, James Hays
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: We address the problem of retrieving images with both a sketch and a text query. We present TASK-former (Text And SKetch transformer), an end-to-end trainable model for image retrieval using a text description and a sketch as input. We argue that both input modalities complement each other in a manner that cannot be achieved easily by either one alone. TASK-former follows the late-fusion dual-encoder approach, similar to CLIP, which allows efficient and scalable retrieval since the retrieval set can be indexed independently of the queries. We empirically demonstrate that using an input sketch (even a poorly drawn one) in addition to text considerably increases retrieval recall compared to traditional text-based image retrieval. To evaluate our approach, we collect 5,000 hand-drawn sketches for images in the test set of the COCO dataset. The collected sketches are available a https://janesjanes.github.io/tsbir/.



### Perceptual Artifacts Localization for Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2208.03357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03357v1)
- **Published**: 2022-08-05 18:50:51+00:00
- **Updated**: 2022-08-05 18:50:51+00:00
- **Authors**: Lingzhi Zhang, Yuqian Zhou, Connelly Barnes, Sohrab Amirghodsi, Zhe Lin, Eli Shechtman, Jianbo Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Image inpainting is an essential task for multiple practical applications like object removal and image editing. Deep GAN-based models greatly improve the inpainting performance in structures and textures within the hole, but might also generate unexpected artifacts like broken structures or color blobs. Users perceive these artifacts to judge the effectiveness of inpainting models, and retouch these imperfect areas to inpaint again in a typical retouching workflow. Inspired by this workflow, we propose a new learning task of automatic segmentation of inpainting perceptual artifacts, and apply the model for inpainting model evaluation and iterative refinement. Specifically, we first construct a new inpainting artifacts dataset by manually annotating perceptual artifacts in the results of state-of-the-art inpainting models. Then we train advanced segmentation networks on this dataset to reliably localize inpainting artifacts within inpainted images. Second, we propose a new interpretable evaluation metric called Perceptual Artifact Ratio (PAR), which is the ratio of objectionable inpainted regions to the entire inpainted area. PAR demonstrates a strong correlation with real user preference. Finally, we further apply the generated masks for iterative image inpainting by combining our approach with multiple recent inpainting methods. Extensive experiments demonstrate the consistent decrease of artifact regions and inpainting quality improvement across the different methods.



### Seamless Iterative Semi-Supervised Correction of Imperfect Labels in Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/2208.03327v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.03327v1)
- **Published**: 2022-08-05 18:52:20+00:00
- **Updated**: 2022-08-05 18:52:20+00:00
- **Authors**: Marawan Elbatel, Christina Bornberg, Manasi Kattel, Enrique Almar, Claudio Marrocco, Alessandro Bria
- **Comment**: To appear at MICCAI 2022 Workshop on Domain Adaptation and
  Representation Transfer (DART)
- **Journal**: None
- **Summary**: In-vitro tests are an alternative to animal testing for the toxicity of medical devices. Detecting cells as a first step, a cell expert evaluates the growth of cells according to cytotoxicity grade under the microscope. Thus, human fatigue plays a role in error making, making the use of deep learning appealing. Due to the high cost of training data annotation, an approach without manual annotation is needed. We propose Seamless Iterative Semi-Supervised correction of Imperfect labels (SISSI), a new method for training object detection models with noisy and missing annotations in a semi-supervised fashion. Our network learns from noisy labels generated with simple image processing algorithms, which are iteratively corrected during self-training. Due to the nature of missing bounding boxes in the pseudo labels, which would negatively affect the training, we propose to train on dynamically generated synthetic-like images using seamless cloning. Our method successfully provides an adaptive early learning correction technique for object detection. The combination of early learning correction that has been applied in classification and semantic segmentation before and synthetic-like image generation proves to be more effective than the usual semi-supervised approach by > 15% AP and > 20% AR across three different readers. Our code is available at https://github.com/marwankefah/SISSI.



### GLASS: Global to Local Attention for Scene-Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/2208.03364v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.03364v1)
- **Published**: 2022-08-05 19:14:43+00:00
- **Updated**: 2022-08-05 19:14:43+00:00
- **Authors**: Roi Ronen, Shahar Tsiper, Oron Anschel, Inbal Lavi, Amir Markovitz, R. Manmatha
- **Comment**: 23 pages, 9 figures, ECCV'22
- **Journal**: None
- **Summary**: In recent years, the dominant paradigm for text spotting is to combine the tasks of text detection and recognition into a single end-to-end framework. Under this paradigm, both tasks are accomplished by operating over a shared global feature map extracted from the input image. Among the main challenges that end-to-end approaches face is the performance degradation when recognizing text across scale variations (smaller or larger text), and arbitrary word rotation angles. In this work, we address these challenges by proposing a novel global-to-local attention mechanism for text spotting, termed GLASS, that fuses together global and local features. The global features are extracted from the shared backbone, preserving contextual information from the entire image, while the local features are computed individually on resized, high-resolution rotated word crops. The information extracted from the local crops alleviates much of the inherent difficulties with scale and word rotation. We show a performance analysis across scales and angles, highlighting improvement over scale and angle extremities. In addition, we introduce an orientation-aware loss term supervising the detection task, and show its contribution to both detection and recognition performance across all angles. Finally, we show that GLASS is general by incorporating it into other leading text spotting architectures, improving their text spotting performance. Our method achieves state-of-the-art results on multiple benchmarks, including the newly released TextOCR.



### A Survey on Visual Map Localization Using LiDARs and Cameras
- **Arxiv ID**: http://arxiv.org/abs/2208.03376v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2208.03376v1)
- **Published**: 2022-08-05 20:11:18+00:00
- **Updated**: 2022-08-05 20:11:18+00:00
- **Authors**: Elhousni Mahdi, Huang Xinming
- **Comment**: Under review
- **Journal**: None
- **Summary**: As the autonomous driving industry is slowly maturing, visual map localization is quickly becoming the standard approach to localize cars as accurately as possible. Owing to the rich data returned by visual sensors such as cameras or LiDARs, researchers are able to build different types of maps with various levels of details, and use them to achieve high levels of vehicle localization accuracy and stability in urban environments. Contrary to the popular SLAM approaches, visual map localization relies on pre-built maps, and is focused solely on improving the localization accuracy by avoiding error accumulation or drift. We define visual map localization as a two-stage process. At the stage of place recognition, the initial position of the vehicle in the map is determined by comparing the visual sensor output with a set of geo-tagged map regions of interest. Subsequently, at the stage of map metric localization, the vehicle is tracked while it moves across the map by continuously aligning the visual sensors' output with the current area of the map that is being traversed. In this paper, we survey, discuss and compare the latest methods for LiDAR based, camera based and cross-modal visual map localization for both stages, in an effort to highlight the strength and weakness of each approach.



### Keys to Better Image Inpainting: Structure and Texture Go Hand in Hand
- **Arxiv ID**: http://arxiv.org/abs/2208.03382v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03382v2)
- **Published**: 2022-08-05 20:42:13+00:00
- **Updated**: 2022-12-02 20:52:30+00:00
- **Authors**: Jitesh Jain, Yuqian Zhou, Ning Yu, Humphrey Shi
- **Comment**: Project page at https://praeclarumjj3.github.io/fcf-inpainting/
- **Journal**: None
- **Summary**: Deep image inpainting has made impressive progress with recent advances in image generation and processing algorithms. We claim that the performance of inpainting algorithms can be better judged by the generated structures and textures. Structures refer to the generated object boundary or novel geometric structures within the hole, while texture refers to high-frequency details, especially man-made repeating patterns filled inside the structural regions. We believe that better structures are usually obtained from a coarse-to-fine GAN-based generator network while repeating patterns nowadays can be better modeled using state-of-the-art high-frequency fast fourier convolutional layers. In this paper, we propose a novel inpainting network combining the advantages of the two designs. Therefore, our model achieves a remarkable visual quality to match state-of-the-art performance in both structure generation and repeating texture synthesis using a single network. Extensive experiments demonstrate the effectiveness of the method, and our conclusions further highlight the two critical factors of image inpainting quality, structures, and textures, as the future design directions of inpainting networks.



### Federated Learning for Medical Applications: A Taxonomy, Current Trends, Challenges, and Future Research Directions
- **Arxiv ID**: http://arxiv.org/abs/2208.03392v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.03392v3)
- **Published**: 2022-08-05 21:41:15+00:00
- **Updated**: 2022-08-15 15:51:29+00:00
- **Authors**: Ashish Rauniyar, Desta Haileselassie Hagos, Debesh Jha, Jan Erik Håkegård, Ulas Bagci, Danda B. Rawat, Vladimir Vlassov
- **Comment**: None
- **Journal**: None
- **Summary**: With the advent of the IoT, AI, and ML/DL algorithms, the data-driven medical application has emerged as a promising tool for designing reliable and scalable diagnostic and prognostic models from medical data. This has attracted a great deal of attention from academia to industry in recent years. This has undoubtedly improved the quality of healthcare delivery. However, these AI-based medical applications still have poor adoption due to their difficulties in satisfying strict security, privacy, and quality of service standards (such as low latency). Moreover, medical data are usually fragmented and private, making it challenging to generate robust results across populations. Recent developments in federated learning (FL) have made it possible to train complex machine-learned models in a distributed manner. Thus, FL has become an active research domain, particularly processing the medical data at the edge of the network in a decentralized way to preserve privacy and security concerns. To this end, this survey paper highlights the current and future of FL technology in medical applications where data sharing is a significant burden. It also review and discuss the current research trends and their outcomes for designing reliable and scalable FL models. We outline the general FL's statistical problems, device challenges, security, privacy concerns, and its potential in the medical domain. Moreover, our study is also focused on medical applications where we highlight the burden of global cancer and the efficient use of FL for the development of computer-aided diagnosis tools for addressing them. We hope that this review serves as a checkpoint that sets forth the existing state-of-the-art works in a thorough manner and offers open problems and future research directions for this field.



### Slice-level Detection of Intracranial Hemorrhage on CT Using Deep Descriptors of Adjacent Slices
- **Arxiv ID**: http://arxiv.org/abs/2208.03403v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03403v2)
- **Published**: 2022-08-05 23:20:37+00:00
- **Updated**: 2023-04-17 18:05:58+00:00
- **Authors**: Dat T. Ngo, Thao T. B. Nguyen, Hieu T. Nguyen, Dung B. Nguyen, Ha Q. Nguyen, Hieu H. Pham
- **Comment**: Accepted for presentation at the 22nd IEEE Statistical Signal
  Processing (SSP) workshop
- **Journal**: None
- **Summary**: The rapid development in representation learning techniques such as deep neural networks and the availability of large-scale, well-annotated medical imaging datasets have to a rapid increase in the use of supervised machine learning in the 3D medical image analysis and diagnosis. In particular, deep convolutional neural networks (D-CNNs) have been key players and were adopted by the medical imaging community to assist clinicians and medical experts in disease diagnosis and treatment. However, training and inferencing deep neural networks such as D-CNN on high-resolution 3D volumes of Computed Tomography (CT) scans for diagnostic tasks pose formidable computational challenges. This challenge raises the need of developing deep learning-based approaches that are robust in learning representations in 2D images, instead 3D scans. In this work, we propose for the first time a new strategy to train \emph{slice-level} classifiers on CT scans based on the descriptors of the adjacent slices along the axis. In particular, each of which is extracted through a convolutional neural network (CNN). This method is applicable to CT datasets with per-slice labels such as the RSNA Intracranial Hemorrhage (ICH) dataset, which aims to predict the presence of ICH and classify it into 5 different sub-types. We obtain a single model in the top 4% best-performing solutions of the RSNA ICH challenge, where model ensembles are allowed. Experiments also show that the proposed method significantly outperforms the baseline model on CQ500. The proposed method is general and can be applied to other 3D medical diagnosis tasks such as MRI imaging. To encourage new advances in the field, we will make our codes and pre-trained model available upon acceptance of the paper.



### A novel deep learning-based approach for sleep apnea detection using single-lead ECG signals
- **Arxiv ID**: http://arxiv.org/abs/2208.03408v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03408v2)
- **Published**: 2022-08-05 23:46:20+00:00
- **Updated**: 2022-09-11 09:45:49+00:00
- **Authors**: Anh-Tu Nguyen, Thao Nguyen, Huy-Khiem Le, Huy-Hieu Pham, Cuong Do
- **Comment**: This work has been accepted for publication by the Asia Pacific
  Signal and Information Processing Association Annual Summit and Conference
  2022 (APSIPA ASC 2022)
- **Journal**: None
- **Summary**: Sleep apnea (SA) is a type of sleep disorder characterized by snoring and chronic sleeplessness, which can lead to serious conditions such as high blood pressure, heart failure, and cardiomyopathy (enlargement of the muscle tissue of the heart). The electrocardiogram (ECG) plays a critical role in identifying SA since it might reveal abnormal cardiac activity. Recent research on ECG-based SA detection has focused on feature engineering techniques that extract specific characteristics from multiple-lead ECG signals and use them as classification model inputs. In this study, a novel method of feature extraction based on the detection of S peaks is proposed to enhance the detection of adjacent SA segments using a single-lead ECG. In particular, ECG features collected from a single lead (V2) are used to identify SA episodes. On the extracted features, a CNN model is trained to detect SA. Experimental results demonstrate that the proposed method detects SA from single-lead ECG data is more accurate than existing state-of-the-art methods, with 91.13% classification accuracy, 92.58% sensitivity, and 88.75% specificity. Moreover, the further usage of features associated with the S peaks enhances the classification accuracy by 0.85%. Our findings indicate that the proposed machine learning system has the potential to be an effective method for detecting SA episodes.



