# Arxiv Papers in cs.CV on 2022-08-09
### Synthetic Aperture Radar Image Change Detection via Layer Attention-Based Noise-Tolerant Network
- **Arxiv ID**: http://arxiv.org/abs/2208.04481v1
- **DOI**: 10.1109/LGRS.2022.3198088
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04481v1)
- **Published**: 2022-08-09 01:04:39+00:00
- **Updated**: 2022-08-09 01:04:39+00:00
- **Authors**: Desen Meng, Feng Gao, Junyu Dong, Qian Du, Heng-Chao Li
- **Comment**: Accepted by IEEE Geoscience and Remote Sensing Letters (GRSL) 2022,
  code is available at https://github.com/summitgao/LANTNet
- **Journal**: None
- **Summary**: Recently, change detection methods for synthetic aperture radar (SAR) images based on convolutional neural networks (CNN) have gained increasing research attention. However, existing CNN-based methods neglect the interactions among multilayer convolutions, and errors involved in the preclassification restrict the network optimization. To this end, we proposed a layer attention-based noise-tolerant network, termed LANTNet. In particular, we design a layer attention module that adaptively weights the feature of different convolution layers. In addition, we design a noise-tolerant loss function that effectively suppresses the impact of noisy labels. Therefore, the model is insensitive to noisy labels in the preclassification results. The experimental results on three SAR datasets show that the proposed LANTNet performs better compared to several state-of-the-art methods. The source codes are available at https://github.com/summitgao/LANTNet



### Speaker-adaptive Lip Reading with User-dependent Padding
- **Arxiv ID**: http://arxiv.org/abs/2208.04498v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04498v1)
- **Published**: 2022-08-09 01:59:30+00:00
- **Updated**: 2022-08-09 01:59:30+00:00
- **Authors**: Minsu Kim, Hyunjun Kim, Yong Man Ro
- **Comment**: Accepted at ECCV2022
- **Journal**: None
- **Summary**: Lip reading aims to predict speech based on lip movements alone. As it focuses on visual information to model the speech, its performance is inherently sensitive to personal lip appearances and movements. This makes the lip reading models show degraded performance when they are applied to unseen speakers due to the mismatch between training and testing conditions. Speaker adaptation technique aims to reduce this mismatch between train and test speakers, thus guiding a trained model to focus on modeling the speech content without being intervened by the speaker variations. In contrast to the efforts made in audio-based speech recognition for decades, the speaker adaptation methods have not well been studied in lip reading. In this paper, to remedy the performance degradation of lip reading model on unseen speakers, we propose a speaker-adaptive lip reading method, namely user-dependent padding. The user-dependent padding is a speaker-specific input that can participate in the visual feature extraction stage of a pre-trained lip reading model. Therefore, the lip appearances and movements information of different speakers can be considered during the visual feature encoding, adaptively for individual speakers. Moreover, the proposed method does not need 1) any additional layers, 2) to modify the learned weights of the pre-trained model, and 3) the speaker label of train data used during pre-train. It can directly adapt to unseen speakers by learning the user-dependent padding only, in a supervised or unsupervised manner. Finally, to alleviate the speaker information insufficiency in public lip reading databases, we label the speaker of a well-known audio-visual database, LRW, and design an unseen-speaker lip reading scenario named LRW-ID.



### Unsupervised Domain Adaptation for Point Cloud Semantic Segmentation via Graph Matching
- **Arxiv ID**: http://arxiv.org/abs/2208.04510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.04510v1)
- **Published**: 2022-08-09 02:30:15+00:00
- **Updated**: 2022-08-09 02:30:15+00:00
- **Authors**: Yikai Bian, Le Hui, Jianjun Qian, Jin Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation for point cloud semantic segmentation has attracted great attention due to its effectiveness in learning with unlabeled data. Most of existing methods use global-level feature alignment to transfer the knowledge from the source domain to the target domain, which may cause the semantic ambiguity of the feature space. In this paper, we propose a graph-based framework to explore the local-level feature alignment between the two domains, which can reserve semantic discrimination during adaptation. Specifically, in order to extract local-level features, we first dynamically construct local feature graphs on both domains and build a memory bank with the graphs from the source domain. In particular, we use optimal transport to generate the graph matching pairs. Then, based on the assignment matrix, we can align the feature distributions between the two domains with the graph-based local feature loss. Furthermore, we consider the correlation between the features of different categories and formulate a category-guided contrastive loss to guide the segmentation model to learn discriminative features on the target domain. Extensive experiments on different synthetic-to-real and real-to-real domain adaptation scenarios demonstrate that our method can achieve state-of-the-art performance.



### Object Detection with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.04511v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.04511v1)
- **Published**: 2022-08-09 02:34:53+00:00
- **Updated**: 2022-08-09 02:34:53+00:00
- **Authors**: Manoosh Samiei, Ruofeng Li
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Object localization has been a crucial task in computer vision field. Methods of localizing objects in an image have been proposed based on the features of the attended pixels. Recently researchers have proposed methods to formulate object localization as a dynamic decision process, which can be solved by a reinforcement learning approach. In this project, we implement a novel active object localization algorithm based on deep reinforcement learning. We compare two different action settings for this MDP: a hierarchical method and a dynamic method. We further perform some ablation studies on the performance of the models by investigating different hyperparameters and various architecture changes.



### Adversarial Learning Based Structural Brain-network Generative Model for Analyzing Mild Cognitive Impairment
- **Arxiv ID**: http://arxiv.org/abs/2208.08896v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2208.08896v1)
- **Published**: 2022-08-09 02:45:53+00:00
- **Updated**: 2022-08-09 02:45:53+00:00
- **Authors**: Heng Kong, Shuqiang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Mild cognitive impairment(MCI) is a precursor of Alzheimer's disease(AD), and the detection of MCI is of great clinical significance. Analyzing the structural brain networks of patients is vital for the recognition of MCI. However, the current studies on structural brain networks are totally dependent on specific toolboxes, which is time-consuming and subjective. Few tools can obtain the structural brain networks from brain diffusion tensor images. In this work, an adversarial learning-based structural brain-network generative model(SBGM) is proposed to directly learn the structural connections from brain diffusion tensor images. By analyzing the differences in structural brain networks across subjects, we found that the structural brain networks of subjects showed a consistent trend from elderly normal controls(NC) to early mild cognitive impairment(EMCI) to late mild cognitive impairment(LMCI): structural connectivity progressed in a progressively weaker direction as the condition worsened. In addition, our proposed model tri-classifies EMCI, LMCI, and NC subjects, achieving a classification accuracy of 83.33\% on the Alzheimer's Disease Neuroimaging Initiative(ADNI) database.



### Attribute Controllable Beautiful Caucasian Face Generation by Aesthetics Driven Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.04517v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.04517v1)
- **Published**: 2022-08-09 03:04:10+00:00
- **Updated**: 2022-08-09 03:04:10+00:00
- **Authors**: Xin Jin, Shu Zhao, Le Zhang, Xin Zhao, Qiang Deng, Chaoen Xiao
- **Comment**: 13 pages, 5 figures. ACM Multimedia 2022 Technical Demos and Videos
  Program
- **Journal**: None
- **Summary**: In recent years, image generation has made great strides in improving the quality of images, producing high-fidelity ones. Also, quite recently, there are architecture designs, which enable GAN to unsupervisedly learn the semantic attributes represented in different layers. However, there is still a lack of research on generating face images more consistent with human aesthetics. Based on EigenGAN [He et al., ICCV 2021], we build the techniques of reinforcement learning into the generator of EigenGAN. The agent tries to figure out how to alter the semantic attributes of the generated human faces towards more preferable ones. To accomplish this, we trained an aesthetics scoring model that can conduct facial beauty prediction. We also can utilize this scoring model to analyze the correlation between face attributes and aesthetics scores. Empirically, using off-the-shelf techniques from reinforcement learning would not work well. So instead, we present a new variant incorporating the ingredients emerging in the reinforcement learning communities in recent years. Compared to the original generated images, the adjusted ones show clear distinctions concerning various attributes. Experimental results using the MindSpore, show the effectiveness of the proposed method. Altered facial images are commonly more attractive, with significantly improved aesthetic levels.



### Aesthetic Attributes Assessment of Images with AMANv2 and DPC-CaptionsV2
- **Arxiv ID**: http://arxiv.org/abs/2208.04522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.04522v1)
- **Published**: 2022-08-09 03:20:59+00:00
- **Updated**: 2022-08-09 03:20:59+00:00
- **Authors**: Xinghui Zhou, Xin Jin, Jianwen Lv, Heng Huang, Ming Mao, Shuai Cui
- **Comment**: 10 pages, 4 figures, on going research. arXiv admin note: text
  overlap with arXiv:1907.04983
- **Journal**: None
- **Summary**: Image aesthetic quality assessment is popular during the last decade. Besides numerical assessment, nature language assessment (aesthetic captioning) has been proposed to describe the generally aesthetic impression of an image. In this paper, we propose aesthetic attribute assessment, which is the aesthetic attributes captioning, i.e., to assess the aesthetic attributes such as composition, lighting usage and color arrangement. It is a non-trivial task to label the comments of aesthetic attributes, which limit the scale of the corresponding datasets. We construct a novel dataset, named DPC-CaptionsV2, by a semi-automatic way. The knowledge is transferred from a small-scale dataset with full annotations to large-scale professional comments from a photography website. Images of DPC-CaptionsV2 contain comments up to 4 aesthetic attributes: composition, lighting, color, and subject. Then, we propose a new version of Aesthetic Multi-Attributes Networks (AMANv2) based on the BUTD model and the VLPSA model. AMANv2 fuses features of a mixture of small-scale PCCD dataset with full annotations and large-scale DPCCaptionsV2 dataset with full annotations. The experimental results of DPCCaptionsV2 show that our method can predict the comments on 4 aesthetic attributes, which are closer to aesthetic topics than those produced by the previous AMAN model. Through the evaluation criteria of image captioning, the specially designed AMANv2 model is better to the CNN-LSTM model and the AMAN model.



### Using Large Context for Kidney Multi-Structure Segmentation from CTA Images
- **Arxiv ID**: http://arxiv.org/abs/2208.04525v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04525v2)
- **Published**: 2022-08-09 03:25:48+00:00
- **Updated**: 2022-08-10 02:01:51+00:00
- **Authors**: Weiwei Cao, Yuzhu Cao
- **Comment**: arXiv admin note: text overlap with arXiv:2012.15136 by other authors
- **Journal**: None
- **Summary**: Accurate and automated segmentation of multi-structure (i.e., kidneys, renal tu-mors, arteries, and veins) from 3D CTA is one of the most important tasks for surgery-based renal cancer treatment (e.g., laparoscopic partial nephrectomy). This paper briefly presents the main technique details of the multi-structure seg-mentation method in MICCAI 2022 KIPA challenge. The main contribution of this paper is that we design the 3D UNet with the large context information cap-turing capability. Our method ranked eighth on the MICCAI 2022 KIPA chal-lenge open testing dataset with a mean position of 8.2. Our code and trained models are publicly available at https://github.com/fengjiejiejiejie/kipa22_nnunet.



### VectorFlow: Combining Images and Vectors for Traffic Occupancy and Flow Prediction
- **Arxiv ID**: http://arxiv.org/abs/2208.04530v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2208.04530v1)
- **Published**: 2022-08-09 03:49:04+00:00
- **Updated**: 2022-08-09 03:49:04+00:00
- **Authors**: Xin Huang, Xiaoyu Tian, Junru Gu, Qiao Sun, Hang Zhao
- **Comment**: Technical report. 5 pages, 1 figure, and 2 tables
- **Journal**: None
- **Summary**: Predicting future behaviors of road agents is a key task in autonomous driving. While existing models have demonstrated great success in predicting marginal agent future behaviors, it remains a challenge to efficiently predict consistent joint behaviors of multiple agents. Recently, the occupancy flow fields representation was proposed to represent joint future states of road agents through a combination of occupancy grid and flow, which supports efficient and consistent joint predictions. In this work, we propose a novel occupancy flow fields predictor to produce accurate occupancy and flow predictions, by combining the power of an image encoder that learns features from a rasterized traffic image and a vector encoder that captures information of continuous agent trajectories and map states. The two encoded features are fused by multiple attention modules before generating final predictions. Our simple but effective model ranks 3rd place on the Waymo Open Dataset Occupancy and Flow Prediction Challenge, and achieves the best performance in the occluded occupancy and flow prediction task.



### Visual Heart Rate Estimation from RGB Facial Video using Spectral Reflectance
- **Arxiv ID**: http://arxiv.org/abs/2208.04947v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04947v1)
- **Published**: 2022-08-09 04:34:04+00:00
- **Updated**: 2022-08-09 04:34:04+00:00
- **Authors**: Bharath Ramakrishnan, Ruijia Deng, Hassan Ali
- **Comment**: Submitted as a student abstract to AAAI 2023
- **Journal**: None
- **Summary**: Estimation of the Heart rate from the facial video has a number of applications in the medical and fitness industries. Additionally, it has become useful in the field of gaming as well. Several approaches have been proposed to seamlessly obtain the Heart rate from the facial video, but these approaches have had issues in dealing with motion and illumination artifacts. In this work, we propose a reliable HR estimation framework using the spectral reflectance of the user, which makes it robust to motion and illumination disturbances. We employ deep learning-based frameworks such as Faster RCNNs to perform face detection as opposed to the Viola Jones algorithm employed by previous approaches. We evaluate our method on the MAHNOB HCI dataset and found that the proposed method is able to outperform previous approaches.Estimation of the Heart rate from facial video has a number of applications in the medical and the fitness industries. Additionally, it has become useful in the field of gaming as well. Several approaches have been proposed to seamlessly obtain the Heart rate from the facial video, but these approaches have had issues in dealing with motion and illumination artifacts. In this work, we propose a reliable HR estimation framework using the spectral reflectance of the user, which makes it robust to motion and illumination disturbances. We employ deep learning-based frameworks such as Faster RCNNs to perform face detection as opposed to the Viola-Jones algorithm employed by previous approaches. We evaluate our method on the MAHNOB HCI dataset and found that the proposed method is able to outperform previous approaches.



### Inconsistencies in the Definition and Annotation of Student Engagement in Virtual Learning Datasets: A Critical Review
- **Arxiv ID**: http://arxiv.org/abs/2208.04548v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04548v2)
- **Published**: 2022-08-09 05:35:16+00:00
- **Updated**: 2023-01-16 21:45:57+00:00
- **Authors**: Shehroz S. Khan, Ali Abedi, Tracey Colella
- **Comment**: None
- **Journal**: None
- **Summary**: Background: Student engagement (SE) in virtual learning can have a major impact on meeting learning objectives and program dropout risks. Developing Artificial Intelligence (AI) models for automatic SE measurement requires annotated datasets. However, existing SE datasets suffer from inconsistent definitions and annotation protocols mostly unaligned with the definition of SE in educational psychology. This issue could be misleading in developing generalizable AI models and make it hard to compare the performance of these models developed on different datasets. The objective of this critical review was to explore the existing SE datasets and highlight inconsistencies in terms of differing engagement definitions and annotation protocols. Methods: Several academic databases were searched for publications introducing new SE datasets. The datasets containing students' single- or multi-modal data in online or offline computer-based virtual learning sessions were included. The definition and annotation of SE in the existing datasets were analyzed based on our defined seven dimensions of engagement annotation: sources, data modalities, timing, temporal resolution, level of abstraction, combination, and quantification. Results: Thirty SE measurement datasets met the inclusion criteria. The reviewed SE datasets used very diverse and inconsistent definitions and annotation protocols. Unexpectedly, very few of the reviewed datasets used existing psychometrically validated scales in their definition of SE. Discussion: The inconsistent definition and annotation of SE are problematic for research on developing comparable AI models for automatic SE measurement. Some of the existing SE definitions and protocols in settings other than virtual learning that have the potential to be used in virtual learning are introduced.



### Disentangled Representation Learning Using ($β$-)VAE and GAN
- **Arxiv ID**: http://arxiv.org/abs/2208.04549v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04549v1)
- **Published**: 2022-08-09 05:37:06+00:00
- **Updated**: 2022-08-09 05:37:06+00:00
- **Authors**: Mohammad Haghir Ebrahimabadi
- **Comment**: None
- **Journal**: None
- **Summary**: Given a dataset of images containing different objects with different features such as shape, size, rotation, and x-y position; and a Variational Autoencoder (VAE); creating a disentangled encoding of these features in the hidden space vector of the VAE was the task of interest in this paper. The dSprite dataset provided the desired features for the required experiments in this research. After training the VAE combined with a Generative Adversarial Network (GAN), each dimension of the hidden vector was disrupted to explore the disentanglement in each dimension. Note that the GAN was used to improve the quality of output image reconstruction.



### Multi-target Tracking of Zebrafish based on Particle Filter
- **Arxiv ID**: http://arxiv.org/abs/2208.04553v1
- **DOI**: 10.1109/ChiCC.2016.7554987
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.04553v1)
- **Published**: 2022-08-09 06:02:55+00:00
- **Updated**: 2022-08-09 06:02:55+00:00
- **Authors**: Heng Cong, Mingzhu Sun, Duoying Zhou, Xin Zhao
- **Comment**: 6 pages, 8 figures, 2016 35th Chinese Control Conference (CCC)
- **Journal**: 2016 35th Chinese Control Conference (CCC). IEEE, 2016:
  10308-10313
- **Summary**: Zebrafish is an excellent model organism, which has been widely used in the fields of biological experiments, drug screening, and swarm intelligence. In recent years, there are a large number of techniques for tracking of zebrafish involved in the study of behaviors, which makes it attack much attention of scientists from many fields. Multi-target tracking of zebrafish is still facing many challenges. The high mobility and uncertainty make it difficult to predict its motion; the similar appearances and texture features make it difficult to establish an appearance model; it is even hard to link the trajectories because of the frequent occlusion. In this paper, we use particle filter to approximate the uncertainty of the motion. Firstly, by analyzing the motion characteristics of zebrafish, we establish an efficient hybrid motion model to predict its positions; then we establish an appearance model based on the predicted positions to predict the postures of every targets, meanwhile weigh the particles by comparing the difference of predicted pose and observation pose ; finally, we get the optimal position of single zebrafish through the weighted position, and use the joint particle filter to process trajectory linking of multiple zebrafish.



### Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder for Image Reconstruction and Generation
- **Arxiv ID**: http://arxiv.org/abs/2208.04554v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4; I.2
- **Links**: [PDF](http://arxiv.org/pdf/2208.04554v1)
- **Published**: 2022-08-09 06:04:25+00:00
- **Updated**: 2022-08-09 06:04:25+00:00
- **Authors**: Mohammad Adiban, Kalin Stefanov, Sabato Marco Siniscalchi, Giampiero Salvi
- **Comment**: 12 pages plus supplementary material. Submitted to BMVC 2022
- **Journal**: None
- **Summary**: We propose a multi-layer variational autoencoder method, we call HR-VQVAE, that learns hierarchical discrete representations of the data. By utilizing a novel objective function, each layer in HR-VQVAE learns a discrete representation of the residual from previous layers through a vector quantized encoder. Furthermore, the representations at each layer are hierarchically linked to those at previous layers. We evaluate our method on the tasks of image reconstruction and generation. Experimental results demonstrate that the discrete representations learned by HR-VQVAE enable the decoder to reconstruct high-quality images with less distortion than the baseline methods, namely VQVAE and VQVAE-2. HR-VQVAE can also generate high-quality and diverse images that outperform state-of-the-art generative models, providing further verification of the efficiency of the learned representations. The hierarchical nature of HR-VQVAE i) reduces the decoding search time, making the method particularly suitable for high-load tasks and ii) allows to increase the codebook size without incurring the codebook collapse problem.



### BabyNet: A Lightweight Network for Infant Reaching Action Recognition in Unconstrained Environments to Support Future Pediatric Rehabilitation Applications
- **Arxiv ID**: http://arxiv.org/abs/2208.04950v2
- **DOI**: 10.1109/RO-MAN50785.2021.9515507
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04950v2)
- **Published**: 2022-08-09 07:38:36+00:00
- **Updated**: 2022-10-12 04:34:41+00:00
- **Authors**: Amel Dechemi, Vikarn Bhakri, Ipsita Sahin, Arjun Modi, Julya Mestas, Pamodya Peiris, Dannya Enriquez Barrundia, Elena Kokkoni, Konstantinos Karydis
- **Comment**: Accepted to RO-MAN 2021
- **Journal**: None
- **Summary**: Action recognition is an important component to improve autonomy of physical rehabilitation devices, such as wearable robotic exoskeletons. Existing human action recognition algorithms focus on adult applications rather than pediatric ones. In this paper, we introduce BabyNet, a light-weight (in terms of trainable parameters) network structure to recognize infant reaching action from off-body stationary cameras. We develop an annotated dataset that includes diverse reaches performed while in a sitting posture by different infants in unconstrained environments (e.g., in home settings, etc.). Our approach uses the spatial and temporal connection of annotated bounding boxes to interpret onset and offset of reaching, and to detect a complete reaching action. We evaluate the efficiency of our proposed approach and compare its performance against other learning-based network structures in terms of capability of capturing temporal inter-dependencies and accuracy of detection of reaching onset and offset. Results indicate our BabyNet can attain solid performance in terms of (average) testing accuracy that exceeds that of other larger networks, and can hence serve as a light-weight data-driven framework for video-based infant reaching action recognition.



### SBPF: Sensitiveness Based Pruning Framework For Convolutional Neural Network On Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2208.04588v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.04588v1)
- **Published**: 2022-08-09 08:05:19+00:00
- **Updated**: 2022-08-09 08:05:19+00:00
- **Authors**: Yiheng Lu, Maoguo Gong, Wei Zhao, Kaiyuan Feng, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: Pruning techniques are used comprehensively to compress convolutional neural networks (CNNs) on image classification. However, the majority of pruning methods require a well pre-trained model to provide useful supporting parameters, such as C1-norm, BatchNorm value and gradient information, which may lead to inconsistency of filter evaluation if the parameters of the pre-trained model are not well optimized. Therefore, we propose a sensitiveness based method to evaluate the importance of each layer from the perspective of inference accuracy by adding extra damage for the original model. Because the performance of the accuracy is determined by the distribution of parameters across all layers rather than individual parameter, the sensitiveness based method will be robust to update of parameters. Namely, we can obtain similar importance evaluation of each convolutional layer between the imperfect-trained and fully trained models. For VGG-16 on CIFAR-10, even when the original model is only trained with 50 epochs, we can get same evaluation of layer importance as the results when the model is trained fully. Then we will remove filters proportional from each layer by the quantified sensitiveness. Our sensitiveness based pruning framework is verified efficiently on VGG-16, a customized Conv-4 and ResNet-18 with CIFAR-10, MNIST and CIFAR-100, respectively.



### Comparison of semi-supervised learning methods for High Content Screening quality control
- **Arxiv ID**: http://arxiv.org/abs/2208.04592v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.04592v1)
- **Published**: 2022-08-09 08:14:36+00:00
- **Updated**: 2022-08-09 08:14:36+00:00
- **Authors**: Umar Masud, Ethan Cohen, Ihab Bendidi, Guillaume Bollot, Auguste Genovesio
- **Comment**: None
- **Journal**: None
- **Summary**: Progress in automated microscopy and quantitative image analysis has promoted high-content screening (HCS) as an efficient drug discovery and research tool. While HCS offers to quantify complex cellular phenotypes from images at high throughput, this process can be obstructed by image aberrations such as out-of-focus image blur, fluorophore saturation, debris, a high level of noise, unexpected auto-fluorescence or empty images. While this issue has received moderate attention in the literature, overlooking these artefacts can seriously hamper downstream image processing tasks and hinder detection of subtle phenotypes. It is therefore of primary concern, and a prerequisite, to use quality control in HCS. In this work, we evaluate deep learning options that do not require extensive image annotations to provide a straightforward and easy to use semi-supervised learning solution to this issue. Concretely, we compared the efficacy of recent self-supervised and transfer learning approaches to provide a base encoder to a high throughput artefact image detector. The results of this study suggest that transfer learning methods should be preferred for this task as they not only performed best here but present the advantage of not requiring sensitive hyperparameter settings nor extensive additional training.



### Generative models-based data labeling for deep networks regression: application to seed maturity estimation from UAV multispectral images
- **Arxiv ID**: http://arxiv.org/abs/2208.04611v1
- **DOI**: 10.3390/rs14205238
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.04611v1)
- **Published**: 2022-08-09 09:06:51+00:00
- **Updated**: 2022-08-09 09:06:51+00:00
- **Authors**: Eric Dericquebourg, Adel Hafiane, Raphael Canals
- **Comment**: None
- **Journal**: None
- **Summary**: Monitoring seed maturity is an increasing challenge in agriculture due to climate change and more restrictive practices. Seeds monitoring in the field is essential to optimize the farming process and to guarantee yield quality through high germination. Traditional methods are based on limited sampling in the field and analysis in laboratory. Moreover, they are time consuming and only allow monitoring sub-sections of the crop field. This leads to a lack of accuracy on the condition of the crop as a whole due to intra-field heterogeneities. Multispectral imagery by UAV allows uniform scan of fields and better capture of crop maturity information. On the other hand, deep learning methods have shown tremendous potential in estimating agronomic parameters, especially maturity. However, they require large labeled datasets. Although large sets of aerial images are available, labeling them with ground truth is a tedious, if not impossible task. In this paper, we propose a method for estimating parsley seed maturity using multispectral UAV imagery, with a new approach for automatic data labeling. This approach is based on parametric and non-parametric models to provide weak labels. We also consider the data acquisition protocol and the performance evaluation of the different steps of the method. Results show good performance, and the non-parametric kernel density estimator model can improve neural network generalization when used as a labeling method, leading to more robust and better performing deep neural models.



### Res-Dense Net for 3D Covid Chest CT-scan classification
- **Arxiv ID**: http://arxiv.org/abs/2208.04613v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.04613v1)
- **Published**: 2022-08-09 09:13:00+00:00
- **Updated**: 2022-08-09 09:13:00+00:00
- **Authors**: Quoc-Huy Trinh, Minh-Van Nguyen, Thien-Phuc Nguyen Dinh
- **Comment**: arXiv admin note: text overlap with arXiv:2106.07524 by other authors
- **Journal**: None
- **Summary**: One of the most contentious areas of research in Medical Image Preprocessing is 3D CT-scan. With the rapid spread of COVID-19, the function of CT-scan in properly and swiftly diagnosing the disease has become critical. It has a positive impact on infection prevention. There are many tasks to diagnose the illness through CT-scan images, include COVID-19. In this paper, we propose a method that using a Stacking Deep Neural Network to detect the Covid 19 through the series of 3D CT-scans images . In our method, we experiment with two backbones are DenseNet 121 and ResNet 101. This method achieves a competitive performance on some evaluation metrics



### Classification of electromagnetic interference induced image noise in an analog video link
- **Arxiv ID**: http://arxiv.org/abs/2208.04614v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.04614v2)
- **Published**: 2022-08-09 09:13:56+00:00
- **Updated**: 2022-08-18 10:06:31+00:00
- **Authors**: Anthony Purcell, Ciarán Eising
- **Comment**: 8 pages, 7 figures, 2 tables
- **Journal**: Proceedings of the 2022 Irish Machine Vision and Image Processing
  Conference
- **Summary**: With the ever-increasing electrification of the vehicle showing no sign of retreating, electronic systems deployed in automotive applications are subject to more stringent Electromagnetic Immunity compliance constraints than ever before, to ensure the proximity of nearby electronic systems will not affect their operation. The EMI compliance testing of an analog camera link requires video quality to be monitored and assessed to validate such compliance, which up to now, has been a manual task. Due to the nature of human interpretation, this is open to inconsistency. Here, we propose a solution using deep learning models that analyse, and grade video content derived from an EMI compliance test. These models are trained using a dataset built entirely from real test image data to ensure the accuracy of the resultant model(s) is maximised. Starting with the standard AlexNet, we propose four models to classify the EMI noise level



### EfficientNet for Brain-Lesion classification
- **Arxiv ID**: http://arxiv.org/abs/2208.04616v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.04616v1)
- **Published**: 2022-08-09 09:21:22+00:00
- **Updated**: 2022-08-09 09:21:22+00:00
- **Authors**: Quoc-Huy Trinh, Trong-Hieu Nguyen Mau, Radmir Zosimov, Minh-Van Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: In the development of technology, there are increasing cases of brain disease, there are more treatments proposed and achieved a positive result. However, with Brain-Lesion, the early diagnoses can improve the possibility for successful treatment and can help patients recuperate better. From this reason, Brain-Lesion is one of the controversial topics in medical images analysis nowadays. With the improvement of the architecture, there is a variety of methods that are proposed and achieve competitive scores. In this paper, we proposed a technique that uses efficient-net for 3D images, especially the Efficient-net B0 for Brain-Lesion classification task solution, and achieve the competitive score. Moreover, we also proposed the method to use Multiscale-EfficientNet to classify the slices of the MRI data



### RDA: Reciprocal Distribution Alignment for Robust Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.04619v2
- **DOI**: 10.1007/978-3-031-20056-4_31
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04619v2)
- **Published**: 2022-08-09 09:24:25+00:00
- **Updated**: 2022-08-12 14:10:51+00:00
- **Authors**: Yue Duan, Lei Qi, Lei Wang, Luping Zhou, Yinghuan Shi
- **Comment**: Accepted by ECCV 2022
- **Journal**: Computer Vision - ECCV 2022, LNCS vol 13690, pp 533-549
- **Summary**: In this work, we propose Reciprocal Distribution Alignment (RDA) to address semi-supervised learning (SSL), which is a hyperparameter-free framework that is independent of confidence threshold and works with both the matched (conventionally) and the mismatched class distributions. Distribution mismatch is an often overlooked but more general SSL scenario where the labeled and the unlabeled data do not fall into the identical class distribution. This may lead to the model not exploiting the labeled data reliably and drastically degrade the performance of SSL methods, which could not be rescued by the traditional distribution alignment. In RDA, we enforce a reciprocal alignment on the distributions of the predictions from two classifiers predicting pseudo-labels and complementary labels on the unlabeled data. These two distributions, carrying complementary information, could be utilized to regularize each other without any prior of class distribution. Moreover, we theoretically show that RDA maximizes the input-output mutual information. Our approach achieves promising performance in SSL under a variety of scenarios of mismatched distributions, as well as the conventional matched SSL setting. Our code is available at: https://github.com/NJUyued/RDA4RobustSSL.



### Efficient Out-of-Distribution Detection of Melanoma with Wavelet-based Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2208.04639v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.04639v2)
- **Published**: 2022-08-09 09:57:56+00:00
- **Updated**: 2022-08-10 12:43:51+00:00
- **Authors**: M. M. Amaan Valiuddin, Christiaan G. A. Viviers, Ruud J. G. van Sloun, Peter H. N. de With, Fons van der Sommen
- **Comment**: Published at 1st Workshop on Cancer Prevention through early
  detecTion (MICCAI 2022)
- **Journal**: None
- **Summary**: Melanoma is a serious form of skin cancer with high mortality rate at later stages. Fortunately, when detected early, the prognosis of melanoma is promising and malignant melanoma incidence rates are relatively low. As a result, datasets are heavily imbalanced which complicates training current state-of-the-art supervised classification AI models. We propose to use generative models to learn the benign data distribution and detect Out-of-Distribution (OOD) malignant images through density estimation. Normalizing Flows (NFs) are ideal candidates for OOD detection due to their ability to compute exact likelihoods. Nevertheless, their inductive biases towards apparent graphical features rather than semantic context hamper accurate OOD detection. In this work, we aim at using these biases with domain-level knowledge of melanoma, to improve likelihood-based OOD detection of malignant images. Our encouraging results demonstrate potential for OOD detection of melanoma using NFs. We achieve a 9% increase in Area Under Curve of the Receiver Operating Characteristics by using wavelet-based NFs. This model requires significantly less parameters for inference making it more applicable on edge devices. The proposed methodology can aid medical experts with diagnosis of skin-cancer patients and continuously increase survival rates. Furthermore, this research paves the way for other areas in oncology with similar data imbalance issues.



### Choose qualified instructor for university based on rule-based weighted expert system
- **Arxiv ID**: http://arxiv.org/abs/2208.04657v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04657v1)
- **Published**: 2022-08-09 10:44:40+00:00
- **Updated**: 2022-08-09 10:44:40+00:00
- **Authors**: Sana Karimian
- **Comment**: 8 pages, 3 figures, 4 tables
- **Journal**: None
- **Summary**: Near the entire university faculty directors must select some qualified professors for respected courses in each academic semester. In this sense, factors such as teaching experience, academic training, competition, etc. are considered. This work is usually done by experts, such as faculty directors, which is time consuming. Up to now, several semi-automatic systems have been proposed to assist heads. In this article, a fully automatic rule-based expert system is developed. The proposed expert system consists of three main stages. First, the knowledge of human experts is entered and designed as a decision tree. In the second step, an expert system is designed based on the provided rules of the generated decision tree. In the third step, an algorithm is proposed to weight the results of the tree based on the quality of the experts. To improve the performance of the expert system, a majority voting algorithm is developed as a post-process step to select the qualified trainer who satisfies the most expert decision tree for each course. The quality of the proposed expert system is evaluated using real data from Iranian universities. The calculated accuracy rate is 85.55, demonstrating the robustness and accuracy of the proposed system. The proposed system has little computational complexity compared to related efficient works. Also, simple implementation and transparent box are other features of the proposed system.



### Continual Prune-and-Select: Class-incremental learning with specialized subnetworks
- **Arxiv ID**: http://arxiv.org/abs/2208.04952v2
- **DOI**: 10.1007/s10489-022-04441-z
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04952v2)
- **Published**: 2022-08-09 10:49:40+00:00
- **Updated**: 2023-01-17 11:14:40+00:00
- **Authors**: Aleksandr Dekhovich, David M. J. Tax, Marcel H. F. Sluiter, Miguel A. Bessa
- **Comment**: None
- **Journal**: Applied Intelligence, 1-16 (2023)
- **Summary**: The human brain is capable of learning tasks sequentially mostly without forgetting. However, deep neural networks (DNNs) suffer from catastrophic forgetting when learning one task after another. We address this challenge considering a class-incremental learning scenario where the DNN sees test data without knowing the task from which this data originates. During training, Continual-Prune-and-Select (CP&S) finds a subnetwork within the DNN that is responsible for solving a given task. Then, during inference, CP&S selects the correct subnetwork to make predictions for that task. A new task is learned by training available neuronal connections of the DNN (previously untrained) to create a new subnetwork by pruning, which can include previously trained connections belonging to other subnetwork(s) because it does not update shared connections. This enables to eliminate catastrophic forgetting by creating specialized regions in the DNN that do not conflict with each other while still allowing knowledge transfer across them. The CP&S strategy is implemented with different subnetwork selection strategies, revealing superior performance to state-of-the-art continual learning methods tested on various datasets (CIFAR-100, CUB-200-2011, ImageNet-100 and ImageNet-1000). In particular, CP&S is capable of sequentially learning 10 tasks from ImageNet-1000 keeping an accuracy around 94% with negligible forgetting, a first-of-its-kind result in class-incremental learning. To the best of the authors' knowledge, this represents an improvement in accuracy above 10% when compared to the best alternative method.



### OL-DN: Online learning based dual-domain network for HEVC intra frame quality enhancement
- **Arxiv ID**: http://arxiv.org/abs/2208.04661v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04661v1)
- **Published**: 2022-08-09 11:06:59+00:00
- **Updated**: 2022-08-09 11:06:59+00:00
- **Authors**: Renwei Yang, Shuyuan Zhu, Xiaozhen Zheng, Bing Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Convolution neural network (CNN) based methods offer effective solutions for enhancing the quality of compressed image and video. However, these methods ignore using the raw data to enhance the quality. In this paper, we adopt the raw data in the quality enhancement for the HEVC intra-coded image by proposing an online learning-based method. When quality enhancement is demanded, we online train our proposed model at encoder side and then use the parameters to update the model of decoder side. This method not only improves model performance, but also makes one model adoptable to multiple coding scenarios. Besides, quantization error in discrete cosine transform (DCT) coefficients is the root cause of various HEVC compression artifacts. Thus, we combine frequency domain priors to assist image reconstruction. We design a DCT based convolution layer, to produce DCT coefficients that are suitable for CNN learning. Experimental results show that our proposed online learning based dual-domain network (OL-DN) has achieved superior performance, compared with the state-of-the-art methods.



### Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.04679v2
- **DOI**: 10.1109/TIP.2020.3031184
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04679v2)
- **Published**: 2022-08-09 11:55:53+00:00
- **Updated**: 2022-08-10 03:17:16+00:00
- **Authors**: Tingtian Li, Yuk-Hee Chan, Daniel P. K. Lun
- **Comment**: None
- **Journal**: None
- **Summary**: When imaging through a semi-reflective medium such as glass, the reflection of another scene can often be found in the captured images. It degrades the quality of the images and affects their subsequent analyses. In this paper, a novel deep neural network approach for solving the reflection problem in imaging is presented. Traditional reflection removal methods not only require long computation time for solving different optimization functions, their performance is also not guaranteed. As array cameras are readily available in nowadays imaging devices, we first suggest in this paper a multiple-image based depth estimation method using a convolutional neural network (CNN). The proposed network avoids the depth ambiguity problem due to the reflection in the image, and directly estimates the depths along the image edges. They are then used to classify the edges as belonging to the background or reflection. Since edges having similar depth values are error prone in the classification, they are removed from the reflection removal process. We suggest a generative adversarial network (GAN) to regenerate the removed background edges. Finally, the estimated background edge map is fed to another auto-encoder network to assist the extraction of the background from the original image. Experimental results show that the proposed reflection removal algorithm achieves superior performance both quantitatively and qualitatively as compared to the state-of-the-art methods. The proposed algorithm also shows much faster speed compared to the existing approaches using the traditional optimization methods.



### Boundary Distance Loss for Intra-/Extra-meatal Segmentation of Vestibular Schwannoma
- **Arxiv ID**: http://arxiv.org/abs/2208.04680v1
- **DOI**: 10.1007/978-3-031-17899-3_8
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.04680v1)
- **Published**: 2022-08-09 11:58:42+00:00
- **Updated**: 2022-08-09 11:58:42+00:00
- **Authors**: Navodini Wijethilake, Aaron Kujawa, Reuben Dorent, Muhammad Asad, Anna Oviedova, Tom Vercauteren, Jonathan Shapey
- **Comment**: Accepted for the MICCAI MLCN workshop 2022
- **Journal**: None
- **Summary**: Vestibular Schwannoma (VS) typically grows from the inner ear to the brain. It can be separated into two regions, intrameatal and extrameatal respectively corresponding to being inside or outside the inner ear canal. The growth of the extrameatal regions is a key factor that determines the disease management followed by the clinicians. In this work, a VS segmentation approach with subdivision into intra-/extra-meatal parts is presented. We annotated a dataset consisting of 227 T2 MRI instances, acquired longitudinally on 137 patients, excluding post-operative instances. We propose a staged approach, with the first stage performing the whole tumour segmentation and the second stage performing the intra-/extra-meatal segmentation using the T2 MRI along with the mask obtained from the first stage. To improve on the accuracy of the predicted meatal boundary, we introduce a task-specific loss which we call Boundary Distance Loss. The performance is evaluated in contrast to the direct intrameatal extrameatal segmentation task performance, i.e. the Baseline. Our proposed method, with the two-stage approach and the Boundary Distance Loss, achieved a Dice score of 0.8279+-0.2050 and 0.7744+-0.1352 for extrameatal and intrameatal regions respectively, significantly improving over the Baseline, which gave Dice score of 0.7939+-0.2325 and 0.7475+-0.1346 for the extrameatal and intrameatal regions respectively.



### How Well Do Vision Transformers (VTs) Transfer To The Non-Natural Image Domain? An Empirical Study Involving Art Classification
- **Arxiv ID**: http://arxiv.org/abs/2208.04693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.04693v1)
- **Published**: 2022-08-09 12:05:18+00:00
- **Updated**: 2022-08-09 12:05:18+00:00
- **Authors**: Vincent Tonkes, Matthia Sabatelli
- **Comment**: None
- **Journal**: None
- **Summary**: Vision Transformers (VTs) are becoming a valuable alternative to Convolutional Neural Networks (CNNs) when it comes to problems involving high-dimensional and spatially organized inputs such as images. However, their Transfer Learning (TL) properties are not yet well studied, and it is not fully known whether these neural architectures can transfer across different domains as well as CNNs. In this paper we study whether VTs that are pre-trained on the popular ImageNet dataset learn representations that are transferable to the non-natural image domain. To do so we consider three well-studied art classification problems and use them as a surrogate for studying the TL potential of four popular VTs. Their performance is extensively compared against that of four common CNNs across several TL experiments. Our results show that VTs exhibit strong generalization properties and that these networks are more powerful feature extractors than CNNs.



### HRF-Net: Holistic Radiance Fields from Sparse Inputs
- **Arxiv ID**: http://arxiv.org/abs/2208.04717v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2208.04717v1)
- **Published**: 2022-08-09 12:23:48+00:00
- **Updated**: 2022-08-09 12:23:48+00:00
- **Authors**: Phong Nguyen-Ha, Lam Huynh, Esa Rahtu, Jiri Matas, Janne Heikkila
- **Comment**: In submission
- **Journal**: None
- **Summary**: We present HRF-Net, a novel view synthesis method based on holistic radiance fields that renders novel views using a set of sparse inputs. Recent generalizing view synthesis methods also leverage the radiance fields but the rendering speed is not real-time. There are existing methods that can train and render novel views efficiently but they can not generalize to unseen scenes. Our approach addresses the problem of real-time rendering for generalizing view synthesis and consists of two main stages: a holistic radiance fields predictor and a convolutional-based neural renderer. This architecture infers not only consistent scene geometry based on the implicit neural fields but also renders new views efficiently using a single GPU. We first train HRF-Net on multiple 3D scenes of the DTU dataset and the network can produce plausible novel views on unseen real and synthetics data using only photometric losses. Moreover, our method can leverage a denser set of reference images of a single scene to produce accurate novel views without relying on additional explicit representations and still maintains the high-speed rendering of the pre-trained model. Experimental results show that HRF-Net outperforms state-of-the-art generalizable neural rendering methods on various synthetic and real datasets.



### Improving COVID-19 CT Classification of CNNs by Learning Parameter-Efficient Representation
- **Arxiv ID**: http://arxiv.org/abs/2208.04718v1
- **DOI**: 10.1016/j.compbiomed.2022.106417
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04718v1)
- **Published**: 2022-08-09 12:24:53+00:00
- **Updated**: 2022-08-09 12:24:53+00:00
- **Authors**: Yujia Xu, Hak-Keung Lam, Guangyu Jia, Jian Jiang, Junkai Liao, Xinqi Bao
- **Comment**: None
- **Journal**: None
- **Summary**: COVID-19 pandemic continues to spread rapidly over the world and causes a tremendous crisis in global human health and the economy. Its early detection and diagnosis are crucial for controlling the further spread. Many deep learning-based methods have been proposed to assist clinicians in automatic COVID-19 diagnosis based on computed tomography imaging. However, challenges still remain, including low data diversity in existing datasets, and unsatisfied detection resulting from insufficient accuracy and sensitivity of deep learning models. To enhance the data diversity, we design augmentation techniques of incremental levels and apply them to the largest open-access benchmark dataset, COVIDx CT-2A. Meanwhile, similarity regularization (SR) derived from contrastive learning is proposed in this study to enable CNNs to learn more parameter-efficient representations, thus improving the accuracy and sensitivity of CNNs. The results on seven commonly used CNNs demonstrate that CNN performance can be improved stably through applying the designed augmentation and SR techniques. In particular, DenseNet121 with SR achieves an average test accuracy of 99.44% in three trials for three-category classification, including normal, non-COVID-19 pneumonia, and COVID-19 pneumonia. And the achieved precision, sensitivity, and specificity for the COVID-19 pneumonia category are 98.40%, 99.59%, and 99.50%, respectively. These statistics suggest that our method has surpassed the existing state-of-the-art methods on the COVIDx CT-2A dataset.



### Aesthetic Language Guidance Generation of Images Using Attribute Comparison
- **Arxiv ID**: http://arxiv.org/abs/2208.04740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.04740v1)
- **Published**: 2022-08-09 12:35:23+00:00
- **Updated**: 2022-08-09 12:35:23+00:00
- **Authors**: Xin Jin, Qiang Deng, Jianwen Lv, Heng Huang, Hao Lou, Chaoen Xiao
- **Comment**: 13 pages, 18 figures, on going research
- **Journal**: None
- **Summary**: With the vigorous development of mobile photography technology, major mobile phone manufacturers are scrambling to improve the shooting ability of equipments and the photo beautification algorithm of software. However, the improvement of intelligent equipments and algorithms cannot replace human subjective photography technology. In this paper, we propose the aesthetic language guidance of image (ALG). We divide ALG into ALG-T and ALG-I according to whether the guiding rules are based on photography templates or guidance images. Whether it is ALG-T or ALG-I, we guide photography from three attributes of color, lighting and composition of the images. The differences of the three attributes between the input images and the photography templates or the guidance images are described in natural language, which is aesthetic natural language guidance (ALG). Also, because of the differences in lighting and composition between landscape images and portrait images, we divide the input images into landscape images and portrait images. Both ALG-T and ALG-I conduct aesthetic language guidance respectively for the two types of input images (landscape images and portrait images).



### Semantic Segmentation-Assisted Instance Feature Fusion for Multi-Level 3D Part Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.04766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.04766v1)
- **Published**: 2022-08-09 13:22:55+00:00
- **Updated**: 2022-08-09 13:22:55+00:00
- **Authors**: Chunyu Sun, Xin Tong, Yang Liu
- **Comment**: Accepted by Computational Visual Media. Project page:
  https://isunchy.github.io/projects/3d_instance_segmentation.html
- **Journal**: None
- **Summary**: Recognizing 3D part instances from a 3D point cloud is crucial for 3D structure and scene understanding. Several learning-based approaches use semantic segmentation and instance center prediction as training tasks and fail to further exploit the inherent relationship between shape semantics and part instances. In this paper, we present a new method for 3D part instance segmentation. Our method exploits semantic segmentation to fuse nonlocal instance features, such as center prediction, and further enhances the fusion scheme in a multi- and cross-level way. We also propose a semantic region center prediction task to train and leverage the prediction results to improve the clustering of instance points. Our method outperforms existing methods with a large-margin improvement in the PartNet benchmark. We also demonstrate that our feature fusion scheme can be applied to other existing methods to improve their performance in indoor scene instance segmentation tasks.



### HyperNST: Hyper-Networks for Neural Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2208.04807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.04807v1)
- **Published**: 2022-08-09 14:34:07+00:00
- **Updated**: 2022-08-09 14:34:07+00:00
- **Authors**: Dan Ruta, Andrew Gilbert, Saeid Motiian, Baldo Faieta, Zhe Lin, John Collomosse
- **Comment**: None
- **Journal**: None
- **Summary**: We present HyperNST; a neural style transfer (NST) technique for the artistic stylization of images, based on Hyper-networks and the StyleGAN2 architecture. Our contribution is a novel method for inducing style transfer parameterized by a metric space, pre-trained for style-based visual search (SBVS). We show for the first time that such space may be used to drive NST, enabling the application and interpolation of styles from an SBVS system. The technical contribution is a hyper-network that predicts weight updates to a StyleGAN2 pre-trained over a diverse gamut of artistic content (portraits), tailoring the style parameterization on a per-region basis using a semantic map of the facial regions. We show HyperNST to exceed state of the art in content preservation for our stylized content while retaining good style transfer performance.



### Longitudinal Prediction of Postnatal Brain Magnetic Resonance Images via a Metamorphic Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2208.04825v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04825v1)
- **Published**: 2022-08-09 15:09:19+00:00
- **Updated**: 2022-08-09 15:09:19+00:00
- **Authors**: Yunzhi Huang, Sahar Ahmad, Luyi Han, Shuai Wang, Zhengwang Wu, Weili Lin, Gang Li, Li Wang, Pew-Thian Yap
- **Comment**: None
- **Journal**: None
- **Summary**: Missing scans are inevitable in longitudinal studies due to either subject dropouts or failed scans. In this paper, we propose a deep learning framework to predict missing scans from acquired scans, catering to longitudinal infant studies. Prediction of infant brain MRI is challenging owing to the rapid contrast and structural changes particularly during the first year of life. We introduce a trustworthy metamorphic generative adversarial network (MGAN) for translating infant brain MRI from one time-point to another. MGAN has three key features: (i) Image translation leveraging spatial and frequency information for detail-preserving mapping; (ii) Quality-guided learning strategy that focuses attention on challenging regions. (iii) Multi-scale hybrid loss function that improves translation of tissue contrast and structural details. Experimental results indicate that MGAN outperforms existing GANs by accurately predicting both contrast and anatomical details.



### From Scratch to Sketch: Deep Decoupled Hierarchical Reinforcement Learning for Robotic Sketching Agent
- **Arxiv ID**: http://arxiv.org/abs/2208.04833v1
- **DOI**: 10.1109/ICRA46639.2022.9811858
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04833v1)
- **Published**: 2022-08-09 15:18:55+00:00
- **Updated**: 2022-08-09 15:18:55+00:00
- **Authors**: Ganghun Lee, Minji Kim, Minsu Lee, Byoung-Tak Zhang
- **Comment**: Accepted to ICRA 2022
- **Journal**: None
- **Summary**: We present an automated learning framework for a robotic sketching agent that is capable of learning stroke-based rendering and motor control simultaneously. We formulate the robotic sketching problem as a deep decoupled hierarchical reinforcement learning; two policies for stroke-based rendering and motor control are learned independently to achieve sub-tasks for drawing, and form a hierarchy when cooperating for real-world drawing. Without hand-crafted features, drawing sequences or trajectories, and inverse kinematics, the proposed method trains the robotic sketching agent from scratch. We performed experiments with a 6-DoF robot arm with 2F gripper to sketch doodles. Our experimental results show that the two policies successfully learned the sub-tasks and collaborated to sketch the target images. Also, the robustness and flexibility were examined by varying drawing tools and surfaces.



### sim2real: Cardiac MR Image Simulation-to-Real Translation via Unsupervised GANs
- **Arxiv ID**: http://arxiv.org/abs/2208.04874v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04874v1)
- **Published**: 2022-08-09 16:06:06+00:00
- **Updated**: 2022-08-09 16:06:06+00:00
- **Authors**: Sina Amirrajab, Yasmina Al Khalil, Cristian Lorenz, Jurgen Weese, Josien Pluim, Marcel Breeuwer
- **Comment**: Accepted to Joint Annual Meeting ISMRM-ESMRMB & ISMRT 31st Annual
  Meeting 07-12 May 2022 | London, England, UK
- **Journal**: None
- **Summary**: There has been considerable interest in the MR physics-based simulation of a database of virtual cardiac MR images for the development of deep-learning analysis networks. However, the employment of such a database is limited or shows suboptimal performance due to the realism gap, missing textures, and the simplified appearance of simulated images. In this work we 1) provide image simulation on virtual XCAT subjects with varying anatomies, and 2) propose sim2real translation network to improve image realism. Our usability experiments suggest that sim2real data exhibits a good potential to augment training data and boost the performance of a segmentation algorithm.



### Localizing the conceptual difference of two scenes using deep learning for house keeping usages
- **Arxiv ID**: http://arxiv.org/abs/2208.04884v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.04884v2)
- **Published**: 2022-08-09 16:25:56+00:00
- **Updated**: 2022-12-09 11:15:37+00:00
- **Authors**: Ali Atghaei, Ehsan Rahnama, Kiavash Azimi
- **Comment**: None
- **Journal**: None
- **Summary**: Finding the conceptual difference between the two images in an industrial environment has been especially important for HSE purposes and there is still no reliable and conformable method to find the major differences to alert the related controllers. Due to the abundance and variety of objects in different environments, the use of supervised learning methods in this field is facing a major problem. Due to the sharp and even slight change in lighting conditions in the two scenes, it is not possible to naively subtract the two images in order to find these differences. The goal of this paper is to find and localize the conceptual differences of two frames of one scene but in two different times and classify the differences to addition, reduction and change in the field. In this paper, we demonstrate a comprehensive solution for this application by presenting the deep learning method and using transfer learning and structural modification of the error function, as well as a process for adding and synthesizing data. An appropriate data set was provided and labeled, and the model results were evaluated on this data set and the possibility of using it in real and industrial applications was explained.



### Sports Video Analysis on Large-Scale Data
- **Arxiv ID**: http://arxiv.org/abs/2208.04897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.04897v1)
- **Published**: 2022-08-09 16:59:24+00:00
- **Updated**: 2022-08-09 16:59:24+00:00
- **Authors**: Dekun Wu, He Zhao, Xingce Bao, Richard P. Wildes
- **Comment**: None
- **Journal**: None
- **Summary**: This paper investigates the modeling of automated machine description on sports video, which has seen much progress recently. Nevertheless, state-of-the-art approaches fall quite short of capturing how human experts analyze sports scenes. There are several major reasons: (1) The used dataset is collected from non-official providers, which naturally creates a gap between models trained on those datasets and real-world applications; (2) previously proposed methods require extensive annotation efforts (i.e., player and ball segmentation at pixel level) on localizing useful visual features to yield acceptable results; (3) very few public datasets are available. In this paper, we propose a novel large-scale NBA dataset for Sports Video Analysis (NSVA) with a focus on captioning, to address the above challenges. We also design a unified approach to process raw videos into a stack of meaningful features with minimum labelling efforts, showing that cross modeling on such features using a transformer architecture leads to strong performance. In addition, we demonstrate the broad application of NSVA by addressing two additional tasks, namely fine-grained sports action recognition and salient player identification. Code and dataset are available at https://github.com/jackwu502/NSVA.



### Deep Learning-Based Objective and Reproducible Osteosarcoma Chemotherapy Response Assessment and Outcome Prediction
- **Arxiv ID**: http://arxiv.org/abs/2208.04910v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.04910v1)
- **Published**: 2022-08-09 17:12:27+00:00
- **Updated**: 2022-08-09 17:12:27+00:00
- **Authors**: David Joon Ho, Narasimhan P. Agaram, Marc-Henri Jean, Stephanie D. Suser, Cynthia Chu, Chad M. Vanderbilt, Paul A. Meyers, Leonard H. Wexler, John H. Healey, Thomas J. Fuchs, Meera R. Hameed
- **Comment**: None
- **Journal**: None
- **Summary**: Osteosarcoma is the most common primary bone cancer whose standard treatment includes pre-operative chemotherapy followed by resection. Chemotherapy response is used for predicting prognosis and further management of patients. Necrosis is routinely assessed post-chemotherapy from histology slides on resection specimens where necrosis ratio is defined as the ratio of necrotic tumor to overall tumor. Patients with necrosis ratio >=90% are known to have better outcome. Manual microscopic review of necrosis ratio from multiple glass slides is semi-quantitative and can have intra- and inter-observer variability. We propose an objective and reproducible deep learning-based approach to estimate necrosis ratio with outcome prediction from scanned hematoxylin and eosin whole slide images. We collected 103 osteosarcoma cases with 3134 WSIs to train our deep learning model, to validate necrosis ratio assessment, and to evaluate outcome prediction. We trained Deep Multi-Magnification Network to segment multiple tissue subtypes including viable tumor and necrotic tumor in pixel-level and to calculate case-level necrosis ratio from multiple WSIs. We showed necrosis ratio estimated by our segmentation model highly correlates with necrosis ratio from pathology reports manually assessed by experts where mean absolute differences for Grades IV (100%), III (>=90%), and II (>=50% and <90%) necrosis response are 4.4%, 4.5%, and 17.8%, respectively. We successfully stratified patients to predict overall survival with p=10^-6 and progression-free survival with p=0.012. Our reproducible approach without variability enabled us to tune cutoff thresholds, specifically for our model and our data set, to 80% for OS and 60% for PFS. Our study indicates deep learning can support pathologists as an objective tool to analyze osteosarcoma from histology for assessing treatment response and predicting patient outcome.



### TSRFormer: Table Structure Recognition with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2208.04921v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.04921v1)
- **Published**: 2022-08-09 17:36:13+00:00
- **Updated**: 2022-08-09 17:36:13+00:00
- **Authors**: Weihong Lin, Zheng Sun, Chixiang Ma, Mingze Li, Jiawei Wang, Lei Sun, Qiang Huo
- **Comment**: Accepted by ACM MultiMedia 2022
- **Journal**: None
- **Summary**: We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognizing the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage DETR based separator prediction approach, dubbed \textbf{Sep}arator \textbf{RE}gression \textbf{TR}ansformer (SepRETR), to predict separation lines from table images directly. To make the two-stage DETR framework work efficiently and effectively for the separation line prediction task, we propose two improvements: 1) A prior-enhanced matching strategy to solve the slow convergence issue of DETR; 2) A new cross attention module to sample features from a high-resolution convolutional feature map directly so that high localization accuracy is achieved with low computational cost. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet and WTW. Furthermore, we have validated the robustness of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.



### Wavelet Score-Based Generative Modeling
- **Arxiv ID**: http://arxiv.org/abs/2208.05003v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2208.05003v1)
- **Published**: 2022-08-09 19:13:34+00:00
- **Updated**: 2022-08-09 19:13:34+00:00
- **Authors**: Florentin Guth, Simon Coste, Valentin De Bortoli, Stephane Mallat
- **Comment**: None
- **Journal**: None
- **Summary**: Score-based generative models (SGMs) synthesize new data samples from Gaussian white noise by running a time-reversed Stochastic Differential Equation (SDE) whose drift coefficient depends on some probabilistic score. The discretization of such SDEs typically requires a large number of time steps and hence a high computational cost. This is because of ill-conditioning properties of the score that we analyze mathematically. We show that SGMs can be considerably accelerated, by factorizing the data distribution into a product of conditional probabilities of wavelet coefficients across scales. The resulting Wavelet Score-based Generative Model (WSGM) synthesizes wavelet coefficients with the same number of time steps at all scales, and its time complexity therefore grows linearly with the image size. This is proved mathematically over Gaussian distributions, and shown numerically over physical processes at phase transition and natural image datasets.



### Human Activity Recognition Using Cascaded Dual Attention CNN and Bi-Directional GRU Framework
- **Arxiv ID**: http://arxiv.org/abs/2208.05034v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.05034v1)
- **Published**: 2022-08-09 20:34:42+00:00
- **Updated**: 2022-08-09 20:34:42+00:00
- **Authors**: Hayat Ullah, Arslan Munir
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-based human activity recognition has emerged as one of the essential research areas in video analytics domain. Over the last decade, numerous advanced deep learning algorithms have been introduced to recognize complex human actions from video streams. These deep learning algorithms have shown impressive performance for the human activity recognition task. However, these newly introduced methods either exclusively focus on model performance or the effectiveness of these models in terms of computational efficiency and robustness, resulting in a biased tradeoff in their proposals to deal with challenging human activity recognition problem. To overcome the limitations of contemporary deep learning models for human activity recognition, this paper presents a computationally efficient yet generic spatial-temporal cascaded framework that exploits the deep discriminative spatial and temporal features for human activity recognition. For efficient representation of human actions, we have proposed an efficient dual attentional convolutional neural network (CNN) architecture that leverages a unified channel-spatial attention mechanism to extract human-centric salient features in video frames. The dual channel-spatial attention layers together with the convolutional layers learn to be more attentive in the spatial receptive fields having objects over the number of feature maps. The extracted discriminative salient features are then forwarded to stacked bi-directional gated recurrent unit (Bi-GRU) for long-term temporal modeling and recognition of human actions using both forward and backward pass gradient learning. Extensive experiments are conducted, where the obtained results show that the proposed framework attains an improvement in execution time up to 167 times in terms of frames per second as compared to most of the contemporary action recognition methods.



### Automatic Ultrasound Image Segmentation of Supraclavicular Nerve Using Dilated U-Net Deep Learning Architecture
- **Arxiv ID**: http://arxiv.org/abs/2208.05050v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.05050v1)
- **Published**: 2022-08-09 21:45:15+00:00
- **Updated**: 2022-08-09 21:45:15+00:00
- **Authors**: Mizuki Miyatake, Subhash Nerella, David Simpson, Natalia Pawlowicz, Sarah Stern, Patrick Tighe, Parisa Rashidi
- **Comment**: None
- **Journal**: None
- **Summary**: Automated object recognition in medical images can facilitate medical diagnosis and treatment. In this paper, we automatically segmented supraclavicular nerves in ultrasound images to assist in injecting peripheral nerve blocks. Nerve blocks are generally used for pain treatment after surgery, where ultrasound guidance is used to inject local anesthetics next to target nerves. This treatment blocks the transmission of pain signals to the brain, which can help improve the rate of recovery from surgery and significantly decrease the requirement for postoperative opioids. However, Ultrasound Guided Regional Anesthesia (UGRA) requires anesthesiologists to visually recognize the actual nerve position in the ultrasound images. This is a complex task given the myriad visual presentations of nerves in ultrasound images, and their visual similarity to many neighboring tissues. In this study, we used an automated nerve detection system for the UGRA Nerve Block treatment. The system can recognize the position of the nerve in ultrasound images using Deep Learning techniques. We developed a model to capture features of nerves by training two deep neural networks with skip connections: two extended U-Net architectures with and without dilated convolutions. This solution could potentially lead to an improved blockade of targeted nerves in regional anesthesia.



### Learning to Complete Object Shapes for Object-level Mapping in Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2208.05067v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2208.05067v1)
- **Published**: 2022-08-09 22:56:33+00:00
- **Updated**: 2022-08-09 22:56:33+00:00
- **Authors**: Binbin Xu, Andrew J. Davison, Stefan Leutenegger
- **Comment**: International Conference on Intelligent Robots and Systems (IROS)
  2022
- **Journal**: None
- **Summary**: In this paper, we propose a novel object-level mapping system that can simultaneously segment, track, and reconstruct objects in dynamic scenes. It can further predict and complete their full geometries by conditioning on reconstructions from depth inputs and a category-level shape prior with the aim that completed object geometry leads to better object reconstruction and tracking accuracy. For each incoming RGB-D frame, we perform instance segmentation to detect objects and build data associations between the detection and the existing object maps. A new object map will be created for each unmatched detection. For each matched object, we jointly optimise its pose and latent geometry representations using geometric residual and differential rendering residual towards its shape prior and completed geometry. Our approach shows better tracking and reconstruction performance compared to methods using traditional volumetric mapping or learned shape prior approaches. We evaluate its effectiveness by quantitatively and qualitatively testing it in both synthetic and real-world sequences.



