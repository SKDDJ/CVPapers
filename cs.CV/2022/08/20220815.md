# Arxiv Papers in cs.CV on 2022-08-15
### InvisibiliTee: Angle-agnostic Cloaking from Person-Tracking Systems with a Tee
- **Arxiv ID**: http://arxiv.org/abs/2208.06962v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T07, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2208.06962v1)
- **Published**: 2022-08-15 01:32:09+00:00
- **Updated**: 2022-08-15 01:32:09+00:00
- **Authors**: Yaxian Li, Bingqing Zhang, Guoping Zhao, Mingyu Zhang, Jiajun Liu, Ziwei Wang, Jirong Wen
- **Comment**: 12 pages, 10 figures and the ICANN 2022 accpeted paper
- **Journal**: None
- **Summary**: After a survey for person-tracking system-induced privacy concerns, we propose a black-box adversarial attack method on state-of-the-art human detection models called InvisibiliTee. The method learns printable adversarial patterns for T-shirts that cloak wearers in the physical world in front of person-tracking systems. We design an angle-agnostic learning scheme which utilizes segmentation of the fashion dataset and a geometric warping process so the adversarial patterns generated are effective in fooling person detectors from all camera angles and for unseen black-box detection models. Empirical results in both digital and physical environments show that with the InvisibiliTee on, person-tracking systems' ability to detect the wearer drops significantly.



### STAR-GNN: Spatial-Temporal Video Representation for Content-based Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2208.06966v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T07, I.4.7
- **Links**: [PDF](http://arxiv.org/pdf/2208.06966v1)
- **Published**: 2022-08-15 01:47:50+00:00
- **Updated**: 2022-08-15 01:47:50+00:00
- **Authors**: Guoping Zhao, Bingqing Zhang, Mingyu Zhang, Yaxian Li, Jiajun Liu, Ji-Rong Wen
- **Comment**: 6 pages, 2 figures, ICME 2022 accepted paper
- **Journal**: None
- **Summary**: We propose a video feature representation learning framework called STAR-GNN, which applies a pluggable graph neural network component on a multi-scale lattice feature graph. The essence of STAR-GNN is to exploit both the temporal dynamics and spatial contents as well as visual connections between regions at different scales in the frames. It models a video with a lattice feature graph in which the nodes represent regions of different granularity, with weighted edges that represent the spatial and temporal links. The contextual nodes are aggregated simultaneously by graph neural networks with parameters trained with retrieval triplet loss. In the experiments, we show that STAR-GNN effectively implements a dynamic attention mechanism on video frame sequences, resulting in the emphasis for dynamic and semantically rich content in the video, and is robust to noise and redundancies. Empirical results show that STAR-GNN achieves state-of-the-art performance for Content-Based Video Retrieval.



### Learning Semantic Correspondence with Sparse Annotations
- **Arxiv ID**: http://arxiv.org/abs/2208.06974v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06974v2)
- **Published**: 2022-08-15 02:24:18+00:00
- **Updated**: 2022-08-17 17:59:18+00:00
- **Authors**: Shuaiyi Huang, Luyu Yang, Bo He, Songyang Zhang, Xuming He, Abhinav Shrivastava
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Finding dense semantic correspondence is a fundamental problem in computer vision, which remains challenging in complex scenes due to background clutter, extreme intra-class variation, and a severe lack of ground truth. In this paper, we aim to address the challenge of label sparsity in semantic correspondence by enriching supervision signals from sparse keypoint annotations. To this end, we first propose a teacher-student learning paradigm for generating dense pseudo-labels and then develop two novel strategies for denoising pseudo-labels. In particular, we use spatial priors around the sparse annotations to suppress the noisy pseudo-labels. In addition, we introduce a loss-driven dynamic label selection strategy for label denoising. We instantiate our paradigm with two variants of learning strategies: a single offline teacher setting, and mutual online teachers setting. Our approach achieves notable improvements on three challenging benchmarks for semantic correspondence and establishes the new state-of-the-art. Project page: https://shuaiyihuang.github.io/publications/SCorrSAN.



### Faster Attention Is What You Need: A Fast Self-Attention Neural Network Backbone Architecture for the Edge via Double-Condensing Attention Condensers
- **Arxiv ID**: http://arxiv.org/abs/2208.06980v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.06980v3)
- **Published**: 2022-08-15 02:47:33+00:00
- **Updated**: 2023-02-03 17:18:22+00:00
- **Authors**: Alexander Wong, Mohammad Javad Shafiee, Saad Abbasi, Saeejith Nair, Mahmoud Famouri
- **Comment**: None
- **Journal**: None
- **Summary**: With the growing adoption of deep learning for on-device TinyML applications, there has been an ever-increasing demand for efficient neural network backbones optimized for the edge. Recently, the introduction of attention condenser networks have resulted in low-footprint, highly-efficient, self-attention neural networks that strike a strong balance between accuracy and speed. In this study, we introduce a faster attention condenser design called double-condensing attention condensers that allow for highly condensed feature embeddings. We further employ a machine-driven design exploration strategy that imposes design constraints based on best practices for greater efficiency and robustness to produce the macro-micro architecture constructs of the backbone. The resulting backbone (which we name AttendNeXt) achieves significantly higher inference throughput on an embedded ARM processor when compared to several other state-of-the-art efficient backbones (>10x faster than FB-Net C at higher accuracy and speed and >10x faster than MobileOne-S1 at smaller size) while having a small model size (>1.37x smaller than MobileNetv3-L at higher accuracy and speed) and strong accuracy (1.1% higher top-1 accuracy than MobileViT XS on ImageNet at higher speed). These promising results demonstrate that exploring different efficient architecture designs and self-attention mechanisms can lead to interesting new building blocks for TinyML applications.



### A Multi-objective Memetic Algorithm for Auto Adversarial Attack Optimization Design
- **Arxiv ID**: http://arxiv.org/abs/2208.06984v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.06984v1)
- **Published**: 2022-08-15 03:03:05+00:00
- **Updated**: 2022-08-15 03:03:05+00:00
- **Authors**: Jialiang Sun, Wen Yao, Tingsong Jiang, Xiaoqian Chen
- **Comment**: None
- **Journal**: None
- **Summary**: The phenomenon of adversarial examples has been revealed in variant scenarios. Recent studies show that well-designed adversarial defense strategies can improve the robustness of deep learning models against adversarial examples. However, with the rapid development of defense technologies, it also tends to be more difficult to evaluate the robustness of the defensed model due to the weak performance of existing manually designed adversarial attacks. To address the challenge, given the defensed model, the efficient adversarial attack with less computational burden and lower robust accuracy is needed to be further exploited. Therefore, we propose a multi-objective memetic algorithm for auto adversarial attack optimization design, which realizes the automatical search for the near-optimal adversarial attack towards defensed models. Firstly, the more general mathematical model of auto adversarial attack optimization design is constructed, where the search space includes not only the attacker operations, magnitude, iteration number, and loss functions but also the connection ways of multiple adversarial attacks. In addition, we develop a multi-objective memetic algorithm combining NSGA-II and local search to solve the optimization problem. Finally, to decrease the evaluation cost during the search, we propose a representative data selection strategy based on the sorting of cross entropy loss values of each images output by models. Experiments on CIFAR10, CIFAR100, and ImageNet datasets show the effectiveness of our proposed method.



### Deepfake Detection using ImageNet models and Temporal Images of 468 Facial Landmarks
- **Arxiv ID**: http://arxiv.org/abs/2208.06990v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06990v1)
- **Published**: 2022-08-15 03:32:28+00:00
- **Updated**: 2022-08-15 03:32:28+00:00
- **Authors**: Christeen T Jose
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents our results and findings on the use of temporal images for deepfake detection. We modelled temporal relations that exist in the movement of 468 facial landmarks across frames of a given video as spatial relations by constructing an image (referred to as temporal image) using the pixel values at these facial landmarks. CNNs are capable of recognizing spatial relationships that exist between the pixels of a given image. 10 different ImageNet models were considered for the study.



### On a Mechanism Framework of Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2208.06995v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, 68T07, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2208.06995v4)
- **Published**: 2022-08-15 03:51:40+00:00
- **Updated**: 2022-12-12 02:47:54+00:00
- **Authors**: Changcun Huang
- **Comment**: v2:lemma 9 modified; v3:proof of lemma 8 changed; v4:definition 13
  modified
- **Journal**: None
- **Summary**: This paper proposes a theoretical framework on the mechanism of autoencoders. To the encoder part, under the main use of dimensionality reduction, we investigate its two fundamental properties: bijective maps and data disentangling. The general construction methods of an encoder that satisfies either or both of the above two properties are given. The generalization mechanism of autoencoders is modeled. Based on the theoretical framework above, we explain some experimental results of variational autoencoders, denoising autoencoders, and linear-unit autoencoders, with emphasis on the interpretation of the lower-dimensional representation of data via encoders; and the mechanism of image restoration through autoencoders is natural to be understood by those explanations. Compared to PCA and decision trees, the advantages of (generalized) autoencoders on dimensionality reduction and classification are demonstrated, respectively. Convolutional neural networks and randomly weighted neural networks are also interpreted by this framework.



### HoW-3D: Holistic 3D Wireframe Perception from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2208.06999v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06999v2)
- **Published**: 2022-08-15 04:05:41+00:00
- **Updated**: 2022-08-19 05:06:58+00:00
- **Authors**: Wenchao Ma, Bin Tan, Nan Xue, Tianfu Wu, Xianwei Zheng, Gui-Song Xia
- **Comment**: To appear in IEEE 3DV 2022. Code and Dataset are available at
  https://github.com/Wenchao-M/HoW-3D
- **Journal**: None
- **Summary**: This paper studies the problem of holistic 3D wireframe perception (HoW-3D), a new task of perceiving both the visible 3D wireframes and the invisible ones from single-view 2D images. As the non-front surfaces of an object cannot be directly observed in a single view, estimating the non-line-of-sight (NLOS) geometries in HoW-3D is a fundamentally challenging problem and remains open in computer vision. We study the problem of HoW-3D by proposing an ABC-HoW benchmark, which is created on top of CAD models sourced from the ABC-dataset with 12k single-view images and the corresponding holistic 3D wireframe models. With our large-scale ABC-HoW benchmark available, we present a novel Deep Spatial Gestalt (DSG) model to learn the visible junctions and line segments as the basis and then infer the NLOS 3D structures from the visible cues by following the Gestalt principles of human vision systems. In our experiments, we demonstrate that our DSG model performs very well in inferring the holistic 3D wireframes from single-view images. Compared with the strong baseline methods, our DSG model outperforms the previous wireframe detectors in detecting the invisible line geometry in single-view images and is even very competitive with prior arts that take high-fidelity PointCloud as inputs on reconstructing 3D wireframes.



### Uni6Dv2: Noise Elimination for 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.06416v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06416v2)
- **Published**: 2022-08-15 04:30:00+00:00
- **Updated**: 2023-03-16 11:02:59+00:00
- **Authors**: Mingshan Sun, Ye Zheng, Tianpeng Bao, Jianqiu Chen, Guoqiang Jin, Liwei Wu, Rui Zhao, Xiaoke Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Uni6D is the first 6D pose estimation approach to employ a unified backbone network to extract features from both RGB and depth images. We discover that the principal reasons of Uni6D performance limitations are Instance-Outside and Instance-Inside noise. Uni6D's simple pipeline design inherently introduces Instance-Outside noise from background pixels in the receptive field, while ignoring Instance-Inside noise in the input depth data. In this paper, we propose a two-step denoising approach for dealing with the aforementioned noise in Uni6D. To reduce noise from non-instance regions, an instance segmentation network is utilized in the first step to crop and mask the instance. A lightweight depth denoising module is proposed in the second step to calibrate the depth feature before feeding it into the pose regression network. Extensive experiments show that our Uni6Dv2 reliably and robustly eliminates noise, outperforming Uni6D without sacrificing too much inference efficiency. It also reduces the need for annotated real data that requires costly labeling.



### Adaptive Joint Optimization for 3D Reconstruction with Differentiable Rendering
- **Arxiv ID**: http://arxiv.org/abs/2208.07003v2
- **DOI**: 10.1109/TVCG.2022.3148245
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07003v2)
- **Published**: 2022-08-15 04:32:41+00:00
- **Updated**: 2022-09-12 08:24:41+00:00
- **Authors**: Jingbo Zhang, Ziyu Wan, Jing Liao
- **Comment**: 12 pages, published on TVCG; Project page:
  https://adjointopti.github.io/adjoin.github.io/
- **Journal**: None
- **Summary**: Due to inevitable noises introduced during scanning and quantization, 3D reconstruction via RGB-D sensors suffers from errors both in geometry and texture, leading to artifacts such as camera drifting, mesh distortion, texture ghosting, and blurriness. Given an imperfect reconstructed 3D model, most previous methods have focused on the refinement of either geometry, texture, or camera pose. Or different optimization schemes and objectives for optimizing each component have been used in previous joint optimization methods, forming a complicated system. In this paper, we propose a novel optimization approach based on differentiable rendering, which integrates the optimization of camera pose, geometry, and texture into a unified framework by enforcing consistency between the rendered results and the corresponding RGB-D inputs. Based on the unified framework, we introduce a joint optimization approach to fully exploit the inter-relationships between geometry, texture, and camera pose, and describe an adaptive interleaving strategy to improve optimization stability and efficiency. Using differentiable rendering, an image-level adversarial loss is applied to further improve the 3D model, making it more photorealistic. Experiments on synthetic and real data using quantitative and qualitative evaluation demonstrated the superiority of our approach in recovering both fine-scale geometry and high-fidelity texture.Code is available at https://adjointopti.github.io/adjoin.github.io/.



### Automatic Landmark Detection and Registration of Brain Cortical Surfaces via Quasi-Conformal Geometry and Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.07010v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2208.07010v1)
- **Published**: 2022-08-15 05:47:51+00:00
- **Updated**: 2022-08-15 05:47:51+00:00
- **Authors**: Yuchen Guo, Qiguang Chen, Gary P. T. Choi, Lok Ming Lui
- **Comment**: None
- **Journal**: None
- **Summary**: In medical imaging, surface registration is extensively used for performing systematic comparisons between anatomical structures, with a prime example being the highly convoluted brain cortical surfaces. To obtain a meaningful registration, a common approach is to identify prominent features on the surfaces and establish a low-distortion mapping between them with the feature correspondence encoded as landmark constraints. Prior registration works have primarily focused on using manually labeled landmarks and solving highly nonlinear optimization problems, which are time-consuming and hence hinder practical applications. In this work, we propose a novel framework for the automatic landmark detection and registration of brain cortical surfaces using quasi-conformal geometry and convolutional neural networks. We first develop a landmark detection network (LD-Net) that allows for the automatic extraction of landmark curves given two prescribed starting and ending points based on the surface geometry. We then utilize the detected landmarks and quasi-conformal theory for achieving the surface registration. Specifically, we develop a coefficient prediction network (CP-Net) for predicting the Beltrami coefficients associated with the desired landmark-based registration and a mapping network called the disk Beltrami solver network (DBS-Net) for generating quasi-conformal mappings from the predicted Beltrami coefficients, with the bijectivity guaranteed by quasi-conformal theory. Experimental results are presented to demonstrate the effectiveness of our proposed framework. Altogether, our work paves a new way for surface-based morphometry and medical shape analysis.



### Automatic Controlling Fish Feeding Machine using Feature Extraction of Nutriment and Ripple Behavior
- **Arxiv ID**: http://arxiv.org/abs/2208.07011v1
- **DOI**: 10.24507/ijicic.17.05.1483
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2208.07011v1)
- **Published**: 2022-08-15 05:52:37+00:00
- **Updated**: 2022-08-15 05:52:37+00:00
- **Authors**: Hilmil Pradana, Keiichi Horio
- **Comment**: None
- **Journal**: International Journal of Innovative Computing, Information and
  Control (IJICIC) October 2021
- **Summary**: Controlling fish feeding machine is challenging problem because experienced fishermen can adequately control based on assumption. To build robust method for reasonable application, we propose automatic controlling fish feeding machine based on computer vision using combination of counting nutriments and estimating ripple behavior using regression and textural feature, respectively. To count number of nutriments, we apply object detection and tracking methods to acknowledge the nutriments moving to sea surface. Recently, object tracking is active research and challenging problem in computer vision. Unfortunately, the robust tracking method for multiple small objects with dense and complex relationships is unsolved problem in aquaculture field with more appearance creatures. Based on the number of nutriments and ripple behavior, we can control fish feeding machine which consistently performs well in real environment. Proposed method presents the agreement for automatic controlling fish feeding by the activation graphs and textural feature of ripple behavior. Our tracking method can precisely track the nutriments in next frame comparing with other methods. Based on computational time, proposed method reaches 3.86 fps while other methods spend lower than 1.93 fps. Quantitative evaluation can promise that proposed method is valuable for aquaculture fish farm with widely applied to real environment.



### Pyramidal Predictive Network: A Model for Visual-frame Prediction Based on Predictive Coding Theory
- **Arxiv ID**: http://arxiv.org/abs/2208.07021v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2208.07021v3)
- **Published**: 2022-08-15 06:28:34+00:00
- **Updated**: 2022-11-15 01:42:44+00:00
- **Authors**: Chaofan Ling, Junpei Zhong, Weihua Li
- **Comment**: None
- **Journal**: None
- **Summary**: Visual-frame prediction is a pixel-dense prediction task that infers future frames from past frames. Lacking of appearance details, low prediction accuracy and high computational overhead are still major problems with current models or methods. In this paper, we propose a novel neural network model inspired by the well-known predictive coding theory to deal with the problems. Predictive coding provides an interesting and reliable computational framework, which will be combined with other theories such as the cerebral cortex at different level oscillates at different frequencies, to design an efficient and reliable predictive network model for visual-frame prediction. Specifically, the model is composed of a series of recurrent and convolutional units forming the top-down and bottom-up streams, respectively. The update frequency of neural units on each of the layer decreases with the increasing of network levels, which results in neurons of higher-level can capture information in longer time dimensions. According to the experimental results, this model shows better compactness and comparable predictive performance with existing works, implying lower computational cost and higher prediction accuracy. Code is available at https://github.com/Ling-CF/PPNet.



### Memory-Driven Text-to-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2208.07022v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.07022v1)
- **Published**: 2022-08-15 06:32:57+00:00
- **Updated**: 2022-08-15 06:32:57+00:00
- **Authors**: Bowen Li, Philip H. S. Torr, Thomas Lukasiewicz
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a memory-driven semi-parametric approach to text-to-image generation, which is based on both parametric and non-parametric techniques. The non-parametric component is a memory bank of image features constructed from a training set of images. The parametric component is a generative adversarial network. Given a new text description at inference time, the memory bank is used to selectively retrieve image features that are provided as basic information of target images, which enables the generator to produce realistic synthetic results. We also incorporate the content information into the discriminator, together with semantic features, allowing the discriminator to make a more reliable prediction. Experimental results demonstrate that the proposed memory-driven semi-parametric approach produces more realistic images than purely parametric approaches, in terms of both visual fidelity and text-image semantic consistency.



### Hierarchical Attention Network for Few-Shot Object Detection via Meta-Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.07039v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07039v3)
- **Published**: 2022-08-15 07:29:31+00:00
- **Updated**: 2022-12-14 09:03:35+00:00
- **Authors**: Dongwoo Park, Jong-Min Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot object detection (FSOD) aims to classify and detect few images of novel categories. Existing meta-learning methods insufficiently exploit features between support and query images owing to structural limitations. We propose a hierarchical attention network with sequentially large receptive fields to fully exploit the query and support images. In addition, meta-learning does not distinguish the categories well because it determines whether the support and query images match. In other words, metric-based learning for classification is ineffective because it does not work directly. Thus, we propose a contrastive learning method called meta-contrastive learning, which directly helps achieve the purpose of the meta-learning strategy. Finally, we establish a new state-of-the-art network, by realizing significant margins. Our method brings 2.3, 1.0, 1.3, 3.4 and 2.4% AP improvements for 1-30 shots object detection on COCO dataset. Our code is available at: https://github.com/infinity7428/hANMCL



### Self-Supervised Vision Transformers for Malware Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.07049v1
- **DOI**: 10.1109/ACCESS.2022.3206445
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.07049v1)
- **Published**: 2022-08-15 07:49:58+00:00
- **Updated**: 2022-08-15 07:49:58+00:00
- **Authors**: Sachith Seneviratne, Ridwan Shariffdeen, Sanka Rasnayaka, Nuran Kasthuriarachchi
- **Comment**: None
- **Journal**: None
- **Summary**: Malware detection plays a crucial role in cyber-security with the increase in malware growth and advancements in cyber-attacks. Previously unseen malware which is not determined by security vendors are often used in these attacks and it is becoming inevitable to find a solution that can self-learn from unlabeled sample data. This paper presents SHERLOCK, a self-supervision based deep learning model to detect malware based on the Vision Transformer (ViT) architecture. SHERLOCK is a novel malware detection method which learns unique features to differentiate malware from benign programs with the use of image-based binary representation. Experimental results using 1.2 million Android applications across a hierarchy of 47 types and 696 families, shows that self-supervised learning can achieve an accuracy of 97% for the binary classification of malware which is higher than existing state-of-the-art techniques. Our proposed model is also able to outperform state-of-the-art techniques for multi-class malware classification of types and family with macro-F1 score of .497 and .491 respectively.



### UPST-NeRF: Universal Photorealistic Style Transfer of Neural Radiance Fields for 3D Scene
- **Arxiv ID**: http://arxiv.org/abs/2208.07059v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07059v2)
- **Published**: 2022-08-15 08:17:35+00:00
- **Updated**: 2022-08-21 15:02:25+00:00
- **Authors**: Yaosen Chen, Qi Yuan, Zhiqiang Li, Yuegen Liu, Wei Wang, Chaoping Xie, Xuming Wen, Qien Yu
- **Comment**: arXiv admin note: text overlap with arXiv:2205.12183 by other authors
- **Journal**: None
- **Summary**: 3D scenes photorealistic stylization aims to generate photorealistic images from arbitrary novel views according to a given style image while ensuring consistency when rendering from different viewpoints. Some existing stylization methods with neural radiance fields can effectively predict stylized scenes by combining the features of the style image with multi-view images to train 3D scenes. However, these methods generate novel view images that contain objectionable artifacts. Besides, they cannot achieve universal photorealistic stylization for a 3D scene. Therefore, a styling image must retrain a 3D scene representation network based on a neural radiation field. We propose a novel 3D scene photorealistic style transfer framework to address these issues. It can realize photorealistic 3D scene style transfer with a 2D style image. We first pre-trained a 2D photorealistic style transfer network, which can meet the photorealistic style transfer between any given content image and style image. Then, we use voxel features to optimize a 3D scene and get the geometric representation of the scene. Finally, we jointly optimize a hyper network to realize the scene photorealistic style transfer of arbitrary style images. In the transfer stage, we use a pre-trained 2D photorealistic network to constrain the photorealistic style of different views and different style images in the 3D scene. The experimental results show that our method not only realizes the 3D photorealistic style transfer of arbitrary style images but also outperforms the existing methods in terms of visual quality and consistency. Project page:https://semchan.github.io/UPST_NeRF.



### A Vision Transformer-Based Approach to Bearing Fault Classification via Vibration Signals
- **Arxiv ID**: http://arxiv.org/abs/2208.07070v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2208.07070v2)
- **Published**: 2022-08-15 08:37:30+00:00
- **Updated**: 2022-09-20 12:42:07+00:00
- **Authors**: Abid Hasan Zim, Aeyan Ashraf, Aquib Iqbal, Asad Malik, Minoru Kuribayashi
- **Comment**: None
- **Journal**: None
- **Summary**: Rolling bearings are the most crucial components of rotating machinery. Identifying defective bearings in a timely manner may prevent the malfunction of an entire machinery system. The mechanical condition monitoring field has entered the big data phase as a result of the fast advancement of machine parts. When working with large amounts of data, the manual feature extraction approach has the drawback of being inefficient and inaccurate. Data-driven methods like the Deep Learning method have been successfully used in recent years for mechanical intelligent fault detection. Convolutional neural networks (CNNs) were mostly used in earlier research to detect and identify bearing faults. The CNN model, however, suffers from the drawback of having trouble managing fault-time information, which results in a lack of classification results. In this study, bearing defects have been classified using a state-of-the-art Vision Transformer (ViT). Bearing defects were classified using Case Western Reserve University (CWRU) bearing failure laboratory experimental data. The research took into account 13 distinct kinds of defects under 0-load situations in addition to normal bearing conditions. Using the short-time Fourier transform (STFT), the vibration signals were converted into 2D time-frequency images. The 2D time-frequency images are used as input parameters for the ViT. The model achieved an overall accuracy of 98.8%.



### Crowd Counting on Heavily Compressed Images with Curriculum Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2208.07075v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07075v2)
- **Published**: 2022-08-15 08:43:21+00:00
- **Updated**: 2023-01-30 14:18:23+00:00
- **Authors**: Arian Bakhtiarnia, Qi Zhang, Alexandros Iosifidis
- **Comment**: None
- **Journal**: None
- **Summary**: JPEG image compression algorithm is a widely used technique for image size reduction in edge and cloud computing settings. However, applying such lossy compression on images processed by deep neural networks can lead to significant accuracy degradation. Inspired by the curriculum learning paradigm, we propose a training approach called curriculum pre-training (CPT) for crowd counting on compressed images, which alleviates the drop in accuracy resulting from lossy compression. We verify the effectiveness of our approach by extensive experiments on three crowd counting datasets, two crowd counting DNN models and various levels of compression. The proposed training method is not overly sensitive to hyper-parameters, and reduces the error, particularly for heavily compressed images, by up to 19.70%.



### Enhancing Deep Learning-based 3-lead ECG Classification with Heartbeat Counting and Demographic Data Integration
- **Arxiv ID**: http://arxiv.org/abs/2208.07088v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07088v1)
- **Published**: 2022-08-15 09:33:36+00:00
- **Updated**: 2022-08-15 09:33:36+00:00
- **Authors**: Khiem H. Le, Hieu H. Pham, Thao B. T. Nguyen, Tu A. Nguyen, Cuong D. Do
- **Comment**: arXiv admin note: text overlap with arXiv:2207.12381
- **Journal**: None
- **Summary**: Nowadays, an increasing number of people are being diagnosed with cardiovascular diseases (CVDs), the leading cause of death globally. The gold standard for identifying these heart problems is via electrocardiogram (ECG). The standard 12-lead ECG is widely used in clinical practice and the majority of current research. However, using a lower number of leads can make ECG more pervasive as it can be integrated with portable or wearable devices. This article introduces two novel techniques to improve the performance of the current deep learning system for 3-lead ECG classification, making it comparable with models that are trained using standard 12-lead ECG. Specifically, we propose a multi-task learning scheme in the form of the number of heartbeats regression and an effective mechanism to integrate patient demographic data into the system. With these two advancements, we got classification performance in terms of F1 scores of 0.9796 and 0.8140 on two large-scale ECG datasets, i.e., Chapman and CPSC-2018, respectively, which surpassed current state-of-the-art ECG classification methods, even those trained on 12-lead data. To encourage further development, our source code is publicly available at https://github.com/lhkhiem28/LightX3ECG.



### Global Consistent Point Cloud Registration Based on Lie-algebraic Cohomology
- **Arxiv ID**: http://arxiv.org/abs/2208.07103v1
- **DOI**: None
- **Categories**: **cs.CV**, 22E60, 14F45, I.4.5; I.3.5
- **Links**: [PDF](http://arxiv.org/pdf/2208.07103v1)
- **Published**: 2022-08-15 10:07:45+00:00
- **Updated**: 2022-08-15 10:07:45+00:00
- **Authors**: Yuxue Ren, Baowei Jiang, Wei Chen, Na Lei, Xianfeng David Gu
- **Comment**: 14 pages,6 figures
- **Journal**: None
- **Summary**: We present a novel, effective method for global point cloud registration problems by geometric topology. Based on many point cloud pairwise registration methods (e.g ICP), we focus on the problem of accumulated error for the composition of transformations along any loops. The major technical contribution of this paper is a linear method for the elimination of errors, using only solving a Poisson equation. We demonstrate the consistency of our method from Hodge-Helmhotz decomposition theorem and experiments on multiple RGBD datasets of real-world scenes. The experimental results also demonstrate that our global registration method runs quickly and provides accurate reconstructions.



### Context-aware Mixture-of-Experts for Unbiased Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2208.07109v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07109v3)
- **Published**: 2022-08-15 10:39:55+00:00
- **Updated**: 2023-01-01 08:02:45+00:00
- **Authors**: Liguang Zhou, Yuhongze Zhou, Tin Lun Lam, Yangsheng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Scene graph generation (SGG) has gained tremendous progress in recent years. However, its underlying long-tailed distribution of predicate classes is a challenging problem. For extremely unbalanced predicate distributions, existing approaches usually construct complicated context encoders to extract the intrinsic relevance of scene context to predicates and complex networks to improve the learning ability of network models for highly imbalanced predicate distributions. To address the unbiased SGG problem, we introduce a simple yet effective method dubbed Context-Aware Mixture-of-Experts (CAME) to improve model diversity and mitigate biased SGG without complicated design. Specifically, we propose to integrate the mixture of experts with a divide and ensemble strategy to remedy the severely long-tailed distribution of predicate classes, which is applicable to the majority of unbiased scene graph generators. The biased SGG is thereby reduced, and the model tends to anticipate more evenly distributed predicate predictions. To differentiate between various predicate distribution levels, experts with the same weights are not sufficiently diverse. In order to enable the network dynamically exploit the rich scene context and further boost the diversity of model, we simply use the built-in module to create a context encoder. The importance of each expert to scene context and each predicate to each expert is dynamically associated with expert weighting (EW) and predicate weighting (PW) strategy. We have conducted extensive experiments on three tasks using the Visual Genome dataset, showing that CAME outperforms recent methods and achieves state-of-the-art performance. Our code will be available publicly.



### A Unified Image Preprocessing Framework For Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2208.07110v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.07110v1)
- **Published**: 2022-08-15 10:41:00+00:00
- **Updated**: 2022-08-15 10:41:00+00:00
- **Authors**: Moqi Zhang, Weihui Deng, Xiaocheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of streaming media technology, increasing communication relies on sound and visual information, which puts a massive burden on online media. Data compression becomes increasingly important to reduce the volume of data transmission and storage. To further improve the efficiency of image compression, researchers utilize various image processing methods to compensate for the limitations of conventional codecs and advanced learning-based compression methods. Instead of modifying the image compression oriented approaches, we propose a unified image compression preprocessing framework, called Kuchen, which aims to further improve the performance of existing codecs. The framework consists of a hybrid data labeling system along with a learning-based backbone to simulate personalized preprocessing. As far as we know, this is the first exploration of setting a unified preprocessing benchmark in image compression tasks. Results demonstrate that the modern codecs optimized by our unified preprocessing framework constantly improve the efficiency of the state-of-the-art compression.



### An Empirical Study of Pseudo-Labeling for Image-based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.07137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07137v1)
- **Published**: 2022-08-15 12:17:46+00:00
- **Updated**: 2022-08-15 12:17:46+00:00
- **Authors**: Xinzhu Ma, Yuan Meng, Yinmin Zhang, Lei Bai, Jun Hou, Shuai Yi, Wanli Ouyang
- **Comment**: tech report
- **Journal**: None
- **Summary**: Image-based 3D detection is an indispensable component of the perception system for autonomous driving. However, it still suffers from the unsatisfying performance, one of the main reasons for which is the limited training data. Unfortunately, annotating the objects in the 3D space is extremely time/resource-consuming, which makes it hard to extend the training set arbitrarily. In this work, we focus on the semi-supervised manner and explore the feasibility of a cheaper alternative, i.e. pseudo-labeling, to leverage the unlabeled data. For this purpose, we conduct extensive experiments to investigate whether the pseudo-labels can provide effective supervision for the baseline models under varying settings. The experimental results not only demonstrate the effectiveness of the pseudo-labeling mechanism for image-based 3D detection (e.g. under monocular setting, we achieve 20.23 AP for moderate level on the KITTI-3D testing set without bells and whistles, improving the baseline model by 6.03 AP), but also show several interesting and noteworthy findings (e.g. the models trained with pseudo-labels perform better than that trained with ground-truth annotations based on the same training data). We hope this work can provide insights for the image-based 3D detection community under a semi-supervised setting. The codes, pseudo-labels, and pre-trained models will be publicly available.



### Perspective Reconstruction of Human Faces by Joint Mesh and Landmark Regression
- **Arxiv ID**: http://arxiv.org/abs/2208.07142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07142v1)
- **Published**: 2022-08-15 12:32:20+00:00
- **Updated**: 2022-08-15 12:32:20+00:00
- **Authors**: Jia Guo, Jinke Yu, Alexandros Lattas, Jiankang Deng
- **Comment**: ECCV 2022 WCPA Workshop Report (1st place)
- **Journal**: None
- **Summary**: Even though 3D face reconstruction has achieved impressive progress, most orthogonal projection-based face reconstruction methods can not achieve accurate and consistent reconstruction results when the face is very close to the camera due to the distortion under the perspective projection. In this paper, we propose to simultaneously reconstruct 3D face mesh in the world space and predict 2D face landmarks on the image plane to address the problem of perspective 3D face reconstruction. Based on the predicted 3D vertices and 2D landmarks, the 6DoF (6 Degrees of Freedom) face pose can be easily estimated by the PnP solver to represent perspective projection. Our approach achieves 1st place on the leader-board of the ECCV 2022 WCPA challenge and our model is visually robust under different identities, expressions and poses. The training code and models are released to facilitate future research.



### Urban precipitation downscaling using deep learning: a smart city application over Austin, Texas, USA
- **Arxiv ID**: http://arxiv.org/abs/2209.06848v1
- **DOI**: None
- **Categories**: **physics.ao-ph**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.06848v1)
- **Published**: 2022-08-15 12:42:20+00:00
- **Updated**: 2022-08-15 12:42:20+00:00
- **Authors**: Manmeet Singh, Nachiketa Acharya, Sajad Jamshidi, Junfeng Jiao, Zong-Liang Yang, Marc Coudert, Zach Baumer, Dev Niyogi
- **Comment**: None
- **Journal**: None
- **Summary**: Urban downscaling is a link to transfer the knowledge from coarser climate information to city scale assessments. These high-resolution assessments need multiyear climatology of past data and future projections, which are complex and computationally expensive to generate using traditional numerical weather prediction models. The city of Austin, Texas, USA has seen tremendous growth in the past decade. Systematic planning for the future requires the availability of fine resolution city-scale datasets. In this study, we demonstrate a novel approach generating a general purpose operator using deep learning to perform urban downscaling. The algorithm employs an iterative super-resolution convolutional neural network (Iterative SRCNN) over the city of Austin, Texas, USA. We show the development of a high-resolution gridded precipitation product (300 m) from a coarse (10 km) satellite-based product (JAXA GsMAP). High resolution gridded datasets of precipitation offer insights into the spatial distribution of heavy to low precipitation events in the past. The algorithm shows improvement in the mean peak-signal-to-noise-ratio and mutual information to generate high resolution gridded product of size 300 m X 300 m relative to the cubic interpolation baseline. Our results have implications for developing high-resolution gridded-precipitation urban datasets and the future planning of smart cities for other cities and other climatic variables.



### Where is VALDO? VAscular Lesions Detection and segmentatiOn challenge at MICCAI 2021
- **Arxiv ID**: http://arxiv.org/abs/2208.07167v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.07167v1)
- **Published**: 2022-08-15 13:09:38+00:00
- **Updated**: 2022-08-15 13:09:38+00:00
- **Authors**: Carole H. Sudre, Kimberlin Van Wijnen, Florian Dubost, Hieab Adams, David Atkinson, Frederik Barkhof, Mahlet A. Birhanu, Esther E. Bron, Robin Camarasa, Nish Chaturvedi, Yuan Chen, Zihao Chen, Shuai Chen, Qi Dou, Tavia Evans, Ivan Ezhov, Haojun Gao, Marta Girones Sanguesa, Juan Domingo Gispert, Beatriz Gomez Anson, Alun D. Hughes, M. Arfan Ikram, Silvia Ingala, H. Rolf Jaeger, Florian Kofler, Hugo J. Kuijf, Denis Kutnar, Minho Lee, Bo Li, Luigi Lorenzini, Bjoern Menze, Jose Luis Molinuevo, Yiwei Pan, Elodie Puybareau, Rafael Rehwald, Ruisheng Su, Pengcheng Shi, Lorna Smith, Therese Tillin, Guillaume Tochon, Helene Urien, Bas H. M. van der Velden, Isabelle F. van der Velpen, Benedikt Wiestler, Frank J. Wolters, Pinar Yilmaz, Marius de Groot, Meike W. Vernooij, Marleen de Bruijne
- **Comment**: None
- **Journal**: None
- **Summary**: Imaging markers of cerebral small vessel disease provide valuable information on brain health, but their manual assessment is time-consuming and hampered by substantial intra- and interrater variability. Automated rating may benefit biomedical research, as well as clinical assessment, but diagnostic reliability of existing algorithms is unknown. Here, we present the results of the \textit{VAscular Lesions DetectiOn and Segmentation} (\textit{Where is VALDO?}) challenge that was run as a satellite event at the international conference on Medical Image Computing and Computer Aided Intervention (MICCAI) 2021. This challenge aimed to promote the development of methods for automated detection and segmentation of small and sparse imaging markers of cerebral small vessel disease, namely enlarged perivascular spaces (EPVS) (Task 1), cerebral microbleeds (Task 2) and lacunes of presumed vascular origin (Task 3) while leveraging weak and noisy labels. Overall, 12 teams participated in the challenge proposing solutions for one or more tasks (4 for Task 1 - EPVS, 9 for Task 2 - Microbleeds and 6 for Task 3 - Lacunes). Multi-cohort data was used in both training and evaluation. Results showed a large variability in performance both across teams and across tasks, with promising results notably for Task 1 - EPVS and Task 2 - Microbleeds and not practically useful results yet for Task 3 - Lacunes. It also highlighted the performance inconsistency across cases that may deter use at an individual level, while still proving useful at a population level.



### A Man-in-the-Middle Attack against Object Detection Systems
- **Arxiv ID**: http://arxiv.org/abs/2208.07174v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.07174v3)
- **Published**: 2022-08-15 13:21:41+00:00
- **Updated**: 2023-08-21 22:42:48+00:00
- **Authors**: Han Wu, Sareh Rowlands, Johan Wahlstrom
- **Comment**: 6 pages, 7 figures
- **Journal**: None
- **Summary**: Object detection systems using deep learning models have become increasingly popular in robotics thanks to the rising power of CPUs and GPUs in embedded systems. However, these models are susceptible to adversarial attacks. While some attacks are limited by strict assumptions on access to the detection system, we propose a novel hardware attack inspired by Man-in-the-Middle attacks in cryptography. This attack generates an Universal Adversarial Perturbation (UAP) and then inject the perturbation between the USB camera and the detection system via a hardware attack. Besides, prior research is misled by an evaluation metric that measures the model accuracy rather than the attack performance. In combination with our proposed evaluation metrics, we significantly increases the strength of adversarial perturbations. These findings raise serious concerns for applications of deep learning models in safety-critical systems, such as autonomous driving.



### One-shot Generative Prior in Hankel-k-space for Parallel Imaging Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2208.07181v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.07181v4)
- **Published**: 2022-08-15 13:36:44+00:00
- **Updated**: 2022-12-08 02:31:53+00:00
- **Authors**: Hong Peng, Chen Jiang, Jing Cheng, Minghui Zhang, Shanshan Wang, Dong Liang, Qiegen Liu
- **Comment**: 10 pages,10 figures,7 tables
- **Journal**: None
- **Summary**: Magnetic resonance imaging serves as an essential tool for clinical diagnosis. However, it suffers from a long acquisition time. The utilization of deep learning, especially the deep generative models, offers aggressive acceleration and better reconstruction in magnetic resonance imaging. Nevertheless, learning the data distribution as prior knowledge and reconstructing the image from limited data remains challenging. In this work, we propose a novel Hankel-k-space generative model (HKGM), which can generate samples from a training set of as little as one k-space data. At the prior learning stage, we first construct a large Hankel matrix from k-space data, then extract multiple structured k-space patches from the large Hankel matrix to capture the internal distribution among different patches. Extracting patches from a Hankel matrix enables the generative model to be learned from redundant and low-rank data space. At the iterative reconstruction stage, it is observed that the desired solution obeys the learned prior knowledge. The intermediate reconstruction solution is updated by taking it as the input of the generative model. The updated result is then alternatively operated by imposing low-rank penalty on its Hankel matrix and data consistency con-strain on the measurement data. Experimental results confirmed that the internal statistics of patches within a single k-space data carry enough information for learning a powerful generative model and provide state-of-the-art reconstruction.



### Generating Pixel Art Character Sprites using GANs
- **Arxiv ID**: http://arxiv.org/abs/2208.06413v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.06413v1)
- **Published**: 2022-08-15 14:14:19+00:00
- **Updated**: 2022-08-15 14:14:19+00:00
- **Authors**: Flávio Coutinho, Luiz Chaimowicz
- **Comment**: This article has been submitted to SBGames 2022
- **Journal**: None
- **Summary**: Iterating on creating pixel art character sprite sheets is essential to the game development process. However, it can take a lot of effort until the final versions containing different poses and animation clips are achieved. This paper investigates using conditional generative adversarial networks to aid the designers in creating such sprite sheets. We propose an architecture based on Pix2Pix to generate images of characters facing a target side (e.g., right) given sprites of them in a source pose (e.g., front). Experiments with small pixel art datasets yielded promising results, resulting in models with varying degrees of generalization, sometimes capable of generating images very close to the ground truth. We analyze the results through visual inspection and quantitatively with FID.



### DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images
- **Arxiv ID**: http://arxiv.org/abs/2208.07227v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2208.07227v2)
- **Published**: 2022-08-15 14:32:10+00:00
- **Updated**: 2023-03-10 07:12:32+00:00
- **Authors**: Bing Wang, Lu Chen, Bo Yang
- **Comment**: ICLR 2023. Our data and code are available at:
  https://github.com/vLAR-group/DM-NeRF
- **Journal**: None
- **Summary**: In this paper, we study the problem of 3D scene geometry decomposition and manipulation from 2D views. By leveraging the recent implicit neural representation techniques, particularly the appealing neural radiance fields, we introduce an object field component to learn unique codes for all individual objects in 3D space only from 2D supervision. The key to this component is a series of carefully designed loss functions to enable every 3D point, especially in non-occupied space, to be effectively optimized even without 3D labels. In addition, we introduce an inverse query algorithm to freely manipulate any specified 3D object shape in the learned scene representation. Notably, our manipulation algorithm can explicitly tackle key issues such as object collisions and visual occlusions. Our method, called DM-NeRF, is among the first to simultaneously reconstruct, decompose, manipulate and render complex 3D scenes in a single pipeline. Extensive experiments on three datasets clearly show that our method can accurately decompose all 3D objects from 2D views, allowing any interested object to be freely manipulated in 3D space such as translation, rotation, size adjustment, and deformation.



### HEFT: Homomorphically Encrypted Fusion of Biometric Templates
- **Arxiv ID**: http://arxiv.org/abs/2208.07241v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2208.07241v1)
- **Published**: 2022-08-15 14:55:08+00:00
- **Updated**: 2022-08-15 14:55:08+00:00
- **Authors**: Luke Sperling, Nalini Ratha, Arun Ross, Vishnu Naresh Boddeti
- **Comment**: IJCB 2022
- **Journal**: None
- **Summary**: This paper proposes a non-interactive end-to-end solution for secure fusion and matching of biometric templates using fully homomorphic encryption (FHE). Given a pair of encrypted feature vectors, we perform the following ciphertext operations, i) feature concatenation, ii) fusion and dimensionality reduction through a learned linear projection, iii) scale normalization to unit $\ell_2$-norm, and iv) match score computation. Our method, dubbed HEFT (Homomorphically Encrypted Fusion of biometric Templates), is custom-designed to overcome the unique constraint imposed by FHE, namely the lack of support for non-arithmetic operations. From an inference perspective, we systematically explore different data packing schemes for computationally efficient linear projection and introduce a polynomial approximation for scale normalization. From a training perspective, we introduce an FHE-aware algorithm for learning the linear projection matrix to mitigate errors induced by approximate normalization. Experimental evaluation for template fusion and matching of face and voice biometrics shows that HEFT (i) improves biometric verification performance by 11.07% and 9.58% AUROC compared to the respective unibiometric representations while compressing the feature vectors by a factor of 16 (512D to 32D), and (ii) fuses a pair of encrypted feature vectors and computes its match score against a gallery of size 1024 in 884 ms. Code and data are available at https://github.com/human-analysis/encrypted-biometric-fusion



### Multi-modal Transformer Path Prediction for Autonomous Vehicle
- **Arxiv ID**: http://arxiv.org/abs/2208.07256v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.07256v1)
- **Published**: 2022-08-15 15:09:26+00:00
- **Updated**: 2022-08-15 15:09:26+00:00
- **Authors**: Chia Hong Tseng, Jie Zhang, Min-Te Sun, Kazuya Sakai, Wei-Shinn Ku
- **Comment**: 9 pages, 12 figures, and 5 tables
- **Journal**: None
- **Summary**: Reasoning about vehicle path prediction is an essential and challenging problem for the safe operation of autonomous driving systems. There exist many research works for path prediction. However, most of them do not use lane information and are not based on the Transformer architecture. By utilizing different types of data collected from sensors equipped on the self-driving vehicles, we propose a path prediction system named Multi-modal Transformer Path Prediction (MTPP) that aims to predict long-term future trajectory of target agents. To achieve more accurate path prediction, the Transformer architecture is adopted in our model. To better utilize the lane information, the lanes which are in opposite direction to target agent are not likely to be taken by the target agent and are consequently filtered out. In addition, consecutive lane chunks are combined to ensure the lane input to be long enough for path prediction. An extensive evaluation is conducted to show the efficacy of the proposed system using nuScene, a real-world trajectory forecasting dataset.



### Self-Supervised Multimodal Fusion Transformer for Passive Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.03765v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.03765v1)
- **Published**: 2022-08-15 15:38:10+00:00
- **Updated**: 2022-08-15 15:38:10+00:00
- **Authors**: Armand K. Koupai, Mohammud J. Bocus, Raul Santos-Rodriguez, Robert J. Piechocki, Ryan McConville
- **Comment**: 9 pages, 7 figures, submitted to IET Wireless Sensor Systems
- **Journal**: None
- **Summary**: The pervasiveness of Wi-Fi signals provides significant opportunities for human sensing and activity recognition in fields such as healthcare. The sensors most commonly used for passive Wi-Fi sensing are based on passive Wi-Fi radar (PWR) and channel state information (CSI) data, however current systems do not effectively exploit the information acquired through multiple sensors to recognise the different activities. In this paper, we explore new properties of the Transformer architecture for multimodal sensor fusion. We study different signal processing techniques to extract multiple image-based features from PWR and CSI data such as spectrograms, scalograms and Markov transition field (MTF). We first propose the Fusion Transformer, an attention-based model for multimodal and multi-sensor fusion. Experimental results show that our Fusion Transformer approach can achieve competitive results compared to a ResNet architecture but with much fewer resources. To further improve our model, we propose a simple and effective framework for multimodal and multi-sensor self-supervised learning (SSL). The self-supervised Fusion Transformer outperforms the baselines, achieving a F1-score of 95.9%. Finally, we show how this approach significantly outperforms the others when trained with as little as 1% (2 minutes) of labelled training data to 20% (40 minutes) of labelled training data.



### Elderly Fall Detection Using CCTV Cameras under Partial Occlusion of the Subjects Body
- **Arxiv ID**: http://arxiv.org/abs/2208.07291v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07291v1)
- **Published**: 2022-08-15 16:02:18+00:00
- **Updated**: 2022-08-15 16:02:18+00:00
- **Authors**: Sara Khalili, Hoda Mohammadzade, Mohammad Mahdi Ahmadi
- **Comment**: 12 pages,7 figures, 5 tables
- **Journal**: None
- **Summary**: One of the possible dangers that older people face in their daily lives is falling. Occlusion is one of the biggest challenges of vision-based fall detection systems and degrades their detection performance considerably. To tackle this problem, we synthesize specifically-designed occluded videos for training fall detection systems using existing datasets. Then, by defining a new cost function, we introduce a framework for weighted training of fall detection models using occluded and un-occluded videos, which can be applied to any learnable fall detection system. Finally, we use both a non-deep and deep model to evaluate the effect of the proposed weighted training method. Experiments show that the proposed method can improve the classification accuracy by 36% for a non-deep model and 55% for a deep model in occlusion conditions. Moreover, it is shown that the proposed training framework can also significantly improve the detection performance of a deep network on normal un-occluded samples.



### Task Oriented Video Coding: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2208.07313v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.07313v3)
- **Published**: 2022-08-15 16:21:54+00:00
- **Updated**: 2022-11-20 21:47:49+00:00
- **Authors**: Daniel Wood
- **Comment**: arXiv admin note: text overlap with arXiv:2201.10162,
  arXiv:2001.03004, arXiv:2001.03569, arXiv:2110.09241, arXiv:2203.05890,
  arXiv:2203.05927, arXiv:2106.03511, arXiv:2001.02915, arXiv:2102.01307,
  arXiv:1812.10067, arXiv:2203.05944, arXiv:2201.02689, arXiv:2105.12653,
  arXiv:2112.10948, arXiv:1803.05788 by other authors
- **Journal**: None
- **Summary**: Video coding technology has been continuously improved for higher compression ratio with higher resolution. However, the state-of-the-art video coding standards, such as H.265/HEVC and Versatile Video Coding, are still designed with the assumption the compressed video will be watched by humans. With the tremendous advance and maturation of deep neural networks in solving computer vision tasks, more and more videos are directly analyzed by deep neural networks without humans' involvement. Such a conventional design for video coding standard is not optimal when the compressed video is used by computer vision applications. While the human visual system is consistently sensitive to the content with high contrast, the impact of pixels on computer vision algorithms is driven by specific computer vision tasks. In this paper, we explore and summarize recent progress on computer vision task oriented video coding and emerging video coding standard, Video Coding for Machines.



### Learn2Trust: A video and streamlit-based educational programme for AI-based medical image analysis targeted towards medical students
- **Arxiv ID**: http://arxiv.org/abs/2208.07314v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2208.07314v1)
- **Published**: 2022-08-15 16:26:13+00:00
- **Updated**: 2022-08-15 16:26:13+00:00
- **Authors**: Hanna Siebert, Marian Himstedt, Mattias Heinrich
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: In order to be able to use artificial intelligence (AI) in medicine without scepticism and to recognise and assess its growing potential, a basic understanding of this topic is necessary among current and future medical staff. Under the premise of "trust through understanding", we developed an innovative online course as a learning opportunity within the framework of the German KI Campus (AI campus) project, which is a self-guided course that teaches the basics of AI for the analysis of medical image data. The main goal is to provide a learning environment for a sufficient understanding of AI in medical image analysis so that further interest in this topic is stimulated and inhibitions towards its use can be overcome by means of positive application experience. The focus was on medical applications and the fundamentals of machine learning. The online course was divided into consecutive lessons, which include theory in the form of explanatory videos, practical exercises in the form of Streamlit and practical exercises and/or quizzes to check learning progress. A survey among the participating medical students in the first run of the course was used to analyse our research hypotheses quantitatively.



### Cross-scale Attention Guided Multi-instance Learning for Crohn's Disease Diagnosis with Pathological Images
- **Arxiv ID**: http://arxiv.org/abs/2208.07322v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.07322v1)
- **Published**: 2022-08-15 16:39:34+00:00
- **Updated**: 2022-08-15 16:39:34+00:00
- **Authors**: Ruining Deng, Can Cui, Lucas W. Remedios, Shunxing Bao, R. Michael Womick, Sophie Chiron, Jia Li, Joseph T. Roland, Ken S. Lau, Qi Liu, Keith T. Wilson, Yaohong Wang, Lori A. Coburn, Bennett A. Landman, Yuankai Huo
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-instance learning (MIL) is widely used in the computer-aided interpretation of pathological Whole Slide Images (WSIs) to solve the lack of pixel-wise or patch-wise annotations. Often, this approach directly applies "natural image driven" MIL algorithms which overlook the multi-scale (i.e. pyramidal) nature of WSIs. Off-the-shelf MIL algorithms are typically deployed on a single-scale of WSIs (e.g., 20x magnification), while human pathologists usually aggregate the global and local patterns in a multi-scale manner (e.g., by zooming in and out between different magnifications). In this study, we propose a novel cross-scale attention mechanism to explicitly aggregate inter-scale interactions into a single MIL network for Crohn's Disease (CD), which is a form of inflammatory bowel disease. The contribution of this paper is two-fold: (1) a cross-scale attention mechanism is proposed to aggregate features from different resolutions with multi-scale interaction; and (2) differential multi-scale attention visualizations are generated to localize explainable lesion patterns. By training ~250,000 H&E-stained Ascending Colon (AC) patches from 20 CD patient and 30 healthy control samples at different scales, our approach achieved a superior Area under the Curve (AUC) score of 0.8924 compared with baseline models. The official implementation is publicly available at https://github.com/hrlblab/CS-MIL.



### SYN-MAD 2022: Competition on Face Morphing Attack Detection Based on Privacy-aware Synthetic Training Data
- **Arxiv ID**: http://arxiv.org/abs/2208.07337v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07337v1)
- **Published**: 2022-08-15 17:06:55+00:00
- **Updated**: 2022-08-15 17:06:55+00:00
- **Authors**: Marco Huber, Fadi Boutros, Anh Thi Luu, Kiran Raja, Raghavendra Ramachandra, Naser Damer, Pedro C. Neto, Tiago Gonçalves, Ana F. Sequeira, Jaime S. Cardoso, João Tremoço, Miguel Lourenço, Sergio Serra, Eduardo Cermeño, Marija Ivanovska, Borut Batagelj, Andrej Kronovšek, Peter Peer, Vitomir Štruc
- **Comment**: Accepted at International Joint Conference on Biometrics (IJCB) 2022
- **Journal**: None
- **Summary**: This paper presents a summary of the Competition on Face Morphing Attack Detection Based on Privacy-aware Synthetic Training Data (SYN-MAD) held at the 2022 International Joint Conference on Biometrics (IJCB 2022). The competition attracted a total of 12 participating teams, both from academia and industry and present in 11 different countries. In the end, seven valid submissions were submitted by the participating teams and evaluated by the organizers. The competition was held to present and attract solutions that deal with detecting face morphing attacks while protecting people's privacy for ethical and legal reasons. To ensure this, the training data was limited to synthetic data provided by the organizers. The submitted solutions presented innovations that led to outperforming the considered baseline in many experimental settings. The evaluation benchmark is now available at: https://github.com/marcohuber/SYN-MAD-2022.



### Action Recognition based on Cross-Situational Action-object Statistics
- **Arxiv ID**: http://arxiv.org/abs/2208.07344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07344v1)
- **Published**: 2022-08-15 17:23:55+00:00
- **Updated**: 2022-08-15 17:23:55+00:00
- **Authors**: Satoshi Tsutsui, Xizi Wang, Guangyuan Weng, Yayun Zhang, David Crandall, Chen Yu
- **Comment**: Accepted to International Conference on Development and Learning
  (ICDL) 2022
- **Journal**: None
- **Summary**: Machine learning models of visual action recognition are typically trained and tested on data from specific situations where actions are associated with certain objects. It is an open question how action-object associations in the training set influence a model's ability to generalize beyond trained situations. We set out to identify properties of training data that lead to action recognition models with greater generalization ability. To do this, we take inspiration from a cognitive mechanism called cross-situational learning, which states that human learners extract the meaning of concepts by observing instances of the same concept across different situations. We perform controlled experiments with various types of action-object associations, and identify key properties of action-object co-occurrence in training data that lead to better classifiers. Given that these properties are missing in the datasets that are typically used to train action classifiers in the computer vision literature, our work provides useful insights on how we should best construct datasets for efficiently training for better generalization.



### Three New Validators and a Large-Scale Benchmark Ranking for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2208.07360v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.07360v4)
- **Published**: 2022-08-15 17:55:26+00:00
- **Updated**: 2023-05-17 23:24:06+00:00
- **Authors**: Kevin Musgrave, Serge Belongie, Ser-Nam Lim
- **Comment**: This paper was previously titled Benchmarking Validation Methods for
  Unsupervised Domain Adaptation. This version contains new experiments,
  analysis, and figures
- **Journal**: None
- **Summary**: Changes to hyperparameters can have a dramatic effect on model accuracy. Thus, the tuning of hyperparameters plays an important role in optimizing machine-learning models. An integral part of the hyperparameter-tuning process is the evaluation of model checkpoints, which is done through the use of "validators". In a supervised setting, these validators evaluate checkpoints by computing accuracy on a validation set that has labels. In contrast, in an unsupervised setting, the validation set has no such labels. Without any labels, it is impossible to compute accuracy, so validators must estimate accuracy instead. But what is the best approach to estimating accuracy? In this paper, we consider this question in the context of unsupervised domain adaptation (UDA). Specifically, we propose three new validators, and we compare and rank them against five other existing validators, on a large dataset of 1,000,000 checkpoints. Extensive experimental results show that two of our proposed validators achieve state-of-the-art performance in various settings. Finally, we find that in many cases, the state-of-the-art is obtained by a simple baseline method. To the best of our knowledge, this is the largest empirical study of UDA validators to date. Code is available at https://www.github.com/KevinMusgrave/powerful-benchmarker.



### Online Pole Segmentation on Range Images for Long-term LiDAR Localization in Urban Environments
- **Arxiv ID**: http://arxiv.org/abs/2208.07364v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.07364v1)
- **Published**: 2022-08-15 17:58:08+00:00
- **Updated**: 2022-08-15 17:58:08+00:00
- **Authors**: Hao Dong, Xieyuanli Chen, Simo Särkkä, Cyrill Stachniss
- **Comment**: 10 pages, 7 figures. arXiv admin note: substantial text overlap with
  arXiv:2108.08621
- **Journal**: None
- **Summary**: Robust and accurate localization is a basic requirement for mobile autonomous systems. Pole-like objects, such as traffic signs, poles, and lamps are frequently used landmarks for localization in urban environments due to their local distinctiveness and long-term stability. In this paper, we present a novel, accurate, and fast pole extraction approach based on geometric features that runs online and has little computational demands. Our method performs all computations directly on range images generated from 3D LiDAR scans, which avoids processing 3D point clouds explicitly and enables fast pole extraction for each scan. We further use the extracted poles as pseudo labels to train a deep neural network for online range image-based pole segmentation. We test both our geometric and learning-based pole extraction methods for localization on different datasets with different LiDAR scanners, routes, and seasonal changes. The experimental results show that our methods outperform other state-of-the-art approaches. Moreover, boosted with pseudo pole labels extracted from multiple datasets, our learning-based method can run across different datasets and achieve even better localization results compared to our geometry-based method. We released our pole datasets to the public for evaluating the performance of pole extractors, as well as the implementation of our approach.



### Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective
- **Arxiv ID**: http://arxiv.org/abs/2208.07365v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.07365v2)
- **Published**: 2022-08-15 17:59:31+00:00
- **Updated**: 2023-06-09 15:06:02+00:00
- **Authors**: Pengfei Wei, Lingdong Kong, Xinghua Qu, Yi Ren, Zhiqiang Xu, Jing Jiang, Xiang Yin
- **Comment**: 18 pages, 9 figures, 7 tables. Code at
  https://github.com/ldkong1205/TranSVAE
- **Journal**: None
- **Summary**: Unsupervised video domain adaptation is a practical yet challenging task. In this work, for the first time, we tackle it from a disentanglement view. Our key idea is to handle the spatial and temporal domain divergence separately through disentanglement. Specifically, we consider the generation of cross-domain videos from two sets of latent factors, one encoding the static information and another encoding the dynamic information. A Transfer Sequential VAE (TranSVAE) framework is then developed to model such generation. To better serve for adaptation, we propose several objectives to constrain the latent factors. With these constraints, the spatial divergence can be readily removed by disentangling the static domain-specific information out, and the temporal divergence is further reduced from both frame- and video-levels through adversarial learning. Extensive experiments on the UCF-HMDB, Jester, and Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE compared with several state-of-the-art methods. The code with reproducible results is publicly accessible.



### SemAug: Semantically Meaningful Image Augmentations for Object Detection Through Language Grounding
- **Arxiv ID**: http://arxiv.org/abs/2208.07407v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07407v1)
- **Published**: 2022-08-15 19:00:56+00:00
- **Updated**: 2022-08-15 19:00:56+00:00
- **Authors**: Morgan Heisler, Amin Banitalebi-Dehkordi, Yong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation is an essential technique in improving the generalization of deep neural networks. The majority of existing image-domain augmentations either rely on geometric and structural transformations, or apply different kinds of photometric distortions. In this paper, we propose an effective technique for image augmentation by injecting contextually meaningful knowledge into the scenes. Our method of semantically meaningful image augmentation for object detection via language grounding, SemAug, starts by calculating semantically appropriate new objects that can be placed into relevant locations in the image (the what and where problems). Then it embeds these objects into their relevant target locations, thereby promoting diversity of object instance distribution. Our method allows for introducing new object instances and categories that may not even exist in the training set. Furthermore, it does not require the additional overhead of training a context network, so it can be easily added to existing architectures. Our comprehensive set of evaluations showed that the proposed method is very effective in improving the generalization, while the overhead is negligible. In particular, for a wide range of model architectures, our method achieved ~2-4% and ~1-2% mAP improvements for the task of object detection on the Pascal VOC and COCO datasets, respectively.



### An Efficient Multi-Scale Fusion Network for 3D Organ at Risk (OAR) Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.07417v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.07417v1)
- **Published**: 2022-08-15 19:40:18+00:00
- **Updated**: 2022-08-15 19:40:18+00:00
- **Authors**: Abhishek Srivastava, Debesh Jha, Elif Keles, Bulent Aydogan, Mohamed Abazeed, Ulas Bagci
- **Comment**: 4 pages, 2 figures, 2 tables
- **Journal**: None
- **Summary**: Accurate segmentation of organs-at-risks (OARs) is a precursor for optimizing radiation therapy planning. Existing deep learning-based multi-scale fusion architectures have demonstrated a tremendous capacity for 2D medical image segmentation. The key to their success is aggregating global context and maintaining high resolution representations. However, when translated into 3D segmentation problems, existing multi-scale fusion architectures might underperform due to their heavy computation overhead and substantial data diet. To address this issue, we propose a new OAR segmentation framework, called OARFocalFuseNet, which fuses multi-scale features and employs focal modulation for capturing global-local context across multiple scales. Each resolution stream is enriched with features from different resolution scales, and multi-scale information is aggregated to model diverse contextual ranges. As a result, feature representations are further boosted. The comprehensive comparisons in our experimental setup with OAR segmentation as well as multi-organ segmentation show that our proposed OARFocalFuseNet outperforms the recent state-of-the-art methods on publicly available OpenKBP datasets and Synapse multi-organ segmentation. Both of the proposed methods (3D-MSF and OARFocalFuseNet) showed promising performance in terms of standard evaluation metrics. Our best performing method (OARFocalFuseNet) obtained a dice coefficient of 0.7995 and hausdorff distance of 5.1435 on OpenKBP datasets and dice coefficient of 0.8137 on Synapse multi-organ segmentation dataset.



### Synergistic Integration of Techniques of VC, Communication Technologies and Unities of Calculation Transportable for Generate a System Embedded That Monitors Pyroclastic Flows in Real Time
- **Arxiv ID**: http://arxiv.org/abs/2208.08884v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY, eess.IV, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2208.08884v1)
- **Published**: 2022-08-15 20:02:22+00:00
- **Updated**: 2022-08-15 20:02:22+00:00
- **Authors**: Kevin Barrera Llanga, Cruz Christian, Viteri Xavier, Mendoza Dario
- **Comment**: 5 pages. Journal of Engineering and Applied Sciences 2018
- **Journal**: None
- **Summary**: At the end of an extensive investigation of the volcanic eruptions in the world, we determined patterns that coincide in this process, this data can be analyzed by artificial vision, obtaining the largest amount of information from images in an embedded system, using monitoring algorithms for compare continuous matrices, control camera positioning and link this information with mass communication technologies. The present work shows the development of a viable early warning technology solution that allows to analyze the behavior of volcanic flows automatically in a rash in real time, with a very high level of efficiency in the analysis of possible trajectories, direction and quantity of the lava flows as well as the massive mass media directed to the affected people.



### Deep Unsupervised Domain Adaptation: A Review of Recent Advances and Perspectives
- **Arxiv ID**: http://arxiv.org/abs/2208.07422v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2208.07422v1)
- **Published**: 2022-08-15 20:05:07+00:00
- **Updated**: 2022-08-15 20:05:07+00:00
- **Authors**: Xiaofeng Liu, Chaehwa Yoo, Fangxu Xing, Hyejin Oh, Georges El Fakhri, Je-Won Kang, Jonghye Woo
- **Comment**: APSIPA Transactions on Signal and Information Processing
- **Journal**: None
- **Summary**: Deep learning has become the method of choice to tackle real-world problems in different domains, partly because of its ability to learn from data and achieve impressive performance on a wide range of applications. However, its success usually relies on two assumptions: (i) vast troves of labeled datasets are required for accurate model fitting, and (ii) training and testing data are independent and identically distributed. Its performance on unseen target domains, thus, is not guaranteed, especially when encountering out-of-distribution data at the adaptation stage. The performance drop on data in a target domain is a critical problem in deploying deep neural networks that are successfully trained on data in a source domain. Unsupervised domain adaptation (UDA) is proposed to counter this, by leveraging both labeled source domain data and unlabeled target domain data to carry out various tasks in the target domain. UDA has yielded promising results on natural image processing, video analysis, natural language processing, time-series data analysis, medical image analysis, etc. In this review, as a rapidly evolving topic, we provide a systematic comparison of its methods and applications. In addition, the connection of UDA with its closely related tasks, e.g., domain generalization and out-of-distribution detection, has also been discussed. Furthermore, deficiencies in current methods and possible promising directions are highlighted.



### WatchPed: Pedestrian Crossing Intention Prediction Using Embedded Sensors of Smartwatch
- **Arxiv ID**: http://arxiv.org/abs/2208.07441v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2208.07441v3)
- **Published**: 2022-08-15 21:27:21+00:00
- **Updated**: 2023-03-16 03:50:31+00:00
- **Authors**: Jibran Ali Abbasi, Navid Mohammad Imran, Lokesh Chandra Das, Myounggyu Won
- **Comment**: None
- **Journal**: None
- **Summary**: The pedestrian crossing intention prediction problem is to estimate whether or not the target pedestrian will cross the street. State-of-the-art techniques heavily depend on visual data acquired through the front camera of the ego-vehicle to make a prediction of the pedestrian's crossing intention. Hence, the efficiency of current methodologies tends to decrease notably in situations where visual input is imprecise, for instance, when the distance between the pedestrian and ego-vehicle is considerable or the illumination levels are inadequate. To address the limitation, in this paper, we present the design, implementation, and evaluation of the first-of-its-kind pedestrian crossing intention prediction model based on integration of motion sensor data gathered through the smartwatch (or smartphone) of the pedestrian. We propose an innovative machine learning framework that effectively integrates motion sensor data with visual input to enhance the predictive accuracy significantly, particularly in scenarios where visual data may be unreliable. Moreover, we perform an extensive data collection process and introduce the first pedestrian intention prediction dataset that features synchronized motion sensor data. The dataset comprises 255 video clips that encompass diverse distances and lighting conditions. We trained our model using the widely-used JAAD and our own datasets and compare the performance with a state-of-the-art model. The results demonstrate that our model outperforms the current state-of-the-art method, particularly in cases where the distance between the pedestrian and the observer is considerable (more than 70 meters) and the lighting conditions are inadequate.



### Conv-Adapter: Exploring Parameter Efficient Transfer Learning for ConvNets
- **Arxiv ID**: http://arxiv.org/abs/2208.07463v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.07463v3)
- **Published**: 2022-08-15 22:51:23+00:00
- **Updated**: 2022-08-24 16:27:24+00:00
- **Authors**: Hao Chen, Ran Tao, Han Zhang, Yidong Wang, Wei Ye, Jindong Wang, Guosheng Hu, Marios Savvides
- **Comment**: wrong version
- **Journal**: None
- **Summary**: While parameter efficient tuning (PET) methods have shown great potential with transformer architecture on Natural Language Processing (NLP) tasks, their effectiveness is still under-studied with large-scale ConvNets on Computer Vision (CV) tasks. This paper proposes Conv-Adapter, a PET module designed for ConvNets. Conv-Adapter is light-weight, domain-transferable, and architecture-agnostic with generalized performance on different tasks. When transferring on downstream tasks, Conv-Adapter learns tasks-specific feature modulation to the intermediate representations of backbone while keeping the pre-trained parameters frozen. By introducing only a tiny amount of learnable parameters, e.g., only 3.5% full fine-tuning parameters of ResNet50, Conv-Adapter outperforms previous PET baseline methods and achieves comparable or surpasses the performance of full fine-tuning on 23 classification tasks of various domains. It also presents superior performance on few-shot classifications, with an average margin of 3.39%. Beyond classification, Conv-Adapter can generalize to detection and segmentation tasks with more than 50% reduction of parameters but comparable performance to the traditional full fine-tuning.



### Towards Inclusive HRI: Using Sim2Real to Address Underrepresentation in Emotion Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2208.07472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07472v1)
- **Published**: 2022-08-15 23:37:13+00:00
- **Updated**: 2022-08-15 23:37:13+00:00
- **Authors**: Saba Akhyani, Mehryar Abbasi Boroujeni, Mo Chen, Angelica Lim
- **Comment**: 8 pages, 10 figures, submitted to IROS2022
- **Journal**: None
- **Summary**: Robots and artificial agents that interact with humans should be able to do so without bias and inequity, but facial perception systems have notoriously been found to work more poorly for certain groups of people than others. In our work, we aim to build a system that can perceive humans in a more transparent and inclusive manner. Specifically, we focus on dynamic expressions on the human face, which are difficult to collect for a broad set of people due to privacy concerns and the fact that faces are inherently identifiable. Furthermore, datasets collected from the Internet are not necessarily representative of the general population. We address this problem by offering a Sim2Real approach in which we use a suite of 3D simulated human models that enables us to create an auditable synthetic dataset covering 1) underrepresented facial expressions, outside of the six basic emotions, such as confusion; 2) ethnic or gender minority groups; and 3) a wide range of viewing angles that a robot may encounter a human in the real world. By augmenting a small dynamic emotional expression dataset containing 123 samples with a synthetic dataset containing 4536 samples, we achieved an improvement in accuracy of 15% on our own dataset and 11% on an external benchmark dataset, compared to the performance of the same model architecture without synthetic training data. We also show that this additional step improves accuracy specifically for racial minorities when the architecture's feature extraction weights are trained from scratch.



### BoW3D: Bag of Words for Real-Time Loop Closing in 3D LiDAR SLAM
- **Arxiv ID**: http://arxiv.org/abs/2208.07473v2
- **DOI**: 10.1109/LRA.2022.3221336
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07473v2)
- **Published**: 2022-08-15 23:46:17+00:00
- **Updated**: 2022-11-18 02:35:19+00:00
- **Authors**: Yunge Cui, Xieyuanli Chen, Yinlong Zhang, Jiahua Dong, Qingxiao Wu, Feng Zhu
- **Comment**: Accepted by IEEE Robotics and Automation Letters (RA-L)/ICRA 2023
- **Journal**: None
- **Summary**: Loop closing is a fundamental part of simultaneous localization and mapping (SLAM) for autonomous mobile systems. In the field of visual SLAM, bag of words (BoW) has achieved great success in loop closure. The BoW features for loop searching can also be used in the subsequent 6-DoF loop correction. However, for 3D LiDAR SLAM, the state-of-the-art methods may fail to effectively recognize the loop in real time, and usually cannot correct the full 6-DoF loop pose. To address this limitation, we present a novel Bag of Words for real-time loop closing in 3D LiDAR SLAM, called BoW3D. Our method not only efficiently recognizes the revisited loop places, but also corrects the full 6-DoF loop pose in real time. BoW3D builds the bag of words based on the 3D LiDAR feature LinK3D, which is efficient, pose-invariant and can be used for accurate point-to-point matching. We furthermore embed our proposed method into 3D LiDAR odometry system to evaluate loop closing performance. We test our method on public dataset, and compare it against other state-of-the-art algorithms. BoW3D shows better performance in terms of F1 max and extended precision scores on most scenarios. It is noticeable that BoW3D takes an average of 48 ms to recognize and correct the loops on KITTI 00 (includes 4K+ 64-ray LiDAR scans), when executed on a notebook with an Intel Core i7 @2.2 GHz processor. We release the implementation of our method here: https://github.com/YungeCui/BoW3D.



