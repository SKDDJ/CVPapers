# Arxiv Papers in cs.CV on 2022-08-23
### Learning Visibility for Robust Dense Human Body Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.10652v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10652v1)
- **Published**: 2022-08-23 00:01:05+00:00
- **Updated**: 2022-08-23 00:01:05+00:00
- **Authors**: Chun-Han Yao, Jimei Yang, Duygu Ceylan, Yi Zhou, Yang Zhou, Ming-Hsuan Yang
- **Comment**: accepted by ECCV 2022
- **Journal**: None
- **Summary**: Estimating 3D human pose and shape from 2D images is a crucial yet challenging task. While prior methods with model-based representations can perform reasonably well on whole-body images, they often fail when parts of the body are occluded or outside the frame. Moreover, these results usually do not faithfully capture the human silhouettes due to their limited representation power of deformable models (e.g., representing only the naked body). An alternative approach is to estimate dense vertices of a predefined template body in the image space. Such representations are effective in localizing vertices within an image but cannot handle out-of-frame body parts. In this work, we learn dense human body estimation that is robust to partial observations. We explicitly model the visibility of human joints and vertices in the x, y, and z axes separately. The visibility in x and y axes help distinguishing out-of-frame cases, and the visibility in depth axis corresponds to occlusions (either self-occlusions or occlusions by other objects). We obtain pseudo ground-truths of visibility labels from dense UV correspondences and train a neural network to predict visibility along with 3D coordinates. We show that visibility can serve as 1) an additional signal to resolve depth ordering ambiguities of self-occluded vertices and 2) a regularization term when fitting a human body model to the predictions. Extensive experiments on multiple 3D human datasets demonstrate that visibility modeling significantly improves the accuracy of human body estimation, especially for partial-body cases. Our project page with code is at: https://github.com/chhankyao/visdb.



### A First Look at Dataset Bias in License Plate Recognition
- **Arxiv ID**: http://arxiv.org/abs/2208.10657v2
- **DOI**: 10.1109/SIBGRAPI55357.2022.9991768
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10657v2)
- **Published**: 2022-08-23 00:20:33+00:00
- **Updated**: 2022-12-30 10:23:26+00:00
- **Authors**: Rayson Laroca, Marcelo Santos, Valter Estevam, Eduardo Luz, David Menotti
- **Comment**: Accepted for presentation at the Conference on Graphics, Patterns and
  Images (SIBGRAPI) 2022
- **Journal**: None
- **Summary**: Public datasets have played a key role in advancing the state of the art in License Plate Recognition (LPR). Although dataset bias has been recognized as a severe problem in the computer vision community, it has been largely overlooked in the LPR literature. LPR models are usually trained and evaluated separately on each dataset. In this scenario, they have often proven robust in the dataset they were trained in but showed limited performance in unseen ones. Therefore, this work investigates the dataset bias problem in the LPR context. We performed experiments on eight datasets, four collected in Brazil and four in mainland China, and observed that each dataset has a unique, identifiable "signature" since a lightweight classification model predicts the source dataset of a license plate (LP) image with more than 95% accuracy. In our discussion, we draw attention to the fact that most LPR models are probably exploiting such signatures to improve the results achieved in each dataset at the cost of losing generalization capability. These results emphasize the importance of evaluating LPR models in cross-dataset setups, as they provide a better indication of generalization (hence real-world performance) than within-dataset ones.



### Unsupervised Fish Trajectory Tracking and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.10662v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10662v1)
- **Published**: 2022-08-23 01:01:27+00:00
- **Updated**: 2022-08-23 01:01:27+00:00
- **Authors**: Alzayat Saleh, Marcus Sheaves, Dean Jerry, Mostafa Rahimi Azghadi
- **Comment**: 16 pages, 11 figures. Submitted to the Engineering Applications of
  Artificial Intelligence journal
- **Journal**: None
- **Summary**: DNN for fish tracking and segmentation based on high-quality labels is expensive. Alternative unsupervised approaches rely on spatial and temporal variations that naturally occur in video data to generate noisy pseudo-ground-truth labels. These pseudo-labels are used to train a multi-task deep neural network. In this paper, we propose a three-stage framework for robust fish tracking and segmentation, where the first stage is an optical flow model, which generates the pseudo labels using spatial and temporal consistency between frames. In the second stage, a self-supervised model refines the pseudo-labels incrementally. In the third stage, the refined labels are used to train a segmentation network. No human annotations are used during the training or inference. Extensive experiments are performed to validate our method on three public underwater video datasets and to demonstrate that it is highly effective for video annotation and segmentation. We also evaluate the robustness of our framework to different imaging conditions and discuss the limitations of our current implementation.



### Learning from Noisy Labels with Coarse-to-Fine Sample Credibility Modeling
- **Arxiv ID**: http://arxiv.org/abs/2208.10683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10683v1)
- **Published**: 2022-08-23 02:06:38+00:00
- **Updated**: 2022-08-23 02:06:38+00:00
- **Authors**: Boshen Zhang, Yuxi Li, Yuanpeng Tu, Jinlong Peng, Yabiao Wang, Cunlin Wu, Yang Xiao, Cairong Zhao
- **Comment**: ECCV 2022: L2ID Workshop
- **Journal**: None
- **Summary**: Training deep neural network (DNN) with noisy labels is practically challenging since inaccurate labels severely degrade the generalization ability of DNN. Previous efforts tend to handle part or full data in a unified denoising flow via identifying noisy data with a coarse small-loss criterion to mitigate the interference from noisy labels, ignoring the fact that the difficulties of noisy samples are different, thus a rigid and unified data selection pipeline cannot tackle this problem well. In this paper, we first propose a coarse-to-fine robust learning method called CREMA, to handle noisy data in a divide-and-conquer manner. In coarse-level, clean and noisy sets are firstly separated in terms of credibility in a statistical sense. Since it is practically impossible to categorize all noisy samples correctly, we further process them in a fine-grained manner via modeling the credibility of each sample. Specifically, for the clean set, we deliberately design a memory-based modulation scheme to dynamically adjust the contribution of each sample in terms of its historical credibility sequence during training, thus alleviating the effect from noisy samples incorrectly grouped into the clean set. Meanwhile, for samples categorized into the noisy set, a selective label update strategy is proposed to correct noisy labels while mitigating the problem of correction error. Extensive experiments are conducted on benchmarks of different modalities, including image classification (CIFAR, Clothing1M etc) and text recognition (IMDB), with either synthetic or natural semantic noises, demonstrating the superiority and generality of CREMA.



### Hierarchical Perceptual Noise Injection for Social Media Fingerprint Privacy Protection
- **Arxiv ID**: http://arxiv.org/abs/2208.10688v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.10688v1)
- **Published**: 2022-08-23 02:20:46+00:00
- **Updated**: 2022-08-23 02:20:46+00:00
- **Authors**: Simin Li, Huangxinxin Xu, Jiakai Wang, Aishan Liu, Fazhi He, Xianglong Liu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Billions of people are sharing their daily life images on social media every day. However, their biometric information (e.g., fingerprint) could be easily stolen from these images. The threat of fingerprint leakage from social media raises a strong desire for anonymizing shared images while maintaining image qualities, since fingerprints act as a lifelong individual biometric password. To guard the fingerprint leakage, adversarial attack emerges as a solution by adding imperceptible perturbations on images. However, existing works are either weak in black-box transferability or appear unnatural. Motivated by visual perception hierarchy (i.e., high-level perception exploits model-shared semantics that transfer well across models while low-level perception extracts primitive stimulus and will cause high visual sensitivities given suspicious stimulus), we propose FingerSafe, a hierarchical perceptual protective noise injection framework to address the mentioned problems. For black-box transferability, we inject protective noises on fingerprint orientation field to perturb the model-shared high-level semantics (i.e., fingerprint ridges). Considering visual naturalness, we suppress the low-level local contrast stimulus by regularizing the response of Lateral Geniculate Nucleus. Our FingerSafe is the first to provide feasible fingerprint protection in both digital (up to 94.12%) and realistic scenarios (Twitter and Facebook, up to 68.75%). Our code can be found at https://github.com/nlsde-safety-team/FingerSafe.



### Spiral Contrastive Learning: An Efficient 3D Representation Learning Method for Unannotated CT Lesions
- **Arxiv ID**: http://arxiv.org/abs/2208.10694v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10694v1)
- **Published**: 2022-08-23 02:31:03+00:00
- **Updated**: 2022-08-23 02:31:03+00:00
- **Authors**: Penghua Zhai, Enwei Zhu, Baolian Qi, Xin Wei, Jinpeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Computed tomography (CT) samples with pathological annotations are difficult to obtain. As a result, the computer-aided diagnosis (CAD) algorithms are trained on small datasets (e.g., LIDC-IDRI with 1,018 samples), limiting their accuracies and reliability. In the past five years, several works have tailored for unsupervised representations of CT lesions via two-dimensional (2D) and three-dimensional (3D) self-supervised learning (SSL) algorithms. The 2D algorithms have difficulty capturing 3D information, and existing 3D algorithms are computationally heavy. Light-weight 3D SSL remains the boundary to explore. In this paper, we propose the spiral contrastive learning (SCL), which yields 3D representations in a computationally efficient manner. SCL first transforms 3D lesions to the 2D plane using an information-preserving spiral transformation, and then learn transformation-invariant features using 2D contrastive learning. For the augmentation, we consider natural image augmentations and medical image augmentations. We evaluate SCL by training a classification head upon the embedding layer. Experimental results show that SCL achieves state-of-the-art accuracy on LIDC-IDRI (89.72%), LNDb (82.09%) and TianChi (90.16%) for unsupervised representation learning. With 10% annotated data for fine-tune, the performance of SCL is comparable to that of supervised learning algorithms (85.75% vs. 85.03% on LIDC-IDRI, 78.20% vs. 73.44% on LNDb and 87.85% vs. 83.34% on TianChi, respectively). Meanwhile, SCL reduces the computational effort by 66.98% compared to other 3D SSL algorithms, demonstrating the efficiency of the proposed method in unsupervised pre-training.



### Structure Regularized Attentive Network for Automatic Femoral Head Necrosis Diagnosis and Localization
- **Arxiv ID**: http://arxiv.org/abs/2208.10695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10695v1)
- **Published**: 2022-08-23 02:31:38+00:00
- **Updated**: 2022-08-23 02:31:38+00:00
- **Authors**: Lingfeng Li, Huaiwei Cong, Gangming Zhao, Junran Peng, Zheng Zhang, Jinpeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, several works have adopted the convolutional neural network (CNN) to diagnose the avascular necrosis of the femoral head (AVNFH) based on X-ray images or magnetic resonance imaging (MRI). However, due to the tissue overlap, X-ray images are difficult to provide fine-grained features for early diagnosis. MRI, on the other hand, has a long imaging time, is more expensive, making it impractical in mass screening. Computed tomography (CT) shows layer-wise tissues, is faster to image, and is less costly than MRI. However, to our knowledge, there is no work on CT-based automated diagnosis of AVNFH. In this work, we collected and labeled a large-scale dataset for AVNFH ranking. In addition, existing end-to-end CNNs only yields the classification result and are difficult to provide more information for doctors in diagnosis. To address this issue, we propose the structure regularized attentive network (SRANet), which is able to highlight the necrotic regions during classification based on patch attention. SRANet extracts features in chunks of images, obtains weight via the attention mechanism to aggregate the features, and constrains them by a structural regularizer with prior knowledge to improve the generalization. SRANet was evaluated on our AVNFH-CT dataset. Experimental results show that SRANet is superior to CNNs for AVNFH classification, moreover, it can localize lesions and provide more information to assist doctors in diagnosis. Our codes are made public at https://github.com/tomas-lilingfeng/SRANet.



### Faint Features Tell: Automatic Vertebrae Fracture Screening Assisted by Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.10698v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10698v2)
- **Published**: 2022-08-23 02:39:08+00:00
- **Updated**: 2022-11-11 03:35:59+00:00
- **Authors**: Xin Wei, Huaiwei Cong, Zheng Zhang, Junran Peng, Guoping Chen, Jinpeng Li
- **Comment**: Accepted by BIBM2022 as short paper
- **Journal**: None
- **Summary**: Long-term vertebral fractures severely affect the life quality of patients, causing kyphotic, lumbar deformity and even paralysis. Computed tomography (CT) is a common clinical examination to screen for this disease at early stages. However, the faint radiological appearances and unspecific symptoms lead to a high risk of missed diagnosis, especially for the mild vertebral fractures. In this paper, we argue that reinforcing the faint fracture features to encourage the inter-class separability is the key to improving the accuracy. Motivated by this, we propose a supervised contrastive learning based model to estimate Genent's Grade of vertebral fracture with CT scans. The supervised contrastive learning, as an auxiliary task, narrows the distance of features within the same class while pushing others away, enhancing the model's capability of capturing subtle features of vertebral fractures. Our method has a specificity of 99% and a sensitivity of 85% in binary classification, and a macro-F1 of 77% in multi-class classification, indicating that contrastive learning significantly improves the accuracy of vertebrae fracture screening. Considering the lack of datasets in this field, we construct a database including 208 samples annotated by experienced radiologists. Our desensitized data and codes will be made publicly available for the community.



### CM-MLP: Cascade Multi-scale MLP with Axial Context Relation Encoder for Edge Segmentation of Medical Image
- **Arxiv ID**: http://arxiv.org/abs/2208.10701v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.10701v1)
- **Published**: 2022-08-23 02:53:37+00:00
- **Updated**: 2022-08-23 02:53:37+00:00
- **Authors**: Jinkai Lv, Yuyong Hu, Quanshui Fu, Zhiwang Zhang, Yuqiang Hu, Lin Lv, Guoqing Yang, Jinpeng Li, Yi Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: The convolutional-based methods provide good segmentation performance in the medical image segmentation task. However, those methods have the following challenges when dealing with the edges of the medical images: (1) Previous convolutional-based methods do not focus on the boundary relationship between foreground and background around the segmentation edge, which leads to the degradation of segmentation performance when the edge changes complexly. (2) The inductive bias of the convolutional layer cannot be adapted to complex edge changes and the aggregation of multiple-segmented areas, resulting in its performance improvement mostly limited to segmenting the body of segmented areas instead of the edge. To address these challenges, we propose the CM-MLP framework on MFI (Multi-scale Feature Interaction) block and ACRE (Axial Context Relation Encoder) block for accurate segmentation of the edge of medical image. In the MFI block, we propose the cascade multi-scale MLP (Cascade MLP) to process all local information from the deeper layers of the network simultaneously and utilize a cascade multi-scale mechanism to fuse discrete local information gradually. Then, the ACRE block is used to make the deep supervision focus on exploring the boundary relationship between foreground and background to modify the edge of the medical image. The segmentation accuracy (Dice) of our proposed CM-MLP framework reaches 96.96%, 96.76%, and 82.54% on three benchmark datasets: CVC-ClinicDB dataset, sub-Kvasir dataset, and our in-house dataset, respectively, which significantly outperform the state-of-the-art method. The source code and trained models will be available at https://github.com/ProgrammerHyy/CM-MLP.



### A Constrained Deformable Convolutional Network for Efficient Single Image Dynamic Scene Blind Deblurring with Spatially-Variant Motion Blur Kernels Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.10711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10711v1)
- **Published**: 2022-08-23 03:28:21+00:00
- **Updated**: 2022-08-23 03:28:21+00:00
- **Authors**: Shu Tang, Yang Wu, Hongxing Qin, Xianzhong Xie, Shuli Yang, Jing Wang
- **Comment**: 13 pages,8 figures
- **Journal**: None
- **Summary**: Most existing deep-learning-based single image dynamic scene blind deblurring (SIDSBD) methods usually design deep networks to directly remove the spatially-variant motion blurs from one inputted motion blurred image, without blur kernels estimation. In this paper, inspired by the Projective Motion Path Blur (PMPB) model and deformable convolution, we propose a novel constrained deformable convolutional network (CDCN) for efficient single image dynamic scene blind deblurring, which simultaneously achieves accurate spatially-variant motion blur kernels estimation and the high-quality image restoration from only one observed motion blurred image. In our proposed CDCN, we first construct a novel multi-scale multi-level multi-input multi-output (MSML-MIMO) encoder-decoder architecture for more powerful features extraction ability. Second, different from the DLVBD methods that use multiple consecutive frames, a novel constrained deformable convolution reblurring (CDCR) strategy is proposed, in which the deformable convolution is first applied to blurred features of the inputted single motion blurred image for learning the sampling points of motion blur kernel of each pixel, which is similar to the estimation of the motion density function of the camera shake in the PMPB model, and then a novel PMPB-based reblurring loss function is proposed to constrain the learned sampling points convergence, which can make the learned sampling points match with the relative motion trajectory of each pixel better and promote the accuracy of the spatially-variant motion blur kernels estimation.



### Threshold-adaptive Unsupervised Focal Loss for Domain Adaptation of Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.10716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10716v1)
- **Published**: 2022-08-23 03:48:48+00:00
- **Updated**: 2022-08-23 03:48:48+00:00
- **Authors**: Weihao Yan, Yeqiang Qian, Chunxiang Wang, Ming Yang
- **Comment**: 10 pages, 8 figure, 7 tables, submitted to T-ITS on April 2, 2022
- **Journal**: None
- **Summary**: Semantic segmentation is an important task for intelligent vehicles to understand the environment. Current deep learning methods require large amounts of labeled data for training. Manual annotation is expensive, while simulators can provide accurate annotations. However, the performance of the semantic segmentation model trained with the data of the simulator will significantly decrease when applied in the actual scene. Unsupervised domain adaptation (UDA) for semantic segmentation has recently gained increasing research attention, aiming to reduce the domain gap and improve the performance on the target domain. In this paper, we propose a novel two-stage entropy-based UDA method for semantic segmentation. In stage one, we design a threshold-adaptative unsupervised focal loss to regularize the prediction in the target domain, which has a mild gradient neutralization mechanism and mitigates the problem that hard samples are barely optimized in entropy-based methods. In stage two, we introduce a data augmentation method named cross-domain image mixing (CIM) to bridge the semantic knowledge from two domains. Our method achieves state-of-the-art 58.4% and 59.6% mIoUs on SYNTHIA-to-Cityscapes and GTA5-to-Cityscapes using DeepLabV2 and competitive performance using the lightweight BiSeNet.



### Bag of Tricks for Out-of-Distribution Generalization
- **Arxiv ID**: http://arxiv.org/abs/2208.10722v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.10722v1)
- **Published**: 2022-08-23 04:15:28+00:00
- **Updated**: 2022-08-23 04:15:28+00:00
- **Authors**: Zining Chen, Weiqiu Wang, Zhicheng Zhao, Aidong Men, Hong Chen
- **Comment**: None
- **Journal**: ECCV 2022 Workshop
- **Summary**: Recently, out-of-distribution (OOD) generalization has attracted attention to the robustness and generalization ability of deep learning based models, and accordingly, many strategies have been made to address different aspects related to this issue. However, most existing algorithms for OOD generalization are complicated and specifically designed for certain dataset. To alleviate this problem, nicochallenge-2022 provides NICO++, a large-scale dataset with diverse context information. In this paper, based on systematic analysis of different schemes on NICO++ dataset, we propose a simple but effective learning framework via coupling bag of tricks, including multi-objective framework design, data augmentations, training and inference strategies. Our algorithm is memory-efficient and easily-equipped, without complicated modules and does not require for large pre-trained models. It achieves an excellent performance with Top-1 accuracy of 88.16% on public test set and 75.65% on private test set, and ranks 1st in domain generalization task of nicochallenge-2022.



### Ultra-high-resolution unpaired stain transformation via Kernelized Instance Normalization
- **Arxiv ID**: http://arxiv.org/abs/2208.10730v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.10730v1)
- **Published**: 2022-08-23 04:47:43+00:00
- **Updated**: 2022-08-23 04:47:43+00:00
- **Authors**: Ming-Yang Ho, Min-Sheng Wu, Che-Ming Wu
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: While hematoxylin and eosin (H&E) is a standard staining procedure, immunohistochemistry (IHC) staining further serves as a diagnostic and prognostic method. However, acquiring special staining results requires substantial costs.   Hence, we proposed a strategy for ultra-high-resolution unpaired image-to-image translation: Kernelized Instance Normalization (KIN), which preserves local information and successfully achieves seamless stain transformation with constant GPU memory usage. Given a patch, corresponding position, and a kernel, KIN computes local statistics using convolution operation. In addition, KIN can be easily plugged into most currently developed frameworks without re-training.   We demonstrate that KIN achieves state-of-the-art stain transformation by replacing instance normalization (IN) layers with KIN layers in three popular frameworks and testing on two histopathological datasets. Furthermore, we manifest the generalizability of KIN with high-resolution natural images. Finally, human evaluation and several objective metrics are used to compare the performance of different approaches.   Overall, this is the first successful study for the ultra-high-resolution unpaired image-to-image translation with constant space complexity. Code is available at: https://github.com/Kaminyou/URUST



### Semi-Automatic Labeling and Semantic Segmentation of Gram-Stained Microscopic Images from DIBaS Dataset
- **Arxiv ID**: http://arxiv.org/abs/2208.10737v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2208.10737v1)
- **Published**: 2022-08-23 05:18:19+00:00
- **Updated**: 2022-08-23 05:18:19+00:00
- **Authors**: Chethan Reddy G. P., Pullagurla Abhijith Reddy, Vidyashree R. Kanabur, Deepu Vijayasenan, Sumam S. David, Sreejith Govindan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a semi-automatic annotation of bacteria genera and species from DIBaS dataset is implemented using clustering and thresholding algorithms. A Deep learning model is trained to achieve the semantic segmentation and classification of the bacteria species. Classification accuracy of 95% is achieved. Deep learning models find tremendous applications in biomedical image processing. Automatic segmentation of bacteria from gram-stained microscopic images is essential to diagnose respiratory and urinary tract infections, detect cancers, etc. Deep learning will aid the biologists to get reliable results in less time. Additionally, a lot of human intervention can be reduced. This work can be helpful to detect bacteria from urinary smear images, sputum smear images, etc to diagnose urinary tract infections, tuberculosis, pneumonia, etc.



### Super-resolution 3D Human Shape from a Single Low-Resolution Image
- **Arxiv ID**: http://arxiv.org/abs/2208.10738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10738v1)
- **Published**: 2022-08-23 05:24:39+00:00
- **Updated**: 2022-08-23 05:24:39+00:00
- **Authors**: Marco Pesavento, Marco Volino, Adrian Hilton
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel framework to reconstruct super-resolution human shape from a single low-resolution input image. The approach overcomes limitations of existing approaches that reconstruct 3D human shape from a single image, which require high-resolution images together with auxiliary data such as surface normal or a parametric model to reconstruct high-detail shape. The proposed framework represents the reconstructed shape with a high-detail implicit function. Analogous to the objective of 2D image super-resolution, the approach learns the mapping from a low-resolution shape to its high-resolution counterpart and it is applied to reconstruct 3D shape detail from low-resolution images. The approach is trained end-to-end employing a novel loss function which estimates the information lost between a low and high-resolution representation of the same 3D surface shape. Evaluation for single image reconstruction of clothed people demonstrates that our method achieves high-detail surface reconstruction from low-resolution images without auxiliary data. Extensive experiments show that the proposed approach can estimate super-resolution human geometries with a significantly higher level of detail than that obtained with previous approaches when applied to low-resolution images.



### Quality-Constant Per-Shot Encoding by Two-Pass Learning-based Rate Factor Prediction
- **Arxiv ID**: http://arxiv.org/abs/2208.10739v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.10739v1)
- **Published**: 2022-08-23 05:25:09+00:00
- **Updated**: 2022-08-23 05:25:09+00:00
- **Authors**: Chunlei Cai, Yi Wang, Xiaobo Li, Tianxiao Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Providing quality-constant streams can simultaneously guarantee user experience and prevent wasting bit-rate. In this paper, we propose a novel deep learning based two-pass encoder parameter prediction framework to decide rate factor (RF), with which encoder can output streams with constant quality. For each one-shot segment in a video, the proposed method firstly extracts spatial, temporal and pre-coding features by an ultra fast pre-process. Based on these features, a RF parameter is predicted by a deep neural network. Video encoder uses the RF to compress segment as the first encoding pass. Then VMAF quality of the first pass encoding is measured. If the quality doesn't meet target, a second pass RF prediction and encoding will be performed. With the help of first pass predicted RF and corresponding actual quality as feedback, the second pass prediction will be highly accurate. Experiments show the proposed method requires only 1.55 times encoding complexity on average, meanwhile the accuracy, that the compressed video's actual VMAF is within $\pm1$ around the target VMAF, reaches 98.88%.



### Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2208.10741v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10741v3)
- **Published**: 2022-08-23 05:27:32+00:00
- **Updated**: 2023-07-19 09:15:05+00:00
- **Authors**: Jungho Lee, Minhyeok Lee, Dogyoon Lee, Sangyoun Lee
- **Comment**: Accepted by ICCV 2023
- **Journal**: None
- **Summary**: Graph convolutional networks (GCNs) are the most commonly used methods for skeleton-based action recognition and have achieved remarkable performance. Generating adjacency matrices with semantically meaningful edges is particularly important for this task, but extracting such edges is challenging problem. To solve this, we propose a hierarchically decomposed graph convolutional network (HD-GCN) architecture with a novel hierarchically decomposed graph (HD-Graph). The proposed HD-GCN effectively decomposes every joint node into several sets to extract major structurally adjacent and distant edges, and uses them to construct an HD-Graph containing those edges in the same semantic spaces of a human skeleton. In addition, we introduce an attention-guided hierarchy aggregation (A-HA) module to highlight the dominant hierarchical edge sets of the HD-Graph. Furthermore, we apply a new six-way ensemble method, which uses only joint and bone stream without any motion stream. The proposed model is evaluated and achieves state-of-the-art performance on four large, popular datasets. Finally, we demonstrate the effectiveness of our model with various comparative experiments.



### Retinal Structure Detection in OCTA Image via Voting-based Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.10745v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.10745v1)
- **Published**: 2022-08-23 05:53:04+00:00
- **Updated**: 2022-08-23 05:53:04+00:00
- **Authors**: Jinkui Hao, Ting Shen, Xueli Zhu, Yonghuai Liu, Ardhendu Behera, Dan Zhang, Bang Chen, Jiang Liu, Jiong Zhang, Yitian Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Automated detection of retinal structures, such as retinal vessels (RV), the foveal avascular zone (FAZ), and retinal vascular junctions (RVJ), are of great importance for understanding diseases of the eye and clinical decision-making. In this paper, we propose a novel Voting-based Adaptive Feature Fusion multi-task network (VAFF-Net) for joint segmentation, detection, and classification of RV, FAZ, and RVJ in optical coherence tomography angiography (OCTA). A task-specific voting gate module is proposed to adaptively extract and fuse different features for specific tasks at two levels: features at different spatial positions from a single encoder, and features from multiple encoders. In particular, since the complexity of the microvasculature in OCTA images makes simultaneous precise localization and classification of retinal vascular junctions into bifurcation/crossing a challenging task, we specifically design a task head by combining the heatmap regression and grid classification. We take advantage of three different \textit{en face} angiograms from various retinal layers, rather than following existing methods that use only a single \textit{en face}. To facilitate further research, part of these datasets with the source code and evaluation benchmark have been released for public access:https://github.com/iMED-Lab/VAFF-Net.



### Digital topological groups
- **Arxiv ID**: http://arxiv.org/abs/2208.10748v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DM, math.AT
- **Links**: [PDF](http://arxiv.org/pdf/2208.10748v1)
- **Published**: 2022-08-23 06:04:49+00:00
- **Updated**: 2022-08-23 06:04:49+00:00
- **Authors**: Dae-Woong Lee, P. Christopher Staecker
- **Comment**: 33 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: In this article, we develop the basic theory of digital topological groups. The basic definitions directly lead to two separate categories, based on the details of the continuity required of the group multiplication. We define $\NP_1$- and $\NP_2$-digital topological groups, and investigate their properties and algebraic structure. The $\NP_2$ category is very restrictive, and we give a complete classification of $\NP_2$-digital topological groups. We also give many examples of $\NP_1$-digital topological groups. We define digital topological group homomorphisms, and describe the digital counterpart of the first isomorphism theorem.



### Neural PCA for Flow-Based Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.10753v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.10753v1)
- **Published**: 2022-08-23 06:18:44+00:00
- **Updated**: 2022-08-23 06:18:44+00:00
- **Authors**: Shen Li, Bryan Hooi
- **Comment**: Accepted to IJCAI 2022
- **Journal**: None
- **Summary**: Of particular interest is to discover useful representations solely from observations in an unsupervised generative manner. However, the question of whether existing normalizing flows provide effective representations for downstream tasks remains mostly unanswered despite their strong ability for sample generation and density estimation. This paper investigates this problem for such a family of generative models that admits exact invertibility. We propose Neural Principal Component Analysis (Neural-PCA) that operates in full dimensionality while capturing principal components in \emph{descending} order. Without exploiting any label information, the principal components recovered store the most informative elements in their \emph{leading} dimensions and leave the negligible in the \emph{trailing} ones, allowing for clear performance improvements of $5\%$-$10\%$ in downstream tasks. Such improvements are empirically found consistent irrespective of the number of latent trailing dimensions dropped. Our work suggests that necessary inductive bias be introduced into generative modelling when representation quality is of interest.



### Learning More May Not Be Better: Knowledge Transferability in Vision and Language Tasks
- **Arxiv ID**: http://arxiv.org/abs/2208.10758v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.10758v1)
- **Published**: 2022-08-23 06:39:18+00:00
- **Updated**: 2022-08-23 06:39:18+00:00
- **Authors**: Tianwei Chen, Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima, Hajime Nagahara
- **Comment**: None
- **Journal**: None
- **Summary**: Is more data always better to train vision-and-language models? We study knowledge transferability in multi-modal tasks. The current tendency in machine learning is to assume that by joining multiple datasets from different tasks their overall performance will improve. However, we show that not all the knowledge transfers well or has a positive impact on related tasks, even when they share a common goal. We conduct an exhaustive analysis based on hundreds of cross-experiments on 12 vision-and-language tasks categorized in 4 groups. Whereas tasks in the same group are prone to improve each other, results show that this is not always the case. Other factors such as dataset size or pre-training stage have also a great impact on how well the knowledge is transferred.



### How good are deep models in understanding the generated images?
- **Arxiv ID**: http://arxiv.org/abs/2208.10760v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10760v2)
- **Published**: 2022-08-23 06:44:43+00:00
- **Updated**: 2022-08-25 03:32:33+00:00
- **Authors**: Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: My goal in this paper is twofold: to study how well deep models can understand the images generated by DALL-E 2 and Midjourney, and to quantitatively evaluate these generative models. Two sets of generated images are collected for object recognition and visual question answering (VQA) tasks. On object recognition, the best model, out of 10 state-of-the-art object recognition models, achieves about 60\% and 80\% top-1 and top-5 accuracy, respectively. These numbers are much lower than the best accuracy on the ImageNet dataset (91\% and 99\%). On VQA, the OFA model scores 77.3\% on answering 241 binary questions across 50 images. This model scores 94.7\% on the binary VQA-v2 dataset. Humans are able to recognize the generated images and answer questions on them easily. We conclude that a) deep models struggle to understand the generated content, and may do better after fine-tuning, and b) there is a large distribution shift between the generated images and the real photographs. The distribution shift appears to be category-dependent. Data is available at: https://drive.google.com/file/d/1n2nCiaXtYJRRF2R73-LNE3zggeU_HeH0/view?usp=sharing.



### CRCNet: Few-shot Segmentation with Cross-Reference and Region-Global Conditional Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.10761v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10761v1)
- **Published**: 2022-08-23 06:46:18+00:00
- **Updated**: 2022-08-23 06:46:18+00:00
- **Authors**: Weide Liu, Chi Zhang, Guosheng Lin, Fayao Liu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2003.10658
- **Journal**: None
- **Summary**: Few-shot segmentation aims to learn a segmentation model that can be generalized to novel classes with only a few training images. In this paper, we propose a Cross-Reference and Local-Global Conditional Networks (CRCNet) for few-shot segmentation. Unlike previous works that only predict the query image's mask, our proposed model concurrently makes predictions for both the support image and the query image. Our network can better find the co-occurrent objects in the two images with a cross-reference mechanism, thus helping the few-shot segmentation task. To further improve feature comparison, we develop a local-global conditional module to capture both global and local relations. We also develop a mask refinement module to refine the prediction of the foreground regions recurrently. Experiments on the PASCAL VOC 2012, MS COCO, and FSS-1000 datasets show that our network achieves new state-of-the-art performance.



### Depth Map Decomposition for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.10762v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10762v1)
- **Published**: 2022-08-23 06:49:54+00:00
- **Updated**: 2022-08-23 06:49:54+00:00
- **Authors**: Jinyoung Jun, Jae-Han Lee, Chul Lee, Chang-Su Kim
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel algorithm for monocular depth estimation that decomposes a metric depth map into a normalized depth map and scale features. The proposed network is composed of a shared encoder and three decoders, called G-Net, N-Net, and M-Net, which estimate gradient maps, a normalized depth map, and a metric depth map, respectively. M-Net learns to estimate metric depths more accurately using relative depth features extracted by G-Net and N-Net. The proposed algorithm has the advantage that it can use datasets without metric depth labels to improve the performance of metric depth estimation. Experimental results on various datasets demonstrate that the proposed algorithm not only provides competitive performance to state-of-the-art algorithms but also yields acceptable results even when only a small amount of metric depth data is available for its training.



### A Low-Cost Lane-Following Algorithm for Cyber-Physical Robots
- **Arxiv ID**: http://arxiv.org/abs/2208.10765v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2208.10765v1)
- **Published**: 2022-08-23 06:55:53+00:00
- **Updated**: 2022-08-23 06:55:53+00:00
- **Authors**: Archit Gupta, Arvind Easwaran
- **Comment**: None
- **Journal**: None
- **Summary**: Duckiebots are low-cost mobile robots that are widely used in the fields of research and education. Although there are existing self-driving algorithms for the Duckietown platform, they are either too complex or perform too poorly to navigate a multi-lane track. Moreover, it is essential to give memory and computational resources to a Duckiebot so it can perform additional tasks such as out-of-distribution input detection. In order to satisfy these constraints, we built a low-cost autonomous driving algorithm capable of driving on a two-lane track. The algorithm uses traditional computer vision techniques to identify the central lane on the track and obtain the relevant steering angle. The steering is then controlled by a PID controller that smoothens the movement of the Duckiebot. The performance of the algorithm was compared to that of the NeurIPS 2018 AI Driving Olympics (AIDO) finalists, and it outperformed all but one finalists. The two main contributions of our algorithm are its low computational requirements and very quick set-up, with ongoing efforts to make it more reliable.



### PIFu for the Real World: A Self-supervised Framework to Reconstruct Dressed Human from Single-view Images
- **Arxiv ID**: http://arxiv.org/abs/2208.10769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10769v1)
- **Published**: 2022-08-23 07:00:44+00:00
- **Updated**: 2022-08-23 07:00:44+00:00
- **Authors**: Zhangyang Xiong, Dong Du, Yushuang Wu, Jingqi Dong, Di Kang, Linchao Bao, Xiaoguang Han
- **Comment**: None
- **Journal**: None
- **Summary**: It is very challenging to accurately reconstruct sophisticated human geometry caused by various poses and garments from a single image. Recently, works based on pixel-aligned implicit function (PIFu) have made a big step and achieved state-of-the-art fidelity on image-based 3D human digitization. However, the training of PIFu relies heavily on expensive and limited 3D ground truth data (i.e. synthetic data), thus hindering its generalization to more diverse real world images. In this work, we propose an end-to-end self-supervised network named SelfPIFu to utilize abundant and diverse in-the-wild images, resulting in largely improved reconstructions when tested on unconstrained in-the-wild images. At the core of SelfPIFu is the depth-guided volume-/surface-aware signed distance fields (SDF) learning, which enables self-supervised learning of a PIFu without access to GT mesh. The whole framework consists of a normal estimator, a depth estimator, and a SDF-based PIFu and better utilizes extra depth GT during training. Extensive experiments demonstrate the effectiveness of our self-supervised framework and the superiority of using depth as input. On synthetic data, our Intersection-Over-Union (IoU) achieves to 93.5%, 18% higher compared with PIFuHD. For in-the-wild images, we conduct user studies on the reconstructed results, the selection rate of our results is over 68% compared with other state-of-the-art methods.



### Learning an Efficient Multimodal Depth Completion Model
- **Arxiv ID**: http://arxiv.org/abs/2208.10771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10771v1)
- **Published**: 2022-08-23 07:03:14+00:00
- **Updated**: 2022-08-23 07:03:14+00:00
- **Authors**: Dewang Hou, Yuanyuan Du, Kai Zhao, Yang Zhao
- **Comment**: To appear in ECCV 2022 workshop and codes are available at
  https://github.com/dwHou/EMDC-PyTorch
- **Journal**: None
- **Summary**: With the wide application of sparse ToF sensors in mobile devices, RGB image-guided sparse depth completion has attracted extensive attention recently, but still faces some problems. First, the fusion of multimodal information requires more network modules to process different modalities. But the application scenarios of sparse ToF measurements usually demand lightweight structure and low computational cost. Second, fusing sparse and noisy depth data with dense pixel-wise RGB data may introduce artifacts. In this paper, a light but efficient depth completion network is proposed, which consists of a two-branch global and local depth prediction module and a funnel convolutional spatial propagation network. The two-branch structure extracts and fuses cross-modal features with lightweight backbones. The improved spatial propagation module can refine the completed depth map gradually. Furthermore, corrected gradient loss is presented for the depth completion problem. Experimental results demonstrate the proposed method can outperform some state-of-the-art methods with a lightweight architecture. The proposed method also wins the championship in the MIPI2022 RGB+TOF depth completion challenge.



### Adversarial Vulnerability of Temporal Feature Networks for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.10773v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.10773v1)
- **Published**: 2022-08-23 07:08:54+00:00
- **Updated**: 2022-08-23 07:08:54+00:00
- **Authors**: Svetlana Pavlitskaya, Nikolai Polley, Michael Weber, J. Marius Zöllner
- **Comment**: Accepted for publication at ECCV 2022 SAIAD workshop
- **Journal**: None
- **Summary**: Taking into account information across the temporal domain helps to improve environment perception in autonomous driving. However, it has not been studied so far whether temporally fused neural networks are vulnerable to deliberately generated perturbations, i.e. adversarial attacks, or whether temporal history is an inherent defense against them. In this work, we study whether temporal feature networks for object detection are vulnerable to universal adversarial attacks. We evaluate attacks of two types: imperceptible noise for the whole image and locally-bound adversarial patch. In both cases, perturbations are generated in a white-box manner using PGD. Our experiments confirm, that attacking even a portion of a temporal input suffices to fool the network. We visually assess generated perturbations to gain insights into the functioning of attacks. To enhance the robustness, we apply adversarial training using 5-PGD. Our experiments on KITTI and nuScenes datasets demonstrate, that a model robustified via K-PGD is able to withstand the studied attacks while keeping the mAP-based performance comparable to that of an unattacked model.



### Efficient Self-Supervision using Patch-based Contrastive Learning for Histopathology Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.10779v2
- **DOI**: 10.7557/18.6798
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.10779v2)
- **Published**: 2022-08-23 07:24:47+00:00
- **Updated**: 2023-01-03 08:45:20+00:00
- **Authors**: Nicklas Boserup, Raghavendra Selvan
- **Comment**: Version of this article (without Appendices) is accepted for an oral
  presentation at the 6th Northern Lights Deep Learning Conference, 2023,
  Troms{\o}, Norway. 15 pages, 8 figures. Source code at
  https://github.com/nickeopti/bach-contrastive-segmentation
- **Journal**: None
- **Summary**: Learning discriminative representations of unlabelled data is a challenging task. Contrastive self-supervised learning provides a framework to learn meaningful representations using learned notions of similarity measures from simple pretext tasks. In this work, we propose a simple and efficient framework for self-supervised image segmentation using contrastive learning on image patches, without using explicit pretext tasks or any further labeled fine-tuning. A fully convolutional neural network (FCNN) is trained in a self-supervised manner to discern features in the input images and obtain confidence maps which capture the network's belief about the objects belonging to the same class. Positive- and negative- patches are sampled based on the average entropy in the confidence maps for contrastive learning. Convergence is assumed when the information separation between the positive patches is small, and the positive-negative pairs is large. The proposed model only consists of a simple FCNN with 10.8k parameters and requires about 5 minutes to converge on the high resolution microscopy datasets, which is orders of magnitude smaller than the relevant self-supervised methods to attain similar performance. We evaluate the proposed method for the task of segmenting nuclei from two histopathology datasets, and show comparable performance with relevant self-supervised and supervised methods.



### Object Detection in Aerial Images with Uncertainty-Aware Graph Network
- **Arxiv ID**: http://arxiv.org/abs/2208.10781v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10781v2)
- **Published**: 2022-08-23 07:29:03+00:00
- **Updated**: 2022-08-24 05:45:37+00:00
- **Authors**: Jongha Kim, Jinheon Baek, Sung Ju Hwang
- **Comment**: ECCV Workshop 2022
- **Journal**: None
- **Summary**: In this work, we propose a novel uncertainty-aware object detection framework with a structured-graph, where nodes and edges are denoted by objects and their spatial-semantic similarities, respectively. Specifically, we aim to consider relationships among objects for effectively contextualizing them. To achieve this, we first detect objects and then measure their semantic and spatial distances to construct an object graph, which is then represented by a graph neural network (GNN) for refining visual CNN features for objects. However, refining CNN features and detection results of every object are inefficient and may not be necessary, as that include correct predictions with low uncertainties. Therefore, we propose to handle uncertain objects by not only transferring the representation from certain objects (sources) to uncertain objects (targets) over the directed graph, but also improving CNN features only on objects regarded as uncertain with their representational outputs from the GNN. Furthermore, we calculate a training loss by giving larger weights on uncertain objects, to concentrate on improving uncertain object predictions while maintaining high performances on certain objects. We refer to our model as Uncertainty-Aware Graph network for object DETection (UAGDet). We then experimentally validate ours on the challenging large-scale aerial image dataset, namely DOTA, that consists of lots of objects with small to large sizes in an image, on which ours improves the performance of the existing object detection network.



### Semantic Driven Energy based Out-of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.10787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10787v1)
- **Published**: 2022-08-23 07:40:34+00:00
- **Updated**: 2022-08-23 07:40:34+00:00
- **Authors**: Abhishek Joshi, Sathish Chalasani, Kiran Nanjunda Iyer
- **Comment**: accepted at International Joint Conference on Neural Networks (IJCNN)
  2022
- **Journal**: None
- **Summary**: Detecting Out-of-Distribution (OOD) samples in real world visual applications like classification or object detection has become a necessary precondition in today's deployment of Deep Learning systems. Many techniques have been proposed, of which Energy based OOD methods have proved to be promising and achieved impressive performance. We propose semantic driven energy based method, which is an end-to-end trainable system and easy to optimize. We distinguish in-distribution samples from out-distribution samples with an energy score coupled with a representation score. We achieve it by minimizing the energy for in-distribution samples and simultaneously learn respective class representations that are closer and maximizing energy for out-distribution samples and pushing their representation further out from known class representation. Moreover, we propose a novel loss function which we call Cluster Focal Loss(CFL) that proved to be simple yet very effective in learning better class wise cluster center representations. We find that, our novel approach enhances outlier detection and achieve state-of-the-art as an energy-based model on common benchmarks. On CIFAR-10 and CIFAR-100 trained WideResNet, our model significantly reduces the relative average False Positive Rate(at True Positive Rate of 95%) by 67.2% and 57.4% respectively, compared to the existing energy based approaches. Further, we extend our framework for object detection and achieve improved performance.



### Extending nnU-Net is all you need
- **Arxiv ID**: http://arxiv.org/abs/2208.10791v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.10791v1)
- **Published**: 2022-08-23 07:54:29+00:00
- **Updated**: 2022-08-23 07:54:29+00:00
- **Authors**: Fabian Isensee, Constantin Ulrich, Tassilo Wald, Klaus H. Maier-Hein
- **Comment**: Fabian Isensee, Constantin Ulrich and Tassilo Wald contributed
  equally
- **Journal**: None
- **Summary**: Semantic segmentation is one of the most popular research areas in medical image computing. Perhaps surprisingly, despite its conceptualization dating back to 2018, nnU-Net continues to provide competitive out-of-the-box solutions for a broad variety of segmentation problems and is regularly used as a development framework for challenge-winning algorithms. Here we use nnU-Net to participate in the AMOS2022 challenge, which comes with a unique set of tasks: not only is the dataset one of the largest ever created and boasts 15 target structures, but the competition also requires submitted solutions to handle both MRI and CT scans. Through careful modification of nnU-net's hyperparameters, the addition of residual connections in the encoder and the design of a custom postprocessing strategy, we were able to substantially improve upon the nnU-Net baseline. Our final ensemble achieves Dice scores of 90.13 for Task 1 (CT) and 89.06 for Task 2 (CT+MRI) in a 5-fold cross-validation on the provided training cases.



### Aging prediction using deep generative model toward the development of preventive medicine
- **Arxiv ID**: http://arxiv.org/abs/2208.10797v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.10797v1)
- **Published**: 2022-08-23 08:04:38+00:00
- **Updated**: 2022-08-23 08:04:38+00:00
- **Authors**: Hisaichi Shibata, Shouhei Hanaoka, Yukihiro Nomura, Naoto Hayashi, Osamu Abe
- **Comment**: None
- **Journal**: None
- **Summary**: From birth to death, we all experience surprisingly ubiquitous changes over time due to aging. If we can predict aging in the digital domain, that is, the digital twin of the human body, we would be able to detect lesions in their very early stages, thereby enhancing the quality of life and extending the life span. We observed that none of the previously developed digital twins of the adult human body explicitly trained longitudinal conversion rules between volumetric medical images with deep generative models, potentially resulting in poor prediction performance of, for example, ventricular volumes. Here, we establish a new digital twin of an adult human body that adopts longitudinally acquired head computed tomography (CT) images for training, enabling prediction of future volumetric head CT images from a single present volumetric head CT image. We, for the first time, adopt one of the three-dimensional flow-based deep generative models to realize this sequential three-dimensional digital twin. We show that our digital twin outperforms the latest methods of prediction of ventricular volumes in relatively short terms.



### Time-lapse image classification using a diffractive neural network
- **Arxiv ID**: http://arxiv.org/abs/2208.10802v1
- **DOI**: 10.1002/aisy.202200387
- **Categories**: **physics.optics**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2208.10802v1)
- **Published**: 2022-08-23 08:16:30+00:00
- **Updated**: 2022-08-23 08:16:30+00:00
- **Authors**: Md Sadman Sakib Rahman, Aydogan Ozcan
- **Comment**: 17 Pages, 4 Figures, 2 Tables
- **Journal**: Advanced Intelligent Systems (2023)
- **Summary**: Diffractive deep neural networks (D2NNs) define an all-optical computing framework comprised of spatially engineered passive surfaces that collectively process optical input information by modulating the amplitude and/or the phase of the propagating light. Diffractive optical networks complete their computational tasks at the speed of light propagation through a thin diffractive volume, without any external computing power while exploiting the massive parallelism of optics. Diffractive networks were demonstrated to achieve all-optical classification of objects and perform universal linear transformations. Here we demonstrate, for the first time, a "time-lapse" image classification scheme using a diffractive network, significantly advancing its classification accuracy and generalization performance on complex input objects by using the lateral movements of the input objects and/or the diffractive network, relative to each other. In a different context, such relative movements of the objects and/or the camera are routinely being used for image super-resolution applications; inspired by their success, we designed a time-lapse diffractive network to benefit from the complementary information content created by controlled or random lateral shifts. We numerically explored the design space and performance limits of time-lapse diffractive networks, revealing a blind testing accuracy of 62.03% on the optical classification of objects from the CIFAR-10 dataset. This constitutes the highest inference accuracy achieved so far using a single diffractive network on the CIFAR-10 dataset. Time-lapse diffractive networks will be broadly useful for the spatio-temporal analysis of input signals using all-optical processors.



### Towards Accurate Facial Landmark Detection via Cascaded Transformers
- **Arxiv ID**: http://arxiv.org/abs/2208.10808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10808v1)
- **Published**: 2022-08-23 08:42:13+00:00
- **Updated**: 2022-08-23 08:42:13+00:00
- **Authors**: Hui Li, Zidong Guo, Seon-Min Rhee, Seungju Han, Jae-Joon Han
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate facial landmarks are essential prerequisites for many tasks related to human faces. In this paper, an accurate facial landmark detector is proposed based on cascaded transformers. We formulate facial landmark detection as a coordinate regression task such that the model can be trained end-to-end. With self-attention in transformers, our model can inherently exploit the structured relationships between landmarks, which would benefit landmark detection under challenging conditions such as large pose and occlusion. During cascaded refinement, our model is able to extract the most relevant image features around the target landmark for coordinate prediction, based on deformable attention mechanism, thus bringing more accurate alignment. In addition, we propose a novel decoder that refines image features and landmark positions simultaneously. With few parameter increasing, the detection performance improves further. Our model achieves new state-of-the-art performance on several standard facial landmark detection benchmarks, and shows good generalization ability in cross-dataset evaluation.



### Multimodal Across Domains Gaze Target Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.10822v1
- **DOI**: 10.1145/3536221.3556624
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2208.10822v1)
- **Published**: 2022-08-23 09:09:00+00:00
- **Updated**: 2022-08-23 09:09:00+00:00
- **Authors**: Francesco Tonini, Cigdem Beyan, Elisa Ricci
- **Comment**: Accepted to 24th ACM International Conference on Multimodal
  Interaction (ICMI 2022)
- **Journal**: None
- **Summary**: This paper addresses the gaze target detection problem in single images captured from the third-person perspective. We present a multimodal deep architecture to infer where a person in a scene is looking. This spatial model is trained on the head images of the person-of- interest, scene and depth maps representing rich context information. Our model, unlike several prior art, do not require supervision of the gaze angles, do not rely on head orientation information and/or location of the eyes of person-of-interest. Extensive experiments demonstrate the stronger performance of our method on multiple benchmark datasets. We also investigated several variations of our method by altering joint-learning of multimodal data. Some variations outperform a few prior art as well. First time in this paper, we inspect domain adaption for gaze target detection, and we empower our multimodal network to effectively handle the domain gap across datasets. The code of the proposed method is available at https://github.com/francescotonini/multimodal-across-domains-gaze-target-detection.



### Satellite Image Search in AgoraEO
- **Arxiv ID**: http://arxiv.org/abs/2208.10830v1
- **DOI**: None
- **Categories**: **cs.DB**, cs.CV, H.2.8; H.3.3; I.4.8; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2208.10830v1)
- **Published**: 2022-08-23 09:27:02+00:00
- **Updated**: 2022-08-23 09:27:02+00:00
- **Authors**: Ahmet Kerem Aksoy, Pavel Dushev, Eleni Tzirita Zacharatou, Holmer Hemsen, Marcela Charfuelan, Jorge-Arnulfo Quiané-Ruiz, Begüm Demir, Volker Markl
- **Comment**: Accepted in VLDB 2022
- **Journal**: None
- **Summary**: The growing operational capability of global Earth Observation (EO) creates new opportunities for data-driven approaches to understand and protect our planet. However, the current use of EO archives is very restricted due to the huge archive sizes and the limited exploration capabilities provided by EO platforms. To address this limitation, we have recently proposed MiLaN, a content-based image retrieval approach for fast similarity search in satellite image archives. MiLaN is a deep hashing network based on metric learning that encodes high-dimensional image features into compact binary hash codes. We use these codes as keys in a hash table to enable real-time nearest neighbor search and highly accurate retrieval. In this demonstration, we showcase the efficiency of MiLaN by integrating it with EarthQube, a browser and search engine within AgoraEO. EarthQube supports interactive visual exploration and Query-by-Example over satellite image repositories. Demo visitors will interact with EarthQube playing the role of different users that search images in a large-scale remote sensing archive by their semantic content and apply other filters.



### Multimodal Crop Type Classification Fusing Multi-Spectral Satellite Time Series with Farmers Crop Rotations and Local Crop Distribution
- **Arxiv ID**: http://arxiv.org/abs/2208.10838v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2208.10838v1)
- **Published**: 2022-08-23 09:41:09+00:00
- **Updated**: 2022-08-23 09:41:09+00:00
- **Authors**: Valentin Barriere, Martin Claverie
- **Comment**: accepted to CECEO22@IJCAI
- **Journal**: None
- **Summary**: Accurate, detailed, and timely crop type mapping is a very valuable information for the institutions in order to create more accurate policies according to the needs of the citizens. In the last decade, the amount of available data dramatically increased, whether it can come from Remote Sensing (using Copernicus Sentinel-2 data) or directly from the farmers (providing in-situ crop information throughout the years and information on crop rotation). Nevertheless, the majority of the studies are restricted to the use of one modality (Remote Sensing data or crop rotation) and never fuse the Earth Observation data with domain knowledge like crop rotations. Moreover, when they use Earth Observation data they are mainly restrained to one year of data, not taking into account the past years. In this context, we propose to tackle a land use and crop type classification task using three data types, by using a Hierarchical Deep Learning algorithm modeling the crop rotations like a language model, the satellite signals like a speech signal and using the crop distribution as additional context vector. We obtained very promising results compared to classical approaches with significant performances, increasing the Accuracy by 5.1 points in a 28-class setting (.948), and the micro-F1 by 9.6 points in a 10-class setting (.887) using only a set of crop of interests selected by an expert. We finally proposed a data-augmentation technique to allow the model to classify the crop before the end of the season, which works surprisingly well in a multimodal setting.



### In-Air Imaging Sonar Sensor Network with Real-Time Processing Using GPUs
- **Arxiv ID**: http://arxiv.org/abs/2208.10839v1
- **DOI**: 10.1007/978-3-030-33509-0_67
- **Categories**: **cs.CV**, cs.NI, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2208.10839v1)
- **Published**: 2022-08-23 09:46:18+00:00
- **Updated**: 2022-08-23 09:46:18+00:00
- **Authors**: Wouter Jansen, Dennis Laurijssen, Robin Kerstens, Walter Daems, Jan Steckel
- **Comment**: 2019 International Conference on P2P, Parallel, Grid, Cloud and
  Internet Computing
- **Journal**: None
- **Summary**: For autonomous navigation and robotic applications, sensing the environment correctly is crucial. Many sensing modalities for this purpose exist. In recent years, one such modality that is being used is in-air imaging sonar. It is ideal in complex environments with rough conditions such as dust or fog. However, like with most sensing modalities, to sense the full environment around the mobile platform, multiple such sensors are needed to capture the full 360-degree range. Currently the processing algorithms used to create this data are insufficient to do so for multiple sensors at a reasonably fast update rate. Furthermore, a flexible and robust framework is needed to easily implement multiple imaging sonar sensors into any setup and serve multiple application types for the data. In this paper we present a sensor network framework designed for this novel sensing modality. Furthermore, an implementation of the processing algorithm on a Graphics Processing Unit is proposed to potentially decrease the computing time to allow for real-time processing of one or more imaging sonar sensors at a sufficiently high update rate.



### FocusFormer: Focusing on What We Need via Architecture Sampler
- **Arxiv ID**: http://arxiv.org/abs/2208.10861v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.10861v1)
- **Published**: 2022-08-23 10:42:56+00:00
- **Updated**: 2022-08-23 10:42:56+00:00
- **Authors**: Jing Liu, Jianfei Cai, Bohan Zhuang
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Vision Transformers (ViTs) have underpinned the recent breakthroughs in computer vision. However, designing the architectures of ViTs is laborious and heavily relies on expert knowledge. To automate the design process and incorporate deployment flexibility, one-shot neural architecture search decouples the supernet training and architecture specialization for diverse deployment scenarios. To cope with an enormous number of sub-networks in the supernet, existing methods treat all architectures equally important and randomly sample some of them in each update step during training. During architecture search, these methods focus on finding architectures on the Pareto frontier of performance and resource consumption, which forms a gap between training and deployment. In this paper, we devise a simple yet effective method, called FocusFormer, to bridge such a gap. To this end, we propose to learn an architecture sampler to assign higher sampling probabilities to those architectures on the Pareto frontier under different resource constraints during supernet training, making them sufficiently optimized and hence improving their performance. During specialization, we can directly use the well-trained architecture sampler to obtain accurate architectures satisfying the given resource constraint, which significantly improves the search efficiency. Extensive experiments on CIFAR-100 and ImageNet show that our FocusFormer is able to improve the performance of the searched architectures while significantly reducing the search cost. For example, on ImageNet, our FocusFormer-Ti with 1.4G FLOPs outperforms AutoFormer-Ti by 0.5% in terms of the Top-1 accuracy.



### A Comprehensive Study of Real-Time Object Detection Networks Across Multiple Domains: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2208.10895v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.10895v2)
- **Published**: 2022-08-23 12:01:16+00:00
- **Updated**: 2023-02-14 16:15:20+00:00
- **Authors**: Elahe Arani, Shruthi Gowda, Ratnajit Mukherjee, Omar Magdy, Senthilkumar Kathiresan, Bahram Zonooz
- **Comment**: Published in Transactions on Machine Learning Research (TMLR) with
  Survey Certification
- **Journal**: Transactions on Machine Learning Research, 2022
- **Summary**: Deep neural network based object detectors are continuously evolving and are used in a multitude of applications, each having its own set of requirements. While safety-critical applications need high accuracy and reliability, low-latency tasks need resource and energy-efficient networks. Real-time detectors, which are a necessity in high-impact real-world applications, are continuously proposed, but they overemphasize the improvements in accuracy and speed while other capabilities such as versatility, robustness, resource and energy efficiency are omitted. A reference benchmark for existing networks does not exist, nor does a standard evaluation guideline for designing new networks, which results in ambiguous and inconsistent comparisons. We, thus, conduct a comprehensive study on multiple real-time detectors (anchor-, keypoint-, and transformer-based) on a wide range of datasets and report results on an extensive set of metrics. We also study the impact of variables such as image size, anchor dimensions, confidence thresholds, and architecture layers on the overall performance. We analyze the robustness of detection networks against distribution shifts, natural corruptions, and adversarial attacks. Also, we provide a calibration analysis to gauge the reliability of the predictions. Finally, to highlight the real-world impact, we conduct two unique case studies, on autonomous driving and healthcare applications. To further gauge the capability of networks in critical real-time applications, we report the performance after deploying the detection networks on edge devices. Our extensive empirical study can act as a guideline for the industrial community to make an informed choice on the existing networks. We also hope to inspire the research community towards a new direction in the design and evaluation of networks that focuses on a bigger and holistic overview for a far-reaching impact.



### Can you recommend content to creatives instead of final consumers? A RecSys based on user's preferred visual styles
- **Arxiv ID**: http://arxiv.org/abs/2208.10902v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.10902v1)
- **Published**: 2022-08-23 12:11:28+00:00
- **Updated**: 2022-08-23 12:11:28+00:00
- **Authors**: Raul Gomez Bruballa, Lauren Burnham-King, Alessandra Sala
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Providing meaningful recommendations in a content marketplace is challenging due to the fact that users are not the final content consumers. Instead, most users are creatives whose interests, linked to the projects they work on, change rapidly and abruptly. To address the challenging task of recommending images to content creators, we design a RecSys that learns visual styles preferences transversal to the semantics of the projects users work on. We analyze the challenges of the task compared to content-based recommendations driven by semantics, propose an evaluation setup, and explain its applications in a global image marketplace.   This technical report is an extension of the paper "Learning Users' Preferred Visual Styles in an Image Marketplace", presented at ACM RecSys '22.



### StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2208.10922v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.10922v1)
- **Published**: 2022-08-23 12:49:01+00:00
- **Updated**: 2022-08-23 12:49:01+00:00
- **Authors**: Dongchan Min, Minyoung Song, Sung Ju Hwang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose StyleTalker, a novel audio-driven talking head generation model that can synthesize a video of a talking person from a single reference image with accurately audio-synced lip shapes, realistic head poses, and eye blinks. Specifically, by leveraging a pretrained image generator and an image encoder, we estimate the latent codes of the talking head video that faithfully reflects the given audio. This is made possible with several newly devised components: 1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A conditional sequential variational autoencoder that learns the latent motion space disentangled from the lip movements, such that we can independently manipulate the motions and lip movements while preserving the identity. 3) An auto-regressive prior augmented with normalizing flow to learn a complex audio-to-motion multi-modal latent space. Equipped with these components, StyleTalker can generate talking head videos not only in a motion-controllable way when another motion source video is given but also in a completely audio-driven manner by inferring realistic motions from the input audio. Through extensive experiments and user studies, we show that our model is able to synthesize talking head videos with impressive perceptual quality which are accurately lip-synced with the input audios, largely outperforming state-of-the-art baselines.



### FS-BAN: Born-Again Networks for Domain Generalization Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2208.10930v4
- **DOI**: 10.1109/TIP.2023.3266172
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10930v4)
- **Published**: 2022-08-23 13:02:14+00:00
- **Updated**: 2023-05-08 15:57:07+00:00
- **Authors**: Yunqing Zhao, Ngai-Man Cheung
- **Comment**: 15 pages, 9 figures, 15 tables. IEEE Transactions on Image Processing
  (TIP), 2023
- **Journal**: None
- **Summary**: Conventional Few-shot classification (FSC) aims to recognize samples from novel classes given limited labeled data. Recently, domain generalization FSC (DG-FSC) has been proposed with the goal to recognize novel class samples from unseen domains. DG-FSC poses considerable challenges to many models due to the domain shift between base classes (used in training) and novel classes (encountered in evaluation). In this work, we make two novel contributions to tackle DG-FSC. Our first contribution is to propose Born-Again Network (BAN) episodic training and comprehensively investigate its effectiveness for DG-FSC. As a specific form of knowledge distillation, BAN has been shown to achieve improved generalization in conventional supervised classification with a closed-set setup. This improved generalization motivates us to study BAN for DG-FSC, and we show that BAN is promising to address the domain shift encountered in DG-FSC. Building on the encouraging findings, our second (major) contribution is to propose Few-Shot BAN (FS-BAN), a novel BAN approach for DG-FSC. Our proposed FS-BAN includes novel multi-task learning objectives: Mutual Regularization, Mismatched Teacher, and Meta-Control Temperature, each of these is specifically designed to overcome central and unique challenges in DG-FSC, namely overfitting and domain discrepancy. We analyze different design choices of these techniques. We conduct comprehensive quantitative and qualitative analysis and evaluation over six datasets and three baseline models. The results suggest that our proposed FS-BAN consistently improves the generalization performance of baseline models and achieves state-of-the-art accuracy for DG-FSC. Project Page: https://yunqing-me.github.io/Born-Again-FS/.



### Improving Computed Tomography (CT) Reconstruction via 3D Shape Induction
- **Arxiv ID**: http://arxiv.org/abs/2208.10937v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.10937v2)
- **Published**: 2022-08-23 13:06:02+00:00
- **Updated**: 2022-11-15 01:57:40+00:00
- **Authors**: Elena Sizikova, Xu Cao, Ashia Lewis, Kenny Moise, Megan Coffee
- **Comment**: Extended Abstract presented at Machine Learning for Health (ML4H)
  symposium 2022, November 28th, 2022, New Orleans, United States & Virtual,
  http://www.ml4h.cc, 11 pages
- **Journal**: None
- **Summary**: Chest computed tomography (CT) imaging adds valuable insight in the diagnosis and management of pulmonary infectious diseases, like tuberculosis (TB). However, due to the cost and resource limitations, only X-ray images may be available for initial diagnosis or follow up comparison imaging during treatment. Due to their projective nature, X-rays images may be more difficult to interpret by clinicians. The lack of publicly available paired X-ray and CT image datasets makes it challenging to train a 3D reconstruction model. In addition, Chest X-ray radiology may rely on different device modalities with varying image quality and there may be variation in underlying population disease spectrum that creates diversity in inputs. We propose shape induction, that is, learning the shape of 3D CT from X-ray without CT supervision, as a novel technique to incorporate realistic X-ray distributions during training of a reconstruction model. Our experiments demonstrate that this process improves both the perceptual quality of generated CT and the accuracy of down-stream classification of pulmonary infectious diseases.



### Deep Structural Causal Shape Models
- **Arxiv ID**: http://arxiv.org/abs/2208.10950v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.10950v1)
- **Published**: 2022-08-23 13:18:20+00:00
- **Updated**: 2022-08-23 13:18:20+00:00
- **Authors**: Rajat Rasal, Daniel C. Castro, Nick Pawlowski, Ben Glocker
- **Comment**: Accepted in 2nd Causality in Vision Workshop at ECCV 2022
- **Journal**: None
- **Summary**: Causal reasoning provides a language to ask important interventional and counterfactual questions beyond purely statistical association. In medical imaging, for example, we may want to study the causal effect of genetic, environmental, or lifestyle factors on the normal and pathological variation of anatomical phenotypes. However, while anatomical shape models of 3D surface meshes, extracted from automated image segmentation, can be reliably constructed, there is a lack of computational tooling to enable causal reasoning about morphological variations. To tackle this problem, we propose deep structural causal shape models (CSMs), which utilise high-quality mesh generation techniques, from geometric deep learning, within the expressive framework of deep structural causal models. CSMs enable subject-specific prognoses through counterfactual mesh generation ("How would this patient's brain structure change if they were ten years older?"), which is in contrast to most current works on purely population-level statistical shape modelling. We demonstrate the capabilities of CSMs at all levels of Pearl's causal hierarchy through a number of qualitative and quantitative experiments leveraging a large dataset of 3D brain structures.



### The Value of Out-of-Distribution Data
- **Arxiv ID**: http://arxiv.org/abs/2208.10967v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2208.10967v5)
- **Published**: 2022-08-23 13:41:01+00:00
- **Updated**: 2023-07-13 10:02:22+00:00
- **Authors**: Ashwin De Silva, Rahul Ramesh, Carey E. Priebe, Pratik Chaudhari, Joshua T. Vogelstein
- **Comment**: Previous versions of this work have been presented at the
  Out-of-Distribution Generalization in Computer Vision (OOD-CV) Workshop (ECCV
  2022) and the Workshop on Distribution Shifts (NeurIPS 2022)
- **Journal**: Proceedings of the 40th International Conference on Machine
  Learning, PMLR 202:7366-7389, 2023
- **Summary**: We expect the generalization error to improve with more samples from a similar task, and to deteriorate with more samples from an out-of-distribution (OOD) task. In this work, we show a counter-intuitive phenomenon: the generalization error of a task can be a non-monotonic function of the number of OOD samples. As the number of OOD samples increases, the generalization error on the target task improves before deteriorating beyond a threshold. In other words, there is value in training on small amounts of OOD data. We use Fisher's Linear Discriminant on synthetic datasets and deep networks on computer vision benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate and analyze this phenomenon. In the idealistic setting where we know which samples are OOD, we show that these non-monotonic trends can be exploited using an appropriately weighted objective of the target and OOD empirical risk. While its practical utility is limited, this does suggest that if we can detect OOD samples, then there may be ways to benefit from them. When we do not know which samples are OOD, we show how a number of go-to strategies such as data-augmentation, hyper-parameter optimization, and pre-training are not enough to ensure that the target generalization error does not deteriorate with the number of OOD samples in the dataset.



### Robust DNN Watermarking via Fixed Embedding Weights with Optimized Distribution
- **Arxiv ID**: http://arxiv.org/abs/2208.10973v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2208.10973v2)
- **Published**: 2022-08-23 13:45:15+00:00
- **Updated**: 2022-11-10 07:59:37+00:00
- **Authors**: Benedetta Tondi, Andrea Costanzo, Mauro Barni
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: Watermarking has been proposed as a way to protect the Intellectual Property Rights of Deep Neural Networks and track their use. Several methods have been proposed to embed the watermark into the trainable parameters of the network (white box watermarking) or into the input-output mapping implemented by the network in correspondence to specific inputs (black box watermarking). In both cases, achieving robustness against fine tuning, model compression and, even more, transfer learning, is one of the most difficult challenges researchers are facing with. In this paper, we propose a new white-box, multi-bit watermarking algorithm with strong robustness properties, including robustness against retraining for transfer learning. Robustness is achieved thanks to a new embedding strategy according to which the watermark message is spread across a number of fixed weights, whose position depends on a secret key. The weights hosting the watermark are set prior to training, and are left unchanged throughout the training procedure. The distribution of the weights carrying the watermark is theoretically optimised to make sure that they are indistinguishable from the non-watermarked weights, while at the same time setting their amplitude to as large as possible values to improve robustness against retraining. We carried out several experiments demonstrating the capability of the proposed scheme to provide high payloads with no significant impact on network accuracy, at the same time ensuring excellent robustness against network modifications an re-use, including retraining and transfer learning.



### Quality Matters: Embracing Quality Clues for Robust 3D Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2208.10976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10976v1)
- **Published**: 2022-08-23 13:47:14+00:00
- **Updated**: 2022-08-23 13:47:14+00:00
- **Authors**: Jinrong Yang, En Yu, Zeming Li, Xiaoping Li, Wenbing Tao
- **Comment**: None
- **Journal**: None
- **Summary**: 3D Multi-Object Tracking (MOT) has achieved tremendous achievement thanks to the rapid development of 3D object detection and 2D MOT. Recent advanced works generally employ a series of object attributes, e.g., position, size, velocity, and appearance, to provide the clues for the association in 3D MOT. However, these cues may not be reliable due to some visual noise, such as occlusion and blur, leading to tracking performance bottleneck. To reveal the dilemma, we conduct extensive empirical analysis to expose the key bottleneck of each clue and how they correlate with each other. The analysis results motivate us to efficiently absorb the merits among all cues, and adaptively produce an optimal tacking manner. Specifically, we present Location and Velocity Quality Learning, which efficiently guides the network to estimate the quality of predicted object attributes. Based on these quality estimations, we propose a quality-aware object association (QOA) strategy to leverage the quality score as an important reference factor for achieving robust association. Despite its simplicity, extensive experiments indicate that the proposed strategy significantly boosts tracking performance by 2.2% AMOTA and our method outperforms all existing state-of-the-art works on nuScenes by a large margin. Moreover, QTrack achieves 48.0% and 51.1% AMOTA tracking performance on the nuScenes validation and test sets, which significantly reduces the performance gap between pure camera and LiDAR based trackers.



### ULISSE: A Tool for One-shot Sky Exploration and its Application to Active Galactic Nuclei Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.10984v1
- **DOI**: 10.1051/0004-6361/202243900
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.10984v1)
- **Published**: 2022-08-23 14:05:30+00:00
- **Updated**: 2022-08-23 14:05:30+00:00
- **Authors**: Lars Doorenbos, Olena Torbaniuk, Stefano Cavuoti, Maurizio Paolillo, Giuseppe Longo, Massimo Brescia, Raphael Sznitman, Pablo Márquez-Neila
- **Comment**: Accepted for publication in A&A
- **Journal**: A&A 666, A171 (2022)
- **Summary**: Modern sky surveys are producing ever larger amounts of observational data, which makes the application of classical approaches for the classification and analysis of objects challenging and time-consuming. However, this issue may be significantly mitigated by the application of automatic machine and deep learning methods. We propose ULISSE, a new deep learning tool that, starting from a single prototype object, is capable of identifying objects sharing the same morphological and photometric properties, and hence of creating a list of candidate sosia. In this work, we focus on applying our method to the detection of AGN candidates in a Sloan Digital Sky Survey galaxy sample, since the identification and classification of Active Galactic Nuclei (AGN) in the optical band still remains a challenging task in extragalactic astronomy. Intended for the initial exploration of large sky surveys, ULISSE directly uses features extracted from the ImageNet dataset to perform a similarity search. The method is capable of rapidly identifying a list of candidates, starting from only a single image of a given prototype, without the need for any time-consuming neural network training. Our experiments show ULISSE is able to identify AGN candidates based on a combination of host galaxy morphology, color and the presence of a central nuclear source, with a retrieval efficiency ranging from 21% to 65% (including composite sources) depending on the prototype, where the random guess baseline is 12%. We find ULISSE to be most effective in retrieving AGN in early-type host galaxies, as opposed to prototypes with spiral- or late-type properties. Based on the results described in this work, ULISSE can be a promising tool for selecting different types of astrophysical objects in current and future wide-field surveys (e.g. Euclid, LSST etc.) that target millions of sources every single night.



### Unsupervised Anomaly Localization with Structural Feature-Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2208.10992v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.10992v1)
- **Published**: 2022-08-23 14:19:46+00:00
- **Updated**: 2022-08-23 14:19:46+00:00
- **Authors**: Felix Meissen, Johannes Paetzold, Georgios Kaissis, Daniel Rueckert
- **Comment**: 10 pages, 5 figures, one table, accepted to the MICCAI 2021 BrainLes
  Workshop
- **Journal**: None
- **Summary**: Unsupervised Anomaly Detection has become a popular method to detect pathologies in medical images as it does not require supervision or labels for training. Most commonly, the anomaly detection model generates a "normal" version of an input image, and the pixel-wise $l^p$-difference of the two is used to localize anomalies. However, large residuals often occur due to imperfect reconstruction of the complex anatomical structures present in most medical images. This method also fails to detect anomalies that are not characterized by large intensity differences to the surrounding tissue. We propose to tackle this problem using a feature-mapping function that transforms the input intensity images into a space with multiple channels where anomalies can be detected along different discriminative feature maps extracted from the original image. We then train an Autoencoder model in this space using structural similarity loss that does not only consider differences in intensity but also in contrast and structure. Our method significantly increases performance on two medical data sets for brain MRI. Code and experiments are available at https://github.com/FeliMe/feature-autoencoder



### An Evolutionary Approach for Creating of Diverse Classifier Ensembles
- **Arxiv ID**: http://arxiv.org/abs/2208.10996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.10996v1)
- **Published**: 2022-08-23 14:23:27+00:00
- **Updated**: 2022-08-23 14:23:27+00:00
- **Authors**: Alvaro R. Ferreira Jr, Fabio A. Faria, Gustavo Carneiro, Vinicius V. de Melo
- **Comment**: None
- **Journal**: None
- **Summary**: Classification is one of the most studied tasks in data mining and machine learning areas and many works in the literature have been presented to solve classification problems for multiple fields of knowledge such as medicine, biology, security, and remote sensing. Since there is no single classifier that achieves the best results for all kinds of applications, a good alternative is to adopt classifier fusion strategies. A key point in the success of classifier fusion approaches is the combination of diversity and accuracy among classifiers belonging to an ensemble. With a large amount of classification models available in the literature, one challenge is the choice of the most suitable classifiers to compose the final classification system, which generates the need of classifier selection strategies. We address this point by proposing a framework for classifier selection and fusion based on a four-step protocol called CIF-E (Classifiers, Initialization, Fitness function, and Evolutionary algorithm). We implement and evaluate 24 varied ensemble approaches following the proposed CIF-E protocol and we are able to find the most accurate approach. A comparative analysis has also been performed among the best approaches and many other baselines from the literature. The experiments show that the proposed evolutionary approach based on Univariate Marginal Distribution Algorithm (UMDA) can outperform the state-of-the-art literature approaches in many well-known UCI datasets.



### Adaptation of MobileNetV2 for Face Detection on Ultra-Low Power Platform
- **Arxiv ID**: http://arxiv.org/abs/2208.11011v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11011v1)
- **Published**: 2022-08-23 14:47:06+00:00
- **Updated**: 2022-08-23 14:47:06+00:00
- **Authors**: Simon Narduzzi, Engin Türetken, Jean-Philippe Thiran, L. Andrea Dunbar
- **Comment**: 6 pages, 4 figures; Accepted at IEEE Swiss Conference on Data Science
  (SDS), Lucerne, 2022
- **Journal**: None
- **Summary**: Designing Deep Neural Networks (DNNs) running on edge hardware remains a challenge. Standard designs have been adopted by the community to facilitate the deployment of Neural Network models. However, not much emphasis is put on adapting the network topology to fit hardware constraints. In this paper, we adapt one of the most widely used architectures for mobile hardware platforms, MobileNetV2, and study the impact of changing its topology and applying post-training quantization. We discuss the impact of the adaptations and the deployment of the model on an embedded hardware platform for face detection.



### AniWho : A Quick and Accurate Way to Classify Anime Character Faces in Images
- **Arxiv ID**: http://arxiv.org/abs/2208.11012v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11012v3)
- **Published**: 2022-08-23 14:50:01+00:00
- **Updated**: 2023-01-10 13:44:47+00:00
- **Authors**: Martinus Grady Naftali, Jason Sebastian Sulistyawan, Kelvin Julian
- **Comment**: 11 pages, 26 figures, 8 tables
- **Journal**: None
- **Summary**: In order to classify Japanese animation-style character faces, this paper attempts to delve further into the many models currently available, including InceptionV3, InceptionResNetV2, MobileNetV2, and EfficientNet, employing transfer learning. This paper demonstrates that EfficientNet-B7, which achieves a top-1 accuracy of 85.08%, has the highest accuracy rate. MobileNetV2, which achieves a less accurate result with a top-1 accuracy of 81.92%, benefits from a significantly faster inference time and fewer required parameters. However, from the experiment, MobileNet-V2 is prone to overfitting; EfficienNet-B0 fixed the overfitting issue but with a cost of a little slower in inference time than MobileNet-V2 but a little more accurate result, top-1 accuracy of 83.46%. This paper also uses a few-shot learning architecture called Prototypical Networks, which offers an adequate substitute for conventional transfer learning techniques.



### Low-Light Video Enhancement with Synthetic Event Guidance
- **Arxiv ID**: http://arxiv.org/abs/2208.11014v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11014v1)
- **Published**: 2022-08-23 14:58:29+00:00
- **Updated**: 2022-08-23 14:58:29+00:00
- **Authors**: Lin Liu, Junfeng An, Jianzhuang Liu, Shanxin Yuan, Xiangyu Chen, Wengang Zhou, Houqiang Li, Yanfeng Wang, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Low-light video enhancement (LLVE) is an important yet challenging task with many applications such as photographing and autonomous driving. Unlike single image low-light enhancement, most LLVE methods utilize temporal information from adjacent frames to restore the color and remove the noise of the target frame. However, these algorithms, based on the framework of multi-frame alignment and enhancement, may produce multi-frame fusion artifacts when encountering extreme low light or fast motion. In this paper, inspired by the low latency and high dynamic range of events, we use synthetic events from multiple frames to guide the enhancement and restoration of low-light videos. Our method contains three stages: 1) event synthesis and enhancement, 2) event and image fusion, and 3) low-light enhancement. In this framework, we design two novel modules (event-image fusion transform and event-guided dual branch) for the second and third stages, respectively. Extensive experiments show that our method outperforms existing low-light video or single image enhancement approaches on both synthetic and real LLVE datasets.



### Adversarial Feature Augmentation for Cross-domain Few-shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2208.11021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11021v1)
- **Published**: 2022-08-23 15:10:22+00:00
- **Updated**: 2022-08-23 15:10:22+00:00
- **Authors**: Yanxu Hu, Andy J. Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Existing methods based on meta-learning predict novel-class labels for (target domain) testing tasks via meta knowledge learned from (source domain) training tasks of base classes. However, most existing works may fail to generalize to novel classes due to the probably large domain discrepancy across domains. To address this issue, we propose a novel adversarial feature augmentation (AFA) method to bridge the domain gap in few-shot learning. The feature augmentation is designed to simulate distribution variations by maximizing the domain discrepancy. During adversarial training, the domain discriminator is learned by distinguishing the augmented features (unseen domain) from the original ones (seen domain), while the domain discrepancy is minimized to obtain the optimal feature encoder. The proposed method is a plug-and-play module that can be easily integrated into existing few-shot learning methods based on meta-learning. Extensive experiments on nine datasets demonstrate the superiority of our method for cross-domain few-shot classification compared with the state of the art. Code is available at https://github.com/youthhoo/AFA_For_Few_shot_learning



### CitySim: A Drone-Based Vehicle Trajectory Dataset for Safety Oriented Research and Digital Twins
- **Arxiv ID**: http://arxiv.org/abs/2208.11036v2
- **DOI**: 10.1177/03611981231185768
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2208.11036v2)
- **Published**: 2022-08-23 15:24:53+00:00
- **Updated**: 2023-07-31 05:04:11+00:00
- **Authors**: Ou Zheng, Mohamed Abdel-Aty, Lishengsa Yue, Amr Abdelraouf, Zijin Wang, Nada Mahmoud
- **Comment**: Transportation Research Record (2023)
- **Journal**: None
- **Summary**: The development of safety-oriented research and applications requires fine-grain vehicle trajectories that not only have high accuracy, but also capture substantial safety-critical events. However, it would be challenging to satisfy both these requirements using the available vehicle trajectory datasets do not have the capacity to satisfy both.This paper introduces the CitySim dataset that has the core objective of facilitating safety-oriented research and applications. CitySim has vehicle trajectories extracted from 1140 minutes of drone videos recorded at 12 locations. It covers a variety of road geometries including freeway basic segments, signalized intersections, stop-controlled intersections, and control-free intersections. CitySim was generated through a five-step procedure that ensured trajectory accuracy. The five-step procedure included video stabilization, object filtering, multi-video stitching, object detection and tracking, and enhanced error filtering. Furthermore, CitySim provides the rotated bounding box information of a vehicle, which was demonstrated to improve safety evaluations. Compared with other video-based critical events, including cut-in, merge, and diverge events, which were validated by distributions of both minimum time-to-collision and minimum post-encroachment time. In addition, CitySim had the capability to facilitate digital-twin-related research by providing relevant assets, such as the recording locations' three-dimensional base maps and signal timings.



### Flat Multi-modal Interaction Transformer for Named Entity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2208.11039v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2208.11039v2)
- **Published**: 2022-08-23 15:25:44+00:00
- **Updated**: 2023-03-09 05:48:21+00:00
- **Authors**: Junyu Lu, Dixiang Zhang, Pingjian Zhang
- **Comment**: Accepted by COLING 2022, oral paper
- **Journal**: None
- **Summary**: Multi-modal named entity recognition (MNER) aims at identifying entity spans and recognizing their categories in social media posts with the aid of images. However, in dominant MNER approaches, the interaction of different modalities is usually carried out through the alternation of self-attention and cross-attention or over-reliance on the gating machine, which results in imprecise and biased correspondence between fine-grained semantic units of text and image. To address this issue, we propose a Flat Multi-modal Interaction Transformer (FMIT) for MNER. Specifically, we first utilize noun phrases in sentences and general domain words to obtain visual cues. Then, we transform the fine-grained semantic representation of the vision and text into a unified lattice structure and design a novel relative position encoding to match different modalities in Transformer. Meanwhile, we propose to leverage entity boundary detection as an auxiliary task to alleviate visual bias. Experiments show that our methods achieve the new state-of-the-art performance on two benchmark datasets.



### Self-Trained Proposal Networks for the Open World
- **Arxiv ID**: http://arxiv.org/abs/2208.11050v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11050v2)
- **Published**: 2022-08-23 15:57:19+00:00
- **Updated**: 2023-01-20 21:17:20+00:00
- **Authors**: Matthew Inkawhich, Nathan Inkawhich, Hai Li, Yiran Chen
- **Comment**: 19 pages, 9 figures, 10 tables
- **Journal**: None
- **Summary**: Current state-of-the-art object proposal networks are trained with a closed-world assumption, meaning they learn to only detect objects of the training classes. These models fail to provide high recall in open-world environments where important novel objects may be encountered. While a handful of recent works attempt to tackle this problem, they fail to consider that the optimal behavior of a proposal network can vary significantly depending on the data and application. Our goal is to provide a flexible proposal solution that can be easily tuned to suit a variety of open-world settings. To this end, we design a Self-Trained Proposal Network (STPN) that leverages an adjustable hybrid architecture, a novel self-training procedure, and dynamic loss components to optimize the tradeoff between known and unknown object detection performance. To thoroughly evaluate our method, we devise several new challenges which invoke varying degrees of label bias by altering known class diversity and label count. We find that in every task, STPN easily outperforms existing baselines (e.g., RPN, OLN). Our method is also highly data efficient, surpassing baseline recall with a fraction of the labeled data.



### IMPaSh: A Novel Domain-shift Resistant Representation for Colorectal Cancer Tissue Classification
- **Arxiv ID**: http://arxiv.org/abs/2208.11052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11052v1)
- **Published**: 2022-08-23 15:59:08+00:00
- **Updated**: 2022-08-23 15:59:08+00:00
- **Authors**: Trinh Thi Le Vuong, Quoc Dang Vu, Mostafa Jahanifar, Simon Graham, Jin Tae Kwak, Nasir Rajpoot
- **Comment**: Accepted in ECCV2022 MCV Workshop
- **Journal**: None
- **Summary**: The appearance of histopathology images depends on tissue type, staining and digitization procedure. These vary from source to source and are the potential causes for domain-shift problems. Owing to this problem, despite the great success of deep learning models in computational pathology, a model trained on a specific domain may still perform sub-optimally when we apply them to another domain. To overcome this, we propose a new augmentation called PatchShuffling and a novel self-supervised contrastive learning framework named IMPaSh for pre-training deep learning models. Using these, we obtained a ResNet50 encoder that can extract image representation resistant to domain-shift. We compared our derived representation against those acquired based on other domain-generalization techniques by using them for the cross-domain classification of colorectal tissue images. We show that the proposed method outperforms other traditional histology domain-adaptation and state-of-the-art self-supervised learning methods. Code is available at: https://github.com/trinhvg/IMPash .



### Neuroevolution-based Classifiers for Deforestation Detection in Tropical Forests
- **Arxiv ID**: http://arxiv.org/abs/2208.11058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11058v1)
- **Published**: 2022-08-23 16:04:12+00:00
- **Updated**: 2022-08-23 16:04:12+00:00
- **Authors**: Guilherme A. Pimenta, Fernanda B. J. R. Dallaqua, Alvaro Fazenda, Fabio A. Faria
- **Comment**: 6 pages, accepted for presentation at the SIBGRAPI 2022
- **Journal**: None
- **Summary**: Tropical forests represent the home of many species on the planet for flora and fauna, retaining billions of tons of carbon footprint, promoting clouds and rain formation, implying a crucial role in the global ecosystem, besides representing the home to countless indigenous peoples. Unfortunately, millions of hectares of tropical forests are lost every year due to deforestation or degradation. To mitigate that fact, monitoring and deforestation detection programs are in use, in addition to public policies for the prevention and punishment of criminals. These monitoring/detection programs generally use remote sensing images, image processing techniques, machine learning methods, and expert photointerpretation to analyze, identify and quantify possible changes in forest cover. Several projects have proposed different computational approaches, tools, and models to efficiently identify recent deforestation areas, improving deforestation monitoring programs in tropical forests. In this sense, this paper proposes the use of pattern classifiers based on neuroevolution technique (NEAT) in tropical forest deforestation detection tasks. Furthermore, a novel framework called e-NEAT has been created and achieved classification results above $90\%$ for balanced accuracy measure in the target application using an extremely reduced and limited training set for learning the classification models. These results represent a relative gain of $6.2\%$ over the best baseline ensemble method compared in this paper



### ZoomNAS: Searching for Whole-body Human Pose Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2208.11547v1
- **DOI**: 10.1109/TPAMI.2022.3197352
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11547v1)
- **Published**: 2022-08-23 16:33:57+00:00
- **Updated**: 2022-08-23 16:33:57+00:00
- **Authors**: Lumin Xu, Sheng Jin, Wentao Liu, Chen Qian, Wanli Ouyang, Ping Luo, Xiaogang Wang
- **Comment**: Accepted by TPAMI
- **Journal**: None
- **Summary**: This paper investigates the task of 2D whole-body human pose estimation, which aims to localize dense landmarks on the entire human body including body, feet, face, and hands. We propose a single-network approach, termed ZoomNet, to take into account the hierarchical structure of the full human body and solve the scale variation of different body parts. We further propose a neural architecture search framework, termed ZoomNAS, to promote both the accuracy and efficiency of whole-body pose estimation. ZoomNAS jointly searches the model architecture and the connections between different sub-modules, and automatically allocates computational complexity for searched sub-modules. To train and evaluate ZoomNAS, we introduce the first large-scale 2D human whole-body dataset, namely COCO-WholeBody V1.0, which annotates 133 keypoints for in-the-wild images. Extensive experiments demonstrate the effectiveness of ZoomNAS and the significance of COCO-WholeBody V1.0.



### DepthFake: a depth-based strategy for detecting Deepfake videos
- **Arxiv ID**: http://arxiv.org/abs/2208.11074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11074v1)
- **Published**: 2022-08-23 16:38:25+00:00
- **Updated**: 2022-08-23 16:38:25+00:00
- **Authors**: Luca Maiano, Lorenzo Papa, Ketbjano Vocaj, Irene Amerini
- **Comment**: 2022 ICPR Workshop on Artificial Intelligence for Multimedia
  Forensics and Disinformation Detection
- **Journal**: None
- **Summary**: Fake content has grown at an incredible rate over the past few years. The spread of social media and online platforms makes their dissemination on a large scale increasingly accessible by malicious actors. In parallel, due to the growing diffusion of fake image generation methods, many Deep Learning-based detection techniques have been proposed. Most of those methods rely on extracting salient features from RGB images to detect through a binary classifier if the image is fake or real. In this paper, we proposed DepthFake, a study on how to improve classical RGB-based approaches with depth-maps. The depth information is extracted from RGB images with recent monocular depth estimation techniques. Here, we demonstrate the effective contribution of depth-maps to the deepfake detection task on robust pre-trained architectures. The proposed RGBD approach is in fact able to achieve an average improvement of 3.20% and up to 11.7% for some deepfake attacks with respect to standard RGB architectures over the FaceForensic++ dataset.



### Robot Active Neural Sensing and Planning in Unknown Cluttered Environments
- **Arxiv ID**: http://arxiv.org/abs/2208.11079v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11079v2)
- **Published**: 2022-08-23 16:56:54+00:00
- **Updated**: 2022-08-24 00:52:09+00:00
- **Authors**: Hanwen Ren, Ahmed H. Qureshi
- **Comment**: https://sites.google.com/view/active-neural-sensing/home
- **Journal**: None
- **Summary**: Active sensing and planning in unknown, cluttered environments is an open challenge for robots intending to provide home service, search and rescue, narrow-passage inspection, and medical assistance. Although many active sensing methods exist, they often consider open spaces, assume known settings, or mostly do not generalize to real-world scenarios. We present the active neural sensing approach that generates the kinematically feasible viewpoint sequences for the robot manipulator with an in-hand camera to gather the minimum number of observations needed to reconstruct the underlying environment. Our framework actively collects the visual RGBD observations, aggregates them into scene representation, and performs object shape inference to avoid unnecessary robot interactions with the environment. We train our approach on synthetic data with domain randomization and demonstrate its successful execution via sim-to-real transfer in reconstructing narrow, covered, real-world cabinet environments cluttered with unknown objects. The natural cabinet scenarios impose significant challenges for robot motion and scene reconstruction due to surrounding obstacles and low ambient lighting conditions. However, despite unfavorable settings, our method exhibits high performance compared to its baselines in terms of various environment reconstruction metrics, including planning speed, the number of viewpoints, and overall scene coverage.



### Consistency Regularization for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2208.11084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11084v1)
- **Published**: 2022-08-23 17:07:06+00:00
- **Updated**: 2022-08-23 17:07:06+00:00
- **Authors**: Kian Boon Koh, Basura Fernando
- **Comment**: ECCV 2022 workshop paper
- **Journal**: None
- **Summary**: Collection of real world annotations for training semantic segmentation models is an expensive process. Unsupervised domain adaptation (UDA) tries to solve this problem by studying how more accessible data such as synthetic data can be used to train and adapt models to real world images without requiring their annotations. Recent UDA methods applies self-learning by training on pixel-wise classification loss using a student and teacher network. In this paper, we propose the addition of a consistency regularization term to semi-supervised UDA by modelling the inter-pixel relationship between elements in networks' output. We demonstrate the effectiveness of the proposed consistency regularization term by applying it to the state-of-the-art DAFormer framework and improving mIoU19 performance on the GTA5 to Cityscapes benchmark by 0.8 and mIou16 performance on the SYNTHIA to Cityscapes benchmark by 1.2.



### Explaining Bias in Deep Face Recognition via Image Characteristics
- **Arxiv ID**: http://arxiv.org/abs/2208.11099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11099v1)
- **Published**: 2022-08-23 17:18:23+00:00
- **Updated**: 2022-08-23 17:18:23+00:00
- **Authors**: Andrea Atzori, Gianni Fenu, Mirko Marras
- **Comment**: Accepted as a full paper at IJCB 2022: 2022 International Joint
  Conference on Biometrics
- **Journal**: None
- **Summary**: In this paper, we propose a novel explanatory framework aimed to provide a better understanding of how face recognition models perform as the underlying data characteristics (protected attributes: gender, ethnicity, age; non-protected attributes: facial hair, makeup, accessories, face orientation and occlusion, image distortion, emotions) on which they are tested change. With our framework, we evaluate ten state-of-the-art face recognition models, comparing their fairness in terms of security and usability on two data sets, involving six groups based on gender and ethnicity. We then analyze the impact of image characteristics on models performance. Our results show that trends appearing in a single-attribute analysis disappear or reverse when multi-attribute groups are considered, and that performance disparities are also related to non-protected attributes. Source code: https://cutt.ly/2XwRLiA.



### Efficient Attention-free Video Shift Transformers
- **Arxiv ID**: http://arxiv.org/abs/2208.11108v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11108v1)
- **Published**: 2022-08-23 17:48:29+00:00
- **Updated**: 2022-08-23 17:48:29+00:00
- **Authors**: Adrian Bulat, Brais Martinez, Georgios Tzimiropoulos
- **Comment**: None
- **Journal**: None
- **Summary**: This paper tackles the problem of efficient video recognition. In this area, video transformers have recently dominated the efficiency (top-1 accuracy vs FLOPs) spectrum. At the same time, there have been some attempts in the image domain which challenge the necessity of the self-attention operation within the transformer architecture, advocating the use of simpler approaches for token mixing. However, there are no results yet for the case of video recognition, where the self-attention operator has a significantly higher impact (compared to the case of images) on efficiency. To address this gap, in this paper, we make the following contributions: (a) we construct a highly efficient \& accurate attention-free block based on the shift operator, coined Affine-Shift block, specifically designed to approximate as closely as possible the operations in the MHSA block of a Transformer layer. Based on our Affine-Shift block, we construct our Affine-Shift Transformer and show that it already outperforms all existing shift/MLP--based architectures for ImageNet classification. (b) We extend our formulation in the video domain to construct Video Affine-Shift Transformer (VAST), the very first purely attention-free shift-based video transformer. (c) We show that VAST significantly outperforms recent state-of-the-art transformers on the most popular action recognition benchmarks for the case of models with low computational and memory footprint. Code will be made available.



### DeepInteraction: 3D Object Detection via Modality Interaction
- **Arxiv ID**: http://arxiv.org/abs/2208.11112v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11112v4)
- **Published**: 2022-08-23 17:52:54+00:00
- **Updated**: 2022-12-08 15:14:27+00:00
- **Authors**: Zeyu Yang, Jiaqi Chen, Zhenwei Miao, Wei Li, Xiatian Zhu, Li Zhang
- **Comment**: To appear at NeurIPS 2022. 16 pages, 7 figure
- **Journal**: None
- **Summary**: Existing top-performance 3D object detectors typically rely on the multi-modal fusion strategy. This design is however fundamentally restricted due to overlooking the modality-specific useful information and finally hampering the model performance. To address this limitation, in this work we introduce a novel modality interaction strategy where individual per-modality representations are learned and maintained throughout for enabling their unique characteristics to be exploited during object detection. To realize this proposed strategy, we design a DeepInteraction architecture characterized by a multi-modal representational interaction encoder and a multi-modal predictive interaction decoder. Experiments on the large-scale nuScenes dataset show that our proposed method surpasses all prior arts often by a large margin. Crucially, our method is ranked at the first position at the highly competitive nuScenes object detection leaderboard.



### Towards Open Set Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.11113v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11113v1)
- **Published**: 2022-08-23 17:53:34+00:00
- **Updated**: 2022-08-23 17:53:34+00:00
- **Authors**: Yuansheng Zhu, Wentao Bao, Qi Yu
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Open Set Video Anomaly Detection (OpenVAD) aims to identify abnormal events from video data where both known anomalies and novel ones exist in testing. Unsupervised models learned solely from normal videos are applicable to any testing anomalies but suffer from a high false positive rate. In contrast, weakly supervised methods are effective in detecting known anomalies but could fail in an open world. We develop a novel weakly supervised method for the OpenVAD problem by integrating evidential deep learning (EDL) and normalizing flows (NFs) into a multiple instance learning (MIL) framework. Specifically, we propose to use graph neural networks and triplet loss to learn discriminative features for training the EDL classifier, where the EDL is capable of identifying the unknown anomalies by quantifying the uncertainty. Moreover, we develop an uncertainty-aware selection strategy to obtain clean anomaly instances and a NFs module to generate the pseudo anomalies. Our method is superior to existing approaches by inheriting the advantages of both the unsupervised NFs and the weakly-supervised MIL framework. Experimental results on multiple real-world video datasets show the effectiveness of our method.



### Distance-Aware Occlusion Detection with Focused Attention
- **Arxiv ID**: http://arxiv.org/abs/2208.11122v1
- **DOI**: 10.1109/TIP.2022.3197984
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11122v1)
- **Published**: 2022-08-23 17:59:15+00:00
- **Updated**: 2022-08-23 17:59:15+00:00
- **Authors**: Yang Li, Yucheng Tu, Xiaoxue Chen, Hao Zhao, Guyue Zhou
- **Comment**: Accepted to IEEE Transactions on Image Processing. Code:
  https://github.com/Yang-Li-2000/Distance-Aware-Occlusion-Detection-with-Focused-Attention.git
- **Journal**: None
- **Summary**: For humans, understanding the relationships between objects using visual signals is intuitive. For artificial intelligence, however, this task remains challenging. Researchers have made significant progress studying semantic relationship detection, such as human-object interaction detection and visual relationship detection. We take the study of visual relationships a step further from semantic to geometric. In specific, we predict relative occlusion and relative distance relationships. However, detecting these relationships from a single image is challenging. Enforcing focused attention to task-specific regions plays a critical role in successfully detecting these relationships. In this work, (1) we propose a novel three-decoder architecture as the infrastructure for focused attention; 2) we use the generalized intersection box prediction task to effectively guide our model to focus on occlusion-specific regions; 3) our model achieves a new state-of-the-art performance on distance-aware relationship detection. Specifically, our model increases the distance F1-score from 33.8% to 38.6% and boosts the occlusion F1-score from 34.4% to 41.2%. Our code is publicly available.



### Multi-domain Learning for Updating Face Anti-spoofing Models
- **Arxiv ID**: http://arxiv.org/abs/2208.11148v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11148v2)
- **Published**: 2022-08-23 18:28:34+00:00
- **Updated**: 2023-04-17 03:47:27+00:00
- **Authors**: Xiao Guo, Yaojie Liu, Anil Jain, Xiaoming Liu
- **Comment**: To appear at ECCV 2022 as an oral presentation. The SiW-Mv2 dataset
  is detailed in the Appendix
- **Journal**: None
- **Summary**: In this work, we study multi-domain learning for face anti-spoofing(MD-FAS), where a pre-trained FAS model needs to be updated to perform equally well on both source and target domains while only using target domain data for updating. We present a new model for MD-FAS, which addresses the forgetting issue when learning new domain data, while possessing a high level of adaptability. First, we devise a simple yet effective module, called spoof region estimator(SRE), to identify spoof traces in the spoof image. Such spoof traces reflect the source pre-trained model's responses that help upgraded models combat catastrophic forgetting during updating. Unlike prior works that estimate spoof traces which generate multiple outputs or a low-resolution binary mask, SRE produces one single, detailed pixel-wise estimate in an unsupervised manner. Secondly, we propose a novel framework, named FAS-wrapper, which transfers knowledge from the pre-trained models and seamlessly integrates with different FAS models. Lastly, to help the community further advance MD-FAS, we construct a new benchmark based on SIW, SIW-Mv2 and Oulu-NPU, and introduce four distinct protocols for evaluation, where source and target domains are different in terms of spoof type, age, ethnicity, and illumination. Our proposed method achieves superior performance on the MD-FAS benchmark than previous methods. Our code and newly curated SIW-Mv2 are publicly available.



### Doc2Graph: a Task Agnostic Document Understanding Framework based on Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.11168v1
- **DOI**: 10.1007/978-3-031-25069-9_22
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11168v1)
- **Published**: 2022-08-23 19:48:10+00:00
- **Updated**: 2022-08-23 19:48:10+00:00
- **Authors**: Andrea Gemelli, Sanket Biswas, Enrico Civitelli, Josep Lladós, Simone Marinai
- **Comment**: TiE ECCV 2022
- **Journal**: Computer Vision -- ECCV 2022 Workshops, vol. 13804, 2023, pp.
  329-344
- **Summary**: Geometric Deep Learning has recently attracted significant interest in a wide range of machine learning fields, including document analysis. The application of Graph Neural Networks (GNNs) has become crucial in various document-related tasks since they can unravel important structural patterns, fundamental in key information extraction processes. Previous works in the literature propose task-driven models and do not take into account the full power of graphs. We propose Doc2Graph, a task-agnostic document understanding framework based on a GNN model, to solve different tasks given different types of documents. We evaluated our approach on two challenging datasets for key information extraction in form understanding, invoice layout analysis and table detection. Our code is freely accessible on https://github.com/andreagemelli/doc2graph.



### A Study on the Impact of Data Augmentation for Training Convolutional Neural Networks in the Presence of Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2208.11176v3
- **DOI**: 10.1109/SIBGRAPI55357.2022.9991791
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11176v3)
- **Published**: 2022-08-23 20:04:17+00:00
- **Updated**: 2023-08-07 11:36:16+00:00
- **Authors**: Emeson Santana, Gustavo Carneiro, Filipe R. Cordeiro
- **Comment**: Paper accepted at SIBGRAPI 2022
- **Journal**: None
- **Summary**: Label noise is common in large real-world datasets, and its presence harms the training process of deep neural networks. Although several works have focused on the training strategies to address this problem, there are few studies that evaluate the impact of data augmentation as a design choice for training deep neural networks. In this work, we analyse the model robustness when using different data augmentations and their improvement on the training with the presence of noisy labels. We evaluate state-of-the-art and classical data augmentation strategies with different levels of synthetic noise for the datasets MNist, CIFAR-10, CIFAR-100, and the real-world dataset Clothing1M. We evaluate the methods using the accuracy metric. Results show that the appropriate selection of data augmentation can drastically improve the model robustness to label noise, increasing up to 177.84% of relative best test accuracy compared to the baseline with no augmentation, and an increase of up to 6% in absolute value with the state-of-the-art DivideMix training strategy.



### AIM 2022 Challenge on Super-Resolution of Compressed Image and Video: Dataset, Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2208.11184v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.11184v2)
- **Published**: 2022-08-23 20:32:38+00:00
- **Updated**: 2022-08-25 14:44:53+00:00
- **Authors**: Ren Yang, Radu Timofte, Xin Li, Qi Zhang, Lin Zhang, Fanglong Liu, Dongliang He, Fu li, He Zheng, Weihang Yuan, Pavel Ostyakov, Dmitry Vyal, Magauiya Zhussip, Xueyi Zou, Youliang Yan, Lei Li, Jingzhu Tang, Ming Chen, Shijie Zhao, Yu Zhu, Xiaoran Qin, Chenghua Li, Cong Leng, Jian Cheng, Claudio Rota, Marco Buzzelli, Simone Bianco, Raimondo Schettini, Dafeng Zhang, Feiyu Huang, Shizhuo Liu, Xiaobing Wang, Zhezhu Jin, Bingchen Li, Xin Li, Mingxi Li, Ding Liu, Wenbin Zou, Peijie Dong, Tian Ye, Yunchen Zhang, Ming Tan, Xin Niu, Mustafa Ayazoglu, Marcos Conde, Ui-Jin Choi, Zhuang Jia, Tianyu Xu, Yijian Zhang, Mao Ye, Dengyan Luo, Xiaofeng Pan, Liuhan Peng
- **Comment**: Camera-ready version
- **Journal**: None
- **Summary**: This paper reviews the Challenge on Super-Resolution of Compressed Image and Video at AIM 2022. This challenge includes two tracks. Track 1 aims at the super-resolution of compressed image, and Track~2 targets the super-resolution of compressed video. In Track 1, we use the popular dataset DIV2K as the training, validation and test sets. In Track 2, we propose the LDV 3.0 dataset, which contains 365 videos, including the LDV 2.0 dataset (335 videos) and 30 additional videos. In this challenge, there are 12 teams and 2 teams that submitted the final results to Track 1 and Track 2, respectively. The proposed methods and solutions gauge the state-of-the-art of super-resolution on compressed image and video. The proposed LDV 3.0 dataset is available at https://github.com/RenYang-home/LDV_dataset. The homepage of this challenge is at https://github.com/RenYang-home/AIM22_CompressSR.



### Achieving Fairness in Dermatological Disease Diagnosis through Automatic Weight Adjusting Federated Learning and Personalization
- **Arxiv ID**: http://arxiv.org/abs/2208.11187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11187v1)
- **Published**: 2022-08-23 20:44:09+00:00
- **Updated**: 2022-08-23 20:44:09+00:00
- **Authors**: Gelei Xu, Yawen Wu, Jingtong Hu, Yiyu Shi
- **Comment**: 8 pages, 2 figures
- **Journal**: None
- **Summary**: Dermatological diseases pose a major threat to the global health, affecting almost one-third of the world's population. Various studies have demonstrated that early diagnosis and intervention are often critical to prognosis and outcome. To this end, the past decade has witnessed the rapid evolvement of deep learning based smartphone apps, which allow users to conveniently and timely identify issues that have emerged around their skins. In order to collect sufficient data needed by deep learning and at the same time protect patient privacy, federated learning is often used, where individual clients aggregate a global model while keeping datasets local. However, existing federated learning frameworks are mostly designed to optimize the overall performance, while common dermatological datasets are heavily imbalanced. When applying federated learning to such datasets, significant disparities in diagnosis accuracy may occur. To address such a fairness issue, this paper proposes a fairness-aware federated learning framework for dermatological disease diagnosis. The framework is divided into two stages: In the first in-FL stage, clients with different skin types are trained in a federated learning process to construct a global model for all skin types. An automatic weight aggregator is used in this process to assign higher weights to the client with higher loss, and the intensity of the aggregator is determined by the level of difference between losses. In the latter post-FL stage, each client fine-tune its personalized model based on the global model in the in-FL stage. To achieve better fairness, models from different epochs are selected for each client to keep the accuracy difference of different skin types within 0.05. Experiments indicate that our proposed framework effectively improves both fairness and accuracy compared with the state-of-the-art.



### Towards cumulative race time regression in sports: I3D ConvNet transfer learning in ultra-distance running events
- **Arxiv ID**: http://arxiv.org/abs/2208.11191v1
- **DOI**: 10.1109/ICPR56361.2022.9956174
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11191v1)
- **Published**: 2022-08-23 20:53:01+00:00
- **Updated**: 2022-08-23 20:53:01+00:00
- **Authors**: David Freire-Obregón, Javier Lorenzo-Navarro, Oliverio J. Santana, Daniel Hernández-Sosa, Modesto Castrillón-Santana
- **Comment**: Accepted at 26th International Conference on Pattern Recognition
  (ICPR 2022)
- **Journal**: None
- **Summary**: Predicting an athlete's performance based on short footage is highly challenging. Performance prediction requires high domain knowledge and enough evidence to infer an appropriate quality assessment. Sports pundits can often infer this kind of information in real-time. In this paper, we propose regressing an ultra-distance runner cumulative race time (CRT), i.e., the time the runner has been in action since the race start, by using only a few seconds of footage as input. We modified the I3D ConvNet backbone slightly and trained a newly added regressor for that purpose. We use appropriate pre-processing of the visual input to enable transfer learning from a specific runner. We show that the resulting neural network can provide a remarkable performance for short input footage: 18 minutes and a half mean absolute error in estimating the CRT for runners who have been in action from 8 to 20 hours. Our methodology has several favorable properties: it does not require a human expert to provide any insight, it can be used at any moment during the race by just observing a runner, and it can inform the race staff about a runner at any given time.



### Modelling Latent Dynamics of StyleGAN using Neural ODEs
- **Arxiv ID**: http://arxiv.org/abs/2208.11197v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11197v2)
- **Published**: 2022-08-23 21:20:38+00:00
- **Updated**: 2023-04-22 20:18:14+00:00
- **Authors**: Weihao Xia, Yujiu Yang, Jing-Hao Xue
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose to model the video dynamics by learning the trajectory of independently inverted latent codes from GANs. The entire sequence is seen as discrete-time observations of a continuous trajectory of the initial latent code, by considering each latent code as a moving particle and the latent space as a high-dimensional dynamic system. The latent codes representing different frames are therefore reformulated as state transitions of the initial frame, which can be modeled by neural ordinary differential equations. The learned continuous trajectory allows us to perform infinite frame interpolation and consistent video manipulation. The latter task is reintroduced for video editing with the advantage of requiring the core operations to be applied to the first frame only while maintaining temporal consistency across all frames. Extensive experiments demonstrate that our method achieves state-of-the-art performance but with much less computation. Code is available at https://github.com/weihaox/dynode_released.



### Graph Neural Networks and Representation Embedding for Table Extraction in PDF Documents
- **Arxiv ID**: http://arxiv.org/abs/2208.11203v1
- **DOI**: 10.1109/ICPR56361.2022.9956590
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11203v1)
- **Published**: 2022-08-23 21:36:01+00:00
- **Updated**: 2022-08-23 21:36:01+00:00
- **Authors**: Andrea Gemelli, Emanuele Vivoli, Simone Marinai
- **Comment**: ICPR 2022
- **Journal**: None
- **Summary**: Tables are widely used in several types of documents since they can bring important information in a structured way. In scientific papers, tables can sum up novel discoveries and summarize experimental results, making the research comparable and easily understandable by scholars. Several methods perform table analysis working on document images, losing useful information during the conversion from the PDF files since OCR tools can be prone to recognition errors, in particular for text inside tables. The main contribution of this work is to tackle the problem of table extraction, exploiting Graph Neural Networks. Node features are enriched with suitably designed representation embeddings. These representations help to better distinguish not only tables from the other parts of the paper, but also table cells from table headers. We experimentally evaluated the proposed approach on a new dataset obtained by merging the information provided in the PubLayNet and PubTables-1M datasets.



### Data augmentation on graphs for table type classification
- **Arxiv ID**: http://arxiv.org/abs/2208.11210v1
- **DOI**: 10.1007/978-3-031-23028-8_25
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11210v1)
- **Published**: 2022-08-23 21:54:46+00:00
- **Updated**: 2022-08-23 21:54:46+00:00
- **Authors**: Davide del Bimbo, Andrea Gemelli, Simone Marinai
- **Comment**: S+SSPR 2022
- **Journal**: None
- **Summary**: Tables are widely used in documents because of their compact and structured representation of information. In particular, in scientific papers, tables can sum up novel discoveries and summarize experimental results, making the research comparable and easily understandable by scholars. Since the layout of tables is highly variable, it would be useful to interpret their content and classify them into categories. This could be helpful to directly extract information from scientific papers, for instance comparing performance of some models given their paper result tables. In this work, we address the classification of tables using a Graph Neural Network, exploiting the table structure for the message passing algorithm in use. We evaluate our model on a subset of the Tab2Know dataset. Since it contains few examples manually annotated, we propose data augmentation techniques directly on the table graph structures. We achieve promising preliminary results, proposing a data augmentation method suitable for graph-based table representation.



### Why is the video analytics accuracy fluctuating, and what can we do about it?
- **Arxiv ID**: http://arxiv.org/abs/2208.12644v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.12644v2)
- **Published**: 2022-08-23 23:16:24+00:00
- **Updated**: 2022-09-15 20:46:09+00:00
- **Authors**: Sibendu Paul, Kunal Rao, Giuseppe Coviello, Murugan Sankaradas, Oliver Po, Y. Charlie Hu, Srimat Chakradhar
- **Comment**: None
- **Journal**: None
- **Summary**: It is a common practice to think of a video as a sequence of images (frames), and re-use deep neural network models that are trained only on images for similar analytics tasks on videos. In this paper, we show that this leap of faith that deep learning models that work well on images will also work well on videos is actually flawed. We show that even when a video camera is viewing a scene that is not changing in any human-perceptible way, and we control for external factors like video compression and environment (lighting), the accuracy of video analytics application fluctuates noticeably. These fluctuations occur because successive frames produced by the video camera may look similar visually, but these frames are perceived quite differently by the video analytics applications. We observed that the root cause for these fluctuations is the dynamic camera parameter changes that a video camera automatically makes in order to capture and produce a visually pleasing video. The camera inadvertently acts as an unintentional adversary because these slight changes in the image pixel values in consecutive frames, as we show, have a noticeably adverse impact on the accuracy of insights from video analytics tasks that re-use image-trained deep learning models. To address this inadvertent adversarial effect from the camera, we explore the use of transfer learning techniques to improve learning in video analytics tasks through the transfer of knowledge from learning on image analytics tasks. In particular, we show that our newly trained Yolov5 model reduces fluctuation in object detection across frames, which leads to better tracking of objects(40% fewer mistakes in tracking). Our paper also provides new directions and techniques to mitigate the camera's adversarial effect on deep learning models used for video analytics applications.



