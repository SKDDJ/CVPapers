# Arxiv Papers in cs.CV on 2022-08-29
### Fluorescence molecular optomic signatures improve identification of tumors in head and neck specimens
- **Arxiv ID**: http://arxiv.org/abs/2208.13314v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, 68T10 (Primary) 68U10 (Secondary), I.5.2; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2208.13314v1)
- **Published**: 2022-08-29 00:06:25+00:00
- **Updated**: 2022-08-29 00:06:25+00:00
- **Authors**: Yao Chen, Samuel S. Streeter, Brady Hunt, Hira S. Sardar, Jason R. Gunn, Laura J. Tafe, Joseph A. Paydarfar, Brian W. Pogue, Keith D. Paulsen, Kimberley S. Samkoe
- **Comment**: 21 pages, 8 figures, 1 table, submitted as a manuscript at Frontiers
  in Medical Technology
- **Journal**: None
- **Summary**: In this study, a radiomics approach was extended to optical fluorescence molecular imaging data for tissue classification, termed 'optomics'. Fluorescence molecular imaging is emerging for precise surgical guidance during head and neck squamous cell carcinoma (HNSCC) resection. However, the tumor-to-normal tissue contrast is confounded by intrinsic physiological limitations of heterogeneous expression of the target molecule, epidermal growth factor receptor (EGFR). Optomics seek to improve tumor identification by probing textural pattern differences in EGFR expression conveyed by fluorescence. A total of 1,472 standardized optomic features were extracted from fluorescence image samples. A supervised machine learning pipeline involving a support vector machine classifier was trained with 25 top-ranked features selected by minimum redundancy maximum relevance criterion. Model predictive performance was compared to fluorescence intensity thresholding method by classifying testing set image patches of resected tissue with histologically confirmed malignancy status. The optomics approach provided consistent improvement in prediction accuracy on all test set samples, irrespective of dose, compared to fluorescence intensity thresholding method (mean accuracies of 89% vs. 81%; P = 0.0072). The improved performance demonstrates that extending the radiomics approach to fluorescence molecular imaging data offers a promising image analysis technique for cancer detection in fluorescence-guided surgery.



### Real-Time Mask Detection Based on SSD-MobileNetV2
- **Arxiv ID**: http://arxiv.org/abs/2208.13333v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13333v1)
- **Published**: 2022-08-29 01:59:22+00:00
- **Updated**: 2022-08-29 01:59:22+00:00
- **Authors**: Chen Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: After the outbreak of COVID-19, mask detection, as the most convenient and effective means of prevention, plays a crucial role in epidemic prevention and control. An excellent automatic real-time mask detection system can reduce a lot of work pressure for relevant staff. However, by analyzing the existing mask detection approaches, we find that they are mostly resource-intensive and do not achieve a good balance between speed and accuracy. And there is no perfect face mask dataset at present. In this paper, we propose a new architecture for mask detection. Our system uses SSD as the mask locator and classifier, and further replaces VGG-16 with MobileNetV2 to extract the features of the image and reduce a lot of parameters. Therefore, our system can be deployed on embedded devices. Transfer learning methods are used to transfer pre-trained models from other domains to our model. Data enhancement methods in our system such as MixUp effectively prevent overfitting. It also effectively reduces the dependence on large-scale datasets. By doing experiments in practical scenarios, the results demonstrate that our system performed well in real-time mask detection.



### Label Propagation for 3D Carotid Vessel Wall Segmentation and Atherosclerosis Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2208.13337v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.13337v1)
- **Published**: 2022-08-29 02:13:44+00:00
- **Updated**: 2022-08-29 02:13:44+00:00
- **Authors**: Shishuai Hu, Zehui Liao, Yong Xia
- **Comment**: Technical report. Solution to CarOtid vessel wall SegMentation and
  atherosclerOsis diagnosiS challenge (COSMOS 2022)
- **Journal**: None
- **Summary**: Carotid vessel wall segmentation is a crucial yet challenging task in the computer-aided diagnosis of atherosclerosis. Although numerous deep learning models have achieved remarkable success in many medical image segmentation tasks, accurate segmentation of carotid vessel wall on magnetic resonance (MR) images remains challenging, due to limited annotations and heterogeneous arteries. In this paper, we propose a semi-supervised label propagation framework to segment lumen, normal vessel walls, and atherosclerotic vessel wall on 3D MR images. By interpolating the provided annotations, we get 3D continuous labels for training 3D segmentation model. With the trained model, we generate pseudo labels for unlabeled slices to incorporate them for model training. Then we use the whole MR scans and the propagated labels to re-train the segmentation model and improve its robustness. We evaluated the label propagation framework on the CarOtid vessel wall SegMentation and atherosclerOsis diagnosiS (COSMOS) Challenge dataset and achieved a QuanM score of 83.41\% on the testing dataset, which got the 1-st place on the online evaluation leaderboard. The results demonstrate the effectiveness of the proposed framework.



### Boundary-Aware Network for Kidney Parsing
- **Arxiv ID**: http://arxiv.org/abs/2208.13338v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.13338v1)
- **Published**: 2022-08-29 02:19:30+00:00
- **Updated**: 2022-08-29 02:19:30+00:00
- **Authors**: Shishuai Hu, Yiwen Ye, Zehui Liao, Yong Xia
- **Comment**: Technical report. Solution to Kidney PArsing Challenge 2022 (KiPA22)
- **Journal**: None
- **Summary**: Kidney structures segmentation is a crucial yet challenging task in the computer-aided diagnosis of surgery-based renal cancer. Although numerous deep learning models have achieved remarkable success in many medical image segmentation tasks, accurate segmentation of kidney structures on computed tomography angiography (CTA) images remains challenging, due to the variable sizes of kidney tumors and the ambiguous boundaries between kidney structures and their surroundings. In this paper, we propose a boundary-aware network (BA-Net) to segment kidneys, kidney tumors, arteries, and veins on CTA scans. This model contains a shared encoder, a boundary decoder, and a segmentation decoder. The multi-scale deep supervision strategy is adopted on both decoders, which can alleviate the issues caused by variable tumor sizes. The boundary probability maps produced by the boundary decoder at each scale are used as attention to enhance the segmentation feature maps. We evaluated the BA-Net on the Kidney PArsing (KiPA) Challenge dataset and achieved an average Dice score of 89.65$\%$ for kidney structure segmentation on CTA scans using 4-fold cross-validation. The results demonstrate the effectiveness of the BA-Net.



### Boundary-Aware Network for Abdominal Multi-Organ Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.13774v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.13774v1)
- **Published**: 2022-08-29 02:24:02+00:00
- **Updated**: 2022-08-29 02:24:02+00:00
- **Authors**: Shishuai Hu, Zehui Liao, Yong Xia
- **Comment**: Technical report. Solution to Multi-Modality Abdominal Multi-Organ
  Segmentation Challenge 2022 (AMOS 2022). arXiv admin note: substantial text
  overlap with arXiv:2208.13338
- **Journal**: None
- **Summary**: Automated abdominal multi-organ segmentation is a crucial yet challenging task in the computer-aided diagnosis of abdominal organ-related diseases. Although numerous deep learning models have achieved remarkable success in many medical image segmentation tasks, accurate segmentation of abdominal organs remains challenging, due to the varying sizes of abdominal organs and the ambiguous boundaries among them. In this paper, we propose a boundary-aware network (BA-Net) to segment abdominal organs on CT scans and MRI scans. This model contains a shared encoder, a boundary decoder, and a segmentation decoder. The multi-scale deep supervision strategy is adopted on both decoders, which can alleviate the issues caused by variable organ sizes. The boundary probability maps produced by the boundary decoder at each scale are used as attention to enhance the segmentation feature maps. We evaluated the BA-Net on the Abdominal Multi-Organ Segmentation (AMOS) Challenge dataset and achieved an average Dice score of 89.29$\%$ for multi-organ segmentation on CT scans and an average Dice score of 71.92$\%$ on MRI scans. The results demonstrate that BA-Net is superior to nnUNet on both segmentation tasks.



### Artificial Neural Networks for Finger Vein Recognition: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2208.13341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13341v1)
- **Published**: 2022-08-29 02:29:07+00:00
- **Updated**: 2022-08-29 02:29:07+00:00
- **Authors**: Yimin Yin, Renye Zhang, Pengfei Liu, Wanxia Deng, Siliang He, Chen Li, Jinghua Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Finger vein recognition is an emerging biometric recognition technology. Different from the other biometric features on the body surface, the venous vascular tissue of the fingers is buried deep inside the skin. Due to this advantage, finger vein recognition is highly stable and private. They are almost impossible to be stolen and difficult to interfere with by external conditions. Unlike the finger vein recognition methods based on traditional machine learning, the artificial neural network technique, especially deep learning, it without relying on feature engineering and have superior performance. To summarize the development of finger vein recognition based on artificial neural networks, this paper collects 149 related papers. First, we introduce the background of finger vein recognition and the motivation of this survey. Then, the development history of artificial neural networks and the representative networks on finger vein recognition tasks are introduced. The public datasets that are widely used in finger vein recognition are then described. After that, we summarize the related finger vein recognition tasks based on classical neural networks and deep neural networks, respectively. Finally, the challenges and potential development directions in finger vein recognition are discussed. To our best knowledge, this paper is the first comprehensive survey focusing on finger vein recognition based on artificial neural networks.



### Long-Tailed Classification of Thorax Diseases on Chest X-Ray: A New Benchmark Study
- **Arxiv ID**: http://arxiv.org/abs/2208.13365v1
- **DOI**: 10.1007/978-3-031-17027-0_3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13365v1)
- **Published**: 2022-08-29 04:34:15+00:00
- **Updated**: 2022-08-29 04:34:15+00:00
- **Authors**: Gregory Holste, Song Wang, Ziyu Jiang, Thomas C. Shen, George Shih, Ronald M. Summers, Yifan Peng, Zhangyang Wang
- **Comment**: DALI 2022 (MICCAI workshop)
- **Journal**: None
- **Summary**: Imaging exams, such as chest radiography, will yield a small set of common findings and a much larger set of uncommon findings. While a trained radiologist can learn the visual presentation of rare conditions by studying a few representative examples, teaching a machine to learn from such a "long-tailed" distribution is much more difficult, as standard methods would be easily biased toward the most frequent classes. In this paper, we present a comprehensive benchmark study of the long-tailed learning problem in the specific domain of thorax diseases on chest X-rays. We focus on learning from naturally distributed chest X-ray data, optimizing classification accuracy over not only the common "head" classes, but also the rare yet critical "tail" classes. To accomplish this, we introduce a challenging new long-tailed chest X-ray benchmark to facilitate research on developing long-tailed learning methods for medical image classification. The benchmark consists of two chest X-ray datasets for 19- and 20-way thorax disease classification, containing classes with as many as 53,000 and as few as 7 labeled training images. We evaluate both standard and state-of-the-art long-tailed learning methods on this new benchmark, analyzing which aspects of these methods are most beneficial for long-tailed medical image classification and summarizing insights for future algorithm design. The datasets, trained models, and code are available at https://github.com/VITA-Group/LongTailCXR.



### Confidence Estimation for Object Detection in Document Images
- **Arxiv ID**: http://arxiv.org/abs/2208.13391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13391v1)
- **Published**: 2022-08-29 06:47:18+00:00
- **Updated**: 2022-08-29 06:47:18+00:00
- **Authors**: Mélodie Boillet, Christopher Kermorvant, Thierry Paquet
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are becoming increasingly powerful and large and always require more labelled data to be trained. However, since annotating data is time-consuming, it is now necessary to develop systems that show good performance while learning on a limited amount of data. These data must be correctly chosen to obtain models that are still efficient. For this, the systems must be able to determine which data should be annotated to achieve the best results.   In this paper, we propose four estimators to estimate the confidence of object detection predictions. The first two are based on Monte Carlo dropout, the third one on descriptive statistics and the last one on the detector posterior probabilities. In the active learning framework, the three first estimators show a significant improvement in performance for the detection of document physical pages and text lines compared to a random selection of images. We also show that the proposed estimator based on descriptive statistics can replace MC dropout, reducing the computational cost without compromising the performances.



### Towards Explaining Demographic Bias through the Eyes of Face Recognition Models
- **Arxiv ID**: http://arxiv.org/abs/2208.13400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13400v1)
- **Published**: 2022-08-29 07:23:06+00:00
- **Updated**: 2022-08-29 07:23:06+00:00
- **Authors**: Biying Fu, Naser Damer
- **Comment**: Accepted at at the 2022 International Joint Conference on Biometrics
  (IJCB 2022)
- **Journal**: None
- **Summary**: Biases inherent in both data and algorithms make the fairness of widespread machine learning (ML)-based decision-making systems less than optimal. To improve the trustfulness of such ML decision systems, it is crucial to be aware of the inherent biases in these solutions and to make them more transparent to the public and developers. In this work, we aim at providing a set of explainability tool that analyse the difference in the face recognition models' behaviors when processing different demographic groups. We do that by leveraging higher-order statistical information based on activation maps to build explainability tools that link the FR models' behavior differences to certain facial regions. The experimental results on two datasets and two face recognition models pointed out certain areas of the face where the FR models react differently for certain demographic groups compared to reference groups. The outcome of these analyses interestingly aligns well with the results of studies that analyzed the anthropometric differences and the human judgment differences on the faces of different demographic groups. This is thus the first study that specifically tries to explain the biased behavior of FR models on different demographic groups and link it directly to the spatial facial features. The code is publicly available here.



### Progressive Self-Distillation for Ground-to-Aerial Perception Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2208.13404v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13404v3)
- **Published**: 2022-08-29 07:30:35+00:00
- **Updated**: 2023-04-16 06:31:23+00:00
- **Authors**: Junjie Hu, Chenyou Fan, Mete Ozay, Hua Feng, Yuan Gao, Tin Lun Lam
- **Comment**: None
- **Journal**: None
- **Summary**: We study a practical yet hasn't been explored problem: how a drone can perceive in an environment from different flight heights. Unlike autonomous driving, where the perception is always conducted from a ground viewpoint, a flying drone may flexibly change its flight height due to specific tasks, requiring the capability for viewpoint invariant perception. Tackling the such problem with supervised learning will incur tremendous costs for data annotation of different flying heights. On the other hand, current semi-supervised learning methods are not effective under viewpoint differences. In this paper, we introduce the ground-to-aerial perception knowledge transfer and propose a progressive semi-supervised learning framework that enables drone perception using only labeled data of ground viewpoint and unlabeled data of flying viewpoints. Our framework has four core components: i) a dense viewpoint sampling strategy that splits the range of vertical flight height into a set of small pieces with evenly-distributed intervals, ii) nearest neighbor pseudo-labeling that infers labels of the nearest neighbor viewpoint with a model learned on the preceding viewpoint, iii) MixView that generates augmented images among different viewpoints to alleviate viewpoint differences, and iv) a progressive distillation strategy to gradually learn until reaching the maximum flying height. We collect a synthesized and a real-world dataset, and we perform extensive experimental analyses to show that our method yields 22.2% and 16.9% accuracy improvement for the synthesized dataset and the real world. Code and datasets are available on https://github.com/FreeformRobotics/Progressive-Self-Distillation-for-Ground-to-Aerial-Perception-Knowledge-Transfer.



### PV-RCNN++: Semantical Point-Voxel Feature Interaction for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.13414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13414v1)
- **Published**: 2022-08-29 08:14:00+00:00
- **Updated**: 2022-08-29 08:14:00+00:00
- **Authors**: Peng Wu, Lipeng Gu, Xuefeng Yan, Haoran Xie, Fu Lee Wang, Gary Cheng, Mingqiang Wei
- **Comment**: 18 pages, 9 figures
- **Journal**: None
- **Summary**: Large imbalance often exists between the foreground points (i.e., objects) and the background points in outdoor LiDAR point clouds. It hinders cutting-edge detectors from focusing on informative areas to produce accurate 3D object detection results. This paper proposes a novel object detection network by semantical point-voxel feature interaction, dubbed PV-RCNN++. Unlike most of existing methods, PV-RCNN++ explores the semantic information to enhance the quality of object detection. First, a semantic segmentation module is proposed to retain more discriminative foreground keypoints. Such a module will guide our PV-RCNN++ to integrate more object-related point-wise and voxel-wise features in the pivotal areas. Then, to make points and voxels interact efficiently, we utilize voxel query based on Manhattan distance to quickly sample voxel-wise features around keypoints. Such the voxel query will reduce the time complexity from O(N) to O(K), compared to the ball query. Further, to avoid being stuck in learning only local features, an attention-based residual PointNet module is designed to expand the receptive field to adaptively aggregate the neighboring voxel-wise features into keypoints. Extensive experiments on the KITTI dataset show that PV-RCNN++ achieves 81.60$\%$, 40.18$\%$, 68.21$\%$ 3D mAP on Car, Pedestrian, and Cyclist, achieving comparable or even better performance to the state-of-the-arts.



### Light-YOLOv5: A Lightweight Algorithm for Improved YOLOv5 in Complex Fire Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2208.13422v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13422v3)
- **Published**: 2022-08-29 08:36:04+00:00
- **Updated**: 2022-12-01 16:24:31+00:00
- **Authors**: Hao Xu, Bo Li, Fei Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: Fire-detection technology is of great importance for successful fire-prevention measures. Image-based fire detection is one effective method. At present, object-detection algorithms are deficient in performing detection speed and accuracy tasks when they are applied in complex fire scenarios. In this study, a lightweight fire-detection algorithm, Light-YOLOv5 (You Only Look Once version five), is presented. First, a separable vision transformer (SepViT) block is used to replace several C3 modules in the final layer of a backbone network to enhance both the contact of the backbone network to global in-formation and the extraction of flame and smoke features; second, a light bidirectional feature pyramid network (Light-BiFPN) is designed to lighten the model while improving the feature extraction and balancing speed and accuracy features during a fire-detection procedure; third, a global attention mechanism (GAM) is fused into the network to cause the model to focus more on the global dimensional features and further improve the detection accuracy of the model; and finally, the Mish activation function and SIoU loss are utilized to simultaneously increase the convergence speed and enhance the accuracy. The experimental results show that compared to the original algorithm, the mean average accuracy (mAP) of Light-YOLOv5 increases by 3.3%, the number of parameters decreases by 27.1%, and the floating point operations (FLOPs) decrease by 19.1%. The detection speed reaches 91.1 FPS, which can detect targets in complex fire scenarios in real time.



### Towards In-distribution Compatibility in Out-of-distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.13433v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.13433v1)
- **Published**: 2022-08-29 09:06:15+00:00
- **Updated**: 2022-08-29 09:06:15+00:00
- **Authors**: Boxi Wu, Jie Jiang, Haidong Ren, Zifan Du, Wenxiao Wang, Zhifeng Li, Deng Cai, Xiaofei He, Binbin Lin, Wei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural network, despite its remarkable capability of discriminating targeted in-distribution samples, shows poor performance on detecting anomalous out-of-distribution data. To address this defect, state-of-the-art solutions choose to train deep networks on an auxiliary dataset of outliers. Various training criteria for these auxiliary outliers are proposed based on heuristic intuitions. However, we find that these intuitively designed outlier training criteria can hurt in-distribution learning and eventually lead to inferior performance. To this end, we identify three causes of the in-distribution incompatibility: contradictory gradient, false likelihood, and distribution shift. Based on our new understandings, we propose a new out-of-distribution detection method by adapting both the top-design of deep models and the loss function. Our method achieves in-distribution compatibility by pursuing less interference with the probabilistic characteristic of in-distribution features. On several benchmarks, our method not only achieves the state-of-the-art out-of-distribution detection performance but also improves the in-distribution accuracy.



### Joint Learning Content and Degradation Aware Feature for Blind Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2208.13436v1
- **DOI**: 10.1145/3503161.3547907
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13436v1)
- **Published**: 2022-08-29 09:12:39+00:00
- **Updated**: 2022-08-29 09:12:39+00:00
- **Authors**: Yifeng Zhou, Chuming Lin, Donghao Luo, Yong Liu, Ying Tai, Chengjie Wang, Mingang Chen
- **Comment**: None
- **Journal**: None
- **Summary**: To achieve promising results on blind image super-resolution (SR), some attempts leveraged the low resolution (LR) images to predict the kernel and improve the SR performance. However, these Supervised Kernel Prediction (SKP) methods are impractical due to the unavailable real-world blur kernels. Although some Unsupervised Degradation Prediction (UDP) methods are proposed to bypass this problem, the \textit{inconsistency} between degradation embedding and SR feature is still challenging. By exploring the correlations between degradation embedding and SR feature, we observe that jointly learning the content and degradation aware feature is optimal. Based on this observation, a Content and Degradation aware SR Network dubbed CDSR is proposed. Specifically, CDSR contains three newly-established modules: (1) a Lightweight Patch-based Encoder (LPE) is applied to jointly extract content and degradation features; (2) a Domain Query Attention based module (DQA) is employed to adaptively reduce the inconsistency; (3) a Codebook-based Space Compress module (CSC) that can suppress the redundant information. Extensive experiments on several benchmarks demonstrate that the proposed CDSR outperforms the existing UDP models and achieves competitive performance on PSNR and SSIM even compared with the state-of-the-art SKP methods.



### Rethinking Skip Connections in Encoder-decoder Networks for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.13441v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.13441v1)
- **Published**: 2022-08-29 09:20:53+00:00
- **Updated**: 2022-08-29 09:20:53+00:00
- **Authors**: Zhitong Lai, Haichao Sun, Rui Tian, Nannan Ding, Zhiguo Wu, Yanjie Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Skip connections are fundamental units in encoder-decoder networks, which are able to improve the feature propagtion of the neural networks. However, most methods with skip connections just connected features with the same resolution in the encoder and the decoder, which ignored the information loss in the encoder with the layers going deeper. To leverage the information loss of the features in shallower layers of the encoder, we propose a full skip connection network (FSCN) for monocular depth estimation task. In addition, to fuse features within skip connections more closely, we present an adaptive concatenation module (ACM). Further more, we conduct extensive experiments on the ourdoor and indoor datasets (i.e., the KITTI dataste and the NYU Depth V2 dataset) for FSCN and FSCN gets the state-of-the-art results.



### Federated Zero-Shot Learning with Mid-Level Semantic Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2208.13465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13465v1)
- **Published**: 2022-08-29 10:05:49+00:00
- **Updated**: 2022-08-29 10:05:49+00:00
- **Authors**: Shitong Sun, Chenyang Si, Shaogang Gong, Guile Wu
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Conventional centralised deep learning paradigms are not feasible when data from different sources cannot be shared due to data privacy or transmission limitation. To resolve this problem, federated learning has been introduced to transfer knowledge across multiple sources (clients) with non-shared data while optimising a globally generalised central model (server). Existing federated learning paradigms mostly focus on transferring holistic high-level knowledge (such as class) across models, which are closely related to specific objects of interest so may suffer from inverse attack. In contrast, in this work, we consider transferring mid-level semantic knowledge (such as attribute) which is not sensitive to specific objects of interest and therefore is more privacy-preserving and scalable. To this end, we formulate a new Federated Zero-Shot Learning (FZSL) paradigm to learn mid-level semantic knowledge at multiple local clients with non-shared local data and cumulatively aggregate a globally generalised central model for deployment. To improve model discriminative ability, we propose to explore semantic knowledge augmentation from external knowledge for enriching the mid-level semantic space in FZSL. Extensive experiments on five zeroshot learning benchmark datasets validate the effectiveness of our approach for optimising a generalisable federated learning model with mid-level semantic knowledge transfer.



### Prompt Tuning with Soft Context Sharing for Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2208.13474v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.13474v1)
- **Published**: 2022-08-29 10:19:10+00:00
- **Updated**: 2022-08-29 10:19:10+00:00
- **Authors**: Kun Ding, Ying Wang, Pengzhang Liu, Qiang Yu, Haojian Zhang, Shiming Xiang, Chunhong Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-language models have recently shown great potential on many computer vision tasks. Meanwhile, prior work demonstrates prompt tuning designed for vision-language models could acquire superior performance on few-shot image recognition compared to linear probe, a strong baseline. In real-world applications, many few-shot tasks are correlated, particularly in a specialized area. However, such information is ignored by previous work. Inspired by the fact that modeling task relationships by multi-task learning can usually boost performance, we propose a novel method SoftCPT (Soft Context Sharing for Prompt Tuning) to fine-tune pre-trained vision-language models on multiple target few-shot tasks, simultaneously. Specifically, we design a task-shared meta network to generate prompt vector for each task using pre-defined task name together with a learnable meta prompt as input. As such, the prompt vectors of all tasks will be shared in a soft manner. The parameters of this shared meta network as well as the meta prompt vector are tuned on the joint training set of all target tasks. Extensive experiments on three multi-task few-shot datasets show that SoftCPT outperforms the representative single-task prompt tuning method CoOp [78] by a large margin, implying the effectiveness of multi-task learning in vision-language prompt tuning. The source code and data will be made publicly available.



### A Practical Calibration Method for RGB Micro-Grid Polarimetric Cameras
- **Arxiv ID**: http://arxiv.org/abs/2208.13485v1
- **DOI**: 10.1109/LRA.2022.3192655
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13485v1)
- **Published**: 2022-08-29 10:39:23+00:00
- **Updated**: 2022-08-29 10:39:23+00:00
- **Authors**: Joaquin Rodriguez, Lew Lew-Yan-Voon, Renato Martins, Olivier Morel
- **Comment**: This is a preprint version of the paper to appear at IEEE Robotics
  and Automation Letters (RAL). The final journal version will be available at
  https://doi.org/10.1109/LRA.2022.3192655
- **Journal**: Robotics and Automation Letters - Volume: 7 - Issue: 4 - October
  2022 - Pages: 9921 - 9928
- **Summary**: Polarimetric imaging has been applied in a growing number of applications in robotic vision (ex. underwater navigation, glare removal, de-hazing, object classification, and depth estimation). One can find on the market RGB Polarization cameras that can capture both color and polarimetric state of the light in a single snapshot. Due to the sensor's characteristic dispersion, and the use of lenses, it is crucial to calibrate these types of cameras so as to obtain correct polarization measurements. The calibration methods that have been developed so far are either not adapted to this type of cameras, or they require complex equipment and time consuming experiments in strict setups. In this paper, we propose a new method to overcome the need for complex optical systems to efficiently calibrate these cameras. We show that the proposed calibration method has several advantages such as that any user can easily calibrate the camera using a uniform, linearly polarized light source without any a priori knowledge of its polarization state, and with a limited number of acquisitions. We will make our calibration code publicly available.



### Unsupervised Semantic Analysis of a Region from Satellite Image Time Series
- **Arxiv ID**: http://arxiv.org/abs/2208.13504v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.13504v2)
- **Published**: 2022-08-29 11:19:36+00:00
- **Updated**: 2022-11-23 15:13:24+00:00
- **Authors**: Carlos Echegoyen, Aritz Pérez, Guzmán Santafé, Unai Pérez-Goya, María Dolores Ugarte
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal sequences of satellite images constitute a highly valuable and abundant resource to analyze a given region. However, the labeled data needed to train most machine learning models are scarce and difficult to obtain. In this context, the current work investigates a fully unsupervised methodology that, given a sequence of images, learns a semantic embedding and then, creates a partition of the ground according to its semantic properties and its evolution over time. We illustrate the methodology by conducting the semantic analysis of a sequence of satellite images of a region of Navarre (Spain). The proposed approach reveals a novel broad perspective of the land, where potentially large areas that share both a similar semantic and a similar temporal evolution are connected in a compact and well-structured manner. The results also show a close relationship between the allocation of the clusters in the geographic space and their allocation in the embedded spaces. The semantic analysis is completed by obtaining the representative sequence of tiles corresponding to each cluster, the linear interpolation between related areas, and a graph that shows the relationships between the clusters, providing a concise semantic summary of the whole region.



### LogicRank: Logic Induced Reranking for Generative Text-to-Image Systems
- **Arxiv ID**: http://arxiv.org/abs/2208.13518v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LO, cs.SC
- **Links**: [PDF](http://arxiv.org/pdf/2208.13518v1)
- **Published**: 2022-08-29 11:40:36+00:00
- **Updated**: 2022-08-29 11:40:36+00:00
- **Authors**: Björn Deiseroth, Patrick Schramowski, Hikaru Shindo, Devendra Singh Dhami, Kristian Kersting
- **Comment**: None
- **Journal**: None
- **Summary**: Text-to-image models have recently achieved remarkable success with seemingly accurate samples in photo-realistic quality. However as state-of-the-art language models still struggle evaluating precise statements consistently, so do language model based image generation processes. In this work we showcase problems of state-of-the-art text-to-image models like DALL-E with generating accurate samples from statements related to the draw bench benchmark. Furthermore we show that CLIP is not able to rerank those generated samples consistently. To this end we propose LogicRank, a neuro-symbolic reasoning framework that can result in a more accurate ranking-system for such precision-demanding settings. LogicRank integrates smoothly into the generation process of text-to-image models and moreover can be used to further fine-tune towards a more logical precise model.



### CIRCLe: Color Invariant Representation Learning for Unbiased Classification of Skin Lesions
- **Arxiv ID**: http://arxiv.org/abs/2208.13528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13528v1)
- **Published**: 2022-08-29 12:06:10+00:00
- **Updated**: 2022-08-29 12:06:10+00:00
- **Authors**: Arezou Pakzad, Kumar Abhishek, Ghassan Hamarneh
- **Comment**: 18 pages, 5 figures, accepted by European Conference on Computer
  Vision (ECCV) ISIC Skin Image Analysis Workshop, 2022
- **Journal**: None
- **Summary**: While deep learning based approaches have demonstrated expert-level performance in dermatological diagnosis tasks, they have also been shown to exhibit biases toward certain demographic attributes, particularly skin types (e.g., light versus dark), a fairness concern that must be addressed. We propose CIRCLe, a skin color invariant deep representation learning method for improving fairness in skin lesion classification. CIRCLe is trained to classify images by utilizing a regularization loss that encourages images with the same diagnosis but different skin types to have similar latent representations. Through extensive evaluation and ablation studies, we demonstrate CIRCLe's superior performance over the state-of-the-art when evaluated on 16k+ images spanning 6 Fitzpatrick skin types and 114 diseases, using classification accuracy, equal opportunity difference (for light versus dark groups), and normalized accuracy range, a new measure we propose to assess fairness on multiple skin type groups.



### Explainability of Deep Learning models for Urban Space perception
- **Arxiv ID**: http://arxiv.org/abs/2208.13555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13555v1)
- **Published**: 2022-08-29 12:44:48+00:00
- **Updated**: 2022-08-29 12:44:48+00:00
- **Authors**: Ruben Sangers, Jan van Gemert, Sander van Cranenburgh
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Deep learning based computer vision models are increasingly used by urban planners to support decision making for shaping urban environments. Such models predict how people perceive the urban environment quality in terms of e.g. its safety or beauty. However, the blackbox nature of deep learning models hampers urban planners to understand what landscape objects contribute to a particularly high quality or low quality urban space perception. This study investigates how computer vision models can be used to extract relevant policy information about peoples' perception of the urban space. To do so, we train two widely used computer vision architectures; a Convolutional Neural Network and a transformer, and apply GradCAM -- a well-known ex-post explainable AI technique -- to highlight the image regions important for the model's prediction. Using these GradCAM visualizations, we manually annotate the objects relevant to the models' perception predictions. As a result, we are able to discover new objects that are not represented in present object detection models used for annotation in previous studies. Moreover, our methodological results suggest that transformer architectures are better suited to be used in combination with GradCAM techniques. Code is available on Github.



### Chosen methods of improving small object recognition with weak recognizable features
- **Arxiv ID**: http://arxiv.org/abs/2208.13591v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.13591v2)
- **Published**: 2022-08-29 13:39:02+00:00
- **Updated**: 2022-09-02 08:20:07+00:00
- **Authors**: Magdalena Stachoń, Marcin Pietroń
- **Comment**: None
- **Journal**: None
- **Summary**: Many object detection models struggle with several problematic aspects of small object detection including the low number of samples, lack of diversity and low features representation. Taking into account that GANs belong to generative models class, their initial objective is to learn to mimic any data distribution. Using the proper GAN model would enable augmenting low precision data increasing their amount and diversity. This solution could potentially result in improved object detection results. Additionally, incorporating GAN-based architecture inside deep learning model can increase accuracy of small objects recognition. In this work the GAN-based method with augmentation is presented to improve small object detection on VOC Pascal dataset. The method is compared with different popular augmentation strategies like object rotations, shifts etc. The experiments are based on FasterRCNN model.



### Towards Robust Face Recognition with Comprehensive Search
- **Arxiv ID**: http://arxiv.org/abs/2208.13600v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13600v2)
- **Published**: 2022-08-29 13:50:02+00:00
- **Updated**: 2022-09-12 13:03:26+00:00
- **Authors**: Manyuan Zhang, Guanglu Song, Yu Liu, Hongsheng Li
- **Comment**: Accepted in ECCV 2022
- **Journal**: None
- **Summary**: Data cleaning, architecture, and loss function design are important factors contributing to high-performance face recognition. Previously, the research community tries to improve the performance of each single aspect but failed to present a unified solution on the joint search of the optimal designs for all three aspects. In this paper, we for the first time identify that these aspects are tightly coupled to each other. Optimizing the design of each aspect actually greatly limits the performance and biases the algorithmic design. Specifically, we find that the optimal model architecture or loss function is closely coupled with the data cleaning. To eliminate the bias of single-aspect research and provide an overall understanding of the face recognition model design, we first carefully design the search space for each aspect, then a comprehensive search method is introduced to jointly search optimal data cleaning, architecture, and loss function design. In our framework, we make the proposed comprehensive search as flexible as possible, by using an innovative reinforcement learning based approach. Extensive experiments on million-level face recognition benchmarks demonstrate the effectiveness of our newly-designed search space for each aspect and the comprehensive search. We outperform expert algorithms developed for each single research track by large margins. More importantly, we analyze the difference between our searched optimal design and the independent design of the single factors. We point out that strong models tend to optimize with more difficult training datasets and loss functions. Our empirical study can provide guidance in future research towards more robust face recognition systems.



### Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment
- **Arxiv ID**: http://arxiv.org/abs/2208.13628v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.13628v2)
- **Published**: 2022-08-29 14:24:08+00:00
- **Updated**: 2022-10-05 11:35:46+00:00
- **Authors**: Mustafa Shukor, Guillaume Couairon, Matthieu Cord
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Vision and Language Pretraining has become the prevalent approach for tackling multimodal downstream tasks. The current trend is to move towards ever larger models and pretraining datasets. This computational headlong rush does not seem reasonable in the long term to move toward sustainable solutions, and de facto excludes academic laboratories with limited resources. In this work, we propose a new framework, dubbed ViCHA, that efficiently exploits the input data to boost the learning by: (a) a new hierarchical cross-modal alignment loss, (b) new self-supervised scheme based on masked image modeling, (c) leveraging image-level annotations, called Visual Concepts, obtained with existing foundation models such as CLIP to boost the performance of the image encoder. Although pretrained on four times less data, our ViCHA strategy outperforms other approaches on several downstream tasks such as Image-Text Retrieval, VQA, Visual Reasoning, Visual Entailment and Visual Grounding. The code will be made publicly available here: https://github.com/mshukor/ViCHA



### Dynamic Data-Free Knowledge Distillation by Easy-to-Hard Learning Strategy
- **Arxiv ID**: http://arxiv.org/abs/2208.13648v3
- **DOI**: 10.1016/j.ins.2023.119202
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13648v3)
- **Published**: 2022-08-29 14:51:57+00:00
- **Updated**: 2023-07-04 01:57:51+00:00
- **Authors**: Jingru Li, Sheng Zhou, Liangcheng Li, Haishuai Wang, Zhi Yu, Jiajun Bu
- **Comment**: Accepted by Information Sciences, Proof version provided
- **Journal**: None
- **Summary**: Data-free knowledge distillation (DFKD) is a widely-used strategy for Knowledge Distillation (KD) whose training data is not available. It trains a lightweight student model with the aid of a large pretrained teacher model without any access to training data. However, existing DFKD methods suffer from inadequate and unstable training process, as they do not adjust the generation target dynamically based on the status of the student model during learning. To address this limitation, we propose a novel DFKD method called CuDFKD. It teaches students by a dynamic strategy that gradually generates easy-to-hard pseudo samples, mirroring how humans learn. Besides, CuDFKD adapts the generation target dynamically according to the status of student model. Moreover, We provide a theoretical analysis of the majorization minimization (MM) algorithm and explain the convergence of CuDFKD. To measure the robustness and fidelity of DFKD methods, we propose two more metrics, and experiments shows CuDFKD has comparable performance to state-of-the-art (SOTA) DFKD methods on all datasets. Experiments also present that our CuDFKD has the fastest convergence and best robustness over other SOTA DFKD methods.



### Learning Binary and Sparse Permutation-Invariant Representations for Fast and Memory Efficient Whole Slide Image Search
- **Arxiv ID**: http://arxiv.org/abs/2208.13653v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13653v2)
- **Published**: 2022-08-29 14:56:36+00:00
- **Updated**: 2022-09-23 13:46:35+00:00
- **Authors**: Sobhan Hemati, Shivam Kalra, Morteza Babaie, H. R. Tizhoosh
- **Comment**: None
- **Journal**: None
- **Summary**: Learning suitable Whole slide images (WSIs) representations for efficient retrieval systems is a non-trivial task. The WSI embeddings obtained from current methods are in Euclidean space not ideal for efficient WSI retrieval. Furthermore, most of the current methods require high GPU memory due to the simultaneous processing of multiple sets of patches. To address these challenges, we propose a novel framework for learning binary and sparse WSI representations utilizing a deep generative modelling and the Fisher Vector. We introduce new loss functions for learning sparse and binary permutation-invariant WSI representations that employ instance-based training achieving better memory efficiency. The learned WSI representations are validated on The Cancer Genomic Atlas (TCGA) and Liver-Kidney-Stomach (LKS) datasets. The proposed method outperforms Yottixel (a recent search engine for histopathology images) both in terms of retrieval accuracy and speed. Further, we achieve competitive performance against SOTA on the public benchmark LKS dataset for WSI classification.



### Latent Heterogeneous Graph Network for Incomplete Multi-View Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.13669v1
- **DOI**: 10.1109/TMM.2022.3154592
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.13669v1)
- **Published**: 2022-08-29 15:14:21+00:00
- **Updated**: 2022-08-29 15:14:21+00:00
- **Authors**: Pengfei Zhu, Xinjie Yao, Yu Wang, Meng Cao, Binyuan Hui, Shuai Zhao, Qinghua Hu
- **Comment**: 13 pages, 9 figures, IEEE Transactions on Multimedia
- **Journal**: IEEE Transactions on Multimedia, early access, February 2022
- **Summary**: Multi-view learning has progressed rapidly in recent years. Although many previous studies assume that each instance appears in all views, it is common in real-world applications for instances to be missing from some views, resulting in incomplete multi-view data. To tackle this problem, we propose a novel Latent Heterogeneous Graph Network (LHGN) for incomplete multi-view learning, which aims to use multiple incomplete views as fully as possible in a flexible manner. By learning a unified latent representation, a trade-off between consistency and complementarity among different views is implicitly realized. To explore the complex relationship between samples and latent representations, a neighborhood constraint and a view-existence constraint are proposed, for the first time, to construct a heterogeneous graph. Finally, to avoid any inconsistencies between training and test phase, a transductive learning technique is applied based on graph learning for classification tasks. Extensive experimental results on real-world datasets demonstrate the effectiveness of our model over existing state-of-the-art approaches.



### Comprehensive study of good model training for prostate segmentation in volumetric MRI
- **Arxiv ID**: http://arxiv.org/abs/2208.13671v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T07, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2208.13671v1)
- **Published**: 2022-08-29 15:14:48+00:00
- **Updated**: 2022-08-29 15:14:48+00:00
- **Authors**: Carlos Nácher Collado
- **Comment**: None
- **Journal**: None
- **Summary**: Prostate cancer was the third most common cancer in 2020 internationally, coming after breast cancer and lung cancer. Furthermore, in recent years prostate cancer has shown an increasing trend. According to clinical experience, if this problem is detected and treated early, there can be a high chance of survival for the patient. One task that helps diagnose prostate cancer is prostate segmentation from magnetic resonance imaging. Manual segmentation performed by clinical experts has its drawbacks such as: the high time and concentration required from observers; and inter- and intra-observer variability. This is why in recent years automatic approaches to segment a prostate based on convolutional neural networks have emerged. Many of them have novel proposed architectures. In this paper I make an exhaustive study of several deep learning models by adjusting them to the task of prostate prediction. I do not use novel architectures, but focus my work more on how to train the networks. My approach is based on a ResNext101 3D encoder and a Unet3D decoder. I provide a study of the importance of resolutions in resampling data, something that no one else has done before.



### Volume Rendering Digest (for NeRF)
- **Arxiv ID**: http://arxiv.org/abs/2209.02417v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, I.3; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2209.02417v1)
- **Published**: 2022-08-29 15:46:14+00:00
- **Updated**: 2022-08-29 15:46:14+00:00
- **Authors**: Andrea Tagliasacchi, Ben Mildenhall
- **Comment**: Overleaf: https://www.overleaf.com/read/fkhpkzxhnyws
- **Journal**: None
- **Summary**: Neural Radiance Fields employ simple volume rendering as a way to overcome the challenges of differentiating through ray-triangle intersections by leveraging a probabilistic notion of visibility. This is achieved by assuming the scene is composed by a cloud of light-emitting particles whose density changes in space. This technical report summarizes the derivations for differentiable volume rendering. It is a condensed version of previous reports, but rewritten in the context of NeRF, and adopting its commonly used notation.



### Deformable Image Registration using Unsupervised Deep Learning for CBCT-guided Abdominal Radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/2208.13686v1
- **DOI**: 10.1088/1361-6560/acc721
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2208.13686v1)
- **Published**: 2022-08-29 15:48:50+00:00
- **Updated**: 2022-08-29 15:48:50+00:00
- **Authors**: Huiqiao Xie, Yang Lei, Yabo Fu, Tonghe Wang, Justin Roper, Jeffrey D. Bradley, Pretesh Patel, Tian Liu, Xiaofeng Yang
- **Comment**: None
- **Journal**: None
- **Summary**: CBCTs in image-guided radiotherapy provide crucial anatomy information for patient setup and plan evaluation. Longitudinal CBCT image registration could quantify the inter-fractional anatomic changes. The purpose of this study is to propose an unsupervised deep learning based CBCT-CBCT deformable image registration. The proposed deformable registration workflow consists of training and inference stages that share the same feed-forward path through a spatial transformation-based network (STN). The STN consists of a global generative adversarial network (GlobalGAN) and a local GAN (LocalGAN) to predict the coarse- and fine-scale motions, respectively. The network was trained by minimizing the image similarity loss and the deformable vector field (DVF) regularization loss without the supervision of ground truth DVFs. During the inference stage, patches of local DVF were predicted by the trained LocalGAN and fused to form a whole-image DVF. The local whole-image DVF was subsequently combined with the GlobalGAN generated DVF to obtain final DVF. The proposed method was evaluated using 100 fractional CBCTs from 20 abdominal cancer patients in the experiments and 105 fractional CBCTs from a cohort of 21 different abdominal cancer patients in a holdout test. Qualitatively, the registration results show great alignment between the deformed CBCT images and the target CBCT image. Quantitatively, the average target registration error (TRE) calculated on the fiducial markers and manually identified landmarks was 1.91+-1.11 mm. The average mean absolute error (MAE), normalized cross correlation (NCC) between the deformed CBCT and target CBCT were 33.42+-7.48 HU, 0.94+-0.04, respectively. This promising registration method could provide fast and accurate longitudinal CBCT alignment to facilitate inter-fractional anatomic changes analysis and prediction.



### SphereDepth: Panorama Depth Estimation from Spherical Domain
- **Arxiv ID**: http://arxiv.org/abs/2208.13714v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13714v3)
- **Published**: 2022-08-29 16:50:19+00:00
- **Updated**: 2022-12-04 16:51:00+00:00
- **Authors**: Qingsong Yan, Qiang Wang, Kaiyong Zhao, Bo Li, Xiaowen Chu, Fei Deng
- **Comment**: Conference accept at 3DV 2022
- **Journal**: None
- **Summary**: The panorama image can simultaneously demonstrate complete information of the surrounding environment and has many advantages in virtual tourism, games, robotics, etc. However, the progress of panorama depth estimation cannot completely solve the problems of distortion and discontinuity caused by the commonly used projection methods. This paper proposes SphereDepth, a novel panorama depth estimation method that predicts the depth directly on the spherical mesh without projection preprocessing. The core idea is to establish the relationship between the panorama image and the spherical mesh and then use a deep neural network to extract features on the spherical domain to predict depth. To address the efficiency challenges brought by the high-resolution panorama data, we introduce two hyper-parameters for the proposed spherical mesh processing framework to balance the inference speed and accuracy. Validated on three public panorama datasets, SphereDepth achieves comparable results with the state-of-the-art methods of panorama depth estimation. Benefiting from the spherical domain setting, SphereDepth can generate a high-quality point cloud and significantly alleviate the issues of distortion and discontinuity.



### StableFace: Analyzing and Improving Motion Stability for Talking Face Generation
- **Arxiv ID**: http://arxiv.org/abs/2208.13717v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2208.13717v1)
- **Published**: 2022-08-29 16:56:35+00:00
- **Updated**: 2022-08-29 16:56:35+00:00
- **Authors**: Jun Ling, Xu Tan, Liyang Chen, Runnan Li, Yuchao Zhang, Sheng Zhao, Li Song
- **Comment**: 12 pages,
- **Journal**: None
- **Summary**: While previous speech-driven talking face generation methods have made significant progress in improving the visual quality and lip-sync quality of the synthesized videos, they pay less attention to lip motion jitters which greatly undermine the realness of talking face videos. What causes motion jitters, and how to mitigate the problem? In this paper, we conduct systematic analyses on the motion jittering problem based on a state-of-the-art pipeline that uses 3D face representations to bridge the input audio and output video, and improve the motion stability with a series of effective designs. We find that several issues can lead to jitters in synthesized talking face video: 1) jitters from the input 3D face representations; 2) training-inference mismatch; 3) lack of dependency modeling among video frames. Accordingly, we propose three effective solutions to address this issue: 1) we propose a gaussian-based adaptive smoothing module to smooth the 3D face representations to eliminate jitters in the input; 2) we add augmented erosions on the input data of the neural renderer in training to simulate the distortion in inference to reduce mismatch; 3) we develop an audio-fused transformer generator to model dependency among video frames. Besides, considering there is no off-the-shelf metric for measuring motion jitters in talking face video, we devise an objective metric (Motion Stability Index, MSI), to quantitatively measure the motion jitters by calculating the reciprocal of variance acceleration. Extensive experimental results show the superiority of our method on motion-stable face video generation, with better quality than previous systems.



### CounTR: Transformer-based Generalised Visual Counting
- **Arxiv ID**: http://arxiv.org/abs/2208.13721v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13721v3)
- **Published**: 2022-08-29 17:02:45+00:00
- **Updated**: 2023-06-02 07:51:22+00:00
- **Authors**: Chang Liu, Yujie Zhong, Andrew Zisserman, Weidi Xie
- **Comment**: Accepted by BMVC2022
- **Journal**: None
- **Summary**: In this paper, we consider the problem of generalised visual object counting, with the goal of developing a computational model for counting the number of objects from arbitrary semantic categories, using arbitrary number of "exemplars", i.e. zero-shot or few-shot counting. To this end, we make the following four contributions: (1) We introduce a novel transformer-based architecture for generalised visual object counting, termed as Counting Transformer (CounTR), which explicitly capture the similarity between image patches or with given "exemplars" with the attention mechanism;(2) We adopt a two-stage training regime, that first pre-trains the model with self-supervised learning, and followed by supervised fine-tuning;(3) We propose a simple, scalable pipeline for synthesizing training images with a large number of instances or that from different semantic categories, explicitly forcing the model to make use of the given "exemplars";(4) We conduct thorough ablation studies on the large-scale counting benchmark, e.g. FSC-147, and demonstrate state-of-the-art performance on both zero and few-shot settings.



### Open-Set Semi-Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.13722v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.13722v1)
- **Published**: 2022-08-29 17:04:30+00:00
- **Updated**: 2022-08-29 17:04:30+00:00
- **Authors**: Yen-Cheng Liu, Chih-Yao Ma, Xiaoliang Dai, Junjiao Tian, Peter Vajda, Zijian He, Zsolt Kira
- **Comment**: Project Page is at https://ycliu93.github.io/projects/ossod.html
- **Journal**: None
- **Summary**: Recent developments for Semi-Supervised Object Detection (SSOD) have shown the promise of leveraging unlabeled data to improve an object detector. However, thus far these methods have assumed that the unlabeled data does not contain out-of-distribution (OOD) classes, which is unrealistic with larger-scale unlabeled datasets. In this paper, we consider a more practical yet challenging problem, Open-Set Semi-Supervised Object Detection (OSSOD). We first find the existing SSOD method obtains a lower performance gain in open-set conditions, and this is caused by the semantic expansion, where the distracting OOD objects are mispredicted as in-distribution pseudo-labels for the semi-supervised training. To address this problem, we consider online and offline OOD detection modules, which are integrated with SSOD methods. With the extensive studies, we found that leveraging an offline OOD detector based on a self-supervised vision transformer performs favorably against online OOD detectors due to its robustness to the interference of pseudo-labeling. In the experiment, our proposed framework effectively addresses the semantic expansion issue and shows consistent improvements on many OSSOD benchmarks, including large-scale COCO-OpenImages. We also verify the effectiveness of our framework under different OSSOD conditions, including varying numbers of in-distribution classes, different degrees of supervision, and different combinations of unlabeled sets.



### Effective Image Tampering Localization with Multi-Scale ConvNeXt Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/2208.13739v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13739v5)
- **Published**: 2022-08-29 17:22:37+00:00
- **Updated**: 2023-01-16 05:48:43+00:00
- **Authors**: Haochen Zhu, Gang Cao, Mo Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: With the widespread use of powerful image editing tools, image tampering becomes easy and realistic. Existing image forensic methods still face challenges of low generalization performance and robustness. In this letter, we propose an effective image tampering localization scheme based on ConvNeXt network and multi-scale feature fusion. Stacked ConvNeXt blocks are used as an encoder to capture hierarchical multi-scale features, which are then fused in decoder for locating tampered pixels accurately. Combined loss and effective data augmentation are adopted to further improve the model performance. Extensive experimental results show that localization performance of our proposed scheme outperforms other state-of-the-art ones. The source code will be available at https://github.com/ZhuHC98/ITL-SSN.



### Frido: Feature Pyramid Diffusion for Complex Scene Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2208.13753v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.13753v2)
- **Published**: 2022-08-29 17:37:29+00:00
- **Updated**: 2022-12-01 06:29:07+00:00
- **Authors**: Wan-Cyuan Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng, Lu Yuan, Yu-Chiang Frank Wang
- **Comment**: AAAI 2023
- **Journal**: None
- **Summary**: Diffusion models (DMs) have shown great potential for high-quality image synthesis. However, when it comes to producing images with complex scenes, how to properly describe both image global structures and object details remains a challenging task. In this paper, we present Frido, a Feature Pyramid Diffusion model performing a multi-scale coarse-to-fine denoising process for image synthesis. Our model decomposes an input image into scale-dependent vector quantized features, followed by a coarse-to-fine gating for producing image output. During the above multi-scale representation learning stage, additional input conditions like text, scene graph, or image layout can be further exploited. Thus, Frido can be also applied for conditional or cross-modality image synthesis. We conduct extensive experiments over various unconditioned and conditional image generation tasks, ranging from text-to-image synthesis, layout-to-image, scene-graph-to-image, to label-to-image. More specifically, we achieved state-of-the-art FID scores on five benchmarks, namely layout-to-image on COCO and OpenImages, scene-graph-to-image on COCO and Visual Genome, and label-to-image on COCO. Code is available at https://github.com/davidhalladay/Frido.



### Synthetic Latent Fingerprint Generator
- **Arxiv ID**: http://arxiv.org/abs/2208.13811v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13811v1)
- **Published**: 2022-08-29 18:02:02+00:00
- **Updated**: 2022-08-29 18:02:02+00:00
- **Authors**: Andre Brasil Vieira Wyzykowski, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Given a full fingerprint image (rolled or slap), we present CycleGAN models to generate multiple latent impressions of the same identity as the full print. Our models can control the degree of distortion, noise, blurriness and occlusion in the generated latent print images to obtain Good, Bad and Ugly latent image categories as introduced in the NIST SD27 latent database. The contributions of our work are twofold: (i) demonstrate the similarity of synthetically generated latent fingerprint images to crime scene latents in NIST SD27 and MSP databases as evaluated by the NIST NFIQ 2 quality measure and ROC curves obtained by a SOTA fingerprint matcher, and (ii) use of synthetic latents to augment small-size latent training databases in the public domain to improve the performance of DeepPrint, a SOTA fingerprint matcher designed for rolled to rolled fingerprint matching on three latent databases (NIST SD27, NIST SD302, and IIITD-SLF). As an example, with synthetic latent data augmentation, the Rank-1 retrieval performance of DeepPrint is improved from 15.50% to 29.07% on challenging NIST SD27 latent database. Our approach for generating synthetic latent fingerprints can be used to improve the recognition performance of any latent matcher and its individual components (e.g., enhancement, segmentation and feature extraction).



### Perfusion assessment via local remote photoplethysmography (rPPG)
- **Arxiv ID**: http://arxiv.org/abs/2208.13840v1
- **DOI**: 10.1109/CVPRW56347.2022.00238
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.13840v1)
- **Published**: 2022-08-29 19:07:15+00:00
- **Updated**: 2022-08-29 19:07:15+00:00
- **Authors**: Benjamin Kossack, Eric Wisotzky, Peter Eisert, Sebastian P. Schraven, Brigitta Globke, Anna Hilsmann
- **Comment**: 10 pages, 6 figures, IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops
- **Journal**: 2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)
- **Summary**: This paper presents an approach to assess the perfusion of visible human tissue from RGB video files. We propose metrics derived from remote photoplethysmography (rPPG) signals to detect whether a tissue is adequately supplied with blood. The perfusion analysis is done in three different scales, offering a flexible approach for different applications. We perform a plane-orthogonal-to-skin rPPG independently for locally defined regions of interest on each scale. From the extracted signals, we derive the signal-to-noise ratio, magnitude in the frequency domain, heart rate, perfusion index as well as correlation between specific rPPG signals in order to locally assess the perfusion of a specific region of human tissue. We show that locally resolved rPPG has a broad range of applications. As exemplary applications, we present results in intraoperative perfusion analysis and visualization during skin and organ transplantation as well as an application for liveliness assessment for the detection of presentation attacks to authentication systems.



### Radial Prediction Domain Adaption Classifier for the MIDOG 2022 challenge
- **Arxiv ID**: http://arxiv.org/abs/2208.13902v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13902v1)
- **Published**: 2022-08-29 21:50:43+00:00
- **Updated**: 2022-08-29 21:50:43+00:00
- **Authors**: Jonas Annuscheit
- **Comment**: Contribution to the MIDOG-2022-Challenge
- **Journal**: None
- **Summary**: In this paper, we describe our contribution to the MIDOG 2022 challenge without using additional data. A challenge to handle the distribution shift between different tissues for detection of mitosis cells. The main characteristics parts can be distinguished into three parts: We modify the Radial Prediction Layer (RPL) to integrate the layer in a domain adaption classifier, the Prediction Domain Adaption Classifier (RP-DAC). This developed variant learns prototypes for each class and brings more related classes closer. We used this to learn the scanner, the tissue, and the case id. We used multiple trained YOLO models with different modified input variants of the image. We combine the outputs of the model with an ensembling strategy. We use the HED color space for data augmentation by calculating different magnitudes for each scanner/tissue type to create more variance in the training set.



### Artificial intelligence-based locoregional markers of brain peritumoral microenvironment
- **Arxiv ID**: http://arxiv.org/abs/2208.14445v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.14445v1)
- **Published**: 2022-08-29 22:04:06+00:00
- **Updated**: 2022-08-29 22:04:06+00:00
- **Authors**: Zahra Riahi Samani, Drew Parker, Hamed Akbari, Spyridon Bakas, Ronald L. Wolf, Steven Brem, Ragini Verma
- **Comment**: None
- **Journal**: None
- **Summary**: In malignant primary brain tumors, cancer cells infiltrate into the peritumoral brain structures which results in inevitable recurrence. Quantitative assessment of infiltrative heterogeneity in the peritumoral region, the area where biopsy or resection can be hazardous, is important for clinical decision making. Previous work on characterizing the infiltrative heterogeneity in the peritumoral region used various imaging modalities, but information of extracellular free water movement restriction has been limitedly explored. Here, we derive a unique set of Artificial Intelligence (AI)-based markers capturing the heterogeneity of tumor infiltration, by characterizing free water movement restriction in the peritumoral region using Diffusion Tensor Imaging (DTI)-based free water volume fraction maps. A novel voxel-wise deep learning-based peritumoral microenvironment index (PMI) is first extracted by leveraging the widely different water diffusivity properties of glioblastomas and brain metastases as regions with and without infiltrations in the peritumoral tissue. Descriptive characteristics of locoregional hubs of uniformly high PMI values are extracted as AI-based markers to capture distinct aspects of infiltrative heterogeneity. The proposed markers are applied to two clinical use cases on an independent population of 275 adult-type diffuse gliomas (CNS WHO grade 4), analyzing the duration of survival among Isocitrate-Dehydrogenase 1 (IDH1)-wildtypes and the differences with IDH1-mutants. Our findings provide a panel of markers as surrogates of infiltration that captures unique insight about underlying biology of peritumoral microstructural heterogeneity, establishing them as biomarkers of prognosis pertaining to survival and molecular stratification, with potential applicability in clinical decision making.



### SB-SSL: Slice-Based Self-Supervised Transformers for Knee Abnormality Classification from MRI
- **Arxiv ID**: http://arxiv.org/abs/2208.13923v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.13923v1)
- **Published**: 2022-08-29 23:08:41+00:00
- **Updated**: 2022-08-29 23:08:41+00:00
- **Authors**: Sara Atito, Syed Muhammad Anwar, Muhammad Awais, Josef Kitler
- **Comment**: Accepted at MICCAI MILLAND workshop
- **Journal**: None
- **Summary**: The availability of large scale data with high quality ground truth labels is a challenge when developing supervised machine learning solutions for healthcare domain. Although, the amount of digital data in clinical workflows is increasing, most of this data is distributed on clinical sites and protected to ensure patient privacy. Radiological readings and dealing with large-scale clinical data puts a significant burden on the available resources, and this is where machine learning and artificial intelligence play a pivotal role. Magnetic Resonance Imaging (MRI) for musculoskeletal (MSK) diagnosis is one example where the scans have a wealth of information, but require a significant amount of time for reading and labeling. Self-supervised learning (SSL) can be a solution for handling the lack of availability of ground truth labels, but generally requires a large amount of training data during the pretraining stage. Herein, we propose a slice-based self-supervised deep learning framework (SB-SSL), a novel slice-based paradigm for classifying abnormality using knee MRI scans. We show that for a limited number of cases (<1000), our proposed framework is capable to identify anterior cruciate ligament tear with an accuracy of 89.17% and an AUC of 0.954, outperforming state-of-the-art without usage of external data during pretraining. This demonstrates that our proposed framework is suited for SSL in the limited data regime.



### SAFE: Sensitivity-Aware Features for Out-of-Distribution Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.13930v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13930v5)
- **Published**: 2022-08-29 23:57:55+00:00
- **Updated**: 2023-08-22 21:33:48+00:00
- **Authors**: Samuel Wilson, Tobias Fischer, Feras Dayoub, Dimity Miller, Niko Sünderhauf
- **Comment**: None
- **Journal**: IEEE International Conference on Computer Vision 2023
- **Summary**: We address the problem of out-of-distribution (OOD) detection for the task of object detection. We show that residual convolutional layers with batch normalisation produce Sensitivity-Aware FEatures (SAFE) that are consistently powerful for distinguishing in-distribution from out-of-distribution detections. We extract SAFE vectors for every detected object, and train a multilayer perceptron on the surrogate task of distinguishing adversarially perturbed from clean in-distribution examples. This circumvents the need for realistic OOD training data, computationally expensive generative models, or retraining of the base object detector. SAFE outperforms the state-of-the-art OOD object detectors on multiple benchmarks by large margins, e.g. reducing the FPR95 by an absolute 30.6% from 48.3% to 17.7% on the OpenImages dataset.



