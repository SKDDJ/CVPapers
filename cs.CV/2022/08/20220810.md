# Arxiv Papers in cs.CV on 2022-08-10
### Collaborative Propagation on Multiple Instance Graphs for 3D Instance Segmentation with Single-point Supervision
- **Arxiv ID**: http://arxiv.org/abs/2208.05110v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05110v3)
- **Published**: 2022-08-10 02:14:39+00:00
- **Updated**: 2023-08-20 10:56:45+00:00
- **Authors**: Shichao Dong, Ruibo Li, Jiacheng Wei, Fayao Liu, Guosheng Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Instance segmentation on 3D point clouds has been attracting increasing attention due to its wide applications, especially in scene understanding areas. However, most existing methods operate on fully annotated data while manually preparing ground-truth labels at point-level is very cumbersome and labor-intensive. To address this issue, we propose a novel weakly supervised method RWSeg that only requires labeling one object with one point. With these sparse weak labels, we introduce a unified framework with two branches to propagate semantic and instance information respectively to unknown regions using self-attention and a cross-graph random walk method. Specifically, we propose a Cross-graph Competing Random Walks (CRW) algorithm that encourages competition among different instance graphs to resolve ambiguities in closely placed objects, improving instance assignment accuracy. RWSeg generates high-quality instance-level pseudo labels. Experimental results on ScanNet-v2 and S3DIS datasets show that our approach achieves comparable performance with fully-supervised methods and outperforms previous weakly-supervised methods by a substantial margin.



### Ghost-free High Dynamic Range Imaging with Context-aware Transformer
- **Arxiv ID**: http://arxiv.org/abs/2208.05114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05114v1)
- **Published**: 2022-08-10 03:00:10+00:00
- **Updated**: 2022-08-10 03:00:10+00:00
- **Authors**: Zhen Liu, Yinglong Wang, Bing Zeng, Shuaicheng Liu
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: High dynamic range (HDR) deghosting algorithms aim to generate ghost-free HDR images with realistic details. Restricted by the locality of the receptive field, existing CNN-based methods are typically prone to producing ghosting artifacts and intensity distortions in the presence of large motion and severe saturation. In this paper, we propose a novel Context-Aware Vision Transformer (CA-ViT) for ghost-free high dynamic range imaging. The CA-ViT is designed as a dual-branch architecture, which can jointly capture both global and local dependencies. Specifically, the global branch employs a window-based Transformer encoder to model long-range object movements and intensity variations to solve ghosting. For the local branch, we design a local context extractor (LCE) to capture short-range image features and use the channel attention mechanism to select informative local details across the extracted features to complement the global branch. By incorporating the CA-ViT as basic components, we further build the HDR-Transformer, a hierarchical network to reconstruct high-quality ghost-free HDR images. Extensive experiments on three benchmark datasets show that our approach outperforms state-of-the-art methods qualitatively and quantitatively with considerably reduced computational budgets. Codes are available at https://github.com/megvii-research/HDR-Transformer



### Self-supervised Multi-modal Training from Uncurated Image and Reports Enables Zero-shot Oversight Artificial Intelligence in Radiology
- **Arxiv ID**: http://arxiv.org/abs/2208.05140v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.05140v4)
- **Published**: 2022-08-10 04:35:58+00:00
- **Updated**: 2023-04-12 10:58:04+00:00
- **Authors**: Sangjoon Park, Eun Sun Lee, Kyung Sook Shin, Jeong Eun Lee, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Oversight AI is an emerging concept in radiology where the AI forms a symbiosis with radiologists by continuously supporting radiologists in their decision-making. Recent advances in vision-language models sheds a light on the long-standing problems of the oversight AI by the understanding both visual and textual concepts and their semantic correspondences. However, there have been limited successes in the application of vision-language models in the medical domain, as the current vision-language models and learning strategies for photographic images and captions call for the web-scale data corpus of image and text pairs which was not often feasible in the medical domain. To address this, here we present a model dubbed Medical Cross-attention Vision-Language model (Medical X-VL), leveraging the key components to be tailored for the medical domain. Our medical X-VL model is based on the following components: self-supervised uni-modal models in medical domain and fusion encoder to bridge them, momentum distillation, sentence-wise contrastive learning for medical reports, and the sentence similarity-adjusted hard negative mining. We experimentally demonstrated that our model enables various zero-shot tasks for oversight AI, ranging from the zero-shot classification to zero-shot error correction. Our model outperformed the current state-of-the-art models in two different medical image database, suggesting the novel clinical usage of our oversight AI model for monitoring human errors. Our method was especially successful in the data-limited setting, which is frequently encountered in the clinics, suggesting the potential widespread applicability in medical domain.



### Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization
- **Arxiv ID**: http://arxiv.org/abs/2208.05163v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.05163v1)
- **Published**: 2022-08-10 05:54:46+00:00
- **Updated**: 2022-08-10 05:54:46+00:00
- **Authors**: Zhengang Li, Mengshu Sun, Alec Lu, Haoyu Ma, Geng Yuan, Yanyue Xie, Hao Tang, Yanyu Li, Miriam Leeser, Zhangyang Wang, Xue Lin, Zhenman Fang
- **Comment**: Published in FPL2022
- **Journal**: None
- **Summary**: Vision transformers (ViTs) are emerging with significantly improved accuracy in computer vision tasks. However, their complex architecture and enormous computation/storage demand impose urgent needs for new hardware accelerator design methodology. This work proposes an FPGA-aware automatic ViT acceleration framework based on the proposed mixed-scheme quantization. To the best of our knowledge, this is the first FPGA-based ViT acceleration framework exploring model quantization. Compared with state-of-the-art ViT quantization work (algorithmic approach only without hardware acceleration), our quantization achieves 0.47% to 1.36% higher Top-1 accuracy under the same bit-width. Compared with the 32-bit floating-point baseline FPGA accelerator, our accelerator achieves around 5.6x improvement on the frame rate (i.e., 56.8 FPS vs. 10.0 FPS) with 0.71% accuracy drop on ImageNet dataset for DeiT-base.



### Leveraging Endo- and Exo-Temporal Regularization for Black-box Video Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2208.05187v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05187v3)
- **Published**: 2022-08-10 07:09:57+00:00
- **Updated**: 2022-11-09 07:30:39+00:00
- **Authors**: Yuecong Xu, Jianfei Yang, Haozhi Cao, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen
- **Comment**: 9 pages, 4 figures, and 4 tables
- **Journal**: None
- **Summary**: To enable video models to be applied seamlessly across video tasks in different environments, various Video Unsupervised Domain Adaptation (VUDA) methods have been proposed to improve the robustness and transferability of video models. Despite improvements made in model robustness, these VUDA methods require access to both source data and source model parameters for adaptation, raising serious data privacy and model portability issues. To cope with the above concerns, this paper firstly formulates Black-box Video Domain Adaptation (BVDA) as a more realistic yet challenging scenario where the source video model is provided only as a black-box predictor. While a few methods for Black-box Domain Adaptation (BDA) are proposed in image domain, these methods cannot apply to video domain since video modality has more complicated temporal features that are harder to align. To address BVDA, we propose a novel Endo and eXo-TEmporal Regularized Network (EXTERN) by applying mask-to-mix strategies and video-tailored regularizations: endo-temporal regularization and exo-temporal regularization, performed across both clip and temporal features, while distilling knowledge from the predictions obtained from the black-box predictor. Empirical results demonstrate the state-of-the-art performance of EXTERN across various cross-domain closed-set and partial-set action recognition benchmarks, which even surpassed most existing video domain adaptation methods with source data accessibility.



### Real-Time Oil Leakage Detection on Aftermarket Motorcycle Damping System with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.05192v2
- **DOI**: 10.3390/s22207951
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05192v2)
- **Published**: 2022-08-10 07:21:57+00:00
- **Updated**: 2022-11-23 07:40:02+00:00
- **Authors**: Federico Bianchi, Stefano Speziali, Andrea Marini, Massimiliano Proietti, Lorenzo Menculini, Alberto Garinei, Gabriele Bellani, Marcello Marconi
- **Comment**: analysis of literature reviewed, n.2 figures added, minor corrections
- **Journal**: None
- **Summary**: In this work, we describe in detail how Deep Learning and Computer Vision can help to detect fault events of the AirTender system, an aftermarket motorcycle damping system component. One of the most effective ways to monitor the AirTender functioning is to look for oil stains on its surface. Starting from real-time images, AirTender is first detected in the motorbike suspension system, simulated indoor, and then, a binary classifier determines whether AirTender is spilling oil or not. The detection is made with the help of the Yolo5 architecture, whereas the classification is carried out with the help of a suitably designed Convolutional Neural Network, OilNet40. In order to detect oil leaks more clearly, we dilute the oil in AirTender with a fluorescent dye with an excitation wavelength peak of approximately 390 nm. AirTender is then illuminated with suitable UV LEDs. The whole system is an attempt to design a low-cost detection setup. An on-board device, such as a mini-computer, is placed near the suspension system and connected to a full hd camera framing AirTender. The on-board device, through our Neural Network algorithm, is then able to localize and classify AirTender as normally functioning (non-leak image) or anomaly (leak image).



### Aesthetic Visual Question Answering of Photographs
- **Arxiv ID**: http://arxiv.org/abs/2208.05798v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05798v1)
- **Published**: 2022-08-10 07:27:57+00:00
- **Updated**: 2022-08-10 07:27:57+00:00
- **Authors**: Xin Jin, Wu Zhou, Xinghui Zhou, Shuai Cui, Le Zhang, Jianwen Lv, Shu Zhao
- **Comment**: 13 pages, 7 figures, on going research
- **Journal**: None
- **Summary**: Aesthetic assessment of images can be categorized into two main forms: numerical assessment and language assessment. Aesthetics caption of photographs is the only task of aesthetic language assessment that has been addressed. In this paper, we propose a new task of aesthetic language assessment: aesthetic visual question and answering (AVQA) of images. If we give a question of images aesthetics, model can predict the answer. We use images from \textit{www.flickr.com}. The objective QA pairs are generated by the proposed aesthetic attributes analysis algorithms. Moreover, we introduce subjective QA pairs that are converted from aesthetic numerical labels and sentiment analysis from large-scale pre-train models. We build the first aesthetic visual question answering dataset, AesVQA, that contains 72,168 high-quality images and 324,756 pairs of aesthetic questions. Two methods for adjusting the data distribution have been proposed and proved to improve the accuracy of existing models. This is the first work that both addresses the task of aesthetic VQA and introduces subjectiveness into VQA tasks. The experimental results reveal that our methods outperform other VQA models on this new task.



### A Detection Method of Temporally Operated Videos Using Robust Hashing
- **Arxiv ID**: http://arxiv.org/abs/2208.05198v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05198v2)
- **Published**: 2022-08-10 07:36:07+00:00
- **Updated**: 2022-08-11 14:42:57+00:00
- **Authors**: Shoko Niwa, Miki Tanaka, Hitoshi Kiya
- **Comment**: To appear in 2022 IEEE 11th Global Conference on Consumer Electronics
  (GCCE 2022)
- **Journal**: None
- **Summary**: SNS providers are known to carry out the recompression and resizing of uploaded videos/images, but most conventional methods for detecting tampered videos/images are not robust enough against such operations. In addition, videos are temporally operated such as the insertion of new frames and the permutation of frames, of which operations are difficult to be detected by using conventional methods. Accordingly, in this paper, we propose a novel method with a robust hashing algorithm for detecting temporally operated videos even when applying resizing and compression to the videos.



### Automatic Camera Control and Directing with an Ultra-High-Definition Collaborative Recording System
- **Arxiv ID**: http://arxiv.org/abs/2208.05213v1
- **DOI**: 10.1145/3485441.3485648
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.05213v1)
- **Published**: 2022-08-10 08:28:08+00:00
- **Updated**: 2022-08-10 08:28:08+00:00
- **Authors**: Bram Vanherle, Tim Vervoort, Nick Michiels, Philippe Bekaert
- **Comment**: None
- **Journal**: CVMP, 2021, 1-10
- **Summary**: Capturing an event from multiple camera angles can give a viewer the most complete and interesting picture of that event. To be suitable for broadcasting, a human director needs to decide what to show at each point in time. This can become cumbersome with an increasing number of camera angles. The introduction of omnidirectional or wide-angle cameras has allowed for events to be captured more completely, making it even more difficult for the director to pick a good shot. In this paper, a system is presented that, given multiple ultra-high resolution video streams of an event, can generate a visually pleasing sequence of shots that manages to follow the relevant action of an event. Due to the algorithm being general purpose, it can be applied to most scenarios that feature humans. The proposed method allows for online processing when real-time broadcasting is required, as well as offline processing when the quality of the camera operation is the priority. Object detection is used to detect humans and other objects of interest in the input streams. Detected persons of interest, along with a set of rules based on cinematic conventions, are used to determine which video stream to show and what part of that stream is virtually framed. The user can provide a number of settings that determine how these rules are interpreted. The system is able to handle input from different wide-angle video streams by removing lens distortions. Using a user study it is shown, for a number of different scenarios, that the proposed automated director is able to capture an event with aesthetically pleasing video compositions and human-like shot switching behavior.



### Exploring Point-BEV Fusion for 3D Point Cloud Object Tracking with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2208.05216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05216v1)
- **Published**: 2022-08-10 08:36:46+00:00
- **Updated**: 2022-08-10 08:36:46+00:00
- **Authors**: Zhipeng Luo, Changqing Zhou, Liang Pan, Gongjie Zhang, Tianrui Liu, Yueru Luo, Haiyu Zhao, Ziwei Liu, Shijian Lu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2112.02857
- **Journal**: None
- **Summary**: With the prevalence of LiDAR sensors in autonomous driving, 3D object tracking has received increasing attention. In a point cloud sequence, 3D object tracking aims to predict the location and orientation of an object in consecutive frames given an object template. Motivated by the success of transformers, we propose Point Tracking TRansformer (PTTR), which efficiently predicts high-quality 3D tracking results in a coarse-to-fine manner with the help of transformer operations. PTTR consists of three novel designs. 1) Instead of random sampling, we design Relation-Aware Sampling to preserve relevant points to the given template during subsampling. 2) We propose a Point Relation Transformer for effective feature aggregation and feature matching between the template and search region. 3) Based on the coarse tracking results, we employ a novel Prediction Refinement Module to obtain the final refined prediction through local feature pooling. In addition, motivated by the favorable properties of the Bird's-Eye View (BEV) of point clouds in capturing object motion, we further design a more advanced framework named PTTR++, which incorporates both the point-wise view and BEV representation to exploit their complementary effect in generating high-quality tracking results. PTTR++ substantially boosts the tracking performance on top of PTTR with low computational overhead. Extensive experiments over multiple datasets show that our proposed approaches achieve superior 3D tracking accuracy and efficiency.



### Dual Domain-Adversarial Learning for Audio-Visual Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/2208.05220v2
- **DOI**: 10.1145/3552458.3556447
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05220v2)
- **Published**: 2022-08-10 08:50:32+00:00
- **Updated**: 2022-08-16 07:10:40+00:00
- **Authors**: Yingzi Fan, Longfei Han, Yue Zhang, Lechao Cheng, Chen Xia, Di Hu
- **Comment**: Accepted by ACM MM workshop 2022(HCMA2022)
- **Journal**: None
- **Summary**: Both visual and auditory information are valuable to determine the salient regions in videos. Deep convolution neural networks (CNN) showcase strong capacity in coping with the audio-visual saliency prediction task. Due to various factors such as shooting scenes and weather, there often exists moderate distribution discrepancy between source training data and target testing data. The domain discrepancy induces to performance degradation on target testing data for CNN models. This paper makes an early attempt to tackle the unsupervised domain adaptation problem for audio-visual saliency prediction. We propose a dual domain-adversarial learning algorithm to mitigate the domain discrepancy between source and target data. First, a specific domain discrimination branch is built up for aligning the auditory feature distributions. Then, those auditory features are fused into the visual features through a cross-modal self-attention module. The other domain discrimination branch is devised to reduce the domain discrepancy of visual features and audio-visual correlations implied by the fused audio-visual features. Experiments on public benchmarks demonstrate that our method can relieve the performance degradation caused by domain discrepancy.



### Neural Mesh-Based Graphics
- **Arxiv ID**: http://arxiv.org/abs/2208.05785v3
- **DOI**: 10.1007/978-3-031-25066-8_45
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05785v3)
- **Published**: 2022-08-10 09:18:28+00:00
- **Updated**: 2022-09-05 12:16:08+00:00
- **Authors**: Shubhendu Jena, Franck Multon, Adnane Boukhayma
- **Comment**: ECCV Workshop 2022 CV4Metaverse. The source code and trained models
  can be obtained at
  https://github.com/Shubhendu-Jena/Neural-Mesh-Based-Graphics
- **Journal**: None
- **Summary**: We revisit NPBG, the popular approach to novel view synthesis that introduced the ubiquitous point feature neural rendering paradigm. We are interested in particular in data-efficient learning with fast view synthesis. We achieve this through a view-dependent mesh-based denser point descriptor rasterization, in addition to a foreground/background scene rendering split, and an improved loss. By training solely on a single scene, we outperform NPBG, which has been trained on ScanNet and then scene finetuned. We also perform competitively with respect to the state-of-the-art method SVS, which has been trained on the full dataset (DTU and Tanks and Temples) and then scene finetuned, in spite of their deeper neural renderer.



### Trustworthy Visual Analytics in Clinical Gait Analysis: A Case Study for Patients with Cerebral Palsy
- **Arxiv ID**: http://arxiv.org/abs/2208.05232v3
- **DOI**: 10.1109/TREX57753.2022.00006
- **Categories**: **cs.HC**, cs.CV, cs.LG, J.3; H.5.m; I.3.6
- **Links**: [PDF](http://arxiv.org/pdf/2208.05232v3)
- **Published**: 2022-08-10 09:21:28+00:00
- **Updated**: 2022-12-19 13:07:15+00:00
- **Authors**: Alexander Rind, Djordje Slijepčević, Matthias Zeppelzauer, Fabian Unglaube, Andreas Kranzl, Brian Horsak
- **Comment**: 7 pages, 4 figures; supplemental material 9 pages, 8 figures
- **Journal**: Proceedings of the 2022 IEEE Workshop on TRust and EXpertise in
  Visual Analytics, TREX (2022) 8-15
- **Summary**: Three-dimensional clinical gait analysis is essential for selecting optimal treatment interventions for patients with cerebral palsy (CP), but generates a large amount of time series data. For the automated analysis of these data, machine learning approaches yield promising results. However, due to their black-box nature, such approaches are often mistrusted by clinicians. We propose gaitXplorer, a visual analytics approach for the classification of CP-related gait patterns that integrates Grad-CAM, a well-established explainable artificial intelligence algorithm, for explanations of machine learning classifications. Regions of high relevance for classification are highlighted in the interactive visual interface. The approach is evaluated in a case study with two clinical gait experts. They inspected the explanations for a sample of eight patients using the visual interface and expressed which relevance scores they found trustworthy and which they found suspicious. Overall, the clinicians gave positive feedback on the approach as it allowed them a better understanding of which regions in the data were relevant for the classification.



### Multi-structure segmentation for renal cancer treatment with modified nn-UNet
- **Arxiv ID**: http://arxiv.org/abs/2208.05241v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.05241v1)
- **Published**: 2022-08-10 09:49:19+00:00
- **Updated**: 2022-08-10 09:49:19+00:00
- **Authors**: Zhenyu Bu
- **Comment**: KiPA2022 Challenge report
- **Journal**: None
- **Summary**: Renal cancer is one of the most prevalent cancers worldwide. Clinical signs of kidney cancer include hematuria and low back discomfort, which are quite distressing to the patient. Due to the rapid growth of artificial intelligence and deep learning, medical image segmentation has evolved dramatically over the past few years. In this paper, we propose modified nn-UNet for kidney multi-structure segmentation. Our solution is founded on the thriving nn-UNet architecture using 3D full resolution U-net. Firstly, various hyperparameters are modified for this particular task. Then, by doubling the number of filters in 3D full resolution nnUNet architecture to achieve a larger network, we may capture a greater receptive field. Finally, we include an axial attention mechanism in the decoder, which can obtain global information during the decoding stage to prevent the loss of local knowledge. Our modified nn-UNet achieves state-of-the-art performance on the KiPA2022 dataset when compared to conventional approaches such as 3D U-Net, MNet, etc.



### Learning Degradation Representations for Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2208.05244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05244v1)
- **Published**: 2022-08-10 09:53:16+00:00
- **Updated**: 2022-08-10 09:53:16+00:00
- **Authors**: Dasong Li, Yi Zhang, Ka Chun Cheung, Xiaogang Wang, Hongwei Qin, Hongsheng Li
- **Comment**: Accepted to ECCV 2022
- **Journal**: ECCV 2022
- **Summary**: In various learning-based image restoration tasks, such as image denoising and image super-resolution, the degradation representations were widely used to model the degradation process and handle complicated degradation patterns. However, they are less explored in learning-based image deblurring as blur kernel estimation cannot perform well in real-world challenging cases. We argue that it is particularly necessary for image deblurring to model degradation representations since blurry patterns typically show much larger variations than noisy patterns or high-frequency textures.In this paper, we propose a framework to learn spatially adaptive degradation representations of blurry images. A novel joint image reblurring and deblurring learning process is presented to improve the expressiveness of degradation representations. To make learned degradation representations effective in reblurring and deblurring, we propose a Multi-Scale Degradation Injection Network (MSDI-Net) to integrate them into the neural networks. With the integration, MSDI-Net can handle various and complicated blurry patterns adaptively. Experiments on the GoPro and RealBlur datasets demonstrate that our proposed deblurring framework with the learned degradation representations outperforms state-of-the-art methods with appealing improvements. The code is released at https://github.com/dasongli1/Learning_degradation.



### Consistency-based Self-supervised Learning for Temporal Anomaly Localization
- **Arxiv ID**: http://arxiv.org/abs/2208.05251v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.05251v1)
- **Published**: 2022-08-10 10:07:34+00:00
- **Updated**: 2022-08-10 10:07:34+00:00
- **Authors**: Aniello Panariello, Angelo Porrello, Simone Calderara, Rita Cucchiara
- **Comment**: Accepted to the WCPA Workshop at ECCV 2022 (1st International
  Workshop and Challenge on People Analysis: From Face, Body and Fashion to 3D
  Virtual Avatars). 13 pages, 2 figures
- **Journal**: None
- **Summary**: This work tackles Weakly Supervised Anomaly detection, in which a predictor is allowed to learn not only from normal examples but also from a few labeled anomalies made available during training. In particular, we deal with the localization of anomalous activities within the video stream: this is a very challenging scenario, as training examples come only with video-level annotations (and not frame-level). Several recent works have proposed various regularization terms to address it i.e. by enforcing sparsity and smoothness constraints over the weakly-learned frame-level anomaly scores. In this work, we get inspired by recent advances within the field of self-supervised learning and ask the model to yield the same scores for different augmentations of the same video sequence. We show that enforcing such an alignment improves the performance of the model on XD-Violence.



### Multi-scale Feature Aggregation for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2208.05256v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05256v2)
- **Published**: 2022-08-10 10:23:12+00:00
- **Updated**: 2022-08-11 13:41:28+00:00
- **Authors**: Xiaoheng Jiang, Xinyi Wu, Hisham Cholakkal, Rao Muhammad Anwer, Jiale Cao Mingliang Xu, Bing Zhou, Yanwei Pang, Fahad Shahbaz Khan
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) based crowd counting methods have achieved promising results in the past few years. However, the scale variation problem is still a huge challenge for accurate count estimation. In this paper, we propose a multi-scale feature aggregation network (MSFANet) that can alleviate this problem to some extent. Specifically, our approach consists of two feature aggregation modules: the short aggregation (ShortAgg) and the skip aggregation (SkipAgg). The ShortAgg module aggregates the features of the adjacent convolution blocks. Its purpose is to make features with different receptive fields fused gradually from the bottom to the top of the network. The SkipAgg module directly propagates features with small receptive fields to features with much larger receptive fields. Its purpose is to promote the fusion of features with small and large receptive fields. Especially, the SkipAgg module introduces the local self-attention features from the Swin Transformer blocks to incorporate rich spatial information. Furthermore, we present a local-and-global based counting loss by considering the non-uniform crowd distribution. Extensive experiments on four challenging datasets (ShanghaiTech dataset, UCF_CC_50 dataset, UCF-QNRF Dataset, WorldExpo'10 dataset) demonstrate the proposed easy-to-implement MSFANet can achieve promising results when compared with the previous state-of-the-art approaches.



### Efficient Joint-Dimensional Search with Solution Space Regularization for Real-Time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.05271v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.05271v1)
- **Published**: 2022-08-10 11:07:33+00:00
- **Updated**: 2022-08-10 11:07:33+00:00
- **Authors**: Peng Ye, Baopu Li, Tao Chen, Jiayuan Fan, Zhen Mei, Chen Lin, Chongyan Zuo, Qinghua Chi, Wanli Ouyan
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is a popular research topic in computer vision, and many efforts have been made on it with impressive results. In this paper, we intend to search an optimal network structure that can run in real-time for this problem. Towards this goal, we jointly search the depth, channel, dilation rate and feature spatial resolution, which results in a search space consisting of about 2.78*10^324 possible choices. To handle such a large search space, we leverage differential architecture search methods. However, the architecture parameters searched using existing differential methods need to be discretized, which causes the discretization gap between the architecture parameters found by the differential methods and their discretized version as the final solution for the architecture search. Hence, we relieve the problem of discretization gap from the innovative perspective of solution space regularization. Specifically, a novel Solution Space Regularization (SSR) loss is first proposed to effectively encourage the supernet to converge to its discrete one. Then, a new Hierarchical and Progressive Solution Space Shrinking method is presented to further achieve high efficiency of searching. In addition, we theoretically show that the optimization of SSR loss is equivalent to the L_0-norm regularization, which accounts for the improved search-evaluation gap. Comprehensive experiments show that the proposed search scheme can efficiently find an optimal network structure that yields an extremely fast speed (175 FPS) of segmentation with a small model size (1 M) while maintaining comparable accuracy.



### Arbitrary Point Cloud Upsampling with Spherical Mixture of Gaussians
- **Arxiv ID**: http://arxiv.org/abs/2208.05274v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05274v2)
- **Published**: 2022-08-10 11:10:16+00:00
- **Updated**: 2023-01-10 11:21:46+00:00
- **Authors**: Anthony Dell'Eva, Marco Orsingher, Massimo Bertozzi
- **Comment**: Accepted to 3DV 2022 (Oral)
- **Journal**: None
- **Summary**: Generating dense point clouds from sparse raw data benefits downstream 3D understanding tasks, but existing models are limited to a fixed upsampling ratio or to a short range of integer values. In this paper, we present APU-SMOG, a Transformer-based model for Arbitrary Point cloud Upsampling (APU). The sparse input is firstly mapped to a Spherical Mixture of Gaussians (SMOG) distribution, from which an arbitrary number of points can be sampled. Then, these samples are fed as queries to the Transformer decoder, which maps them back to the target surface. Extensive qualitative and quantitative evaluations show that APU-SMOG outperforms state-of-the-art fixed-ratio methods, while effectively enabling upsampling with any scaling factor, including non-integer values, with a single trained model. The code is available at https://github.com/apusmog/apusmog/



### Semantic Self-adaptation: Enhancing Generalization with a Single Sample
- **Arxiv ID**: http://arxiv.org/abs/2208.05788v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05788v2)
- **Published**: 2022-08-10 12:29:01+00:00
- **Updated**: 2023-07-21 11:50:11+00:00
- **Authors**: Sherwin Bahmani, Oliver Hahn, Eduard Zamfir, Nikita Araslanov, Daniel Cremers, Stefan Roth
- **Comment**: Published in TMLR (July 2023); OpenReview:
  https://openreview.net/forum?id=ILNqQhGbLx; Code:
  https://github.com/visinf/self-adaptive; Video: https://youtu.be/s4DG65ic0EA
- **Journal**: None
- **Summary**: The lack of out-of-domain generalization is a critical weakness of deep networks for semantic segmentation. Previous studies relied on the assumption of a static model, i. e., once the training process is complete, model parameters remain fixed at test time. In this work, we challenge this premise with a self-adaptive approach for semantic segmentation that adjusts the inference process to each input sample. Self-adaptation operates on two levels. First, it fine-tunes the parameters of convolutional layers to the input image using consistency regularization. Second, in Batch Normalization layers, self-adaptation interpolates between the training and the reference distribution derived from a single test sample. Despite both techniques being well known in the literature, their combination sets new state-of-the-art accuracy on synthetic-to-real generalization benchmarks. Our empirical study suggests that self-adaptation may complement the established practice of model regularization at training time for improving deep network generalization to out-of-domain data. Our code and pre-trained models are available at https://github.com/visinf/self-adaptive.



### Generative Transfer Learning: Covid-19 Classification with a few Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2208.05305v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.05305v1)
- **Published**: 2022-08-10 12:37:52+00:00
- **Updated**: 2022-08-10 12:37:52+00:00
- **Authors**: Suvarna Kadam, Vinay G. Vaidya
- **Comment**: AI4SG-21: The 3rd Workshop on Artificial Intelligence for Social
  Good, IJCAI 2021, Montreal Canada. 7 pages, 6 Figures, 4 Tables.
  github.io/AI4SG2021/
- **Journal**: None
- **Summary**: Detection of diseases through medical imaging is preferred due to its non-invasive nature. Medical imaging supports multiple modalities of data that enable a thorough and quick look inside a human body. However, interpreting imaging data is often time-consuming and requires a great deal of human expertise. Deep learning models can expedite interpretation and alleviate the work of human experts. However, these models are data-intensive and require significant labeled images for training. During novel disease outbreaks such as Covid-19, we often do not have the required labeled imaging data, especially at the start of the epidemic. Deep Transfer Learning addresses this problem by using a pretrained model in the public domain, e.g. any variant of either VGGNet, ResNet, Inception, DenseNet, etc., as a feature learner to quickly adapt the target task from fewer samples. Most pretrained models are deep with complex architectures. They are trained with large multi-class datasets such as ImageNet, with significant human efforts in architecture design and hyper parameters tuning. We presented 1 a simpler generative source model, pretrained on a single but related concept, can perform as effectively as existing larger pretrained models. We demonstrate the usefulness of generative transfer learning that requires less compute and training data, for Few Shot Learning (FSL) with a Covid-19 binary classification use case. We compare classic deep transfer learning with our approach and also report FSL results with three settings of 84, 20, and 10 training samples. The model implementation of generative FSL for Covid-19 classification is available publicly at https://github.com/suvarnak/GenerativeFSLCovid.git.



### Language Supervised Training for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2208.05318v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.05318v1)
- **Published**: 2022-08-10 12:55:56+00:00
- **Updated**: 2022-08-10 12:55:56+00:00
- **Authors**: Wangmeng Xiang, Chao Li, Yuxuan Zhou, Biao Wang, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Skeleton-based action recognition has drawn a lot of attention for its computation efficiency and robustness to lighting conditions. Existing skeleton-based action recognition methods are typically formulated as a one-hot classification task without fully utilizing the semantic relations between actions. For example, "make victory sign" and "thumb up" are two actions of hand gestures, whose major difference lies in the movement of hands. This information is agnostic from the categorical one-hot encoding of action classes but could be unveiled in the language description of actions. Therefore, utilizing action language descriptions in training could potentially benefit representation learning. In this work, we propose a Language Supervised Training (LST) approach for skeleton-based action recognition. More specifically, we employ a large-scale language model as the knowledge engine to provide text descriptions for body parts movements of actions, and propose a multi-modal training scheme by utilizing the text encoder to generate feature vectors for different body parts and supervise the skeleton encoder for action representation learning. Experiments show that our proposed LST method achieves noticeable improvements over various baseline models without extra computation cost at inference. LST achieves new state-of-the-arts on popular skeleton-based action recognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and NW-UCLA. The code can be found at https://github.com/MartinXM/LST.



### MD-Net: Multi-Detector for Local Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/2208.05350v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05350v1)
- **Published**: 2022-08-10 13:52:31+00:00
- **Updated**: 2022-08-10 13:52:31+00:00
- **Authors**: Emanuele Santellani, Christian Sormann, Mattia Rossi, Andreas Kuhn, Friedrich Fraundorfer
- **Comment**: Accepted at ICPR 2022
- **Journal**: None
- **Summary**: Establishing a sparse set of keypoint correspon dences between images is a fundamental task in many computer vision pipelines. Often, this translates into a computationally expensive nearest neighbor search, where every keypoint descriptor at one image must be compared with all the descriptors at the others. In order to lower the computational cost of the matching phase, we propose a deep feature extraction network capable of detecting a predefined number of complementary sets of keypoints at each image. Since only the descriptors within the same set need to be compared across the different images, the matching phase computational complexity decreases with the number of sets. We train our network to predict the keypoints and compute the corresponding descriptors jointly. In particular, in order to learn complementary sets of keypoints, we introduce a novel unsupervised loss which penalizes intersections among the different sets. Additionally, we propose a novel descriptor-based weighting scheme meant to penalize the detection of keypoints with non-discriminative descriptors. With extensive experiments we show that our feature extraction network, trained only on synthetically warped images and in a fully unsupervised manner, achieves competitive results on 3D reconstruction and re-localization tasks at a reduced matching complexity.



### High-Frequency Space Diffusion Models for Accelerated MRI
- **Arxiv ID**: http://arxiv.org/abs/2208.05481v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.05481v4)
- **Published**: 2022-08-10 14:04:20+00:00
- **Updated**: 2022-12-14 02:13:16+00:00
- **Authors**: Chentao Cao, Zhuo-Xu Cui, Shaonan Liu, Hairong Zheng, Dong Liang, Yanjie Zhu
- **Comment**: submitted to IEEE TMI
- **Journal**: None
- **Summary**: Diffusion models with continuous stochastic differential equations (SDEs) have shown superior performances in image generation. It can be used as a deep generative prior to solve the inverse problem in MR reconstruction. However, the existing VP-SDE can be treated as maximizing the energy of the MR image to be reconstructed and may lead to SDE sequence divergence. The VE-SDE based MR reconstruction is not consistent with actual diffusion process. In addition, both VE- and VP-SDEs-based models suffer from a time-consuming sampling procedure, resulting long reconstruction time. In this study, a new SDE focusing on the diffusion process in high-frequency space is designed specifically for robust MR reconstruction based on diffusion models. Experiments on the publicly fastMRI dataset show that HFS-SDE based reconstruction method outperforms the parallel imaging, supervised deep learning, and existing VE- and VP-SDEs-based methods in terms of reconstruction accuracy. It also improves the stability of MR reconstruction and accelerates sampling procedure of reverse diffusion.



### Towards Daily High-resolution Inundation Observations using Deep Learning and EO
- **Arxiv ID**: http://arxiv.org/abs/2208.09135v2
- **DOI**: None
- **Categories**: **physics.geo-ph**, cs.CV, cs.LG, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2208.09135v2)
- **Published**: 2022-08-10 14:04:50+00:00
- **Updated**: 2022-09-02 13:14:26+00:00
- **Authors**: Antara Dasgupta, Lasse Hybbeneth, Björn Waske
- **Comment**: None
- **Journal**: None
- **Summary**: Satellite remote sensing presents a cost-effective solution for synoptic flood monitoring, and satellite-derived flood maps provide a computationally efficient alternative to numerical flood inundation models traditionally used. While satellites do offer timely inundation information when they happen to cover an ongoing flood event, they are limited by their spatiotemporal resolution in terms of their ability to dynamically monitor flood evolution at various scales. Constantly improving access to new satellite data sources as well as big data processing capabilities has unlocked an unprecedented number of possibilities in terms of data-driven solutions to this problem. Specifically, the fusion of data from satellites, such as the Copernicus Sentinels, which have high spatial and low temporal resolution, with data from NASA SMAP and GPM missions, which have low spatial but high temporal resolutions could yield high-resolution flood inundation at a daily scale. Here a Convolutional-Neural-Network is trained using flood inundation maps derived from Sentinel-1 Synthetic Aperture Radar and various hydrological, topographical, and land-use based predictors for the first time, to predict high-resolution probabilistic maps of flood inundation. The performance of UNet and SegNet model architectures for this task is evaluated, using flood masks derived from Sentinel-1 and Sentinel-2, separately with 95 percent-confidence intervals. The Area under the Curve (AUC) of the Precision Recall Curve (PR-AUC) is used as the main evaluation metric, due to the inherently imbalanced nature of classes in a binary flood mapping problem, with the best model delivering a PR-AUC of 0.85.



### CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2208.05358v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, I.2.7; I.2.10; I.2.6; I.4.8; I.1.4
- **Links**: [PDF](http://arxiv.org/pdf/2208.05358v1)
- **Published**: 2022-08-10 14:08:34+00:00
- **Updated**: 2022-08-10 14:08:34+00:00
- **Authors**: Adam Dahlgren Lindström, Savitha Sam Abraham
- **Comment**: NeSy 2022, 16th International Workshop on Neural-Symbolic Learning
  and Reasoning, Cumberland Lodge, Windsor, UK
- **Journal**: None
- **Summary**: We introduce CLEVR-Math, a multi-modal math word problems dataset consisting of simple math word problems involving addition/subtraction, represented partly by a textual description and partly by an image illustrating the scenario. The text describes actions performed on the scene that is depicted in the image. Since the question posed may not be about the scene in the image, but about the state of the scene before or after the actions are applied, the solver envision or imagine the state changes due to these actions. Solving these word problems requires a combination of language, visual and mathematical reasoning. We apply state-of-the-art neural and neuro-symbolic models for visual question answering on CLEVR-Math and empirically evaluate their performances. Our results show how neither method generalise to chains of operations. We discuss the limitations of the two in addressing the task of multi-modal word problem solving.



### PatchDropout: Economizing Vision Transformers Using Patch Dropout
- **Arxiv ID**: http://arxiv.org/abs/2208.07220v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.07220v2)
- **Published**: 2022-08-10 14:08:55+00:00
- **Updated**: 2022-10-04 12:58:29+00:00
- **Authors**: Yue Liu, Christos Matsoukas, Fredrik Strand, Hossein Azizpour, Kevin Smith
- **Comment**: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
- **Journal**: None
- **Summary**: Vision transformers have demonstrated the potential to outperform CNNs in a variety of vision tasks. But the computational and memory requirements of these models prohibit their use in many applications, especially those that depend on high-resolution images, such as medical image classification. Efforts to train ViTs more efficiently are overly complicated, necessitating architectural changes or intricate training schemes. In this work, we show that standard ViT models can be efficiently trained at high resolution by randomly dropping input image patches. This simple approach, PatchDropout, reduces FLOPs and memory by at least 50% in standard natural image datasets such as ImageNet, and those savings only increase with image size. On CSAW, a high-resolution medical dataset, we observe a 5 times savings in computation and memory using PatchDropout, along with a boost in performance. For practitioners with a fixed computational or memory budget, PatchDropout makes it possible to choose image resolution, hyperparameters, or model size to get the most performance out of their model.



### E Pluribus Unum Interpretable Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.05369v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T45, 68T07, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2208.05369v1)
- **Published**: 2022-08-10 14:37:03+00:00
- **Updated**: 2022-08-10 14:37:03+00:00
- **Authors**: George Dimas, Eirini Cholopoulou, Dimitris K. Iakovidis
- **Comment**: Journal paper under review
- **Journal**: None
- **Summary**: The adoption of Convolutional Neural Network (CNN) models in high-stake domains is hindered by their inability to meet society's demand for transparency in decision-making. So far, a growing number of methodologies have emerged for developing CNN models that are interpretable by design. However, such models are not capable of providing interpretations in accordance with human perception, while maintaining competent performance. In this paper, we tackle these challenges with a novel, general framework for instantiating inherently interpretable CNN models, named E Pluribus Unum Interpretable CNN (EPU-CNN). An EPU-CNN model consists of CNN sub-networks, each of which receives a different representation of an input image expressing a perceptual feature, such as color or texture. The output of an EPU-CNN model consists of the classification prediction and its interpretation, in terms of relative contributions of perceptual features in different regions of the input image. EPU-CNN models have been extensively evaluated on various publicly available datasets, as well as a contributed benchmark dataset. Medical datasets are used to demonstrate the applicability of EPU-CNN for risk-sensitive decisions in medicine. The experimental results indicate that EPU-CNN models can achieve a comparable or better classification performance than other CNN architectures while providing humanly perceivable interpretations.



### Exploring Anchor-based Detection for Ego4D Natural Language Query
- **Arxiv ID**: http://arxiv.org/abs/2208.05375v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.05375v1)
- **Published**: 2022-08-10 14:43:37+00:00
- **Updated**: 2022-08-10 14:43:37+00:00
- **Authors**: Sipeng Zheng, Qi Zhang, Bei Liu, Qin Jin, Jianlong Fu
- **Comment**: CVPR 2022 Ego4D Workshop NLQ Challenge
- **Journal**: None
- **Summary**: In this paper we provide the technique report of Ego4D natural language query challenge in CVPR 2022. Natural language query task is challenging due to the requirement of comprehensive understanding of video contents. Most previous works address this task based on third-person view datasets while few research interest has been placed in the ego-centric view by far. Great progress has been made though, we notice that previous works can not adapt well to ego-centric view datasets e.g., Ego4D mainly because of two reasons: 1) most queries in Ego4D have a excessively small temporal duration (e.g., less than 5 seconds); 2) queries in Ego4D are faced with much more complex video understanding of long-term temporal orders. Considering these, we propose our solution of this challenge to solve the above issues.



### KiPA22 Report: U-Net with Contour Regularization for Renal Structures Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.05772v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.05772v2)
- **Published**: 2022-08-10 15:07:20+00:00
- **Updated**: 2022-09-06 09:08:01+00:00
- **Authors**: Kangqing Ye, Peng Liu, Xiaoyang Zou, Qin Zhou, Guoyan Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Three-dimensional (3D) integrated renal structures (IRS) segmentation is important in clinical practice. With the advancement of deep learning techniques, many powerful frameworks focusing on medical image segmentation are proposed. In this challenge, we utilized the nnU-Net framework, which is the state-of-the-art method for medical image segmentation. To reduce the outlier prediction for the tumor label, we combine contour regularization (CR) loss of the tumor label with Dice loss and cross-entropy loss to improve this phenomenon.



### Benchmarking Joint Face Spoofing and Forgery Detection with Visual and Physiological Cues
- **Arxiv ID**: http://arxiv.org/abs/2208.05401v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05401v1)
- **Published**: 2022-08-10 15:41:48+00:00
- **Updated**: 2022-08-10 15:41:48+00:00
- **Authors**: Zitong Yu, Rizhao Cai, Zhi Li, Wenhan Yang, Jingang Shi, Alex C. Kot
- **Comment**: submitted to IEEE Transactions on Information Forensics and Security
  (TIFS)
- **Journal**: None
- **Summary**: Face anti-spoofing (FAS) and face forgery detection play vital roles in securing face biometric systems from presentation attacks (PAs) and vicious digital manipulation (e.g., deepfakes). Despite promising performance upon large-scale data and powerful deep models, the generalization problem of existing approaches is still an open issue. Most of recent approaches focus on 1) unimodal visual appearance or physiological (i.e., remote photoplethysmography (rPPG)) cues; and 2) separated feature representation for FAS or face forgery detection. On one side, unimodal appearance and rPPG features are respectively vulnerable to high-fidelity face 3D mask and video replay attacks, inspiring us to design reliable multi-modal fusion mechanisms for generalized face attack detection. On the other side, there are rich common features across FAS and face forgery detection tasks (e.g., periodic rPPG rhythms and vanilla appearance for bonafides), providing solid evidence to design a joint FAS and face forgery detection system in a multi-task learning fashion. In this paper, we establish the first joint face spoofing and forgery detection benchmark using both visual appearance and physiological rPPG cues. To enhance the rPPG periodicity discrimination, we design a two-branch physiological network using both facial spatio-temporal rPPG signal map and its continuous wavelet transformed counterpart as inputs. To mitigate the modality bias and improve the fusion efficacy, we conduct a weighted batch and layer normalization for both appearance and rPPG features before multi-modal fusion. We find that the generalization capacities of both unimodal (appearance or rPPG) and multi-modal (appearance+rPPG) models can be obviously improved via joint training on these two tasks. We hope this new benchmark will facilitate the future research of both FAS and deepfake detection communities.



### CIAO! A Contrastive Adaptation Mechanism for Non-Universal Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2208.07221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.07221v1)
- **Published**: 2022-08-10 15:46:05+00:00
- **Updated**: 2022-08-10 15:46:05+00:00
- **Authors**: Pablo Barros, Alessandra Sciutti
- **Comment**: None
- **Journal**: None
- **Summary**: Current facial expression recognition systems demand an expensive re-training routine when deployed to different scenarios than they were trained for. Biasing them towards learning specific facial characteristics, instead of performing typical transfer learning methods, might help these systems to maintain high performance in different tasks, but with a reduced training effort. In this paper, we propose Contrastive Inhibitory Adaptati On (CIAO), a mechanism that adapts the last layer of facial encoders to depict specific affective characteristics on different datasets. CIAO presents an improvement in facial expression recognition performance over six different datasets with very unique affective representations, in particular when compared with state-of-the-art models. In our discussions, we make an in-depth analysis of how the learned high-level facial features are represented, and how they contribute to each individual dataset's characteristics. We finalize our study by discussing how CIAO positions itself within the range of recent findings on non-universal facial expressions perception, and its impact on facial expression recognition research.



### Determining HEDP Foams' Quality with Multi-View Deep Learning Classification
- **Arxiv ID**: http://arxiv.org/abs/2208.07196v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2208.07196v1)
- **Published**: 2022-08-10 16:41:57+00:00
- **Updated**: 2022-08-10 16:41:57+00:00
- **Authors**: Nadav Schneider, Matan Rusanovsky, Raz Gvishi, Gal Oren
- **Comment**: None
- **Journal**: None
- **Summary**: High energy density physics (HEDP) experiments commonly involve a dynamic wave-front propagating inside a low-density foam. This effect affects its density and hence, its transparency. A common problem in foam production is the creation of defective foams. Accurate information on their dimension and homogeneity is required to classify the foams' quality. Therefore, those parameters are being characterized using a 3D-measuring laser confocal microscope. For each foam, five images are taken: two 2D images representing the top and bottom surface foam planes and three images of side cross-sections from 3D scannings. An expert has to do the complicated, harsh, and exhausting work of manually classifying the foam's quality through the image set and only then determine whether the foam can be used in experiments or not. Currently, quality has two binary levels of normal vs. defective. At the same time, experts are commonly required to classify a sub-class of normal-defective, i.e., foams that are defective but might be sufficient for the needed experiment. This sub-class is problematic due to inconclusive judgment that is primarily intuitive. In this work, we present a novel state-of-the-art multi-view deep learning classification model that mimics the physicist's perspective by automatically determining the foams' quality classification and thus aids the expert. Our model achieved 86\% accuracy on upper and lower surface foam planes and 82\% on the entire set, suggesting interesting heuristics to the problem. A significant added value in this work is the ability to regress the foam quality instead of binary deduction and even explain the decision visually. The source code used in this work, as well as other relevant sources, are available at: https://github.com/Scientific-Computing-Lab-NRCN/Multi-View-Foams.git



### Detecting COVID-19 from digitized ECG printouts using 1D convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2208.05433v2
- **DOI**: 10.1371/journal.pone.0277081
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.05433v2)
- **Published**: 2022-08-10 16:44:28+00:00
- **Updated**: 2022-10-05 15:05:23+00:00
- **Authors**: Thao Nguyen, Hieu H. Pham, Huy Khiem Le, Anh Tu Nguyen, Ngoc Tien Thanh, Cuong Do
- **Comment**: Accepted with minor revision by Plos One
- **Journal**: None
- **Summary**: The COVID-19 pandemic has exposed the vulnerability of healthcare services worldwide, raising the need to develop novel tools to provide rapid and cost-effective screening and diagnosis. Clinical reports indicated that COVID-19 infection may cause cardiac injury, and electrocardiograms (ECG) may serve as a diagnostic biomarker for COVID-19. This study aims to utilize ECG signals to detect COVID-19 automatically. We propose a novel method to extract ECG signals from ECG paper records, which are then fed into a one-dimensional convolution neural network (1D-CNN) to learn and diagnose the disease. To evaluate the quality of digitized signals, R peaks in the paper-based ECG images are labeled. Afterward, RR intervals calculated from each image are compared to RR intervals of the corresponding digitized signal. Experiments on the COVID-19 ECG images dataset demonstrate that the proposed digitization method is able to capture correctly the original signals, with a mean absolute error of 28.11 ms. Our proposed 1D-CNN model, which is trained on the digitized ECG signals, allows identifying individuals with COVID-19 and other subjects accurately, with classification accuracies of 98.42%, 95.63%, and 98.50% for classifying COVID-19 vs. Normal, COVID-19 vs. Abnormal Heartbeats, and COVID-19 vs. other classes, respectively. Furthermore, the proposed method also achieves a high-level of performance for the multi-classification task. Our findings indicate that a deep learning system trained on digitized ECG signals can serve as a potential tool for diagnosing COVID-19.



### EvolveHypergraph: Group-Aware Dynamic Relational Reasoning for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2208.05470v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2208.05470v1)
- **Published**: 2022-08-10 17:57:10+00:00
- **Updated**: 2022-08-10 17:57:10+00:00
- **Authors**: Jiachen Li, Chuanbo Hua, Jinkyoo Park, Hengbo Ma, Victoria Dax, Mykel J. Kochenderfer
- **Comment**: None
- **Journal**: None
- **Summary**: While the modeling of pair-wise relations has been widely studied in multi-agent interacting systems, its ability to capture higher-level and larger-scale group-wise activities is limited. In this paper, we propose a group-aware relational reasoning approach (named EvolveHypergraph) with explicit inference of the underlying dynamically evolving relational structures, and we demonstrate its effectiveness for multi-agent trajectory prediction. In addition to the edges between a pair of nodes (i.e., agents), we propose to infer hyperedges that adaptively connect multiple nodes to enable group-aware relational reasoning in an unsupervised manner without fixing the number of hyperedges. The proposed approach infers the dynamically evolving relation graphs and hypergraphs over time to capture the evolution of relations, which are used by the trajectory predictor to obtain future states. Moreover, we propose to regularize the smoothness of the relation evolution and the sparsity of the inferred graphs or hypergraphs, which effectively improves training stability and enhances the explainability of inferred relations. The proposed approach is validated on both synthetic crowd simulations and multiple real-world benchmark datasets. Our approach infers explainable, reasonable group-aware relations and achieves state-of-the-art performance in long-term prediction.



### Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP
- **Arxiv ID**: http://arxiv.org/abs/2208.05516v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.05516v4)
- **Published**: 2022-08-10 18:24:23+00:00
- **Updated**: 2023-02-01 03:18:41+00:00
- **Authors**: Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, Ludwig Schmidt
- **Comment**: Oral paper at NeurIPS 2022
- **Journal**: None
- **Summary**: Web-crawled datasets have enabled remarkable generalization capabilities in recent image-text models such as CLIP (Contrastive Language-Image pre-training) or Flamingo, but little is known about the dataset creation processes. In this work, we introduce a testbed of six publicly available data sources - YFCC, LAION, Conceptual Captions, WIT, RedCaps, Shutterstock - to investigate how pre-training distributions induce robustness in CLIP. We find that the performance of the pre-training data varies substantially across distribution shifts, with no single data source dominating. Moreover, we systematically study the interactions between these data sources and find that combining multiple sources does not necessarily yield better models, but rather dilutes the robustness of the best individual data source. We complement our empirical findings with theoretical insights from a simple setting, where combining the training data also results in diluted robustness. In addition, our theoretical model provides a candidate explanation for the success of the CLIP-based data filtering technique recently employed in the LAION dataset. Overall our results demonstrate that simply gathering a large amount of data from the web is not the most effective way to build a pre-training dataset for robust generalization, necessitating further study into dataset design. Code is available at https://github.com/mlfoundations/clip_quality_not_quantity.



### Semi-supervised segmentation of tooth from 3D Scanned Dental Arches
- **Arxiv ID**: http://arxiv.org/abs/2208.05539v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.05539v1)
- **Published**: 2022-08-10 19:56:47+00:00
- **Updated**: 2022-08-10 19:56:47+00:00
- **Authors**: Ammar Alsheghri, Farnoosh Ghadiri, Ying Zhang, Olivier Lessard, Julia Keren, Farida Cheriet, Francois Guibault
- **Comment**: None
- **Journal**: In Medical Imaging 2022: Image Processing (Vol. 12032, pp.
  766-771). SPIE 2022
- **Summary**: Teeth segmentation is an important topic in dental restorations that is essential for crown generation, diagnosis, and treatment planning. In the dental field, the variability of input data is high and there are no publicly available 3D dental arch datasets. Although there has been improvement in the field provided by recent deep learning architectures on 3D data, there still exists some problems such as properly identifying missing teeth in an arch. We propose to use spectral clustering as a self-supervisory signal to joint-train neural networks for segmentation of 3D arches. Our approach is motivated by the observation that K-means clustering provides cues to capture margin lines related to human perception. The main idea is to automatically generate training data by decomposing unlabeled 3D arches into segments relying solely on geometric information. The network is then trained using a joint loss that combines a supervised loss of annotated input and a self-supervised loss of non-labeled input. Our collected data has a variety of arches including arches with missing teeth. Our experimental results show improvement over the fully supervised state-of-the-art MeshSegNet when using semi-supervised learning. Finally, we contribute code and a dataset.



### Towards Automating Retinoscopy for Refractive Error Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2208.05552v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.05552v1)
- **Published**: 2022-08-10 20:32:13+00:00
- **Updated**: 2022-08-10 20:32:13+00:00
- **Authors**: Aditya Aggarwal, Siddhartha Gairola, Uddeshya Upadhyay, Akshay P Vasishta, Diwakar Rao, Aditya Goyal, Kaushik Murali, Nipun Kwatra, Mohit Jain
- **Comment**: This paper is accepted for publication in IMWUT 2022
- **Journal**: None
- **Summary**: Refractive error is the most common eye disorder and is the key cause behind correctable visual impairment, responsible for nearly 80% of the visual impairment in the US. Refractive error can be diagnosed using multiple methods, including subjective refraction, retinoscopy, and autorefractors. Although subjective refraction is the gold standard, it requires cooperation from the patient and hence is not suitable for infants, young children, and developmentally delayed adults. Retinoscopy is an objective refraction method that does not require any input from the patient. However, retinoscopy requires a lens kit and a trained examiner, which limits its use for mass screening. In this work, we automate retinoscopy by attaching a smartphone to a retinoscope and recording retinoscopic videos with the patient wearing a custom pair of paper frames. We develop a video processing pipeline that takes retinoscopic videos as input and estimates the net refractive error based on our proposed extension of the retinoscopy mathematical model. Our system alleviates the need for a lens kit and can be performed by an untrained examiner. In a clinical trial with 185 eyes, we achieved a sensitivity of 91.0% and specificity of 74.0% on refractive error diagnosis. Moreover, the mean absolute error of our approach was 0.75$\pm$0.67D on net refractive error estimation compared to subjective refraction measurements. Our results indicate that our approach has the potential to be used as a retinoscopy-based refractive error screening tool in real-world medical settings.



### Patching open-vocabulary models by interpolating weights
- **Arxiv ID**: http://arxiv.org/abs/2208.05592v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.05592v2)
- **Published**: 2022-08-10 23:47:43+00:00
- **Updated**: 2022-10-11 22:33:14+00:00
- **Authors**: Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, Ludwig Schmidt
- **Comment**: 36th Conference on Neural Information Processing Systems (NeurIPS
  2022)
- **Journal**: None
- **Summary**: Open-vocabulary models like CLIP achieve high accuracy across many image classification tasks. However, there are still settings where their zero-shot performance is far from optimal. We study model patching, where the goal is to improve accuracy on specific tasks without degrading accuracy on tasks where performance is already adequate. Towards this goal, we introduce PAINT, a patching method that uses interpolations between the weights of a model before fine-tuning and the weights after fine-tuning on a task to be patched. On nine tasks where zero-shot CLIP performs poorly, PAINT increases accuracy by 15 to 60 percentage points while preserving accuracy on ImageNet within one percentage point of the zero-shot model. PAINT also allows a single model to be patched on multiple tasks and improves with model scale. Furthermore, we identify cases of broad transfer, where patching on one task increases accuracy on other tasks even when the tasks have disjoint classes. Finally, we investigate applications beyond common benchmarks such as counting or reducing the impact of typographic attacks on CLIP. Our findings demonstrate that it is possible to expand the set of tasks on which open-vocabulary models achieve high accuracy without re-training them from scratch.



### Evaluating the Quality and Diversity of DCGAN-based Generatively Synthesized Diabetic Retinopathy Imagery
- **Arxiv ID**: http://arxiv.org/abs/2208.05593v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.05593v3)
- **Published**: 2022-08-10 23:50:01+00:00
- **Updated**: 2023-08-30 12:39:29+00:00
- **Authors**: Cristina-Madalina Dragan, Muhammad Muneeb Saad, Mubashir Husain Rehmani, Ruairi O'Reilly
- **Comment**: 29 Pages, 8 Figures, submitted to MEDAL23: Advances in Deep
  Generative Models for Medical Artificial Intelligence (Springer Nature
  series)
- **Journal**: None
- **Summary**: Publicly available diabetic retinopathy (DR) datasets are imbalanced, containing limited numbers of images with DR. This imbalance contributes to overfitting when training machine learning classifiers. The impact of this imbalance is exacerbated as the severity of the DR stage increases, affecting the classifiers' diagnostic capacity. The imbalance can be addressed using Generative Adversarial Networks (GANs) to augment the datasets with synthetic images. Generating synthetic images is advantageous if high-quality and diversified images are produced. To evaluate the quality and diversity of synthetic images, several evaluation metrics, such as Multi-Scale Structural Similarity Index (MS-SSIM), Cosine Distance (CD), and Fr\'echet Inception Distance (FID) are used. Understanding the effectiveness of each metric in evaluating the quality and diversity of GAN-based synthetic images is critical to select images for augmentation. To date, there has been limited analysis of the appropriateness of these metrics in the context of biomedical imagery. This work contributes an empirical assessment of these evaluation metrics as applied to synthetic Proliferative DR imagery generated by a Deep Convolutional GAN (DCGAN). Furthermore, the metrics' capacity to indicate the quality and diversity of synthetic images and a correlation with classifier performance is undertaken. This enables a quantitative selection of synthetic imagery and an informed augmentation strategy. Results indicate that FID is suitable for evaluating the quality, while MS-SSIM and CD are suitable for evaluating the diversity of synthetic imagery. Furthermore, the superior performance of Convolutional Neural Network (CNN) and EfficientNet classifiers, as indicated by the F1 and AUC scores, for the augmented datasets demonstrates the efficacy of synthetic imagery to augment the imbalanced dataset.



