# Arxiv Papers in cs.CV on 2022-06-20
### DualCoOp: Fast Adaptation to Multi-Label Recognition with Limited Annotations
- **Arxiv ID**: http://arxiv.org/abs/2206.09541v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09541v1)
- **Published**: 2022-06-20 02:36:54+00:00
- **Updated**: 2022-06-20 02:36:54+00:00
- **Authors**: Ximeng Sun, Ping Hu, Kate Saenko
- **Comment**: None
- **Journal**: None
- **Summary**: Solving multi-label recognition (MLR) for images in the low-label regime is a challenging task with many real-world applications. Recent work learns an alignment between textual and visual spaces to compensate for insufficient image labels, but loses accuracy because of the limited amount of available MLR annotations. In this work, we utilize the strong alignment of textual and visual features pretrained with millions of auxiliary image-text pairs and propose Dual Context Optimization (DualCoOp) as a unified framework for partial-label MLR and zero-shot MLR. DualCoOp encodes positive and negative contexts with class names as part of the linguistic input (i.e. prompts). Since DualCoOp only introduces a very light learnable overhead upon the pretrained vision-language framework, it can quickly adapt to multi-label recognition tasks that have limited annotations and even unseen classes. Experiments on standard multi-label recognition benchmarks across two challenging low-label settings demonstrate the advantages of our approach over state-of-the-art methods.



### Variational Distillation for Multi-View Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.09548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09548v1)
- **Published**: 2022-06-20 03:09:46+00:00
- **Updated**: 2022-06-20 03:09:46+00:00
- **Authors**: Xudong Tian, Zhizhong Zhang, Cong Wang, Wensheng Zhang, Yanyun Qu, Lizhuang Ma, Zongze Wu, Yuan Xie, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Information Bottleneck (IB) based multi-view learning provides an information theoretic principle for seeking shared information contained in heterogeneous data descriptions. However, its great success is generally attributed to estimate the multivariate mutual information which is intractable when the network becomes complicated. Moreover, the representation learning tradeoff, {\it i.e.}, prediction-compression and sufficiency-consistency tradeoff, makes the IB hard to satisfy both requirements simultaneously. In this paper, we design several variational information bottlenecks to exploit two key characteristics ({\it i.e.}, sufficiency and consistency) for multi-view representation learning. Specifically, we propose a Multi-View Variational Distillation (MV$^2$D) strategy to provide a scalable, flexible and analytical solution to fitting MI by giving arbitrary input of viewpoints but without explicitly estimating it. Under rigorously theoretical guarantee, our approach enables IB to grasp the intrinsic correlation between observations and semantic labels, producing predictive and compact representations naturally. Also, our information-theoretic constraint can effectively neutralize the sensitivity to heterogeneous data by eliminating both task-irrelevant and view-specific information, preventing both tradeoffs in multiple view cases. To verify our theoretically grounded strategies, we apply our approaches to various benchmarks under three different applications. Extensive experiments to quantitatively and qualitatively demonstrate the effectiveness of our approach against state-of-the-art methods.



### Dynamic Message Propagation Network for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.09552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09552v1)
- **Published**: 2022-06-20 03:27:48+00:00
- **Updated**: 2022-06-20 03:27:48+00:00
- **Authors**: Baian Chen, Zhilei Chen, Xiaowei Hu, Jun Xu, Haoran Xie, Mingqiang Wei, Jing Qin
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: This paper presents a novel deep neural network framework for RGB-D salient object detection by controlling the message passing between the RGB images and depth maps on the feature level and exploring the long-range semantic contexts and geometric information on both RGB and depth features to infer salient objects. To achieve this, we formulate a dynamic message propagation (DMP) module with the graph neural networks and deformable convolutions to dynamically learn the context information and to automatically predict filter weights and affinity matrices for message propagation control. We further embed this module into a Siamese-based network to process the RGB image and depth map respectively and design a multi-level feature fusion (MFF) module to explore the cross-level information between the refined RGB and depth features. Compared with 17 state-of-the-art methods on six benchmark datasets for RGB-D salient object detection, experimental results show that our method outperforms all the others, both quantitatively and visually.



### Capturing and Inferring Dense Full-Body Human-Scene Contact
- **Arxiv ID**: http://arxiv.org/abs/2206.09553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09553v1)
- **Published**: 2022-06-20 03:31:00+00:00
- **Updated**: 2022-06-20 03:31:00+00:00
- **Authors**: Chun-Hao P. Huang, Hongwei Yi, Markus HÃ¶schle, Matvey Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel Scharstein, Michael J. Black
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for "Real scenes, Interaction, Contact and Humans." RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body. Using RICH, we train a network that predicts dense body-scene contacts from a single RGB image. Our key insight is that regions in contact are always occluded so the network needs the ability to explore the whole image for evidence. We use a transformer to learn such non-local relationships and propose a new Body-Scene contact TRansfOrmer (BSTRO). Very few methods explore 3D contact; those that do focus on the feet only, detect foot contact as a post-processing step, or infer contact from body pose without looking at the scene. To our knowledge, BSTRO is the first method to directly estimate 3D body-scene contact from a single image. We demonstrate that BSTRO significantly outperforms the prior art. The code and dataset are available at https://rich.is.tue.mpg.de.



### Saliency Guided Inter- and Intra-Class Relation Constraints for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.09554v1
- **DOI**: 10.1109/TMM.2022.3157481
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09554v1)
- **Published**: 2022-06-20 03:40:56+00:00
- **Updated**: 2022-06-20 03:40:56+00:00
- **Authors**: Tao Chen, Yazhou Yao, Lei Zhang, Qiong Wang, Guo-Sen Xie, Fumin Shen
- **Comment**: TMM2022, 11 pages, 5 figures
- **Journal**: None
- **Summary**: Weakly supervised semantic segmentation with only image-level labels aims to reduce annotation costs for the segmentation task. Existing approaches generally leverage class activation maps (CAMs) to locate the object regions for pseudo label generation. However, CAMs can only discover the most discriminative parts of objects, thus leading to inferior pixel-level pseudo labels. To address this issue, we propose a saliency guided Inter- and Intra-Class Relation Constrained (I$^2$CRC) framework to assist the expansion of the activated object regions in CAMs. Specifically, we propose a saliency guided class-agnostic distance module to pull the intra-category features closer by aligning features to their class prototypes. Further, we propose a class-specific distance module to push the inter-class features apart and encourage the object region to have a higher activation than the background. Besides strengthening the capability of the classification network to activate more integral object regions in CAMs, we also introduce an object guided label refinement module to take a full use of both the segmentation prediction and the initial labels for obtaining superior pseudo-labels. Extensive experiments on PASCAL VOC 2012 and COCO datasets demonstrate well the effectiveness of I$^2$CRC over other state-of-the-art counterparts. The source codes, models, and data have been made available at \url{https://github.com/NUST-Machine-Intelligence-Laboratory/I2CRC}.



### A Novel Long-term Iterative Mining Scheme for Video Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.09564v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09564v1)
- **Published**: 2022-06-20 04:27:47+00:00
- **Updated**: 2022-06-20 04:27:47+00:00
- **Authors**: Chenglizhao Chen, Hengsen Wang, Yuming Fang, Chong Peng
- **Comment**: None
- **Journal**: None
- **Summary**: The existing state-of-the-art (SOTA) video salient object detection (VSOD) models have widely followed short-term methodology, which dynamically determines the balance between spatial and temporal saliency fusion by solely considering the current consecutive limited frames. However, the short-term methodology has one critical limitation, which conflicts with the real mechanism of our visual system -- a typical long-term methodology. As a result, failure cases keep showing up in the results of the current SOTA models, and the short-term methodology becomes the major technical bottleneck. To solve this problem, this paper proposes a novel VSOD approach, which performs VSOD in a complete long-term way. Our approach converts the sequential VSOD, a sequential task, to a data mining problem, i.e., decomposing the input video sequence to object proposals in advance and then mining salient object proposals as much as possible in an easy-to-hard way. Since all object proposals are simultaneously available, the proposed approach is a complete long-term approach, which can alleviate some difficulties rooted in conventional short-term approaches. In addition, we devised an online updating scheme that can grasp the most representative and trustworthy pattern profile of the salient objects, outputting framewise saliency maps with rich details and smoothing both spatially and temporally. The proposed approach outperforms almost all SOTA models on five widely used benchmark datasets.



### Guardian Angel: A Novel Walking Aid for the Visually Impaired
- **Arxiv ID**: http://arxiv.org/abs/2206.09570v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09570v1)
- **Published**: 2022-06-20 04:57:40+00:00
- **Updated**: 2022-06-20 04:57:40+00:00
- **Authors**: Ko-Wei Tai, HuaYen Lee, Hsin-Huei Chen, Jeng-Sheng Yeh, Ming Ouhyoung
- **Comment**: 2 pages, 1 figure
- **Journal**: None
- **Summary**: This work introduces Guardian Angel, an Android App that assists visually impaired people to avoid danger in complex traffic environment. The system, consisting of object detection by pretrained YOLO model, distance estimation and moving direction estimation, provides information about surrounding vehicles and alarms users of potential danger without expensive special purpose device. With an experiment of 8 subjects, we corroborate that in terms of satisfaction score in pedestrian-crossing experiment with the assistance of our App using a smartphone is better than when without under 99% confidence level. The time needed to cross a road is shorter on average with the assistance of our system, however, not reaching significant difference by our experiment. The App has been released in Google Play Store, open to the public for free.



### C-SENN: Contrastive Self-Explaining Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2206.09575v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09575v2)
- **Published**: 2022-06-20 05:23:02+00:00
- **Updated**: 2022-06-27 01:48:08+00:00
- **Authors**: Yoshihide Sawada, Keigo Nakamura
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In this study, we use a self-explaining neural network (SENN), which learns unsupervised concepts, to acquire concepts that are easy for people to understand automatically. In concept learning, the hidden layer retains verbalizable features relevant to the output, which is crucial when adapting to real-world environments where explanations are required. However, it is known that the interpretability of concepts output by SENN is reduced in general settings, such as autonomous driving scenarios. Thus, this study combines contrastive learning with concept learning to improve the readability of concepts and the accuracy of tasks. We call this model Contrastive Self-Explaining Neural Network (C-SENN).



### Explicit and implicit models in infrared and visible image fusion
- **Arxiv ID**: http://arxiv.org/abs/2206.09581v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09581v1)
- **Published**: 2022-06-20 06:05:09+00:00
- **Updated**: 2022-06-20 06:05:09+00:00
- **Authors**: Zixuan Wang, Bin Sun
- **Comment**: 8 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: Infrared and visible images, as multi-modal image pairs, show significant differences in the expression of the same scene. The image fusion task is faced with two problems: one is to maintain the unique features between different modalities, and the other is to maintain features at various levels like local and global features. This paper discusses the limitations of deep learning models in image fusion and the corresponding optimization strategies. Based on artificially designed structures and constraints, we divide models into explicit models, and implicit models that adaptively learn high-level features or can establish global pixel associations. Ten models for comparison experiments on 21 test sets were screened. The qualitative and quantitative results show that the implicit models have more comprehensive ability to learn image features. At the same time, the stability of them needs to be improved. Aiming at the advantages and limitations to be solved by existing algorithms, we discuss the main problems of multi-modal image fusion and future research directions.



### 5th Place Solution for YouTube-VOS Challenge 2022: Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.09585v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.09585v1)
- **Published**: 2022-06-20 06:14:27+00:00
- **Updated**: 2022-06-20 06:14:27+00:00
- **Authors**: Wangwang Yang, Jinming Su, Yiting Duan, Tingyi Guo, Junfeng Luo
- **Comment**: 5th Place Solution for Video Object Segmentation in the 4th
  Large-scale Video Object Segmentation Challenge, CVPR 2022 Workshop
- **Journal**: None
- **Summary**: Video object segmentation (VOS) has made significant progress with the rise of deep learning. However, there still exist some thorny problems, for example, similar objects are easily confused and tiny objects are difficult to be found. To solve these problems and further improve the performance of VOS, we propose a simple yet effective solution for this task. In the solution, we first analyze the distribution of the Youtube-VOS dataset and supplement the dataset by introducing public static and video segmentation datasets. Then, we improve three network architectures with different characteristics and train several networks to learn the different characteristics of objects in videos. After that, we use a simple way to integrate all results to ensure that different models complement each other. Finally, subtle post-processing is carried out to ensure accurate video object segmentation with precise boundaries. Extensive experiments on Youtube-VOS dataset show that the proposed solution achieves the state-of-the-art performance with an 86.1% overall score on the YouTube-VOS 2022 test set, which is 5th place on the video object segmentation track of the Youtube-VOS Challenge 2022.



### DALL-E for Detection: Language-driven Compositional Image Synthesis for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.09592v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09592v3)
- **Published**: 2022-06-20 06:43:17+00:00
- **Updated**: 2022-12-22 00:55:29+00:00
- **Authors**: Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Neel Joshi, Laurent Itti, Vibhav Vineet
- **Comment**: v3(same as v2) version, update structure (add foreground generation,
  stable diffusion), add more experiments
- **Journal**: None
- **Summary**: We propose a new paradigm to automatically generate training data with accurate labels at scale using the text-toimage synthesis frameworks (e.g., DALL-E, Stable Diffusion, etc.). The proposed approach decouples training data generation into foreground object mask generation and background (context) image generation. For foreground object mask generation, we use a simple textual template with object class name as input to DALL-E to generate a diverse set of foreground images. A foreground-background segmentation algorithm is then used to generate foreground object masks. Next, in order to generate context images, first a language description of the context is generated by applying an image captioning method on a small set of images representing the context. These language descriptions are then used to generate diverse sets of context images using the DALL-E framework. These are then composited with object masks generated in the first step to provide an augmented training set for a classifier. We demonstrate the advantages of our approach on four object detection datasets including on Pascal VOC and COCO object detection tasks. Furthermore, we also highlight the compositional nature of our data generation approach on out-of-distribution and zero-shot data generation scenarios.



### Efficient and Flexible Sublabel-Accurate Energy Minimization
- **Arxiv ID**: http://arxiv.org/abs/2206.09596v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2206.09596v1)
- **Published**: 2022-06-20 06:58:55+00:00
- **Updated**: 2022-06-20 06:58:55+00:00
- **Authors**: Zhakshylyk Nurlanov, Daniel Cremers, Florian Bernard
- **Comment**: To be published at ICPR 2022, Copyright 2022 IEEE
- **Journal**: None
- **Summary**: We address the problem of minimizing a class of energy functions consisting of data and smoothness terms that commonly occur in machine learning, computer vision, and pattern recognition. While discrete optimization methods are able to give theoretical optimality guarantees, they can only handle a finite number of labels and therefore suffer from label discretization bias. Existing continuous optimization methods can find sublabel-accurate solutions, but they are not efficient for large label spaces. In this work, we propose an efficient sublabel-accurate method that utilizes the best properties of both continuous and discrete models. We separate the problem into two sequential steps: (i) global discrete optimization for selecting the label range, and (ii) efficient continuous sublabel-accurate local refinement of a convex approximation of the energy function in the chosen range. Doing so allows us to achieve a boost in time and memory efficiency while practically keeping the accuracy at the same level as continuous convex relaxation methods, and in addition, providing theoretical optimality guarantees at the level of discrete methods. Finally, we show the flexibility of the proposed approach to general pairwise smoothness terms, so that it is applicable to a wide range of regularizations. Experiments on the illustrating example of the image denoising problem demonstrate the properties of the proposed method. The code reproducing experiments is available at \url{https://github.com/nurlanov-zh/sublabel-accurate-alpha-expansion}.



### Winning the CVPR'2022 AQTC Challenge: A Two-stage Function-centric Approach
- **Arxiv ID**: http://arxiv.org/abs/2206.09597v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2206.09597v2)
- **Published**: 2022-06-20 07:02:23+00:00
- **Updated**: 2022-06-22 13:07:41+00:00
- **Authors**: Shiwei Wu, Weidong He, Tong Xu, Hao Wang, Enhong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Affordance-centric Question-driven Task Completion for Egocentric Assistant(AQTC) is a novel task which helps AI assistant learn from instructional videos and scripts and guide the user step-by-step. In this paper, we deal with the AQTC via a two-stage Function-centric approach, which consists of Question2Function Module to ground the question with the related function and Function2Answer Module to predict the action based on the historical steps. We evaluated several possible solutions in each module and obtained significant gains compared to the given baselines. Our code is available at \url{https://github.com/starsholic/LOVEU-CVPR22-AQTC}.



### Distortion-Aware Network Pruning and Feature Reuse for Real-time Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.09604v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.09604v2)
- **Published**: 2022-06-20 07:20:02+00:00
- **Updated**: 2022-12-15 07:21:01+00:00
- **Authors**: Hyunsu Rhee, Dongchan Min, Sunil Hwang, Bruno Andreis, Sung Ju Hwang
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time video segmentation is a crucial task for many real-world applications such as autonomous driving and robot control. Since state-of-the-art semantic segmentation models are often too heavy for real-time applications despite their impressive performance, researchers have proposed lightweight architectures with speed-accuracy trade-offs, achieving real-time speed at the expense of reduced accuracy. In this paper, we propose a novel framework to speed up any architecture with skip-connections for real-time vision tasks by exploiting the temporal locality in videos. Specifically, at the arrival of each frame, we transform the features from the previous frame to reuse them at specific spatial bins. We then perform partial computation of the backbone network on the regions of the current frame that captures temporal differences between the current and previous frame. This is done by dynamically dropping out residual blocks using a gating mechanism which decides which blocks to drop based on inter-frame distortion. We validate our Spatial-Temporal Mask Generator (STMG) on video semantic segmentation benchmarks with multiple backbone networks, and show that our method largely speeds up inference with minimal loss of accuracy.



### A Comprehensive Survey on Video Saliency Detection with Auditory Information: the Audio-visual Consistency Perceptual is the Key!
- **Arxiv ID**: http://arxiv.org/abs/2206.13390v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2206.13390v1)
- **Published**: 2022-06-20 07:25:13+00:00
- **Updated**: 2022-06-20 07:25:13+00:00
- **Authors**: Chenglizhao Chen, Mengke Song, Wenfeng Song, Li Guo, Muwei Jian
- **Comment**: None
- **Journal**: None
- **Summary**: Video saliency detection (VSD) aims at fast locating the most attractive objects/things/patterns in a given video clip. Existing VSD-related works have mainly relied on the visual system but paid less attention to the audio aspect, while, actually, our audio system is the most vital complementary part to our visual system. Also, audio-visual saliency detection (AVSD), one of the most representative research topics for mimicking human perceptual mechanisms, is currently in its infancy, and none of the existing survey papers have touched on it, especially from the perspective of saliency detection. Thus, the ultimate goal of this paper is to provide an extensive review to bridge the gap between audio-visual fusion and saliency detection. In addition, as another highlight of this review, we have provided a deep insight into key factors which could directly determine the performances of AVSD deep models, and we claim that the audio-visual consistency degree (AVC) -- a long-overlooked issue, can directly influence the effectiveness of using audio to benefit its visual counterpart when performing saliency detection. Moreover, in order to make the AVC issue more practical and valuable for future followers, we have newly equipped almost all existing publicly available AVSD datasets with additional frame-wise AVC labels. Based on these upgraded datasets, we have conducted extensive quantitative evaluations to ground our claim on the importance of AVC in the AVSD task. In a word, both our ideas and new sets serve as a convenient platform with preliminaries and guidelines, all of which are very potential to facilitate future works in promoting state-of-the-art (SOTA) performance further.



### SJ-HD^2R: Selective Joint High Dynamic Range and Denoising Imaging for Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2206.09611v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09611v2)
- **Published**: 2022-06-20 07:49:56+00:00
- **Updated**: 2022-11-03 18:43:30+00:00
- **Authors**: Wei Li, Shuai Xiao, Tianhong Dai, Shanxin Yuan, Tao Wang, Cheng Li, Fenglong Song
- **Comment**: None
- **Journal**: None
- **Summary**: Ghosting artifacts, motion blur, and low fidelity in highlight are the main challenges in High Dynamic Range (HDR) imaging from multiple Low Dynamic Range (LDR) images. These issues come from using the medium-exposed image as the reference frame in previous methods. To deal with them, we propose to use the under-exposed image as the reference to avoid these issues. However, the heavy noise in dark regions of the under-exposed image becomes a new problem. Therefore, we propose a joint HDR and denoising pipeline, containing two sub-networks: (i) a pre-denoising network (PreDNNet) to adaptively denoise input LDRs by exploiting exposure priors; (ii) a pyramid cascading fusion network (PCFNet), introducing an attention mechanism and cascading structure in a multi-scale manner. To further leverage these two paradigms, we propose a selective and joint HDR and denoising (SJ-HD$^2$R) imaging framework, utilizing scenario-specific priors to conduct the path selection with an accuracy of more than 93.3$\%$. We create the first joint HDR and denoising benchmark dataset, which contains a variety of challenging HDR and denoising scenes and supports the switching of the reference image. Extensive experiment results show that our method achieves superior performance to previous methods.



### Revisiting lp-constrained Softmax Loss: A Comprehensive Study
- **Arxiv ID**: http://arxiv.org/abs/2206.09616v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09616v1)
- **Published**: 2022-06-20 08:03:12+00:00
- **Updated**: 2022-06-20 08:03:12+00:00
- **Authors**: Chintan Trivedi, Konstantinos Makantasis, Antonios Liapis, Georgios N. Yannakakis
- **Comment**: None
- **Journal**: None
- **Summary**: Normalization is a vital process for any machine learning task as it controls the properties of data and affects model performance at large. The impact of particular forms of normalization, however, has so far been investigated in limited domain-specific classification tasks and not in a general fashion. Motivated by the lack of such a comprehensive study, in this paper we investigate the performance of lp-constrained softmax loss classifiers across different norm orders, magnitudes, and data dimensions in both proof-of-concept classification problems and real-world popular image classification tasks. Experimental results suggest collectively that lp-constrained softmax loss classifiers not only can achieve more accurate classification results but, at the same time, appear to be less prone to overfitting. The core findings hold across the three popular deep learning architectures tested and eight datasets examined, and suggest that lp normalization is a recommended data representation practice for image classification in terms of performance and convergence, and against overfitting.



### Diversified Adversarial Attacks based on Conjugate Gradient Method
- **Arxiv ID**: http://arxiv.org/abs/2206.09628v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09628v2)
- **Published**: 2022-06-20 08:24:02+00:00
- **Updated**: 2022-07-19 23:59:31+00:00
- **Authors**: Keiichiro Yamamura, Haruki Sato, Nariaki Tateiwa, Nozomi Hata, Toru Mitsutake, Issa Oe, Hiroki Ishikura, Katsuki Fujisawa
- **Comment**: Proceedings of the 39th International Conference on Machine Learning
  (ICML 2022)
- **Journal**: None
- **Summary**: Deep learning models are vulnerable to adversarial examples, and adversarial attacks used to generate such examples have attracted considerable research interest. Although existing methods based on the steepest descent have achieved high attack success rates, ill-conditioned problems occasionally reduce their performance. To address this limitation, we utilize the conjugate gradient (CG) method, which is effective for this type of problem, and propose a novel attack algorithm inspired by the CG method, named the Auto Conjugate Gradient (ACG) attack. The results of large-scale evaluation experiments conducted on the latest robust models show that, for most models, ACG was able to find more adversarial examples with fewer iterations than the existing SOTA algorithm Auto-PGD (APGD). We investigated the difference in search performance between ACG and APGD in terms of diversification and intensification, and define a measure called Diversity Index (DI) to quantify the degree of diversity. From the analysis of the diversity using this index, we show that the more diverse search of the proposed method remarkably improves its attack success rate.



### What Can be Seen is What You Get: Structure Aware Point Cloud Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.09664v1
- **DOI**: 10.1109/IV51971.2022.9827116
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09664v1)
- **Published**: 2022-06-20 09:10:59+00:00
- **Updated**: 2022-06-20 09:10:59+00:00
- **Authors**: Frederik Hasecke, Martin Alsfasser, Anton Kummert
- **Comment**: Published in IEEE IV 2022
- **Journal**: 33rd IEEE Intelligent Vehicles Symposium, Aachen, Germany, June
  5th - June 9th 2022
- **Summary**: To train a well performing neural network for semantic segmentation, it is crucial to have a large dataset with available ground truth for the network to generalize on unseen data. In this paper we present novel point cloud augmentation methods to artificially diversify a dataset. Our sensor-centric methods keep the data structure consistent with the lidar sensor capabilities. Due to these new methods, we are able to enrich low-value data with high-value instances, as well as create entirely new scenes. We validate our methods on multiple neural networks with the public SemanticKITTI dataset and demonstrate that all networks improve compared to their respective baseline. In addition, we show that our methods enable the use of very small datasets, saving annotation time, training time and the associated costs.



### MSANet: Multi-Similarity and Attention Guidance for Boosting Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.09667v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09667v1)
- **Published**: 2022-06-20 09:14:17+00:00
- **Updated**: 2022-06-20 09:14:17+00:00
- **Authors**: Ehtesham Iqbal, Sirojbek Safarov, Seongdeok Bang
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot segmentation aims to segment unseen-class objects given only a handful of densely labeled samples. Prototype learning, where the support feature yields a singleor several prototypes by averaging global and local object information, has been widely used in FSS. However, utilizing only prototype vectors may be insufficient to represent the features for all training data. To extract abundant features and make more precise predictions, we propose a Multi-Similarity and Attention Network (MSANet) including two novel modules, a multi-similarity module and an attention module. The multi-similarity module exploits multiple feature-maps of support images and query images to estimate accurate semantic relationships. The attention module instructs the network to concentrate on class-relevant information. The network is tested on standard FSS datasets, PASCAL-5i 1-shot, PASCAL-5i 5-shot, COCO-20i 1-shot, and COCO-20i 5-shot. The MSANet with the backbone of ResNet-101 achieves the state-of-the-art performance for all 4-benchmark datasets with mean intersection over union (mIoU) of 69.13%, 73.99%, 51.09%, 56.80%, respectively. Code is available at https://github.com/AIVResearch/MSANet



### Deep reinforced active learning for multi-class image classification
- **Arxiv ID**: http://arxiv.org/abs/2206.13391v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13391v1)
- **Published**: 2022-06-20 09:30:55+00:00
- **Updated**: 2022-06-20 09:30:55+00:00
- **Authors**: Emma Slade, Kim M. Branson
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: High accuracy medical image classification can be limited by the costs of acquiring more data as well as the time and expertise needed to label existing images. In this paper, we apply active learning to medical image classification, a method which aims to maximise model performance on a minimal subset from a larger pool of data. We present a new active learning framework, based on deep reinforcement learning, to learn an active learning query strategy to label images based on predictions from a convolutional neural network. Our framework modifies the deep-Q network formulation, allowing us to pick data based additionally on geometric arguments in the latent space of the classifier, allowing for high accuracy multi-class classification in a batch-based active learning setting, enabling the agent to label datapoints that are both diverse and about which it is most uncertain. We apply our framework to two medical imaging datasets and compare with standard query strategies as well as the most recent reinforcement learning based active learning approach for image classification.



### Distribution Regularized Self-Supervised Learning for Domain Adaptation of Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.09683v1
- **DOI**: 10.1016/j.imavis.2022.104504
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09683v1)
- **Published**: 2022-06-20 09:52:49+00:00
- **Updated**: 2022-06-20 09:52:49+00:00
- **Authors**: Javed Iqbal, Hamza Rawal, Rehan Hafiz, Yu-Tseh Chi, Mohsen Ali
- **Comment**: Accepted for publication at Image and Vision Computing
- **Journal**: None
- **Summary**: This paper proposes a novel pixel-level distribution regularization scheme (DRSL) for self-supervised domain adaptation of semantic segmentation. In a typical setting, the classification loss forces the semantic segmentation model to greedily learn the representations that capture inter-class variations in order to determine the decision (class) boundary. Due to the domain shift, this decision boundary is unaligned in the target domain, resulting in noisy pseudo labels adversely affecting self-supervised domain adaptation. To overcome this limitation, along with capturing inter-class variation, we capture pixel-level intra-class variations through class-aware multi-modal distribution learning (MMDL). Thus, the information necessary for capturing the intra-class variations is explicitly disentangled from the information necessary for inter-class discrimination. Features captured thus are much more informative, resulting in pseudo-labels with low noise. This disentanglement allows us to perform separate alignments in discriminative space and multi-modal distribution space, using cross-entropy based self-learning for the former. For later, we propose a novel stochastic mode alignment method, by explicitly decreasing the distance between the target and source pixels that map to the same mode. The distance metric learning loss, computed over pseudo-labels and backpropagated from multi-modal modeling head, acts as the regularizer over the base network shared with the segmentation head. The results from comprehensive experiments on synthetic to real domain adaptation setups, i.e., GTA-V/SYNTHIA to Cityscapes, show that DRSL outperforms many existing approaches (a minimum margin of 2.3% and 2.5% in mIoU for SYNTHIA to Cityscapes).



### Remote Sensing Image Classification using Transfer Learning and Attention Based Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2206.13392v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13392v1)
- **Published**: 2022-06-20 10:05:38+00:00
- **Updated**: 2022-06-20 10:05:38+00:00
- **Authors**: Lam Pham, Khoa Tran, Dat Ngo, Jasmin Lampert, Alexander Schindler
- **Comment**: None
- **Journal**: None
- **Summary**: The task of remote sensing image scene classification (RSISC), which aims at classifying remote sensing images into groups of semantic categories based on their contents, has taken the important role in a wide range of applications such as urban planning, natural hazards detection, environment monitoring,vegetation mapping, or geospatial object detection. During the past years, research community focusing on RSISC task has shown significant effort to publish diverse datasets as well as propose different approaches to deal with the RSISC challenges. Recently, almost proposed RSISC systems base on deep learning models which prove powerful and outperform traditional approaches using image processing and machine learning. In this paper, we also leverage the power of deep learning technology, evaluate a variety of deep neural network architectures, indicate main factors affecting the performance of a RSISC system. Given the comprehensive analysis, we propose a deep learning based framework for RSISC, which makes use of the transfer learning technique and multihead attention scheme. The proposed deep learning framework is evaluated on the benchmark NWPU-RESISC45 dataset and achieves the best classification accuracy of 94.7% which shows competitive to the state-of-the-art systems and potential for real-life applications.



### FoR$^2$M: Recognition and Repair of Foldings in Mesh Surfaces. Application to 3D Object Degradation
- **Arxiv ID**: http://arxiv.org/abs/2206.09699v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2206.09699v1)
- **Published**: 2022-06-20 10:43:32+00:00
- **Updated**: 2022-06-20 10:43:32+00:00
- **Authors**: K. Sfikas, P. Perakis, T. Theoharis
- **Comment**: None
- **Journal**: None
- **Summary**: Triangular meshes are the most popular representations of 3D objects, but many mesh surfaces contain topological singularities that represent a challenge for displaying or further processing them properly. One such singularity is the self-intersections that may be present in mesh surfaces that have been created by a scanning procedure or by a deformation transformation, such as off-setting.   Mesh foldings comprise a special case of mesh surface self-intersections, where the faces of the 3D model intersect and become reversed, with respect to the unfolded part of the mesh surface. A novel method for the recognition and repair of mesh surface foldings is presented, which exploits the structural characteristics of the foldings in order to efficiently detect the folded regions. Following detection, the foldings are removed and any gaps so created are filled based on the geometry of the 3D model. The proposed method is directly applicable to simple mesh surface representations while it does not perform any embedding of the 3D mesh (i.e. voxelization, projection). Target of the proposed method is to facilitate mesh degradation procedures in a fashion that retains the original structure, given the operator, in the most efficient manner.



### Cross-Modal Transformer GAN: A Brain Structure-Function Deep Fusing Framework for Alzheimer's Disease
- **Arxiv ID**: http://arxiv.org/abs/2206.13393v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2206.13393v2)
- **Published**: 2022-06-20 11:38:55+00:00
- **Updated**: 2022-07-14 12:58:05+00:00
- **Authors**: Junren Pan, Shuqiang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-modal fusion of different types of neuroimaging data has shown great promise for predicting the progression of Alzheimer's Disease(AD). However, most existing methods applied in neuroimaging can not efficiently fuse the functional and structural information from multi-modal neuroimages. In this work, a novel cross-modal transformer generative adversarial network(CT-GAN) is proposed to fuse functional information contained in resting-state functional magnetic resonance imaging (rs-fMRI) and structural information contained in Diffusion Tensor Imaging (DTI). The developed bi-attention mechanism can match functional information to structural information efficiently and maximize the capability of extracting complementary information from rs-fMRI and DTI. By capturing the deep complementary information between structural features and functional features, the proposed CT-GAN can detect the AD-related brain connectivity, which could be used as a bio-marker of AD. Experimental results show that the proposed model can not only improve classification performance but also detect the AD-related brain connectivity effectively.



### Semantic Labeling of High Resolution Images Using EfficientUNets and Transformers
- **Arxiv ID**: http://arxiv.org/abs/2206.09731v2
- **DOI**: 10.1109/TGRS.2023.3268159
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09731v2)
- **Published**: 2022-06-20 12:03:54+00:00
- **Updated**: 2022-06-22 06:08:03+00:00
- **Authors**: Hasan AlMarzouqi, Lyes Saad Saoud
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation necessitates approaches that learn high-level characteristics while dealing with enormous amounts of data. Convolutional neural networks (CNNs) can learn unique and adaptive features to achieve this aim. However, due to the large size and high spatial resolution of remote sensing images, these networks cannot analyze an entire scene efficiently. Recently, deep transformers have proven their capability to record global interactions between different objects in the image. In this paper, we propose a new segmentation model that combines convolutional neural networks with transformers, and show that this mixture of local and global feature extraction techniques provides significant advantages in remote sensing segmentation. In addition, the proposed model includes two fusion layers that are designed to represent multi-modal inputs and output of the network efficiently. The input fusion layer extracts feature maps summarizing the relationship between image content and elevation maps (DSM). The output fusion layer uses a novel multi-task segmentation strategy where class labels are identified using class-specific feature extraction layers and loss functions. Finally, a fast-marching method is used to convert all unidentified class labels to their closest known neighbors. Our results demonstrate that the proposed methodology improves segmentation accuracy compared to state-of-the-art techniques.



### Geo-NI: Geometry-aware Neural Interpolation for Light Field Rendering
- **Arxiv ID**: http://arxiv.org/abs/2206.09736v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09736v1)
- **Published**: 2022-06-20 12:25:34+00:00
- **Updated**: 2022-06-20 12:25:34+00:00
- **Authors**: Gaochang Wu, Yuemei Zhou, Yebin Liu, Lu Fang, Tianyou Chai
- **Comment**: 13 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: In this paper, we present a Geometry-aware Neural Interpolation (Geo-NI) framework for light field rendering. Previous learning-based approaches either rely on the capability of neural networks to perform direct interpolation, which we dubbed Neural Interpolation (NI), or explore scene geometry for novel view synthesis, also known as Depth Image-Based Rendering (DIBR). Instead, we incorporate the ideas behind these two kinds of approaches by launching the NI with a novel DIBR pipeline. Specifically, the proposed Geo-NI first performs NI using input light field sheared by a set of depth hypotheses. Then the DIBR is implemented by assigning the sheared light fields with a novel reconstruction cost volume according to the reconstruction quality under different depth hypotheses. The reconstruction cost is interpreted as a blending weight to render the final output light field by blending the reconstructed light fields along the dimension of depth hypothesis. By combining the superiorities of NI and DIBR, the proposed Geo-NI is able to render views with large disparity with the help of scene geometry while also reconstruct non-Lambertian effect when depth is prone to be ambiguous. Extensive experiments on various datasets demonstrate the superior performance of the proposed geometry-aware light field rendering framework.



### Developing a Free and Open-source Automated Building Exterior Crack Inspection Software for Construction and Facility Managers
- **Arxiv ID**: http://arxiv.org/abs/2206.09742v1
- **DOI**: 10.1109/ACCESS.2023.3296793
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.09742v1)
- **Published**: 2022-06-20 12:43:33+00:00
- **Updated**: 2022-06-20 12:43:33+00:00
- **Authors**: Pi Ko, Samuel A. Prieto, Borja Garcia de Soto
- **Comment**: None
- **Journal**: None
- **Summary**: Inspection of cracks is an important process for properly monitoring and maintaining a building. However, manual crack inspection is time-consuming, inconsistent, and dangerous (e.g., in tall buildings). Due to the development of open-source AI technologies, the increase in available Unmanned Aerial Vehicles (UAVs) and the availability of smartphone cameras, it has become possible to automate the building crack inspection process. This study presents the development of an easy-to-use, free and open-source Automated Building Exterior Crack Inspection Software (ABECIS) for construction and facility managers, using state-of-the-art segmentation algorithms to identify concrete cracks and generate a quantitative and qualitative report. ABECIS was tested using images collected from a UAV and smartphone cameras in real-world conditions and a controlled laboratory environment. From the raw output of the algorithm, the median Intersection over Unions for the test experiments is (1) 0.686 for indoor crack detection experiment in a controlled lab environment using a commercial drone, (2) 0.186 for indoor crack detection at a construction site using a smartphone and (3) 0.958 for outdoor crack detection on university campus using a commercial drone. These IoU results can be improved significantly to over 0.8 when a human operator selectively removes the false positives. In general, ABECIS performs best for outdoor drone images, and combining the algorithm predictions with human verification/intervention offers very accurate crack detection results. The software is available publicly and can be downloaded for out-of-the-box use at: https://github.com/SMART-NYUAD/ABECIS



### Visualizing and Understanding Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.09753v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09753v2)
- **Published**: 2022-06-20 13:01:46+00:00
- **Updated**: 2023-04-23 12:07:45+00:00
- **Authors**: Fawaz Sammani, Boris Joukovsky, Nikos Deligiannis
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive learning has revolutionized the field of computer vision, learning rich representations from unlabeled data, which generalize well to diverse vision tasks. Consequently, it has become increasingly important to explain these approaches and understand their inner workings mechanisms. Given that contrastive models are trained with interdependent and interacting inputs and aim to learn invariance through data augmentation, the existing methods for explaining single-image systems (e.g., image classification models) are inadequate as they fail to account for these factors. Additionally, there is a lack of evaluation metrics designed to assess pairs of explanations, and no analytical studies have been conducted to investigate the effectiveness of different techniques used to explaining contrastive learning. In this work, we design visual explanation methods that contribute towards understanding similarity learning tasks from pairs of images. We further adapt existing metrics, used to evaluate visual explanations of image classification systems, to suit pairs of explanations and evaluate our proposed methods with these metrics. Finally, we present a thorough analysis of visual explainability methods for contrastive learning, establish their correlation with downstream tasks and demonstrate the potential of our approaches to investigate their merits and drawbacks.



### Time Gated Convolutional Neural Networks for Crop Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.09756v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09756v1)
- **Published**: 2022-06-20 13:05:29+00:00
- **Updated**: 2022-06-20 13:05:29+00:00
- **Authors**: Longlong Weng, Yashu Kang, Kezhao Jiang, Chunlei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presented a state-of-the-art framework, Time Gated Convolutional Neural Network (TGCNN) that takes advantage of temporal information and gating mechanisms for the crop classification problem. Besides, several vegetation indices were constructed to expand dimensions of input data to take advantage of spectral information. Both spatial (channel-wise) and temporal (step-wise) correlation are considered in TGCNN. Specifically, our preliminary analysis indicates that step-wise information is of greater importance in this data set. Lastly, the gating mechanism helps capture high-order relationship. Our TGCNN solution achieves $0.973$ F1 score, $0.977$ AUC ROC and $0.948$ IoU, respectively. In addition, it outperforms three other benchmarks in different local tasks (Kenya, Brazil and Togo). Overall, our experiments demonstrate that TGCNN is advantageous in this earth observation time series classification task.



### Test-time image-to-image translation ensembling improves out-of-distribution generalization in histopathology
- **Arxiv ID**: http://arxiv.org/abs/2206.09769v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09769v3)
- **Published**: 2022-06-20 13:31:51+00:00
- **Updated**: 2022-06-30 15:47:10+00:00
- **Authors**: Marin Scalbert, Maria Vakalopoulou, Florent CouziniÃ©-Devy
- **Comment**: Accepted at MICCAI2022 Conference
- **Journal**: None
- **Summary**: Histopathology whole slide images (WSIs) can reveal significant inter-hospital variability such as illumination, color or optical artifacts. These variations, caused by the use of different scanning protocols across medical centers (staining, scanner), can strongly harm algorithms generalization on unseen protocols. This motivates development of new methods to limit such drop of performances. In this paper, to enhance robustness on unseen target protocols, we propose a new test-time data augmentation based on multi domain image-to-image translation. It allows to project images from unseen protocol into each source domain before classifying them and ensembling the predictions. This test-time augmentation method results in a significant boost of performances for domain generalization. To demonstrate its effectiveness, our method has been evaluated on 2 different histopathology tasks where it outperforms conventional domain generalization, standard H&E specific color augmentation/normalization and standard test-time augmentation techniques. Our code is publicly available at https://gitlab.com/vitadx/articles/test-time-i2i-translation-ensembling.



### Real-time Full-stack Traffic Scene Perception for Autonomous Driving with Roadside Cameras
- **Arxiv ID**: http://arxiv.org/abs/2206.09770v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.09770v1)
- **Published**: 2022-06-20 13:33:52+00:00
- **Updated**: 2022-06-20 13:33:52+00:00
- **Authors**: Zhengxia Zou, Rusheng Zhang, Shengyin Shen, Gaurav Pandey, Punarjay Chakravarty, Armin Parchami, Henry X. Liu
- **Comment**: This paper is accepted and presented in ICRA 2022
- **Journal**: None
- **Summary**: We propose a novel and pragmatic framework for traffic scene perception with roadside cameras. The proposed framework covers a full-stack of roadside perception pipeline for infrastructure-assisted autonomous driving, including object detection, object localization, object tracking, and multi-camera information fusion. Unlike previous vision-based perception frameworks rely upon depth offset or 3D annotation at training, we adopt a modular decoupling design and introduce a landmark-based 3D localization method, where the detection and localization can be well decoupled so that the model can be easily trained based on only 2D annotations. The proposed framework applies to either optical or thermal cameras with pinhole or fish-eye lenses. Our framework is deployed at a two-lane roundabout located at Ellsworth Rd. and State St., Ann Arbor, MI, USA, providing 7x24 real-time traffic flow monitoring and high-precision vehicle trajectory extraction. The whole system runs efficiently on a low-power edge computing device with all-component end-to-end delay of less than 20ms.



### Knowledge Distillation for Oriented Object Detection on Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2206.09796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09796v1)
- **Published**: 2022-06-20 14:24:16+00:00
- **Updated**: 2022-06-20 14:24:16+00:00
- **Authors**: Yicheng Xiao, Junpeng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural network with increased number of parameters has achieved improved precision in task of object detection on natural images, where objects of interests are annotated with horizontal boundary boxes. On aerial images captured from the bird-view perspective, these improvements on model architecture and deeper convolutional layers can also boost the performance on oriented object detection task. However, it is hard to directly apply those state-of-the-art object detectors on the devices with limited computation resources, which necessitates lightweight models through model compression. In order to address this issue, we present a model compression method for rotated object detection on aerial images by knowledge distillation, namely KD-RNet. With a well-trained teacher oriented object detector with a large number of parameters, the obtained object category and location information are both transferred to a compact student network in KD-RNet by collaborative training strategy. Transferring the category information is achieved by knowledge distillation on predicted probability distribution, and a soft regression loss is adopted for handling displacement in location information transfer. The experimental result on a large-scale aerial object detection dataset (DOTA) demonstrates that the proposed KD-RNet model can achieve improved mean-average precision (mAP) with reduced number of parameters, at the same time, KD-RNet boost the performance on providing high quality detections with higher overlap with groundtruth annotations.



### Self-Supervised Consistent Quantization for Fully Unsupervised Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2206.09806v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09806v1)
- **Published**: 2022-06-20 14:39:59+00:00
- **Updated**: 2022-06-20 14:39:59+00:00
- **Authors**: Guile Wu, Chao Zhang, Stephan Liwicki
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Unsupervised image retrieval aims to learn an efficient retrieval system without expensive data annotations, but most existing methods rely heavily on handcrafted feature descriptors or pre-trained feature extractors. To minimize human supervision, recent advance proposes deep fully unsupervised image retrieval aiming at training a deep model from scratch to jointly optimize visual features and quantization codes. However, existing approach mainly focuses on instance contrastive learning without considering underlying semantic structure information, resulting in sub-optimal performance. In this work, we propose a novel self-supervised consistent quantization approach to deep fully unsupervised image retrieval, which consists of part consistent quantization and global consistent quantization. In part consistent quantization, we devise part neighbor semantic consistency learning with codeword diversity regularization. This allows to discover underlying neighbor structure information of sub-quantized representations as self-supervision. In global consistent quantization, we employ contrastive learning for both embedding and quantized representations and fuses these representations for consistent contrastive regularization between instances. This can make up for the loss of useful representation information during quantization and regularize consistency between instances. With a unified learning objective of part and global consistent quantization, our approach exploits richer self-supervision cues to facilitate model learning. Extensive experiments on three benchmark datasets show the superiority of our approach over the state-of-the-art methods.



### Shapley-NAS: Discovering Operation Contribution for Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2206.09811v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09811v1)
- **Published**: 2022-06-20 14:41:49+00:00
- **Updated**: 2022-06-20 14:41:49+00:00
- **Authors**: Han Xiao, Ziwei Wang, Zheng Zhu, Jie Zhou, Jiwen Lu
- **Comment**: Accepted to CVPR2022
- **Journal**: None
- **Summary**: In this paper, we propose a Shapley value based method to evaluate operation contribution (Shapley-NAS) for neural architecture search. Differentiable architecture search (DARTS) acquires the optimal architectures by optimizing the architecture parameters with gradient descent, which significantly reduces the search cost. However, the magnitude of architecture parameters updated by gradient descent fails to reveal the actual operation importance to the task performance and therefore harms the effectiveness of obtained architectures. By contrast, we propose to evaluate the direct influence of operations on validation accuracy. To deal with the complex relationships between supernet components, we leverage Shapley value to quantify their marginal contributions by considering all possible combinations. Specifically, we iteratively optimize the supernet weights and update the architecture parameters by evaluating operation contributions via Shapley value, so that the optimal architectures are derived by selecting the operations that contribute significantly to the tasks. Since the exact computation of Shapley value is NP-hard, the Monte-Carlo sampling based algorithm with early truncation is employed for efficient approximation, and the momentum update mechanism is adopted to alleviate fluctuation of the sampling process. Extensive experiments on various datasets and various search spaces show that our Shapley-NAS outperforms the state-of-the-art methods by a considerable margin with light search cost. The code is available at https://github.com/Euphoria16/Shapley-NAS.git



### CS$^2$: A Controllable and Simultaneous Synthesizer of Images and Annotations with Minimal Human Intervention
- **Arxiv ID**: http://arxiv.org/abs/2206.13394v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13394v1)
- **Published**: 2022-06-20 15:09:10+00:00
- **Updated**: 2022-06-20 15:09:10+00:00
- **Authors**: Xiaodan Xing, Jiahao Huang, Yang Nan, Yinzhe Wu, Chengjia Wang, Zhifan Gao, Simon Walsh, Guang Yang
- **Comment**: 11 figures, Accepted by MICCAI 2022
- **Journal**: None
- **Summary**: The destitution of image data and corresponding expert annotations limit the training capacities of AI diagnostic models and potentially inhibit their performance. To address such a problem of data and label scarcity, generative models have been developed to augment the training datasets. Previously proposed generative models usually require manually adjusted annotations (e.g., segmentation masks) or need pre-labeling. However, studies have found that these pre-labeling based methods can induce hallucinating artifacts, which might mislead the downstream clinical tasks, while manual adjustment could be onerous and subjective. To avoid manual adjustment and pre-labeling, we propose a novel controllable and simultaneous synthesizer (dubbed CS$^2$) in this study to generate both realistic images and corresponding annotations at the same time. Our CS$^2$ model is trained and validated using high resolution CT (HRCT) data collected from COVID-19 patients to realize an efficient infections segmentation with minimal human intervention. Our contributions include 1) a conditional image synthesis network that receives both style information from reference CT images and structural information from unsupervised segmentation masks, and 2) a corresponding segmentation mask synthesis network to automatically segment these synthesized images simultaneously. Our experimental studies on HRCT scans collected from COVID-19 patients demonstrate that our CS$^2$ model can lead to realistic synthesized datasets and promising segmentation results of COVID infections compared to the state-of-the-art nnUNet trained and fine-tuned in a fully supervised manner.



### Business Document Information Extraction: Towards Practical Benchmarks
- **Arxiv ID**: http://arxiv.org/abs/2206.11229v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.11229v1)
- **Published**: 2022-06-20 15:23:49+00:00
- **Updated**: 2022-06-20 15:23:49+00:00
- **Authors**: MatyÃ¡Å¡ SkalickÃ½, Å tÄpÃ¡n Å imsa, Michal UÅiÄÃ¡Å, Milan Å ulc
- **Comment**: Accepted to CLEF 2022
- **Journal**: None
- **Summary**: Information extraction from semi-structured documents is crucial for frictionless business-to-business (B2B) communication. While machine learning problems related to Document Information Extraction (IE) have been studied for decades, many common problem definitions and benchmarks do not reflect domain-specific aspects and practical needs for automating B2B document communication. We review the landscape of Document IE problems, datasets and benchmarks. We highlight the practical aspects missing in the common definitions and define the Key Information Localization and Extraction (KILE) and Line Item Recognition (LIR) problems. There is a lack of relevant datasets and benchmarks for Document IE on semi-structured business documents as their content is typically legally protected or sensitive. We discuss potential sources of available documents including synthetic data.



### Practical Deepfake Detection: Vulnerabilities in Global Contexts
- **Arxiv ID**: http://arxiv.org/abs/2206.09842v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2206.09842v1)
- **Published**: 2022-06-20 15:24:55+00:00
- **Updated**: 2022-06-20 15:24:55+00:00
- **Authors**: Yang A. Chuming, Daniel J. Wu, Ken Hong
- **Comment**: 6 pages, 6 figures, presented as a workshop paper at Responsible AI @
  ICLR 2021
- **Journal**: None
- **Summary**: Recent advances in deep learning have enabled realistic digital alterations to videos, known as deepfakes. This technology raises important societal concerns regarding disinformation and authenticity, galvanizing the development of numerous deepfake detection algorithms. At the same time, there are significant differences between training data and in-the-wild video data, which may undermine their practical efficacy. We simulate data corruption techniques and examine the performance of a state-of-the-art deepfake detection algorithm on corrupted variants of the FaceForensics++ dataset.   While deepfake detection models are robust against video corruptions that align with training-time augmentations, we find that they remain vulnerable to video corruptions that simulate decreases in video quality. Indeed, in the controversial case of the video of Gabonese President Bongo's new year address, the algorithm, which confidently authenticates the original video, judges highly corrupted variants of the video to be fake. Our work opens up both technical and ethical avenues of exploration into practical deepfake detection in global contexts.



### Contextual Squeeze-and-Excitation for Efficient Few-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.09843v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09843v3)
- **Published**: 2022-06-20 15:25:08+00:00
- **Updated**: 2023-01-11 11:21:41+00:00
- **Authors**: Massimiliano Patacchiola, John Bronskill, Aliaksandra Shysheya, Katja Hofmann, Sebastian Nowozin, Richard E. Turner
- **Comment**: Advances in Neural Information Processing Systems (NeurIPS 2022)
- **Journal**: None
- **Summary**: Recent years have seen a growth in user-centric applications that require effective knowledge transfer across tasks in the low-data regime. An example is personalization, where a pretrained system is adapted by learning on small amounts of labeled data belonging to a specific user. This setting requires high accuracy under low computational complexity, therefore the Pareto frontier of accuracy vs. adaptation cost plays a crucial role. In this paper we push this Pareto frontier in the few-shot image classification setting with a key contribution: a new adaptive block called Contextual Squeeze-and-Excitation (CaSE) that adjusts a pretrained neural network on a new task to significantly improve performance with a single forward pass of the user data (context). We use meta-trained CaSE blocks to conditionally adapt the body of a network and a fine-tuning routine to adapt a linear head, defining a method called UpperCaSE. UpperCaSE achieves a new state-of-the-art accuracy relative to meta-learners on the 26 datasets of VTAB+MD and on a challenging real-world personalization benchmark (ORBIT), narrowing the gap with leading fine-tuning methods with the benefit of orders of magnitude lower adaptation cost.



### M&M Mix: A Multimodal Multiview Transformer Ensemble
- **Arxiv ID**: http://arxiv.org/abs/2206.09852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09852v1)
- **Published**: 2022-06-20 15:31:13+00:00
- **Updated**: 2022-06-20 15:31:13+00:00
- **Authors**: Xuehan Xiong, Anurag Arnab, Arsha Nagrani, Cordelia Schmid
- **Comment**: Technical report for Epic-Kitchens challenge 2022
- **Journal**: None
- **Summary**: This report describes the approach behind our winning solution to the 2022 Epic-Kitchens Action Recognition Challenge. Our approach builds upon our recent work, Multiview Transformer for Video Recognition (MTV), and adapts it to multimodal inputs. Our final submission consists of an ensemble of Multimodal MTV (M&M) models varying backbone sizes and input modalities. Our approach achieved 52.8% Top-1 accuracy on the test set in action classes, which is 4.1% higher than last year's winning entry.



### DisCoVQA: Temporal Distortion-Content Transformers for Video Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2206.09853v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2206.09853v1)
- **Published**: 2022-06-20 15:31:27+00:00
- **Updated**: 2022-06-20 15:31:27+00:00
- **Authors**: Haoning Wu, Chaofeng Chen, Liang Liao, Jingwen Hou, Wenxiu Sun, Qiong Yan, Weisi Lin
- **Comment**: None
- **Journal**: None
- **Summary**: The temporal relationships between frames and their influences on video quality assessment (VQA) are still under-studied in existing works. These relationships lead to two important types of effects for video quality. Firstly, some temporal variations (such as shaking, flicker, and abrupt scene transitions) are causing temporal distortions and lead to extra quality degradations, while other variations (e.g. those related to meaningful happenings) do not. Secondly, the human visual system often has different attention to frames with different contents, resulting in their different importance to the overall video quality. Based on prominent time-series modeling ability of transformers, we propose a novel and effective transformer-based VQA method to tackle these two issues. To better differentiate temporal variations and thus capture the temporal distortions, we design a transformer-based Spatial-Temporal Distortion Extraction (STDE) module. To tackle with temporal quality attention, we propose the encoder-decoder-like temporal content transformer (TCT). We also introduce the temporal sampling on features to reduce the input length for the TCT, so as to improve the learning effectiveness and efficiency of this module. Consisting of the STDE and the TCT, the proposed Temporal Distortion-Content Transformers for Video Quality Assessment (DisCoVQA) reaches state-of-the-art performance on several VQA benchmarks without any extra pre-training datasets and up to 10% better generalization ability than existing methods. We also conduct extensive ablation experiments to prove the effectiveness of each part in our proposed model, and provide visualizations to prove that the proposed modules achieve our intention on modeling these temporal issues. We will publish our codes and pretrained weights later.



### WiFi-based Spatiotemporal Human Action Perception
- **Arxiv ID**: http://arxiv.org/abs/2206.09867v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09867v1)
- **Published**: 2022-06-20 16:03:45+00:00
- **Updated**: 2022-06-20 16:03:45+00:00
- **Authors**: Yanling Hao, Zhiyuan Shi, Yuanwei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: WiFi-based sensing for human activity recognition (HAR) has recently become a hot topic as it brings great benefits when compared with video-based HAR, such as eliminating the demands of line-of-sight (LOS) and preserving privacy. Making the WiFi signals to 'see' the action, however, is quite coarse and thus still in its infancy. An end-to-end spatiotemporal WiFi signal neural network (STWNN) is proposed to enable WiFi-only sensing in both line-of-sight and non-line-of-sight scenarios. Especially, the 3D convolution module is able to explore the spatiotemporal continuity of WiFi signals, and the feature self-attention module can explicitly maintain dominant features. In addition, a novel 3D representation for WiFi signals is designed to preserve multi-scale spatiotemporal information. Furthermore, a small wireless-vision dataset (WVAR) is synchronously collected to extend the potential of STWNN to 'see' through occlusions. Quantitative and qualitative results on WVAR and the other three public benchmark datasets demonstrate the effectiveness of our approach on both accuracy and shift consistency.



### Gait Cycle Reconstruction and Human Identification from Occluded Sequences
- **Arxiv ID**: http://arxiv.org/abs/2206.13395v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.13395v1)
- **Published**: 2022-06-20 16:04:31+00:00
- **Updated**: 2022-06-20 16:04:31+00:00
- **Authors**: Abhishek Paul, Manav Mukesh Jain, Jinesh Jain, Pratik Chattopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Gait-based person identification from videos captured at surveillance sites using Computer Vision-based techniques is quite challenging since these walking sequences are usually corrupted with occlusion, and a complete cycle of gait is not always available. In this work, we propose an effective neural network-based model to reconstruct the occluded frames in an input sequence before carrying out gait recognition. Specifically, we employ LSTM networks to predict an embedding for each occluded frame both from the forward and the backward directions, and next fuse the predictions from the two LSTMs by employing a network of residual blocks and convolutional layers. While the LSTMs are trained to minimize the mean-squared loss, the fusion network is trained to optimize the pixel-wise cross-entropy loss between the ground-truth and the reconstructed samples. Evaluation of our approach has been done using synthetically occluded sequences generated from the OU-ISIR LP and CASIA-B data and real-occluded sequences present in the TUM-IITKGP data. The effectiveness of the proposed reconstruction model has been verified through the Dice score and gait-based recognition accuracy using some popular gait recognition methods. Comparative study with existing occlusion handling methods in gait recognition highlights the superiority of our proposed occlusion reconstruction approach over the others.



### Understanding Robust Learning through the Lens of Representation Similarities
- **Arxiv ID**: http://arxiv.org/abs/2206.09868v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09868v2)
- **Published**: 2022-06-20 16:06:20+00:00
- **Updated**: 2022-09-15 17:47:42+00:00
- **Authors**: Christian Cianfarani, Arjun Nitin Bhagoji, Vikash Sehwag, Ben Y. Zhao, Prateek Mittal, Haitao Zheng
- **Comment**: 35 pages, 29 figures; Accepted to Neurips 2022
- **Journal**: None
- **Summary**: Representation learning, i.e. the generation of representations useful for downstream applications, is a task of fundamental importance that underlies much of the success of deep neural networks (DNNs). Recently, robustness to adversarial examples has emerged as a desirable property for DNNs, spurring the development of robust training methods that account for adversarial examples. In this paper, we aim to understand how the properties of representations learned by robust training differ from those obtained from standard, non-robust training. This is critical to diagnosing numerous salient pitfalls in robust networks, such as, degradation of performance on benign inputs, poor generalization of robustness, and increase in over-fitting. We utilize a powerful set of tools known as representation similarity metrics, across three vision datasets, to obtain layer-wise comparisons between robust and non-robust DNNs with different training procedures, architectural parameters and adversarial constraints. Our experiments highlight hitherto unseen properties of robust representations that we posit underlie the behavioral differences of robust networks. We discover a lack of specialization in robust networks' representations along with a disappearance of `block structure'. We also find overfitting during robust training largely impacts deeper layers. These, along with other findings, suggest ways forward for the design and training of better robust networks.



### Breaking Down Out-of-Distribution Detection: Many Methods Based on OOD Training Data Estimate a Combination of the Same Core Quantities
- **Arxiv ID**: http://arxiv.org/abs/2206.09880v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09880v1)
- **Published**: 2022-06-20 16:32:49+00:00
- **Updated**: 2022-06-20 16:32:49+00:00
- **Authors**: Julian Bitterwolf, Alexander Meinke, Maximilian Augustin, Matthias Hein
- **Comment**: None
- **Journal**: None
- **Summary**: It is an important problem in trustworthy machine learning to recognize out-of-distribution (OOD) inputs which are inputs unrelated to the in-distribution task. Many out-of-distribution detection methods have been suggested in recent years. The goal of this paper is to recognize common objectives as well as to identify the implicit scoring functions of different OOD detection methods. We focus on the sub-class of methods that use surrogate OOD data during training in order to learn an OOD detection score that generalizes to new unseen out-distributions at test time. We show that binary discrimination between in- and (different) out-distributions is equivalent to several distinct formulations of the OOD detection problem. When trained in a shared fashion with a standard classifier, this binary discriminator reaches an OOD detection performance similar to that of Outlier Exposure. Moreover, we show that the confidence loss which is used by Outlier Exposure has an implicit scoring function which differs in a non-trivial fashion from the theoretically optimal scoring function in the case where training and test out-distribution are the same, which again is similar to the one used when training an Energy-Based OOD detector or when adding a background class. In practice, when trained in exactly the same way, all these methods perform similarly.



### Deep Learning-Based Defect Classification and Detection in SEM Images
- **Arxiv ID**: http://arxiv.org/abs/2206.13505v1
- **DOI**: 10.1117/12.2622550
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13505v1)
- **Published**: 2022-06-20 16:34:11+00:00
- **Updated**: 2022-06-20 16:34:11+00:00
- **Authors**: Bappaditya Deya, Dipam Goswamif, Sandip Haldera, Kasem Khalilb, Philippe Leraya, Magdy A. Bayoumi
- **Comment**: None
- **Journal**: In Metrology, Inspection, and Process Control XXXVI, SPIE (2022)
- **Summary**: This proposes a novel ensemble deep learning-based model to accurately classify, detect and localize different defect categories for aggressive pitches and thin resists (High NA applications).In particular, we train RetinaNet models using different ResNet, VGGNet architectures as backbone and present the comparison between the accuracies of these models and their performance analysis on SEM images with different types of defect patterns such as bridge, break and line collapses. Finally, we propose a preference-based ensemble strategy to combine the output predictions from different models in order to achieve better performance on classification and detection of defects. As CDSEM images inherently contain a significant level of noise, detailed feature information is often shadowed by noise. For certain resist profiles, the challenge is also to differentiate between a microbridge, footing, break, and zones of probable breaks. Therefore, we have applied an unsupervised machine learning model to denoise the SEM images to remove the False-Positive defects and optimize the effect of stochastic noise on structured pixels for better metrology and enhanced defect inspection. We repeated the defect inspection step with the same trained model and performed a comparative analysis for "robustness" and "accuracy" metric with conventional approach for both noisy/denoised image pair. The proposed ensemble method demonstrates improvement of the average precision metric (mAP) of the most difficult defect classes. In this work we have developed a novel robust supervised deep learning training scheme to accurately classify as well as localize different defect types in SEM images with high degree of accuracy. Our proposed approach demonstrates its effectiveness both quantitatively and qualitatively.



### KOLOMVERSE: KRISO open large-scale image dataset for object detection in the maritime universe
- **Arxiv ID**: http://arxiv.org/abs/2206.09885v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09885v1)
- **Published**: 2022-06-20 16:45:12+00:00
- **Updated**: 2022-06-20 16:45:12+00:00
- **Authors**: Abhilasha Nanda, Sung Won Cho, Hyeopwoo Lee, Jin Hyoung Park
- **Comment**: 13 Pages, 12 figures, submitted to NeurIPS 2022 Datasets and
  Benchmarks Track (Under Review)
- **Journal**: None
- **Summary**: Over the years, datasets have been developed for various object detection tasks. Object detection in the maritime domain is essential for the safety and navigation of ships. However, there is still a lack of publicly available large-scale datasets in the maritime domain. To overcome this challenge, we present KOLOMVERSE, an open large-scale image dataset for object detection in the maritime domain by KRISO (Korea Research Institute of Ships and Ocean Engineering). We collected 5,845 hours of video data captured from 21 territorial waters of South Korea. Through an elaborate data quality assessment process, we gathered around 2,151,470 4K resolution images from the video data. This dataset considers various environments: weather, time, illumination, occlusion, viewpoint, background, wind speed, and visibility. The KOLOMVERSE consists of five classes (ship, buoy, fishnet buoy, lighthouse and wind farm) for maritime object detection. The dataset has images of 3840$\times$2160 pixels and to our knowledge, it is by far the largest publicly available dataset for object detection in the maritime domain. We performed object detection experiments and evaluated our dataset on several pre-trained state-of-the-art architectures to show the effectiveness and usefulness of our dataset. The dataset is available at: \url{https://github.com/MaritimeDataset/KOLOMVERSE}.



### Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point Clouds with Masked Occupancy Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2206.09900v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09900v6)
- **Published**: 2022-06-20 17:15:50+00:00
- **Updated**: 2023-04-29 00:54:33+00:00
- **Authors**: Chen Min, Xinli Xu, Dawei Zhao, Liang Xiao, Yiming Nie, Bin Dai
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Current perception models in autonomous driving rely heavily on large-scale labeled LiDAR data, which is costly and time-consuming to annotate. In this work, we aim to facilitate research on self-supervised masked learning using the vast amount of unlabeled LiDAR data available in autonomous driving. However, existing masked point autoencoding methods only focus on small-scale indoor point clouds and struggle to adapt to outdoor scenes, which usually have a large number of non-evenly distributed LiDAR points. To address these challenges, we propose a new self-supervised masked learning method named Occupancy-MAE, specifically designed for large-scale outdoor LiDAR points. We leverage the gradually sparse occupancy structure of large-scale outdoor LiDAR point clouds and introduce a range-aware random masking strategy and a pretext task of occupancy prediction. Occupancy-MAE randomly masks voxels of LiDAR point clouds based on their distance to LiDAR and predicts the masked occupancy structure of the whole 3D scene. This simple occupancy prediction objective encourages Occupancy-MAE to extract high-level semantic information to recover the masked voxel from only a small amount of visible voxels. Extensive experiments demonstrate the effectiveness of Occupancy-MAE across several downstream tasks. For the 3D object detection task, Occupancy-MAE reduces the labeled data required for car detection on KITTI by half and boosts small object detection by around 2% mAP on Waymo. For the 3D semantic segmentation task, Occupancy-MAE outperforms training from scratch by around 2% mIOU on nuScenes. For the unsupervised domain adaptation task, Occupancy-MAE improves the performance by about 0.5\% ~ 1% mAP. Our results show that it is feasible to pre-train unlabeled large-scale LiDAR point clouds with masked autoencoding to enhance the 3D perception ability of autonomous driving.



### ORFD: A Dataset and Benchmark for Off-Road Freespace Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.09907v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09907v2)
- **Published**: 2022-06-20 17:22:57+00:00
- **Updated**: 2022-06-26 13:28:17+00:00
- **Authors**: Chen Min, Weizhong Jiang, Dawei Zhao, Jiaolong Xu, Liang Xiao, Yiming Nie, Bin Dai
- **Comment**: Accepted by ICRA2022
- **Journal**: None
- **Summary**: Freespace detection is an essential component of autonomous driving technology and plays an important role in trajectory planning. In the last decade, deep learning-based free space detection methods have been proved feasible. However, these efforts were focused on urban road environments and few deep learning-based methods were specifically designed for off-road free space detection due to the lack of off-road benchmarks. In this paper, we present the ORFD dataset, which, to our knowledge, is the first off-road free space detection dataset. The dataset was collected in different scenes (woodland, farmland, grassland, and countryside), different weather conditions (sunny, rainy, foggy, and snowy), and different light conditions (bright light, daylight, twilight, darkness), which totally contains 12,198 LiDAR point cloud and RGB image pairs with the traversable area, non-traversable area and unreachable area annotated in detail. We propose a novel network named OFF-Net, which unifies Transformer architecture to aggregate local and global information, to meet the requirement of large receptive fields for free space detection tasks. We also propose the cross-attention to dynamically fuse LiDAR and RGB image information for accurate off-road free space detection. Dataset and code are publicly available athttps://github.com/chaytonmin/OFF-Net.



### Short Video Uprising: How #BlackLivesMatter Content on TikTok Challenges the Protest Paradigm
- **Arxiv ID**: http://arxiv.org/abs/2206.09946v1
- **DOI**: 10.36190/2022.42
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09946v1)
- **Published**: 2022-06-20 18:05:07+00:00
- **Updated**: 2022-06-20 18:05:07+00:00
- **Authors**: Yanru Jiang, Xin Jin, Qinhao Deng
- **Comment**: Workshop Proceedings of the 16th International AAAI Conference on Web
  and Social Media
- **Journal**: None
- **Summary**: This study uses TikTok (N = 8,173) to examine how short-form video platforms challenge the protest paradigm in the recent Black Lives Matter movement. A computer-mediated visual analysis, computer vision, is employed to identify the presence of four visual frames of protest (riot, confrontation, spectacle, and debate) in multimedia content. Results of descriptive statistics and the t-test indicate that the three delegitimizing frames - riot, confrontation, and spectacle - are rarely found on TikTok, whereas the debate frame, that empowers marginalized communities, dominates the public sphere. However, although the three delegitimizing frames receive lower social media visibility, as measured by views, likes, shares, followers, and durations, legitimizing elements, such as the debate frame, minority identities, and unofficial sources, are not generally favored by TikTok audiences. This study concludes that while short-form video platforms could potentially challenge the protest paradigm on the content creators' side, the audiences' preference as measured by social media visibility might still be moderately associated with the protest paradigm.



### Global Context Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2206.09959v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09959v5)
- **Published**: 2022-06-20 18:42:44+00:00
- **Updated**: 2023-06-06 08:17:18+00:00
- **Authors**: Ali Hatamizadeh, Hongxu Yin, Greg Heinrich, Jan Kautz, Pavlo Molchanov
- **Comment**: Accepted to ICML 2023
- **Journal**: None
- **Summary**: We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision. Our method leverages global context self-attention modules, joint with standard local self-attention, to effectively and efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the lack of the inductive bias in ViTs, and propose to leverage a modified fused inverted residual blocks in our architecture. Our proposed GC ViT achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the variants of GC ViT with 51M, 90M and 201M parameters achieve 84.3%, 85.0% and 85.7% Top-1 accuracy, respectively, at 224 image resolution and without any pre-training, hence surpassing comparably-sized prior art such as CNN-based ConvNeXt and ViT-based MaxViT and Swin Transformer by a large margin. Pre-trained GC ViT backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation using MS COCO and ADE20K datasets outperform prior work consistently. Specifically, GC ViT with a 4-scale DINO detection head achieves a box AP of 58.3 on MS COCO dataset.



### Open Set Classification of Untranscribed Handwritten Documents
- **Arxiv ID**: http://arxiv.org/abs/2206.13342v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13342v1)
- **Published**: 2022-06-20 20:43:50+00:00
- **Updated**: 2022-06-20 20:43:50+00:00
- **Authors**: JosÃ© RamÃ³n Prieto, Juan JosÃ© Flores, Enrique Vidal, Alejandro H. Toselli, David Garrido, Carlos Alonso
- **Comment**: None
- **Journal**: None
- **Summary**: Huge amounts of digital page images of important manuscripts are preserved in archives worldwide. The amounts are so large that it is generally unfeasible for archivists to adequately tag most of the documents with the required metadata so as to low proper organization of the archives and effective exploration by scholars and the general public. The class or ``typology'' of a document is perhaps the most important tag to be included in the metadata. The technical problem is one of automatic classification of documents, each consisting of a set of untranscribed handwritten text images, by the textual contents of the images. The approach considered is based on ``probabilistic indexing'', a relatively novel technology which allows to effectively represent the intrinsic word-level uncertainty exhibited by handwritten text images. We assess the performance of this approach on a large collection of complex notarial manuscripts from the Spanish Archivo Host\'orico Provincial de C\'adiz, with promising results.



### When Does Re-initialization Work?
- **Arxiv ID**: http://arxiv.org/abs/2206.10011v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2206.10011v2)
- **Published**: 2022-06-20 21:23:15+00:00
- **Updated**: 2023-04-02 22:19:08+00:00
- **Authors**: Sheheryar Zaidi, Tudor Berariu, Hyunjik Kim, JÃ¶rg Bornschein, Claudia Clopath, Yee Whye Teh, Razvan Pascanu
- **Comment**: Published in PMLR Volume 187; spotlight presentation at I Can't
  Believe It's Not Better Workshop at NeurIPS 2022
- **Journal**: None
- **Summary**: Re-initializing a neural network during training has been observed to improve generalization in recent works. Yet it is neither widely adopted in deep learning practice nor is it often used in state-of-the-art training protocols. This raises the question of when re-initialization works, and whether it should be used together with regularization techniques such as data augmentation, weight decay and learning rate schedules. In this work, we conduct an extensive empirical comparison of standard training with a selection of re-initialization methods to answer this question, training over 15,000 models on a variety of image classification benchmarks. We first establish that such methods are consistently beneficial for generalization in the absence of any other regularization. However, when deployed alongside other carefully tuned regularization techniques, re-initialization methods offer little to no added benefit for generalization, although optimal generalization performance becomes less sensitive to the choice of learning rate and weight decay hyperparameters. To investigate the impact of re-initialization methods on noisy data, we also consider learning under label noise. Surprisingly, in this case, re-initialization significantly improves upon standard training, even in the presence of other carefully tuned regularization techniques.



### Test Time Transform Prediction for Open Set Histopathological Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.10033v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.10033v2)
- **Published**: 2022-06-20 22:39:48+00:00
- **Updated**: 2022-06-27 12:18:48+00:00
- **Authors**: Adrian Galdran, Katherine J. Hewitt, Narmin L. Ghaffari, Jakob N. Kather, Gustavo Carneiro, Miguel A. GonzÃ¡lez Ballester
- **Comment**: Accepted to MICCAI 2022
- **Journal**: None
- **Summary**: Tissue typology annotation in Whole Slide histological images is a complex and tedious, yet necessary task for the development of computational pathology models. We propose to address this problem by applying Open Set Recognition techniques to the task of jointly classifying tissue that belongs to a set of annotated classes, e.g. clinically relevant tissue categories, while rejecting in test time Open Set samples, i.e. images that belong to categories not present in the training set. To this end, we introduce a new approach for Open Set histopathological image recognition based on training a model to accurately identify image categories and simultaneously predict which data augmentation transform has been applied. In test time, we measure model confidence in predicting this transform, which we expect to be lower for images in the Open Set. We carry out comprehensive experiments in the context of colorectal cancer assessment from histological images, which provide evidence on the strengths of our approach to automatically identify samples from unknown categories. Code is released at https://github.com/agaldran/t3po .



### MPA: MultiPath++ Based Architecture for Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2206.10041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.10041v1)
- **Published**: 2022-06-20 23:06:55+00:00
- **Updated**: 2022-06-20 23:06:55+00:00
- **Authors**: Stepan Konev
- **Comment**: CVPR 2022, Workshop on Autonomous Driving
- **Journal**: None
- **Summary**: Autonomous driving technology is developing rapidly and nowadays first autonomous rides are being provided in city areas. This requires the highest standards for the safety and reliability of the technology. Motion prediction part of the general self-driving pipeline plays a crucial role in providing these qualities. In this work we present one of the solutions for Waymo Motion Prediction Challenge 2022 based on MultiPath++ ranked the 3rd as of May, 26 2022. Our source code is publicly available on GitHub.



