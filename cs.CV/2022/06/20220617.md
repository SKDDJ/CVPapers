# Arxiv Papers in cs.CV on 2022-06-17
### Unsupervised Kinematic Motion Detection for Part-segmented 3D Shape Collections
- **Arxiv ID**: http://arxiv.org/abs/2206.08497v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08497v1)
- **Published**: 2022-06-17 00:50:36+00:00
- **Updated**: 2022-06-17 00:50:36+00:00
- **Authors**: Xianghao Xu, Yifan Ruan, Srinath Sridhar, Daniel Ritchie
- **Comment**: SIGGRAPH 2022
- **Journal**: None
- **Summary**: 3D models of manufactured objects are important for populating virtual worlds and for synthetic data generation for vision and robotics. To be most useful, such objects should be articulated: their parts should move when interacted with. While articulated object datasets exist, creating them is labor-intensive. Learning-based prediction of part motions can help, but all existing methods require annotated training data. In this paper, we present an unsupervised approach for discovering articulated motions in a part-segmented 3D shape collection. Our approach is based on a concept we call category closure: any valid articulation of an object's parts should keep the object in the same semantic category (e.g. a chair stays a chair). We operationalize this concept with an algorithm that optimizes a shape's part motion parameters such that it can transform into other shapes in the collection. We evaluate our approach by using it to re-discover part motions from the PartNet-Mobility dataset. For almost all shape categories, our method's predicted motion parameters have low error with respect to ground truth annotations, outperforming two supervised motion prediction methods.



### What do navigation agents learn about their environment?
- **Arxiv ID**: http://arxiv.org/abs/2206.08500v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.08500v1)
- **Published**: 2022-06-17 01:33:43+00:00
- **Updated**: 2022-06-17 01:33:43+00:00
- **Authors**: Kshitij Dwivedi, Gemma Roig, Aniruddha Kembhavi, Roozbeh Mottaghi
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Today's state of the art visual navigation agents typically consist of large deep learning models trained end to end. Such models offer little to no interpretability about the learned skills or the actions of the agent taken in response to its environment. While past works have explored interpreting deep learning models, little attention has been devoted to interpreting embodied AI systems, which often involve reasoning about the structure of the environment, target characteristics and the outcome of one's actions. In this paper, we introduce the Interpretability System for Embodied agEnts (iSEE) for Point Goal and Object Goal navigation agents. We use iSEE to probe the dynamic representations produced by these agents for the presence of information about the agent as well as the environment. We demonstrate interesting insights about navigation agents using iSEE, including the ability to encode reachable locations (to avoid obstacles), visibility of the target, progress from the initial spawn location as well as the dramatic effect on the behaviors of agents when we mask out critical individual neurons. The code is available at: https://github.com/allenai/iSEE



### Neural Architecture Adaptation for Object Detection by Searching Channel Dimensions and Mapping Pre-trained Parameters
- **Arxiv ID**: http://arxiv.org/abs/2206.08509v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.08509v1)
- **Published**: 2022-06-17 02:01:56+00:00
- **Updated**: 2022-06-17 02:01:56+00:00
- **Authors**: Harim Jung, Myeong-Seok Oh, Cheoljong Yang, Seong-Whan Lee
- **Comment**: Accepted to ICPR 2022
- **Journal**: None
- **Summary**: Most object detection frameworks use backbone architectures originally designed for image classification, conventionally with pre-trained parameters on ImageNet. However, image classification and object detection are essentially different tasks and there is no guarantee that the optimal backbone for classification is also optimal for object detection. Recent neural architecture search (NAS) research has demonstrated that automatically designing a backbone specifically for object detection helps improve the overall accuracy. In this paper, we introduce a neural architecture adaptation method that can optimize the given backbone for detection purposes, while still allowing the use of pre-trained parameters. We propose to adapt both the micro- and macro-architecture by searching for specific operations and the number of layers, in addition to the output channel dimensions of each block. It is important to find the optimal channel depth, as it greatly affects the feature representation capability and computation cost. We conduct experiments with our searched backbone for object detection and demonstrate that our backbone outperforms both manually designed and searched state-of-the-art backbones on the COCO dataset.



### Effective Solid State LiDAR Odometry Using Continuous-time Filter Registration
- **Arxiv ID**: http://arxiv.org/abs/2206.08517v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08517v1)
- **Published**: 2022-06-17 02:41:48+00:00
- **Updated**: 2022-06-17 02:41:48+00:00
- **Authors**: Xin Zheng, Jianke Zhu
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Solid-state LiDARs are more compact and cheaper than the conventional mechanical multi-line spinning LiDARs, which have become increasingly popular in autonomous driving recently. However, there are several challenges for these new LiDAR sensors, including severe motion distortions, small field of view and sparse point cloud, which hinder them from being widely used in LiDAR odometry. To tackle these problems, we present an effective continuous-time LiDAR odometry (ECTLO) method for the Risley prism-based LiDARs with non-repetitive scanning patterns. To account for the noisy data, a filter-based point-to-plane Gaussian Mixture Model is used for robust registration. Moreover, a LiDAR-only continuous-time motion model is employed to relieve the inevitable distortions. To facilitate the implicit data association in parallel, we maintain all map points within a single range image. Extensive experiments have been conducted on various testbeds using the solid-state LiDARs with different scanning patterns, whose promising results demonstrate the efficacy of our proposed approach.



### VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2206.08522v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08522v2)
- **Published**: 2022-06-17 03:07:18+00:00
- **Updated**: 2022-08-17 17:18:43+00:00
- **Authors**: Kaizhi Zheng, Xiaotong Chen, Odest Chadwicke Jenkins, Xin Eric Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Benefiting from language flexibility and compositionality, humans naturally intend to use language to command an embodied agent for complex tasks such as navigation and object manipulation. In this work, we aim to fill the blank of the last mile of embodied agents -- object manipulation by following human guidance, e.g., "move the red mug next to the box while keeping it upright." To this end, we introduce an Automatic Manipulation Solver (AMSolver) system and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it, containing various language instructions on categorized robotic manipulation tasks. Specifically, modular rule-based task templates are created to automatically generate robot demonstrations with language instructions, consisting of diverse object shapes and appearances, action types, and motion constraints. We also develop a keypoint-based model 6D-CLIPort to deal with multi-view observations and language input and output a sequence of 6 degrees of freedom (DoF) actions. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation.



### CDNet: Contrastive Disentangled Network for Fine-Grained Image Categorization of Ocular B-Scan Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2206.08524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08524v1)
- **Published**: 2022-06-17 03:12:52+00:00
- **Updated**: 2022-06-17 03:12:52+00:00
- **Authors**: Ruilong Dan, Yunxiang Li, Yijie Wang, Gangyong Jia, Ruiquan Ge, Juan Ye, Qun Jin, Yaqi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Precise and rapid categorization of images in the B-scan ultrasound modality is vital for diagnosing ocular diseases. Nevertheless, distinguishing various diseases in ultrasound still challenges experienced ophthalmologists. Thus a novel contrastive disentangled network (CDNet) is developed in this work, aiming to tackle the fine-grained image categorization (FGIC) challenges of ocular abnormalities in ultrasound images, including intraocular tumor (IOT), retinal detachment (RD), posterior scleral staphyloma (PSS), and vitreous hemorrhage (VH). Three essential components of CDNet are the weakly-supervised lesion localization module (WSLL), contrastive multi-zoom (CMZ) strategy, and hyperspherical contrastive disentangled loss (HCD-Loss), respectively. These components facilitate feature disentanglement for fine-grained recognition in both the input and output aspects. The proposed CDNet is validated on our ZJU Ocular Ultrasound Dataset (ZJUOUSD), consisting of 5213 samples. Furthermore, the generalization ability of CDNet is validated on two public and widely-used chest X-ray FGIC benchmarks. Quantitative and qualitative results demonstrate the efficacy of our proposed CDNet, which achieves state-of-the-art performance in the FGIC task. Code is available at: https://github.com/ZeroOneGame/CDNet-for-OUS-FGIC .



### Large-Margin Representation Learning for Texture Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.08537v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.08537v1)
- **Published**: 2022-06-17 04:07:45+00:00
- **Updated**: 2022-06-17 04:07:45+00:00
- **Authors**: Jonathan de Matos, Luiz Eduardo Soares de Oliveira, Alceu de Souza Britto Junior, Alessandro Lameiras Koerich
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: This paper presents a novel approach combining convolutional layers (CLs) and large-margin metric learning for training supervised models on small datasets for texture classification. The core of such an approach is a loss function that computes the distances between instances of interest and support vectors. The objective is to update the weights of CLs iteratively to learn a representation with a large margin between classes. Each iteration results in a large-margin discriminant model represented by support vectors based on such a representation. The advantage of the proposed approach w.r.t. convolutional neural networks (CNNs) is two-fold. First, it allows representation learning with a small amount of data due to the reduced number of parameters compared to an equivalent CNN. Second, it has a low training cost since the backpropagation considers only support vectors. The experimental results on texture and histopathologic image datasets have shown that the proposed approach achieves competitive accuracy with lower computational cost and faster convergence when compared to equivalent CNNs.



### Multi-Classification of Brain Tumor Images Using Transfer Learning Based Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2206.08543v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08543v1)
- **Published**: 2022-06-17 04:30:40+00:00
- **Updated**: 2022-06-17 04:30:40+00:00
- **Authors**: Pramit Dutta, Khaleda Akhter Sathi, Md. Saiful Islam
- **Comment**: 7 pages, 4 figures, 2 tables, International Virtual Conference on
  ARTIFICIAL INTELLIGENCE FOR SMART COMMUNITY, Malaysia
- **Journal**: Conference proceedings \c{opyright} 2023 International Conference
  on Artificial Intelligence for Smart Community
- **Summary**: In recent advancement towards computer based diagnostics system, the classification of brain tumor images is a challenging task. This paper mainly focuses on elevating the classification accuracy of brain tumor images with transfer learning based deep neural network. The classification approach is started with the image augmentation operation including rotation, zoom, hori-zontal flip, width shift, height shift, and shear to increase the diversity in image datasets. Then the general features of the input brain tumor images are extracted based on a pre-trained transfer learning method comprised of Inception-v3. Fi-nally, the deep neural network with 4 customized layers is employed for classi-fying the brain tumors in most frequent brain tumor types as meningioma, glioma, and pituitary. The proposed model acquires an effective performance with an overall accuracy of 96.25% which is much improved than some existing multi-classification methods. Whereas, the fine-tuning of hyper-parameters and inclusion of customized DNN with the Inception-v3 model results in an im-provement of the classification accuracy.



### Texture Generation Using A Graph Generative Adversarial Network And Differentiable Rendering
- **Arxiv ID**: http://arxiv.org/abs/2206.08547v2
- **DOI**: 10.1007/978-3-031-25825-1_28
- **Categories**: **cs.CV**, I.5
- **Links**: [PDF](http://arxiv.org/pdf/2206.08547v2)
- **Published**: 2022-06-17 04:56:03+00:00
- **Updated**: 2023-02-08 00:11:19+00:00
- **Authors**: Dharma KC, Clayton T. Morrison, Bradley Walls
- **Comment**: The final publication is available at Springer via
  http://dx.doi.org/10.1007/978-3-031-25825-1_28
- **Journal**: Springer.13836.(2023)388-401
- **Summary**: Novel photo-realistic texture synthesis is an important task for generating novel scenes, including asset generation for 3D simulations. However, to date, these methods predominantly generate textured objects in 2D space. If we rely on 2D object generation, then we need to make a computationally expensive forward pass each time we change the camera viewpoint or lighting. Recent work that can generate textures in 3D requires 3D component segmentation that is expensive to acquire. In this work, we present a novel conditional generative architecture that we call a graph generative adversarial network (GGAN) that can generate textures in 3D by learning object component information in an unsupervised way. In this framework, we do not need an expensive forward pass whenever the camera viewpoint or lighting changes, and we do not need expensive 3D part information for training, yet the model can generalize to unseen 3D meshes and generate appropriate novel 3D textures. We compare this approach against state-of-the-art texture generation methods and demonstrate that the GGAN obtains significantly better texture generation quality (according to Frechet inception distance). We release our model source code as open source.



### Rarity Score : A New Metric to Evaluate the Uncommonness of Synthesized Images
- **Arxiv ID**: http://arxiv.org/abs/2206.08549v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08549v2)
- **Published**: 2022-06-17 05:16:16+00:00
- **Updated**: 2022-06-26 13:55:00+00:00
- **Authors**: Jiyeon Han, Hwanil Choi, Yunjey Choi, Junho Kim, Jung-Woo Ha, Jaesik Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Evaluation metrics in image synthesis play a key role to measure performances of generative models. However, most metrics mainly focus on image fidelity. Existing diversity metrics are derived by comparing distributions, and thus they cannot quantify the diversity or rarity degree of each generated image. In this work, we propose a new evaluation metric, called `rarity score', to measure the individual rarity of each image synthesized by generative models. We first show empirical observation that common samples are close to each other and rare samples are far from each other in nearest-neighbor distances of feature space. We then use our metric to demonstrate that the extent to which different generative models produce rare images can be effectively compared. We also propose a method to compare rarities between datasets that share the same concept such as CelebA-HQ and FFHQ. Finally, we analyze the use of metrics in different designs of feature spaces to better understand the relationship between feature spaces and resulting sparse images. Code will be publicly available online for the research community.



### COVID-19 Detection using Transfer Learning with Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2206.08557v1
- **DOI**: 10.1109/ICREST51555.2021.9331029
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08557v1)
- **Published**: 2022-06-17 05:30:14+00:00
- **Updated**: 2022-06-17 05:30:14+00:00
- **Authors**: Pramit Dutta, Tanny Roy, Nafisa Anjum
- **Comment**: 4 pages, 4 figures, 2nd International Conference on Robotics,
  Electrical and Signal Processing Techniques (ICREST), DHAKA, Bangladesh
- **Journal**: 2nd International Conference on Robotics, Electrical and Signal
  Processing Techniques (ICREST), DHAKA, Bangladesh, 2021, pp. 429-432
- **Summary**: The Novel Coronavirus disease 2019 (COVID-19) is a fatal infectious disease, first recognized in December 2019 in Wuhan, Hubei, China, and has gone on an epidemic situation. Under these circumstances, it became more important to detect COVID-19 in infected people. Nowadays, the testing kits are gradually lessening in number compared to the number of infected population. Under recent prevailing conditions, the diagnosis of lung disease by analyzing chest CT (Computed Tomography) images has become an important tool for both diagnosis and prophecy of COVID-19 patients. In this study, a Transfer learning strategy (CNN) for detecting COVID-19 infection from CT images has been proposed. In the proposed model, a multilayer Convolutional neural network (CNN) with Transfer learning model Inception V3 has been designed. Similar to CNN, it uses convolution and pooling to extract features, but this transfer learning model contains weights of dataset Imagenet. Thus it can detect features very effectively which gives it an upper hand for achieving better accuracy.



### Active Data Discovery: Mining Unknown Data using Submodular Information Measures
- **Arxiv ID**: http://arxiv.org/abs/2206.08566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08566v1)
- **Published**: 2022-06-17 05:52:18+00:00
- **Updated**: 2022-06-17 05:52:18+00:00
- **Authors**: Suraj Kothawade, Shivang Chopra, Saikat Ghosh, Rishabh Iyer
- **Comment**: None
- **Journal**: None
- **Summary**: Active Learning is a very common yet powerful framework for iteratively and adaptively sampling subsets of the unlabeled sets with a human in the loop with the goal of achieving labeling efficiency. Most real world datasets have imbalance either in classes and slices, and correspondingly, parts of the dataset are rare. As a result, there has been a lot of work in designing active learning approaches for mining these rare data instances. Most approaches assume access to a seed set of instances which contain these rare data instances. However, in the event of more extreme rareness, it is reasonable to assume that these rare data instances (either classes or slices) may not even be present in the seed labeled set, and a critical need for the active learning paradigm is to efficiently discover these rare data instances. In this work, we provide an active data discovery framework which can mine unknown data slices and classes efficiently using the submodular conditional gain and submodular conditional mutual information functions. We provide a general algorithmic framework which works in a number of scenarios including image classification and object detection and works with both rare classes and rare slices present in the unlabeled set. We show significant accuracy and labeling efficiency gains with our approach compared to existing state-of-the-art active learning approaches for actively discovering these rare classes and slices.



### Rectify ViT Shortcut Learning by Visual Saliency
- **Arxiv ID**: http://arxiv.org/abs/2206.08567v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T07, 68T10, J.6; I.5.2; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2206.08567v1)
- **Published**: 2022-06-17 05:54:07+00:00
- **Updated**: 2022-06-17 05:54:07+00:00
- **Authors**: Chong Ma, Lin Zhao, Yuzhong Chen, David Weizhong Liu, Xi Jiang, Tuo Zhang, Xintao Hu, Dinggang Shen, Dajiang Zhu, Tianming Liu
- **Comment**: NeurIPS2022 Under Review
- **Journal**: None
- **Summary**: Shortcut learning is common but harmful to deep learning models, leading to degenerated feature representations and consequently jeopardizing the model's generalizability and interpretability. However, shortcut learning in the widely used Vision Transformer framework is largely unknown. Meanwhile, introducing domain-specific knowledge is a major approach to rectifying the shortcuts, which are predominated by background related factors. For example, in the medical imaging field, eye-gaze data from radiologists is an effective human visual prior knowledge that has the great potential to guide the deep learning models to focus on meaningful foreground regions of interest. However, obtaining eye-gaze data is time-consuming, labor-intensive and sometimes even not practical. In this work, we propose a novel and effective saliency-guided vision transformer (SGT) model to rectify shortcut learning in ViT with the absence of eye-gaze data. Specifically, a computational visual saliency model is adopted to predict saliency maps for input image samples. Then, the saliency maps are used to distil the most informative image patches. In the proposed SGT, the self-attention among image patches focus only on the distilled informative ones. Considering this distill operation may lead to global information lost, we further introduce, in the last encoder layer, a residual connection that captures the self-attention across all the image patches. The experiment results on four independent public datasets show that our SGT framework can effectively learn and leverage human prior knowledge without eye gaze data and achieves much better performance than baselines. Meanwhile, it successfully rectifies the harmful shortcut learning and significantly improves the interpretability of the ViT model, demonstrating the promise of transferring human prior knowledge derived visual saliency in rectifying shortcut learning



### Multi-Contextual Predictions with Vision Transformer for Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.08568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08568v1)
- **Published**: 2022-06-17 05:54:31+00:00
- **Updated**: 2022-06-17 05:54:31+00:00
- **Authors**: Joo-Yeon Lee, Woo-Jeoung Nam, Seong-Whan Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Video Anomaly Detection(VAD) has been traditionally tackled in two main methodologies: the reconstruction-based approach and the prediction-based one. As the reconstruction-based methods learn to generalize the input image, the model merely learns an identity function and strongly causes the problem called generalizing issue. On the other hand, since the prediction-based ones learn to predict a future frame given several previous frames, they are less sensitive to the generalizing issue. However, it is still uncertain if the model can learn the spatio-temporal context of a video. Our intuition is that the understanding of the spatio-temporal context of a video plays a vital role in VAD as it provides precise information on how the appearance of an event in a video clip changes. Hence, to fully exploit the context information for anomaly detection in video circumstances, we designed the transformer model with three different contextual prediction streams: masked, whole and partial. By learning to predict the missing frames of consecutive normal frames, our model can effectively learn various normality patterns in the video, which leads to a high reconstruction error at the abnormal cases that are unsuitable to the learned context. To verify the effectiveness of our approach, we assess our model on the public benchmark datasets: USCD Pedestrian 2, CUHK Avenue and ShanghaiTech and evaluate the performance with the anomaly score metric of reconstruction error. The results demonstrate that our proposed approach achieves a competitive performance compared to the existing video anomaly detection methods.



### Enhanced Bi-directional Motion Estimation for Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2206.08572v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08572v3)
- **Published**: 2022-06-17 06:08:43+00:00
- **Updated**: 2022-11-07 02:46:13+00:00
- **Authors**: Xin Jin, Longhai Wu, Guotao Shen, Youxin Chen, Jie Chen, Jayoon Koo, Cheul-hee Hahm
- **Comment**: Accepted by WACV 2023
- **Journal**: None
- **Summary**: We present a novel simple yet effective algorithm for motion-based video frame interpolation. Existing motion-based interpolation methods typically rely on a pre-trained optical flow model or a U-Net based pyramid network for motion estimation, which either suffer from large model size or limited capacity in handling complex and large motion cases. In this work, by carefully integrating intermediateoriented forward-warping, lightweight feature encoder, and correlation volume into a pyramid recurrent framework, we derive a compact model to simultaneously estimate the bidirectional motion between input frames. It is 15 times smaller in size than PWC-Net, yet enables more reliable and flexible handling of challenging motion cases. Based on estimated bi-directional motion, we forward-warp input frames and their context features to intermediate frame, and employ a synthesis network to estimate the intermediate frame from warped representations. Our method achieves excellent performance on a broad range of video frame interpolation benchmarks. Code and trained models are available at \url{https://github.com/srcn-ivl/EBME}.



### A Flexible Diffusion Model
- **Arxiv ID**: http://arxiv.org/abs/2206.10365v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.10365v1)
- **Published**: 2022-06-17 06:46:58+00:00
- **Updated**: 2022-06-17 06:46:58+00:00
- **Authors**: Weitao Du, Tao Yang, He Zhang, Yuanqi Du
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion (score-based) generative models have been widely used for modeling various types of complex data, including images, audios, and point clouds. Recently, the deep connection between forward-backward stochastic differential equations (SDEs) and diffusion-based models has been revealed, and several new variants of SDEs are proposed (e.g., sub-VP, critically-damped Langevin) along this line. Despite the empirical success of the hand-crafted fixed forward SDEs, a great quantity of proper forward SDEs remain unexplored. In this work, we propose a general framework for parameterizing the diffusion model, especially the spatial part of the forward SDE. An abstract formalism is introduced with theoretical guarantees, and its connection with previous diffusion models is leveraged. We demonstrate the theoretical advantage of our method from an optimization perspective. Numerical experiments on synthetic datasets, MINIST and CIFAR10 are also presented to validate the effectiveness of our framework.



### HairFIT: Pose-Invariant Hairstyle Transfer via Flow-based Hair Alignment and Semantic-Region-Aware Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2206.08585v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08585v1)
- **Published**: 2022-06-17 06:55:20+00:00
- **Updated**: 2022-06-17 06:55:20+00:00
- **Authors**: Chaeyeon Chung, Taewoo Kim, Hyelin Nam, Seunghwan Choi, Gyojung Gu, Sunghyun Park, Jaegul Choo
- **Comment**: BMVC 2021 Oral Presentation
- **Journal**: None
- **Summary**: Hairstyle transfer is the task of modifying a source hairstyle to a target one. Although recent hairstyle transfer models can reflect the delicate features of hairstyles, they still have two major limitations. First, the existing methods fail to transfer hairstyles when a source and a target image have different poses (e.g., viewing direction or face size), which is prevalent in the real world. Also, the previous models generate unrealistic images when there is a non-trivial amount of regions in the source image occluded by its original hair. When modifying long hair to short hair, shoulders or backgrounds occluded by the long hair need to be inpainted. To address these issues, we propose a novel framework for pose-invariant hairstyle transfer, HairFIT. Our model consists of two stages: 1) flow-based hair alignment and 2) hair synthesis. In the hair alignment stage, we leverage a keypoint-based optical flow estimator to align a target hairstyle with a source pose. Then, we generate a final hairstyle-transferred image in the hair synthesis stage based on Semantic-region-aware Inpainting Mask (SIM) estimator. Our SIM estimator divides the occluded regions in the source image into different semantic regions to reflect their distinct features during the inpainting. To demonstrate the effectiveness of our model, we conduct quantitative and qualitative evaluations using multi-view datasets, K-hairstyle and VoxCeleb. The results indicate that HairFIT achieves a state-of-the-art performance by successfully transferring hairstyles between images of different poses, which has never been achieved before.



### Uncovering variability in human driving behavior through automatic extraction of similar traffic scenes from large naturalistic datasets
- **Arxiv ID**: http://arxiv.org/abs/2206.13386v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13386v2)
- **Published**: 2022-06-17 06:59:51+00:00
- **Updated**: 2023-03-15 07:43:31+00:00
- **Authors**: Olger Siebinga, Arkady Zgonnikov, David Abbink
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, multiple naturalistic traffic datasets of human-driven trajectories have been published (e.g., highD, NGSim, and pNEUMA). These datasets have been used in studies that investigate variability in human driving behavior, for example for scenario-based validation of autonomous vehicle (AV) behavior, modeling driver behavior, or validating driver models. Thus far, these studies focused on the variability on an operational level (e.g., velocity profiles during a lane change), not on a tactical level (i.e., to change lanes or not). Investigating the variability on both levels is necessary to develop driver models and AVs that include multiple tactical behaviors. To expose multi-level variability, the human responses to the same traffic scene could be investigated. However, no method exists to automatically extract similar scenes from datasets. Here, we present a four-step extraction method that uses the Hausdorff distance, a mathematical distance metric for sets. We performed a case study on the highD dataset that showed that the method is practically applicable. The human responses to the selected scenes exposed the variability on both the tactical and operational levels. With this new method, the variability in operational and tactical human behavior can be investigated, without the need for costly and time-consuming driving-simulator experiments.



### On Efficient Real-Time Semantic Segmentation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2206.08605v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.08605v2)
- **Published**: 2022-06-17 08:00:27+00:00
- **Updated**: 2022-08-16 08:47:00+00:00
- **Authors**: Christopher J. Holder, Muhammad Shafique
- **Comment**: 19 pages, 13 figures, 4 tables This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible
- **Journal**: None
- **Summary**: Semantic segmentation is the problem of assigning a class label to every pixel in an image, and is an important component of an autonomous vehicle vision stack for facilitating scene understanding and object detection. However, many of the top performing semantic segmentation models are extremely complex and cumbersome, and as such are not suited to deployment onboard autonomous vehicle platforms where computational resources are limited and low-latency operation is a vital requirement. In this survey, we take a thorough look at the works that aim to address this misalignment with more compact and efficient models capable of deployment on low-memory embedded systems while meeting the constraint of real-time inference. We discuss several of the most prominent works in the field, placing them within a taxonomy based on their major contributions, and finally we evaluate the inference speed of the discussed models under consistent hardware and software setups that represent a typical research environment with high-end GPU and a realistic deployed scenario using low-memory embedded GPU hardware. Our experimental results demonstrate that many works are capable of real-time performance on resource-constrained hardware, while illustrating the consistent trade-off between latency and accuracy.



### Masked Autoencoders for Generic Event Boundary Detection CVPR'2022 Kinetics-GEBD Challenge
- **Arxiv ID**: http://arxiv.org/abs/2206.08610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08610v1)
- **Published**: 2022-06-17 08:10:27+00:00
- **Updated**: 2022-06-17 08:10:27+00:00
- **Authors**: Rui He, Yuanxi Sun, Youzeng Li, Zuwei Huang, Feng Hu, Xu Cheng, Jie Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Generic Event Boundary Detection (GEBD) tasks aim at detecting generic, taxonomy-free event boundaries that segment a whole video into chunks. In this paper, we apply Masked Autoencoders to improve algorithm performance on the GEBD tasks. Our approach mainly adopted the ensemble of Masked Autoencoders fine-tuned on the GEBD task as a self-supervised learner with other base models. Moreover, we also use a semi-supervised pseudo-label method to take full advantage of the abundant unlabeled Kinetics-400 data while training. In addition, we propose a soft-label method to partially balance the positive and negative samples and alleviate the problem of ambiguous labeling in this task. Lastly, a tricky segmentation alignment policy is implemented to refine boundaries predicted by our models to more accurate locations. With our approach, we achieved 85.94% on the F1-score on the Kinetics-GEBD test set, which improved the F1-score by 2.31% compared to the winner of the 2021 Kinetics-GEBD Challenge. Our code is available at https://github.com/ContentAndMaterialPortrait/MAE-GEBD.



### OADAT: Experimental and Synthetic Clinical Optoacoustic Data for Standardized Image Processing
- **Arxiv ID**: http://arxiv.org/abs/2206.08612v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08612v2)
- **Published**: 2022-06-17 08:11:26+00:00
- **Updated**: 2023-05-03 15:40:04+00:00
- **Authors**: Firat Ozdemir, Berkan Lafci, Xosé Luís Deán-Ben, Daniel Razansky, Fernando Perez-Cruz
- **Comment**: Accepted to TMLR. 32 pages, 24 figures, 9 tables
- **Journal**: Transactions on Machine Learning Research (2023) 2835-8856
- **Summary**: Optoacoustic (OA) imaging is based on excitation of biological tissues with nanosecond-duration laser pulses followed by subsequent detection of ultrasound waves generated via light-absorption-mediated thermoelastic expansion. OA imaging features a powerful combination between rich optical contrast and high resolution in deep tissues. This enabled the exploration of a number of attractive new applications both in clinical and laboratory settings. However, no standardized datasets generated with different types of experimental set-up and associated processing methods are available to facilitate advances in broader applications of OA in clinical settings. This complicates an objective comparison between new and established data processing methods, often leading to qualitative results and arbitrary interpretations of the data. In this paper, we provide both experimental and synthetic OA raw signals and reconstructed image domain datasets rendered with different experimental parameters and tomographic acquisition geometries. We further provide trained neural networks to tackle three important challenges related to OA image processing, namely accurate reconstruction under limited view tomographic conditions, removal of spatial undersampling artifacts and anatomical segmentation for improved image reconstruction. Specifically, we define 44 experiments corresponding to the aforementioned challenges as benchmarks to be used as a reference for the development of more advanced processing methods.



### Understanding Aesthetics with Language: A Photo Critique Dataset for Aesthetic Assessment
- **Arxiv ID**: http://arxiv.org/abs/2206.08614v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2206.08614v3)
- **Published**: 2022-06-17 08:16:20+00:00
- **Updated**: 2022-09-21 15:30:50+00:00
- **Authors**: Daniel Vera Nieto, Luigi Celona, Clara Fernandez-Labrador
- **Comment**: Accepted to NeurIPS Track on Datasets and Benchmarks 2022
- **Journal**: None
- **Summary**: Computational inference of aesthetics is an ill-defined task due to its subjective nature. Many datasets have been proposed to tackle the problem by providing pairs of images and aesthetic scores based on human ratings. However, humans are better at expressing their opinion, taste, and emotions by means of language rather than summarizing them in a single number. In fact, photo critiques provide much richer information as they reveal how and why users rate the aesthetics of visual stimuli. In this regard, we propose the Reddit Photo Critique Dataset (RPCD), which contains tuples of image and photo critiques. RPCD consists of 74K images and 220K comments and is collected from a Reddit community used by hobbyists and professional photographers to improve their photography skills by leveraging constructive community feedback. The proposed dataset differs from previous aesthetics datasets mainly in three aspects, namely (i) the large scale of the dataset and the extension of the comments criticizing different aspects of the image, (ii) it contains mostly UltraHD images, and (iii) it can easily be extended to new data as it is collected through an automatic pipeline. To the best of our knowledge, in this work, we propose the first attempt to estimate the aesthetic quality of visual stimuli from the critiques. To this end, we exploit the polarity of the sentiment of criticism as an indicator of aesthetic judgment. We demonstrate how sentiment polarity correlates positively with the aesthetic judgment available for two aesthetic assessment benchmarks. Finally, we experiment with several models by using the sentiment scores as a target for ranking images. Dataset and baselines are available (https://github.com/mediatechnologycenter/aestheval).



### Learning Using Privileged Information for Zero-Shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.08632v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45(Primary) 68T07(Secondary), I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2206.08632v2)
- **Published**: 2022-06-17 08:46:09+00:00
- **Updated**: 2022-06-22 09:53:07+00:00
- **Authors**: Zhiyi Gao, Yonghong Hou, Wanqing Li, Zihui Guo, Bin Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-Shot Action Recognition (ZSAR) aims to recognize video actions that have never been seen during training. Most existing methods assume a shared semantic space between seen and unseen actions and intend to directly learn a mapping from a visual space to the semantic space. This approach has been challenged by the semantic gap between the visual space and semantic space. This paper presents a novel method that uses object semantics as privileged information to narrow the semantic gap and, hence, effectively, assist the learning. In particular, a simple hallucination network is proposed to implicitly extract object semantics during testing without explicitly extracting objects and a cross-attention module is developed to augment visual feature with the object semantics. Experiments on the Olympic Sports, HMDB51 and UCF101 datasets have shown that the proposed method outperforms the state-of-the-art methods by a large margin.



### Minimum Noticeable Difference based Adversarial Privacy Preserving Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2206.08638v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.08638v1)
- **Published**: 2022-06-17 09:02:12+00:00
- **Updated**: 2022-06-17 09:02:12+00:00
- **Authors**: Wen Sun, Jian Jin, Weisi Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models are found to be vulnerable to adversarial examples, as wrong predictions can be caused by small perturbation in input for deep learning models. Most of the existing works of adversarial image generation try to achieve attacks for most models, while few of them make efforts on guaranteeing the perceptual quality of the adversarial examples. High quality adversarial examples matter for many applications, especially for the privacy preserving. In this work, we develop a framework based on the Minimum Noticeable Difference (MND) concept to generate adversarial privacy preserving images that have minimum perceptual difference from the clean ones but are able to attack deep learning models. To achieve this, an adversarial loss is firstly proposed to make the deep learning models attacked by the adversarial images successfully. Then, a perceptual quality-preserving loss is developed by taking the magnitude of perturbation and perturbation-caused structural and gradient changes into account, which aims to preserve high perceptual quality for adversarial image generation. To the best of our knowledge, this is the first work on exploring quality-preserving adversarial image generation based on the MND concept for privacy preserving. To evaluate its performance in terms of perceptual quality, the deep models on image classification and face recognition are tested with the proposed method and several anchor methods in this work. Extensive experimental results demonstrate that the proposed MND framework is capable of generating adversarial images with remarkably improved performance metrics (e.g., PSNR, SSIM, and MOS) than that generated with the anchor methods.



### Uncertainty-aware Evaluation of Time-Series Classification for Online Handwriting Recognition with Domain Shift
- **Arxiv ID**: http://arxiv.org/abs/2206.08640v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 62F15, H.1.1
- **Links**: [PDF](http://arxiv.org/pdf/2206.08640v1)
- **Published**: 2022-06-17 09:05:01+00:00
- **Updated**: 2022-06-17 09:05:01+00:00
- **Authors**: Andreas Klaß, Sven M. Lorenz, Martin W. Lauer-Schmaltz, David Rügamer, Bernd Bischl, Christopher Mutschler, Felix Ott
- **Comment**: None
- **Journal**: None
- **Summary**: For many applications, analyzing the uncertainty of a machine learning model is indispensable. While research of uncertainty quantification (UQ) techniques is very advanced for computer vision applications, UQ methods for spatio-temporal data are less studied. In this paper, we focus on models for online handwriting recognition, one particular type of spatio-temporal data. The data is observed from a sensor-enhanced pen with the goal to classify written characters. We conduct a broad evaluation of aleatoric (data) and epistemic (model) UQ based on two prominent techniques for Bayesian inference, Stochastic Weight Averaging-Gaussian (SWAG) and Deep Ensembles. Next to a better understanding of the model, UQ techniques can detect out-of-distribution data and domain shifts when combining right-handed and left-handed writers (an underrepresented group).



### Diverse Multiple Trajectory Prediction Using a Two-stage Prediction Network Trained with Lane Loss
- **Arxiv ID**: http://arxiv.org/abs/2206.08641v2
- **DOI**: 10.1109/LRA.2022.3231525
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08641v2)
- **Published**: 2022-06-17 09:09:51+00:00
- **Updated**: 2023-01-04 05:44:55+00:00
- **Authors**: Sanmin Kim, Hyeongseok Jeon, Junwon Choi, Dongsuk Kum
- **Comment**: RA-L accepted
- **Journal**: IEEE Robotics and Automation Letters (2022)
- **Summary**: Prior arts in the field of motion predictions for autonomous driving tend to focus on finding a trajectory that is close to the ground truth trajectory. Such problem formulations and approaches, however, frequently lead to loss of diversity and biased trajectory predictions. Therefore, they are unsuitable for real-world autonomous driving where diverse and road-dependent multimodal trajectory predictions are critical for safety. To this end, this study proposes a novel loss function, \textit{Lane Loss}, that ensures map-adaptive diversity and accommodates geometric constraints. A two-stage trajectory prediction architecture with a novel trajectory candidate proposal module, \textit{Trajectory Prediction Attention (TPA)}, is trained with Lane Loss encourages multiple trajectories to be diversely distributed, covering feasible maneuvers in a map-aware manner. Furthermore, considering that the existing trajectory performance metrics are focusing on evaluating the accuracy based on the ground truth future trajectory, a quantitative evaluation metric is also suggested to evaluate the diversity of predicted multiple trajectories. The experiments performed on the Argoverse dataset show that the proposed method significantly improves the diversity of the predicted trajectories without sacrificing the prediction accuracy.



### Local Slot Attention for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2206.08645v2
- **DOI**: 10.1145/3512527.3531366
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08645v2)
- **Published**: 2022-06-17 09:21:26+00:00
- **Updated**: 2022-06-22 02:32:32+00:00
- **Authors**: Yifeng Zhuang, Qiang Sun, Yanwei Fu, Lifeng Chen, Xiangyang Xue
- **Comment**: ICMR 2022
- **Journal**: None
- **Summary**: Vision-and-language navigation (VLN), a frontier study aiming to pave the way for general-purpose robots, has been a hot topic in the computer vision and natural language processing community. The VLN task requires an agent to navigate to a goal location following natural language instructions in unfamiliar environments.   Recently, transformer-based models have gained significant improvements on the VLN task. Since the attention mechanism in the transformer architecture can better integrate inter- and intra-modal information of vision and language.   However, there exist two problems in current transformer-based models.   1) The models process each view independently without taking the integrity of the objects into account.   2) During the self-attention operation in the visual modality, the views that are spatially distant can be inter-weaved with each other without explicit restriction. This kind of mixing may introduce extra noise instead of useful information.   To address these issues, we propose 1) A slot-attention based module to incorporate information from segmentation of the same object. 2) A local attention mask mechanism to limit the visual attention span. The proposed modules can be easily plugged into any VLN architecture and we use the Recurrent VLN-Bert as our base model. Experiments on the R2R dataset show that our model has achieved the state-of-the-art results.



### All Mistakes Are Not Equal: Comprehensive Hierarchy Aware Multi-label Predictions (CHAMP)
- **Arxiv ID**: http://arxiv.org/abs/2206.08653v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08653v1)
- **Published**: 2022-06-17 09:32:48+00:00
- **Updated**: 2022-06-17 09:32:48+00:00
- **Authors**: Ashwin Vaswani, Gaurav Aggarwal, Praneeth Netrapalli, Narayan G Hegde
- **Comment**: None
- **Journal**: None
- **Summary**: This paper considers the problem of Hierarchical Multi-Label Classification (HMC), where (i) several labels can be present for each example, and (ii) labels are related via a domain-specific hierarchy tree. Guided by the intuition that all mistakes are not equal, we present Comprehensive Hierarchy Aware Multi-label Predictions (CHAMP), a framework that penalizes a misprediction depending on its severity as per the hierarchy tree. While there have been works that apply such an idea to single-label classification, to the best of our knowledge, there are limited such works for multilabel classification focusing on the severity of mistakes. The key reason is that there is no clear way of quantifying the severity of a misprediction a priori in the multilabel setting. In this work, we propose a simple but effective metric to quantify the severity of a mistake in HMC, naturally leading to CHAMP. Extensive experiments on six public HMC datasets across modalities (image, audio, and text) demonstrate that incorporating hierarchical information leads to substantial gains as CHAMP improves both AUPRC (2.6% median percentage improvement) and hierarchical metrics (2.85% median percentage improvement), over stand-alone hierarchical or multilabel classification methods. Compared to standard multilabel baselines, CHAMP provides improved AUPRC in both robustness (8.87% mean percentage improvement ) and less data regimes. Further, our method provides a framework to enhance existing multilabel classification algorithms with better mistakes (18.1% mean percentage increment).



### Learning Implicit Feature Alignment Function for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.08655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08655v1)
- **Published**: 2022-06-17 09:40:14+00:00
- **Updated**: 2022-06-17 09:40:14+00:00
- **Authors**: Hanzhe Hu, Yinbo Chen, Jiarui Xu, Shubhankar Borse, Hong Cai, Fatih Porikli, Xiaolong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Integrating high-level context information with low-level details is of central importance in semantic segmentation. Towards this end, most existing segmentation models apply bilinear up-sampling and convolutions to feature maps of different scales, and then align them at the same resolution. However, bilinear up-sampling blurs the precise information learned in these feature maps and convolutions incur extra computation costs. To address these issues, we propose the Implicit Feature Alignment function (IFA). Our method is inspired by the rapidly expanding topic of implicit neural representations, where coordinate-based neural networks are used to designate fields of signals. In IFA, feature vectors are viewed as representing a 2D field of information. Given a query coordinate, nearby feature vectors with their relative coordinates are taken from the multi-level feature maps and then fed into an MLP to generate the corresponding output. As such, IFA implicitly aligns the feature maps at different levels and is capable of producing segmentation maps in arbitrary resolutions. We demonstrate the efficacy of IFA on multiple datasets, including Cityscapes, PASCAL Context, and ADE20K. Our method can be combined with improvement on various architectures, and it achieves state-of-the-art computation-accuracy trade-off on common benchmarks. Code will be made available at https://github.com/hzhupku/IFA.



### BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.08657v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.08657v5)
- **Published**: 2022-06-17 09:42:35+00:00
- **Updated**: 2023-06-09 12:36:33+00:00
- **Authors**: Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan
- **Comment**: Accepted by AAAI 2023, Oral
- **Journal**: None
- **Summary**: Vision-Language (VL) models with the Two-Tower architecture have dominated visual-language representation learning in recent years. Current VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a deep cross-modal encoder, or feed the last-layer uni-modal representations from the deep pre-trained uni-modal encoders into the top cross-modal encoder. Both approaches potentially restrict vision-language representation learning and limit model performance. In this paper, we propose BridgeTower, which introduces multiple bridge layers that build a connection between the top layers of uni-modal encoders and each layer of the cross-modal encoder. This enables effective bottom-up cross-modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni-modal encoders in the cross-modal encoder. Pre-trained with only 4M images, BridgeTower achieves state-of-the-art performance on various downstream vision-language tasks. In particular, on the VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming the previous state-of-the-art model METER by 1.09% with the same pre-training data and almost negligible additional parameters and computational costs. Notably, when further scaling the model, BridgeTower achieves an accuracy of 81.15%, surpassing models that are pre-trained on orders-of-magnitude larger datasets. Code and checkpoints are available at https://github.com/microsoft/BridgeTower.



### FiT: Parameter Efficient Few-shot Transfer Learning for Personalized and Federated Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.08671v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.08671v2)
- **Published**: 2022-06-17 10:17:20+00:00
- **Updated**: 2023-02-02 20:22:18+00:00
- **Authors**: Aliaksandra Shysheya, John Bronskill, Massimiliano Patacchiola, Sebastian Nowozin, Richard E Turner
- **Comment**: None
- **Journal**: None
- **Summary**: Modern deep learning systems are increasingly deployed in situations such as personalization and federated learning where it is necessary to support i) learning on small amounts of data, and ii) communication efficient distributed training protocols. In this work, we develop FiLM Transfer (FiT) which fulfills these requirements in the image classification setting by combining ideas from transfer learning (fixed pretrained backbones and fine-tuned FiLM adapter layers) and meta-learning (automatically configured Naive Bayes classifiers and episodic training) to yield parameter efficient models with superior classification accuracy at low-shot. The resulting parameter efficiency is key for enabling few-shot learning, inexpensive model updates for personalization, and communication efficient federated learning. We experiment with FiT on a wide range of downstream datasets and show that it achieves better classification accuracy than the leading Big Transfer (BiT) algorithm at low-shot and achieves state-of-the art accuracy on the challenging VTAB-1k benchmark, with fewer than 1% of the updateable parameters. Finally, we demonstrate the parameter efficiency and superior accuracy of FiT in distributed low-shot applications including model personalization and federated learning where model update size is an important performance metric.



### AggNet: Learning to Aggregate Faces for Group Membership Verification
- **Arxiv ID**: http://arxiv.org/abs/2206.08683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08683v1)
- **Published**: 2022-06-17 10:48:34+00:00
- **Updated**: 2022-06-17 10:48:34+00:00
- **Authors**: Marzieh Gheisari, Javad Amirian, Teddy Furon, Laurent Amsaleg
- **Comment**: None
- **Journal**: None
- **Summary**: In some face recognition applications, we are interested to verify whether an individual is a member of a group, without revealing their identity. Some existing methods, propose a mechanism for quantizing precomputed face descriptors into discrete embeddings and aggregating them into one group representation. However, this mechanism is only optimized for a given closed set of individuals and needs to learn the group representations from scratch every time the groups are changed. In this paper, we propose a deep architecture that jointly learns face descriptors and the aggregation mechanism for better end-to-end performances. The system can be applied to new groups with individuals never seen before and the scheme easily manages new memberships or membership endings. We show through experiments on multiple large-scale wild-face datasets, that the proposed method leads to higher verification performance compared to other baselines.



### Sparse Double Descent: Where Network Pruning Aggravates Overfitting
- **Arxiv ID**: http://arxiv.org/abs/2206.08684v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08684v1)
- **Published**: 2022-06-17 11:02:15+00:00
- **Updated**: 2022-06-17 11:02:15+00:00
- **Authors**: Zheng He, Zeke Xie, Quanzhi Zhu, Zengchang Qin
- **Comment**: ICML 2022
- **Journal**: None
- **Summary**: People usually believe that network pruning not only reduces the computational cost of deep networks, but also prevents overfitting by decreasing model capacity. However, our work surprisingly discovers that network pruning sometimes even aggravates overfitting. We report an unexpected sparse double descent phenomenon that, as we increase model sparsity via network pruning, test performance first gets worse (due to overfitting), then gets better (due to relieved overfitting), and gets worse at last (due to forgetting useful information). While recent studies focused on the deep double descent with respect to model overparameterization, they failed to recognize that sparsity may also cause double descent. In this paper, we have three main contributions. First, we report the novel sparse double descent phenomenon through extensive experiments. Second, for this phenomenon, we propose a novel learning distance interpretation that the curve of $\ell_{2}$ learning distance of sparse models (from initialized parameters to final parameters) may correlate with the sparse double descent curve well and reflect generalization better than minima flatness. Third, in the context of sparse double descent, a winning ticket in the lottery ticket hypothesis surprisingly may not always win.



### FreeREA: Training-Free Evolution-based Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2207.05135v2
- **DOI**: 10.1109/WACV56688.2023.00154
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.05135v2)
- **Published**: 2022-06-17 11:16:28+00:00
- **Updated**: 2023-05-10 10:04:17+00:00
- **Authors**: Niccolò Cavagnero, Luca Robbiano, Barbara Caputo, Giuseppe Averta
- **Comment**: 16 pages, 4 figures
- **Journal**: 2023 IEEE/CVF Winter Conference on Applications of Computer Vision
  (WACV)
- **Summary**: In the last decade, most research in Machine Learning contributed to the improvement of existing models, with the aim of increasing the performance of neural networks for the solution of a variety of different tasks. However, such advancements often come at the cost of an increase of model memory and computational requirements. This represents a significant limitation for the deployability of research output in realistic settings, where the cost, the energy consumption, and the complexity of the framework play a crucial role. To solve this issue, the designer should search for models that maximise the performance while limiting its footprint. Typical approaches to reach this goal rely either on manual procedures, which cannot guarantee the optimality of the final design, or upon Neural Architecture Search algorithms to automatise the process, at the expenses of extremely high computational time. This paper provides a solution for the fast identification of a neural network that maximises the model accuracy while preserving size and computational constraints typical of tiny devices. Our approach, named FreeREA, is a custom cell-based evolution NAS algorithm that exploits an optimised combination of training-free metrics to rank architectures during the search, thus without need of model training. Our experiments, carried out on the common benchmarks NAS-Bench-101 and NATS-Bench, demonstrate that i) FreeREA is a fast, efficient, and effective search method for models automatic design; ii) it outperforms State of the Art training-based and training-free techniques in all the datasets and benchmarks considered, and iii) it can easily generalise to constrained scenarios, representing a competitive solution for fast Neural Architecture Search in generic constrained applications. The code is available at \url{https://github.com/NiccoloCavagnero/FreeREA}.



### Towards Real-Time Visual Tracking with Graded Color-names Features
- **Arxiv ID**: http://arxiv.org/abs/2206.08701v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08701v1)
- **Published**: 2022-06-17 11:38:37+00:00
- **Updated**: 2022-06-17 11:38:37+00:00
- **Authors**: Lin Li, Guoli Wang, Xuemei Guo
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: MeanShift algorithm has been widely used in tracking tasks because of its simplicity and efficiency. However, the traditional MeanShift algorithm needs to label the initial region of the target, which reduces the applicability of the algorithm. Furthermore, it is only applicable to the scene with a large overlap rate between the target area and the candidate area. Therefore, when the target speed is fast, the target scale change, shape deformation or the target occlusion occurs, the tracking performance will be deteriorated. In this paper, we address the challenges above-mentioned by developing a tracking method that combines the background models and the graded features of color-names under the MeanShift framework. This method significantly improve performance in the above scenarios. In addition, it facilitates the balance between detection accuracy and detection speed. Experimental results demonstrate the validation of the proposed method.



### Maximum Class Separation as Inductive Bias in One Matrix
- **Arxiv ID**: http://arxiv.org/abs/2206.08704v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08704v2)
- **Published**: 2022-06-17 11:43:02+00:00
- **Updated**: 2022-10-22 19:21:30+00:00
- **Authors**: Tejaswi Kasarla, Gertjan J. Burghouts, Max van Spengler, Elise van der Pol, Rita Cucchiara, Pascal Mettes
- **Comment**: None
- **Journal**: None
- **Summary**: Maximizing the separation between classes constitutes a well-known inductive bias in machine learning and a pillar of many traditional algorithms. By default, deep networks are not equipped with this inductive bias and therefore many alternative solutions have been proposed through differential optimization. Current approaches tend to optimize classification and separation jointly: aligning inputs with class vectors and separating class vectors angularly. This paper proposes a simple alternative: encoding maximum separation as an inductive bias in the network by adding one fixed matrix multiplication before computing the softmax activations. The main observation behind our approach is that separation does not require optimization but can be solved in closed-form prior to training and plugged into a network. We outline a recursive approach to obtain the matrix consisting of maximally separable vectors for any number of classes, which can be added with negligible engineering effort and computational overhead. Despite its simple nature, this one matrix multiplication provides real impact. We show that our proposal directly boosts classification, long-tailed recognition, out-of-distribution detection, and open-set recognition, from CIFAR to ImageNet. We find empirically that maximum separation works best as a fixed bias; making the matrix learnable adds nothing to the performance. The closed-form implementation and code to reproduce the experiments are available on github.



### An Algorithm for the SE(3)-Transformation on Neural Implicit Maps for Remapping Functions
- **Arxiv ID**: http://arxiv.org/abs/2206.08712v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.08712v1)
- **Published**: 2022-06-17 12:08:51+00:00
- **Updated**: 2022-06-17 12:08:51+00:00
- **Authors**: Yijun Yuan, Andreas Nuechter
- **Comment**: Accepted to RAL2022, code at https://github.com/Jarrome/IMT_Mapping
- **Journal**: None
- **Summary**: Implicit representations are widely used for object reconstruction due to their efficiency and flexibility. In 2021, a novel structure named neural implicit map has been invented for incremental reconstruction. A neural implicit map alleviates the problem of inefficient memory cost of previous online 3D dense reconstruction while producing better quality. % However, the neural implicit map suffers the limitation that it does not support remapping as the frames of scans are encoded into a deep prior after generating the neural implicit map. This means, that neither this generation process is invertible, nor a deep prior is transformable. The non-remappable property makes it not possible to apply loop-closure techniques. % We present a neural implicit map based transformation algorithm to fill this gap. As our neural implicit map is transformable, our model supports remapping for this special map of latent features. % Experiments show that our remapping module is capable to well-transform neural implicit maps to new poses. Embedded into a SLAM framework, our mapping model is able to tackle the remapping of loop closures and demonstrates high-quality surface reconstruction. % Our implementation is available at github\footnote{\url{https://github.com/Jarrome/IMT_Mapping}} for the research community.



### CTooth: A Fully Annotated 3D Dataset and Benchmark for Tooth Volume Segmentation on Cone Beam Computed Tomography Images
- **Arxiv ID**: http://arxiv.org/abs/2206.08778v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.08778v1)
- **Published**: 2022-06-17 13:48:35+00:00
- **Updated**: 2022-06-17 13:48:35+00:00
- **Authors**: Weiwei Cui, Yaqi Wang, Qianni Zhang, Huiyu Zhou, Dan Song, Xingyong Zuo, Gangyong Jia, Liaoyuan Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: 3D tooth segmentation is a prerequisite for computer-aided dental diagnosis and treatment. However, segmenting all tooth regions manually is subjective and time-consuming. Recently, deep learning-based segmentation methods produce convincing results and reduce manual annotation efforts, but it requires a large quantity of ground truth for training. To our knowledge, there are few tooth data available for the 3D segmentation study. In this paper, we establish a fully annotated cone beam computed tomography dataset CTooth with tooth gold standard. This dataset contains 22 volumes (7363 slices) with fine tooth labels annotated by experienced radiographic interpreters. To ensure a relative even data sampling distribution, data variance is included in the CTooth including missing teeth and dental restoration. Several state-of-the-art segmentation methods are evaluated on this dataset. Afterwards, we further summarise and apply a series of 3D attention-based Unet variants for segmenting tooth volumes. This work provides a new benchmark for the tooth volume segmentation task. Experimental evidence proves that attention modules of the 3D UNet structure boost responses in tooth areas and inhibit the influence of background and noise. The best performance is achieved by 3D Unet with SKNet attention module, of 88.04 \% Dice and 78.71 \% IOU, respectively. The attention-based Unet framework outperforms other state-of-the-art methods on the CTooth dataset. The codebase and dataset are released.



### DU-Net based Unsupervised Contrastive Learning for Cancer Segmentation in Histology Images
- **Arxiv ID**: http://arxiv.org/abs/2206.08791v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.08791v1)
- **Published**: 2022-06-17 14:04:31+00:00
- **Updated**: 2022-06-17 14:04:31+00:00
- **Authors**: Yilong Li, Yaqi Wang, Huiyu Zhou, Huaqiong Wang, Gangyong Jia, Qianni Zhang
- **Comment**: arXiv admin note: text overlap with arXiv:2002.05709 by other authors
- **Journal**: None
- **Summary**: In this paper, we introduce an unsupervised cancer segmentation framework for histology images. The framework involves an effective contrastive learning scheme for extracting distinctive visual representations for segmentation. The encoder is a Deep U-Net (DU-Net) structure that contains an extra fully convolution layer compared to the normal U-Net. A contrastive learning scheme is developed to solve the problem of lacking training sets with high-quality annotations on tumour boundaries. A specific set of data augmentation techniques are employed to improve the discriminability of the learned colour features from contrastive learning. Smoothing and noise elimination are conducted using convolutional Conditional Random Fields. The experiments demonstrate competitive performance in segmentation even better than some popular supervised networks.



### FD-CAM: Improving Faithfulness and Discriminability of Visual Explanation for CNNs
- **Arxiv ID**: http://arxiv.org/abs/2206.08792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08792v1)
- **Published**: 2022-06-17 14:08:39+00:00
- **Updated**: 2022-06-17 14:08:39+00:00
- **Authors**: Hui Li, Zihao Li, Rui Ma, Tieru Wu
- **Comment**: Accepted by ICPR 2022 and also accepted by CVPR 2022 Explainable
  Artificial Intelligence for Computer Vision (XAI4CV) Workshop
- **Journal**: None
- **Summary**: Class activation map (CAM) has been widely studied for visual explanation of the internal working mechanism of convolutional neural networks. The key of existing CAM-based methods is to compute effective weights to combine activation maps in the target convolution layer. Existing gradient and score based weighting schemes have shown superiority in ensuring either the discriminability or faithfulness of the CAM, but they normally cannot excel in both properties. In this paper, we propose a novel CAM weighting scheme, named FD-CAM, to improve both the faithfulness and discriminability of the CAM-based CNN visual explanation. First, we improve the faithfulness and discriminability of the score-based weights by performing a grouped channel switching operation. Specifically, for each channel, we compute its similarity group and switch the group of channels on or off simultaneously to compute changes in the class prediction score as the weights. Then, we combine the improved score-based weights with the conventional gradient-based weights so that the discriminability of the final CAM can be further improved. We perform extensive comparisons with the state-of-the-art CAM algorithms. The quantitative and qualitative results show our FD-CAM can produce more faithful and more discriminative visual explanations of the CNNs. We also conduct experiments to verify the effectiveness of the proposed grouped channel switching and weight combination scheme on improving the results. Our code is available at https://github.com/crishhh1998/FD-CAM.



### The Importance of Background Information for Out of Distribution Generalization
- **Arxiv ID**: http://arxiv.org/abs/2206.08794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08794v1)
- **Published**: 2022-06-17 14:12:29+00:00
- **Updated**: 2022-06-17 14:12:29+00:00
- **Authors**: Jupinder Parmar, Khaled Saab, Brian Pogatchnik, Daniel Rubin, Christopher Ré
- **Comment**: 6 pages, 2 figures
- **Journal**: None
- **Summary**: Domain generalization in medical image classification is an important problem for trustworthy machine learning to be deployed in healthcare. We find that existing approaches for domain generalization which utilize ground-truth abnormality segmentations to control feature attributions have poor out-of-distribution (OOD) performance relative to the standard baseline of empirical risk minimization (ERM). We investigate what regions of an image are important for medical image classification and show that parts of the background, that which is not contained in the abnormality segmentation, provides helpful signal. We then develop a new task-specific mask which covers all relevant regions. Utilizing this new segmentation mask significantly improves the performance of the existing methods on the OOD test sets. To obtain better generalization results than ERM, we find it necessary to scale up the training data size in addition to the usage of these task-specific masks.



### Video Shadow Detection via Spatio-Temporal Interpolation Consistency Training
- **Arxiv ID**: http://arxiv.org/abs/2206.08801v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08801v1)
- **Published**: 2022-06-17 14:29:51+00:00
- **Updated**: 2022-06-17 14:29:51+00:00
- **Authors**: Xiao Lu, Yihong Cao, Sheng Liu, Chengjiang Long, Zipei Chen, Xuanyu Zhou, Yimin Yang, Chunxia Xiao
- **Comment**: Accepted in CVPR2022
- **Journal**: None
- **Summary**: It is challenging to annotate large-scale datasets for supervised video shadow detection methods. Using a model trained on labeled images to the video frames directly may lead to high generalization error and temporal inconsistent results. In this paper, we address these challenges by proposing a Spatio-Temporal Interpolation Consistency Training (STICT) framework to rationally feed the unlabeled video frames together with the labeled images into an image shadow detection network training. Specifically, we propose the Spatial and Temporal ICT, in which we define two new interpolation schemes, \textit{i.e.}, the spatial interpolation and the temporal interpolation. We then derive the spatial and temporal interpolation consistency constraints accordingly for enhancing generalization in the pixel-wise classification task and for encouraging temporal consistent predictions, respectively. In addition, we design a Scale-Aware Network for multi-scale shadow knowledge learning in images, and propose a scale-consistency constraint to minimize the discrepancy among the predictions at different scales. Our proposed approach is extensively validated on the ViSha dataset and a self-annotated dataset. Experimental results show that, even without video labels, our approach is better than most state of the art supervised, semi-supervised or unsupervised image/video shadow detection methods and other methods in related tasks. Code and dataset are available at \url{https://github.com/yihong-97/STICT}.



### Open-Sampling: Exploring Out-of-Distribution data for Re-balancing Long-tailed datasets
- **Arxiv ID**: http://arxiv.org/abs/2206.08802v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08802v3)
- **Published**: 2022-06-17 14:29:52+00:00
- **Updated**: 2022-07-05 05:50:29+00:00
- **Authors**: Hongxin Wei, Lue Tao, Renchunzi Xie, Lei Feng, Bo An
- **Comment**: Accepted by ICML 2022
- **Journal**: None
- **Summary**: Deep neural networks usually perform poorly when the training dataset suffers from extreme class imbalance. Recent studies found that directly training with out-of-distribution data (i.e., open-set samples) in a semi-supervised manner would harm the generalization performance. In this work, we theoretically show that out-of-distribution data can still be leveraged to augment the minority classes from a Bayesian perspective. Based on this motivation, we propose a novel method called Open-sampling, which utilizes open-set noisy labels to re-balance the class priors of the training dataset. For each open-set instance, the label is sampled from our pre-defined distribution that is complementary to the distribution of original class priors. We empirically show that Open-sampling not only re-balances the class priors but also encourages the neural network to learn separable representations. Extensive experiments demonstrate that our proposed method significantly outperforms existing data re-balancing methods and can boost the performance of existing state-of-the-art methods.



### Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2206.08826v2
- **DOI**: 10.1093/jamia/ocac168
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08826v2)
- **Published**: 2022-06-17 15:10:00+00:00
- **Updated**: 2022-09-23 19:50:57+00:00
- **Authors**: Michal Golovanevsky, Carsten Eickhoff, Ritambhara Singh
- **Comment**: 11 pages, 5 figures
- **Journal**: Journal of the American Medical Informatics Association, 2022;
  ocac168
- **Summary**: Alzheimer's Disease (AD) is the most common neurodegenerative disorder with one of the most complex pathogeneses, making effective and clinically actionable decision support difficult. The objective of this study was to develop a novel multimodal deep learning framework to aid medical professionals in AD diagnosis. We present a Multimodal Alzheimer's Disease Diagnosis framework (MADDi) to accurately detect the presence of AD and mild cognitive impairment (MCI) from imaging, genetic, and clinical data. MADDi is novel in that we use cross-modal attention, which captures interactions between modalities - a method not previously explored in this domain. We perform multi-class classification, a challenging task considering the strong similarities between MCI and AD. We compare with previous state-of-the-art models, evaluate the importance of attention, and examine the contribution of each modality to the model's performance. MADDi classifies MCI, AD, and controls with 96.88% accuracy on a held-out test set. When examining the contribution of different attention schemes, we found that the combination of cross-modal attention with self-attention performed the best, and no attention layers in the model performed the worst, with a 7.9% difference in F1-Scores. Our experiments underlined the importance of structured clinical data to help machine learning models contextualize and interpret the remaining modalities. Extensive ablation studies showed that any multimodal mixture of input features without access to structured clinical information suffered marked performance losses. This study demonstrates the merit of combining multiple input modalities via cross-modal attention to deliver highly accurate AD diagnostic decision support.



### A Comparative Study of Confidence Calibration in Deep Learning: From Computer Vision to Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2206.08833v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08833v1)
- **Published**: 2022-06-17 15:27:24+00:00
- **Updated**: 2022-06-17 15:27:24+00:00
- **Authors**: Riqiang Gao, Thomas Li, Yucheng Tang, Zhoubing Xu, Michael Kammer, Sanja L. Antic, Kim Sandler, Fabien Moldonado, Thomas A. Lasko, Bennett Landman
- **Comment**: 17 pages, 6 figures
- **Journal**: None
- **Summary**: Although deep learning prediction models have been successful in the discrimination of different classes, they can often suffer from poor calibration across challenging domains including healthcare. Moreover, the long-tail distribution poses great challenges in deep learning classification problems including clinical disease prediction. There are approaches proposed recently to calibrate deep prediction in computer vision, but there are no studies found to demonstrate how the representative models work in different challenging contexts. In this paper, we bridge the confidence calibration from computer vision to medical imaging with a comparative study of four high-impact calibration models. Our studies are conducted in different contexts (natural image classification and lung cancer risk estimation) including in balanced vs. imbalanced training sets and in computer vision vs. medical imaging. Our results support key findings: (1) We achieve new conclusions which are not studied under different learning contexts, e.g., combining two calibration models that both mitigate the overconfident prediction can lead to under-confident prediction, and simpler calibration models from the computer vision domain tend to be more generalizable to medical imaging. (2) We highlight the gap between general computer vision tasks and medical imaging prediction, e.g., calibration methods ideal for general computer vision tasks may in fact damage the calibration of medical imaging prediction. (3) We also reinforce previous conclusions in natural image classification settings. We believe that this study has merits to guide readers to choose calibration models and understand gaps between general computer vision and medical imaging domains.



### Entity-Graph Enhanced Cross-Modal Pretraining for Instance-level Product Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2206.08842v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.DB, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2206.08842v1)
- **Published**: 2022-06-17 15:40:45+00:00
- **Updated**: 2022-06-17 15:40:45+00:00
- **Authors**: Xiao Dong, Xunlin Zhan, Yunchao Wei, Xiaoyong Wei, Yaowei Wang, Minlong Lu, Xiaochun Cao, Xiaodan Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Our goal in this research is to study a more realistic environment in which we can conduct weakly-supervised multi-modal instance-level product retrieval for fine-grained product categories. We first contribute the Product1M datasets, and define two real practical instance-level retrieval tasks to enable the evaluations on the price comparison and personalized recommendations. For both instance-level tasks, how to accurately pinpoint the product target mentioned in the visual-linguistic data and effectively decrease the influence of irrelevant contents is quite challenging. To address this, we exploit to train a more effective cross-modal pertaining model which is adaptively capable of incorporating key concept information from the multi-modal data, by using an entity graph whose node and edge respectively denote the entity and the similarity relation between entities. Specifically, a novel Entity-Graph Enhanced Cross-Modal Pretraining (EGE-CMP) model is proposed for instance-level commodity retrieval, that explicitly injects entity knowledge in both node-based and subgraph-based ways into the multi-modal networks via a self-supervised hybrid-stream transformer, which could reduce the confusion between different object contents, thereby effectively guiding the network to focus on entities with real semantic. Experimental results well verify the efficacy and generalizability of our EGE-CMP, outperforming several SOTA cross-modal baselines like CLIP, UNITER and CAPTURE.



### MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2206.08853v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08853v2)
- **Published**: 2022-06-17 15:53:05+00:00
- **Updated**: 2022-11-22 07:59:47+00:00
- **Authors**: Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, Anima Anandkumar
- **Comment**: Outstanding Paper Award at NeurIPS 2022. Project website:
  https://minedojo.org
- **Journal**: None
- **Summary**: Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MineDojo, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MineDojo's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of open-ended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.



### DGMIL: Distribution Guided Multiple Instance Learning for Whole Slide Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.08861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08861v1)
- **Published**: 2022-06-17 16:04:30+00:00
- **Updated**: 2022-06-17 16:04:30+00:00
- **Authors**: Linhao Qu, Xiaoyuan Luo, Shaolei Liu, Manning Wang, Zhijian Song
- **Comment**: accepted by MICCAI 2022
- **Journal**: None
- **Summary**: Multiple Instance Learning (MIL) is widely used in analyzing histopathological Whole Slide Images (WSIs). However, existing MIL methods do not explicitly model the data distribution, and instead they only learn a bag-level or instance-level decision boundary discriminatively by training a classifier. In this paper, we propose DGMIL: a feature distribution guided deep MIL framework for WSI classification and positive patch localization. Instead of designing complex discriminative network architectures, we reveal that the inherent feature distribution of histopathological image data can serve as a very effective guide for instance classification. We propose a cluster-conditioned feature distribution modeling method and a pseudo label-based iterative feature space refinement strategy so that in the final feature space the positive and negative instances can be easily separated. Experiments on the CAMELYON16 dataset and the TCGA Lung Cancer dataset show that our method achieves new SOTA for both global classification and positive patch localization tasks.



### Fast Lossless Neural Compression with Integer-Only Discrete Flows
- **Arxiv ID**: http://arxiv.org/abs/2206.08869v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2206.08869v1)
- **Published**: 2022-06-17 16:15:36+00:00
- **Updated**: 2022-06-17 16:15:36+00:00
- **Authors**: Siyu Wang, Jianfei Chen, Chongxuan Li, Jun Zhu, Bo Zhang
- **Comment**: Accepted as a conference paper at International Conference on Machine
  Learning (ICML) 2022
- **Journal**: None
- **Summary**: By applying entropy codecs with learned data distributions, neural compressors have significantly outperformed traditional codecs in terms of compression ratio. However, the high inference latency of neural networks hinders the deployment of neural compressors in practical applications. In this work, we propose Integer-only Discrete Flows (IODF), an efficient neural compressor with integer-only arithmetic. Our work is built upon integer discrete flows, which consists of invertible transformations between discrete random variables. We propose efficient invertible transformations with integer-only arithmetic based on 8-bit quantization. Our invertible transformation is equipped with learnable binary gates to remove redundant filters during inference. We deploy IODF with TensorRT on GPUs, achieving 10x inference speedup compared to the fastest existing neural compressors, while retaining the high compression rates on ImageNet32 and ImageNet64.



### Improving Generalization of Metric Learning via Listwise Self-distillation
- **Arxiv ID**: http://arxiv.org/abs/2206.08880v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.08880v1)
- **Published**: 2022-06-17 16:28:39+00:00
- **Updated**: 2022-06-17 16:28:39+00:00
- **Authors**: Zelong Zeng, Fan Yang, Zheng Wang, Shin'ichi Satoh
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Most deep metric learning (DML) methods employ a strategy that forces all positive samples to be close in the embedding space while keeping them away from negative ones. However, such a strategy ignores the internal relationships of positive (negative) samples and often leads to overfitting, especially in the presence of hard samples and mislabeled samples. In this work, we propose a simple yet effective regularization, namely Listwise Self-Distillation (LSD), which progressively distills a model's own knowledge to adaptively assign a more appropriate distance target to each sample pair in a batch. LSD encourages smoother embeddings and information mining within positive (negative) samples as a way to mitigate overfitting and thus improve generalization. Our LSD can be directly integrated into general DML frameworks. Extensive experiments show that LSD consistently boosts the performance of various metric learning methods on multiple datasets.



### Edge-Aided Sensor Data Sharing in Vehicular Communication Networks
- **Arxiv ID**: http://arxiv.org/abs/2206.08882v1
- **DOI**: None
- **Categories**: **cs.MA**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2206.08882v1)
- **Published**: 2022-06-17 16:30:56+00:00
- **Updated**: 2022-06-17 16:30:56+00:00
- **Authors**: Rui Song, Anupama Hegde, Numan Senel, Alois Knoll, Andreas Festag
- **Comment**: Accepted for IEEE 95th Vehicular Technology Conference
  (VTC2022-Spring)
- **Journal**: None
- **Summary**: Sensor data sharing in vehicular networks can significantly improve the range and accuracy of environmental perception for connected automated vehicles. Different concepts and schemes for dissemination and fusion of sensor data have been developed. It is common to these schemes that measurement errors of the sensors impair the perception quality and can result in road traffic accidents. Specifically, when the measurement error from the sensors (also referred as measurement noise) is unknown and time varying, the performance of the data fusion process is restricted, which represents a major challenge in the calibration of sensors. In this paper, we consider sensor data sharing and fusion in a vehicular network with both, vehicle-to-infrastructure and vehicle-to-vehicle communication. We propose a method, named Bidirectional Feedback Noise Estimation (BiFNoE), in which an edge server collects and caches sensor measurement data from vehicles. The edge estimates the noise and the targets alternately in double dynamic sliding time windows and enhances the distributed cooperative environment sensing at each vehicle with low communication costs. We evaluate the proposed algorithm and data dissemination strategy in an application scenario by simulation and show that the perception accuracy is on average improved by around 80 % with only 12 kbps uplink and 28 kbps downlink bandwidth.



### CtrlFormer: Learning Transferable State Representation for Visual Control via Transformer
- **Arxiv ID**: http://arxiv.org/abs/2206.08883v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.08883v1)
- **Published**: 2022-06-17 16:32:08+00:00
- **Updated**: 2022-06-17 16:32:08+00:00
- **Authors**: Yao Mu, Shoufa Chen, Mingyu Ding, Jianyu Chen, Runjian Chen, Ping Luo
- **Comment**: ICML 2022
- **Journal**: None
- **Summary**: Transformer has achieved great successes in learning vision and language representation, which is general across various downstream tasks. In visual control, learning transferable state representation that can transfer between different control tasks is important to reduce the training sample size. However, porting Transformer to sample-efficient visual control remains a challenging and unsolved problem. To this end, we propose a novel Control Transformer (CtrlFormer), possessing many appealing benefits that prior arts do not have. Firstly, CtrlFormer jointly learns self-attention mechanisms between visual tokens and policy tokens among different control tasks, where multitask representation can be learned and transferred without catastrophic forgetting. Secondly, we carefully design a contrastive reinforcement learning paradigm to train CtrlFormer, enabling it to achieve high sample efficiency, which is important in control problems. For example, in the DMControl benchmark, unlike recent advanced methods that failed by producing a zero score in the "Cartpole" task after transfer learning with 100k samples, CtrlFormer can achieve a state-of-the-art score with only 100k samples while maintaining the performance of previous tasks. The code and models are released in our project homepage.



### Incorporating intratumoral heterogeneity into weakly-supervised deep learning models via variance pooling
- **Arxiv ID**: http://arxiv.org/abs/2206.08885v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2206.08885v2)
- **Published**: 2022-06-17 16:35:35+00:00
- **Updated**: 2022-11-19 12:42:39+00:00
- **Authors**: Iain Carmichael, Andrew H. Song, Richard J. Chen, Drew F. K. Williamson, Tiffany Y. Chen, Faisal Mahmood
- **Comment**: MICCAI 2022
- **Journal**: None
- **Summary**: Supervised learning tasks such as cancer survival prediction from gigapixel whole slide images (WSIs) are a critical challenge in computational pathology that requires modeling complex features of the tumor microenvironment. These learning tasks are often solved with deep multi-instance learning (MIL) models that do not explicitly capture intratumoral heterogeneity. We develop a novel variance pooling architecture that enables a MIL model to incorporate intratumoral heterogeneity into its predictions. Two interpretability tools based on representative patches are illustrated to probe the biological signals captured by these models. An empirical study with 4,479 gigapixel WSIs from the Cancer Genome Atlas shows that adding variance pooling onto MIL frameworks improves survival prediction performance for five cancer types.



### Disentangling Model Multiplicity in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.08890v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08890v2)
- **Published**: 2022-06-17 16:53:12+00:00
- **Updated**: 2023-01-31 10:40:52+00:00
- **Authors**: Ari Heljakka, Martin Trapp, Juho Kannala, Arno Solin
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Model multiplicity is a well-known but poorly understood phenomenon that undermines the generalisation guarantees of machine learning models. It appears when two models with similar training-time performance differ in their predictions and real-world performance characteristics. This observed 'predictive' multiplicity (PM) also implies elusive differences in the internals of the models, their 'representational' multiplicity (RM). We introduce a conceptual and experimental setup for analysing RM by measuring activation similarity via singular vector canonical correlation analysis (SVCCA). We show that certain differences in training methods systematically result in larger RM than others and evaluate RM and PM over a finite sample as predictors for generalizability. We further correlate RM with PM measured by the variance in i.i.d. and out-of-distribution test predictions in four standard image data sets. Finally, instead of attempting to eliminate RM, we call for its systematic measurement and maximal exposure.



### SimA: Simple Softmax-free Attention for Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2206.08898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08898v1)
- **Published**: 2022-06-17 17:15:01+00:00
- **Updated**: 2022-06-17 17:15:01+00:00
- **Authors**: Soroush Abbasi Koohpayegani, Hamed Pirsiavash
- **Comment**: Code is available here:
  $\href{https://github.com/UCDvision/sima}{\text{This https URL}}$
- **Journal**: None
- **Summary**: Recently, vision transformers have become very popular. However, deploying them in many applications is computationally expensive partly due to the Softmax layer in the attention block. We introduce a simple but effective, Softmax-free attention block, SimA, which normalizes query and key matrices with simple $\ell_1$-norm instead of using Softmax layer. Then, the attention block in SimA is a simple multiplication of three matrices, so SimA can dynamically change the ordering of the computation at the test time to achieve linear computation on the number of tokens or the number of channels. We empirically show that SimA applied to three SOTA variations of transformers, DeiT, XCiT, and CvT, results in on-par accuracy compared to the SOTA models, without any need for Softmax layer. Interestingly, changing SimA from multi-head to single-head has only a small effect on the accuracy, which simplifies the attention block further. The code is available here: $\href{https://github.com/UCDvision/sima}{\text{This https URL}}$



### A Roadmap for Greater Public Use of Privacy-Sensitive Government Data: Workshop Report
- **Arxiv ID**: http://arxiv.org/abs/2208.01636v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.01636v1)
- **Published**: 2022-06-17 17:20:29+00:00
- **Updated**: 2022-06-17 17:20:29+00:00
- **Authors**: Chris Clifton, Bradley Malin, Anna Oganian, Ramesh Raskar, Vivek Sharma
- **Comment**: 23 pages
- **Journal**: None
- **Summary**: Government agencies collect and manage a wide range of ever-growing datasets. While such data has the potential to support research and evidence-based policy making, there are concerns that the dissemination of such data could infringe upon the privacy of the individuals (or organizations) from whom such data was collected. To appraise the current state of data sharing, as well as learn about opportunities for stimulating such sharing at a faster pace, a virtual workshop was held on May 21st and 26th, 2021, sponsored by the National Science Foundation and National Institute of Standards and Technologies, where a multinational collection of researchers and practitioners were brought together to discuss their experiences and learn about recently developed technologies for managing privacy while sharing data. The workshop specifically focused on challenges and successes in government data sharing at various levels. The first day focused on successful examples of new technology applied to sharing of public data, including formal privacy techniques, synthetic data, and cryptographic approaches. Day two emphasized brainstorming sessions on some of the challenges and directions to address them.



### Colonoscopy 3D Video Dataset with Paired Depth from 2D-3D Registration
- **Arxiv ID**: http://arxiv.org/abs/2206.08903v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08903v2)
- **Published**: 2022-06-17 17:23:50+00:00
- **Updated**: 2022-11-23 15:58:44+00:00
- **Authors**: Taylor L. Bobrow, Mayank Golhar, Rohan Vijayan, Venkata S. Akshintala, Juan R. Garcia, Nicholas J. Durr
- **Comment**: None
- **Journal**: None
- **Summary**: Screening colonoscopy is an important clinical application for several 3D computer vision techniques, including depth estimation, surface reconstruction, and missing region detection. However, the development, evaluation, and comparison of these techniques in real colonoscopy videos remain largely qualitative due to the difficulty of acquiring ground truth data. In this work, we present a Colonoscopy 3D Video Dataset (C3VD) acquired with a high definition clinical colonoscope and high-fidelity colon models for benchmarking computer vision methods in colonoscopy. We introduce a novel multimodal 2D-3D registration technique to register optical video sequences with ground truth rendered views of a known 3D model. The different modalities are registered by transforming optical images to depth maps with a Generative Adversarial Network and aligning edge features with an evolutionary optimizer. This registration method achieves an average translation error of 0.321 millimeters and an average rotation error of 0.159 degrees in simulation experiments where error-free ground truth is available. The method also leverages video information, improving registration accuracy by 55.6% for translation and 60.4% for rotation compared to single frame registration. 22 short video sequences were registered to generate 10,015 total frames with paired ground truth depth, surface normals, optical flow, occlusion, six degree-of-freedom pose, coverage maps, and 3D models. The dataset also includes screening videos acquired by a gastroenterologist with paired ground truth pose and 3D surface models. The dataset and registration source code are available at durr.jhu.edu/C3VD.



### Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks
- **Arxiv ID**: http://arxiv.org/abs/2206.08916v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08916v2)
- **Published**: 2022-06-17 17:53:47+00:00
- **Updated**: 2022-10-04 22:37:32+00:00
- **Authors**: Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and demos for Unified-IO are available at: https://unified-io.allenai.org.



### VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix
- **Arxiv ID**: http://arxiv.org/abs/2206.08919v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08919v1)
- **Published**: 2022-06-17 17:56:47+00:00
- **Updated**: 2022-06-17 17:56:47+00:00
- **Authors**: Teng Wang, Wenhao Jiang, Zhichao Lu, Feng Zheng, Ran Cheng, Chengguo Yin, Ping Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Existing vision-language pre-training (VLP) methods primarily rely on paired image-text datasets, which are either annotated by enormous human labors, or crawled from the internet followed by elaborate data cleaning techniques. To reduce the dependency on well-aligned image-text pairs, it is promising to directly leverage the large-scale text-only and image-only corpora. This paper proposes a data augmentation method, namely cross-modal CutMix (CMC), for implicit cross-modal alignment learning in unpaired VLP. Specifically, CMC transforms natural sentences from the textual view into a multi-modal view, where visually-grounded words in a sentence are randomly replaced by diverse image patches with similar semantics. There are several appealing proprieties of the proposed CMC. First, it enhances the data diversity while keeping the semantic meaning intact for tackling problems where the aligned data are scarce; Second, by attaching cross-modal noise on uni-modal data, it guides models to learn token-level interactions across modalities for better denoising. Furthermore, we present a new unpaired VLP method, dubbed as VLMixer, that integrates CMC with contrastive learning to pull together the uni-modal and multi-modal views for better instance-level alignments among different modalities. Extensive experiments on five downstream tasks show that VLMixer could surpass previous state-of-the-art unpaired VLP methods.



### VectorMapNet: End-to-end Vectorized HD Map Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.08920v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.08920v6)
- **Published**: 2022-06-17 17:57:13+00:00
- **Updated**: 2023-06-26 05:40:58+00:00
- **Authors**: Yicheng Liu, Tianyuan Yuan, Yue Wang, Yilun Wang, Hang Zhao
- **Comment**: Accepted by ICML 2023
- **Journal**: None
- **Summary**: Autonomous driving systems require High-Definition (HD) semantic maps to navigate around urban roads. Existing solutions approach the semantic mapping problem by offline manual annotation, which suffers from serious scalability issues. Recent learning-based methods produce dense rasterized segmentation predictions to construct maps. However, these predictions do not include instance information of individual map elements and require heuristic post-processing to obtain vectorized maps. To tackle these challenges, we introduce an end-to-end vectorized HD map learning pipeline, termed VectorMapNet. VectorMapNet takes onboard sensor observations and predicts a sparse set of polylines in the bird's-eye view. This pipeline can explicitly model the spatial relation between map elements and generate vectorized maps that are friendly to downstream autonomous driving tasks. Extensive experiments show that VectorMapNet achieve strong map learning performance on both nuScenes and Argoverse2 dataset, surpassing previous state-of-the-art methods by 14.2 mAP and 14.6mAP. Qualitatively, VectorMapNet is capable of generating comprehensive maps and capturing fine-grained details of road geometry. To the best of our knowledge, VectorMapNet is the first work designed towards end-to-end vectorized map learning from onboard observations. Our project website is available at \url{https://tsinghua-mars-lab.github.io/vectormapnet/}.



### Cross-task Attention Mechanism for Dense Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.08927v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.08927v1)
- **Published**: 2022-06-17 17:59:45+00:00
- **Updated**: 2022-06-17 17:59:45+00:00
- **Authors**: Ivan Lopes, Tuan-Hung Vu, Raoul de Charette
- **Comment**: 10 figures, 6 tables, 23 pages
- **Journal**: None
- **Summary**: Multi-task learning has recently become a promising solution for a comprehensive understanding of complex scenes. Not only being memory-efficient, multi-task models with an appropriate design can favor exchange of complementary signals across tasks. In this work, we jointly address 2D semantic segmentation, and two geometry-related tasks, namely dense depth, surface normal estimation as well as edge estimation showing their benefit on indoor and outdoor datasets. We propose a novel multi-task learning architecture that exploits pair-wise cross-task exchange through correlation-guided attention and self-attention to enhance the average representation learning for all tasks. We conduct extensive experiments considering three multi-task setups, showing the benefit of our proposal in comparison to competitive baselines in both synthetic and real benchmarks. We also extend our method to the novel multi-task unsupervised domain adaptation setting. Our code is available at https://github.com/cv-rits/DenseMTL.



### TAVA: Template-free Animatable Volumetric Actors
- **Arxiv ID**: http://arxiv.org/abs/2206.08929v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.08929v2)
- **Published**: 2022-06-17 17:59:59+00:00
- **Updated**: 2022-06-21 03:14:02+00:00
- **Authors**: Ruilong Li, Julian Tanke, Minh Vo, Michael Zollhofer, Jurgen Gall, Angjoo Kanazawa, Christoph Lassner
- **Comment**: Code: https://github.com/facebookresearch/tava; Project Website:
  https://www.liruilong.cn/projects/tava/
- **Journal**: None
- **Summary**: Coordinate-based volumetric representations have the potential to generate photo-realistic virtual avatars from images. However, virtual avatars also need to be controllable even to a novel pose that may not have been observed. Traditional techniques, such as LBS, provide such a function; yet it usually requires a hand-designed body template, 3D scan data, and limited appearance models. On the other hand, neural representation has been shown to be powerful in representing visual details, but are under explored on deforming dynamic articulated actors. In this paper, we propose TAVA, a method to create T emplate-free Animatable Volumetric Actors, based on neural representations. We rely solely on multi-view data and a tracked skeleton to create a volumetric model of an actor, which can be animated at the test time given novel pose. Since TAVA does not require a body template, it is applicable to humans as well as other creatures such as animals. Furthermore, TAVA is designed such that it can recover accurate dense correspondences, making it amenable to content-creation and editing tasks. Through extensive experiments, we demonstrate that the proposed method generalizes well to novel poses as well as unseen views and showcase basic editing capabilities.



### CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.08948v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08948v1)
- **Published**: 2022-06-17 18:01:01+00:00
- **Updated**: 2022-06-17 18:01:01+00:00
- **Authors**: Qihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, Liang-Chieh Chen
- **Comment**: CVPR 2022 Oral
- **Journal**: None
- **Summary**: We propose Clustering Mask Transformer (CMT-DeepLab), a transformer-based framework for panoptic segmentation designed around clustering. It rethinks the existing transformer architectures used in segmentation and detection; CMT-DeepLab considers the object queries as cluster centers, which fill the role of grouping the pixels when applied to segmentation. The clustering is computed with an alternating procedure, by first assigning pixels to the clusters by their feature affinity, and then updating the cluster centers and pixel features. Together, these operations comprise the Clustering Mask Transformer (CMT) layer, which produces cross-attention that is denser and more consistent with the final segmentation task. CMT-DeepLab improves the performance over prior art significantly by 4.4% PQ, achieving a new state-of-the-art of 55.7% PQ on the COCO test-dev set.



### Bag of Image Patch Embedding Behind the Success of Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.08954v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.08954v2)
- **Published**: 2022-06-17 18:11:23+00:00
- **Updated**: 2023-06-13 00:48:40+00:00
- **Authors**: Yubei Chen, Adrien Bardes, Zengyi Li, Yann LeCun
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning (SSL) has recently achieved tremendous empirical advancements in learning image representation. However, our understanding of the principle behind learning such a representation is still limited. This work shows that joint-embedding SSL approaches primarily learn a representation of image patches, which reflects their co-occurrence. Such a connection to co-occurrence modeling can be established formally, and it supplements the prevailing invariance perspective. We empirically show that learning a representation for fixed-scale patches and aggregating local patch representations as the image representation achieves similar or even better results than the baseline methods. We denote this process as BagSSL. Even with 32x32 patch representation, BagSSL achieves 62% top-1 linear probing accuracy on ImageNet. On the other hand, with a multi-scale pretrained model, we show that the whole image embedding is approximately the average of local patch embeddings. While the SSL representation is relatively invariant at the global scale, we show that locality is preserved when we zoom into local patch-level representation. Further, we show that patch representation aggregation can improve various SOTA baseline methods by a large margin. The patch representation is considerably easier to understand, and this work makes a step to demystify self-supervised representation learning.



### MultiEarth 2022 -- The Champion Solution for Image-to-Image Translation Challenge via Generation Models
- **Arxiv ID**: http://arxiv.org/abs/2207.00001v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.00001v1)
- **Published**: 2022-06-17 18:33:10+00:00
- **Updated**: 2022-06-17 18:33:10+00:00
- **Authors**: Yuchuan Gou, Bo Peng, Hongchen Liu, Hang Zhou, Jui-Hsin Lai
- **Comment**: CVPR 2022, MultiEarth 2022, Image-to-Image translation, competition
- **Journal**: None
- **Summary**: The MultiEarth 2022 Image-to-Image Translation challenge provides a well-constrained test bed for generating the corresponding RGB Sentinel-2 imagery with the given Sentinel-1 VV & VH imagery. In this challenge, we designed various generation models and found the SPADE [1] and pix2pixHD [2] models could perform our best results. In our self-evaluation, the SPADE-2 model with L1-loss can achieve 0.02194 MAE score and 31.092 PSNR dB. In our final submission, the best model can achieve 0.02795 MAE score ranked No.1 on the leader board.



### KitBit: A New AI Model for Solving Intelligence Tests and Numerical Series
- **Arxiv ID**: http://arxiv.org/abs/2206.08965v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08965v2)
- **Published**: 2022-06-17 18:40:11+00:00
- **Updated**: 2022-11-22 18:23:20+00:00
- **Authors**: Víctor Corsino, José Manuel Gilpérez, Luis Herrera
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: The resolution of intelligence tests, in particular numerical sequences, has been of great interest in the evaluation of AI systems. We present a new computational model called KitBit that uses a reduced set of algorithms and their combinations to build a predictive model that finds the underlying pattern in numerical sequences, such as those included in IQ tests and others of much greater complexity. We present the fundamentals of the model and its application in different cases. First, the system is tested on a set of number series used in IQ tests collected from various sources. Next, our model is successfully applied on the sequences used to evaluate the models reported in the literature. In both cases, the system is capable of solving these types of problems in less than a second using standard computing power. Finally, KitBit's algorithms have been applied for the first time to the complete set of entire sequences of the well-known OEIS database. We find a pattern in the form of a list of algorithms and predict the following terms in the largest number of series to date. These results demonstrate the potential of KitBit to solve complex problems that could be represented numerically.



### MultiEarth 2022 -- The Champion Solution for the Matrix Completion Challenge via Multimodal Regression and Generation
- **Arxiv ID**: http://arxiv.org/abs/2206.08970v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08970v1)
- **Published**: 2022-06-17 18:55:05+00:00
- **Updated**: 2022-06-17 18:55:05+00:00
- **Authors**: Bo Peng, Hongchen Liu, Hang Zhou, Yuchuan Gou, Jui-Hsin Lai
- **Comment**: CVPR 2022, MultiEarth 2022, Matrix Completion Challenge
- **Journal**: None
- **Summary**: Earth observation satellites have been continuously monitoring the earth environment for years at different locations and spectral bands with different modalities. Due to complex satellite sensing conditions (e.g., weather, cloud, atmosphere, orbit), some observations for certain modalities, bands, locations, and times may not be available. The MultiEarth Matrix Completion Challenge in CVPR 2022 [1] provides the multimodal satellite data for addressing such data sparsity challenges with the Amazon Rainforest as the region of interest. This work proposes an adaptive real-time multimodal regression and generation framework and achieves superior performance on unseen test queries in this challenge with an LPIPS of 0.2226, a PSNR of 123.0372, and an SSIM of 0.6347.



### Multi-scale Super-resolution Magnetic Resonance Spectroscopic Imaging with Adjustable Sharpness
- **Arxiv ID**: http://arxiv.org/abs/2206.08984v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.08984v1)
- **Published**: 2022-06-17 19:33:41+00:00
- **Updated**: 2022-06-17 19:33:41+00:00
- **Authors**: Siyuan Dong, Gilbert Hangel, Wolfgang Bogner, Georg Widhalm, Karl Rössler, Siegfried Trattnig, Chenyu You, Robin de Graaf, John Onofrey, James Duncan
- **Comment**: Accepted by MICCAI 2022
- **Journal**: None
- **Summary**: Magnetic Resonance Spectroscopic Imaging (MRSI) is a valuable tool for studying metabolic activities in the human body, but the current applications are limited to low spatial resolutions. The existing deep learning-based MRSI super-resolution methods require training a separate network for each upscaling factor, which is time-consuming and memory inefficient. We tackle this multi-scale super-resolution problem using a Filter Scaling strategy that modulates the convolution filters based on the upscaling factor, such that a single network can be used for various upscaling factors. Observing that each metabolite has distinct spatial characteristics, we also modulate the network based on the specific metabolite. Furthermore, our network is conditioned on the weight of adversarial loss so that the perceptual sharpness of the super-resolved metabolic maps can be adjusted within a single network. We incorporate these network conditionings using a novel Multi-Conditional Module. The experiments were carried out on a 1H-MRSI dataset from 15 high-grade glioma patients. Results indicate that the proposed network achieves the best performance among several multi-scale super-resolution methods and can provide super-resolved metabolic maps with adjustable sharpness.



### TransResU-Net: Transformer based ResU-Net for Real-Time Colonoscopy Polyp Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.08985v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08985v1)
- **Published**: 2022-06-17 19:36:37+00:00
- **Updated**: 2022-06-17 19:36:37+00:00
- **Authors**: Nikhil Kumar Tomar, Annie Shergill, Brandon Rieders, Ulas Bagci, Debesh Jha
- **Comment**: None
- **Journal**: None
- **Summary**: Colorectal cancer (CRC) is one of the most common causes of cancer and cancer-related mortality worldwide. Performing colon cancer screening in a timely fashion is the key to early detection. Colonoscopy is the primary modality used to diagnose colon cancer. However, the miss rate of polyps, adenomas and advanced adenomas remains significantly high. Early detection of polyps at the precancerous stage can help reduce the mortality rate and the economic burden associated with colorectal cancer. Deep learning-based computer-aided diagnosis (CADx) system may help gastroenterologists to identify polyps that may otherwise be missed, thereby improving the polyp detection rate. Additionally, CADx system could prove to be a cost-effective system that improves long-term colorectal cancer prevention. In this study, we proposed a deep learning-based architecture for automatic polyp segmentation, called Transformer ResU-Net (TransResU-Net). Our proposed architecture is built upon residual blocks with ResNet-50 as the backbone and takes the advantage of transformer self-attention mechanism as well as dilated convolution(s). Our experimental results on two publicly available polyp segmentation benchmark datasets showed that TransResU-Net obtained a highly promising dice score and a real-time speed. With high efficacy in our performance metrics, we concluded that TransResU-Net could be a strong benchmark for building a real-time polyp detection system for the early diagnosis, treatment, and prevention of colorectal cancer. The source code of the proposed TransResU-Net is publicly available at https://github.com/nikhilroxtomar/TransResUNet.



### Shadows Shed Light on 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/2206.08990v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2206.08990v1)
- **Published**: 2022-06-17 19:58:11+00:00
- **Updated**: 2022-06-17 19:58:11+00:00
- **Authors**: Ruoshi Liu, Sachit Menon, Chengzhi Mao, Dennis Park, Simon Stent, Carl Vondrick
- **Comment**: 19 pages, 10 figures
- **Journal**: None
- **Summary**: 3D reconstruction is a fundamental problem in computer vision, and the task is especially challenging when the object to reconstruct is partially or fully occluded. We introduce a method that uses the shadows cast by an unobserved object in order to infer the possible 3D volumes behind the occlusion. We create a differentiable image formation model that allows us to jointly infer the 3D shape of an object, its pose, and the position of a light source. Since the approach is end-to-end differentiable, we are able to integrate learned priors of object geometry in order to generate realistic 3D shapes of different object categories. Experiments and visualizations show that the method is able to generate multiple possible solutions that are consistent with the observation of the shadow. Our approach works even when the position of the light source and object pose are both unknown. Our approach is also robust to real-world images where ground-truth shadow mask is unknown.



### Robust Group Synchronization via Quadratic Programming
- **Arxiv ID**: http://arxiv.org/abs/2206.08994v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NA, math.NA, 90C26, 90C17, 68Q87, 65C20, 90-08, 60-08, G.1.6; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2206.08994v1)
- **Published**: 2022-06-17 20:08:03+00:00
- **Updated**: 2022-06-17 20:08:03+00:00
- **Authors**: Yunpeng Shi, Cole Wyeth, Gilad Lerman
- **Comment**: Accepted to ICML 2022
- **Journal**: None
- **Summary**: We propose a novel quadratic programming formulation for estimating the corruption levels in group synchronization, and use these estimates to solve this problem. Our objective function exploits the cycle consistency of the group and we thus refer to our method as detection and estimation of structural consistency (DESC). This general framework can be extended to other algebraic and geometric structures. Our formulation has the following advantages: it can tolerate corruption as high as the information-theoretic bound, it does not require a good initialization for the estimates of group elements, it has a simple interpretation, and under some mild conditions the global minimum of our objective function exactly recovers the corruption levels. We demonstrate the competitive accuracy of our approach on both synthetic and real data experiments of rotation averaging.



### Diffusion models as plug-and-play priors
- **Arxiv ID**: http://arxiv.org/abs/2206.09012v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09012v3)
- **Published**: 2022-06-17 21:11:36+00:00
- **Updated**: 2023-01-08 23:21:37+00:00
- **Authors**: Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, Dimitris Samaras
- **Comment**: NeurIPS 2022; code: https://github.com/AlexGraikos/diffusion_priors
- **Journal**: None
- **Summary**: We consider the problem of inferring high-dimensional data $\mathbf{x}$ in a model that consists of a prior $p(\mathbf{x})$ and an auxiliary differentiable constraint $c(\mathbf{x},\mathbf{y})$ on $x$ given some additional information $\mathbf{y}$. In this paper, the prior is an independently trained denoising diffusion generative model. The auxiliary constraint is expected to have a differentiable form, but can come from diverse sources. The possibility of such inference turns diffusion models into plug-and-play modules, thereby allowing a range of potential applications in adapting models to new domains and tasks, such as conditional generation or image segmentation. The structure of diffusion models allows us to perform approximate inference by iterating differentiation through the fixed denoising network enriched with different amounts of noise at each step. Considering many noised versions of $\mathbf{x}$ in evaluation of its fitness is a novel search mechanism that may lead to new algorithms for solving combinatorial optimization problems.



### Landscape Learning for Neural Network Inversion
- **Arxiv ID**: http://arxiv.org/abs/2206.09027v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09027v1)
- **Published**: 2022-06-17 22:05:29+00:00
- **Updated**: 2022-06-17 22:05:29+00:00
- **Authors**: Ruoshi Liu, Chengzhi Mao, Purva Tendulkar, Hao Wang, Carl Vondrick
- **Comment**: 15 pages, 9 figures
- **Journal**: None
- **Summary**: Many machine learning methods operate by inverting a neural network at inference time, which has become a popular technique for solving inverse problems in computer vision, robotics, and graphics. However, these methods often involve gradient descent through a highly non-convex loss landscape, causing the optimization process to be unstable and slow. We introduce a method that learns a loss landscape where gradient descent is efficient, bringing massive improvement and acceleration to the inversion process. We demonstrate this advantage on a number of methods for both generative and discriminative tasks, including GAN inversion, adversarial defense, and 3D human pose reconstruction.



### Towards Better Selective Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.09034v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09034v4)
- **Published**: 2022-06-17 22:23:11+00:00
- **Updated**: 2023-03-01 20:36:43+00:00
- **Authors**: Leo Feng, Mohamed Osama Ahmed, Hossein Hajimirsadeghi, Amir Abdi
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of Selective Classification where the objective is to achieve the best performance on a predetermined ratio (coverage) of the dataset. Recent state-of-the-art selective methods come with architectural changes either via introducing a separate selection head or an extra abstention logit. In this paper, we challenge the aforementioned methods. The results suggest that the superior performance of state-of-the-art methods is owed to training a more generalizable classifier rather than their proposed selection mechanisms. We argue that the best performing selection mechanism should instead be rooted in the classifier itself. Our proposed selection strategy uses the classification scores and achieves better results by a significant margin, consistently, across all coverages and all datasets, without any added compute cost. Furthermore, inspired by semi-supervised learning, we propose an entropy-based regularizer that improves the performance of selective classification methods. Our proposed selection mechanism with the proposed entropy-based regularizer achieves new state-of-the-art results.



### Validation of Vector Data using Oblique Images
- **Arxiv ID**: http://arxiv.org/abs/2206.09038v1
- **DOI**: 10.1145/1463434.1463464
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09038v1)
- **Published**: 2022-06-17 22:45:31+00:00
- **Updated**: 2022-06-17 22:45:31+00:00
- **Authors**: Pragyana Mishra, Eyal Ofek, Gur Kimchi
- **Comment**: In Proceedings of 16th ACM SIGSPATIAL International Conference on
  Advances in Geographic Information Systems (ACM GIS'08)
- **Journal**: Proceedings of the 16th ACM SIGSPATIAL International Conference on
  Advances in Geographic Information Systems (ACM GIS '08), pp. 1-10. 2008
- **Summary**: Oblique images are aerial photographs taken at oblique angles to the earth's surface. Projections of vector and other geospatial data in these images depend on camera parameters, positions of the geospatial entities, surface terrain, occlusions, and visibility. This paper presents a robust and scalable algorithm to detect inconsistencies in vector data using oblique images. The algorithm uses image descriptors to encode the local appearance of a geospatial entity in images. These image descriptors combine color, pixel-intensity gradients, texture, and steerable filter responses. A Support Vector Machine classifier is trained to detect image descriptors that are not consistent with underlying vector data, digital elevation maps, building models, and camera parameters. In this paper, we train the classifier on visible road segments and non-road data. Thereafter, the trained classifier detects inconsistencies in vectors, which include both occluded and misaligned road segments. The consistent road segments validate our vector, DEM, and 3-D model data for those areas while inconsistent segments point out errors. We further show that a search for descriptors that are consistent with visible road segments in the neighborhood of a misaligned road yields the desired road alignment that is consistent with pixels in the image.



