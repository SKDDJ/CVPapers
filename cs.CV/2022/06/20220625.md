# Arxiv Papers in cs.CV on 2022-06-25
### From Shallow to Deep: Compositional Reasoning over Graphs for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2206.12533v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12533v1)
- **Published**: 2022-06-25 02:20:02+00:00
- **Updated**: 2022-06-25 02:20:02+00:00
- **Authors**: Zihao Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In order to achieve a general visual question answering (VQA) system, it is essential to learn to answer deeper questions that require compositional reasoning on the image and external knowledge. Meanwhile, the reasoning process should be explicit and explainable to understand the working mechanism of the model. It is effortless for human but challenging for machines. In this paper, we propose a Hierarchical Graph Neural Module Network (HGNMN) that reasons over multi-layer graphs with neural modules to address the above issues. Specifically, we first encode the image by multi-layer graphs from the visual, semantic and commonsense views since the clues that support the answer may exist in different modalities. Our model consists of several well-designed neural modules that perform specific functions over graphs, which can be used to conduct multi-step reasoning within and between different graphs. Compared to existing modular networks, we extend visual reasoning from one graph to more graphs. We can explicitly trace the reasoning process according to module weights and graph attentions. Experiments show that our model not only achieves state-of-the-art performance on the CRIC dataset but also obtains explicit and explainable reasoning procedures.



### SLIC: Self-Supervised Learning with Iterative Clustering for Human Action Videos
- **Arxiv ID**: http://arxiv.org/abs/2206.12534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12534v1)
- **Published**: 2022-06-25 02:20:51+00:00
- **Updated**: 2022-06-25 02:20:51+00:00
- **Authors**: Salar Hosseini Khorasgani, Yuxuan Chen, Florian Shkurti
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: Self-supervised methods have significantly closed the gap with end-to-end supervised learning for image classification. In the case of human action videos, however, where both appearance and motion are significant factors of variation, this gap remains significant. One of the key reasons for this is that sampling pairs of similar video clips, a required step for many self-supervised contrastive learning methods, is currently done conservatively to avoid false positives. A typical assumption is that similar clips only occur temporally close within a single video, leading to insufficient examples of motion similarity. To mitigate this, we propose SLIC, a clustering-based self-supervised contrastive learning method for human action videos. Our key contribution is that we improve upon the traditional intra-video positive sampling by using iterative clustering to group similar video instances. This enables our method to leverage pseudo-labels from the cluster assignments to sample harder positives and negatives. SLIC outperforms state-of-the-art video retrieval baselines by +15.4% on top-1 recall on UCF101 and by +5.7% when directly transferred to HMDB51. With end-to-end finetuning for action classification, SLIC achieves 83.2% top-1 accuracy (+0.8%) on UCF101 and 54.5% on HMDB51 (+1.6%). SLIC is also competitive with the state-of-the-art in action classification after self-supervised pretraining on Kinetics400.



### FastBVP-Net: a lightweight pulse extraction network for measuring heart rhythm via facial videos
- **Arxiv ID**: http://arxiv.org/abs/2206.12558v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12558v3)
- **Published**: 2022-06-25 05:24:52+00:00
- **Updated**: 2022-12-21 16:11:22+00:00
- **Authors**: Jialiang Zhuang, Yuheng Chen, Yun Zhang, Xiujuan Zheng
- **Comment**: 9 pages, 2figures
- **Journal**: None
- **Summary**: Remote photoplethysmography (rPPG) is an attractive camera-based health monitoring method that can measure the heart rhythm from facial videos. Many well-established deep-learning models have been reported to measure heart rate (HR) and heart rate variability (HRV). However, most of these models usually require a 30-second facial video and enormous computational resources to obtain accurate and robust results, which significantly limits their applications in real-world scenarios. Hence, we propose a lightweight pulse extraction network, FastBVP-Net, to quickly measure heart rhythm via facial videos. The proposed FastBVP-Net uses a multi-frequency mode signal fusion (MMSF) mechanism to characterize the different modes of the raw signals in a decompose module and reconstruct the blood volume pulse (BVP) signal under a complex noise environment in a compose module. Meanwhile, an oversampling training scheme is used to solve the over-fitting problem caused by the limitations of the datasets. Then, the HR and HRV can be estimated based on the extracted BVP signals. Comprehensive experiments are conducted on the benchmark datasets to validate the proposed FastBVP-Net. For intra-dataset and cross-dataset testing, the proposed approach achieves better performance for HR and HRV estimation from 30-second facial videos with fewer computational burdens than the current well-established methods. Moreover, the proposed approach also achieves competitive results from 15-second facial videos. Therefore, the proposed FastBVP-Net has the potential to be applied in many real-world scenarios with shorter videos.



### CV 3315 Is All You Need : Semantic Segmentation Competition
- **Arxiv ID**: http://arxiv.org/abs/2206.12571v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12571v2)
- **Published**: 2022-06-25 06:27:57+00:00
- **Updated**: 2022-07-04 08:40:52+00:00
- **Authors**: Akide Liu, Zihan Wang
- **Comment**: arXiv admin note: text overlap with arXiv:2105.15203 by other authors
- **Journal**: None
- **Summary**: This competition focus on Urban-Sense Segmentation based on the vehicle camera view. Class highly unbalanced Urban-Sense images dataset challenge the existing solutions and further studies. Deep Conventional neural network-based semantic segmentation methods such as encoder-decoder architecture and multi-scale and pyramid-based approaches become flexible solutions applicable to real-world applications. In this competition, we mainly review the literature and conduct experiments on transformer-driven methods especially SegFormer, to achieve an optimal trade-off between performance and efficiency. For example, SegFormer-B0 achieved 74.6% mIoU with the smallest FLOPS, 15.6G, and the largest model, SegFormer- B5 archived 80.2% mIoU. According to multiple factors, including individual case failure analysis, individual class performance, training pressure and efficiency estimation, the final candidate model for the competition is SegFormer- B2 with 50.6 GFLOPS and 78.5% mIoU evaluated on the testing set. Checkout our code implementation at https://vmv.re/cv3315.



### RSTAM: An Effective Black-Box Impersonation Attack on Face Recognition using a Mobile and Compact Printer
- **Arxiv ID**: http://arxiv.org/abs/2206.12590v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12590v1)
- **Published**: 2022-06-25 08:16:55+00:00
- **Updated**: 2022-06-25 08:16:55+00:00
- **Authors**: Xiaoliang Liu, Furao Shen, Jian Zhao, Changhai Nie
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition has achieved considerable progress in recent years thanks to the development of deep neural networks, but it has recently been discovered that deep neural networks are vulnerable to adversarial examples. This means that face recognition models or systems based on deep neural networks are also susceptible to adversarial examples. However, the existing methods of attacking face recognition models or systems with adversarial examples can effectively complete white-box attacks but not black-box impersonation attacks, physical attacks, or convenient attacks, particularly on commercial face recognition systems. In this paper, we propose a new method to attack face recognition models or systems called RSTAM, which enables an effective black-box impersonation attack using an adversarial mask printed by a mobile and compact printer. First, RSTAM enhances the transferability of the adversarial masks through our proposed random similarity transformation strategy. Furthermore, we propose a random meta-optimization strategy for ensembling several pre-trained face models to generate more general adversarial masks. Finally, we conduct experiments on the CelebA-HQ, LFW, Makeup Transfer (MT), and CASIA-FaceV5 datasets. The performance of the attacks is also evaluated on state-of-the-art commercial face recognition systems: Face++, Baidu, Aliyun, Tencent, and Microsoft. Extensive experiments show that RSTAM can effectively perform black-box impersonation attacks on face recognition models or systems.



### Asymmetric Transfer Hashing with Adaptive Bipartite Graph Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.12592v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12592v2)
- **Published**: 2022-06-25 08:24:34+00:00
- **Updated**: 2022-12-27 06:21:19+00:00
- **Authors**: Jianglin Lu, Jie Zhou, Yudong Chen, Witold Pedrycz, Kwok-Wai Hung
- **Comment**: None
- **Journal**: None
- **Summary**: Thanks to the efficient retrieval speed and low storage consumption, learning to hash has been widely used in visual retrieval tasks. However, existing hashing methods assume that the query and retrieval samples lie in homogeneous feature space within the same domain. As a result, they cannot be directly applied to heterogeneous cross-domain retrieval. In this paper, we propose a Generalized Image Transfer Retrieval (GITR) problem, which encounters two crucial bottlenecks: 1) the query and retrieval samples may come from different domains, leading to an inevitable {domain distribution gap}; 2) the features of the two domains may be heterogeneous or misaligned, bringing up an additional {feature gap}. To address the GITR problem, we propose an Asymmetric Transfer Hashing (ATH) framework with its unsupervised/semi-supervised/supervised realizations. Specifically, ATH characterizes the domain distribution gap by the discrepancy between two asymmetric hash functions, and minimizes the feature gap with the help of a novel adaptive bipartite graph constructed on cross-domain data. By jointly optimizing asymmetric hash functions and the bipartite graph, not only can knowledge transfer be achieved but information loss caused by feature alignment can also be avoided. Meanwhile, to alleviate negative transfer, the intrinsic geometrical structure of single-domain data is preserved by involving a domain affinity graph. Extensive experiments on both single-domain and cross-domain benchmarks under different GITR subtasks indicate the superiority of our ATH method in comparison with the state-of-the-art hashing methods.



### Non-iterative Coarse-to-fine Registration based on Single-pass Deep Cumulative Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.12596v2
- **DOI**: 10.1007/978-3-031-16446-0_9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12596v2)
- **Published**: 2022-06-25 08:34:59+00:00
- **Updated**: 2022-06-30 19:38:14+00:00
- **Authors**: Mingyuan Meng, Lei Bi, Dagan Feng, Jinman Kim
- **Comment**: Accepted at International Conference on Medical Image Computing and
  Computer Assisted Intervention (MICCAI 2022)
- **Journal**: International Conference on Medical Image Computing and
  Computer-Assisted Intervention, pp. 88-97, 2022
- **Summary**: Deformable image registration is a crucial step in medical image analysis for finding a non-linear spatial transformation between a pair of fixed and moving images. Deep registration methods based on Convolutional Neural Networks (CNNs) have been widely used as they can perform image registration in a fast and end-to-end manner. However, these methods usually have limited performance for image pairs with large deformations. Recently, iterative deep registration methods have been used to alleviate this limitation, where the transformations are iteratively learned in a coarse-to-fine manner. However, iterative methods inevitably prolong the registration runtime, and tend to learn separate image features for each iteration, which hinders the features from being leveraged to facilitate the registration at later iterations. In this study, we propose a Non-Iterative Coarse-to-finE registration Network (NICE-Net) for deformable image registration. In the NICE-Net, we propose: (i) a Single-pass Deep Cumulative Learning (SDCL) decoder that can cumulatively learn coarse-to-fine transformations within a single pass (iteration) of the network, and (ii) a Selectively-propagated Feature Learning (SFL) encoder that can learn common image features for the whole coarse-to-fine registration process and selectively propagate the features as needed. Extensive experiments on six public datasets of 3D brain Magnetic Resonance Imaging (MRI) show that our proposed NICE-Net can outperform state-of-the-art iterative deep registration methods while only requiring similar runtime to non-iterative methods.



### Learn to Predict How Humans Manipulate Large-sized Objects from Interactive Motions
- **Arxiv ID**: http://arxiv.org/abs/2206.12612v1
- **DOI**: 10.1109/LRA.2022.3151614
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2206.12612v1)
- **Published**: 2022-06-25 09:55:39+00:00
- **Updated**: 2022-06-25 09:55:39+00:00
- **Authors**: Weilin Wan, Lei Yang, Lingjie Liu, Zhuoying Zhang, Ruixing Jia, Yi-King Choi, Jia Pan, Christian Theobalt, Taku Komura, Wenping Wang
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters ( Volume: 7, Issue: 2, April
  2022)
- **Summary**: Understanding human intentions during interactions has been a long-lasting theme, that has applications in human-robot interaction, virtual reality and surveillance. In this study, we focus on full-body human interactions with large-sized daily objects and aim to predict the future states of objects and humans given a sequential observation of human-object interaction. As there is no such dataset dedicated to full-body human interactions with large-sized daily objects, we collected a large-scale dataset containing thousands of interactions for training and evaluation purposes. We also observe that an object's intrinsic physical properties are useful for the object motion prediction, and thus design a set of object dynamic descriptors to encode such intrinsic properties. We treat the object dynamic descriptors as a new modality and propose a graph neural network, HO-GCN, to fuse motion data and dynamic descriptors for the prediction task. We show the proposed network that consumes dynamic descriptors can achieve state-of-the-art prediction results and help the network better generalize to unseen objects. We also demonstrate the predicted results are useful for human-robot collaborations.



### BokehMe: When Neural Rendering Meets Classical Rendering
- **Arxiv ID**: http://arxiv.org/abs/2206.12614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12614v1)
- **Published**: 2022-06-25 10:00:32+00:00
- **Updated**: 2022-06-25 10:00:32+00:00
- **Authors**: Juewen Peng, Zhiguo Cao, Xianrui Luo, Hao Lu, Ke Xian, Jianming Zhang
- **Comment**: Accepted by CVPR 2022 (Oral); Project:
  https://juewenpeng.github.io/BokehMe/
- **Journal**: None
- **Summary**: We propose BokehMe, a hybrid bokeh rendering framework that marries a neural renderer with a classical physically motivated renderer. Given a single image and a potentially imperfect disparity map, BokehMe generates high-resolution photo-realistic bokeh effects with adjustable blur size, focal plane, and aperture shape. To this end, we analyze the errors from the classical scattering-based method and derive a formulation to calculate an error map. Based on this formulation, we implement the classical renderer by a scattering-based method and propose a two-stage neural renderer to fix the erroneous areas from the classical renderer. The neural renderer employs a dynamic multi-scale scheme to efficiently handle arbitrary blur sizes, and it is trained to handle imperfect disparity input. Experiments show that our method compares favorably against previous methods on both synthetic image data and real image data with predicted disparity. A user study is further conducted to validate the advantage of our method.



### SAT: Self-adaptive training for fashion compatibility prediction
- **Arxiv ID**: http://arxiv.org/abs/2206.12622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12622v1)
- **Published**: 2022-06-25 11:05:31+00:00
- **Updated**: 2022-06-25 11:05:31+00:00
- **Authors**: Ling Xiao, Toshihiko Yamasaki
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a self-adaptive training (SAT) model for fashion compatibility prediction. It focuses on the learning of some hard items, such as those that share similar color, texture, and pattern features but are considered incompatible due to the aesthetics or temporal shifts. Specifically, we first design a method to define hard outfits and a difficulty score (DS) is defined and assigned to each outfit based on the difficulty in recommending an item for it. Then, we propose a self-adaptive triplet loss (SATL), where the DS of the outfit is considered. Finally, we propose a very simple conditional similarity network combining the proposed SATL to achieve the learning of hard items in the fashion compatibility prediction. Experiments on the publicly available Polyvore Outfits and Polyvore Outfits-D datasets demonstrate our SAT's effectiveness in fashion compatibility prediction. Besides, our SATL can be easily extended to other conditional similarity networks to improve their performance.



### Inverted Semantic-Index for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2206.12623v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2206.12623v1)
- **Published**: 2022-06-25 11:21:56+00:00
- **Updated**: 2022-06-25 11:21:56+00:00
- **Authors**: Ying Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the construction of inverted index for large-scale image retrieval. The inverted index proposed by J. Sivic brings a significant acceleration by reducing distance computations with only a small fraction of the database. The state-of-the-art inverted indices aim to build finer partitions that produce a concise and accurate candidate list. However, partitioning in these frameworks is generally achieved by unsupervised clustering methods which ignore the semantic information of images. In this paper, we replace the clustering method with image classification, during the construction of codebook. We then propose a merging and splitting method to solve the problem that the number of partitions is unchangeable in the inverted semantic-index. Next, we combine our semantic-index with the product quantization (PQ) so as to alleviate the accuracy loss caused by PQ compression. Finally, we evaluate our model on large-scale image retrieval benchmarks. Experiment results demonstrate that our model can significantly improve the retrieval accuracy by generating high-quality candidate lists.



### Tensor Recovery Based on A Novel Non-convex Function Minimax Logarithmic Concave Penalty Function
- **Arxiv ID**: http://arxiv.org/abs/2206.13506v1
- **DOI**: 10.1109/TIP.2023.3282072
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13506v1)
- **Published**: 2022-06-25 12:26:53+00:00
- **Updated**: 2022-06-25 12:26:53+00:00
- **Authors**: Hongbing Zhang, Xinyi Liu, Chang Liu, Hongtao Fan, Yajing Li, Xinyun Zhu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2201.12709,
  arXiv:2109.12257
- **Journal**: None
- **Summary**: Non-convex relaxation methods have been widely used in tensor recovery problems, and compared with convex relaxation methods, can achieve better recovery results. In this paper, a new non-convex function, Minimax Logarithmic Concave Penalty (MLCP) function, is proposed, and some of its intrinsic properties are analyzed, among which it is interesting to find that the Logarithmic function is an upper bound of the MLCP function. The proposed function is generalized to tensor cases, yielding tensor MLCP and weighted tensor $L\gamma$-norm. Consider that its explicit solution cannot be obtained when applying it directly to the tensor recovery problem. Therefore, the corresponding equivalence theorems to solve such problem are given, namely, tensor equivalent MLCP theorem and equivalent weighted tensor $L\gamma$-norm theorem. In addition, we propose two EMLCP-based models for classic tensor recovery problems, namely low-rank tensor completion (LRTC) and tensor robust principal component analysis (TRPCA), and design proximal alternate linearization minimization (PALM) algorithms to solve them individually. Furthermore, based on the Kurdyka-{\L}ojasiwicz property, it is proved that the solution sequence of the proposed algorithm has finite length and converges to the critical point globally. Finally, Extensive experiments show that proposed algorithm achieve good results, and it is confirmed that the MLCP function is indeed better than the Logarithmic function in the minimization problem, which is consistent with the analysis of theoretical properties.



### SC-Transformer++: Structured Context Transformer for Generic Event Boundary Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.12634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12634v1)
- **Published**: 2022-06-25 12:27:13+00:00
- **Updated**: 2022-06-25 12:27:13+00:00
- **Authors**: Dexiang Hong, Xiaoqi Ma, Xinyao Wang, Congcong Li, Yufei Wang, Longyin Wen
- **Comment**: winner method at LOVEU@CVPR'22 Generic Event Boundary Detection
  Challenge
- **Journal**: None
- **Summary**: This report presents the algorithm used in the submission of Generic Event Boundary Detection (GEBD) Challenge at CVPR 2022. In this work, we improve the existing Structured Context Transformer (SC-Transformer) method for GEBD. Specifically, a transformer decoder module is added after transformer encoders to extract high quality frame features. The final classification is performed jointly on the results of the original binary classifier and a newly introduced multi-class classifier branch. To enrich motion information, optical flow is introduced as a new modality. Finally, model ensemble is used to further boost performance. The proposed method achieves 86.49% F1 score on Kinetics-GEBD test set. which improves 2.86% F1 score compared to the previous SOTA method.



### BIMS-PU: Bi-Directional and Multi-Scale Point Cloud Upsampling
- **Arxiv ID**: http://arxiv.org/abs/2206.12648v1
- **DOI**: 10.1109/LRA.2022.3183932
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12648v1)
- **Published**: 2022-06-25 13:13:37+00:00
- **Updated**: 2022-06-25 13:13:37+00:00
- **Authors**: Yechao Bai, Xiaogang Wang, Marcelo H. Ang Jr, Daniela Rus
- **Comment**: Accepted to RA-L 2022. in IEEE Robotics and Automation Letters
- **Journal**: in IEEE Robotics and Automation Letters, vol. 7, no. 3, pp.
  7447-7454, July 2022
- **Summary**: The learning and aggregation of multi-scale features are essential in empowering neural networks to capture the fine-grained geometric details in the point cloud upsampling task. Most existing approaches extract multi-scale features from a point cloud of a fixed resolution, hence obtain only a limited level of details. Though an existing approach aggregates a feature hierarchy of different resolutions from a cascade of upsampling sub-network, the training is complex with expensive computation. To address these issues, we construct a new point cloud upsampling pipeline called BIMS-PU that integrates the feature pyramid architecture with a bi-directional up and downsampling path. Specifically, we decompose the up/downsampling procedure into several up/downsampling sub-steps by breaking the target sampling factor into smaller factors. The multi-scale features are naturally produced in a parallel manner and aggregated using a fast feature fusion method. Supervision signal is simultaneously applied to all upsampled point clouds of different scales. Moreover, we formulate a residual block to ease the training of our model. Extensive quantitative and qualitative experiments on different datasets show that our method achieves superior results to state-of-the-art approaches. Last but not least, we demonstrate that point cloud upsampling can improve robot perception by ameliorating the 3D data quality.



### Sentiment Analysis with R: Natural Language Processing for Semi-Automated Assessments of Qualitative Data
- **Arxiv ID**: http://arxiv.org/abs/2206.12649v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, stat.AP, D.1.5; D.3.0; I.7; J.4; K.4
- **Links**: [PDF](http://arxiv.org/pdf/2206.12649v1)
- **Published**: 2022-06-25 13:25:39+00:00
- **Updated**: 2022-06-25 13:25:39+00:00
- **Authors**: Dennis Klinkhammer
- **Comment**: 14 pages, 6 figures
- **Journal**: None
- **Summary**: Sentiment analysis is a sub-discipline in the field of natural language processing and computational linguistics and can be used for automated or semi-automated analyses of text documents. One of the aims of these analyses is to recognize an expressed attitude as positive or negative as it can be contained in comments on social media platforms or political documents and speeches as well as fictional and nonfictional texts. Regarding analyses of comments on social media platforms, this is an extension of the previous tutorial on semi-automated screenings of social media network data. A longitudinal perspective regarding social media comments as well as cross-sectional perspectives regarding fictional and nonfictional texts, e.g. entire books and libraries, can lead to extensive text documents. Their analyses can be simplified and accelerated by using sentiment analysis with acceptable inter-rater reliability. Therefore, this tutorial introduces the basic functions for performing a sentiment analysis with R and explains how text documents can be analysed step by step - regardless of their underlying formatting. All prerequisites and steps are described in detail and associated codes are available on GitHub. A comparison of two political speeches illustrates a possible use case.



### Machine Learning-based Biological Ageing Estimation Technologies: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2206.12650v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12650v1)
- **Published**: 2022-06-25 13:38:39+00:00
- **Updated**: 2022-06-25 13:38:39+00:00
- **Authors**: Zhaonian Zhang, Richard Jiang, Danny Crookes, Paul Chazot
- **Comment**: in Recent Advances in AI-enabled Automated Medical Diagnosis
  https://www.routledge.com/Recent-Advances-in-AI-enabled-Automated-Medical-Diagnosis/Jiang-Crookes-Wei-Zhang-Chazot/p/book/9781032008431
- **Journal**: Recent Advances in AI-enabled Automated Medical Diagnosis, 2022
- **Summary**: In recent years, there are various methods of estimating Biological Age (BA) have been developed. Especially with the development of machine learning (ML), there are more and more types of BA predictions, and the accuracy has been greatly improved. The models for the estimation of BA play an important role in monitoring healthy aging, and could provide new tools to detect health status in the general population and give warnings to sub-healthy people. We will mainly review three age prediction methods by using ML. They are based on blood biomarkers, facial images, and structural neuroimaging features. For now, the model using blood biomarkers is the simplest, most direct, and most accurate method. The face image method is affected by various aspects such as race, environment, etc., the prediction accuracy is not very good, which cannot make a great contribution to the medical field. In summary, we are here to track the way forward in the era of big data for us and other potential general populations and show ways to leverage the vast amounts of data available today.



### Review on Social Behavior Analysis of Laboratory Animals: From Methodologies to Applications
- **Arxiv ID**: http://arxiv.org/abs/2206.12651v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12651v1)
- **Published**: 2022-06-25 13:40:35+00:00
- **Updated**: 2022-06-25 13:40:35+00:00
- **Authors**: Ziping Jiang, Paul L. Chazot, Richard Jiang
- **Comment**: https://www.routledge.com/Recent-Advances-in-AI-enabled-Automated-Medical-Diagnosis/Jiang-Crookes-Wei-Zhang-Chazot/p/book/9781032008431
- **Journal**: Recent Advances in AI-enabled Automated Medical Diagnosis, 2022
- **Summary**: As the bridge between genetic and physiological aspects, animal behaviour analysis is one of the most significant topics in biology and ecological research. However, identifying, tracking and recording animal behaviour are labour intensive works that require professional knowledge. To mitigate the spend for annotating data, researchers turn to computer vision techniques for automatic label algorithms, since most of the data are recorded visually. In this work, we explore a variety of behaviour detection algorithms, covering traditional vision methods, statistical methods and deep learning methods. The objective of this work is to provide a thorough investigation of related work, furnishing biologists with a scratch of efficient animal behaviour detection methods. Apart from that, we also discuss the strengths and weaknesses of those algorithms to provide some insights for those who already delve into this field.



### Diagnostic Communication and Visual System based on Vehicle UDS Protocol
- **Arxiv ID**: http://arxiv.org/abs/2206.12653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12653v1)
- **Published**: 2022-06-25 13:47:56+00:00
- **Updated**: 2022-06-25 13:47:56+00:00
- **Authors**: Hong Zhang, Ding Li
- **Comment**: None
- **Journal**: None
- **Summary**: Unified Diagnostic Services (UDS) is a diagnostic communication protocol used in electronic control units (ECUs) within automotive electronics, which is specified in the ISO 14229-1. It is derived from ISO 14230-3 (KWP2000) and the now obsolete ISO 15765-3 (Diagnostic Communication over Controller Area Network (DoCAN). 'Unified' in this context means that it is an international and not a company-specific standard. By now this communication protocol is used in all new ECUs made by Tier 1 suppliers of Original Equipment Manufacturer (OEM), and is incorporated into other standards, such as AUTOSAR. The ECUs in modern vehicles control nearly all functions, including electronic fuel injection (EFI), engine control, the transmission, anti-lock braking system, door locks, braking, window operation, and more.



### Enhanced Deep Animation Video Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2206.12657v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.12657v1)
- **Published**: 2022-06-25 14:00:48+00:00
- **Updated**: 2022-06-25 14:00:48+00:00
- **Authors**: Wang Shen, Cheng Ming, Wenbo Bao, Guangtao Zhai, Li Chen, Zhiyong Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Existing learning-based frame interpolation algorithms extract consecutive frames from high-speed natural videos to train the model. Compared to natural videos, cartoon videos are usually in a low frame rate. Besides, the motion between consecutive cartoon frames is typically nonlinear, which breaks the linear motion assumption of interpolation algorithms. Thus, it is unsuitable for generating a training set directly from cartoon videos. For better adapting frame interpolation algorithms from nature video to animation video, we present AutoFI, a simple and effective method to automatically render training data for deep animation video interpolation. AutoFI takes a layered architecture to render synthetic data, which ensures the assumption of linear motion. Experimental results show that AutoFI performs favorably in training both DAIN and ANIN. However, most frame interpolation algorithms will still fail in error-prone areas, such as fast motion or large occlusion. Besides AutoFI, we also propose a plug-and-play sketch-based post-processing module, named SktFI, to refine the final results using user-provided sketches manually. With AutoFI and SktFI, the interpolated animation frames show high perceptual quality.



### Learning to Infer 3D Shape Programs with Differentiable Renderer
- **Arxiv ID**: http://arxiv.org/abs/2206.12675v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12675v1)
- **Published**: 2022-06-25 15:44:05+00:00
- **Updated**: 2022-06-25 15:44:05+00:00
- **Authors**: Yichao Liang
- **Comment**: Technical report written in 2020; 10 pages, 5 figures. arXiv admin
  note: substantial text overlap with arXiv:1901.02875 by other authors
- **Journal**: None
- **Summary**: Given everyday artifacts, such as tables and chairs, humans recognize high-level regularities within them, such as the symmetries of a table, the repetition of its legs, while possessing low-level priors of their geometries, e.g., surfaces are smooth and edges are sharp. This kind of knowledge constitutes an important part of human perceptual understanding and reasoning. Representations of and how to reason in such knowledge, and the acquisition thereof, are still open questions in artificial intelligence (AI) and cognitive science. Building on the previous proposal of the \emph{3D shape programs} representation alone with the accompanying neural generator and executor from \citet{tian2019learning}, we propose an analytical yet differentiable executor that is more faithful and controllable in interpreting shape programs (particularly in extrapolation) and more sample efficient (requires no training). These facilitate the generator's learning when ground truth programs are not available, and should be especially useful when new shape-program components are enrolled either by human designers or -- in the context of library learning -- algorithms themselves. Preliminary experiments on using it for adaptation illustrate the aforesaid advantages of the proposed module, encouraging similar methods being explored in building machines that learn to reason with the kind of knowledge described above, and even learn this knowledge itself.



### UltraMNIST Classification: A Benchmark to Train CNNs for Very Large Images
- **Arxiv ID**: http://arxiv.org/abs/2206.12681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12681v1)
- **Published**: 2022-06-25 16:04:28+00:00
- **Updated**: 2022-06-25 16:04:28+00:00
- **Authors**: Deepak K. Gupta, Udbhav Bamba, Abhishek Thakur, Akash Gupta, Suraj Sharan, Ertugrul Demir, Dilip K. Prasad
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural network (CNN) approaches available in the current literature are designed to work primarily with low-resolution images. When applied on very large images, challenges related to GPU memory, smaller receptive field than needed for semantic correspondence and the need to incorporate multi-scale features arise. The resolution of input images can be reduced, however, with significant loss of critical information. Based on the outlined issues, we introduce a novel research problem of training CNN models for very large images, and present 'UltraMNIST dataset', a simple yet representative benchmark dataset for this task. UltraMNIST has been designed using the popular MNIST digits with additional levels of complexity added to replicate well the challenges of real-world problems. We present two variants of the problem: 'UltraMNIST classification' and 'Budget-aware UltraMNIST classification'. The standard UltraMNIST classification benchmark is intended to facilitate the development of novel CNN training methods that make the effective use of the best available GPU resources. The budget-aware variant is intended to promote development of methods that work under constrained GPU memory. For the development of competitive solutions, we present several baseline models for the standard benchmark and its budget-aware variant. We study the effect of reducing resolution on the performance and present results for baseline models involving pretrained backbones from among the popular state-of-the-art models. Finally, with the presented benchmark dataset and the baselines, we hope to pave the ground for a new generation of CNN methods suitable for handling large images in an efficient and resource-light manner.



### Defense against adversarial attacks on deep convolutional neural networks through nonlocal denoising
- **Arxiv ID**: http://arxiv.org/abs/2206.12685v1
- **DOI**: 10.11591/ijai.v11.i3.pp961-968
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12685v1)
- **Published**: 2022-06-25 16:11:25+00:00
- **Updated**: 2022-06-25 16:11:25+00:00
- **Authors**: Sandhya Aneja, Nagender Aneja, Pg Emeroylariffion Abas, Abdul Ghani Naim
- **Comment**: None
- **Journal**: IAES International Journal of Artificial Intelligence, Vol. 11,
  No. 3, September 2022, pp. 961~968, ISSN: 2252-8938
- **Summary**: Despite substantial advances in network architecture performance, the susceptibility of adversarial attacks makes deep learning challenging to implement in safety-critical applications. This paper proposes a data-centric approach to addressing this problem. A nonlocal denoising method with different luminance values has been used to generate adversarial examples from the Modified National Institute of Standards and Technology database (MNIST) and Canadian Institute for Advanced Research (CIFAR-10) data sets. Under perturbation, the method provided absolute accuracy improvements of up to 9.3% in the MNIST data set and 13% in the CIFAR-10 data set. Training using transformed images with higher luminance values increases the robustness of the classifier. We have shown that transfer learning is disadvantageous for adversarial machine learning. The results indicate that simple adversarial examples can improve resilience and make deep learning easier to apply in various applications.



### RandStainNA: Learning Stain-Agnostic Features from Histology Slides by Bridging Stain Augmentation and Normalization
- **Arxiv ID**: http://arxiv.org/abs/2206.12694v1
- **DOI**: 10.1007/978-3-031-16434-7_21
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12694v1)
- **Published**: 2022-06-25 16:43:59+00:00
- **Updated**: 2022-06-25 16:43:59+00:00
- **Authors**: Yiqing Shen, Yulin Luo, Dinggang Shen, Jing Ke
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Stain variations often decrease the generalization ability of deep learning based approaches in digital histopathology analysis. Two separate proposals, namely stain normalization (SN) and stain augmentation (SA), have been spotlighted to reduce the generalization error, where the former alleviates the stain shift across different medical centers using template image and the latter enriches the accessible stain styles by the simulation of more stain variations. However, their applications are bounded by the selection of template images and the construction of unrealistic styles. To address the problems, we unify SN and SA with a novel RandStainNA scheme, which constrains variable stain styles in a practicable range to train a stain agnostic deep learning model. The RandStainNA is applicable to stain normalization in a collection of color spaces i.e. HED, HSV, LAB. Additionally, we propose a random color space selection scheme to gain extra performance improvement. We evaluate our method by two diagnostic tasks i.e. tissue subtype classification and nuclei segmentation, with various network backbones. The performance superiority over both SA and SN yields that the proposed RandStainNA can consistently improve the generalization ability, that our models can cope with more incoming clinical datasets with unpredicted stain styles. The codes is available at https://github.com/yiqings/RandStainNA.



### Correction Algorithm of Sampling Effect and Its Application
- **Arxiv ID**: http://arxiv.org/abs/2207.00004v1
- **DOI**: None
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.00004v1)
- **Published**: 2022-06-25 17:44:47+00:00
- **Updated**: 2022-06-25 17:44:47+00:00
- **Authors**: Yunqi Sun, Jianfeng Zhou
- **Comment**: 7 pages, 7 figures
- **Journal**: None
- **Summary**: The sampling effect of the imaging acquisition device is long considered to be a modulation process of the input signal, introducing additional error into the signal acquisition process. This paper proposes a correction algorithm for the modulation process that solves the sampling effect with high accuracy. We examine the algorithm with perfect continuous Gaussian images and selected digitized images, which indicate an accuracy increase of 106 for Gaussian images, 102 at 15 times of Shannon interpolation for digitized images, and 105 at 101 times of Shannon interpolation for digitized images. The accuracy limit of the Gaussian image comes from the truncation error, while the accuracy limit of the digitized images comes from their finite resolution, which can be improved by increasing the time of Shannon interpolation.



### Anatomy-Guided Weakly-Supervised Abnormality Localization in Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2206.12704v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12704v1)
- **Published**: 2022-06-25 18:33:27+00:00
- **Updated**: 2022-06-25 18:33:27+00:00
- **Authors**: Ke Yu, Shantanu Ghosh, Zhexiong Liu, Christopher Deible, Kayhan Batmanghelich
- **Comment**: Accepted by MICCAI 20222
- **Journal**: None
- **Summary**: Creating a large-scale dataset of abnormality annotation on medical images is a labor-intensive and costly task. Leveraging weak supervision from readily available data such as radiology reports can compensate lack of large-scale data for anomaly detection methods. However, most of the current methods only use image-level pathological observations, failing to utilize the relevant anatomy mentions in reports. Furthermore, Natural Language Processing (NLP)-mined weak labels are noisy due to label sparsity and linguistic ambiguity. We propose an Anatomy-Guided chest X-ray Network (AGXNet) to address these issues of weak annotation. Our framework consists of a cascade of two networks, one responsible for identifying anatomical abnormalities and the second responsible for pathological observations. The critical component in our framework is an anatomy-guided attention module that aids the downstream observation network in focusing on the relevant anatomical regions generated by the anatomy network. We use Positive Unlabeled (PU) learning to account for the fact that lack of mention does not necessarily mean a negative label. Our quantitative and qualitative results on the MIMIC-CXR dataset demonstrate the effectiveness of AGXNet in disease and anatomical abnormality localization. Experiments on the NIH Chest X-ray dataset show that the learned feature representations are transferable and can achieve the state-of-the-art performances in disease classification and competitive disease localization results. Our code is available at https://github.com/batmanlab/AGXNet



### p-Meta: Towards On-device Deep Model Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2206.12705v1
- **DOI**: 10.1145/3534678.3539293
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.12705v1)
- **Published**: 2022-06-25 18:36:59+00:00
- **Updated**: 2022-06-25 18:36:59+00:00
- **Authors**: Zhongnan Qu, Zimu Zhou, Yongxin Tong, Lothar Thiele
- **Comment**: Published in SIGKDD 2022
- **Journal**: None
- **Summary**: Data collected by IoT devices are often private and have a large diversity across users. Therefore, learning requires pre-training a model with available representative data samples, deploying the pre-trained model on IoT devices, and adapting the deployed model on the device with local data. Such an on-device adaption for deep learning empowered applications demands data and memory efficiency. However, existing gradient-based meta learning schemes fail to support memory-efficient adaptation. To this end, we propose p-Meta, a new meta learning method that enforces structure-wise partial parameter updates while ensuring fast generalization to unseen tasks. Evaluations on few-shot image classification and reinforcement learning tasks show that p-Meta not only improves the accuracy but also substantially reduces the peak dynamic memory by a factor of 2.5 on average compared to state-of-the-art few-shot adaptation methods.



### Defending Multimodal Fusion Models against Single-Source Adversaries
- **Arxiv ID**: http://arxiv.org/abs/2206.12714v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, 68T01, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2206.12714v1)
- **Published**: 2022-06-25 18:57:02+00:00
- **Updated**: 2022-06-25 18:57:02+00:00
- **Authors**: Karren Yang, Wan-Yi Lin, Manash Barman, Filipe Condessa, Zico Kolter
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Beyond achieving high performance across many vision tasks, multimodal models are expected to be robust to single-source faults due to the availability of redundant information between modalities. In this paper, we investigate the robustness of multimodal neural networks against worst-case (i.e., adversarial) perturbations on a single modality. We first show that standard multimodal fusion models are vulnerable to single-source adversaries: an attack on any single modality can overcome the correct information from multiple unperturbed modalities and cause the model to fail. This surprising vulnerability holds across diverse multimodal tasks and necessitates a solution. Motivated by this finding, we propose an adversarially robust fusion strategy that trains the model to compare information coming from all the input sources, detect inconsistencies in the perturbed modality compared to the other modalities, and only allow information from the unperturbed modalities to pass through. Our approach significantly improves on state-of-the-art methods in single-source robustness, achieving gains of 7.8-25.2% on action recognition, 19.7-48.2% on object detection, and 1.6-6.7% on sentiment analysis, without degrading performance on unperturbed (i.e., clean) data.



### Empirical Evaluation of Physical Adversarial Patch Attacks Against Overhead Object Detection Models
- **Arxiv ID**: http://arxiv.org/abs/2206.12725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12725v1)
- **Published**: 2022-06-25 20:05:11+00:00
- **Updated**: 2022-06-25 20:05:11+00:00
- **Authors**: Gavin S. Hartnett, Li Ang Zhang, Caolionn O'Connell, Andrew J. Lohn, Jair Aguirre
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial patches are images designed to fool otherwise well-performing neural network-based computer vision models. Although these attacks were initially conceived of and studied digitally, in that the raw pixel values of the image were perturbed, recent work has demonstrated that these attacks can successfully transfer to the physical world. This can be accomplished by printing out the patch and adding it into scenes of newly captured images or video footage. In this work we further test the efficacy of adversarial patch attacks in the physical world under more challenging conditions. We consider object detection models trained on overhead imagery acquired through aerial or satellite cameras, and we test physical adversarial patches inserted into scenes of a desert environment. Our main finding is that it is far more difficult to successfully implement the adversarial patch attacks under these conditions than in the previously considered conditions. This has important implications for AI safety as the real-world threat posed by adversarial examples may be overstated.



### Self-Supervised 3D Monocular Object Detection by Recycling Bounding Boxes
- **Arxiv ID**: http://arxiv.org/abs/2206.12738v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12738v1)
- **Published**: 2022-06-25 21:48:43+00:00
- **Updated**: 2022-06-25 21:48:43+00:00
- **Authors**: Sugirtha T, Sridevi M, Khailash Santhakumar, Hao Liu, B Ravi Kiran, Thomas Gauthier, Senthil Yogamani
- **Comment**: Published at ICCVW-SSLAD 2021. arXiv admin note: substantial text
  overlap with arXiv:2104.10786
- **Journal**: None
- **Summary**: Modern object detection architectures are moving towards employing self-supervised learning (SSL) to improve performance detection with related pretext tasks. Pretext tasks for monocular 3D object detection have not yet been explored yet in literature. The paper studies the application of established self-supervised bounding box recycling by labeling random windows as the pretext task. The classifier head of the 3D detector is trained to classify random windows containing different proportions of the ground truth objects, thus handling the foreground-background imbalance. We evaluate the pretext task using the RTM3D detection model as baseline, with and without the application of data augmentation. We demonstrate improvements of between 2-3 % in mAP 3D and 0.9-1.5 % BEV scores using SSL over the baseline scores. We propose the inverse class frequency re-weighted (ICFW) mAP score that highlights improvements in detection for low frequency classes in a class imbalanced dataset with long tails. We demonstrate improvements in ICFW both mAP 3D and BEV scores to take into account the class imbalance in the KITTI validation dataset. We see 4-5 % increase in ICFW metric with the pretext task.



### Multi Visual Modality Fall Detection Dataset
- **Arxiv ID**: http://arxiv.org/abs/2206.12740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12740v1)
- **Published**: 2022-06-25 21:54:26+00:00
- **Updated**: 2022-06-25 21:54:26+00:00
- **Authors**: Stefan Denkovski, Shehroz S. Khan, Brandon Malamis, Sae Young Moon, Bing Ye, Alex Mihailidis
- **Comment**: None
- **Journal**: None
- **Summary**: Falls are one of the leading cause of injury-related deaths among the elderly worldwide. Effective detection of falls can reduce the risk of complications and injuries. Fall detection can be performed using wearable devices or ambient sensors; these methods may struggle with user compliance issues or false alarms. Video cameras provide a passive alternative; however, regular RGB cameras are impacted by changing lighting conditions and privacy concerns. From a machine learning perspective, developing an effective fall detection system is challenging because of the rarity and variability of falls. Many existing fall detection datasets lack important real-world considerations, such as varied lighting, continuous activities of daily living (ADLs), and camera placement. The lack of these considerations makes it difficult to develop predictive models that can operate effectively in the real world. To address these limitations, we introduce a novel multi-modality dataset (MUVIM) that contains four visual modalities: infra-red, depth, RGB and thermal cameras. These modalities offer benefits such as obfuscated facial features and improved performance in low-light conditions. We formulated fall detection as an anomaly detection problem, in which a customized spatio-temporal convolutional autoencoder was trained only on ADLs so that a fall would increase the reconstruction error. Our results showed that infra-red cameras provided the highest level of performance (AUC ROC=0.94), followed by thermal (AUC ROC=0.87), depth (AUC ROC=0.86) and RGB (AUC ROC=0.83). This research provides a unique opportunity to analyze the utility of camera modalities in detecting falls in a home setting while balancing performance, passiveness, and privacy.



### Sequential image recovery using joint hierarchical Bayesian learning
- **Arxiv ID**: http://arxiv.org/abs/2206.12745v2
- **DOI**: 10.1007/s10915-023-02234-1
- **Categories**: **cs.CV**, cs.NA, math.NA, 15A29, 62F15, 65F22, 65K10, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/2206.12745v2)
- **Published**: 2022-06-25 22:31:32+00:00
- **Updated**: 2023-05-19 15:04:48+00:00
- **Authors**: Yao Xiao, Jan Glaubitz
- **Comment**: 24 pages, 15 figures
- **Journal**: J Sci Comput 96, 4 (2023)
- **Summary**: Recovering temporal image sequences (videos) based on indirect, noisy, or incomplete data is an essential yet challenging task. We specifically consider the case where each data set is missing vital information, which prevents the accurate recovery of the individual images. Although some recent (variational) methods have demonstrated high-resolution image recovery based on jointly recovering sequential images, there remain robustness issues due to parameter tuning and restrictions on the type of the sequential images. Here, we present a method based on hierarchical Bayesian learning for the joint recovery of sequential images that incorporates prior intra- and inter-image information. Our method restores the missing information in each image by "borrowing" it from the other images. As a result, \emph{all} of the individual reconstructions yield improved accuracy. Our method can be used for various data acquisitions and allows for uncertainty quantification. Some preliminary results indicate its potential use for sequential deblurring and magnetic resonance imaging.



