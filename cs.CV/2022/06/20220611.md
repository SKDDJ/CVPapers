# Arxiv Papers in cs.CV on 2022-06-11
### A Benchmark for Compositional Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2206.05379v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.05379v1)
- **Published**: 2022-06-11 00:04:49+00:00
- **Updated**: 2022-06-11 00:04:49+00:00
- **Authors**: Aimen Zerroug, Mohit Vaishnav, Julien Colin, Sebastian Musslick, Thomas Serre
- **Comment**: None
- **Journal**: None
- **Summary**: A fundamental component of human vision is our ability to parse complex visual scenes and judge the relations between their constituent objects. AI benchmarks for visual reasoning have driven rapid progress in recent years with state-of-the-art systems now reaching human accuracy on some of these benchmarks. Yet, a major gap remains in terms of the sample efficiency with which humans and AI systems learn new visual reasoning tasks. Humans' remarkable efficiency at learning has been at least partially attributed to their ability to harness compositionality -- such that they can efficiently take advantage of previously gained knowledge when learning new tasks. Here, we introduce a novel visual reasoning benchmark, Compositional Visual Relations (CVR), to drive progress towards the development of more data-efficient learning algorithms. We take inspiration from fluidic intelligence and non-verbal reasoning tests and describe a novel method for creating compositions of abstract rules and associated image datasets at scale. Our proposed benchmark includes measures of sample efficiency, generalization and transfer across task rules, as well as the ability to leverage compositionality. We systematically evaluate modern neural architectures and find that, surprisingly, convolutional architectures surpass transformer-based architectures across all performance measures in most data regimes. However, all computational models are a lot less data efficient compared to humans even after learning informative visual representations using self-supervision. Overall, we hope that our challenge will spur interest in the development of neural architectures that can learn to harness compositionality toward more efficient learning.



### Transformer-based Self-Supervised Fish Segmentation in Underwater Videos
- **Arxiv ID**: http://arxiv.org/abs/2206.05390v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05390v1)
- **Published**: 2022-06-11 01:20:48+00:00
- **Updated**: 2022-06-11 01:20:48+00:00
- **Authors**: Alzayat Saleh, Marcus Sheaves, Dean Jerry, Mostafa Rahimi Azghadi
- **Comment**: 11 pages, 6 figures. Submitted to the journal, International Journal
  of Intelligent Systems
- **Journal**: None
- **Summary**: Underwater fish segmentation to estimate fish body measurements is still largely unsolved due to the complex underwater environment. Relying on fully-supervised segmentation models requires collecting per-pixel labels, which is time-consuming and prone to overfitting. Self-supervised learning methods can help avoid the requirement of large annotated training datasets, however, to be useful in real-world applications, they should achieve good segmentation quality. In this paper, we introduce a Transformer-based method that uses self-supervision for high-quality fish segmentation. Our proposed model is trained on videos -- without any annotations -- to perform fish segmentation in underwater videos taken in situ in the wild. We show that when trained on a set of underwater videos from one dataset, the proposed model surpasses previous CNN-based and Transformer-based self-supervised methods and achieves performance relatively close to supervised methods on two new unseen underwater video datasets. This demonstrates the great generalisability of our model and the fact that it does not need a pre-trained model. In addition, we show that, due to its dense representation learning, our model is compute-efficient. We provide quantitative and qualitative results that demonstrate our model's significant capabilities.



### Applications of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey
- **Arxiv ID**: http://arxiv.org/abs/2206.05394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05394v1)
- **Published**: 2022-06-11 01:59:54+00:00
- **Updated**: 2022-06-11 01:59:54+00:00
- **Authors**: Alzayat Saleh, Marcus Sheaves, Dean Jerry, Mostafa Rahimi Azghadi
- **Comment**: 26 pages, 7 figures. Submitted to the journal, Expert Systems With
  Applications
- **Journal**: None
- **Summary**: Marine ecosystems and their fish habitats are becoming increasingly important due to their integral role in providing a valuable food source and conservation outcomes. Due to their remote and difficult to access nature, marine environments and fish habitats are often monitored using underwater cameras. These cameras generate a massive volume of digital data, which cannot be efficiently analysed by current manual processing methods, which involve a human observer. DL is a cutting-edge AI technology that has demonstrated unprecedented performance in analysing visual data. Despite its application to a myriad of domains, its use in underwater fish habitat monitoring remains under explored. In this paper, we provide a tutorial that covers the key concepts of DL, which help the reader grasp a high-level understanding of how DL works. The tutorial also explains a step-by-step procedure on how DL algorithms should be developed for challenging applications such as underwater fish monitoring. In addition, we provide a comprehensive survey of key deep learning techniques for fish habitat monitoring including classification, counting, localization, and segmentation. Furthermore, we survey publicly available underwater fish datasets, and compare various DL techniques in the underwater fish monitoring domains. We also discuss some challenges and opportunities in the emerging field of deep learning for fish habitat processing. This paper is written to serve as a tutorial for marine scientists who would like to grasp a high-level understanding of DL, develop it for their applications by following our step-by-step tutorial, and see how it is evolving to facilitate their research efforts. At the same time, it is suitable for computer scientists who would like to survey state-of-the-art DL-based methodologies for fish habitat monitoring.



### E2PN: Efficient SE(3)-Equivariant Point Network
- **Arxiv ID**: http://arxiv.org/abs/2206.05398v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.05398v3)
- **Published**: 2022-06-11 02:15:46+00:00
- **Updated**: 2023-06-14 02:58:30+00:00
- **Authors**: Minghan Zhu, Maani Ghaffari, William A. Clark, Huei Peng
- **Comment**: CVPR 2023, 16 pages. See https://github.com/minghanz/E2PN for code
- **Journal**: None
- **Summary**: This paper proposes a convolution structure for learning SE(3)-equivariant features from 3D point clouds. It can be viewed as an equivariant version of kernel point convolutions (KPConv), a widely used convolution form to process point cloud data. Compared with existing equivariant networks, our design is simple, lightweight, fast, and easy to be integrated with existing task-specific point cloud learning pipelines. We achieve these desirable properties by combining group convolutions and quotient representations. Specifically, we discretize SO(3) to finite groups for their simplicity while using SO(2) as the stabilizer subgroup to form spherical quotient feature fields to save computations. We also propose a permutation layer to recover SO(3) features from spherical features to preserve the capacity to distinguish rotations. Experiments show that our method achieves comparable or superior performance in various tasks, including object classification, pose estimation, and keypoint-matching, while consuming much less memory and running faster than existing work. The proposed method can foster the development of equivariant models for real-world applications based on point clouds.



### High-Definition Map Generation Technologies For Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2206.05400v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05400v2)
- **Published**: 2022-06-11 02:32:11+00:00
- **Updated**: 2022-06-24 15:23:05+00:00
- **Authors**: Zhibin Bao, Sabir Hossain, Haoxiang Lang, Xianke Lin
- **Comment**: 25 pages, 17 figures, submitted to a journal
- **Journal**: None
- **Summary**: Autonomous driving has been among the most popular and challenging topics in the past few years. On the road to achieving full autonomy, researchers have utilized various sensors, such as LiDAR, camera, Inertial Measurement Unit (IMU), and GPS, and developed intelligent algorithms for autonomous driving applications such as object detection, object segmentation, obstacle avoidance, and path planning. High-definition (HD) maps have drawn lots of attention in recent years. Because of the high precision and informative level of HD maps in localization, it has immediately become one of the critical components of autonomous driving. From big organizations like Baidu Apollo, NVIDIA, and TomTom to individual researchers, researchers have created HD maps for different scenes and purposes for autonomous driving. It is necessary to review the state-of-the-art methods for HD map generation. This paper reviews recent HD map generation technologies that leverage both 2D and 3D map generation. This review introduces the concept of HD maps and their usefulness in autonomous driving and gives a detailed overview of HD map generation techniques. We will also discuss the limitations of the current HD map generation technologies to motivate future research.



### VAC2: Visual Analysis of Combined Causality in Event Sequences
- **Arxiv ID**: http://arxiv.org/abs/2206.05420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05420v1)
- **Published**: 2022-06-11 04:53:23+00:00
- **Updated**: 2022-06-11 04:53:23+00:00
- **Authors**: Sujia Zhu, Yue Shen, Zihao Zhu, Wang Xia, Baofeng Chang, Ronghua Liang, Guodao Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Identifying causality behind complex systems plays a significant role in different domains, such as decision making, policy implementations, and management recommendations. However, existing causality studies on temporal event sequences data mainly focus on individual causal discovery, which is incapable of exploiting combined causality. To fill the absence of combined causes discovery on temporal event sequence data,eliminating and recruiting principles are defined to balance the effectiveness and controllability on cause combinations. We also leverage the Granger causality algorithm based on the reactive point processes to describe impelling or inhibiting behavior patterns among entities. In addition, we design an informative and aesthetic visual metaphor of "electrocircuit" to encode aggregated causality for ensuring our causality visualization is non-overlapping and non-intersecting. Diverse sorting strategies and aggregation layout are also embedded into our parallel-based, directed and weighted hypergraph for illustrating combined causality. Our developed combined causality visual analysis system can help users effectively explore combined causes as well as an individual cause. This interactive system supports multi-level causality exploration with diverse ordering strategies and a focus and context technique to help users obtain different levels of information abstraction. The usefulness and effectiveness of the system are further evaluated by conducting a pilot user study and two case studies on event sequence data.



### Access Control of Semantic Segmentation Models Using Encrypted Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/2206.05422v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05422v1)
- **Published**: 2022-06-11 05:02:01+00:00
- **Updated**: 2022-06-11 05:02:01+00:00
- **Authors**: Hiroki Ito, AprilPyone MaungMaung, Sayaka Shiota, Hitoshi Kiya
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an access control method with a secret key for semantic segmentation models for the first time so that unauthorized users without a secret key cannot benefit from the performance of trained models. The method enables us not only to provide a high segmentation performance to authorized users but to also degrade the performance for unauthorized users. We first point out that, for the application of semantic segmentation, conventional access control methods which use encrypted images for classification tasks are not directly applicable due to performance degradation. Accordingly, in this paper, selected feature maps are encrypted with a secret key for training and testing models, instead of input images. In an experiment, the protected models allowed authorized users to obtain almost the same performance as that of non-protected models but also with robustness against unauthorized access without a key.



### Precise Affordance Annotation for Egocentric Action Video Datasets
- **Arxiv ID**: http://arxiv.org/abs/2206.05424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05424v1)
- **Published**: 2022-06-11 05:13:19+00:00
- **Updated**: 2022-06-11 05:13:19+00:00
- **Authors**: Zecheng Yu, Yifei Huang, Ryosuke Furuta, Takuma Yagi, Yusuke Goutsu, Yoichi Sato
- **Comment**: Technical report for CVPR 2022 EPIC-Ego4D Workshop
- **Journal**: None
- **Summary**: Object affordance is an important concept in human-object interaction, providing information on action possibilities based on human motor capacity and objects' physical property thus benefiting tasks such as action anticipation and robot imitation learning. However, existing datasets often: 1) mix up affordance with object functionality; 2) confuse affordance with goal-related action; and 3) ignore human motor capacity. This paper proposes an efficient annotation scheme to address these issues by combining goal-irrelevant motor actions and grasp types as affordance labels and introducing the concept of mechanical action to represent the action possibilities between two objects. We provide new annotations by applying this scheme to the EPIC-KITCHENS dataset and test our annotation with tasks such as affordance recognition. We qualitatively verify that models trained with our annotation can distinguish affordance and mechanical actions.



### Learned reconstruction methods with convergence guarantees
- **Arxiv ID**: http://arxiv.org/abs/2206.05431v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05431v3)
- **Published**: 2022-06-11 06:08:25+00:00
- **Updated**: 2022-09-14 12:48:52+00:00
- **Authors**: Subhadip Mukherjee, Andreas Hauptmann, Ozan Öktem, Marcelo Pereyra, Carola-Bibiane Schönlieb
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep learning has achieved remarkable empirical success for image reconstruction. This has catalyzed an ongoing quest for precise characterization of correctness and reliability of data-driven methods in critical use-cases, for instance in medical imaging. Notwithstanding the excellent performance and efficacy of deep learning-based methods, concerns have been raised regarding their stability, or lack thereof, with serious practical implications. Significant advances have been made in recent years to unravel the inner workings of data-driven image recovery methods, challenging their widely perceived black-box nature. In this article, we will specify relevant notions of convergence for data-driven image reconstruction, which will form the basis of a survey of learned methods with mathematically rigorous reconstruction guarantees. An example that is highlighted is the role of ICNN, offering the possibility to combine the power of deep learning with classical convex regularization theory for devising methods that are provably convergent.   This survey article is aimed at both methodological researchers seeking to advance the frontiers of our understanding of data-driven image reconstruction methods as well as practitioners, by providing an accessible description of useful convergence concepts and by placing some of the existing empirical practices on a solid mathematical foundation.



### Luminance-Guided Chrominance Image Enhancement for HEVC Intra Coding
- **Arxiv ID**: http://arxiv.org/abs/2206.05432v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05432v1)
- **Published**: 2022-06-11 06:10:14+00:00
- **Updated**: 2022-06-11 06:10:14+00:00
- **Authors**: Hewei Liu, Renwei Yang, Shuyuan Zhu, Xing Wen, Bing Zeng
- **Comment**: ISCAS 2022
- **Journal**: None
- **Summary**: In this paper, we propose a luminance-guided chrominance image enhancement convolutional neural network for HEVC intra coding. Specifically, we firstly develop a gated recursive asymmetric-convolution block to restore each degraded chrominance image, which generates an intermediate output. Then, guided by the luminance image, the quality of this intermediate output is further improved, which finally produces the high-quality chrominance image. When our proposed method is adopted in the compression of color images with HEVC intra coding, it achieves 28.96% and 16.74% BD-rate gains over HEVC for the U and V images, respectively, which accordingly demonstrate its superiority.



### Differentiable Projection from Optical Coherence Tomography B-Scan without Retinal Layer Segmentation Supervision
- **Arxiv ID**: http://arxiv.org/abs/2206.05472v1
- **DOI**: 10.1109/ISBI52829.2022.9761656
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05472v1)
- **Published**: 2022-06-11 08:57:54+00:00
- **Updated**: 2022-06-11 08:57:54+00:00
- **Authors**: Dingyi Rong, Jiancheng Yang, Bingbing Ni, Bilian Ke
- **Comment**: ISBI2022
- **Journal**: None
- **Summary**: Projection map (PM) from optical coherence tomography (OCT) B-scan is an important tool to diagnose retinal diseases, which typically requires retinal layer segmentation. In this study, we present a novel end-to-end framework to predict PMs from B-scans. Instead of segmenting retinal layers explicitly, we represent them implicitly as predicted coordinates. By pixel interpolation on uniformly sampled coordinates between retinal layers, the corresponding PMs could be easily obtained with pooling. Notably, all the operators are differentiable; therefore, this Differentiable Projection Module (DPM) enables end-to-end training with the ground truth of PMs rather than retinal layer segmentation. Our framework produces high-quality PMs, significantly outperforming baselines, including a vanilla CNN without DPM and an optimization-based DPM without a deep prior. Furthermore, the proposed DPM, as a novel neural representation of areas/volumes between curves/surfaces, could be of independent interest for geometric deep learning.



### Kaggle Kinship Recognition Challenge: Introduction of Convolution-Free Model to boost conventional
- **Arxiv ID**: http://arxiv.org/abs/2206.05488v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.05488v1)
- **Published**: 2022-06-11 10:22:28+00:00
- **Updated**: 2022-06-11 10:22:28+00:00
- **Authors**: Mingchuan Tian, Guangway Teng, Yipeng Bao
- **Comment**: None
- **Journal**: None
- **Summary**: This work aims to explore a convolution-free base classifier that can be used to widen the variations of the conventional ensemble classifier. Specifically, we propose Vision Transformers as base classifiers to combine with CNNs for a unique ensemble solution in Kaggle kinship recognition. In this paper, we verify our proposed idea by implementing and optimizing variants of the Vision Transformer model on top of the existing CNN models. The combined models achieve better scores than conventional ensemble classifiers based solely on CNN variants. We demonstrate that highly optimized CNN ensembles publicly available on the Kaggle Discussion board can easily achieve a significant boost in ROC score by simply ensemble with variants of the Vision Transformer model due to low correlation.



### An Evaluation of OCR on Egocentric Data
- **Arxiv ID**: http://arxiv.org/abs/2206.05496v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05496v1)
- **Published**: 2022-06-11 10:37:20+00:00
- **Updated**: 2022-06-11 10:37:20+00:00
- **Authors**: Valentin Popescu, Dima Damen, Toby Perrett
- **Comment**: Extended Abstract, EPIC workshop at CVPR 22
- **Journal**: None
- **Summary**: In this paper, we evaluate state-of-the-art OCR methods on Egocentric data. We annotate text in EPIC-KITCHENS images, and demonstrate that existing OCR methods struggle with rotated text, which is frequently observed on objects being handled. We introduce a simple rotate-and-merge procedure which can be applied to pre-trained OCR models that halves the normalized edit distance error. This suggests that future OCR attempts should incorporate rotation into model design and training procedures.



### A Review of Causality for Learning Algorithms in Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2206.05498v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GL
- **Links**: [PDF](http://arxiv.org/pdf/2206.05498v2)
- **Published**: 2022-06-11 11:04:13+00:00
- **Updated**: 2022-11-26 10:25:19+00:00
- **Authors**: Athanasios Vlontzos, Daniel Rueckert, Bernhard Kainz
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA)
  https://www.melba-journal.org/papers/2022:028.html". ; Paper ID: 2022:028
- **Journal**: Machine.Learning.for.Biomedical.Imaging. 1 (2022)
- **Summary**: Medical image analysis is a vibrant research area that offers doctors and medical practitioners invaluable insight and the ability to accurately diagnose and monitor disease. Machine learning provides an additional boost for this area. However, machine learning for medical image analysis is particularly vulnerable to natural biases like domain shifts that affect algorithmic performance and robustness. In this paper we analyze machine learning for medical image analysis within the framework of Technology Readiness Levels and review how causal analysis methods can fill a gap when creating robust and adaptable medical image analysis algorithms. We review methods using causality in medical imaging AI/ML and find that causal analysis has the potential to mitigate critical problems for clinical translation but that uptake and clinical downstream research has been limited so far.



### Toward Real-world Single Image Deraining: A New Benchmark and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2206.05514v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05514v2)
- **Published**: 2022-06-11 12:26:59+00:00
- **Updated**: 2022-11-19 13:11:27+00:00
- **Authors**: Wei Li, Qiming Zhang, Jing Zhang, Zhen Huang, Xinmei Tian, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Single image deraining (SID) in real scenarios attracts increasing attention in recent years. Due to the difficulty in obtaining real-world rainy/clean image pairs, previous real datasets suffer from low-resolution images, homogeneous rain streaks, limited background variation, and even misalignment of image pairs, resulting in incomprehensive evaluation of SID methods. To address these issues, we establish a new high-quality dataset named RealRain-1k, consisting of $1,120$ high-resolution paired clean and rainy images with low- and high-density rain streaks, respectively. Images in RealRain-1k are automatically generated from a large number of real-world rainy video clips through a simple yet effective rain density-controllable filtering method, and have good properties of high image resolution, background diversity, rain streaks variety, and strict spatial alignment. RealRain-1k also provides abundant rain streak layers as a byproduct, enabling us to build a large-scale synthetic dataset named SynRain-13k by pasting the rain streak layers on abundant natural images. Based on them and existing datasets, we benchmark more than 10 representative SID methods on three tracks: (1) fully supervised learning on RealRain-1k, (2) domain generalization to real datasets, and (3) syn-to-real transfer learning. The experimental results (1) show the difference of representative methods in image restoration performance and model complexity, (2) validate the significance of the proposed datasets for model generalization, and (3) provide useful insights on the superiority of learning from diverse domains and shed lights on the future research on real-world SID. The datasets will be released at https://github.com/hiker-lw/RealRain-1k



### Deep Learning-Based MR Image Re-parameterization
- **Arxiv ID**: http://arxiv.org/abs/2206.05516v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05516v1)
- **Published**: 2022-06-11 12:39:37+00:00
- **Updated**: 2022-06-11 12:39:37+00:00
- **Authors**: Abhijeet Narang, Abhigyan Raj, Mihaela Pop, Mehran Ebrahimi
- **Comment**: to be published in The 8th International Conference on Biomedical
  Engineering & Sciences, Luxor (MGM), Las Vegas, USA, 25-28 July 2022
- **Journal**: None
- **Summary**: Magnetic resonance (MR) image re-parameterization refers to the process of generating via simulations of an MR image with a new set of MRI scanning parameters. Different parameter values generate distinct contrast between different tissues, helping identify pathologic tissue. Typically, more than one scan is required for diagnosis; however, acquiring repeated scans can be costly, time-consuming, and difficult for patients. Thus, using MR image re-parameterization to predict and estimate the contrast in these imaging scans can be an effective alternative. In this work, we propose a novel deep learning (DL) based convolutional model for MRI re-parameterization. Based on our preliminary results, DL-based techniques hold the potential to learn the non-linearities that govern the re-parameterization.



### A Two-stage Method for Non-extreme Value Salt-and-Pepper Noise Removal
- **Arxiv ID**: http://arxiv.org/abs/2206.05520v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05520v2)
- **Published**: 2022-06-11 13:04:22+00:00
- **Updated**: 2022-06-20 15:24:15+00:00
- **Authors**: Renwei Yang, YiKe Liu, Bing Zeng
- **Comment**: UESTC course project
- **Journal**: None
- **Summary**: There are several previous methods based on neural network can have great performance in denoising salt and pepper noise. However, those methods are based on a hypothesis that the value of salt and pepper noise is exactly 0 and 255. It is not true in the real world. The result of those methods deviate sharply when the value is different from 0 and 255. To overcome this weakness, our method aims at designing a convolutional neural network to detect the noise pixels in a wider range of value and then a filter is used to modify pixel value to 0, which is beneficial for further filtering. Additionally, another convolutional neural network is used to conduct the denoising and restoration work.



### A Simplified Un-Supervised Learning Based Approach for Ink Mismatch Detection in Handwritten Hyper-Spectral Document Images
- **Arxiv ID**: http://arxiv.org/abs/2206.05539v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05539v1)
- **Published**: 2022-06-11 14:38:25+00:00
- **Updated**: 2022-06-11 14:38:25+00:00
- **Authors**: Muhammad Farhan Humayun, Hassan Waseem Malik, Ahmed Ahsan Alvi
- **Comment**: None
- **Journal**: None
- **Summary**: Hyper-spectral imaging has become the latest trend in the field of optical imaging systems. Among various other applications, hyper-spectral imaging has been widely used for analysis of printed and handwritten documents. This paper proposes an efficient technique for estimating the number of different but visibly similar inks present in a Hyper spectral Document Image. Our approach is based on un-supervised learning and does not require any prior knowledge of the dataset. The algorithm was tested on the iVision HHID dataset and has achieved comparable results with the state of the algorithms present in the literature. This work can prove to be effective when employed during the early stages of forgery detection in Hyper-spectral Document Images.



### Surround-View Cameras based Holistic Visual Perception for Automated Driving
- **Arxiv ID**: http://arxiv.org/abs/2206.05542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05542v1)
- **Published**: 2022-06-11 14:51:30+00:00
- **Updated**: 2022-06-11 14:51:30+00:00
- **Authors**: Varun Ravi Kumar
- **Comment**: Doctoral thesis
- **Journal**: None
- **Summary**: The formation of eyes led to the big bang of evolution. The dynamics changed from a primitive organism waiting for the food to come into contact for eating food being sought after by visual sensors. The human eye is one of the most sophisticated developments of evolution, but it still has defects. Humans have evolved a biological perception algorithm capable of driving cars, operating machinery, piloting aircraft, and navigating ships over millions of years. Automating these capabilities for computers is critical for various applications, including self-driving cars, augmented reality, and architectural surveying. Near-field visual perception in the context of self-driving cars can perceive the environment in a range of $0-10$ meters and 360{\deg} coverage around the vehicle. It is a critical decision-making component in the development of safer automated driving. Recent advances in computer vision and deep learning, in conjunction with high-quality sensors such as cameras and LiDARs, have fueled mature visual perception solutions. Until now, far-field perception has been the primary focus. Another significant issue is the limited processing power available for developing real-time applications. Because of this bottleneck, there is frequently a trade-off between performance and run-time efficiency. We concentrate on the following issues in order to address them: 1) Developing near-field perception algorithms with high performance and low computational complexity for various visual perception tasks such as geometric and semantic tasks using convolutional neural networks. 2) Using Multi-Task Learning to overcome computational bottlenecks by sharing initial convolutional layers between tasks and developing optimization strategies that balance tasks.



### A Unified Continuous Learning Framework for Multi-modal Knowledge Discovery and Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2206.05555v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05555v1)
- **Published**: 2022-06-11 16:05:06+00:00
- **Updated**: 2022-06-11 16:05:06+00:00
- **Authors**: Zhihao Fan, Zhongyu Wei, Jingjing Chen, Siyuan Wang, Zejun Li, Jiarong Xu, Xuanjing Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modal pre-training and knowledge discovery are two important research topics in multi-modal machine learning. Nevertheless, none of existing works make attempts to link knowledge discovery with knowledge guided multi-modal pre-training. In this paper, we propose to unify them into a continuous learning framework for mutual improvement. Taking the open-domain uni-modal datasets of images and texts as input, we maintain a knowledge graph as the foundation to support these two tasks. For knowledge discovery, a pre-trained model is used to identify cross-modal links on the graph. For model pre-training, the knowledge graph is used as the external knowledge to guide the model updating. These two steps are iteratively performed in our framework for continuous learning. The experimental results on MS-COCO and Flickr30K with respect to both knowledge discovery and the pre-trained model validate the effectiveness of our framework.



### MammoDL: Mammographic Breast Density Estimation using Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.05575v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05575v3)
- **Published**: 2022-06-11 17:38:09+00:00
- **Updated**: 2022-12-20 19:54:26+00:00
- **Authors**: Ramya Muthukrishnan, Angelina Heyler, Keshava Katti, Sarthak Pati, Walter Mankowski, Aprupa Alahari, Michael Sanborn, Emily F. Conant, Christopher Scott, Stacey Winham, Celine Vachon, Pratik Chaudhari, Despina Kontos, Spyridon Bakas
- **Comment**: Breast Cancer Risk, Digital Mammography, Breast Density, Deep
  Learning, Machine Learning, Federated Learning, OpenFL
- **Journal**: None
- **Summary**: Assessing breast cancer risk from imaging remains a subjective process, in which radiologists employ simple computer aided detection (CAD) systems or qualitative visual assessment to estimate breast percent density (PD). Machine learning (ML) models have become the most promising way to quantify breast cancer risk for early, accurate, and equitable diagnoses, but training such models in medical research is often restricted to small, single-institution data. Since patient demographics and imaging characteristics may vary considerably across imaging sites, models trained on single-institution data tend not to generalize well. In response to this problem, MammoDL is proposed, an open-source software tool that leverages a U-Net architecture to accurately estimate breast PD and complexity from mammography. With the Open Federated Learning (OpenFL) library, this solution enables secure training on datasets across multiple institutions. MammoDL is a leaner, more flexible model than its predecessors, boasting improved generalization due to federation-enabled training on larger, more representative datasets.



### Machine learning approaches for COVID-19 detection from chest X-ray imaging: A Systematic Review
- **Arxiv ID**: http://arxiv.org/abs/2206.05615v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05615v1)
- **Published**: 2022-06-11 21:17:42+00:00
- **Updated**: 2022-06-11 21:17:42+00:00
- **Authors**: Harold Brayan Arteaga-Arteaga, Melissa delaPava, Alejandro Mora-Rubio, Mario Alejandro Bravo-Ortíz, Jesus Alejandro Alzate-Grisales, Daniel Arias-Garzón, Luis Humberto López-Murillo, Felipe Buitrago-Carmona, Juan Pablo Villa-Pulgarín, Esteban Mercado-Ruiz, Simon Orozco-Arias, M. Hassaballah, Maria de la Iglesia-Vaya, Oscar Cardona-Morales, Reinel Tabares-Soto
- **Comment**: None
- **Journal**: None
- **Summary**: There is a necessity to develop affordable, and reliable diagnostic tools, which allow containing the COVID-19 spreading. Machine Learning (ML) algorithms have been proposed to design support decision-making systems to assess chest X-ray images, which have proven to be useful to detect and evaluate disease progression. Many research articles are published around this subject, which makes it difficult to identify the best approaches for future work. This paper presents a systematic review of ML applied to COVID-19 detection using chest X-ray images, aiming to offer a baseline for researchers in terms of methods, architectures, databases, and current limitations.



### Federated Learning with Research Prototypes for Multi-Center MRI-based Detection of Prostate Cancer with Diverse Histopathology
- **Arxiv ID**: http://arxiv.org/abs/2206.05617v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2206.05617v1)
- **Published**: 2022-06-11 21:28:17+00:00
- **Updated**: 2022-06-11 21:28:17+00:00
- **Authors**: Abhejit Rajagopal, Ekaterina Redekop, Anil Kemisetti, Rushi Kulkarni, Steven Raman, Kirti Magudia, Corey W. Arnold, Peder E. Z. Larson
- **Comment**: under review
- **Journal**: None
- **Summary**: Early prostate cancer detection and staging from MRI are extremely challenging tasks for both radiologists and deep learning algorithms, but the potential to learn from large and diverse datasets remains a promising avenue to increase their generalization capability both within- and across clinics. To enable this for prototype-stage algorithms, where the majority of existing research remains, in this paper we introduce a flexible federated learning framework for cross-site training, validation, and evaluation of deep prostate cancer detection algorithms. Our approach utilizes an abstracted representation of the model architecture and data, which allows unpolished prototype deep learning models to be trained without modification using the NVFlare federated learning framework. Our results show increases in prostate cancer detection and classification accuracy using a specialized neural network model and diverse prostate biopsy data collected at two University of California research hospitals, demonstrating the efficacy of our approach in adapting to different datasets and improving MR-biomarker discovery. We open-source our FLtools system, which can be easily adapted to other deep learning projects for medical imaging.



### Synthetic PET via Domain Translation of 3D MRI
- **Arxiv ID**: http://arxiv.org/abs/2206.05618v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05618v1)
- **Published**: 2022-06-11 21:32:40+00:00
- **Updated**: 2022-06-11 21:32:40+00:00
- **Authors**: Abhejit Rajagopal, Yutaka Natsuaki, Kristen Wangerin, Mahdjoub Hamdi, Hongyu An, John J. Sunderland, Richard Laforest, Paul E. Kinahan, Peder E. Z. Larson, Thomas A. Hope
- **Comment**: under review
- **Journal**: None
- **Summary**: Historically, patient datasets have been used to develop and validate various reconstruction algorithms for PET/MRI and PET/CT. To enable such algorithm development, without the need for acquiring hundreds of patient exams, in this paper we demonstrate a deep learning technique to generate synthetic but realistic whole-body PET sinograms from abundantly-available whole-body MRI. Specifically, we use a dataset of 56 $^{18}$F-FDG-PET/MRI exams to train a 3D residual UNet to predict physiologic PET uptake from whole-body T1-weighted MRI. In training we implemented a balanced loss function to generate realistic uptake across a large dynamic range and computed losses along tomographic lines of response to mimic the PET acquisition. The predicted PET images are forward projected to produce synthetic PET time-of-flight (ToF) sinograms that can be used with vendor-provided PET reconstruction algorithms, including using CT-based attenuation correction (CTAC) and MR-based attenuation correction (MRAC). The resulting synthetic data recapitulates physiologic $^{18}$F-FDG uptake, e.g. high uptake localized to the brain and bladder, as well as uptake in liver, kidneys, heart and muscle. To simulate abnormalities with high uptake, we also insert synthetic lesions. We demonstrate that this synthetic PET data can be used interchangeably with real PET data for the PET quantification task of comparing CT and MR-based attenuation correction methods, achieving $\leq 7.6\%$ error in mean-SUV compared to using real data. These results together show that the proposed synthetic PET data pipeline can be reasonably used for development, evaluation, and validation of PET/MRI reconstruction methods.



### Deep Learning Models for Automated Classification of Dog Emotional States from Facial Expressions
- **Arxiv ID**: http://arxiv.org/abs/2206.05619v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05619v1)
- **Published**: 2022-06-11 21:37:38+00:00
- **Updated**: 2022-06-11 21:37:38+00:00
- **Authors**: Tali Boneh-Shitrit, Shir Amir, Annika Bremhorst, Daniel S. Mills, Stefanie Riemer, Dror Fried, Anna Zamansky
- **Comment**: None
- **Journal**: None
- **Summary**: Similarly to humans, facial expressions in animals are closely linked with emotional states. However, in contrast to the human domain, automated recognition of emotional states from facial expressions in animals is underexplored, mainly due to difficulties in data collection and establishment of ground truth concerning emotional states of non-verbal users. We apply recent deep learning techniques to classify (positive) anticipation and (negative) frustration of dogs on a dataset collected in a controlled experimental setting. We explore the suitability of different backbones (e.g. ResNet, ViT) under different supervisions to this task, and find that features of a self-supervised pretrained ViT (DINO-ViT) are superior to the other alternatives. To the best of our knowledge, this work is the first to address the task of automatic classification of canine emotions on data acquired in a controlled experiment.



### Exploring the Intersection between Neural Architecture Search and Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.05625v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.NE, 68T07, I.2.2; D.1.2; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2206.05625v2)
- **Published**: 2022-06-11 22:26:53+00:00
- **Updated**: 2023-06-15 17:04:02+00:00
- **Authors**: Mohamed Shahawy, Elhadj Benkhelifa, David White
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the significant advances achieved in Artificial Neural Networks (ANNs), their design process remains notoriously tedious, depending primarily on intuition, experience and trial-and-error. This human-dependent process is often time-consuming and prone to errors. Furthermore, the models are generally bound to their training contexts, with no considerations to their surrounding environments. Continual adaptiveness and automation of neural networks is of paramount importance to several domains where model accessibility is limited after deployment (e.g IoT devices, self-driving vehicles, etc.). Additionally, even accessible models require frequent maintenance post-deployment to overcome issues such as Concept/Data Drift, which can be cumbersome and restrictive. By leveraging and combining approaches from Neural Architecture Search (NAS) and Continual Learning (CL), more robust and adaptive agents can be developed. This study conducts the first extensive review on the intersection between NAS and CL, formalizing the prospective Continually-Adaptive Neural Networks (CANNs) paradigm and outlining research directions for lifelong autonomous ANNs.



