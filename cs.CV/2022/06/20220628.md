# Arxiv Papers in cs.CV on 2022-06-28
### How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels
- **Arxiv ID**: http://arxiv.org/abs/2206.13673v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.13673v3)
- **Published**: 2022-06-28 00:24:12+00:00
- **Updated**: 2022-10-14 01:02:15+00:00
- **Authors**: Tobias Fischer, Michael Milford
- **Comment**: 8 pages
- **Journal**: IEEE Robotics and Automation Letters 2022
- **Summary**: Event cameras continue to attract interest due to desirable characteristics such as high dynamic range, low latency, virtually no motion blur, and high energy efficiency. One of the potential applications that would benefit from these characteristics lies in visual place recognition for robot localization, i.e. matching a query observation to the corresponding reference place in the database. In this letter, we explore the distinctiveness of event streams from a small subset of pixels (in the tens or hundreds). We demonstrate that the absolute difference in the number of events at those pixel locations accumulated into event frames can be sufficient for the place recognition task, when pixels that display large variations in the reference set are used. Using such sparse (over image coordinates) but varying (variance over the number of events per pixel location) pixels enables frequent and computationally cheap updates of the location estimates. Furthermore, when event frames contain a constant number of events, our method takes full advantage of the event-driven nature of the sensory stream and displays promising robustness to changes in velocity. We evaluate our proposed approach on the Brisbane-Event-VPR dataset in an outdoor driving scenario, as well as the newly contributed indoor QCR-Event-VPR dataset that was captured with a DAVIS346 camera mounted on a mobile robotic platform. Our results show that our approach achieves competitive performance when compared to several baseline methods on those datasets, and is particularly well suited for compute- and energy-constrained platforms such as interplanetary rovers.



### Towards Global-Scale Crowd+AI Techniques to Map and Assess Sidewalks for People with Disabilities
- **Arxiv ID**: http://arxiv.org/abs/2206.13677v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2206.13677v2)
- **Published**: 2022-06-28 01:05:08+00:00
- **Updated**: 2022-08-19 02:35:34+00:00
- **Authors**: Maryam Hosseini, Mikey Saugstad, Fabio Miranda, Andres Sevtsuk, Claudio T. Silva, Jon E. Froehlich
- **Comment**: CVPR 2022 AVA (Accessibility, Vision, and Autonomy Meet) Workshop
- **Journal**: None
- **Summary**: There is a lack of data on the location, condition, and accessibility of sidewalks across the world, which not only impacts where and how people travel but also fundamentally limits interactive mapping tools and urban analytics. In this paper, we describe initial work in semi-automatically building a sidewalk network topology from satellite imagery using hierarchical multi-scale attention models, inferring surface materials from street-level images using active learning-based semantic segmentation, and assessing sidewalk condition and accessibility features using Crowd+AI. We close with a call to create a database of labeled satellite and streetscape scenes for sidewalks and sidewalk accessibility issues along with standardized benchmarks.



### POEM: Out-of-Distribution Detection with Posterior Sampling
- **Arxiv ID**: http://arxiv.org/abs/2206.13687v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13687v1)
- **Published**: 2022-06-28 01:35:43+00:00
- **Updated**: 2022-06-28 01:35:43+00:00
- **Authors**: Yifei Ming, Ying Fan, Yixuan Li
- **Comment**: ICML 2022 (Long Talk); First two authors contributed equally
- **Journal**: Thirty-ninth International Conference on Machine Learning (2022)
- **Summary**: Out-of-distribution (OOD) detection is indispensable for machine learning models deployed in the open world. Recently, the use of an auxiliary outlier dataset during training (also known as outlier exposure) has shown promising performance. As the sample space for potential OOD data can be prohibitively large, sampling informative outliers is essential. In this work, we propose a novel posterior sampling-based outlier mining framework, POEM, which facilitates efficient use of outlier data and promotes learning a compact decision boundary between ID and OOD data for improved detection. We show that POEM establishes state-of-the-art performance on common benchmarks. Compared to the current best method that uses a greedy sampling strategy, POEM improves the relative performance by 42.0% and 24.2% (FPR95) on CIFAR-10 and CIFAR-100, respectively. We further provide theoretical insights on the effectiveness of POEM for OOD detection.



### The Third Place Solution for CVPR2022 AVA Accessibility Vision and Autonomy Challenge
- **Arxiv ID**: http://arxiv.org/abs/2206.13718v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.13718v1)
- **Published**: 2022-06-28 03:05:37+00:00
- **Updated**: 2022-06-28 03:05:37+00:00
- **Authors**: Bo Yan, Leilei Cao, Zhuang Li, Hongbin Wang
- **Comment**: The third place solution for CVPR2022 AVA Accessibility Vision and
  Autonomy Challenge
- **Journal**: None
- **Summary**: The goal of AVA challenge is to provide vision-based benchmarks and methods relevant to accessibility. In this paper, we introduce the technical details of our submission to the CVPR2022 AVA Challenge. Firstly, we conducted some experiments to help employ proper model and data augmentation strategy for this task. Secondly, an effective training strategy was applied to improve the performance. Thirdly, we integrated the results from two different segmentation frameworks to improve the performance further. Experimental results demonstrate that our approach can achieve a competitive result on the AVA test set. Finally, our approach achieves 63.008\%AP@0.50:0.95 on the test set of CVPR2022 AVA Challenge.



### Boosting R-CNN: Reweighting R-CNN Samples by RPN's Error for Underwater Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.13728v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13728v3)
- **Published**: 2022-06-28 03:29:20+00:00
- **Updated**: 2022-10-05 02:51:29+00:00
- **Authors**: Pinhao Song, Pengteng Li, Linhui Dai, Tao Wang, Zhan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Complicated underwater environments bring new challenges to object detection, such as unbalanced light conditions, low contrast, occlusion, and mimicry of aquatic organisms. Under these circumstances, the objects captured by the underwater camera will become vague, and the generic detectors often fail on these vague objects. This work aims to solve the problem from two perspectives: uncertainty modeling and hard example mining. We propose a two-stage underwater detector named boosting R-CNN, which comprises three key components. First, a new region proposal network named RetinaRPN is proposed, which provides high-quality proposals and considers objectness and IoU prediction for uncertainty to model the object prior probability. Second, the probabilistic inference pipeline is introduced to combine the first-stage prior uncertainty and the second-stage classification score to model the final detection score. Finally, we propose a new hard example mining method named boosting reweighting. Specifically, when the region proposal network miscalculates the object prior probability for a sample, boosting reweighting will increase the classification loss of the sample in the R-CNN head during training, while reducing the loss of easy samples with accurately estimated priors. Thus, a robust detection head in the second stage can be obtained. During the inference stage, the R-CNN has the capability to rectify the error of the first stage to improve the performance. Comprehensive experiments on two underwater datasets and two generic object detection datasets demonstrate the effectiveness and robustness of our method.



### A Comprehensive Survey on Deep Gait Recognition: Algorithms, Datasets and Challenges
- **Arxiv ID**: http://arxiv.org/abs/2206.13732v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13732v2)
- **Published**: 2022-06-28 03:36:12+00:00
- **Updated**: 2023-08-05 06:03:55+00:00
- **Authors**: Chuanfu Shen, Shiqi Yu, Jilong Wang, George Q. Huang, Liang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Gait recognition aims to identify a person at a distance, serving as a promising solution for long-distance and less-cooperation pedestrian recognition. Recently, significant advancements in gait recognition have achieved inspiring success in many challenging scenarios by utilizing deep learning techniques. Against the backdrop that deep gait recognition has achieved almost perfect performance in laboratory datasets, much recent research has introduced new challenges for gait recognition, including robust deep representation modeling, in-the-wild gait recognition, and even recognition from new visual sensors such as infrared and depth cameras. Meanwhile, the increasing performance of gait recognition might also reveal concerns about biometrics security and privacy prevention for society. We provide a comprehensive survey on recent literature using deep learning and a discussion on the privacy and security of gait biometrics. This survey reviews the existing deep gait recognition methods through a novel view based on our proposed taxonomy. The proposed taxonomy differs from the conventional taxonomy of categorizing available gait recognition methods into the model- or appearance-based methods, while our taxonomic hierarchy considers deep gait recognition from two perspectives: deep representation learning and deep network architectures, illustrating the current approaches from both micro and macro levels. We also include up-to-date reviews of datasets and performance evaluations on diverse scenarios. Finally, we introduce privacy and security concerns on gait biometrics and discuss outstanding challenges and potential directions for future research.



### Adversarial Consistency for Single Domain Generalization in Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.13737v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13737v2)
- **Published**: 2022-06-28 03:47:57+00:00
- **Updated**: 2022-06-29 20:23:27+00:00
- **Authors**: Yanwu Xu, Shaoan Xie, Maxwell Reynolds, Matthew Ragoza, Mingming Gong, Kayhan Batmanghelich
- **Comment**: MICCAI2022 accpted
- **Journal**: None
- **Summary**: An organ segmentation method that can generalize to unseen contrasts and scanner settings can significantly reduce the need for retraining of deep learning models. Domain Generalization (DG) aims to achieve this goal. However, most DG methods for segmentation require training data from multiple domains during training. We propose a novel adversarial domain generalization method for organ segmentation trained on data from a \emph{single} domain. We synthesize the new domains via learning an adversarial domain synthesizer (ADS) and presume that the synthetic domains cover a large enough area of plausible distributions so that unseen domains can be interpolated from synthetic domains. We propose a mutual information regularizer to enforce the semantic consistency between images from the synthetic domains, which can be estimated by patch-level contrastive learning. We evaluate our method for various organ segmentation for unseen modalities, scanning protocols, and scanner sites.



### GAN-based Super-Resolution and Segmentation of Retinal Layers in Optical coherence tomography Scans
- **Arxiv ID**: http://arxiv.org/abs/2206.13740v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13740v1)
- **Published**: 2022-06-28 03:53:40+00:00
- **Updated**: 2022-06-28 03:53:40+00:00
- **Authors**: Paria Jeihouni, Omid Dehzangi, Annahita Amireskandari, Ali Rezai, Nasser M. Nasrabadi
- **Comment**: 5 pages,7 figures
- **Journal**: None
- **Summary**: In this paper, we design a Generative Adversarial Network (GAN)-based solution for super-resolution and segmentation of optical coherence tomography (OCT) scans of the retinal layers. OCT has been identified as a non-invasive and inexpensive modality of imaging to discover potential biomarkers for the diagnosis and progress determination of neurodegenerative diseases, such as Alzheimer's Disease (AD). Current hypotheses presume the thickness of the retinal layers, which are analyzable within OCT scans, can be effective biomarkers. As a logical first step, this work concentrates on the challenging task of retinal layer segmentation and also super-resolution for higher clarity and accuracy. We propose a GAN-based segmentation model and evaluate incorporating popular networks, namely, U-Net and ResNet, in the GAN architecture with additional blocks of transposed convolution and sub-pixel convolution for the task of upscaling OCT images from low to high resolution by a factor of four. We also incorporate the Dice loss as an additional reconstruction loss term to improve the performance of this joint optimization task. Our best model configuration empirically achieved the Dice coefficient of 0.867 and mIOU of 0.765.



### 3D Multi-Object Tracking with Differentiable Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2206.13785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13785v1)
- **Published**: 2022-06-28 06:46:32+00:00
- **Updated**: 2022-06-28 06:46:32+00:00
- **Authors**: Dominik Schmauser, Zeju Qiu, Norman Müller, Matthias Nießner
- **Comment**: Project page: https://domischmauser.github.io/3D_MOT/
- **Journal**: None
- **Summary**: We propose a novel approach for joint 3D multi-object tracking and reconstruction from RGB-D sequences in indoor environments. To this end, we detect and reconstruct objects in each frame while predicting dense correspondences mappings into a normalized object space. We leverage those correspondences to inform a graph neural network to solve for the optimal, temporally-consistent 7-DoF pose trajectories of all objects. The novelty of our method is two-fold: first, we propose a new graph-based approach for differentiable pose estimation over time to learn optimal pose trajectories; second, we present a joint formulation of reconstruction and pose estimation along the time axis for robust and geometrically consistent multi-object tracking. In order to validate our approach, we introduce a new synthetic dataset comprising 2381 unique indoor sequences with a total of 60k rendered RGB-D images for multi-object tracking with moving objects and camera positions derived from the synthetic 3D-FRONT dataset. We demonstrate that our method improves the accumulated MOTA score for all test sequences by 24.8% over existing state-of-the-art methods. In several ablations on synthetic and real-world sequences, we show that our graph-based, fully end-to-end-learnable approach yields a significant boost in tracking performance.



### FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.13803v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.13803v3)
- **Published**: 2022-06-28 07:37:38+00:00
- **Updated**: 2023-07-26 01:46:05+00:00
- **Authors**: Nannan Wu, Li Yu, Xin Yang, Kwang-Ting Cheng, Zengqiang Yan
- **Comment**: This paper has been accepted by MICCAI 2023
- **Journal**: None
- **Summary**: Federated learning (FL), training deep models from decentralized data without privacy leakage, has shown great potential in medical image computing recently. However, considering the ubiquitous class imbalance in medical data, FL can exhibit performance degradation, especially for minority classes (e.g. rare diseases). Existing methods towards this problem mainly focus on training a balanced classifier to eliminate class prior bias among classes, but neglect to explore better representation to facilitate classification performance. In this paper, we present a privacy-preserving FL method named FedIIC to combat class imbalance from two perspectives: feature learning and classifier learning. In feature learning, two levels of contrastive learning are designed to extract better class-specific features with imbalanced data in FL. In classifier learning, per-class margins are dynamically set according to real-time difficulty and class priors, which helps the model learn classes equally. Experimental results on publicly-available datasets demonstrate the superior performance of FedIIC in dealing with both real-world and simulated multi-source medical imaging data under class imbalance. Code is available at https://github.com/wnn2000/FedIIC.



### Cross-Forgery Analysis of Vision Transformers and CNNs for Deepfake Image Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.13829v1
- **DOI**: 10.1145/3512732.3533582
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13829v1)
- **Published**: 2022-06-28 08:50:22+00:00
- **Updated**: 2022-06-28 08:50:22+00:00
- **Authors**: Davide Alessandro Coccomini, Roberto Caldelli, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato
- **Comment**: None
- **Journal**: None
- **Summary**: Deepfake Generation Techniques are evolving at a rapid pace, making it possible to create realistic manipulated images and videos and endangering the serenity of modern society. The continual emergence of new and varied techniques brings with it a further problem to be faced, namely the ability of deepfake detection models to update themselves promptly in order to be able to identify manipulations carried out using even the most recent methods. This is an extremely complex problem to solve, as training a model requires large amounts of data, which are difficult to obtain if the deepfake generation method is too recent. Moreover, continuously retraining a network would be unfeasible. In this paper, we ask ourselves if, among the various deep learning techniques, there is one that is able to generalise the concept of deepfake to such an extent that it does not remain tied to one or more specific deepfake generation methods used in the training set. We compared a Vision Transformer with an EfficientNetV2 on a cross-forgery context based on the ForgeryNet dataset. From our experiments, It emerges that EfficientNetV2 has a greater tendency to specialize often obtaining better results on training methods while Vision Transformers exhibit a superior generalization ability that makes them more competent even on images generated with new methodologies.



### When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2206.13850v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2206.13850v1)
- **Published**: 2022-06-28 09:29:55+00:00
- **Updated**: 2022-06-28 09:29:55+00:00
- **Authors**: Madhu Vankadari, Stuart Golodetz, Sourav Garg, Sangyun Shin, Andrew Markham, Niki Trigoni
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised deep learning methods for joint depth and ego-motion estimation can yield accurate trajectories without needing ground-truth training data. However, as they typically use photometric losses, their performance can degrade significantly when the assumptions these losses make (e.g. temporal illumination consistency, a static scene, and the absence of noise and occlusions) are violated. This limits their use for e.g. nighttime sequences, which tend to contain many point light sources (including on dynamic objects) and low signal-to-noise ratio (SNR) in darker image regions. In this paper, we show how to use a combination of three techniques to allow the existing photometric losses to work for both day and nighttime images. First, we introduce a per-pixel neural intensity transformation to compensate for the light changes that occur between successive frames. Second, we predict a per-pixel residual flow map that we use to correct the reprojection correspondences induced by the estimated ego-motion and depth from the networks. And third, we denoise the training images to improve the robustness and accuracy of our approach. These changes allow us to train a single model for both day and nighttime images without needing separate encoders or extra feature networks like existing methods. We perform extensive experiments and ablation studies on the challenging Oxford RobotCar dataset to demonstrate the efficacy of our approach for both day and nighttime sequences.



### Accurate and Real-time Pseudo Lidar Detection: Is Stereo Neural Network Really Necessary?
- **Arxiv ID**: http://arxiv.org/abs/2206.13858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13858v1)
- **Published**: 2022-06-28 09:53:00+00:00
- **Updated**: 2022-06-28 09:53:00+00:00
- **Authors**: Haitao Meng, Changcai Li, Gang Chen, Alois Knoll
- **Comment**: None
- **Journal**: None
- **Summary**: The proposal of Pseudo-Lidar representation has significantly narrowed the gap between visual-based and active Lidar-based 3D object detection. However, current researches exclusively focus on pushing the accuracy improvement of Pseudo-Lidar by taking the advantage of complex and time-consuming neural networks. Seldom explore the profound characteristics of Pseudo-Lidar representation to obtain the promoting opportunities. In this paper, we dive deep into the pseudo Lidar representation and argue that the performance of 3D object detection is not fully dependent on the high precision stereo depth estimation. We demonstrate that even for the unreliable depth estimation, with proper data processing and refining, it can achieve comparable 3D object detection accuracy. With this finding, we further show the possibility that utilizing fast but inaccurate stereo matching algorithms in the Pseudo-Lidar system to achieve low latency responsiveness. In the experiments, we develop a system with a less powerful stereo matching predictor and adopt the proposed refinement schemes to improve the accuracy. The evaluation on the KITTI benchmark shows that the presented system achieves competitive accuracy to the state-of-the-art approaches with only 23 ms computing, showing it is a suitable candidate for deploying to real car-hold applications.



### When are Post-hoc Conceptual Explanations Identifiable?
- **Arxiv ID**: http://arxiv.org/abs/2206.13872v5
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13872v5)
- **Published**: 2022-06-28 10:21:17+00:00
- **Updated**: 2023-06-06 07:01:53+00:00
- **Authors**: Tobias Leemann, Michael Kirchhof, Yao Rong, Enkelejda Kasneci, Gjergji Kasneci
- **Comment**: v5: UAI2023 camera-ready including supplementary material. The first
  two authors contributed equally
- **Journal**: None
- **Summary**: Interest in understanding and factorizing learned embedding spaces through conceptual explanations is steadily growing. When no human concept labels are available, concept discovery methods search trained embedding spaces for interpretable concepts like object shape or color that can provide post-hoc explanations for decisions. Unlike previous work, we argue that concept discovery should be identifiable, meaning that a number of known concepts can be provably recovered to guarantee reliability of the explanations. As a starting point, we explicitly make the connection between concept discovery and classical methods like Principal Component Analysis and Independent Component Analysis by showing that they can recover independent concepts under non-Gaussian distributions. For dependent concepts, we propose two novel approaches that exploit functional compositionality properties of image-generating processes. Our provably identifiable concept discovery methods substantially outperform competitors on a battery of experiments including hundreds of trained models and dependent concepts, where they exhibit up to 29 % better alignment with the ground truth. Our results highlight the strict conditions under which reliable concept discovery without human labels can be guaranteed and provide a formal foundation for the domain. Our code is available online.



### Improving Worst Case Visual Localization Coverage via Place-specific Sub-selection in Multi-camera Systems
- **Arxiv ID**: http://arxiv.org/abs/2206.13883v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13883v1)
- **Published**: 2022-06-28 10:59:39+00:00
- **Updated**: 2022-06-28 10:59:39+00:00
- **Authors**: Stephen Hausler, Ming Xu, Sourav Garg, Punarjay Chakravarty, Shubham Shrivastava, Ankit Vora, Michael Milford
- **Comment**: 8 pages, 5 figures, To be published in RA-L 2022
- **Journal**: None
- **Summary**: 6-DoF visual localization systems utilize principled approaches rooted in 3D geometry to perform accurate camera pose estimation of images to a map. Current techniques use hierarchical pipelines and learned 2D feature extractors to improve scalability and increase performance. However, despite gains in typical recall@0.25m type metrics, these systems still have limited utility for real-world applications like autonomous vehicles because of their `worst' areas of performance - the locations where they provide insufficient recall at a certain required error tolerance. Here we investigate the utility of using `place specific configurations', where a map is segmented into a number of places, each with its own configuration for modulating the pose estimation step, in this case selecting a camera within a multi-camera system. On the Ford AV benchmark dataset, we demonstrate substantially improved worst-case localization performance compared to using off-the-shelf pipelines - minimizing the percentage of the dataset which has low recall at a certain error tolerance, as well as improved overall localization performance. Our proposed approach is particularly applicable to the crowdsharing model of autonomous vehicle deployment, where a fleet of AVs are regularly traversing a known route.



### Generating near-infrared facial expression datasets with dimensional affect labels
- **Arxiv ID**: http://arxiv.org/abs/2206.13887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13887v1)
- **Published**: 2022-06-28 11:06:32+00:00
- **Updated**: 2022-06-28 11:06:32+00:00
- **Authors**: Calvin Chen, Stefan Winkler
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expression analysis has long been an active research area of computer vision. Traditional methods mainly analyse images for prototypical discrete emotions; as a result, they do not provide an accurate depiction of the complex emotional states in humans. Furthermore, illumination variance remains a challenge for face analysis in the visible light spectrum. To address these issues, we propose using a dimensional model based on valence and arousal to represent a wider range of emotions, in combination with near infra-red (NIR) imagery, which is more robust to illumination changes. Since there are no existing NIR facial expression datasets with valence-arousal labels available, we present two complementary data augmentation methods (face morphing and CycleGAN approach) to create NIR image datasets with dimensional emotion labels from existing categorical and/or visible-light datasets. Our experiments show that these generated NIR datasets are comparable to existing datasets in terms of data quality and baseline prediction performance.



### AS-IntroVAE: Adversarial Similarity Distance Makes Robust IntroVAE
- **Arxiv ID**: http://arxiv.org/abs/2206.13903v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13903v3)
- **Published**: 2022-06-28 11:40:17+00:00
- **Updated**: 2022-10-31 10:46:30+00:00
- **Authors**: Changjie Lu, Shen Zheng, Zirui Wang, Omar Dib, Gaurav Gupta
- **Comment**: ACML conference paper
- **Journal**: None
- **Summary**: Recently, introspective models like IntroVAE and S-IntroVAE have excelled in image generation and reconstruction tasks. The principal characteristic of introspective models is the adversarial learning of VAE, where the encoder attempts to distinguish between the real and the fake (i.e., synthesized) images. However, due to the unavailability of an effective metric to evaluate the difference between the real and the fake images, the posterior collapse and the vanishing gradient problem still exist, reducing the fidelity of the synthesized images. In this paper, we propose a new variation of IntroVAE called Adversarial Similarity Distance Introspective Variational Autoencoder (AS-IntroVAE). We theoretically analyze the vanishing gradient problem and construct a new Adversarial Similarity Distance (AS-Distance) using the 2-Wasserstein distance and the kernel trick. With weight annealing on AS-Distance and KL-Divergence, the AS-IntroVAE are able to generate stable and high-quality images. The posterior collapse problem is addressed by making per-batch attempts to transform the image so that it better fits the prior distribution in the latent space. Compared with the per-image approach, this strategy fosters more diverse distributions in the latent space, allowing our model to produce images of great diversity. Comprehensive experiments on benchmark datasets demonstrate the effectiveness of AS-IntroVAE on image generation and reconstruction tasks.



### Robustifying Vision Transformer without Retraining from Scratch by Test-Time Class-Conditional Feature Alignment
- **Arxiv ID**: http://arxiv.org/abs/2206.13951v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13951v1)
- **Published**: 2022-06-28 12:14:15+00:00
- **Updated**: 2022-06-28 12:14:15+00:00
- **Authors**: Takeshi Kojima, Yutaka Matsuo, Yusuke Iwasawa
- **Comment**: Accepted to IJCAI-ECAI2022. Code is available at
  https://github.com/kojima-takeshi188/CFA
- **Journal**: None
- **Summary**: Vision Transformer (ViT) is becoming more popular in image processing. Specifically, we investigate the effectiveness of test-time adaptation (TTA) on ViT, a technique that has emerged to correct its prediction during test-time by itself. First, we benchmark various test-time adaptation approaches on ViT-B16 and ViT-L16. It is shown that the TTA is effective on ViT and the prior-convention (sensibly selecting modulation parameters) is not necessary when using proper loss function. Based on the observation, we propose a new test-time adaptation method called class-conditional feature alignment (CFA), which minimizes both the class-conditional distribution differences and the whole distribution differences of the hidden representation between the source and target in an online manner. Experiments of image classification tasks on common corruption (CIFAR-10-C, CIFAR-100-C, and ImageNet-C) and domain adaptation (digits datasets and ImageNet-Sketch) show that CFA stably outperforms the existing baselines on various datasets. We also verify that CFA is model agnostic by experimenting on ResNet, MLP-Mixer, and several ViT variants (ViT-AugReg, DeiT, and BeiT). Using BeiT backbone, CFA achieves 19.8% top-1 error rate on ImageNet-C, outperforming the existing test-time adaptation baseline 44.0%. This is a state-of-the-art result among TTA methods that do not need to alter training phase.



### Multi-Prior Learning via Neural Architecture Search for Blind Face Restoration
- **Arxiv ID**: http://arxiv.org/abs/2206.13962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13962v1)
- **Published**: 2022-06-28 12:29:53+00:00
- **Updated**: 2022-06-28 12:29:53+00:00
- **Authors**: Yanjiang Yu, Puyang Zhang, Kaihao Zhang, Wenhan Luo, Changsheng Li, Ye Yuan, Guoren Wang
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: Blind Face Restoration (BFR) aims to recover high-quality face images from low-quality ones and usually resorts to facial priors for improving restoration performance. However, current methods still suffer from two major difficulties: 1) how to derive a powerful network architecture without extensive hand tuning; 2) how to capture complementary information from multiple facial priors in one network to improve restoration performance. To this end, we propose a Face Restoration Searching Network (FRSNet) to adaptively search the suitable feature extraction architecture within our specified search space, which can directly contribute to the restoration quality. On the basis of FRSNet, we further design our Multiple Facial Prior Searching Network (MFPSNet) with a multi-prior learning scheme. MFPSNet optimally extracts information from diverse facial priors and fuses the information into image features, ensuring that both external guidance and internal features are reserved. In this way, MFPSNet takes full advantage of semantic-level (parsing maps), geometric-level (facial heatmaps), reference-level (facial dictionaries) and pixel-level (degraded images) information and thus generates faithful and realistic images. Quantitative and qualitative experiments show that MFPSNet performs favorably on both synthetic and real-world datasets against the state-of-the-art BFR methods. The codes are publicly available at: https://github.com/YYJ1anG/MFPSNet.



### Primitive Graph Learning for Unified Vector Mapping
- **Arxiv ID**: http://arxiv.org/abs/2206.13963v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13963v2)
- **Published**: 2022-06-28 12:33:18+00:00
- **Updated**: 2022-11-11 01:11:01+00:00
- **Authors**: Lei Wang, Min Dai, Jianan He, Jingwei Huang, Mingwei Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale vector mapping is important for transportation, city planning, and survey and census. We propose GraphMapper, a unified framework for end-to-end vector map extraction from satellite images. Our key idea is a novel unified representation of shapes of different topologies named "primitive graph", which is a set of shape primitives and their pairwise relationship matrix. Then, we convert vector shape prediction, regularization, and topology reconstruction into a unique primitive graph learning problem. Specifically, GraphMapper is a generic primitive graph learning network based on global shape context modelling through multi-head-attention. An embedding space sorting method is developed for accurate primitive relationship modelling. We empirically demonstrate the effectiveness of GraphMapper on two challenging mapping tasks, building footprint regularization and road network topology reconstruction. Our model outperforms state-of-the-art methods in both tasks on public benchmarks. All code will be publicly available.



### Learning Gait Representation from Massive Unlabelled Walking Videos: A Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2206.13964v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13964v1)
- **Published**: 2022-06-28 12:33:42+00:00
- **Updated**: 2022-06-28 12:33:42+00:00
- **Authors**: Chao Fan, Saihui Hou, Jilong Wang, Yongzhen Huang, Shiqi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Gait depicts individuals' unique and distinguishing walking patterns and has become one of the most promising biometric features for human identification. As a fine-grained recognition task, gait recognition is easily affected by many factors and usually requires a large amount of completely annotated data that is costly and insatiable. This paper proposes a large-scale self-supervised benchmark for gait recognition with contrastive learning, aiming to learn the general gait representation from massive unlabelled walking videos for practical applications via offering informative walking priors and diverse real-world variations. Specifically, we collect a large-scale unlabelled gait dataset GaitLU-1M consisting of 1.02M walking sequences and propose a conceptually simple yet empirically powerful baseline model GaitSSB. Experimentally, we evaluate the pre-trained model on four widely-used gait benchmarks, CASIA-B, OU-MVLP, GREW and Gait3D with or without transfer learning. The unsupervised results are comparable to or even better than the early model-based and GEI-based methods. After transfer learning, our method outperforms existing methods by a large margin in most cases. Theoretically, we discuss the critical issues for gait-specific contrastive framework and present some insights for further study. As far as we know, GaitLU-1M is the first large-scale unlabelled gait dataset, and GaitSSB is the first method that achieves remarkable unsupervised results on the aforementioned benchmarks. The source code of GaitSSB will be integrated into OpenGait which is available at https://github.com/ShiqiYu/OpenGait.



### Information Entropy Initialized Concrete Autoencoder for Optimal Sensor Placement and Reconstruction of Geophysical Fields
- **Arxiv ID**: http://arxiv.org/abs/2206.13968v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.ao-ph, 68T07, 76U60, 86A05, I.4.5; I.4.2; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2206.13968v1)
- **Published**: 2022-06-28 12:43:38+00:00
- **Updated**: 2022-06-28 12:43:38+00:00
- **Authors**: Nikita Turko, Alexander Lobashev, Konstantin Ushakov, Maxim Kaurkin, Rashit Ibrayev
- **Comment**: 18 pages, 6 figures
- **Journal**: None
- **Summary**: We propose a new approach to the optimal placement of sensors for the problem of reconstructing geophysical fields from sparse measurements. Our method consists of two stages. In the first stage, we estimate the variability of the physical field as a function of spatial coordinates by approximating its information entropy through the Conditional PixelCNN network. To calculate the entropy, a new ordering of a two-dimensional data array (spiral ordering) is proposed, which makes it possible to obtain the entropy of a physical field simultaneously for several spatial scales. In the second stage, the entropy of the physical field is used to initialize the distribution of optimal sensor locations. This distribution is further optimized with the Concrete Autoencoder architecture with the straight-through gradient estimator and adversarial loss to simultaneously minimize the number of sensors and maximize reconstruction accuracy. Our method scales linearly with data size, unlike commonly used Principal Component Analysis. We demonstrate our method on the two examples: (a) temperature and (b) salinity fields around the Barents Sea and the Svalbard group of islands. For these examples, we compute the reconstruction error of our method and a few baselines. We test our approach against two baselines (1) PCA with QR factorization and (2) climatology. We find out that the obtained optimal sensor locations have clear physical interpretation and correspond to the boundaries between sea currents.



### Improving Disease Classification Performance and Explainability of Deep Learning Models in Radiology with Heatmap Generators
- **Arxiv ID**: http://arxiv.org/abs/2207.00157v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2207.00157v1)
- **Published**: 2022-06-28 13:03:50+00:00
- **Updated**: 2022-06-28 13:03:50+00:00
- **Authors**: Akino Watanabe, Sara Ketabi, Khashayar, Namdar, Farzad Khalvati
- **Comment**: None
- **Journal**: None
- **Summary**: As deep learning is widely used in the radiology field, the explainability of such models is increasingly becoming essential to gain clinicians' trust when using the models for diagnosis. In this research, three experiment sets were conducted with a U-Net architecture to improve the classification performance while enhancing the heatmaps corresponding to the model's focus through incorporating heatmap generators during training. All of the experiments used the dataset that contained chest radiographs, associated labels from one of the three conditions ("normal", "congestive heart failure (CHF)", and "pneumonia"), and numerical information regarding a radiologist's eye-gaze coordinates on the images. The paper (A. Karargyris and Moradi, 2021) that introduced this dataset developed a U-Net model, which was treated as the baseline model for this research, to show how the eye-gaze data can be used in multi-modal training for explainability improvement. To compare the classification performances, the 95% confidence intervals (CI) of the area under the receiver operating characteristic curve (AUC) were measured. The best method achieved an AUC of 0.913 (CI: 0.860-0.966). The greatest improvements were for the "pneumonia" and "CHF" classes, which the baseline model struggled most to classify, resulting in AUCs of 0.859 (CI: 0.732-0.957) and 0.962 (CI: 0.933-0.989), respectively. The proposed method's decoder was also able to produce probability masks that highlight the determining image parts in model classifications, similarly as the radiologist's eye-gaze data. Hence, this work showed that incorporating heatmap generators and eye-gaze information into training can simultaneously improve disease classification and provide explainable visuals that align well with how the radiologist viewed the chest radiographs when making diagnosis.



### Increasing Confidence in Adversarial Robustness Evaluations
- **Arxiv ID**: http://arxiv.org/abs/2206.13991v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13991v1)
- **Published**: 2022-06-28 13:28:13+00:00
- **Updated**: 2022-06-28 13:28:13+00:00
- **Authors**: Roland S. Zimmermann, Wieland Brendel, Florian Tramer, Nicholas Carlini
- **Comment**: Oral at CVPR 2022 Workshop (Art of Robustness). Project website
  https://zimmerrol.github.io/active-tests/
- **Journal**: None
- **Summary**: Hundreds of defenses have been proposed to make deep neural networks robust against minimal (adversarial) input perturbations. However, only a handful of these defenses held up their claims because correctly evaluating robustness is extremely challenging: Weak attacks often fail to find adversarial examples even if they unknowingly exist, thereby making a vulnerable network look robust. In this paper, we propose a test to identify weak attacks, and thus weak defense evaluations. Our test slightly modifies a neural network to guarantee the existence of an adversarial example for every sample. Consequentially, any correct attack must succeed in breaking this modified network. For eleven out of thirteen previously-published defenses, the original evaluation of the defense fails our test, while stronger attacks that break these defenses pass it. We hope that attack unit tests - such as ours - will be a major component in future robustness evaluations and increase confidence in an empirical field that is currently riddled with skepticism.



### Detecting tiny objects in aerial images: A normalized Wasserstein distance and a new benchmark
- **Arxiv ID**: http://arxiv.org/abs/2206.13996v1
- **DOI**: 10.1016/j.isprsjprs.2022.06.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13996v1)
- **Published**: 2022-06-28 13:33:06+00:00
- **Updated**: 2022-06-28 13:33:06+00:00
- **Authors**: Chang Xu, Jinwang Wang, Wen Yang, Huai Yu, Lei Yu, Gui-Song Xia
- **Comment**: Accepted by ISPRS Journal of Photogrammetry and Remote Sensing
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing (2022)
  190:79-93
- **Summary**: Tiny object detection (TOD) in aerial images is challenging since a tiny object only contains a few pixels. State-of-the-art object detectors do not provide satisfactory results on tiny objects due to the lack of supervision from discriminative features. Our key observation is that the Intersection over Union (IoU) metric and its extensions are very sensitive to the location deviation of the tiny objects, which drastically deteriorates the quality of label assignment when used in anchor-based detectors. To tackle this problem, we propose a new evaluation metric dubbed Normalized Wasserstein Distance (NWD) and a new RanKing-based Assigning (RKA) strategy for tiny object detection. The proposed NWD-RKA strategy can be easily embedded into all kinds of anchor-based detectors to replace the standard IoU threshold-based one, significantly improving label assignment and providing sufficient supervision information for network training. Tested on four datasets, NWD-RKA can consistently improve tiny object detection performance by a large margin. Besides, observing prominent noisy labels in the Tiny Object Detection in Aerial Images (AI-TOD) dataset, we are motivated to meticulously relabel it and release AI-TOD-v2 and its corresponding benchmark. In AI-TOD-v2, the missing annotation and location error problems are considerably mitigated, facilitating more reliable training and validation processes. Embedding NWD-RKA into DetectoRS, the detection performance achieves 4.3 AP points improvement over state-of-the-art competitors on AI-TOD-v2. Datasets, codes, and more visualizations are available at: https://chasel-tsui.github.io/AI-TOD-v2/



### Stain Isolation-based Guidance for Improved Stain Translation
- **Arxiv ID**: http://arxiv.org/abs/2207.00431v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.00431v1)
- **Published**: 2022-06-28 13:33:48+00:00
- **Updated**: 2022-06-28 13:33:48+00:00
- **Authors**: Nicolas Brieu, Felix J. Segerer, Ansh Kapil, Philipp Wortmann, Guenter Schmidt
- **Comment**: Short Paper - MIDL2022 (Medical Imaging with Deep Learning)
- **Journal**: None
- **Summary**: Unsupervised and unpaired domain translation using generative adversarial neural networks, and more precisely CycleGAN, is state of the art for the stain translation of histopathology images. It often, however, suffers from the presence of cycle-consistent but non structure-preserving errors. We propose an alternative approach to the set of methods which, relying on segmentation consistency, enable the preservation of pathology structures. Focusing on immunohistochemistry (IHC) and multiplexed immunofluorescence (mIF), we introduce a simple yet effective guidance scheme as a loss function that leverages the consistency of stain translation with stain isolation. Qualitative and quantitative experiments show the ability of the proposed approach to improve translation between the two domains.



### Show Me Your Face, And I'll Tell You How You Speak
- **Arxiv ID**: http://arxiv.org/abs/2206.14009v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.14009v1)
- **Published**: 2022-06-28 13:52:47+00:00
- **Updated**: 2022-06-28 13:52:47+00:00
- **Authors**: Christen Millerdurai, Lotfy Abdel Khaliq, Timon Ulrich
- **Comment**: None
- **Journal**: None
- **Summary**: When we speak, the prosody and content of the speech can be inferred from the movement of our lips. In this work, we explore the task of lip to speech synthesis, i.e., learning to generate speech given only the lip movements of a speaker where we focus on learning accurate lip to speech mappings for multiple speakers in unconstrained, large vocabulary settings. We capture the speaker's voice identity through their facial characteristics, i.e., age, gender, ethnicity and condition them along with the lip movements to generate speaker identity aware speech. To this end, we present a novel method "Lip2Speech", with key design choices to achieve accurate lip to speech synthesis in unconstrained scenarios. We also perform various experiments and extensive evaluation using quantitative, qualitative metrics and human evaluation.



### Taxonomy and evolution predicting using deep learning in images
- **Arxiv ID**: http://arxiv.org/abs/2206.14011v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14011v1)
- **Published**: 2022-06-28 13:54:14+00:00
- **Updated**: 2022-06-28 13:54:14+00:00
- **Authors**: Jiewen Xiao, Wenbin Liao, Ming Zhang, Jing Wang, Jianxin Wang, Yihua Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Molecular and morphological characters, as important parts of biological taxonomy, are contradictory but need to be integrated. Organism's image recognition and bioinformatics are emerging and hot problems nowadays but with a gap between them. In this work, a multi-branching recognition framework mediated by genetic information bridges this barrier, which establishes the link between macro-morphology and micro-molecular information of mushrooms. The novel multi-perspective structure is proposed to fuse the feature images from three branching models, which significantly improves the accuracy of recognition by about 10% and up to more than 90%. Further, genetic information is implemented to the mushroom image recognition task by using genetic distance embeddings as the representation space for predicting image distance and species identification. Semantic overfitting of traditional classification tasks and the granularity of fine-grained image recognition are also discussed in depth for the first time. The generalizability of the model was investigated in fine-grained scenarios using zero-shot learning tasks, which could predict the taxonomic and evolutionary information of unseen samples. We presented the first method to map images to DNA, namely used an encoder mapping image to genetic distances, and then decoded DNA through a pre-trained decoder, where the total test accuracy on 37 species for DNA prediction is 87.45%. This study creates a novel recognition framework by systematically studying the mushroom image recognition problem, bridging the gap between macroscopic biological information and microscopic molecular information, which will provide a new reference for intelligent biometrics in the future.



### Rethinking Adversarial Examples for Location Privacy Protection
- **Arxiv ID**: http://arxiv.org/abs/2206.14020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14020v1)
- **Published**: 2022-06-28 14:09:09+00:00
- **Updated**: 2022-06-28 14:09:09+00:00
- **Authors**: Trung-Nghia Le, Ta Gu, Huy H. Nguyen, Isao Echizen
- **Comment**: None
- **Journal**: None
- **Summary**: We have investigated a new application of adversarial examples, namely location privacy protection against landmark recognition systems. We introduce mask-guided multimodal projected gradient descent (MM-PGD), in which adversarial examples are trained on different deep models. Image contents are protected by analyzing the properties of regions to identify the ones most suitable for blending in adversarial examples. We investigated two region identification strategies: class activation map-based MM-PGD, in which the internal behaviors of trained deep models are targeted; and human-vision-based MM-PGD, in which regions that attract less human attention are targeted. Experiments on the Places365 dataset demonstrated that these strategies are potentially effective in defending against black-box landmark recognition systems without the need for much image manipulation.



### Deep Neural Networks pruning via the Structured Perspective Regularization
- **Arxiv ID**: http://arxiv.org/abs/2206.14056v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2206.14056v1)
- **Published**: 2022-06-28 14:58:51+00:00
- **Updated**: 2022-06-28 14:58:51+00:00
- **Authors**: Matteo Cacciola, Antonio Frangioni, Xinlin Li, Andrea Lodi
- **Comment**: None
- **Journal**: None
- **Summary**: In Machine Learning, Artificial Neural Networks (ANNs) are a very powerful tool, broadly used in many applications. Often, the selected (deep) architectures include many layers, and therefore a large amount of parameters, which makes training, storage and inference expensive. This motivated a stream of research about compressing the original networks into smaller ones without excessively sacrificing performances. Among the many proposed compression approaches, one of the most popular is \emph{pruning}, whereby entire elements of the ANN (links, nodes, channels, \ldots) and the corresponding weights are deleted. Since the nature of the problem is inherently combinatorial (what elements to prune and what not), we propose a new pruning method based on Operational Research tools. We start from a natural Mixed-Integer-Programming model for the problem, and we use the Perspective Reformulation technique to strengthen its continuous relaxation. Projecting away the indicator variables from this reformulation yields a new regularization term, which we call the Structured Perspective Regularization, that leads to structured pruning of the initial architecture. We test our method on some ResNet architectures applied to CIFAR-10, CIFAR-100 and ImageNet datasets, obtaining competitive performances w.r.t.~the state of the art for structured pruning.



### Continual Learning with Transformers for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.14085v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.14085v1)
- **Published**: 2022-06-28 15:30:10+00:00
- **Updated**: 2022-06-28 15:30:10+00:00
- **Authors**: Beyza Ermis, Giovanni Zappella, Martin Wistuba, Aditya Rawal, Cedric Archambeau
- **Comment**: Appeared in CVPR CLVision workshop. arXiv admin note: substantial
  text overlap with arXiv:2203.04640
- **Journal**: None
- **Summary**: In many real-world scenarios, data to train machine learning models become available over time. However, neural network models struggle to continually learn new concepts without forgetting what has been learnt in the past. This phenomenon is known as catastrophic forgetting and it is often difficult to prevent due to practical constraints, such as the amount of data that can be stored or the limited computation sources that can be used. Moreover, training large neural networks, such as Transformers, from scratch is very costly and requires a vast amount of training data, which might not be available in the application domain of interest. A recent trend indicates that dynamic architectures based on an expansion of the parameters can reduce catastrophic forgetting efficiently in continual learning, but this needs complex tuning to balance the growing number of parameters and barely share any information across tasks. As a result, they struggle to scale to a large number of tasks without significant overhead. In this paper, we validate in the computer vision domain a recent solution called Adaptive Distillation of Adapters (ADA), which is developed to perform continual learning using pre-trained Transformers and Adapters on text classification tasks. We empirically demonstrate on different classification tasks that this method maintains a good predictive performance without retraining the model or increasing the number of model parameters over the time. Besides it is significantly faster at inference time compared to the state-of-the-art methods.



### An adaptive bi-objective optimization algorithm for the satellite image data downlink scheduling problem considering request split
- **Arxiv ID**: http://arxiv.org/abs/2207.00168v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00168v1)
- **Published**: 2022-06-28 15:37:34+00:00
- **Updated**: 2022-06-28 15:37:34+00:00
- **Authors**: Zhongxiang Chang, Abraham P. Punnen, Zhongbao Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: The satellite image data downlink scheduling problem (SIDSP) is well studied in literature for traditional satellites. With recent developments in satellite technology, SIDSP for modern satellites became more complicated, adding new dimensions of complexities and additional opportunities for the effective use of the satellite. In this paper, we introduce the dynamic two-phase satellite image data downlink scheduling problem (D-SIDSP) which combines two interlinked operations of image data segmentation and image data downlink, in a dynamic way, and thereby offering additional modelling flexibility and renewed capabilities. D-SIDSP is formulated as a bi-objective problem of optimizing the image data transmission rate and the service-balance degree. Harnessing the power of an adaptive large neighborhood search algorithm (ALNS) with a nondominated sorting genetic algorithm II (NSGA-II), an adaptive bi-objective memetic algorithm, ALNS+NSGA-II, is developed to solve D-SIDSP. Results of extensive computational experiments carried out using benchmark instances are also presented. Our experimental results disclose that the algorithm ALNS+NSGA-II is a viable alternative to solve D-SIDSP more efficiently and demonstrates superior outcomes based on various performance metrics. The paper also offers new benchmark instances for D-SIDSP that can be used in future research works on the topic.



### RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network
- **Arxiv ID**: http://arxiv.org/abs/2206.14098v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.14098v2)
- **Published**: 2022-06-28 15:48:05+00:00
- **Updated**: 2023-04-28 23:54:40+00:00
- **Authors**: Vitaliy Chiley, Vithursan Thangarasa, Abhay Gupta, Anshul Samar, Joel Hestness, Dennis DeCoste
- **Comment**: Presented at MLSys 2023. Code available from Cerebras Systems:
  https://github.com/CerebrasResearch/RevBiFPN
- **Journal**: None
- **Summary**: This work introduces RevSilo, the first reversible bidirectional multi-scale feature fusion module. Like other reversible methods, RevSilo eliminates the need to store hidden activations by recomputing them. However, existing reversible methods do not apply to multi-scale feature fusion and are, therefore, not applicable to a large class of networks. Bidirectional multi-scale feature fusion promotes local and global coherence and has become a de facto design principle for networks targeting spatially sensitive tasks, e.g., HRNet (Sun et al., 2019a) and EfficientDet (Tan et al., 2020). These networks achieve state-of-the-art results across various computer vision tasks when paired with high-resolution inputs. However, training them requires substantial accelerator memory for saving large, multi-resolution activations. These memory requirements inherently cap the size of neural networks, limiting improvements that come from scale. Operating across resolution scales, RevSilo alleviates these issues. Stacking RevSilos, we create RevBiFPN, a fully reversible bidirectional feature pyramid network. RevBiFPN is competitive with networks such as EfficientNet while using up to 19.8x lesser training memory for image classification. When fine-tuned on MS COCO, RevBiFPN provides up to a 2.5% boost in AP over HRNet using fewer MACs and a 2.4x reduction in training-time memory.



### SSL-Lanes: Self-Supervised Learning for Motion Forecasting in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2206.14116v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.14116v3)
- **Published**: 2022-06-28 16:23:25+00:00
- **Updated**: 2022-09-10 15:49:05+00:00
- **Authors**: Prarthana Bhattacharyya, Chengjie Huang, Krzysztof Czarnecki
- **Comment**: Accepted to CoRL-2022
- **Journal**: None
- **Summary**: Self-supervised learning (SSL) is an emerging technique that has been successfully employed to train convolutional neural networks (CNNs) and graph neural networks (GNNs) for more transferable, generalizable, and robust representation learning. However its potential in motion forecasting for autonomous driving has rarely been explored. In this study, we report the first systematic exploration and assessment of incorporating self-supervision into motion forecasting. We first propose to investigate four novel self-supervised learning tasks for motion forecasting with theoretical rationale and quantitative and qualitative comparisons on the challenging large-scale Argoverse dataset. Secondly, we point out that our auxiliary SSL-based learning setup not only outperforms forecasting methods which use transformers, complicated fusion mechanisms and sophisticated online dense goal candidate optimization algorithms in terms of performance accuracy, but also has low inference time and architectural complexity. Lastly, we conduct several experiments to understand why SSL improves motion forecasting. Code is open-sourced at \url{https://github.com/AutoVision-cloud/SSL-Lanes}.



### Visualizing and Alleviating the Effect of Radial Distortion on Camera Calibration Using Principal Lines
- **Arxiv ID**: http://arxiv.org/abs/2206.14164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14164v1)
- **Published**: 2022-06-28 17:21:26+00:00
- **Updated**: 2022-06-28 17:21:26+00:00
- **Authors**: Jen-Hui Chuang, Hsin-Yi Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Preparing appropriate images for camera calibration is crucial to obtain accurate results. In this paper, new suggestions for preparing such data to alleviate the adverse effect of radial distortion for a calibration procedure using principal lines are developed through the investigations of: (i) identifying directions of checkerboard movements in an image which will result in maximum (and minimum) influence on the calibration results, and (ii) inspecting symmetry and monotonicity of such effect in (i) using the above principal lines. Accordingly, it is suggested that the estimation of principal point should based on linearly independent pairs of nearly parallel principal lines, with a member in each pair corresponds to a near 180-degree rotation (in the image plane) of the other. Experimental results show that more robust and consistent calibration results for the foregoing estimation can actually be obtained, compared with the renowned algebraic methods which estimate distortion parameters explicitly.



### High-Resolution Virtual Try-On with Misalignment and Occlusion-Handled Conditions
- **Arxiv ID**: http://arxiv.org/abs/2206.14180v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.14180v2)
- **Published**: 2022-06-28 17:47:53+00:00
- **Updated**: 2022-07-20 09:26:42+00:00
- **Authors**: Sangyun Lee, Gyojung Gu, Sunghyun Park, Seunghwan Choi, Jaegul Choo
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Image-based virtual try-on aims to synthesize an image of a person wearing a given clothing item. To solve the task, the existing methods warp the clothing item to fit the person's body and generate the segmentation map of the person wearing the item before fusing the item with the person. However, when the warping and the segmentation generation stages operate individually without information exchange, the misalignment between the warped clothes and the segmentation map occurs, which leads to the artifacts in the final image. The information disconnection also causes excessive warping near the clothing regions occluded by the body parts, so-called pixel-squeezing artifacts. To settle the issues, we propose a novel try-on condition generator as a unified module of the two stages (i.e., warping and segmentation generation stages). A newly proposed feature fusion block in the condition generator implements the information exchange, and the condition generator does not create any misalignment or pixel-squeezing artifacts. We also introduce discriminator rejection that filters out the incorrect segmentation map predictions and assures the performance of virtual try-on frameworks. Experiments on a high-resolution dataset demonstrate that our model successfully handles the misalignment and occlusion, and significantly outperforms the baselines. Code is available at https://github.com/sangyun884/HR-VITON.



### Pedestrian 3D Bounding Box Prediction
- **Arxiv ID**: http://arxiv.org/abs/2206.14195v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.14195v1)
- **Published**: 2022-06-28 17:59:45+00:00
- **Updated**: 2022-06-28 17:59:45+00:00
- **Authors**: Saeed Saadatnejad, Yi Zhou Ju, Alexandre Alahi
- **Comment**: Accepted and published in hEART2022 (the 10th Symposium of the
  European Association for Research in Transportation):
  http://www.heart-web.org/
- **Journal**: None
- **Summary**: Safety is still the main issue of autonomous driving, and in order to be globally deployed, they need to predict pedestrians' motions sufficiently in advance. While there is a lot of research on coarse-grained (human center prediction) and fine-grained predictions (human body keypoints prediction), we focus on 3D bounding boxes, which are reasonable estimates of humans without modeling complex motion details for autonomous vehicles. This gives the flexibility to predict in longer horizons in real-world settings. We suggest this new problem and present a simple yet effective model for pedestrians' 3D bounding box prediction. This method follows an encoder-decoder architecture based on recurrent neural networks, and our experiments show its effectiveness in both the synthetic (JTA) and real-world (NuScenes) datasets. The learned representation has useful information to enhance the performance of other tasks, such as action anticipation. Our code is available online: https://github.com/vita-epfl/bounding-box-prediction



### Masked World Models for Visual Control
- **Arxiv ID**: http://arxiv.org/abs/2206.14244v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.14244v3)
- **Published**: 2022-06-28 18:42:27+00:00
- **Updated**: 2023-05-27 09:29:48+00:00
- **Authors**: Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, Pieter Abbeel
- **Comment**: Project website: https://sites.google.com/view/mwm-rl. Accepted to
  CoRL 2022
- **Journal**: None
- **Summary**: Visual model-based reinforcement learning (RL) has the potential to enable sample-efficient robot learning from visual observations. Yet the current approaches typically train a single model end-to-end for learning both visual representations and dynamics, making it difficult to accurately model the interaction between robots and small objects. In this work, we introduce a visual model-based RL framework that decouples visual representation learning and dynamics learning. Specifically, we train an autoencoder with convolutional layers and vision transformers (ViT) to reconstruct pixels given masked convolutional features, and learn a latent dynamics model that operates on the representations from the autoencoder. Moreover, to encode task-relevant information, we introduce an auxiliary reward prediction objective for the autoencoder. We continually update both autoencoder and dynamics model using online samples collected from environment interaction. We demonstrate that our decoupling approach achieves state-of-the-art performance on a variety of visual robotic tasks from Meta-world and RLBench, e.g., we achieve 81.7% success rate on 50 visual robotic manipulation tasks from Meta-world, while the baseline achieves 67.9%. Code is available on the project website: https://sites.google.com/view/mwm-rl.



### SImProv: Scalable Image Provenance Framework for Robust Content Attribution
- **Arxiv ID**: http://arxiv.org/abs/2206.14245v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14245v2)
- **Published**: 2022-06-28 18:42:36+00:00
- **Updated**: 2023-05-08 12:05:05+00:00
- **Authors**: Alexander Black, Tu Bui, Simon Jenni, Zhifei Zhang, Viswanathan Swaminanthan, John Collomosse
- **Comment**: Under consideration at Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: We present SImProv - a scalable image provenance framework to match a query image back to a trusted database of originals and identify possible manipulations on the query. SImProv consists of three stages: a scalable search stage for retrieving top-k most similar images; a re-ranking and near-duplicated detection stage for identifying the original among the candidates; and finally a manipulation detection and visualization stage for localizing regions within the query that may have been manipulated to differ from the original. SImProv is robust to benign image transformations that commonly occur during online redistribution, such as artifacts due to noise and recompression degradation, as well as out-of-place transformations due to image padding, warping, and changes in size and shape. Robustness towards out-of-place transformations is achieved via the end-to-end training of a differentiable warping module within the comparator architecture. We demonstrate effective retrieval and manipulation detection over a dataset of 100 million images.



### GAN-based Intrinsic Exploration For Sample Efficient Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.14256v1
- **DOI**: 10.5220/0010825500003116
- **Categories**: **cs.LG**, cs.AI, cs.CV, 68T20 (Primary) 68T05, 68T07 (Secondary), I.2.8
- **Links**: [PDF](http://arxiv.org/pdf/2206.14256v1)
- **Published**: 2022-06-28 19:16:52+00:00
- **Updated**: 2022-06-28 19:16:52+00:00
- **Authors**: Doğay Kamar, Nazım Kemal Üre, Gözde Ünal
- **Comment**: None
- **Journal**: International Conference on Agents and Artificial Intelligence -
  ICAART, Volume 2, 264-272 (2022)
- **Summary**: In this study, we address the problem of efficient exploration in reinforcement learning. Most common exploration approaches depend on random action selection, however these approaches do not work well in environments with sparse or no rewards. We propose Generative Adversarial Network-based Intrinsic Reward Module that learns the distribution of the observed states and sends an intrinsic reward that is computed as high for states that are out of distribution, in order to lead agent to unexplored states. We evaluate our approach in Super Mario Bros for a no reward setting and in Montezuma's Revenge for a sparse reward setting and show that our approach is indeed capable of exploring efficiently. We discuss a few weaknesses and conclude by discussing future works.



### ZoDIAC: Zoneout Dropout Injection Attention Calculation
- **Arxiv ID**: http://arxiv.org/abs/2206.14263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14263v1)
- **Published**: 2022-06-28 19:36:11+00:00
- **Updated**: 2022-06-28 19:36:11+00:00
- **Authors**: Zanyar Zohourianshahzadi, Jugal Kalita
- **Comment**: This work has been submitted to SN-AIRE journal and is currently
  under review
- **Journal**: None
- **Summary**: Recently the use of self-attention has yielded to state-of-the-art results in vision-language tasks such as image captioning as well as natural language understanding and generation (NLU and NLG) tasks and computer vision tasks such as image classification. This is since self-attention maps the internal interactions among the elements of input source and target sequences. Although self-attention successfully calculates the attention values and maps the relationships among the elements of input source and target sequence, yet there is no mechanism to control the intensity of attention. In real world, when communicating with each other face to face or vocally, we tend to express different visual and linguistic context with various amounts of intensity. Some words might carry (be spoken with) more stress and weight indicating the importance of that word in the context of the whole sentence. Based on this intuition, we propose Zoneout Dropout Injection Attention Calculation (ZoDIAC) in which the intensities of attention values in the elements of the input sequence are calculated with respect to the context of the elements of input sequence. The results of our experiments reveal that employing ZoDIAC leads to better performance in comparison with the self-attention module in the Transformer model. The ultimate goal is to find out if we could modify self-attention module in the Transformer model with a method that is potentially extensible to other models that leverage on self-attention at their core. Our findings suggest that this particular goal deserves further attention and investigation by the research community.   The code for ZoDIAC is available on www.github.com/zanyarz/zodiac .



### Reinforcement Learning in Medical Image Analysis: Concepts, Applications, Challenges, and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/2206.14302v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.14302v1)
- **Published**: 2022-06-28 22:07:17+00:00
- **Updated**: 2022-06-28 22:07:17+00:00
- **Authors**: Mingzhe Hu, Jiahan Zhang, Luke Matkovic, Tian Liu, Xiaofeng Yang
- **Comment**: 30 pages, 13 figures
- **Journal**: None
- **Summary**: Motivation: Medical image analysis involves tasks to assist physicians in qualitative and quantitative analysis of lesions or anatomical structures, significantly improving the accuracy and reliability of diagnosis and prognosis. Traditionally, these tasks are finished by physicians or medical physicists and lead to two major problems: (i) low efficiency; (ii) biased by personal experience. In the past decade, many machine learning methods have been applied to accelerate and automate the image analysis process. Compared to the enormous deployments of supervised and unsupervised learning models, attempts to use reinforcement learning in medical image analysis are scarce. This review article could serve as the stepping-stone for related research. Significance: From our observation, though reinforcement learning has gradually gained momentum in recent years, many researchers in the medical analysis field find it hard to understand and deploy in clinics. One cause is lacking well-organized review articles targeting readers lacking professional computer science backgrounds. Rather than providing a comprehensive list of all reinforcement learning models in medical image analysis, this paper may help the readers to learn how to formulate and solve their medical image analysis research as reinforcement learning problems. Approach & Results: We selected published articles from Google Scholar and PubMed. Considering the scarcity of related articles, we also included some outstanding newest preprints. The papers are carefully reviewed and categorized according to the type of image analysis task. We first review the basic concepts and popular models of reinforcement learning. Then we explore the applications of reinforcement learning models in landmark detection. Finally, we conclude the article by discussing the reviewed reinforcement learning approaches' limitations and possible improvements.



### Multistep Automated Data Labelling Procedure (MADLaP) for Thyroid Nodules on Ultrasound: An Artificial Intelligence Approach for Automating Image Annotation
- **Arxiv ID**: http://arxiv.org/abs/2206.14305v1
- **DOI**: 10.1016/j.artmed.2023.102553
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2206.14305v1)
- **Published**: 2022-06-28 22:10:37+00:00
- **Updated**: 2022-06-28 22:10:37+00:00
- **Authors**: Jikai Zhang, Maciej M. Mazurowski, Brian C. Allen, Benjamin Wildman-Torbiner
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning (ML) for diagnosis of thyroid nodules on ultrasound is an active area of research. However, ML tools require large, well-labelled datasets, the curation of which is time-consuming and labor-intensive. The purpose of our study was to develop and test a deep-learning-based tool to facilitate and automate the data annotation process for thyroid nodules; we named our tool Multistep Automated Data Labelling Procedure (MADLaP). MADLaP was designed to take multiple inputs included pathology reports, ultrasound images, and radiology reports. Using multiple step-wise modules including rule-based natural language processing, deep-learning-based imaging segmentation, and optical character recognition, MADLaP automatically identified images of a specific thyroid nodule and correctly assigned a pathology label. The model was developed using a training set of 378 patients across our health system and tested on a separate set of 93 patients. Ground truths for both sets were selected by an experienced radiologist. Performance metrics including yield (how many labeled images the model produced) and accuracy (percentage correct) were measured using the test set. MADLaP achieved a yield of 63% and an accuracy of 83%. The yield progressively increased as the input data moved through each module, while accuracy peaked part way through. Error analysis showed that inputs from certain examination sites had lower accuracy (40%) than the other sites (90%, 100%). MADLaP successfully created curated datasets of labeled ultrasound images of thyroid nodules. While accurate, the relatively suboptimal yield of MADLaP exposed some challenges when trying to automatically label radiology images from heterogeneous sources. The complex task of image curation and annotation could be automated, allowing for enrichment of larger datasets for use in machine learning development.



### Generative Neural Articulated Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2206.14314v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2206.14314v3)
- **Published**: 2022-06-28 22:49:42+00:00
- **Updated**: 2023-01-09 06:59:32+00:00
- **Authors**: Alexander W. Bergman, Petr Kellnhofer, Wang Yifan, Eric R. Chan, David B. Lindell, Gordon Wetzstein
- **Comment**: Project website:
  http://www.computationalimaging.org/publications/gnarf/
- **Journal**: None
- **Summary**: Unsupervised learning of 3D-aware generative adversarial networks (GANs) using only collections of single-view 2D photographs has very recently made much progress. These 3D GANs, however, have not been demonstrated for human bodies and the generated radiance fields of existing frameworks are not directly editable, limiting their applicability in downstream tasks. We propose a solution to these challenges by developing a 3D GAN framework that learns to generate radiance fields of human bodies or faces in a canonical pose and warp them using an explicit deformation field into a desired body pose or facial expression. Using our framework, we demonstrate the first high-quality radiance field generation results for human bodies. Moreover, we show that our deformation-aware training procedure significantly improves the quality of generated bodies or faces when editing their poses or facial expressions compared to a 3D GAN that is not trained with explicit deformations.



