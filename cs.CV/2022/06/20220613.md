# Arxiv Papers in cs.CV on 2022-06-13
### TC-SfM: Robust Track-Community-Based Structure-from-Motion
- **Arxiv ID**: http://arxiv.org/abs/2206.05866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05866v1)
- **Published**: 2022-06-13 01:09:12+00:00
- **Updated**: 2022-06-13 01:09:12+00:00
- **Authors**: Lei Wang, Linlin Ge, Shan Luo, Zihan Yan, Zhaopeng Cui, Jieqing Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Structure-from-Motion (SfM) aims to recover 3D scene structures and camera poses based on the correspondences between input images, and thus the ambiguity caused by duplicate structures (i.e., different structures with strong visual resemblance) always results in incorrect camera poses and 3D structures. To deal with the ambiguity, most existing studies resort to additional constraint information or implicit inference by analyzing two-view geometries or feature points. In this paper, we propose to exploit high-level information in the scene, i.e., the spatial contextual information of local regions, to guide the reconstruction. Specifically, a novel structure is proposed, namely, {\textit{track-community}}, in which each community consists of a group of tracks and represents a local segment in the scene. A community detection algorithm is used to partition the scene into several segments. Then, the potential ambiguous segments are detected by analyzing the neighborhood of tracks and corrected by checking the pose consistency. Finally, we perform partial reconstruction on each segment and align them with a novel bidirectional consistency cost function which considers both 3D-3D correspondences and pairwise relative camera poses. Experimental results demonstrate that our approach can robustly alleviate reconstruction failure resulting from visually indistinguishable structures and accurately merge the partial reconstructions.



### Perceptual Quality Assessment of Virtual Reality Videos in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2206.08751v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.08751v2)
- **Published**: 2022-06-13 02:22:57+00:00
- **Updated**: 2023-07-03 06:41:26+00:00
- **Authors**: Wen Wen, Mu Li, Yiru Yao, Xiangjie Sui, Yabin Zhang, Long Lan, Yuming Fang, Kede Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Investigating how people perceive virtual reality videos in the wild (\ie, those captured by everyday users) is a crucial and challenging task in VR-related applications due to complex \textit{authentic} distortions localized in space and time. Existing panoramic video databases only consider synthetic distortions, assume fixed viewing conditions, and are limited in size. To overcome these shortcomings, we construct the VR Video Quality in the Wild (VRVQW) database, which is one of the first of its kind, and contains $502$ user-generated videos with diverse content and distortion characteristics. Based on VRVQW, we conduct a formal psychophysical experiment to record the scanpaths and perceived quality scores from $139$ participants under two different viewing conditions. We provide a thorough statistical analysis of the recorded data, observing significant impact of viewing conditions on both human scanpaths and perceived quality. Moreover, we develop an objective quality assessment model for VR videos based on pseudocylindrical representation and convolution. Results on the proposed VRVQW show that our method is superior to existing video quality assessment models, only underperforming viewport-based models that otherwise rely on human scanpaths for projection. Last, we explore the additional use of the VRVQW dataset to benchmark saliency detection techniques, highlighting the need for further research. We have made the database and code available at \url{https://github.com/limuhit/VR-Video-Quality-in-the-Wild}.



### Deploying Convolutional Networks on Untrusted Platforms Using 2D Holographic Reduced Representations
- **Arxiv ID**: http://arxiv.org/abs/2206.05893v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2206.05893v1)
- **Published**: 2022-06-13 03:31:39+00:00
- **Updated**: 2022-06-13 03:31:39+00:00
- **Authors**: Mohammad Mahmudul Alam, Edward Raff, Tim Oates, James Holt
- **Comment**: To appear in the Proceedings of the 39 th International Conference on
  Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022
- **Journal**: None
- **Summary**: Due to the computational cost of running inference for a neural network, the need to deploy the inferential steps on a third party's compute environment or hardware is common. If the third party is not fully trusted, it is desirable to obfuscate the nature of the inputs and outputs, so that the third party can not easily determine what specific task is being performed. Provably secure protocols for leveraging an untrusted party exist but are too computational demanding to run in practice. We instead explore a different strategy of fast, heuristic security that we call Connectionist Symbolic Pseudo Secrets. By leveraging Holographic Reduced Representations (HRR), we create a neural network with a pseudo-encryption style defense that empirically shows robustness to attack, even under threat models that unrealistically favor the adversary.



### Improve Ranking Correlation of Super-net through Training Scheme from One-shot NAS to Few-shot NAS
- **Arxiv ID**: http://arxiv.org/abs/2206.05896v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05896v2)
- **Published**: 2022-06-13 04:02:12+00:00
- **Updated**: 2022-06-14 03:07:09+00:00
- **Authors**: Jiawei Liu, Kaiyu Zhang, Weitai Hu, Qing Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The algorithms of one-shot neural architecture search(NAS) have been widely used to reduce computation consumption. However, because of the interference among the subnets in which weights are shared, the subnets inherited from these super-net trained by those algorithms have poor consistency in precision ranking. To address this problem, we propose a step-by-step training super-net scheme from one-shot NAS to few-shot NAS. In the training scheme, we firstly train super-net in a one-shot way, and then we disentangle the weights of super-net by splitting them into multi-subnets and training them gradually. Finally, our method ranks 4th place in the CVPR2022 3rd Lightweight NAS Challenge Track1. Our code is available at https://github.com/liujiawei2333/CVPR2022-NAS-competition-Track-1-4th-solution.



### $\texttt{GradICON}$: Approximate Diffeomorphisms via Gradient Inverse Consistency
- **Arxiv ID**: http://arxiv.org/abs/2206.05897v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05897v3)
- **Published**: 2022-06-13 04:03:49+00:00
- **Updated**: 2023-04-04 05:44:39+00:00
- **Authors**: Lin Tian, Hastings Greer, François-Xavier Vialard, Roland Kwitt, Raúl San José Estépar, Richard Jarrett Rushmore, Nikolaos Makris, Sylvain Bouix, Marc Niethammer
- **Comment**: 29 pages, 16 figures, CVPR 2023
- **Journal**: None
- **Summary**: We present an approach to learning regular spatial transformations between image pairs in the context of medical image registration. Contrary to optimization-based registration techniques and many modern learning-based methods, we do not directly penalize transformation irregularities but instead promote transformation regularity via an inverse consistency penalty. We use a neural network to predict a map between a source and a target image as well as the map when swapping the source and target images. Different from existing approaches, we compose these two resulting maps and regularize deviations of the $\bf{Jacobian}$ of this composition from the identity matrix. This regularizer -- $\texttt{GradICON}$ -- results in much better convergence when training registration models compared to promoting inverse consistency of the composition of maps directly while retaining the desirable implicit regularization effects of the latter. We achieve state-of-the-art registration performance on a variety of real-world medical image datasets using a single set of hyperparameters and a single non-dataset-specific training protocol.



### Pixel to Binary Embedding Towards Robustness for CNNs
- **Arxiv ID**: http://arxiv.org/abs/2206.05898v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05898v1)
- **Published**: 2022-06-13 04:06:12+00:00
- **Updated**: 2022-06-13 04:06:12+00:00
- **Authors**: Ikki Kishida, Hideki Nakayama
- **Comment**: Accepted to ICPR2022
- **Journal**: None
- **Summary**: There are several problems with the robustness of Convolutional Neural Networks (CNNs). For example, the prediction of CNNs can be changed by adding a small magnitude of noise to an input, and the performances of CNNs are degraded when the distribution of input is shifted by a transformation never seen during training (e.g., the blur effect). There are approaches to replace pixel values with binary embeddings to tackle the problem of adversarial perturbations, which successfully improve robustness. In this work, we propose Pixel to Binary Embedding (P2BE) to improve the robustness of CNNs. P2BE is a learnable binary embedding method as opposed to previous hand-coded binary embedding methods. P2BE outperforms other binary embedding methods in robustness against adversarial perturbations and visual corruptions that are not shown during training.



### Geometrically Guided Integrated Gradients
- **Arxiv ID**: http://arxiv.org/abs/2206.05903v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, F.2.2; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2206.05903v2)
- **Published**: 2022-06-13 05:05:43+00:00
- **Updated**: 2022-06-16 23:30:12+00:00
- **Authors**: Md Mahfuzur Rahman, Noah Lewis, Sergey Plis
- **Comment**: 19 pages, 23 figures, funding sources added
- **Journal**: None
- **Summary**: Interpretability methods for deep neural networks mainly focus on the sensitivity of the class score with respect to the original or perturbed input, usually measured using actual or modified gradients. Some methods also use a model-agnostic approach to understanding the rationale behind every prediction. In this paper, we argue and demonstrate that local geometry of the model parameter space relative to the input can also be beneficial for improved post-hoc explanations. To achieve this goal, we introduce an interpretability method called "geometrically-guided integrated gradients" that builds on top of the gradient calculation along a linear path as traditionally used in integrated gradient methods. However, instead of integrating gradient information, our method explores the model's dynamic behavior from multiple scaled versions of the input and captures the best possible attribution for each input. We demonstrate through extensive experiments that the proposed approach outperforms vanilla and integrated gradients in subjective and quantitative assessment. We also propose a "model perturbation" sanity check to complement the traditionally used "model randomization" test.



### INDIGO: Intrinsic Multimodality for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2206.05912v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05912v1)
- **Published**: 2022-06-13 05:41:09+00:00
- **Updated**: 2022-06-13 05:41:09+00:00
- **Authors**: Puneet Mangla, Shivam Chandhok, Milan Aggarwal, Vineeth N Balasubramanian, Balaji Krishnamurthy
- **Comment**: Under Submission
- **Journal**: None
- **Summary**: For models to generalize under unseen domains (a.k.a domain generalization), it is crucial to learn feature representations that are domain-agnostic and capture the underlying semantics that makes up an object category. Recent advances towards weakly supervised vision-language models that learn holistic representations from cheap weakly supervised noisy text annotations have shown their ability on semantic understanding by capturing object characteristics that generalize under different domains. However, when multiple source domains are involved, the cost of curating textual annotations for every image in the dataset can blow up several times, depending on their number. This makes the process tedious and infeasible, hindering us from directly using these supervised vision-language approaches to achieve the best generalization on an unseen domain. Motivated from this, we study how multimodal information from existing pre-trained multimodal networks can be leveraged in an "intrinsic" way to make systems generalize under unseen domains. To this end, we propose IntriNsic multimodality for DomaIn GeneralizatiOn (INDIGO), a simple and elegant way of leveraging the intrinsic modality present in these pre-trained multimodal networks along with the visual modality to enhance generalization to unseen domains at test-time. We experiment on several Domain Generalization settings (ClosedDG, OpenDG, and Limited sources) and show state-of-the-art generalization performance on unseen domains. Further, we provide a thorough analysis to develop a holistic understanding of INDIGO.



### LinK3D: Linear Keypoints Representation for 3D LiDAR Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2206.05927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05927v1)
- **Published**: 2022-06-13 06:50:56+00:00
- **Updated**: 2022-06-13 06:50:56+00:00
- **Authors**: Yunge Cui, Yinlong Zhang, Jiahua Dong, Haibo Sun, Feng Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Feature extraction and matching are the basic parts of many computer vision tasks, such as 2D or 3D object detection, recognition, and registration. As we all know, 2D feature extraction and matching have already been achieved great success. Unfortunately, in the field of 3D, the current methods fail to support the extensive application of 3D LiDAR sensors in vision tasks, due to the poor descriptiveness and inefficiency. To address this limitation, we propose a novel 3D feature representation method: Linear Keypoints representation for 3D LiDAR point cloud, called LinK3D. The novelty of LinK3D lies in that it fully considers the characteristics (such as sparsity, complexity of scenarios) of LiDAR point cloud, and represents current keypoint with its robust neighbor keypoints, which provide strong constraint on the description of current keypoint. The proposed LinK3D has been evaluated on two public datasets (i.e., KITTI, Steven VLP16), and the experimental results show that our method greatly outperforms the state-of-the-arts in matching performance. More importantly, LinK3D shows excellent real-time performance (based on the frequence 10 Hz of LiDAR). LinK3D only takes an average of 32 milliseconds to extract features from the point cloud collected by a 64-ray laser beam, and takes merely about 8 milliseconds to match two LiDAR scans when executed in a notebook with an Intel Core i7 @2.2 GHz processor. Moreover, our method can be widely extended to a variety of 3D vision applications. In this paper, we has applied our LinK3D to 3D registration, LiDAR odometry and place recognition tasks, and achieved competitive results compared with the state-of-the-art methods.



### Faster Optimization-Based Meta-Learning Adaptation Phase
- **Arxiv ID**: http://arxiv.org/abs/2206.05930v1
- **DOI**: 10.15588/1607-3274-2022-1-10
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05930v1)
- **Published**: 2022-06-13 06:57:17+00:00
- **Updated**: 2022-06-13 06:57:17+00:00
- **Authors**: Kostiantyn Khabarlak
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks require a large amount of annotated data to learn. Meta-learning algorithms propose a way to decrease the number of training samples to only a few. One of the most prominent optimization-based meta-learning algorithms is Model-Agnostic Meta-Learning (MAML). However, the key procedure of adaptation to new tasks in MAML is quite slow. In this work we propose an improvement to MAML meta-learning algorithm. We introduce Lambda patterns by which we restrict which weight are updated in the network during the adaptation phase. This makes it possible to skip certain gradient computations. The fastest pattern is selected given an allowed quality degradation threshold parameter. In certain cases, quality improvement is possible by a careful pattern selection. The experiments conducted have shown that via Lambda adaptation pattern selection, it is possible to significantly improve the MAML method in the following areas: adaptation time has been decreased by a factor of 3 with minimal accuracy loss; accuracy for one-step adaptation has been substantially improved.



### Fluorescence angiography classification in colorectal surgery -- A preliminary report
- **Arxiv ID**: http://arxiv.org/abs/2206.05935v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05935v1)
- **Published**: 2022-06-13 07:10:59+00:00
- **Updated**: 2022-06-13 07:10:59+00:00
- **Authors**: Antonio S Soares, Sophia Bano, Neil T Clancy, Laurence B Lovat, Danail Stoyanov, Manish Chand
- **Comment**: None
- **Journal**: None
- **Summary**: Background: Fluorescence angiography has shown very promising results in reducing anastomotic leaks by allowing the surgeon to select optimally perfused tissue. However, subjective interpretation of the fluorescent signal still hinders broad application of the technique, as significant variation between different surgeons exists. Our aim is to develop an artificial intelligence algorithm to classify colonic tissue as 'perfused' or 'not perfused' based on intraoperative fluorescence angiography data.   Methods: A classification model with a Resnet architecture was trained on a dataset of fluorescence angiography videos of colorectal resections at a tertiary referral centre. Frames corresponding to fluorescent and non-fluorescent segments of colon were used to train a classification algorithm. Validation using frames from patients not used in the training set was performed, including both data collected using the same equipment and data collected using a different camera. Performance metrics were calculated, and saliency maps used to further analyse the output. A decision boundary was identified based on the tissue classification.   Results: A convolutional neural network was successfully trained on 1790 frames from 7 patients and validated in 24 frames from 14 patients. The accuracy on the training set was 100%, on the validation set was 80%. Recall and precision were respectively 100% and 100% on the training set and 68.8% and 91.7% on the validation set.   Conclusion: Automated classification of intraoperative fluorescence angiography with a high degree of accuracy is possible and allows automated decision boundary identification. This will enable surgeons to standardise the technique of fluorescence angiography. A web based app was made available to deploy the algorithm.



### PRO-TIP: Phantom for RObust automatic ultrasound calibration by TIP detection
- **Arxiv ID**: http://arxiv.org/abs/2206.05962v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2206.05962v1)
- **Published**: 2022-06-13 08:18:33+00:00
- **Updated**: 2022-06-13 08:18:33+00:00
- **Authors**: Matteo Ronchetti, Julia Rackerseder, Maria Tirindelli, Mehrdad Salehi, Nassir Navab, Wolfgang Wein, Oliver Zettinig
- **Comment**: This preprint was submitted to MICCAI 2022. The Version of Record of
  this contribution will be published in Springer LNCS
- **Journal**: None
- **Summary**: We propose a novel method to automatically calibrate tracked ultrasound probes. To this end we design a custom phantom consisting of nine cones with different heights. The tips are used as key points to be matched between multiple sweeps. We extract them using a convolutional neural network to segment the cones in every ultrasound frame and then track them across the sweep. The calibration is robustly estimated using RANSAC and later refined employing image based techniques. Our phantom can be 3D-printed and offers many advantages over state-of-the-art methods. The phantom design and algorithm code are freely available online. Since our phantom does not require a tracking target on itself, ease of use is improved over currently used techniques. The fully automatic method generalizes to new probes and different vendors, as shown in our experiments. Our approach produces results comparable to calibrations obtained by a domain expert.



### ATDN vSLAM: An all-through Deep Learning-Based Solution for Visual Simultaneous Localization and Mapping
- **Arxiv ID**: http://arxiv.org/abs/2206.05963v3
- **DOI**: 10.3311/PPee.20437
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05963v3)
- **Published**: 2022-06-13 08:19:54+00:00
- **Updated**: 2022-07-15 14:34:17+00:00
- **Authors**: Mátyás Szántó, György R. Bogár, László Vajta
- **Comment**: Published in Periodica Polytechnica Electrical Engineering 11 pages
- **Journal**: Periodica Polytechnica Electrical Engineering and Computer
  Science, 66(3), pp. 236-247, 2022
- **Summary**: In this paper, a novel solution is introduced for visual Simultaneous Localization and Mapping (vSLAM) that is built up of Deep Learning components. The proposed architecture is a highly modular framework in which each component offers state of the art results in their respective fields of vision-based deep learning solutions. The paper shows that with the synergic integration of these individual building blocks, a functioning and efficient all-through deep neural (ATDN) vSLAM system can be created. The Embedding Distance Loss function is introduced and using it the ATDN architecture is trained. The resulting system managed to achieve 4.4% translation and 0.0176 deg/m rotational error on a subset of the KITTI dataset. The proposed architecture can be used for efficient and low-latency autonomous driving (AD) aiding database creation as well as a basis for autonomous vehicle (AV) control.



### GoToNet: Fast Monocular Scene Exposure and Exploration
- **Arxiv ID**: http://arxiv.org/abs/2206.05967v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05967v1)
- **Published**: 2022-06-13 08:28:31+00:00
- **Updated**: 2022-06-13 08:28:31+00:00
- **Authors**: Tom Avrech, Evgenii Zheltonozhskii, Chaim Baskin, Ehud Rivlin
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous scene exposure and exploration, especially in localization or communication-denied areas, useful for finding targets in unknown scenes, remains a challenging problem in computer navigation. In this work, we present a novel method for real-time environment exploration, whose only requirements are a visually similar dataset for pre-training, enough lighting in the scene, and an on-board forward-looking RGB camera for environmental sensing. As opposed to existing methods, our method requires only one look (image) to make a good tactical decision, and therefore works at a non-growing, constant time. Two direction predictions, characterized by pixels dubbed the Goto and Lookat pixels, comprise the core of our method. These pixels encode the recommended flight instructions in the following way: the Goto pixel defines the direction in which the agent should move by one distance unit, and the Lookat pixel defines the direction in which the camera should be pointing at in the next step. These flying-instruction pixels are optimized to expose the largest amount of currently unexplored areas.   Our method presents a novel deep learning-based navigation approach that is able to solve this problem and demonstrate its ability in an even more complicated setup, i.e., when computational power is limited. In addition, we propose a way to generate a navigation-oriented dataset, enabling efficient training of our method using RGB and depth images. Tests conducted in a simulator evaluating both the sparse pixels' coordinations inferring process, and 2D and 3D test flights aimed to unveil areas and decrease distances to targets achieve promising results. Comparison against a state-of-the-art algorithm shows our method is able to overperform it, that while measuring the new voxels per camera pose, minimum distance to target, percentage of surface voxels seen, and compute time metrics.



### Hypernetwork-Based Adaptive Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2206.05970v3
- **DOI**: 10.48550/ARXIV.2206.05970
- **Categories**: **cs.CV**, eess.IV, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2206.05970v3)
- **Published**: 2022-06-13 08:33:14+00:00
- **Updated**: 2023-02-26 07:19:18+00:00
- **Authors**: Shai Aharon, Gil Ben-Artzi
- **Comment**: 5 pages, 5 Figures, ICASSP 2023
- **Journal**: ICASSP 2023
- **Summary**: Adaptive image restoration models can restore images with different degradation levels at inference time without the need to retrain the model. We present an approach that is highly accurate and allows a significant reduction in the number of parameters. In contrast to existing methods, our approach can restore images using a single fixed-size model, regardless of the number of degradation levels. On popular datasets, our approach yields state-of-the-art results in terms of size and accuracy for a variety of image restoration tasks, including denoising, deJPEG, and super-resolution.



### Efficient Human-in-the-loop System for Guiding DNNs Attention
- **Arxiv ID**: http://arxiv.org/abs/2206.05981v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05981v3)
- **Published**: 2022-06-13 09:04:32+00:00
- **Updated**: 2023-02-06 13:11:44+00:00
- **Authors**: Yi He, Xi Yang, Chia-Ming Chang, Haoran Xie, Takeo Igarashi
- **Comment**: 13 pages, 11 figures, proceeding of ACM IUI 2023, video
  https://youtu.be/2MD-z6vXKJ4
- **Journal**: None
- **Summary**: Attention guidance is an approach to addressing dataset bias in deep learning, where the model relies on incorrect features to make decisions. Focusing on image classification tasks, we propose an efficient human-in-the-loop system to interactively direct the attention of classifiers to the regions specified by users, thereby reducing the influence of co-occurrence bias and improving the transferability and interpretability of a DNN. Previous approaches for attention guidance require the preparation of pixel-level annotations and are not designed as interactive systems. We present a new interactive method to allow users to annotate images with simple clicks, and study a novel active learning strategy to significantly reduce the number of annotations. We conducted both a numerical evaluation and a user study to evaluate the proposed system on multiple datasets. Compared to the existing non-active-learning approach which usually relies on huge amounts of polygon-based segmentation masks to fine-tune or train the DNNs, our system can save lots of labor and money and obtain a fine-tuned network that works better even when the dataset is biased. The experiment results indicate that the proposed system is efficient, reasonable, and reliable.



### Learning Fashion Compatibility from In-the-wild Images
- **Arxiv ID**: http://arxiv.org/abs/2206.05982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05982v1)
- **Published**: 2022-06-13 09:05:25+00:00
- **Updated**: 2022-06-13 09:05:25+00:00
- **Authors**: Additya Popli, Vijay Kumar, Sujit Jos, Saraansh Tandon
- **Comment**: Accepted to ICPR 2022
- **Journal**: None
- **Summary**: Complementary fashion recommendation aims at identifying items from different categories (e.g. shirt, footwear, etc.) that "go well together" as an outfit. Most existing approaches learn representation for this task using labeled outfit datasets containing manually curated compatible item combinations. In this work, we propose to learn representations for compatibility prediction from in-the-wild street fashion images through self-supervised learning by leveraging the fact that people often wear compatible outfits. Our pretext task is formulated such that the representations of different items worn by the same person are closer compared to those worn by other people. Additionally, to reduce the domain gap between in-the-wild and catalog images during inference, we introduce an adversarial loss that minimizes the difference in feature distribution between the two domains. We conduct our experiments on two popular fashion compatibility benchmarks - Polyvore and Polyvore-Disjoint outfits, and outperform existing self-supervised approaches, particularly significant in cross-dataset setting where training and testing images are from different sources.



### Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling
- **Arxiv ID**: http://arxiv.org/abs/2206.06014v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.06014v1)
- **Published**: 2022-06-13 10:13:39+00:00
- **Updated**: 2022-06-13 10:13:39+00:00
- **Authors**: Yuanbang Liang, Jing Wu, Yu-Kun Lai, Yipeng Qin
- **Comment**: Accepted at ICML 2022. Our code is available at:
  https://github.com/Byronliang8/HubnessGANSampling
- **Journal**: None
- **Summary**: Despite the extensive studies on Generative Adversarial Networks (GANs), how to reliably sample high-quality images from their latent spaces remains an under-explored topic. In this paper, we propose a novel GAN latent sampling method by exploring and exploiting the hubness priors of GAN latent distributions. Our key insight is that the high dimensionality of the GAN latent space will inevitably lead to the emergence of hub latents that usually have much larger sampling densities than other latents in the latent space. As a result, these hub latents are better trained and thus contribute more to the synthesis of high-quality images. Unlike the a posterior "cherry-picking", our method is highly efficient as it is an a priori method that identifies high-quality latents before the synthesis of images. Furthermore, we show that the well-known but purely empirical truncation trick is a naive approximation to the central clustering effect of hub latents, which not only uncovers the rationale of the truncation trick, but also indicates the superiority and fundamentality of our method. Extensive experimental results demonstrate the effectiveness of the proposed method.



### Virtual embeddings and self-consistency for self-supervised learning
- **Arxiv ID**: http://arxiv.org/abs/2206.06023v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.06023v2)
- **Published**: 2022-06-13 10:20:28+00:00
- **Updated**: 2022-06-15 22:59:01+00:00
- **Authors**: Tariq Bdair, Hossam Abdelhamid, Nassir Navab, Shadi Albarqouni
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised Learning (SSL) has recently gained much attention due to the high cost and data limitation in the training of supervised learning models. The current paradigm in the SSL is to utilize data augmentation at the input space to create different views of the same images and train a model to maximize the representations between similar images and minimize them for different ones. While this approach achieves state-of-the-art (SOTA) results in various downstream tasks, it still lakes the opportunity to investigate the latent space augmentation. This paper proposes TriMix, a novel concept for SSL that generates virtual embeddings through linear interpolation of the data, thus providing the model with novel representations. Our strategy focuses on training the model to extract the original embeddings from virtual ones, hence, better representation learning. Additionally, we propose a self-consistency term that improves the consistency between the virtual and actual embeddings. We validate TriMix on eight benchmark datasets consisting of natural and medical images with an improvement of 2.71% and 0.41% better than the second-best models for both data types. Further, our approach outperformed the current methods in semi-supervised learning, particularly in low data regimes. Besides, our pre-trained models showed better transfer to other datasets.



### Deep ensemble learning for segmenting tuberculosis-consistent manifestations in chest radiographs
- **Arxiv ID**: http://arxiv.org/abs/2206.06065v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2206.06065v1)
- **Published**: 2022-06-13 11:51:45+00:00
- **Updated**: 2022-06-13 11:51:45+00:00
- **Authors**: Sivaramakrishnan Rajaraman, Feng Yang, Ghada Zamzmi, Peng Guo, Zhiyun Xue, Sameer K Antani
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Automated segmentation of tuberculosis (TB)-consistent lesions in chest X-rays (CXRs) using deep learning (DL) methods can help reduce radiologist effort, supplement clinical decision-making, and potentially result in improved patient treatment. The majority of works in the literature discuss training automatic segmentation models using coarse bounding box annotations. However, the granularity of the bounding box annotation could result in the inclusion of a considerable fraction of false positives and negatives at the pixel level that may adversely impact overall semantic segmentation performance. This study (i) evaluates the benefits of using fine-grained annotations of TB-consistent lesions and (ii) trains and constructs ensembles of the variants of U-Net models for semantically segmenting TB-consistent lesions in both original and bone-suppressed frontal CXRs. We evaluated segmentation performance using several ensemble methods such as bitwise AND, bitwise-OR, bitwise-MAX, and stacking. We observed that the stacking ensemble demonstrated superior segmentation performance (Dice score: 0.5743, 95% confidence interval: (0.4055,0.7431)) compared to the individual constituent models and other ensemble methods. To the best of our knowledge, this is the first study to apply ensemble learning to improve fine-grained TB-consistent lesion segmentation performance.



### Better Teacher Better Student: Dynamic Prior Knowledge for Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2206.06067v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06067v4)
- **Published**: 2022-06-13 11:52:13+00:00
- **Updated**: 2023-03-23 11:17:31+00:00
- **Authors**: Zengyu Qiu, Xinzhu Ma, Kunlin Yang, Chunya Liu, Jun Hou, Shuai Yi, Wanli Ouyang
- **Comment**: ICLR'23 accepted
- **Journal**: None
- **Summary**: Knowledge distillation (KD) has shown very promising capabilities in transferring learning representations from large models (teachers) to small models (students). However, as the capacity gap between students and teachers becomes larger, existing KD methods fail to achieve better results. Our work shows that the `prior knowledge' is vital to KD, especially when applying large teachers. Particularly, we propose the dynamic prior knowledge (DPK), which integrates part of teacher's features as the prior knowledge before the feature distillation. This means that our method also takes the teacher's feature as `input', not just `target'. Besides, we dynamically adjust the ratio of the prior knowledge during the training phase according to the feature gap, thus guiding the student in an appropriate difficulty. To evaluate the proposed method, we conduct extensive experiments on two image classification benchmarks (i.e. CIFAR100 and ImageNet) and an object detection benchmark (i.e. MS COCO. The results demonstrate the superiority of our method in performance under varying settings. Besides, our DPK makes the performance of the student model positively correlated with that of the teacher model, which means that we can further boost the accuracy of students by applying larger teachers. More importantly, DPK provides a fast solution in teacher model selection for any given model.



### Annular Computational Imaging: Capture Clear Panoramic Images through Simple Lens
- **Arxiv ID**: http://arxiv.org/abs/2206.06070v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2206.06070v2)
- **Published**: 2022-06-13 11:57:57+00:00
- **Updated**: 2022-12-29 12:09:59+00:00
- **Authors**: Qi Jiang, Hao Shi, Lei Sun, Shaohua Gao, Kailun Yang, Kaiwei Wang
- **Comment**: Accepted to IEEE Transactions on Computational Imaging (TCI). Code
  and datasets are publicly available at
  https://github.com/zju-jiangqi/ACI-PI2RNet
- **Journal**: None
- **Summary**: Panoramic Annular Lens (PAL) composed of few lenses has great potential in panoramic surrounding sensing tasks for mobile and wearable devices because of its tiny size and large Field of View (FoV). However, the image quality of tiny-volume PAL confines to optical limit due to the lack of lenses for aberration correction. In this paper, we propose an Annular Computational Imaging (ACI) framework to break the optical limit of light-weight PAL design. To facilitate learning-based image restoration, we introduce a wave-based simulation pipeline for panoramic imaging and tackle the synthetic-to-real gap through multiple data distributions. The proposed pipeline can be easily adapted to any PAL with design parameters and is suitable for loose-tolerance designs. Furthermore, we design the Physics Informed Image Restoration Network (PI2RNet) considering the physical priors of panoramic imaging and single-pass physics-informed engine. At the dataset level, we create the DIVPano dataset and the extensive experiments on it illustrate that our proposed network sets the new state of the art in the panoramic image restoration under spatially-variant degradation. In addition, the evaluation of the proposed ACI on a simple PAL with only 3 spherical lenses reveals the delicate balance between high-quality panoramic imaging and compact design. To the best of our knowledge, we are the first to explore Computational Imaging (CI) in PAL. Code and datasets are publicly available at https://github.com/zju-jiangqi/ACI-PI2RNet.



### AR-NeRF: Unsupervised Learning of Depth and Defocus Effects from Natural Images with Aperture Rendering Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2206.06100v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.06100v1)
- **Published**: 2022-06-13 12:41:59+00:00
- **Updated**: 2022-06-13 12:41:59+00:00
- **Authors**: Takuhiro Kaneko
- **Comment**: Accepted to CVPR 2022. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/ar-nerf/
- **Journal**: None
- **Summary**: Fully unsupervised 3D representation learning has gained attention owing to its advantages in data collection. A successful approach involves a viewpoint-aware approach that learns an image distribution based on generative models (e.g., generative adversarial networks (GANs)) while generating various view images based on 3D-aware models (e.g., neural radiance fields (NeRFs)). However, they require images with various views for training, and consequently, their application to datasets with few or limited viewpoints remains a challenge. As a complementary approach, an aperture rendering GAN (AR-GAN) that employs a defocus cue was proposed. However, an AR-GAN is a CNN-based model and represents a defocus independently from a viewpoint change despite its high correlation, which is one of the reasons for its performance. As an alternative to an AR-GAN, we propose an aperture rendering NeRF (AR-NeRF), which can utilize viewpoint and defocus cues in a unified manner by representing both factors in a common ray-tracing framework. Moreover, to learn defocus-aware and defocus-independent representations in a disentangled manner, we propose aperture randomized training, for which we learn to generate images while randomizing the aperture size and latent codes independently. During our experiments, we applied AR-NeRF to various natural image datasets, including flower, bird, and face images, the results of which demonstrate the utility of AR-NeRF for unsupervised learning of the depth and defocus effects.



### Learning Feature Disentanglement and Dynamic Fusion for Recaptured Image Forensic
- **Arxiv ID**: http://arxiv.org/abs/2206.06103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06103v1)
- **Published**: 2022-06-13 12:47:13+00:00
- **Updated**: 2022-06-13 12:47:13+00:00
- **Authors**: Shuyu Miao, Lin Zheng, Hong Jin
- **Comment**: Accepted by CVPR2022 workshop
- **Journal**: None
- **Summary**: Image recapture seriously breaks the fairness of artificial intelligent (AI) systems, which deceives the system by recapturing others' images. Most of the existing recapture models can only address a single pattern of recapture (e.g., moire, edge, artifact, and others) based on the datasets with simulated recaptured images using fixed electronic devices. In this paper, we explicitly redefine image recapture forensic task as four patterns of image recapture recognition, i.e., moire recapture, edge recapture, artifact recapture, and other recapture. Meanwhile, we propose a novel Feature Disentanglement and Dynamic Fusion (FDDF) model to adaptively learn the most effective recapture feature representation for covering different recapture pattern recognition. Furthermore, we collect a large-scale Real-scene Universal Recapture (RUR) dataset containing various recapture patterns, which is about five times the number of previously published datasets. To the best of our knowledge, we are the first to propose a general model and a general real-scene large-scale dataset for recaptured image forensic. Extensive experiments show that our proposed FDDF can achieve state-of-the-art performance on the RUR dataset.



### Satellite-based high-resolution maps of cocoa planted area for Côte d'Ivoire and Ghana
- **Arxiv ID**: http://arxiv.org/abs/2206.06119v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.06119v5)
- **Published**: 2022-06-13 12:58:35+00:00
- **Updated**: 2023-05-09 08:58:11+00:00
- **Authors**: Nikolai Kalischek, Nico Lang, Cécile Renier, Rodrigo Caye Daudt, Thomas Addoah, William Thompson, Wilma J. Blaser-Hart, Rachael Garrett, Konrad Schindler, Jan D. Wegner
- **Comment**: None
- **Journal**: None
- **Summary**: C\^ote d'Ivoire and Ghana, the world's largest producers of cocoa, account for two thirds of the global cocoa production. In both countries, cocoa is the primary perennial crop, providing income to almost two million farmers. Yet precise maps of cocoa planted area are missing, hindering accurate quantification of expansion in protected areas, production and yields, and limiting information available for improved sustainability governance. Here, we combine cocoa plantation data with publicly available satellite imagery in a deep learning framework and create high-resolution maps of cocoa plantations for both countries, validated in situ. Our results suggest that cocoa cultivation is an underlying driver of over 37% and 13% of forest loss in protected areas in C\^ote d'Ivoire and Ghana, respectively, and that official reports substantially underestimate the planted area, up to 40% in Ghana. These maps serve as a crucial building block to advance understanding of conservation and economic development in cocoa producing regions.



### Brain tumour segmentation with incomplete imaging data
- **Arxiv ID**: http://arxiv.org/abs/2206.06120v3
- **DOI**: 10.1093/braincomms/fcad118
- **Categories**: **cs.CV**, cs.AI, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2206.06120v3)
- **Published**: 2022-06-13 12:58:54+00:00
- **Updated**: 2023-02-22 14:12:48+00:00
- **Authors**: James K Ruffle, Samia Mohinta, Robert J Gray, Harpreet Hyare, Parashkev Nachev
- **Comment**: 26 pages, 8 figures, 4 supplementary tables
- **Journal**: None
- **Summary**: The complex heterogeneity of brain tumours is increasingly recognized to demand data of magnitudes and richness only fully-inclusive, large-scale collections drawn from routine clinical care could plausibly offer. This is a task contemporary machine learning could facilitate, especially in neuroimaging, but its ability to deal with incomplete data common in real world clinical practice remains unknown. Here we apply state-of-the-art methods to large scale, multi-site MRI data to quantify the comparative fidelity of automated tumour segmentation models replicating the various levels of sequence availability observed in the clinical reality. We compare deep learning (nnU-Net-derived) segmentation models with all possible combinations of T1, contrast-enhanced T1, T2, and FLAIR sequences, trained and validated with five-fold cross-validation on the 2021 BraTS-RSNA glioma population of 1251 patients, with further testing on a real-world 50 patient sample diverse in not only MRI scanner and field strength, but a random selection of pre- and post-operative imaging also. Models trained on incomplete imaging data segmented lesions well, often equivalently to those trained on complete data, exhibiting Dice coefficients of 0.907 (single sequence) to 0.945 (full datasets) for whole tumours, and 0.701 (single sequence) to 0.891 (full datasets) for component tissue types. Incomplete data segmentation models could accurately detect enhancing tumour in the absence of contrast imaging, quantifying its volume with an R2 between 0.95-0.97, and were invariant to lesion morphometry. Deep learning segmentation models characterize tumours well when missing data and can even detect enhancing tissue without the use of contrast. This suggests translation to clinical practice, where incomplete data is common, may be easier than hitherto believed, and may be of value in reducing dependence on contrast use.



### Singular Value Fine-tuning: Few-shot Segmentation requires Few-parameters Fine-tuning
- **Arxiv ID**: http://arxiv.org/abs/2206.06122v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06122v2)
- **Published**: 2022-06-13 13:00:14+00:00
- **Updated**: 2022-10-10 06:30:49+00:00
- **Authors**: Yanpeng Sun, Qiang Chen, Xiangyu He, Jian Wang, Haocheng Feng, Junyu Han, Errui Ding, Jian Cheng, Zechao Li, Jingdong Wang
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: Freezing the pre-trained backbone has become a standard paradigm to avoid overfitting in few-shot segmentation. In this paper, we rethink the paradigm and explore a new regime: {\em fine-tuning a small part of parameters in the backbone}. We present a solution to overcome the overfitting problem, leading to better model generalization on learning novel classes. Our method decomposes backbone parameters into three successive matrices via the Singular Value Decomposition (SVD), then {\em only fine-tunes the singular values} and keeps others frozen. The above design allows the model to adjust feature representations on novel classes while maintaining semantic clues within the pre-trained backbone. We evaluate our {\em Singular Value Fine-tuning (SVF)} approach on various few-shot segmentation methods with different backbones. We achieve state-of-the-art results on both Pascal-5$^i$ and COCO-20$^i$ across 1-shot and 5-shot settings. Hopefully, this simple baseline will encourage researchers to rethink the role of backbone fine-tuning in few-shot settings. The source code and models will be available at https://github.com/syp2ysy/SVF.



### SyntheX: Scaling Up Learning-based X-ray Image Analysis Through In Silico Experiments
- **Arxiv ID**: http://arxiv.org/abs/2206.06127v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.06127v1)
- **Published**: 2022-06-13 13:08:41+00:00
- **Updated**: 2022-06-13 13:08:41+00:00
- **Authors**: Cong Gao, Benjamin D. Killeen, Yicheng Hu, Robert B. Grupp, Russell H. Taylor, Mehran Armand, Mathias Unberath
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial intelligence (AI) now enables automated interpretation of medical images for clinical use. However, AI's potential use for interventional images (versus those involved in triage or diagnosis), such as for guidance during surgery, remains largely untapped. This is because surgical AI systems are currently trained using post hoc analysis of data collected during live surgeries, which has fundamental and practical limitations, including ethical considerations, expense, scalability, data integrity, and a lack of ground truth. Here, we demonstrate that creating realistic simulated images from human models is a viable alternative and complement to large-scale in situ data collection. We show that training AI image analysis models on realistically synthesized data, combined with contemporary domain generalization or adaptation techniques, results in models that on real data perform comparably to models trained on a precisely matched real data training set. Because synthetic generation of training data from human-based models scales easily, we find that our model transfer paradigm for X-ray image analysis, which we refer to as SyntheX, can even outperform real data-trained models due to the effectiveness of training on a larger dataset. We demonstrate the potential of SyntheX on three clinical tasks: Hip image analysis, surgical robotic tool detection, and COVID-19 lung lesion segmentation. SyntheX provides an opportunity to drastically accelerate the conception, design, and evaluation of intelligent systems for X-ray-based medicine. In addition, simulated image environments provide the opportunity to test novel instrumentation, design complementary surgical approaches, and envision novel techniques that improve outcomes, save time, or mitigate human error, freed from the ethical and practical considerations of live human data collection.



### 2nd Place Solution for ICCV 2021 VIPriors Image Classification Challenge: An Attract-and-Repulse Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2206.06168v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06168v1)
- **Published**: 2022-06-13 13:54:33+00:00
- **Updated**: 2022-06-13 13:54:33+00:00
- **Authors**: Yilu Guo, Shicai Yang, Weijie Chen, Liang Ma, Di Xie, Shiliang Pu
- **Comment**: 2nd Place Solution for ICCV 2021 VIPriors Image Classification
  Challenge
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have achieved significant success in image classification by utilizing large-scale datasets. However, it is still of great challenge to learn from scratch on small-scale datasets efficiently and effectively. With limited training datasets, the concepts of categories will be ambiguous since the over-parameterized CNNs tend to simply memorize the dataset, leading to poor generalization capacity. Therefore, it is crucial to study how to learn more discriminative representations while avoiding over-fitting. Since the concepts of categories tend to be ambiguous, it is important to catch more individual-wise information. Thus, we propose a new framework, termed Attract-and-Repulse, which consists of Contrastive Regularization (CR) to enrich the feature representations, Symmetric Cross Entropy (SCE) to balance the fitting for different classes and Mean Teacher to calibrate label information. Specifically, SCE and CR learn discriminative representations while alleviating over-fitting by the adaptive trade-off between the information of classes (attract) and instances (repulse). After that, Mean Teacher is used to further improve the performance via calibrating more accurate soft pseudo labels. Sufficient experiments validate the effectiveness of the Attract-and-Repulse framework. Together with other strategies, such as aggressive data augmentation, TenCrop inference, and models ensembling, we achieve the second place in ICCV 2021 VIPriors Image Classification Challenge.



### Transductive CLIP with Class-Conditional Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.06177v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.06177v1)
- **Published**: 2022-06-13 14:04:57+00:00
- **Updated**: 2022-06-13 14:04:57+00:00
- **Authors**: Junchu Huang, Weijie Chen, Shicai Yang, Di Xie, Shiliang Pu, Yueting Zhuang
- **Comment**: Published in IEEE ICASSP 2022
- **Journal**: IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP) 2022
- **Summary**: Inspired by the remarkable zero-shot generalization capacity of vision-language pre-trained model, we seek to leverage the supervision from CLIP model to alleviate the burden of data labeling. However, such supervision inevitably contains the label noise, which significantly degrades the discriminative power of the classification model. In this work, we propose Transductive CLIP, a novel framework for learning a classification network with noisy labels from scratch. Firstly, a class-conditional contrastive learning mechanism is proposed to mitigate the reliance on pseudo labels and boost the tolerance to noisy labels. Secondly, ensemble labels is adopted as a pseudo label updating strategy to stabilize the training of deep neural networks with noisy labels. This framework can reduce the impact of noisy labels from CLIP model effectively by combining both techniques. Experiments on multiple benchmark datasets demonstrate the substantial improvements over other state-of-the-art methods.



### Learning a Degradation-Adaptive Network for Light Field Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2206.06214v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.06214v1)
- **Published**: 2022-06-13 14:44:46+00:00
- **Updated**: 2022-06-13 14:44:46+00:00
- **Authors**: Yingqian Wang, Zhengyu Liang, Longguang Wang, Jungang Yang, Wei An, Yulan Guo
- **Comment**: 13 pages, 9 figures
- **Journal**: None
- **Summary**: Recent years have witnessed the great advances of deep neural networks (DNNs) in light field (LF) image super-resolution (SR). However, existing DNN-based LF image SR methods are developed on a single fixed degradation (e.g., bicubic downsampling), and thus cannot be applied to super-resolve real LF images with diverse degradations. In this paper, we propose the first method to handle LF image SR with multiple degradations. In our method, a practical LF degradation model that considers blur and noise is developed to approximate the degradation process of real LF images. Then, a degradation-adaptive network (LF-DAnet) is designed to incorporate the degradation prior into the SR process. By training on LF images with multiple synthetic degradations, our method can learn to adapt to different degradations while incorporating the spatial and angular information. Extensive experiments on both synthetically degraded and real-world LFs demonstrate the effectiveness of our method. Compared with existing state-of-the-art single and LF image SR methods, our method achieves superior SR performance under a wide range of degradations, and generalizes better to real LF images. Codes and models are available at https://github.com/YingqianWang/LF-DAnet.



### Making Sense of Dependence: Efficient Black-box Explanations Using Dependence Measure
- **Arxiv ID**: http://arxiv.org/abs/2206.06219v3
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML, stat.OT
- **Links**: [PDF](http://arxiv.org/pdf/2206.06219v3)
- **Published**: 2022-06-13 14:49:17+00:00
- **Updated**: 2022-09-27 14:17:31+00:00
- **Authors**: Paul Novello, Thomas Fel, David Vigouroux
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: This paper presents a new efficient black-box attribution method based on Hilbert-Schmidt Independence Criterion (HSIC), a dependence measure based on Reproducing Kernel Hilbert Spaces (RKHS). HSIC measures the dependence between regions of an input image and the output of a model based on kernel embeddings of distributions. It thus provides explanations enriched by RKHS representation capabilities. HSIC can be estimated very efficiently, significantly reducing the computational cost compared to other black-box attribution methods. Our experiments show that HSIC is up to 8 times faster than the previous best black-box attribution methods while being as faithful. Indeed, we improve or match the state-of-the-art of both black-box and white-box attribution methods for several fidelity metrics on Imagenet with various recent model architectures. Importantly, we show that these advances can be transposed to efficiently and faithfully explain object detection models such as YOLOv4. Finally, we extend the traditional attribution methods by proposing a new kernel enabling an ANOVA-like orthogonal decomposition of importance scores based on HSIC, allowing us to evaluate not only the importance of each image patch but also the importance of their pairwise interactions. Our implementation is available at https://github.com/paulnovello/HSIC-Attribution-Method.



### Prostate Cancer Malignancy Detection and localization from mpMRI using auto-Deep Learning: One Step Closer to Clinical Utilization
- **Arxiv ID**: http://arxiv.org/abs/2206.06235v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.06235v1)
- **Published**: 2022-06-13 15:14:59+00:00
- **Updated**: 2022-06-13 15:14:59+00:00
- **Authors**: Weiwei Zong, Eric Carver, Simeng Zhu, Eric Schaff, Daniel Chapman, Joon Lee, Hassan Bagher Ebadian, Indrin Chetty, Benjamin Movsas, Winston Wen, Tarik Alafif, Xiangyun Zong
- **Comment**: arXiv admin note: text overlap with arXiv:1903.12331
- **Journal**: None
- **Summary**: Automatic diagnosis of malignant prostate cancer patients from mpMRI has been studied heavily in the past years. Model interpretation and domain drift have been the main road blocks for clinical utilization. As an extension from our previous work where we trained a customized convolutional neural network on a public cohort with 201 patients and the cropped 2D patches around the region of interest were used as the input, the cropped 2.5D slices of the prostate glands were used as the input, and the optimal model were searched in the model space using autoKeras. Something different was peripheral zone (PZ) and central gland (CG) were trained and tested separately, the PZ detector and CG detector were demonstrated effectively in highlighting the most suspicious slices out of a sequence, hopefully to greatly ease the workload for the physicians.



### Transformer Lesion Tracker
- **Arxiv ID**: http://arxiv.org/abs/2206.06252v1
- **DOI**: 10.1007/978-3-031-16446-0_19
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06252v1)
- **Published**: 2022-06-13 15:35:24+00:00
- **Updated**: 2022-06-13 15:35:24+00:00
- **Authors**: Wen Tang, Han Kang, Haoyue Zhang, Pengxin Yu, Corey W. Arnold, Rongguo Zhang
- **Comment**: Accepted MICCAI 2022
- **Journal**: None
- **Summary**: Evaluating lesion progression and treatment response via longitudinal lesion tracking plays a critical role in clinical practice. Automated approaches for this task are motivated by prohibitive labor costs and time consumption when lesion matching is done manually. Previous methods typically lack the integration of local and global information. In this work, we propose a transformer-based approach, termed Transformer Lesion Tracker (TLT). Specifically, we design a Cross Attention-based Transformer (CAT) to capture and combine both global and local information to enhance feature extraction. We also develop a Registration-based Anatomical Attention Module (RAAM) to introduce anatomical information to CAT so that it can focus on useful feature knowledge. A Sparse Selection Strategy (SSS) is presented for selecting features and reducing memory footprint in Transformer training. In addition, we use a global regression to further improve model performance. We conduct experiments on a public dataset to show the superiority of our method and find that our model performance has improved the average Euclidean center error by at least 14.3% (6mm vs. 7mm) compared with the state-of-the-art (SOTA). Code is available at https://github.com/TangWen920812/TLT.



### RPLHR-CT Dataset and Transformer Baseline for Volumetric Super-Resolution from CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2206.06253v1
- **DOI**: 10.1007/978-3-031-16446-0_33
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.06253v1)
- **Published**: 2022-06-13 15:35:59+00:00
- **Updated**: 2022-06-13 15:35:59+00:00
- **Authors**: Pengxin Yu, Haoyue Zhang, Han Kang, Wen Tang, Corey W. Arnold, Rongguo Zhang
- **Comment**: Accepted MICCAI 2022
- **Journal**: None
- **Summary**: In clinical practice, anisotropic volumetric medical images with low through-plane resolution are commonly used due to short acquisition time and lower storage cost. Nevertheless, the coarse resolution may lead to difficulties in medical diagnosis by either physicians or computer-aided diagnosis algorithms. Deep learning-based volumetric super-resolution (SR) methods are feasible ways to improve resolution, with convolutional neural networks (CNN) at their core. Despite recent progress, these methods are limited by inherent properties of convolution operators, which ignore content relevance and cannot effectively model long-range dependencies. In addition, most of the existing methods use pseudo-paired volumes for training and evaluation, where pseudo low-resolution (LR) volumes are generated by a simple degradation of their high-resolution (HR) counterparts. However, the domain gap between pseudo- and real-LR volumes leads to the poor performance of these methods in practice. In this paper, we build the first public real-paired dataset RPLHR-CT as a benchmark for volumetric SR, and provide baseline results by re-implementing four state-of-the-art CNN-based methods. Considering the inherent shortcoming of CNN, we also propose a transformer volumetric super-resolution network (TVSRN) based on attention mechanisms, dispensing with convolutions entirely. This is the first research to use a pure transformer for CT volumetric SR. The experimental results show that TVSRN significantly outperforms all baselines on both PSNR and SSIM. Moreover, the TVSRN method achieves a better trade-off between the image quality, the number of parameters, and the running time. Data and code are available at https://github.com/smilenaxx/RPLHR-CT.



### Featurized Query R-CNN
- **Arxiv ID**: http://arxiv.org/abs/2206.06258v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06258v3)
- **Published**: 2022-06-13 15:40:19+00:00
- **Updated**: 2022-06-20 07:04:34+00:00
- **Authors**: Wenqiang Zhang, Tianheng Cheng, Xinggang Wang, Shaoyu Chen, Qian Zhang, Wenyu Liu
- **Comment**: Tech Report
- **Journal**: None
- **Summary**: The query mechanism introduced in the DETR method is changing the paradigm of object detection and recently there are many query-based methods have obtained strong object detection performance. However, the current query-based detection pipelines suffer from the following two issues. Firstly, multi-stage decoders are required to optimize the randomly initialized object queries, incurring a large computation burden. Secondly, the queries are fixed after training, leading to unsatisfying generalization capability. To remedy the above issues, we present featurized object queries predicted by a query generation network in the well-established Faster R-CNN framework and develop a Featurized Query R-CNN. Extensive experiments on the COCO dataset show that our Featurized Query R-CNN obtains the best speed-accuracy trade-off among all R-CNN detectors, including the recent state-of-the-art Sparse R-CNN detector. The code is available at {https://github.com/hustvl/Featurized-QueryRCNN.



### Automatic Polyp Segmentation with Multiple Kernel Dilated Convolution Network
- **Arxiv ID**: http://arxiv.org/abs/2206.06264v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.06264v2)
- **Published**: 2022-06-13 15:47:38+00:00
- **Updated**: 2022-06-23 16:34:13+00:00
- **Authors**: Nikhil Kumar Tomar, Abhishek Srivastava, Ulas Bagci, Debesh Jha
- **Comment**: None
- **Journal**: Published CBMS 2022
- **Summary**: The detection and removal of precancerous polyps through colonoscopy is the primary technique for the prevention of colorectal cancer worldwide. However, the miss rate of colorectal polyp varies significantly among the endoscopists. It is well known that a computer-aided diagnosis (CAD) system can assist endoscopists in detecting colon polyps and minimize the variation among endoscopists. In this study, we introduce a novel deep learning architecture, named MKDCNet, for automatic polyp segmentation robust to significant changes in polyp data distribution. MKDCNet is simply an encoder-decoder neural network that uses the pre-trained ResNet50 as the encoder and novel multiple kernel dilated convolution (MKDC) block that expands the field of view to learn more robust and heterogeneous representation. Extensive experiments on four publicly available polyp datasets and cell nuclei dataset show that the proposed MKDCNet outperforms the state-of-the-art methods when trained and tested on the same dataset as well when tested on unseen polyp datasets from different distributions. With rich results, we demonstrated the robustness of the proposed architecture. From an efficiency perspective, our algorithm can process at (approx 45) frames per second on RTX 3090 GPU. MKDCNet can be a strong benchmark for building real-time systems for clinical colonoscopies. The code of the proposed MKDCNet is available at https://github.com/nikhilroxtomar/MKDCNet.



### MMMNA-Net for Overall Survival Time Prediction of Brain Tumor Patients
- **Arxiv ID**: http://arxiv.org/abs/2206.06267v1
- **DOI**: 10.1109/EMBC48229.2022.9871639
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.06267v1)
- **Published**: 2022-06-13 15:51:53+00:00
- **Updated**: 2022-06-13 15:51:53+00:00
- **Authors**: Wen Tang, Haoyue Zhang, Pengxin Yu, Han Kang, Rongguo Zhang
- **Comment**: Accepted EMBC 2022
- **Journal**: None
- **Summary**: Overall survival (OS) time is one of the most important evaluation indices for gliomas situations. Multimodal Magnetic Resonance Imaging (MRI) scans play an important role in the study of glioma prognosis OS time. Several deep learning-based methods are proposed for the OS time prediction on multi-modal MRI problems. However, these methods usually fuse multi-modal information at the beginning or at the end of the deep learning networks and lack the fusion of features from different scales. In addition, the fusion at the end of networks always adapts global with global (eg. fully connected after concatenation of global average pooling output) or local with local (eg. bilinear pooling), which loses the information of local with global. In this paper, we propose a novel method for multi-modal OS time prediction of brain tumor patients, which contains an improved nonlocal features fusion module introduced on different scales. Our method obtains a relative 8.76% improvement over the current state-of-art method (0.6989 vs. 0.6426 on accuracy). Extensive testing demonstrates that our method could adapt to situations with missing modalities. The code is available at https://github.com/TangWen920812/mmmna-net.



### Learning Joint Surface Atlases
- **Arxiv ID**: http://arxiv.org/abs/2206.06273v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.06273v1)
- **Published**: 2022-06-13 16:01:06+00:00
- **Updated**: 2022-06-13 16:01:06+00:00
- **Authors**: Theo Deprelle, Thibault Groueix, Noam Aigerman, Vladimir G. Kim, Mathieu Aubry
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes new techniques for learning atlas-like representations of 3D surfaces, i.e. homeomorphic transformations from a 2D domain to surfaces. Compared to prior work, we propose two major contributions. First, instead of mapping a fixed 2D domain, such as a set of square patches, to the surface, we learn a continuous 2D domain with arbitrary topology by optimizing a point sampling distribution represented as a mixture of Gaussians. Second, we learn consistent mappings in both directions: charts, from the 3D surface to 2D domain, and parametrizations, their inverse. We demonstrate that this improves the quality of the learned surface representation, as well as its consistency in a collection of related shapes. It thus leads to improvements for applications such as correspondence estimation, texture transfer, and consistent UV mapping. As an additional technical contribution, we outline that, while incorporating normal consistency has clear benefits, it leads to issues in the optimization, and that these issues can be mitigated using a simple repulsive regularization. We demonstrate that our contributions provide better surface representation than existing baselines.



### From a few Accurate 2D Correspondences to 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2206.08749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.08749v1)
- **Published**: 2022-06-13 16:15:53+00:00
- **Updated**: 2022-06-13 16:15:53+00:00
- **Authors**: Trung-Kien Le, Ping Li
- **Comment**: None
- **Journal**: None
- **Summary**: Key points, correspondences, projection matrices, point clouds and dense clouds are the skeletons in image-based 3D reconstruction, of which point clouds have the important role in generating a realistic and natural model for a 3D reconstructed object. To achieve a good 3D reconstruction, the point clouds must be almost everywhere in the surface of the object. In this article, with a main purpose to build the point clouds covering the entire surface of the object, we propose a new feature named a geodesic feature or geo-feature. Based on the new geo-feature, if there are several (given) initial world points on the object's surface along with all accurately estimated projection matrices, some new world points on the geodesics connecting any two of these given world points will be reconstructed. Then the regions on the surface bordering by these initial world points will be covered by the point clouds. Thus, if the initial world points are around the surface, the point clouds will cover the entire surface.   This article proposes a new method to estimate the world points and projection matrices from their correspondences. This method derives the closed-form and iterative solutions for the world points and projection matrices and proves that when the number of world points is less than seven and the number of images is at least five, the proposed solutions are global optimal. We propose an algorithm named World points from their Correspondences (WPfC) to estimate the world points and projection matrices from their correspondences, and another algorithm named Creating Point Clouds (CrPC) to create the point clouds from the world points and projection matrices given by the first algorithm.



### Silver-Bullet-3D at ManiSkill 2021: Learning-from-Demonstrations and Heuristic Rule-based Methods for Object Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2206.06289v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.06289v1)
- **Published**: 2022-06-13 16:20:42+00:00
- **Updated**: 2022-06-13 16:20:42+00:00
- **Authors**: Yingwei Pan, Yehao Li, Yiheng Zhang, Qi Cai, Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei
- **Comment**: Accepted by ICLR 2022 Workshop on Generalizable Policy Learning in
  Physical World. Top-performing systems for both no interaction and no
  restriction tracks in SAPIEN ManiSkill Challenge 2021. The source code and
  model are publicly available at: https://github.com/caiqi/Silver-Bullet-3D/
- **Journal**: None
- **Summary**: This paper presents an overview and comparative analysis of our systems designed for the following two tracks in SAPIEN ManiSkill Challenge 2021:   No Interaction Track: The No Interaction track targets for learning policies from pre-collected demonstration trajectories. We investigate both imitation learning-based approach, i.e., imitating the observed behavior using classical supervised learning techniques, and offline reinforcement learning-based approaches, for this track. Moreover, the geometry and texture structures of objects and robotic arms are exploited via Transformer-based networks to facilitate imitation learning.   No Restriction Track: In this track, we design a Heuristic Rule-based Method (HRM) to trigger high-quality object manipulation by decomposing the task into a series of sub-tasks. For each sub-task, the simple rule-based controlling strategies are adopted to predict actions that can be applied to robotic arms.   To ease the implementations of our systems, all the source codes and pre-trained models are available at \url{https://github.com/caiqi/Silver-Bullet-3D/}.



### Exploring Structure-aware Transformer over Interaction Proposals for Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.06291v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2206.06291v1)
- **Published**: 2022-06-13 16:21:08+00:00
- **Updated**: 2022-06-13 16:21:08+00:00
- **Authors**: Yong Zhang, Yingwei Pan, Ting Yao, Rui Huang, Tao Mei, Chang-Wen Chen
- **Comment**: CVPR 2022; Code is publicly available at:
  https://github.com/zyong812/STIP
- **Journal**: None
- **Summary**: Recent high-performing Human-Object Interaction (HOI) detection techniques have been highly influenced by Transformer-based object detector (i.e., DETR). Nevertheless, most of them directly map parametric interaction queries into a set of HOI predictions through vanilla Transformer in a one-stage manner. This leaves rich inter- or intra-interaction structure under-exploited. In this work, we design a novel Transformer-style HOI detector, i.e., Structure-aware Transformer over Interaction Proposals (STIP), for HOI detection. Such design decomposes the process of HOI set prediction into two subsequent phases, i.e., an interaction proposal generation is first performed, and then followed by transforming the non-parametric interaction proposals into HOI predictions via a structure-aware Transformer. The structure-aware Transformer upgrades vanilla Transformer by encoding additionally the holistically semantic structure among interaction proposals as well as the locally spatial structure of human/object within each interaction proposal, so as to strengthen HOI predictions. Extensive experiments conducted on V-COCO and HICO-DET benchmarks have demonstrated the effectiveness of STIP, and superior results are reported when comparing with the state-of-the-art HOI detectors. Source code is available at \url{https://github.com/zyong812/STIP}.



### MLP-3D: A MLP-like 3D Architecture with Grouped Time Mixing
- **Arxiv ID**: http://arxiv.org/abs/2206.06292v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2206.06292v1)
- **Published**: 2022-06-13 16:21:33+00:00
- **Updated**: 2022-06-13 16:21:33+00:00
- **Authors**: Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Tao Mei
- **Comment**: CVPR 2022; Code is publicly available at:
  https://github.com/ZhaofanQiu/MLP-3D
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have been regarded as the go-to models for visual recognition. More recently, convolution-free networks, based on multi-head self-attention (MSA) or multi-layer perceptrons (MLPs), become more and more popular. Nevertheless, it is not trivial when utilizing these newly-minted networks for video recognition due to the large variations and complexities in video data. In this paper, we present MLP-3D networks, a novel MLP-like 3D architecture for video recognition. Specifically, the architecture consists of MLP-3D blocks, where each block contains one MLP applied across tokens (i.e., token-mixing MLP) and one MLP applied independently to each token (i.e., channel MLP). By deriving the novel grouped time mixing (GTM) operations, we equip the basic token-mixing MLP with the ability of temporal modeling. GTM divides the input tokens into several temporal groups and linearly maps the tokens in each group with the shared projection matrix. Furthermore, we devise several variants of GTM with different grouping strategies, and compose each variant in different blocks of MLP-3D network by greedy architecture search. Without the dependence on convolutions or attention mechanisms, our MLP-3D networks achieves 68.5\%/81.4\% top-1 accuracy on Something-Something V2 and Kinetics-400 datasets, respectively. Despite with fewer computations, the results are comparable to state-of-the-art widely-used 3D CNNs and video transformers. Source code is available at https://github.com/ZhaofanQiu/MLP-3D.



### Learning Domain Adaptive Object Detection with Probabilistic Teacher
- **Arxiv ID**: http://arxiv.org/abs/2206.06293v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.06293v1)
- **Published**: 2022-06-13 16:24:22+00:00
- **Updated**: 2022-06-13 16:24:22+00:00
- **Authors**: Meilin Chen, Weijie Chen, Shicai Yang, Jie Song, Xinchao Wang, Lei Zhang, Yunfeng Yan, Donglian Qi, Yueting Zhuang, Di Xie, Shiliang Pu
- **Comment**: To appear in ICML 2022. Code is coming soon:
  https://github.com/hikvision-research/ProbabilisticTeacher
- **Journal**: International Conference on Machine Learning (ICML), 2022
- **Summary**: Self-training for unsupervised domain adaptive object detection is a challenging task, of which the performance depends heavily on the quality of pseudo boxes. Despite the promising results, prior works have largely overlooked the uncertainty of pseudo boxes during self-training. In this paper, we present a simple yet effective framework, termed as Probabilistic Teacher (PT), which aims to capture the uncertainty of unlabeled target data from a gradually evolving teacher and guides the learning of a student in a mutually beneficial manner. Specifically, we propose to leverage the uncertainty-guided consistency training to promote classification adaptation and localization adaptation, rather than filtering pseudo boxes via an elaborate confidence threshold. In addition, we conduct anchor adaptation in parallel with localization adaptation, since anchor can be regarded as a learnable parameter. Together with this framework, we also present a novel Entropy Focal Loss (EFL) to further facilitate the uncertainty-guided self-training. Equipped with EFL, PT outperforms all previous baselines by a large margin and achieve new state-of-the-arts.



### SNeS: Learning Probably Symmetric Neural Surfaces from Incomplete Data
- **Arxiv ID**: http://arxiv.org/abs/2206.06340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06340v1)
- **Published**: 2022-06-13 17:37:50+00:00
- **Updated**: 2022-06-13 17:37:50+00:00
- **Authors**: Eldar Insafutdinov, Dylan Campbell, João F. Henriques, Andrea Vedaldi
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: We present a method for the accurate 3D reconstruction of partly-symmetric objects. We build on the strengths of recent advances in neural reconstruction and rendering such as Neural Radiance Fields (NeRF). A major shortcoming of such approaches is that they fail to reconstruct any part of the object which is not clearly visible in the training image, which is often the case for in-the-wild images and videos. When evidence is lacking, structural priors such as symmetry can be used to complete the missing information. However, exploiting such priors in neural rendering is highly non-trivial: while geometry and non-reflective materials may be symmetric, shadows and reflections from the ambient scene are not symmetric in general. To address this, we apply a soft symmetry constraint to the 3D geometry and material properties, having factored appearance into lighting, albedo colour and reflectivity. We evaluate our method on the recently introduced CO3D dataset, focusing on the car category due to the challenge of reconstructing highly-reflective materials. We show that it can reconstruct unobserved regions with high fidelity and render high-quality novel view images.



### Unsupervised inter-frame motion correction for whole-body dynamic PET using convolutional long short-term memory in a convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2206.06341v1
- **DOI**: 10.1016/j.media.2022.102524
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2206.06341v1)
- **Published**: 2022-06-13 17:38:16+00:00
- **Updated**: 2022-06-13 17:38:16+00:00
- **Authors**: Xueqi Guo, Bo Zhou, David Pigg, Bruce Spottiswoode, Michael E. Casey, Chi Liu, Nicha C. Dvornek
- **Comment**: Preprint submitted to Medical Image Analysis
- **Journal**: None
- **Summary**: Subject motion in whole-body dynamic PET introduces inter-frame mismatch and seriously impacts parametric imaging. Traditional non-rigid registration methods are generally computationally intense and time-consuming. Deep learning approaches are promising in achieving high accuracy with fast speed, but have yet been investigated with consideration for tracer distribution changes or in the whole-body scope. In this work, we developed an unsupervised automatic deep learning-based framework to correct inter-frame body motion. The motion estimation network is a convolutional neural network with a combined convolutional long short-term memory layer, fully utilizing dynamic temporal features and spatial information. Our dataset contains 27 subjects each under a 90-min FDG whole-body dynamic PET scan. With 9-fold cross-validation, compared with both traditional and deep learning baselines, we demonstrated that the proposed network obtained superior performance in enhanced qualitative and quantitative spatial alignment between parametric $K_{i}$ and $V_{b}$ images and in significantly reduced parametric fitting error. We also showed the potential of the proposed motion correction method for impacting downstream analysis of the estimated parametric images, improving the ability to distinguish malignant from benign hypermetabolic regions of interest. Once trained, the motion estimation inference time of our proposed network was around 460 times faster than the conventional registration baseline, showing its potential to be easily applied in clinical settings.



### Bringing Image Scene Structure to Video via Frame-Clip Consistency of Object Tokens
- **Arxiv ID**: http://arxiv.org/abs/2206.06346v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06346v3)
- **Published**: 2022-06-13 17:45:05+00:00
- **Updated**: 2022-11-29 12:40:50+00:00
- **Authors**: Elad Ben-Avraham, Roei Herzig, Karttikeya Mangalam, Amir Bar, Anna Rohrbach, Leonid Karlinsky, Trevor Darrell, Amir Globerson
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Recent action recognition models have achieved impressive results by integrating objects, their locations and interactions. However, obtaining dense structured annotations for each frame is tedious and time-consuming, making these methods expensive to train and less scalable. At the same time, if a small set of annotated images is available, either within or outside the domain of interest, how could we leverage these for a video downstream task? We propose a learning framework StructureViT (SViT for short), which demonstrates how utilizing the structure of a small number of images only available during training can improve a video model. SViT relies on two key insights. First, as both images and videos contain structured information, we enrich a transformer model with a set of \emph{object tokens} that can be used across images and videos. Second, the scene representations of individual frames in video should "align" with those of still images. This is achieved via a \emph{Frame-Clip Consistency} loss, which ensures the flow of structured information between images and videos. We explore a particular instantiation of scene structure, namely a \emph{Hand-Object Graph}, consisting of hands and objects with their locations as nodes, and physical relations of contact/no-contact as edges. SViT shows strong performance improvements on multiple video understanding tasks and datasets. Furthermore, it won in the Ego4D CVPR'22 Object State Localization challenge. For code and pretrained models, visit the project page at \url{https://eladb3.github.io/SViT/}



### EnergyMatch: Energy-based Pseudo-Labeling for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.06359v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.06359v1)
- **Published**: 2022-06-13 17:55:07+00:00
- **Updated**: 2022-06-13 17:55:07+00:00
- **Authors**: Zhuoran Yu, Yin Li, Yong Jae Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Recent state-of-the-art methods in semi-supervised learning (SSL) combine consistency regularization with confidence-based pseudo-labeling. To obtain high-quality pseudo-labels, a high confidence threshold is typically adopted. However, it has been shown that softmax-based confidence scores in deep networks can be arbitrarily high for samples far from the training data, and thus, the pseudo-labels for even high-confidence unlabeled samples may still be unreliable. In this work, we present a new perspective of pseudo-labeling: instead of relying on model confidence, we instead measure whether an unlabeled sample is likely to be "in-distribution"; i.e., close to the current training data. To classify whether an unlabeled sample is "in-distribution" or "out-of-distribution", we adopt the energy score from out-of-distribution detection literature. As training progresses and more unlabeled samples become in-distribution and contribute to training, the combined labeled and pseudo-labeled data can better approximate the true distribution to improve the model. Experiments demonstrate that our energy-based pseudo-labeling method, albeit conceptually simple, significantly outperforms confidence-based methods on imbalanced SSL benchmarks, and achieves competitive performance on class-balanced data. For example, it produces a 4-6% absolute accuracy improvement on CIFAR10-LT when the imbalance ratio is higher than 50. When combined with state-of-the-art long-tailed SSL methods, further improvements are attained.



### ARF: Artistic Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2206.06360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06360v1)
- **Published**: 2022-06-13 17:55:31+00:00
- **Updated**: 2022-06-13 17:55:31+00:00
- **Authors**: Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu, Eli Shechtman, Noah Snavely
- **Comment**: Project page: https://www.cs.cornell.edu/projects/arf/
- **Journal**: None
- **Summary**: We present a method for transferring the artistic features of an arbitrary style image to a 3D scene. Previous methods that perform 3D stylization on point clouds or meshes are sensitive to geometric reconstruction errors for complex real-world scenes. Instead, we propose to stylize the more robust radiance field representation. We find that the commonly used Gram matrix-based loss tends to produce blurry results without faithful brushstrokes, and introduce a nearest neighbor-based loss that is highly effective at capturing style details while maintaining multi-view consistency. We also propose a novel deferred back-propagation method to enable optimization of memory-intensive radiance fields using style losses defined on full-resolution rendered images. Our extensive evaluation demonstrates that our method outperforms baselines by generating artistic appearance that more closely resembles the style image. Please check our project page for video results and open-source implementations: https://www.cs.cornell.edu/projects/arf/ .



### Discovering Object Masks with Transformers for Unsupervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.06363v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.06363v1)
- **Published**: 2022-06-13 17:59:43+00:00
- **Updated**: 2022-06-13 17:59:43+00:00
- **Authors**: Wouter Van Gansbeke, Simon Vandenhende, Luc Van Gool
- **Comment**: Code: https://github.com/wvangansbeke/MaskDistill
- **Journal**: None
- **Summary**: The task of unsupervised semantic segmentation aims to cluster pixels into semantically meaningful groups. Specifically, pixels assigned to the same cluster should share high-level semantic properties like their object or part category. This paper presents MaskDistill: a novel framework for unsupervised semantic segmentation based on three key ideas. First, we advocate a data-driven strategy to generate object masks that serve as a pixel grouping prior for semantic segmentation. This approach omits handcrafted priors, which are often designed for specific scene compositions and limit the applicability of competing frameworks. Second, MaskDistill clusters the object masks to obtain pseudo-ground-truth for training an initial object segmentation model. Third, we leverage this model to filter out low-quality object masks. This strategy mitigates the noise in our pixel grouping prior and results in a clean collection of masks which we use to train a final segmentation model. By combining these components, we can considerably outperform previous works for unsupervised semantic segmentation on PASCAL (+11% mIoU) and COCO (+4% mask AP50). Interestingly, as opposed to existing approaches, our framework does not latch onto low-level image cues and is not limited to object-centric datasets. The code and models will be made available.



### Compositional Mixture Representations for Vision and Text
- **Arxiv ID**: http://arxiv.org/abs/2206.06404v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.06404v1)
- **Published**: 2022-06-13 18:16:40+00:00
- **Updated**: 2022-06-13 18:16:40+00:00
- **Authors**: Stephan Alaniz, Marco Federici, Zeynep Akata
- **Comment**: Workshop on Learning with Limited Labelled Data for Image and Video
  Understanding (L3D-IVU), CVPR 2022
- **Journal**: None
- **Summary**: Learning a common representation space between vision and language allows deep networks to relate objects in the image to the corresponding semantic meaning. We present a model that learns a shared Gaussian mixture representation imposing the compositionality of the text onto the visual domain without having explicit location supervision. By combining the spatial transformer with a representation learning approach we learn to split images into separately encoded patches to associate visual and textual representations in an interpretable manner. On variations of MNIST and CIFAR10, our model is able to perform weakly supervised object detection and demonstrates its ability to extrapolate to unseen combination of objects.



### GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2206.06420v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.06420v3)
- **Published**: 2022-06-13 18:59:31+00:00
- **Updated**: 2023-04-21 13:45:17+00:00
- **Authors**: Wenhao Li, Hong Liu, Tianyu Guo, Runwei Ding, Hao Tang
- **Comment**: Open Sourced
- **Journal**: None
- **Summary**: Modern multi-layer perceptron (MLP) models have shown competitive results in learning visual representations without self-attention. However, existing MLP models are not good at capturing local details and lack prior knowledge of human body configurations, which limits their modeling power for skeletal representation learning. To address these issues, we propose a simple yet effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines MLPs and graph convolutional networks (GCNs) in a global-local-graphical unified architecture for 3D human pose estimation. GraphMLP incorporates the graph structure of human bodies into an MLP model to meet the domain-specific demand of the 3D human pose, while allowing for both local and global spatial interactions. Furthermore, we propose to flexibly and efficiently extend the GraphMLP to the video domain and show that complex temporal dynamics can be effectively modeled in a simple way with negligible computational cost gains in the sequence length. To the best of our knowledge, this is the first MLP-Like architecture for 3D human pose estimation in a single frame and a video sequence. Extensive experiments show that the proposed GraphMLP achieves state-of-the-art performance on two datasets, i.e., Human3.6M and MPI-INF-3DHP. Code and models are available at https://github.com/Vegetebird/GraphMLP.



### A Multi-purpose Realistic Haze Benchmark with Quantifiable Haze Levels and Ground Truth
- **Arxiv ID**: http://arxiv.org/abs/2206.06427v3
- **DOI**: 10.1109/TIP.2023.3245994
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06427v3)
- **Published**: 2022-06-13 19:14:06+00:00
- **Updated**: 2023-02-11 20:36:36+00:00
- **Authors**: Priya Narayanan, Xin Hu, Zhenyu Wu, Matthew D Thielke, John G Rogers, Andre V Harrison, John A D'Agostino, James D Brown, Long P Quang, James R Uplinger, Heesung Kwon, Zhangyang Wang
- **Comment**: This paper has been ACCEPTED for publication as a REGULAR paper in
  the IEEE Transactions on Image Processing (TIP)
- **Journal**: None
- **Summary**: Imagery collected from outdoor visual environments is often degraded due to the presence of dense smoke or haze. A key challenge for research in scene understanding in these degraded visual environments (DVE) is the lack of representative benchmark datasets. These datasets are required to evaluate state-of-the-art vision algorithms (e.g., detection and tracking) in degraded settings. In this paper, we address some of these limitations by introducing the first realistic hazy image benchmark, from both aerial and ground view, with paired haze-free images, and in-situ haze density measurements. This dataset was produced in a controlled environment with professional smoke generating machines that covered the entire scene, and consists of images captured from the perspective of both an unmanned aerial vehicle (UAV) and an unmanned ground vehicle (UGV). We also evaluate a set of representative state-of-the-art dehazing approaches as well as object detectors on the dataset. The full dataset presented in this paper, including the ground truth object classification bounding boxes and haze density measurements, is provided for the community to evaluate their algorithms at: https://a2i2-archangel.vision. A subset of this dataset has been used for the ``Object Detection in Haze'' Track of CVPR UG2 2022 challenge at http://cvpr2022.ug2challenge.org/track1.html.



### ReViSe: Remote Vital Signs Measurement Using Smartphone Camera
- **Arxiv ID**: http://arxiv.org/abs/2206.08748v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2206.08748v2)
- **Published**: 2022-06-13 19:20:11+00:00
- **Updated**: 2022-12-22 15:09:19+00:00
- **Authors**: Donghao Qiao, Amtul Haq Ayesha, Farhana Zulkernine, Raihan Masroor, Nauman Jaffar
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an end-to-end framework to measure people's vital signs including Heart Rate (HR), Heart Rate Variability (HRV), Oxygen Saturation (SpO2) and Blood Pressure (BP) based on the rPPG methodology from the video of a user's face captured with a smartphone camera. We extract face landmarks with a deep learning-based neural network model in real-time. Multiple face patches also called Regions-of-Interest (RoIs) are extracted by using the predicted face landmarks. Several filters are applied to reduce the noise from the RoIs in the extracted cardiac signals called Blood Volume Pulse (BVP) signal. The measurements of HR, HRV and SpO2 are validated on two public rPPG datasets namely the TokyoTech rPPG and the Pulse Rate Detection (PURE) datasets, on which our models achieved the following Mean Absolute Errors (MAE): a) for HR, 1.73Beats-Per-Minute (bpm) and 3.95bpm respectively; b) for HRV, 18.55ms and 25.03ms respectively, and c) for SpO2, an MAE of 1.64% on the PURE dataset. We validated our end-to-end rPPG framework, ReViSe, in daily living environment, and thereby created the Video-HR dataset. Our HR estimation model achieved an MAE of 2.49bpm on this dataset. Since no publicly available rPPG datasets existed for BP measurement with face videos, we used a dataset with signals from fingertip sensor to train our deep learning-based BP estimation model and also created our own video dataset, Video-BP. On our Video-BP dataset, our BP estimation model achieved an MAE of 6.7mmHg for Systolic Blood Pressure (SBP), and an MAE of 9.6mmHg for Diastolic Blood Pressure (DBP). ReViSe framework has been validated on datasets with videos recorded in daily living environment as opposed to less noisy laboratory environment as reported by most state-of-the-art techniques.



### A Training Method For VideoPose3D With Ideology of Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.06430v1
- **DOI**: 10.1109/CONF-SPML54095.2021.00041
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06430v1)
- **Published**: 2022-06-13 19:25:27+00:00
- **Updated**: 2022-06-13 19:25:27+00:00
- **Authors**: Hao Bai
- **Comment**: Published by IEEE, on conference CONF-SPML
- **Journal**: None
- **Summary**: Action recognition and pose estimation from videos are closely related to understand human motions, but more literature focuses on how to solve pose estimation tasks alone from action recognition. This research shows a faster and more flexible training method for VideoPose3D which is based on action recognition. This model is fed with the same type of action as the type that will be estimated, and different types of actions can be trained separately. Evidence has shown that, for common pose-estimation tasks, this model requires a relatively small amount of data to carry out similar results with the original research, and for action-oriented tasks, it outperforms the original research by 4.5% with a limited receptive field size and training epoch on Velocity Error of MPJPE. This model can handle both action-oriented and common pose-estimation problems.



### ICP Algorithm: Theory, Practice And Its SLAM-oriented Taxonomy
- **Arxiv ID**: http://arxiv.org/abs/2206.06435v1
- **DOI**: 10.54254/2755-2721/2/20220512
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06435v1)
- **Published**: 2022-06-13 19:29:25+00:00
- **Updated**: 2022-06-13 19:29:25+00:00
- **Authors**: Hao Bai
- **Comment**: Accepted by CONF-CDS'22
- **Journal**: None
- **Summary**: The Iterative Closest Point (ICP) algorithm is one of the most important algorithms for geometric alignment of three-dimensional surface registration, which is frequently used in computer vision tasks, including the Simultaneous Localization And Mapping (SLAM) tasks. In this paper, we illustrate the theoretical principles of the ICP algorithm, how it can be used in surface registration tasks, and the traditional taxonomy of the variants of the ICP algorithm. As SLAM is becoming a popular topic, we also introduce a SLAM-oriented taxonomy of the ICP algorithm, based on the characteristics of each type of SLAM task, including whether the SLAM task is online or not and whether the landmarks are present as features in the SLAM task. We make a synthesis of each type of SLAM task by comparing several up-to-date research papers and analyzing their implementation details.



### Fitting Segmentation Networks on Varying Image Resolutions using Splatting
- **Arxiv ID**: http://arxiv.org/abs/2206.06445v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.06445v2)
- **Published**: 2022-06-13 19:53:02+00:00
- **Updated**: 2022-06-15 10:07:37+00:00
- **Authors**: Mikael Brudfors, Yael Balbastre, John Ashburner, Geraint Rees, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso
- **Comment**: Accepted for MIUA 2022
- **Journal**: None
- **Summary**: Data used in image segmentation are not always defined on the same grid. This is particularly true for medical images, where the resolution, field-of-view and orientation can differ across channels and subjects. Images and labels are therefore commonly resampled onto the same grid, as a pre-processing step. However, the resampling operation introduces partial volume effects and blurring, thereby changing the effective resolution and reducing the contrast between structures. In this paper we propose a splat layer, which automatically handles resolution mismatches in the input data. This layer pushes each image onto a mean space where the forward pass is performed. As the splat operator is the adjoint to the resampling operator, the mean-space prediction can be pulled back to the native label space, where the loss function is computed. Thus, the need for explicit resolution adjustment using interpolation is removed. We show on two publicly available datasets, with simulated and real multi-modal magnetic resonance images, that this model improves segmentation results compared to resampling as a pre-processing step.



### Automated Coronary Calcium Scoring using U-Net Models through Semi-supervised Learning on Non-Gated CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2206.10455v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.10455v2)
- **Published**: 2022-06-13 20:02:02+00:00
- **Updated**: 2022-09-22 03:03:15+00:00
- **Authors**: Sanskriti Singh
- **Comment**: There is no correlation between gated and non-gated CT scans causing
  the points used in the training and results to be flawed. It was inaccurately
  assumed that there was a correlation between the scans
- **Journal**: None
- **Summary**: Every year, thousands of innocent people die due to heart attacks. Often undiagnosed heart attacks can hit people by surprise since many current medical plans don't cover the costs to require the searching of calcification on these scans. Only if someone is suspected to have a heart problem, a gated CT scan is taken, otherwise, there's no way for the patient to be aware of a possible heart attack/disease. While nongated CT scans are more periodically taken, it is harder to detect calcification and is usually taken for a purpose other than locating calcification in arteries. In fact, in real time coronary artery calcification scores are only calculated on gated CT scans, not nongated CT scans. After training a unet model on the Coronary Calcium and chest CT's gated scans, it received a DICE coefficient of 0.95 on its untouched test set. This model was used to predict on nongated CT scans, performing with a mean absolute error (MAE) of 674.19 and bucket classification accuracy of 41% (5 classes). Through the analysis of the images and the information stored in the images, mathematical equations were derived and used to automatically crop the images around the location of the heart. By performing semi-supervised learning the new cropped nongated scans were able to closely resemble gated CT scans, improving the performance by 91% in MAE (62.38) and 23% in accuracy.



### Assessing Privacy Leakage in Synthetic 3-D PET Imaging using Transversal GAN
- **Arxiv ID**: http://arxiv.org/abs/2206.06448v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.06448v1)
- **Published**: 2022-06-13 20:02:32+00:00
- **Updated**: 2022-06-13 20:02:32+00:00
- **Authors**: Robert V. Bergen, Jean-Francois Rajotte, Fereshteh Yousefirizi, Arman Rahmim, Raymond T. Ng
- **Comment**: arXiv admin note: text overlap with arXiv:2111.01866
- **Journal**: None
- **Summary**: Training computer-vision related algorithms on medical images for disease diagnosis or image segmentation is difficult in large part due to privacy concerns. For this reason, generative image models are highly sought after to facilitate data sharing. However, 3-D generative models are understudied, and investigation of their privacy leakage is needed. We introduce our 3-D generative model, Transversal GAN (TrGAN), using head & neck PET images which are conditioned on tumour masks as a case study. We define quantitative measures of image fidelity, utility and privacy for our model. These metrics are evaluated in the course of training to identify ideal fidelity, utility and privacy trade-offs and establish the relationships between these parameters. We show that the discriminator of the TrGAN is vulnerable to attack, and that an attacker can identify which samples were used in training with almost perfect accuracy (AUC = 0.99). We also show that an attacker with access to only the generator cannot reliably classify whether a sample had been used for training (AUC = 0.51). This suggests that TrGAN generators, but not discriminators, may be used for sharing synthetic 3-D PET data with minimal privacy risk while maintaining good utility and fidelity.



### Self-Supervised Representation Learning With MUlti-Segmental Informational Coding (MUSIC)
- **Arxiv ID**: http://arxiv.org/abs/2206.06461v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.06461v1)
- **Published**: 2022-06-13 20:37:48+00:00
- **Updated**: 2022-06-13 20:37:48+00:00
- **Authors**: Chuang Niu, Ge Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised representation learning maps high-dimensional data into a meaningful embedding space, where samples of similar semantic contents are close to each other. Most of the recent representation learning methods maximize cosine similarity or minimize the distance between the embedding features of different views from the same sample usually on the $l2$ normalized unit hypersphere. To prevent the trivial solutions that all samples have the same embedding feature, various techniques have been developed, such as contrastive learning, stop gradient, variance and covariance regularization, etc. In this study, we propose MUlti-Segmental Informational Coding (MUSIC) for self-supervised representation learning. MUSIC divides the embedding feature into multiple segments that discriminatively partition samples into different semantic clusters and different segments focus on different partition principles. Information theory measurements are directly used to optimize MUSIC and theoretically guarantee trivial solutions are avoided. MUSIC does not depend on commonly used techniques, such as memory bank or large batches, asymmetry networks, gradient stopping, momentum weight updating, etc, making the training framework flexible. Our experiments demonstrate that MUSIC achieves better results than most related Barlow Twins and VICReg methods on ImageNet classification with linear probing, and requires neither deep projectors nor large feature dimensions. Code will be made available.



### Revisiting the Shape-Bias of Deep Learning for Dermoscopic Skin Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.06466v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.06466v1)
- **Published**: 2022-06-13 20:59:06+00:00
- **Updated**: 2022-06-13 20:59:06+00:00
- **Authors**: Adriano Lucieri, Fabian Schmeisser, Christoph Peter Balada, Shoaib Ahmed Siddiqui, Andreas Dengel, Sheraz Ahmed
- **Comment**: Submitted preprint accepted for MIUA 2022
- **Journal**: None
- **Summary**: It is generally believed that the human visual system is biased towards the recognition of shapes rather than textures. This assumption has led to a growing body of work aiming to align deep models' decision-making processes with the fundamental properties of human vision. The reliance on shape features is primarily expected to improve the robustness of these models under covariate shift. In this paper, we revisit the significance of shape-biases for the classification of skin lesion images. Our analysis shows that different skin lesion datasets exhibit varying biases towards individual image features. Interestingly, despite deep feature extractors being inclined towards learning entangled features for skin lesion classification, individual features can still be decoded from this entangled representation. This indicates that these features are still represented in the learnt embedding spaces of the models, but not used for classification. In addition, the spectral analysis of different datasets shows that in contrast to common visual recognition, dermoscopic skin lesion classification, by nature, is reliant on complex feature combinations beyond shape-bias. As a natural consequence, shifting away from the prevalent desire of shape-biasing models can even improve skin lesion classifiers in some cases.



### RigNeRF: Fully Controllable Neural 3D Portraits
- **Arxiv ID**: http://arxiv.org/abs/2206.06481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06481v1)
- **Published**: 2022-06-13 21:28:34+00:00
- **Updated**: 2022-06-13 21:28:34+00:00
- **Authors**: ShahRukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli Shechtman, Zhixin Shu
- **Comment**: The project page can be found here:
  http://shahrukhathar.github.io/2022/06/06/RigNeRF.html
- **Journal**: None
- **Summary**: Volumetric neural rendering methods, such as neural radiance fields (NeRFs), have enabled photo-realistic novel view synthesis. However, in their standard form, NeRFs do not support the editing of objects, such as a human head, within a scene. In this work, we propose RigNeRF, a system that goes beyond just novel view synthesis and enables full control of head pose and facial expressions learned from a single portrait video. We model changes in head pose and facial expressions using a deformation field that is guided by a 3D morphable face model (3DMM). The 3DMM effectively acts as a prior for RigNeRF that learns to predict only residuals to the 3DMM deformations and allows us to render novel (rigid) poses and (non-rigid) expressions that were not present in the input sequence. Using only a smartphone-captured short video of a subject for training, we demonstrate the effectiveness of our method on free view synthesis of a portrait scene with explicit head pose and expression controls. The project page can be found here: http://shahrukhathar.github.io/2022/06/06/RigNeRF.html



### On Image Segmentation With Noisy Labels: Characterization and Volume Properties of the Optimal Solutions to Accuracy and Dice
- **Arxiv ID**: http://arxiv.org/abs/2206.06484v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.06484v4)
- **Published**: 2022-06-13 21:30:29+00:00
- **Updated**: 2023-03-31 13:20:56+00:00
- **Authors**: Marcus Nordström, Henrik Hult, Jonas Söderberg, Fredrik Löfman
- **Comment**: None
- **Journal**: None
- **Summary**: We study two of the most popular performance metrics in medical image segmentation, Accuracy and Dice, when the target labels are noisy. For both metrics, several statements related to characterization and volume properties of the set of optimal segmentations are proved, and associated experiments are provided. Our main insights are: (i) the volume of the solutions to both metrics may deviate significantly from the expected volume of the target, (ii) the volume of a solution to Accuracy is always less than or equal to the volume of a solution to Dice and (iii) the optimal solutions to both of these metrics coincide when the set of feasible segmentations is constrained to the set of segmentations with the volume equal to the expected volume of the target.



### The Modality Focusing Hypothesis: Towards Understanding Crossmodal Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2206.06487v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.06487v3)
- **Published**: 2022-06-13 21:34:21+00:00
- **Updated**: 2023-03-02 01:12:05+00:00
- **Authors**: Zihui Xue, Zhengqi Gao, Sucheng Ren, Hang Zhao
- **Comment**: Accepted by ICLR 2023 (top-5%). The first three authors contribute
  equally. Project website: https://zihuixue.github.io/MFH/index.html
- **Journal**: None
- **Summary**: Crossmodal knowledge distillation (KD) extends traditional knowledge distillation to the area of multimodal learning and demonstrates great success in various applications. To achieve knowledge transfer across modalities, a pretrained network from one modality is adopted as the teacher to provide supervision signals to a student network learning from another modality. In contrast to the empirical success reported in prior works, the working mechanism of crossmodal KD remains a mystery. In this paper, we present a thorough understanding of crossmodal KD. We begin with two case studies and demonstrate that KD is not a universal cure in crossmodal knowledge transfer. We then present the modality Venn diagram to understand modality relationships and the modality focusing hypothesis revealing the decisive factor in the efficacy of crossmodal KD. Experimental results on 6 multimodal datasets help justify our hypothesis, diagnose failure cases, and point directions to improve crossmodal knowledge transfer in the future.



### Multimodal Learning with Transformers: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2206.06488v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.06488v2)
- **Published**: 2022-06-13 21:36:09+00:00
- **Updated**: 2023-05-10 02:11:30+00:00
- **Authors**: Peng Xu, Xiatian Zhu, David A. Clifton
- **Comment**: This paper is accepted by IEEE TPAMI
- **Journal**: None
- **Summary**: Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and big data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal big data era, (2) a theoretical review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.



### BEHAVIOR in Habitat 2.0: Simulator-Independent Logical Task Description for Benchmarking Embodied AI Agents
- **Arxiv ID**: http://arxiv.org/abs/2206.06489v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.06489v1)
- **Published**: 2022-06-13 21:37:31+00:00
- **Updated**: 2022-06-13 21:37:31+00:00
- **Authors**: Ziang Liu, Roberto Martín-Martín, Fei Xia, Jiajun Wu, Li Fei-Fei
- **Comment**: None
- **Journal**: None
- **Summary**: Robots excel in performing repetitive and precision-sensitive tasks in controlled environments such as warehouses and factories, but have not been yet extended to embodied AI agents providing assistance in household tasks. Inspired by the catalyzing effect that benchmarks have played in the AI fields such as computer vision and natural language processing, the community is looking for new benchmarks for embodied AI. Prior work in embodied AI benchmark defines tasks using a different formalism, often specific to one environment, simulator or domain, making it hard to develop general and comparable solutions. In this work, we bring a subset of BEHAVIOR activities into Habitat 2.0 to benefit from its fast simulation speed, as a first step towards demonstrating the ease of adapting activities defined in the logic space into different simulators.



### Learning Task-Independent Game State Representations from Unlabeled Images
- **Arxiv ID**: http://arxiv.org/abs/2206.06490v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06490v1)
- **Published**: 2022-06-13 21:37:58+00:00
- **Updated**: 2022-06-13 21:37:58+00:00
- **Authors**: Chintan Trivedi, Konstantinos Makantasis, Antonios Liapis, Georgios N. Yannakakis
- **Comment**: Conference on Games (CoG) 2022
- **Journal**: None
- **Summary**: Self-supervised learning (SSL) techniques have been widely used to learn compact and informative representations from high-dimensional complex data. In many computer vision tasks, such as image classification, such methods achieve state-of-the-art results that surpass supervised learning approaches. In this paper, we investigate whether SSL methods can be leveraged for the task of learning accurate state representations of games, and if so, to what extent. For this purpose, we collect game footage frames and corresponding sequences of games' internal state from three different 3D games: VizDoom, the CARLA racing simulator and the Google Research Football Environment. We train an image encoder with three widely used SSL algorithms using solely the raw frames, and then attempt to recover the internal state variables from the learned representations. Our results across all three games showcase significantly higher correlation between SSL representations and the game's internal state compared to pre-trained baseline models such as ImageNet. Such findings suggest that SSL-based visual encoders can yield general -- not tailored to a specific task -- yet informative game representations solely from game pixel information. Such representations can, in turn, form the basis for boosting the performance of downstream learning tasks in games, including gameplaying, content generation and player modeling.



### Spiking Neural Networks for Frame-based and Event-based Single Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2206.06506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06506v1)
- **Published**: 2022-06-13 22:22:32+00:00
- **Updated**: 2022-06-13 22:22:32+00:00
- **Authors**: Sami Barchid, José Mennesson, Jason Eshraghian, Chaabane Djéraba, Mohammed Bennamoun
- **Comment**: 21 pages, 12 figures
- **Journal**: None
- **Summary**: Spiking neural networks have shown much promise as an energy-efficient alternative to artificial neural networks. However, understanding the impacts of sensor noises and input encodings on the network activity and performance remains difficult with common neuromorphic vision baselines like classification. Therefore, we propose a spiking neural network approach for single object localization trained using surrogate gradient descent, for frame- and event-based sensors. We compare our method with similar artificial neural networks and show that our model has competitive/better performance in accuracy, robustness against various corruptions, and has lower energy consumption. Moreover, we study the impact of neural coding schemes for static images in accuracy, robustness, and energy efficiency. Our observations differ importantly from previous studies on bio-plausible learning rules, which helps in the design of surrogate gradient trained architectures, and offers insight to design priorities in future neuromorphic technologies in terms of noise characteristics and data encoding methods.



### Generalizable Method for Face Anti-Spoofing with Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.06510v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.06510v1)
- **Published**: 2022-06-13 22:44:14+00:00
- **Updated**: 2022-06-13 22:44:14+00:00
- **Authors**: Nikolay Sergievskiy, Roman Vlasov, Roman Trusov
- **Comment**: None
- **Journal**: None
- **Summary**: Face anti-spoofing has drawn a lot of attention due to the high security requirements in biometric authentication systems. Bringing face biometric to commercial hardware became mostly dependent on developing reliable methods for detecting fake login sessions without specialized sensors. Current CNN-based method perform well on the domains they were trained for, but often show poor generalization on previously unseen datasets. In this paper we describe a method for utilizing unsupervised pretraining for improving performance across multiple datasets without any adaptation, introduce the Entry Antispoofing Dataset for supervised fine-tuning, and propose a multi-class auxiliary classification layer for augmenting the binary classification task of detecting spoofing attempts with explicit interpretable signals. We demonstrate the efficiency of our model by achieving state-of-the-art results on cross-dataset testing on MSU-MFSD, Replay-Attack, and OULU-NPU datasets.



### Estimating Pose from Pressure Data for Smart Beds with Deep Image-based Pose Estimators
- **Arxiv ID**: http://arxiv.org/abs/2206.06518v1
- **DOI**: 10.1007/s10489-021-02418-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06518v1)
- **Published**: 2022-06-13 23:29:28+00:00
- **Updated**: 2022-06-13 23:29:28+00:00
- **Authors**: Vandad Davoodnia, Saeed Ghorbani, Ali Etemad
- **Comment**: The version of record of this article, first published in Applied
  Intelligence, is available online at Publisher's website
  https://doi.org/10.1007/s10489-021-02418-y. arXiv admin note: substantial
  text overlap with arXiv:1908.08919
- **Journal**: Applied Intelligence (2021): 1-15
- **Summary**: In-bed pose estimation has shown value in fields such as hospital patient monitoring, sleep studies, and smart homes. In this paper, we explore different strategies for detecting body pose from highly ambiguous pressure data, with the aid of pre-existing pose estimators. We examine the performance of pre-trained pose estimators by using them either directly or by re-training them on two pressure datasets. We also explore other strategies utilizing a learnable pre-processing domain adaptation step, which transforms the vague pressure maps to a representation closer to the expected input space of common purpose pose estimation modules. Accordingly, we used a fully convolutional network with multiple scales to provide the pose-specific characteristics of the pressure maps to the pre-trained pose estimation module. Our complete analysis of different approaches shows that the combination of learnable pre-processing module along with re-training pre-existing image-based pose estimators on the pressure data is able to overcome issues such as highly vague pressure points to achieve very high pose estimation accuracy.



### LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.06522v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.06522v2)
- **Published**: 2022-06-13 23:51:56+00:00
- **Updated**: 2022-10-31 17:37:15+00:00
- **Authors**: Yi-Lin Sung, Jaemin Cho, Mohit Bansal
- **Comment**: NeurIPS 2022 (our code is available at:
  https://github.com/ylsung/Ladder-Side-Tuning)
- **Journal**: None
- **Summary**: Fine-tuning large pre-trained models on downstream tasks has been adopted in a variety of domains recently. However, it is costly to update the entire parameter set of large pre-trained models. Although recently proposed parameter-efficient transfer learning (PETL) techniques allow updating a small subset of parameters (e.g. only using 2% of parameters) inside a pre-trained backbone network for a new task, they only reduce the training memory requirement by up to 30%. This is because the gradient computation for the trainable parameters still requires backpropagation through the large pre-trained backbone model. To address this, we propose Ladder Side-Tuning (LST), a new PETL technique that can reduce training memory requirements by more substantial amounts. Unlike existing parameter-efficient methods that insert additional parameters inside backbone networks, we train a ladder side network, a small and separate network that takes intermediate activations as input via shortcut connections (called ladders) from backbone networks and makes predictions. LST has significantly lower memory requirements than previous methods, because it does not require backpropagation through the backbone network, but instead only through the side network and ladder connections. We evaluate our method with various models (T5 and CLIP-T5) on both NLP (GLUE) and vision-and-language (VQA, GQA, NLVR2 , MSCOCO) tasks. LST saves 69% of the memory costs to fine-tune the whole network, while other methods only save 26% of that in similar parameter usages (hence, 2.7x more memory savings). Moreover, LST achieves higher accuracy than Adapter and LoRA in a low-memory regime. To further show the advantage of this better memory efficiency, we also apply LST to larger T5 models, attaining better GLUE performance than full fine-tuning and other PETL methods. The accuracy-efficiency trade-off also holds on VL tasks.



