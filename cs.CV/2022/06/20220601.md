# Arxiv Papers in cs.CV on 2022-06-01
### PAGER: Progressive Attribute-Guided Extendable Robust Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2206.00162v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.00162v2)
- **Published**: 2022-06-01 00:35:42+00:00
- **Updated**: 2022-08-23 00:22:34+00:00
- **Authors**: Zohreh Azizi, C. -C. Jay Kuo
- **Comment**: 19 pages, 12 figures, 2 tables
- **Journal**: None
- **Summary**: This work presents a generative modeling approach based on successive subspace learning (SSL). Unlike most generative models in the literature, our method does not utilize neural networks to analyze the underlying source distribution and synthesize images. The resulting method, called the progressive attribute-guided extendable robust image generative (PAGER) model, has advantages in mathematical transparency, progressive content generation, lower training time, robust performance with fewer training samples, and extendibility to conditional image generation. PAGER consists of three modules: core generator, resolution enhancer, and quality booster. The core generator learns the distribution of low-resolution images and performs unconditional image generation. The resolution enhancer increases image resolution via conditional generation. Finally, the quality booster adds finer details to generated images. Extensive experiments on MNIST, Fashion-MNIST, and CelebA datasets are conducted to demonstrate generative performance of PAGER.



### Discovering the Hidden Vocabulary of DALLE-2
- **Arxiv ID**: http://arxiv.org/abs/2206.00169v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.00169v1)
- **Published**: 2022-06-01 01:14:48+00:00
- **Updated**: 2022-06-01 01:14:48+00:00
- **Authors**: Giannis Daras, Alexandros G. Dimakis
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: We discover that DALLE-2 seems to have a hidden vocabulary that can be used to generate images with absurd prompts. For example, it seems that \texttt{Apoploe vesrreaitais} means birds and \texttt{Contarra ccetnxniams luryca tanniounons} (sometimes) means bugs or pests. We find that these prompts are often consistent in isolation but also sometimes in combinations. We present our black-box method to discover words that seem random but have some correspondence to visual concepts. This creates important security and interpretability challenges.



### Learning Sequential Contexts using Transformer for 3D Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2206.00171v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00171v2)
- **Published**: 2022-06-01 01:22:29+00:00
- **Updated**: 2022-06-11 20:46:14+00:00
- **Authors**: Leyla Khaleghi, Joshua Marshall, Ali Etemad
- **Comment**: Accepted to ICPR'22
- **Journal**: None
- **Summary**: 3D hand pose estimation (HPE) is the process of locating the joints of the hand in 3D from any visual input. HPE has recently received an increased amount of attention due to its key role in a variety of human-computer interaction applications. Recent HPE methods have demonstrated the advantages of employing videos or multi-view images, allowing for more robust HPE systems. Accordingly, in this study, we propose a new method to perform Sequential learning with Transformer for Hand Pose (SeTHPose) estimation. Our SeTHPose pipeline begins by extracting visual embeddings from individual hand images. We then use a transformer encoder to learn the sequential context along time or viewing angles and generate accurate 2D hand joint locations. Then, a graph convolutional neural network with a U-Net configuration is used to convert the 2D hand joint locations to 3D poses. Our experiments show that SeTHPose performs well on both hand sequence varieties, temporal and angular. Also, SeTHPose outperforms other methods in the field to achieve new state-of-the-art results on two public available sequential datasets, STB and MuViHand.



### Labeling Where Adapting Fails: Cross-Domain Semantic Segmentation with Point Supervision via Active Selection
- **Arxiv ID**: http://arxiv.org/abs/2206.00181v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00181v2)
- **Published**: 2022-06-01 01:52:28+00:00
- **Updated**: 2022-06-04 12:50:09+00:00
- **Authors**: Fei Pan, Francois Rameau, Junsik Kim, In So Kweon
- **Comment**: None
- **Journal**: None
- **Summary**: Training models dedicated to semantic segmentation requires a large amount of pixel-wise annotated data. Due to their costly nature, these annotations might not be available for the task at hand. To alleviate this problem, unsupervised domain adaptation approaches aim at aligning the feature distributions between the labeled source and the unlabeled target data. While these strategies lead to noticeable improvements, their effectiveness remains limited. To guide the domain adaptation task more efficiently, previous works attempted to include human interactions in this process under the form of sparse single-pixel annotations in the target data. In this work, we propose a new domain adaptation framework for semantic segmentation with annotated points via active selection. First, we conduct an unsupervised domain adaptation of the model; from this adaptation, we use an entropy-based uncertainty measurement for target points selection. Finally, to minimize the domain gap, we propose a domain adaptation framework utilizing these target points annotated by human annotators. Experimental results on benchmark datasets show the effectiveness of our methods against existing unsupervised domain adaptation approaches. The propose pipeline is generic and can be included as an extra module to existing domain adaptation strategies.



### Differentiable Soft-Masked Attention
- **Arxiv ID**: http://arxiv.org/abs/2206.00182v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6; I.4.8; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2206.00182v2)
- **Published**: 2022-06-01 02:05:13+00:00
- **Updated**: 2022-08-05 14:09:12+00:00
- **Authors**: Ali Athar, Jonathon Luiten, Alexander Hermans, Deva Ramanan, Bastian Leibe
- **Comment**: arXiv admin note: text overlap with arXiv:2112.09131
- **Journal**: None
- **Summary**: Transformers have become prevalent in computer vision due to their performance and flexibility in modelling complex operations. Of particular significance is the 'cross-attention' operation, which allows a vector representation (e.g. of an object in an image) to be learned by attending to an arbitrarily sized set of input features. Recently, "Masked Attention" was proposed in which a given object representation only attends to those image pixel features for which the segmentation mask of that object is active. This specialization of attention proved beneficial for various image and video segmentation tasks. In this paper, we propose another specialization of attention which enables attending over `soft-masks' (those with continuous mask probabilities instead of binary values), and is also differentiable through these mask probabilities, thus allowing the mask used for attention to be learned within the network without requiring direct loss supervision. This can be useful for several applications. Specifically, we employ our "Differentiable Soft-Masked Attention" for the task of Weakly-Supervised Video Object Segmentation (VOS), where we develop a transformer-based network for VOS which only requires a single annotated image frame for training, but can also benefit from cycle consistency training on a video with just one annotated frame. Although there is no loss for masks in unlabeled frames, the network is still able to segment objects in those frames due to our novel attention formulation. Code: https://github.com/Ali2500/HODOR/blob/main/hodor/modelling/encoder/soft_masked_attention.py



### CAFA: Class-Aware Feature Alignment for Test-Time Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2206.00205v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00205v2)
- **Published**: 2022-06-01 03:02:07+00:00
- **Updated**: 2022-11-24 07:36:42+00:00
- **Authors**: Sanghun Jung, Jungsoo Lee, Nanhee Kim, Amirreza Shaban, Byron Boots, Jaegul Choo
- **Comment**: None
- **Journal**: None
- **Summary**: Despite recent advancements in deep learning, deep neural networks continue to suffer from performance degradation when applied to new data that differs from training data. Test-time adaptation (TTA) aims to address this challenge by adapting a model to unlabeled data at test time. TTA can be applied to pretrained networks without modifying their training procedures, enabling them to utilize a well-formed source distribution for adaptation. One possible approach is to align the representation space of test samples to the source distribution (\textit{i.e.,} feature alignment). However, performing feature alignment in TTA is especially challenging in that access to labeled source data is restricted during adaptation. That is, a model does not have a chance to learn test data in a class-discriminative manner, which was feasible in other adaptation tasks (\textit{e.g.,} unsupervised domain adaptation) via supervised losses on the source data. Based on this observation, we propose a simple yet effective feature alignment loss, termed as Class-Aware Feature Alignment (CAFA), which simultaneously 1) encourages a model to learn target representations in a class-discriminative manner and 2) effectively mitigates the distribution shifts at test time. Our method does not require any hyper-parameters or additional losses, which are required in previous approaches. We conduct extensive experiments on 6 different datasets and show our proposed method consistently outperforms existing baselines.



### LiDAR-MIMO: Efficient Uncertainty Estimation for LiDAR-based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.00214v1
- **DOI**: 10.1109/IV51971.2022.9827244
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00214v1)
- **Published**: 2022-06-01 03:47:32+00:00
- **Updated**: 2022-06-01 03:47:32+00:00
- **Authors**: Matthew Pitropov, Chengjie Huang, Vahdat Abdelzad, Krzysztof Czarnecki, Steven Waslander
- **Comment**: 8 pages, 4 figures and 5 tables. Accepted in IEEE IV 2022
- **Journal**: None
- **Summary**: The estimation of uncertainty in robotic vision, such as 3D object detection, is an essential component in developing safe autonomous systems aware of their own performance. However, the deployment of current uncertainty estimation methods in 3D object detection remains challenging due to timing and computational constraints. To tackle this issue, we propose LiDAR-MIMO, an adaptation of the multi-input multi-output (MIMO) uncertainty estimation method to the LiDAR-based 3D object detection task. Our method modifies the original MIMO by performing multi-input at the feature level to ensure the detection, uncertainty estimation, and runtime performance benefits are retained despite the limited capacity of the underlying detector and the large computational costs of point cloud processing. We compare LiDAR-MIMO with MC dropout and ensembles as baselines and show comparable uncertainty estimation results with only a small number of output heads. Further, LiDAR-MIMO can be configured to be twice as fast as MC dropout and ensembles, while achieving higher mAP than MC dropout and approaching that of ensembles.



### Cross-domain Detection Transformer based on Spatial-aware and Semantic-aware Token Alignment
- **Arxiv ID**: http://arxiv.org/abs/2206.00222v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00222v1)
- **Published**: 2022-06-01 04:13:22+00:00
- **Updated**: 2022-06-01 04:13:22+00:00
- **Authors**: Jinhong Deng, Xiaoyue Zhang, Wen Li, Lixin Duan
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Detection transformers like DETR have recently shown promising performance on many object detection tasks, but the generalization ability of those methods is still quite challenging for cross-domain adaptation scenarios. To address the cross-domain issue, a straightforward way is to perform token alignment with adversarial training in transformers. However, its performance is often unsatisfactory as the tokens in detection transformers are quite diverse and represent different spatial and semantic information. In this paper, we propose a new method called Spatial-aware and Semantic-aware Token Alignment (SSTA) for cross-domain detection transformers. In particular, we take advantage of the characteristics of cross-attention as used in detection transformer and propose the spatial-aware token alignment (SpaTA) and the semantic-aware token alignment (SemTA) strategies to guide the token alignment across domains. For spatial-aware token alignment, we can extract the information from the cross-attention map (CAM) to align the distribution of tokens according to their attention to object queries. For semantic-aware token alignment, we inject the category information into the cross-attention map and construct domain embedding to guide the learning of a multi-class discriminator so as to model the category relationship and achieve category-level token alignment during the entire adaptation process. We conduct extensive experiments on several widely-used benchmarks, and the results clearly show the effectiveness of our proposed method over existing state-of-the-art baselines.



### Rethinking the Augmentation Module in Contrastive Learning: Learning Hierarchical Augmentation Invariance with Expanded Views
- **Arxiv ID**: http://arxiv.org/abs/2206.00227v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00227v2)
- **Published**: 2022-06-01 04:30:46+00:00
- **Updated**: 2022-08-22 03:28:52+00:00
- **Authors**: Junbo Zhang, Kaisheng Ma
- **Comment**: Accepted to CVPR 2022
- **Journal**: 2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)
- **Summary**: A data augmentation module is utilized in contrastive learning to transform the given data example into two views, which is considered essential and irreplaceable. However, the predetermined composition of multiple data augmentations brings two drawbacks. First, the artificial choice of augmentation types brings specific representational invariances to the model, which have different degrees of positive and negative effects on different downstream tasks. Treating each type of augmentation equally during training makes the model learn non-optimal representations for various downstream tasks and limits the flexibility to choose augmentation types beforehand. Second, the strong data augmentations used in classic contrastive learning methods may bring too much invariance in some cases, and fine-grained information that is essential to some downstream tasks may be lost. This paper proposes a general method to alleviate these two problems by considering where and what to contrast in a general contrastive learning framework. We first propose to learn different augmentation invariances at different depths of the model according to the importance of each data augmentation instead of learning representational invariances evenly in the backbone. We then propose to expand the contrast content with augmentation embeddings to reduce the misleading effects of strong data augmentations. Experiments based on several baseline methods demonstrate that we learn better representations for various benchmarks on classification, detection, and segmentation downstream tasks.



### Fair Comparison between Efficient Attentions
- **Arxiv ID**: http://arxiv.org/abs/2206.00244v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.00244v1)
- **Published**: 2022-06-01 06:00:13+00:00
- **Updated**: 2022-06-01 06:00:13+00:00
- **Authors**: Jiuk Hong, Chaehyeon Lee, Soyoun Bang, Heechul Jung
- **Comment**: 4 pages abstract
- **Journal**: None
- **Summary**: Transformers have been successfully used in various fields and are becoming the standard tools in computer vision. However, self-attention, a core component of transformers, has a quadratic complexity problem, which limits the use of transformers in various vision tasks that require dense prediction. Many studies aiming at solving this problem have been reported proposed. However, no comparative study of these methods using the same scale has been reported due to different model configurations, training schemes, and new methods. In our paper, we validate these efficient attention models on the ImageNet1K classification task by changing only the attention operation and examining which efficient attention is better.



### Visual Transformer for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.06323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.06323v1)
- **Published**: 2022-06-01 06:13:09+00:00
- **Updated**: 2022-06-01 06:13:09+00:00
- **Authors**: Michael Yang
- **Comment**: In preparation for short paper of conferences. I am using the name
  Michael Yang
- **Journal**: None
- **Summary**: Convolutional Neural networks (CNN) have been the first choice of paradigm in many computer vision applications. The convolution operation however has a significant weakness which is it only operates on a local neighborhood of pixels, thus it misses global information of the surrounding neighbors. Transformers, or Self-attention networks to be more specific, on the other hand, have emerged as a recent advance to capture long range interactions of the input, but they have mostly been applied to sequence modeling tasks such as Neural Machine Translation, Image captioning and other Natural Language Processing tasks. Transformers has been applied to natural language related tasks and achieved promising results. However, its applications in visual related tasks are far from being satisfying. Taking into consideration of both the weaknesses of Convolutional Neural Networks and those of the Transformers, in this paper, we consider the use of self-attention for discriminative visual tasks, object detection, as an alternative to convolutions. In this paper, we propose our model: DetTransNet. Extensive experiments show that our model leads to consistent improvements in object detection on COCO across many different models and scales, including ResNets, while keeping the number of parameters similar. In particular, our method achieves a 1.2% Average Precision improvement on COCO object detection task over other baseline models.



### Interpretable Deep Learning Classifier by Detection of Prototypical Parts on Kidney Stones Images
- **Arxiv ID**: http://arxiv.org/abs/2206.00252v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.00252v2)
- **Published**: 2022-06-01 06:32:31+00:00
- **Updated**: 2022-06-02 03:06:35+00:00
- **Authors**: Daniel Flores-Araiza, Francisco Lopez-Tiro, Elias Villalvazo-Avila, Jonathan El-Beze, Jacques Hubert, Gilberto Ochoa-Ruiz, Christian Daul
- **Comment**: Extended abstract accepted at LatinX in Computer Vision Research
  Workshop, at CVPR 2022
- **Journal**: None
- **Summary**: Identifying the type of kidney stones can allow urologists to determine their formation cause, improving the early prescription of appropriate treatments to diminish future relapses. However, currently, the associated ex-vivo diagnosis (known as morpho-constitutional analysis, MCA) is time-consuming, expensive, and requires a great deal of experience, as it requires a visual analysis component that is highly operator dependant. Recently, machine learning methods have been developed for in-vivo endoscopic stone recognition. Shallow methods have been demonstrated to be reliable and interpretable but exhibit low accuracy, while deep learning-based methods yield high accuracy but are not explainable. However, high stake decisions require understandable computer-aided diagnosis (CAD) to suggest a course of action based on reasonable evidence, rather than merely prescribe one. Herein, we investigate means for learning part-prototypes (PPs) that enable interpretable models. Our proposal suggests a classification for a kidney stone patch image and provides explanations in a similar way as those used on the MCA method.



### PaGO-LOAM: Robust Ground-Optimized LiDAR Odometry
- **Arxiv ID**: http://arxiv.org/abs/2206.00266v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.00266v1)
- **Published**: 2022-06-01 06:50:44+00:00
- **Updated**: 2022-06-01 06:50:44+00:00
- **Authors**: Dong-Uk Seo, Hyungtae Lim, Seungjae Lee, Hyun Myung
- **Comment**: 7 pages, 5 figures, conference
- **Journal**: None
- **Summary**: Numerous researchers have conducted studies to achieve fast and robust ground-optimized LiDAR odometry methods for terrestrial mobile platforms. In particular, ground-optimized LiDAR odometry usually employs ground segmentation as a preprocessing method. This is because most of the points in a 3D point cloud captured by a 3D LiDAR sensor on a terrestrial platform are from the ground. However, the effect of the performance of ground segmentation on LiDAR odometry is still not closely examined. In this paper, a robust ground-optimized LiDAR odometry framework is proposed to facilitate the study to check the effect of ground segmentation on LiDAR SLAM based on the state-of-the-art (SOTA) method. By using our proposed odometry framework, it is easy and straightforward to test whether ground segmentation algorithms help extract well-described features and thus improve SLAM performance. In addition, by leveraging the SOTA ground segmentation method called Patchwork, which shows robust ground segmentation even in complex and uneven urban environments with little performance perturbation, a novel ground-optimized LiDAR odometry is proposed, called PaGO-LOAM. The methods were tested using the KITTI odometry dataset. \textit{PaGO-LOAM} shows robust and accurate performance compared with the baseline method. Our code is available at https://github.com/url-kaist/AlterGround-LeGO-LOAM.



### Vision GNN: An Image is Worth Graph of Nodes
- **Arxiv ID**: http://arxiv.org/abs/2206.00272v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00272v3)
- **Published**: 2022-06-01 07:01:04+00:00
- **Updated**: 2022-11-04 14:45:03+00:00
- **Authors**: Kai Han, Yunhe Wang, Jianyuan Guo, Yehui Tang, Enhua Wu
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: Network architecture plays a key role in the deep learning-based computer vision system. The widely-used convolutional neural network and transformer treat the image as a grid or sequence structure, which is not flexible to capture irregular and complex objects. In this paper, we propose to represent the image as a graph structure and introduce a new Vision GNN (ViG) architecture to extract graph-level feature for visual tasks. We first split the image to a number of patches which are viewed as nodes, and construct a graph by connecting the nearest neighbors. Based on the graph representation of images, we build our ViG model to transform and exchange information among all the nodes. ViG consists of two basic modules: Grapher module with graph convolution for aggregating and updating graph information, and FFN module with two linear layers for node feature transformation. Both isotropic and pyramid architectures of ViG are built with different model sizes. Extensive experiments on image recognition and object detection tasks demonstrate the superiority of our ViG architecture. We hope this pioneering study of GNN on general visual tasks will provide useful inspiration and experience for future research. The PyTorch code is available at https://github.com/huawei-noah/Efficient-AI-Backbones and the MindSpore code is available at https://gitee.com/mindspore/models.



### Point-Teaching: Weakly Semi-Supervised Object Detection with Point Annotations
- **Arxiv ID**: http://arxiv.org/abs/2206.00274v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00274v2)
- **Published**: 2022-06-01 07:04:38+00:00
- **Updated**: 2022-10-24 05:19:22+00:00
- **Authors**: Yongtao Ge, Qiang Zhou, Xinlong Wang, Zhibin Wang, Hao Li, Chunhua Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Point annotations are considerably more time-efficient than bounding box annotations. However, how to use cheap point annotations to boost the performance of semi-supervised object detection remains largely unsolved. In this work, we present Point-Teaching, a weakly semi-supervised object detection framework to fully exploit the point annotations. Specifically, we propose a Hungarian-based point matching method to generate pseudo labels for point annotated images. We further propose multiple instance learning (MIL) approaches at the level of images and points to supervise the object detector with point annotations. Finally, we propose a simple-yet-effective data augmentation, termed point-guided copy-paste, to reduce the impact of the unmatched points. Experiments demonstrate the effectiveness of our method on a few datasets and various data regimes.



### Automatic Bounding Box Annotation with Small Training Data Sets for Industrial Manufacturing
- **Arxiv ID**: http://arxiv.org/abs/2206.00280v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2206.00280v1)
- **Published**: 2022-06-01 07:32:32+00:00
- **Updated**: 2022-06-01 07:32:32+00:00
- **Authors**: Manuela Geiß, Raphael Wagner, Martin Baresch, Josef Steiner, Michael Zwick
- **Comment**: None
- **Journal**: None
- **Summary**: In the past few years, object detection has attracted a lot of attention in the context of human-robot collaboration and Industry 5.0 due to enormous quality improvements in deep learning technologies. In many applications, object detection models have to be able to quickly adapt to a changing environment, i.e., to learn new objects. A crucial but challenging prerequisite for this is the automatic generation of new training data which currently still limits the broad application of object detection methods in industrial manufacturing. In this work, we discuss how to adapt state-of-the-art object detection methods for the task of automatic bounding box annotation for the use case where the background is homogeneous and the object's label is provided by a human. We compare an adapted version of Faster R-CNN and the Scaled Yolov4-p5 architecture and show that both can be trained to distinguish unknown objects from a complex but homogeneous background using only a small amount of training data.



### Needle In A Haystack, Fast: Benchmarking Image Perceptual Similarity Metrics At Scale
- **Arxiv ID**: http://arxiv.org/abs/2206.00282v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.PF, H.3.1; I.4.10; I.4.7; I.5.5; I.5.4; K.4
- **Links**: [PDF](http://arxiv.org/pdf/2206.00282v1)
- **Published**: 2022-06-01 07:36:14+00:00
- **Updated**: 2022-06-01 07:36:14+00:00
- **Authors**: Cyril Vallez, Andrei Kucharavy, Ljiljana Dolamic
- **Comment**: 26 pages, 10 figures
- **Journal**: None
- **Summary**: The advent of the internet, followed shortly by the social media made it ubiquitous in consuming and sharing information between anyone with access to it. The evolution in the consumption of media driven by this change, led to the emergence of images as means to express oneself, convey information and convince others efficiently. With computer vision algorithms progressing radically over the last decade, it is become easier and easier to study at scale the role of images in the flow of information online. While the research questions and overall pipelines differ radically, almost all start with a crucial first step - evaluation of global perceptual similarity between different images. That initial step is crucial for overall pipeline performance and processes most images. A number of algorithms are available and currently used to perform it, but so far no comprehensive review was available to guide the choice of researchers as to the choice of an algorithm best suited to their question, assumptions and computational resources. With this paper we aim to fill this gap, showing that classical computer vision methods are not necessarily the best approach, whereas a pair of relatively little used methods - Dhash perceptual hash and SimCLR v2 ResNets achieve excellent performance, scale well and are computationally efficient.



### Efficient Multi-Purpose Cross-Attention Based Image Alignment Block for Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/2206.00291v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.00291v1)
- **Published**: 2022-06-01 07:51:35+00:00
- **Updated**: 2022-06-01 07:51:35+00:00
- **Authors**: Bahri Batuhan Bilecen, Alparslan Fisne, Mustafa Ayazoglu
- **Comment**: Accepted into Embedded Vision Workshop 2022 of CVPR 2022
- **Journal**: None
- **Summary**: Image alignment, also known as image registration, is a critical block used in many computer vision problems. One of the key factors in alignment is efficiency, as inefficient aligners can cause significant overhead to the overall problem. In the literature, there are some blocks that appear to do the alignment operation, although most do not focus on efficiency. Therefore, an image alignment block which can both work in time and/or space and can work on edge devices would be beneficial for almost all networks dealing with multiple images. Given its wide usage and importance, we propose an efficient, cross-attention-based, multi-purpose image alignment block (XABA) suitable to work within edge devices. Using cross-attention, we exploit the relationships between features extracted from images. To make cross-attention feasible for real-time image alignment problems and handle large motions, we provide a pyramidal block based cross-attention scheme. This also captures local relationships besides reducing memory requirements and number of operations. Efficient XABA models achieve real-time requirements of running above 20 FPS performance on NVIDIA Jetson Xavier with 30W power consumption compared to other powerful computers. Used as a sub-block in a larger network, XABA also improves multi-image super-resolution network performance in comparison to other alignment methods.



### Supervised Denoising of Diffusion-Weighted Magnetic Resonance Images Using a Convolutional Neural Network and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.00305v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.00305v1)
- **Published**: 2022-06-01 08:14:35+00:00
- **Updated**: 2022-06-01 08:14:35+00:00
- **Authors**: Jakub Jurek, Andrzej Materka, Kamil Ludwisiak, Agata Majos, Kamil Gorczewski, Kamil Cepuch, Agata Zawadzka
- **Comment**: Preprint submitted to NeuroImage
- **Journal**: None
- **Summary**: In this paper, we propose a method for denoising diffusion-weighted images (DWI) of the brain using a convolutional neural network trained on realistic, synthetic MR data. We compare our results to averaging of repeated scans, a widespread method used in clinics to improve signal-to-noise ratio of MR images. To obtain training data for transfer learning, we model, in a data-driven fashion, the effects of echo-planar imaging (EPI): Nyquist ghosting and ramp sampling. We introduce these effects to the digital phantom of brain anatomy (BrainWeb). Instead of simulating pseudo-random noise with a defined probability distribution, we perform noise scans with a brain-DWI-designed protocol to obtain realistic noise maps. We combine them with the simulated, noise-free EPI images. We also measure the Point Spread Function in a DW image of an AJR-approved geometrical phantom and inter-scan movement in a brain scan of a healthy volunteer. Their influence on image denoising and averaging of repeated images is investigated at different signal-to-noise ratio levels. Denoising performance is evaluated quantitatively using the simulated EPI images and qualitatively in real EPI DWI of the brain. We show that the application of our method allows for a significant reduction in scan time by lowering the number of repeated scans. Visual comparisons made in the acquired brain images indicate that the denoised single-repetition images are less noisy than multi-repetition averaged images. We also analyse the convolutional neural network denoiser and point out the challenges accompanying this denoising method.



### Label-Efficient Online Continual Object Detection in Streaming Video
- **Arxiv ID**: http://arxiv.org/abs/2206.00309v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00309v2)
- **Published**: 2022-06-01 08:22:34+00:00
- **Updated**: 2023-08-23 15:51:28+00:00
- **Authors**: Jay Zhangjie Wu, David Junhao Zhang, Wynne Hsu, Mengmi Zhang, Mike Zheng Shou
- **Comment**: ICCV 2023
- **Journal**: None
- **Summary**: Humans can watch a continuous video stream and effortlessly perform continual acquisition and transfer of new knowledge with minimal supervision yet retaining previously learnt experiences. In contrast, existing continual learning (CL) methods require fully annotated labels to effectively learn from individual frames in a video stream. Here, we examine a more realistic and challenging problem$\unicode{x2014}$Label-Efficient Online Continual Object Detection (LEOCOD) in streaming video. We propose a plug-and-play module, Efficient-CLS, that can be easily inserted into and improve existing continual learners for object detection in video streams with reduced data annotation costs and model retraining time. We show that our method has achieved significant improvement with minimal forgetting across all supervision levels on two challenging CL benchmarks for streaming real-world videos. Remarkably, with only 25% annotated video frames, our method still outperforms the base CL learners, which are trained with 100% annotations on all video frames. The data and source code will be publicly available at https://github.com/showlab/Efficient-CLS.



### MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2206.00311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00311v1)
- **Published**: 2022-06-01 08:27:19+00:00
- **Updated**: 2022-06-01 08:27:19+00:00
- **Authors**: Pengyuan Lyu, Chengquan Zhang, Shanshan Liu, Meina Qiao, Yangliu Xu, Liang Wu, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a model pretraining technique, named MaskOCR, for text recognition. Our text recognition architecture is an encoder-decoder transformer: the encoder extracts the patch-level representations, and the decoder recognizes the text from the representations. Our approach pretrains both the encoder and the decoder in a sequential manner. (i) We pretrain the encoder in a self-supervised manner over a large set of unlabeled real text images. We adopt the masked image modeling approach, which shows the effectiveness for general images, expecting that the representations take on semantics. (ii) We pretrain the decoder over a large set of synthesized text images in a supervised manner and enhance the language modeling capability of the decoder by randomly masking some text image patches occupied by characters input to the encoder and accordingly the representations input to the decoder. Experiments show that the proposed MaskOCR approach achieves superior results on the benchmark datasets, including Chinese and English text images.



### RLSS: A Deep Reinforcement Learning Algorithm for Sequential Scene Generation
- **Arxiv ID**: http://arxiv.org/abs/2206.02544v1
- **DOI**: 10.1109/WACV51458.2022.00278
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.02544v1)
- **Published**: 2022-06-01 08:39:33+00:00
- **Updated**: 2022-06-01 08:39:33+00:00
- **Authors**: Azimkhon Ostonov, Peter Wonka, Dominik L. Michels
- **Comment**: Accepted at the IEEE Winter Conference on Applications of Computer
  Vision, WACV 2022
- **Journal**: 2022 IEEE/CVF Winter Conference on Applications of Computer Vision
  (WACV), 2022, pp. 2723-2732
- **Summary**: We present RLSS: a reinforcement learning algorithm for sequential scene generation. This is based on employing the proximal policy optimization (PPO) algorithm for generative problems. In particular, we consider how to effectively reduce the action space by including a greedy search algorithm in the learning process. Our experiments demonstrate that our method converges for a relatively large number of actions and learns to generate scenes with predefined design objectives. This approach is placing objects iteratively in the virtual scene. In each step, the network chooses which objects to place and selects positions which result in maximal reward. A high reward is assigned if the last action resulted in desired properties whereas the violation of constraints is penalized. We demonstrate the capability of our method to generate plausible and diverse scenes efficiently by solving indoor planning problems and generating Angry Birds levels.



### CellCentroidFormer: Combining Self-attention and Convolution for Cell Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.00338v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.00338v2)
- **Published**: 2022-06-01 09:04:39+00:00
- **Updated**: 2022-06-14 16:25:54+00:00
- **Authors**: Royden Wagner, Karl Rohr
- **Comment**: Accepted at MIUA 2022; Added experiments with CircleNets and extended
  figure captions
- **Journal**: None
- **Summary**: Cell detection in microscopy images is important to study how cells move and interact with their environment. Most recent deep learning-based methods for cell detection use convolutional neural networks (CNNs). However, inspired by the success in other computer vision applications, vision transformers (ViTs) are also used for this purpose. We propose a novel hybrid CNN-ViT model for cell detection in microscopy images to exploit the advantages of both types of deep learning models. We employ an efficient CNN, that was pre-trained on the ImageNet dataset, to extract image features and utilize transfer learning to reduce the amount of required training data. Extracted image features are further processed by a combination of convolutional and transformer layers, so that the convolutional layers can focus on local information and the transformer layers on global information. Our centroid-based cell detection method represents cells as ellipses and is end-to-end trainable. Furthermore, we show that our proposed model can outperform fully convolutional one-stage detectors on four different 2D microscopy datasets. Code is available at: https://github.com/roydenwa/cell-centroid-former



### Towards view-invariant vehicle speed detection from driving simulator images
- **Arxiv ID**: http://arxiv.org/abs/2206.00343v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.00343v2)
- **Published**: 2022-06-01 09:14:45+00:00
- **Updated**: 2022-09-28 07:55:08+00:00
- **Authors**: Antonio Hernández Martínez, David Fernandez Llorca, Iván García Daza
- **Comment**: 14th International Joint Conference on Knowledge Discovery, Knowledge
  Engineering and Knowledge Management (IC3K 2022)
- **Journal**: None
- **Summary**: The use of cameras for vehicle speed measurement is much more cost effective compared to other technologies such as inductive loops, radar or laser. However, accurate speed measurement remains a challenge due to the inherent limitations of cameras to provide accurate range estimates. In addition, classical vision-based methods are very sensitive to extrinsic calibration between the camera and the road. In this context, the use of data-driven approaches appears as an interesting alternative. However, data collection requires a complex and costly setup to record videos under real traffic conditions from the camera synchronized with a high-precision speed sensor to generate the ground truth speed values. It has recently been demonstrated that the use of driving simulators (e.g., CARLA) can serve as a robust alternative for generating large synthetic datasets to enable the application of deep learning techniques for vehicle speed estimation for a single camera. In this paper, we study the same problem using multiple cameras in different virtual locations and with different extrinsic parameters. We address the question of whether complex 3D-CNN architectures are capable of implicitly learning view-invariant speeds using a single model, or whether view-specific models are more appropriate. The results are very promising as they show that a single model with data from multiple views reports even better accuracy than camera-specific models, paving the way towards a view-invariant vehicle speed measurement system.



### Self-Supervised Learning as a Means To Reduce the Need for Labeled Data in Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2206.00344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00344v1)
- **Published**: 2022-06-01 09:20:30+00:00
- **Updated**: 2022-06-01 09:20:30+00:00
- **Authors**: Marin Benčević, Marija Habijan, Irena Galić, Aleksandra Pizurica
- **Comment**: Accepted by 30th European Signal Processing Conference, EUSIPCO 2022
- **Journal**: None
- **Summary**: One of the largest problems in medical image processing is the lack of annotated data. Labeling medical images often requires highly trained experts and can be a time-consuming process. In this paper, we evaluate a method of reducing the need for labeled data in medical image object detection by using self-supervised neural network pretraining. We use a dataset of chest X-ray images with bounding box labels for 13 different classes of anomalies. The networks are pretrained on a percentage of the dataset without labels and then fine-tuned on the rest of the dataset. We show that it is possible to achieve similar performance to a fully supervised model in terms of mean average precision and accuracy with only 60\% of the labeled data. We also show that it is possible to increase the maximum performance of a fully-supervised model by adding a self-supervised pretraining step, and this effect can be observed with even a small amount of unlabeled data for pretraining.



### A Survey on Deep Learning for Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.00356v3
- **DOI**: 10.1016/j.media.2023.102863
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.00356v3)
- **Published**: 2022-06-01 09:43:10+00:00
- **Updated**: 2023-06-20 08:28:38+00:00
- **Authors**: Zahra Mirikharaji, Kumar Abhishek, Alceu Bissoto, Catarina Barata, Sandra Avila, Eduardo Valle, M. Emre Celebi, Ghassan Hamarneh
- **Comment**: Published in Medical Image Analysis (2023); 55 pages, 10 figures;
  Mirikharaji and Abhishek: Joint first authors; Celebi and Hamarneh: Joint
  senior authors
- **Journal**: Medical Image Analysis (2023): 102863
- **Summary**: Skin cancer is a major public health problem that could benefit from computer-aided diagnosis to reduce the burden of this common disease. Skin lesion segmentation from images is an important step toward achieving this goal. However, the presence of natural and artificial artifacts (e.g., hair and air bubbles), intrinsic factors (e.g., lesion shape and contrast), and variations in image acquisition conditions make skin lesion segmentation a challenging task. Recently, various researchers have explored the applicability of deep learning models to skin lesion segmentation. In this survey, we cross-examine 177 research papers that deal with deep learning-based segmentation of skin lesions. We analyze these works along several dimensions, including input data (datasets, preprocessing, and synthetic data generation), model design (architecture, modules, and losses), and evaluation aspects (data annotation requirements and segmentation performance). We discuss these dimensions both from the viewpoint of select seminal works, and from a systematic viewpoint, examining how those choices have influenced current trends, and how their limitations should be addressed. To facilitate comparisons, we summarize all examined works in a comprehensive table as well as an interactive table available online at https://github.com/sfu-mial/skin-lesion-segmentation-survey.



### DeepCluE: Enhanced Image Clustering via Multi-layer Ensembles in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2206.00359v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.00359v1)
- **Published**: 2022-06-01 09:51:38+00:00
- **Updated**: 2022-06-01 09:51:38+00:00
- **Authors**: Dong Huang, Ding-Hua Chen, Xiangji Chen, Chang-Dong Wang, Jian-Huang Lai
- **Comment**: None
- **Journal**: None
- **Summary**: Deep clustering has recently emerged as a promising technique for complex image clustering. Despite the significant progress, previous deep clustering works mostly tend to construct the final clustering by utilizing a single layer of representation, e.g., by performing $K$-means on the last fully-connected layer or by associating some clustering loss to a specific layer. However, few of them have considered the possibilities and potential benefits of jointly leveraging multi-layer representations for enhancing the deep clustering performance. In light of this, this paper presents a Deep Clustering via Ensembles (DeepCluE) approach, which bridges the gap between deep clustering and ensemble clustering by harnessing the power of multiple layers in deep neural networks. Particularly, we utilize a weight-sharing convolutional neural network as the backbone, which is trained with both the instance-level contrastive learning (via an instance projector) and the cluster-level contrastive learning (via a cluster projector) in an unsupervised manner. Thereafter, multiple layers of feature representations are extracted from the trained network, upon which a set of diversified base clusterings can be generated via a highly efficient clusterer. Then, the reliability of the clusters in multiple base clusterings is automatically estimated by exploiting an entropy-based criterion, based on which the multiple base clusterings are further formulated into a weighted-cluster bipartite graph. By partitioning this bipartite graph via transfer cut, the final image clustering result can therefore be obtained. Experimental results on six image datasets confirm the advantages of our DeepCluE approach over the state-of-the-art deep clustering approaches.



### Elucidating the Design Space of Diffusion-Based Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2206.00364v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2206.00364v2)
- **Published**: 2022-06-01 10:03:24+00:00
- **Updated**: 2022-10-11 13:20:30+00:00
- **Authors**: Tero Karras, Miika Aittala, Timo Aila, Samuli Laine
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.



### Strongly Augmented Contrastive Clustering
- **Arxiv ID**: http://arxiv.org/abs/2206.00380v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.00380v2)
- **Published**: 2022-06-01 10:30:59+00:00
- **Updated**: 2022-07-14 11:28:50+00:00
- **Authors**: Xiaozhi Deng, Dong Huang, Ding-Hua Chen, Chang-Dong Wang, Jian-Huang Lai
- **Comment**: None
- **Journal**: None
- **Summary**: Deep clustering has attracted increasing attention in recent years due to its capability of joint representation learning and clustering via deep neural networks. In its latest developments, the contrastive learning has emerged as an effective technique to substantially enhance the deep clustering performance. However, the existing contrastive learning based deep clustering algorithms mostly focus on some carefully-designed augmentations (often with limited transformations to preserve the structure), referred to as weak augmentations, but cannot go beyond the weak augmentations to explore the more opportunities in stronger augmentations (with more aggressive transformations or even severe distortions). In this paper, we present an end-to-end deep clustering approach termed Strongly Augmented Contrastive Clustering (SACC), which extends the conventional two-augmentation-view paradigm to multiple views and jointly leverages strong and weak augmentations for strengthened deep clustering. Particularly, we utilize a backbone network with triply-shared weights, where a strongly augmented view and two weakly augmented views are incorporated. Based on the representations produced by the backbone, the weak-weak view pair and the strong-weak view pairs are simultaneously exploited for the instance-level contrastive learning (via an instance projector) and the cluster-level contrastive learning (via a cluster projector), which, together with the backbone, can be jointly optimized in a purely unsupervised manner. Experimental results on five challenging image datasets have shown the superiority of our SACC approach over the state-of-the-art. The code is available at https://github.com/dengxiaozhi/SACC.



### Empirical Study of Quality Image Assessment for Synthesis of Fetal Head Ultrasound Imaging with DCGANs
- **Arxiv ID**: http://arxiv.org/abs/2206.01731v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2206.01731v2)
- **Published**: 2022-06-01 10:36:43+00:00
- **Updated**: 2022-06-27 23:05:21+00:00
- **Authors**: Thea Bautista, Jacqueline Matthew, Hamideh Kerdegari, Laura Peralta Pereira, Miguel Xochicale
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present an empirical study of DCGANs, including hyperparameter heuristics and image quality assessment, as a way to address the scarcity of datasets to investigate fetal head ultrasound. We present experiments to show the impact of different image resolutions, epochs, dataset size input, and learning rates for quality image assessment on four metrics: mutual information (MI), Fr\'echet inception distance (FID), peak-signal-to-noise ratio (PSNR), and local binary pattern vector (LBPv). The results show that FID and LBPv have stronger relationship with clinical image quality scores. The resources to reproduce this work are available at \url{https://github.com/budai4medtech/miua2022}.



### Generalized Supervised Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.00384v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.00384v2)
- **Published**: 2022-06-01 10:38:21+00:00
- **Updated**: 2023-05-21 16:50:33+00:00
- **Authors**: Jaewon Kim, Hyukjong Lee, Jooyoung Chang, Sang Min Park
- **Comment**: None
- **Journal**: None
- **Summary**: With the recent promising results of contrastive learning in the self-supervised learning paradigm, supervised contrastive learning has successfully extended these contrastive approaches to supervised contexts, outperforming cross-entropy on various datasets. However, supervised contrastive learning inherently employs label information in a binary form--either positive or negative--using a one-hot target vector. This structure struggles to adapt to methods that exploit label information as a probability distribution, such as CutMix and knowledge distillation. In this paper, we introduce a generalized supervised contrastive loss, which measures cross-entropy between label similarity and latent similarity. This concept enhances the capabilities of supervised contrastive loss by fully utilizing the label distribution and enabling the adaptation of various existing techniques for training modern neural networks. Leveraging this generalized supervised contrastive loss, we construct a tailored framework: the Generalized Supervised Contrastive Learning (GenSCL). Compared to existing contrastive learning frameworks, GenSCL incorporates additional enhancements, including advanced image-based regularization techniques and an arbitrary teacher classifier. When applied to ResNet50 with the Momentum Contrast technique, GenSCL achieves a top-1 accuracy of 77.3% on ImageNet, a 4.1% relative improvement over traditional supervised contrastive learning. Moreover, our method establishes new state-of-the-art accuracies of 98.2% and 87.0% on CIFAR10 and CIFAR100 respectively when applied to ResNet50, marking the highest reported figures for this architecture.



### DiVAE: Photorealistic Images Synthesis with Denoising Diffusion Decoder
- **Arxiv ID**: http://arxiv.org/abs/2206.00386v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.00386v1)
- **Published**: 2022-06-01 10:39:12+00:00
- **Updated**: 2022-06-01 10:39:12+00:00
- **Authors**: Jie Shi, Chenfei Wu, Jian Liang, Xiang Liu, Nan Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Recently most successful image synthesis models are multi stage process to combine the advantages of different methods, which always includes a VAE-like model for faithfully reconstructing embedding to image and a prior model to generate image embedding. At the same time, diffusion models have shown be capacity to generate high-quality synthetic images. Our work proposes a VQ-VAE architecture model with a diffusion decoder (DiVAE) to work as the reconstructing component in image synthesis. We explore how to input image embedding into diffusion model for excellent performance and find that simple modification on diffusion's UNet can achieve it. Training on ImageNet, Our model achieves state-of-the-art results and generates more photorealistic images specifically. In addition, we apply the DiVAE with an Auto-regressive generator on conditional synthesis tasks to perform more human-feeling and detailed samples.



### A comparative study between vision transformers and CNNs in digital pathology
- **Arxiv ID**: http://arxiv.org/abs/2206.00389v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2206.00389v1)
- **Published**: 2022-06-01 10:41:11+00:00
- **Updated**: 2022-06-01 10:41:11+00:00
- **Authors**: Luca Deininger, Bernhard Stimpel, Anil Yuce, Samaneh Abbasi-Sureshjani, Simon Schönenberger, Paolo Ocampo, Konstanty Korski, Fabien Gaire
- **Comment**: 8 pages, 2 figures, accepted for workshop T4Vision (CVPR 2022)
- **Journal**: None
- **Summary**: Recently, vision transformers were shown to be capable of outperforming convolutional neural networks when pretrained on sufficient amounts of data. In comparison to convolutional neural networks, vision transformers have a weaker inductive bias and therefore allow a more flexible feature detection. Due to their promising feature detection, this work explores vision transformers for tumor detection in digital pathology whole slide images in four tissue types, and for tissue type identification. We compared the patch-wise classification performance of the vision transformer DeiT-Tiny to the state-of-the-art convolutional neural network ResNet18. Due to the sparse availability of annotated whole slide images, we further compared both models pretrained on large amounts of unlabeled whole-slide images using state-of-the-art self-supervised approaches. The results show that the vision transformer performed slightly better than the ResNet18 for three of four tissue types for tumor detection while the ResNet18 performed slightly better for the remaining tasks. The aggregated predictions of both models on slide level were correlated, indicating that the models captured similar imaging features. All together, the vision transformer models performed on par with the ResNet18 while requiring more effort to train. In order to surpass the performance of convolutional neural networks, vision transformers might require more challenging tasks to benefit from their weak inductive bias.



### Towards Generalisable Audio Representations for Audio-Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/2206.00393v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, cs.RO, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2206.00393v1)
- **Published**: 2022-06-01 11:00:07+00:00
- **Updated**: 2022-06-01 11:00:07+00:00
- **Authors**: Shunqi Mao, Chaoyi Zhang, Heng Wang, Weidong Cai
- **Comment**: CVPR 2022 Embodied AI Workshop
- **Journal**: None
- **Summary**: In audio-visual navigation (AVN), an intelligent agent needs to navigate to a constantly sound-making object in complex 3D environments based on its audio and visual perceptions. While existing methods attempt to improve the navigation performance with preciously designed path planning or intricate task settings, none has improved the model generalisation on unheard sounds with task settings unchanged. We thus propose a contrastive learning-based method to tackle this challenge by regularising the audio encoder, where the sound-agnostic goal-driven latent representations can be learnt from various audio signals of different classes. In addition, we consider two data augmentation strategies to enrich the training sounds. We demonstrate that our designs can be easily equipped to existing AVN frameworks to obtain an immediate performance gain (13.4%$\uparrow$ in SPL on Replica and 12.2%$\uparrow$ in SPL on MP3D). Our project is available at https://AV-GeN.github.io/.



### Learning Invariant Visual Representations for Compositional Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.00415v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00415v3)
- **Published**: 2022-06-01 11:33:33+00:00
- **Updated**: 2022-07-18 10:05:50+00:00
- **Authors**: Tian Zhang, Kongming Liang, Ruoyi Du, Xian Sun, Zhanyu Ma, Jun Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Compositional Zero-Shot Learning (CZSL) aims to recognize novel compositions using knowledge learned from seen attribute-object compositions in the training set. Previous works mainly project an image and a composition into a common embedding space to measure their compatibility score. However, both attributes and objects share the visual representations learned above, leading the model to exploit spurious correlations and bias towards seen pairs. Instead, we reconsider CZSL as an out-of-distribution generalization problem. If an object is treated as a domain, we can learn object-invariant features to recognize the attributes attached to any object reliably. Similarly, attribute-invariant features can also be learned when recognizing the objects with attributes as domains. Specifically, we propose an invariant feature learning framework to align different domains at the representation and gradient levels to capture the intrinsic characteristics associated with the tasks. Experiments on two CZSL benchmarks demonstrate that the proposed method significantly outperforms the previous state-of-the-art.



### OmniXAI: A Library for Explainable AI
- **Arxiv ID**: http://arxiv.org/abs/2206.01612v8
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, 68T09, 68T20, 68T01, I.2.6; I.2.5
- **Links**: [PDF](http://arxiv.org/pdf/2206.01612v8)
- **Published**: 2022-06-01 11:35:37+00:00
- **Updated**: 2022-12-12 09:26:32+00:00
- **Authors**: Wenzhuo Yang, Hung Le, Tanmay Laud, Silvio Savarese, Steven C. H. Hoi
- **Comment**: Github repo: https://github.com/salesforce/OmniXAI
- **Journal**: None
- **Summary**: We introduce OmniXAI (short for Omni eXplainable AI), an open-source Python library of eXplainable AI (XAI), which offers omni-way explainable AI capabilities and various interpretable machine learning techniques to address the pain points of understanding and interpreting the decisions made by machine learning (ML) in practice. OmniXAI aims to be a one-stop comprehensive library that makes explainable AI easy for data scientists, ML researchers and practitioners who need explanation for various types of data, models and explanation methods at different stages of ML process (data exploration, feature engineering, model development, evaluation, and decision-making, etc). In particular, our library includes a rich family of explanation methods integrated in a unified interface, which supports multiple data types (tabular data, images, texts, time-series), multiple types of ML models (traditional ML in Scikit-learn and deep learning models in PyTorch/TensorFlow), and a range of diverse explanation methods including "model-specific" and "model-agnostic" ones (such as feature-attribution explanation, counterfactual explanation, gradient-based explanation, etc). For practitioners, the library provides an easy-to-use unified interface to generate the explanations for their applications by only writing a few lines of codes, and also a GUI dashboard for visualization of different explanations for more insights about decisions. In this technical report, we present OmniXAI's design principles, system architectures, and major functionalities, and also demonstrate several example use cases across different types of data, tasks, and models.



### Evaluating Gaussian Grasp Maps for Generative Grasping Models
- **Arxiv ID**: http://arxiv.org/abs/2206.00432v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.00432v1)
- **Published**: 2022-06-01 12:17:20+00:00
- **Updated**: 2022-06-01 12:17:20+00:00
- **Authors**: William Prew, Toby P. Breckon, Magnus Bordewich, Ulrik Beierholm
- **Comment**: 9 pages, 6 figures, to be published in IJCNN 2022
- **Journal**: None
- **Summary**: Generalising robotic grasping to previously unseen objects is a key task in general robotic manipulation. The current method for training many antipodal generative grasping models rely on a binary ground truth grasp map generated from the centre thirds of correctly labelled grasp rectangles. However, these binary maps do not accurately reflect the positions in which a robotic arm can correctly grasp a given object. We propose a continuous Gaussian representation of annotated grasps to generate ground truth training data which achieves a higher success rate on a simulated robotic grasping benchmark. Three modern generative grasping networks are trained with either binary or Gaussian grasp maps, along with recent advancements from the robotic grasping literature, such as discretisation of grasp angles into bins and an attentional loss function. Despite negligible difference according to the standard rectangle metric, Gaussian maps better reproduce the training data and therefore improve success rates when tested on the same simulated robot arm by avoiding collisions with the object: achieving 87.94\% accuracy. Furthermore, the best performing model is shown to operate with a high success rate when transferred to a real robotic arm, at high inference speeds, without the need for transfer learning. The system is then shown to be capable of performing grasps on an antagonistic physical object dataset benchmark.



### CD$^2$: Fine-grained 3D Mesh Reconstruction With Twice Chamfer Distance
- **Arxiv ID**: http://arxiv.org/abs/2206.00447v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00447v3)
- **Published**: 2022-06-01 12:29:25+00:00
- **Updated**: 2023-01-29 16:02:20+00:00
- **Authors**: Rongfei Zeng, Mai Su, Ruiyun Yu, Xingwei Wang
- **Comment**: Just accepted by TOMM
- **Journal**: None
- **Summary**: Monocular 3D reconstruction is to reconstruct the shape of object and its other information from a single RGB image. In 3D reconstruction, polygon mesh, with detailed surface information and low computational cost, is the most prevalent expression form obtained from deep learning models. However, the state-of-the-art schemes fail to directly generate well-structured meshes, and we identify that most meshes have severe Vertices Clustering (VC) and Illegal Twist (IT) problems. By analyzing the mesh deformation process, we pinpoint that the inappropriate usage of Chamfer Distance (CD) loss is a root cause of VC and IT problems in deep learning model. In this paper, we initially demonstrate these two problems induced by CD loss with visual examples and quantitative analyses. Then, we propose a fine-grained reconstruction method CD$^2$ by employing Chamfer distance twice to perform a plausible and adaptive deformation. Extensive experiments on two 3D datasets and comparisons with five latest schemes demonstrate that our CD$^2$ directly generates a well-structured mesh and outperforms others in terms of several quantitative metrics.



### PanopticDepth: A Unified Framework for Depth-aware Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.00468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00468v1)
- **Published**: 2022-06-01 13:00:49+00:00
- **Updated**: 2022-06-01 13:00:49+00:00
- **Authors**: Naiyu Gao, Fei He, Jian Jia, Yanhu Shan, Haoyang Zhang, Xin Zhao, Kaiqi Huang
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: This paper presents a unified framework for depth-aware panoptic segmentation (DPS), which aims to reconstruct 3D scene with instance-level semantics from one single image. Prior works address this problem by simply adding a dense depth regression head to panoptic segmentation (PS) networks, resulting in two independent task branches. This neglects the mutually-beneficial relations between these two tasks, thus failing to exploit handy instance-level semantic cues to boost depth accuracy while also producing sub-optimal depth maps. To overcome these limitations, we propose a unified framework for the DPS task by applying a dynamic convolution technique to both the PS and depth prediction tasks. Specifically, instead of predicting depth for all pixels at a time, we generate instance-specific kernels to predict depth and segmentation masks for each instance. Moreover, leveraging the instance-wise depth estimation scheme, we add additional instance-level depth cues to assist with supervising the depth learning via a new depth loss. Extensive experiments on Cityscapes-DPS and SemKITTI-DPS show the effectiveness and promise of our method. We hope our unified solution to DPS can lead a new paradigm in this area. Code is available at https://github.com/NaiyuGao/PanopticDepth.



### Augmentation Component Analysis: Modeling Similarity via the Augmentation Overlaps
- **Arxiv ID**: http://arxiv.org/abs/2206.00471v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.00471v3)
- **Published**: 2022-06-01 13:03:58+00:00
- **Updated**: 2023-02-16 15:12:39+00:00
- **Authors**: Lu Han, Han-Jia Ye, De-Chuan Zhan
- **Comment**: Accept to ICLR 2023
- **Journal**: None
- **Summary**: Self-supervised learning aims to learn a embedding space where semantically similar samples are close. Contrastive learning methods pull views of samples together and push different samples away, which utilizes semantic invariance of augmentation but ignores the relationship between samples. To better exploit the power of augmentation, we observe that semantically similar samples are more likely to have similar augmented views. Therefore, we can take the augmented views as a special description of a sample. In this paper, we model such a description as the augmentation distribution and we call it augmentation feature. The similarity in augmentation feature reflects how much the views of two samples overlap and is related to their semantical similarity. Without computational burdens to explicitly estimate values of the augmentation feature, we propose Augmentation Component Analysis (ACA) with a contrastive-like loss to learn principal components and an on-the-fly projection loss to embed data. ACA equals an efficient dimension reduction by PCA and extracts low-dimensional embeddings, theoretically preserving the similarity of augmentation distribution between samples. Empirical results show our method can achieve competitive results against various traditional contrastive learning methods on different benchmarks.



### Where are my Neighbors? Exploiting Patches Relations in Self-Supervised Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2206.00481v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.00481v2)
- **Published**: 2022-06-01 13:25:32+00:00
- **Updated**: 2022-10-13 14:11:34+00:00
- **Authors**: Guglielmo Camporese, Elena Izzo, Lamberto Ballan
- **Comment**: Accepted to BMVC 2022
- **Journal**: None
- **Summary**: Vision Transformers (ViTs) enabled the use of the transformer architecture on vision tasks showing impressive performances when trained on big datasets. However, on relatively small datasets, ViTs are less accurate given their lack of inductive bias. To this end, we propose a simple but still effective Self-Supervised Learning (SSL) strategy to train ViTs, that without any external annotation or external data, can significantly improve the results. Specifically, we define a set of SSL tasks based on relations of image patches that the model has to solve before or jointly the supervised task. Differently from ViT, our RelViT model optimizes all the output tokens of the transformer encoder that are related to the image patches, thus exploiting more training signals at each training step. We investigated our methods on several image benchmarks finding that RelViT improves the SSL state-of-the-art methods by a large margin, especially on small datasets. Code is available at: https://github.com/guglielmocamporese/relvit.



### Attack-Agnostic Adversarial Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.00489v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00489v1)
- **Published**: 2022-06-01 13:41:40+00:00
- **Updated**: 2022-06-01 13:41:40+00:00
- **Authors**: Jiaxin Cheng, Mohamed Hussein, Jay Billa, Wael AbdAlmageed
- **Comment**: None
- **Journal**: None
- **Summary**: The growing number of adversarial attacks in recent years gives attackers an advantage over defenders, as defenders must train detectors after knowing the types of attacks, and many models need to be maintained to ensure good performance in detecting any upcoming attacks. We propose a way to end the tug-of-war between attackers and defenders by treating adversarial attack detection as an anomaly detection problem so that the detector is agnostic to the attack. We quantify the statistical deviation caused by adversarial perturbations in two aspects. The Least Significant Component Feature (LSCF) quantifies the deviation of adversarial examples from the statistics of benign samples and Hessian Feature (HF) reflects how adversarial examples distort the landscape of the model's optima by measuring the local loss curvature. Empirical results show that our method can achieve an overall ROC AUC of 94.9%, 89.7%, and 94.6% on CIFAR10, CIFAR100, and SVHN, respectively, and has comparable performance to adversarial detectors trained with adversarial examples on most of the attacks.



### Semantic Room Wireframe Detection from a Single View
- **Arxiv ID**: http://arxiv.org/abs/2206.00491v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00491v1)
- **Published**: 2022-06-01 13:44:50+00:00
- **Updated**: 2022-06-01 13:44:50+00:00
- **Authors**: David Gillsjö, Gabrielle Flood, Kalle Åström
- **Comment**: Accepted for ICPR2022
- **Journal**: None
- **Summary**: Reconstruction of indoor surfaces with limited texture information or with repeated textures, a situation common in walls and ceilings, may be difficult with a monocular Structure from Motion system. We propose a Semantic Room Wireframe Detection task to predict a Semantic Wireframe from a single perspective image. Such predictions may be used with shape priors to estimate the Room Layout and aid reconstruction. To train and test the proposed algorithm we create a new set of annotations from the simulated Structured3D dataset. We show qualitatively that the SRW-Net handles complex room geometries better than previous Room Layout Estimation algorithms while quantitatively out-performing the baseline in non-semantic Wireframe Detection.



### Proximally Sensitive Error for Anomaly Detection and Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.00506v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.00506v1)
- **Published**: 2022-06-01 14:06:04+00:00
- **Updated**: 2022-06-01 14:06:04+00:00
- **Authors**: Amogh Gudi, Fritjof Büttner, Jan van Gemert
- **Comment**: None
- **Journal**: None
- **Summary**: Mean squared error (MSE) is one of the most widely used metrics to expression differences between multi-dimensional entities, including images. However, MSE is not locally sensitive as it does not take into account the spatial arrangement of the (pixel) differences, which matters for structured data types like images. Such spatial arrangements carry information about the source of the differences; therefore, an error function that also incorporates the location of errors can lead to a more meaningful distance measure. We introduce Proximally Sensitive Error (PSE), through which we suggest that a regional emphasis in the error measure can 'highlight' semantic differences between images over syntactic/random deviations. We demonstrate that this emphasis can be leveraged upon for the task of anomaly/occlusion detection. We further explore its utility as a loss function to help a model focus on learning representations of semantic objects instead of minimizing syntactic reconstruction noise.



### Landslide4Sense: Reference Benchmark Data and Deep Learning Models for Landslide Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.00515v3
- **DOI**: 10.1109/TGRS.2022.3215209
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.00515v3)
- **Published**: 2022-06-01 14:18:23+00:00
- **Updated**: 2022-12-20 11:10:48+00:00
- **Authors**: Omid Ghorbanzadeh, Yonghao Xu, Pedram Ghamisi, Michael Kopp, David Kreil
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp.
  1-17, 2022
- **Summary**: This study introduces \textit{Landslide4Sense}, a reference benchmark for landslide detection from remote sensing. The repository features 3,799 image patches fusing optical layers from Sentinel-2 sensors with the digital elevation model and slope layer derived from ALOS PALSAR. The added topographical information facilitates the accurate detection of landslide borders, which recent researches have shown to be challenging using optical data alone. The extensive data set supports deep learning (DL) studies in landslide detection and the development and validation of methods for the systematic update of landslide inventories. The benchmark data set has been collected at four different times and geographical locations: Iburi (September 2018), Kodagu (August 2018), Gorkha (April 2015), and Taiwan (August 2009). Each image pixel is labelled as belonging to a landslide or not, incorporating various sources and thorough manual annotation. We then evaluate the landslide detection performance of 11 state-of-the-art DL segmentation models: U-Net, ResU-Net, PSPNet, ContextNet, DeepLab-v2, DeepLab-v3+, FCN-8s, LinkNet, FRRN-A, FRRN-B, and SQNet. All models were trained from scratch on patches from one quarter of each study area and tested on independent patches from the other three quarters. Our experiments demonstrate that ResU-Net outperformed the other models for the landslide detection task. We make the multi-source landslide benchmark data (Landslide4Sense) and the tested DL models publicly available at \url{https://www.iarai.ac.at/landslide4sense}, establishing an important resource for remote sensing, computer vision, and machine learning communities in studies of image classification in general and applications to landslide detection in particular.



### Amodal Cityscapes: A New Dataset, its Generation, and an Amodal Semantic Segmentation Challenge Baseline
- **Arxiv ID**: http://arxiv.org/abs/2206.00527v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00527v1)
- **Published**: 2022-06-01 14:38:33+00:00
- **Updated**: 2022-06-01 14:38:33+00:00
- **Authors**: Jasmin Breitenstein, Tim Fingscheidt
- **Comment**: This paper is accepted at IEEE Intelligent Vehicles Symposium 2022
- **Journal**: None
- **Summary**: Amodal perception terms the ability of humans to imagine the entire shapes of occluded objects. This gives humans an advantage to keep track of everything that is going on, especially in crowded situations. Typical perception functions, however, lack amodal perception abilities and are therefore at a disadvantage in situations with occlusions. Complex urban driving scenarios often experience many different types of occlusions and, therefore, amodal perception for automated vehicles is an important task to investigate. In this paper, we consider the task of amodal semantic segmentation and propose a generic way to generate datasets to train amodal semantic segmentation methods. We use this approach to generate an amodal Cityscapes dataset. Moreover, we propose and evaluate a method as baseline on Amodal Cityscapes, showing its applicability for amodal semantic segmentation in automotive environment perception. We provide the means to re-generate this dataset on github.



### Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines
- **Arxiv ID**: http://arxiv.org/abs/2206.00535v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2206.00535v3)
- **Published**: 2022-06-01 14:43:49+00:00
- **Updated**: 2023-04-10 17:14:43+00:00
- **Authors**: Camilo Fosco, Emilie Josephs, Alex Andonian, Allen Lee, Xi Wang, Aude Oliva
- **Comment**: 9 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Deepfakes pose a serious threat to digital well-being by fueling misinformation. As deepfakes get harder to recognize with the naked eye, human users become increasingly reliant on deepfake detection models to decide if a video is real or fake. Currently, models yield a prediction for a video's authenticity, but do not integrate a method for alerting a human user. We introduce a framework for amplifying artifacts in deepfake videos to make them more detectable by people. We propose a novel, semi-supervised Artifact Attention module, which is trained on human responses to create attention maps that highlight video artifacts. These maps make two contributions. First, they improve the performance of our deepfake detection classifier. Second, they allow us to generate novel "Deepfake Caricatures": transformations of the deepfake that exacerbate artifacts to improve human detection. In a user study, we demonstrate that Caricatures greatly increase human detection, across video presentation times and user engagement levels. Overall, we demonstrate the success of a human-centered approach to designing deepfake mitigation methods.



### Impact of loss function in Deep Learning methods for accurate retinal vessel segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.00536v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.00536v1)
- **Published**: 2022-06-01 14:47:18+00:00
- **Updated**: 2022-06-01 14:47:18+00:00
- **Authors**: Daniela Herrera, Gilberto Ochoa-Ruiz, Miguel Gonzalez-Mendoza, Christian Mata
- **Comment**: Paper submitted to MICAI 2022
- **Journal**: None
- **Summary**: The retinal vessel network studied through fundus images contributes to the diagnosis of multiple diseases not only found in the eye. The segmentation of this system may help the specialized task of analyzing these images by assisting in the quantification of morphological characteristics. Due to its relevance, several Deep Learning-based architectures have been tested for tackling this problem automatically. However, the impact of loss function selection on the segmentation of the intricate retinal blood vessel system hasn't been systematically evaluated. In this work, we present the comparison of the loss functions Binary Cross Entropy, Dice, Tversky, and Combo loss using the deep learning architectures (i.e. U-Net, Attention U-Net, and Nested UNet) with the DRIVE dataset. Their performance is assessed using four metrics: the AUC, the mean squared error, the dice score, and the Hausdorff distance. The models were trained with the same number of parameters and epochs. Using dice score and AUC, the best combination was SA-UNet with Combo loss, which had an average of 0.9442 and 0.809 respectively. The best average of Hausdorff distance and mean square error were obtained using the Nested U-Net with the Dice loss function, which had an average of 6.32 and 0.0241 respectively. The results showed that there is a significant difference in the selection of loss function



### The Fully Convolutional Transformer for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.00566v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.00566v2)
- **Published**: 2022-06-01 15:22:41+00:00
- **Updated**: 2023-01-29 18:19:04+00:00
- **Authors**: Athanasios Tragakis, Chaitanya Kaul, Roderick Murray-Smith, Dirk Husmeier
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV), 2023, pp. 3660-3669
- **Summary**: We propose a novel transformer model, capable of segmenting medical images of varying modalities. Challenges posed by the fine grained nature of medical image analysis mean that the adaptation of the transformer for their analysis is still at nascent stages. The overwhelming success of the UNet lay in its ability to appreciate the fine-grained nature of the segmentation task, an ability which existing transformer based models do not currently posses. To address this shortcoming, we propose The Fully Convolutional Transformer (FCT), which builds on the proven ability of Convolutional Neural Networks to learn effective image representations, and combines them with the ability of Transformers to effectively capture long-term dependencies in its inputs. The FCT is the first fully convolutional Transformer model in medical imaging literature. It processes its input in two stages, where first, it learns to extract long range semantic dependencies from the input image, and then learns to capture hierarchical global attributes from the features. FCT is compact, accurate and robust. Our results show that it outperforms all existing transformer architectures by large margins across multiple medical image segmentation datasets of varying data modalities without the need for any pre-training. FCT outperforms its immediate competitor on the ACDC dataset by 1.3%, on the Synapse dataset by 4.4%, on the Spleen dataset by 1.2% and on ISIC 2017 dataset by 1.1% on the dice metric, with up to five times fewer parameters. Our code, environments and models will be available via GitHub.



### Dog nose print matching with dual global descriptor based on Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.00580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00580v1)
- **Published**: 2022-06-01 15:49:44+00:00
- **Updated**: 2022-06-01 15:49:44+00:00
- **Authors**: Bin Li, Zhongan Wang, Nan Wu, Shuai Shi, Qijun Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies in biometric-based identification tasks have shown that deep learning methods can achieve better performance. These methods generally extract the global features as descriptor to represent the original image. Nonetheless, it does not perform well for biometric identification under fine-grained tasks. The main reason is that the single image descriptor contains insufficient information to represent image. In this paper, we present a dual global descriptor model, which combines multiple global descriptors to exploit multi level image features. Moreover, we utilize a contrastive loss to enlarge the distance between image representations of confusing classes. The proposed framework achieves the top2 on the CVPR2022 Biometrics Workshop Pet Biometric Challenge. The source code and trained models are publicly available at: https://github.com/flyingsheepbin/pet-biometrics



### Topological Deep Learning: Going Beyond Graph Data
- **Arxiv ID**: http://arxiv.org/abs/2206.00606v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SI, math.AT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2206.00606v3)
- **Published**: 2022-06-01 16:21:28+00:00
- **Updated**: 2023-05-19 22:13:16+00:00
- **Authors**: Mustafa Hajij, Ghada Zamzmi, Theodore Papamarkou, Nina Miolane, Aldo Guzmán-Sáenz, Karthikeyan Natesan Ramamurthy, Tolga Birdal, Tamal K. Dey, Soham Mukherjee, Shreyas N. Samaga, Neal Livesay, Robin Walters, Paul Rosen, Michael T. Schaub
- **Comment**: None
- **Journal**: None
- **Summary**: Topological deep learning is a rapidly growing field that pertains to the development of deep learning models for data supported on topological domains such as simplicial complexes, cell complexes, and hypergraphs, which generalize many domains encountered in scientific computations. In this paper, we present a unifying deep learning framework built upon a richer data structure that includes widely adopted topological domains.   Specifically, we first introduce combinatorial complexes, a novel type of topological domain. Combinatorial complexes can be seen as generalizations of graphs that maintain certain desirable properties. Similar to hypergraphs, combinatorial complexes impose no constraints on the set of relations. In addition, combinatorial complexes permit the construction of hierarchical higher-order relations, analogous to those found in simplicial and cell complexes. Thus, combinatorial complexes generalize and combine useful traits of both hypergraphs and cell complexes, which have emerged as two promising abstractions that facilitate the generalization of graph neural networks to topological spaces.   Second, building upon combinatorial complexes and their rich combinatorial and algebraic structure, we develop a general class of message-passing combinatorial complex neural networks (CCNNs), focusing primarily on attention-based CCNNs. We characterize permutation and orientation equivariances of CCNNs, and discuss pooling and unpooling operations within CCNNs in detail.   Third, we evaluate the performance of CCNNs on tasks related to mesh shape analysis and graph learning. Our experiments demonstrate that CCNNs have competitive performance as compared to state-of-the-art deep learning models specifically tailored to the same tasks. Our findings demonstrate the advantages of incorporating higher-order relations into deep learning models in different applications.



### On the Choice of Data for Efficient Training and Validation of End-to-End Driving Models
- **Arxiv ID**: http://arxiv.org/abs/2206.00608v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.00608v1)
- **Published**: 2022-06-01 16:25:28+00:00
- **Updated**: 2022-06-01 16:25:28+00:00
- **Authors**: Marvin Klingner, Konstantin Müller, Mona Mirzaie, Jasmin Breitenstein, Jan-Aike Termöhlen, Tim Fingscheidt
- **Comment**: Accepted at CVPR VDU Workshop 2022
- **Journal**: None
- **Summary**: The emergence of data-driven machine learning (ML) has facilitated significant progress in many complicated tasks such as highly-automated driving. While much effort is put into improving the ML models and learning algorithms in such applications, little focus is put into how the training data and/or validation setting should be designed. In this paper we investigate the influence of several data design choices regarding training and validation of deep driving models trainable in an end-to-end fashion. Specifically, (i) we investigate how the amount of training data influences the final driving performance, and which performance limitations are induced through currently used mechanisms to generate training data. (ii) Further, we show by correlation analysis, which validation design enables the driving performance measured during validation to generalize well to unknown test environments. (iii) Finally, we investigate the effect of random seeding and non-determinism, giving insights which reported improvements can be deemed significant. Our evaluations using the popular CARLA simulator provide recommendations regarding data generation and driving route selection for an efficient future development of end-to-end driving models.



### Dual-stream spatiotemporal networks with feature sharing for monitoring animals in the home cage
- **Arxiv ID**: http://arxiv.org/abs/2206.00614v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00614v2)
- **Published**: 2022-06-01 16:32:25+00:00
- **Updated**: 2022-11-03 23:02:29+00:00
- **Authors**: Ezechukwu I. Nwokedi, Rasneer S. Bains, Luc Bidaut, Xujiong Ye, Sara Wells, James M. Brown
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a spatiotemporal deep learning approach for mouse behavioural classification in the home-cage. Using a series of dual-stream architectures with assorted modifications to increase performance, we introduce a novel feature sharing approach that jointly processes the streams at regular intervals throughout the network. To investigate the efficacy of this approach, models were evaluated by dissociating the streams and training/testing in the same rigorous manner as the main classifiers. Using an annotated, publicly available dataset of a singly-housed mice, we achieve prediction accuracy of 86.47% using an ensemble of a Inception-based network and an attention-based network, both of which utilize this feature sharing. We also demonstrate through ablation studies that for all models, the feature-sharing architectures consistently perform better than conventional ones having separate streams. The best performing models were further evaluated on other activity datasets, both mouse and human. Future work will investigate the effectiveness of feature sharing to behavioural classification in the unsupervised anomaly detection domain.



### Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2206.00621v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.00621v2)
- **Published**: 2022-06-01 16:45:24+00:00
- **Updated**: 2023-06-12 12:47:16+00:00
- **Authors**: Yan Zeng, Wangchunshu Zhou, Ao Luo, Ziming Cheng, Xinsong Zhang
- **Comment**: ACL 2023
- **Journal**: None
- **Summary**: In this paper, we introduce Cross-View Language Modeling, a simple and effective pre-training framework that unifies cross-lingual and cross-modal pre-training with shared architectures and objectives. Our approach is motivated by a key observation that cross-lingual and cross-modal pre-training share the same goal of aligning two different views of the same object into a common semantic space. To this end, the cross-view language modeling framework considers both multi-modal data (i.e., image-caption pairs) and multi-lingual data (i.e., parallel sentence pairs) as two different views of the same object, and trains the model to align the two views by maximizing the mutual information between them with conditional masked language modeling and contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal Language Model, with the cross-view language modeling framework. Empirical results on IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual image-text retrieval datasets show that while conceptually simpler, CCLM significantly outperforms the prior state-of-the-art with an average absolute improvement of over 10%. Moreover, CCLM is the first multi-lingual multi-modal pre-trained model that surpasses the translate-test performance of representative English vision-language models by zero-shot cross-lingual transfer.



### CLIP4IDC: CLIP for Image Difference Captioning
- **Arxiv ID**: http://arxiv.org/abs/2206.00629v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00629v2)
- **Published**: 2022-06-01 17:02:08+00:00
- **Updated**: 2022-10-18 12:30:01+00:00
- **Authors**: Zixin Guo, Tzu-Jui Julius Wang, Jorma Laaksonen
- **Comment**: Accepted to AACL-IJCNLP 2022
- **Journal**: None
- **Summary**: Image Difference Captioning (IDC) aims at generating sentences to describe differences between two similar-looking images. Conventional approaches learn an IDC model with a pre-trained and usually frozen visual feature extractor. Accordingly, two major issues may arise: (1) a large domain gap usually exists between the pre-training datasets used for training such a visual encoder and that of the downstream IDC task, and (2) the visual feature extractor, when separately encoding two images, often does not effectively encode the visual changes between two images. Due to the excellent zero-shot performance of the recently proposed CLIP, we thus propose CLIP4IDC to transfer a CLIP model for the IDC task to address those issues. Different from directly fine-tuning CLIP to generate sentences, we introduce an adaptation training process to adapt CLIP's visual encoder to capture and align differences in image pairs based on the textual descriptions. Experiments on three IDC benchmark datasets, CLEVR-Change, Spot-the-Diff, and Image-Editing-Request, demonstrate the effectiveness of CLIP4IDC.



### Unifying Voxel-based Representation with Transformer for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.00630v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00630v2)
- **Published**: 2022-06-01 17:02:40+00:00
- **Updated**: 2022-10-13 03:32:33+00:00
- **Authors**: Yanwei Li, Yilun Chen, Xiaojuan Qi, Zeming Li, Jian Sun, Jiaya Jia
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: In this work, we present a unified framework for multi-modality 3D object detection, named UVTR. The proposed method aims to unify multi-modality representations in the voxel space for accurate and robust single- or cross-modality 3D detection. To this end, the modality-specific space is first designed to represent different inputs in the voxel feature space. Different from previous work, our approach preserves the voxel space without height compression to alleviate semantic ambiguity and enable spatial connections. To make full use of the inputs from different sensors, the cross-modality interaction is then proposed, including knowledge transfer and modality fusion. In this way, geometry-aware expressions in point clouds and context-rich features in images are well utilized for better performance and robustness. The transformer decoder is applied to efficiently sample features from the unified space with learnable positions, which facilitates object-level interactions. In general, UVTR presents an early attempt to represent different modalities in a unified framework. It surpasses previous work in single- or multi-modality entries. The proposed method achieves leading performance in the nuScenes test set for both object detection and the following object tracking task. Code is made publicly available at https://github.com/dvlab-research/UVTR.



### Floorplan Restoration by Structure Hallucinating Transformer Cascades
- **Arxiv ID**: http://arxiv.org/abs/2206.00645v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.00645v3)
- **Published**: 2022-06-01 17:29:26+00:00
- **Updated**: 2023-05-31 04:25:55+00:00
- **Authors**: Sepidehsadat Hosseini, Yasutaka Furukawa
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an extreme floorplan reconstruction task, a new benchmark for the task, and a neural architecture as a solution. Given a partial floorplan reconstruction inferred or curated from panorama images, the task is to reconstruct a complete floorplan including invisible architectural structures. The proposed neural network 1) encodes an input partial floorplan into a set of latent vectors by convolutional neural networks and a Transformer; and 2) reconstructs an entire floorplan while hallucinating invisible rooms and doors by cascading Transformer decoders. Qualitative and quantitative evaluations demonstrate effectiveness of our approach over the benchmark of 701 houses, outperforming the state-of-the-art reconstruction techniques. We will share our code, models, and data.



### MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2206.00665v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00665v2)
- **Published**: 2022-06-01 17:58:15+00:00
- **Updated**: 2022-10-12 12:16:03+00:00
- **Authors**: Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, Andreas Geiger
- **Comment**: Project page: https://niujinshuchong.github.io/monosdf/
- **Journal**: None
- **Summary**: In recent years, neural implicit surface reconstruction methods have become popular for multi-view 3D reconstruction. In contrast to traditional multi-view stereo methods, these approaches tend to produce smoother and more complete reconstructions due to the inductive smoothness bias of neural networks. State-of-the-art neural implicit methods allow for high-quality reconstructions of simple scenes from many input views. Yet, their performance drops significantly for larger and more complex scenes and scenes captured from sparse viewpoints. This is caused primarily by the inherent ambiguity in the RGB reconstruction loss that does not provide enough constraints, in particular in less-observed and textureless areas. Motivated by recent advances in the area of monocular geometry prediction, we systematically explore the utility these cues provide for improving neural implicit surface reconstruction. We demonstrate that depth and normal cues, predicted by general-purpose monocular estimators, significantly improve reconstruction quality and optimization time. Further, we analyse and investigate multiple design choices for representing neural implicit surfaces, ranging from monolithic MLP models over single-grid to multi-resolution grid representations. We observe that geometric monocular priors improve performance both for small-scale single-object as well as large-scale multi-object scenes, independent of the choice of representation.



### Context-Driven Detection of Invertebrate Species in Deep-Sea Video
- **Arxiv ID**: http://arxiv.org/abs/2206.00718v1
- **DOI**: 10.1007/s11263-023-01755-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00718v1)
- **Published**: 2022-06-01 18:59:46+00:00
- **Updated**: 2022-06-01 18:59:46+00:00
- **Authors**: R. Austin McEver, Bowen Zhang, Connor Levenson, A S M Iftekhar, B. S. Manjunath
- **Comment**: None
- **Journal**: International Journal of Computer Vision 2023
- **Summary**: Each year, underwater remotely operated vehicles (ROVs) collect thousands of hours of video of unexplored ocean habitats revealing a plethora of information regarding biodiversity on Earth. However, fully utilizing this information remains a challenge as proper annotations and analysis require trained scientists time, which is both limited and costly. To this end, we present a Dataset for Underwater Substrate and Invertebrate Analysis (DUSIA), a benchmark suite and growing large-scale dataset to train, validate, and test methods for temporally localizing four underwater substrates as well as temporally and spatially localizing 59 underwater invertebrate species. DUSIA currently includes over ten hours of footage across 25 videos captured in 1080p at 30 fps by an ROV following pre planned transects across the ocean floor near the Channel Islands of California. Each video includes annotations indicating the start and end times of substrates across the video in addition to counts of species of interest. Some frames are annotated with precise bounding box locations for invertebrate species of interest, as seen in Figure 1. To our knowledge, DUSIA is the first dataset of its kind for deep sea exploration, with video from a moving camera, that includes substrate annotations and invertebrate species that are present at significant depths where sunlight does not penetrate. Additionally, we present the novel context-driven object detector (CDD) where we use explicit substrate classification to influence an object detection network to simultaneously predict a substrate and species class influenced by that substrate. We also present a method for improving training on partially annotated bounding box frames. Finally, we offer a baseline method for automating the counting of invertebrate species of interest.



### Dataset Distillation using Neural Feature Regression
- **Arxiv ID**: http://arxiv.org/abs/2206.00719v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.00719v2)
- **Published**: 2022-06-01 19:02:06+00:00
- **Updated**: 2022-10-24 05:49:56+00:00
- **Authors**: Yongchao Zhou, Ehsan Nezhadarya, Jimmy Ba
- **Comment**: NeurIPS 2022 camera-ready version
- **Journal**: None
- **Summary**: Dataset distillation aims to learn a small synthetic dataset that preserves most of the information from the original dataset. Dataset distillation can be formulated as a bi-level meta-learning problem where the outer loop optimizes the meta-dataset and the inner loop trains a model on the distilled data. Meta-gradient computation is one of the key challenges in this formulation, as differentiating through the inner loop learning procedure introduces significant computation and memory costs. In this paper, we address these challenges using neural Feature Regression with Pooling (FRePo), achieving the state-of-the-art performance with an order of magnitude less memory requirement and two orders of magnitude faster training than previous methods. The proposed algorithm is analogous to truncated backpropagation through time with a pool of models to alleviate various types of overfitting in dataset distillation. FRePo significantly outperforms the previous methods on CIFAR100, Tiny ImageNet, and ImageNet-1K. Furthermore, we show that high-quality distilled data can greatly improve various downstream applications, such as continual learning and membership inference defense. Please check out our webpage at https://sites.google.com/view/frepo.



### Cascaded Video Generation for Videos In-the-Wild
- **Arxiv ID**: http://arxiv.org/abs/2206.00735v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.00735v1)
- **Published**: 2022-06-01 19:50:50+00:00
- **Updated**: 2022-06-01 19:50:50+00:00
- **Authors**: Lluis Castrejon, Nicolas Ballas, Aaron Courville
- **Comment**: Accepted to the 26th International Conference on Pattern Recognition
  (ICPR 2022). arXiv admin note: substantial text overlap with arXiv:2106.02719
- **Journal**: None
- **Summary**: Videos can be created by first outlining a global view of the scene and then adding local details. Inspired by this idea we propose a cascaded model for video generation which follows a coarse to fine approach. First our model generates a low resolution video, establishing the global scene structure, which is then refined by subsequent cascade levels operating at larger resolutions. We train each cascade level sequentially on partial views of the videos, which reduces the computational complexity of our model and makes it scalable to high-resolution videos with many frames. We empirically validate our approach on UCF101 and Kinetics-600, for which our model is competitive with the state-of-the-art. We further demonstrate the scaling capabilities of our model and train a three-level model on the BDD100K dataset which generates 256x256 pixels videos with 48 frames.



### Residual Multiplicative Filter Networks for Multiscale Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2206.00746v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.00746v2)
- **Published**: 2022-06-01 20:16:28+00:00
- **Updated**: 2022-10-26 18:31:40+00:00
- **Authors**: Shayan Shekarforoush, David B. Lindell, David J. Fleet, Marcus A. Brubaker
- **Comment**: NeurIPS 2022, Project page: https://shekshaa.github.io/ResidualMFN
- **Journal**: None
- **Summary**: Coordinate networks like Multiplicative Filter Networks (MFNs) and BACON offer some control over the frequency spectrum used to represent continuous signals such as images or 3D volumes. Yet, they are not readily applicable to problems for which coarse-to-fine estimation is required, including various inverse problems in which coarse-to-fine optimization plays a key role in avoiding poor local minima. We introduce a new coordinate network architecture and training scheme that enables coarse-to-fine optimization with fine-grained control over the frequency support of learned reconstructions. This is achieved with two key innovations. First, we incorporate skip connections so that structure at one scale is preserved when fitting finer-scale structure. Second, we propose a novel initialization scheme to provide control over the model frequency spectrum at each stage of optimization. We demonstrate how these modifications enable multiscale optimization for coarse-to-fine fitting to natural images. We then evaluate our model on synthetically generated datasets for the the problem of single-particle cryo-EM reconstruction. We learn high resolution multiscale structures, on par with the state-of-the art.



### Dynamic Linear Transformer for 3D Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.00771v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00771v2)
- **Published**: 2022-06-01 21:15:01+00:00
- **Updated**: 2023-02-01 17:58:54+00:00
- **Authors**: Zheyuan Zhang, Ulas Bagci
- **Comment**: 8 Pages
- **Journal**: None
- **Summary**: Transformer-based neural networks have surpassed promising performance on many biomedical image segmentation tasks due to a better global information modeling from the self-attention mechanism. However, most methods are still designed for 2D medical images while ignoring the essential 3D volume information. The main challenge for 3D transformer-based segmentation methods is the quadratic complexity introduced by the self-attention mechanism \cite{vaswani2017attention}. In this paper, we propose a novel transformer architecture for 3D medical image segmentation using an encoder-decoder style architecture with linear complexity. Furthermore, we newly introduce a dynamic token concept to further reduce the token numbers for self-attention calculation. Taking advantage of the global information modeling, we provide uncertainty maps from different hierarchy stages. We evaluate this method on multiple challenging CT pancreas segmentation datasets. Our promising results show that our novel 3D Transformer-based segmentor could provide promising highly feasible segmentation performance and accurate uncertainty quantification using single annotation. Code is available https://github.com/freshman97/LinTransUNet.



### Binding Dancers Into Attractors
- **Arxiv ID**: http://arxiv.org/abs/2206.02558v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.02558v1)
- **Published**: 2022-06-01 22:01:29+00:00
- **Updated**: 2022-06-01 22:01:29+00:00
- **Authors**: Franziska Kaltenberger, Sebastian Otte, Martin V. Butz
- **Comment**: None
- **Journal**: None
- **Summary**: To effectively perceive and process observations in our environment, feature binding and perspective taking are crucial cognitive abilities. Feature binding combines observed features into one entity, called a Gestalt. Perspective taking transfers the percept into a canonical, observer-centered frame of reference. Here we propose a recurrent neural network model that solves both challenges. We first train an LSTM to predict 3D motion dynamics from a canonical perspective. We then present similar motion dynamics with novel viewpoints and feature arrangements. Retrospective inference enables the deduction of the canonical perspective. Combined with a robust mutual-exclusive softmax selection scheme, random feature arrangements are reordered and precisely bound into known Gestalt percepts. To corroborate evidence for the architecture's cognitive validity, we examine its behavior on the silhouette illusion, which elicits two competitive Gestalt interpretations of a rotating dancer. Our system flexibly binds the information of the rotating figure into the alternative attractors resolving the illusion's ambiguity and imagining the respective depth interpretation and the corresponding direction of rotation. We finally discuss the potential universality of the proposed mechanisms.



### Delivering Document Conversion as a Cloud Service with High Throughput and Responsiveness
- **Arxiv ID**: http://arxiv.org/abs/2206.00785v1
- **DOI**: 10.1109/CLOUD55607.2022.00060
- **Categories**: **cs.DL**, cs.CV, cs.DC, I.7.5; I.2.1; C.1.4; C.4
- **Links**: [PDF](http://arxiv.org/pdf/2206.00785v1)
- **Published**: 2022-06-01 22:30:30+00:00
- **Updated**: 2022-06-01 22:30:30+00:00
- **Authors**: Christoph Auer, Michele Dolfi, André Carvalho, Cesar Berrospi Ramis, Peter W. J. Staar
- **Comment**: 11 pages, 7 figures, to be published in IEEE CLOUD 2022
- **Journal**: None
- **Summary**: Document understanding is a key business process in the data-driven economy since documents are central to knowledge discovery and business insights. Converting documents into a machine-processable format is a particular challenge here due to their huge variability in formats and complex structure. Accordingly, many algorithms and machine-learning methods emerged to solve particular tasks such as Optical Character Recognition (OCR), layout analysis, table-structure recovery, figure understanding, etc. We observe the adoption of such methods in document understanding solutions offered by all major cloud providers. Yet, publications outlining how such services are designed and optimized to scale in the cloud are scarce. In this paper, we focus on the case of document conversion to illustrate the particular challenges of scaling a complex data processing pipeline with a strong reliance on machine-learning methods on cloud infrastructure. Our key objective is to achieve high scalability and responsiveness for different workload profiles in a well-defined resource budget. We outline the requirements, design, and implementation choices of our document conversion service and reflect on the challenges we faced. Evidence for the scaling behavior and resource efficiency is provided for two alternative workload distribution strategies and deployment configurations. Our best-performing method achieves sustained throughput of over one million PDF pages per hour on 3072 CPU cores across 192 nodes.



### Efficient Self-supervised Vision Pretraining with Local Masked Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2206.00790v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00790v2)
- **Published**: 2022-06-01 22:46:34+00:00
- **Updated**: 2022-06-20 13:28:04+00:00
- **Authors**: Jun Chen, Ming Hu, Boyang Li, Mohamed Elhoseiny
- **Comment**: Add code
- **Journal**: None
- **Summary**: Self-supervised learning for computer vision has achieved tremendous progress and improved many downstream vision tasks such as image classification, semantic segmentation, and object detection. Among these, generative self-supervised vision learning approaches such as MAE and BEiT show promising performance. However, their global masked reconstruction mechanism is computationally demanding. To address this issue, we propose local masked reconstruction (LoMaR), a simple yet effective approach that performs masked reconstruction within a small window of 7$\times$7 patches on a simple Transformer encoder, improving the trade-off between efficiency and accuracy compared to global masked reconstruction over the entire image. Extensive experiments show that LoMaR reaches 84.1% top-1 accuracy on ImageNet-1K classification, outperforming MAE by 0.5%. After finetuning the pretrained LoMaR on 384$\times$384 images, it can reach 85.4% top-1 accuracy, surpassing MAE by 0.6%. On MS COCO, LoMaR outperforms MAE by 0.5 $\text{AP}^\text{box}$ on object detection and 0.5 $\text{AP}^\text{mask}$ on instance segmentation. LoMaR is especially more computation-efficient on pretraining high-resolution images, e.g., it is 3.1$\times$ faster than MAE with 0.2% higher classification accuracy on pretraining 448$\times$448 images. This local masked reconstruction learning mechanism can be easily integrated into any other generative self-supervised learning approach. Our code is publicly available in https://github.com/junchen14/LoMaR.



### Multi-scale frequency separation network for image deblurring
- **Arxiv ID**: http://arxiv.org/abs/2206.00798v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00798v3)
- **Published**: 2022-06-01 23:48:35+00:00
- **Updated**: 2022-12-09 00:34:56+00:00
- **Authors**: Yanni Zhang, Qiang Li, Miao Qi, Di Liu, Jun Kong, Jianzhong Wang
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Image deblurring aims to restore the detailed texture information or structures from blurry images, which has become an indispensable step in many computer vision tasks. Although various methods have been proposed to deal with the image deblurring problem, most of them treated the blurry image as a whole and neglected the characteristics of different image frequencies. In this paper, we present a new method called multi-scale frequency separation network (MSFS-Net) for image deblurring. MSFS-Net introduces the frequency separation module (FSM) into an encoder-decoder network architecture to capture the low- and high-frequency information of image at multiple scales. Then, a cycle-consistency strategy and a contrastive learning module (CLM) are respectively designed to retain the low-frequency information and recover the high-frequency information during deblurring. At last, the features of different scales are fused by a cross-scale feature fusion module (CSFFM). Extensive experiments on benchmark datasets show that the proposed network achieves state-of-the-art performance.



### CcHarmony: Color-checker based Image Harmonization Dataset
- **Arxiv ID**: http://arxiv.org/abs/2206.00800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.00800v1)
- **Published**: 2022-06-01 23:57:16+00:00
- **Updated**: 2022-06-01 23:57:16+00:00
- **Authors**: Haoxu Huang, Li Niu
- **Comment**: None
- **Journal**: None
- **Summary**: Image harmonization targets at adjusting the foreground in a composite image to make it compatible with the background, producing a more realistic and harmonious image. Training deep image harmonization network requires abundant training data, but it is extremely difficult to acquire training pairs of composite images and ground-truth harmonious images. Therefore, existing works turn to adjust the foreground appearance in a real image to create a synthetic composite image. However, such adjustment may not faithfully reflect the natural illumination change of foreground. In this work, we explore a novel transitive way to construct image harmonization dataset. Specifically, based on the existing datasets with recorded illumination information, we first convert the foreground in a real image to the standard illumination condition, and then convert it to another illumination condition, which is combined with the original background to form a synthetic composite image. In this manner, we construct an image harmonization dataset called ccHarmony, which is named after color checker (cc). The dataset is available at https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony.



