# Arxiv Papers in cs.CV on 2022-06-26
### Spatiotemporal Data Mining: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2206.12753v1
- **DOI**: None
- **Categories**: **cs.DB**, cs.CV, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12753v1)
- **Published**: 2022-06-26 00:08:06+00:00
- **Updated**: 2022-06-26 00:08:06+00:00
- **Authors**: Arun Sharma, Zhe Jiang, Shashi Shekhar
- **Comment**: None
- **Journal**: None
- **Summary**: Spatiotemporal data mining aims to discover interesting, useful but non-trivial patterns in big spatial and spatiotemporal data. They are used in various application domains such as public safety, ecology, epidemiology, earth science, etc. This problem is challenging because of the high societal cost of spurious patterns and exorbitant computational cost. Recent surveys of spatiotemporal data mining need update due to rapid growth. In addition, they did not adequately survey parallel techniques for spatiotemporal data mining. This paper provides a more up-to-date survey of spatiotemporal data mining methods. Furthermore, it has a detailed survey of parallel formulations of spatiotemporal data mining.



### Training Your Sparse Neural Network Better with Any Mask
- **Arxiv ID**: http://arxiv.org/abs/2206.12755v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12755v2)
- **Published**: 2022-06-26 00:37:33+00:00
- **Updated**: 2022-06-28 01:41:17+00:00
- **Authors**: Ajay Jaiswal, Haoyu Ma, Tianlong Chen, Ying Ding, Zhangyang Wang
- **Comment**: Accepted by ICML 2022
- **Journal**: None
- **Summary**: Pruning large neural networks to create high-quality, independently trainable sparse masks, which can maintain similar performance to their dense counterparts, is very desirable due to the reduced space and time complexity. As research effort is focused on increasingly sophisticated pruning methods that leads to sparse subnetworks trainable from the scratch, we argue for an orthogonal, under-explored theme: improving training techniques for pruned sub-networks, i.e. sparse training. Apart from the popular belief that only the quality of sparse masks matters for sparse training, in this paper we demonstrate an alternative opportunity: one can carefully customize the sparse training techniques to deviate from the default dense network training protocols, consisting of introducing ``ghost" neurons and skip connections at the early stage of training, and strategically modifying the initialization as well as labels. Our new sparse training recipe is generally applicable to improving training from scratch with various sparse masks. By adopting our newly curated techniques, we demonstrate significant performance gains across various popular datasets (CIFAR-10, CIFAR-100, TinyImageNet), architectures (ResNet-18/32/104, Vgg16, MobileNet), and sparse mask options (lottery ticket, SNIP/GRASP, SynFlow, or even randomly pruning), compared to the default training protocols, especially at high sparsity levels. Code is at https://github.com/VITA-Group/ToST



### Exploiting Transformation Invariance and Equivariance for Self-supervised Sound Localisation
- **Arxiv ID**: http://arxiv.org/abs/2206.12772v2
- **DOI**: 10.1145/3503161.3548317
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2206.12772v2)
- **Published**: 2022-06-26 03:00:02+00:00
- **Updated**: 2022-08-15 07:27:59+00:00
- **Authors**: Jinxiang Liu, Chen Ju, Weidi Xie, Ya Zhang
- **Comment**: Camera-ready Version for ACMMM 2022, Project page is
  https://jinxiang-liu.github.io/SSL-TIE/
- **Journal**: None
- **Summary**: We present a simple yet effective self-supervised framework for audio-visual representation learning, to localize the sound source in videos. To understand what enables to learn useful representations, we systematically investigate the effects of data augmentations, and reveal that (1) composition of data augmentations plays a critical role, i.e. explicitly encouraging the audio-visual representations to be invariant to various transformations~({\em transformation invariance}); (2) enforcing geometric consistency substantially improves the quality of learned representations, i.e. the detected sound source should follow the same transformation applied on input video frames~({\em transformation equivariance}). Extensive experiments demonstrate that our model significantly outperforms previous methods on two sound localization benchmarks, namely, Flickr-SoundNet and VGG-Sound. Additionally, we also evaluate audio retrieval and cross-modal retrieval tasks. In both cases, our self-supervised models demonstrate superior retrieval performances, even competitive with the supervised approach in audio retrieval. This reveals the proposed framework learns strong multi-modal representations that are beneficial to sound localisation and generalization to further applications. \textit{All codes will be available}.



### Representative Teacher Keys for Knowledge Distillation Model Compression Based on Attention Mechanism for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.12788v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.12788v4)
- **Published**: 2022-06-26 05:08:50+00:00
- **Updated**: 2022-10-20 05:35:23+00:00
- **Authors**: Jun-Teng Yang, Sheng-Che Kao, Scott C. -H. Huang
- **Comment**: eight pages, six figures, three tables
- **Journal**: None
- **Summary**: With the improvement of AI chips (e.g., GPU, TPU, and NPU) and the fast development of the Internet of Things (IoT), some robust deep neural networks (DNNs) are usually composed of millions or even hundreds of millions of parameters. Such a large model may not be suitable for directly deploying on low computation and low capacity units (e.g., edge devices). Knowledge distillation (KD) has recently been recognized as a powerful model compression method to decrease the model parameters effectively. The central concept of KD is to extract useful information from the feature maps of a large model (i.e., teacher model) as a reference to successfully train a small model (i.e., student model) in which the model size is much smaller than the teacher one. Although many KD methods have been proposed to utilize the information from the feature maps of intermediate layers in the teacher model, most did not consider the similarity of feature maps between the teacher model and the student model. As a result, it may make the student model learn useless information. Inspired by the attention mechanism, we propose a novel KD method called representative teacher key (RTK) that not only considers the similarity of feature maps but also filters out the useless information to improve the performance of the target student model. In the experiments, we validate our proposed method with several backbone networks (e.g., ResNet and WideResNet) and datasets (e.g., CIFAR10, CIFAR100, SVHN, and CINIC10). The results show that our proposed RTK can effectively improve the classification accuracy of the state-of-the-art attention-based KD method.



### CTMQ: Cyclic Training of Convolutional Neural Networks with Multiple Quantization Steps
- **Arxiv ID**: http://arxiv.org/abs/2206.12794v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.12794v1)
- **Published**: 2022-06-26 05:54:12+00:00
- **Updated**: 2022-06-26 05:54:12+00:00
- **Authors**: HyunJin Kim, Jungwoo Shin, Alberto A. Del Barrio
- **Comment**: submitted to NeurIPS 2022
- **Journal**: None
- **Summary**: This paper proposes a training method having multiple cyclic training for achieving enhanced performance in low-bit quantized convolutional neural networks (CNNs). Quantization is a popular method for obtaining lightweight CNNs, where the initialization with a pretrained model is widely used to overcome degraded performance in low-resolution quantization. However, large quantization errors between real values and their low-bit quantized ones cause difficulties in achieving acceptable performance for complex networks and large datasets. The proposed training method softly delivers the knowledge of pretrained models to low-bit quantized models in multiple quantization steps. In each quantization step, the trained weights of a model are used to initialize the weights of the next model with the quantization bit depth reduced by one. With small change of the quantization bit depth, the performance gap can be bridged, thus providing better weight initialization. In cyclic training, after training a low-bit quantized model, its trained weights are used in the initialization of its accurate model to be trained. By using better training ability of the accurate model in an iterative manner, the proposed method can produce enhanced trained weights for the low-bit quantized model in each cycle. Notably, the training method can advance Top-1 and Top-5 accuracies of the binarized ResNet-18 on the ImageNet dataset by 5.80% and 6.85%, respectively.



### Class Impression for Data-free Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.00005v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.00005v2)
- **Published**: 2022-06-26 06:20:17+00:00
- **Updated**: 2022-07-04 15:09:55+00:00
- **Authors**: Sana Ayromlou, Purang Abolmaesumi, Teresa Tsang, Xiaoxiao Li
- **Comment**: To be appeared in MICCAI 2022
- **Journal**: None
- **Summary**: Standard deep learning-based classification approaches require collecting all samples from all classes in advance and are trained offline. This paradigm may not be practical in real-world clinical applications, where new classes are incrementally introduced through the addition of new data. Class incremental learning is a strategy allowing learning from such data. However, a major challenge is catastrophic forgetting, i.e., performance degradation on previous classes when adapting a trained model to new data. Prior methodologies to alleviate this challenge save a portion of training data require perpetual storage of such data that may introduce privacy issues. Here, we propose a novel data-free class incremental learning framework that first synthesizes data from the model trained on previous classes to generate a \ours. Subsequently, it updates the model by combining the synthesized data with new class data. Furthermore, we incorporate a cosine normalized Cross-entropy loss to mitigate the adverse effects of the imbalance, a margin loss to increase separation among previous classes and new ones, and an intra-domain contrastive loss to generalize the model trained on the synthesized data to real data. We compare our proposed framework with state-of-the-art methods in class incremental learning, where we demonstrate improvement in accuracy for the classification of 11,062 echocardiography cine series of patients.



### Multiple Instance Learning with Mixed Supervision in Gleason Grading
- **Arxiv ID**: http://arxiv.org/abs/2206.12798v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12798v1)
- **Published**: 2022-06-26 06:28:44+00:00
- **Updated**: 2022-06-26 06:28:44+00:00
- **Authors**: Hao Bian, Zhuchen Shao, Yang Chen, Yifeng Wang, Haoqian Wang, Jian Zhang, Yongbing Zhang
- **Comment**: Accepted by MICCAI 2022
- **Journal**: None
- **Summary**: With the development of computational pathology, deep learning methods for Gleason grading through whole slide images (WSIs) have excellent prospects. Since the size of WSIs is extremely large, the image label usually contains only slide-level label or limited pixel-level labels. The current mainstream approach adopts multi-instance learning to predict Gleason grades. However, some methods only considering the slide-level label ignore the limited pixel-level labels containing rich local information. Furthermore, the method of additionally considering the pixel-level labels ignores the inaccuracy of pixel-level labels. To address these problems, we propose a mixed supervision Transformer based on the multiple instance learning framework. The model utilizes both slide-level label and instance-level labels to achieve more accurate Gleason grading at the slide level. The impact of inaccurate instance-level labels is further reduced by introducing an efficient random masking strategy in the mixed supervision training process. We achieve the state-of-the-art performance on the SICAPv2 dataset, and the visual analysis shows the accurate prediction results of instance level. The source code is available at https://github.com/bianhao123/Mixed_supervision.



### A Comparison of AIS, X-Band Marine Radar Systems and Camera Surveillance Systems in the Collection of Tracking Data
- **Arxiv ID**: http://arxiv.org/abs/2206.12809v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.12809v1)
- **Published**: 2022-06-26 07:15:48+00:00
- **Updated**: 2022-06-26 07:15:48+00:00
- **Authors**: Yassir Zardoua, Abdelali Astito, Mohammed Boulaala
- **Comment**: None
- **Journal**: International Journal of Recent Research and Applied Studies,
  Volume 7, Issue 4 (1) April 2020
- **Summary**: Maritime traffic has increased in recent years, especially in terms of seaborne trade. To ensure safety, security, and protection of the marine environment, several systems have been deployed. To overcome some of their inconveniences, the collected data is typically fused. The fused data is used for various purposes, one of our interest is target tracking. The most relevant systems in that context are AIS and X-band marine radar. Many works consider that visual data provided by camera surveillance systems enable additional advantages. Therefore, many tracking algorithms using visual data (images) have been developed. Yet, there is little emphasis on the reasons making the integration of camera systems important. Thus, our main aim in this paper is to analyze the aforementioned surveillance systems for target tracking and conclude some of the maritime security improvements resulted from the integration of cameras to the overall maritime surveillance system.



### Breast Cancer Classification using Deep Learned Features Boosted with Handcrafted Features
- **Arxiv ID**: http://arxiv.org/abs/2206.12815v2
- **DOI**: 10.1016/j.bspc.2023.105353
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12815v2)
- **Published**: 2022-06-26 07:54:09+00:00
- **Updated**: 2023-01-16 09:51:51+00:00
- **Authors**: Unaiza Sajid, Rizwan Ahmed Khan, Shahid Munir Shah, Sheeraz Arif
- **Comment**: None
- **Journal**: Biomedical Signal Processing and Control 2023
- **Summary**: Breast cancer is one of the leading causes of death among women across the globe. It is difficult to treat if detected at advanced stages, however, early detection can significantly increase chances of survival and improves lives of millions of women. Given the widespread prevalence of breast cancer, it is of utmost importance for the research community to come up with the framework for early detection, classification and diagnosis. Artificial intelligence research community in coordination with medical practitioners are developing such frameworks to automate the task of detection. With the surge in research activities coupled with availability of large datasets and enhanced computational powers, it expected that AI framework results will help even more clinicians in making correct predictions. In this article, a novel framework for classification of breast cancer using mammograms is proposed. The proposed framework combines robust features extracted from novel Convolutional Neural Network (CNN) features with handcrafted features including HOG (Histogram of Oriented Gradients) and LBP (Local Binary Pattern). The obtained results on CBIS-DDSM dataset exceed state of the art.



### Perceptual Conversational Head Generation with Regularized Driver and Enhanced Renderer
- **Arxiv ID**: http://arxiv.org/abs/2206.12837v2
- **DOI**: 10.1145/3503161.3551577
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12837v2)
- **Published**: 2022-06-26 10:12:59+00:00
- **Updated**: 2022-08-02 03:51:46+00:00
- **Authors**: Ailin Huang, Zhewei Huang, Shuchang Zhou
- **Comment**: Ailin and Zhewei contributed equally to this work. ACM MM22 workshop
  paper
- **Journal**: None
- **Summary**: This paper reports our solution for ACM Multimedia ViCo 2022 Conversational Head Generation Challenge, which aims to generate vivid face-to-face conversation videos based on audio and reference images. Our solution focuses on training a generalized audio-to-head driver using regularization and assembling a high-visual quality renderer. We carefully tweak the audio-to-behavior model and post-process the generated video using our foreground-background fusion module. We get first place in the listening head generation track and second place in the talking head generation track on the official leaderboard. Our code is available at https://github.com/megvii-research/MM2022-ViCoPerceptualHeadGeneration.



### RoME: Role-aware Mixture-of-Expert Transformer for Text-to-Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2206.12845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12845v1)
- **Published**: 2022-06-26 11:12:49+00:00
- **Updated**: 2022-06-26 11:12:49+00:00
- **Authors**: Burak Satar, Hongyuan Zhu, Hanwang Zhang, Joo Hwee Lim
- **Comment**: Preprint, under review in TCSVT Journal
- **Journal**: None
- **Summary**: Seas of videos are uploaded daily with the popularity of social channels; thus, retrieving the most related video contents with user textual queries plays a more crucial role. Most methods consider only one joint embedding space between global visual and textual features without considering the local structures of each modality. Some other approaches consider multiple embedding spaces consisting of global and local features separately, ignoring rich inter-modality correlations.   We propose a novel mixture-of-expert transformer RoME that disentangles the text and the video into three levels; the roles of spatial contexts, temporal contexts, and object contexts. We utilize a transformer-based attention mechanism to fully exploit visual and text embeddings at both global and local levels with mixture-of-experts for considering inter-modalities and structures' correlations. The results indicate that our method outperforms the state-of-the-art methods on the YouCook2 and MSR-VTT datasets, given the same visual backbone without pre-training. Finally, we conducted extensive ablation studies to elucidate our design choices.



### Semantic Role Aware Correlation Transformer for Text to Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2206.12849v1
- **DOI**: 10.1109/ICIP42928.2021.9506267
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12849v1)
- **Published**: 2022-06-26 11:28:03+00:00
- **Updated**: 2022-06-26 11:28:03+00:00
- **Authors**: Burak Satar, Hongyuan Zhu, Xavier Bresson, Joo Hwee Lim
- **Comment**: Camera-ready for ICIP 2021
- **Journal**: IEEE International Conference on Image Processing (ICIP), 2021,
  pp. 1334-1338
- **Summary**: With the emergence of social media, voluminous video clips are uploaded every day, and retrieving the most relevant visual content with a language query becomes critical. Most approaches aim to learn a joint embedding space for plain textual and visual contents without adequately exploiting their intra-modality structures and inter-modality correlations. This paper proposes a novel transformer that explicitly disentangles the text and video into semantic roles of objects, spatial contexts and temporal contexts with an attention scheme to learn the intra- and inter-role correlations among the three roles to discover discriminative features for matching at different levels. The preliminary results on popular YouCook2 indicate that our approach surpasses a current state-of-the-art method, with a high margin in all metrics. It also overpasses two SOTA methods in terms of two metrics.



### Image Aesthetics Assessment Using Graph Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2206.12869v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12869v2)
- **Published**: 2022-06-26 12:52:46+00:00
- **Updated**: 2022-06-28 08:28:01+00:00
- **Authors**: Koustav Ghosal, Aljosa Smolic
- **Comment**: International Conference on Pattern Recognition (ICPR), 2022
- **Journal**: None
- **Summary**: Aspect ratio and spatial layout are two of the principal factors determining the aesthetic value of a photograph. But, incorporating these into the traditional convolution-based frameworks for the task of image aesthetics assessment is problematic. The aspect ratio of the photographs gets distorted while they are resized/cropped to a fixed dimension to facilitate training batch sampling. On the other hand, the convolutional filters process information locally and are limited in their ability to model the global spatial layout of a photograph. In this work, we present a two-stage framework based on graph neural networks and address both these problems jointly. First, we propose a feature-graph representation in which the input image is modelled as a graph, maintaining its original aspect ratio and resolution. Second, we propose a graph neural network architecture that takes this feature-graph and captures the semantic relationship between the different regions of the input image using visual attention. Our experiments show that the proposed framework advances the state-of-the-art results in aesthetic score regression on the Aesthetic Visual Analysis (AVA) benchmark.



### FingerGAN: A Constrained Fingerprint Generation Scheme for Latent Fingerprint Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2206.12885v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12885v1)
- **Published**: 2022-06-26 14:05:21+00:00
- **Updated**: 2022-06-26 14:05:21+00:00
- **Authors**: Yanming Zhu, Xuefei Yin, Jiankun Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Latent fingerprint enhancement is an essential pre-processing step for latent fingerprint identification. Most latent fingerprint enhancement methods try to restore corrupted gray ridges/valleys. In this paper, we propose a new method that formulates the latent fingerprint enhancement as a constrained fingerprint generation problem within a generative adversarial network (GAN) framework. We name the proposed network as FingerGAN. It can enforce its generated fingerprint (i.e, enhanced latent fingerprint) indistinguishable from the corresponding ground-truth instance in terms of the fingerprint skeleton map weighted by minutia locations and the orientation field regularized by the FOMFE model. Because minutia is the primary feature for fingerprint recognition and minutia can be retrieved directly from the fingerprint skeleton map, we offer a holistic framework which can perform latent fingerprint enhancement in the context of directly optimizing minutia information. This will help improve latent fingerprint identification performance significantly. Experimental results on two public latent fingerprint databases demonstrate that our method outperforms the state of the arts significantly. The codes will be available for non-commercial purposes from \url{https://github.com/HubYZ/LatentEnhancement}.



### Woodscape Fisheye Object Detection for Autonomous Driving -- CVPR 2022 OmniCV Workshop Challenge
- **Arxiv ID**: http://arxiv.org/abs/2206.12912v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12912v1)
- **Published**: 2022-06-26 16:07:37+00:00
- **Updated**: 2022-06-26 16:07:37+00:00
- **Authors**: Saravanabalagi Ramachandran, Ganesh Sistu, Varun Ravi Kumar, John McDonald, Senthil Yogamani
- **Comment**: Workshop on Omnidirectional Computer Vision (OmniCV) at Conference on
  Computer Vision and Pattern Recognition (CVPR) 2021
- **Journal**: None
- **Summary**: Object detection is a comprehensively studied problem in autonomous driving. However, it has been relatively less explored in the case of fisheye cameras. The strong radial distortion breaks the translation invariance inductive bias of Convolutional Neural Networks. Thus, we present the WoodScape fisheye object detection challenge for autonomous driving which was held as part of the CVPR 2022 Workshop on Omnidirectional Computer Vision (OmniCV). This is one of the first competitions focused on fisheye camera object detection. We encouraged the participants to design models which work natively on fisheye images without rectification. We used CodaLab to host the competition based on the publicly available WoodScape fisheye dataset. In this paper, we provide a detailed analysis on the competition which attracted the participation of 120 global teams and a total of 1492 submissions. We briefly discuss the details of the winning methods and analyze their qualitative and quantitative results.



### Video Anomaly Detection via Prediction Network with Enhanced Spatio-Temporal Memory Exchange
- **Arxiv ID**: http://arxiv.org/abs/2206.12914v1
- **DOI**: 10.1109/ICASSP43922.2022.9747376
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.12914v1)
- **Published**: 2022-06-26 16:10:56+00:00
- **Updated**: 2022-06-26 16:10:56+00:00
- **Authors**: Guodong Shen, Yuqi Ouyang, Victor Sanchez
- **Comment**: Accepted at ICASSP 2022
- **Journal**: None
- **Summary**: Video anomaly detection is a challenging task because most anomalies are scarce and non-deterministic. Many approaches investigate the reconstruction difference between normal and abnormal patterns, but neglect that anomalies do not necessarily correspond to large reconstruction errors. To address this issue, we design a Convolutional LSTM Auto-Encoder prediction framework with enhanced spatio-temporal memory exchange using bi-directionalilty and a higher-order mechanism. The bi-directional structure promotes learning the temporal regularity through forward and backward predictions. The unique higher-order mechanism further strengthens spatial information interaction between the encoder and the decoder. Considering the limited receptive fields in Convolutional LSTMs, we also introduce an attention module to highlight informative features for prediction. Anomalies are eventually identified by comparing the frames with their corresponding predictions. Evaluations on three popular benchmarks show that our framework outperforms most existing prediction-based anomaly detection methods.



### Non-Parametric Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2206.12921v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12921v1)
- **Published**: 2022-06-26 16:34:37+00:00
- **Updated**: 2022-06-26 16:34:37+00:00
- **Authors**: Jeong-Sik Lee, Hyun-Chul Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Recent feed-forward neural methods of arbitrary image style transfer mainly utilized encoded feature map upto its second-order statistics, i.e., linearly transformed the encoded feature map of a content image to have the same mean and variance (or covariance) of a target style feature map. In this work, we extend the second-order statistical feature matching into a general distribution matching based on the understanding that style of an image is represented by the distribution of responses from receptive fields. For this generalization, first, we propose a new feature transform layer that exactly matches the feature map distribution of content image into that of target style image. Second, we analyze the recent style losses consistent with our new feature transform layer to train a decoder network which generates a style transferred image from the transformed feature map. Based on our experimental results, it is proven that the stylized images obtained with our method are more similar with the target style images in all existing style measures without losing content clearness.



### Video Activity Localisation with Uncertainties in Temporal Boundary
- **Arxiv ID**: http://arxiv.org/abs/2206.12923v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12923v2)
- **Published**: 2022-06-26 16:45:56+00:00
- **Updated**: 2022-07-21 08:29:17+00:00
- **Authors**: Jiabo Huang, Hailin Jin, Shaogang Gong, Yang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Current methods for video activity localisation over time assume implicitly that activity temporal boundaries labelled for model training are determined and precise. However, in unscripted natural videos, different activities mostly transit smoothly, so that it is intrinsically ambiguous to determine in labelling precisely when an activity starts and ends over time. Such uncertainties in temporal labelling are currently ignored in model training, resulting in learning mis-matched video-text correlation with poor generalisation in test. In this work, we solve this problem by introducing Elastic Moment Bounding (EMB) to accommodate flexible and adaptive activity temporal boundaries towards modelling universally interpretable video-text correlation with tolerance to underlying temporal uncertainties in pre-fixed annotations. Specifically, we construct elastic boundaries adaptively by mining and discovering frame-wise temporal endpoints that can maximise the alignment between video segments and query sentences. To enable both more accurate matching (segment content attention) and more robust localisation (segment elastic boundaries), we optimise the selection of frame-wise endpoints subject to segment-wise contents by a novel Guided Attention mechanism. Extensive experiments on three video activity localisation benchmarks demonstrate compellingly the EMB's advantages over existing methods without modelling uncertainty.



### Vision Transformer for Contrastive Clustering
- **Arxiv ID**: http://arxiv.org/abs/2206.12925v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12925v2)
- **Published**: 2022-06-26 17:00:35+00:00
- **Updated**: 2022-07-10 08:58:47+00:00
- **Authors**: Hua-Bao Ling, Bowen Zhu, Dong Huang, Ding-Hua Chen, Chang-Dong Wang, Jian-Huang Lai
- **Comment**: None
- **Journal**: None
- **Summary**: Vision Transformer (ViT) has shown its advantages over the convolutional neural network (CNN) with its ability to capture global long-range dependencies for visual representation learning. Besides ViT, contrastive learning is another popular research topic recently. While previous contrastive learning works are mostly based on CNNs, some recent studies have attempted to combine ViT and contrastive learning for enhanced self-supervised learning. Despite the considerable progress, these combinations of ViT and contrastive learning mostly focus on the instance-level contrastiveness, which often overlook the global contrastiveness and also lack the ability to directly learn the clustering result (e.g., for images). In view of this, this paper presents a novel deep clustering approach termed Vision Transformer for Contrastive Clustering (VTCC), which for the first time, to our knowledge, unifies the Transformer and the contrastive learning for the image clustering task. Specifically, with two random augmentations performed on each image, we utilize a ViT encoder with two weight-sharing views as the backbone. To remedy the potential instability of the ViT, we incorporate a convolutional stem to split each augmented sample into a sequence of patches, which uses multiple stacked small convolutions instead of a big convolution in the patch projection layer. By learning the feature representations for the sequences of patches via the backbone, an instance projector and a cluster projector are further utilized to perform the instance-level contrastive learning and the global clustering structure learning, respectively. Experiments on eight image datasets demonstrate the stability (during the training-from-scratch) and the superiority (in clustering performance) of our VTCC approach over the state-of-the-art.



### SVBR-NET: A Non-Blind Spatially Varying Defocus Blur Removal Network
- **Arxiv ID**: http://arxiv.org/abs/2206.12930v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.12930v1)
- **Published**: 2022-06-26 17:21:12+00:00
- **Updated**: 2022-06-26 17:21:12+00:00
- **Authors**: Ali Karaali, Claudio Rosito Jung
- **Comment**: Accepted to ICIP2022
- **Journal**: None
- **Summary**: Defocus blur is a physical consequence of the optical sensors used in most cameras. Although it can be used as a photographic style, it is commonly viewed as an image degradation modeled as the convolution of a sharp image with a spatially-varying blur kernel. Motivated by the advance of blur estimation methods in the past years, we propose a non-blind approach for image deblurring that can deal with spatially-varying kernels. We introduce two encoder-decoder sub-networks that are fed with the blurry image and the estimated blur map, respectively, and produce as output the deblurred (deconvolved) image. Each sub-network presents several skip connections that allow data propagation from layers spread apart, and also inter-subnetwork skip connections that ease the communication between the modules. The network is trained with synthetically blur kernels that are augmented to emulate blur maps produced by existing blur estimation methods, and our experimental results show that our method works well when combined with a variety of blur estimation methods.



### PROTOtypical Logic Tensor Networks (PROTO-LTN) for Zero Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.00433v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.00433v1)
- **Published**: 2022-06-26 18:34:07+00:00
- **Updated**: 2022-06-26 18:34:07+00:00
- **Authors**: Simone Martone, Francesco Manigrasso, Lamberti Fabrizio, Lia Morra
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic image interpretation can vastly benefit from approaches that combine sub-symbolic distributed representation learning with the capability to reason at a higher level of abstraction. Logic Tensor Networks (LTNs) are a class of neuro-symbolic systems based on a differentiable, first-order logic grounded into a deep neural network. LTNs replace the classical concept of training set with a knowledge base of fuzzy logical axioms. By defining a set of differentiable operators to approximate the role of connectives, predicates, functions and quantifiers, a loss function is automatically specified so that LTNs can learn to satisfy the knowledge base. We focus here on the subsumption or \texttt{isOfClass} predicate, which is fundamental to encode most semantic image interpretation tasks. Unlike conventional LTNs, which rely on a separate predicate for each class (e.g., dog, cat), each with its own set of learnable weights, we propose a common \texttt{isOfClass} predicate, whose level of truth is a function of the distance between an object embedding and the corresponding class prototype. The PROTOtypical Logic Tensor Networks (PROTO-LTN) extend the current formulation by grounding abstract concepts as parametrized class prototypes in a high-dimensional embedding space, while reducing the number of parameters required to ground the knowledge base. We show how this architecture can be effectively trained in the few and zero-shot learning scenarios. Experiments on Generalized Zero Shot Learning benchmarks validate the proposed implementation as a competitive alternative to traditional embedding-based approaches. The proposed formulation opens up new opportunities in zero shot learning settings, as the LTN formalism allows to integrate background knowledge in the form of logical axioms to compensate for the lack of labelled examples.



### Object Detection and Tracking with Autonomous UAV
- **Arxiv ID**: http://arxiv.org/abs/2206.12941v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.12941v1)
- **Published**: 2022-06-26 18:48:59+00:00
- **Updated**: 2022-06-26 18:48:59+00:00
- **Authors**: A. Huzeyfe Demir, Berke Yavas, Mehmet Yazici, Dogukan Aksu, M. Ali Aydin
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a combat Unmanned Air Vehicle (UAV) is modeled in the simulation environment. The rotary wing UAV is successfully performed various tasks such as locking on the targets, tracking, and sharing the relevant data with surrounding vehicles. Different software technologies such as API communication, ground control station configuration, autonomous movement algorithms, computer vision, and deep learning are employed.



### Multi-view Feature Augmentation with Adaptive Class Activation Mapping
- **Arxiv ID**: http://arxiv.org/abs/2206.12943v3
- **DOI**: 10.24963/ijcai.2021/94
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12943v3)
- **Published**: 2022-06-26 19:05:27+00:00
- **Updated**: 2022-11-06 07:05:43+00:00
- **Authors**: Xiang Gao, Yingjie Tian, Zhiquan Qi
- **Comment**: An arxiv version of the paper published in Proceedings of the
  Thirtieth International Joint Conference on Artificial Intelligence
  (IJCAI-21). See https://www.ijcai.org/proceedings/2021/94
- **Journal**: Proceedings of the Thirtieth International Joint Conference on
  Artificial Intelligence. Main Track. 2021. Pages 678-684
- **Summary**: We propose an end-to-end-trainable feature augmentation module built for image classification that extracts and exploits multi-view local features to boost model performance. Different from using global average pooling (GAP) to extract vectorized features from only the global view, we propose to sample and ensemble diverse multi-view local features to improve model robustness. To sample class-representative local features, we incorporate a simple auxiliary classifier head (comprising only one 1$\times$1 convolutional layer) which efficiently and adaptively attends to class-discriminative local regions of feature maps via our proposed AdaCAM (Adaptive Class Activation Mapping). Extensive experiments demonstrate consistent and noticeable performance gains achieved by our multi-view feature augmentation module.



### AFT-VO: Asynchronous Fusion Transformers for Multi-View Visual Odometry Estimation
- **Arxiv ID**: http://arxiv.org/abs/2206.12946v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.12946v2)
- **Published**: 2022-06-26 19:29:08+00:00
- **Updated**: 2022-09-16 13:47:18+00:00
- **Authors**: Nimet Kaygusuz, Oscar Mendez, Richard Bowden
- **Comment**: None
- **Journal**: None
- **Summary**: Motion estimation approaches typically employ sensor fusion techniques, such as the Kalman Filter, to handle individual sensor failures. More recently, deep learning-based fusion approaches have been proposed, increasing the performance and requiring less model-specific implementations. However, current deep fusion approaches often assume that sensors are synchronised, which is not always practical, especially for low-cost hardware. To address this limitation, in this work, we propose AFT-VO, a novel transformer-based sensor fusion architecture to estimate VO from multiple sensors. Our framework combines predictions from asynchronous multi-view cameras and accounts for the time discrepancies of measurements coming from different sources.   Our approach first employs a Mixture Density Network (MDN) to estimate the probability distributions of the 6-DoF poses for every camera in the system. Then a novel transformer-based fusion module, AFT-VO, is introduced, which combines these asynchronous pose estimations, along with their confidences. More specifically, we introduce Discretiser and Source Encoding techniques which enable the fusion of multi-source asynchronous signals.   We evaluate our approach on the popular nuScenes and KITTI datasets. Our experiments demonstrate that multi-view fusion for VO estimation provides robust and accurate trajectories, outperforming the state of the art in both challenging weather and lighting conditions.



### Nonwatertight Mesh Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2206.12952v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12952v1)
- **Published**: 2022-06-26 19:53:00+00:00
- **Updated**: 2022-06-26 19:53:00+00:00
- **Authors**: Partha Ghosh
- **Comment**: arXiv admin note: text overlap with arXiv:2106.03452 by other authors
- **Journal**: None
- **Summary**: Reconstructing 3D non-watertight mesh from an unoriented point cloud is an unexplored area in computer vision and computer graphics. In this project, we tried to tackle this problem by extending the learning-based watertight mesh reconstruction pipeline presented in the paper 'Shape as Points'. The core of our approach is to cast the problem as a semantic segmentation problem that identifies the region in the 3D volume where the mesh surface lies and extracts the surfaces from the detected regions. Our approach achieves compelling results compared to the baseline techniques.



### Szloca: towards a framework for full 3D tracking through a single camera in context of interactive arts
- **Arxiv ID**: http://arxiv.org/abs/2206.12958v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2206.12958v1)
- **Published**: 2022-06-26 20:09:47+00:00
- **Updated**: 2022-06-26 20:09:47+00:00
- **Authors**: Sahaj Garg
- **Comment**: None
- **Journal**: None
- **Summary**: Realtime virtual data of objects and human presence in a large area holds a valuable key in enabling many experiences and applications in various industries and with exponential rise in the technological development of artificial intelligence, computer vision has expanded the possibilities of tracking and classifying things through just video inputs, which is also surpassing the limitations of most popular and common hardware setups known traditionally to detect human pose and position, such as low field of view and limited tracking capacity. The benefits of using computer vision in application development is large as it augments traditional input sources (like video streams) and can be integrated in many environments and platforms. In the context of new media interactive arts, based on physical movements and expanding over large areas or gallaries, this research presents a novel way and a framework towards obtaining data and virtual representation of objects/people - such as three-dimensional positions, skeltons/pose and masks from a single rgb camera. Looking at the state of art through some recent developments and building on prior research in the field of computer vision, the paper also proposes an original method to obtain three dimensional position data from monocular images, the model does not rely on complex training of computer vision systems but combines prior computer vision research and adds a capacity to represent z depth, ieto represent a world position in 3 axis from a 2d input source.



### Probabilistic PolarGMM: Unsupervised Cluster Learning of Very Noisy Projection Images of Unknown Pose
- **Arxiv ID**: http://arxiv.org/abs/2206.12959v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CE, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12959v1)
- **Published**: 2022-06-26 20:20:10+00:00
- **Updated**: 2022-06-26 20:20:10+00:00
- **Authors**: Supawit Chockchowwat, Chandrajit L. Bajaj
- **Comment**: 13 pages, including appendices
- **Journal**: None
- **Summary**: A crucial step in single particle analysis (SPA) of cryogenic electron microscopy (Cryo-EM), 2D classification and alignment takes a collection of noisy particle images to infer orientations and group similar images together. Averaging these aligned and clustered noisy images produces a set of clean images, ready for further analysis such as 3D reconstruction. Fourier-Bessel steerable principal component analysis (FBsPCA) enables an efficient, adaptable, low-rank rotation operator. We extend the FBsPCA to additionally handle translations. In this extended FBsPCA representation, we use a probabilistic polar-coordinate Gaussian mixture model to learn soft clusters in an unsupervised fashion using an expectation maximization (EM) algorithm. The obtained rotational clusters are thus additionally robust to the presence of pairwise alignment imperfections. Multiple benchmarks from simulated Cryo-EM datasets show probabilistic PolarGMM's improved performance in comparisons with standard single-particle Cryo-EM tools, EMAN2 and RELION, in terms of various clustering metrics and alignment errors.



### Self-Healing Robust Neural Networks via Closed-Loop Control
- **Arxiv ID**: http://arxiv.org/abs/2206.12963v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.12963v1)
- **Published**: 2022-06-26 20:25:35+00:00
- **Updated**: 2022-06-26 20:25:35+00:00
- **Authors**: Zhuotong Chen, Qianxiao Li, Zheng Zhang
- **Comment**: 48 pages, 5 figures
- **Journal**: None
- **Summary**: Despite the wide applications of neural networks, there have been increasing concerns about their vulnerability issue. While numerous attack and defense techniques have been developed, this work investigates the robustness issue from a new angle: can we design a self-healing neural network that can automatically detect and fix the vulnerability issue by itself? A typical self-healing mechanism is the immune system of a human body. This biology-inspired idea has been used in many engineering designs but is rarely investigated in deep learning. This paper considers the post-training self-healing of a neural network, and proposes a closed-loop control formulation to automatically detect and fix the errors caused by various attacks or perturbations. We provide a margin-based analysis to explain how this formulation can improve the robustness of a classifier. To speed up the inference of the proposed self-healing network, we solve the control problem via improving the Pontryagin Maximum Principle-based solver. Lastly, we present an error estimation of the proposed framework for neural networks with nonlinear activation functions. We validate the performance on several network architectures against various perturbations. Since the self-healing method does not need a-priori information about data perturbations/attacks, it can handle a broad class of unforeseen perturbations.



### VLCap: Vision-Language with Contrastive Learning for Coherent Video Paragraph Captioning
- **Arxiv ID**: http://arxiv.org/abs/2206.12972v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12972v2)
- **Published**: 2022-06-26 20:51:05+00:00
- **Updated**: 2022-08-06 19:38:10+00:00
- **Authors**: Kashu Yamazaki, Sang Truong, Khoa Vo, Michael Kidd, Chase Rainwater, Khoa Luu, Ngan Le
- **Comment**: accepted by The 29th IEEE International Conference on Image
  Processing (IEEE ICIP) 2022
- **Journal**: None
- **Summary**: In this paper, we leverage the human perceiving process, that involves vision and language interaction, to generate a coherent paragraph description of untrimmed videos. We propose vision-language (VL) features consisting of two modalities, i.e., (i) vision modality to capture global visual content of the entire scene and (ii) language modality to extract scene elements description of both human and non-human objects (e.g. animals, vehicles, etc), visual and non-visual elements (e.g. relations, activities, etc). Furthermore, we propose to train our proposed VLCap under a contrastive learning VL loss. The experiments and ablation studies on ActivityNet Captions and YouCookII datasets show that our VLCap outperforms existing SOTA methods on both accuracy and diversity metrics.



### Detecting Schizophrenia with 3D Structural Brain MRI Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.12980v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2206.12980v2)
- **Published**: 2022-06-26 21:44:33+00:00
- **Updated**: 2022-07-07 16:36:51+00:00
- **Authors**: Junhao Zhang, Vishwanatha M. Rao, Ye Tian, Yanting Yang, Nicolas Acosta, Zihan Wan, Pin-Yu Lee, Chloe Zhang, Lawrence S. Kegeles, Scott A. Small, Jia Guo
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Schizophrenia is a chronic neuropsychiatric disorder that causes distinct structural alterations within the brain. We hypothesize that deep learning applied to a structural neuroimaging dataset could detect disease-related alteration and improve classification and diagnostic accuracy. We tested this hypothesis using a single, widely available, and conventional T1-weighted MRI scan, from which we extracted the 3D whole-brain structure using standard post-processing methods. A deep learning model was then developed, optimized, and evaluated on three open datasets with T1-weighted MRI scans of patients with schizophrenia. Our proposed model outperformed the benchmark model, which was also trained with structural MR images using a 3D CNN architecture. Our model is capable of almost perfectly (area under the ROC curve = 0.987) distinguishing schizophrenia patients from healthy controls on unseen structural MRI scans. Regional analysis localized subcortical regions and ventricles as the most predictive brain regions. Subcortical structures serve a pivotal role in cognitive, affective, and social functions in humans, and structural abnormalities of these regions have been associated with schizophrenia. Our finding corroborates that schizophrenia is associated with widespread alterations in subcortical brain structure and the subcortical structural information provides prominent features in diagnostic classification. Together, these results further demonstrate the potential of deep learning to improve schizophrenia diagnosis and identify its structural neuroimaging signatures from a single, standard T1-weighted brain MRI.



### Automatic Generation of Product-Image Sequence in E-commerce
- **Arxiv ID**: http://arxiv.org/abs/2206.12994v1
- **DOI**: 10.1145/3534678.3539149
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.12994v1)
- **Published**: 2022-06-26 23:38:42+00:00
- **Updated**: 2022-06-26 23:38:42+00:00
- **Authors**: Xiaochuan Fan, Chi Zhang, Yong Yang, Yue Shang, Xueying Zhang, Zhen He, Yun Xiao, Bo Long, Lingfei Wu
- **Comment**: Accepted by KDD 2022 ADS
- **Journal**: None
- **Summary**: Product images are essential for providing desirable user experience in an e-commerce platform. For a platform with billions of products, it is extremely time-costly and labor-expensive to manually pick and organize qualified images. Furthermore, there are the numerous and complicated image rules that a product image needs to comply in order to be generated/selected. To address these challenges, in this paper, we present a new learning framework in order to achieve Automatic Generation of Product-Image Sequence (AGPIS) in e-commerce. To this end, we propose a Multi-modality Unified Image-sequence Classifier (MUIsC), which is able to simultaneously detect all categories of rule violations through learning. MUIsC leverages textual review feedback as the additional training target and utilizes product textual description to provide extra semantic information. Based on offline evaluations, we show that the proposed MUIsC significantly outperforms various baselines. Besides MUIsC, we also integrate some other important modules in the proposed framework, such as primary image selection, noncompliant content detection, and image deduplication. With all these modules, our framework works effectively and efficiently in JD.com recommendation platform. By Dec 2021, our AGPIS framework has generated high-standard images for about 1.5 million products and achieves 13.6% in reject rate.



