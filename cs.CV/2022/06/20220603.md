# Arxiv Papers in cs.CV on 2022-06-03
### Detecting Pulmonary Embolism from Computed Tomography Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2206.01344v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.01344v1)
- **Published**: 2022-06-03 00:01:47+00:00
- **Updated**: 2022-06-03 00:01:47+00:00
- **Authors**: Chia-Hung Yang, Yun-Chien Cheng, Chin Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: The clinical symptoms of pulmonary embolism (PE) are very diverse and non-specific, which makes it difficult to diagnose. In addition, pulmonary embolism has multiple triggers and is one of the major causes of vascular death. Therefore, if it can be detected and treated quickly, it can significantly reduce the risk of death in hospitalized patients. In the detection process, the cost of computed tomography pulmonary angiography (CTPA) is high, and angiography requires the injection of contrast agents, which increase the risk of damage to the patient. Therefore, this study will use a deep learning approach to detect pulmonary embolism in all patients who take a CT image of the chest using a convolutional neural network. With the proposed pulmonary embolism detection system, we can detect the possibility of pulmonary embolism at the same time as the patient's first CT image, and schedule the CTPA test immediately, saving more than a week of CT image screening time and providing timely diagnosis and treatment to the patient.



### Adversarial Attacks on Human Vision
- **Arxiv ID**: http://arxiv.org/abs/2206.01365v1
- **DOI**: 10.1109/MMUL.2015.59
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01365v1)
- **Published**: 2022-06-03 02:05:04+00:00
- **Updated**: 2022-06-03 02:05:04+00:00
- **Authors**: Victor A. Mateescu, Ivan V. Bajić
- **Comment**: 21 pages, 8 figures, 1 table
- **Journal**: Extended version of IEEE MultiMedia, vol. 23, no. 1, pp. 82-91,
  Jan.-Mar. 2016
- **Summary**: This article presents an introduction to visual attention retargeting, its connection to visual saliency, the challenges associated with it, and ideas for how it can be approached. The difficulty of attention retargeting as a saliency inversion problem lies in the lack of one-to-one mapping between saliency and the image domain, in addition to the possible negative impact of saliency alterations on image aesthetics. A few approaches from recent literature to solve this challenging problem are reviewed, and several suggestions for future development are presented.



### Supernet Training for Federated Image Classification under System Heterogeneity
- **Arxiv ID**: http://arxiv.org/abs/2206.01366v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01366v5)
- **Published**: 2022-06-03 02:21:01+00:00
- **Updated**: 2022-10-06 03:00:50+00:00
- **Authors**: Taehyeon Kim, Se-Young Yun
- **Comment**: Oral paper on ICML 22 Workshop: "Dynamic Neural Networks"; Under
  review
- **Journal**: None
- **Summary**: Efficient deployment of deep neural networks across many devices and resource constraints, particularly on edge devices, is one of the most challenging problems in the presence of data-privacy preservation issues. Conventional approaches have evolved to either improve a single global model while keeping each local heterogeneous training data decentralized (i.e. data heterogeneity; Federated Learning (FL)) or to train an overarching network that supports diverse architectural settings to address heterogeneous systems equipped with different computational capabilities (i.e. system heterogeneity; Neural Architecture Search). However, few studies have considered both directions simultaneously. This paper proposes the federation of supernet training (FedSup) framework to consider both scenarios simultaneously, i.e., where clients send and receive a supernet that contains all possible architectures sampled from itself. The approach is inspired by observing that averaging parameters during model aggregation for FL is similar to weight-sharing in supernet training. Thus, the proposed FedSup framework combines a weight-sharing approach widely used for training single shot models with FL averaging (FedAvg). Furthermore, we develop an efficient algorithm (E-FedSup) by sending the sub-model to clients on the broadcast stage to reduce communication costs and training overhead, including several strategies to enhance supernet training in the FL environment. We verify the proposed approach with extensive empirical evaluations. The resulting framework also ensures data and model heterogeneity robustness on several standard benchmarks.



### Incremental Learning Meets Transfer Learning: Application to Multi-site Prostate MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.01369v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01369v2)
- **Published**: 2022-06-03 02:32:01+00:00
- **Updated**: 2022-07-31 00:53:20+00:00
- **Authors**: Chenyu You, Jinlin Xiang, Kun Su, Xiaoran Zhang, Siyuan Dong, John Onofrey, Lawrence Staib, James S. Duncan
- **Comment**: None
- **Journal**: None
- **Summary**: Many medical datasets have recently been created for medical image segmentation tasks, and it is natural to question whether we can use them to sequentially train a single model that (1) performs better on all these datasets, and (2) generalizes well and transfers better to the unknown target site domain. Prior works have achieved this goal by jointly training one model on multi-site datasets, which achieve competitive performance on average but such methods rely on the assumption about the availability of all training data, thus limiting its effectiveness in practical deployment. In this paper, we propose a novel multi-site segmentation framework called incremental-transfer learning (ITL), which learns a model from multi-site datasets in an end-to-end sequential fashion. Specifically, "incremental" refers to training sequentially constructed datasets, and "transfer" is achieved by leveraging useful information from the linear combination of embedding features on each dataset. In addition, we introduce our ITL framework, where we train the network including a site-agnostic encoder with pre-trained weights and at most two segmentation decoder heads. We also design a novel site-level incremental loss in order to generalize well on the target domain. Second, we show for the first time that leveraging our ITL training scheme is able to alleviate challenging catastrophic forgetting problems in incremental learning. We conduct experiments using five challenging benchmark datasets to validate the effectiveness of our incremental-transfer learning approach. Our approach makes minimal assumptions on computation resources and domain-specific expertise, and hence constitutes a strong starting point in multi-site medical image segmentation.



### Towards Improving the Generation Quality of Autoregressive Slot VAEs
- **Arxiv ID**: http://arxiv.org/abs/2206.01370v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.01370v2)
- **Published**: 2022-06-03 02:41:59+00:00
- **Updated**: 2023-02-04 21:09:19+00:00
- **Authors**: Patrick Emami, Pan He, Sanjay Ranka, Anand Rangarajan
- **Comment**: 39 pages, 16 figures. Under review. Code and videos available at
  https://github.com/pemami4911/segregate-relate-imagine
- **Journal**: None
- **Summary**: Unconditional scene inference and generation are challenging to learn jointly with a single compositional model. Despite encouraging progress on models that extract object-centric representations ("slots") from images, unconditional generation of scenes from slots has received less attention. This is primarily because learning the multi-object relations necessary to imagine coherent scenes is difficult. We hypothesize that most existing slot-based models have a limited ability to learn object correlations. We propose two improvements that strengthen slot correlation learning. The first is to condition the slots on a global, scene-level variable that captures higher-order correlations between slots. Second, we address the fundamental lack of a canonical order for objects by proposing to learn a consistent order to use for the autoregressive generation of scene objects. Specifically, we train an autoregressive slot prior to sequentially generate scene objects following the learned order. Slot inference entails estimating a randomly ordered set of slots using existing approaches for extracting slots from images, then aligning those slots to ordered slots generated autoregressively with the prior. Our experiments across three multi-object environments demonstrate clear gains in scene generation quality. Detailed ablation studies are also provided that validate the two proposed improvements.



### Mutual- and Self- Prototype Alignment for Semi-supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.01739v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01739v1)
- **Published**: 2022-06-03 02:59:22+00:00
- **Updated**: 2022-06-03 02:59:22+00:00
- **Authors**: Zhenxi Zhang, Chunna Tian, Zhicheng Jiao
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Semi-supervised learning methods have been explored in medical image segmentation tasks due to the scarcity of pixel-level annotation in the real scenario. Proto-type alignment based consistency constraint is an intuitional and plausible solu-tion to explore the useful information in the unlabeled data. In this paper, we propose a mutual- and self- prototype alignment (MSPA) framework to better utilize the unlabeled data. In specific, mutual-prototype alignment enhances the information interaction between labeled and unlabeled data. The mutual-prototype alignment imposes two consistency constraints in reverse directions between the unlabeled and labeled data, which enables the consistent embedding and model discriminability on unlabeled data. The proposed self-prototype alignment learns more stable region-wise features within unlabeled images, which optimizes the classification margin in semi-supervised segmentation by boosting the intra-class compactness and inter-class separation on the feature space. Extensive experimental results on three medical datasets demonstrate that with a small amount of labeled data, MSPA achieves large improvements by leveraging the unlabeled data. Our method also outperforms seven state-of-the-art semi-supervised segmentation methods on all three datasets.



### Denoising Fast X-Ray Fluorescence Raster Scans of Paintings
- **Arxiv ID**: http://arxiv.org/abs/2206.01740v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01740v1)
- **Published**: 2022-06-03 03:17:18+00:00
- **Updated**: 2022-06-03 03:17:18+00:00
- **Authors**: Henry Chopp, Alicia McGeachy, Matthias Alfeld, Oliver Cossairt, Marc Walton, Aggelos Katsaggelos
- **Comment**: None
- **Journal**: None
- **Summary**: Macro x-ray fluorescence (XRF) imaging of cultural heritage objects, while a popular non-invasive technique for providing elemental distribution maps, is a slow acquisition process in acquiring high signal-to-noise ratio XRF volumes. Typically on the order of tenths of a second per pixel, a raster scanning probe counts the number of photons at different energies emitted by the object under x-ray illumination. In an effort to reduce the scan times without sacrificing elemental map and XRF volume quality, we propose using dictionary learning with a Poisson noise model as well as a color image-based prior to restore noisy, rapidly acquired XRF data.



### CF-YOLO: Cross Fusion YOLO for Object Detection in Adverse Weather with a High-quality Real Snow Dataset
- **Arxiv ID**: http://arxiv.org/abs/2206.01381v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01381v1)
- **Published**: 2022-06-03 04:00:26+00:00
- **Updated**: 2022-06-03 04:00:26+00:00
- **Authors**: Qiqi Ding, Peng Li, Xuefeng Yan, Ding Shi, Luming Liang, Weiming Wang, Haoran Xie, Jonathan Li, Mingqiang Wei
- **Comment**: 10pages
- **Journal**: None
- **Summary**: Snow is one of the toughest adverse weather conditions for object detection (OD). Currently, not only there is a lack of snowy OD datasets to train cutting-edge detectors, but also these detectors have difficulties learning latent information beneficial for detection in snow. To alleviate the two above problems, we first establish a real-world snowy OD dataset, named RSOD. Besides, we develop an unsupervised training strategy with a distinctive activation function, called $Peak \ Act$, to quantitatively evaluate the effect of snow on each object. Peak Act helps grading the images in RSOD into four-difficulty levels. To our knowledge, RSOD is the first quantitatively evaluated and graded snowy OD dataset. Then, we propose a novel Cross Fusion (CF) block to construct a lightweight OD network based on YOLOv5s (call CF-YOLO). CF is a plug-and-play feature aggregation module, which integrates the advantages of Feature Pyramid Network and Path Aggregation Network in a simpler yet more flexible form. Both RSOD and CF lead our CF-YOLO to possess an optimization ability for OD in real-world snow. That is, CF-YOLO can handle unfavorable detection problems of vagueness, distortion and covering of snow. Experiments show that our CF-YOLO achieves better detection results on RSOD, compared to SOTAs. The code and dataset are available at https://github.com/qqding77/CF-YOLO-and-RSOD.



### Falconn++: A Locality-sensitive Filtering Approach for Approximate Nearest Neighbor Search
- **Arxiv ID**: http://arxiv.org/abs/2206.01382v3
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01382v3)
- **Published**: 2022-06-03 04:02:02+00:00
- **Updated**: 2022-10-22 08:45:28+00:00
- **Authors**: Ninh Pham, Tao Liu
- **Comment**: To appear in NeurIPS 2022
- **Journal**: None
- **Summary**: We present Falconn++, a novel locality-sensitive filtering approach for approximate nearest neighbor search on angular distance. Falconn++ can filter out potential far away points in any hash bucket \textit{before} querying, which results in higher quality candidates compared to other hashing-based solutions. Theoretically, Falconn++ asymptotically achieves lower query time complexity than Falconn, an optimal locality-sensitive hashing scheme on angular distance. Empirically, Falconn++ achieves higher recall-speed tradeoffs than Falconn on many real-world data sets. Falconn++ is also competitive with HNSW, an efficient representative of graph-based solutions on high search recall regimes.



### Patcher: Patch Transformers with Mixture of Experts for Precise Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.01741v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01741v2)
- **Published**: 2022-06-03 04:02:39+00:00
- **Updated**: 2023-05-29 23:52:49+00:00
- **Authors**: Yanglan Ou, Ye Yuan, Xiaolei Huang, Stephen T. C. Wong, John Volpi, James Z. Wang, Kelvin Wong
- **Comment**: MICCAI 2022
- **Journal**: None
- **Summary**: We present a new encoder-decoder Vision Transformer architecture, Patcher, for medical image segmentation. Unlike standard Vision Transformers, it employs Patcher blocks that segment an image into large patches, each of which is further divided into small patches. Transformers are applied to the small patches within a large patch, which constrains the receptive field of each pixel. We intentionally make the large patches overlap to enhance intra-patch communication. The encoder employs a cascade of Patcher blocks with increasing receptive fields to extract features from local to global levels. This design allows Patcher to benefit from both the coarse-to-fine feature extraction common in CNNs and the superior spatial relationship modeling of Transformers. We also propose a new mixture-of-experts (MoE) based decoder, which treats the feature maps from the encoder as experts and selects a suitable set of expert features to predict the label for each pixel. The use of MoE enables better specializations of the expert features and reduces interference between them during inference. Extensive experiments demonstrate that Patcher outperforms state-of-the-art Transformer- and CNN-based approaches significantly on stroke lesion segmentation and polyp segmentation. Code for Patcher is released with publication to facilitate future research.



### End-to-End 3D Hand Pose Estimation from Stereo Cameras
- **Arxiv ID**: http://arxiv.org/abs/2206.01384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01384v1)
- **Published**: 2022-06-03 04:18:58+00:00
- **Updated**: 2022-06-03 04:18:58+00:00
- **Authors**: Yuncheng Li, Zehao Xue, Yingying Wang, Liuhao Ge, Zhou Ren, Jonathan Rodriguez
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes an end-to-end approach to estimate full 3D hand pose from stereo cameras. Most existing methods of estimating hand pose from stereo cameras apply stereo matching to obtain depth map and use depth-based solution to estimate hand pose. In contrast, we propose to bypass the stereo matching and directly estimate the 3D hand pose from the stereo image pairs. The proposed neural network architecture extends from any keypoint predictor to estimate the sparse disparity of the hand joints. In order to effectively train the model, we propose a large scale synthetic dataset that is composed of stereo image pairs and ground truth 3D hand pose annotations. Experiments show that the proposed approach outperforms the existing methods based on the stereo depth.



### Dynamic Structured Illumination Microscopy with a Neural Space-time Model
- **Arxiv ID**: http://arxiv.org/abs/2206.01397v2
- **DOI**: None
- **Categories**: **physics.optics**, cs.CV, cs.GR, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2206.01397v2)
- **Published**: 2022-06-03 05:24:06+00:00
- **Updated**: 2022-07-29 00:10:20+00:00
- **Authors**: Ruiming Cao, Fanglin Linda Liu, Li-Hao Yeh, Laura Waller
- **Comment**: None
- **Journal**: None
- **Summary**: Structured illumination microscopy (SIM) reconstructs a super-resolved image from multiple raw images captured with different illumination patterns; hence, acquisition speed is limited, making it unsuitable for dynamic scenes. We propose a new method, Speckle Flow SIM, that uses static patterned illumination with moving samples and models the sample motion during data capture in order to reconstruct the dynamic scene with super-resolution. Speckle Flow SIM relies on sample motion to capture a sequence of raw images. The spatio-temporal relationship of the dynamic scene is modeled using a neural space-time model with coordinate-based multi-layer perceptrons (MLPs), and the motion dynamics and the super-resolved scene are jointly recovered. We validate Speckle Flow SIM for coherent imaging in simulation and build a simple, inexpensive experimental setup with off-the-shelf components. We demonstrate that Speckle Flow SIM can reconstruct a dynamic scene with deformable motion and 1.88x the diffraction-limited resolution in experiment.



### Learning Probabilistic Topological Representations Using Discrete Morse Theory
- **Arxiv ID**: http://arxiv.org/abs/2206.01742v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01742v2)
- **Published**: 2022-06-03 06:00:26+00:00
- **Updated**: 2022-10-01 19:39:30+00:00
- **Authors**: Xiaoling Hu, Dimitris Samaras, Chao Chen
- **Comment**: 16 pages, 11 figures
- **Journal**: None
- **Summary**: Accurate delineation of fine-scale structures is a very important yet challenging problem. Existing methods use topological information as an additional training loss, but are ultimately making pixel-wise predictions. In this paper, we propose the first deep learning based method to learn topological/structural representations. We use discrete Morse theory and persistent homology to construct an one-parameter family of structures as the topological/structural representation space. Furthermore, we learn a probabilistic model that can perform inference tasks in such a topological/structural representation space. Our method generates true structures rather than pixel-maps, leading to better topological integrity in automatic segmentation tasks. It also facilitates semi-automatic interactive annotation/proofreading via the sampling of structures and structure-aware uncertainty.



### MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2206.01408v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01408v2)
- **Published**: 2022-06-03 06:31:11+00:00
- **Updated**: 2023-05-29 10:35:58+00:00
- **Authors**: Yixiong Chen, Li Liu, Jingxian Li, Hua Jiang, Chris Ding, Zongwei Zhou
- **Comment**: MICCAI 2023
- **Journal**: None
- **Summary**: In medical image analysis, transfer learning is a powerful method for deep neural networks (DNNs) to generalize well on limited medical data. Prior efforts have focused on developing pre-training algorithms on domains such as lung ultrasound, chest X-ray, and liver CT to bridge domain gaps. However, we find that model fine-tuning also plays a crucial role in adapting medical knowledge to target tasks. The common fine-tuning method is manually picking transferable layers (e.g., the last few layers) to update, which is labor-expensive. In this work, we propose a meta-learning-based LR tuner, named MetaLR, to make different layers automatically co-adapt to downstream tasks based on their transferabilities across domains. MetaLR learns appropriate LRs for different layers in an online manner, preventing highly transferable layers from forgetting their medical representation abilities and driving less transferable layers to adapt actively to new domains. Extensive experiments on various medical applications show that MetaLR outperforms previous state-of-the-art (SOTA) fine-tuning strategies. Codes are released.



### Learning an Adaptation Function to Assess Image Visual Similarities
- **Arxiv ID**: http://arxiv.org/abs/2206.01417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01417v1)
- **Published**: 2022-06-03 07:15:00+00:00
- **Updated**: 2022-06-03 07:15:00+00:00
- **Authors**: Olivier Risser-Maroix, Amine Marzouki, Hala Djeghim, Camille Kurtz, Nicolas Lomenie
- **Comment**: None
- **Journal**: ORASIS 2021, Centre National de la Recherche Scientifique [CNRS],
  Sep 2021, Saint Ferr{\'e}ol, France
- **Summary**: Human perception is routinely assessing the similarity between images, both for decision making and creative thinking. But the underlying cognitive process is not really well understood yet, hence difficult to be mimicked by computer vision systems. State-of-the-art approaches using deep architectures are often based on the comparison of images described as feature vectors learned for image categorization task. As a consequence, such features are powerful to compare semantically related images but not really efficient to compare images visually similar but semantically unrelated. Inspired by previous works on neural features adaptation to psycho-cognitive representations, we focus here on the specific task of learning visual image similarities when analogy matters. We propose to compare different supervised, semi-supervised and self-supervised networks, pre-trained on distinct scales and contents datasets (such as ImageNet-21k, ImageNet-1K or VGGFace2) to conclude which model may be the best to approximate the visual cortex and learn only an adaptation function corresponding to the approximation of the the primate IT cortex through the metric learning framework. Our experiments conducted on the Totally Looks Like image dataset highlight the interest of our method, by increasing the retrieval scores of the best model @1 by 2.25x. This research work was recently accepted for publication at the ICIP 2021 international conference [1]. In this new article, we expand on this previous work by using and comparing new pre-trained feature extractors on other datasets.



### Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2021
- **Arxiv ID**: http://arxiv.org/abs/2206.02573v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.02573v1)
- **Published**: 2022-06-03 07:37:48+00:00
- **Updated**: 2022-06-03 07:37:48+00:00
- **Authors**: Yi Cheng, Fen Fang, Ying Sun
- **Comment**: None
- **Journal**: None
- **Summary**: In this report, we present the technical details of our approach to the EPIC-KITCHENS-100 Unsupervised Domain Adaptation (UDA) Challenge for Action Recognition. The EPIC-KITCHENS-100 dataset consists of daily kitchen activities focusing on the interaction between human hands and their surrounding objects. It is very challenging to accurately recognize these fine-grained activities, due to the presence of distracting objects and visually similar action classes, especially in the unlabelled target domain. Based on an existing method for video domain adaptation, i.e., TA3N, we propose to learn hand-centric features by leveraging the hand bounding box information for UDA on fine-grained action recognition. This helps reduce the distraction from background as well as facilitate the learning of domain-invariant features. To achieve high quality hand localization, we adopt an uncertainty-aware domain adaptation network, i.e., MEAA, to train a domain-adaptive hand detector, which only uses very limited hand bounding box annotations in the source domain but can generalize well to the unlabelled target domain. Our submission achieved the 1st place in terms of top-1 action recognition accuracy, using only RGB and optical flow modalities as input.



### Learning rich optical embeddings for privacy-preserving lensless image classification
- **Arxiv ID**: http://arxiv.org/abs/2206.01429v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01429v1)
- **Published**: 2022-06-03 07:38:09+00:00
- **Updated**: 2022-06-03 07:38:09+00:00
- **Authors**: Eric Bezzam, Martin Vetterli, Matthieu Simeoni
- **Comment**: 29 pages, 23 figures, under review
- **Journal**: None
- **Summary**: By replacing the lens with a thin optical element, lensless imaging enables new applications and solutions beyond those supported by traditional camera design and post-processing, e.g. compact and lightweight form factors and visual privacy. The latter arises from the highly multiplexed measurements of lensless cameras, which require knowledge of the imaging system to recover a recognizable image. In this work, we exploit this unique multiplexing property: casting the optics as an encoder that produces learned embeddings directly at the camera sensor. We do so in the context of image classification, where we jointly optimize the encoder's parameters and those of an image classifier in an end-to-end fashion. Our experiments show that jointly learning the lensless optical encoder and the digital processing allows for lower resolution embeddings at the sensor, and hence better privacy as it is much harder to recover meaningful images from these measurements. Additional experiments show that such an optimization allows for lensless measurements that are more robust to typical real-world image transformations. While this work focuses on classification, the proposed programmable lensless camera and end-to-end optimization can be applied to other computational imaging tasks.



### LenslessPiCam: A Hardware and Software Platform for Lensless Computational Imaging with a Raspberry Pi
- **Arxiv ID**: http://arxiv.org/abs/2206.01430v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01430v1)
- **Published**: 2022-06-03 07:39:21+00:00
- **Updated**: 2022-06-03 07:39:21+00:00
- **Authors**: Eric Bezzam, Sepand Kashani, Martin Vetterli, Matthieu Simeoni
- **Comment**: None
- **Journal**: None
- **Summary**: Lensless imaging seeks to replace/remove the lens in a conventional imaging system. The earliest cameras were in fact lensless, relying on long exposure times to form images on the other end of a small aperture in a darkened room/container (camera obscura). The introduction of a lens allowed for more light throughput and therefore shorter exposure times, while retaining sharp focus. The incorporation of digital sensors readily enabled the use of computational imaging techniques to post-process and enhance raw images (e.g. via deblurring, inpainting, denoising, sharpening). Recently, imaging scientists have started leveraging computational imaging as an integral part of lensless imaging systems, allowing them to form viewable images from the highly multiplexed raw measurements of lensless cameras (see [5] and references therein for a comprehensive treatment of lensless imaging). This represents a real paradigm shift in camera system design as there is more flexibility to cater the hardware to the application at hand (e.g. lightweight or flat designs). This increased flexibility comes however at the price of a more demanding post-processing of the raw digital recordings and a tighter integration of sensing and computation, often difficult to achieve in practice due to inefficient interactions between the various communities of scientists involved. With LenslessPiCam, we provide an easily accessible hardware and software framework to enable researchers, hobbyists, and students to implement and explore practical and computational aspects of lensless imaging. We also provide detailed guides and exercises so that LenslessPiCam can be used as an educational resource, and point to results from our graduate-level signal processing course.



### On the duality between contrastive and non-contrastive self-supervised learning
- **Arxiv ID**: http://arxiv.org/abs/2206.02574v3
- **DOI**: 10.48550/arXiv.2206.02574
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02574v3)
- **Published**: 2022-06-03 08:04:12+00:00
- **Updated**: 2023-06-26 12:01:56+00:00
- **Authors**: Quentin Garrido, Yubei Chen, Adrien Bardes, Laurent Najman, Yann Lecun
- **Comment**: The Eleventh International Conference on Learning Representations,
  2023, Kigali, Rwanda
- **Journal**: None
- **Summary**: Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg's with careful hyperparameter tuning, improving significantly over known baselines. We also challenge the popular assumption that non-contrastive methods need large output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and non-contrastive methods in certain regimes can be closed given better network design choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an important direction to build a better understanding of self-supervised learning.



### Exploring Transformers for Behavioural Biometrics: A Case Study in Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.01441v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2206.01441v1)
- **Published**: 2022-06-03 08:08:40+00:00
- **Updated**: 2022-06-03 08:08:40+00:00
- **Authors**: Paula Delgado-Santos, Ruben Tolosana, Richard Guest, Farzin Deravi, Ruben Vera-Rodriguez
- **Comment**: None
- **Journal**: None
- **Summary**: Biometrics on mobile devices has attracted a lot of attention in recent years as it is considered a user-friendly authentication method. This interest has also been motivated by the success of Deep Learning (DL). Architectures based on Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) have been established to be convenient for the task, improving the performance and robustness in comparison to traditional machine learning techniques. However, some aspects must still be revisited and improved. To the best of our knowledge, this is the first article that intends to explore and propose novel gait biometric recognition systems based on Transformers, which currently obtain state-of-the-art performance in many applications. Several state-of-the-art architectures (Vanilla, Informer, Autoformer, Block-Recurrent Transformer, and THAT) are considered in the experimental framework. In addition, new configurations of the Transformers are proposed to further increase the performance. Experiments are carried out using the two popular public databases whuGAIT and OU-ISIR. The results achieved prove the high ability of the proposed Transformer, outperforming state-of-the-art CNN and RNN architectures.



### Orthogonal Transform based Generative Adversarial Network for Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2206.01743v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.01743v1)
- **Published**: 2022-06-03 08:17:52+00:00
- **Updated**: 2022-06-03 08:17:52+00:00
- **Authors**: Ahlad Kumar, Mantra Sanathra, Manish Khare, Vijeta Khare
- **Comment**: 12 pages, 14 figures
- **Journal**: None
- **Summary**: Image dehazing has become one of the crucial preprocessing steps for any computer vision task. Most of the dehazing methods try to estimate the transmission map along with the atmospheric light to get the dehazed image in the image domain. In this paper, we propose a novel end-to-end architecture that directly estimates dehazed image in Krawtchouk transform domain. For this a customized Krawtchouk Convolution Layer (KCL) in the architecture is added. KCL is constructed using Krawtchouk basis functions which converts the image from the spatial domain to the Krawtchouk transform domain. Another convolution layer is added at the end of the architecture named as Inverse Krawtchouk Convolution Layer (IKCL) which converts the image back to the spatial domain from the transform domain. It has been observed that the haze is mainly present in lower frequencies of hazy images, wherein the Krawtchouk transform helps to analyze the high and low frequencies of the images separately. We have divided our architecture into two branches, the upper branch deals with the higher frequencies while the lower branch deals with the lower frequencies of the image. The lower branch is made deeper in terms of the layers as compared to the upper branch to address the haze present in the lower frequencies. Using the proposed Orthogonal Transform based Generative Adversarial Network (OTGAN) architecture for image dehazing, we were able to achieve competitive results when compared to the present state-of-the-art methods.



### Zero-Shot Bird Species Recognition by Learning from Field Guides
- **Arxiv ID**: http://arxiv.org/abs/2206.01466v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.01466v1)
- **Published**: 2022-06-03 09:13:46+00:00
- **Updated**: 2022-06-03 09:13:46+00:00
- **Authors**: Andrés C. Rodríguez, Stefano D'Aronco, Rodrigo Caye Daudt, Jan D. Wegner, Konrad Schindler
- **Comment**: None
- **Journal**: None
- **Summary**: We exploit field guides to learn bird species recognition, in particular zero-shot recognition of unseen species. The illustrations contained in field guides deliberately focus on discriminative properties of a species, and can serve as side information to transfer knowledge from seen to unseen classes. We study two approaches: (1) a contrastive encoding of illustrations that can be fed into zero-shot learning schemes; and (2) a novel method that leverages the fact that illustrations are also images and as such structurally more similar to photographs than other kinds of side information. Our results show that illustrations from field guides, which are readily available for a wide range of species, are indeed a competitive source of side information. On the iNaturalist2021 subset, we obtain a harmonic mean from 749 seen and 739 unseen classes greater than $45\%$ (@top-10) and $15\%$ (@top-1). Which shows that field guides are a valuable option for challenging real-world scenarios with many species.



### The Importance of Image Interpretation: Patterns of Semantic Misclassification in Real-World Adversarial Images
- **Arxiv ID**: http://arxiv.org/abs/2206.01467v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.01467v2)
- **Published**: 2022-06-03 09:17:22+00:00
- **Updated**: 2022-12-13 18:18:50+00:00
- **Authors**: Zhengyu Zhao, Nga Dang, Martha Larson
- **Comment**: International Conference on Multimedia Modeling (MMM) 2023. Resources
  are publicly available at
  https://github.com/ZhengyuZhao/Targeted-Transfer/tree/main/human_eval
- **Journal**: None
- **Summary**: Adversarial images are created with the intention of causing an image classifier to produce a misclassification. In this paper, we propose that adversarial images should be evaluated based on semantic mismatch, rather than label mismatch, as used in current work. In other words, we propose that an image of a "mug" would be considered adversarial if classified as "turnip", but not as "cup", as current systems would assume. Our novel idea of taking semantic misclassification into account in the evaluation of adversarial images offers two benefits. First, it is a more realistic conceptualization of what makes an image adversarial, which is important in order to fully understand the implications of adversarial images for security and privacy. Second, it makes it possible to evaluate the transferability of adversarial images to a real-world classifier, without requiring the classifier's label set to have been available during the creation of the images. The paper carries out an evaluation of a transfer attack on a real-world image classifier that is made possible by our semantic misclassification approach. The attack reveals patterns in the semantics of adversarial misclassifications that could not be investigated using conventional label mismatch.



### Distributional loss for convolutional neural network regression and application to GNSS multi-path estimation
- **Arxiv ID**: http://arxiv.org/abs/2206.01473v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2206.01473v1)
- **Published**: 2022-06-03 09:45:12+00:00
- **Updated**: 2022-06-03 09:45:12+00:00
- **Authors**: Thomas Gonzalez, Antoine Blais, Nicolas Couëllan, Christian Ruiz
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) have been widely used in image classification. Over the years, they have also benefited from various enhancements and they are now considered as state of the art techniques for image like data. However, when they are used for regression to estimate some function value from images, fewer recommendations are available. In this study, a novel CNN regression model is proposed. It combines convolutional neural layers to extract high level features representations from images with a soft labelling technique. More specifically, as the deep regression task is challenging, the idea is to account for some uncertainty in the targets that are seen as distributions around their mean. The estimations are carried out by the model in the form of distributions. Building from earlier work, a specific histogram loss function based on the Kullback-Leibler (KL) divergence is applied during training. The model takes advantage of the CNN feature representation and is able to carry out estimation from multi-channel input images. To assess and illustrate the technique, the model is applied to Global Navigation Satellite System (GNSS) multi-path estimation where multi-path signal parameters have to be estimated from correlator output images from the I and Q channels. The multi-path signal delay, magnitude, Doppler shift frequency and phase parameters are estimated from synthetically generated datasets of satellite signals. Experiments are conducted under various receiving conditions and various input images resolutions to test the estimation performances quality and robustness. The results show that the proposed soft labelling CNN technique using distributional loss outperforms classical CNN regression under all conditions. Furthermore, the extra learning performance achieved by the model allows the reduction of input image resolution from 80x80 down to 40x40 or sometimes 20x20.



### YOLOv5s-GTB: light-weighted and improved YOLOv5s for bridge crack detection
- **Arxiv ID**: http://arxiv.org/abs/2206.01498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01498v1)
- **Published**: 2022-06-03 10:52:59+00:00
- **Updated**: 2022-06-03 10:52:59+00:00
- **Authors**: Xiao Ruiqiang
- **Comment**: None
- **Journal**: None
- **Summary**: In response to the situation that the conventional bridge crack manual detection method has a large amount of human and material resources wasted, this study is aimed to propose a light-weighted, high-precision, deep learning-based bridge apparent crack recognition model that can be deployed in mobile devices' scenarios. In order to enhance the performance of YOLOv5, firstly, the data augmentation methods are supplemented, and then the YOLOv5 series algorithm is trained to select a suitable basic framework. The YOLOv5s is identified as the basic framework for the light-weighted crack detection model through experiments for comparison and validation.By replacing the traditional DarkNet backbone network of YOLOv5s with GhostNet backbone network, introducing Transformer multi-headed self-attention mechanism and bi-directional feature pyramid network (BiFPN) to replace the commonly used feature pyramid network, the improved model not only has 42% fewer parameters and faster inference response, but also significantly outperforms the original model in terms of accuracy and mAP (8.5% and 1.1% improvement, respectively). Luckily each improved part has a positive impact on the result. This paper provides a feasible idea to establish a digital operation management system in the field of highway and bridge in the future and to implement the whole life cycle structure health monitoring of civil infrastructure in China.



### Anomaly detection in surveillance videos using transformer based attention model
- **Arxiv ID**: http://arxiv.org/abs/2206.01524v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01524v2)
- **Published**: 2022-06-03 12:19:39+00:00
- **Updated**: 2022-06-06 10:04:53+00:00
- **Authors**: Kapil Deshpande, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali Agarwal
- **Comment**: None
- **Journal**: None
- **Summary**: Surveillance footage can catch a wide range of realistic anomalies. This research suggests using a weakly supervised strategy to avoid annotating anomalous segments in training videos, which is time consuming. In this approach only video level labels are used to obtain frame level anomaly scores. Weakly supervised video anomaly detection (WSVAD) suffers from the wrong identification of abnormal and normal instances during the training process. Therefore it is important to extract better quality features from the available videos. WIth this motivation, the present paper uses better quality transformer-based features named Videoswin Features followed by the attention layer based on dilated convolution and self attention to capture long and short range dependencies in temporal domain. This gives us a better understanding of available videos. The proposed framework is validated on real-world dataset i.e. ShanghaiTech Campus dataset which results in competitive performance than current state-of-the-art methods. The model and the code are available at https://github.com/kapildeshpande/Anomaly-Detection-in-Surveillance-Videos



### Detection of Fibrosis in Cine Magnetic Resonance Images Using Artificial Intelligence Techniques
- **Arxiv ID**: http://arxiv.org/abs/2206.01745v1
- **DOI**: 10.7775/rac.v90.i2.20504
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01745v1)
- **Published**: 2022-06-03 14:14:14+00:00
- **Updated**: 2022-06-03 14:14:14+00:00
- **Authors**: Ariel. H. Curiale, Facundo Cabrera, Pablo Jimenez, Jorgelina Medus, GermÁn Mato, MatÍas E. Calandrelli
- **Comment**: None
- **Journal**: None
- **Summary**: Background: Artificial intelligence techniques have demonstrated great potential in cardiology, especially to detect imperceptible patterns for the human eye. In this sense, these techniques seem to be adequate to identify patterns in the myocardial texture which could lead to characterize and quantify fibrosis. Purpose: The aim of this study was to postulate a new artificial intelligence method to identify fibrosis in cine cardiac magnetic resonance (CMR) imaging. Methods: A retrospective observational study was carried out in a population of 75 subjects from a clinical center of San Carlos de Bariloche. The proposed method analyzes the myocardial texture in cine CMR images using a convolutional neural network to determine local myocardial tissue damage. Results: An accuracy of 89% for quantifying local tissue damage was observed for the validation data set and 70% for the test set. In addition, the qualitative analysis showed a high spatial correlation in lesion location. Conclusions: The postulated method enables to spatially identify fibrosis using only the information from cine nuclear magnetic resonance studies, demonstrating the potential of this technique to quantify myocardial viability in the future or to study the lesions etiology



### Automatic Quantification of Volumes and Biventricular Function in Cardiac Resonance. Validation of a New Artificial Intelligence Approach
- **Arxiv ID**: http://arxiv.org/abs/2206.01746v1
- **DOI**: 10.7775/rac.v89.i4.20427
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2206.01746v1)
- **Published**: 2022-06-03 14:17:12+00:00
- **Updated**: 2022-06-03 14:17:12+00:00
- **Authors**: Ariel H. Curiale, MatÍas E. Calandrelli, Lucca Dellazoppa, Mariano Trevisan, Jorge Luis BociÁn, Juan Pablo Bonifacio, GermÁn Mato
- **Comment**: None
- **Journal**: None
- **Summary**: Background: Artificial intelligence techniques have shown great potential in cardiology, especially in quantifying cardiac biventricular function, volume, mass, and ejection fraction (EF). However, its use in clinical practice is not straightforward due to its poor reproducibility with cases from daily practice, among other reasons. Objectives: To validate a new artificial intelligence tool in order to quantify the cardiac biventricular function (volume, mass, and EF). To analyze its robustness in the clinical area, and the computational times compared with conventional methods. Methods: A total of 189 patients were analyzed: 89 from a regional center and 100 from a public center. The method proposes two convolutional networks that include anatomical information of the heart to reduce classification errors. Results: A high concordance (Pearson coefficient) was observed between manual quantification and the proposed quantification of cardiac function (0.98, 0.92, 0.96 and 0.8 for volumes and biventricular EF) in about 5 seconds per study. Conclusions: This method quantifies biventricular function and volumes in seconds with an accuracy equivalent to that of a specialist.



### Pruning for Feature-Preserving Circuits in CNNs
- **Arxiv ID**: http://arxiv.org/abs/2206.01627v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.01627v2)
- **Published**: 2022-06-03 15:12:40+00:00
- **Updated**: 2023-04-17 02:58:12+00:00
- **Authors**: Chris Hamblin, Talia Konkle, George Alvarez
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Deep convolutional neural networks are a powerful model class for a range of computer vision problems, but it is difficult to interpret the image filtering process they implement, given their sheer size. In this work, we introduce a method for extracting 'feature-preserving circuits' from deep CNNs, leveraging methods from saliency-based neural network pruning. These circuits are modular sub-functions, embedded within the network, containing only a subset of convolutional kernels relevant to a target feature. We compare the efficacy of 3 saliency-criteria for extracting these sparse circuits. Further, we show how 'sub-feature' circuits can be extracted, that preserve a feature's responses to particular images, dividing the feature into even sparser filtering processes. We also develop a tool for visualizing 'circuit diagrams', which render the entire image filtering process implemented by circuits in a parsable format.



### Analysis of face detection, face landmarking, and face recognition performance with masked face images
- **Arxiv ID**: http://arxiv.org/abs/2207.06478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06478v1)
- **Published**: 2022-06-03 15:16:58+00:00
- **Updated**: 2022-06-03 15:16:58+00:00
- **Authors**: Ožbej Golob
- **Comment**: 4 pages, 3 figures, 6 tables
- **Journal**: None
- **Summary**: Face recognition has become an essential task in our lives. However, the current COVID-19 pandemic has led to the widespread use of face masks. The effect of wearing face masks is currently an understudied issue. The aim of this paper is to analyze face detection, face landmarking, and face recognition performance with masked face images. HOG and CNN face detectors are used for face detection in combination with 5-point and 68-point face landmark predictors and VGG16 face recognition model is used for face recognition on masked and unmasked images. We found that the performance of face detection, face landmarking, and face recognition is negatively impacted by face masks



### Reinforcement Learning with Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2206.01634v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.01634v1)
- **Published**: 2022-06-03 15:22:08+00:00
- **Updated**: 2022-06-03 15:22:08+00:00
- **Authors**: Danny Driess, Ingmar Schubert, Pete Florence, Yunzhu Li, Marc Toussaint
- **Comment**: None
- **Journal**: None
- **Summary**: It is a long-standing problem to find effective representations for training reinforcement learning (RL) agents. This paper demonstrates that learning state representations with supervision from Neural Radiance Fields (NeRFs) can improve the performance of RL compared to other learned representations or even low-dimensional, hand-engineered state information. Specifically, we propose to train an encoder that maps multiple image observations to a latent space describing the objects in the scene. The decoder built from a latent-conditioned NeRF serves as the supervision signal to learn the latent space. An RL algorithm then operates on the learned latent space as its state representation. We call this NeRF-RL. Our experiments indicate that NeRF as supervision leads to a latent space better suited for the downstream RL tasks involving robotic object manipulations like hanging mugs on hooks, pushing objects, or opening doors. Video: https://dannydriess.github.io/nerf-rl



### Mirror modular cloning and fast quantum associative retrieval
- **Arxiv ID**: http://arxiv.org/abs/2206.01644v1
- **DOI**: None
- **Categories**: **quant-ph**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01644v1)
- **Published**: 2022-06-03 15:42:04+00:00
- **Updated**: 2022-06-03 15:42:04+00:00
- **Authors**: M. C. Diamantini, C. A. Trugenberger
- **Comment**: None
- **Journal**: None
- **Summary**: We show that a quantum state can be perfectly cloned up to global mirroring with a unitary transformation that depends on one single parameter. We then show that this is equivalent to "perfect" cloning for quantum associative memories which, as a consequence efficiently hold exponentially more information than their classical counterparts. Finally, we present a quantum associative retrieval algorithm which can correct corrupted inputs and is exponentially faster than the Grover algorithm.



### Integrating Prior Knowledge in Contrastive Learning with Kernel
- **Arxiv ID**: http://arxiv.org/abs/2206.01646v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01646v2)
- **Published**: 2022-06-03 15:43:08+00:00
- **Updated**: 2023-05-30 11:13:06+00:00
- **Authors**: Benoit Dufumier, Carlo Alberto Barbano, Robin Louiset, Edouard Duchesnay, Pietro Gori
- **Comment**: ICML 2023
- **Journal**: None
- **Summary**: Data augmentation is a crucial component in unsupervised contrastive learning (CL). It determines how positive samples are defined and, ultimately, the quality of the learned representation. In this work, we open the door to new perspectives for CL by integrating prior knowledge, given either by generative models -- viewed as prior representations -- or weak attributes in the positive and negative sampling. To this end, we use kernel theory to propose a novel loss, called decoupled uniformity, that i) allows the integration of prior knowledge and ii) removes the negative-positive coupling in the original InfoNCE loss. We draw a connection between contrastive learning and conditional mean embedding theory to derive tight bounds on the downstream classification loss. In an unsupervised setting, we empirically demonstrate that CL benefits from generative models to improve its representation both on natural and medical images. In a weakly supervised scenario, our framework outperforms other unconditional and conditional CL approaches.



### D'ARTAGNAN: Counterfactual Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2206.01651v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01651v2)
- **Published**: 2022-06-03 15:53:32+00:00
- **Updated**: 2022-06-30 13:02:24+00:00
- **Authors**: Hadrien Reynaud, Athanasios Vlontzos, Mischa Dombrowski, Ciarán Lee, Arian Beqiri, Paul Leeson, Bernhard Kainz
- **Comment**: Accepted for MICCAI 2022
- **Journal**: None
- **Summary**: Causally-enabled machine learning frameworks could help clinicians to identify the best course of treatments by answering counterfactual questions. We explore this path for the case of echocardiograms by looking into the variation of the Left Ventricle Ejection Fraction, the most essential clinical metric gained from these examinations. We combine deep neural networks, twin causal networks and generative adversarial methods for the first time to build D'ARTAGNAN (Deep ARtificial Twin-Architecture GeNerAtive Networks), a novel causal generative model. We demonstrate the soundness of our approach on a synthetic dataset before applying it to cardiac ultrasound videos to answer the question: "What would this echocardiogram look like if the patient had a different ejection fraction?". To do so, we generate new ultrasound videos, retaining the video style and anatomy of the original patient, while modifying the Ejection Fraction conditioned on a given input. We achieve an SSIM score of 0.79 and an R2 score of 0.51 on the counterfactual videos. Code and models are available at: https://github.com/HReynaud/dartagnan.



### Metrics reloaded: Recommendations for image analysis validation
- **Arxiv ID**: http://arxiv.org/abs/2206.01653v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01653v6)
- **Published**: 2022-06-03 15:56:51+00:00
- **Updated**: 2023-06-30 10:49:37+00:00
- **Authors**: Lena Maier-Hein, Annika Reinke, Patrick Godau, Minu D. Tizabi, Florian Büttner, Evangelia Christodoulou, Ben Glocker, Fabian Isensee, Jens Kleesiek, Michal Kozubek, Mauricio Reyes, Michael A. Riegler, Manuel Wiesenfarth, A. Emre Kavur, Carole H. Sudre, Michael Baumgartner, Matthias Eisenmann, Doreen Heckmann-Nötzel, A. Tim Rädsch, Laura Acion, Michela Antonelli, Tal Arbel, Spyridon Bakas, Arriel Benis, Matthew Blaschko, M. Jorge Cardoso, Veronika Cheplygina, Beth A. Cimini, Gary S. Collins, Keyvan Farahani, Luciana Ferrer, Adrian Galdran, Bram van Ginneken, Robert Haase, Daniel A. Hashimoto, Michael M. Hoffman, Merel Huisman, Pierre Jannin, Charles E. Kahn, Dagmar Kainmueller, Bernhard Kainz, Alexandros Karargyris, Alan Karthikesalingam, Hannes Kenngott, Florian Kofler, Annette Kopp-Schneider, Anna Kreshuk, Tahsin Kurc, Bennett A. Landman, Geert Litjens, Amin Madani, Klaus Maier-Hein, Anne L. Martel, Peter Mattson, Erik Meijering, Bjoern Menze, Karel G. M. Moons, Henning Müller, Brennan Nichyporuk, Felix Nickel, Jens Petersen, Nasir Rajpoot, Nicola Rieke, Julio Saez-Rodriguez, Clara I. Sánchez, Shravya Shetty, Maarten van Smeden, Ronald M. Summers, Abdel A. Taha, Aleksei Tiulpin, Sotirios A. Tsaftaris, Ben Van Calster, Gaël Varoquaux, Paul F. Jäger
- **Comment**: Shared first authors: Lena Maier-Hein, Annika Reinke. arXiv admin
  note: substantial text overlap with arXiv:2104.05642
- **Journal**: None
- **Summary**: Increasing evidence shows that flaws in machine learning (ML) algorithm validation are an underestimated global problem. Particularly in automatic biomedical image analysis, chosen performance metrics often do not reflect the domain interest, thus failing to adequately measure scientific progress and hindering translation of ML techniques into practice. To overcome this, our large international expert consortium created Metrics Reloaded, a comprehensive framework guiding researchers in the problem-aware selection of metrics. Following the convergence of ML methodology across application domains, Metrics Reloaded fosters the convergence of validation methodology. The framework was developed in a multi-stage Delphi process and is based on the novel concept of a problem fingerprint - a structured representation of the given problem that captures all aspects that are relevant for metric selection, from the domain interest to the properties of the target structure(s), data set and algorithm output. Based on the problem fingerprint, users are guided through the process of choosing and applying appropriate validation metrics while being made aware of potential pitfalls. Metrics Reloaded targets image analysis problems that can be interpreted as a classification task at image, object or pixel level, namely image-level classification, object detection, semantic segmentation, and instance segmentation tasks. To improve the user experience, we implemented the framework in the Metrics Reloaded online tool, which also provides a point of access to explore weaknesses, strengths and specific recommendations for the most common validation metrics. The broad applicability of our framework across domains is demonstrated by an instantiation for various biological and medical image analysis use cases.



### Identification via Retinal Vessels Combining LBP and HOG
- **Arxiv ID**: http://arxiv.org/abs/2206.01658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01658v1)
- **Published**: 2022-06-03 16:08:38+00:00
- **Updated**: 2022-06-03 16:08:38+00:00
- **Authors**: Ali Noori
- **Comment**: None
- **Journal**: None
- **Summary**: With development of information technology and necessity for high security, using different identification methods has become very important. Each biometric feature has its own advantages and disadvantages and choosing each of them depends on our usage. Retinal scanning is a bio scale method for identification. The retina is composed of vessels and optical disk. The vessels distribution pattern is one the remarkable retinal identification methods. In this paper, a new approach is presented for identification via retinal images using LBP and hog methods. In the proposed method, it will be tried to separate the retinal vessels accurately via machine vision techniques which will have good sustainability in rotation and size change. HOG-based or LBP-based methods or their combination can be used for separation and also HSV color space can be used too. Having extracted the features, the similarity criteria can be used for identification. The implementation of proposed method and its comparison with one of the newly-presented methods in this area shows better performance of the proposed method.



### Style-Content Disentanglement in Language-Image Pretraining Representations for Zero-Shot Sketch-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2206.01661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01661v1)
- **Published**: 2022-06-03 16:14:37+00:00
- **Updated**: 2022-06-03 16:14:37+00:00
- **Authors**: Jan Zuiderveld
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose and validate a framework to leverage language-image pretraining representations for training-free zero-shot sketch-to-image synthesis. We show that disentangled content and style representations can be utilized to guide image generators to employ them as sketch-to-image generators without (re-)training any parameters. Our approach for disentangling style and content entails a simple method consisting of elementary arithmetic assuming compositionality of information in representations of input sketches. Our results demonstrate that this approach is competitive with state-of-the-art instance-level open-domain sketch-to-image models, while only depending on pretrained off-the-shelf models and a fraction of the data.



### Egocentric Video-Language Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2206.01670v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.01670v2)
- **Published**: 2022-06-03 16:28:58+00:00
- **Updated**: 2022-10-13 03:31:05+00:00
- **Authors**: Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao, Rongcheng Tu, Wenzhe Zhao, Weijie Kong, Chengfei Cai, Hongfa Wang, Dima Damen, Bernard Ghanem, Wei Liu, Mike Zheng Shou
- **Comment**: Accepted by NeurIPS 2022. Double champions at Ego4D and
  EPIC-Kitchens, CVPR 2022 challenges. 23 pages, 13 figures, 12 tables. Code:
  https://github.com/showlab/EgoVLP
- **Journal**: None
- **Summary**: Video-Language Pretraining (VLP), which aims to learn transferable representation to advance a wide range of video-text downstream tasks, has recently received increasing attention. Best performing works rely on large-scale, 3rd-person video-text datasets, such as HowTo100M. In this work, we exploit the recently released Ego4D dataset to pioneer Egocentric VLP along three directions. (i) We create EgoClip, a 1st-person video-text pretraining dataset comprising 3.8M clip-text pairs well-chosen from Ego4D, covering a large variety of human daily activities. (ii) We propose a novel pretraining objective, dubbed EgoNCE, which adapts video-text contrastive learning to the egocentric domain by mining egocentric-aware positive and negative samples. (iii) We introduce EgoMCQ, a development benchmark that is close to EgoClip and hence can support effective validation and fast exploration of our design decisions in EgoClip and EgoNCE. Furthermore, we demonstrate strong performance on five egocentric downstream tasks across three datasets: video-text retrieval on EPIC-KITCHENS-100; action recognition on Charades-Ego; natural language query, moment query, and object state change classification on Ego4D challenge benchmarks. The dataset and code are available at https://github.com/showlab/EgoVLP.



### Dynamic Kernel Selection for Improved Generalization and Memory Efficiency in Meta-learning
- **Arxiv ID**: http://arxiv.org/abs/2206.01690v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01690v1)
- **Published**: 2022-06-03 17:09:26+00:00
- **Updated**: 2022-06-03 17:09:26+00:00
- **Authors**: Arnav Chavan, Rishabh Tiwari, Udbhav Bamba, Deepak K. Gupta
- **Comment**: Published at CVPR 2022
- **Journal**: None
- **Summary**: Gradient based meta-learning methods are prone to overfit on the meta-training set, and this behaviour is more prominent with large and complex networks. Moreover, large networks restrict the application of meta-learning models on low-power edge devices. While choosing smaller networks avoid these issues to a certain extent, it affects the overall generalization leading to reduced performance. Clearly, there is an approximately optimal choice of network architecture that is best suited for every meta-learning problem, however, identifying it beforehand is not straightforward. In this paper, we present MetaDOCK, a task-specific dynamic kernel selection strategy for designing compressed CNN models that generalize well on unseen tasks in meta-learning. Our method is based on the hypothesis that for a given set of similar tasks, not all kernels of the network are needed by each individual task. Rather, each task uses only a fraction of the kernels, and the selection of the kernels per task can be learnt dynamically as a part of the inner update steps. MetaDOCK compresses the meta-model as well as the task-specific inner models, thus providing significant reduction in model size for each task, and through constraining the number of active kernels for every task, it implicitly mitigates the issue of meta-overfitting. We show that for the same inference budget, pruned versions of large CNN models obtained using our approach consistently outperform the conventional choices of CNN models. MetaDOCK couples well with popular meta-learning approaches such as iMAML. The efficacy of our method is validated on CIFAR-fs and mini-ImageNet datasets, and we have observed that our approach can provide improvements in model accuracy of up to 2% on standard meta-learning benchmark, while reducing the model size by more than 75%.



### Gradient Obfuscation Checklist Test Gives a False Sense of Security
- **Arxiv ID**: http://arxiv.org/abs/2206.01705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01705v1)
- **Published**: 2022-06-03 17:27:10+00:00
- **Updated**: 2022-06-03 17:27:10+00:00
- **Authors**: Nikola Popovic, Danda Pani Paudel, Thomas Probst, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: One popular group of defense techniques against adversarial attacks is based on injecting stochastic noise into the network. The main source of robustness of such stochastic defenses however is often due to the obfuscation of the gradients, offering a false sense of security. Since most of the popular adversarial attacks are optimization-based, obfuscated gradients reduce their attacking ability, while the model is still susceptible to stronger or specifically tailored adversarial attacks. Recently, five characteristics have been identified, which are commonly observed when the improvement in robustness is mainly caused by gradient obfuscation. It has since become a trend to use these five characteristics as a sufficient test, to determine whether or not gradient obfuscation is the main source of robustness. However, these characteristics do not perfectly characterize all existing cases of gradient obfuscation, and therefore can not serve as a basis for a conclusive test. In this work, we present a counterexample, showing this test is not sufficient for concluding that gradient obfuscation is not the main cause of improvements in robustness.



### Compositional Visual Generation with Composable Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2206.01714v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.01714v6)
- **Published**: 2022-06-03 17:47:04+00:00
- **Updated**: 2023-01-17 17:08:51+00:00
- **Authors**: Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, Joshua B. Tenenbaum
- **Comment**: ECCV 2022. First three authors contributed equally. Project website:
  https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/
- **Journal**: None
- **Summary**: Large text-guided diffusion models, such as DALLE-2, are able to generate stunning photorealistic images given natural language descriptions. While such models are highly flexible, they struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects. In this paper, we propose an alternative structured approach for compositional generation using diffusion models. An image is generated by composing a set of diffusion models, with each of them modeling a certain component of the image. To do this, we interpret diffusion models as energy-based models in which the data distributions defined by the energy functions may be explicitly combined. The proposed method can generate scenes at test time that are substantially more complex than those seen in training, composing sentence descriptions, object relations, human facial attributes, and even generalizing to new combinations that are rarely seen in the real world. We further illustrate how our approach may be used to compose pre-trained text-guided diffusion models and generate photorealistic images containing all the details described in the input descriptions, including the binding of certain object attributes that have been shown difficult for DALLE-2. These results point to the effectiveness of the proposed method in promoting structured generalization for visual generation. Project page: https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/



### A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2206.01718v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2206.01718v1)
- **Published**: 2022-06-03 17:52:27+00:00
- **Updated**: 2022-06-03 17:52:27+00:00
- **Authors**: Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi
- **Comment**: None
- **Journal**: None
- **Summary**: The Visual Question Answering (VQA) task aspires to provide a meaningful testbed for the development of AI models that can jointly reason over visual and natural language inputs. Despite a proliferation of VQA datasets, this goal is hindered by a set of common limitations. These include a reliance on relatively simplistic questions that are repetitive in both concepts and linguistic structure, little world knowledge needed outside of the paired image, and limited reasoning required to arrive at the correct answer. We introduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer. In contrast to the existing knowledge-based VQA datasets, the questions generally cannot be answered by simply querying a knowledge base, and instead require some form of commonsense reasoning about the scene depicted in the image. We demonstrate the potential of this new dataset through a detailed analysis of its contents and baseline performance measurements over a variety of state-of-the-art vision-language models. Project page: http://a-okvqa.allenai.org/



### Revisiting the "Video" in Video-Language Understanding
- **Arxiv ID**: http://arxiv.org/abs/2206.01720v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.01720v1)
- **Published**: 2022-06-03 17:57:33+00:00
- **Updated**: 2022-06-03 17:57:33+00:00
- **Authors**: Shyamal Buch, Cristóbal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, Juan Carlos Niebles
- **Comment**: CVPR 2022 (Oral)
- **Journal**: None
- **Summary**: What makes a video task uniquely suited for videos, beyond what can be understood from a single image? Building on recent progress in self-supervised image-language models, we revisit this question in the context of video and language tasks. We propose the atemporal probe (ATP), a new model for video-language analysis which provides a stronger bound on the baseline accuracy of multimodal models constrained by image-level understanding. By applying this model to standard discriminative video and language tasks, such as video question answering and text-to-video retrieval, we characterize the limitations and potential of current video-language benchmarks. We find that understanding of event temporality is often not necessary to achieve strong or state-of-the-art performance, even compared with recent large-scale video-language models and in contexts intended to benchmark deeper video-level understanding. We also demonstrate how ATP can improve both video-language dataset and model design. We describe a technique for leveraging ATP to better disentangle dataset subsets with a higher concentration of temporally challenging data, improving benchmarking efficacy for causal and temporal understanding. Further, we show that effectively integrating ATP into full video-level temporal models can improve efficiency and state-of-the-art accuracy.



### SNAKE: Shape-aware Neural 3D Keypoint Field
- **Arxiv ID**: http://arxiv.org/abs/2206.01724v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.01724v2)
- **Published**: 2022-06-03 17:58:43+00:00
- **Updated**: 2022-10-17 07:45:17+00:00
- **Authors**: Chengliang Zhong, Peixing You, Xiaoxue Chen, Hao Zhao, Fuchun Sun, Guyue Zhou, Xiaodong Mu, Chuang Gan, Wenbing Huang
- **Comment**: Accepted by NeurIPS 2022. Codes are available at
  https://github.com/zhongcl-thu/SNAKE
- **Journal**: None
- **Summary**: Detecting 3D keypoints from point clouds is important for shape reconstruction, while this work investigates the dual question: can shape reconstruction benefit 3D keypoint detection? Existing methods either seek salient features according to statistics of different orders or learn to predict keypoints that are invariant to transformation. Nevertheless, the idea of incorporating shape reconstruction into 3D keypoint detection is under-explored. We argue that this is restricted by former problem formulations. To this end, a novel unsupervised paradigm named SNAKE is proposed, which is short for shape-aware neural 3D keypoint field. Similar to recent coordinate-based radiance or distance field, our network takes 3D coordinates as inputs and predicts implicit shape indicators and keypoint saliency simultaneously, thus naturally entangling 3D keypoint detection and shape reconstruction. We achieve superior performance on various public benchmarks, including standalone object datasets ModelNet40, KeypointNet, SMPL meshes and scene-level datasets 3DMatch and Redwood. Intrinsic shape awareness brings several advantages as follows. (1) SNAKE generates 3D keypoints consistent with human semantic annotation, even without such supervision. (2) SNAKE outperforms counterparts in terms of repeatability, especially when the input point clouds are down-sampled. (3) the generated keypoints allow accurate geometric registration, notably in a zero-shot setting. Codes are available at https://github.com/zhongcl-thu/SNAKE



### Radar Guided Dynamic Visual Attention for Resource-Efficient RGB Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.01772v1
- **DOI**: 10.1109/IJCNN55064.2022.9892184
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.01772v1)
- **Published**: 2022-06-03 18:29:55+00:00
- **Updated**: 2022-06-03 18:29:55+00:00
- **Authors**: Hemant Kumawat, Saibal Mukhopadhyay
- **Comment**: Accepted in International Joint Conference on Neural Networks (IJCNN)
  2022
- **Journal**: 2022 International Joint Conference on Neural Networks (IJCNN)
- **Summary**: An autonomous system's perception engine must provide an accurate understanding of the environment for it to make decisions. Deep learning based object detection networks experience degradation in the performance and robustness for small and far away objects due to a reduction in object's feature map as we move to higher layers of the network. In this work, we propose a novel radar-guided spatial attention for RGB images to improve the perception quality of autonomous vehicles operating in a dynamic environment. In particular, our method improves the perception of small and long range objects, which are often not detected by the object detectors in RGB mode. The proposed method consists of two RGB object detectors, namely the Primary detector and a lightweight Secondary detector. The primary detector takes a full RGB image and generates primary detections. Next, the radar proposal framework creates regions of interest (ROIs) for object proposals by projecting the radar point cloud onto the 2D RGB image. These ROIs are cropped and fed to the secondary detector to generate secondary detections which are then fused with the primary detections via non-maximum suppression. This method helps in recovering the small objects by preserving the object's spatial features through an increase in their receptive field. We evaluate our fusion method on the challenging nuScenes dataset and show that our fusion method with SSD-lite as primary and secondary detector improves the baseline primary yolov3 detector's recall by 14% while requiring three times fewer computational resources.



### Monkeypox Image Data collection
- **Arxiv ID**: http://arxiv.org/abs/2206.01774v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01774v1)
- **Published**: 2022-06-03 18:35:19+00:00
- **Updated**: 2022-06-03 18:35:19+00:00
- **Authors**: Md Manjurul Ahsan, Muhammad Ramiz Uddin, Shahana Akter Luna
- **Comment**: This is the attempt of creating monkeypox image dataset collected
  from various sources and it will continue to update by collectiong samples
  from journals and other public access domains
- **Journal**: None
- **Summary**: This paper explains the initial Monkeypox Open image data collection procedure. It was created by assembling images collected from websites, newspapers, and online portals and currently contains around 1905 images after data augmentation.



### Real-Time Super-Resolution for Real-World Images on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2206.01777v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01777v1)
- **Published**: 2022-06-03 18:44:53+00:00
- **Updated**: 2022-06-03 18:44:53+00:00
- **Authors**: Jie Cai, Zibo Meng, Jiaming Ding, Chiu Man Ho
- **Comment**: arXiv admin note: text overlap with arXiv:2004.13674
- **Journal**: None
- **Summary**: Image Super-Resolution (ISR), which aims at recovering High-Resolution (HR) images from the corresponding Low-Resolution (LR) counterparts. Although recent progress in ISR has been remarkable. However, they are way too computationally intensive to be deployed on edge devices, since most of the recent approaches are deep learning-based. Besides, these methods always fail in real-world scenes, since most of them adopt a simple fixed "ideal" bicubic downsampling kernel from high-quality images to construct LR/HR training pairs which may lose track of frequency-related details. In this work, an approach for real-time ISR on mobile devices is presented, which is able to deal with a wide range of degradations in real-world scenarios. Extensive experiments on traditional super-resolution datasets (Set5, Set14, BSD100, Urban100, Manga109, DIV2K) and real-world images with a variety of degradations demonstrate that our method outperforms the state-of-art methods, resulting in higher PSNR and SSIM, lower noise and better visual quality. Most importantly, our method achieves real-time performance on mobile or edge devices.



### R2U++: A Multiscale Recurrent Residual U-Net with Dense Skip Connections for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.01793v1
- **DOI**: 10.1007/s00521-022-07419-7
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.01793v1)
- **Published**: 2022-06-03 19:42:44+00:00
- **Updated**: 2022-06-03 19:42:44+00:00
- **Authors**: Mehreen Mubashar, Hazrat Ali, Christer Gronlund, Shoaib Azmat
- **Comment**: Paper accepted in Neural Computing and Applications (2022). Please
  cite the final version available from Springer website
  https://link.springer.com/article/10.1007/s00521-022-07419-7
- **Journal**: None
- **Summary**: U-Net is a widely adopted neural network in the domain of medical image segmentation. Despite its quick embracement by the medical imaging community, its performance suffers on complicated datasets. The problem can be ascribed to its simple feature extracting blocks: encoder/decoder, and the semantic gap between encoder and decoder. Variants of U-Net (such as R2U-Net) have been proposed to address the problem of simple feature extracting blocks by making the network deeper, but it does not deal with the semantic gap problem. On the other hand, another variant UNET++ deals with the semantic gap problem by introducing dense skip connections but has simple feature extraction blocks. To overcome these issues, we propose a new U-Net based medical image segmentation architecture R2U++. In the proposed architecture, the adapted changes from vanilla U-Net are: (1) the plain convolutional backbone is replaced by a deeper recurrent residual convolution block. The increased field of view with these blocks aids in extracting crucial features for segmentation which is proven by improvement in the overall performance of the network. (2) The semantic gap between encoder and decoder is reduced by dense skip pathways. These pathways accumulate features coming from multiple scales and apply concatenation accordingly. The modified architecture has embedded multi-depth models, and an ensemble of outputs taken from varying depths improves the performance on foreground objects appearing at various scales in the images. The performance of R2U++ is evaluated on four distinct medical imaging modalities: electron microscopy (EM), X-rays, fundus, and computed tomography (CT). The average gain achieved in IoU score is 1.5+-0.37% and in dice score is 0.9+-0.33% over UNET++, whereas, 4.21+-2.72 in IoU and 3.47+-1.89 in dice score over R2U-Net across different medical imaging segmentation datasets.



### Additive MIL: Intrinsically Interpretable Multiple Instance Learning for Pathology
- **Arxiv ID**: http://arxiv.org/abs/2206.01794v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.01794v2)
- **Published**: 2022-06-03 19:43:06+00:00
- **Updated**: 2022-10-16 17:20:21+00:00
- **Authors**: Syed Ashar Javed, Dinkar Juyal, Harshith Padigela, Amaro Taylor-Weiner, Limin Yu, Aaditya Prakash
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple Instance Learning (MIL) has been widely applied in pathology towards solving critical problems such as automating cancer diagnosis and grading, predicting patient prognosis, and therapy response. Deploying these models in a clinical setting requires careful inspection of these black boxes during development and deployment to identify failures and maintain physician trust. In this work, we propose a simple formulation of MIL models, which enables interpretability while maintaining similar predictive performance. Our Additive MIL models enable spatial credit assignment such that the contribution of each region in the image can be exactly computed and visualized. We show that our spatial credit assignment coincides with regions used by pathologists during diagnosis and improves upon classical attention heatmaps from attention MIL models. We show that any existing MIL model can be made additive with a simple change in function composition. We also show how these models can debug model failures, identify spurious features, and highlight class-wise regions of interest, enabling their use in high-stakes environments such as clinical decision-making.



### Learning sRGB-to-Raw-RGB De-rendering with Content-Aware Metadata
- **Arxiv ID**: http://arxiv.org/abs/2206.01813v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01813v1)
- **Published**: 2022-06-03 20:43:17+00:00
- **Updated**: 2022-06-03 20:43:17+00:00
- **Authors**: Seonghyeon Nam, Abhijith Punnappurath, Marcus A. Brubaker, Michael S. Brown
- **Comment**: CVPR 2022 (GitHub:
  https://github.com/SamsungLabs/content-aware-metadata)
- **Journal**: None
- **Summary**: Most camera images are rendered and saved in the standard RGB (sRGB) format by the camera's hardware. Due to the in-camera photo-finishing routines, nonlinear sRGB images are undesirable for computer vision tasks that assume a direct relationship between pixel values and scene radiance. For such applications, linear raw-RGB sensor images are preferred. Saving images in their raw-RGB format is still uncommon due to the large storage requirement and lack of support by many imaging applications. Several "raw reconstruction" methods have been proposed that utilize specialized metadata sampled from the raw-RGB image at capture time and embedded in the sRGB image. This metadata is used to parameterize a mapping function to de-render the sRGB image back to its original raw-RGB format when needed. Existing raw reconstruction methods rely on simple sampling strategies and global mapping to perform the de-rendering. This paper shows how to improve the de-rendering results by jointly learning sampling and reconstruction. Our experiments show that our learned sampling can adapt to the image content to produce better raw reconstructions than existing methods. We also describe an online fine-tuning strategy for the reconstruction network to improve results further.



### EAANet: Efficient Attention Augmented Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2206.01821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01821v1)
- **Published**: 2022-06-03 21:22:12+00:00
- **Updated**: 2022-06-03 21:22:12+00:00
- **Authors**: Runqing Zhang, Tianshu Zhu
- **Comment**: 8 pages, 4 figures. Not published
- **Journal**: None
- **Summary**: Humans can effectively find salient regions in complex scenes. Self-attention mechanisms were introduced into Computer Vision (CV) to achieve this. Attention Augmented Convolutional Network (AANet) is a mixture of convolution and self-attention, which increases the accuracy of a typical ResNet. However, The complexity of self-attention is O(n2) in terms of computation and memory usage with respect to the number of input tokens. In this project, we propose EAANet: Efficient Attention Augmented Convolutional Networks, which incorporates efficient self-attention mechanisms in a convolution and self-attention hybrid architecture to reduce the model's memory footprint. Our best model show performance improvement over AA-Net and ResNet18. We also explore different methods to augment Convolutional Network with self-attention mechanisms and show the difficulty of training those methods compared to ResNet. Finally, we show that augmenting efficient self-attention mechanisms with ResNet scales better with input size than normal self-attention mechanisms. Therefore, our EAANet is more capable of working with high-resolution images.



### The Gamma Generalized Normal Distribution: A Descriptor of SAR Imagery
- **Arxiv ID**: http://arxiv.org/abs/2206.01826v1
- **DOI**: 10.1016/j.cam.2018.07.045
- **Categories**: **stat.ME**, cs.CV, eess.IV, math.ST, physics.data-an, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/2206.01826v1)
- **Published**: 2022-06-03 21:34:54+00:00
- **Updated**: 2022-06-03 21:34:54+00:00
- **Authors**: G. M. Cordeiro, R. J. Cintra, L. C. Rêgo, A. D. C. Nascimento
- **Comment**: 21 pages, 6 figures, 6 tables
- **Journal**: Journal of Computational and Applied Mathematics, vol. 347, pages
  257-272, February 2019
- **Summary**: We propose a new four-parameter distribution for modeling synthetic aperture radar (SAR) imagery named the gamma generalized normal (GGN) by combining the gamma and generalized normal distributions. A mathematical characterization of the new distribution is provided by identifying the limit behavior and by calculating the density and moment expansions. The GGN model performance is evaluated on both synthetic and actual data and, for that, maximum likelihood estimation and random number generation are discussed. The proposed distribution is compared with the beta generalized normal distribution (BGN), which has already shown to appropriately represent SAR imagery. The performance of these two distributions are measured by means of statistics which provide evidence that the GGN can outperform the BGN distribution in some contexts.



### Drawing out of Distribution with Neuro-Symbolic Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2206.01829v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, cs.SC
- **Links**: [PDF](http://arxiv.org/pdf/2206.01829v2)
- **Published**: 2022-06-03 21:40:22+00:00
- **Updated**: 2022-06-27 11:21:52+00:00
- **Authors**: Yichao Liang, Joshua B. Tenenbaum, Tuan Anh Le, N. Siddharth
- **Comment**: Preprint. Under review. 25 pages
- **Journal**: None
- **Summary**: Learning general-purpose representations from perceptual inputs is a hallmark of human intelligence. For example, people can write out numbers or characters, or even draw doodles, by characterizing these tasks as different instantiations of the same generic underlying process -- compositional arrangements of different forms of pen strokes. Crucially, learning to do one task, say writing, implies reasonable competence at another, say drawing, on account of this shared process. We present Drawing out of Distribution (DooD), a neuro-symbolic generative model of stroke-based drawing that can learn such general-purpose representations. In contrast to prior work, DooD operates directly on images, requires no supervision or expensive test-time inference, and performs unsupervised amortised inference with a symbolic stroke model that better enables both interpretability and generalization. We evaluate DooD on its ability to generalise across both data and tasks. We first perform zero-shot transfer from one dataset (e.g. MNIST) to another (e.g. Quickdraw), across five different datasets, and show that DooD clearly outperforms different baselines. An analysis of the learnt representations further highlights the benefits of adopting a symbolic stroke model. We then adopt a subset of the Omniglot challenge tasks, and evaluate its ability to generate new exemplars (both unconditionally and conditionally), and perform one-shot classification, showing that DooD matches the state of the art. Taken together, we demonstrate that DooD does indeed capture general-purpose representations across both data and task, and takes a further step towards building general and robust concept-learning systems.



### Spatial Feature Mapping for 6DoF Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2206.01831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01831v1)
- **Published**: 2022-06-03 21:44:10+00:00
- **Updated**: 2022-06-03 21:44:10+00:00
- **Authors**: Jianhan Mei, Xudong Jiang, Henghui Ding
- **Comment**: Pattern Recognition
- **Journal**: None
- **Summary**: This work aims to estimate 6Dof (6D) object pose in background clutter. Considering the strong occlusion and background noise, we propose to utilize the spatial structure for better tackling this challenging task. Observing that the 3D mesh can be naturally abstracted by a graph, we build the graph using 3D points as vertices and mesh connections as edges. We construct the corresponding mapping from 2D image features to 3D points for filling the graph and fusion of the 2D and 3D features. Afterward, a Graph Convolutional Network (GCN) is applied to help the feature exchange among objects' points in 3D space. To address the problem of rotation symmetry ambiguity for objects, a spherical convolution is utilized and the spherical features are combined with the convolutional features that are mapped to the graph. Predefined 3D keypoints are voted and the 6DoF pose is obtained via the fitting optimization. Two scenarios of inference, one with the depth information and the other without it are discussed. Tested on the datasets of YCB-Video and LINEMOD, the experiments demonstrate the effectiveness of our proposed method.



### Coffee Roast Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2206.01841v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.01841v1)
- **Published**: 2022-06-03 22:23:29+00:00
- **Updated**: 2022-06-03 22:23:29+00:00
- **Authors**: Sakdipat Ontoum, Thitaree Khemanantakul, Pornphat Sroison, Tuul Triyason, Bunthit Watanapa
- **Comment**: 6 pages, 13 figures, 3 tables, this work was presented at the CSC498
  COMPUTER SCIENCE CAPSTONE PROJECT I and CSC499 COMPUTER SCIENCE CAPSTONE
  PROJECT II courses
- **Journal**: None
- **Summary**: As the coffee industry has grown, there would be more demand for roasted coffee beans, as well as increased rivalry for selling coffee and attracting customers. As the flavor of each variety of coffee is dependent on the degree of roasting of the coffee beans, it is vital to maintain a consistent quality related to the degree of roasting. Each barista has their own method for determining the degree of roasting. However, extrinsic circumstances such as light, fatigue, and other factors may alter their judgment. As a result, the quality of the coffee cannot be controlled. The Coffee Roast Intelligence application is a machine learning-based study of roasted coffee bean degrees classification produced as an Android application platform that identifies the color of coffee beans by photographing or uploading them while roasting. This application displays the text showing at what level the coffee beans have been roasted, as well as informs the percent chance of class prediction to the consumers. Users may also keep track of the result of the predictions related to the roasting level of coffee beans.



### Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning
- **Arxiv ID**: http://arxiv.org/abs/2206.01843v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2206.01843v2)
- **Published**: 2022-06-03 22:33:09+00:00
- **Updated**: 2022-09-14 22:33:34+00:00
- **Authors**: Yujia Xie, Luowei Zhou, Xiyang Dai, Lu Yuan, Nguyen Bach, Ce Liu, Michael Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: People say, "A picture is worth a thousand words". Then how can we get the rich information out of the image? We argue that by using visual clues to bridge large pretrained vision foundation models and language models, we can do so without any extra cross-modal training. Thanks to the strong zero-shot capability of foundation models, we start by constructing a rich semantic representation of the image (e.g., image tags, object attributes / locations, captions) as a structured textual prompt, called visual clues, using a vision foundation model. Based on visual clues, we use large language model to produce a series of comprehensive descriptions for the visual content, which is then verified by the vision model again to select the candidate that aligns best with the image. We evaluate the quality of generated descriptions by quantitative and qualitative measurement. The results demonstrate the effectiveness of such a structured semantic representation.



