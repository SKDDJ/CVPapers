# Arxiv Papers in cs.CV on 2022-06-19
### TBraTS: Trusted Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.09309v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09309v3)
- **Published**: 2022-06-19 02:26:30+00:00
- **Updated**: 2022-07-28 09:34:54+00:00
- **Authors**: Ke Zou, Xuedong Yuan, Xiaojing Shen, Meng Wang, Huazhu Fu
- **Comment**: 11 pages, 4 figures, Accepted by MICCAI 2022
- **Journal**: None
- **Summary**: Despite recent improvements in the accuracy of brain tumor segmentation, the results still exhibit low levels of confidence and robustness. Uncertainty estimation is one effective way to change this situation, as it provides a measure of confidence in the segmentation results. In this paper, we propose a trusted brain tumor segmentation network which can generate robust segmentation results and reliable uncertainty estimations without excessive computational burden and modification of the backbone network. In our method, uncertainty is modeled explicitly using subjective logic theory, which treats the predictions of backbone neural network as subjective opinions by parameterizing the class probabilities of the segmentation as a Dirichlet distribution. Meanwhile, the trusted segmentation framework learns the function that gathers reliable evidence from the feature leading to the final segmentation results. Overall, our unified trusted segmentation framework endows the model with reliability and robustness to out-of-distribution samples. To evaluate the effectiveness of our model in robustness and reliability, qualitative and quantitative experiments are conducted on the BraTS 2019 dataset.



### EATFormer: Improving Vision Transformer Inspired by Evolutionary Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2206.09325v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.ET
- **Links**: [PDF](http://arxiv.org/pdf/2206.09325v1)
- **Published**: 2022-06-19 04:49:35+00:00
- **Updated**: 2022-06-19 04:49:35+00:00
- **Authors**: Jiangning Zhang, Xiangtai Li, Yabiao Wang, Chengjie Wang, Yibo Yang, Yong Liu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Motivated by biological evolution, this paper explains the rationality of Vision Transformer by analogy with the proven practical Evolutionary Algorithm (EA) and derives that both have consistent mathematical formulation. Then inspired by effective EA variants, we propose a novel pyramid EATFormer backbone that only contains the proposed \emph{EA-based Transformer} (EAT) block, which consists of three residual parts, \ie, \emph{Multi-Scale Region Aggregation} (MSRA), \emph{Global and Local Interaction} (GLI), and \emph{Feed-Forward Network} (FFN) modules, to model multi-scale, interactive, and individual information separately. Moreover, we design a \emph{Task-Related Head} (TRH) docked with transformer backbone to complete final information fusion more flexibly and \emph{improve} a \emph{Modulated Deformable MSA} (MD-MSA) to dynamically model irregular locations. Massive quantitative and quantitative experiments on image classification, downstream tasks, and explanatory experiments demonstrate the effectiveness and superiority of our approach over State-Of-The-Art (SOTA) methods. \Eg, our Mobile (1.8M), Tiny (6.1M), Small (24.3M), and Base (49.0M) models achieve 69.4, 78.4, 83.1, and 83.9 Top-1 only trained on ImageNet-1K with naive training recipe; EATFormer-Tiny/Small/Base armed Mask-R-CNN obtain 45.4/47.4/49.0 box AP and 41.4/42.9/44.2 mask AP on COCO detection, surpassing contemporary MPViT-T, Swin-T, and Swin-S by 0.6/1.4/0.5 box AP and 0.4/1.3/0.9 mask AP separately with less FLOPs; Our EATFormer-Small/Base achieve 47.3/49.3 mIoU on ADE20K by Upernet that exceeds Swin-T/S by 2.8/1.7. Code will be available at \url{https://https://github.com/zhangzjn/EATFormer}.



### What is Where by Looking: Weakly-Supervised Open-World Phrase-Grounding without Text Inputs
- **Arxiv ID**: http://arxiv.org/abs/2206.09358v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09358v2)
- **Published**: 2022-06-19 09:07:30+00:00
- **Updated**: 2022-06-27 13:04:52+00:00
- **Authors**: Tal Shaharabany, Yoad Tewel, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Given an input image, and nothing else, our method returns the bounding boxes of objects in the image and phrases that describe the objects. This is achieved within an open world paradigm, in which the objects in the input image may not have been encountered during the training of the localization mechanism. Moreover, training takes place in a weakly supervised setting, where no bounding boxes are provided. To achieve this, our method combines two pre-trained networks: the CLIP image-to-text matching score and the BLIP image captioning tool. Training takes place on COCO images and their captions and is based on CLIP. Then, during inference, BLIP is used to generate a hypothesis regarding various regions of the current image. Our work generalizes weakly supervised segmentation and phrase grounding and is shown empirically to outperform the state of the art in both domains. It also shows very convincing results in the novel task of weakly-supervised open-world purely visual phrase-grounding presented in our work. For example, on the datasets used for benchmarking phrase-grounding, our method results in a very modest degradation in comparison to methods that employ human captions as an additional input. Our code is available at https://github.com/talshaharabany/what-is-where-by-looking and a live demo can be found at https://replicate.com/talshaharabany/what-is-where-by-looking.



### Productive Reproducible Workflows for DNNs: A Case Study for Industrial Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.09359v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.PF, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/2206.09359v1)
- **Published**: 2022-06-19 09:10:13+00:00
- **Updated**: 2022-06-19 09:10:13+00:00
- **Authors**: Perry Gibson, Jos√© Cano
- **Comment**: 7 pages, 5 figures, AccML 2022
- **Journal**: None
- **Summary**: As Deep Neural Networks (DNNs) have become an increasingly ubiquitous workload, the range of libraries and tooling available to aid in their development and deployment has grown significantly. Scalable, production quality tools are freely available under permissive licenses, and are accessible enough to enable even small teams to be very productive. However within the research community, awareness and usage of said tools is not necessarily widespread, and researchers may be missing out on potential productivity gains from exploiting the latest tools and workflows. This paper presents a case study where we discuss our recent experience producing an end-to-end artificial intelligence application for industrial defect detection. We detail the high level deep learning libraries, containerized workflows, continuous integration/deployment pipelines, and open source code templates we leveraged to produce a competitive result, matching the performance of other ranked solutions to our three target datasets. We highlight the value that exploiting such systems can bring, even for research, and detail our solution and present our best results in terms of accuracy and inference time on a server class GPU, as well as inference times on a server class CPU, and a Raspberry Pi 4.



### Towards Generalizable Person Re-identification with a Bi-stream Generative Model
- **Arxiv ID**: http://arxiv.org/abs/2206.09362v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09362v2)
- **Published**: 2022-06-19 09:18:25+00:00
- **Updated**: 2022-06-26 09:31:07+00:00
- **Authors**: Xin Xu, Wei Liu, Zheng Wang, Ruiming Hu, Qi Tian
- **Comment**: There is a mistake of equation 1
- **Journal**: None
- **Summary**: Generalizable person re-identification (re-ID) has attracted growing attention due to its powerful adaptation capability in the unseen data domain. However, existing solutions often neglect either crossing cameras (e.g., illumination and resolution differences) or pedestrian misalignments (e.g., viewpoint and pose discrepancies), which easily leads to poor generalization capability when adapted to the new domain. In this paper, we formulate these difficulties as: 1) Camera-Camera (CC) problem, which denotes the various human appearance changes caused by different cameras; 2) Camera-Person (CP) problem, which indicates the pedestrian misalignments caused by the same identity person under different camera viewpoints or changing pose. To solve the above issues, we propose a Bi-stream Generative Model (BGM) to learn the fine-grained representations fused with camera-invariant global feature and pedestrian-aligned local feature, which contains an encoding network and two stream decoding sub-networks. Guided by original pedestrian images, one stream is employed to learn a camera-invariant global feature for the CC problem via filtering cross-camera interference factors. For the CP problem, another stream learns a pedestrian-aligned local feature for pedestrian alignment using information-complete densely semantically aligned part maps. Moreover, a part-weighted loss function is presented to reduce the influence of missing parts on pedestrian alignment. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods on the large-scale generalizable re-ID benchmarks, involving domain generalization setting and cross-domain setting.



### Semi-supervised Change Detection of Small Water Bodies Using RGB and Multispectral Images in Peruvian Rainforests
- **Arxiv ID**: http://arxiv.org/abs/2206.09365v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2206.09365v1)
- **Published**: 2022-06-19 09:30:48+00:00
- **Updated**: 2022-06-19 09:30:48+00:00
- **Authors**: Kangning Cui, Seda Camalan, Ruoning Li, Victor P. Pauca, Sarra Alqahtani, Robert J. Plemmons, Miles Silman, Evan N. Dethier, David Lutz, Raymond H. Chan
- **Comment**: 8 pages, 5 figures. Accepted to Proceedings of IEEE WHISPERS 2022
- **Journal**: None
- **Summary**: Artisanal and Small-scale Gold Mining (ASGM) is an important source of income for many households, but it can have large social and environmental effects, especially in rainforests of developing countries. The Sentinel-2 satellites collect multispectral images that can be used for the purpose of detecting changes in water extent and quality which indicates the locations of mining sites. This work focuses on the recognition of ASGM activities in Peruvian Amazon rainforests. We tested several semi-supervised classifiers based on Support Vector Machines (SVMs) to detect the changes of water bodies from 2019 to 2021 in the Madre de Dios region, which is one of the global hotspots of ASGM activities. Experiments show that SVM-based models can achieve reasonable performance for both RGB (using Cohen's $\kappa$ 0.49) and 6-channel images (using Cohen's $\kappa$ 0.71) with very limited annotations. The efficacy of incorporating Lab color space for change detection is analyzed as well.



### mvHOTA: A multi-view higher order tracking accuracy metric to measure spatial and temporal associations in multi-point detection
- **Arxiv ID**: http://arxiv.org/abs/2206.09372v2
- **DOI**: 10.1080/21681163.2022.2159535
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.09372v2)
- **Published**: 2022-06-19 10:31:53+00:00
- **Updated**: 2023-01-23 10:44:12+00:00
- **Authors**: Lalith Sharan, Halvar Kelm, Gabriele Romano, Matthias Karck, Raffaele De Simone, Sandy Engelhardt
- **Comment**: 16 pages, 9 figures
- **Journal**: Computer Methods in Biomechanics and Biomedical Engineering:
  Imaging & Visualization (2022) 1-9
- **Summary**: Multi-point tracking is a challenging task that involves detecting points in the scene and tracking them across a sequence of frames. Computing detection-based measures like the F-measure on a frame-by-frame basis is not sufficient to assess the overall performance, as it does not interpret performance in the temporal domain. The main evaluation metric available comes from Multi-object tracking (MOT) methods to benchmark performance on datasets such as KITTI with the recently proposed higher order tracking accuracy (HOTA) metric, which is capable of providing a better description of the performance over metrics such as MOTA, DetA, and IDF1. While the HOTA metric takes into account temporal associations, it does not provide a tailored means to analyse the spatial associations of a dataset in a multi-camera setup. Moreover, there are differences in evaluating the detection task for points when compared to objects (point distances vs. bounding box overlap). Therefore in this work, we propose a multi-view higher order tracking metric (mvHOTA) to determine the accuracy of multi-point (multi-instance and multi-class) tracking methods, while taking into account temporal and spatial associations.mvHOTA can be interpreted as the geometric mean of detection, temporal, and spatial associations, thereby providing equal weighting to each of the factors. We demonstrate the use of this metric to evaluate the tracking performance on an endoscopic point detection dataset from a previously organised surgical data science challenge. Furthermore, we compare with other adjusted MOT metrics for this use-case, discuss the properties of mvHOTA, and show how the proposed multi-view Association and the Occlusion index (OI) facilitate analysis of methods with respect to handling of occlusions. The code is available at https://github.com/Cardio-AI/mvhota.



### A Self-Guided Framework for Radiology Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2206.09378v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09378v1)
- **Published**: 2022-06-19 11:09:27+00:00
- **Updated**: 2022-06-19 11:09:27+00:00
- **Authors**: Jun Li, Shibo Li, Ying Hu, Huiren Tao
- **Comment**: 11 pages, 3 figures, accepted by Medical Image Computing and Computer
  Assisted Intervention 2022(MICCAI 2022)
- **Journal**: None
- **Summary**: Automatic radiology report generation is essential to computer-aided diagnosis. Through the success of image captioning, medical report generation has been achievable. However, the lack of annotated disease labels is still the bottleneck of this area. In addition, the image-text data bias problem and complex sentences make it more difficult to generate accurate reports. To address these gaps, we pre-sent a self-guided framework (SGF), a suite of unsupervised and supervised deep learning methods to mimic the process of human learning and writing. In detail, our framework obtains the domain knowledge from medical reports with-out extra disease labels and guides itself to extract fined-grain visual features as-sociated with the text. Moreover, SGF successfully improves the accuracy and length of medical report generation by incorporating a similarity comparison mechanism that imitates the process of human self-improvement through compar-ative practice. Extensive experiments demonstrate the utility of our SGF in the majority of cases, showing its superior performance over state-of-the-art meth-ods. Our results highlight the capacity of the proposed framework to distinguish fined-grained visual details between words and verify its advantage in generating medical reports.



### Scalable Neural Data Server: A Data Recommender for Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.09386v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09386v1)
- **Published**: 2022-06-19 12:07:32+00:00
- **Updated**: 2022-06-19 12:07:32+00:00
- **Authors**: Tianshi Cao, Sasha Doubov, David Acuna, Sanja Fidler
- **Comment**: Neurips 2021
- **Journal**: Advances in Neural Information Processing Systems, Volume 34,
  pages 8984-8997, year 2021
- **Summary**: Absence of large-scale labeled data in the practitioner's target domain can be a bottleneck to applying machine learning algorithms in practice. Transfer learning is a popular strategy for leveraging additional data to improve the downstream performance, but finding the most relevant data to transfer from can be challenging. Neural Data Server (NDS), a search engine that recommends relevant data for a given downstream task, has been previously proposed to address this problem. NDS uses a mixture of experts trained on data sources to estimate similarity between each source and the downstream task. Thus, the computational cost to each user grows with the number of sources. To address these issues, we propose Scalable Neural Data Server (SNDS), a large-scale search engine that can theoretically index thousands of datasets to serve relevant ML data to end users. SNDS trains the mixture of experts on intermediary datasets during initialization, and represents both data sources and downstream tasks by their proximity to the intermediary datasets. As such, computational cost incurred by SNDS users remains fixed as new datasets are added to the server. We validate SNDS on a plethora of real world tasks and find that data recommended by SNDS improves downstream task performance over baselines. We also demonstrate the scalability of SNDS by showing its ability to select relevant data for transfer outside of the natural image setting.



### Towards Adversarial Attack on Vision-Language Pre-training Models
- **Arxiv ID**: http://arxiv.org/abs/2206.09391v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2206.09391v2)
- **Published**: 2022-06-19 12:55:45+00:00
- **Updated**: 2022-10-20 02:32:02+00:00
- **Authors**: Jiaming Zhang, Qi Yi, Jitao Sang
- **Comment**: Accepted by ACM MM2022. Code is available in GitHub
- **Journal**: None
- **Summary**: While vision-language pre-training model (VLP) has shown revolutionary improvements on various vision-language (V+L) tasks, the studies regarding its adversarial robustness remain largely unexplored. This paper studied the adversarial attack on popular VLP models and V+L tasks. First, we analyzed the performance of adversarial attacks under different settings. By examining the influence of different perturbed objects and attack targets, we concluded some key observations as guidance on both designing strong multimodal adversarial attack and constructing robust VLP models. Second, we proposed a novel multimodal attack method on the VLP models called Collaborative Multimodal Adversarial Attack (Co-Attack), which collectively carries out the attacks on the image modality and the text modality. Experimental results demonstrated that the proposed method achieves improved attack performances on different V+L downstream tasks and VLP models. The analysis observations and novel attack method hopefully provide new understanding into the adversarial robustness of VLP models, so as to contribute their safe and reliable deployment in more real-world scenarios. Code is available at https://github.com/adversarial-for-goodness/Co-Attack.



### JPEG Compression-Resistant Low-Mid Adversarial Perturbation against Unauthorized Face Recognition System
- **Arxiv ID**: http://arxiv.org/abs/2206.09410v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09410v1)
- **Published**: 2022-06-19 14:15:49+00:00
- **Updated**: 2022-06-19 14:15:49+00:00
- **Authors**: Jiaming Zhang, Qi Yi, Jitao Sang
- **Comment**: None
- **Journal**: None
- **Summary**: It has been observed that the unauthorized use of face recognition system raises privacy problems. Using adversarial perturbations provides one possible solution to address this issue. A critical issue to exploit adversarial perturbation against unauthorized face recognition system is that: The images uploaded to the web need to be processed by JPEG compression, which weakens the effectiveness of adversarial perturbation. Existing JPEG compression-resistant methods fails to achieve a balance among compression resistance, transferability, and attack effectiveness. To this end, we propose a more natural solution called low frequency adversarial perturbation (LFAP). Instead of restricting the adversarial perturbations, we turn to regularize the source model to employing more low-frequency features by adversarial training. Moreover, to better influence model in different frequency components, we proposed the refined low-mid frequency adversarial perturbation (LMFAP) considering the mid frequency components as the productive complement. We designed a variety of settings in this study to simulate the real-world application scenario, including cross backbones, supervisory heads, training datasets and testing datasets. Quantitative and qualitative experimental results validate the effectivenss of proposed solutions.



### Terrain Classification using Transfer Learning on Hyperspectral Images: A Comparative study
- **Arxiv ID**: http://arxiv.org/abs/2206.09414v1
- **DOI**: 10.1109/INDICON56171.2022.10040049
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09414v1)
- **Published**: 2022-06-19 14:36:33+00:00
- **Updated**: 2022-06-19 14:36:33+00:00
- **Authors**: Uphar Singh, Kumar Saurabh, Neelaksh Trehan, Ranjana Vyas, O. P. Vyas
- **Comment**: None
- **Journal**: None
- **Summary**: A Hyperspectral image contains much more number of channels as compared to a RGB image, hence containing more information about entities within the image. The convolutional neural network (CNN) and the Multi-Layer Perceptron (MLP) have been proven to be an effective method of image classification. However, they suffer from the issues of long training time and requirement of large amounts of the labeled data, to achieve the expected outcome. These issues become more complex while dealing with hyperspectral images. To decrease the training time and reduce the dependence on large labeled dataset, we propose using the method of transfer learning. The hyperspectral dataset is preprocessed to a lower dimension using PCA, then deep learning models are applied to it for the purpose of classification. The features learned by this model are then used by the transfer learning model to solve a new classification problem on an unseen dataset. A detailed comparison of CNN and multiple MLP architectural models is performed, to determine an optimum architecture that suits best the objective. The results show that the scaling of layers not always leads to increase in accuracy but often leads to overfitting, and also an increase in the training time.The training time is reduced to greater extent by applying the transfer learning approach rather than just approaching the problem by directly training a new model on large datasets, without much affecting the accuracy.



### Agricultural Plantation Classification using Transfer Learning Approach based on CNN
- **Arxiv ID**: http://arxiv.org/abs/2206.09420v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09420v1)
- **Published**: 2022-06-19 14:43:31+00:00
- **Updated**: 2022-06-19 14:43:31+00:00
- **Authors**: Uphar Singh, Tushar Musale, Ranjana Vyas, O. P. Vyas
- **Comment**: None
- **Journal**: None
- **Summary**: Hyper-spectral images are images captured from a satellite that gives spatial and spectral information of specific region.A Hyper-spectral image contains much more number of channels as compared to a RGB image, hence containing more information about entities within the image. It makes them well suited for the classification of objects in a snap. In the past years, the efficiency of hyper-spectral image recognition has increased significantly with deep learning. The Convolution Neural Network(CNN) and Multi-Layer Perceptron(MLP) has demonstrated to be an excellent process of classifying images. However, they suffer from the issues of long training time and requirement of large amounts of the labeled data, to achieve the expected outcome. These issues become more complex while dealing with hyper-spectral images. To decrease the training time and reduce the dependence on large labeled data-set, we propose using the method of transfer learning.The features learned by CNN and MLP models are then used by the transfer learning model to solve a new classification problem on an unseen dataset. A detailed comparison of CNN and multiple MLP architectural models is performed, to determine an optimum architecture that suits best the objective. The results show that the scaling of layers not always leads to increase in accuracy but often leads to over-fitting, and also an increase in the training time.The training time is reduced to greater extent by applying the transfer learning approach rather than just approaching the problem by directly training a new model on large data-sets, without much affecting the accuracy.



### SNN2ANN: A Fast and Memory-Efficient Training Framework for Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2206.09449v1
- **DOI**: 10.1016/j.patcog.2023.109826
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09449v1)
- **Published**: 2022-06-19 16:52:56+00:00
- **Updated**: 2022-06-19 16:52:56+00:00
- **Authors**: Jianxiong Tang, Jianhuang Lai, Xiaohua Xie, Lingxiao Yang, Wei-Shi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Spiking neural networks are efficient computation models for low-power environments. Spike-based BP algorithms and ANN-to-SNN (ANN2SNN) conversions are successful techniques for SNN training. Nevertheless, the spike-base BP training is slow and requires large memory costs. Though ANN2NN provides a low-cost way to train SNNs, it requires many inference steps to mimic the well-trained ANN for good performance. In this paper, we propose a SNN-to-ANN (SNN2ANN) framework to train the SNN in a fast and memory-efficient way. The SNN2ANN consists of 2 components: a) a weight sharing architecture between ANN and SNN and b) spiking mapping units. Firstly, the architecture trains the weight-sharing parameters on the ANN branch, resulting in fast training and low memory costs for SNN. Secondly, the spiking mapping units ensure that the activation values of the ANN are the spiking features. As a result, the classification error of the SNN can be optimized by training the ANN branch. Besides, we design an adaptive threshold adjustment (ATA) algorithm to address the noisy spike problem. Experiment results show that our SNN2ANN-based models perform well on the benchmark datasets (CIFAR10, CIFAR100, and Tiny-ImageNet). Moreover, the SNN2ANN can achieve comparable accuracy under 0.625x time steps, 0.377x training time, 0.27x GPU memory costs, and 0.33x spike activities of the Spike-based BP model.



### 3D Object Detection for Autonomous Driving: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/2206.09474v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.09474v2)
- **Published**: 2022-06-19 19:43:11+00:00
- **Updated**: 2023-04-04 01:46:59+00:00
- **Authors**: Jiageng Mao, Shaoshuai Shi, Xiaogang Wang, Hongsheng Li
- **Comment**: Accepted to International Journal of Computer Vision (IJCV). Project
  page is at
  https://github.com/PointsCoder/Awesome-3D-Object-Detection-for-Autonomous-Driving
- **Journal**: None
- **Summary**: Autonomous driving, in recent years, has been receiving increasing attention for its potential to relieve drivers' burdens and improve the safety of driving. In modern autonomous driving pipelines, the perception system is an indispensable component, aiming to accurately estimate the status of surrounding environments and provide reliable observations for prediction and planning. 3D object detection, which intelligently predicts the locations, sizes, and categories of the critical 3D objects near an autonomous vehicle, is an important part of a perception system. This paper reviews the advances in 3D object detection for autonomous driving. First, we introduce the background of 3D object detection and discuss the challenges in this task. Second, we conduct a comprehensive survey of the progress in 3D object detection from the aspects of models and sensory inputs, including LiDAR-based, camera-based, and multi-modal detection approaches. We also provide an in-depth analysis of the potentials and challenges in each category of methods. Additionally, we systematically investigate the applications of 3D object detection in driving systems. Finally, we conduct a performance analysis of the 3D object detection approaches, and we further summarize the research trends over the years and prospect the future directions of this area.



### StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2206.09479v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09479v3)
- **Published**: 2022-06-19 20:12:41+00:00
- **Updated**: 2023-08-18 20:39:27+00:00
- **Authors**: Minguk Kang, Joonghyuk Shin, Jaesik Park
- **Comment**: 32 pages, IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI, 2023)
- **Journal**: None
- **Summary**: Generative Adversarial Network (GAN) is one of the state-of-the-art generative models for realistic image synthesis. While training and evaluating GAN becomes increasingly important, the current GAN research ecosystem does not provide reliable benchmarks for which the evaluation is conducted consistently and fairly. Furthermore, because there are few validated GAN implementations, researchers devote considerable time to reproducing baselines. We study the taxonomy of GAN approaches and present a new open-source library named StudioGAN. StudioGAN supports 7 GAN architectures, 9 conditioning methods, 4 adversarial losses, 12 regularization modules, 3 differentiable augmentations, 7 evaluation metrics, and 5 evaluation backbones. With our training and evaluation protocol, we present a large-scale benchmark using various datasets (CIFAR10, ImageNet, AFHQv2, FFHQ, and Baby/Papa/Granpa-ImageNet) and 3 different evaluation backbones (InceptionV3, SwAV, and Swin Transformer). Unlike other benchmarks used in the GAN community, we train representative GANs, including BigGAN and StyleGAN series in a unified training pipeline and quantify generation performance with 7 evaluation metrics. The benchmark evaluates other cutting-edge generative models (e.g., StyleGAN-XL, ADM, MaskGIT, and RQ-Transformer). StudioGAN provides GAN implementations, training, and evaluation scripts with the pre-trained weights. StudioGAN is available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.



### Video frame interpolation for high dynamic range sequences captured with dual-exposure sensors
- **Arxiv ID**: http://arxiv.org/abs/2206.09485v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09485v3)
- **Published**: 2022-06-19 20:29:34+00:00
- **Updated**: 2023-05-31 13:58:01+00:00
- **Authors**: Uƒüur √áoƒüalan, Mojtaba Bemana, Hans-Peter Seidel, Karol Myszkowski
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: Video frame interpolation (VFI) enables many important applications that might involve the temporal domain, such as slow motion playback, or the spatial domain, such as stop motion sequences. We are focusing on the former task, where one of the key challenges is handling high dynamic range (HDR) scenes in the presence of complex motion. To this end, we explore possible advantages of dual-exposure sensors that readily provide sharp short and blurry long exposures that are spatially registered and whose ends are temporally aligned. This way, motion blur registers temporally continuous information on the scene motion that, combined with the sharp reference, enables more precise motion sampling within a single camera shot. We demonstrate that this facilitates a more complex motion reconstruction in the VFI task, as well as HDR frame reconstruction that so far has been considered only for the originally captured frames, not in-between interpolated frames. We design a neural network trained in these tasks that clearly outperforms existing solutions. We also propose a metric for scene motion complexity that provides important insights into the performance of VFI methods at the test time.



### Unbiased Teacher v2: Semi-supervised Object Detection for Anchor-free and Anchor-based Detectors
- **Arxiv ID**: http://arxiv.org/abs/2206.09500v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09500v1)
- **Published**: 2022-06-19 22:57:48+00:00
- **Updated**: 2022-06-19 22:57:48+00:00
- **Authors**: Yen-Cheng Liu, Chih-Yao Ma, Zsolt Kira
- **Comment**: Project Page is at
  http://ycliu93.github.io/projects/unbiasedteacher2.html
- **Journal**: None
- **Summary**: With the recent development of Semi-Supervised Object Detection (SS-OD) techniques, object detectors can be improved by using a limited amount of labeled data and abundant unlabeled data. However, there are still two challenges that are not addressed: (1) there is no prior SS-OD work on anchor-free detectors, and (2) prior works are ineffective when pseudo-labeling bounding box regression. In this paper, we present Unbiased Teacher v2, which shows the generalization of SS-OD method to anchor-free detectors and also introduces Listen2Student mechanism for the unsupervised regression loss. Specifically, we first present a study examining the effectiveness of existing SS-OD methods on anchor-free detectors and find that they achieve much lower performance improvements under the semi-supervised setting. We also observe that box selection with centerness and the localization-based labeling used in anchor-free detectors cannot work well under the semi-supervised setting. On the other hand, our Listen2Student mechanism explicitly prevents misleading pseudo-labels in the training of bounding box regression; we specifically develop a novel pseudo-labeling selection mechanism based on the Teacher and Student's relative uncertainties. This idea contributes to favorable improvement in the regression branch in the semi-supervised setting. Our method, which works for both anchor-free and anchor-based methods, consistently performs favorably against the state-of-the-art methods in VOC, COCO-standard, and COCO-additional.



### A Parallel Implementation of Computing Mean Average Precision
- **Arxiv ID**: http://arxiv.org/abs/2206.09504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.09504v1)
- **Published**: 2022-06-19 23:23:52+00:00
- **Updated**: 2022-06-19 23:23:52+00:00
- **Authors**: Beinan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Mean Average Precision (mAP) has been widely used for evaluating the quality of object detectors, but an efficient implementation is still absent. Current implementations can only count true positives (TP's) and false positives (FP's) for one class at a time by looping through every detection of that class sequentially. Not only are these approaches inefficient, but they are also inconvenient for reporting validation mAP during training. We propose a parallelized alternative that can process mini-batches of detected bounding boxes (DTBB's) and ground truth bounding boxes (GTBB's) as inference goes such that mAP can be instantly calculated after inference is finished. Loops and control statements in sequential implementations are replaced with extensive uses of broadcasting, masking, and indexing. All operators involved are supported by popular machine learning frameworks such as PyTorch and TensorFlow. As a result, our implementation is much faster and can easily fit into typical training routines. A PyTorch version of our implementation is available at https://github.com/bwangca/fast-map.



### Hybrid Facial Expression Recognition (FER2013) Model for Real-Time Emotion Classification and Prediction
- **Arxiv ID**: http://arxiv.org/abs/2206.09509v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.09509v2)
- **Published**: 2022-06-19 23:43:41+00:00
- **Updated**: 2022-08-01 07:49:41+00:00
- **Authors**: Ozioma Collins Oguine, Kanyifeechukwu Jane Oguine, Hashim Ibrahim Bisallah, Daniel Ofuani
- **Comment**: 8 Pages, 8 Figures, 5 Tables
- **Journal**: None
- **Summary**: Facial Expression Recognition is a vital research topic in most fields ranging from artificial intelligence and gaming to Human-Computer Interaction (HCI) and Psychology. This paper proposes a hybrid model for Facial Expression recognition, which comprises a Deep Convolutional Neural Network (DCNN) and Haar Cascade deep learning architectures. The objective is to classify real-time and digital facial images into one of the seven facial emotion categories considered. The DCNN employed in this research has more convolutional layers, ReLU Activation functions, and multiple kernels to enhance filtering depth and facial feature extraction. In addition, a haar cascade model was also mutually used to detect facial features in real-time images and video frames. Grayscale images from the Kaggle repository (FER-2013) and then exploited Graphics Processing Unit (GPU) computation to expedite the training and validation process. Pre-processing and data augmentation techniques are applied to improve training efficiency and classification performance. The experimental results show a significantly improved classification performance compared to state-of-the-art (SoTA) experiments and research. Also, compared to other conventional models, this paper validates that the proposed architecture is superior in classification performance with an improvement of up to 6%, totaling up to 70% accuracy, and with less execution time of 2098.8s.



