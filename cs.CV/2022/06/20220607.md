# Arxiv Papers in cs.CV on 2022-06-07
### Saliency Cards: A Framework to Characterize and Compare Saliency Methods
- **Arxiv ID**: http://arxiv.org/abs/2206.02958v2
- **DOI**: 10.1145/3593013.3593997
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02958v2)
- **Published**: 2022-06-07 01:21:49+00:00
- **Updated**: 2023-05-30 20:17:18+00:00
- **Authors**: Angie Boggust, Harini Suresh, Hendrik Strobelt, John V. Guttag, Arvind Satyanarayan
- **Comment**: Published at FAccT 2023, 19 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: Saliency methods are a common class of machine learning interpretability techniques that calculate how important each input feature is to a model's output. We find that, with the rapid pace of development, users struggle to stay informed of the strengths and limitations of new methods and, thus, choose methods for unprincipled reasons (e.g., popularity). Moreover, despite a corresponding rise in evaluation metrics, existing approaches assume universal desiderata for saliency methods (e.g., faithfulness) that do not account for diverse user needs. In response, we introduce saliency cards: structured documentation of how saliency methods operate and their performance across a battery of evaluative metrics. Through a review of 25 saliency method papers and 33 method evaluations, we identify 10 attributes that users should account for when choosing a method. We group these attributes into three categories that span the process of computing and interpreting saliency: methodology, or how the saliency is calculated; sensitivity, or the relationship between the saliency and the underlying model and data; and, perceptibility, or how an end user ultimately interprets the result. By collating this information, saliency cards allow users to more holistically assess and compare the implications of different methods. Through nine semi-structured interviews with users from various backgrounds, including researchers, radiologists, and computational biologists, we find that saliency cards provide a detailed vocabulary for discussing individual methods and allow for a more systematic selection of task-appropriate methods. Moreover, with saliency cards, we are able to analyze the research landscape in a more structured fashion to identify opportunities for new methods and evaluation metrics for unmet user needs.



### HMRNet: High and Multi-Resolution Network with Bidirectional Feature Calibration for Brain Structure Segmentation in Radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/2206.02959v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02959v1)
- **Published**: 2022-06-07 01:23:40+00:00
- **Updated**: 2022-06-07 01:23:40+00:00
- **Authors**: Hao Fu, Guotai Wang, Wenhui Lei, Wei Xu, Qianfei Zhao, Shichuan Zhang, Kang Li, Shaoting Zhang
- **Comment**: 11 pages, 6 figures, Accepted by IEEE JBHI
- **Journal**: None
- **Summary**: Accurate segmentation of Anatomical brain Barriers to Cancer spread (ABCs) plays an important role for automatic delineation of Clinical Target Volume (CTV) of brain tumors in radiotherapy. Despite that variants of U-Net are state-of-the-art segmentation models, they have limited performance when dealing with ABCs structures with various shapes and sizes, especially thin structures (e.g., the falx cerebri) that span only few slices. To deal with this problem, we propose a High and Multi-Resolution Network (HMRNet) that consists of a multi-scale feature learning branch and a high-resolution branch, which can maintain the high-resolution contextual information and extract more robust representations of anatomical structures with various scales. We further design a Bidirectional Feature Calibration (BFC) block to enable the two branches to generate spatial attention maps for mutual feature calibration. Considering the different sizes and positions of ABCs structures, our network was applied after a rough localization of each structure to obtain fine segmentation results. Experiments on the MICCAI 2020 ABCs challenge dataset showed that: 1) Our proposed two-stage segmentation strategy largely outperformed methods segmenting all the structures in just one stage; 2) The proposed HMRNet with two branches can maintain high-resolution representations and is effective to improve the performance on thin structures; 3) The proposed BFC block outperformed existing attention methods using monodirectional feature calibration. Our method won the second place of ABCs 2020 challenge and has a potential for more accurate and reasonable delineation of CTV of brain tumors.



### Masked Unsupervised Self-training for Label-free Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.02967v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.02967v2)
- **Published**: 2022-06-07 02:03:06+00:00
- **Updated**: 2023-03-10 01:15:56+00:00
- **Authors**: Junnan Li, Silvio Savarese, Steven C. H. Hoi
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art computer vision models are mostly trained with supervised learning using human-labeled images, which limits their scalability due to the expensive annotation cost. While self-supervised representation learning has achieved impressive progress, it still requires a second stage of finetuning on labeled data. On the other hand, models pre-trained with large-scale text-image supervision (e.g., CLIP) have enabled zero-shot transfer to downstream image classification tasks. However, the zero-shot performance of CLIP-like models are often insufficient for real-world adoption. In this paper, we aim to leverage the abundant unlabeled data from a target domain to improve the performance of a pre-trained zero-shot classifier, by unsupervised finetuning of the pre-trained model. We propose Masked Unsupervised Self-Training (MUST), a new unsupervised adaptation method which leverages two different and complementary sources of training signals: pseudo-labels and raw images. MUST jointly optimizes three objectives to learn both class-level global feature and pixel-level local feature and enforces a regularization between the two. We demonstrate the efficacy of MUST on a variety of downstream tasks, where it improves upon CLIP by a large margin. MUST also outperforms supervised few-shot adaptation methods. It achieves a top-1 accuracy of 77.7% on ImageNet using ViT-B, +9.4% higher than CLIP, and +6.2% higher than 16-shot CLIP adaptation. Our code is available at https://github.com/salesforce/MUST.



### DETR++: Taming Your Multi-Scale Detection Transformer
- **Arxiv ID**: http://arxiv.org/abs/2206.02977v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.02977v1)
- **Published**: 2022-06-07 02:38:31+00:00
- **Updated**: 2022-06-07 02:38:31+00:00
- **Authors**: Chi Zhang, Lijuan Liu, Xiaoxue Zang, Frederick Liu, Hao Zhang, Xinying Song, Jindong Chen
- **Comment**: T4V: Transformers for Vision workshop @ CVPR 2022
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have dominated the field of detection ever since the success of AlexNet in ImageNet classification [12]. With the sweeping reform of Transformers [27] in natural language processing, Carion et al. [2] introduce the Transformer-based detection method, i.e., DETR. However, due to the quadratic complexity in the self-attention mechanism in the Transformer, DETR is never able to incorporate multi-scale features as performed in existing CNN-based detectors, leading to inferior results in small object detection. To mitigate this issue and further improve performance of DETR, in this work, we investigate different methods to incorporate multi-scale features and find that a Bi-directional Feature Pyramid (BiFPN) works best with DETR in further raising the detection precision. With this discovery, we propose DETR++, a new architecture that improves detection results by 1.9% AP on MS COCO 2017, 11.5% AP on RICO icon detection, and 9.1% AP on RICO layout extraction over existing baselines.



### Structured Context Transformer for Generic Event Boundary Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.02985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02985v1)
- **Published**: 2022-06-07 03:00:24+00:00
- **Updated**: 2022-06-07 03:00:24+00:00
- **Authors**: Congcong Li, Xinyao Wang, Dexiang Hong, Yufei Wang, Libo Zhang, Tiejian Luo, Longyin Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Generic Event Boundary Detection (GEBD) aims to detect moments where humans naturally perceive as event boundaries. In this paper, we present Structured Context Transformer (or SC-Transformer) to solve the GEBD task, which can be trained in an end-to-end fashion. Specifically, we use the backbone convolutional neural network (CNN) to extract the features of each video frame. To capture temporal context information of each frame, we design the structure context transformer (SC-Transformer) by re-partitioning input frame sequence. Note that, the overall computation complexity of SC-Transformer is linear to the video length. After that, the group similarities are computed to capture the differences between frames. Then, a lightweight fully convolutional network is used to determine the event boundaries based on the grouped similarity maps. To remedy the ambiguities of boundary annotations, the Gaussian kernel is adopted to preprocess the ground-truth event boundaries to further boost the accuracy. Extensive experiments conducted on the challenging Kinetics-GEBD and TAPOS datasets demonstrate the effectiveness of the proposed method compared to the state-of-the-art methods.



### TadML: A fast temporal action detection with Mechanics-MLP
- **Arxiv ID**: http://arxiv.org/abs/2206.02997v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02997v1)
- **Published**: 2022-06-07 04:07:48+00:00
- **Updated**: 2022-06-07 04:07:48+00:00
- **Authors**: Bowen Deng, Dongchang Liu
- **Comment**: 8 pages,3 figures
- **Journal**: None
- **Summary**: Temporal Action Detection(TAD) is a crucial but challenging task in video understanding.It is aimed at detecting both the type and start-end frame for each action instance in a long, untrimmed video.Most current models adopt both RGB and Optical-Flow streams for the TAD task. Thus, original RGB frames must be converted manually into Optical-Flow frames with additional computation and time cost, which is an obstacle to achieve real-time processing. At present, many models adopt two-stage strategies, which would slow the inference speed down and complicatedly tuning on proposals generating.By comparison, we propose a one-stage anchor-free temporal localization method with RGB stream only, in which a novel Newtonian \emph{Mechanics-MLP} architecture is established. It has comparable accuracy with all existing state-of-the-art models, while surpasses the inference speed of these methods by a large margin. The typical inference speed in this paper is astounding 4.44 video per second on THUMOS14. In applications, because there is no need to convert optical flow, the inference speed will be faster.It also proves that \emph{MLP} has great potential in downstream tasks such as TAD. The source code is available at \url{https://github.com/BonedDeng/TadML}



### PP-OCRv3: More Attempts for the Improvement of Ultra Lightweight OCR System
- **Arxiv ID**: http://arxiv.org/abs/2206.03001v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03001v2)
- **Published**: 2022-06-07 04:33:50+00:00
- **Updated**: 2022-06-14 12:00:10+00:00
- **Authors**: Chenxia Li, Weiwei Liu, Ruoyu Guo, Xiaoting Yin, Kaitao Jiang, Yongkun Du, Yuning Du, Lingfeng Zhu, Baohua Lai, Xiaoguang Hu, Dianhai Yu, Yanjun Ma
- **Comment**: arXiv admin note: text overlap with arXiv:2109.03144
- **Journal**: None
- **Summary**: Optical character recognition (OCR) technology has been widely used in various scenes, as shown in Figure 1. Designing a practical OCR system is still a meaningful but challenging task. In previous work, considering the efficiency and accuracy, we proposed a practical ultra lightweight OCR system (PP-OCR), and an optimized version PP-OCRv2. In order to further improve the performance of PP-OCRv2, a more robust OCR system PP-OCRv3 is proposed in this paper. PP-OCRv3 upgrades the text detection model and text recognition model in 9 aspects based on PP-OCRv2. For text detector, we introduce a PAN module with large receptive field named LK-PAN, a FPN module with residual attention mechanism named RSE-FPN, and DML distillation strategy. For text recognizer, the base model is replaced from CRNN to SVTR, and we introduce lightweight text recognition network SVTR LCNet, guided training of CTC by attention, data augmentation strategy TextConAug, better pre-trained model by self-supervised TextRotNet, UDML, and UIM to accelerate the model and improve the effect. Experiments on real data show that the hmean of PP-OCRv3 is 5% higher than PP-OCRv2 under comparable inference speed. All the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.



### Transformer-based Personalized Attention Mechanism for Medical Images with Clinical Records
- **Arxiv ID**: http://arxiv.org/abs/2206.03003v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.2.1; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2206.03003v2)
- **Published**: 2022-06-07 04:35:22+00:00
- **Updated**: 2023-01-27 09:54:07+00:00
- **Authors**: Yusuke Takagi, Noriaki Hashimoto, Hiroki Masuda, Hiroaki Miyoshi, Koichi Ohshima, Hidekata Hontani, Ichiro Takeuchi
- **Comment**: None
- **Journal**: Takagi, Yusuke, et al. "Transformer-based personalized attention
  mechanism for medical images with clinical records." Journal of Pathology
  Informatics (2023): 100185
- **Summary**: In medical image diagnosis, identifying the attention region, i.e., the region of interest for which the diagnosis is made, is an important task. Various methods have been developed to automatically identify target regions from given medical images. However, in actual medical practice, the diagnosis is made based not only on the images but also on a variety of clinical records. This means that pathologists examine medical images with some prior knowledge of the patients and that the attention regions may change depending on the clinical records. In this study, we propose a method called the Personalized Attention Mechanism (PersAM), by which the attention regions in medical images are adaptively changed according to the clinical records. The primary idea of the PersAM method is to encode the relationships between the medical images and clinical records using a variant of Transformer architecture. To demonstrate the effectiveness of the PersAM method, we applied it to a large-scale digital pathology problem of identifying the subtypes of 842 malignant lymphoma patients based on their gigapixel whole slide images and clinical records.



### Self-Knowledge Distillation based Self-Supervised Learning for Covid-19 Detection from Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2206.03009v1
- **DOI**: 10.1109/ICASSP43922.2022.9746540
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.03009v1)
- **Published**: 2022-06-07 04:55:44+00:00
- **Updated**: 2022-06-07 04:55:44+00:00
- **Authors**: Guang Li, Ren Togo, Takahiro Ogawa, Miki Haseyama
- **Comment**: Published as a conference paper at ICASSP 2022
- **Journal**: None
- **Summary**: The global outbreak of the Coronavirus 2019 (COVID-19) has overloaded worldwide healthcare systems. Computer-aided diagnosis for COVID-19 fast detection and patient triage is becoming critical. This paper proposes a novel self-knowledge distillation based self-supervised learning method for COVID-19 detection from chest X-ray images. Our method can use self-knowledge of images based on similarities of their visual features for self-supervised learning. Experimental results show that our method achieved an HM score of 0.988, an AUC of 0.999, and an accuracy of 0.957 on the largest open COVID-19 chest X-ray dataset.



### MS-RNN: A Flexible Multi-Scale Framework for Spatiotemporal Predictive Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.03010v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03010v5)
- **Published**: 2022-06-07 04:57:58+00:00
- **Updated**: 2023-04-28 02:44:47+00:00
- **Authors**: Zhifeng Ma, Hao Zhang, Jie Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Spatiotemporal predictive learning, which predicts future frames through historical prior knowledge with the aid of deep learning, is widely used in many fields. Previous work essentially improves the model performance by widening or deepening the network, but it also brings surging memory overhead, which seriously hinders the development and application of this technology. In order to improve the performance without increasing memory consumption, we focus on scale, which is another dimension to improve model performance but with low memory requirement. The effectiveness has been widely demonstrated in many CNN-based tasks such as image classification and semantic segmentation, but it has not been fully explored in recent RNN models. In this paper, learning from the benefit of multi-scale, we propose a general framework named Multi-Scale RNN (MS-RNN) to boost recent RNN models for spatiotemporal predictive learning. By integrating different scales, we enhance the existing models with both improved performance and greatly reduced overhead. We verify the MS-RNN framework by exhaustive experiments with eight popular RNN models (ConvLSTM, TrajGRU, PredRNN, PredRNN++, MIM, MotionRNN, PredRNN-V2, and PrecipLSTM) on four different datasets (Moving MNIST, TaxiBJ, KTH, and Germany). The results show the efficiency that the RNN models incorporating our framework have much lower memory cost but better performance than before. Our code is released at \url{https://github.com/mazhf/MS-RNN}.



### TriBYOL: Triplet BYOL for Self-Supervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.03012v1
- **DOI**: 10.1109/ICASSP43922.2022.9746967
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.03012v1)
- **Published**: 2022-06-07 05:03:05+00:00
- **Updated**: 2022-06-07 05:03:05+00:00
- **Authors**: Guang Li, Ren Togo, Takahiro Ogawa, Miki Haseyama
- **Comment**: Published as a conference paper at ICASSP 2022
- **Journal**: None
- **Summary**: This paper proposes a novel self-supervised learning method for learning better representations with small batch sizes. Many self-supervised learning methods based on certain forms of the siamese network have emerged and received significant attention. However, these methods need to use large batch sizes to learn good representations and require heavy computational resources. We present a new triplet network combined with a triple-view loss to improve the performance of self-supervised representation learning with small batch sizes. Experimental results show that our method can drastically outperform state-of-the-art self-supervised learning methods on several datasets in small-batch cases. Our method provides a feasible solution for self-supervised learning with real-world high-resolution images that uses small batch sizes.



### The Devil is in the Labels: Noisy Label Correction for Robust Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2206.03014v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03014v1)
- **Published**: 2022-06-07 05:03:57+00:00
- **Updated**: 2022-06-07 05:03:57+00:00
- **Authors**: Lin Li, Long Chen, Yifeng Huang, Zhimeng Zhang, Songyang Zhang, Jun Xiao
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Unbiased SGG has achieved significant progress over recent years. However, almost all existing SGG models have overlooked the ground-truth annotation qualities of prevailing SGG datasets, i.e., they always assume: 1) all the manually annotated positive samples are equally correct; 2) all the un-annotated negative samples are absolutely background. In this paper, we argue that both assumptions are inapplicable to SGG: there are numerous "noisy" groundtruth predicate labels that break these two assumptions, and these noisy samples actually harm the training of unbiased SGG models. To this end, we propose a novel model-agnostic NoIsy label CorrEction strategy for SGG: NICE. NICE can not only detect noisy samples but also reassign more high-quality predicate labels to them. After the NICE training, we can obtain a cleaner version of SGG dataset for model training. Specifically, NICE consists of three components: negative Noisy Sample Detection (Neg-NSD), positive NSD (Pos-NSD), and Noisy Sample Correction (NSC). Firstly, in Neg-NSD, we formulate this task as an out-of-distribution detection problem, and assign pseudo labels to all detected noisy negative samples. Then, in Pos-NSD, we use a clustering-based algorithm to divide all positive samples into multiple sets, and treat the samples in the noisiest set as noisy positive samples. Lastly, in NSC, we use a simple but effective weighted KNN to reassign new predicate labels to noisy positive samples. Extensive results on different backbones and tasks have attested to the effectiveness and generalization abilities of each component of NICE.



### Development of Automatic Endotracheal Tube and Carina Detection on Portable Supine Chest Radiographs using Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2206.03017v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.03017v1)
- **Published**: 2022-06-07 05:18:46+00:00
- **Updated**: 2022-06-07 05:18:46+00:00
- **Authors**: Chi-Yeh Chen, Min-Hsin Huang, Yung-Nien Sun, Chao-Han Lai
- **Comment**: None
- **Journal**: None
- **Summary**: The image quality of portable supine chest radiographs is inherently poor due to low contrast and high noise. The endotracheal intubation detection requires the locations of the endotracheal tube (ETT) tip and carina. The goal is to find the distance between the ETT tip and the carina in chest radiography. To overcome such a problem, we propose a feature extraction method with Mask R-CNN. The Mask R-CNN predicts a tube and a tracheal bifurcation in an image. Then, the feature extraction method is used to find the feature point of the ETT tip and that of the carina. Therefore, the ETT-carina distance can be obtained. In our experiments, our results can exceed 96\% in terms of recall and precision. Moreover, the object error is less than $4.7751\pm 5.3420$ mm, and the ETT-carina distance errors are less than $5.5432\pm 6.3100$ mm. The external validation shows that the proposed method is a high-robustness system. According to the Pearson correlation coefficient, we have a strong correlation between the board-certified intensivists and our result in terms of ETT-carina distance.



### Deep Learning Techniques for Visual Counting
- **Arxiv ID**: http://arxiv.org/abs/2206.03033v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03033v2)
- **Published**: 2022-06-07 06:20:40+00:00
- **Updated**: 2022-06-08 16:29:22+00:00
- **Authors**: Luca Ciampi
- **Comment**: Version with high-quality images can be found at
  https://etd.adm.unipi.it/theses/available/etd-04262022-163702/. arXiv admin
  note: text overlap with arXiv:1802.03601, arXiv:1707.01202, arXiv:1809.02165,
  arXiv:1901.06026, arXiv:1808.01244 by other authors
- **Journal**: None
- **Summary**: In this dissertation, we investigated and enhanced Deep Learning (DL) techniques for counting objects, like pedestrians, cells or vehicles, in still images or video frames. In particular, we tackled the challenge related to the lack of data needed for training current DL-based solutions. Given that the budget for labeling is limited, data scarcity still represents an open problem that prevents the scalability of existing solutions based on the supervised learning of neural networks and that is responsible for a significant drop in performance at inference time when new scenarios are presented to these algorithms. We introduced solutions addressing this issue from several complementary sides, collecting datasets gathered from virtual environments automatically labeled, proposing Domain Adaptation strategies aiming at mitigating the domain gap existing between the training and test data distributions, and presenting a counting strategy in a weakly labeled data scenario, i.e., in the presence of non-negligible disagreement between multiple annotators. Moreover, we tackled the non-trivial engineering challenges coming out of the adoption of Convolutional Neural Network-based techniques in environments with limited power resources, introducing solutions for counting vehicles and pedestrians directly onboard embedded vision systems, i.e., devices equipped with constrained computational capabilities that can capture images and elaborate them.



### COVIDx CT-3: A Large-scale, Multinational, Open-Source Benchmark Dataset for Computer-aided COVID-19 Screening from Chest CT Images
- **Arxiv ID**: http://arxiv.org/abs/2206.03043v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.03043v3)
- **Published**: 2022-06-07 06:35:48+00:00
- **Updated**: 2022-11-16 13:09:28+00:00
- **Authors**: Hayden Gunraj, Tia Tuinstra, Alexander Wong
- **Comment**: 6 pages, MED-NeurIPS 2022 workshop
- **Journal**: None
- **Summary**: Computed tomography (CT) has been widely explored as a COVID-19 screening and assessment tool to complement RT-PCR testing. To assist radiologists with CT-based COVID-19 screening, a number of computer-aided systems have been proposed. However, many proposed systems are built using CT data which is limited in both quantity and diversity. Motivated to support efforts in the development of machine learning-driven screening systems, we introduce COVIDx CT-3, a large-scale multinational benchmark dataset for detection of COVID-19 cases from chest CT images. COVIDx CT-3 includes 431,205 CT slices from 6,068 patients across at least 17 countries, which to the best of our knowledge represents the largest, most diverse dataset of COVID-19 CT images in open-access form. Additionally, we examine the data diversity and potential biases of the COVIDx CT-3 dataset, finding that significant geographic and class imbalances remain despite efforts to curate data from a wide variety of sources.



### Layered Depth Refinement with Mask Guidance
- **Arxiv ID**: http://arxiv.org/abs/2206.03048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03048v1)
- **Published**: 2022-06-07 06:42:44+00:00
- **Updated**: 2022-06-07 06:42:44+00:00
- **Authors**: Soo Ye Kim, Jianming Zhang, Simon Niklaus, Yifei Fan, Simon Chen, Zhe Lin, Munchurl Kim
- **Comment**: Accepted to CVPR 2022 (camera-ready version)
- **Journal**: None
- **Summary**: Depth maps are used in a wide range of applications from 3D rendering to 2D image effects such as Bokeh. However, those predicted by single image depth estimation (SIDE) models often fail to capture isolated holes in objects and/or have inaccurate boundary regions. Meanwhile, high-quality masks are much easier to obtain, using commercial auto-masking tools or off-the-shelf methods of segmentation and matting or even by manual editing. Hence, in this paper, we formulate a novel problem of mask-guided depth refinement that utilizes a generic mask to refine the depth prediction of SIDE models. Our framework performs layered refinement and inpainting/outpainting, decomposing the depth map into two separate layers signified by the mask and the inverse mask. As datasets with both depth and mask annotations are scarce, we propose a self-supervised learning scheme that uses arbitrary masks and RGB-D datasets. We empirically show that our method is robust to different types of masks and initial depth predictions, accurately refining depth values in inner and outer mask boundary regions. We further analyze our model with an ablation study and demonstrate results on real applications. More information can be found at https://sooyekim.github.io/MaskDepth/ .



### Siamese Encoder-based Spatial-Temporal Mixer for Growth Trend Prediction of Lung Nodules on CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2206.03049v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.03049v1)
- **Published**: 2022-06-07 06:44:56+00:00
- **Updated**: 2022-06-07 06:44:56+00:00
- **Authors**: Jiansheng Fang, Jingwen Wang, Anwei Li, Yuguang Yan, Yonghe Hou, Chao Song, Hongbo Liu, Jiang Liu
- **Comment**: MICCAI 2022
- **Journal**: None
- **Summary**: In the management of lung nodules, we are desirable to predict nodule evolution in terms of its diameter variation on Computed Tomography (CT) scans and then provide a follow-up recommendation according to the predicted result of the growing trend of the nodule. In order to improve the performance of growth trend prediction for lung nodules, it is vital to compare the changes of the same nodule in consecutive CT scans. Motivated by this, we screened out 4,666 subjects with more than two consecutive CT scans from the National Lung Screening Trial (NLST) dataset to organize a temporal dataset called NLSTt. In specific, we first detect and pair regions of interest (ROIs) covering the same nodule based on registered CT scans. After that, we predict the texture category and diameter size of the nodules through models. Last, we annotate the evolution class of each nodule according to its changes in diameter. Based on the built NLSTt dataset, we propose a siamese encoder to simultaneously exploit the discriminative features of 3D ROIs detected from consecutive CT scans. Then we novelly design a spatial-temporal mixer (STM) to leverage the interval changes of the same nodule in sequential 3D ROIs and capture spatial dependencies of nodule regions and the current 3D ROI. According to the clinical diagnosis routine, we employ hierarchical loss to pay more attention to growing nodules. The extensive experiments on our organized dataset demonstrate the advantage of our proposed method. We also conduct experiments on an in-house dataset to evaluate the clinical utility of our method by comparing it against skilled clinicians.



### Spatial Parsing and Dynamic Temporal Pooling networks for Human-Object Interaction detection
- **Arxiv ID**: http://arxiv.org/abs/2206.03061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03061v1)
- **Published**: 2022-06-07 07:26:06+00:00
- **Updated**: 2022-06-07 07:26:06+00:00
- **Authors**: Hongsheng Li, Guangming Zhu, Wu Zhen, Lan Ni, Peiyi Shen, Liang Zhang, Ning Wang, Cong Hua
- **Comment**: Accepted by IJCNN2022
- **Journal**: None
- **Summary**: The key of Human-Object Interaction(HOI) recognition is to infer the relationship between human and objects. Recently, the image's Human-Object Interaction(HOI) detection has made significant progress. However, there is still room for improvement in video HOI detection performance. Existing one-stage methods use well-designed end-to-end networks to detect a video segment and directly predict an interaction.   It makes the model learning and further optimization of the network more complex. This paper introduces the Spatial Parsing and Dynamic Temporal Pooling (SPDTP) network, which takes the entire video as a spatio-temporal graph with human and object nodes as input. Unlike existing methods, our proposed network predicts the difference between interactive and non-interactive pairs through explicit spatial parsing, and then performs interaction recognition. Moreover, we propose a learnable and differentiable Dynamic Temporal Module(DTM) to emphasize the keyframes of the video and suppress the redundant frame. Furthermore, the experimental results show that SPDTP can pay more attention to active human-object pairs and valid keyframes. Overall, we achieve state-of-the-art performance on CAD-120 dataset and Something-Else dataset.



### Object Scan Context: Object-centric Spatial Descriptor for Place Recognition within 3D Point Cloud Map
- **Arxiv ID**: http://arxiv.org/abs/2206.03062v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03062v2)
- **Published**: 2022-06-07 07:27:28+00:00
- **Updated**: 2022-09-17 03:21:44+00:00
- **Authors**: Haodong Yuan, Yudong Zhang, Shengyin Fan, Xue Li, Jian Wang
- **Comment**: 7 pages, 11 figures
- **Journal**: None
- **Summary**: Place recognition technology endows a SLAM algorithm with the ability to eliminate accumulated errors and to relocalize itself. Existing methods on point cloud-based place recognition often leverage the matching of global descriptors which are lidar-centric. These methods have the following two major defects: place recognition cannot be performed when the distance between the two point clouds is far, and only the rotation angle can be calculated without the offset in the X and Y direction. To solve these two problems, we propose a novel global descriptor, which is built around the Main Object, in this way, descriptors are no longer dependent on the observation position. We analyze the theory that this method can solve the above two problems, and conduct a lot of experiments on KITTI Odometry and KITTI360, which show that our method has obvious advantages over state-of-the-art methods.



### A Simple and Efficient Pipeline to Build an End-to-End Spatial-Temporal Action Detector
- **Arxiv ID**: http://arxiv.org/abs/2206.03064v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03064v2)
- **Published**: 2022-06-07 07:31:56+00:00
- **Updated**: 2022-10-27 13:11:48+00:00
- **Authors**: Lin Sui, Chen-Lin Zhang, Lixin Gu, Feng Han
- **Comment**: Accepted By WACV 2023
- **Journal**: None
- **Summary**: Spatial-temporal action detection is a vital part of video understanding. Current spatial-temporal action detection methods mostly use an object detector to obtain person candidates and classify these person candidates into different action categories. So-called two-stage methods are heavy and hard to apply in real-world applications. Some existing methods build one-stage pipelines, But a large performance drop exists with the vanilla one-stage pipeline and extra classification modules are needed to achieve comparable performance. In this paper, we explore a simple and effective pipeline to build a strong one-stage spatial-temporal action detector. The pipeline is composed by two parts: one is a simple end-to-end spatial-temporal action detector. The proposed end-to-end detector has minor architecture changes to current proposal-based detectors and does not add extra action classification modules. The other part is a novel labeling strategy to utilize unlabeled frames in sparse annotated data. We named our model as SE-STAD. The proposed SE-STAD achieves around 2% mAP boost and around 80% FLOPs reduction. Our code will be released at https://github.com/4paradigm-CV/SE-STAD.



### Recent Advances for Quantum Neural Networks in Generative Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.03066v1
- **DOI**: None
- **Categories**: **quant-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.03066v1)
- **Published**: 2022-06-07 07:32:57+00:00
- **Updated**: 2022-06-07 07:32:57+00:00
- **Authors**: Jinkai Tian, Xiaoyu Sun, Yuxuan Du, Shanshan Zhao, Qing Liu, Kaining Zhang, Wei Yi, Wanrong Huang, Chaoyue Wang, Xingyao Wu, Min-Hsiu Hsieh, Tongliang Liu, Wenjing Yang, Dacheng Tao
- **Comment**: The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: Quantum computers are next-generation devices that hold promise to perform calculations beyond the reach of classical computers. A leading method towards achieving this goal is through quantum machine learning, especially quantum generative learning. Due to the intrinsic probabilistic nature of quantum mechanics, it is reasonable to postulate that quantum generative learning models (QGLMs) may surpass their classical counterparts. As such, QGLMs are receiving growing attention from the quantum physics and computer science communities, where various QGLMs that can be efficiently implemented on near-term quantum machines with potential computational advantages are proposed. In this paper, we review the current progress of QGLMs from the perspective of machine learning. Particularly, we interpret these QGLMs, covering quantum circuit born machines, quantum generative adversarial networks, quantum Boltzmann machines, and quantum autoencoders, as the quantum extension of classical generative learning models. In this context, we explore their intrinsic relation and their fundamental differences. We further summarize the potential applications of QGLMs in both conventional machine learning tasks and quantum physics. Last, we discuss the challenges and further research directions for QGLMs.



### EiX-GNN : Concept-level eigencentrality explainer for graph neural networks
- **Arxiv ID**: http://arxiv.org/abs/2206.03491v6
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.03491v6)
- **Published**: 2022-06-07 07:45:45+00:00
- **Updated**: 2023-03-10 10:34:33+00:00
- **Authors**: Adrien Raison, Pascal Bourdon, David Helbert
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, deep prediction models, especially graph neural networks, have a majorplace in critical applications. In such context, those models need to be highlyinterpretable or being explainable by humans, and at the societal scope, this understandingmay also be feasible for humans that do not have a strong prior knowledgein models and contexts that need to be explained. In the literature, explainingis a human knowledge transfer process regarding a phenomenon between an explainerand an explainee. We propose EiX-GNN (Eigencentrality eXplainer forGraph Neural Networks) a new powerful method for explaining graph neural networksthat encodes computationally this social explainer-to-explainee dependenceunderlying in the explanation process. To handle this dependency, we introducethe notion of explainee concept assimibility which allows explainer to adapt itsexplanation to explainee background or expectation. We lead a qualitative studyto illustrate our explainee concept assimibility notion on real-world data as wellas a qualitative study that compares, according to objective metrics established inthe literature, fairness and compactness of our method with respect to performingstate-of-the-art methods. It turns out that our method achieves strong results inboth aspects.



### Pushing the Limits of Learning-based Traversability Analysis for Autonomous Driving on CPU
- **Arxiv ID**: http://arxiv.org/abs/2206.03083v1
- **DOI**: 10.1007/978-3-031-22216-0_36
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.03083v1)
- **Published**: 2022-06-07 07:57:34+00:00
- **Updated**: 2022-06-07 07:57:34+00:00
- **Authors**: Daniel Fusaro, Emilio Olivastri, Daniele Evangelista, Marco Imperoli, Emanuele Menegatti, Alberto Pretto
- **Comment**: Accepted to 17th International Conference on Intelligent Autonomous
  Systems (IAS-17)
- **Journal**: Proceedings of the 17th International Conference on Intelligent
  Autonomous Systems (IAS 2022)
- **Summary**: Self-driving vehicles and autonomous ground robots require a reliable and accurate method to analyze the traversability of the surrounding environment for safe navigation. This paper proposes and evaluates a real-time machine learning-based Traversability Analysis method that combines geometric features with appearance-based features in a hybrid approach based on a SVM classifier. In particular, we show that integrating a new set of geometric and visual features and focusing on important implementation details enables a noticeable boost in performance and reliability. The proposed approach has been compared with state-of-the-art Deep Learning approaches on a public dataset of outdoor driving scenarios. It reaches an accuracy of 89.2% in scenarios of varying complexity, demonstrating its effectiveness and robustness. The method runs fully on CPU and reaches comparable results with respect to the other methods, operates faster, and requires fewer hardware resources.



### Online Deep Clustering with Video Track Consistency
- **Arxiv ID**: http://arxiv.org/abs/2206.03086v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03086v1)
- **Published**: 2022-06-07 08:11:00+00:00
- **Updated**: 2022-06-07 08:11:00+00:00
- **Authors**: Alessandra Alfani, Federico Becattini, Lorenzo Seidenari, Alberto Del Bimbo
- **Comment**: Accepted at ICPR2022 as oral
- **Journal**: None
- **Summary**: Several unsupervised and self-supervised approaches have been developed in recent years to learn visual features from large-scale unlabeled datasets. Their main drawback however is that these methods are hardly able to recognize visual features of the same object if it is simply rotated or the perspective of the camera changes. To overcome this limitation and at the same time exploit a useful source of supervision, we take into account video object tracks. Following the intuition that two patches in a track should have similar visual representations in a learned feature space, we adopt an unsupervised clustering-based approach and constrain such representations to be labeled as the same category since they likely belong to the same object or object part. Experimental results on two downstream tasks on different datasets demonstrate the effectiveness of our Online Deep Clustering with Video Track Consistency (ODCT) approach compared to prior work, which did not leverage temporal information. In addition we show that exploiting an unsupervised class-agnostic, yet noisy, track generator yields to better accuracy compared to relying on costly and precise track annotations.



### Critical Regularizations for Neural Surface Reconstruction in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2206.03087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03087v1)
- **Published**: 2022-06-07 08:11:22+00:00
- **Updated**: 2022-06-07 08:11:22+00:00
- **Authors**: Jingyang Zhang, Yao Yao, Shiwei Li, Tian Fang, David McKinnon, Yanghai Tsin, Long Quan
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Neural implicit functions have recently shown promising results on surface reconstructions from multiple views. However, current methods still suffer from excessive time complexity and poor robustness when reconstructing unbounded or complex scenes. In this paper, we present RegSDF, which shows that proper point cloud supervisions and geometry regularizations are sufficient to produce high-quality and robust reconstruction results. Specifically, RegSDF takes an additional oriented point cloud as input, and optimizes a signed distance field and a surface light field within a differentiable rendering framework. We also introduce the two critical regularizations for this optimization. The first one is the Hessian regularization that smoothly diffuses the signed distance values to the entire distance field given noisy and incomplete input. And the second one is the minimal surface regularization that compactly interpolates and extrapolates the missing geometry. Extensive experiments are conducted on DTU, BlendedMVS, and Tanks and Temples datasets. Compared with recent neural surface reconstruction approaches, RegSDF is able to reconstruct surfaces with fine details even for open scenes with complex topologies and unstructured camera trajectories.



### Dual Swin-Transformer based Mutual Interactive Network for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.03105v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03105v1)
- **Published**: 2022-06-07 08:35:41+00:00
- **Updated**: 2022-06-07 08:35:41+00:00
- **Authors**: Chao Zeng, Sam Kwong
- **Comment**: None
- **Journal**: None
- **Summary**: Salient Object Detection is the task of predicting the human attended region in a given scene. Fusing depth information has been proven effective in this task. The main challenge of this problem is how to aggregate the complementary information from RGB modality and depth modality. However, conventional deep models heavily rely on CNN feature extractors, and the long-range contextual dependencies are usually ignored. In this work, we propose Dual Swin-Transformer based Mutual Interactive Network. We adopt Swin-Transformer as the feature extractor for both RGB and depth modality to model the long-range dependencies in visual inputs. Before fusing the two branches of features into one, attention-based modules are applied to enhance features from each modality. We design a self-attention-based cross-modality interaction module and a gated modality attention module to leverage the complementary information between the two modalities. For the saliency decoding, we create different stages enhanced with dense connections and keep a decoding memory while the multi-level encoding features are considered simultaneously. Considering the inaccurate depth map issue, we collect the RGB features of early stages into a skip convolution module to give more guidance from RGB modality to the final saliency prediction. In addition, we add edge supervision to regularize the feature learning process. Comprehensive experiments on five standard RGB-D SOD benchmark datasets over four evaluation metrics demonstrate the superiority of the proposed DTMINet method.



### Medical Image Registration via Neural Fields
- **Arxiv ID**: http://arxiv.org/abs/2206.03111v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03111v2)
- **Published**: 2022-06-07 08:43:31+00:00
- **Updated**: 2022-08-22 20:31:41+00:00
- **Authors**: Shanlin Sun, Kun Han, Hao Tang, Deying Kong, Junayed Naushad, Xiangyi Yan, Xiaohui Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Image registration is an essential step in many medical image analysis tasks. Traditional methods for image registration are primarily optimization-driven, finding the optimal deformations that maximize the similarity between two images. Recent learning-based methods, trained to directly predict transformations between two images, run much faster, but suffer from performance deficiencies due to model generalization and the inefficiency in handling individual image specific deformations. Here we present a new neural net based image registration framework, called NIR (Neural Image Registration), which is based on optimization but utilizes deep neural nets to model deformations between image pairs. NIR represents the transformation between two images with a continuous function implemented via neural fields, receiving a 3D coordinate as input and outputting the corresponding deformation vector. NIR provides two ways of generating deformation field: directly output a displacement vector field for general deformable registration, or output a velocity vector field and integrate the velocity field to derive the deformation field for diffeomorphic image registration. The optimal registration is discovered by updating the parameters of the neural field via stochastic gradient descent. We describe several design choices that facilitate model optimization, including coordinate encoding, sinusoidal activation, coordinate sampling, and intensity sampling. Experiments on two 3D MR brain scan datasets demonstrate that NIR yields state-of-the-art performance in terms of both registration accuracy and regularity, while running significantly faster than traditional optimization-based methods.



### Wavelet Prior Attention Learning in Axial Inpainting Network
- **Arxiv ID**: http://arxiv.org/abs/2206.03113v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03113v2)
- **Published**: 2022-06-07 08:45:27+00:00
- **Updated**: 2022-06-14 06:54:56+00:00
- **Authors**: Chenjie Cao, Chengrong Wang, Yuntao Zhang, Yanwei Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Image inpainting is the task of filling masked or unknown regions of an image with visually realistic contents, which has been remarkably improved by Deep Neural Networks (DNNs) recently. Essentially, as an inverse problem, the inpainting has the underlying challenges of reconstructing semantically coherent results without texture artifacts. Many previous efforts have been made via exploiting attention mechanisms and prior knowledge, such as edges and semantic segmentation. However, these works are still limited in practice by an avalanche of learnable prior parameters and prohibitive computational burden. To this end, we propose a novel model -- Wavelet prior attention learning in Axial Inpainting Network (WAIN), whose generator contains the encoder, decoder, as well as two key components of Wavelet image Prior Attention (WPA) and stacked multi-layer Axial-Transformers (ATs). Particularly, the WPA guides the high-level feature aggregation in the multi-scale frequency domain, alleviating the textual artifacts. Stacked ATs employ unmasked clues to help model reasonable features along with low-level features of horizontal and vertical axes, improving the semantic coherence. Extensive quantitative and qualitative experiments on Celeba-HQ and Places2 datasets are conducted to validate that our WAIN can achieve state-of-the-art performance over the competitors. The codes and models will be released.



### Self-Training of Handwritten Word Recognition for Synthetic-to-Real Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2206.03149v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03149v2)
- **Published**: 2022-06-07 09:43:25+00:00
- **Updated**: 2022-06-30 10:25:41+00:00
- **Authors**: Fabian Wolf, Gernot A. Fink
- **Comment**: Accepted for publication in International Conference on Pattern
  Recognition (ICPR) 2022
- **Journal**: None
- **Summary**: Performances of Handwritten Text Recognition (HTR) models are largely determined by the availability of labeled and representative training samples. However, in many application scenarios labeled samples are scarce or costly to obtain. In this work, we propose a self-training approach to train a HTR model solely on synthetic samples and unlabeled data. The proposed training scheme uses an initial model trained on synthetic data to make predictions for the unlabeled target dataset. Starting from this initial model with rather poor performance, we show that a considerable adaptation is possible by training against the predicted pseudo-labels. Moreover, the investigated self-training strategy does not require any manually annotated training samples. We evaluate the proposed method on four widely used benchmark datasets and show its effectiveness on closing the gap to a model trained in a fully-supervised manner.



### Utility of Equivariant Message Passing in Cortical Mesh Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.03164v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.03164v2)
- **Published**: 2022-06-07 10:24:18+00:00
- **Updated**: 2022-06-15 11:29:39+00:00
- **Authors**: Dániel Unyi, Ferdinando Insalata, Petar Veličković, Bálint Gyires-Tóth
- **Comment**: 13 pages, 3 figures, accepted for MIUA 2022
- **Journal**: None
- **Summary**: The automated segmentation of cortical areas has been a long-standing challenge in medical image analysis. The complex geometry of the cortex is commonly represented as a polygon mesh, whose segmentation can be addressed by graph-based learning methods. When cortical meshes are misaligned across subjects, current methods produce significantly worse segmentation results, limiting their ability to handle multi-domain data. In this paper, we investigate the utility of E(n)-equivariant graph neural networks (EGNNs), comparing their performance against plain graph neural networks (GNNs). Our evaluation shows that GNNs outperform EGNNs on aligned meshes, due to their ability to leverage the presence of a global coordinate system. On misaligned meshes, the performance of plain GNNs drop considerably, while E(n)-equivariant message passing maintains the same segmentation results. The best results can also be obtained by using plain GNNs on realigned data (co-registered meshes in a global coordinate system).



### Improving Image Captioning with Control Signal of Sentence Quality
- **Arxiv ID**: http://arxiv.org/abs/2206.03196v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03196v2)
- **Published**: 2022-06-07 11:38:03+00:00
- **Updated**: 2023-03-09 02:46:41+00:00
- **Authors**: Zhangzi Zhu, Hong Qu
- **Comment**: Accepted by ICASSP2023
- **Journal**: None
- **Summary**: In the dataset of image captioning, each image is aligned with several descriptions. Despite the fact that the quality of these descriptions varies, existing captioning models treat them equally in the training process. In this paper, we propose a new control signal of sentence quality, which is taken as an additional input to the captioning model. By integrating the control signal information, captioning models are aware of the quality level of the target sentences and handle them differently. Moreover, we propose a novel reinforcement training method specially designed for the control signal of sentence quality: Quality-oriented Self-Annotated Training (Q-SAT). Extensive experiments on MSCOCO dataset show that without extra information from ground truth captions, models controlled by the highest quality level outperform baseline models on accuracy-based evaluation metrics, which validates the effectiveness of our proposed methods.



### Omnivision forecasting: combining satellite observations with sky images for improved intra-hour solar energy predictions
- **Arxiv ID**: http://arxiv.org/abs/2206.03207v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T45, I.4.8; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2206.03207v1)
- **Published**: 2022-06-07 11:52:09+00:00
- **Updated**: 2022-06-07 11:52:09+00:00
- **Authors**: Quentin Paletta, Guillaume Arbod, Joan Lasenby
- **Comment**: Submitted to Renewable Energy
- **Journal**: None
- **Summary**: Integration of intermittent renewable energy sources into electric grids in large proportions is challenging. A well-established approach aimed at addressing this difficulty involves the anticipation of the upcoming energy supply variability to adapt the response of the grid. In solar energy, short-term changes in electricity production caused by occluding clouds can be predicted at different time scales from all-sky cameras (up to 30-min ahead) and satellite observations (up to 6h ahead). In this study, we integrate these two complementary points of view on the cloud cover in a single machine learning framework to improve intra-hour (up to 60-min ahead) irradiance forecasting. Both deterministic and probabilistic predictions are evaluated in different weather conditions (clear-sky, cloudy, overcast) and with different input configurations (sky images, satellite observations and/or past irradiance values). Our results show that the hybrid model benefits predictions in clear-sky conditions and improves longer-term forecasting. This study lays the groundwork for future novel approaches of combining sky images and satellite observations in a single learning framework to advance solar nowcasting.



### Deep Neural Patchworks: Coping with Large Segmentation Tasks
- **Arxiv ID**: http://arxiv.org/abs/2206.03210v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.03210v1)
- **Published**: 2022-06-07 12:07:18+00:00
- **Updated**: 2022-06-07 12:07:18+00:00
- **Authors**: Marco Reisert, Maximilian Russe, Samer Elsheikh, Elias Kellner, Henrik Skibbe
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks are the way to solve arbitrary image segmentation tasks. However, when images are large, memory demands often exceed the available resources, in particular on a common GPU. Especially in biomedical imaging, where 3D images are common, the problems are apparent. A typical approach to solve this limitation is to break the task into smaller subtasks by dividing images into smaller image patches. Another approach, if applicable, is to look at the 2D image sections separately, and to solve the problem in 2D. Often, the loss of global context makes such approaches less effective; important global information might not be present in the current image patch, or the selected 2D image section. Here, we propose Deep Neural Patchworks (DNP), a segmentation framework that is based on hierarchical and nested stacking of patch-based networks that solves the dilemma between global context and memory limitations.



### Rotation-Equivariant Conditional Spherical Neural Fields for Learning a Natural Illumination Prior
- **Arxiv ID**: http://arxiv.org/abs/2206.03858v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03858v3)
- **Published**: 2022-06-07 13:02:49+00:00
- **Updated**: 2022-10-14 09:33:49+00:00
- **Authors**: James A. D. Gardner, Bernhard Egger, William A. P. Smith
- **Comment**: NeurIPS 2022 - Project Website: jadgardner.github.io/RENI
- **Journal**: None
- **Summary**: Inverse rendering is an ill-posed problem. Previous work has sought to resolve this by focussing on priors for object or scene shape or appearance. In this work, we instead focus on a prior for natural illuminations. Current methods rely on spherical harmonic lighting or other generic representations and, at best, a simplistic prior on the parameters. We propose a conditional neural field representation based on a variational auto-decoder with a SIREN network and, extending Vector Neurons, build equivariance directly into the network. Using this, we develop a rotation-equivariant, high dynamic range (HDR) neural illumination model that is compact and able to express complex, high-frequency features of natural environment maps. Training our model on a curated dataset of 1.6K HDR environment maps of natural scenes, we compare it against traditional representations, demonstrate its applicability for an inverse rendering task and show environment map completion from partial observations. A PyTorch implementation, our dataset and trained models can be found at jadgardner.github.io/RENI.



### Towards better Interpretable and Generalizable AD detection using Collective Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2206.03247v2
- **DOI**: 10.1016/j.compmedimag.2022.102171
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.03247v2)
- **Published**: 2022-06-07 13:02:53+00:00
- **Updated**: 2023-01-04 10:46:57+00:00
- **Authors**: Huy-Dung Nguyen, Michaël Clément, Boris Mansencal, Pierrick Coupé
- **Comment**: None
- **Journal**: None
- **Summary**: Alzheimer's Disease is the most common cause of dementia. Accurate diagnosis and prognosis of this disease are essential to design an appropriate treatment plan, increasing the life expectancy of the patient. Intense research has been conducted on the use of machine learning to identify Alzheimer's Disease from neuroimaging data, such as structural magnetic resonance imaging. In recent years, advances of deep learning in computer vision suggest a new research direction for this problem. Current deep learning-based approaches in this field, however, have a number of drawbacks, including the interpretability of model decisions, a lack of generalizability information and a lower performance compared to traditional machine learning techniques. In this paper, we design a two-stage framework to overcome these limitations. In the first stage, an ensemble of 125 U-Nets is used to grade the input image, producing a 3D map that reflects the disease severity at voxel-level. This map can help to localize abnormal brain areas caused by the disease. In the second stage, we model a graph per individual using the generated grading map and other information about the subject. We propose to use a graph convolutional neural network classifier for the final classification. As a result, our framework demonstrates comparative performance to the state-of-the-art methods in different datasets for both diagnosis and prognosis. We also demonstrate that the use of a large ensemble of U-Nets offers a better generalization capacity for our framework.



### On the Effectiveness of Fine-tuning Versus Meta-reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.03271v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.03271v2)
- **Published**: 2022-06-07 13:24:00+00:00
- **Updated**: 2023-02-16 16:54:58+00:00
- **Authors**: Zhao Mandi, Pieter Abbeel, Stephen James
- **Comment**: None
- **Journal**: None
- **Summary**: Intelligent agents should have the ability to leverage knowledge from previously learned tasks in order to learn new ones quickly and efficiently. Meta-learning approaches have emerged as a popular solution to achieve this. However, meta-reinforcement learning (meta-RL) algorithms have thus far been restricted to simple environments with narrow task distributions. Moreover, the paradigm of pretraining followed by fine-tuning to adapt to new tasks has emerged as a simple yet effective solution in supervised and self-supervised learning. This calls into question the benefits of meta-learning approaches also in reinforcement learning, which typically come at the cost of high complexity. We hence investigate meta-RL approaches in a variety of vision-based benchmarks, including Procgen, RLBench, and Atari, where evaluations are made on completely novel tasks. Our findings show that when meta-learning approaches are evaluated on different tasks (rather than different variations of the same task), multi-task pretraining with fine-tuning on new tasks performs equally as well, or better, than meta-pretraining with meta test-time adaptation. This is encouraging for future research, as multi-task pretraining tends to be simpler and computationally cheaper than meta-RL. From these findings, we advocate for evaluating future meta-RL methods on more challenging tasks and including multi-task pretraining with fine-tuning as a simple, yet strong baseline.



### Parotid Gland MRI Segmentation Based on Swin-Unet and Multimodal Images
- **Arxiv ID**: http://arxiv.org/abs/2206.03336v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.03336v2)
- **Published**: 2022-06-07 14:20:53+00:00
- **Updated**: 2022-12-26 13:56:21+00:00
- **Authors**: Zi'an Xu, Yin Dai, Fayu Liu, Siqi Li, Sheng Liu, Lifu Shi, Jun Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Background and objective: Parotid gland tumors account for approximately 2% to 10% of head and neck tumors. Preoperative tumor localization, differential diagnosis, and subsequent selection of appropriate treatment for parotid gland tumors are critical. However, the relative rarity of these tumors and the highly dispersed tissue types have left an unmet need for a subtle differential diagnosis of such neoplastic lesions based on preoperative radiomics. Recently, deep learning methods have developed rapidly, especially Transformer beats the traditional convolutional neural network in computer vision. Many new Transformer-based networks have been proposed for computer vision tasks. Methods: In this study, multicenter multimodal parotid gland MR images were collected. The Swin-Unet which was based on Transformer was used. MR images of short time inversion recovery, T1-weighted and T2-weighted modalities were combined into three-channel data to train the network. We achieved segmentation of the region of interest for parotid gland and tumor. Results: The Dice-Similarity Coefficient of the model on the test set was 88.63%, Mean Pixel Accuracy was 99.31%, Mean Intersection over Union was 83.99%, and Hausdorff Distance was 3.04. Then a series of comparison experiments were designed in this paper to further validate the segmentation performance of the algorithm. Conclusions: Experimental results showed that our method has good results for parotid gland and tumor segmentation. The Transformer-based network outperforms the traditional convolutional neural network in the field of medical images.



### cViL: Cross-Lingual Training of Vision-Language Models using Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2206.03354v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.03354v2)
- **Published**: 2022-06-07 14:46:30+00:00
- **Updated**: 2022-06-09 05:40:02+00:00
- **Authors**: Kshitij Gupta, Devansh Gautam, Radhika Mamidi
- **Comment**: Accepted at ICPR 2022; 9 pages
- **Journal**: None
- **Summary**: Vision-and-language tasks are gaining popularity in the research community, but the focus is still mainly on English. We propose a pipeline that utilizes English-only vision-language models to train a monolingual model for a target language. We propose to extend OSCAR+, a model which leverages object tags as anchor points for learning image-text alignments, to train on visual question answering datasets in different languages. We propose a novel approach to knowledge distillation to train the model in other languages using parallel sentences. Compared to other models that use the target language in the pretraining corpora, we can leverage an existing English model to transfer the knowledge to the target language using significantly lesser resources. We also release a large-scale visual question answering dataset in Japanese and Hindi language. Though we restrict our work to visual question answering, our model can be extended to any sequence-level classification task, and it can be extended to other languages as well. This paper focuses on two languages for the visual question answering task - Japanese and Hindi. Our pipeline outperforms the current state-of-the-art models by a relative increase of 4.4% and 13.4% respectively in accuracy.



### An efficient semi-supervised quality control system trained using physics-based MRI-artefact generators and adversarial training
- **Arxiv ID**: http://arxiv.org/abs/2206.03359v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.03359v1)
- **Published**: 2022-06-07 14:53:35+00:00
- **Updated**: 2022-06-07 14:53:35+00:00
- **Authors**: Daniele Ravi, Frederik Barkhof, Daniel C. Alexander, Geoffrey JM Parker, Arman Eshaghi
- **Comment**: None
- **Journal**: None
- **Summary**: Large medical imaging data sets are becoming increasingly available. A common challenge in these data sets is to ensure that each sample meets minimum quality requirements devoid of significant artefacts. Despite a wide range of existing automatic methods having been developed to identify imperfections and artefacts in medical imaging, they mostly rely on data-hungry methods. In particular, the lack of sufficient scans with artefacts available for training has created a barrier in designing and deploying machine learning in clinical research. To tackle this problem, we propose a novel framework having four main components: (1) a set of artefact generators inspired by magnetic resonance physics to corrupt brain MRI scans and augment a training dataset, (2) a set of abstract and engineered features to represent images compactly, (3) a feature selection process that depends on the class of artefact to improve classification performance, and (4) a set of Support Vector Machine (SVM) classifiers trained to identify artefacts. Our novel contributions are threefold: first, we use the novel physics-based artefact generators to generate synthetic brain MRI scans with controlled artefacts as a data augmentation technique. This will avoid the labour-intensive collection and labelling process of scans with rare artefacts. Second, we propose a large pool of abstract and engineered image features developed to identify 9 different artefacts for structural MRI. Finally, we use an artefact-based feature selection block that, for each class of artefacts, finds the set of features that provide the best classification performance. We performed validation experiments on a large data set of scans with artificially-generated artefacts, and in a multiple sclerosis clinical trial where real artefacts were identified by experts, showing that the proposed pipeline outperforms traditional methods.



### Hierarchical Similarity Learning for Aliasing Suppression Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2206.03361v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.03361v1)
- **Published**: 2022-06-07 14:55:32+00:00
- **Updated**: 2022-06-07 14:55:32+00:00
- **Authors**: Yuqing Liu, Qi Jia, Jian Zhang, Xin Fan, Shanshe Wang, Siwei Ma, Wen Gao
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: As a highly ill-posed issue, single image super-resolution (SISR) has been widely investigated in recent years. The main task of SISR is to recover the information loss caused by the degradation procedure. According to the Nyquist sampling theory, the degradation leads to aliasing effect and makes it hard to restore the correct textures from low-resolution (LR) images. In practice, there are correlations and self-similarities among the adjacent patches in the natural images. This paper considers the self-similarity and proposes a hierarchical image super-resolution network (HSRNet) to suppress the influence of aliasing. We consider the SISR issue in the optimization perspective, and propose an iterative solution pattern based on the half-quadratic splitting (HQS) method. To explore the texture with local image prior, we design a hierarchical exploration block (HEB) and progressive increase the receptive field. Furthermore, multi-level spatial attention (MSA) is devised to obtain the relations of adjacent feature and enhance the high-frequency information, which acts as a crucial role for visual experience. Experimental result shows HSRNet achieves better quantitative and visual performance than other works, and remits the aliasing more effectively.



### Localizing Semantic Patches for Accelerating Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.03367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03367v1)
- **Published**: 2022-06-07 15:01:54+00:00
- **Updated**: 2022-06-07 15:01:54+00:00
- **Authors**: Chuanguang Yang, Zhulin An, Yongjun Xu
- **Comment**: Accepted by ICME-2022
- **Journal**: None
- **Summary**: Existing works often focus on reducing the architecture redundancy for accelerating image classification but ignore the spatial redundancy of the input image. This paper proposes an efficient image classification pipeline to solve this problem. We first pinpoint task-aware regions over the input image by a lightweight patch proposal network called AnchorNet. We then feed these localized semantic patches with much smaller spatial redundancy into a general classification network. Unlike the popular design of deep CNN, we aim to carefully design the Receptive Field of AnchorNet without intermediate convolutional paddings. This ensures the exact mapping from a high-level spatial location to the specific input image patch. The contribution of each patch is interpretable. Moreover, AnchorNet is compatible with any downstream architecture. Experimental results on ImageNet show that our method outperforms SOTA dynamic inference methods with fewer inference costs. Our code is available at https://github.com/winycg/AnchorNet.



### IL-MCAM: An interactive learning and multi-channel attention mechanism-based weakly supervised colorectal histopathology image classification approach
- **Arxiv ID**: http://arxiv.org/abs/2206.03368v1
- **DOI**: 10.1016/j.compbiomed.2022.105265
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03368v1)
- **Published**: 2022-06-07 15:03:05+00:00
- **Updated**: 2022-06-07 15:03:05+00:00
- **Authors**: Haoyuan Chen, Chen Li, Xiaoyan Li, Md Mamunur Rahaman, Weiming Hu, Yixin Li, Wanli Liu, Changhao Sun, Hongzan Sun, Xinyu Huang, Marcin Grzegorzek
- **Comment**: None
- **Journal**: Computers in Biology and Medicine, Volume 143, April 2022, 105265
- **Summary**: In recent years, colorectal cancer has become one of the most significant diseases that endanger human health. Deep learning methods are increasingly important for the classification of colorectal histopathology images. However, existing approaches focus more on end-to-end automatic classification using computers rather than human-computer interaction. In this paper, we propose an IL-MCAM framework. It is based on attention mechanisms and interactive learning. The proposed IL-MCAM framework includes two stages: automatic learning (AL) and interactivity learning (IL). In the AL stage, a multi-channel attention mechanism model containing three different attention mechanism channels and convolutional neural networks is used to extract multi-channel features for classification. In the IL stage, the proposed IL-MCAM framework continuously adds misclassified images to the training set in an interactive approach, which improves the classification ability of the MCAM model. We carried out a comparison experiment on our dataset and an extended experiment on the HE-NCT-CRC-100K dataset to verify the performance of the proposed IL-MCAM framework, achieving classification accuracies of 98.98% and 99.77%, respectively. In addition, we conducted an ablation experiment and an interchangeability experiment to verify the ability and interchangeability of the three channels. The experimental results show that the proposed IL-MCAM framework has excellent performance in the colorectal histopathological image classification tasks.



### Garment Avatars: Realistic Cloth Driving using Pattern Registration
- **Arxiv ID**: http://arxiv.org/abs/2206.03373v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03373v1)
- **Published**: 2022-06-07 15:06:55+00:00
- **Updated**: 2022-06-07 15:06:55+00:00
- **Authors**: Oshri Halimi, Fabian Prada, Tuur Stuyck, Donglai Xiang, Timur Bagautdinov, He Wen, Ron Kimmel, Takaaki Shiratori, Chenglei Wu, Yaser Sheikh
- **Comment**: None
- **Journal**: None
- **Summary**: Virtual telepresence is the future of online communication. Clothing is an essential part of a person's identity and self-expression. Yet, ground truth data of registered clothes is currently unavailable in the required resolution and accuracy for training telepresence models for realistic cloth animation. Here, we propose an end-to-end pipeline for building drivable representations for clothing. The core of our approach is a multi-view patterned cloth tracking algorithm capable of capturing deformations with high accuracy. We further rely on the high-quality data produced by our tracking method to build a Garment Avatar: an expressive and fully-drivable geometry model for a piece of clothing. The resulting model can be animated using a sparse set of views and produces highly realistic reconstructions which are faithful to the driving signals. We demonstrate the efficacy of our pipeline on a realistic virtual telepresence application, where a garment is being reconstructed from two views, and a user can pick and swap garment design as they wish. In addition, we show a challenging scenario when driven exclusively with body pose, our drivable garment avatar is capable of producing realistic cloth geometry of significantly higher quality than the state-of-the-art.



### Shape, Light, and Material Decomposition from Images using Monte Carlo Rendering and Denoising
- **Arxiv ID**: http://arxiv.org/abs/2206.03380v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.03380v2)
- **Published**: 2022-06-07 15:19:18+00:00
- **Updated**: 2022-10-04 09:15:36+00:00
- **Authors**: Jon Hasselgren, Nikolai Hofmann, Jacob Munkberg
- **Comment**: Project website: https://nvlabs.github.io/nvdiffrecmc/
- **Journal**: None
- **Summary**: Recent advances in differentiable rendering have enabled high-quality reconstruction of 3D scenes from multi-view images. Most methods rely on simple rendering algorithms: pre-filtered direct lighting or learned representations of irradiance. We show that a more realistic shading model, incorporating ray tracing and Monte Carlo integration, substantially improves decomposition into shape, materials & lighting. Unfortunately, Monte Carlo integration provides estimates with significant noise, even at large sample counts, which makes gradient-based inverse rendering very challenging. To address this, we incorporate multiple importance sampling and denoising in a novel inverse rendering pipeline. This substantially improves convergence and enables gradient-based optimization at low sample counts. We present an efficient method to jointly reconstruct geometry (explicit triangle meshes), materials, and lighting, which substantially improves material and light separation compared to previous work. We argue that denoising can become an integral part of high quality inverse rendering pipelines.



### Tutel: Adaptive Mixture-of-Experts at Scale
- **Arxiv ID**: http://arxiv.org/abs/2206.03382v2
- **DOI**: None
- **Categories**: **cs.DC**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.03382v2)
- **Published**: 2022-06-07 15:20:20+00:00
- **Updated**: 2023-06-05 15:05:24+00:00
- **Authors**: Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, Joe Chau, Peng Cheng, Fan Yang, Mao Yang, Yongqiang Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Sparsely-gated mixture-of-experts (MoE) has been widely adopted to scale deep learning models to trillion-plus parameters with fixed computational cost. The algorithmic performance of MoE relies on its token routing mechanism that forwards each input token to the right sub-models or experts. While token routing dynamically determines the amount of expert workload at runtime, existing systems suffer inefficient computation due to their static execution, namely static parallelism and pipelining, which does not adapt to the dynamic workload. We present Flex, a highly scalable stack design and implementation for MoE with dynamically adaptive parallelism and pipelining. Flex designs an identical layout for distributing MoE model parameters and input data, which can be leveraged by all possible parallelism or pipelining methods without any mathematical inequivalence or tensor migration overhead. This enables adaptive parallelism/pipelining optimization at zero cost during runtime. Based on this key design, Flex also implements various MoE acceleration techniques. Aggregating all techniques, Flex finally delivers huge speedup at any scale -- 4.96x and 5.75x speedup of a single MoE layer over 16 and 2,048 A100 GPUs, respectively, over the previous state-of-the-art. Our evaluation shows that Flex efficiently and effectively runs a real-world MoE-based model named SwinV2-MoE, built upon Swin Transformer V2, a state-of-the-art computer vision architecture. On efficiency, Flex accelerates SwinV2-MoE, achieving up to 1.55x and 2.11x speedup in training and inference over Fairseq, respectively. On effectiveness, the SwinV2-MoE model achieves superior accuracy in both pre-training and down-stream computer vision tasks such as COCO object detection than the counterpart dense model, indicating the readiness of Flex for end-to-end real-world model training and inference.



### Towards a General Purpose CNN for Long Range Dependencies in $N$D
- **Arxiv ID**: http://arxiv.org/abs/2206.03398v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.03398v2)
- **Published**: 2022-06-07 15:48:02+00:00
- **Updated**: 2022-07-05 09:50:20+00:00
- **Authors**: David W. Romero, David M. Knigge, Albert Gu, Erik J. Bekkers, Efstratios Gavves, Jakub M. Tomczak, Mark Hoogendoorn
- **Comment**: First two authors contributed equally to this work
- **Journal**: None
- **Summary**: The use of Convolutional Neural Networks (CNNs) is widespread in Deep Learning due to a range of desirable model properties which result in an efficient and effective machine learning framework. However, performant CNN architectures must be tailored to specific tasks in order to incorporate considerations such as the input length, resolution, and dimentionality. In this work, we overcome the need for problem-specific CNN architectures with our Continuous Convolutional Neural Network (CCNN): a single CNN architecture equipped with continuous convolutional kernels that can be used for tasks on data of arbitrary resolution, dimensionality and length without structural changes. Continuous convolutional kernels model long range dependencies at every layer, and remove the need for downsampling layers and task-dependent depths needed in current CNN architectures. We show the generality of our approach by applying the same CCNN to a wide set of tasks on sequential (1$\mathrm{D}$) and visual data (2$\mathrm{D}$). Our CCNN performs competitively and often outperforms the current state-of-the-art across all tasks considered.



### Fast and Robust Non-Rigid Registration Using Accelerated Majorization-Minimization
- **Arxiv ID**: http://arxiv.org/abs/2206.03410v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2206.03410v4)
- **Published**: 2022-06-07 16:00:33+00:00
- **Updated**: 2023-02-19 10:52:25+00:00
- **Authors**: Yuxin Yao, Bailin Deng, Weiwei Xu, Juyong Zhang
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: Non-rigid 3D registration, which deforms a source 3D shape in a non-rigid way to align with a target 3D shape, is a classical problem in computer vision. Such problems can be challenging because of imperfect data (noise, outliers and partial overlap) and high degrees of freedom. Existing methods typically adopt the $\ell_p$ type robust norm to measure the alignment error and regularize the smoothness of deformation, and use a proximal algorithm to solve the resulting non-smooth optimization problem. However, the slow convergence of such algorithms limits their wide applications. In this paper, we propose a formulation for robust non-rigid registration based on a globally smooth robust norm for alignment and regularization, which can effectively handle outliers and partial overlaps. The problem is solved using the majorization-minimization algorithm, which reduces each iteration to a convex quadratic problem with a closed-form solution. We further apply Anderson acceleration to speed up the convergence of the solver, enabling the solver to run efficiently on devices with limited compute capability. Extensive experiments demonstrate the effectiveness of our method for non-rigid alignment between two shapes with outliers and partial overlaps, with quantitative evaluation showing that it outperforms state-of-the-art methods in terms of registration accuracy and computational speed. The source code is available at https://github.com/yaoyx689/AMM_NRR.



### Deep Learning based Direct Segmentation Assisted by Deformable Image Registration for Cone-Beam CT based Auto-Segmentation for Adaptive Radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/2206.03413v2
- **DOI**: 10.1088/1361-6560/acb4d7
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.03413v2)
- **Published**: 2022-06-07 16:03:33+00:00
- **Updated**: 2022-10-10 15:11:10+00:00
- **Authors**: Xiao Liang, Howard Morgan, Ti Bai, Michael Dohopolski, Dan Nguyen, Steve Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Cone-beam CT (CBCT)-based online adaptive radiotherapy calls for accurate auto-segmentation to reduce the time cost for physicians to edit contours. However, deep learning (DL)-based direct segmentation of CBCT images is a challenging task, mainly due to the poor image quality and lack of well-labelled large training datasets. Deformable image registration (DIR) is often used to propagate the manual contours on the planning CT (pCT) of the same patient to CBCT. In this work, we undertake solving the problems mentioned above with the assistance of DIR. Our method consists of three main components. First, we use deformed pCT contours derived from multiple DIR methods between pCT and CBCT as pseudo labels for initial training of the DL-based direct segmentation model. Second, we use deformed pCT contours from another DIR algorithm as influencer volumes to define the region of interest for DL-based direct segmentation. Third, the initially trained DL model is further fine-tuned using a smaller set of true labels. We found that DL-based direct segmentation on CBCT trained with pseudo labels and without influencer volumes shows poor performance compared to DIR-based segmentation. However, adding deformed pCT contours as influencer volumes in the direct segmentation network dramatically improves segmentation performance, reaching the accuracy level of DIR-based segmentation. The DL model with influencer volumes can be further improved through fine-tuning using a smaller set of true labels. Experiments showed that 7 out of 19 structures have an at least 0.2 Dice similarity coefficient increase compared to DIR-based segmentation. A DL-based direct CBCT segmentation model can be improved to outperform DIR-based segmentation models by using deformed pCT contours as pseudo labels and influencer volumes for initial training, and by using a smaller set of true labels for model fine tuning.



### Revealing Single Frame Bias for Video-and-Language Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.03428v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2206.03428v1)
- **Published**: 2022-06-07 16:28:30+00:00
- **Updated**: 2022-06-07 16:28:30+00:00
- **Authors**: Jie Lei, Tamara L. Berg, Mohit Bansal
- **Comment**: 19 pages, 8 figures
- **Journal**: None
- **Summary**: Training an effective video-and-language model intuitively requires multiple frames as model inputs. However, it is unclear whether using multiple frames is beneficial to downstream tasks, and if yes, whether the performance gain is worth the drastically-increased computation and memory costs resulting from using more frames. In this work, we explore single-frame models for video-and-language learning. On a diverse set of video-and-language tasks (including text-to-video retrieval and video question answering), we show the surprising result that, with large-scale pre-training and a proper frame ensemble strategy at inference time, a single-frame trained model that does not consider temporal information can achieve better performance than existing methods that use multiple frames for training. This result reveals the existence of a strong "static appearance bias" in popular video-and-language datasets. Therefore, to allow for a more comprehensive evaluation of video-and-language models, we propose two new retrieval tasks based on existing fine-grained action recognition datasets that encourage temporal modeling. Our code is available at https://github.com/jayleicn/singularity



### Generating Long Videos of Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2206.03429v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2206.03429v2)
- **Published**: 2022-06-07 16:29:51+00:00
- **Updated**: 2022-06-09 06:24:12+00:00
- **Authors**: Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei A. Efros, Tero Karras
- **Comment**: None
- **Journal**: None
- **Summary**: We present a video generation model that accurately reproduces object motion, changes in camera viewpoint, and new content that arises over time. Existing video generation methods often fail to produce new content as a function of time while maintaining consistencies expected in real environments, such as plausible dynamics and object persistence. A common failure case is for content to never change due to over-reliance on inductive biases to provide temporal consistency, such as a single latent code that dictates content for the entire video. On the other extreme, without long-term consistency, generated videos may morph unrealistically between different scenes. To address these limitations, we prioritize the time axis by redesigning the temporal latent representation and learning long-term consistency from data by training on longer videos. To this end, we leverage a two-phase training strategy, where we separately train using longer videos at a low resolution and shorter videos at a high resolution. To evaluate the capabilities of our model, we introduce two new benchmark datasets with explicit focus on long-term temporal dynamics.



### Robot Self-Calibration Using Actuated 3D Sensors
- **Arxiv ID**: http://arxiv.org/abs/2206.03430v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, I.2.9; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2206.03430v1)
- **Published**: 2022-06-07 16:35:08+00:00
- **Updated**: 2022-06-07 16:35:08+00:00
- **Authors**: Arne Peters
- **Comment**: 15 pages, 9 figures
- **Journal**: None
- **Summary**: Both, robot and hand-eye calibration haven been object to research for decades. While current approaches manage to precisely and robustly identify the parameters of a robot's kinematic model, they still rely on external devices, such as calibration objects, markers and/or external sensors. Instead of trying to fit the recorded measurements to a model of a known object, this paper treats robot calibration as an offline SLAM problem, where scanning poses are linked to a fixed point in space by a moving kinematic chain. As such, the presented framework allows robot calibration using nothing but an arbitrary eye-in-hand depth sensor, thus enabling fully autonomous self-calibration without any external tools. My new approach is utilizes a modified version of the Iterative Closest Point algorithm to run bundle adjustment on multiple 3D recordings estimating the optimal parameters of the kinematic model. A detailed evaluation of the system is shown on a real robot with various attached 3D sensors. The presented results show that the system reaches precision comparable to a dedicated external tracking system at a fraction of its cost.



### Self-supervised Domain Adaptation in Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2206.03431v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03431v2)
- **Published**: 2022-06-07 16:35:08+00:00
- **Updated**: 2022-06-27 15:20:17+00:00
- **Authors**: Pha Nguyen, Thanh-Dat Truong, Miaoqing Huang, Yi Liang, Ngan Le, Khoa Luu
- **Comment**: Accepted at ICIP 2022
- **Journal**: None
- **Summary**: Self-training crowd counting has not been attentively explored though it is one of the important challenges in computer vision. In practice, the fully supervised methods usually require an intensive resource of manual annotation. In order to address this challenge, this work introduces a new approach to utilize existing datasets with ground truth to produce more robust predictions on unlabeled datasets, named domain adaptation, in crowd counting. While the network is trained with labeled data, samples without labels from the target domain are also added to the training process. In this process, the entropy map is computed and minimized in addition to the adversarial training process designed in parallel. Experiments on Shanghaitech, UCF_CC_50, and UCF-QNRF datasets prove a more generalized improvement of our method over the other state-of-the-arts in the cross-domain setting.



### Can CNNs Be More Robust Than Transformers?
- **Arxiv ID**: http://arxiv.org/abs/2206.03452v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03452v2)
- **Published**: 2022-06-07 17:17:07+00:00
- **Updated**: 2023-03-06 05:51:33+00:00
- **Authors**: Zeyu Wang, Yutong Bai, Yuyin Zhou, Cihang Xie
- **Comment**: ICLR2023. Code is available at https://github.com/UCSC-VLAA/RobustCNN
- **Journal**: None
- **Summary**: The recent success of Vision Transformers is shaking the long dominance of Convolutional Neural Networks (CNNs) in image recognition for a decade. Specifically, in terms of robustness on out-of-distribution samples, recent research finds that Transformers are inherently more robust than CNNs, regardless of different training setups. Moreover, it is believed that such superiority of Transformers should largely be credited to their self-attention-like architectures per se. In this paper, we question that belief by closely examining the design of Transformers. Our findings lead to three highly effective architecture designs for boosting robustness, yet simple enough to be implemented in several lines of code, namely a) patchifying input images, b) enlarging kernel size, and c) reducing activation layers and normalization layers. Bringing these components together, we are able to build pure CNN architectures without any attention-like operations that are as robust as, or even more robust than, Transformers. We hope this work can help the community better understand the design of robust neural architectures. The code is publicly available at https://github.com/UCSC-VLAA/RobustCNN.



### Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2206.03461v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2206.03461v1)
- **Published**: 2022-06-07 17:30:43+00:00
- **Updated**: 2022-06-07 17:30:43+00:00
- **Authors**: Walter H. L. Pinaya, Mark S. Graham, Robert Gray, Pedro F Da Costa, Petru-Daniel Tudosiu, Paul Wright, Yee H. Mah, Andrew D. MacKinnon, James T. Teo, Rolf Jager, David Werring, Geraint Rees, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso
- **Comment**: None
- **Journal**: None
- **Summary**: Deep generative models have emerged as promising tools for detecting arbitrary anomalies in data, dispensing with the necessity for manual labelling. Recently, autoregressive transformers have achieved state-of-the-art performance for anomaly detection in medical imaging. Nonetheless, these models still have some intrinsic weaknesses, such as requiring images to be modelled as 1D sequences, the accumulation of errors during the sampling process, and the significant inference times associated with transformers. Denoising diffusion probabilistic models are a class of non-autoregressive generative models recently shown to produce excellent samples in computer vision (surpassing Generative Adversarial Networks), and to achieve log-likelihoods that are competitive with transformers while having fast inference times. Diffusion models can be applied to the latent representations learnt by autoencoders, making them easily scalable and great candidates for application to high dimensional data, such as medical images. Here, we propose a method based on diffusion models to detect and segment anomalies in brain imaging. By training the models on healthy data and then exploring its diffusion and reverse steps across its Markov chain, we can identify anomalous areas in the latent space and hence identify anomalies in the pixel space. Our diffusion models achieve competitive performance compared with autoregressive approaches across a series of experiments with 2D CT and MRI data involving synthetic and real pathological lesions with much reduced inference times, making their usage clinically viable.



### SHRED: 3D Shape Region Decomposition with Learned Local Operations
- **Arxiv ID**: http://arxiv.org/abs/2206.03480v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.03480v2)
- **Published**: 2022-06-07 17:55:15+00:00
- **Updated**: 2022-10-03 14:54:25+00:00
- **Authors**: R. Kenny Jones, Aalia Habib, Daniel Ritchie
- **Comment**: SIGGRAPH ASIA 2022
- **Journal**: None
- **Summary**: We present SHRED, a method for 3D SHape REgion Decomposition. SHRED takes a 3D point cloud as input and uses learned local operations to produce a segmentation that approximates fine-grained part instances. We endow SHRED with three decomposition operations: splitting regions, fixing the boundaries between regions, and merging regions together. Modules are trained independently and locally, allowing SHRED to generate high-quality segmentations for categories not seen during training. We train and evaluate SHRED with fine-grained segmentations from PartNet; using its merge-threshold hyperparameter, we show that SHRED produces segmentations that better respect ground-truth annotations compared with baseline methods, at any desired decomposition granularity. Finally, we demonstrate that SHRED is useful for downstream applications, out-performing all baselines on zero-shot fine-grained part instance segmentation and few-shot fine-grained semantic segmentation when combined with methods that learn to label shape regions.



### Detection Hub: Unifying Object Detection Datasets via Query Adaptation on Language Embedding
- **Arxiv ID**: http://arxiv.org/abs/2206.03484v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.03484v2)
- **Published**: 2022-06-07 17:59:44+00:00
- **Updated**: 2023-03-29 18:00:08+00:00
- **Authors**: Lingchen Meng, Xiyang Dai, Yinpeng Chen, Pengchuan Zhang, Dongdong Chen, Mengchen Liu, Jianfeng Wang, Zuxuan Wu, Lu Yuan, Yu-Gang Jiang
- **Comment**: CVPR camera ready
- **Journal**: None
- **Summary**: Combining multiple datasets enables performance boost on many computer vision tasks. But similar trend has not been witnessed in object detection when combining multiple datasets due to two inconsistencies among detection datasets: taxonomy difference and domain gap. In this paper, we address these challenges by a new design (named Detection Hub) that is dataset-aware and category-aligned. It not only mitigates the dataset inconsistency but also provides coherent guidance for the detector to learn across multiple datasets. In particular, the dataset-aware design is achieved by learning a dataset embedding that is used to adapt object queries as well as convolutional kernels in detection heads. The categories across datasets are semantically aligned into a unified space by replacing one-hot category representations with word embedding and leveraging the semantic coherence of language embedding. Detection Hub fulfills the benefits of large data on object detection. Experiments demonstrate that joint training on multiple datasets achieves significant performance gains over training on each dataset alone. Detection Hub further achieves SoTA performance on UODB benchmark with wide variety of datasets.



### A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of Natural Movies from Brain Activity
- **Arxiv ID**: http://arxiv.org/abs/2206.03544v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.03544v3)
- **Published**: 2022-06-07 19:27:22+00:00
- **Updated**: 2022-06-10 22:15:21+00:00
- **Authors**: Ganit Kupershmidt, Roman Beliy, Guy Gaziv, Michal Irani
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstructing natural videos from fMRI brain recordings is very challenging, for two main reasons: (i) As fMRI data acquisition is difficult, we only have a limited amount of supervised samples, which is not enough to cover the huge space of natural videos; and (ii) The temporal resolution of fMRI recordings is much lower than the frame rate of natural videos. In this paper, we propose a self-supervised approach for natural-movie reconstruction. By employing cycle-consistency over Encoding-Decoding natural videos, we can: (i) exploit the full framerate of the training videos, and not be limited only to clips that correspond to fMRI recordings; (ii) exploit massive amounts of external natural videos which the subjects never saw inside the fMRI machine. These enable increasing the applicable training data by several orders of magnitude, introducing natural video priors to the decoding network, as well as temporal coherence. Our approach significantly outperforms competing methods, since those train only on the limited supervised data. We further introduce a new and simple temporal prior of natural videos, which - when folded into our fMRI decoder further - allows us to reconstruct videos at a higher frame-rate (HFR) of up to x8 of the original fMRI sample rate.



### Extending Momentum Contrast with Cross Similarity Consistency Regularization
- **Arxiv ID**: http://arxiv.org/abs/2206.04676v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.04676v1)
- **Published**: 2022-06-07 20:06:56+00:00
- **Updated**: 2022-06-07 20:06:56+00:00
- **Authors**: Mehdi Seyfi, Amin Banitalebi-Dehkordi, Yong Zhang
- **Comment**: IEEE Transactions on Circuits and Systems for Video Technology
- **Journal**: None
- **Summary**: Contrastive self-supervised representation learning methods maximize the similarity between the positive pairs, and at the same time tend to minimize the similarity between the negative pairs. However, in general the interplay between the negative pairs is ignored as they do not put in place special mechanisms to treat negative pairs differently according to their specific differences and similarities. In this paper, we present Extended Momentum Contrast (XMoCo), a self-supervised representation learning method founded upon the legacy of the momentum-encoder unit proposed in the MoCo family configurations. To this end, we introduce a cross consistency regularization loss, with which we extend the transformation consistency to dissimilar images (negative pairs). Under the cross consistency regularization rule, we argue that semantic representations associated with any pair of images (positive or negative) should preserve their cross-similarity under pretext transformations. Moreover, we further regularize the training loss by enforcing a uniform distribution of similarity over the negative pairs across a batch. The proposed regularization can easily be added to existing self-supervised learning algorithms in a plug-and-play fashion. Empirically, we report a competitive performance on the standard Imagenet-1K linear head classification benchmark. In addition, by transferring the learned representations to common downstream tasks, we show that using XMoCo with the prevalently utilized augmentations can lead to improvements in the performance of such tasks. We hope the findings of this paper serve as a motivation for researchers to take into consideration the important interplay among the negative examples in self-supervised learning.



### Spatial Cross-Attention Improves Self-Supervised Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.05028v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05028v1)
- **Published**: 2022-06-07 21:14:52+00:00
- **Updated**: 2022-06-07 21:14:52+00:00
- **Authors**: Mehdi Seyfi, Amin Banitalebi-Dehkordi, Yong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised representation learning methods like SwAV are proved to be effective in learning visual semantics of a target dataset. The main idea behind these methods is that different views of a same image represent the same semantics. In this paper, we further introduce an add-on module to facilitate the injection of the knowledge accounting for spatial cross correlations among the samples. This in turn results in distilling intra-class information including feature level locations and cross similarities between same-class instances. The proposed add-on can be added to existing methods such as the SwAV. We can later remove the add-on module for inference without any modification of the learned weights. Through an extensive set of empirical evaluations, we verify that our method yields an improved performance in detecting the class activation maps, top-1 classification accuracy, and down-stream tasks such as object detection, with different configuration settings.



### ObPose: Leveraging Pose for Object-Centric Scene Inference and Generation in 3D
- **Arxiv ID**: http://arxiv.org/abs/2206.03591v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2206.03591v3)
- **Published**: 2022-06-07 21:15:18+00:00
- **Updated**: 2023-06-09 20:18:14+00:00
- **Authors**: Yizhe Wu, Oiwi Parker Jones, Ingmar Posner
- **Comment**: 14 pages, 4 figures
- **Journal**: None
- **Summary**: We present ObPose, an unsupervised object-centric inference and generation model which learns 3D-structured latent representations from RGB-D scenes. Inspired by prior art in 2D representation learning, ObPose considers a factorised latent space, separately encoding object location (where) and appearance (what). ObPose further leverages an object's pose (i.e. location and orientation), defined via a minimum volume principle, as a novel inductive bias for learning the where component. To achieve this, we propose an efficient, voxelised approximation approach to recover the object shape directly from a neural radiance field (NeRF). As a consequence, ObPose models each scene as a composition of NeRFs, richly representing individual objects. To evaluate the quality of the learned representations, ObPose is evaluated quantitatively on the YCB, MultiShapeNet, and CLEVR datatasets for unsupervised scene segmentation, outperforming the current state-of-the-art in 3D scene inference (ObSuRF) by a significant margin. Generative results provide qualitative demonstration that the same ObPose model can both generate novel scenes and flexibly edit the objects in them. These capacities again reflect the quality of the learned latents and the benefits of disentangling the where and what components of a scene. Key design choices made in the ObPose encoder are validated with ablations.



### Neural Network Compression via Effective Filter Analysis and Hierarchical Pruning
- **Arxiv ID**: http://arxiv.org/abs/2206.03596v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.03596v1)
- **Published**: 2022-06-07 21:30:47+00:00
- **Updated**: 2022-06-07 21:30:47+00:00
- **Authors**: Ziqi Zhou, Li Lian, Yilong Yin, Ze Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Network compression is crucial to making the deep networks to be more efficient, faster, and generalizable to low-end hardware. Current network compression methods have two open problems: first, there lacks a theoretical framework to estimate the maximum compression rate; second, some layers may get over-prunned, resulting in significant network performance drop. To solve these two problems, this study propose a gradient-matrix singularity analysis-based method to estimate the maximum network redundancy. Guided by that maximum rate, a novel and efficient hierarchical network pruning algorithm is developed to maximally condense the neuronal network structure without sacrificing network performance. Substantial experiments are performed to demonstrate the efficacy of the new method for pruning several advanced convolutional neural network (CNN) architectures. Compared to existing pruning methods, the proposed pruning algorithm achieved state-of-the-art performance. At the same or similar compression ratio, the new method provided the highest network prediction accuracy as compared to other methods.



### OneRing: A Simple Method for Source-free Open-partial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2206.03600v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.03600v2)
- **Published**: 2022-06-07 21:39:54+00:00
- **Updated**: 2023-01-11 13:08:25+00:00
- **Authors**: Shiqi Yang, Yaxing Wang, Kai Wang, Shangling Jui, Joost van de Weijer
- **Comment**: Updated. It only focuses on source-free open-partial domain
  adaptation, to avoid any potential misunderstanding
- **Journal**: None
- **Summary**: In this paper, we investigate Source-free Open-partial Domain Adaptation (SF-OPDA), which addresses the situation where there exist both domain and category shifts between source and target domains. Under the SF-OPDA setting, which aims to address data privacy concerns, the model cannot access source data anymore during target adaptation. We propose a novel training scheme to learn a (n+1)-way classifier to predict the n source classes and the unknown class, where samples of only known source categories are available for training. Furthermore, for target adaptation, we simply adopt a weighted entropy minimization to adapt the source pretrained model to the unlabeled target domain without source data. In experiments, we show our simple method surpasses current OPDA approaches which demand source data during adaptation. When augmented with a closed-set domain adaptation approach during target adaptation, our source-free method further outperforms the current state-of-the-art OPDA method by 2.5%, 7.2% and 13% on Office-31, Office-Home and VisDA respectively.



### A new method incorporating deep learning with shape priors for left ventricular segmentation in myocardial perfusion SPECT images
- **Arxiv ID**: http://arxiv.org/abs/2206.03603v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.03603v1)
- **Published**: 2022-06-07 22:12:11+00:00
- **Updated**: 2022-06-07 22:12:11+00:00
- **Authors**: Fubao Zhu, Jinyu Zhao, Chen Zhao, Shaojie Tang, Jiaofen Nan, Yanting Li, Zhongqiang Zhao, Jianzhou Shi, Zenghong Chen, Zhixin Jiang, Weihua Zhou
- **Comment**: 21 pages, 14 figures
- **Journal**: None
- **Summary**: Background: The assessment of left ventricular (LV) function by myocardial perfusion SPECT (MPS) relies on accurate myocardial segmentation. The purpose of this paper is to develop and validate a new method incorporating deep learning with shape priors to accurately extract the LV myocardium for automatic measurement of LV functional parameters. Methods: A segmentation architecture that integrates a three-dimensional (3D) V-Net with a shape deformation module was developed. Using the shape priors generated by a dynamic programming (DP) algorithm, the model output was then constrained and guided during the model training for quick convergence and improved performance. A stratified 5-fold cross-validation was used to train and validate our models. Results: Results of our proposed method agree well with those from the ground truth. Our proposed model achieved a Dice similarity coefficient (DSC) of 0.9573(0.0244), 0.9821(0.0137), and 0.9903(0.0041), a Hausdorff distances (HD) of 6.7529(2.7334) mm, 7.2507(3.1952) mm, and 7.6121(3.0134) mm in extracting the endocardium, myocardium, and epicardium, respectively. Conclusion: Our proposed method achieved a high accuracy in extracting LV myocardial contours and assessing LV function.



### Predictive Modeling of Charge Levels for Battery Electric Vehicles using CNN EfficientNet and IGTD Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2206.03612v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2206.03612v1)
- **Published**: 2022-06-07 22:56:40+00:00
- **Updated**: 2022-06-07 22:56:40+00:00
- **Authors**: Seongwoo Choi, Chongzhou Fang, David Haddad, Minsung Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have been a good solution for understanding a vast image dataset. As the increased number of battery-equipped electric vehicles is flourishing globally, there has been much research on understanding which charge levels electric vehicle drivers would choose to charge their vehicles to get to their destination without any prevention. We implemented deep learning approaches to analyze the tabular datasets to understand their state of charge and which charge levels they would choose. In addition, we implemented the Image Generator for Tabular Dataset algorithm to utilize tabular datasets as image datasets to train convolutional neural networks. Also, we integrated other CNN architecture such as EfficientNet to prove that CNN is a great learner for reading information from images that were converted from the tabular dataset, and able to predict charge levels for battery-equipped electric vehicles. We also evaluated several optimization methods to enhance the learning rate of the models and examined further analysis on improving the model architecture.



