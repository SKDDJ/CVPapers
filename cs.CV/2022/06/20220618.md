# Arxiv Papers in cs.CV on 2022-06-18
### ScePT: Scene-consistent, Policy-based Trajectory Predictions for Planning
- **Arxiv ID**: http://arxiv.org/abs/2206.13387v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.13387v1)
- **Published**: 2022-06-18 00:00:02+00:00
- **Updated**: 2022-06-18 00:00:02+00:00
- **Authors**: Yuxiao Chen, Boris Ivanovic, Marco Pavone
- **Comment**: None
- **Journal**: None
- **Summary**: Trajectory prediction is a critical functionality of autonomous systems that share environments with uncontrolled agents, one prominent example being self-driving vehicles. Currently, most prediction methods do not enforce scene consistency, i.e., there are a substantial amount of self-collisions between predicted trajectories of different agents in the scene. Moreover, many approaches generate individual trajectory predictions per agent instead of joint trajectory predictions of the whole scene, which makes downstream planning difficult. In this work, we present ScePT, a policy planning-based trajectory prediction model that generates accurate, scene-consistent trajectory predictions suitable for autonomous system motion planning. It explicitly enforces scene consistency and learns an agent interaction policy that can be used for conditional prediction. Experiments on multiple real-world pedestrians and autonomous vehicle datasets show that ScePT} matches current state-of-the-art prediction accuracy with significantly improved scene consistency. We also demonstrate ScePT's ability to work with a downstream contingency planner.



### Augmented Imagefication: A Data-driven Fault Detection Method for Aircraft Air Data Sensors
- **Arxiv ID**: http://arxiv.org/abs/2206.09055v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.09055v2)
- **Published**: 2022-06-18 00:06:53+00:00
- **Updated**: 2022-06-28 13:19:58+00:00
- **Authors**: Hang Zhao, Jinyi Ma, Zhongzhi Li, Yiqun Dong, Jianliang Ai
- **Comment**: a crucial design defect to acquire flying data by simulation
- **Journal**: None
- **Summary**: In this paper, a novel data-driven approach named Augmented Imagefication for Fault detection (FD) of aircraft air data sensors (ADS) is proposed. Exemplifying the FD problem of aircraft air data sensors, an online FD scheme on edge device based on deep neural network (DNN) is developed. First, the aircraft inertial reference unit measurements is adopted as equivalent inputs, which is scalable to different aircraft/flight cases. Data associated with 6 different aircraft/flight conditions are collected to provide diversity (scalability) in the training/testing database. Then Augmented Imagefication is proposed for the DNN-based prediction of flying conditions. The raw data are reshaped as a grayscale image for convolutional operation, and the necessity of augmentation is analyzed and pointed out. Different kinds of augmented method, i.e. Flip, Repeat, Tile and their combinations are discussed, the result shows that the All Repeat operation in both axes of image matrix leads to the best performance of DNN. The interpretability of DNN is studied based on Grad-CAM, which provide a better understanding and further solidifies the robustness of DNN. Next the DNN model, VGG-16 with augmented imagefication data is optimized for mobile hardware deployment. After pruning of DNN, a lightweight model (98.79% smaller than original VGG-16) with high accuracy (slightly up by 0.27%) and fast speed (time delay is reduced by 87.54%) is obtained. And the hyperparameters optimization of DNN based on TPE is implemented and the best combination of hyperparameters is determined (learning rate 0.001, iterative epochs 600, and batch size 100 yields the highest accuracy at 0.987). Finally, a online FD deployment based on edge device, Jetson Nano, is developed and the real time monitoring of aircraft is achieved. We believe that this method is instructive for addressing the FD problems in other similar fields.



### CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks
- **Arxiv ID**: http://arxiv.org/abs/2206.09059v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09059v2)
- **Published**: 2022-06-18 00:16:37+00:00
- **Updated**: 2022-11-24 21:40:45+00:00
- **Authors**: Tejas Srinivasan, Ting-Yun Chang, Leticia Leonor Pinto Alva, Georgios Chochlakis, Mohammad Rostami, Jesse Thomason
- **Comment**: Accepted to NeurIPS 2022 Datasets and Benchmarks track
- **Journal**: None
- **Summary**: Current state-of-the-art vision-and-language models are evaluated on tasks either individually or in a multi-task setting, overlooking the challenges of continually learning (CL) tasks as they arrive. Existing CL benchmarks have facilitated research on task adaptation and mitigating "catastrophic forgetting", but are limited to vision-only and language-only tasks. We present CLiMB, a benchmark to study the challenge of learning multimodal tasks in a CL setting, and to systematically evaluate how upstream continual learning can rapidly generalize to new multimodal and unimodal tasks. CLiMB includes implementations of several CL algorithms and a modified Vision-Language Transformer (ViLT) model that can be deployed on both multimodal and unimodal tasks. We find that common CL methods can help mitigate forgetting during multimodal task learning, but do not enable cross-task knowledge transfer. We envision that CLiMB will facilitate research on a new class of CL algorithms for this challenging multimodal setting.



### Rotated Digit Recognition by Variational Autoencoders with Fixed Output Distributions
- **Arxiv ID**: http://arxiv.org/abs/2206.13388v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13388v1)
- **Published**: 2022-06-18 00:21:49+00:00
- **Updated**: 2022-06-18 00:21:49+00:00
- **Authors**: David Yevick
- **Comment**: None
- **Journal**: None
- **Summary**: This paper demonstrates that a simple modification of the variational autoencoder (VAE) formalism enables the method to identify and classify rotated and distorted digits. In particular, the conventional objective (cost) function employed during the training process of a VAE both quantifies the agreement between the input and output data records and ensures that the latent space representation of the input data record is statistically generated with an appropriate mean and standard deviation. After training, simulated data realizations are generated by decoding appropriate latent space points. Since, however, standard VAE:s trained on randomly rotated MNIST digits cannot reliably distinguish between different digit classes since the rotated input data is effectively compared to a similarly rotated output data record. In contrast, an alternative implementation in which the objective function compares the output associated with each rotated digit to a corresponding fixed unreferenced reference digit is shown here to discriminate accurately among the rotated digits in latent space even when the dimension of the latent space is 2 or 3.



### Design of Supervision-Scalable Learning Systems: Methodology and Performance Benchmarking
- **Arxiv ID**: http://arxiv.org/abs/2206.09061v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09061v2)
- **Published**: 2022-06-18 00:26:15+00:00
- **Updated**: 2022-08-17 02:44:48+00:00
- **Authors**: Yijing Yang, Hongyu Fu, C. -C. Jay Kuo
- **Comment**: 16 pages, 12 figures, 4 tables, under consideration at Pattern
  Recognition
- **Journal**: None
- **Summary**: The design of robust learning systems that offer stable performance under a wide range of supervision degrees is investigated in this work. We choose the image classification problem as an illustrative example and focus on the design of modularized systems that consist of three learning modules: representation learning, feature learning and decision learning. We discuss ways to adjust each module so that the design is robust with respect to different training sample numbers. Based on these ideas, we propose two families of learning systems. One adopts the classical histogram of oriented gradients (HOG) features while the other uses successive-subspace-learning (SSL) features. We test their performance against LeNet-5, which is an end-to-end optimized neural network, for MNIST and Fashion-MNIST datasets. The number of training samples per image class goes from the extremely weak supervision condition (i.e., 1 labeled sample per class) to the strong supervision condition (i.e., 4096 labeled sample per class) with gradual transition in between (i.e., $2^n$, $n=0, 1, \cdots, 12$). Experimental results show that the two families of modularized learning systems have more robust performance than LeNet-5. They both outperform LeNet-5 by a large margin for small $n$ and have performance comparable with that of LeNet-5 for large $n$.



### Self-Supervised Learning for Videos: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2207.00419v3
- **DOI**: 10.1145/3577925
- **Categories**: **cs.CV**, cs.MM, A.1; I.4.0; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2207.00419v3)
- **Published**: 2022-06-18 00:26:52+00:00
- **Updated**: 2023-07-19 16:00:08+00:00
- **Authors**: Madeline C. Schiappa, Yogesh S. Rawat, Mubarak Shah
- **Comment**: ACM CSUR (December 2022). Project Link: https://bit.ly/3Oimc7Q
- **Journal**: ACM Comput. Surv. (December 2022)
- **Summary**: The remarkable success of deep learning in various domains relies on the availability of large-scale annotated datasets. However, obtaining annotations is expensive and requires great effort, which is especially challenging for videos. Moreover, the use of human-generated annotations leads to models with biased learning and poor domain generalization and robustness. As an alternative, self-supervised learning provides a way for representation learning which does not require annotations and has shown promise in both image and video domains. Different from the image domain, learning video representations are more challenging due to the temporal dimension, bringing in motion and other environmental dynamics. This also provides opportunities for video-exclusive ideas that advance self-supervised learning in the video and multimodal domain. In this survey, we provide a review of existing approaches on self-supervised learning focusing on the video domain. We summarize these methods into four different categories based on their learning objectives: 1) pretext tasks, 2) generative learning, 3) contrastive learning, and 4) cross-modal agreement. We further introduce the commonly used datasets, downstream evaluation tasks, insights into the limitations of existing works, and the potential future directions in this area.



### Free-form Lesion Synthesis Using a Partial Convolution Generative Adversarial Network for Enhanced Deep Learning Liver Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.09065v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09065v2)
- **Published**: 2022-06-18 00:40:41+00:00
- **Updated**: 2022-10-25 05:28:42+00:00
- **Authors**: Yingao Liu, Fei Yang, Yidong Yang
- **Comment**: The paper is under review by JACMP-Journal of Applied Medical Physics
- **Journal**: None
- **Summary**: Automatic deep learning segmentation models has been shown to improve both the segmentation efficiency and the accuracy. However, training a robust segmentation model requires considerably large labeled training samples, which may be impractical. This study aimed to develop a deep learning framework for generating synthetic lesions that can be used to enhance network training. The lesion synthesis network is a modified generative adversarial network (GAN). Specifically, we innovated a partial convolution strategy to construct an Unet-like generator. The discriminator is designed using Wasserstein GAN with gradient penalty and spectral normalization. A mask generation method based on principal component analysis was developed to model various lesion shapes. The generated masks are then converted into liver lesions through a lesion synthesis network. The lesion synthesis framework was evaluated for lesion textures, and the synthetic lesions were used to train a lesion segmentation network to further validate the effectiveness of this framework. All the networks are trained and tested on the public dataset from LITS. The synthetic lesions generated by the proposed approach have very similar histogram distributions compared to the real lesions for the two employed texture parameters, GLCM-energy and GLCM-correlation. The Kullback-Leibler divergence of GLCM-energy and GLCM-correlation were 0.01 and 0.10, respectively. Including the synthetic lesions in the tumor segmentation network improved the segmentation dice performance of U-Net significantly from 67.3% to 71.4% (p<0.05). Meanwhile, the volume precision and sensitivity improve from 74.6% to 76.0% (p=0.23) and 66.1% to 70.9% (p<0.01), respectively. The synthetic data significantly improves the segmentation performance.



### Attention-based Dynamic Subspace Learners for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2206.09068v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09068v1)
- **Published**: 2022-06-18 00:44:40+00:00
- **Updated**: 2022-06-18 00:44:40+00:00
- **Authors**: Sukesh Adiga V, Jose Dolz, Herve Lombaert
- **Comment**: None
- **Journal**: None
- **Summary**: Learning similarity is a key aspect in medical image analysis, particularly in recommendation systems or in uncovering the interpretation of anatomical data in images. Most existing methods learn such similarities in the embedding space over image sets using a single metric learner. Images, however, have a variety of object attributes such as color, shape, or artifacts. Encoding such attributes using a single metric learner is inadequate and may fail to generalize. Instead, multiple learners could focus on separate aspects of these attributes in subspaces of an overarching embedding. This, however, implies the number of learners to be found empirically for each new dataset. This work, Dynamic Subspace Learners, proposes to dynamically exploit multiple learners by removing the need of knowing apriori the number of learners and aggregating new subspace learners during training. Furthermore, the visual interpretability of such subspace learning is enforced by integrating an attention module into our method. This integrated attention mechanism provides a visual insight of discriminative image features that contribute to the clustering of image sets and a visual explanation of the embedding features. The benefits of our attention-based dynamic subspace learners are evaluated in the application of image clustering, image retrieval, and weakly supervised segmentation. Our method achieves competitive results with the performances of multiple learners baselines and significantly outperforms the classification network in terms of clustering and retrieval scores on three different public benchmark datasets. Moreover, our attention maps offer a proxy-labels, which improves the segmentation accuracy up to 15% in Dice scores when compared to state-of-the-art interpretation techniques.



### Analysis & Computational Complexity Reduction of Monocular and Stereo Depth Estimation Techniques
- **Arxiv ID**: http://arxiv.org/abs/2206.09071v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09071v1)
- **Published**: 2022-06-18 00:47:33+00:00
- **Updated**: 2022-06-18 00:47:33+00:00
- **Authors**: Rajeev Patwari, Varo Ly
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate depth estimation with lowest compute and energy cost is a crucial requirement for unmanned and battery operated autonomous systems. Robotic applications require real time depth estimation for navigation and decision making under rapidly changing 3D surroundings. A high accuracy algorithm may provide the best depth estimation but may consume tremendous compute and energy resources. A general trade-off is to choose less accurate methods for initial depth estimate and a more accurate yet compute intensive method when needed. Previous work has shown this trade-off can be improved by developing a state-of-the-art method (AnyNet) to improve stereo depth estimation.   We studied both the monocular and stereo vision depth estimation methods and investigated methods to reduce computational complexity of these methods. This was our baseline. Consequently, our experiments show reduction of monocular depth estimation model size by ~75% reduces accuracy by less than 2% (SSIM metric). Our experiments with the novel stereo vision method (AnyNet) show that accuracy of depth estimation does not degrade more than 3% (three pixel error metric) in spite of reduction in model size by ~20%. We have shown that smaller models can indeed perform competitively.



### Context-aware Proposal Network for Temporal Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.09082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09082v1)
- **Published**: 2022-06-18 01:43:43+00:00
- **Updated**: 2022-06-18 01:43:43+00:00
- **Authors**: Xiang Wang, Huaxin Zhang, Shiwei Zhang, Changxin Gao, Yuanjie Shao, Nong Sang
- **Comment**: First place winning solution for temporal action detection task in
  CVPR-2022 AcitivityNet Challenge. arXiv admin note: substantial text overlap
  with arXiv:2106.11812
- **Journal**: None
- **Summary**: This technical report presents our first place winning solution for temporal action detection task in CVPR-2022 AcitivityNet Challenge. The task aims to localize temporal boundaries of action instances with specific classes in long untrimmed videos. Recent mainstream attempts are based on dense boundary matchings and enumerate all possible combinations to produce proposals. We argue that the generated proposals contain rich contextual information, which may benefits detection confidence prediction. To this end, our method mainly consists of the following three steps: 1) action classification and feature extraction by Slowfast, CSN, TimeSformer, TSP, I3D-flow, VGGish-audio, TPN and ViViT; 2) proposal generation. Our proposed Context-aware Proposal Network (CPN) builds on top of BMN, GTAD and PRN to aggregate contextual information by randomly masking some proposal features. 3) action detection. The final detection prediction is calculated by assigning the proposals with corresponding video-level classifcation results. Finally, we ensemble the results under different feature combination settings and achieve 45.8% performance on the test set, which improves the champion result in CVPR-2021 ActivityNet Challenge by 1.1% in terms of average mAP.



### A Dynamic Data Driven Approach for Explainable Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2206.09089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09089v1)
- **Published**: 2022-06-18 02:41:51+00:00
- **Updated**: 2022-06-18 02:41:51+00:00
- **Authors**: Zachary A Daniels, Dimitris Metaxas
- **Comment**: Unpublished draft of book chapter
- **Journal**: None
- **Summary**: Scene-understanding is an important topic in the area of Computer Vision, and illustrates computational challenges with applications to a wide range of domains including remote sensing, surveillance, smart agriculture, robotics, autonomous driving, and smart cities. We consider the active explanation-driven understanding and classification of scenes. Suppose that an agent utilizing one or more sensors is placed in an unknown environment, and based on its sensory input, the agent needs to assign some label to the perceived scene. The agent can adjust its sensor(s) to capture additional details about the scene, but there is a cost associated with sensor manipulation, and as such, it is important for the agent to understand the scene in a fast and efficient manner. It is also important that the agent understand not only the global state of a scene (e.g., the category of the scene or the major events taking place in the scene) but also the characteristics/properties of the scene that support decisions and predictions made about the global state of the scene. Finally, when the agent encounters an unknown scene category, it must be capable of refusing to assign a label to the scene, requesting aid from a human, and updating its underlying knowledge base and machine learning models based on feedback provided by the human. We introduce a dynamic data driven framework for the active explanation-driven classification of scenes. Our framework is entitled ACUMEN: Active Classification and Understanding Method by Explanation-driven Networks. To demonstrate the utility of the proposed ACUMEN approach and show how it can be adapted to a domain-specific application, we focus on an example case study involving the classification of indoor scenes using an active robotic agent with vision-based sensors, i.e., an electro-optical camera.



### Embodied Scene-aware Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2206.09106v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09106v3)
- **Published**: 2022-06-18 03:50:19+00:00
- **Updated**: 2022-10-13 20:31:36+00:00
- **Authors**: Zhengyi Luo, Shun Iwase, Ye Yuan, Kris Kitani
- **Comment**: NeurIPS 2022. Project website:
  https://zhengyiluo.github.io/projects/embodied_pose/. Zhengyi Luo and Shun
  Iwase contributed equally
- **Journal**: None
- **Summary**: We propose embodied scene-aware human pose estimation where we estimate 3D poses based on a simulated agent's proprioception and scene awareness, along with external third-person observations. Unlike prior methods that often resort to multistage optimization, non-causal inference, and complex contact modeling to estimate human pose and human scene interactions, our method is one-stage, causal, and recovers global 3D human poses in a simulated environment. Since 2D third-person observations are coupled with the camera pose, we propose to disentangle the camera pose and use a multi-step projection gradient defined in the global coordinate frame as the movement cue for our embodied agent. Leveraging a physics simulation and prescanned scenes (e.g., 3D mesh), we simulate our agent in everyday environments (library, office, bedroom, etc.) and equip our agent with environmental sensors to intelligently navigate and interact with the geometries of the scene. Our method also relies only on 2D keypoints and can be trained on synthetic datasets derived from popular human motion databases. To evaluate, we use the popular H36M and PROX datasets and achieve high quality pose estimation on the challenging PROX dataset without ever using PROX motion sequences for training. Code and videos are available on the project page.



### VReBERT: A Simple and Flexible Transformer for Visual Relationship Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.09111v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.09111v1)
- **Published**: 2022-06-18 04:08:19+00:00
- **Updated**: 2022-06-18 04:08:19+00:00
- **Authors**: Yu Cui, Moshiur Farazi
- **Comment**: Published at International Conference on Pattern Recognition (ICPR)
  2022, Montreal Quebec
- **Journal**: None
- **Summary**: Visual Relationship Detection (VRD) impels a computer vision model to 'see' beyond an individual object instance and 'understand' how different objects in a scene are related. The traditional way of VRD is first to detect objects in an image and then separately predict the relationship between the detected object instances. Such a disjoint approach is prone to predict redundant relationship tags (i.e., predicate) between the same object pair with similar semantic meaning, or incorrect ones that have a similar meaning to the ground truth but are semantically incorrect. To remedy this, we propose to jointly train a VRD model with visual object features and semantic relationship features. To this end, we propose VReBERT, a BERT-like transformer model for Visual Relationship Detection with a multi-stage training strategy to jointly process visual and semantic features. We show that our simple BERT-like model is able to outperform the state-of-the-art VRD models in predicate prediction. Furthermore, we show that by using the pre-trained VReBERT model, our model pushes the state-of-the-art zero-shot predicate prediction by a significant margin (+8.49 R@50 and +8.99 R@100).



### Demystifying the Adversarial Robustness of Random Transformation Defenses
- **Arxiv ID**: http://arxiv.org/abs/2207.03574v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03574v2)
- **Published**: 2022-06-18 04:14:38+00:00
- **Updated**: 2022-07-15 10:19:22+00:00
- **Authors**: Chawin Sitawarin, Zachary Golan-Strieb, David Wagner
- **Comment**: ICML 2022 (short presentation), AAAI 2022 AdvML Workshop (best paper,
  oral presentation)
- **Journal**: None
- **Summary**: Neural networks' lack of robustness against attacks raises concerns in security-sensitive settings such as autonomous vehicles. While many countermeasures may look promising, only a few withstand rigorous evaluation. Defenses using random transformations (RT) have shown impressive results, particularly BaRT (Raff et al., 2019) on ImageNet. However, this type of defense has not been rigorously evaluated, leaving its robustness properties poorly understood. Their stochastic properties make evaluation more challenging and render many proposed attacks on deterministic models inapplicable. First, we show that the BPDA attack (Athalye et al., 2018a) used in BaRT's evaluation is ineffective and likely overestimates its robustness. We then attempt to construct the strongest possible RT defense through the informed selection of transformations and Bayesian optimization for tuning their parameters. Furthermore, we create the strongest possible attack to evaluate our RT defense. Our new attack vastly outperforms the baseline, reducing the accuracy by 83% compared to the 19% reduction by the commonly used EoT attack ($4.3\times$ improvement). Our result indicates that the RT defense on the Imagenette dataset (a ten-class subset of ImageNet) is not robust against adversarial examples. Extending the study further, we use our new attack to adversarially train RT defense (called AdvRT), resulting in a large robustness gain. Code is available at https://github.com/wagner-group/demystify-random-transform.



### Bear the Query in Mind: Visual Grounding with Query-conditioned Convolution
- **Arxiv ID**: http://arxiv.org/abs/2206.09114v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09114v2)
- **Published**: 2022-06-18 04:26:39+00:00
- **Updated**: 2022-06-22 02:38:03+00:00
- **Authors**: Chonghan Chen, Qi Jiang, Chih-Hao Wang, Noel Chen, Haohan Wang, Xiang Li, Bhiksha Raj
- **Comment**: None
- **Journal**: None
- **Summary**: Visual grounding is a task that aims to locate a target object according to a natural language expression. As a multi-modal task, feature interaction between textual and visual inputs is vital. However, previous solutions mainly handle each modality independently before fusing them together, which does not take full advantage of relevant textual information while extracting visual features. To better leverage the textual-visual relationship in visual grounding, we propose a Query-conditioned Convolution Module (QCM) that extracts query-aware visual features by incorporating query information into the generation of convolutional kernels. With our proposed QCM, the downstream fusion module receives visual features that are more discriminative and focused on the desired object described in the expression, leading to more accurate predictions. Extensive experiments on three popular visual grounding datasets demonstrate that our method achieves state-of-the-art performance. In addition, the query-aware visual features are informative enough to achieve comparable performance to the latest methods when directly used for prediction without further multi-modal fusion.



### A Combined PCA-MLP Network for Early Breast Cancer Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.09128v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09128v1)
- **Published**: 2022-06-18 06:17:40+00:00
- **Updated**: 2022-06-18 06:17:40+00:00
- **Authors**: Md. Wahiduzzaman Khan Arnob, Arunima Dey Pooja, Md. Saif Hassan Onim
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer is the second most responsible for all cancer types and has been the cause of numerous deaths over the years, especially among women. Any improvisation of the existing diagnosis system for the detection of cancer can contribute to minimizing the death ratio. Moreover, cancer detection at an early stage has recently been a prime research area in the scientific community to enhance the survival rate. Proper choice of machine learning tools can ensure early-stage prognosis with high accuracy. In this paper, we have studied different machine learning algorithms to detect whether a patient is likely to face breast cancer or not. Due to the implicit behavior of early-stage features, we have implemented a multilayer perception model with the integration of PCA and suggested it to be more viable than other detection algorithms. Our 4 layers MLP-PCA network has obtained the best accuracy of 100% with a mean of 90.48% accuracy on the BCCD dataset.



### Replacing Labeled Real-image Datasets with Auto-generated Contours
- **Arxiv ID**: http://arxiv.org/abs/2206.09132v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09132v1)
- **Published**: 2022-06-18 06:43:38+00:00
- **Updated**: 2022-06-18 06:43:38+00:00
- **Authors**: Hirokatsu Kataoka, Ryo Hayamizu, Ryosuke Yamada, Kodai Nakashima, Sora Takashima, Xinyu Zhang, Edgar Josafat Martinez-Noriega, Nakamasa Inoue, Rio Yokota
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: In the present work, we show that the performance of formula-driven supervised learning (FDSL) can match or even exceed that of ImageNet-21k without the use of real images, human-, and self-supervision during the pre-training of Vision Transformers (ViTs). For example, ViT-Base pre-trained on ImageNet-21k shows 81.8% top-1 accuracy when fine-tuned on ImageNet-1k and FDSL shows 82.7% top-1 accuracy when pre-trained under the same conditions (number of images, hyperparameters, and number of epochs). Images generated by formulas avoid the privacy/copyright issues, labeling cost and errors, and biases that real images suffer from, and thus have tremendous potential for pre-training general models. To understand the performance of the synthetic images, we tested two hypotheses, namely (i) object contours are what matter in FDSL datasets and (ii) increased number of parameters to create labels affects performance improvement in FDSL pre-training. To test the former hypothesis, we constructed a dataset that consisted of simple object contour combinations. We found that this dataset can match the performance of fractals. For the latter hypothesis, we found that increasing the difficulty of the pre-training task generally leads to better fine-tuning accuracy.



### A Perceptually Optimized and Self-Calibrated Tone Mapping Operator
- **Arxiv ID**: http://arxiv.org/abs/2206.09146v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09146v3)
- **Published**: 2022-06-18 08:06:29+00:00
- **Updated**: 2023-08-25 10:48:15+00:00
- **Authors**: Peibei Cao, Chenyang Le, Yuming Fang, Kede Ma
- **Comment**: 15 pages,17 figures
- **Journal**: None
- **Summary**: With the increasing popularity and accessibility of high dynamic range (HDR) photography, tone mapping operators (TMOs) for dynamic range compression are practically demanding. In this paper, we develop a two-stage neural network-based TMO that is self-calibrated and perceptually optimized. In Stage one, motivated by the physiology of the early stages of the human visual system, we first decompose an HDR image into a normalized Laplacian pyramid. We then use two lightweight deep neural networks (DNNs), taking the normalized representation as input and estimating the Laplacian pyramid of the corresponding LDR image. We optimize the tone mapping network by minimizing the normalized Laplacian pyramid distance (NLPD), a perceptual metric aligning with human judgments of tone-mapped image quality. In Stage two, the input HDR image is self-calibrated to compute the final LDR image. We feed the same HDR image but rescaled with different maximum luminances to the learned tone mapping network, and generate a pseudo-multi-exposure image stack with different detail visibility and color saturation. We then train another lightweight DNN to fuse the LDR image stack into a desired LDR image by maximizing a variant of the structural similarity index for multi-exposure image fusion (MEF-SSIM), which has been proven perceptually relevant to fused image quality. The proposed self-calibration mechanism through MEF enables our TMO to accept uncalibrated HDR images, while being physiology-driven. Extensive experiments show that our method produces images with consistently better visual quality. Additionally, since our method builds upon three lightweight DNNs, it is among the fastest local TMOs.



### Deep Compatible Learning for Partially-Supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.09148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09148v1)
- **Published**: 2022-06-18 08:38:31+00:00
- **Updated**: 2022-06-18 08:38:31+00:00
- **Authors**: Ke Zhang, Xiahai Zhuang
- **Comment**: 16 pages, 13 figures
- **Journal**: None
- **Summary**: Partially-supervised learning can be challenging for segmentation due to the lack of supervision for unlabeled structures, and the methods directly applying fully-supervised learning could lead to incompatibility, meaning ground truth is not in the solution set of the optimization problem given the loss function. To address the challenge, we propose a deep compatible learning (DCL) framework, which trains a single multi-label segmentation network using images with only partial structures annotated. We first formulate the partially-supervised segmentation as an optimization problem compatible with missing labels, and prove its compatibility. Then, we equip the model with a conditional segmentation strategy, to propagate labels from multiple partially-annotated images to the target. Additionally, we propose a dual learning strategy, which learns two opposite mappings of label propagation simultaneously, to provide substantial supervision for unlabeled structures. The two strategies are formulated into compatible forms, termed as conditional compatibility and dual compatibility, respectively. We show this framework is generally applicable for conventional loss functions. The approach attains significant performance improvement over existing methods, especially in the situation where only a small training dataset is available. Results on three segmentation tasks have shown that the proposed framework could achieve performance matching fully-supervised models.



### REVECA -- Rich Encoder-decoder framework for Video Event CAptioner
- **Arxiv ID**: http://arxiv.org/abs/2206.09178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.09178v1)
- **Published**: 2022-06-18 11:10:12+00:00
- **Updated**: 2022-06-18 11:10:12+00:00
- **Authors**: Jaehyuk Heo, YongGi Jeong, Sunwoo Kim, Jaehee Kim, Pilsung Kang
- **Comment**: The IEEE/CVF Computer Vision and Pattern Recognition Conference
  (CVPR). LOng-form VidEo Understanding (LOVEU) workshop
- **Journal**: None
- **Summary**: We describe an approach used in the Generic Boundary Event Captioning challenge at the Long-Form Video Understanding Workshop held at CVPR 2022. We designed a Rich Encoder-decoder framework for Video Event CAptioner (REVECA) that utilizes spatial and temporal information from the video to generate a caption for the corresponding the event boundary. REVECA uses frame position embedding to incorporate information before and after the event boundary. Furthermore, it employs features extracted using the temporal segment network and temporal-based pairwise difference method to learn temporal information. A semantic segmentation mask for the attentional pooling process is adopted to learn the subject of an event. Finally, LoRA is applied to fine-tune the image encoder to enhance the learning efficiency. REVECA yielded an average score of 50.97 on the Kinetics-GEBC test data, which is an improvement of 10.17 over the baseline method. Our code is available in https://github.com/TooTouch/REVECA.



### Gender Artifacts in Visual Datasets
- **Arxiv ID**: http://arxiv.org/abs/2206.09191v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09191v2)
- **Published**: 2022-06-18 12:09:19+00:00
- **Updated**: 2022-08-22 12:36:44+00:00
- **Authors**: Nicole Meister, Dora Zhao, Angelina Wang, Vikram V. Ramaswamy, Ruth Fong, Olga Russakovsky
- **Comment**: None
- **Journal**: None
- **Summary**: Gender biases are known to exist within large-scale visual datasets and can be reflected or even amplified in downstream models. Many prior works have proposed methods for mitigating gender biases, often by attempting to remove gender expression information from images. To understand the feasibility and practicality of these approaches, we investigate what $\textit{gender artifacts}$ exist within large-scale visual datasets. We define a $\textit{gender artifact}$ as a visual cue that is correlated with gender, focusing specifically on those cues that are learnable by a modern image classifier and have an interpretable human corollary. Through our analyses, we find that gender artifacts are ubiquitous in the COCO and OpenImages datasets, occurring everywhere from low-level information (e.g., the mean value of the color channels) to the higher-level composition of the image (e.g., pose and location of people). Given the prevalence of gender artifacts, we claim that attempts to remove gender artifacts from such datasets are largely infeasible. Instead, the responsibility lies with researchers and practitioners to be aware that the distribution of images within datasets is highly gendered and hence develop methods which are robust to these distributional shifts across groups.



### Multi-Modality Image Super-Resolution using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2206.09193v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09193v2)
- **Published**: 2022-06-18 12:19:31+00:00
- **Updated**: 2022-06-22 15:27:35+00:00
- **Authors**: Aref Abedjooy, Mehran Ebrahimi
- **Comment**: to be published in the Proceedings of 16th International Conference
  on Computer Graphics, Visualization, Computer Vision and Image Processing
  (CGVCVIP), Lisbon, Portugal, July 2022
- **Journal**: None
- **Summary**: Over the past few years deep learning-based techniques such as Generative Adversarial Networks (GANs) have significantly improved solutions to image super-resolution and image-to-image translation problems. In this paper, we propose a solution to the joint problem of image super-resolution and multi-modality image-to-image translation. The problem can be stated as the recovery of a high-resolution image in a modality, given a low-resolution observation of the same image in an alternative modality. Our paper offers two models to address this problem and will be evaluated on the recovery of high-resolution day images given low-resolution night images of the same scene. Promising qualitative and quantitative results will be presented for each model.



### 3D unsupervised anomaly detection and localization through virtual multi-view projection and reconstruction: Clinical validation on low-dose chest computed tomography
- **Arxiv ID**: http://arxiv.org/abs/2206.13385v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13385v1)
- **Published**: 2022-06-18 13:22:00+00:00
- **Updated**: 2022-06-18 13:22:00+00:00
- **Authors**: Kyung-Su Kim, Seong Je Oh, Ju Hwan Lee, Myung Jin Chung
- **Comment**: Kyung-Su Kim and Seong Je Oh have contributed equally to this work as
  the co-first author. Kyung-Su Kim (kskim.doc@gmail.com) and Myung Jin Chung
  (mj1.chung@samsung.com) have contributed equally to this work as the
  co-corresponding author
- **Journal**: None
- **Summary**: Computer-aided diagnosis for low-dose computed tomography (CT) based on deep learning has recently attracted attention as a first-line automatic testing tool because of its high accuracy and low radiation exposure. However, existing methods rely on supervised learning, imposing an additional burden to doctors for collecting disease data or annotating spatial labels for network training, consequently hindering their implementation. We propose a method based on a deep neural network for computer-aided diagnosis called virtual multi-view projection and reconstruction for unsupervised anomaly detection. Presumably, this is the first method that only requires data from healthy patients for training to identify three-dimensional (3D) regions containing any anomalies. The method has three key components. Unlike existing computer-aided diagnosis tools that use conventional CT slices as the network input, our method 1) improves the recognition of 3D lung structures by virtually projecting an extracted 3D lung region to obtain two-dimensional (2D) images from diverse views to serve as network inputs, 2) accommodates the input diversity gain for accurate anomaly detection, and 3) achieves 3D anomaly/disease localization through a novel 3D map restoration method using multiple 2D anomaly maps. The proposed method based on unsupervised learning improves the patient-level anomaly detection by 10% (area under the curve, 0.959) compared with a gold standard based on supervised learning (area under the curve, 0.848), and it localizes the anomaly region with 93% accuracy, demonstrating its high performance.



### Camera Adaptation for Fundus-Image-Based CVD Risk Estimation
- **Arxiv ID**: http://arxiv.org/abs/2206.09202v1
- **DOI**: 10.1007/978-3-031-16434-7_57
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09202v1)
- **Published**: 2022-06-18 13:28:16+00:00
- **Updated**: 2022-06-18 13:28:16+00:00
- **Authors**: Zhihong Lin, Danli Shi, Donghao Zhang, Xianwen Shang, Mingguang He, Zongyuan Ge
- **Comment**: This preprint has not undergone peer review (when applicable) or any
  post-submission improvements or corrections. The Version of Record of this
  contribution will be added soon
- **Journal**: None
- **Summary**: Recent studies have validated the association between cardiovascular disease (CVD) risk and retinal fundus images. Combining deep learning (DL) and portable fundus cameras will enable CVD risk estimation in various scenarios and improve healthcare democratization. However, there are still significant issues to be solved. One of the top priority issues is the different camera differences between the databases for research material and the samples in the production environment. Most high-quality retinography databases ready for research are collected from high-end fundus cameras, and there is a significant domain discrepancy between different cameras. To fully explore the domain discrepancy issue, we first collect a Fundus Camera Paired (FCP) dataset containing pair-wise fundus images captured by the high-end Topcon retinal camera and the low-end Mediwork portable fundus camera of the same patients. Then, we propose a cross-laterality feature alignment pre-training scheme and a self-attention camera adaptor module to improve the model robustness. The cross-laterality feature alignment training encourages the model to learn common knowledge from the same patient's left and right fundus images and improve model generalization. Meanwhile, the device adaptation module learns feature transformation from the target domain to the source domain. We conduct comprehensive experiments on both the UK Biobank database and our FCP data. The experimental results show that the CVD risk regression accuracy and the result consistency over two cameras are improved with our proposed method. The code is available here: \url{https://github.com/linzhlalala/CVD-risk-based-on-retinal-fundus-images}



### AI-based computer-aided diagnostic system of chest digital tomography synthesis: Demonstrating comparative advantage with X-ray-based AI systems
- **Arxiv ID**: http://arxiv.org/abs/2206.13504v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13504v1)
- **Published**: 2022-06-18 13:29:55+00:00
- **Updated**: 2022-06-18 13:29:55+00:00
- **Authors**: Kyung-Su Kim, Ju Hwan Lee, Seong Je Oh, Myung Jin Chung
- **Comment**: Kyung-Su Kim, Ju Hwan Lee, and Seong Je Oh have contributed equally
  to this work as the co-first author. Kyung-Su Kim (kskim.doc@gmail.com) and
  Myung Jin Chung (mj1.chung@samsung.com) have contributed equally to this work
  as the co-corresponding author
- **Journal**: None
- **Summary**: Compared with chest X-ray (CXR) imaging, which is a single image projected from the front of the patient, chest digital tomosynthesis (CDTS) imaging can be more advantageous for lung lesion detection because it acquires multiple images projected from multiple angles of the patient. Various clinical comparative analysis and verification studies have been reported to demonstrate this, but there were no artificial intelligence (AI)-based comparative analysis studies. Existing AI-based computer-aided detection (CAD) systems for lung lesion diagnosis have been developed mainly based on CXR images; however, CAD-based on CDTS, which uses multi-angle images of patients in various directions, has not been proposed and verified for its usefulness compared to CXR-based counterparts. This study develops/tests a CDTS-based AI CAD system to detect lung lesions to demonstrate performance improvements compared to CXR-based AI CAD. We used multiple projection images as input for the CDTS-based AI model and a single-projection image as input for the CXR-based AI model to fairly compare and evaluate the performance between models. The proposed CDTS-based AI CAD system yielded sensitivities of 0.782 and 0.785 and accuracies of 0.895 and 0.837 for the performance of detecting tuberculosis and pneumonia, respectively, against normal subjects. These results show higher performance than sensitivities of 0.728 and 0.698 and accuracies of 0.874 and 0.826 for detecting tuberculosis and pneumonia through the CXR-based AI CAD, which only uses a single projection image in the frontal direction. We found that CDTS-based AI CAD improved the sensitivity of tuberculosis and pneumonia by 5.4% and 8.7% respectively, compared to CXR-based AI CAD without loss of accuracy. Therefore, we comparatively prove that CDTS-based AI CAD technology can improve performance more than CXR, enhancing the clinical applicability of CDTS.



### EST: Evaluating Scientific Thinking in Artificial Agents
- **Arxiv ID**: http://arxiv.org/abs/2206.09203v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09203v1)
- **Published**: 2022-06-18 13:32:41+00:00
- **Updated**: 2022-06-18 13:32:41+00:00
- **Authors**: Manjie Xu, Guangyuan Jiang, Chi Zhang, Song-Chun Zhu, Yixin Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Theoretical ideas and empirical research have shown us a seemingly surprising result: children, even very young toddlers, demonstrate learning and thinking in a strikingly similar manner to scientific reasoning in formal research. Encountering a novel phenomenon, children make hypotheses against data, conduct causal inference from observation, test their theory via experimentation, and correct the proposition if inconsistency arises. Rounds of such processes continue until the underlying mechanism is found. Towards building machines that can learn and think like people, one natural question for us to ask is: whether the intelligence we achieve today manages to perform such a scientific thinking process, and if any, at what level. In this work, we devise the EST environment for evaluating the scientific thinking ability in artificial agents. Motivated by the stream of research on causal discovery, we build our interactive EST environment based on Blicket detection. Specifically, in each episode of EST, an agent is presented with novel observations and asked to figure out all objects' Blicketness. At each time step, the agent proposes new experiments to validate its hypothesis and updates its current belief. By evaluating Reinforcement Learning (RL) agents on both a symbolic and visual version of this task, we notice clear failure of today's learning methods in reaching a level of intelligence comparable to humans. Such inefficacy of learning in scientific thinking calls for future research in building humanlike intelligence.



### Multi-Modality Image Inpainting using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2206.09210v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09210v2)
- **Published**: 2022-06-18 14:06:14+00:00
- **Updated**: 2022-06-22 15:24:17+00:00
- **Authors**: Aref Abedjooy, Mehran Ebrahimi
- **Comment**: to be published in the Proceedings of 26th Int'l Conf on Image
  Processing, Computer Vision, & Pattern Recognition (IPCV), July 2022
- **Journal**: None
- **Summary**: Deep learning techniques, especially Generative Adversarial Networks (GANs) have significantly improved image inpainting and image-to-image translation tasks over the past few years. To the best of our knowledge, the problem of combining the image inpainting task with the multi-modality image-to-image translation remains intact. In this paper, we propose a model to address this problem. The model will be evaluated on combined night-to-day image translation and inpainting, along with promising qualitative and quantitative results.



### 3D Face Parsing via Surface Parameterization and 2D Semantic Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/2206.09221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09221v1)
- **Published**: 2022-06-18 15:21:24+00:00
- **Updated**: 2022-06-18 15:21:24+00:00
- **Authors**: Wenyuan Sun, Ping Zhou, Yangang Wang, Zongpu Yu, Jing Jin, Guangquan Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Face parsing assigns pixel-wise semantic labels as the face representation for computers, which is the fundamental part of many advanced face technologies. Compared with 2D face parsing, 3D face parsing shows more potential to achieve better performance and further application, but it is still challenging due to 3D mesh data computation. Recent works introduced different methods for 3D surface segmentation, while the performance is still limited. In this paper, we propose a method based on the "3D-2D-3D" strategy to accomplish 3D face parsing. The topological disk-like 2D face image containing spatial and textural information is transformed from the sampled 3D face data through the face parameterization algorithm, and a specific 2D network called CPFNet is proposed to achieve the semantic segmentation of the 2D parameterized face data with multi-scale technologies and feature aggregation. The 2D semantic result is then inversely re-mapped to 3D face data, which finally achieves the 3D face parsing. Experimental results show that both CPFNet and the "3D-2D-3D" strategy accomplish high-quality 3D face parsing and outperform state-of-the-art 2D networks as well as 3D methods in both qualitative and quantitative comparisons.



### UI Layers Merger: Merging UI layers via Visual Learning and Boundary Prior
- **Arxiv ID**: http://arxiv.org/abs/2206.13389v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.13389v3)
- **Published**: 2022-06-18 16:09:28+00:00
- **Updated**: 2022-09-03 07:03:02+00:00
- **Authors**: Yun-nong Chen, Yan-kun Zhen, Chu-ning Shi, Jia-zhi Li, Liu-qing Chen, Ze-jian Li, Ling-yun Sun, Ting-ting Zhou, Yan-fang Chang
- **Comment**: 15 pages, accepted to Frontiers of Information Technology &
  Electronic Engineering. This is a preprint version, the copyright belongs to
  the Springer Nature journals
- **Journal**: None
- **Summary**: With the fast-growing GUI development workload in the Internet industry, some work on intelligent methods attempted to generate maintainable front-end code from UI screenshots. It can be more suitable for utilizing UI design drafts that contain UI metadata. However, fragmented layers inevitably appear in the UI design drafts which greatly reduces the quality of code generation. None of the existing GUI automated techniques detects and merges the fragmented layers to improve the accessibility of generated code. In this paper, we propose UI Layers Merger (UILM), a vision-based method, which can automatically detect and merge fragmented layers into UI components. Our UILM contains Merging Area Detector (MAD) and a layers merging algorithm. MAD incorporates the boundary prior knowledge to accurately detect the boundaries of UI components. Then, the layers merging algorithm can search out the associated layers within the components' boundaries and merge them into a whole part. We present a dynamic data augmentation approach to boost the performance of MAD. We also construct a large-scale UI dataset for training the MAD and testing the performance of UILM. The experiment shows that the proposed method outperforms the best baseline regarding merging area detection and achieves a decent accuracy regarding layers merging.



### GaLeNet: Multimodal Learning for Disaster Prediction, Management and Relief
- **Arxiv ID**: http://arxiv.org/abs/2206.09242v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09242v1)
- **Published**: 2022-06-18 16:45:57+00:00
- **Updated**: 2022-06-18 16:45:57+00:00
- **Authors**: Rohit Saha, Mengyi Fang, Angeline Yasodhara, Kyryl Truskovskyi, Azin Asgarian, Daniel Homola, Raahil Shah, Frederik Dieleman, Jack Weatheritt, Thomas Rogers
- **Comment**: Accepted to CVPR 2022 Workshop on Multimodal Learning for Earth and
  Environment
- **Journal**: None
- **Summary**: After a natural disaster, such as a hurricane, millions are left in need of emergency assistance. To allocate resources optimally, human planners need to accurately analyze data that can flow in large volumes from several sources. This motivates the development of multimodal machine learning frameworks that can integrate multiple data sources and leverage them efficiently. To date, the research community has mainly focused on unimodal reasoning to provide granular assessments of the damage. Moreover, previous studies mostly rely on post-disaster images, which may take several days to become available. In this work, we propose a multimodal framework (GaLeNet) for assessing the severity of damage by complementing pre-disaster images with weather data and the trajectory of the hurricane. Through extensive experiments on data from two hurricanes, we demonstrate (i) the merits of multimodal approaches compared to unimodal methods, and (ii) the effectiveness of GaLeNet at fusing various modalities. Furthermore, we show that GaLeNet can leverage pre-disaster images in the absence of post-disaster images, preventing substantial delays in decision making.



### Structured Light with Redundancy Codes
- **Arxiv ID**: http://arxiv.org/abs/2206.09243v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09243v1)
- **Published**: 2022-06-18 16:52:30+00:00
- **Updated**: 2022-06-18 16:52:30+00:00
- **Authors**: Zhanghao Sun, Yu Zhang, Yicheng Wu, Dong Huo, Yiming Qian, Jian Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Structured light (SL) systems acquire high-fidelity 3D geometry with active illumination projection. Conventional systems exhibit challenges when working in environments with strong ambient illumination, global illumination and cross-device interference. This paper proposes a general-purposed technique to improve the robustness of SL by projecting redundant optical signals in addition to the native SL patterns. In this way, projected signals become more distinguishable from errors. Thus the geometry information can be more easily recovered using simple signal processing and the ``coding gain" in performance is obtained. We propose three applications using our redundancy codes: (1) Self error-correction for SL imaging under strong ambient light, (2) Error detection for adaptive reconstruction under global illumination, and (3) Interference filtering with device-specific projection sequence encoding, especially for event camera-based SL and light curtain devices. We systematically analyze the design rules and signal processing algorithms in these applications. Corresponding hardware prototypes are built for evaluations on real-world complex scenes. Experimental results on the synthetic and real data demonstrate the significant performance improvements in SL systems with our redundancy codes.



### GAN2X: Non-Lambertian Inverse Rendering of Image GANs
- **Arxiv ID**: http://arxiv.org/abs/2206.09244v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09244v4)
- **Published**: 2022-06-18 16:58:49+00:00
- **Updated**: 2022-11-14 10:03:57+00:00
- **Authors**: Xingang Pan, Ayush Tewari, Lingjie Liu, Christian Theobalt
- **Comment**: Accepted to 3DV 2022. The video demo is available at the project
  page: https://vcai.mpi-inf.mpg.de/projects/GAN2X/
- **Journal**: None
- **Summary**: 2D images are observations of the 3D physical world depicted with the geometry, material, and illumination components. Recovering these underlying intrinsic components from 2D images, also known as inverse rendering, usually requires a supervised setting with paired images collected from multiple viewpoints and lighting conditions, which is resource-demanding. In this work, we present GAN2X, a new method for unsupervised inverse rendering that only uses unpaired images for training. Unlike previous Shape-from-GAN approaches that mainly focus on 3D shapes, we take the first attempt to also recover non-Lambertian material properties by exploiting the pseudo paired data generated by a GAN. To achieve precise inverse rendering, we devise a specularity-aware neural surface representation that continuously models the geometry and material properties. A shading-based refinement technique is adopted to further distill information in the target image and recover more fine details. Experiments demonstrate that GAN2X can accurately decompose 2D images to 3D shape, albedo, and specular properties for different object categories, and achieves the state-of-the-art performance for unsupervised single-view 3D face reconstruction. We also show its applications in downstream tasks including real image editing and lifting 2D GANs to decomposed 3D GANs.



### Multistream Gaze Estimation with Anatomical Eye Region Isolation by Synthetic to Real Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.09256v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09256v1)
- **Published**: 2022-06-18 17:57:32+00:00
- **Updated**: 2022-06-18 17:57:32+00:00
- **Authors**: Zunayed Mahmud, Paul Hungler, Ali Etemad
- **Comment**: 14 pages, 10 figures, 12 tables. This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice
- **Journal**: None
- **Summary**: We propose a novel neural pipeline, MSGazeNet, that learns gaze representations by taking advantage of the eye anatomy information through a multistream framework. Our proposed solution comprises two components, first a network for isolating anatomical eye regions, and a second network for multistream gaze estimation. The eye region isolation is performed with a U-Net style network which we train using a synthetic dataset that contains eye region masks for the visible eyeball and the iris region. The synthetic dataset used in this stage is a new dataset consisting of 60,000 eye images, which we create using an eye-gaze simulator, UnityEyes. Successive to training, the eye region isolation network is then transferred to the real domain for generating masks for the real-world eye images. In order to successfully make the transfer, we exploit domain randomization in the training process, which allows for the synthetic images to benefit from a larger variance with the help of augmentations that resemble artifacts. The generated eye region masks along with the raw eye images are then used together as a multistream input to our gaze estimation network. We evaluate our framework on three benchmark gaze estimation datasets, MPIIGaze, Eyediap, and UTMultiview, where we set a new state-of-the-art on Eyediap and UTMultiview datasets by obtaining a performance gain of 7.57% and 1.85% respectively, while achieving competitive performance on MPIIGaze. We also study the robustness of our method with respect to the noise in the data and demonstrate that our model is less sensitive to noisy data. Lastly, we perform a variety of experiments including ablation studies to evaluate the contribution of different components and design choices in our solution.



### SAViR-T: Spatially Attentive Visual Reasoning with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2206.09265v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09265v2)
- **Published**: 2022-06-18 18:26:20+00:00
- **Updated**: 2022-06-22 02:00:11+00:00
- **Authors**: Pritish Sahu, Kalliopi Basioti, Vladimir Pavlovic
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel computational model, "SAViR-T", for the family of visual reasoning problems embodied in the Raven's Progressive Matrices (RPM). Our model considers explicit spatial semantics of visual elements within each image in the puzzle, encoded as spatio-visual tokens, and learns the intra-image as well as the inter-image token dependencies, highly relevant for the visual reasoning task. Token-wise relationship, modeled through a transformer-based SAViR-T architecture, extract group (row or column) driven representations by leveraging the group-rule coherence and use this as the inductive bias to extract the underlying rule representations in the top two row (or column) per token in the RPM. We use this relation representations to locate the correct choice image that completes the last row or column for the RPM. Extensive experiments across both synthetic RPM benchmarks, including RAVEN, I-RAVEN, RAVEN-FAIR, and PGM, and the natural image-based "V-PROM" demonstrate that SAViR-T sets a new state-of-the-art for visual reasoning, exceeding prior models' performance by a considerable margin.



### DECK: Model Hardening for Defending Pervasive Backdoors
- **Arxiv ID**: http://arxiv.org/abs/2206.09272v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.09272v1)
- **Published**: 2022-06-18 19:46:06+00:00
- **Updated**: 2022-06-18 19:46:06+00:00
- **Authors**: Guanhong Tao, Yingqi Liu, Siyuan Cheng, Shengwei An, Zhuo Zhang, Qiuling Xu, Guangyu Shen, Xiangyu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Pervasive backdoors are triggered by dynamic and pervasive input perturbations. They can be intentionally injected by attackers or naturally exist in normally trained models. They have a different nature from the traditional static and localized backdoors that can be triggered by perturbing a small input area with some fixed pattern, e.g., a patch with solid color. Existing defense techniques are highly effective for traditional backdoors. However, they may not work well for pervasive backdoors, especially regarding backdoor removal and model hardening. In this paper, we propose a novel model hardening technique against pervasive backdoors, including both natural and injected backdoors. We develop a general pervasive attack based on an encoder-decoder architecture enhanced with a special transformation layer. The attack can model a wide range of existing pervasive backdoor attacks and quantify them by class distances. As such, using the samples derived from our attack in adversarial training can harden a model against these backdoor vulnerabilities. Our evaluation on 9 datasets with 15 model structures shows that our technique can enlarge class distances by 59.65% on average with less than 1% accuracy degradation and no robustness loss, outperforming five hardening techniques such as adversarial training, universal adversarial training, MOTH, etc. It can reduce the attack success rate of six pervasive backdoor attacks from 99.06% to 1.94%, surpassing seven state-of-the-art backdoor removal techniques.



### From Universal Humanoid Control to Automatic Physically Valid Character Creation
- **Arxiv ID**: http://arxiv.org/abs/2206.09286v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.09286v1)
- **Published**: 2022-06-18 22:04:44+00:00
- **Updated**: 2022-06-18 22:04:44+00:00
- **Authors**: Zhengyi Luo, Ye Yuan, Kris M. Kitani
- **Comment**: Project page: https://zhengyiluo.github.io/projects/agent_design/
- **Journal**: None
- **Summary**: Automatically designing virtual humans and humanoids holds great potential in aiding the character creation process in games, movies, and robots. In some cases, a character creator may wish to design a humanoid body customized for certain motions such as karate kicks and parkour jumps. In this work, we propose a humanoid design framework to automatically generate physically valid humanoid bodies conditioned on sequence(s) of pre-specified human motions. First, we learn a generalized humanoid controller trained on a large-scale human motion dataset that features diverse human motion and body shapes. Second, we use a design-and-control framework to optimize a humanoid's physical attributes to find body designs that can better imitate the pre-specified human motion sequence(s). Leveraging the pre-trained humanoid controller and physics simulation as guidance, our method is able to discover new humanoid designs that are customized to perform pre-specified human motions.



### Rethinking Bayesian Deep Learning Methods for Semi-Supervised Volumetric Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.09293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.09293v1)
- **Published**: 2022-06-18 23:07:04+00:00
- **Updated**: 2022-06-18 23:07:04+00:00
- **Authors**: Jianfeng Wang, Thomas Lukasiewicz
- **Comment**: To appear at CVPR 2022, and the supplementary material can be found
  at the official site. The source codes are at
  https://github.com/Jianf-Wang/GBDL
- **Journal**: None
- **Summary**: Recently, several Bayesian deep learning methods have been proposed for semi-supervised medical image segmentation. Although they have achieved promising results on medical benchmarks, some problems are still existing. Firstly, their overall architectures belong to the discriminative models, and hence, in the early stage of training, they only use labeled data for training, which might make them overfit to the labeled data. Secondly, in fact, they are only partially based on Bayesian deep learning, as their overall architectures are not designed under the Bayesian framework. However, unifying the overall architecture under the Bayesian perspective can make the architecture have a rigorous theoretical basis, so that each part of the architecture can have a clear probabilistic interpretation. Therefore, to solve the problems, we propose a new generative Bayesian deep learning (GBDL) architecture. GBDL belongs to the generative models, whose target is to estimate the joint distribution of input medical volumes and their corresponding labels. Estimating the joint distribution implicitly involves the distribution of data, so both labeled and unlabeled data can be utilized in the early stage of training, which alleviates the potential overfitting problem. Besides, GBDL is completely designed under the Bayesian framework, and thus we give its full Bayesian formulation, which lays a theoretical probabilistic foundation for our architecture. Extensive experiments show that our GBDL outperforms previous state-of-the-art methods in terms of four commonly used evaluation indicators on three public medical datasets.



