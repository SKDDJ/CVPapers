# Arxiv Papers in cs.CV on 2022-06-27
### Multi-Scale Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.13028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13028v1)
- **Published**: 2022-06-27 03:17:33+00:00
- **Updated**: 2022-06-27 03:17:33+00:00
- **Authors**: Zhan Chen, Sicheng Li, Bing Yang, Qinghan Li, Hong Liu
- **Comment**: 10 pages, 4 figures, accepted by AAAI 2021
- **Journal**: None
- **Summary**: Graph convolutional networks have been widely used for skeleton-based action recognition due to their excellent modeling ability of non-Euclidean data. As the graph convolution is a local operation, it can only utilize the short-range joint dependencies and short-term trajectory but fails to directly model the distant joints relations and long-range temporal information that are vital to distinguishing various actions. To solve this problem, we present a multi-scale spatial graph convolution (MS-GC) module and a multi-scale temporal graph convolution (MT-GC) module to enrich the receptive field of the model in spatial and temporal dimensions. Concretely, the MS-GC and MT-GC modules decompose the corresponding local graph convolution into a set of sub-graph convolution, forming a hierarchical residual architecture. Without introducing additional parameters, the features will be processed with a series of sub-graph convolutions, and each node could complete multiple spatial and temporal aggregations with its neighborhoods. The final equivalent receptive field is accordingly enlarged, which is capable of capturing both short- and long-range dependencies in spatial and temporal domains. By coupling these two modules as a basic block, we further propose a multi-scale spatial temporal graph convolutional network (MST-GCN), which stacks multiple blocks to learn effective motion representations for action recognition. The proposed MST-GCN achieves remarkable performance on three challenging benchmark datasets, NTU RGB+D, NTU-120 RGB+D and Kinetics-Skeleton, for skeleton-based action recognition.



### A Strategy Optimized Pix2pix Approach for SAR-to-Optical Image Translation Task
- **Arxiv ID**: http://arxiv.org/abs/2206.13042v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13042v3)
- **Published**: 2022-06-27 04:41:37+00:00
- **Updated**: 2022-07-04 05:54:16+00:00
- **Authors**: Fujian Cheng, Yashu Kang, Chunlei Chen, Kezhao Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: This technical report summarizes the analysis and approach on the image-to-image translation task in the Multimodal Learning for Earth and Environment Challenge (MultiEarth 2022). In terms of strategy optimization, cloud classification is utilized to filter optical images with dense cloud coverage to aid the supervised learning alike approach. The commonly used pix2pix framework with a few optimizations is applied to build the model. A weighted combination of mean squared error and mean absolute error is incorporated in the loss function. As for evaluation, peak to signal ratio and structural similarity were both considered in our preliminary analysis. Lastly, our method achieved the second place with a final error score of 0.0412. The results indicate great potential towards SAR-to-optical translation in remote sensing tasks, specifically for the support of long-term environmental monitoring and protection.



### Deep Optical Coding Design in Computational Imaging
- **Arxiv ID**: http://arxiv.org/abs/2207.00164v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00164v2)
- **Published**: 2022-06-27 04:41:48+00:00
- **Updated**: 2022-08-17 14:48:39+00:00
- **Authors**: Henry Arguello, Jorge Bacca, Hasindu Kariyawasam, Edwin Vargas, Miguel Marquez, Ramith Hettiarachchi, Hans Garcia, Kithmini Herath, Udith Haputhanthri, Balpreet Singh Ahluwalia, Peter So, Dushan N. Wadduwage, Chamira U. S. Edussooriya
- **Comment**: None
- **Journal**: None
- **Summary**: Computational optical imaging (COI) systems leverage optical coding elements (CE) in their setups to encode a high-dimensional scene in a single or multiple snapshots and decode it by using computational algorithms. The performance of COI systems highly depends on the design of its main components: the CE pattern and the computational method used to perform a given task. Conventional approaches rely on random patterns or analytical designs to set the distribution of the CE. However, the available data and algorithm capabilities of deep neural networks (DNNs) have opened a new horizon in CE data-driven designs that jointly consider the optical encoder and computational decoder. Specifically, by modeling the COI measurements through a fully differentiable image formation model that considers the physics-based propagation of light and its interaction with the CEs, the parameters that define the CE and the computational decoder can be optimized in an end-to-end (E2E) manner. Moreover, by optimizing just CEs in the same framework, inference tasks can be performed from pure optics. This work surveys the recent advances on CE data-driven design and provides guidelines on how to parametrize different optical elements to include them in the E2E framework. Since the E2E framework can handle different inference applications by changing the loss function and the DNN, we present low-level tasks such as spectral imaging reconstruction or high-level tasks such as pose estimation with privacy preserving enhanced by using optimal task-based optical architectures. Finally, we illustrate classification and 3D object recognition applications performed at the speed of the light using all-optics DNN.



### Automated Systems For Diagnosis of Dysgraphia in Children: A Survey and Novel Framework
- **Arxiv ID**: http://arxiv.org/abs/2206.13043v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13043v1)
- **Published**: 2022-06-27 04:44:34+00:00
- **Updated**: 2022-06-27 04:44:34+00:00
- **Authors**: Jayakanth Kunhoth, Somaya Al-Maadeed, Suchithra Kunhoth, Younus Akbari
- **Comment**: None
- **Journal**: None
- **Summary**: Learning disabilities, which primarily interfere with the basic learning skills such as reading, writing and math, are known to affect around 10% of children in the world. The poor motor skills and motor coordination as part of the neurodevelopmental disorder can become a causative factor for the difficulty in learning to write (dysgraphia), hindering the academic track of an individual. The signs and symptoms of dysgraphia include but are not limited to irregular handwriting, improper handling of writing medium, slow or labored writing, unusual hand position, etc. The widely accepted assessment criterion for all the types of learning disabilities is the examination performed by medical experts. The few available artificial intelligence-powered screening systems for dysgraphia relies on the distinctive features of handwriting from the corresponding images.This work presents a review of the existing automated dysgraphia diagnosis systems for children in the literature. The main focus of the work is to review artificial intelligence-based systems for dysgraphia diagnosis in children. This work discusses the data collection method, important handwriting features, machine learning algorithms employed in the literature for the diagnosis of dysgraphia. Apart from that, this article discusses some of the non-artificial intelligence-based automated systems also. Furthermore, this article discusses the drawbacks of existing systems and proposes a novel framework for dysgraphia diagnosis.



### SearchMorph:Multi-scale Correlation Iterative Network for Deformable Registration
- **Arxiv ID**: http://arxiv.org/abs/2206.13076v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13076v3)
- **Published**: 2022-06-27 06:37:02+00:00
- **Updated**: 2022-07-18 13:52:37+00:00
- **Authors**: Xiao Fan, Shuxin Zhuang, Zhemin Zhuang, Ye Yuan, Shunmin Qiu, Alex Noel Joseph Raj, Yibiao Rong
- **Comment**: None
- **Journal**: None
- **Summary**: Deformable image registration can obtain dynamic information about images, which is of great significance in medical image analysis. The unsupervised deep learning registration method can quickly achieve high registration accuracy without labels. However, these methods generally suffer from uncorrelated features, poor ability to register large deformations and details, and unnatural deformation fields. To address the issues above, we propose an unsupervised multi-scale correlation iterative registration network (SearchMorph). In the proposed network, we introduce a correlation layer to strengthen the relevance between features and construct a correlation pyramid to provide multi-scale relevance information for the network. We also design a deformation field iterator, which improves the ability of the model to register details and large deformations through the search module and GRU while ensuring that the deformation field is realistic. We use single-temporal brain MR images and multi-temporal echocardiographic sequences to evaluate the model's ability to register large deformations and details. The experimental results demonstrate that the method in this paper achieves the highest registration accuracy and the lowest folding point ratio using a short elapsed time to state-of-the-art.



### Video2StyleGAN: Encoding Video in Latent Space for Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2206.13078v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13078v1)
- **Published**: 2022-06-27 06:48:15+00:00
- **Updated**: 2022-06-27 06:48:15+00:00
- **Authors**: Jiyang Yu, Jingen Liu, Jing Huang, Wei Zhang, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Many recent works have been proposed for face image editing by leveraging the latent space of pretrained GANs. However, few attempts have been made to directly apply them to videos, because 1) they do not guarantee temporal consistency, 2) their application is limited by their processing speed on videos, and 3) they cannot accurately encode details of face motion and expression. To this end, we propose a novel network to encode face videos into the latent space of StyleGAN for semantic face video manipulation. Based on the vision transformer, our network reuses the high-resolution portion of the latent vector to enforce temporal consistency. To capture subtle face motions and expressions, we design novel losses that involve sparse facial landmarks and dense 3D face mesh. We have thoroughly evaluated our approach and successfully demonstrated its application to various face video manipulations. Particularly, we propose a novel network for pose/expression control in a 3D coordinate system. Both qualitative and quantitative results have shown that our approach can significantly outperform existing single image methods, while achieving real-time (66 fps) speed.



### Dynamic Bank Learning for Semi-supervised Federated Image Diagnosis with Class Imbalance
- **Arxiv ID**: http://arxiv.org/abs/2206.13079v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.13079v1)
- **Published**: 2022-06-27 06:51:48+00:00
- **Updated**: 2022-06-27 06:51:48+00:00
- **Authors**: Meirui Jiang, Hongzheng Yang, Xiaoxiao Li, Quande Liu, Pheng-Ann Heng, Qi Dou
- **Comment**: Early accepted by 25th International Conference on Medical Image
  Computing and Computer Assisted Intervention (MICCAI'22)
- **Journal**: None
- **Summary**: Despite recent progress on semi-supervised federated learning (FL) for medical image diagnosis, the problem of imbalanced class distributions among unlabeled clients is still unsolved for real-world use. In this paper, we study a practical yet challenging problem of class imbalanced semi-supervised FL (imFed-Semi), which allows all clients to have only unlabeled data while the server just has a small amount of labeled data. This imFed-Semi problem is addressed by a novel dynamic bank learning scheme, which improves client training by exploiting class proportion information. This scheme consists of two parts, i.e., the dynamic bank construction to distill various class proportions for each local client, and the sub-bank classification to impose the local model to learn different class proportions. We evaluate our approach on two public real-world medical datasets, including the intracranial hemorrhage diagnosis with 25,000 CT slices and skin lesion diagnosis with 10,015 dermoscopy images. The effectiveness of our method has been validated with significant performance improvements (7.61% and 4.69%) compared with the second-best on the accuracy, as well as comprehensive analytical studies. Code is available at https://github.com/med-air/imFedSemi.



### PST: Plant Segmentation Transformer for 3D Point Clouds of rapeseed plants at the podding stage
- **Arxiv ID**: http://arxiv.org/abs/2206.13082v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13082v2)
- **Published**: 2022-06-27 06:56:48+00:00
- **Updated**: 2022-08-07 10:07:27+00:00
- **Authors**: Ruiming Du, Zhihong Ma, Pengyao Xie, Yong He, Haiyan Cen
- **Comment**: 44 pages, 10 figures
- **Journal**: None
- **Summary**: Segmentation of plant point clouds to obtain high-precise morphological traits is essential for plant phenotyping. Although the fast development of deep learning has boosted much research on segmentation of plant point clouds, previous studies mainly focus on the hard voxelization-based or down-sampling-based methods, which are limited to segmenting simple plant organs. Segmentation of complex plant point clouds with a high spatial resolution still remains challenging. In this study, we proposed a deep learning network plant segmentation transformer (PST) to achieve the semantic and instance segmentation of rapeseed plants point clouds acquired by handheld laser scanning (HLS) with the high spatial resolution, which can characterize the tiny siliques as the main traits targeted. PST is composed of: (i) a dynamic voxel feature encoder (DVFE) to aggregate the point features with the raw spatial resolution; (ii) the dual window sets attention blocks to capture the contextual information; and (iii) a dense feature propagation module to obtain the final dense point feature map. The results proved that PST and PST-PointGroup (PG) achieved superior performance in semantic and instance segmentation tasks. For the semantic segmentation, the mean IoU, mean Precision, mean Recall, mean F1-score, and overall accuracy of PST were 93.96%, 97.29%, 96.52%, 96.88%, and 97.07%, achieving an improvement of 7.62%, 3.28%, 4.8%, 4.25%, and 3.88% compared to the second-best state-of-the-art network PAConv. For instance segmentation, PST-PG reached 89.51%, 89.85%, 88.83% and 82.53% in mCov, mWCov, mPerc90, and mRec90, achieving an improvement of 2.93%, 2.21%, 1.99%, and 5.9% compared to the original PG. This study proves that the deep-learning-based point cloud segmentation method has a great potential for resolving dense plant point clouds with complex morphological traits.



### RankSEG: A Consistent Ranking-based Framework for Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.13086v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, math.ST, stat.TH, 62C05, 62C12, G.3; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2206.13086v2)
- **Published**: 2022-06-27 07:12:31+00:00
- **Updated**: 2023-05-24 09:07:43+00:00
- **Authors**: Ben Dai, Chunlin Li
- **Comment**: 50 pages
- **Journal**: None
- **Summary**: Segmentation has emerged as a fundamental field of computer vision and natural language processing, which assigns a label to every pixel/feature to extract regions of interest from an image/text. To evaluate the performance of segmentation, the Dice and IoU metrics are used to measure the degree of overlap between the ground truth and the predicted segmentation. In this paper, we establish a theoretical foundation of segmentation with respect to the Dice/IoU metrics, including the Bayes rule and Dice-/IoU-calibration, analogous to classification-calibration or Fisher consistency in classification. We prove that the existing thresholding-based framework with most operating losses are not consistent with respect to the Dice/IoU metrics, and thus may lead to a suboptimal solution. To address this pitfall, we propose a novel consistent ranking-based framework, namely RankDice/RankIoU, inspired by plug-in rules of the Bayes segmentation rule. Three numerical algorithms with GPU parallel execution are developed to implement the proposed framework in large-scale and high-dimensional segmentation. We study statistical properties of the proposed framework. We show it is Dice-/IoU-calibrated, and its excess risk bounds and the rate of convergence are also provided. The numerical effectiveness of RankDice/mRankDice is demonstrated in various simulated examples and Fine-annotated CityScapes, Pascal VOC and Kvasir-SEG datasets with state-of-the-art deep learning architectures.



### Lesion-Aware Contrastive Representation Learning for Histopathology Whole Slide Images Analysis
- **Arxiv ID**: http://arxiv.org/abs/2206.13115v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13115v1)
- **Published**: 2022-06-27 08:39:51+00:00
- **Updated**: 2022-06-27 08:39:51+00:00
- **Authors**: Jun Li, Yushan Zheng, Kun Wu, Jun Shi, Fengying Xie, Zhiguo Jiang
- **Comment**: accepted for MICCAI 2022
- **Journal**: None
- **Summary**: Local representation learning has been a key challenge to promote the performance of the histopathological whole slide images analysis. The previous representation learning methods followed the supervised learning paradigm. However, manual annotation for large-scale WSIs is time-consuming and labor-intensive. Hence, the self-supervised contrastive learning has recently attracted intensive attention. The present contrastive learning methods treat each sample as a single class, which suffers from class collision problems, especially in the domain of histopathology image analysis. In this paper, we proposed a novel contrastive representation learning framework named Lesion-Aware Contrastive Learning (LACL) for histopathology whole slide image analysis. We built a lesion queue based on the memory bank structure to store the representations of different classes of WSIs, which allowed the contrastive model to selectively define the negative pairs during the training. Moreover, We designed a queue refinement strategy to purify the representations stored in the lesion queue. The experimental results demonstrate that LACL achieves the best performance in histopathology image representation learning on different datasets, and outperforms state-of-the-art methods under different WSI classification benchmarks. The code is available at https://github.com/junl21/lacl.



### SARNet: Semantic Augmented Registration of Large-Scale Urban Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2206.13117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13117v1)
- **Published**: 2022-06-27 08:49:11+00:00
- **Updated**: 2022-06-27 08:49:11+00:00
- **Authors**: Chao Liu, Jianwei Guo, Dong-Ming Yan, Zhirong Liang, Xiaopeng Zhang, Zhanglin Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Registering urban point clouds is a quite challenging task due to the large-scale, noise and data incompleteness of LiDAR scanning data. In this paper, we propose SARNet, a novel semantic augmented registration network aimed at achieving efficient registration of urban point clouds at city scale. Different from previous methods that construct correspondences only in the point-level space, our approach fully exploits semantic features as assistance to improve registration accuracy. Specifically, we extract per-point semantic labels with advanced semantic segmentation networks and build a prior semantic part-to-part correspondence. Then we incorporate the semantic information into a learning-based registration pipeline, consisting of three core modules: a semantic-based farthest point sampling module to efficiently filter out outliers and dynamic objects; a semantic-augmented feature extraction module for learning more discriminative point descriptors; a semantic-refined transformation estimation module that utilizes prior semantic matching as a mask to refine point correspondences by reducing false matching for better convergence. We evaluate the proposed SARNet extensively by using real-world data from large regions of urban scenes and comparing it with alternative methods. The code is available at https://github.com/WinterCodeForEverything/SARNet.



### Unsupervised Domain Adaptation Using Feature Disentanglement And GCNs For Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.13123v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13123v1)
- **Published**: 2022-06-27 09:02:16+00:00
- **Updated**: 2022-06-27 09:02:16+00:00
- **Authors**: Dwarikanath Mahapatra
- **Comment**: None
- **Journal**: None
- **Summary**: The success of deep learning has set new benchmarks for many medical image analysis tasks. However, deep models often fail to generalize in the presence of distribution shifts between training (source) data and test (target) data. One method commonly employed to counter distribution shifts is domain adaptation: using samples from the target domain to learn to account for shifted distributions. In this work we propose an unsupervised domain adaptation approach that uses graph neural networks and, disentangled semantic and domain invariant structural features, allowing for better performance across distribution shifts. We propose an extension to swapped autoencoders to obtain more discriminative features. We test the proposed method for classification on two challenging medical image datasets with distribution shifts - multi center chest Xray images and histopathology images. Experiments show our method achieves state-of-the-art results compared to other domain adaptation methods.



### Representing motion as a sequence of latent primitives, a flexible approach for human motion modelling
- **Arxiv ID**: http://arxiv.org/abs/2206.13142v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13142v2)
- **Published**: 2022-06-27 09:43:07+00:00
- **Updated**: 2022-09-01 15:23:10+00:00
- **Authors**: Mathieu Marsot, Stefanie Wuhrer, Jean-Sebastien Franco, Anne Hélène Olivier
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new representation of human body motion which encodes a full motion in a sequence of latent motion primitives. Recently, task generic motion priors have been introduced and propose a coherent representation of human motion based on a single latent code, with encouraging results for many tasks. Extending these methods to longer motion with various duration and framerate is all but straightforward as one latent code proves inefficient to encode longer term variability. Our hypothesis is that long motions are better represented as a succession of actions than in a single block. By leveraging a sequence-to-sequence architecture, we propose a model that simultaneously learns a temporal segmentation of motion and a prior on the motion segments. To provide flexibility with temporal resolution and motion duration, our representation is continuous in time and can be queried for any timestamp. We show experimentally that our method leads to a significant improvement over state-of-the-art motion priors on a spatio-temporal completion task on sparse pointclouds. Code will be made available upon publication.



### Bi-VLDoc: Bidirectional Vision-Language Modeling for Visually-Rich Document Understanding
- **Arxiv ID**: http://arxiv.org/abs/2206.13155v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2206.13155v1)
- **Published**: 2022-06-27 09:58:34+00:00
- **Updated**: 2022-06-27 09:58:34+00:00
- **Authors**: Chuwei Luo, Guozhi Tang, Qi Zheng, Cong Yao, Lianwen Jin, Chenliang Li, Yang Xue, Luo Si
- **Comment**: Under review
- **Journal**: None
- **Summary**: Multi-modal document pre-trained models have proven to be very effective in a variety of visually-rich document understanding (VrDU) tasks. Though existing document pre-trained models have achieved excellent performance on standard benchmarks for VrDU, the way they model and exploit the interactions between vision and language on documents has hindered them from better generalization ability and higher accuracy. In this work, we investigate the problem of vision-language joint representation learning for VrDU mainly from the perspective of supervisory signals. Specifically, a pre-training paradigm called Bi-VLDoc is proposed, in which a bidirectional vision-language supervision strategy and a vision-language hybrid-attention mechanism are devised to fully explore and utilize the interactions between these two modalities, to learn stronger cross-modal document representations with richer semantics. Benefiting from the learned informative cross-modal document representations, Bi-VLDoc significantly advances the state-of-the-art performance on three widely-used document understanding benchmarks, including Form Understanding (from 85.14% to 93.44%), Receipt Information Extraction (from 96.01% to 97.84%), and Document Classification (from 96.08% to 97.12%). On Document Visual QA, Bi-VLDoc achieves the state-of-the-art performance compared to previous single model methods.



### Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.13156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13156v1)
- **Published**: 2022-06-27 10:00:12+00:00
- **Updated**: 2022-06-27 10:00:12+00:00
- **Authors**: Yushan Zheng, Jun Li, Jun Shi, Fengying Xie, Zhiguo Jiang
- **Comment**: accepted for MICCAI 2022
- **Journal**: None
- **Summary**: Transformer has been widely used in histopathology whole slide image (WSI) classification for the purpose of tumor grading, prognosis analysis, etc. However, the design of token-wise self-attention and positional embedding strategy in the common Transformer limits the effectiveness and efficiency in the application to gigapixel histopathology images. In this paper, we propose a kernel attention Transformer (KAT) for histopathology WSI classification. The information transmission of the tokens is achieved by cross-attention between the tokens and a set of kernels related to a set of positional anchors on the WSI. Compared to the common Transformer structure, the proposed KAT can better describe the hierarchical context information of the local regions of the WSI and meanwhile maintains a lower computational complexity. The proposed method was evaluated on a gastric dataset with 2040 WSIs and an endometrial dataset with 2560 WSIs, and was compared with 6 state-of-the-art methods. The experimental results have demonstrated the proposed KAT is effective and efficient in the task of histopathology WSI classification and is superior to the state-of-the-art methods. The code is available at https://github.com/zhengyushan/kat.



### Context-Aware Transformers For Spinal Cancer Detection and Radiological Grading
- **Arxiv ID**: http://arxiv.org/abs/2206.13173v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13173v1)
- **Published**: 2022-06-27 10:31:03+00:00
- **Updated**: 2022-06-27 10:31:03+00:00
- **Authors**: Rhydian Windsor, Amir Jamaludin, Timor Kadir, Andrew Zisserman
- **Comment**: Pre-print of paper accepted to MICCAI 2022. 15 pages, 7 figures
- **Journal**: None
- **Summary**: This paper proposes a novel transformer-based model architecture for medical imaging problems involving analysis of vertebrae. It considers two applications of such models in MR images: (a) detection of spinal metastases and the related conditions of vertebral fractures and metastatic cord compression, (b) radiological grading of common degenerative changes in intervertebral discs. Our contributions are as follows: (i) We propose a Spinal Context Transformer (SCT), a deep-learning architecture suited for the analysis of repeated anatomical structures in medical imaging such as vertebral bodies (VBs). Unlike previous related methods, SCT considers all VBs as viewed in all available image modalities together, making predictions for each based on context from the rest of the spinal column and all available imaging modalities. (ii) We apply the architecture to a novel and important task: detecting spinal metastases and the related conditions of cord compression and vertebral fractures/collapse from multi-series spinal MR scans. This is done using annotations extracted from free-text radiological reports as opposed to bespoke annotation. However, the resulting model shows strong agreement with vertebral-level bespoke radiologist annotations on the test set. (iii) We also apply SCT to an existing problem: radiological grading of inter-vertebral discs (IVDs) in lumbar MR scans for common degenerative changes.We show that by considering the context of vertebral bodies in the image, SCT improves the accuracy for several gradings compared to previously published model.



### Discrete Morse Sandwich: Fast Computation of Persistence Diagrams for Scalar Data -- An Algorithm and A Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2206.13932v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CG, cs.CV, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13932v2)
- **Published**: 2022-06-27 10:54:24+00:00
- **Updated**: 2023-01-13 11:08:47+00:00
- **Authors**: Pierre Guillou, Jules Vidal, Julien Tierny
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces an efficient algorithm for persistence diagram computation, given an input piecewise linear scalar field $f$ defined on a $d$-dimensional simplicial complex $K$, with $d \leq 3$. Our work revisits the seminal algorithm "PairSimplices" [31], [103] with discrete Morse theory (DMT) [34], [80], which greatly reduces the number of input simplices to consider. Further, we also extend to DMT and accelerate the stratification strategy described in "PairSimplices" for the fast computation of the $0^{th}$ and $(d - 1)^{th}$ diagrams, noted $D_0(f)$ and $D_{d-1}(f)$. Minima-saddle persistence pairs ($D_0(f)$) and saddle-maximum persistence pairs ($D_{d-1}(f)$) are efficiently computed by processing, with a Union-Find, the unstable sets of $1$-saddles and the stable sets of $(d - 1)$-saddles. This fast pre-computation for the dimensions $0$ and $(d - 1)$ enables an aggressive specialization of [4] to the 3D case, which results in a drastic reduction of the number of input simplices for the computation of $D_1(f)$, the intermediate layer of the sandwich. Finally, we document several performance improvements via shared-memory parallelism. We provide an open-source implementation of our algorithm for reproducibility purposes. We also contribute a reproducible benchmark package, which exploits three-dimensional data from a public repository and compares our algorithm to a variety of publicly available implementations. Extensive experiments indicate that our algorithm improves by two orders of magnitude the time performance of the seminal "PairSimplices" algorithm it extends. Moreover, it also improves memory footprint and time performance over a selection of 14 competing approaches, with a substantial gain over the fastest available approaches, while producing a strictly identical output.



### Self-supervised Learning in Remote Sensing: A Review
- **Arxiv ID**: http://arxiv.org/abs/2206.13188v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13188v2)
- **Published**: 2022-06-27 11:04:47+00:00
- **Updated**: 2022-09-02 08:48:34+00:00
- **Authors**: Yi Wang, Conrad M Albrecht, Nassim Ait Ali Braham, Lichao Mou, Xiao Xiang Zhu
- **Comment**: Accepted by IEEE Geoscience and Remote Sensing Magazine. 32 pages, 22
  content pages
- **Journal**: None
- **Summary**: In deep learning research, self-supervised learning (SSL) has received great attention triggering interest within both the computer vision and remote sensing communities. While there has been a big success in computer vision, most of the potential of SSL in the domain of earth observation remains locked. In this paper, we provide an introduction to, and a review of the concepts and latest developments in SSL for computer vision in the context of remote sensing. Further, we provide a preliminary benchmark of modern SSL algorithms on popular remote sensing datasets, verifying the potential of SSL in remote sensing and providing an extended study on data augmentations. Finally, we identify a list of promising directions of future research in SSL for earth observation (SSL4EO) to pave the way for fruitful interaction of both domains.



### MGNet: Monocular Geometric Scene Understanding for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2206.13199v1
- **DOI**: 10.1109/ICCV48922.2021.01551
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13199v1)
- **Published**: 2022-06-27 11:27:55+00:00
- **Updated**: 2022-06-27 11:27:55+00:00
- **Authors**: Markus Schön, Michael Buchholz, Klaus Dietmayer
- **Comment**: None
- **Journal**: 2021 IEEE/CVF International Conference on Computer Vision (ICCV),
  2021, pp. 15784-15795
- **Summary**: We introduce MGNet, a multi-task framework for monocular geometric scene understanding. We define monocular geometric scene understanding as the combination of two known tasks: Panoptic segmentation and self-supervised monocular depth estimation. Panoptic segmentation captures the full scene not only semantically, but also on an instance basis. Self-supervised monocular depth estimation uses geometric constraints derived from the camera measurement model in order to measure depth from monocular video sequences only. To the best of our knowledge, we are the first to propose the combination of these two tasks in one single model. Our model is designed with focus on low latency to provide fast inference in real-time on a single consumer-grade GPU. During deployment, our model produces dense 3D point clouds with instance aware semantic labels from single high-resolution camera images. We evaluate our model on two popular autonomous driving benchmarks, i.e., Cityscapes and KITTI, and show competitive performance among other real-time capable methods. Source code is available at https://github.com/markusschoen/MGNet.



### Learning with Weak Annotations for Robust Maritime Obstacle Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.13263v2
- **DOI**: 10.3390/s22239139
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13263v2)
- **Published**: 2022-06-27 12:52:26+00:00
- **Updated**: 2022-11-25 11:33:11+00:00
- **Authors**: Lojze Žust, Matej Kristan
- **Comment**: Published in MDPI Sensors, 23 pages, 8 figures
- **Journal**: Sensors 2022, 22, 9139
- **Summary**: Robust maritime obstacle detection is critical for safe navigation of autonomous boats and timely collision avoidance. The current state-of-the-art is based on deep segmentation networks trained on large datasets. However, per-pixel ground truth labeling of such datasets is labor-intensive and expensive. We propose a new scaffolding learning regime (SLR) that leverages weak annotations consisting of water edges, the horizon location, and obstacle bounding boxes to train segmentation-based obstacle detection networks, thereby reducing the required ground truth labeling effort by a factor of twenty. SLR trains an initial model from weak annotations and then alternates between re-estimating the segmentation pseudo-labels and improving the network parameters. Experiments show that maritime obstacle segmentation networks trained using SLR on weak annotations not only match but outperform the same networks trained with dense ground truth labels, which is a remarkable result. In addition to the increased accuracy, SLR also increases domain generalization and can be used for domain adaptation with a low manual annotation load. The SLR code and pre-trained models are available at https://github.com/lojzezust/SLR .



### Monocular Depth Decomposition of Semi-Transparent Volume Renderings
- **Arxiv ID**: http://arxiv.org/abs/2206.13282v2
- **DOI**: 10.1109/TVCG.2023.3245305
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13282v2)
- **Published**: 2022-06-27 13:18:02+00:00
- **Updated**: 2023-02-18 13:35:30+00:00
- **Authors**: Dominik Engel, Sebastian Hartwig, Timo Ropinski
- **Comment**: accepted at IEEE TVCG 2023
- **Journal**: None
- **Summary**: Neural networks have shown great success in extracting geometric information from color images. Especially, monocular depth estimation networks are increasingly reliable in real-world scenes. In this work we investigate the applicability of such monocular depth estimation networks to semi-transparent volume rendered images. As depth is notoriously difficult to define in a volumetric scene without clearly defined surfaces, we consider different depth computations that have emerged in practice, and compare state-of-the-art monocular depth estimation approaches for these different interpretations during an evaluation considering different degrees of opacity in the renderings. Additionally, we investigate how these networks can be extended to further obtain color and opacity information, in order to create a layered representation of the scene based on a single color image. This layered representation consists of spatially separated semi-transparent intervals that composite to the original input rendering. In our experiments we show that existing approaches to monocular depth estimation can be adapted to perform well on semi-transparent volume renderings, which has several applications in the area of scientific visualization, like re-composition with additional objects and labels or additional shading.



### LaRa: Latents and Rays for Multi-Camera Bird's-Eye-View Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.13294v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2206.13294v2)
- **Published**: 2022-06-27 13:37:50+00:00
- **Updated**: 2022-11-26 14:56:22+00:00
- **Authors**: Florent Bartoccioni, Éloi Zablocki, Andrei Bursuc, Patrick Pérez, Matthieu Cord, Karteek Alahari
- **Comment**: None
- **Journal**: CoRL 2022 https://openreview.net/forum?id=abd_D-iVjk0
- **Summary**: Recent works in autonomous driving have widely adopted the bird's-eye-view (BEV) semantic map as an intermediate representation of the world. Online prediction of these BEV maps involves non-trivial operations such as multi-camera data extraction as well as fusion and projection into a common topview grid. This is usually done with error-prone geometric operations (e.g., homography or back-projection from monocular depth estimation) or expensive direct dense mapping between image pixels and pixels in BEV (e.g., with MLP or attention). In this work, we present 'LaRa', an efficient encoder-decoder, transformer-based model for vehicle semantic segmentation from multiple cameras. Our approach uses a system of cross-attention to aggregate information over multiple sensors into a compact, yet rich, collection of latent representations. These latent representations, after being processed by a series of self-attention blocks, are then reprojected with a second cross-attention in the BEV space. We demonstrate that our model outperforms the best previous works using transformers on nuScenes. The code and trained models are available at https://github.com/valeoai/LaRa



### Diffusion Deformable Model for 4D Temporal Medical Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2206.13295v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13295v1)
- **Published**: 2022-06-27 13:37:57+00:00
- **Updated**: 2022-06-27 13:37:57+00:00
- **Authors**: Boah Kim, Jong Chul Ye
- **Comment**: Accepted for MICCAI 2022
- **Journal**: None
- **Summary**: Temporal volume images with 3D+t (4D) information are often used in medical imaging to statistically analyze temporal dynamics or capture disease progression. Although deep-learning-based generative models for natural images have been extensively studied, approaches for temporal medical image generation such as 4D cardiac volume data are limited. In this work, we present a novel deep learning model that generates intermediate temporal volumes between source and target volumes. Specifically, we propose a diffusion deformable model (DDM) by adapting the denoising diffusion probabilistic model that has recently been widely investigated for realistic image generation. Our proposed DDM is composed of the diffusion and the deformation modules so that DDM can learn spatial deformation information between the source and target volumes and provide a latent code for generating intermediate frames along a geodesic path. Once our model is trained, the latent code estimated from the diffusion module is simply interpolated and fed into the deformation module, which enables DDM to generate temporal frames along the continuous trajectory while preserving the topology of the source image. We demonstrate the proposed method with the 4D cardiac MR image generation between the diastolic and systolic phases for each subject. Compared to the existing deformation methods, our DDM achieves high performance on temporal volume generation.



### Consistency-preserving Visual Question Answering in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2206.13296v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13296v1)
- **Published**: 2022-06-27 13:38:50+00:00
- **Updated**: 2022-06-27 13:38:50+00:00
- **Authors**: Sergio Tascon-Morales, Pablo Márquez-Neila, Raphael Sznitman
- **Comment**: Appears in Medical Image Computing and Computer Assisted
  Interventions (MICCAI), 2022
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) models take an image and a natural-language question as input and infer the answer to the question. Recently, VQA systems in medical imaging have gained popularity thanks to potential advantages such as patient engagement and second opinions for clinicians. While most research efforts have been focused on improving architectures and overcoming data-related limitations, answer consistency has been overlooked even though it plays a critical role in establishing trustworthy models. In this work, we propose a novel loss function and corresponding training procedure that allows the inclusion of relations between questions into the training process. Specifically, we consider the case where implications between perception and reasoning questions are known a-priori. To show the benefits of our approach, we evaluate it on the clinically relevant task of Diabetic Macular Edema (DME) staging from fundus imaging. Our experiments show that our method outperforms state-of-the-art baselines, not only by improving model consistency, but also in terms of overall model accuracy. Our code and data are available at https://github.com/sergiotasconmorales/consistency_vqa.



### PARTICUL: Part Identification with Confidence measure using Unsupervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.13304v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13304v1)
- **Published**: 2022-06-27 13:44:49+00:00
- **Updated**: 2022-06-27 13:44:49+00:00
- **Authors**: Romain Xu-Darme, Georges Quénot, Zakaria Chihani, Marie-Christine Rousset
- **Comment**: Accepted at XAIE: 2nd Workshop on Explainable and Ethical AI -- ICPR
  2022
- **Journal**: None
- **Summary**: In this paper, we present PARTICUL, a novel algorithm for unsupervised learning of part detectors from datasets used in fine-grained recognition. It exploits the macro-similarities of all images in the training set in order to mine for recurring patterns in the feature space of a pre-trained convolutional neural network. We propose new objective functions enforcing the locality and unicity of the detected parts. Additionally, we embed our detectors with a confidence measure based on correlation scores, allowing the system to estimate the visibility of each part. We apply our method on two public fine-grained datasets (Caltech-UCSD Bird 200 and Stanford Cars) and show that our detectors can consistently highlight parts of the object while providing a good measure of the confidence in their prediction. We also demonstrate that these detectors can be directly used to build part-based fine-grained classifiers that provide a good compromise between the transparency of prototype-based approaches and the performance of non-interpretable methods.



### Automatic identification of segmentation errors for radiotherapy using geometric learning
- **Arxiv ID**: http://arxiv.org/abs/2206.13317v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.13317v1)
- **Published**: 2022-06-27 14:01:52+00:00
- **Updated**: 2022-06-27 14:01:52+00:00
- **Authors**: Edward G. A. Henderson, Andrew F. Green, Marcel van Herk, Eliana M. Vasquez Osorio
- **Comment**: Accepted in 25th International Conference on Medical Image Computing
  and Computer Assisted Intervention (MICCAI 2022). This preprint has not
  undergone peer review or any post-submission improvements or corrections
- **Journal**: None
- **Summary**: Automatic segmentation of organs-at-risk (OARs) in CT scans using convolutional neural networks (CNNs) is being introduced into the radiotherapy workflow. However, these segmentations still require manual editing and approval by clinicians prior to clinical use, which can be time consuming. The aim of this work was to develop a tool to automatically identify errors in 3D OAR segmentations without a ground truth. Our tool uses a novel architecture combining a CNN and graph neural network (GNN) to leverage the segmentation's appearance and shape. The proposed model is trained using self-supervised learning using a synthetically-generated dataset of segmentations of the parotid and with realistic contouring errors. The effectiveness of our model is assessed with ablation tests, evaluating the efficacy of different portions of the architecture as well as the use of transfer learning from an unsupervised pretext task. Our best performing model predicted errors on the parotid gland with a precision of 85.0% & 89.7% for internal and external errors respectively, and recall of 66.5% & 68.6%. This offline QA tool could be used in the clinical pathway, potentially decreasing the time clinicians spend correcting contours by detecting regions which require their attention. All our code is publicly available at https://github.com/rrr-uom-projects/contour_auto_QATool.



### Key-frame Guided Network for Thyroid Nodule Recognition using Ultrasound Videos
- **Arxiv ID**: http://arxiv.org/abs/2206.13318v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13318v3)
- **Published**: 2022-06-27 14:03:26+00:00
- **Updated**: 2022-06-30 04:01:12+00:00
- **Authors**: Yuchen Wang, Zhongyu Li, Xiangxiang Cui, Liangliang Zhang, Xiang Luo, Meng Yang, Shi Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound examination is widely used in the clinical diagnosis of thyroid nodules (benign/malignant). However, the accuracy relies heavily on radiologist experience. Although deep learning techniques have been investigated for thyroid nodules recognition. Current solutions are mainly based on static ultrasound images, with limited temporal information used and inconsistent with clinical diagnosis. This paper proposes a novel method for the automated recognition of thyroid nodules through an exhaustive exploration of ultrasound videos and key-frames. We first propose a detection-localization framework to automatically identify the clinical key-frame with a typical nodule in each ultrasound video. Based on the localized key-frame, we develop a key-frame guided video classification model for thyroid nodule recognition. Besides, we introduce a motion attention module to help the network focus on significant frames in an ultrasound video, which is consistent with clinical diagnosis. The proposed thyroid nodule recognition framework is validated on clinically collected ultrasound videos, demonstrating superior performance compared with other state-of-the-art methods.



### Prior-Guided One-shot Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2206.13329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13329v1)
- **Published**: 2022-06-27 14:19:56+00:00
- **Updated**: 2022-06-27 14:19:56+00:00
- **Authors**: Peijie Dong, Xin Niu, Lujun Li, Linzhen Xie, Wenbin Zou, Tian Ye, Zimian Wei, Hengyue Pan
- **Comment**: Official 3st Place Solution for the Second workshop Neural
  Architecture Search Second lightweight NAS Challenge 2022 - Track1 Supernet
  Track. Official leaderboard:
  https://aistudio.baidu.com/aistudio/competition/detail/149/0/leaderboard CVPR
  2022 Workshop: https://cvpr-nas.com/competition
- **Journal**: None
- **Summary**: Neural architecture search methods seek optimal candidates with efficient weight-sharing supernet training. However, recent studies indicate poor ranking consistency about the performance between stand-alone architectures and shared-weight networks. In this paper, we present Prior-Guided One-shot NAS (PGONAS) to strengthen the ranking correlation of supernets. Specifically, we first explore the effect of activation functions and propose a balanced sampling strategy based on the Sandwich Rule to alleviate weight coupling in the supernet. Then, FLOPs and Zen-Score are adopted to guide the training of supernet with ranking correlation loss. Our PGONAS ranks 3rd place in the supernet Track Track of CVPR2022 Second lightweight NAS challenge. Code is available in https://github.com/pprp/CVPR2022-NAS?competition-Track1-3th-solution.



### Distributional Gaussian Processes Layers for Out-of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.13346v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2206.13346v1)
- **Published**: 2022-06-27 14:49:48+00:00
- **Updated**: 2022-06-27 14:49:48+00:00
- **Authors**: Sebastian G. Popescu, David J. Sharp, James H. Cole, Konstantinos Kamnitsas, Ben Glocker
- **Comment**: Published in Journal of Machine Learning for Biomedical Imaging:
  Special Issue: Information Processing in Medical Imaging (IPMI) 2021
- **Journal**: None
- **Summary**: Machine learning models deployed on medical imaging tasks must be equipped with out-of-distribution detection capabilities in order to avoid erroneous predictions. It is unsure whether out-of-distribution detection models reliant on deep neural networks are suitable for detecting domain shifts in medical imaging. Gaussian Processes can reliably separate in-distribution data points from out-of-distribution data points via their mathematical construction. Hence, we propose a parameter efficient Bayesian layer for hierarchical convolutional Gaussian Processes that incorporates Gaussian Processes operating in Wasserstein-2 space to reliably propagate uncertainty. This directly replaces convolving Gaussian Processes with a distance-preserving affine operator on distributions. Our experiments on brain tissue-segmentation show that the resulting architecture approaches the performance of well-established deterministic segmentation algorithms (U-Net), which has not been achieved with previous hierarchical Gaussian Processes. Moreover, by applying the same segmentation model to out-of-distribution data (i.e., images with pathology such as brain tumors), we show that our uncertainty estimates result in out-of-distribution detection that outperforms the capabilities of previous Bayesian networks and reconstruction-based approaches that learn normative distributions. To facilitate future work our code is publicly available.



### iExam: A Novel Online Exam Monitoring and Analysis System Based on Face Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.13356v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13356v1)
- **Published**: 2022-06-27 15:03:25+00:00
- **Updated**: 2022-06-27 15:03:25+00:00
- **Authors**: Xu Yang, Daoyuan Wu, Xiao Yi, Jimmy H. M. Lee, Tan Lee
- **Comment**: This is a technical report from the Chinese University of Hong Kong
- **Journal**: None
- **Summary**: Online exams via video conference software like Zoom have been adopted in many schools due to COVID-19. While it is convenient, it is challenging for teachers to supervise online exams from simultaneously displayed student Zoom windows. In this paper, we propose iExam, an intelligent online exam monitoring and analysis system that can not only use face detection to assist invigilators in real-time student identification, but also be able to detect common abnormal behaviors (including face disappearing, rotating faces, and replacing with a different person during the exams) via a face recognition-based post-exam video analysis. To build such a novel system in its first kind, we overcome three challenges. First, we discover a lightweight approach to capturing exam video streams and analyzing them in real time. Second, we utilize the left-corner names that are displayed on each student's Zoom window and propose an improved OCR (optical character recognition) technique to automatically gather the ground truth for the student faces with dynamic positions. Third, we perform several experimental comparisons and optimizations to efficiently shorten the training and testing time required on teachers' PC. Our evaluation shows that iExam achieves high accuracy, 90.4% for real-time face detection and 98.4% for post-exam face recognition, while maintaining acceptable runtime performance. We have made iExam's source code available at https://github.com/VPRLab/iExam.



### TextDCT: Arbitrary-Shaped Text Detection via Discrete Cosine Transform Mask
- **Arxiv ID**: http://arxiv.org/abs/2206.13381v1
- **DOI**: 10.1109/TMM.2022.3186431
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13381v1)
- **Published**: 2022-06-27 15:42:25+00:00
- **Updated**: 2022-06-27 15:42:25+00:00
- **Authors**: Yuchen Su, Zhiwen Shao, Yong Zhou, Fanrong Meng, Hancheng Zhu, Bing Liu, Rui Yao
- **Comment**: This paper has been accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Arbitrary-shaped scene text detection is a challenging task due to the variety of text changes in font, size, color, and orientation. Most existing regression based methods resort to regress the masks or contour points of text regions to model the text instances. However, regressing the complete masks requires high training complexity, and contour points are not sufficient to capture the details of highly curved texts. To tackle the above limitations, we propose a novel light-weight anchor-free text detection framework called TextDCT, which adopts the discrete cosine transform (DCT) to encode the text masks as compact vectors. Further, considering the imbalanced number of training samples among pyramid layers, we only employ a single-level head for top-down prediction. To model the multi-scale texts in a single-level head, we introduce a novel positive sampling strategy by treating the shrunk text region as positive samples, and design a feature awareness module (FAM) for spatial-awareness and scale-awareness by fusing rich contextual information and focusing on more significant features. Moreover, we propose a segmented non-maximum suppression (S-NMS) method that can filter low-quality mask regressions. Extensive experiments are conducted on four challenging datasets, which demonstrate our TextDCT obtains competitive performance on both accuracy and efficiency. Specifically, TextDCT achieves F-measure of 85.1 at 17.2 frames per second (FPS) and F-measure of 84.9 at 15.1 FPS for CTW1500 and Total-Text datasets, respectively.



### Mushroom image recognition and distance generation based on attention-mechanism model and genetic information
- **Arxiv ID**: http://arxiv.org/abs/2206.13383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13383v1)
- **Published**: 2022-06-27 15:43:03+00:00
- **Updated**: 2022-06-27 15:43:03+00:00
- **Authors**: Wenbin Liao, Jiewen Xiao, Chengbo Zhao, Yonggong Han, ZhiJie Geng, Jianxin Wang, Yihua Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The species identification of Macrofungi, i.e. mushrooms, has always been a challenging task. There are still a large number of poisonous mushrooms that have not been found, which poses a risk to people's life. However, the traditional identification method requires a large number of experts with knowledge in the field of taxonomy for manual identification, it is not only inefficient but also consumes a lot of manpower and capital costs. In this paper, we propose a new model based on attention-mechanism, MushroomNet, which applies the lightweight network MobileNetV3 as the backbone model, combined with the attention structure proposed by us, and has achieved excellent performance in the mushroom recognition task. On the public dataset, the test accuracy of the MushroomNet model has reached 83.9%, and on the local dataset, the test accuracy has reached 77.4%. The proposed attention mechanisms well focused attention on the bodies of mushroom image for mixed channel attention and the attention heat maps visualized by Grad-CAM. Further, in this study, genetic distance was added to the mushroom image recognition task, the genetic distance was used as the representation space, and the genetic distance between each pair of mushroom species in the dataset was used as the embedding of the genetic distance representation space, so as to predict the image distance and species. identify. We found that using the MES activation function can predict the genetic distance of mushrooms very well, but the accuracy is lower than that of SoftMax. The proposed MushroomNet was demonstrated it shows great potential for automatic and online mushroom image and the proposed automatic procedure would assist and be a reference to traditional mushroom classification.



### Transfer Learning via Test-Time Neural Networks Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2206.13399v1
- **DOI**: 10.5220/0010907900003124
- **Categories**: **cs.LG**, cs.AI, cs.CV, 68T07, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2206.13399v1)
- **Published**: 2022-06-27 15:46:05+00:00
- **Updated**: 2022-06-27 15:46:05+00:00
- **Authors**: Bruno Casella, Alessio Barbaro Chisari, Sebastiano Battiato, Mario Valerio Giuffrida
- **Comment**: 8 pages
- **Journal**: Proceedings of the 17th international joint conference on computer
  vision, imaging and computer graphics theory and applications, VISIGRAPP
  2022, volume 5: VISAPP, online streaming, february 6-8, 2022, 2022, pp.
  642-649
- **Summary**: It has been demonstrated that deep neural networks outperform traditional machine learning. However, deep networks lack generalisability, that is, they will not perform as good as in a new (testing) set drawn from a different distribution due to the domain shift. In order to tackle this known issue, several transfer learning approaches have been proposed, where the knowledge of a trained model is transferred into another to improve performance with different data. However, most of these approaches require additional training steps, or they suffer from catastrophic forgetting that occurs when a trained model has overwritten previously learnt knowledge. We address both problems with a novel transfer learning approach that uses network aggregation. We train dataset-specific networks together with an aggregation network in a unified framework. The loss function includes two main components: a task-specific loss (such as cross-entropy) and an aggregation loss. The proposed aggregation loss allows our model to learn how trained deep network parameters can be aggregated with an aggregation operator. We demonstrate that the proposed approach learns model aggregation at test time without any further training step, reducing the burden of transfer learning to a simple arithmetical operation. The proposed approach achieves comparable performance w.r.t. the baseline. Besides, if the aggregation operator has an inverse, we will show that our model also inherently allows for selective forgetting, i.e., the aggregated model can forget one of the datasets it was trained on, retaining information on the others.



### Explicitly incorporating spatial information to recurrent networks for agriculture
- **Arxiv ID**: http://arxiv.org/abs/2206.13406v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13406v1)
- **Published**: 2022-06-27 15:57:42+00:00
- **Updated**: 2022-06-27 15:57:42+00:00
- **Authors**: Claus Smitt, Michael Halstead, Alireza Ahmadi, Chris McCool
- **Comment**: None
- **Journal**: None
- **Summary**: In agriculture, the majority of vision systems perform still image classification. Yet, recent work has highlighted the potential of spatial and temporal cues as a rich source of information to improve the classification performance. In this paper, we propose novel approaches to explicitly capture both spatial and temporal information to improve the classification of deep convolutional neural networks. We leverage available RGB-D images and robot odometry to perform inter-frame feature map spatial registration. This information is then fused within recurrent deep learnt models, to improve their accuracy and robustness. We demonstrate that this can considerably improve the classification performance with our best performing spatial-temporal model (ST-Atte) achieving absolute performance improvements for intersection-over-union (IoU[%]) of 4.7 for crop-weed segmentation and 2.6 for fruit (sweet pepper) segmentation. Furthermore, we show that these approaches are robust to variable framerates and odometry errors, which are frequently observed in real-world applications.



### RES: A Robust Framework for Guiding Visual Explanation
- **Arxiv ID**: http://arxiv.org/abs/2206.13413v1
- **DOI**: 10.1145/3534678.3539419
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13413v1)
- **Published**: 2022-06-27 16:06:27+00:00
- **Updated**: 2022-06-27 16:06:27+00:00
- **Authors**: Yuyang Gao, Tong Steven Sun, Guangji Bai, Siyi Gu, Sungsoo Ray Hong, Liang Zhao
- **Comment**: Published in KDD 2022
- **Journal**: In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA
- **Summary**: Despite the fast progress of explanation techniques in modern Deep Neural Networks (DNNs) where the main focus is handling "how to generate the explanations", advanced research questions that examine the quality of the explanation itself (e.g., "whether the explanations are accurate") and improve the explanation quality (e.g., "how to adjust the model to generate more accurate explanations when explanations are inaccurate") are still relatively under-explored. To guide the model toward better explanations, techniques in explanation supervision - which add supervision signals on the model explanation - have started to show promising effects on improving both the generalizability as and intrinsic interpretability of Deep Neural Networks. However, the research on supervising explanations, especially in vision-based applications represented through saliency maps, is in its early stage due to several inherent challenges: 1) inaccuracy of the human explanation annotation boundary, 2) incompleteness of the human explanation annotation region, and 3) inconsistency of the data distribution between human annotation and model explanation maps. To address the challenges, we propose a generic RES framework for guiding visual explanation by developing a novel objective that handles inaccurate boundary, incomplete region, and inconsistent distribution of human annotations, with a theoretical justification on model generalizability. Extensive experiments on two real-world image datasets demonstrate the effectiveness of the proposed framework on enhancing both the reasonability of the explanation and the performance of the backbone DNNs model.



### DeStripe: A Self2Self Spatio-Spectral Graph Neural Network with Unfolded Hessian for Stripe Artifact Removal in Light-sheet Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2206.13419v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13419v1)
- **Published**: 2022-06-27 16:13:57+00:00
- **Updated**: 2022-06-27 16:13:57+00:00
- **Authors**: Yu Liu, Kurt Weiss, Nassir Navab, Carsten Marr, Jan Huisken, Tingying Peng
- **Comment**: Accepted by 25th International Conference on Medical Image Computing
  and Computer Assisted Intervention
- **Journal**: None
- **Summary**: Light-sheet fluorescence microscopy (LSFM) is a cutting-edge volumetric imaging technique that allows for three-dimensional imaging of mesoscopic samples with decoupled illumination and detection paths. Although the selective excitation scheme of such a microscope provides intrinsic optical sectioning that minimizes out-of-focus fluorescence background and sample photodamage, it is prone to light absorption and scattering effects, which results in uneven illumination and striping artifacts in the images adversely. To tackle this issue, in this paper, we propose a blind stripe artifact removal algorithm in LSFM, called DeStripe, which combines a self-supervised spatio-spectral graph neural network with unfolded Hessian prior. Specifically, inspired by the desirable properties of Fourier transform in condensing striping information into isolated values in the frequency domain, DeStripe firstly localizes the potentially corrupted Fourier coefficients by exploiting the structural difference between unidirectional stripe artifacts and more isotropic foreground images. Affected Fourier coefficients can then be fed into a graph neural network for recovery, with a Hessian regularization unrolled to further ensure structures in the standard image space are well preserved. Since in realistic, stripe-free LSFM barely exists with a standard image acquisition protocol, DeStripe is equipped with a Self2Self denoising loss term, enabling artifact elimination without access to stripe-free ground truth images. Competitive experimental results demonstrate the efficacy of DeStripe in recovering corrupted biomarkers in LSFM with both synthetic and real stripe artifacts.



### ContraReg: Contrastive Learning of Multi-modality Unsupervised Deformable Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2206.13434v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13434v1)
- **Published**: 2022-06-27 16:27:53+00:00
- **Updated**: 2022-06-27 16:27:53+00:00
- **Authors**: Neel Dey, Jo Schlemper, Seyed Sadegh Mohseni Salehi, Bo Zhou, Guido Gerig, Michal Sofka
- **Comment**: Accepted by MICCAI 2022. 13 pages, 6 figures, and 1 table
- **Journal**: None
- **Summary**: Establishing voxelwise semantic correspondence across distinct imaging modalities is a foundational yet formidable computer vision task. Current multi-modality registration techniques maximize hand-crafted inter-domain similarity functions, are limited in modeling nonlinear intensity-relationships and deformations, and may require significant re-engineering or underperform on new tasks, datasets, and domain pairs. This work presents ContraReg, an unsupervised contrastive representation learning approach to multi-modality deformable registration. By projecting learned multi-scale local patch features onto a jointly learned inter-domain embedding space, ContraReg obtains representations useful for non-rigid multi-modality alignment. Experimentally, ContraReg achieves accurate and robust results with smooth and invertible deformations across a series of baselines and ablations on a neonatal T1-T2 brain MRI registration task with all methods validated over a wide range of deformation regularization strengths.



### Optimizing Video Prediction via Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2206.13454v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13454v1)
- **Published**: 2022-06-27 17:03:46+00:00
- **Updated**: 2022-06-27 17:03:46+00:00
- **Authors**: Yue Wu, Qiang Wen, Qifeng Chen
- **Comment**: Accepted by the CVPR 2022
- **Journal**: None
- **Summary**: Video prediction is an extrapolation task that predicts future frames given past frames, and video frame interpolation is an interpolation task that estimates intermediate frames between two frames. We have witnessed the tremendous advancement of video frame interpolation, but the general video prediction in the wild is still an open question. Inspired by the photo-realistic results of video frame interpolation, we present a new optimization framework for video prediction via video frame interpolation, in which we solve an extrapolation problem based on an interpolation model. Our video prediction framework is based on optimization with a pretrained differentiable video frame interpolation module without the need for a training dataset, and thus there is no domain gap issue between training and test data. Also, our approach does not need any additional information such as semantic or instance maps, which makes our framework applicable to any video. Extensive experiments on the Cityscapes, KITTI, DAVIS, Middlebury, and Vimeo90K datasets show that our video prediction results are robust in general scenarios, and our approach outperforms other video prediction methods that require a large amount of training data or extra semantic information.



### IBISCape: A Simulated Benchmark for multi-modal SLAM Systems Evaluation in Large-scale Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/2206.13455v2
- **DOI**: 10.1007/s10846-022-01753-7
- **Categories**: **eess.IV**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.13455v2)
- **Published**: 2022-06-27 17:04:06+00:00
- **Updated**: 2022-10-20 09:06:07+00:00
- **Authors**: Abanob Soliman, Fabien Bonardi, Désiré Sidibé, Samia Bouchafa
- **Comment**: Accepted for publication in the Journal of Intelligent & Robotic
  Systems
- **Journal**: J Intell Robot Syst 106, 53 (2022)
- **Summary**: The development process of high-fidelity SLAM systems depends on their validation upon reliable datasets. Towards this goal, we propose IBISCape, a simulated benchmark that includes data synchronization and acquisition APIs for telemetry from heterogeneous sensors: stereo-RGB/DVS, Depth, IMU, and GPS, along with the ground truth scene segmentation and vehicle ego-motion. Our benchmark is built upon the CARLA simulator, whose back-end is the Unreal Engine rendering a high dynamic scenery simulating the real world. Moreover, we offer 34 multi-modal datasets suitable for autonomous vehicles navigation, including scenarios for scene understanding evaluation like accidents, along with a wide range of frame quality based on a dynamic weather simulation class integrated with our APIs. We also introduce the first calibration targets to CARLA maps to solve the unknown distortion parameters problem of CARLA simulated DVS and RGB cameras. Finally, using IBISCape sequences, we evaluate four ORB-SLAM3 systems (monocular RGB, stereo RGB, Stereo Visual Inertial (SVI), and RGB-D) performance and BASALT Visual-Inertial Odometry (VIO) system on various sequences collected in simulated large-scale dynamic environments.   Keywords: benchmark, multi-modal, datasets, Odometry, Calibration, DVS, SLAM



### Learn Fast, Segment Well: Fast Object Segmentation Learning on the iCub Robot
- **Arxiv ID**: http://arxiv.org/abs/2206.13462v1
- **DOI**: 10.1109/TRO.2022.3164331
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.13462v1)
- **Published**: 2022-06-27 17:14:04+00:00
- **Updated**: 2022-06-27 17:14:04+00:00
- **Authors**: Federico Ceola, Elisa Maiettini, Giulia Pasquale, Giacomo Meanti, Lorenzo Rosasco, Lorenzo Natale
- **Comment**: \copyright 2022 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: The visual system of a robot has different requirements depending on the application: it may require high accuracy or reliability, be constrained by limited resources or need fast adaptation to dynamically changing environments. In this work, we focus on the instance segmentation task and provide a comprehensive study of different techniques that allow adapting an object segmentation model in presence of novel objects or different domains. We propose a pipeline for fast instance segmentation learning designed for robotic applications where data come in stream. It is based on an hybrid method leveraging on a pre-trained CNN for feature extraction and fast-to-train Kernel-based classifiers. We also propose a training protocol that allows to shorten the training time by performing feature extraction during the data acquisition. We benchmark the proposed pipeline on two robotics datasets and we deploy it on a real robot, i.e. the iCub humanoid. To this aim, we adapt our method to an incremental setting in which novel objects are learned on-line by the robot. The code to reproduce the experiments is publicly available on GitHub.



### An Atlas for the Pinhole Camera
- **Arxiv ID**: http://arxiv.org/abs/2206.13468v2
- **DOI**: 10.1007/s10208-022-09592-6
- **Categories**: **math.AG**, cs.CV, math.AC, 14Q25, 94A08
- **Links**: [PDF](http://arxiv.org/pdf/2206.13468v2)
- **Published**: 2022-06-27 17:23:57+00:00
- **Updated**: 2022-10-03 19:10:12+00:00
- **Authors**: Sameer Agarwal, Timothy Duff, Max Lieblich, Rekha Thomas
- **Comment**: 47 pages with references and appendices, final version
- **Journal**: JFoCM, 2022
- **Summary**: We introduce an atlas of algebro-geometric objects associated with image formation in pinhole cameras. The nodes of the atlas are algebraic varieties or their vanishing ideals related to each other by projection or elimination and restriction or specialization respectively. This atlas offers a unifying framework for the study of problems in 3D computer vision. We initiate the study of the atlas by completely characterizing a part of the atlas stemming from the triangulation problem. We conclude with several open problems and generalizations of the atlas.



### Effective training-time stacking for ensembling of deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/2206.13491v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13491v1)
- **Published**: 2022-06-27 17:52:53+00:00
- **Updated**: 2022-06-27 17:52:53+00:00
- **Authors**: Polina Proscura, Alexey Zaytsev
- **Comment**: None
- **Journal**: None
- **Summary**: Ensembling is a popular and effective method for improving machine learning (ML) models. It proves its value not only in classical ML but also for deep learning. Ensembles enhance the quality and trustworthiness of ML solutions, and allow uncertainty estimation. However, they come at a price: training ensembles of deep learning models eat a huge amount of computational resources.   A snapshot ensembling collects models in the ensemble along a single training path. As it runs training only one time, the computational time is similar to the training of one model. However, the quality of models along the training path is different: typically, later models are better if no overfitting occurs. So, the models are of varying utility.   Our method improves snapshot ensembling by selecting and weighting ensemble members along the training path. It relies on training-time likelihoods without looking at validation sample errors that standard stacking methods do. Experimental evidence for Fashion MNIST, CIFAR-10, and CIFAR-100 datasets demonstrates the superior quality of the proposed weighted ensembles c.t. vanilla ensembling of deep learning models.



### Robustness Implies Generalization via Data-Dependent Generalization Bounds
- **Arxiv ID**: http://arxiv.org/abs/2206.13497v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.PR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2206.13497v4)
- **Published**: 2022-06-27 17:58:06+00:00
- **Updated**: 2022-08-03 05:27:54+00:00
- **Authors**: Kenji Kawaguchi, Zhun Deng, Kyle Luh, Jiaoyang Huang
- **Comment**: Accepted by ICML 2022, and selected for ICML long presentation (top
  2% of submissions)
- **Journal**: None
- **Summary**: This paper proves that robustness implies generalization via data-dependent generalization bounds. As a result, robustness and generalization are shown to be connected closely in a data-dependent manner. Our bounds improve previous bounds in two directions, to solve an open problem that has seen little development since 2010. The first is to reduce the dependence on the covering number. The second is to remove the dependence on the hypothesis space. We present several examples, including ones for lasso and deep learning, in which our bounds are provably preferable. The experiments on real-world data and theoretical models demonstrate near-exponential improvements in various situations. To achieve these improvements, we do not require additional assumptions on the unknown distribution; instead, we only incorporate an observable and computable property of the training samples. A key technical innovation is an improved concentration bound for multinomial random variables that is of independent interest beyond robustness and generalization.



### Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior
- **Arxiv ID**: http://arxiv.org/abs/2206.13498v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2206.13498v2)
- **Published**: 2022-06-27 17:59:08+00:00
- **Updated**: 2023-05-29 21:50:51+00:00
- **Authors**: Jean-Stanislas Denain, Jacob Steinhardt
- **Comment**: Fixed backdoor localization results, made changes to abstract and
  introduction
- **Journal**: None
- **Summary**: Model visualizations provide information that outputs alone might miss. But can we trust that model visualizations reflect model behavior? For instance, can they diagnose abnormal behavior such as planted backdoors or overregularization? To evaluate visualization methods, we test whether they assign different visualizations to anomalously trained models and normal models. We find that while existing methods can detect models with starkly anomalous behavior, they struggle to identify more subtle anomalies. Moreover, they often fail to recognize the inputs that induce anomalous behavior, e.g. images containing a spurious cue. These results reveal blind spots and limitations of some popular model visualizations. By introducing a novel evaluation framework for visualizations, our work paves the way for developing more reliable model transparency methods in the future.



### Prompting Decision Transformer for Few-Shot Policy Generalization
- **Arxiv ID**: http://arxiv.org/abs/2206.13499v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.13499v1)
- **Published**: 2022-06-27 17:59:17+00:00
- **Updated**: 2022-06-27 17:59:17+00:00
- **Authors**: Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua B. Tenenbaum, Chuang Gan
- **Comment**: ICML 2022. Project page: https://mxu34.github.io/PromptDT/
- **Journal**: None
- **Summary**: Humans can leverage prior experience and learn novel tasks from a handful of demonstrations. In contrast to offline meta-reinforcement learning, which aims to achieve quick adaptation through better algorithm design, we investigate the effect of architecture inductive bias on the few-shot learning capability. We propose a Prompt-based Decision Transformer (Prompt-DT), which leverages the sequential modeling ability of the Transformer architecture and the prompt framework to achieve few-shot adaptation in offline RL. We design the trajectory prompt, which contains segments of the few-shot demonstrations, and encodes task-specific information to guide policy generation. Our experiments in five MuJoCo control benchmarks show that Prompt-DT is a strong few-shot learner without any extra finetuning on unseen target tasks. Prompt-DT outperforms its variants and strong meta offline RL baselines by a large margin with a trajectory prompt containing only a few timesteps. Prompt-DT is also robust to prompt length changes and can generalize to out-of-distribution (OOD) environments.



### Neural Neural Textures Make Sim2Real Consistent
- **Arxiv ID**: http://arxiv.org/abs/2206.13500v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.13500v2)
- **Published**: 2022-06-27 17:59:45+00:00
- **Updated**: 2022-12-15 12:37:36+00:00
- **Authors**: Ryan Burgert, Jinghuan Shang, Xiang Li, Michael Ryoo
- **Comment**: 9 pages, 10 figures (without references or appendix); 16 pages, 16
  figures (with appendix)
- **Journal**: None
- **Summary**: Unpaired image translation algorithms can be used for sim2real tasks, but many fail to generate temporally consistent results. We present a new approach that combines differentiable rendering with image translation to achieve temporal consistency over indefinite timescales, using surface consistency losses and \emph{neural neural textures}. We call this algorithm TRITON (Texture Recovering Image Translation Network): an unsupervised, end-to-end, stateless sim2real algorithm that leverages the underlying 3D geometry of input scenes by generating realistic-looking learnable neural textures. By settling on a particular texture for the objects in a scene, we ensure consistency between frames statelessly. Unlike previous algorithms, TRITON is not limited to camera movements -- it can handle the movement of objects as well, making it useful for downstream tasks such as robotic manipulation.



### Programmatic Concept Learning for Human Motion Description and Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2206.13502v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2206.13502v1)
- **Published**: 2022-06-27 17:59:57+00:00
- **Updated**: 2022-06-27 17:59:57+00:00
- **Authors**: Sumith Kulal, Jiayuan Mao, Alex Aiken, Jiajun Wu
- **Comment**: CVPR 2022. Project page:
  https://sumith1896.github.io/motion-concepts/
- **Journal**: None
- **Summary**: We introduce Programmatic Motion Concepts, a hierarchical motion representation for human actions that captures both low-level motion and high-level description as motion concepts. This representation enables human motion description, interactive editing, and controlled synthesis of novel video sequences within a single framework. We present an architecture that learns this concept representation from paired video and action sequences in a semi-supervised manner. The compactness of our representation also allows us to present a low-resource training recipe for data-efficient learning. By outperforming established baselines, especially in the small data regime, we demonstrate the efficiency and effectiveness of our framework for multiple applications.



### ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.13559v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13559v3)
- **Published**: 2022-06-27 18:02:29+00:00
- **Updated**: 2022-10-13 06:34:19+00:00
- **Authors**: Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, Hongsheng Li
- **Comment**: Accepted in NeurIPS 2022
- **Journal**: None
- **Summary**: Capitalizing on large pre-trained models for various downstream tasks of interest have recently emerged with promising performance. Due to the ever-growing model size, the standard full fine-tuning based task adaptation strategy becomes prohibitively costly in terms of model training and storage. This has led to a new research direction in parameter-efficient transfer learning. However, existing attempts typically focus on downstream tasks from the same modality (e.g., image understanding) of the pre-trained model. This creates a limit because in some specific modalities, (e.g., video understanding) such a strong pre-trained model with sufficient knowledge is less or not available. In this work, we investigate such a novel cross-modality transfer learning setting, namely parameter-efficient image-to-video transfer learning. To solve this problem, we propose a new Spatio-Temporal Adapter (ST-Adapter) for parameter-efficient fine-tuning per video task. With a built-in spatio-temporal reasoning capability in a compact design, ST-Adapter enables a pre-trained image model without temporal knowledge to reason about dynamic video content at a small (~8%) per-task parameter cost, requiring approximately 20 times fewer updated parameters compared to previous work. Extensive experiments on video action recognition tasks show that our ST-Adapter can match or even outperform the strong full fine-tuning strategy and state-of-the-art video models, whilst enjoying the advantage of parameter efficiency. The code and model are available at https://github.com/linziyi96/st-adapter



### A View Independent Classification Framework for Yoga Postures
- **Arxiv ID**: http://arxiv.org/abs/2206.13577v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13577v2)
- **Published**: 2022-06-27 18:40:34+00:00
- **Updated**: 2022-08-14 10:59:11+00:00
- **Authors**: Mustafa Chasmai, Nirjhar Das, Aman Bhardwaj, Rahul Garg
- **Comment**: None
- **Journal**: None
- **Summary**: Yoga is a globally acclaimed and widely recommended practice for a healthy living. Maintaining correct posture while performing a Yogasana is of utmost importance. In this work, we employ transfer learning from Human Pose Estimation models for extracting 136 key-points spread all over the body to train a Random Forest classifier which is used for estimation of the Yogasanas. The results are evaluated on an in-house collected extensive yoga video database of 51 subjects recorded from 4 different camera angles. We propose a 3 step scheme for evaluating the generalizability of a Yoga classifier by testing it on 1) unseen frames, 2) unseen subjects, and 3) unseen camera angles. We argue that for most of the applications, validation accuracies on unseen subjects and unseen camera angles would be most important. We empirically analyze over three public datasets, the advantage of transfer learning and the possibilities of target leakage. We further demonstrate that the classification accuracies critically depend on the cross validation method employed and can often be misleading. To promote further research, we have made key-points dataset and code publicly available.



### NeuRIS: Neural Reconstruction of Indoor Scenes Using Normal Priors
- **Arxiv ID**: http://arxiv.org/abs/2206.13597v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13597v2)
- **Published**: 2022-06-27 19:22:03+00:00
- **Updated**: 2022-10-16 14:30:57+00:00
- **Authors**: Jiepeng Wang, Peng Wang, Xiaoxiao Long, Christian Theobalt, Taku Komura, Lingjie Liu, Wenping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstructing 3D indoor scenes from 2D images is an important task in many computer vision and graphics applications. A main challenge in this task is that large texture-less areas in typical indoor scenes make existing methods struggle to produce satisfactory reconstruction results. We propose a new method, named NeuRIS, for high quality reconstruction of indoor scenes. The key idea of NeuRIS is to integrate estimated normal of indoor scenes as a prior in a neural rendering framework for reconstructing large texture-less shapes and, importantly, to do this in an adaptive manner to also enable the reconstruction of irregular shapes with fine details. Specifically, we evaluate the faithfulness of the normal priors on-the-fly by checking the multi-view consistency of reconstruction during the optimization process. Only the normal priors accepted as faithful will be utilized for 3D reconstruction, which typically happens in the regions of smooth shapes possibly with weak texture. However, for those regions with small objects or thin structures, for which the normal priors are usually unreliable, we will only rely on visual features of the input images, since such regions typically contain relatively rich visual features (e.g., shade changes and boundary contours). Extensive experiments show that NeuRIS significantly outperforms the state-of-the-art methods in terms of reconstruction quality.



### On-device Synaptic Memory Consolidation using Fowler-Nordheim Quantum-tunneling
- **Arxiv ID**: http://arxiv.org/abs/2206.14581v1
- **DOI**: None
- **Categories**: **cs.ET**, cs.AI, cs.AR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.14581v1)
- **Published**: 2022-06-27 19:44:18+00:00
- **Updated**: 2022-06-27 19:44:18+00:00
- **Authors**: Mustafizur Rahman, Subhankar Bose, Shantanu Chakrabartty
- **Comment**: None
- **Journal**: None
- **Summary**: Synaptic memory consolidation has been heralded as one of the key mechanisms for supporting continual learning in neuromorphic Artificial Intelligence (AI) systems. Here we report that a Fowler-Nordheim (FN) quantum-tunneling device can implement synaptic memory consolidation similar to what can be achieved by algorithmic consolidation models like the cascade and the elastic weight consolidation (EWC) models. The proposed FN-synapse not only stores the synaptic weight but also stores the synapse's historical usage statistic on the device itself. We also show that the operation of the FN-synapse is near-optimal in terms of the synaptic lifetime and we demonstrate that a network comprising FN-synapses outperforms a comparable EWC network for a small benchmark continual learning task. With an energy footprint of femtojoules per synaptic update, we believe that the proposed FN-synapse provides an ultra-energy-efficient approach for implementing both synaptic memory consolidation and persistent learning.



### Perspective (In)consistency of Paint by Text
- **Arxiv ID**: http://arxiv.org/abs/2206.14617v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.AI, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2206.14617v1)
- **Published**: 2022-06-27 19:52:33+00:00
- **Updated**: 2022-06-27 19:52:33+00:00
- **Authors**: Hany Farid
- **Comment**: None
- **Journal**: None
- **Summary**: Type "a sea otter with a pearl earring by Johannes Vermeer" or "a photo of a teddy bear on a skateboard in Times Square" into OpenAI's DALL-E-2 paint-by-text synthesis engine and you will not be disappointed by the delightful and eerily pertinent results. The ability to synthesize highly realistic images -- with seemingly no limitation other than our imagination -- is sure to yield many exciting and creative applications. These images are also likely to pose new challenges to the photo-forensic community. Motivated by the fact that paint by text is not based on explicit geometric modeling, and the human visual system's often obliviousness to even glaring geometric inconsistencies, we provide an initial exploration of the perspective consistency of DALL-E-2 synthesized images to determine if geometric-based forensic analyses will prove fruitful in detecting this new breed of synthetic media.



### Reducing Annotation Need in Self-Explanatory Models for Lung Nodule Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2206.13608v2
- **DOI**: 10.1007/978-3-031-17976-1_4
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13608v2)
- **Published**: 2022-06-27 20:01:41+00:00
- **Updated**: 2022-07-25 15:21:07+00:00
- **Authors**: Jiahao Lu, Chong Yin, Oswin Krause, Kenny Erleben, Michael Bachmann Nielsen, Sune Darkner
- **Comment**: 10 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Feature-based self-explanatory methods explain their classification in terms of human-understandable features. In the medical imaging community, this semantic matching of clinical knowledge adds significantly to the trustworthiness of the AI. However, the cost of additional annotation of features remains a pressing issue. We address this problem by proposing cRedAnno, a data-/annotation-efficient self-explanatory approach for lung nodule diagnosis. cRedAnno considerably reduces the annotation need by introducing self-supervised contrastive learning to alleviate the burden of learning most parameters from annotation, replacing end-to-end training with two-stage training. When training with hundreds of nodule samples and only 1% of their annotations, cRedAnno achieves competitive accuracy in predicting malignancy, meanwhile significantly surpassing most previous works in predicting nodule attributes. Visualisation of the learned space further indicates that the correlation between the clustering of malignancy and nodule attributes coincides with clinical knowledge. Our complete code is open-source available: https://github.com/diku-dk/credanno.



### Flexible-Rate Learned Hierarchical Bi-Directional Video Compression With Motion Refinement and Frame-Level Bit Allocation
- **Arxiv ID**: http://arxiv.org/abs/2206.13613v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13613v1)
- **Published**: 2022-06-27 20:18:52+00:00
- **Updated**: 2022-06-27 20:18:52+00:00
- **Authors**: Eren Cetin, M. Akin Yilmaz, A. Murat Tekalp
- **Comment**: Accepted for publication in IEEE International Conference on Image
  Processing (ICIP 2022)
- **Journal**: None
- **Summary**: This paper presents improvements and novel additions to our recent work on end-to-end optimized hierarchical bi-directional video compression to further advance the state-of-the-art in learned video compression. As an improvement, we combine motion estimation and prediction modules and compress refined residual motion vectors for improved rate-distortion performance. As novel addition, we adapted the gain unit proposed for image compression to flexible-rate video compression in two ways: first, the gain unit enables a single encoder model to operate at multiple rate-distortion operating points; second, we exploit the gain unit to control bit allocation among intra-coded vs. bi-directionally coded frames by fine tuning corresponding models for truly flexible-rate learned video coding. Experimental results demonstrate that we obtain state-of-the-art rate-distortion performance exceeding those of all prior art in learned video coding.



### Patch Selection for Melanoma Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.13626v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13626v1)
- **Published**: 2022-06-27 20:52:53+00:00
- **Updated**: 2022-06-27 20:52:53+00:00
- **Authors**: Guillaume Lachaud, Patricia Conde-Cespedes, Maria Trocan
- **Comment**: None
- **Journal**: None
- **Summary**: In medical image processing, the most important information is often located on small parts of the image. Patch-based approaches aim at using only the most relevant parts of the image. Finding ways to automatically select the patches is a challenge. In this paper, we investigate two criteria to choose patches: entropy and a spectral similarity criterion. We perform experiments at different levels of patch size. We train a Convolutional Neural Network on the subsets of patches and analyze the training time. We find that, in addition to requiring less preprocessing time, the classifiers trained on the datasets of patches selected based on entropy converge faster than on those selected based on the spectral similarity criterion and, furthermore, lead to higher accuracy. Moreover, patches of high entropy lead to faster convergence and better accuracy than patches of low entropy.



### Multi-scale Network with Attentional Multi-resolution Fusion for Point Cloud Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.13628v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.13628v1)
- **Published**: 2022-06-27 21:03:33+00:00
- **Updated**: 2022-06-27 21:03:33+00:00
- **Authors**: Yuyan Li, Ye Duan
- **Comment**: ICPR 2022, poster
- **Journal**: None
- **Summary**: In this paper, we present a comprehensive point cloud semantic segmentation network that aggregates both local and global multi-scale information. First, we propose an Angle Correlation Point Convolution (ACPConv) module to effectively learn the local shapes of points. Second, based upon ACPConv, we introduce a local multi-scale split (MSS) block that hierarchically connects features within one single block and gradually enlarges the receptive field which is beneficial for exploiting the local context. Third, inspired by HRNet which has excellent performance on 2D image vision tasks, we build an HRNet customized for point cloud to learn global multi-scale context. Lastly, we introduce a point-wise attention fusion approach that fuses multi-resolution predictions and further improves point cloud semantic segmentation performance. Our experimental results and ablations on several benchmark datasets show that our proposed method is effective and able to achieve state-of-the-art performances compared to existing methods.



### Toward an ImageNet Library of Functions for Global Optimization Benchmarking
- **Arxiv ID**: http://arxiv.org/abs/2206.13630v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.13630v1)
- **Published**: 2022-06-27 21:05:00+00:00
- **Updated**: 2022-06-27 21:05:00+00:00
- **Authors**: Boris Yazmir, Ofer M. Shir
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge of search-landscape features of BlackBox Optimization (BBO) problems offers valuable information in light of the Algorithm Selection and/or Configuration problems. Exploratory Landscape Analysis (ELA) models have gained success in identifying predefined human-derived features and in facilitating portfolio selectors to address those challenges. Unlike ELA approaches, the current study proposes to transform the identification problem into an image recognition problem, with a potential to detect conception-free, machine-driven landscape features. To this end, we introduce the notion of Landscape Images, which enables us to generate imagery instances per a benchmark function, and then target the classification challenge over a diverse generalized dataset of functions. We address it as a supervised multi-class image recognition problem and apply basic artificial neural network models to solve it. The efficacy of our approach is numerically validated on the noise free BBOB and IOHprofiler benchmarking suites. This evident successful learning is another step toward automated feature extraction and local structure deduction of BBO problems. By using this definition of landscape images, and by capitalizing on existing capabilities of image recognition algorithms, we foresee the construction of an ImageNet-like library of functions for training generalized detectors that rely on machine-driven features.



### Omni-Seg: A Scale-aware Dynamic Network for Renal Pathological Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.13632v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13632v2)
- **Published**: 2022-06-27 21:09:55+00:00
- **Updated**: 2023-01-18 20:42:54+00:00
- **Authors**: Ruining Deng, Quan Liu, Can Cui, Tianyuan Yao, Jun Long, Zuhayr Asad, R. Michael Womick, Zheyu Zhu, Agnes B. Fogo, Shilin Zhao, Haichun Yang, Yuankai Huo
- **Comment**: None
- **Journal**: None
- **Summary**: Comprehensive semantic segmentation on renal pathological images is challenging due to the heterogeneous scales of the objects. For example, on a whole slide image (WSI), the cross-sectional areas of glomeruli can be 64 times larger than that of the peritubular capillaries, making it impractical to segment both objects on the same patch, at the same scale. To handle this scaling issue, prior studies have typically trained multiple segmentation networks in order to match the optimal pixel resolution of heterogeneous tissue types. This multi-network solution is resource-intensive and fails to model the spatial relationship between tissue types. In this paper, we propose the Omni-Seg+ network, a scale-aware dynamic neural network that achieves multi-object (six tissue types) and multi-scale (5X to 40X scale) pathological image segmentation via a single neural network. The contribution of this paper is three-fold: (1) a novel scale-aware controller is proposed to generalize the dynamic neural network from single-scale to multi-scale; (2) semi-supervised consistency regularization of pseudo-labels is introduced to model the inter-scale correlation of unannotated tissue types into a single end-to-end learning paradigm; and (3) superior scale-aware generalization is evidenced by directly applying a model trained on human kidney images to mouse kidney images, without retraining. By learning from ~150,000 human pathological image patches from six tissue types at three different resolutions, our approach achieved superior segmentation performance according to human visual assessment and evaluation of image-omics (i.e., spatial transcriptomics). The official implementation is available at https://github.com/ddrrnn123/Omni-Seg.



### Feature Refinement to Improve High Resolution Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2206.13644v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.13644v2)
- **Published**: 2022-06-27 21:59:12+00:00
- **Updated**: 2022-06-29 18:16:30+00:00
- **Authors**: Prakhar Kulshreshtha, Brian Pugh, Salma Jiddi
- **Comment**: 5 pages, 5 figures, Published in CVPR Workshop on Computer Vision for
  Augmented and Virtual Reality, New Orleans, LA, 2022
- **Journal**: None
- **Summary**: In this paper, we address the problem of degradation in inpainting quality of neural networks operating at high resolutions. Inpainting networks are often unable to generate globally coherent structures at resolutions higher than their training set. This is partially attributed to the receptive field remaining static, despite an increase in image resolution. Although downscaling the image prior to inpainting produces coherent structure, it inherently lacks detail present at higher resolutions. To get the best of both worlds, we optimize the intermediate featuremaps of a network by minimizing a multiscale consistency loss at inference. This runtime optimization improves the inpainting results and establishes a new state-of-the-art for high resolution inpainting. Code is available at: https://github.com/geomagical/lama-with-refiner/tree/refinement.



