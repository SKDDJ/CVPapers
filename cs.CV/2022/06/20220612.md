# Arxiv Papers in cs.CV on 2022-06-12
### An Unsupervised Deep-Learning Method for Bone Age Assessment
- **Arxiv ID**: http://arxiv.org/abs/2206.05641v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05641v1)
- **Published**: 2022-06-12 02:31:36+00:00
- **Updated**: 2022-06-12 02:31:36+00:00
- **Authors**: Hao Zhu, Wan-Jing Nie, Yue-Jie Hou, Qi-Meng Du, Si-Jing Li, Chi-Chun Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: The bone age, reflecting the degree of development of the bones, can be used to predict the adult height and detect endocrine diseases of children. Both examinations of radiologists and variability of operators have a significant impact on bone age assessment. To decrease human intervention , machine learning algorithms are used to assess the bone age automatically. However, conventional supervised deep-learning methods need pre-labeled data. In this paper, based on the convolutional auto-encoder with constraints (CCAE), an unsupervised deep-learning model proposed in the classification of the fingerprint, we propose this model for the classification of the bone age and baptize it BA-CCAE. In the proposed BA-CCAE model, the key regions of the raw X-ray images of the bone age are encoded, yielding the latent vectors. The K-means clustering algorithm is used to obtain the final classifications by grouping the latent vectors of the bone images. A set of experiments on the Radiological Society of North America pediatric bone age dataset (RSNA) show that the accuracy of classifications at 48-month intervals is 76.15%. Although the accuracy now is lower than most of the existing supervised models, the proposed BA-CCAE model can establish the classification of bone age without any pre-labeled data, and to the best of our knowledge, the proposed BA-CCAE is one of the few trails using the unsupervised deep-learning method for the bone age assessment.



### A Fast Alternating Minimization Algorithm for Coded Aperture Snapshot Spectral Imaging Based on Sparsity and Deep Image Priors
- **Arxiv ID**: http://arxiv.org/abs/2206.05647v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2206.05647v1)
- **Published**: 2022-06-12 03:29:14+00:00
- **Updated**: 2022-06-12 03:29:14+00:00
- **Authors**: Qile Zhao, Xianhong Zhao, Xu Ma, Xudong Chen, Gonzalo R. Arce
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Coded aperture snapshot spectral imaging (CASSI) is a technique used to reconstruct three-dimensional hyperspectral images (HSIs) from one or several two-dimensional projection measurements. However, fewer projection measurements or more spectral channels leads to a severly ill-posed problem, in which case regularization methods have to be applied. In order to significantly improve the accuracy of reconstruction, this paper proposes a fast alternating minimization algorithm based on the sparsity and deep image priors (Fama-SDIP) of natural images. By integrating deep image prior (DIP) into the principle of compressive sensing (CS) reconstruction, the proposed algorithm can achieve state-of-the-art results without any training dataset. Extensive experiments show that Fama-SDIP method significantly outperforms prevailing leading methods on simulation and real HSI datasets.



### Indirect-Instant Attention Optimization for Crowd Counting in Dense Scenes
- **Arxiv ID**: http://arxiv.org/abs/2206.05648v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05648v1)
- **Published**: 2022-06-12 03:29:50+00:00
- **Updated**: 2022-06-12 03:29:50+00:00
- **Authors**: Suyu Han, Guodong Wang, Donghua Liu
- **Comment**: None
- **Journal**: None
- **Summary**: One of appealing approaches to guiding learnable parameter optimization, such as feature maps, is global attention, which enlightens network intelligence at a fraction of the cost. However, its loss calculation process still falls short: 1)We can only produce one-dimensional 'pseudo labels' for attention, since the artificial threshold involved in the procedure is not robust; 2) The attention awaiting loss calculation is necessarily high-dimensional, and decreasing it by convolution will inevitably introduce additional learnable parameters, thus confusing the source of the loss. To this end, we devise a simple but efficient Indirect-Instant Attention Optimization (IIAO) module based on SoftMax-Attention , which transforms high-dimensional attention map into a one-dimensional feature map in the mathematical sense for loss calculation midway through the network, while automatically providing adaptive multi-scale fusion to feature pyramid module. The special transformation yields relatively coarse features and, originally, the predictive fallibility of regions varies by crowd density distribution, so we tailor the Regional Correlation Loss (RCLoss) to retrieve continuous error-prone regions and smooth spatial information . Extensive experiments have proven that our approach surpasses previous SOTA methods in many benchmark datasets.



### TileGen: Tileable, Controllable Material Generation and Capture
- **Arxiv ID**: http://arxiv.org/abs/2206.05649v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05649v2)
- **Published**: 2022-06-12 03:32:05+00:00
- **Updated**: 2022-06-20 03:22:52+00:00
- **Authors**: Xilong Zhou, Miloš Hašan, Valentin Deschaintre, Paul Guerrero, Kalyan Sunkavalli, Nima Kalantari
- **Comment**: 18 pages, 19 figures
- **Journal**: None
- **Summary**: Recent methods (e.g. MaterialGAN) have used unconditional GANs to generate per-pixel material maps, or as a prior to reconstruct materials from input photographs. These models can generate varied random material appearance, but do not have any mechanism to constrain the generated material to a specific category or to control the coarse structure of the generated material, such as the exact brick layout on a brick wall. Furthermore, materials reconstructed from a single input photo commonly have artifacts and are generally not tileable, which limits their use in practical content creation pipelines. We propose TileGen, a generative model for SVBRDFs that is specific to a material category, always tileable, and optionally conditional on a provided input structure pattern. TileGen is a variant of StyleGAN whose architecture is modified to always produce tileable (periodic) material maps. In addition to the standard "style" latent code, TileGen can optionally take a condition image, giving a user direct control over the dominant spatial (and optionally color) features of the material. For example, in brick materials, the user can specify a brick layout and the brick color, or in leather materials, the locations of wrinkles and folds. Our inverse rendering approach can find a material perceptually matching a single target photograph by optimization. This reconstruction can also be conditional on a user-provided pattern. The resulting materials are tileable, can be larger than the target image, and are editable by varying the condition.



### Preprocessing Enhanced Image Compression for Machine Vision
- **Arxiv ID**: http://arxiv.org/abs/2206.05650v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05650v1)
- **Published**: 2022-06-12 03:36:38+00:00
- **Updated**: 2022-06-12 03:36:38+00:00
- **Authors**: Guo Lu, Xingtong Ge, Tianxiong Zhong, Jing Geng, Qiang Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, more and more images are compressed and sent to the back-end devices for the machine analysis tasks~(\textit{e.g.,} object detection) instead of being purely watched by humans. However, most traditional or learned image codecs are designed to minimize the distortion of the human visual system without considering the increased demand from machine vision systems. In this work, we propose a preprocessing enhanced image compression method for machine vision tasks to address this challenge. Instead of relying on the learned image codecs for end-to-end optimization, our framework is built upon the traditional non-differential codecs, which means it is standard compatible and can be easily deployed in practical applications. Specifically, we propose a neural preprocessing module before the encoder to maintain the useful semantic information for the downstream tasks and suppress the irrelevant information for bitrate saving. Furthermore, our neural preprocessing module is quantization adaptive and can be used in different compression ratios. More importantly, to jointly optimize the preprocessing module with the downstream machine vision tasks, we introduce the proxy network for the traditional non-differential codecs in the back-propagation stage. We provide extensive experiments by evaluating our compression method for two representative downstream tasks with different backbone networks. Experimental results show our method achieves a better trade-off between the coding bitrate and the performance of the downstream machine vision tasks by saving about 20% bitrate.



### STD-NET: Search of Image Steganalytic Deep-learning Architecture via Hierarchical Tensor Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2206.05651v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2206.05651v1)
- **Published**: 2022-06-12 03:46:08+00:00
- **Updated**: 2022-06-12 03:46:08+00:00
- **Authors**: Shunquan Tan, Qiushi Li, Laiyuan Li, Bin Li, Jiwu Huang
- **Comment**: Submitted to IEEE T-DSC
- **Journal**: None
- **Summary**: Recent studies shows that the majority of existing deep steganalysis models have a large amount of redundancy, which leads to a huge waste of storage and computing resources. The existing model compression method cannot flexibly compress the convolutional layer in residual shortcut block so that a satisfactory shrinking rate cannot be obtained. In this paper, we propose STD-NET, an unsupervised deep-learning architecture search approach via hierarchical tensor decomposition for image steganalysis. Our proposed strategy will not be restricted by various residual connections, since this strategy does not change the number of input and output channels of the convolution block. We propose a normalized distortion threshold to evaluate the sensitivity of each involved convolutional layer of the base model to guide STD-NET to compress target network in an efficient and unsupervised approach, and obtain two network structures of different shapes with low computation cost and similar performance compared with the original one. Extensive experiments have confirmed that, on one hand, our model can achieve comparable or even better detection performance in various steganalytic scenarios due to the great adaptivity of the obtained network architecture. On the other hand, the experimental results also demonstrate that our proposed strategy is more efficient and can remove more redundancy compared with previous steganalytic network compression methods.



### APT-36K: A Large-scale Benchmark for Animal Pose Estimation and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2206.05683v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05683v2)
- **Published**: 2022-06-12 07:18:36+00:00
- **Updated**: 2022-10-13 01:47:35+00:00
- **Authors**: Yuxiang Yang, Junjie Yang, Yufei Xu, Jing Zhang, Long Lan, Dacheng Tao
- **Comment**: Neurips 2022 dataset and benchmark track
- **Journal**: None
- **Summary**: Animal pose estimation and tracking (APT) is a fundamental task for detecting and tracking animal keypoints from a sequence of video frames. Previous animal-related datasets focus either on animal tracking or single-frame animal pose estimation, and never on both aspects. The lack of APT datasets hinders the development and evaluation of video-based animal pose estimation and tracking methods, limiting real-world applications, e.g., understanding animal behavior in wildlife conservation. To fill this gap, we make the first step and propose APT-36K, i.e., the first large-scale benchmark for animal pose estimation and tracking. Specifically, APT-36K consists of 2,400 video clips collected and filtered from 30 animal species with 15 frames for each video, resulting in 36,000 frames in total. After manual annotation and careful double-check, high-quality keypoint and tracking annotations are provided for all the animal instances. Based on APT-36K, we benchmark several representative models on the following three tracks: (1) supervised animal pose estimation on a single frame under intra- and inter-domain transfer learning settings, (2) inter-species domain generalization test for unseen animals, and (3) animal pose estimation with animal tracking. Based on the experimental results, we gain some empirical insights and show that APT-36K provides a valuable animal pose estimation and tracking benchmark, offering new challenges and opportunities for future research. The code and dataset will be made publicly available at https://github.com/pandorgan/APT-36K.



### DRNet: Decomposition and Reconstruction Network for Remote Physiological Measurement
- **Arxiv ID**: http://arxiv.org/abs/2206.05687v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05687v2)
- **Published**: 2022-06-12 07:40:10+00:00
- **Updated**: 2022-06-20 14:37:19+00:00
- **Authors**: Yuhang Dong, Gongping Yang, Yilong Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Remote photoplethysmography (rPPG) based physiological measurement has great application values in affective computing, non-contact health monitoring, telehealth monitoring, etc, which has become increasingly important especially during the COVID-19 pandemic. Existing methods are generally divided into two groups. The first focuses on mining the subtle blood volume pulse (BVP) signals from face videos, but seldom explicitly models the noises that dominate face video content. They are susceptible to the noises and may suffer from poor generalization ability in unseen scenarios. The second focuses on modeling noisy data directly, resulting in suboptimal performance due to the lack of regularity of these severe random noises. In this paper, we propose a Decomposition and Reconstruction Network (DRNet) focusing on the modeling of physiological features rather than noisy data. A novel cycle loss is proposed to constrain the periodicity of physiological information. Besides, a plug-and-play Spatial Attention Block (SAB) is proposed to enhance features along with the spatial location information. Furthermore, an efficient Patch Cropping (PC) augmentation strategy is proposed to synthesize augmented samples with different noise and features. Extensive experiments on different public datasets as well as the cross-database testing demonstrate the effectiveness of our approach.



### PD-DWI: Predicting response to neoadjuvant chemotherapy in invasive breast cancer with Physiologically-Decomposed Diffusion-Weighted MRI machine-learning model
- **Arxiv ID**: http://arxiv.org/abs/2206.05695v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2206.05695v1)
- **Published**: 2022-06-12 08:59:49+00:00
- **Updated**: 2022-06-12 08:59:49+00:00
- **Authors**: Maya Gilad, Moti Freiman
- **Comment**: Accepted to Medical Image Computing and Computer Assisted
  Intervention - MICCAI 2022 to be held during Sept 18-22 in Singapore
- **Journal**: None
- **Summary**: Early prediction of pathological complete response (pCR) following neoadjuvant chemotherapy (NAC) for breast cancer plays a critical role in surgical planning and optimizing treatment strategies. Recently, machine and deep-learning based methods were suggested for early pCR prediction from multi-parametric MRI (mp-MRI) data including dynamic contrast-enhanced MRI and diffusion-weighted MRI (DWI) with moderate success. We introduce PD-DWI, a physiologically decomposed DWI machine-learning model to predict pCR from DWI and clinical data. Our model first decomposes the raw DWI data into the various physiological cues that are influencing the DWI signal and then uses the decomposed data, in addition to clinical variables, as the input features of a radiomics-based XGBoost model. We demonstrated the added-value of our PD-DWI model over conventional machine-learning approaches for pCR prediction from mp-MRI data using the publicly available Breast Multi-parametric MRI for prediction of NAC Response (BMMR2) challenge. Our model substantially improves the area under the curve (AUC), compared to the current best result on the leaderboard (0.8849 vs. 0.8397) for the challenge test set. PD-DWI has the potential to improve prediction of pCR following NAC for breast cancer, reduce overall mp-MRI acquisition times and eliminate the need for contrast-agent injection.



### DPCN++: Differentiable Phase Correlation Network for Versatile Pose Registration
- **Arxiv ID**: http://arxiv.org/abs/2206.05707v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.05707v1)
- **Published**: 2022-06-12 10:00:34+00:00
- **Updated**: 2022-06-12 10:00:34+00:00
- **Authors**: Zexi Chen, Yiyi Liao, Haozhe Du, Haodong Zhang, Xuecheng Xu, Haojian Lu, Rong Xiong, Yue Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Pose registration is critical in vision and robotics. This paper focuses on the challenging task of initialization-free pose registration up to 7DoF for homogeneous and heterogeneous measurements. While recent learning-based methods show promise using differentiable solvers, they either rely on heuristically defined correspondences or are prone to local minima. We present a differentiable phase correlation (DPC) solver that is globally convergent and correspondence-free. When combined with simple feature extraction networks, our general framework DPCN++ allows for versatile pose registration with arbitrary initialization. Specifically, the feature extraction networks first learn dense feature grids from a pair of homogeneous/heterogeneous measurements. These feature grids are then transformed into a translation and scale invariant spectrum representation based on Fourier transform and spherical radial aggregation, decoupling translation and scale from rotation. Next, the rotation, scale, and translation are independently and efficiently estimated in the spectrum step-by-step using the DPC solver. The entire pipeline is differentiable and trained end-to-end. We evaluate DCPN++ on a wide range of registration tasks taking different input modalities, including 2D bird's-eye view images, 3D object and scene measurements, and medical images. Experimental results demonstrate that DCPN++ outperforms both classical and learning-based baselines, especially on partially observed and heterogeneous measurements.



### Narrowing the Gap: Improved Detector Training with Noisy Location Annotations
- **Arxiv ID**: http://arxiv.org/abs/2206.05708v1
- **DOI**: 10.1109/TIP.2022.3211468
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05708v1)
- **Published**: 2022-06-12 10:03:01+00:00
- **Updated**: 2022-06-12 10:03:01+00:00
- **Authors**: Shaoru Wang, Jin Gao, Bing Li, Weiming Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods require massive of annotated data for optimizing parameters. For example, datasets attached with accurate bounding box annotations are essential for modern object detection tasks. However, labeling with such pixel-wise accuracy is laborious and time-consuming, and elaborate labeling procedures are indispensable for reducing man-made noise, involving annotation review and acceptance testing. In this paper, we focus on the impact of noisy location annotations on the performance of object detection approaches and aim to, on the user side, reduce the adverse effect of the noise. First, noticeable performance degradation is experimentally observed for both one-stage and two-stage detectors when noise is introduced to the bounding box annotations. For instance, our synthesized noise results in performance decrease from 38.9% AP to 33.6% AP for FCOS detector on COCO test split, and 37.8%AP to 33.7%AP for Faster R-CNN. Second, a self-correction technique based on a Bayesian filter for prediction ensemble is proposed to better exploit the noisy location annotations following a Teacher-Student learning paradigm. Experiments for both synthesized and real-world scenarios consistently demonstrate the effectiveness of our approach, e.g., our method increases the degraded performance of the FCOS detector from 33.6% AP to 35.6% AP on COCO.



### Graph-based Spatial Transformer with Memory Replay for Multi-future Pedestrian Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2206.05712v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05712v1)
- **Published**: 2022-06-12 10:25:12+00:00
- **Updated**: 2022-06-12 10:25:12+00:00
- **Authors**: Lihuan Li, Maurice Pagnucco, Yang Song
- **Comment**: This paper has been accepted by CVPR 2022. Reference: Li, L.,
  Pagnucco, M. and Song, Y., 2022. Graph-Based Spatial Transformer With Memory
  Replay for Multi-Future Pedestrian Trajectory Prediction. In Proceedings of
  the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp.
  2231-2241)
- **Journal**: None
- **Summary**: Pedestrian trajectory prediction is an essential and challenging task for a variety of real-life applications such as autonomous driving and robotic motion planning. Besides generating a single future path, predicting multiple plausible future paths is becoming popular in some recent work on trajectory prediction. However, existing methods typically emphasize spatial interactions between pedestrians and surrounding areas but ignore the smoothness and temporal consistency of predictions. Our model aims to forecast multiple paths based on a historical trajectory by modeling multi-scale graph-based spatial transformers combined with a trajectory smoothing algorithm named ``Memory Replay'' utilizing a memory graph. Our method can comprehensively exploit the spatial information as well as correct the temporally inconsistent trajectories (e.g., sharp turns). We also propose a new evaluation metric named ``Percentage of Trajectory Usage'' to evaluate the comprehensiveness of diverse multi-future predictions. Our extensive experiments show that the proposed model achieves state-of-the-art performance on multi-future prediction and competitive results for single-future prediction. Code released at https://github.com/Jacobieee/ST-MR.



### Crowd Localization from Gaussian Mixture Scoped Knowledge and Scoped Teacher
- **Arxiv ID**: http://arxiv.org/abs/2206.05717v2
- **DOI**: 10.1109/TIP.2023.3251727
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05717v2)
- **Published**: 2022-06-12 11:07:15+00:00
- **Updated**: 2023-02-23 10:24:41+00:00
- **Authors**: Juncheng Wang, Junyu Gao, Yuan Yuan, Qi Wang
- **Comment**: Accepted by IEEE TIP
- **Journal**: None
- **Summary**: Crowd localization is to predict each instance head position in crowd scenarios. Since the distance of instances being to the camera are variant, there exists tremendous gaps among scales of instances within an image, which is called the intrinsic scale shift. The core reason of intrinsic scale shift being one of the most essential issues in crowd localization is that it is ubiquitous in crowd scenes and makes scale distribution chaotic.   To this end, the paper concentrates on access to tackle the chaos of the scale distribution incurred by intrinsic scale shift. We propose Gaussian Mixture Scope (GMS) to regularize the chaotic scale distribution. Concretely, the GMS utilizes a Gaussian mixture distribution to adapt to scale distribution and decouples the mixture model into sub-normal distributions to regularize the chaos within the sub-distributions. Then, an alignment is introduced to regularize the chaos among sub-distributions. However, despite that GMS is effective in regularizing the data distribution, it amounts to dislodging the hard samples in training set, which incurs overfitting. We assert that it is blamed on the block of transferring the latent knowledge exploited by GMS from data to model. Therefore, a Scoped Teacher playing a role of bridge in knowledge transform is proposed. What' s more, the consistency regularization is also introduced to implement knowledge transform. To that effect, the further constraints are deployed on Scoped Teacher to derive feature consistence between teacher and student end.   With proposed GMS and Scoped Teacher implemented on five mainstream datasets of crowd localization, the extensive experiments demonstrate the superiority of our work. Moreover, comparing with existing crowd locators, our work achieves state-of-the-art via F1-meansure comprehensively on five datasets.



### Object Occlusion of Adding New Categories in Objection Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.05730v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05730v2)
- **Published**: 2022-06-12 13:02:23+00:00
- **Updated**: 2022-06-14 08:38:23+00:00
- **Authors**: Boyang Deng, Meiyan Lin, Shoulun Long
- **Comment**: None
- **Journal**: None
- **Summary**: Building instance detection models that are data efficient and can handle rare object categories is an important challenge in computer vision. But data collection methods and metrics are lack of research towards real scenarios application using neural network. Here, we perform a systematic study of the Object Occlusion data collection and augmentation methods where we imitate object occlusion relationship in target scenarios. However, we find that the simple mechanism of object occlusion is good enough and can provide acceptable accuracy in real scenarios adding new category. We illustate that only adding 15 images of new category in a half million training dataset with hundreds categories, can give this new category 95% accuracy in unseen test dataset including thousands of images of this category.



### SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse Views
- **Arxiv ID**: http://arxiv.org/abs/2206.05737v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05737v2)
- **Published**: 2022-06-12 13:34:03+00:00
- **Updated**: 2022-08-02 10:53:02+00:00
- **Authors**: Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, Wenping Wang
- **Comment**: Project page: https://www.xxlong.site/SparseNeuS/
- **Journal**: None
- **Summary**: We introduce SparseNeuS, a novel neural rendering based method for the task of surface reconstruction from multi-view images. This task becomes more difficult when only sparse images are provided as input, a scenario where existing neural reconstruction approaches usually produce incomplete or distorted results. Moreover, their inability of generalizing to unseen new scenes impedes their application in practice. Contrarily, SparseNeuS can generalize to new scenes and work well with sparse images (as few as 2 or 3). SparseNeuS adopts signed distance function (SDF) as the surface representation, and learns generalizable priors from image features by introducing geometry encoding volumes for generic surface prediction. Moreover, several strategies are introduced to effectively leverage sparse views for high-quality reconstruction, including 1) a multi-level geometry reasoning framework to recover the surfaces in a coarse-to-fine manner; 2) a multi-scale color blending scheme for more reliable color prediction; 3) a consistency-aware fine-tuning scheme to control the inconsistent regions caused by occlusion and noise. Extensive experiments demonstrate that our approach not only outperforms the state-of-the-art methods, but also exhibits good efficiency, generalizability, and flexibility.



### Bootstrapping Multi-view Representations for Fake News Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.05741v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05741v3)
- **Published**: 2022-06-12 14:06:55+00:00
- **Updated**: 2022-09-22 10:22:27+00:00
- **Authors**: Qichao Ying, Xiaoxiao Hu, Yangming Zhou, Zhenxing Qian, Dan Zeng, Shiming Ge
- **Comment**: Authors are from Fudan University, China. Under Review
- **Journal**: None
- **Summary**: Previous researches on multimedia fake news detection include a series of complex feature extraction and fusion networks to gather useful information from the news. However, how cross-modal consistency relates to the fidelity of news and how features from different modalities affect the decision-making are still open questions. This paper presents a novel scheme of Bootstrapping Multi-view Representations (BMR) for fake news detection. Given a multi-modal news, we extract representations respectively from the views of the text, the image pattern and the image semantics. Improved Multi-gate Mixture-of-Expert networks (iMMoE) are proposed for feature refinement and fusion. Representations from each view are separately used to coarsely predict the fidelity of the whole news, and the multimodal representations are able to predict the cross-modal consistency. With the prediction scores, we reweigh each view of the representations and bootstrap them for fake news detection. Extensive experiments conducted on typical fake news detection datasets prove that the proposed BMR outperforms state-of-the-art schemes.



### Consistent Attack: Universal Adversarial Perturbation on Embodied Vision Navigation
- **Arxiv ID**: http://arxiv.org/abs/2206.05751v4
- **DOI**: 10.1016/j.patrec.2023.03.001
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05751v4)
- **Published**: 2022-06-12 14:45:11+00:00
- **Updated**: 2023-03-25 08:43:16+00:00
- **Authors**: Chengyang Ying, You Qiaoben, Xinning Zhou, Hang Su, Wenbo Ding, Jianyong Ai
- **Comment**: None
- **Journal**: Pattern Recognition Letters (PRL), 2023
- **Summary**: Embodied agents in vision navigation coupled with deep neural networks have attracted increasing attention. However, deep neural networks have been shown vulnerable to malicious adversarial noises, which may potentially cause catastrophic failures in Embodied Vision Navigation. Among different adversarial noises, universal adversarial perturbations (UAP), i.e., a constant image-agnostic perturbation applied on every input frame of the agent, play a critical role in Embodied Vision Navigation since they are computation-efficient and application-practical during the attack. However, existing UAP methods ignore the system dynamics of Embodied Vision Navigation and might be sub-optimal. In order to extend UAP to the sequential decision setting, we formulate the disturbed environment under the universal noise $\delta$, as a $\delta$-disturbed Markov Decision Process ($\delta$-MDP). Based on the formulation, we analyze the properties of $\delta$-MDP and propose two novel Consistent Attack methods, named Reward UAP and Trajectory UAP, for attacking Embodied agents, which consider the dynamic of the MDP and calculate universal noises by estimating the disturbed distribution and the disturbed Q function. For various victim models, our Consistent Attack can cause a significant drop in their performance in the PointGoal task in Habitat with different datasets and different scenes. Extensive experimental results indicate that there exist serious potential risks for applying Embodied Vision Navigation methods to the real world.



### SeATrans: Learning Segmentation-Assisted diagnosis model via Transformer
- **Arxiv ID**: http://arxiv.org/abs/2206.05763v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05763v2)
- **Published**: 2022-06-12 15:10:33+00:00
- **Updated**: 2022-06-22 12:25:40+00:00
- **Authors**: Junde Wu, Huihui Fang, Fangxin Shang, Dalu Yang, Zhaowei Wang, Jing Gao, Yehui Yang, Yanwu Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Clinically, the accurate annotation of lesions/tissues can significantly facilitate the disease diagnosis. For example, the segmentation of optic disc/cup (OD/OC) on fundus image would facilitate the glaucoma diagnosis, the segmentation of skin lesions on dermoscopic images is helpful to the melanoma diagnosis, etc. With the advancement of deep learning techniques, a wide range of methods proved the lesions/tissues segmentation can also facilitate the automated disease diagnosis models. However, existing methods are limited in the sense that they can only capture static regional correlations in the images. Inspired by the global and dynamic nature of Vision Transformer, in this paper, we propose Segmentation-Assisted diagnosis Transformer (SeATrans) to transfer the segmentation knowledge to the disease diagnosis network. Specifically, we first propose an asymmetric multi-scale interaction strategy to correlate each single low-level diagnosis feature with multi-scale segmentation features. Then, an effective strategy called SeA-block is adopted to vitalize diagnosis feature via correlated segmentation features. To model the segmentation-diagnosis interaction, SeA-block first embeds the diagnosis feature based on the segmentation information via the encoder, and then transfers the embedding back to the diagnosis feature space by a decoder. Experimental results demonstrate that SeATrans surpasses a wide range of state-of-the-art (SOTA) segmentation-assisted diagnosis methods on several disease diagnosis tasks.



### A Semantic Consistency Feature Alignment Object Detection Model Based on Mixed-Class Distribution Metrics
- **Arxiv ID**: http://arxiv.org/abs/2206.05765v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.05765v1)
- **Published**: 2022-06-12 15:14:45+00:00
- **Updated**: 2022-06-12 15:14:45+00:00
- **Authors**: Lijun Gou, Jinrong Yang, Hangcheng Yu, Pan Wang, Xiaoping Li, Chao Deng
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation is critical in various computer vision tasks, such as object detection, instance segmentation, etc. They attempt to reduce domain bias-induced performance degradation while also promoting model application speed. Previous works in domain adaptation object detection attempt to align image-level and instance-level shifts to eventually minimize the domain discrepancy, but they may align single-class features to mixed-class features in image-level domain adaptation because each image in the object detection task may be more than one class and object. In order to achieve single-class with single-class alignment and mixed-class with mixed-class alignment, we treat the mixed-class of the feature as a new class and propose a mixed-classes $H-divergence$ for object detection to achieve homogenous feature alignment and reduce negative transfer. Then, a Semantic Consistency Feature Alignment Model (SCFAM) based on mixed-classes $H-divergence$ was also presented. To improve single-class and mixed-class semantic information and accomplish semantic separation, the SCFAM model proposes Semantic Prediction Models (SPM) and Semantic Bridging Components (SBC). And the weight of the pix domain discriminator loss is then changed based on the SPM result to reduce sample imbalance. Extensive unsupervised domain adaption experiments on widely used datasets illustrate our proposed approach's robust object detection in domain bias settings.



### DSCA: A Dual-Stream Network with Cross-Attention on Whole-Slide Image Pyramids for Cancer Prognosis
- **Arxiv ID**: http://arxiv.org/abs/2206.05782v4
- **DOI**: 10.1016/j.eswa.2023.120280
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05782v4)
- **Published**: 2022-06-12 16:29:56+00:00
- **Updated**: 2023-03-28 10:44:40+00:00
- **Authors**: Pei Liu, Bo Fu, Feng Ye, Rui Yang, Bin Xu, Luping Ji
- **Comment**: 12 pages, 6 figures, 7 tables
- **Journal**: Expert Systems with Applications, 120280 (2023)
- **Summary**: The cancer prognosis on gigapixel Whole-Slide Images (WSIs) has always been a challenging task. To further enhance WSI visual representations, existing methods have explored image pyramids, instead of single-resolution images, in WSIs. In spite of this, they still face two major problems: high computational cost and the unnoticed semantical gap in multi-resolution feature fusion. To tackle these problems, this paper proposes to efficiently exploit WSI pyramids from a new perspective, the dual-stream network with cross-attention (DSCA). Our key idea is to utilize two sub-streams to process the WSI patches with two resolutions, where a square pooling is devised in a high-resolution stream to significantly reduce computational costs, and a cross-attention-based method is proposed to properly handle the fusion of dual-stream features. We validate our DSCA on three publicly-available datasets with a total number of 3,101 WSIs from 1,911 patients. Our experiments and ablation studies verify that (i) the proposed DSCA could outperform existing state-of-the-art methods in cancer prognosis, by an average C-Index improvement of around 4.6%; (ii) our DSCA network is more efficient in computation -- it has more learnable parameters (6.31M vs. 860.18K) but less computational costs (2.51G vs. 4.94G), compared to a typical existing multi-resolution network. (iii) the key components of DSCA, dual-stream and cross-attention, indeed contribute to our model's performance, gaining an average C-Index rise of around 2.0% while maintaining a relatively-small computational load. Our DSCA could serve as an alternative and effective tool for WSI-based cancer prognosis.



### Analysis of Branch Specialization and its Application in Image Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2206.05810v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2206.05810v1)
- **Published**: 2022-06-12 18:27:01+00:00
- **Updated**: 2022-06-12 18:27:01+00:00
- **Authors**: Jonathan Brokman, Guy Gilboa
- **Comment**: None
- **Journal**: None
- **Summary**: Branched neural networks have been used extensively for a variety of tasks. Branches are sub-parts of the model that perform independent processing followed by aggregation. It is known that this setting induces a phenomenon called Branch Specialization, where different branches become experts in different sub-tasks. Such observations were qualitative by nature. In this work, we present a methodological analysis of Branch Specialization. We explain the role of gradient descent in this phenomenon. We show that branched generative networks naturally decompose animal images to meaningful channels of fur, whiskers and spots and face images to channels such as different illumination components and face parts.



### COLD Fusion: Calibrated and Ordinal Latent Distribution Fusion for Uncertainty-Aware Multimodal Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.05833v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2206.05833v1)
- **Published**: 2022-06-12 20:25:21+00:00
- **Updated**: 2022-06-12 20:25:21+00:00
- **Authors**: Mani Kumar Tellamekala, Shahin Amiriparian, Björn W. Schuller, Elisabeth André, Timo Giesbrecht, Michel Valstar
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically recognising apparent emotions from face and voice is hard, in part because of various sources of uncertainty, including in the input data and the labels used in a machine learning framework. This paper introduces an uncertainty-aware audiovisual fusion approach that quantifies modality-wise uncertainty towards emotion prediction. To this end, we propose a novel fusion framework in which we first learn latent distributions over audiovisual temporal context vectors separately, and then constrain the variance vectors of unimodal latent distributions so that they represent the amount of information each modality provides w.r.t. emotion recognition. In particular, we impose Calibration and Ordinal Ranking constraints on the variance vectors of audiovisual latent distributions. When well-calibrated, modality-wise uncertainty scores indicate how much their corresponding predictions may differ from the ground truth labels. Well-ranked uncertainty scores allow the ordinal ranking of different frames across the modalities. To jointly impose both these constraints, we propose a softmax distributional matching loss. In both classification and regression settings, we compare our uncertainty-aware fusion model with standard model-agnostic fusion baselines. Our evaluation on two emotion recognition corpora, AVEC 2019 CES and IEMOCAP, shows that audiovisual emotion recognition can considerably benefit from well-calibrated and well-ranked latent uncertainty measures.



### GLIPv2: Unifying Localization and Vision-Language Understanding
- **Arxiv ID**: http://arxiv.org/abs/2206.05836v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2206.05836v2)
- **Published**: 2022-06-12 20:31:28+00:00
- **Updated**: 2022-10-11 23:27:03+00:00
- **Authors**: Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, Jianfeng Gao
- **Comment**: NeurIPS 2022; updated with reviewers' comments addressed; Code is
  released at https://github.com/microsoft/GLIP
- **Journal**: None
- **Summary**: We present GLIPv2, a grounded VL understanding model, that serves both localization tasks (e.g., object detection, instance segmentation) and Vision-Language (VL) understanding tasks (e.g., VQA, image captioning). GLIPv2 elegantly unifies localization pre-training and Vision-Language Pre-training (VLP) with three pre-training tasks: phrase grounding as a VL reformulation of the detection task, region-word contrastive learning as a novel region-word level contrastive learning task, and the masked language modeling. This unification not only simplifies the previous multi-stage VLP procedure but also achieves mutual benefits between localization and understanding tasks. Experimental results show that a single GLIPv2 model (all model weights are shared) achieves near SoTA performance on various localization and understanding tasks. The model also shows (1) strong zero-shot and few-shot adaption performance on open-vocabulary object detection tasks and (2) superior grounding capability on VL understanding tasks. Code will be released at https://github.com/microsoft/GLIP.



### NeuralODF: Learning Omnidirectional Distance Fields for 3D Shape Representation
- **Arxiv ID**: http://arxiv.org/abs/2206.05837v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05837v3)
- **Published**: 2022-06-12 20:59:26+00:00
- **Updated**: 2022-09-01 01:47:20+00:00
- **Authors**: Trevor Houchens, Cheng-You Lu, Shivam Duggal, Rao Fu, Srinath Sridhar
- **Comment**: None
- **Journal**: None
- **Summary**: In visual computing, 3D geometry is represented in many different forms including meshes, point clouds, voxel grids, level sets, and depth images. Each representation is suited for different tasks thus making the transformation of one representation into another (forward map) an important and common problem. We propose Omnidirectional Distance Fields (ODFs), a new 3D shape representation that encodes geometry by storing the depth to the object's surface from any 3D position in any viewing direction. Since rays are the fundamental unit of an ODF, it can be used to easily transform to and from common 3D representations like meshes or point clouds. Different from level set methods that are limited to representing closed surfaces, ODFs are unsigned and can thus model open surfaces (e.g., garments). We demonstrate that ODFs can be effectively learned with a neural network (NeuralODF) despite the inherent discontinuities at occlusion boundaries. We also introduce efficient forward mapping algorithms for transforming ODFs to and from common 3D representations. Specifically, we introduce an efficient Jumping Cubes algorithm for generating meshes from ODFs. Experiments demonstrate that NeuralODF can learn to capture high-quality shape by overfitting to a single object, and also learn to generalize on common shape categories.



### Efficiency Comparison of AI classification algorithms for Image Detection and Recognition in Real-time
- **Arxiv ID**: http://arxiv.org/abs/2206.05842v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.05842v1)
- **Published**: 2022-06-12 21:31:40+00:00
- **Updated**: 2022-06-12 21:31:40+00:00
- **Authors**: Musarrat Saberin Nipun, Rejwan Bin Sulaiman, Amer Kareem
- **Comment**: None
- **Journal**: None
- **Summary**: Face detection and identification is the most difficult and often used task in Artificial Intelligence systems. The goal of this study is to present and compare the results of several face detection and recognition algorithms used in the system. This system begins with a training image of a human, then continues on to the test image, identifying the face, comparing it to the trained face, and finally classifying it using OpenCV classifiers. This research will discuss the most effective and successful tactics used in the system, which are implemented using Python, OpenCV, and Matplotlib. It may also be used in locations with CCTV, such as public spaces, shopping malls, and ATM booths.



### FisheyeEX: Polar Outpainting for Extending the FoV of Fisheye Lens
- **Arxiv ID**: http://arxiv.org/abs/2206.05844v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05844v1)
- **Published**: 2022-06-12 21:38:50+00:00
- **Updated**: 2022-06-12 21:38:50+00:00
- **Authors**: Kang Liao, Chunyu Lin, Yunchao Wei, Yao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Fisheye lens gains increasing applications in computational photography and assisted driving because of its wide field of view (FoV). However, the fisheye image generally contains invalid black regions induced by its imaging model. In this paper, we present a FisheyeEX method that extends the FoV of the fisheye lens by outpainting the invalid regions, improving the integrity of captured scenes. Compared with the rectangle and undistorted image, there are two challenges for fisheye image outpainting: irregular painting regions and distortion synthesis. Observing the radial symmetry of the fisheye image, we first propose a polar outpainting strategy to extrapolate the coherent semantics from the center to the outside region. Such an outpainting manner considers the distribution pattern of radial distortion and the circle boundary, boosting a more reasonable completion direction. For the distortion synthesis, we propose a spiral distortion-aware perception module, in which the learning path keeps consistent with the distortion prior of the fisheye image. Subsequently, a scene revision module rearranges the generated pixels with the estimated distortion to match the fisheye image, thus extending the FoV. In the experiment, we evaluate the proposed FisheyeEX on three popular outdoor datasets: Cityscapes, BDD100k, and KITTI, and one real-world fisheye image dataset. The results demonstrate that our approach significantly outperforms the state-of-the-art methods, gaining around 27% more content beyond the original fisheye image.



### InBiaseD: Inductive Bias Distillation to Improve Generalization and Robustness through Shape-awareness
- **Arxiv ID**: http://arxiv.org/abs/2206.05846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05846v1)
- **Published**: 2022-06-12 21:56:29+00:00
- **Updated**: 2022-06-12 21:56:29+00:00
- **Authors**: Shruthi Gowda, Bahram Zonooz, Elahe Arani
- **Comment**: Accepted at 1st Conference on Lifelong Learning Agents (CoLLAs 2022)
- **Journal**: None
- **Summary**: Humans rely less on spurious correlations and trivial cues, such as texture, compared to deep neural networks which lead to better generalization and robustness. It can be attributed to the prior knowledge or the high-level cognitive inductive bias present in the brain. Therefore, introducing meaningful inductive bias to neural networks can help learn more generic and high-level representations and alleviate some of the shortcomings. We propose InBiaseD to distill inductive bias and bring shape-awareness to the neural networks. Our method includes a bias alignment objective that enforces the networks to learn more generic representations that are less vulnerable to unintended cues in the data which results in improved generalization performance. InBiaseD is less susceptible to shortcut learning and also exhibits lower texture bias. The better representations also aid in improving robustness to adversarial attacks and we hence plugin InBiaseD seamlessly into the existing adversarial training schemes to show a better trade-off between generalization and robustness.



### Modeling Generalized Specialist Approach To Train Quality Resilient Snapshot Ensemble
- **Arxiv ID**: http://arxiv.org/abs/2206.05853v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.05853v1)
- **Published**: 2022-06-12 22:40:46+00:00
- **Updated**: 2022-06-12 22:40:46+00:00
- **Authors**: Ghalib Ahmed Tahir, Chu Kiong Loo, Zongying Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) apply well with food image recognition due to the ability to learn discriminative visual features. Nevertheless, recognizing distorted images is challenging for existing CNNs. Hence, the study modelled a generalized specialist approach to train a quality resilient ensemble. The approach aids the models in the ensemble framework retain general skills of recognizing clean images and shallow skills of classifying noisy images with one deep expertise area on a particular distortion. Subsequently, a novel data augmentation random quality mixup (RQMixUp) is combined with snapshot ensembling to train G-Specialist. During each training cycle of G-Specialist, a model is fine-tuned on the synthetic images generated by RQMixup, intermixing clean and distorted images of a particular distortion at a randomly chosen level. Resultantly, each snapshot in the ensemble gained expertise on several distortion levels, with shallow skills on other quality distortions. Next, the filter outputs from diverse experts were fused for higher accuracy. The learning process has no additional cost due to a single training process to train experts, compatible with a wide range of supervised CNNs for transfer learning. Finally, the experimental analysis on three real-world food and a Malaysian food database showed significant improvement for distorted images with competitive classification performance on pristine food images.



### A Directed-Evolution Method for Sparsification and Compression of Neural Networks with Application to Object Identification and Segmentation and considerations of optimal quantization using small number of bits
- **Arxiv ID**: http://arxiv.org/abs/2206.05859v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, 68T01, 68T05, 68T07, 68T30, I.2.0; I.2.4; F.4.1; K.3.2
- **Links**: [PDF](http://arxiv.org/pdf/2206.05859v1)
- **Published**: 2022-06-12 23:49:08+00:00
- **Updated**: 2022-06-12 23:49:08+00:00
- **Authors**: Luiz M Franca-Neto
- **Comment**: 12 pages total, 5 figures, 2 appendices
- **Journal**: None
- **Summary**: This work introduces Directed-Evolution (DE) method for sparsification of neural networks, where the relevance of parameters to the network accuracy is directly assessed and the parameters that produce the least effect on accuracy when tentatively zeroed are indeed zeroed. DE method avoids a potentially combinatorial explosion of all possible candidate sets of parameters to be zeroed in large networks by mimicking evolution in the natural world. DE uses a distillation context [5]. In this context, the original network is the teacher and DE evolves the student neural network to the sparsification goal while maintaining minimal divergence between teacher and student. After the desired sparsification level is reached in each layer of the network by DE, a variety of quantization alternatives are used on the surviving parameters to find the lowest number of bits for their representation with acceptable loss of accuracy. A procedure to find optimal distribution of quantization levels in each sparsified layer is presented. Suitable final lossless encoding of the surviving quantized parameters is used for the final parameter representation. DE was used in sample of representative neural networks using MNIST, FashionMNIST and COCO data sets with progressive larger networks. An 80 classes YOLOv3 with more than 60 million parameters network trained on COCO dataset reached 90% sparsification and correctly identifies and segments all objects identified by the original network with more than 80% confidence using 4bit parameter quantization. Compression between 40x and 80x. It has not escaped the authors that techniques from different methods can be nested. Once the best parameter set for sparsification is identified in a cycle of DE, a decision on zeroing only a sub-set of those parameters can be made using a combination of criteria like parameter magnitude and Hessian approximations.



