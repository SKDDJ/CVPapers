# Arxiv Papers in cs.CV on 2022-06-30
### Personalized Showcases: Generating Multi-Modal Explanations for Recommendations
- **Arxiv ID**: http://arxiv.org/abs/2207.00422v2
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.00422v2)
- **Published**: 2022-06-30 01:43:58+00:00
- **Updated**: 2023-04-06 17:59:43+00:00
- **Authors**: An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian McAuley
- **Comment**: Accepted to SIGIR-23, with additional dataset details. Code and data:
  https://github.com/zzxslp/Gest
- **Journal**: None
- **Summary**: Existing explanation models generate only text for recommendations but still struggle to produce diverse contents. In this paper, to further enrich explanations, we propose a new task named personalized showcases, in which we provide both textual and visual information to explain our recommendations. Specifically, we first select a personalized image set that is the most relevant to a user's interest toward a recommended item. Then, natural language explanations are generated accordingly given our selected images. For this new task, we collect a large-scale dataset from Google Local (i.e.,~maps) and construct a high-quality subset for generating multi-modal explanations. We propose a personalized multi-modal framework which can generate diverse and visually-aligned explanations via contrastive learning. Experiments show that our framework benefits from different modalities as inputs, and is able to produce more diverse and expressive explanations compared to previous methods on a variety of evaluation metrics.



### Boosting 3D Object Detection by Simulating Multimodality on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2206.14971v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14971v1)
- **Published**: 2022-06-30 01:44:30+00:00
- **Updated**: 2022-06-30 01:44:30+00:00
- **Authors**: Wu Zheng, Mingxuan Hong, Li Jiang, Chi-Wing Fu
- **Comment**: Published in CVPR 2022 as Oral
- **Journal**: None
- **Summary**: This paper presents a new approach to boost a single-modality (LiDAR) 3D object detector by teaching it to simulate features and responses that follow a multi-modality (LiDAR-image) detector. The approach needs LiDAR-image data only when training the single-modality detector, and once well-trained, it only needs LiDAR data at inference. We design a novel framework to realize the approach: response distillation to focus on the crucial response samples and avoid the background samples; sparse-voxel distillation to learn voxel semantics and relations from the estimated crucial voxels; a fine-grained voxel-to-point distillation to better attend to features of small and distant objects; and instance distillation to further enhance the deep-feature consistency. Experimental results on the nuScenes dataset show that our approach outperforms all SOTA LiDAR-only 3D detectors and even surpasses the baseline LiDAR-image detector on the key NDS metric, filling 72% mAP gap between the single- and multi-modality detectors.



### Benchmarking the Robustness of Deep Neural Networks to Common Corruptions in Digital Pathology
- **Arxiv ID**: http://arxiv.org/abs/2206.14973v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.14973v1)
- **Published**: 2022-06-30 01:53:46+00:00
- **Updated**: 2022-06-30 01:53:46+00:00
- **Authors**: Yunlong Zhang, Yuxuan Sun, Honglin Li, Sunyi Zheng, Chenglu Zhu, Lin Yang
- **Comment**: MICAAI2022
- **Journal**: None
- **Summary**: When designing a diagnostic model for a clinical application, it is crucial to guarantee the robustness of the model with respect to a wide range of image corruptions. Herein, an easy-to-use benchmark is established to evaluate how deep neural networks perform on corrupted pathology images. Specifically, corrupted images are generated by injecting nine types of common corruptions into validation images. Besides, two classification and one ranking metrics are designed to evaluate the prediction and confidence performance under corruption. Evaluated on two resulting benchmark datasets, we find that (1) a variety of deep neural network models suffer from a significant accuracy decrease (double the error on clean images) and the unreliable confidence estimation on corrupted images; (2) A low correlation between the validation and test errors while replacing the validation set with our benchmark can increase the correlation. Our codes are available on https://github.com/superjamessyx/robustness_benchmark.



### A Unified End-to-End Retriever-Reader Framework for Knowledge-based VQA
- **Arxiv ID**: http://arxiv.org/abs/2206.14989v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14989v1)
- **Published**: 2022-06-30 02:35:04+00:00
- **Updated**: 2022-06-30 02:35:04+00:00
- **Authors**: Yangyang Guo, Liqiang Nie, Yongkang Wong, Yibing Liu, Zhiyong Cheng, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge-based Visual Question Answering (VQA) expects models to rely on external knowledge for robust answer prediction. Though significant it is, this paper discovers several leading factors impeding the advancement of current state-of-the-art methods. On the one hand, methods which exploit the explicit knowledge take the knowledge as a complement for the coarsely trained VQA model. Despite their effectiveness, these approaches often suffer from noise incorporation and error propagation. On the other hand, pertaining to the implicit knowledge, the multi-modal implicit knowledge for knowledge-based VQA still remains largely unexplored. This work presents a unified end-to-end retriever-reader framework towards knowledge-based VQA. In particular, we shed light on the multi-modal implicit knowledge from vision-language pre-training models to mine its potential in knowledge reasoning. As for the noise problem encountered by the retrieval operation on explicit knowledge, we design a novel scheme to create pseudo labels for effective knowledge supervision. This scheme is able to not only provide guidance for knowledge retrieval, but also drop these instances potentially error-prone towards question answering. To validate the effectiveness of the proposed method, we conduct extensive experiments on the benchmark dataset. The experimental results reveal that our method outperforms existing baselines by a noticeable margin. Beyond the reported numbers, this paper further spawns several insights on knowledge utilization for future research with some empirical findings.



### Cross-domain Federated Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.14996v2
- **DOI**: 10.1109/ICME55011.2023.00254
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14996v2)
- **Published**: 2022-06-30 03:09:59+00:00
- **Updated**: 2023-08-28 16:38:19+00:00
- **Authors**: Shangchao Su, Bin Li, Chengzhi Zhang, Mingzhao Yang, Xiangyang Xue
- **Comment**: ICME 2023
- **Journal**: None
- **Summary**: Detection models trained by one party (including server) may face severe performance degradation when distributed to other users (clients). Federated learning can enable multi-party collaborative learning without leaking client data. In this paper, we focus on a special cross-domain scenario in which the server has large-scale labeled data and multiple clients only have a small amount of labeled data; meanwhile, there exist differences in data distributions among the clients. In this case, traditional federated learning methods can't help a client learn both the global knowledge of all participants and its own unique knowledge. To make up for this limitation, we propose a cross-domain federated object detection framework, named FedOD. The proposed framework first performs the federated training to obtain a public global aggregated model through multi-teacher distillation, and sends the aggregated model back to each client for fine-tuning its personalized local model. After a few rounds of communication, on each client we can perform weighted ensemble inference on the public global model and the personalized local model. We establish a federated object detection dataset which has significant background differences and instance differences based on multiple public autonomous driving datasets, and then conduct extensive experiments on the dataset. The experimental results validate the effectiveness of the proposed method.



### Spatial Transformer Network with Transfer Learning for Small-scale Fine-grained Skeleton-based Tai Chi Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.15002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15002v1)
- **Published**: 2022-06-30 03:25:17+00:00
- **Updated**: 2022-06-30 03:25:17+00:00
- **Authors**: Lin Yuan, Zhen He, Qiang Wang, Leiyang Xu, Xiang Ma
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: Human action recognition is a quite hugely investigated area where most remarkable action recognition networks usually use large-scale coarse-grained action datasets of daily human actions as inputs to state the superiority of their networks. We intend to recognize our small-scale fine-grained Tai Chi action dataset using neural networks and propose a transfer-learning method using NTU RGB+D dataset to pre-train our network. More specifically, the proposed method first uses a large-scale NTU RGB+D dataset to pre-train the Transformer-based network for action recognition to extract common features among human motion. Then we freeze the network weights except for the fully connected (FC) layer and take our Tai Chi actions as inputs only to train the initialized FC weights. Experimental results show that our general model pipeline can reach a high accuracy of small-scale fine-grained Tai Chi action recognition with even few inputs and demonstrate that our method achieves the state-of-the-art performance compared with previous Tai Chi action recognition methods.



### GSCLIP : A Framework for Explaining Distribution Shifts in Natural Language
- **Arxiv ID**: http://arxiv.org/abs/2206.15007v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.15007v1)
- **Published**: 2022-06-30 04:06:26+00:00
- **Updated**: 2022-06-30 04:06:26+00:00
- **Authors**: Zhiying Zhu, Weixin Liang, James Zou
- **Comment**: Accepted by ICML 2022 DataPerf
- **Journal**: None
- **Summary**: Helping end users comprehend the abstract distribution shifts can greatly facilitate AI deployment. Motivated by this, we propose a novel task, dataset explanation. Given two image data sets, dataset explanation aims to automatically point out their dataset-level distribution shifts with natural language. Current techniques for monitoring distribution shifts provide inadequate information to understand datasets with the goal of improving data quality. Therefore, we introduce GSCLIP, a training-free framework to solve the dataset explanation task. In GSCLIP, we propose the selector as the first quantitative evaluation method to identify explanations that are proper to summarize dataset shifts. Furthermore, we leverage this selector to demonstrate the superiority of a generator based on language model generation. Systematic evaluation on natural data shift verifies that GSCLIP, a combined system of a hybrid generator group and an efficient selector is not only easy-to-use but also powerful for dataset explanation at scale.



### Exploring Temporally Dynamic Data Augmentation for Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.15015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15015v1)
- **Published**: 2022-06-30 04:34:34+00:00
- **Updated**: 2022-06-30 04:34:34+00:00
- **Authors**: Taeoh Kim, Jinhyung Kim, Minho Shim, Sangdoo Yun, Myunggu Kang, Dongyoon Wee, Sangyoun Lee
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Data augmentation has recently emerged as an essential component of modern training recipes for visual recognition tasks. However, data augmentation for video recognition has been rarely explored despite its effectiveness. Few existing augmentation recipes for video recognition naively extend the image augmentation methods by applying the same operations to the whole video frames. Our main idea is that the magnitude of augmentation operations for each frame needs to be changed over time to capture the real-world video's temporal variations. These variations should be generated as diverse as possible using fewer additional hyper-parameters during training. Through this motivation, we propose a simple yet effective video data augmentation framework, DynaAugment. The magnitude of augmentation operations on each frame is changed by an effective mechanism, Fourier Sampling that parameterizes diverse, smooth, and realistic temporal variations. DynaAugment also includes an extended search space suitable for video for automatic data augmentation methods. DynaAugment experimentally demonstrates that there are additional performance rooms to be improved from static augmentations on diverse video models. Specifically, we show the effectiveness of DynaAugment on various video datasets and tasks: large-scale video recognition (Kinetics-400 and Something-Something-v2), small-scale video recognition (UCF- 101 and HMDB-51), fine-grained video recognition (Diving-48 and FineGym), video action segmentation on Breakfast, video action localization on THUMOS'14, and video object detection on MOT17Det. DynaAugment also enables video models to learn more generalized representation to improve the model robustness on the corrupted videos.



### Timestamp-Supervised Action Segmentation with Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2206.15031v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15031v4)
- **Published**: 2022-06-30 05:56:24+00:00
- **Updated**: 2022-08-02 13:49:19+00:00
- **Authors**: Hamza Khan, Sanjay Haresh, Awais Ahmed, Shakeeb Siddiqui, Andrey Konin, M. Zeeshan Zia, Quoc-Huy Tran
- **Comment**: Accepted to IROS 2022
- **Journal**: None
- **Summary**: We introduce a novel approach for temporal activity segmentation with timestamp supervision. Our main contribution is a graph convolutional network, which is learned in an end-to-end manner to exploit both frame features and connections between neighboring frames to generate dense framewise labels from sparse timestamp labels. The generated dense framewise labels can then be used to train the segmentation model. In addition, we propose a framework for alternating learning of both the segmentation model and the graph convolutional model, which first initializes and then iteratively refines the learned models. Detailed experiments on four public datasets, including 50 Salads, GTEA, Breakfast, and Desktop Assembly, show that our method is superior to the multi-layer perceptron baseline, while performing on par with or better than the state of the art in temporal activity segmentation with timestamp supervision.



### PVT-COV19D: Pyramid Vision Transformer for COVID-19 Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2206.15069v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.15069v1)
- **Published**: 2022-06-30 07:05:50+00:00
- **Updated**: 2022-06-30 07:05:50+00:00
- **Authors**: Lilang Zheng, Jiaxuan Fang, Xiaorun Tang, Hanzhang Li, Jiaxin Fan, Tianyi Wang, Rui Zhou, Zhaoyan Yan
- **Comment**: 8 pages,1 figure
- **Journal**: None
- **Summary**: With the outbreak of COVID-19, a large number of relevant studies have emerged in recent years. We propose an automatic COVID-19 diagnosis framework based on lung CT scan images, the PVT-COV19D. In order to accommodate the different dimensions of the image input, we first classified the images using Transformer models, then sampled the images in the dataset according to normal distribution, and fed the sampling results into the modified PVTv2 model for training. A large number of experiments on the COV19-CT-DB dataset demonstrate the effectiveness of the proposed method.



### COVID Detection and Severity Prediction with 3D-ConvNeXt and Custom Pretrainings
- **Arxiv ID**: http://arxiv.org/abs/2206.15073v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.15073v2)
- **Published**: 2022-06-30 07:09:28+00:00
- **Updated**: 2022-08-17 14:38:43+00:00
- **Authors**: Daniel Kienzle, Julian Lorenz, Robin Schön, Katja Ludwig, Rainer Lienhart
- **Comment**: 17 pages, 3 figures, informations about challenge submission
- **Journal**: None
- **Summary**: Since COVID strongly affects the respiratory system, lung CT-scans can be used for the analysis of a patients health. We introduce a neural network for the prediction of the severity of lung damage and the detection of a COVID-infection using three-dimensional CT-data. Therefore, we adapt the recent ConvNeXt model to process three-dimensional data. Furthermore, we design and analyze different pretraining methods specifically designed to improve the models ability to handle three-dimensional CT-data. We rank 2nd in the 1st COVID19 Severity Detection Challenge and 3rd in the 2nd COVID19 Detection Challenge.



### UniDAformer: Unified Domain Adaptive Panoptic Segmentation Transformer via Hierarchical Mask Calibration
- **Arxiv ID**: http://arxiv.org/abs/2206.15083v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15083v2)
- **Published**: 2022-06-30 07:32:23+00:00
- **Updated**: 2023-03-22 05:52:23+00:00
- **Authors**: Jingyi Zhang, Jiaxing Huang, Xiaoqin Zhang, Shijian Lu
- **Comment**: Accepted to CVPR2023
- **Journal**: None
- **Summary**: Domain adaptive panoptic segmentation aims to mitigate data annotation challenge by leveraging off-the-shelf annotated data in one or multiple related source domains. However, existing studies employ two separate networks for instance segmentation and semantic segmentation which lead to excessive network parameters as well as complicated and computationally intensive training and inference processes. We design UniDAformer, a unified domain adaptive panoptic segmentation transformer that is simple but can achieve domain adaptive instance segmentation and semantic segmentation simultaneously within a single network. UniDAformer introduces Hierarchical Mask Calibration (HMC) that rectifies inaccurate predictions at the level of regions, superpixels and pixels via online self-training on the fly. It has three unique features: 1) it enables unified domain adaptive panoptic adaptation; 2) it mitigates false predictions and improves domain adaptive panoptic segmentation effectively; 3) it is end-to-end trainable with a much simpler training and inference pipeline. Extensive experiments over multiple public benchmarks show that UniDAformer achieves superior domain adaptive panoptic segmentation as compared with the state-of-the-art.



### Skeleton-based Action Recognition via Adaptive Cross-Form Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.15085v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2206.15085v1)
- **Published**: 2022-06-30 07:40:03+00:00
- **Updated**: 2022-06-30 07:40:03+00:00
- **Authors**: Xuanhan Wang, Yan Dai, Lianli Gao, Jingkuan Song
- **Comment**: None
- **Journal**: None
- **Summary**: Skeleton-based action recognition aims to project skeleton sequences to action categories, where skeleton sequences are derived from multiple forms of pre-detected points. Compared with earlier methods that focus on exploring single-form skeletons via Graph Convolutional Networks (GCNs), existing methods tend to improve GCNs by leveraging multi-form skeletons due to their complementary cues. However, these methods (either adapting structure of GCNs or model ensemble) require the co-existence of all forms of skeletons during both training and inference stages, while a typical situation in real life is the existence of only partial forms for inference. To tackle this issue, we present Adaptive Cross-Form Learning (ACFL), which empowers well-designed GCNs to generate complementary representation from single-form skeletons without changing model capacity. Specifically, each GCN model in ACFL not only learns action representation from the single-form skeletons, but also adaptively mimics useful representations derived from other forms of skeletons. In this way, each GCN can learn how to strengthen what has been learned, thus exploiting model potential and facilitating action recognition as well. Extensive experiments conducted on three challenging benchmarks, i.e., NTU-RGB+D 120, NTU-RGB+D 60 and UAV-Human, demonstrate the effectiveness and generalizability of the proposed method. Specifically, the ACFL significantly improves various GCN models (i.e., CTR-GCN, MS-G3D, and Shift-GCN), achieving a new record for skeleton-based action recognition.



### MKIoU Loss: Towards Accurate Oriented Object Detection in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2206.15109v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15109v1)
- **Published**: 2022-06-30 08:17:01+00:00
- **Updated**: 2022-06-30 08:17:01+00:00
- **Authors**: Xinyi Yu, Jiangping Lu, Xinyi Yu, Mi Lin, Linlin Ou
- **Comment**: None
- **Journal**: None
- **Summary**: Oriented bounding box regression is crucial for oriented object detection. However, regression-based methods often suffer from boundary problems and the inconsistency between loss and evaluation metrics. In this paper, a modulated Kalman IoU loss of approximate SkewIoU is proposed, named MKIoU. To avoid boundary problems, we convert the oriented bounding box to Gaussian distribution, then use the Kalman filter to approximate the intersection area. However, there exists significant difference between the calculated and actual intersection areas. Thus, we propose a modulation factor to adjust the sensitivity of angle deviation and width-height offset to loss variation, making the loss more consistent with the evaluation metric. Furthermore, the Gaussian modeling method avoids the boundary problem but causes the angle confusion of square objects simultaneously. Thus, the Gaussian Angle Loss (GA Loss) is presented to solve this problem by adding a corrected loss for square targets. The proposed GA Loss can be easily extended to other Gaussian-based methods. Experiments on three publicly available aerial image datasets, DOTA, UCAS-AOD, and HRSC2016, show the effectiveness of the proposed method.



### TENET: Transformer Encoding Network for Effective Temporal Flow on Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2207.00170v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.00170v1)
- **Published**: 2022-06-30 08:39:52+00:00
- **Updated**: 2022-06-30 08:39:52+00:00
- **Authors**: Yuting Wang, Hangning Zhou, Zhigang Zhang, Chen Feng, Huadong Lin, Chaofei Gao, Yizhi Tang, Zhenting Zhao, Shiyu Zhang, Jie Guo, Xuefeng Wang, Ziyao Xu, Chi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This technical report presents an effective method for motion prediction in autonomous driving. We develop a Transformer-based method for input encoding and trajectory prediction. Besides, we propose the Temporal Flow Header to enhance the trajectory encoding. In the end, an efficient K-means ensemble method is used. Using our Transformer network and ensemble method, we win the first place of Argoverse 2 Motion Forecasting Challenge with the state-of-the-art brier-minFDE score of 1.90.



### Detecting and Recovering Adversarial Examples from Extracting Non-robust and Highly Predictive Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2206.15128v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15128v2)
- **Published**: 2022-06-30 08:48:28+00:00
- **Updated**: 2022-08-30 14:17:12+00:00
- **Authors**: Mingyu Dong, Jiahao Chen, Diqun Yan, Jingxing Gao, Li Dong, Rangding Wang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have been shown to be vulnerable against adversarial examples (AEs) which are maliciously designed to fool target models. The normal examples (NEs) added with imperceptible adversarial perturbation, can be a security threat to DNNs. Although the existing AEs detection methods have achieved a high accuracy, they failed to exploit the information of the AEs detected. Thus, based on high-dimension perturbation extraction, we propose a model-free AEs detection method, the whole process of which is free from querying the victim model. Research shows that DNNs are sensitive to the high-dimension features. The adversarial perturbation hiding in the adversarial example belongs to the high-dimension feature which is highly predictive and non-robust. DNNs learn more details from high-dimension data than others. In our method, the perturbation extractor can extract the adversarial perturbation from AEs as high-dimension feature, then the trained AEs discriminator determines whether the input is an AE. Experimental results show that the proposed method can not only detect the adversarial examples with high accuracy, but also detect the specific category of the AEs. Meanwhile, the extracted perturbation can be used to recover the AEs to NEs.



### InsMix: Towards Realistic Generative Data Augmentation for Nuclei Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.15134v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.15134v1)
- **Published**: 2022-06-30 08:58:05+00:00
- **Updated**: 2022-06-30 08:58:05+00:00
- **Authors**: Yi Lin, Zeyu Wang, Kwang-Ting Cheng, Hao Chen
- **Comment**: Accepted by MICCAI 2022 (early accepted)
- **Journal**: None
- **Summary**: Nuclei Segmentation from histology images is a fundamental task in digital pathology analysis. However, deep-learning-based nuclei segmentation methods often suffer from limited annotations. This paper proposes a realistic data augmentation method for nuclei segmentation, named InsMix, that follows a Copy-Paste-Smooth principle and performs morphology-constrained generative instance augmentation. Specifically, we propose morphology constraints that enable the augmented images to acquire luxuriant information about nuclei while maintaining their morphology characteristics (e.g., geometry and location). To fully exploit the pixel redundancy of the background and improve the model's robustness, we further propose a background perturbation method, which randomly shuffles the background patches without disordering the original nuclei distribution. To achieve contextual consistency between original and template instances, a smooth-GAN is designed with a foreground similarity encoder (FSE) and a triplet loss. We validated the proposed method on two datasets, i.e., Kumar and CPS datasets. Experimental results demonstrate the effectiveness of each component and the superior performance achieved by our method to the state-of-the-art methods.



### DFGC 2022: The Second DeepFake Game Competition
- **Arxiv ID**: http://arxiv.org/abs/2206.15138v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15138v2)
- **Published**: 2022-06-30 09:13:06+00:00
- **Updated**: 2022-10-04 01:56:02+00:00
- **Authors**: Bo Peng, Wei Xiang, Yue Jiang, Wei Wang, Jing Dong, Zhenan Sun, Zhen Lei, Siwei Lyu
- **Comment**: Accepted by IJCB 2022
- **Journal**: None
- **Summary**: This paper presents the summary report on our DFGC 2022 competition. The DeepFake is rapidly evolving, and realistic face-swaps are becoming more deceptive and difficult to detect. On the contrary, methods for detecting DeepFakes are also improving. There is a two-party game between DeepFake creators and defenders. This competition provides a common platform for benchmarking the game between the current state-of-the-arts in DeepFake creation and detection methods. The main research question to be answered by this competition is the current state of the two adversaries when competed with each other. This is the second edition after the last year's DFGC 2021, with a new, more diverse video dataset, a more realistic game setting, and more reasonable evaluation metrics. With this competition, we aim to stimulate research ideas for building better defenses against the DeepFake threats. We also release our DFGC 2022 dataset contributed by both our participants and ourselves to enrich the DeepFake data resources for the research community (https://github.com/NiCE-X/DFGC-2022).



### BoxGraph: Semantic Place Recognition and Pose Estimation from 3D LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2206.15154v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.15154v1)
- **Published**: 2022-06-30 09:39:08+00:00
- **Updated**: 2022-06-30 09:39:08+00:00
- **Authors**: Georgi Pramatarov, Daniele De Martini, Matthew Gadd, Paul Newman
- **Comment**: Accepted for publication at the IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS) 2022
- **Journal**: None
- **Summary**: This paper is about extremely robust and lightweight localisation using LiDAR point clouds based on instance segmentation and graph matching. We model 3D point clouds as fully-connected graphs of semantically identified components where each vertex corresponds to an object instance and encodes its shape. Optimal vertex association across graphs allows for full 6-Degree-of-Freedom (DoF) pose estimation and place recognition by measuring similarity. This representation is very concise, condensing the size of maps by a factor of 25 against the state-of-the-art, requiring only 3kB to represent a 1.4MB laser scan. We verify the efficacy of our system on the SemanticKITTI dataset, where we achieve a new state-of-the-art in place recognition, with an average of 88.4% recall at 100% precision where the next closest competitor follows with 64.9%. We also show accurate metric pose estimation performance - estimating 6-DoF pose with median errors of 10 cm and 0.33 deg.



### HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.15157v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.15157v3)
- **Published**: 2022-06-30 09:40:05+00:00
- **Updated**: 2023-08-11 11:06:09+00:00
- **Authors**: Tim Broedermann, Christos Sakaridis, Dengxin Dai, Luc Van Gool
- **Comment**: IEEE International Conference on Intelligent Transportation Systems
  (ITSC) 2023
- **Journal**: None
- **Summary**: Besides standard cameras, autonomous vehicles typically include multiple additional sensors, such as lidars and radars, which help acquire richer information for perceiving the content of the driving scene. While several recent works focus on fusing certain pairs of sensors - such as camera with lidar or radar - by using architectural components specific to the examined setting, a generic and modular sensor fusion architecture is missing from the literature. In this work, we propose HRFuser, a modular architecture for multi-modal 2D object detection. It fuses multiple sensors in a multi-resolution fashion and scales to an arbitrary number of input modalities. The design of HRFuser is based on state-of-the-art high-resolution networks for image-only dense prediction and incorporates a novel multi-window cross-attention block as the means to perform fusion of multiple modalities at multiple resolutions. We demonstrate via extensive experiments on nuScenes and the adverse conditions DENSE datasets that our model effectively leverages complementary features from additional modalities, substantially improving upon camera-only performance and consistently outperforming state-of-the-art 3D and 2D fusion methods evaluated on 2D object detection metrics. The source code is publicly available.



### LiDAR-as-Camera for End-to-End Driving
- **Arxiv ID**: http://arxiv.org/abs/2206.15170v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO, 68T07, 68T40, I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2206.15170v1)
- **Published**: 2022-06-30 10:06:49+00:00
- **Updated**: 2022-06-30 10:06:49+00:00
- **Authors**: Ardi Tampuu, Romet Aidla, Jan Are van Gent, Tambet Matiisen
- **Comment**: None
- **Journal**: None
- **Summary**: The core task of any autonomous driving system is to transform sensory inputs into driving commands. In end-to-end driving, this is achieved via a neural network, with one or multiple cameras as the most commonly used input and low-level driving command, e.g. steering angle, as output. However, depth-sensing has been shown in simulation to make the end-to-end driving task easier. On a real car, combining depth and visual information can be challenging, due to the difficulty of obtaining good spatial and temporal alignment of the sensors. To alleviate alignment problems, Ouster LiDARs can output surround-view LiDAR-images with depth, intensity, and ambient radiation channels. These measurements originate from the same sensor, rendering them perfectly aligned in time and space. We demonstrate that such LiDAR-images are sufficient for the real-car road-following task and perform at least equally to camera-based models in the tested conditions, with the difference increasing when needing to generalize to new weather conditions. In the second direction of study, we reveal that the temporal smoothness of off-policy prediction sequences correlates equally well with actual on-policy driving ability as the commonly used mean absolute error.



### A Medical Image Fusion Method based on MDLatLRRv2
- **Arxiv ID**: http://arxiv.org/abs/2206.15179v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.15179v3)
- **Published**: 2022-06-30 10:31:30+00:00
- **Updated**: 2023-08-27 04:12:59+00:00
- **Authors**: Xu Song, Xiao-Jun Wu, Hui Li
- **Comment**: There are some errors that need to be corrected
- **Journal**: None
- **Summary**: Since MDLatLRR only considers detailed parts (salient features) of input images extracted by latent low-rank representation (LatLRR), it doesn't use base parts (principal features) extracted by LatLRR effectively. Therefore, we proposed an improved multi-level decomposition method called MDLatLRRv2 which effectively analyzes and utilizes all the image features obtained by LatLRR. Then we apply MDLatLRRv2 to medical image fusion. The base parts are fused by average strategy and the detail parts are fused by nuclear-norm operation. The comparison with the existing methods demonstrates that the proposed method can achieve state-of-the-art fusion performance in objective and subjective assessment.



### The (de)biasing effect of GAN-based augmentation methods on skin lesion images
- **Arxiv ID**: http://arxiv.org/abs/2206.15182v1
- **DOI**: 10.1007/978-3-031-16452-1_42
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.15182v1)
- **Published**: 2022-06-30 10:32:35+00:00
- **Updated**: 2022-06-30 10:32:35+00:00
- **Authors**: Agnieszka Mikołajczyk, Sylwia Majchrowska, Sandra Carrasco Limeros
- **Comment**: Accepted to MICCAI2022
- **Journal**: In: Wang, L., Dou, Q., Fletcher, P.T., Speidel, S., Li, S. (eds)
  Medical Image Computing and Computer Assisted Intervention - MICCAI 2022.
  MICCAI 2022. Lecture Notes in Computer Science, vol 13438. Springer, Cham
- **Summary**: New medical datasets are now more open to the public, allowing for better and more extensive research. Although prepared with the utmost care, new datasets might still be a source of spurious correlations that affect the learning process. Moreover, data collections are usually not large enough and are often unbalanced. One approach to alleviate the data imbalance is using data augmentation with Generative Adversarial Networks (GANs) to extend the dataset with high-quality images. GANs are usually trained on the same biased datasets as the target data, resulting in more biased instances. This work explored unconditional and conditional GANs to compare their bias inheritance and how the synthetic data influenced the models. We provided extensive manual data annotation of possibly biasing artifacts on the well-known ISIC dataset with skin lesions. In addition, we examined classification models trained on both real and synthetic data with counterfactual bias explanations. Our experiments showed that GANs inherited biases and sometimes even amplified them, leading to even stronger spurious correlations. Manual data annotation and synthetic images are publicly available for reproducible scientific research.



### Out-of-Distribution Detection for Long-tailed and Fine-grained Skin Lesion Images
- **Arxiv ID**: http://arxiv.org/abs/2206.15186v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.15186v1)
- **Published**: 2022-06-30 10:53:27+00:00
- **Updated**: 2022-06-30 10:53:27+00:00
- **Authors**: Deval Mehta, Yaniv Gal, Adrian Bowling, Paul Bonnington, Zongyuan Ge
- **Comment**: Accepted to MICCAI 2022 (top 13% paper; early accept)
- **Journal**: None
- **Summary**: Recent years have witnessed a rapid development of automated methods for skin lesion diagnosis and classification. Due to an increasing deployment of such systems in clinics, it has become important to develop a more robust system towards various Out-of-Distribution(OOD) samples (unknown skin lesions and conditions). However, the current deep learning models trained for skin lesion classification tend to classify these OOD samples incorrectly into one of their learned skin lesion categories. To address this issue, we propose a simple yet strategic approach that improves the OOD detection performance while maintaining the multi-class classification accuracy for the known categories of skin lesion. To specify, this approach is built upon a realistic scenario of a long-tailed and fine-grained OOD detection task for skin lesion images. Through this approach, 1) First, we target the mixup amongst middle and tail classes to address the long-tail problem. 2) Later, we combine the above mixup strategy with prototype learning to address the fine-grained nature of the dataset. The unique contribution of this paper is two-fold, justified by extensive experiments. First, we present a realistic problem setting of OOD task for skin lesion. Second, we propose an approach to target the long-tailed and fine-grained aspects of the problem setting simultaneously to increase the OOD performance.



### Simulating reaction time for Eureka effect in visual object recognition using artificial neural network
- **Arxiv ID**: http://arxiv.org/abs/2207.00815v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.00815v1)
- **Published**: 2022-06-30 10:58:12+00:00
- **Updated**: 2022-06-30 10:58:12+00:00
- **Authors**: Kazufumi Hosoda, Shigeto Seno, Tsutomu Murata
- **Comment**: 2 pages, 2 figures
- **Journal**: None
- **Summary**: The human brain can recognize objects hidden in even severely degraded images after observing them for a while, which is known as a type of Eureka effect, possibly associated with human creativity. A previous psychological study suggests that the basis of this "Eureka recognition" is neural processes of coincidence of multiple stochastic activities. Here we constructed an artificial-neural-network-based model that simulated the characteristics of the human Eureka recognition.



### Multi-Granularity Regularized Re-Balancing for Class Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.15189v1
- **DOI**: 10.1109/TKDE.2022.3188335
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15189v1)
- **Published**: 2022-06-30 11:04:51+00:00
- **Updated**: 2022-06-30 11:04:51+00:00
- **Authors**: Huitong Chen, Yu Wang, Qinghua Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models suffer from catastrophic forgetting when learning new tasks incrementally. Incremental learning has been proposed to retain the knowledge of old classes while learning to identify new classes. A typical approach is to use a few exemplars to avoid forgetting old knowledge. In such a scenario, data imbalance between old and new classes is a key issue that leads to performance degradation of the model. Several strategies have been designed to rectify the bias towards the new classes due to data imbalance. However, they heavily rely on the assumptions of the bias relation between old and new classes. Therefore, they are not suitable for complex real-world applications. In this study, we propose an assumption-agnostic method, Multi-Granularity Regularized re-Balancing (MGRB), to address this problem. Re-balancing methods are used to alleviate the influence of data imbalance; however, we empirically discover that they would under-fit new classes. To this end, we further design a novel multi-granularity regularization term that enables the model to consider the correlations of classes in addition to re-balancing the data. A class hierarchy is first constructed by grouping the semantically or visually similar classes. The multi-granularity regularization then transforms the one-hot label vector into a continuous label distribution, which reflects the relations between the target class and other classes based on the constructed class hierarchy. Thus, the model can learn the inter-class relational information, which helps enhance the learning of both old and new classes. Experimental results on both public datasets and a real-world fault diagnosis dataset verify the effectiveness of the proposed method.



### Implicit U-Net for volumetric medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.15217v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.15217v1)
- **Published**: 2022-06-30 12:00:40+00:00
- **Updated**: 2022-06-30 12:00:40+00:00
- **Authors**: Sergio Naval Marimont, Giacomo Tarroni
- **Comment**: 11 pages, 4 figures, Accepted MIUA 2022
- **Journal**: None
- **Summary**: U-Net has been the go-to architecture for medical image segmentation tasks, however computational challenges arise when extending the U-Net architecture to 3D images. We propose the Implicit U-Net architecture that adapts the efficient Implicit Representation paradigm to supervised image segmentation tasks. By combining a convolutional feature extractor with an implicit localization network, our implicit U-Net has 40% less parameters than the equivalent U-Net. Moreover, we propose training and inference procedures to capitalize sparse predictions. When comparing to an equivalent fully convolutional U-Net, Implicit U-Net reduces by approximately 30% inference and training time as well as training memory footprint while achieving comparable results in our experiments with two different abdominal CT scan datasets.



### CTrGAN: Cycle Transformers GAN for Gait Transfer
- **Arxiv ID**: http://arxiv.org/abs/2206.15248v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15248v4)
- **Published**: 2022-06-30 12:53:45+00:00
- **Updated**: 2023-01-07 12:59:41+00:00
- **Authors**: Shahar Mahpod, Noam Gaash, Hay Hoffman, Gil Ben-Artzi
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel approach for gait transfer from unconstrained videos in-the-wild. In contrast to motion transfer, the objective here is not to imitate the source's motions by the target, but rather to replace the walking source with the target, while transferring the target's typical gait. Our approach can be trained only once with multiple sources and is able to transfer the gait of the target from unseen sources, eliminating the need for retraining for each new source independently. Furthermore, we propose a novel metrics for gait transfer based on gait recognition models that enable to quantify the quality of the transferred gait, and show that existing techniques yield a discrepancy that can be easily detected.   We introduce Cycle Transformers GAN (CTrGAN), that consist of a decoder and encoder, both Transformers, where the attention is on the temporal domain between complete images rather than the spatial domain between patches. Using a widely-used gait recognition dataset, we demonstrate that our approach is capable of producing over an order of magnitude more realistic personalized gaits than existing methods, even when used with sources that were not available during training. As part of our solution, we present a detector that determines whether a video is real or generated by our model.



### Localizing the Recurrent Laryngeal Nerve via Ultrasound with a Bayesian Shape Framework
- **Arxiv ID**: http://arxiv.org/abs/2206.15254v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.15254v1)
- **Published**: 2022-06-30 13:04:42+00:00
- **Updated**: 2022-06-30 13:04:42+00:00
- **Authors**: Haoran Dou, Luyi Han, Yushuang He, Jun Xu, Nishant Ravikumar, Ritse Mann, Alejandro F. Frangi, Pew-Thian Yap, Yunzhi Huang
- **Comment**: Early Accepted by MICCAI 2022
- **Journal**: None
- **Summary**: Tumor infiltration of the recurrent laryngeal nerve (RLN) is a contraindication for robotic thyroidectomy and can be difficult to detect via standard laryngoscopy. Ultrasound (US) is a viable alternative for RLN detection due to its safety and ability to provide real-time feedback. However, the tininess of the RLN, with a diameter typically less than 3mm, poses significant challenges to the accurate localization of the RLN. In this work, we propose a knowledge-driven framework for RLN localization, mimicking the standard approach surgeons take to identify the RLN according to its surrounding organs. We construct a prior anatomical model based on the inherent relative spatial relationships between organs. Through Bayesian shape alignment (BSA), we obtain the candidate coordinates of the center of a region of interest (ROI) that encloses the RLN. The ROI allows a decreased field of view for determining the refined centroid of the RLN using a dual-path identification network, based on multi-scale semantic information. Experimental results indicate that the proposed method achieves superior hit rates and substantially smaller distance errors compared with state-of-the-art methods.



### Neural Rendering for Stereo 3D Reconstruction of Deformable Tissues in Robotic Surgery
- **Arxiv ID**: http://arxiv.org/abs/2206.15255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15255v1)
- **Published**: 2022-06-30 13:06:27+00:00
- **Updated**: 2022-06-30 13:06:27+00:00
- **Authors**: Yuehao Wang, Yonghao Long, Siu Hin Fan, Qi Dou
- **Comment**: 11 pages, 4 figures, conference
- **Journal**: None
- **Summary**: Reconstruction of the soft tissues in robotic surgery from endoscopic stereo videos is important for many applications such as intra-operative navigation and image-guided robotic surgery automation. Previous works on this task mainly rely on SLAM-based approaches, which struggle to handle complex surgical scenes. Inspired by recent progress in neural rendering, we present a novel framework for deformable tissue reconstruction from binocular captures in robotic surgery under the single-viewpoint setting. Our framework adopts dynamic neural radiance fields to represent deformable surgical scenes in MLPs and optimize shapes and deformations in a learning-based manner. In addition to non-rigid deformations, tool occlusion and poor 3D clues from a single viewpoint are also particular challenges in soft tissue reconstruction. To overcome these difficulties, we present a series of strategies of tool mask-guided ray casting, stereo depth-cueing ray marching and stereo depth-supervised optimization. With experiments on DaVinci robotic surgery videos, our method significantly outperforms the current state-of-the-art reconstruction method for handling various complex non-rigid deformations. To our best knowledge, this is the first work leveraging neural rendering for surgical scene 3D reconstruction with remarkable potential demonstrated. Code is available at: https://github.com/med-air/EndoNeRF.



### Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D Camera
- **Arxiv ID**: http://arxiv.org/abs/2206.15258v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2206.15258v2)
- **Published**: 2022-06-30 13:09:39+00:00
- **Updated**: 2022-10-14 16:32:26+00:00
- **Authors**: Hongrui Cai, Wanquan Feng, Xuetao Feng, Yan Wang, Juyong Zhang
- **Comment**: Project page: https://ustc3dv.github.io/ndr
- **Journal**: None
- **Summary**: We propose Neural-DynamicReconstruction (NDR), a template-free method to recover high-fidelity geometry and motions of a dynamic scene from a monocular RGB-D camera. In NDR, we adopt the neural implicit function for surface representation and rendering such that the captured color and depth can be fully utilized to jointly optimize the surface and deformations. To represent and constrain the non-rigid deformations, we propose a novel neural invertible deforming network such that the cycle consistency between arbitrary two frames is automatically satisfied. Considering that the surface topology of dynamic scene might change over time, we employ a topology-aware strategy to construct the topology-variant correspondence for the fused frames. NDR also further refines the camera poses in a global optimization manner. Experiments on public datasets and our collected dataset demonstrate that NDR outperforms existing monocular dynamic reconstruction methods.



### Submission to Generic Event Boundary Detection Challenge@CVPR 2022: Local Context Modeling and Global Boundary Decoding Approach
- **Arxiv ID**: http://arxiv.org/abs/2206.15268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15268v1)
- **Published**: 2022-06-30 13:19:53+00:00
- **Updated**: 2022-06-30 13:19:53+00:00
- **Authors**: Jiaqi Tang, Zhaoyang Liu, Jing Tan, Chen Qian, Wayne Wu, Limin Wang
- **Comment**: arXiv admin note: text overlap with arXiv:2112.04771
- **Journal**: None
- **Summary**: Generic event boundary detection (GEBD) is an important yet challenging task in video understanding, which aims at detecting the moments where humans naturally perceive event boundaries. In this paper, we present a local context modeling and global boundary decoding approach for GEBD task. Local context modeling sub-network is proposed to perceive diverse patterns of generic event boundaries, and it generates powerful video representations and reliable boundary confidence. Based on them, global boundary decoding sub-network is exploited to decode event boundaries from a global view. Our proposed method achieves 85.13% F1-score on Kinetics-GEBD testing set, which achieves a more than 22% F1-score boost compared to the baseline method. The code is available at https://github.com/JackyTown/GEBD_Challenge_CVPR2022.



### Augment like there's no tomorrow: Consistently performing neural networks for medical imaging
- **Arxiv ID**: http://arxiv.org/abs/2206.15274v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.15274v2)
- **Published**: 2022-06-30 13:25:34+00:00
- **Updated**: 2022-12-02 12:51:20+00:00
- **Authors**: Joona Pohjonen, Carolin Stürenberg, Atte Föhr, Reija Randen-Brady, Lassi Luomala, Jouni Lohi, Esa Pitkänen, Antti Rannikko, Tuomas Mirtti
- **Comment**: Code for the paper is available from
  https://www.github.com/jopo666/StrongAugment
- **Journal**: None
- **Summary**: Deep neural networks have achieved impressive performance in a wide variety of medical imaging tasks. However, these models often fail on data not used during training, such as data originating from a different medical centre. How to recognize models suffering from this fragility, and how to design robust models are the main obstacles to clinical adoption. Here, we present general methods to identify causes for model generalisation failures and how to circumvent them. First, we use $\textit{distribution-shifted datasets}$ to show that models trained with current state-of-the-art methods are highly fragile to variability encountered in clinical practice, and then develop a $\textit{strong augmentation}$ strategy to address this fragility. Distribution-shifted datasets allow us to discover this fragility, which can otherwise remain undetected after validation against multiple external datasets. Strong augmentation allows us to train robust models achieving consistent performance under shifts from the training data distribution. Importantly, we demonstrate that strong augmentation yields biomedical imaging models which retain high performance when applied to real-world clinical data. Our results pave the way for the development and evaluation of reliable and robust neural networks in clinical practice.



### Multiclass-SGCN: Sparse Graph-based Trajectory Prediction with Agent Class Embedding
- **Arxiv ID**: http://arxiv.org/abs/2206.15275v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15275v1)
- **Published**: 2022-06-30 13:28:53+00:00
- **Updated**: 2022-06-30 13:28:53+00:00
- **Authors**: Ruochen Li, Stamos Katsigiannis, Hubert P. H. Shum
- **Comment**: None
- **Journal**: None
- **Summary**: Trajectory prediction of road users in real-world scenarios is challenging because their movement patterns are stochastic and complex. Previous pedestrian-oriented works have been successful in modelling the complex interactions among pedestrians, but fail in predicting trajectories when other types of road users are involved (e.g., cars, cyclists, etc.), because they ignore user types. Although a few recent works construct densely connected graphs with user label information, they suffer from superfluous spatial interactions and temporal dependencies. To address these issues, we propose Multiclass-SGCN, a sparse graph convolution network based approach for multi-class trajectory prediction that takes into consideration velocity and agent label information and uses a novel interaction mask to adaptively decide the spatial and temporal connections of agents based on their interaction scores. The proposed approach significantly outperformed state-of-the-art approaches on the Stanford Drone Dataset, providing more realistic and plausible trajectory predictions.



### TINC: Temporally Informed Non-Contrastive Learning for Disease Progression Modeling in Retinal OCT Volumes
- **Arxiv ID**: http://arxiv.org/abs/2206.15282v1
- **DOI**: 10.1007/978-3-031-16434-7_60
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.15282v1)
- **Published**: 2022-06-30 13:42:09+00:00
- **Updated**: 2022-06-30 13:42:09+00:00
- **Authors**: Taha Emre, Arunava Chakravarty, Antoine Rivail, Sophie Riedl, Ursula Schmidt-Erfurth, Hrvoje Bogunović
- **Comment**: Accepted at MICCAI 2022
- **Journal**: None
- **Summary**: Recent contrastive learning methods achieved state-of-the-art in low label regimes. However, the training requires large batch sizes and heavy augmentations to create multiple views of an image. With non-contrastive methods, the negatives are implicitly incorporated in the loss, allowing different images and modalities as pairs. Although the meta-information (i.e., age, sex) in medical imaging is abundant, the annotations are noisy and prone to class imbalance. In this work, we exploited already existing temporal information (different visits from a patient) in a longitudinal optical coherence tomography (OCT) dataset using temporally informed non-contrastive loss (TINC) without increasing complexity and need for negative pairs. Moreover, our novel pair-forming scheme can avoid heavy augmentations and implicitly incorporates the temporal information in the pairs. Finally, these representations learned from the pretraining are more successful in predicting disease progression where the temporal information is crucial for the downstream task. More specifically, our model outperforms existing models in predicting the risk of conversion within a time frame from intermediate age-related macular degeneration (AMD) to the late wet-AMD stage.



### Self-SuperFlow: Self-supervised Scene Flow Prediction in Stereo Sequences
- **Arxiv ID**: http://arxiv.org/abs/2206.15296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15296v1)
- **Published**: 2022-06-30 13:55:17+00:00
- **Updated**: 2022-06-30 13:55:17+00:00
- **Authors**: Katharina Bendig, René Schuster, Didier Stricker
- **Comment**: Accepted at ICIP 2022
- **Journal**: None
- **Summary**: In recent years, deep neural networks showed their exceeding capabilities in addressing many computer vision tasks including scene flow prediction. However, most of the advances are dependent on the availability of a vast amount of dense per pixel ground truth annotations, which are very difficult to obtain for real life scenarios. Therefore, synthetic data is often relied upon for supervision, resulting in a representation gap between the training and test data. Even though a great quantity of unlabeled real world data is available, there is a huge lack in self-supervised methods for scene flow prediction. Hence, we explore the extension of a self-supervised loss based on the Census transform and occlusion-aware bidirectional displacements for the problem of scene flow prediction. Regarding the KITTI scene flow benchmark, our method outperforms the corresponding supervised pre-training of the same network and shows improved generalization capabilities while achieving much faster convergence.



### Interpretable Anomaly Detection in Echocardiograms with Dynamic Variational Trajectory Models
- **Arxiv ID**: http://arxiv.org/abs/2206.15316v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.CO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2206.15316v2)
- **Published**: 2022-06-30 14:42:18+00:00
- **Updated**: 2022-11-22 08:53:43+00:00
- **Authors**: Alain Ryser, Laura Manduchi, Fabian Laumer, Holger Michel, Sven Wellmann, Julia E. Vogt
- **Comment**: accepted at IMLH workshop ICML 2022
- **Journal**: Proceedings of the 7th Machine Learning for Healthcare Conference,
  PMLR 182:425-458, 2022
- **Summary**: We propose a novel anomaly detection method for echocardiogram videos. The introduced method takes advantage of the periodic nature of the heart cycle to learn three variants of a variational latent trajectory model (TVAE). While the first two variants (TVAE-C and TVAE-R) model strict periodic movements of the heart, the third (TVAE-S) is more general and allows shifts in the spatial representation throughout the video. All models are trained on the healthy samples of a novel in-house dataset of infant echocardiogram videos consisting of multiple chamber views to learn a normative prior of the healthy population. During inference, maximum a posteriori (MAP) based anomaly detection is performed to detect out-of-distribution samples in our dataset. The proposed method reliably identifies severe congenital heart defects, such as Ebstein's Anomaly or Shone-complex. Moreover, it achieves superior performance over MAP-based anomaly detection with standard variational autoencoders when detecting pulmonary hypertension and right ventricular dilation. Finally, we demonstrate that the proposed method enables interpretable explanations of its output through heatmaps highlighting the regions corresponding to anomalous heart structures.



### Neural Annotation Refinement: Development of a New 3D Dataset for Adrenal Gland Analysis
- **Arxiv ID**: http://arxiv.org/abs/2206.15328v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.15328v2)
- **Published**: 2022-06-30 14:59:15+00:00
- **Updated**: 2022-07-08 03:48:02+00:00
- **Authors**: Jiancheng Yang, Rui Shi, Udaranga Wickramasinghe, Qikui Zhu, Bingbing Ni, Pascal Fua
- **Comment**: MICCAI 2022
- **Journal**: None
- **Summary**: The human annotations are imperfect, especially when produced by junior practitioners. Multi-expert consensus is usually regarded as golden standard, while this annotation protocol is too expensive to implement in many real-world projects. In this study, we propose a method to refine human annotation, named Neural Annotation Refinement (NeAR). It is based on a learnable implicit function, which decodes a latent vector into represented shape. By integrating the appearance as an input of implicit functions, the appearance-aware NeAR fixes the annotation artefacts. Our method is demonstrated on the application of adrenal gland analysis. We first show that the NeAR can repair distorted golden standards on a public adrenal gland segmentation dataset. Besides, we develop a new Adrenal gLand ANalysis (ALAN) dataset with the proposed NeAR, where each case consists of a 3D shape of adrenal gland and its diagnosis label (normal vs. abnormal) assigned by experts. We show that models trained on the shapes repaired by the NeAR can diagnose adrenal glands better than the original ones. The ALAN dataset will be open-source, with 1,584 shapes for adrenal gland diagnosis, which serves as a new benchmark for medical shape analysis. Code and dataset are available at https://github.com/M3DV/NeAR.



### Revisiting Competitive Coding Approach for Palmprint Recognition: A Linear Discriminant Analysis Perspective
- **Arxiv ID**: http://arxiv.org/abs/2206.15349v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 15-04, I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/2206.15349v1)
- **Published**: 2022-06-30 15:18:39+00:00
- **Updated**: 2022-06-30 15:18:39+00:00
- **Authors**: Lingfei Song, Hua Huang
- **Comment**: 12 pages, 14 figures
- **Journal**: None
- **Summary**: The competitive Coding approach (CompCode) is one of the most promising methods for palmprint recognition. Due to its high performance and simple formulation, it has been continuously studied for many years. However, although numerous variations of CompCode have been proposed, a detailed analysis of the method is still absent. In this paper, we provide a detailed analysis of CompCode from the perspective of linear discriminant analysis (LDA) for the first time. A non-trivial sufficient condition under which the CompCode is optimal in the sense of Fisher's criterion is presented. Based on our analysis, we examined the statistics of palmprints and concluded that CompCode deviates from the optimal condition. To mitigate the deviation, we propose a new method called Class-Specific CompCode that improves CompCode by excluding non-palm-line areas from matching. A nonlinear mapping of the competitive code is also applied in this method to further enhance accuracy. Experiments on two public databases demonstrate the effectiveness of the proposed method.



### Deep Learning to See: Towards New Foundations of Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2206.15351v1
- **DOI**: 10.1007/978-3-030-90987-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15351v1)
- **Published**: 2022-06-30 15:20:36+00:00
- **Updated**: 2022-06-30 15:20:36+00:00
- **Authors**: Alessandro Betti, Marco Gori, Stefano Melacci
- **Comment**: None
- **Journal**: None
- **Summary**: The remarkable progress in computer vision over the last few years is, by and large, attributed to deep learning, fueled by the availability of huge sets of labeled data, and paired with the explosive growth of the GPU paradigm. While subscribing to this view, this book criticizes the supposed scientific progress in the field and proposes the investigation of vision within the framework of information-based laws of nature. Specifically, the present work poses fundamental questions about vision that remain far from understood, leading the reader on a journey populated by novel challenges resonating with the foundations of machine learning. The central thesis is that for a deeper understanding of visual computational processes, it is necessary to look beyond the applications of general purpose machine learning algorithms and focus instead on appropriate learning theories that take into account the spatiotemporal nature of the visual signal.



### Learning Underrepresented Classes from Decentralized Partially Labeled Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2206.15353v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.15353v1)
- **Published**: 2022-06-30 15:28:18+00:00
- **Updated**: 2022-06-30 15:28:18+00:00
- **Authors**: Nanqing Dong, Michael Kampffmeyer, Irina Voiculescu
- **Comment**: Accepted by MICCAI 2022
- **Journal**: None
- **Summary**: Using decentralized data for federated training is one promising emerging research direction for alleviating data scarcity in the medical domain. However, in contrast to large-scale fully labeled data commonly seen in general object recognition tasks, the local medical datasets are more likely to only have images annotated for a subset of classes of interest due to high annotation costs. In this paper, we consider a practical yet under-explored problem, where underrepresented classes only have few labeled instances available and only exist in a few clients of the federated system. We show that standard federated learning approaches fail to learn robust multi-label classifiers with extreme class imbalance and address it by proposing a novel federated learning framework, FedFew. FedFew consists of three stages, where the first stage leverages federated self-supervised learning to learn class-agnostic representations. In the second stage, the decentralized partially labeled data are exploited to learn an energy-based multi-label classifier for the common classes. Finally, the underrepresented classes are detected based on the energy and a prototype-based nearest-neighbor model is proposed for few-shot matching. We evaluate FedFew on multi-label thoracic disease classification tasks and demonstrate that it outperforms the federated baselines by a large margin.



### No Reason for No Supervision: Improved Generalization in Supervised Models
- **Arxiv ID**: http://arxiv.org/abs/2206.15369v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.15369v2)
- **Published**: 2022-06-30 15:43:51+00:00
- **Updated**: 2023-03-10 10:22:17+00:00
- **Authors**: Mert Bulent Sariyildiz, Yannis Kalantidis, Karteek Alahari, Diane Larlus
- **Comment**: Accepted to ICLR 2023 (spotlight)
- **Journal**: None
- **Summary**: We consider the problem of training a deep neural network on a given classification task, e.g., ImageNet-1K (IN1K), so that it excels at both the training task as well as at other (future) transfer tasks. These two seemingly contradictory properties impose a trade-off between improving the model's generalization and maintaining its performance on the original task. Models trained with self-supervised learning tend to generalize better than their supervised counterparts for transfer learning; yet, they still lag behind supervised models on IN1K. In this paper, we propose a supervised learning setup that leverages the best of both worlds. We extensively analyze supervised training using multi-scale crops for data augmentation and an expendable projector head, and reveal that the design of the projector allows us to control the trade-off between performance on the training task and transferability. We further replace the last layer of class weights with class prototypes computed on the fly using a memory bank and derive two models: t-ReX that achieves a new state of the art for transfer learning and outperforms top methods such as DINO and PAWS on IN1K, and t-ReX* that matches the highly optimized RSB-A1 model on IN1K while performing better on transfer tasks. Code and pretrained models: https://europe.naverlabs.com/t-rex



### PolarFormer: Multi-camera 3D Object Detection with Polar Transformer
- **Arxiv ID**: http://arxiv.org/abs/2206.15398v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.15398v6)
- **Published**: 2022-06-30 16:32:48+00:00
- **Updated**: 2023-01-16 02:24:33+00:00
- **Authors**: Yanqin Jiang, Li Zhang, Zhenwei Miao, Xiatian Zhu, Jin Gao, Weiming Hu, Yu-Gang Jiang
- **Comment**: Accepted to AAAI2023
- **Journal**: None
- **Summary**: 3D object detection in autonomous driving aims to reason "what" and "where" the objects of interest present in a 3D world. Following the conventional wisdom of previous 2D object detection, existing methods often adopt the canonical Cartesian coordinate system with perpendicular axis. However, we conjugate that this does not fit the nature of the ego car's perspective, as each onboard camera perceives the world in shape of wedge intrinsic to the imaging geometry with radical (non-perpendicular) axis. Hence, in this paper we advocate the exploitation of the Polar coordinate system and propose a new Polar Transformer (PolarFormer) for more accurate 3D object detection in the bird's-eye-view (BEV) taking as input only multi-camera 2D images. Specifically, we design a cross attention based Polar detection head without restriction to the shape of input structure to deal with irregular Polar grids. For tackling the unconstrained object scale variations along Polar's distance dimension, we further introduce a multi-scalePolar representation learning strategy. As a result, our model can make best use of the Polar representation rasterized via attending to the corresponding image observation in a sequence-to-sequence fashion subject to the geometric constraints. Thorough experiments on the nuScenes dataset demonstrate that our PolarFormer outperforms significantly state-of-the-art 3D object detection alternatives.



### MEAD: A Multi-Armed Approach for Evaluation of Adversarial Examples Detectors
- **Arxiv ID**: http://arxiv.org/abs/2206.15415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15415v1)
- **Published**: 2022-06-30 17:05:45+00:00
- **Updated**: 2022-06-30 17:05:45+00:00
- **Authors**: Federica Granese, Marine Picot, Marco Romanelli, Francisco Messina, Pablo Piantanida
- **Comment**: This paper has been accepted to appear in the Proceedings of the 2022
  European Conference on Machine Learning and Data Mining (ECML-PKDD), 19th to
  the 23rd of September, Grenoble, France
- **Journal**: None
- **Summary**: Detection of adversarial examples has been a hot topic in the last years due to its importance for safely deploying machine learning algorithms in critical applications. However, the detection methods are generally validated by assuming a single implicitly known attack strategy, which does not necessarily account for real-life threats. Indeed, this can lead to an overoptimistic assessment of the detectors' performance and may induce some bias in the comparison between competing detection schemes. We propose a novel multi-armed framework, called MEAD, for evaluating detectors based on several attack strategies to overcome this limitation. Among them, we make use of three new objectives to generate attacks. The proposed performance metric is based on the worst-case scenario: detection is successful if and only if all different attacks are correctly recognized. Empirically, we show the effectiveness of our approach. Moreover, the poor performance obtained for state-of-the-art detectors opens a new exciting line of research.



### Category-Level 6D Object Pose Estimation in the Wild: A Semi-Supervised Learning Approach and A New Dataset
- **Arxiv ID**: http://arxiv.org/abs/2206.15436v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.15436v1)
- **Published**: 2022-06-30 17:30:57+00:00
- **Updated**: 2022-06-30 17:30:57+00:00
- **Authors**: Yang Fu, Xiaolong Wang
- **Comment**: Project page: https://oasisyang.github.io/semi-pose
- **Journal**: None
- **Summary**: 6D object pose estimation is one of the fundamental problems in computer vision and robotics research. While a lot of recent efforts have been made on generalizing pose estimation to novel object instances within the same category, namely category-level 6D pose estimation, it is still restricted in constrained environments given the limited number of annotated data. In this paper, we collect Wild6D, a new unlabeled RGBD object video dataset with diverse instances and backgrounds. We utilize this data to generalize category-level 6D object pose estimation in the wild with semi-supervised learning. We propose a new model, called Rendering for Pose estimation network RePoNet, that is jointly trained using the free ground-truths with the synthetic data, and a silhouette matching objective function on the real-world data. Without using any 3D annotations on real data, our method outperforms state-of-the-art methods on the previous dataset and our Wild6D test set (with manual annotations for evaluation) by a large margin. Project page with Wild6D data: https://oasisyang.github.io/semi-pose .



### Asymmetry Disentanglement Network for Interpretable Acute Ischemic Stroke Infarct Segmentation in Non-Contrast CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2206.15445v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.15445v1)
- **Published**: 2022-06-30 17:39:28+00:00
- **Updated**: 2022-06-30 17:39:28+00:00
- **Authors**: Haomiao Ni, Yuan Xue, Kelvin Wong, John Volpi, Stephen T. C. Wong, James Z. Wang, Xiaolei Huang
- **Comment**: MICCAI 2022
- **Journal**: None
- **Summary**: Accurate infarct segmentation in non-contrast CT (NCCT) images is a crucial step toward computer-aided acute ischemic stroke (AIS) assessment. In clinical practice, bilateral symmetric comparison of brain hemispheres is usually used to locate pathological abnormalities. Recent research has explored asymmetries to assist with AIS segmentation. However, most previous symmetry-based work mixed different types of asymmetries when evaluating their contribution to AIS. In this paper, we propose a novel Asymmetry Disentanglement Network (ADN) to automatically separate pathological asymmetries and intrinsic anatomical asymmetries in NCCTs for more effective and interpretable AIS segmentation. ADN first performs asymmetry disentanglement based on input NCCTs, which produces different types of 3D asymmetry maps. Then a synthetic, intrinsic-asymmetry-compensated and pathology-asymmetry-salient NCCT volume is generated and later used as input to a segmentation network. The training of ADN incorporates domain knowledge and adopts a tissue-type aware regularization loss function to encourage clinically-meaningful pathological asymmetry extraction. Coupled with an unsupervised 3D transformation network, ADN achieves state-of-the-art AIS segmentation performance on a public NCCT dataset. In addition to the superior performance, we believe the learned clinically-interpretable asymmetry maps can also provide insights towards a better understanding of AIS assessment. Our code is available at https://github.com/nihaomiao/MICCAI22_ADN.



### Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations
- **Arxiv ID**: http://arxiv.org/abs/2206.15462v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.15462v3)
- **Published**: 2022-06-30 17:55:12+00:00
- **Updated**: 2023-06-12 17:59:37+00:00
- **Authors**: Ziyan Yang, Kushal Kafle, Franck Dernoncourt, Vicente Ordonez
- **Comment**: CVPR 2023. Code: https://github.com/uvavision/AMC-grounding Project
  Webpage: https://vislang.ai/amc
- **Journal**: None
- **Summary**: We propose a margin-based loss for vision-language model pretraining that encourages gradient-based explanations that are consistent with region-level annotations. We refer to this objective as Attention Mask Consistency (AMC) and demonstrate that it produces superior visual grounding performance compared to models that rely instead on region-level annotations for explicitly training an object detector such as Faster R-CNN. AMC works by encouraging gradient-based explanation masks that focus their attention scores mostly within annotated regions of interest for images that contain such annotations. Particularly, a model trained with AMC on top of standard vision-language modeling objectives obtains a state-of-the-art accuracy of 86.59% in the Flickr30k visual grounding benchmark, an absolute improvement of 5.48% when compared to the best previous model. Our approach also performs exceedingly well on established benchmarks for referring expression comprehension and offers the added benefit by design of gradient-based explanations that better align with human annotations.



### Watch and Match: Supercharging Imitation with Regularized Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/2206.15469v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.15469v2)
- **Published**: 2022-06-30 17:58:18+00:00
- **Updated**: 2023-02-20 20:54:59+00:00
- **Authors**: Siddhant Haldar, Vaibhav Mathur, Denis Yarats, Lerrel Pinto
- **Comment**: Code and robot videos are available on https://rot-robot.github.io/
- **Journal**: None
- **Summary**: Imitation learning holds tremendous promise in learning policies efficiently for complex decision making problems. Current state-of-the-art algorithms often use inverse reinforcement learning (IRL), where given a set of expert demonstrations, an agent alternatively infers a reward function and the associated optimal policy. However, such IRL approaches often require substantial online interactions for complex control problems. In this work, we present Regularized Optimal Transport (ROT), a new imitation learning algorithm that builds on recent advances in optimal transport based trajectory-matching. Our key technical insight is that adaptively combining trajectory-matching rewards with behavior cloning can significantly accelerate imitation even with only a few demonstrations. Our experiments on 20 visual control tasks across the DeepMind Control Suite, the OpenAI Robotics Suite, and the Meta-World Benchmark demonstrate an average of 7.8X faster imitation to reach 90% of expert performance compared to prior state-of-the-art methods. On real-world robotic manipulation, with just one demonstration and an hour of online training, ROT achieves an average success rate of 90.1% across 14 tasks.



### Dressing Avatars: Deep Photorealistic Appearance for Physically Simulated Clothing
- **Arxiv ID**: http://arxiv.org/abs/2206.15470v2
- **DOI**: 10.1145/3550454.3555456
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.15470v2)
- **Published**: 2022-06-30 17:58:20+00:00
- **Updated**: 2022-09-19 20:30:00+00:00
- **Authors**: Donglai Xiang, Timur Bagautdinov, Tuur Stuyck, Fabian Prada, Javier Romero, Weipeng Xu, Shunsuke Saito, Jingfan Guo, Breannan Smith, Takaaki Shiratori, Yaser Sheikh, Jessica Hodgins, Chenglei Wu
- **Comment**: SIGGRAPH Asia 2022 (ACM ToG) camera ready. The supplementary video
  can be found on
  https://research.facebook.com/publications/dressing-avatars-deep-photorealistic-appearance-for-physically-simulated-clothing/
- **Journal**: None
- **Summary**: Despite recent progress in developing animatable full-body avatars, realistic modeling of clothing - one of the core aspects of human self-expression - remains an open challenge. State-of-the-art physical simulation methods can generate realistically behaving clothing geometry at interactive rates. Modeling photorealistic appearance, however, usually requires physically-based rendering which is too expensive for interactive applications. On the other hand, data-driven deep appearance models are capable of efficiently producing realistic appearance, but struggle at synthesizing geometry of highly dynamic clothing and handling challenging body-clothing configurations. To this end, we introduce pose-driven avatars with explicit modeling of clothing that exhibit both photorealistic appearance learned from real-world data and realistic clothing dynamics. The key idea is to introduce a neural clothing appearance model that operates on top of explicit geometry: at training time we use high-fidelity tracking, whereas at animation time we rely on physically simulated geometry. Our core contribution is a physically-inspired appearance network, capable of generating photorealistic appearance with view-dependent and dynamic shadowing effects even for unseen body-clothing configurations. We conduct a thorough evaluation of our model and demonstrate diverse animation results on several subjects and different types of clothing. Unlike previous work on photorealistic full-body avatars, our approach can produce much richer dynamics and more realistic deformations even for many examples of loose clothing. We also demonstrate that our formulation naturally allows clothing to be used with avatars of different people while staying fully animatable, thus enabling, for the first time, photorealistic avatars with novel clothing.



### On-Device Training Under 256KB Memory
- **Arxiv ID**: http://arxiv.org/abs/2206.15472v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.15472v3)
- **Published**: 2022-06-30 17:59:08+00:00
- **Updated**: 2022-11-10 18:10:18+00:00
- **Authors**: Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, Song Han
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: On-device training enables the model to adapt to new data collected from the sensors by fine-tuning a pre-trained model. Users can benefit from customized AI models without having to transfer the data to the cloud, protecting the privacy. However, the training memory consumption is prohibitive for IoT devices that have tiny memory resources. We propose an algorithm-system co-design framework to make on-device training possible with only 256KB of memory. On-device training faces two unique challenges: (1) the quantized graphs of neural networks are hard to optimize due to low bit-precision and the lack of normalization; (2) the limited hardware resource does not allow full back-propagation. To cope with the optimization difficulty, we propose Quantization-Aware Scaling to calibrate the gradient scales and stabilize 8-bit quantized training. To reduce the memory footprint, we propose Sparse Update to skip the gradient computation of less important layers and sub-tensors. The algorithm innovation is implemented by a lightweight training system, Tiny Training Engine, which prunes the backward computation graph to support sparse updates and offload the runtime auto-differentiation to compile time. Our framework is the first solution to enable tiny on-device training of convolutional neural networks under 256KB SRAM and 1MB Flash without auxiliary memory, using less than 1/1000 of the memory of PyTorch and TensorFlow while matching the accuracy on tinyML application VWW. Our study enables IoT devices not only to perform inference but also to continuously adapt to new data for on-device lifelong learning. A video demo can be found here: https://youtu.be/XaDCO8YtmBw.



### LaserMix for Semi-Supervised LiDAR Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.00026v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.00026v3)
- **Published**: 2022-06-30 18:00:04+00:00
- **Updated**: 2023-04-13 12:53:04+00:00
- **Authors**: Lingdong Kong, Jiawei Ren, Liang Pan, Ziwei Liu
- **Comment**: CVPR 2023 (Highlight); 26 pages, 10 figures, 12 tables; Code at
  https://github.com/ldkong1205/LaserMix
- **Journal**: None
- **Summary**: Densely annotating LiDAR point clouds is costly, which restrains the scalability of fully-supervised learning methods. In this work, we study the underexplored semi-supervised learning (SSL) in LiDAR segmentation. Our core idea is to leverage the strong spatial cues of LiDAR point clouds to better exploit unlabeled data. We propose LaserMix to mix laser beams from different LiDAR scans, and then encourage the model to make consistent and confident predictions before and after mixing. Our framework has three appealing properties: 1) Generic: LaserMix is agnostic to LiDAR representations (e.g., range view and voxel), and hence our SSL framework can be universally applied. 2) Statistically grounded: We provide a detailed analysis to theoretically explain the applicability of the proposed framework. 3) Effective: Comprehensive experimental analysis on popular LiDAR segmentation datasets (nuScenes, SemanticKITTI, and ScribbleKITTI) demonstrates our effectiveness and superiority. Notably, we achieve competitive results over fully-supervised counterparts with 2x to 5x fewer labels and improve the supervised-only baseline significantly by 10.8% on average. We hope this concise yet high-performing framework could facilitate future research in semi-supervised LiDAR segmentation. Code is publicly available.



### Semantic Image Synthesis via Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2207.00050v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00050v2)
- **Published**: 2022-06-30 18:31:51+00:00
- **Updated**: 2022-11-22 13:51:42+00:00
- **Authors**: Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, Houqiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in various image generation tasks compared with Generative Adversarial Nets (GANs). Recent work on semantic image synthesis mainly follows the \emph{de facto} GAN-based approaches, which may lead to unsatisfactory quality or diversity of generated images. In this paper, we propose a novel framework based on DDPM for semantic image synthesis. Unlike previous conditional diffusion model directly feeds the semantic layout and noisy image as input to a U-Net structure, which may not fully leverage the information in the input semantic mask, our framework processes semantic layout and noisy image differently. It feeds noisy image to the encoder of the U-Net structure while the semantic layout to the decoder by multi-layer spatially-adaptive normalization operators. To further improve the generation quality and semantic interpretability in semantic image synthesis, we introduce the classifier-free guidance sampling strategy, which acknowledge the scores of an unconditional model for sampling process. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed method, achieving state-of-the-art performance in terms of fidelity (FID) and diversity (LPIPS).



### Visual Pre-training for Navigation: What Can We Learn from Noise?
- **Arxiv ID**: http://arxiv.org/abs/2207.00052v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.00052v3)
- **Published**: 2022-06-30 18:35:00+00:00
- **Updated**: 2023-07-26 21:55:11+00:00
- **Authors**: Yanwei Wang, Ching-Yun Ko, Pulkit Agrawal
- **Comment**: IROS 2023
- **Journal**: None
- **Summary**: One powerful paradigm in visual navigation is to predict actions from observations directly. Training such an end-to-end system allows representations useful for downstream tasks to emerge automatically. However, the lack of inductive bias makes this system data inefficient. We hypothesize a sufficient representation of the current view and the goal view for a navigation policy can be learned by predicting the location and size of a crop of the current view that corresponds to the goal. We further show that training such random crop prediction in a self-supervised fashion purely on synthetic noise images transfers well to natural home images. The learned representation can then be bootstrapped to learn a navigation policy efficiently with little interaction data. The code is available at https://yanweiw.github.io/noise2ptz



### MultiViz: Towards Visualizing and Understanding Multimodal Models
- **Arxiv ID**: http://arxiv.org/abs/2207.00056v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.00056v3)
- **Published**: 2022-06-30 18:42:06+00:00
- **Updated**: 2023-03-06 19:39:18+00:00
- **Authors**: Paul Pu Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, Ruslan Salakhutdinov
- **Comment**: ICLR 2023. Code available at: https://github.com/pliang279/MultiViz
- **Journal**: None
- **Summary**: The promise of multimodal models for real-world applications has inspired research in visualizing and understanding their internal mechanics with the end goal of empowering stakeholders to visualize model behavior, perform model debugging, and promote trust in machine learning models. However, modern multimodal models are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize the internal modeling of multimodal interactions in these models? Our paper aims to fill this gap by proposing MultiViz, a method for analyzing the behavior of multimodal models by scaffolding the problem of interpretability into 4 stages: (1) unimodal importance: how each modality contributes towards downstream modeling and prediction, (2) cross-modal interactions: how different modalities relate with each other, (3) multimodal representations: how unimodal and cross-modal interactions are represented in decision-level features, and (4) multimodal prediction: how decision-level features are composed to make a prediction. MultiViz is designed to operate on diverse modalities, models, tasks, and research areas. Through experiments on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available, will be regularly updated with new interpretation tools and metrics, and welcomes inputs from the community.



### Rethinking Unsupervised Domain Adaptation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.00067v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.00067v2)
- **Published**: 2022-06-30 19:13:23+00:00
- **Updated**: 2022-09-21 06:46:01+00:00
- **Authors**: Zhijie Wang, Masanori Suganuma, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) adapts a model trained on one domain (called source) to a novel domain (called target) using only unlabeled data. Due to its high annotation cost, researchers have developed many UDA methods for semantic segmentation, which assume no labeled sample is available in the target domain. We question the practicality of this assumption for two reasons. First, after training a model with a UDA method, we must somehow verify the model before deployment. Second, UDA methods have at least a few hyper-parameters that need to be determined. The surest solution to these is to evaluate the model using validation data, i.e., a certain amount of labeled target-domain samples. This question about the basic assumption of UDA leads us to rethink UDA from a data-centric point of view. Specifically, we assume we have access to a minimum level of labeled data. Then, we ask how much is necessary to find good hyper-parameters of existing UDA methods. We then consider what if we use the same data for supervised training of the same model, e.g., finetuning. We conducted experiments to answer these questions with popular scenarios, {GTA5, SYNTHIA}$\rightarrow$Cityscapes. We found that i) choosing good hyper-parameters needs only a few labeled images for some UDA methods whereas a lot more for others; and ii) simple finetuning works surprisingly well; it outperforms many UDA methods if only several dozens of labeled images are available.



### Sparse Periodic Systolic Dataflow for Lowering Latency and Power Dissipation of Convolutional Neural Network Accelerators
- **Arxiv ID**: http://arxiv.org/abs/2207.00068v1
- **DOI**: 10.1145/3531437.3539715
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00068v1)
- **Published**: 2022-06-30 19:16:46+00:00
- **Updated**: 2022-06-30 19:16:46+00:00
- **Authors**: Jung Hwan Heo, Arash Fayyazi, Amirhossein Esmaili, Massoud Pedram
- **Comment**: 6 pages, Published in ACM/IEEE International Symposium on Low Power
  Electronics and Design (ISLPED) 2022
- **Journal**: None
- **Summary**: This paper introduces the sparse periodic systolic (SPS) dataflow, which advances the state-of-the-art hardware accelerator for supporting lightweight neural networks. Specifically, the SPS dataflow enables a novel hardware design approach unlocked by an emergent pruning scheme, periodic pattern-based sparsity (PPS). By exploiting the regularity of PPS, our sparsity-aware compiler optimally reorders the weights and uses a simple indexing unit in hardware to create matches between the weights and activations. Through the compiler-hardware codesign, SPS dataflow enjoys higher degrees of parallelism while being free of the high indexing overhead and without model accuracy loss. Evaluated on popular benchmarks such as VGG and ResNet, the SPS dataflow and accompanying neural network compiler outperform prior work in convolutional neural network (CNN) accelerator designs targeting FPGA devices. Against other sparsity-supporting weight storage formats, SPS results in 4.49x energy efficiency gain while lowering storage requirements by 3.67x for total weight storage (non-pruned weights plus indexing) and 22,044x for indexing memory.



### Rapid and stain-free quantification of viral plaque via lens-free holography and deep learning
- **Arxiv ID**: http://arxiv.org/abs/2207.00089v2
- **DOI**: 10.1038/s41551-023-01057-7
- **Categories**: **physics.ins-det**, cs.CV, physics.app-ph, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2207.00089v2)
- **Published**: 2022-06-30 20:12:36+00:00
- **Updated**: 2023-06-22 20:29:10+00:00
- **Authors**: Tairan Liu, Yuzhu Li, Hatice Ceylan Koydemir, Yijie Zhang, Ethan Yang, Merve Eryilmaz, Hongda Wang, Jingxi Li, Bijie Bai, Guangdong Ma, Aydogan Ozcan
- **Comment**: 24 Pages, 6 Figures
- **Journal**: Nature Biomedical Engineering (2023)
- **Summary**: We present a rapid and stain-free quantitative viral plaque assay using lensfree holographic imaging and deep learning. This cost-effective, compact, and automated device significantly reduces the incubation time needed for traditional plaque assays while preserving their advantages over other virus quantification methods. This device captures ~0.32 Giga-pixel/hour phase information of the objects per test well, covering an area of ~30x30 mm^2, in a label-free manner, eliminating staining entirely. We demonstrated the success of this computational method using vesicular stomatitis virus (VSV), herpes simplex virus (HSV-1) and encephalomyocarditis virus (EMCV). Using a neural network, this stain-free device automatically detected the first cell lysing events due to the VSV viral replication as early as 5 hours after the incubation, and achieved >90% detection rate for the VSV plaque-forming units (PFUs) with 100% specificity in <20 hours, providing major time savings compared to the traditional plaque assays that take at least 48 hours. Similarly, this stain-free device reduced the needed incubation time by ~48 hours for HSV-1 and ~20 hours for EMCV, achieving >90% detection rate with 100% specificity. We also demonstrated that this data-driven plaque assay offers the capability of quantifying the infected area of the cell monolayer, performing automated counting and quantification of PFUs and virus-infected areas over a 10-fold larger dynamic range of virus concentration than standard viral plaque assays. This compact, low-cost, automated PFU quantification device can be broadly used in virology research, vaccine development, and clinical applications.



### End-to-end Learning for Image-based Detection of Molecular Alterations in Digital Pathology
- **Arxiv ID**: http://arxiv.org/abs/2207.00095v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2207.00095v2)
- **Published**: 2022-06-30 20:30:33+00:00
- **Updated**: 2022-07-19 13:57:47+00:00
- **Authors**: Marvin Teichmann, Andre Aichert, Hanibal Bohnenberger, Philipp Ströbel, Tobias Heimann
- **Comment**: MICCAI 2022; 8.5 Pages, 4 Figures
- **Journal**: None
- **Summary**: Current approaches for classification of whole slide images (WSI) in digital pathology predominantly utilize a two-stage learning pipeline. The first stage identifies areas of interest (e.g. tumor tissue), while the second stage processes cropped tiles from these areas in a supervised fashion. During inference, a large number of tiles are combined into a unified prediction for the entire slide. A major drawback of such approaches is the requirement for task-specific auxiliary labels which are not acquired in clinical routine. We propose a novel learning pipeline for WSI classification that is trainable end-to-end and does not require any auxiliary annotations. We apply our approach to predict molecular alterations for a number of different use-cases, including detection of microsatellite instability in colorectal tumors and prediction of specific mutations for colon, lung, and breast cancer cases from The Cancer Genome Atlas. Results reach AUC scores of up to 94% and are shown to be competitive with state of the art two-stage pipelines. We believe our approach can facilitate future research in digital pathology and contribute to solve a large range of problems around the prediction of cancer phenotypes, hopefully enabling personalized therapies for more patients in future.



### GaitForeMer: Self-Supervised Pre-Training of Transformers via Human Motion Forecasting for Few-Shot Gait Impairment Severity Estimation
- **Arxiv ID**: http://arxiv.org/abs/2207.00106v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.00106v1)
- **Published**: 2022-06-30 21:29:47+00:00
- **Updated**: 2022-06-30 21:29:47+00:00
- **Authors**: Mark Endo, Kathleen L. Poston, Edith V. Sullivan, Li Fei-Fei, Kilian M. Pohl, Ehsan Adeli
- **Comment**: Accepted as a conference paper at MICCAI (Medical Image Computing and
  Computer Assisted Intervention) 2022
- **Journal**: None
- **Summary**: Parkinson's disease (PD) is a neurological disorder that has a variety of observable motor-related symptoms such as slow movement, tremor, muscular rigidity, and impaired posture. PD is typically diagnosed by evaluating the severity of motor impairments according to scoring systems such as the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). Automated severity prediction using video recordings of individuals provides a promising route for non-intrusive monitoring of motor impairments. However, the limited size of PD gait data hinders model ability and clinical potential. Because of this clinical data scarcity and inspired by the recent advances in self-supervised large-scale language models like GPT-3, we use human motion forecasting as an effective self-supervised pre-training task for the estimation of motor impairment severity. We introduce GaitForeMer, Gait Forecasting and impairment estimation transforMer, which is first pre-trained on public datasets to forecast gait movements and then applied to clinical data to predict MDS-UPDRS gait impairment severity. Our method outperforms previous approaches that rely solely on clinical data by a large margin, achieving an F1 score of 0.76, precision of 0.79, and recall of 0.75. Using GaitForeMer, we show how public human movement data repositories can assist clinical use cases through learning universal motion representations. The code is available at https://github.com/markendo/GaitForeMer .



### Rethinking Surgical Captioning: End-to-End Window-Based MLP Transformer Using Patches
- **Arxiv ID**: http://arxiv.org/abs/2207.00113v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00113v1)
- **Published**: 2022-06-30 21:57:33+00:00
- **Updated**: 2022-06-30 21:57:33+00:00
- **Authors**: Mengya Xu, Mobarakol Islam, Hongliang Ren
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Surgical captioning plays an important role in surgical instruction prediction and report generation. However, the majority of captioning models still rely on the heavy computational object detector or feature extractor to extract regional features. In addition, the detection model requires additional bounding box annotation which is costly and needs skilled annotators. These lead to inference delay and limit the captioning model to deploy in real-time robotic surgery. For this purpose, we design an end-to-end detector and feature extractor-free captioning model by utilizing the patch-based shifted window technique. We propose Shifted Window-Based Multi-Layer Perceptrons Transformer Captioning model (SwinMLP-TranCAP) with faster inference speed and less computation. SwinMLP-TranCAP replaces the multi-head attention module with window-based multi-head MLP. Such deployments primarily focus on image understanding tasks, but very few works investigate the caption generation task. SwinMLP-TranCAP is also extended into a video version for video captioning tasks using 3D patches and windows. Compared with previous detector-based or feature extractor-based models, our models greatly simplify the architecture design while maintaining performance on two surgical datasets. The code is publicly available at https://github.com/XuMengyaAmy/SwinMLP_TranCAP.



### ProSelfLC: Progressive Self Label Correction Towards A Low-Temperature Entropy State
- **Arxiv ID**: http://arxiv.org/abs/2207.00118v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.00118v2)
- **Published**: 2022-06-30 22:23:33+00:00
- **Updated**: 2022-09-06 12:45:42+00:00
- **Authors**: Xinshao Wang, Yang Hua, Elyor Kodirov, Sankha Subhra Mukherjee, David A. Clifton, Neil M. Robertson
- **Comment**: To ease the reading, a summary of changes is put in the beginning.
  Our source code is available at
  https://github.com/XinshaoAmosWang/ProSelfLC-AT
- **Journal**: None
- **Summary**: There is a family of label modification approaches including self and non-self label correction (LC), and output regularisation. They are widely used for training robust deep neural networks (DNNs), but have not been mathematically and thoroughly analysed together. We study them and discover three key issues: (1) We are more interested in adopting Self LC as it leverages its own knowledge and requires no auxiliary models. However, it is unclear how to adaptively trust a learner as the training proceeds. (2) Some methods penalise while the others reward low-entropy (i.e., high-confidence) predictions, prompting us to ask which one is better. (3) Using the standard training setting, a learned model becomes less confident when severe noise exists. Self LC using high-entropy knowledge would generate high-entropy targets.   To resolve the issue (1), inspired by a well-accepted finding, i.e., deep neural networks learn meaningful patterns before fitting noise, we propose a novel end-to-end method named ProSelfLC, which is designed according to the learning time and prediction entropy. Concretely, for any data point, we progressively and adaptively trust its predicted probability distribution versus its annotated one if a network has been trained for a relatively long time and the prediction is of low entropy. For the issue (2), the effectiveness of ProSelfLC defends entropy minimisation. By ProSelfLC, we empirically prove that it is more effective to redefine a semantic low-entropy state and optimise the learner toward it. To address the issue (3), we decrease the entropy of self knowledge using a low temperature before exploiting it to correct labels, so that the revised labels redefine low-entropy target probability distributions.   We demonstrate the effectiveness of ProSelfLC through extensive experiments in both clean and noisy settings, and on both image and protein datasets.



