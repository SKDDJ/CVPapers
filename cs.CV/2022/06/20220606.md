# Arxiv Papers in cs.CV on 2022-06-06
### HIFI-Net: A Novel Network for Enhancement to Underwater Images
- **Arxiv ID**: http://arxiv.org/abs/2206.02295v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.02295v1)
- **Published**: 2022-06-06 00:46:18+00:00
- **Updated**: 2022-06-06 00:46:18+00:00
- **Authors**: Jiajia Zhou, Junbin Zhuang, Yan Zheng, Di Wu
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: A novel network for enhancement to underwater images is proposed in this paper. It contains a Reinforcement Fusion Module for Haar wavelet images (RFM-Haar) based on Reinforcement Fusion Unit (RFU), which is used to fuse an original image and some important information within it. Fusion is achieved for better enhancement. As this network make "Haar Images into Fusion Images", it is called HIFI-Net. The experimental results show the proposed HIFI-Net performs best among many state-of-the-art methods on three datasets at three normal metrics and a new metric.



### Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation
- **Arxiv ID**: http://arxiv.org/abs/2206.02307v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02307v4)
- **Published**: 2022-06-06 01:30:03+00:00
- **Updated**: 2023-03-10 20:00:17+00:00
- **Authors**: Chenyu You, Weicheng Dai, Yifei Min, Lawrence Staib, James S. Duncan
- **Comment**: Accepted at Information Processing in Medical Imaging (IPMI 2023)
- **Journal**: None
- **Summary**: Contrastive learning has shown great promise over annotation scarcity problems in the context of medical image segmentation. Existing approaches typically assume a balanced class distribution for both labeled and unlabeled medical images. However, medical image data in reality is commonly imbalanced (i.e., multi-class label imbalance), which naturally yields blurry contours and usually incorrectly labels rare objects. Moreover, it remains unclear whether all negative samples are equally negative. In this work, we present ACTION, an Anatomical-aware ConTrastive dIstillatiON framework, for semi-supervised medical image segmentation. Specifically, we first develop an iterative contrastive distillation algorithm by softly labeling the negatives rather than binary supervision between positive and negative pairs. We also capture more semantically similar features from the randomly chosen negative set compared to the positives to enforce the diversity of the sampled data. Second, we raise a more important question: Can we really handle imbalanced samples to yield better performance? Hence, the key innovation in ACTION is to learn global semantic relationship across the entire dataset and local anatomical features among the neighbouring pixels with minimal additional memory footprint. During the training, we introduce anatomical contrast by actively sampling a sparse set of hard negative pixels, which can generate smoother segmentation boundaries and more accurate predictions. Extensive experiments across two benchmark datasets and different unlabeled settings show that ACTION significantly outperforms the current state-of-the-art semi-supervised methods.



### Evaluation-oriented Knowledge Distillation for Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.02325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02325v1)
- **Published**: 2022-06-06 02:49:40+00:00
- **Updated**: 2022-06-06 02:49:40+00:00
- **Authors**: Yuge Huang, Jiaxiang Wu, Xingkun Xu, Shouhong Ding
- **Comment**: CVPR2022 Oral
- **Journal**: None
- **Summary**: Knowledge distillation (KD) is a widely-used technique that utilizes large networks to improve the performance of compact models. Previous KD approaches usually aim to guide the student to mimic the teacher's behavior completely in the representation space. However, such one-to-one corresponding constraints may lead to inflexible knowledge transfer from the teacher to the student, especially those with low model capacities. Inspired by the ultimate goal of KD methods, we propose a novel Evaluation oriented KD method (EKD) for deep face recognition to directly reduce the performance gap between the teacher and student models during training. Specifically, we adopt the commonly used evaluation metrics in face recognition, i.e., False Positive Rate (FPR) and True Positive Rate (TPR) as the performance indicator. According to the evaluation protocol, the critical pair relations that cause the TPR and FPR difference between the teacher and student models are selected. Then, the critical relations in the student are constrained to approximate the corresponding ones in the teacher by a novel rank-based loss function, giving more flexibility to the student with low capacity. Extensive experimental results on popular benchmarks demonstrate the superiority of our EKD over state-of-the-art competitors.



### JigsawHSI: a network for Hyperspectral Image classification
- **Arxiv ID**: http://arxiv.org/abs/2206.02327v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68T07, I.4.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2206.02327v2)
- **Published**: 2022-06-06 02:56:51+00:00
- **Updated**: 2022-06-10 22:04:10+00:00
- **Authors**: Jaime Moraga, H. Sebnem Duzgun
- **Comment**: 7 pages, 7 figures, not peer reviewed
- **Journal**: None
- **Summary**: This article describes Jigsaw, a convolutional neural network (CNN) used in geosciences and based on Inception but tailored for geoscientific analyses. Introduces JigsawHSI (based on Jigsaw) and uses it on the land-use land-cover (LULC) classification problem with the Indian Pines, Pavia University and Salinas hyperspectral image data sets. The network is compared against HybridSN, a spectral-spatial 3D-CNN followed by 2D-CNN that achieves state-of-the-art results on the datasets. This short article proves that JigsawHSI is able to meet or exceed HybridSN's performance in all three cases. Additionally, the use of jigsaw in geosciences is highlighted, while the code and toolkit are made available.



### MASNet:Improve Performance of Siamese Networks with Mutual-attention for Remote Sensing Change Detection Tasks
- **Arxiv ID**: http://arxiv.org/abs/2206.02331v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.02331v1)
- **Published**: 2022-06-06 03:21:53+00:00
- **Updated**: 2022-06-06 03:21:53+00:00
- **Authors**: Hongbin Zhou, Yupeng Ren, Qiankun Li, Jun Yin, Yonggang Lin
- **Comment**: XXIV ISPRS Congress
- **Journal**: None
- **Summary**: Siamese networks are widely used for remote sensing change detection tasks. A vanilla siamese network has two identical feature extraction branches which share weights, these two branches work independently and the feature maps are not fused until about to be sent to a decoder head. However we find that it is critical to exchange information between two feature extraction branches at early stage for change detection task. In this work we present Mutual-Attention Siamese Network (MASNet), a general siamese network with mutual-attention plug-in, so to exchange information between the two feature extraction branches. We show that our modification improve the performance of siamese networks on multi change detection datasets, and it works for both convolutional neural network and visual transformer.



### OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression
- **Arxiv ID**: http://arxiv.org/abs/2206.02338v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02338v2)
- **Published**: 2022-06-06 03:54:53+00:00
- **Updated**: 2022-10-01 13:45:55+00:00
- **Authors**: Wanhua Li, Xiaoke Huang, Zheng Zhu, Yansong Tang, Xiu Li, Jie Zhou, Jiwen Lu
- **Comment**: Accepted by NeurIPS2022. Code is available at
  https://github.com/xk-huang/OrdinalCLIP
- **Journal**: None
- **Summary**: This paper presents a language-powered paradigm for ordinal regression. Existing methods usually treat each rank as a category and employ a set of weights to learn these concepts. These methods are easy to overfit and usually attain unsatisfactory performance as the learned concepts are mainly derived from the training set. Recent large pre-trained vision-language models like CLIP have shown impressive performance on various visual tasks. In this paper, we propose to learn the rank concepts from the rich semantic CLIP latent space. Specifically, we reformulate this task as an image-language matching problem with a contrastive objective, which regards labels as text and obtains a language prototype from a text encoder for each rank. While prompt engineering for CLIP is extremely time-consuming, we propose OrdinalCLIP, a differentiable prompting method for adapting CLIP for ordinal regression. OrdinalCLIP consists of learnable context tokens and learnable rank embeddings; The learnable rank embeddings are constructed by explicitly modeling numerical continuity, resulting in well-ordered, compact language prototypes in the CLIP space. Once learned, we can only save the language prototypes and discard the huge language model, resulting in zero additional computational overhead compared with the linear head counterpart. Experimental results show that our paradigm achieves competitive performance in general ordinal regression tasks, and gains improvements in few-shot and distribution shift settings for age estimation. The code is available at https://github.com/xk-huang/OrdinalCLIP.



### WHU-Stereo: A Challenging Benchmark for Stereo Matching of High-Resolution Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2206.02342v1
- **DOI**: 10.1109/TGRS.2023.3245205
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.02342v1)
- **Published**: 2022-06-06 04:01:46+00:00
- **Updated**: 2022-06-06 04:01:46+00:00
- **Authors**: Shenhong Li, Sheng He, San Jiang, Wanshou Jiang, Lin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Stereo matching of high-resolution satellite images (HRSI) is still a fundamental but challenging task in the field of photogrammetry and remote sensing. Recently, deep learning (DL) methods, especially convolutional neural networks (CNNs), have demonstrated tremendous potential for stereo matching on public benchmark datasets. However, datasets for stereo matching of satellite images are scarce. To facilitate further research, this paper creates and publishes a challenging dataset, termed WHU-Stereo, for stereo matching DL network training and testing. This dataset is created by using airborne LiDAR point clouds and high-resolution stereo imageries taken from the Chinese GaoFen-7 satellite (GF-7). The WHU-Stereo dataset contains more than 1700 epipolar rectified image pairs, which cover six areas in China and includes various kinds of landscapes. We have assessed the accuracy of ground-truth disparity maps, and it is proved that our dataset achieves comparable precision compared with existing state-of-the-art stereo matching datasets. To verify its feasibility, in experiments, the hand-crafted SGM stereo matching algorithm and recent deep learning networks have been tested on the WHU-Stereo dataset. Experimental results show that deep learning networks can be well trained and achieves higher performance than hand-crafted SGM algorithm, and the dataset has great potential in remote sensing application. The WHU-Stereo dataset can serve as a challenging benchmark for stereo matching of high-resolution satellite images, and performance evaluation of deep learning models. Our dataset is available at https://github.com/Sheng029/WHU-Stereo



### Contrastive Graph Multimodal Model for Text Classification in Videos
- **Arxiv ID**: http://arxiv.org/abs/2206.02343v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02343v1)
- **Published**: 2022-06-06 04:06:21+00:00
- **Updated**: 2022-06-06 04:06:21+00:00
- **Authors**: Ye Liu, Changchong Lu, Chen Lin, Di Yin, Bo Ren
- **Comment**: None
- **Journal**: None
- **Summary**: The extraction of text information in videos serves as a critical step towards semantic understanding of videos. It usually involved in two steps: (1) text recognition and (2) text classification. To localize texts in videos, we can resort to large numbers of text recognition methods based on OCR technology. However, to our knowledge, there is no existing work focused on the second step of video text classification, which will limit the guidance to downstream tasks such as video indexing and browsing. In this paper, we are the first to address this new task of video text classification by fusing multimodal information to deal with the challenging scenario where different types of video texts may be confused with various colors, unknown fonts and complex layouts. In addition, we tailor a specific module called CorrelationNet to reinforce feature representation by explicitly extracting layout information. Furthermore, contrastive learning is utilized to explore inherent connections between samples using plentiful unlabeled videos. Finally, we construct a new well-defined industrial dataset from the news domain, called TI-News, which is dedicated to building and evaluating video text recognition and classification applications. Extensive experiments on TI-News demonstrate the effectiveness of our method.



### Anomaly Detection with Test Time Augmentation and Consistency Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2206.02345v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.02345v1)
- **Published**: 2022-06-06 04:27:06+00:00
- **Updated**: 2022-06-06 04:27:06+00:00
- **Authors**: Haowei He, Jiaye Teng, Yang Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are known to be vulnerable to unseen data: they may wrongly assign high confidence stcores to out-distribuion samples. Recent works try to solve the problem using representation learning methods and specific metrics. In this paper, we propose a simple, yet effective post-hoc anomaly detection algorithm named Test Time Augmentation Anomaly Detection (TTA-AD), inspired by a novel observation. Specifically, we observe that in-distribution data enjoy more consistent predictions for its original and augmented versions on a trained network than out-distribution data, which separates in-distribution and out-distribution samples. Experiments on various high-resolution image benchmark datasets demonstrate that TTA-AD achieves comparable or better detection performance under dataset-vs-dataset anomaly detection settings with a 60%~90\% running time reduction of existing classifier-based algorithms. We provide empirical verification that the key to TTA-AD lies in the remaining classes between augmented features, which has long been partially ignored by previous works. Additionally, we use RUNS as a surrogate to analyze our algorithm theoretically.



### Invariant Grounding for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2206.02349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02349v1)
- **Published**: 2022-06-06 04:37:52+00:00
- **Updated**: 2022-06-06 04:37:52+00:00
- **Authors**: Yicong Li, Xiang Wang, Junbin Xiao, Wei Ji, Tat-Seng Chua
- **Comment**: CVPR2022 Oral
- **Journal**: None
- **Summary**: Video Question Answering (VideoQA) is the task of answering questions about a video. At its core is understanding the alignments between visual scenes in video and linguistic semantics in question to yield the answer. In leading VideoQA models, the typical learning objective, empirical risk minimization (ERM), latches on superficial correlations between video-question pairs and answers as the alignments. However, ERM can be problematic, because it tends to over-exploit the spurious correlations between question-irrelevant scenes and answers, instead of inspecting the causal effect of question-critical scenes. As a result, the VideoQA models suffer from unreliable reasoning. In this work, we first take a causal look at VideoQA and argue that invariant grounding is the key to ruling out the spurious correlations. Towards this end, we propose a new learning framework, Invariant Grounding for VideoQA (IGV), to ground the question-critical scene, whose causal relations with answers are invariant across different interventions on the complement. With IGV, the VideoQA models are forced to shield the answering process from the negative influence of spurious correlations, which significantly improves the reasoning ability. Experiments on three benchmark datasets validate the superiority of IGV in terms of accuracy, visual explainability, and generalization ability over the leading baselines.



### FIFA: Making Fairness More Generalizable in Classifiers Trained on Imbalanced Data
- **Arxiv ID**: http://arxiv.org/abs/2206.02792v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.CY, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2206.02792v1)
- **Published**: 2022-06-06 04:39:25+00:00
- **Updated**: 2022-06-06 04:39:25+00:00
- **Authors**: Zhun Deng, Jiayao Zhang, Linjun Zhang, Ting Ye, Yates Coley, Weijie J. Su, James Zou
- **Comment**: None
- **Journal**: None
- **Summary**: Algorithmic fairness plays an important role in machine learning and imposing fairness constraints during learning is a common approach. However, many datasets are imbalanced in certain label classes (e.g. "healthy") and sensitive subgroups (e.g. "older patients"). Empirically, this imbalance leads to a lack of generalizability not only of classification, but also of fairness properties, especially in over-parameterized models. For example, fairness-aware training may ensure equalized odds (EO) on the training data, but EO is far from being satisfied on new users. In this paper, we propose a theoretically-principled, yet Flexible approach that is Imbalance-Fairness-Aware (FIFA). Specifically, FIFA encourages both classification and fairness generalization and can be flexibly combined with many existing fair learning methods with logits-based losses. While our main focus is on EO, FIFA can be directly applied to achieve equalized opportunity (EqOpt); and under certain conditions, it can also be applied to other fairness notions. We demonstrate the power of FIFA by combining it with a popular fair classification algorithm, and the resulting algorithm achieves significantly better fairness generalization on several real-world datasets.



### Beyond Just Vision: A Review on Self-Supervised Representation Learning on Multimodal and Temporal Data
- **Arxiv ID**: http://arxiv.org/abs/2206.02353v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02353v2)
- **Published**: 2022-06-06 04:59:44+00:00
- **Updated**: 2022-06-08 03:13:04+00:00
- **Authors**: Shohreh Deldari, Hao Xue, Aaqib Saeed, Jiayuan He, Daniel V. Smith, Flora D. Salim
- **Comment**: 36 pages, 5 figures, 9 tables, Survey paper
- **Journal**: None
- **Summary**: Recently, Self-Supervised Representation Learning (SSRL) has attracted much attention in the field of computer vision, speech, natural language processing (NLP), and recently, with other types of modalities, including time series from sensors. The popularity of self-supervised learning is driven by the fact that traditional models typically require a huge amount of well-annotated data for training. Acquiring annotated data can be a difficult and costly process. Self-supervised methods have been introduced to improve the efficiency of training data through discriminative pre-training of models using supervisory signals that have been freely obtained from the raw data. Unlike existing reviews of SSRL that have pre-dominately focused upon methods in the fields of CV or NLP for a single modality, we aim to provide the first comprehensive review of multimodal self-supervised learning methods for temporal data. To this end, we 1) provide a comprehensive categorization of existing SSRL methods, 2) introduce a generic pipeline by defining the key components of a SSRL framework, 3) compare existing models in terms of their objective function, network architecture and potential applications, and 4) review existing multimodal techniques in each category and various modalities. Finally, we present existing weaknesses and future opportunities. We believe our work develops a perspective on the requirements of SSRL in domains that utilise multimodal and/or temporal data



### Relation Matters: Foreground-aware Graph-based Relational Reasoning for Domain Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.02355v1
- **DOI**: 10.1109/TPAMI.2022.3179445
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02355v1)
- **Published**: 2022-06-06 05:12:48+00:00
- **Updated**: 2022-06-06 05:12:48+00:00
- **Authors**: Chaoqi Chen, Jiongcheng Li, Hong-Yu Zhou, Xiaoguang Han, Yue Huang, Xinghao Ding, Yizhou Yu
- **Comment**: Accepted by IEEE T-PAMI
- **Journal**: None
- **Summary**: Domain Adaptive Object Detection (DAOD) focuses on improving the generalization ability of object detectors via knowledge transfer. Recent advances in DAOD strive to change the emphasis of the adaptation process from global to local in virtue of fine-grained feature alignment methods. However, both the global and local alignment approaches fail to capture the topological relations among different foreground objects as the explicit dependencies and interactions between and within domains are neglected. In this case, only seeking one-vs-one alignment does not necessarily ensure the precise knowledge transfer. Moreover, conventional alignment-based approaches may be vulnerable to catastrophic overfitting regarding those less transferable regions (e.g. backgrounds) due to the accumulation of inaccurate localization results in the target domain. To remedy these issues, we first formulate DAOD as an open-set domain adaptation problem, in which the foregrounds and backgrounds are seen as the ``known classes'' and ``unknown class'' respectively. Accordingly, we propose a new and general framework for DAOD, named Foreground-aware Graph-based Relational Reasoning (FGRR), which incorporates graph structures into the detection pipeline to explicitly model the intra- and inter-domain foreground object relations on both pixel and semantic spaces, thereby endowing the DAOD model with the capability of relational reasoning beyond the popular alignment-based paradigm. The inter-domain visual and semantic correlations are hierarchically modeled via bipartite graph structures, and the intra-domain relations are encoded via graph attention mechanisms. Empirical results demonstrate that the proposed FGRR exceeds the state-of-the-art performance on four DAOD benchmarks.



### Implementation of a Modified U-Net for Medical Image Segmentation on Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/2206.02358v1
- **DOI**: 10.1109/TCSII.2022.3181132
- **Categories**: **eess.SP**, cs.AI, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2206.02358v1)
- **Published**: 2022-06-06 05:25:19+00:00
- **Updated**: 2022-06-06 05:25:19+00:00
- **Authors**: Owais Ali, Hazrat Ali, Syed Ayaz Ali Shah, Aamir Shahzad
- **Comment**: Preprint of paper accepted in IEEE Transactions on Circuits and
  Systems II: Express Brief
- **Journal**: None
- **Summary**: Deep learning techniques, particularly convolutional neural networks, have shown great potential in computer vision and medical imaging applications. However, deep learning models are computationally demanding as they require enormous computational power and specialized processing hardware for model training. To make these models portable and compatible for prototyping, their implementation on low-power devices is imperative. In this work, we present the implementation of Modified U-Net on Intel Movidius Neural Compute Stick 2 (NCS-2) for the segmentation of medical images. We selected U-Net because, in medical image segmentation, U-Net is a prominent model that provides improved performance for medical image segmentation even if the dataset size is small. The modified U-Net model is evaluated for performance in terms of dice score. Experiments are reported for segmentation task on three medical imaging datasets: BraTs dataset of brain MRI, heart MRI dataset, and Ziehl-Neelsen sputum smear microscopy image (ZNSDB) dataset. For the proposed model, we reduced the number of parameters from 30 million in the U-Net model to 0.49 million in the proposed architecture. Experimental results show that the modified U-Net provides comparable performance while requiring significantly lower resources and provides inference on the NCS-2. The maximum dice scores recorded are 0.96 for the BraTs dataset, 0.94 for the heart MRI dataset, and 0.74 for the ZNSDB dataset.



### Scan2Part: Fine-grained and Hierarchical Part-level Understanding of Real-World 3D Scans
- **Arxiv ID**: http://arxiv.org/abs/2206.02366v1
- **DOI**: 10.5220/0010848200003124
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02366v1)
- **Published**: 2022-06-06 05:43:10+00:00
- **Updated**: 2022-06-06 05:43:10+00:00
- **Authors**: Alexandr Notchenko, Vladislav Ishimtsev, Alexey Artemov, Vadim Selyutin, Emil Bogomolov, Evgeny Burnaev
- **Comment**: In Proceedings of the 17th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications
- **Journal**: None
- **Summary**: We propose Scan2Part, a method to segment individual parts of objects in real-world, noisy indoor RGB-D scans. To this end, we vary the part hierarchies of objects in indoor scenes and explore their effect on scene understanding models. Specifically, we use a sparse U-Net-based architecture that captures the fine-scale detail of the underlying 3D scan geometry by leveraging a multi-scale feature hierarchy. In order to train our method, we introduce the Scan2Part dataset, which is the first large-scale collection providing detailed semantic labels at the part level in the real-world setting. In total, we provide 242,081 correspondences between 53,618 PartNet parts of 2,477 ShapeNet objects and 1,506 ScanNet scenes, at two spatial resolutions of 2 cm$^3$ and 5 cm$^3$. As output, we are able to predict fine-grained per-object part labels, even when the geometry is coarse or partially missing.



### Sports Re-ID: Improving Re-Identification Of Players In Broadcast Videos Of Team Sports
- **Arxiv ID**: http://arxiv.org/abs/2206.02373v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02373v1)
- **Published**: 2022-06-06 06:06:23+00:00
- **Updated**: 2022-06-06 06:06:23+00:00
- **Authors**: Bharath Comandur
- **Comment**: None
- **Journal**: None
- **Summary**: This work focuses on player re-identification in broadcast videos of team sports. Specifically, we focus on identifying the same player in images captured from different camera viewpoints during any given moment of a match. This task differs from traditional applications of person re-id in a few important ways. Firstly, players from the same team wear highly similar clothes, thereby making it harder to tell them apart. Secondly, there are only a few number of samples for each identity, which makes it harder to train a re-id system. Thirdly, the resolutions of the images are often quite low and vary a lot. This combined with heavy occlusions and fast movements of players greatly increase the challenges for re-id. In this paper, we propose a simple but effective hierarchical data sampling procedure and a centroid loss function that, when used together, increase the mean average precision (mAP) by 7 - 11.5 and the rank-1 (R1) by 8.8 - 14.9 without any change in the network or hyper-parameters used. Our data sampling procedure improves the similarity of the training and test distributions, and thereby aids in creating better estimates of the centroids of the embeddings (or feature vectors). Surprisingly, our study shows that in the presence of severely limited data, as is the case for our application, a simple centroid loss function based on euclidean distances significantly outperforms the popular triplet-centroid loss function. We show comparable improvements for both convolutional networks and vision transformers. Our approach is among the top ranked methods in the SoccerNet Re-Identification Challenge 2022 leaderboard (test-split) with a mAP of 86.0 and a R1 of 81.5. On the sequestered challenge split, we achieve an mAP of 84.9 and a R1 of 80.1. Research on re-id for sports-related applications is very limited and our work presents one of the first discussions in the literature on this.



### CorticalFlow: A Diffeomorphic Mesh Deformation Module for Cortical Surface Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2206.02374v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.02374v1)
- **Published**: 2022-06-06 06:10:31+00:00
- **Updated**: 2022-06-06 06:10:31+00:00
- **Authors**: Léo Lebrat, Rodrigo Santa Cruz, Frédéric de Gournay, Darren Fu, Pierrick Bourgeat, Jurgen Fripp, Clinton Fookes, Olivier Salvado
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce CorticalFlow, a new geometric deep-learning model that, given a 3-dimensional image, learns to deform a reference template towards a targeted object. To conserve the template mesh's topological properties, we train our model over a set of diffeomorphic transformations. This new implementation of a flow Ordinary Differential Equation (ODE) framework benefits from a small GPU memory footprint, allowing the generation of surfaces with several hundred thousand vertices. To reduce topological errors introduced by its discrete resolution, we derive numeric conditions which improve the manifoldness of the predicted triangle mesh. To exhibit the utility of CorticalFlow, we demonstrate its performance for the challenging task of brain cortical surface reconstruction. In contrast to current state-of-the-art, CorticalFlow produces superior surfaces while reducing the computation time from nine and a half minutes to one second. More significantly, CorticalFlow enforces the generation of anatomically plausible surfaces; the absence of which has been a major impediment restricting the clinical relevance of such surface reconstruction methods.



### BInGo: Bayesian Intrinsic Groupwise Registration via Explicit Hierarchical Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2206.02377v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02377v2)
- **Published**: 2022-06-06 06:13:24+00:00
- **Updated**: 2022-12-11 10:42:13+00:00
- **Authors**: Xin Wang, Xinzhe Luo, Xiahai Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal groupwise registration aligns internal structures in a group of medical images. Current approaches to this problem involve developing similarity measures over the joint intensity profile of all images, which may be computationally prohibitive for large image groups and unstable under various conditions. To tackle these issues, we propose BInGo, a general unsupervised hierarchical Bayesian framework based on deep learning, to learn intrinsic structural representations to measure the similarity of multimodal images. Particularly, a variational auto-encoder with a novel posterior is proposed, which facilitates the disentanglement learning of structural representations and spatial transformations, and characterizes the imaging process from the common structure with shape transition and appearance variation. Notably, BInGo is scalable to learn from small groups, whereas being tested for large-scale groupwise registration, thus significantly reducing computational costs. We compared BInGo with five iterative or deep learning methods on three public intrasubject and intersubject datasets, i.e. BraTS, MS-CMR of the heart, and Learn2Reg abdomen MR-CT, and demonstrated its superior accuracy and computational efficiency, even for very large group sizes (e.g., over 1300 2D images from MS-CMR in each group).



### Semi-Supervised Segmentation of Mitochondria from Electron Microscopy Images Using Spatial Continuity
- **Arxiv ID**: http://arxiv.org/abs/2206.02392v1
- **DOI**: 10.1109/ISBI52829.2022.9761519
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.02392v1)
- **Published**: 2022-06-06 06:52:19+00:00
- **Updated**: 2022-06-06 06:52:19+00:00
- **Authors**: Yunpeng Xiao, Youpeng Zhao, Ge Yang
- **Comment**: 4 pages of main text, 5 pages of supplementary material and 1 page of
  references
- **Journal**: 2022 IEEE 19th International Symposium on Biomedical Imaging
  (ISBI). IEEE, 2022: 1-5
- **Summary**: Morphology of mitochondria plays critical roles in mediating their physiological functions. Accurate segmentation of mitochondria from 3D electron microscopy (EM) images is essential to quantitative characterization of their morphology at the nanometer scale. Fully supervised deep learning models developed for this task achieve excellent performance but require substantial amounts of annotated data for training. However, manual annotation of EM images is laborious and time-consuming because of their large volumes, limited contrast, and low signal-to-noise ratios (SNRs). To overcome this challenge, we propose a semi-supervised deep learning model that segments mitochondria by leveraging the spatial continuity of their structural, morphological, and contextual information in both labeled and unlabeled images. We use random piecewise affine transformation to synthesize comprehensive and realistic mitochondrial morphology for augmentation of training data. Experiments on the EPFL dataset show that our model achieves performance similar as that of state-of-the-art fully supervised models but requires only ~20% of their annotated training data. Our semi-supervised model is versatile and can also accurately segment other spatially continuous structures from EM images. Data and code of this study are openly accessible at https://github.com/cbmi-group/MPP.



### Topological Optimized Convolutional Visual Recurrent Network for Brain Tumor Segmentation and Classification
- **Arxiv ID**: http://arxiv.org/abs/2207.13021v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68U10, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2207.13021v1)
- **Published**: 2022-06-06 07:04:05+00:00
- **Updated**: 2022-06-06 07:04:05+00:00
- **Authors**: Dhananjay Joshi, Kapil Kumar Nagwanshi, Nitin S. Choubey, Naveen Singh Rajput
- **Comment**: None
- **Journal**: None
- **Summary**: In today's world of health care, brain tumor (BT) detection has become a common occurrence. However, the manual BT classification approach is time-consuming and only available at a few diagnostic centres. So Deep Convolutional Neural Network (DCNN) is introduced in the medical field for making accurate diagnoses and aiding in the patient's treatment before surgery. But these networks have problems such as overfitting and being unable to extract necessary features for classification. To overcome these problems, we developed the TDA-IPH and Convolutional Transfer learning and Visual Recurrent learning with Elephant Herding Optimization hyper-parameter tuning (CTVR-EHO) models for BT segmentation and classification. Initially, the Topological Data Analysis based Improved Persistent Homology (TDA-IPH) is designed to segment the BT image. Then, from the segmented image, features are extracted simultaneously using TL via the AlexNet model and Bidirectional Visual Long Short Term Memory (Bi-VLSTM). Elephant Herding Optimization (EHO) is used to tune the hyper parameters of both networks to get an optimal result. Finally, extracted features are concatenated and classified using the softmax activation layer. The simulation result of this proposed CTVR-EHO and TDA-IPH method is analysed based on some metrics such as precision, accuracy, recall, loss, and F score. When compared to other existing BT segmentation and classification models, the proposed CTVR-EHO and TDA-IPH approaches show high accuracy (99.8%), high recall (99.23%), high precision (99.67%), and high F score (99.59%).



### Image Protection for Robust Cropping Localization and Recovery
- **Arxiv ID**: http://arxiv.org/abs/2206.02405v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02405v5)
- **Published**: 2022-06-06 07:26:29+00:00
- **Updated**: 2023-03-15 02:21:30+00:00
- **Authors**: Qichao Ying, Hang Zhou, Xiaoxiao Hu, Zhenxing Qian, Sheng Li, Xinpeng Zhang
- **Comment**: Accepted by IEEE ICME 2023
- **Journal**: None
- **Summary**: Existing image cropping detection schemes ignore that recovering the cropped-out contents can unveil the purpose of the behaved cropping attack. This paper presents \textbf{CLR}-Net, a novel image protection scheme addressing the combined challenge of image \textbf{C}ropping \textbf{L}ocalization and \textbf{R}ecovery. We first protect the original image by introducing imperceptible perturbations. Then, typical image post-processing attacks are simulated to erode the protected image. On the recipient's side, we predict the cropping mask and recover the original image. Besides, we propose a novel \textbf{F}ine-\textbf{G}rained generative \textbf{JPEG} simulator (FG-JPEG) as well as a feature alignment network to improve the real-world robustness. Comprehensive experiments prove that the quality of the recovered image and the accuracy of crop localization are both satisfactory.



### Is More Data All You Need? A Causal Exploration
- **Arxiv ID**: http://arxiv.org/abs/2206.02409v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02409v1)
- **Published**: 2022-06-06 08:02:54+00:00
- **Updated**: 2022-06-06 08:02:54+00:00
- **Authors**: Athanasios Vlontzos, Hadrien Reynaud, Bernhard Kainz
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Curating a large scale medical imaging dataset for machine learning applications is both time consuming and expensive. Balancing the workload between model development, data collection and annotations is difficult for machine learning practitioners, especially under time constraints. Causal analysis is often used in medicine and economics to gain insights about the effects of actions and policies. In this paper we explore the effect of dataset interventions on the output of image classification models. Through a causal approach we investigate the effects of the quantity and type of data we need to incorporate in a dataset to achieve better performance for specific subtasks. The main goal of this paper is to highlight the potential of causal analysis as a tool for resource optimization for developing medical imaging ML applications. We explore this concept with a synthetic dataset and an exemplary use-case for Diabetic Retinopathy image analysis.



### Slim-neck by GSConv: A better design paradigm of detector architectures for autonomous vehicles
- **Arxiv ID**: http://arxiv.org/abs/2206.02424v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02424v2)
- **Published**: 2022-06-06 08:34:52+00:00
- **Updated**: 2022-08-17 07:31:56+00:00
- **Authors**: Hulin Li, Jun Li, Hanbing Wei, Zheng Liu, Zhenfei Zhan, Qiliang Ren
- **Comment**: 18 pages, 12 figures
- **Journal**: None
- **Summary**: Object detection is a significant downstream task in computer vision. For the on-board edge computing platforms, a giant model is difficult to achieve the real-time detection requirement. And, a lightweight model built from a large number of the depth-wise separable convolution layers cannot achieve the sufficient accuracy. We introduce a new lightweight convolution technique, GSConv, to lighten the model but maintain the accuracy. The GSConv accomplishes an excellent trade-off between the model's accuracy and speed. And, we provide a design paradigm, slim-neck, to achieve a higher computational cost-effectiveness of the detectors. The effectiveness of our approach was robustly demonstrated in over twenty sets comparative experiments. In particular, the detectors of ameliorated by our approach obtains state-of-the-art results (e.g. 70.9% mAP0.5 for the SODA10M at a speed of ~ 100FPS on a Tesla T4 GPU) compared with the originals. Code is available at https://github.com/alanli1997/slim-neck-by-gsconv



### mmFormer: Multimodal Medical Transformer for Incomplete Multimodal Learning of Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.02425v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02425v2)
- **Published**: 2022-06-06 08:41:56+00:00
- **Updated**: 2022-08-04 07:23:42+00:00
- **Authors**: Yao Zhang, Nanjun He, Jiawei Yang, Yuexiang Li, Dong Wei, Yawen Huang, Yang Zhang, Zhiqiang He, Yefeng Zheng
- **Comment**: Accepted to MICCAI 2022
- **Journal**: None
- **Summary**: Accurate brain tumor segmentation from Magnetic Resonance Imaging (MRI) is desirable to joint learning of multimodal images. However, in clinical practice, it is not always possible to acquire a complete set of MRIs, and the problem of missing modalities causes severe performance degradation in existing multimodal segmentation methods. In this work, we present the first attempt to exploit the Transformer for multimodal brain tumor segmentation that is robust to any combinatorial subset of available modalities. Concretely, we propose a novel multimodal Medical Transformer (mmFormer) for incomplete multimodal learning with three main components: the hybrid modality-specific encoders that bridge a convolutional encoder and an intra-modal Transformer for both local and global context modeling within each modality; an inter-modal Transformer to build and align the long-range correlations across modalities for modality-invariant features with global semantics corresponding to tumor region; a decoder that performs a progressive up-sampling and fusion with the modality-invariant features to generate robust segmentation. Besides, auxiliary regularizers are introduced in both encoder and decoder to further enhance the model's robustness to incomplete modalities. We conduct extensive experiments on the public BraTS $2018$ dataset for brain tumor segmentation. The results demonstrate that the proposed mmFormer outperforms the state-of-the-art methods for incomplete multimodal brain tumor segmentation on almost all subsets of incomplete modalities, especially by an average 19.07% improvement of Dice on tumor segmentation with only one available modality. The code is available at https://github.com/YaoZhang93/mmFormer.



### Universal Photometric Stereo Network using Global Lighting Contexts
- **Arxiv ID**: http://arxiv.org/abs/2206.02452v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02452v1)
- **Published**: 2022-06-06 09:32:06+00:00
- **Updated**: 2022-06-06 09:32:06+00:00
- **Authors**: Satoshi Ikehata
- **Comment**: Accepted to CVPR2022. Code and Dataset at
  https://satoshi-ikehata.github.io/cvpr2022/univps_cvpr2022.html
- **Journal**: None
- **Summary**: This paper tackles a new photometric stereo task, named universal photometric stereo. Unlike existing tasks that assumed specific physical lighting models; hence, drastically limited their usability, a solution algorithm of this task is supposed to work for objects with diverse shapes and materials under arbitrary lighting variations without assuming any specific models. To solve this extremely challenging task, we present a purely data-driven method, which eliminates the prior assumption of lighting by replacing the recovery of physical lighting parameters with the extraction of the generic lighting representation, named global lighting contexts. We use them like lighting parameters in a calibrated photometric stereo network to recover surface normal vectors pixelwisely. To adapt our network to a wide variety of shapes, materials and lightings, it is trained on a new synthetic dataset which simulates the appearance of objects in the wild. Our method is compared with other state-of-the-art uncalibrated photometric stereo methods on our test data to demonstrate the significance of our method.



### What do CNNs Learn in the First Layer and Why? A Linear Systems Perspective
- **Arxiv ID**: http://arxiv.org/abs/2206.02454v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02454v2)
- **Published**: 2022-06-06 09:33:33+00:00
- **Updated**: 2023-02-14 14:38:20+00:00
- **Authors**: Rhea Chowers, Yair Weiss
- **Comment**: None
- **Journal**: None
- **Summary**: It has previously been reported that the representation that is learned in the first layer of deep Convolutional Neural Networks (CNNs) is highly consistent across initializations and architectures. In this work, we quantify this consistency by considering the first layer as a filter bank and measuring its energy distribution. We find that the energy distribution is very different from that of the initial weights and is remarkably consistent across random initializations, datasets, architectures and even when the CNNs are trained with random labels. In order to explain this consistency, we derive an analytical formula for the energy profile of linear CNNs and show that this profile is mostly dictated by the second order statistics of image patches in the training set and it will approach a whitening transformation when the number of iterations goes to infinity. Finally, we show that this formula for linear CNNs also gives an excellent fit for the energy profiles learned by commonly used nonlinear CNNs such as ResNet and VGG, and that the first layer of these CNNs indeed perform approximate whitening of their inputs.



### NORPPA: NOvel Ringed seal re-identification by Pelage Pattern Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2206.02498v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02498v3)
- **Published**: 2022-06-06 11:04:16+00:00
- **Updated**: 2022-06-19 20:27:52+00:00
- **Authors**: Ekaterina Nepovinnykh, Ilia Chelak, Tuomas Eerola, Heikki Kälviäinen
- **Comment**: 22 pages, 13 figures, 5 tables
- **Journal**: None
- **Summary**: We propose a method for Saimaa ringed seal (Pusa hispida saimensis) re-identification. Access to large image volumes through camera trapping and crowdsourcing provides novel possibilities for animal monitoring and conservation and calls for automatic methods for analysis, in particular, when re-identifying individual animals from the images. The proposed method NOvel Ringed seal re-identification by Pelage Pattern Aggregation (NORPPA) utilizes the permanent and unique pelage pattern of Saimaa ringed seals and content-based image retrieval techniques. First, the query image is preprocessed, and each seal instance is segmented. Next, the seal's pelage pattern is extracted using a U-net encoder-decoder based method. Then, CNN-based affine invariant features are embedded and aggregated into Fisher Vectors. Finally, the cosine distance between the Fisher Vectors is used to find the best match from a database of known individuals. We perform extensive experiments of various modifications of the method on a new challenging Saimaa ringed seals re-identification dataset. The proposed method is shown to produce the best re-identification accuracy on our dataset in comparisons with alternative approaches.



### BehavePassDB: Public Database for Mobile Behavioral Biometrics and Benchmark Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2206.02502v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02502v2)
- **Published**: 2022-06-06 11:21:15+00:00
- **Updated**: 2022-10-04 11:21:34+00:00
- **Authors**: Giuseppe Stragapede, Ruben Vera-Rodriguez, Ruben Tolosana, Aythami Morales
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Mobile behavioral biometrics have become a popular topic of research, reaching promising results in terms of authentication, exploiting a multimodal combination of touchscreen and background sensor data. However, there is no way of knowing whether state-of-the-art classifiers in the literature can distinguish between the notion of user and device. In this article, we present a new database, BehavePassDB, structured into separate acquisition sessions and tasks to mimic the most common aspects of mobile Human-Computer Interaction (HCI). BehavePassDB is acquired through a dedicated mobile app installed on the subjects' devices, also including the case of different users on the same device for evaluation. We propose a standard experimental protocol and benchmark for the research community to perform a fair comparison of novel approaches with the state of the art. We propose and evaluate a system based on Long-Short Term Memory (LSTM) architecture with triplet loss and modality fusion at score level.



### Single pixel imaging at high pixel resolutions
- **Arxiv ID**: http://arxiv.org/abs/2206.02510v1
- **DOI**: 10.1364/OE.460025
- **Categories**: **physics.optics**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02510v1)
- **Published**: 2022-06-06 11:44:43+00:00
- **Updated**: 2022-06-06 11:44:43+00:00
- **Authors**: Rafał Stojek, Anna Pastuszczak, Piotr Wróbel, Rafał Kotyński
- **Comment**: Paper accepted to Optics Express on 23/05/2022
- **Journal**: None
- **Summary**: The usually reported pixel resolution of single pixel imaging (SPI) varies between $32 \times 32$ and $256 \times 256$ pixels falling far below imaging standards with classical methods. Low resolution results from the trade-off between the acceptable compression ratio, the limited DMD modulation frequency, and reasonable reconstruction time, and has not improved significantly during the decade of intensive research on SPI. In this paper we show that image measurement at the full resolution of the DMD, which lasts only a fraction of a second, is possible for sparse images or in a situation when the field of view is limited but is a priori unknown. We propose the sampling and reconstruction strategies that enable us to reconstruct sparse images at the resolution of $1024 \times 768$ within the time of $0.3~$s. Non-sparse images are reconstructed with less details. The compression ratio is on the order of $0.4 \%$ which corresponds to an acquisition frequency of $7~$Hz. Sampling is differential, binary, and non-adaptive, and includes information on multiple partitioning of the image which later allows us to determine the actual field of view. Reconstruction is based on the differential Fourier domain regularized inversion (D-FDRI). The proposed SPI framework is an alternative to both adaptive SPI, which is challenging to implement in real time, and to classical compressive sensing image recovery methods, which are very slow at high resolutions.



### [Reproducibility Report] Explainable Deep One-Class Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.02598v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2206.02598v1)
- **Published**: 2022-06-06 13:10:04+00:00
- **Updated**: 2022-06-06 13:10:04+00:00
- **Authors**: Joao P. C. Bertoldo, Etienne Decencière
- **Comment**: Submitted to the ML Reproducibility Challenge 2021 Fall
- **Journal**: None
- **Summary**: Fully Convolutional Data Description (FCDD), an explainable version of the Hypersphere Classifier (HSC), directly addresses image anomaly detection (AD) and pixel-wise AD without any post-hoc explainer methods. The authors claim that FCDD achieves results comparable with the state-of-the-art in sample-wise AD on Fashion-MNIST and CIFAR-10 and exceeds the state-of-the-art on the pixel-wise task on MVTec-AD. We reproduced the main results of the paper using the author's code with minor changes and provide runtime requirements to achieve if (CPU memory, GPU memory, and training time). We propose another analysis methodology using a critical difference diagram, and further investigate the test performance of the model during the training phase.



### Real-World Image Super-Resolution by Exclusionary Dual-Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.02609v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02609v1)
- **Published**: 2022-06-06 13:28:15+00:00
- **Updated**: 2022-06-06 13:28:15+00:00
- **Authors**: Hao Li, Jinghui Qin, Zhijing Yang, Pengxu Wei, Jinshan Pan, Liang Lin, Yukai Shi
- **Comment**: IEEE TMM 2022; Considering large volume of RealSR datasets, a
  multi-dataset sampling scheme is developed
- **Journal**: None
- **Summary**: Real-world image super-resolution is a practical image restoration problem that aims to obtain high-quality images from in-the-wild input, has recently received considerable attention with regard to its tremendous application potentials. Although deep learning-based methods have achieved promising restoration quality on real-world image super-resolution datasets, they ignore the relationship between L1- and perceptual- minimization and roughly adopt auxiliary large-scale datasets for pre-training. In this paper, we discuss the image types within a corrupted image and the property of perceptual- and Euclidean- based evaluation protocols. Then we propose a method, Real-World image Super-Resolution by Exclusionary Dual-Learning (RWSR-EDL) to address the feature diversity in perceptual- and L1- based cooperative learning. Moreover, a noise-guidance data collection strategy is developed to address the training time consumption in multiple datasets optimization. When an auxiliary dataset is incorporated, RWSR-EDL achieves promising results and repulses any training time increment by adopting the noise-guidance data collection strategy. Extensive experiments show that RWSR-EDL achieves competitive performance over state-of-the-art methods on four in-the-wild image super-resolution datasets.



### VPIT: Real-time Embedded Single Object 3D Tracking Using Voxel Pseudo Images
- **Arxiv ID**: http://arxiv.org/abs/2206.02619v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02619v1)
- **Published**: 2022-06-06 14:02:06+00:00
- **Updated**: 2022-06-06 14:02:06+00:00
- **Authors**: Illia Oleksiienko, Paraskevi Nousi, Nikolaos Passalis, Anastasios Tefas, Alexandros Iosifidis
- **Comment**: 10 pages, 5 figures, 4 tables. This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible
- **Journal**: None
- **Summary**: In this paper, we propose a novel voxel-based 3D single object tracking (3D SOT) method called Voxel Pseudo Image Tracking (VPIT). VPIT is the first method that uses voxel pseudo images for 3D SOT. The input point cloud is structured by pillar-based voxelization, and the resulting pseudo image is used as an input to a 2D-like Siamese SOT method. The pseudo image is created in the Bird's-eye View (BEV) coordinates, and therefore the objects in it have constant size. Thus, only the object rotation can change in the new coordinate system and not the object scale. For this reason, we replace multi-scale search with a multi-rotation search, where differently rotated search regions are compared against a single target representation to predict both position and rotation of the object. Experiments on KITTI Tracking dataset show that VPIT is the fastest 3D SOT method and maintains competitive Success and Precision values. Application of a SOT method in a real-world scenario meets with limitations such as lower computational capabilities of embedded devices and a latency-unforgiving environment, where the method is forced to skip certain data frames if the inference speed is not high enough. We implement a real-time evaluation protocol and show that other methods lose most of their performance on embedded devices, while VPIT maintains its ability to track the object.



### Hardware-accelerated Mars Sample Localization via deep transfer learning from photorealistic simulations
- **Arxiv ID**: http://arxiv.org/abs/2206.02622v2
- **DOI**: 10.1109/LRA.2022.3219306
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.02622v2)
- **Published**: 2022-06-06 14:05:25+00:00
- **Updated**: 2022-11-04 16:31:24+00:00
- **Authors**: Raúl Castilla-Arquillo, Carlos Jesús Pérez-del-Pulgar, Gonzalo Jesús Paz-Delgado, Levin Gerdes
- **Comment**: Preprint version only. Final version at IEEE Xplore. Accepted for
  IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: The goal of the Mars Sample Return campaign is to collect soil samples from the surface of Mars and return them to Earth for further study. The samples will be acquired and stored in metal tubes by the Perseverance rover and deposited on the Martian surface. As part of this campaign, it is expected that the Sample Fetch Rover will be in charge of localizing and gathering up to 35 sample tubes over 150 Martian sols. Autonomous capabilities are critical for the success of the overall campaign and for the Sample Fetch Rover in particular. This work proposes a novel system architecture for the autonomous detection and pose estimation of the sample tubes. For the detection stage, a Deep Neural Network and transfer learning from a synthetic dataset are proposed. The dataset is created from photorealistic 3D simulations of Martian scenarios. Additionally, the sample tubes poses are estimated using Computer Vision techniques such as contour detection and line fitting on the detected area. Finally, laboratory tests of the Sample Localization procedure are performed using the ExoMars Testing Rover on a Mars-like testbed. These tests validate the proposed approach in different hardware architectures, providing promising results related to the sample detection and pose estimation.



### Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.02647v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02647v1)
- **Published**: 2022-06-06 14:35:14+00:00
- **Updated**: 2022-06-06 14:35:14+00:00
- **Authors**: Richard J. Chen, Chengkuan Chen, Yicong Li, Tiffany Y. Chen, Andrew D. Trister, Rahul G. Krishnan, Faisal Mahmood
- **Comment**: Accepted to CVPR 2022 (Oral)
- **Journal**: None
- **Summary**: Vision Transformers (ViTs) and their multi-scale and hierarchical variations have been successful at capturing image representations but their use has been generally studied for low-resolution images (e.g. - 256x256, 384384). For gigapixel whole-slide imaging (WSI) in computational pathology, WSIs can be as large as 150000x150000 pixels at 20X magnification and exhibit a hierarchical structure of visual tokens across varying resolutions: from 16x16 images capture spatial patterns among cells, to 4096x4096 images characterizing interactions within the tissue microenvironment. We introduce a new ViT architecture called the Hierarchical Image Pyramid Transformer (HIPT), which leverages the natural hierarchical structure inherent in WSIs using two levels of self-supervised learning to learn high-resolution image representations. HIPT is pretrained across 33 cancer types using 10,678 gigapixel WSIs, 408,218 4096x4096 images, and 104M 256x256 images. We benchmark HIPT representations on 9 slide-level tasks, and demonstrate that: 1) HIPT with hierarchical pretraining outperforms current state-of-the-art methods for cancer subtyping and survival prediction, 2) self-supervised ViTs are able to model important inductive biases about the hierarchical structure of phenotypes in the tumor microenvironment.



### Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees
- **Arxiv ID**: http://arxiv.org/abs/2206.02659v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.ST, stat.ML, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/2206.02659v5)
- **Published**: 2022-06-06 14:52:46+00:00
- **Updated**: 2023-08-07 01:20:01+00:00
- **Authors**: Haotian Ju, Dongyue Li, Hongyang R. Zhang
- **Comment**: 37 pages. Appeared in ICML 2022
- **Journal**: None
- **Summary**: We consider fine-tuning a pretrained deep neural network on a target task. We study the generalization properties of fine-tuning to understand the problem of overfitting, which has often been observed (e.g., when the target dataset is small or when the training labels are noisy). Existing generalization measures for deep networks depend on notions such as distance from the initialization (i.e., the pretrained network) of the fine-tuned model and noise stability properties of deep networks. This paper identifies a Hessian-based distance measure through PAC-Bayesian analysis, which is shown to correlate well with observed generalization gaps of fine-tuned models. Theoretically, we prove Hessian distance-based generalization bounds for fine-tuned models. We also describe an extended study of fine-tuning against label noise, where overfitting is against a critical problem; We present an algorithm and a generalization error guarantee for this algorithm under a class conditional independent noise model. Empirically, we observe that the Hessian-based distance measure can match the scale of the observed generalization gap of fine-tuned models in practice. We also test our algorithm on several image classification tasks with noisy training labels, showing notable gains over prior methods, and the Hessian distance measure of the fine-tuned model decreases substantially.



### Learning with Capsules: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2206.02664v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.02664v1)
- **Published**: 2022-06-06 15:05:36+00:00
- **Updated**: 2022-06-06 15:05:36+00:00
- **Authors**: Fabio De Sousa Ribeiro, Kevin Duarte, Miles Everett, Georgios Leontidis, Mubarak Shah
- **Comment**: 29 pages, 43 figures
- **Journal**: None
- **Summary**: Capsule networks were proposed as an alternative approach to Convolutional Neural Networks (CNNs) for learning object-centric representations, which can be leveraged for improved generalization and sample complexity. Unlike CNNs, capsule networks are designed to explicitly model part-whole hierarchical relationships by using groups of neurons to encode visual entities, and learn the relationships between those entities. Promising early results achieved by capsule networks have motivated the deep learning community to continue trying to improve their performance and scalability across several application areas. However, a major hurdle for capsule network research has been the lack of a reliable point of reference for understanding their foundational ideas and motivations. The aim of this survey is to provide a comprehensive overview of the capsule network research landscape, which will serve as a valuable resource for the community going forward. To that end, we start with an introduction to the fundamental concepts and motivations behind capsule networks, such as equivariant inference in computer vision. We then cover the technical advances in the capsule routing mechanisms and the various formulations of capsule networks, e.g. generative and geometric. Additionally, we provide a detailed explanation of how capsule networks relate to the popular attention mechanism in Transformers, and highlight non-trivial conceptual similarities between them in the context of representation learning. Afterwards, we explore the extensive applications of capsule networks in computer vision, video and motion, graph representation learning, natural language processing, medical imaging and many others. To conclude, we provide an in-depth discussion regarding the main hurdles in capsule network research, and highlight promising research directions for future work.



### Canonical Cortical Graph Neural Networks and its Application for Speech Enhancement in Audio-Visual Hearing Aids
- **Arxiv ID**: http://arxiv.org/abs/2206.02671v3
- **DOI**: 10.1016/j.neucom.2022.11.081
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2206.02671v3)
- **Published**: 2022-06-06 15:20:07+00:00
- **Updated**: 2023-01-31 14:14:49+00:00
- **Authors**: Leandro A. Passos, João Paulo Papa, Amir Hussain, Ahsan Adeel
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the recent success of machine learning algorithms, most models face drawbacks when considering more complex tasks requiring interaction between different sources, such as multimodal input data and logical time sequences. On the other hand, the biological brain is highly sharpened in this sense, empowered to automatically manage and integrate such streams of information. In this context, this work draws inspiration from recent discoveries in brain cortical circuits to propose a more biologically plausible self-supervised machine learning approach. This combines multimodal information using intra-layer modulations together with Canonical Correlation Analysis, and a memory mechanism to keep track of temporal data, the overall approach termed Canonical Cortical Graph Neural networks. This is shown to outperform recent state-of-the-art models in terms of clean audio reconstruction and energy efficiency for a benchmark audio-visual speech dataset. The enhanced performance is demonstrated through a reduced and smother neuron firing rate distribution. suggesting that the proposed model is amenable for speech enhancement in future audio-visual hearing aid devices.



### Separable Self-attention for Mobile Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2206.02680v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.02680v1)
- **Published**: 2022-06-06 15:31:35+00:00
- **Updated**: 2022-06-06 15:31:35+00:00
- **Authors**: Sachin Mehta, Mohammad Rastegari
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across several mobile vision tasks, including classification and detection. Though these models have fewer parameters, they have high latency as compared to convolutional neural network-based models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention (MHA) in transformers, which requires $O(k^2)$ time complexity with respect to the number of tokens (or patches) $k$. Moreover, MHA requires costly operations (e.g., batch-wise matrix multiplication) for computing self-attention, impacting latency on resource-constrained devices. This paper introduces a separable self-attention method with linear complexity, i.e. $O(k)$. A simple yet effective characteristic of the proposed method is that it uses element-wise operations for computing self-attention, making it a good choice for resource-constrained devices. The improved model, MobileViTv2, is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection. With about three million parameters, MobileViTv2 achieves a top-1 accuracy of 75.6% on the ImageNet dataset, outperforming MobileViT by about 1% while running $3.2\times$ faster on a mobile device.   Our source code is available at: \url{https://github.com/apple/ml-cvnets}



### FuSS: Fusing Superpixels for Improved Segmentation Consistency
- **Arxiv ID**: http://arxiv.org/abs/2206.02714v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.02714v1)
- **Published**: 2022-06-06 16:14:19+00:00
- **Updated**: 2022-06-06 16:14:19+00:00
- **Authors**: Ian Nunes, Matheus B. Pereira, Hugo Oliveira, Jefersson A. Dos Santos, Marcus Poggi
- **Comment**: submitted to IEEEACCESS. 19 pages
- **Journal**: None
- **Summary**: In this work, we propose two different approaches to improve the semantic consistency of Open Set Semantic Segmentation. First, we propose a method called OpenGMM that extends the OpenPCS framework using a Gaussian Mixture of Models to model the distribution of pixels for each class in a multimodal manner. The second approach is a post-processing which uses superpixels to enforce highly homogeneous regions to behave equally, rectifying erroneous classified pixels within these regions, we also proposed a novel superpixel method called FuSS. All tests were performed on ISPRS Vaihingen and Potsdam datasets, and both methods were capable to improve quantitative and qualitative results for both datasets. Besides that, the post-process with FuSS achieved state-of-the-art results for both datasets. The official implementation is available at: \url{https://github.com/iannunes/FuSS}.



### Day-to-Night Image Synthesis for Training Nighttime Neural ISPs
- **Arxiv ID**: http://arxiv.org/abs/2206.02715v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02715v1)
- **Published**: 2022-06-06 16:15:45+00:00
- **Updated**: 2022-06-06 16:15:45+00:00
- **Authors**: Abhijith Punnappurath, Abdullah Abuolaim, Abdelrahman Abdelhamed, Alex Levinshtein, Michael S. Brown
- **Comment**: None
- **Journal**: None
- **Summary**: Many flagship smartphone cameras now use a dedicated neural image signal processor (ISP) to render noisy raw sensor images to the final processed output. Training nightmode ISP networks relies on large-scale datasets of image pairs with: (1) a noisy raw image captured with a short exposure and a high ISO gain; and (2) a ground truth low-noise raw image captured with a long exposure and low ISO that has been rendered through the ISP. Capturing such image pairs is tedious and time-consuming, requiring careful setup to ensure alignment between the image pairs. In addition, ground truth images are often prone to motion blur due to the long exposure. To address this problem, we propose a method that synthesizes nighttime images from daytime images. Daytime images are easy to capture, exhibit low-noise (even on smartphone cameras) and rarely suffer from motion blur. We outline a processing framework to convert daytime raw images to have the appearance of realistic nighttime raw images with different levels of noise. Our procedure allows us to easily produce aligned noisy and clean nighttime image pairs. We show the effectiveness of our synthesis framework by training neural ISPs for nightmode rendering. Furthermore, we demonstrate that using our synthetic nighttime images together with small amounts of real data (e.g., 5% to 10%) yields performance almost on par with training exclusively on real nighttime images. Our dataset and code are available at https://github.com/SamsungLabs/day-to-night.



### Scene Aware Person Image Generation through Global Contextual Conditioning
- **Arxiv ID**: http://arxiv.org/abs/2206.02717v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2206.02717v1)
- **Published**: 2022-06-06 16:18:15+00:00
- **Updated**: 2022-06-06 16:18:15+00:00
- **Authors**: Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, Umapada Pal, Michael Blumenstein
- **Comment**: Accepted in The International Conference on Pattern Recognition
  (ICPR) 2022
- **Journal**: None
- **Summary**: Person image generation is an intriguing yet challenging problem. However, this task becomes even more difficult under constrained situations. In this work, we propose a novel pipeline to generate and insert contextually relevant person images into an existing scene while preserving the global semantics. More specifically, we aim to insert a person such that the location, pose, and scale of the person being inserted blends in with the existing persons in the scene. Our method uses three individual networks in a sequential pipeline. At first, we predict the potential location and the skeletal structure of the new person by conditioning a Wasserstein Generative Adversarial Network (WGAN) on the existing human skeletons present in the scene. Next, the predicted skeleton is refined through a shallow linear network to achieve higher structural accuracy in the generated image. Finally, the target image is generated from the refined skeleton using another generative network conditioned on a given image of the target person. In our experiments, we achieve high-resolution photo-realistic generation results while preserving the general context of the scene. We conclude our paper with multiple qualitative and quantitative benchmarks on the results.



### FedNST: Federated Noisy Student Training for Automatic Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.02797v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.AI, cs.CL, cs.CV, cs.DC, cs.LG, I.2.11
- **Links**: [PDF](http://arxiv.org/pdf/2206.02797v2)
- **Published**: 2022-06-06 16:18:45+00:00
- **Updated**: 2022-07-12 20:03:11+00:00
- **Authors**: Haaris Mehmood, Agnieszka Dobrowolska, Karthikeyan Saravanan, Mete Ozay
- **Comment**: Accepted at Interspeech 2022
- **Journal**: None
- **Summary**: Federated Learning (FL) enables training state-of-the-art Automatic Speech Recognition (ASR) models on user devices (clients) in distributed systems, hence preventing transmission of raw user data to a central server. A key challenge facing practical adoption of FL for ASR is obtaining ground-truth labels on the clients. Existing approaches rely on clients to manually transcribe their speech, which is impractical for obtaining large training corpora. A promising alternative is using semi-/self-supervised learning approaches to leverage unlabelled user data. To this end, we propose FedNST, a novel method for training distributed ASR models using private and unlabelled user data. We explore various facets of FedNST, such as training models with different proportions of labelled and unlabelled data, and evaluate the proposed approach on 1173 simulated clients. Evaluating FedNST on LibriSpeech, where 960 hours of speech data is split equally into server (labelled) and client (unlabelled) data, showed a 22.5% relative word error rate reduction} (WERR) over a supervised baseline trained only on server data.



### Revisiting Realistic Test-Time Training: Sequential Inference and Adaptation by Anchored Clustering
- **Arxiv ID**: http://arxiv.org/abs/2206.02721v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.02721v2)
- **Published**: 2022-06-06 16:23:05+00:00
- **Updated**: 2022-10-14 09:44:43+00:00
- **Authors**: Yongyi Su, Xun Xu, Kui Jia
- **Comment**: NeurIPS 2022 accepted paper
- **Journal**: None
- **Summary**: Deploying models on target domain data subject to distribution shift requires adaptation. Test-time training (TTT) emerges as a solution to this adaptation under a realistic scenario where access to full source domain data is not available and instant inference on target domain is required. Despite many efforts into TTT, there is a confusion over the experimental settings, thus leading to unfair comparisons. In this work, we first revisit TTT assumptions and categorize TTT protocols by two key factors. Among the multiple protocols, we adopt a realistic sequential test-time training (sTTT) protocol, under which we further develop a test-time anchored clustering (TTAC) approach to enable stronger test-time feature learning. TTAC discovers clusters in both source and target domain and match the target clusters to the source ones to improve generalization. Pseudo label filtering and iterative updating are developed to improve the effectiveness and efficiency of anchored clustering. We demonstrate that under all TTT protocols TTAC consistently outperforms the state-of-the-art methods on six TTT datasets. We hope this work will provide a fair benchmarking of TTT methods and future research should be compared within respective protocols. A demo code is available at https://github.com/Gorilla-Lab-SCUT/TTAC.



### People Tracking in Panoramic Video for Guiding Robots
- **Arxiv ID**: http://arxiv.org/abs/2206.02735v1
- **DOI**: 10.1007/978-3-031-22216-0_28
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.02735v1)
- **Published**: 2022-06-06 16:44:38+00:00
- **Updated**: 2022-06-06 16:44:38+00:00
- **Authors**: Alberto Bacchin, Filippo Berno, Emanuele Menegatti, Alberto Pretto
- **Comment**: Accepted to 17th International Conference on Intelligent Autonomous
  Systems (IAS-17)
- **Journal**: Proceedings of the 17th International Conference on Intelligent
  Autonomous Systems (IAS 2022)
- **Summary**: A guiding robot aims to effectively bring people to and from specific places within environments that are possibly unknown to them. During this operation the robot should be able to detect and track the accompanied person, trying never to lose sight of her/him. A solution to minimize this event is to use an omnidirectional camera: its 360{\deg} Field of View (FoV) guarantees that any framed object cannot leave the FoV if not occluded or very far from the sensor. However, the acquired panoramic videos introduce new challenges in perception tasks such as people detection and tracking, including the large size of the images to be processed, the distortion effects introduced by the cylindrical projection and the periodic nature of panoramic images. In this paper, we propose a set of targeted methods that allow to effectively adapt to panoramic videos a standard people detection and tracking pipeline originally designed for perspective cameras. Our methods have been implemented and tested inside a deep learning-based people detection and tracking framework with a commercial 360{\deg} camera. Experiments performed on datasets specifically acquired for guiding robot applications and on a real service robot show the effectiveness of the proposed approach over other state-of-the-art systems. We release with this paper the acquired and annotated datasets and the open-source implementation of our method.



### CORE: Consistent Representation Learning for Face Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.02749v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2206.02749v1)
- **Published**: 2022-06-06 17:15:56+00:00
- **Updated**: 2022-06-06 17:15:56+00:00
- **Authors**: Yunsheng Ni, Depu Meng, Changqian Yu, Chengbin Quan, Dongchun Ren, Youjian Zhao
- **Comment**: Accepted by CVPRW 2022
- **Journal**: None
- **Summary**: Face manipulation techniques develop rapidly and arouse widespread public concerns. Despite that vanilla convolutional neural networks achieve acceptable performance, they suffer from the overfitting issue. To relieve this issue, there is a trend to introduce some erasing-based augmentations. We find that these methods indeed attempt to implicitly induce more consistent representations for different augmentations via assigning the same label for different augmented images. However, due to the lack of explicit regularization, the consistency between different representations is less satisfactory. Therefore, we constrain the consistency of different representations explicitly and propose a simple yet effective framework, COnsistent REpresentation Learning (CORE). Specifically, we first capture the different representations with different augmentations, then regularize the cosine distance of the representations to enhance the consistency. Extensive experiments (in-dataset and cross-dataset) demonstrate that CORE performs favorably against state-of-the-art face forgery detection methods.



### Dual Decomposition of Convex Optimization Layers for Consistent Attention in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2206.02761v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.02761v2)
- **Published**: 2022-06-06 17:38:00+00:00
- **Updated**: 2022-06-07 14:07:51+00:00
- **Authors**: Tom Ron, Michal Weiler-Sagie, Tamir Hazan
- **Comment**: 12 pages, 5 figures. In proceedings of the 39th International
  Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022.
  Copyright 2022 by the author(s)
- **Journal**: None
- **Summary**: A key concern in integrating machine learning models in medicine is the ability to interpret their reasoning. Popular explainability methods have demonstrated satisfactory results in natural image recognition, yet in medical image analysis, many of these approaches provide partial and noisy explanations. Recently, attention mechanisms have shown compelling results both in their predictive performance and in their interpretable qualities. A fundamental trait of attention is that it leverages salient parts of the input which contribute to the model's prediction. To this end, our work focuses on the explanatory value of attention weight distributions. We propose a multi-layer attention mechanism that enforces consistent interpretations between attended convolutional layers using convex optimization. We apply duality to decompose the consistency constraints between the layers by reparameterizing their attention probability distributions. We further suggest learning the dual witness by optimizing with respect to our objective; thus, our implementation uses standard back-propagation, hence it is highly efficient. While preserving predictive performance, our proposed method leverages weakly annotated medical imaging data and provides complete and faithful explanations to the model's prediction.



### Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts
- **Arxiv ID**: http://arxiv.org/abs/2206.02770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02770v1)
- **Published**: 2022-06-06 17:51:59+00:00
- **Updated**: 2022-06-06 17:51:59+00:00
- **Authors**: Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, Neil Houlsby
- **Comment**: None
- **Journal**: None
- **Summary**: Large sparsely-activated models have obtained excellent performance in multiple domains. However, such models are typically trained on a single modality at a time. We present the Language-Image MoE, LIMoE, a sparse mixture of experts model capable of multimodal learning. LIMoE accepts both images and text simultaneously, while being trained using a contrastive loss. MoEs are a natural fit for a multimodal backbone, since expert layers can learn an appropriate partitioning of modalities. However, new challenges arise; in particular, training stability and balanced expert utilization, for which we propose an entropy-based regularization scheme. Across multiple scales, we demonstrate remarkable performance improvement over dense models of equivalent computational cost. LIMoE-L/16 trained comparably to CLIP-L/14 achieves 78.6% zero-shot ImageNet accuracy (vs. 76.2%), and when further scaled to H/14 (with additional data) it achieves 84.1%, comparable to state-of-the-art methods which use larger custom per-modality backbones and pre-training schemes. We analyse the quantitative and qualitative behavior of LIMoE, and demonstrate phenomena such as differing treatment of the modalities and the organic emergence of modality-specific experts.



### Volumetric Disentanglement for 3D Scene Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2206.02776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02776v1)
- **Published**: 2022-06-06 17:57:07+00:00
- **Updated**: 2022-06-06 17:57:07+00:00
- **Authors**: Sagie Benaim, Frederik Warburg, Peter Ebert Christensen, Serge Belongie
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, advances in differential volumetric rendering enabled significant breakthroughs in the photo-realistic and fine-detailed reconstruction of complex 3D scenes, which is key for many virtual reality applications. However, in the context of augmented reality, one may also wish to effect semantic manipulations or augmentations of objects within a scene. To this end, we propose a volumetric framework for (i) disentangling or separating, the volumetric representation of a given foreground object from the background, and (ii) semantically manipulating the foreground object, as well as the background. Our framework takes as input a set of 2D masks specifying the desired foreground object for training views, together with the associated 2D views and poses, and produces a foreground-background disentanglement that respects the surrounding illumination, reflections, and partial occlusions, which can be applied to both training and novel views. Our method enables the separate control of pixel color and depth as well as 3D similarity transformations of both the foreground and background objects. We subsequently demonstrate the applicability of our framework on a number of downstream manipulation tasks including object camouflage, non-negative 3D object inpainting, 3D object translation, 3D object inpainting, and 3D text-based object manipulation. Full results are given in our project webpage at https://sagiebenaim.github.io/volumetric-disentanglement/



### Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.02777v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02777v3)
- **Published**: 2022-06-06 17:57:25+00:00
- **Updated**: 2022-12-12 15:40:34+00:00
- **Authors**: Feng Li, Hao Zhang, Huaizhe xu, Shilong Liu, Lei Zhang, Lionel M. Ni, Heung-Yeung Shum
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present Mask DINO, a unified object detection and segmentation framework. Mask DINO extends DINO (DETR with Improved Denoising Anchor Boxes) by adding a mask prediction branch which supports all image segmentation tasks (instance, panoptic, and semantic). It makes use of the query embeddings from DINO to dot-product a high-resolution pixel embedding map to predict a set of binary masks. Some key components in DINO are extended for segmentation through a shared architecture and training process. Mask DINO is simple, efficient, and scalable, and it can benefit from joint large-scale detection and segmentation datasets. Our experiments show that Mask DINO significantly outperforms all existing specialized segmentation methods, both on a ResNet-50 backbone and a pre-trained model with SwinL backbone. Notably, Mask DINO establishes the best results to date on instance segmentation (54.5 AP on COCO), panoptic segmentation (59.4 PQ on COCO), and semantic segmentation (60.8 mIoU on ADE20K) among models under one billion parameters. Code is available at \url{https://github.com/IDEACVR/MaskDINO}.



### Blended Latent Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2206.02779v2
- **DOI**: 10.1145/3592450
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.02779v2)
- **Published**: 2022-06-06 17:58:04+00:00
- **Updated**: 2023-04-30 17:43:11+00:00
- **Authors**: Omri Avrahami, Ohad Fried, Dani Lischinski
- **Comment**: Accepted to SIGGRAPH 2023. Project page:
  https://omriavrahami.com/blended-latent-diffusion-page/
- **Journal**: None
- **Summary**: The tremendous progress in neural image generation, coupled with the emergence of seemingly omnipotent vision-language models has finally enabled text-based interfaces for creating and editing images. Handling generic images requires a diverse underlying generative model, hence the latest works utilize diffusion models, which were shown to surpass GANs in terms of diversity. One major drawback of diffusion models, however, is their relatively slow inference time. In this paper, we present an accelerated solution to the task of local text-driven editing of generic images, where the desired edits are confined to a user-provided mask. Our solution leverages a recent text-to-image Latent Diffusion Model (LDM), which speeds up diffusion by operating in a lower-dimensional latent space. We first convert the LDM into a local image editor by incorporating Blended Diffusion into it. Next we propose an optimization-based solution for the inherent inability of this LDM to accurately reconstruct images. Finally, we address the scenario of performing local edits using thin masks. We evaluate our method against the available baselines both qualitatively and quantitatively and demonstrate that in addition to being faster, our method achieves better precision than the baselines while mitigating some of their artifacts.



### GenSDF: Two-Stage Learning of Generalizable Signed Distance Functions
- **Arxiv ID**: http://arxiv.org/abs/2206.02780v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.02780v2)
- **Published**: 2022-06-06 17:58:29+00:00
- **Updated**: 2022-10-05 02:09:09+00:00
- **Authors**: Gene Chou, Ilya Chugunov, Felix Heide
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate the generalization capabilities of neural signed distance functions (SDFs) for learning 3D object representations for unseen and unlabeled point clouds. Existing methods can fit SDFs to a handful of object classes and boast fine detail or fast inference speeds, but do not generalize well to unseen shapes. We introduce a two-stage semi-supervised meta-learning approach that transfers shape priors from labeled to unlabeled data to reconstruct unseen object categories. The first stage uses an episodic training scheme to simulate training on unlabeled data and meta-learns initial shape priors. The second stage then introduces unlabeled data with disjoint classes in a semi-supervised scheme to diversify these priors and achieve generalization. We assess our method on both synthetic data and real collected point clouds. Experimental results and analysis validate that our approach outperforms existing neural SDF methods and is capable of robust zero-shot inference on 100+ unseen classes. Code can be found at https://github.com/princeton-computational-imaging/gensdf.



### EVAC+: Multi-scale V-net with Deep Feature CRF Layers for Brain Extraction
- **Arxiv ID**: http://arxiv.org/abs/2206.02837v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02837v3)
- **Published**: 2022-06-06 18:21:21+00:00
- **Updated**: 2023-01-06 02:55:01+00:00
- **Authors**: Jong Sung Park, Shreyas Fadnavis, Eleftherios Garyfallidis
- **Comment**: Replaced with advancements in the model and results
- **Journal**: None
- **Summary**: Brain extraction is one of the first steps of pre-processing 3D brain MRI data and a prerequisite for any forthcoming brain imaging analyses. However, it is not a simple segmentation problem due to the complex structure of the brain and human head. Although multiple solutions have been proposed in the literature, we are still far from having truly robust methods. While previous methods have used machine learning with structural/geometric priors, with the development of Deep Learning (DL), there has been an increase in proposed Neural Network architectures. Most models focus on improving the training data and loss functions with little change in the architecture. However, the amount of accessible training data with expert-labelled ground truth vary between groups. Moreover, the labels are created not from scratch but from outputs of non-DL methods. Thus, most DL method's performance depend on the amount and quality of data one has. In this paper, we propose a novel architecture we call EVAC+ to work around this issue. We show that EVAC+ has 3 major advantages compared to other networks: (1) Multi-scale input with limited random augmentation for efficient learning, (2) a unique way of using Conditional Random Fields Recurrent Layer and (3) a loss function specifically created to enhance this architecture. We compare our model to state-of-the-art non-DL and DL methods. Results show that even with little change in the traditional architecture and limited training resources, EVAC+ achieves a high and stable Dice Coefficient and Jaccard Index along with a desirable lower surface distance. Ultimately, our model provides a robust way of accurately reducing segmentation errors in complex multi-tissue interfacing areas of brain.



### Invertible Sharpening Network for MRI Reconstruction Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2206.02838v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.02838v1)
- **Published**: 2022-06-06 18:21:48+00:00
- **Updated**: 2022-06-06 18:21:48+00:00
- **Authors**: Siyuan Dong, Eric Z. Chen, Lin Zhao, Xiao Chen, Yikang Liu, Terrence Chen, Shanhui Sun
- **Comment**: Accepted by MICCAI 2022
- **Journal**: None
- **Summary**: High-quality MRI reconstruction plays a critical role in clinical applications. Deep learning-based methods have achieved promising results on MRI reconstruction. However, most state-of-the-art methods were designed to optimize the evaluation metrics commonly used for natural images, such as PSNR and SSIM, whereas the visual quality is not primarily pursued. Compared to the fully-sampled images, the reconstructed images are often blurry, where high-frequency features might not be sharp enough for confident clinical diagnosis. To this end, we propose an invertible sharpening network (InvSharpNet) to improve the visual quality of MRI reconstructions. During training, unlike the traditional methods that learn to map the input data to the ground truth, InvSharpNet adapts a backward training strategy that learns a blurring transform from the ground truth (fully-sampled image) to the input data (blurry reconstruction). During inference, the learned blurring transform can be inverted to a sharpening transform leveraging the network's invertibility. The experiments on various MRI datasets demonstrate that InvSharpNet can improve reconstruction sharpness with few artifacts. The results were also evaluated by radiologists, indicating better visual quality and diagnostic confidence of our proposed method.



### Spatial Acoustic Projection for 3D Imaging Sonar Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2206.02840v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02840v1)
- **Published**: 2022-06-06 18:24:14+00:00
- **Updated**: 2022-06-06 18:24:14+00:00
- **Authors**: Sascha Arnold, Bilal Wehbe
- **Comment**: Preprint
- **Journal**: IEEE International Conference on Robotics and Automation (ICRA)
  2022
- **Summary**: In this work we present a novel method for reconstructing 3D surfaces using a multi-beam imaging sonar. We integrate the intensities measured by the sonar from different viewpoints for fixed cell positions in a 3D grid. For each cell we integrate a feature vector that holds the mean intensity for a discretized range of viewpoints. Based on the feature vectors and independent sparse range measurements that act as ground truth information, we train convolutional neural networks that allow us to predict the signed distance and direction to the nearest surface for each cell. The predicted signed distances can be projected into a truncated signed distance field (TSDF) along the predicted directions. Utilizing the marching cubes algorithm, a polygon mesh can be rendered from the TSDF. Our method allows a dense 3D reconstruction from a limited set of viewpoints and was evaluated on three real-world datasets.



### A Deeper Dive Into What Deep Spatiotemporal Networks Encode: Quantifying Static vs. Dynamic Information
- **Arxiv ID**: http://arxiv.org/abs/2206.02846v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02846v1)
- **Published**: 2022-06-06 18:39:37+00:00
- **Updated**: 2022-06-06 18:39:37+00:00
- **Authors**: Matthew Kowal, Mennatullah Siam, Md Amirul Islam, Neil D. B. Bruce, Richard P. Wildes, Konstantinos G. Derpanis
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Deep spatiotemporal models are used in a variety of computer vision tasks, such as action recognition and video object segmentation. Currently, there is a limited understanding of what information is captured by these models in their intermediate representations. For example, while it has been observed that action recognition algorithms are heavily influenced by visual appearance in single static frames, there is no quantitative methodology for evaluating such static bias in the latent representation compared to bias toward dynamic information (e.g. motion). We tackle this challenge by proposing a novel approach for quantifying the static and dynamic biases of any spatiotemporal model. To show the efficacy of our approach, we analyse two widely studied tasks, action recognition and video object segmentation. Our key findings are threefold: (i) Most examined spatiotemporal models are biased toward static information; although, certain two-stream architectures with cross-connections show a better balance between the static and dynamic information captured. (ii) Some datasets that are commonly assumed to be biased toward dynamics are actually biased toward static information. (iii) Individual units (channels) in an architecture can be biased toward static, dynamic or a combination of the two.



### GLF-CR: SAR-Enhanced Cloud Removal with Global-Local Fusion
- **Arxiv ID**: http://arxiv.org/abs/2206.02850v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02850v3)
- **Published**: 2022-06-06 18:53:19+00:00
- **Updated**: 2022-08-09 09:24:27+00:00
- **Authors**: Fang Xu, Yilei Shi, Patrick Ebel, Lei Yu, Gui-Song Xia, Wen Yang, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: The challenge of the cloud removal task can be alleviated with the aid of Synthetic Aperture Radar (SAR) images that can penetrate cloud cover. However, the large domain gap between optical and SAR images as well as the severe speckle noise of SAR images may cause significant interference in SAR-based cloud removal, resulting in performance degeneration. In this paper, we propose a novel global-local fusion based cloud removal (GLF-CR) algorithm to leverage the complementary information embedded in SAR images. Exploiting the power of SAR information to promote cloud removal entails two aspects. The first, global fusion, guides the relationship among all local optical windows to maintain the structure of the recovered region consistent with the remaining cloud-free regions. The second, local fusion, transfers complementary information embedded in the SAR image that corresponds to cloudy areas to generate reliable texture details of the missing regions, and uses dynamic filtering to alleviate the performance degradation caused by speckle noise. Extensive evaluation demonstrates that the proposed algorithm can yield high quality cloud-free images and outperform state-of-the-art cloud removal algorithms with a gain about 1.7dB in terms of PSNR on SEN12MS-CR dataset.



### SpikiLi: A Spiking Simulation of LiDAR based Real-time Object Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2206.02876v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.02876v1)
- **Published**: 2022-06-06 20:05:17+00:00
- **Updated**: 2022-06-06 20:05:17+00:00
- **Authors**: Sambit Mohapatra, Thomas Mesquida, Mona Hodaei, Senthil Yogamani, Heinrich Gotzig, Patrick Mader
- **Comment**: Accepted at Workshop on Event Sensing and Neuromorphic Engineering -
  8th International Conference on Event-based Control, Communication, and
  Signal Processing
- **Journal**: None
- **Summary**: Spiking Neural Networks are a recent and new neural network design approach that promises tremendous improvements in power efficiency, computation efficiency, and processing latency. They do so by using asynchronous spike-based data flow, event-based signal generation, processing, and modifying the neuron model to resemble biological neurons closely. While some initial works have shown significant initial evidence of applicability to common deep learning tasks, their applications in complex real-world tasks has been relatively low. In this work, we first illustrate the applicability of spiking neural networks to a complex deep learning task namely Lidar based 3D object detection for automated driving. Secondly, we make a step-by-step demonstration of simulating spiking behavior using a pre-trained convolutional neural network. We closely model essential aspects of spiking neural networks in simulation and achieve equivalent run-time and accuracy on a GPU. When the model is realized on a neuromorphic hardware, we expect to have significantly improved power efficiency.



### Mesh-based Dynamics with Occlusion Reasoning for Cloth Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2206.02881v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02881v2)
- **Published**: 2022-06-06 20:15:02+00:00
- **Updated**: 2022-06-22 23:50:54+00:00
- **Authors**: Zixuan Huang, Xingyu Lin, David Held
- **Comment**: RSS 2022,
  $\href{https://sites.google.com/view/occlusion-reason/home}{\text{project
  website}}$
- **Journal**: None
- **Summary**: Self-occlusion is challenging for cloth manipulation, as it makes it difficult to estimate the full state of the cloth. Ideally, a robot trying to unfold a crumpled or folded cloth should be able to reason about the cloth's occluded regions. We leverage recent advances in pose estimation for cloth to build a system that uses explicit occlusion reasoning to unfold a crumpled cloth. Specifically, we first learn a model to reconstruct the mesh of the cloth. However, the model will likely have errors due to the complexities of the cloth configurations and due to ambiguities from occlusions. Our main insight is that we can further refine the predicted reconstruction by performing test-time finetuning with self-supervised losses. The obtained reconstructed mesh allows us to use a mesh-based dynamics model for planning while reasoning about occlusions. We evaluate our system both on cloth flattening as well as on cloth canonicalization, in which the objective is to manipulate the cloth into a canonical pose. Our experiments show that our method significantly outperforms prior methods that do not explicitly account for occlusions or perform test-time optimization. Videos and visualizations can be found on our $\href{https://sites.google.com/view/occlusion-reason/home}{\text{project website}}.$



### Polymorphic-GAN: Generating Aligned Samples across Multiple Domains with Learned Morph Maps
- **Arxiv ID**: http://arxiv.org/abs/2206.02903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.02903v1)
- **Published**: 2022-06-06 21:03:02+00:00
- **Updated**: 2022-06-06 21:03:02+00:00
- **Authors**: Seung Wook Kim, Karsten Kreis, Daiqing Li, Antonio Torralba, Sanja Fidler
- **Comment**: CVPR 2022 Oral
- **Journal**: None
- **Summary**: Modern image generative models show remarkable sample quality when trained on a single domain or class of objects. In this work, we introduce a generative adversarial network that can simultaneously generate aligned image samples from multiple related domains. We leverage the fact that a variety of object classes share common attributes, with certain geometric differences. We propose Polymorphic-GAN which learns shared features across all domains and a per-domain morph layer to morph shared features according to each domain. In contrast to previous works, our framework allows simultaneous modelling of images with highly varying geometries, such as images of human faces, painted and artistic faces, as well as multiple different animal faces. We demonstrate that our model produces aligned samples for all domains and show how it can be used for applications such as segmentation transfer and cross-domain image editing, as well as training in low-data regimes. Additionally, we apply our Polymorphic-GAN on image-to-image translation tasks and show that we can greatly surpass previous approaches in cases where the geometric differences between domains are large.



### Learning Image Representations for Content Based Image Retrieval of Radiotherapy Treatment Plans
- **Arxiv ID**: http://arxiv.org/abs/2206.02912v2
- **DOI**: 10.1088/1361-6560/accdb0
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2206.02912v2)
- **Published**: 2022-06-06 21:27:08+00:00
- **Updated**: 2022-08-23 17:45:04+00:00
- **Authors**: Charles Huang, Varun Vasudevan, Oscar Pastor-Serrano, Md Tauhidul Islam, Yusuke Nomura, Piotr Dubrowski, Jen-Yeu Wang, Joseph B. Schulz, Yong Yang, Lei Xing
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: Knowledge based planning (KBP) typically involves training an end-to-end deep learning model to predict dose distributions. However, training end-to-end methods may be associated with practical limitations due to the limited size of medical datasets that are often used. To address these limitations, we propose a content based image retrieval (CBIR) method for retrieving dose distributions of previously planned patients based on anatomical similarity. Approach: Our proposed CBIR method trains a representation model that produces latent space embeddings of a patient's anatomical information. The latent space embeddings of new patients are then compared against those of previous patients in a database for image retrieval of dose distributions. All source code for this project is available on github. Main Results: The retrieval performance of various CBIR methods is evaluated on a dataset consisting of both publicly available plans and clinical plans from our institution. This study compares various encoding methods, ranging from simple autoencoders to more recent Siamese networks like SimSiam, and the best performance was observed for the multitask Siamese network. Significance: Applying CBIR to inform subsequent treatment planning potentially addresses many limitations associated with end-to-end KBP. Our current results demonstrate that excellent image retrieval performance can be obtained through slight changes to previously developed Siamese networks. We hope to integrate CBIR into automated planning workflow in future works, potentially through methods like the MetaPlanner framework.



### Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2206.02916v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.02916v2)
- **Published**: 2022-06-06 21:32:26+00:00
- **Updated**: 2022-11-19 03:48:09+00:00
- **Authors**: Zhiwei Deng, Olga Russakovsky
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an algorithm that compresses the critical information of a large dataset into compact addressable memories. These memories can then be recalled to quickly re-train a neural network and recover the performance (instead of storing and re-training on the full original dataset). Building upon the dataset distillation framework, we make a key observation that a shared common representation allows for more efficient and effective distillation. Concretely, we learn a set of bases (aka ``memories'') which are shared between classes and combined through learned flexible addressing functions to generate a diverse set of training examples. This leads to several benefits: 1) the size of compressed data does not necessarily grow linearly with the number of classes; 2) an overall higher compression rate with more effective distillation is achieved; and 3) more generalized queries are allowed beyond recalling the original classes. We demonstrate state-of-the-art results on the dataset distillation task across six benchmarks, including up to 16.5% and 9.7% in retained accuracy improvement when distilling CIFAR10 and CIFAR100 respectively. We then leverage our framework to perform continual learning, achieving state-of-the-art results on four benchmarks, with 23.2% accuracy improvement on MANY. The code is released on our project webpage https://github.com/princetonvisualai/RememberThePast-DatasetDistillation.



### Memory-efficient model-based deep learning with convergence and robustness guarantees
- **Arxiv ID**: http://arxiv.org/abs/2206.04797v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.04797v4)
- **Published**: 2022-06-06 21:56:11+00:00
- **Updated**: 2023-02-28 04:18:22+00:00
- **Authors**: Aniket Pramanik, M. Bridget Zimmerman, Mathews Jacob
- **Comment**: None
- **Journal**: None
- **Summary**: Computational imaging has been revolutionized by compressed sensing algorithms, which offer guaranteed uniqueness, convergence, and stability properties. Model-based deep learning methods that combine imaging physics with learned regularization priors have emerged as more powerful alternatives for image recovery. The main focus of this paper is to introduce a memory efficient model-based algorithm with similar theoretical guarantees as CS methods. The proposed iterative algorithm alternates between a gradient descent involving the score function and a conjugate gradient algorithm to encourage data consistency. The score function is modeled as a monotone convolutional neural network. Our analysis shows that the monotone constraint is necessary and sufficient to enforce the uniqueness of the fixed point in arbitrary inverse problems. In addition, it also guarantees the convergence to a fixed point, which is robust to input perturbations. We introduce two implementations of the proposed MOL framework, which differ in the way the monotone property is imposed. The first approach enforces a strict monotone constraint, while the second one relies on an approximation. The guarantees are not valid for the second approach in the strict sense. However, our empirical studies show that the convergence and robustness of both approaches are comparable, while the less constrained approximate implementation offers better performance. The proposed deep equilibrium formulation is significantly more memory efficient than unrolled methods, which allows us to apply it to 3D or 2D+time problems that current unrolled algorithms cannot handle.



