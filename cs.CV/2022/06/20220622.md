# Arxiv Papers in cs.CV on 2022-06-22
### Not Just Streaks: Towards Ground Truth for Single Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/2206.10779v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.10779v2)
- **Published**: 2022-06-22 00:10:06+00:00
- **Updated**: 2022-08-28 18:27:27+00:00
- **Authors**: Yunhao Ba, Howard Zhang, Ethan Yang, Akira Suzuki, Arnold Pfahnl, Chethan Chinder Chandrappa, Celso de Melo, Suya You, Stefano Soatto, Alex Wong, Achuta Kadambi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a large-scale dataset of real-world rainy and clean image pairs and a method to remove degradations, induced by rain streaks and rain accumulation, from the image. As there exists no real-world dataset for deraining, current state-of-the-art methods rely on synthetic data and thus are limited by the sim2real domain gap; moreover, rigorous evaluation remains a challenge due to the absence of a real paired dataset. We fill this gap by collecting a real paired deraining dataset through meticulous control of non-rain variations. Our dataset enables paired training and quantitative evaluation for diverse real-world rain phenomena (e.g. rain streaks and rain accumulation). To learn a representation robust to rain phenomena, we propose a deep neural network that reconstructs the underlying scene by minimizing a rain-robust loss between rainy and clean images. Extensive experiments demonstrate that our model outperforms the state-of-the-art deraining methods on real rainy images under various conditions. Project website: https://visual.ee.ucla.edu/gt_rain.htm/.



### Scaling Autoregressive Models for Content-Rich Text-to-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2206.10789v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.10789v1)
- **Published**: 2022-06-22 01:11:29+00:00
- **Updated**: 2022-06-22 01:11:29+00:00
- **Authors**: Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu
- **Comment**: Preprint
- **Journal**: None
- **Summary**: We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.



### Imitation Learning for Generalizable Self-driving Policy with Sim-to-real Transfer
- **Arxiv ID**: http://arxiv.org/abs/2206.10797v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.10797v1)
- **Published**: 2022-06-22 01:36:14+00:00
- **Updated**: 2022-06-22 01:36:14+00:00
- **Authors**: Zoltán Lőrincz, Márton Szemenyei, Róbert Moni
- **Comment**: Accepted by ICLR 2022 Workshop on Generalizable Policy Learning in
  Physical World. Source code is available at:
  https://github.com/lzoltan35/duckietown_imitation_learning
- **Journal**: None
- **Summary**: Imitation Learning uses the demonstrations of an expert to uncover the optimal policy and it is suitable for real-world robotics tasks as well. In this case, however, the training of the agent is carried out in a simulation environment due to safety, economic and time constraints. Later, the agent is applied in the real-life domain using sim-to-real methods. In this paper, we apply Imitation Learning methods that solve a robotics task in a simulated environment and use transfer learning to apply these solutions in the real-world environment. Our task is set in the Duckietown environment, where the robotic agent has to follow the right lane based on the input images of a single forward-facing camera. We present three Imitation Learning and two sim-to-real methods capable of achieving this task. A detailed comparison is provided on these techniques to highlight their advantages and disadvantages.



### SVoRT: Iterative Transformer for Slice-to-Volume Registration in Fetal Brain MRI
- **Arxiv ID**: http://arxiv.org/abs/2206.10802v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.10802v1)
- **Published**: 2022-06-22 01:55:42+00:00
- **Updated**: 2022-06-22 01:55:42+00:00
- **Authors**: Junshen Xu, Daniel Moyer, P. Ellen Grant, Polina Golland, Juan Eugenio Iglesias, Elfar Adalsteinsson
- **Comment**: Accepted by MICCAI 2022
- **Journal**: None
- **Summary**: Volumetric reconstruction of fetal brains from multiple stacks of MR slices, acquired in the presence of almost unpredictable and often severe subject motion, is a challenging task that is highly sensitive to the initialization of slice-to-volume transformations. We propose a novel slice-to-volume registration method using Transformers trained on synthetically transformed data, which model multiple stacks of MR slices as a sequence. With the attention mechanism, our model automatically detects the relevance between slices and predicts the transformation of one slice using information from other slices. We also estimate the underlying 3D volume to assist slice-to-volume registration and update the volume and transformations alternately to improve accuracy. Results on synthetic data show that our method achieves lower registration error and better reconstruction quality compared with existing state-of-the-art methods. Experiments with real-world MRI data are also performed to demonstrate the ability of the proposed model to improve the quality of 3D reconstruction under severe fetal motion.



### SSMI: How to Make Objects of Interest Disappear without Accessing Object Detectors?
- **Arxiv ID**: http://arxiv.org/abs/2206.10809v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.10809v1)
- **Published**: 2022-06-22 02:16:35+00:00
- **Updated**: 2022-06-22 02:16:35+00:00
- **Authors**: Hui Xia, Rui Zhang, Zi Kang, Shuliang Jiang
- **Comment**: 6 pages, 2 figures
- **Journal**: None
- **Summary**: Most black-box adversarial attack schemes for object detectors mainly face two shortcomings: requiring access to the target model and generating inefficient adversarial examples (failing to make objects disappear in large numbers). To overcome these shortcomings, we propose a black-box adversarial attack scheme based on semantic segmentation and model inversion (SSMI). We first locate the position of the target object using semantic segmentation techniques. Next, we design a neighborhood background pixel replacement to replace the target region pixels with background pixels to ensure that the pixel modifications are not easily detected by human vision. Finally, we reconstruct a machine-recognizable example and use the mask matrix to select pixels in the reconstructed example to modify the benign image to generate an adversarial example. Detailed experimental results show that SSMI can generate efficient adversarial examples to evade human-eye perception and make objects of interest disappear. And more importantly, SSMI outperforms existing same kinds of attacks. The maximum increase in new and disappearing labels is 16%, and the maximum decrease in mAP metrics for object detection is 36%.



### A Simple Baseline for Video Restoration with Grouped Spatial-temporal Shift
- **Arxiv ID**: http://arxiv.org/abs/2206.10810v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.10810v2)
- **Published**: 2022-06-22 02:16:47+00:00
- **Updated**: 2023-05-22 09:56:01+00:00
- **Authors**: Dasong Li, Xiaoyu Shi, Yi Zhang, Ka Chun Cheung, Simon See, Xiaogang Wang, Hongwei Qin, Hongsheng Li
- **Comment**: Accepted to CVPR2023
- **Journal**: 2023 Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition
- **Summary**: Video restoration, which aims to restore clear frames from degraded videos, has numerous important applications. The key to video restoration depends on utilizing inter-frame information. However, existing deep learning methods often rely on complicated network architectures, such as optical flow estimation, deformable convolution, and cross-frame self-attention layers, resulting in high computational costs. In this study, we propose a simple yet effective framework for video restoration. Our approach is based on grouped spatial-temporal shift, which is a lightweight and straightforward technique that can implicitly capture inter-frame correspondences for multi-frame aggregation. By introducing grouped spatial shift, we attain expansive effective receptive fields. Combined with basic 2D convolution, this simple framework can effectively aggregate inter-frame information. Extensive experiments demonstrate that our framework outperforms the previous state-of-the-art method, while using less than a quarter of its computational cost, on both video deblurring and video denoising tasks. These results indicate the potential for our approach to significantly reduce computational overhead while maintaining high-quality results. Code is avaliable at https://github.com/dasongli1/Shift-Net.



### Fighting Fire with Fire: Avoiding DNN Shortcuts through Priming
- **Arxiv ID**: http://arxiv.org/abs/2206.10816v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.10816v1)
- **Published**: 2022-06-22 03:03:09+00:00
- **Updated**: 2022-06-22 03:03:09+00:00
- **Authors**: Chuan Wen, Jianing Qian, Jierui Lin, Jiaye Teng, Dinesh Jayaraman, Yang Gao
- **Comment**: 28 pages, 13 figures, ICML2022
- **Journal**: None
- **Summary**: Across applications spanning supervised classification and sequential control, deep learning has been reported to find "shortcut" solutions that fail catastrophically under minor changes in the data distribution. In this paper, we show empirically that DNNs can be coaxed to avoid poor shortcuts by providing an additional "priming" feature computed from key input features, usually a coarse output estimate. Priming relies on approximate domain knowledge of these task-relevant key input features, which is often easy to obtain in practical settings. For example, one might prioritize recent frames over past frames in a video input for visual imitation learning, or salient foreground over background pixels for image classification. On NICO image classification, MuJoCo continuous control, and CARLA autonomous driving, our priming strategy works significantly better than several popular state-of-the-art approaches for feature selection and data augmentation. We connect these empirical findings to recent theoretical results on DNN optimization, and argue theoretically that priming distracts the optimizer away from poor shortcuts by creating better, simpler shortcuts.



### Coupling Visual Semantics of Artificial Neural Networks and Human Brain Function via Synchronized Activations
- **Arxiv ID**: http://arxiv.org/abs/2206.10821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.10821v1)
- **Published**: 2022-06-22 03:32:17+00:00
- **Updated**: 2022-06-22 03:32:17+00:00
- **Authors**: Lin Zhao, Haixing Dai, Zihao Wu, Zhenxiang Xiao, Lu Zhang, David Weizhong Liu, Xintao Hu, Xi Jiang, Sheng Li, Dajiang Zhu, Tianming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial neural networks (ANNs), originally inspired by biological neural networks (BNNs), have achieved remarkable successes in many tasks such as visual representation learning. However, whether there exists semantic correlations/connections between the visual representations in ANNs and those in BNNs remains largely unexplored due to both the lack of an effective tool to link and couple two different domains, and the lack of a general and effective framework of representing the visual semantics in BNNs such as human functional brain networks (FBNs). To answer this question, we propose a novel computational framework, Synchronized Activations (Sync-ACT), to couple the visual representation spaces and semantics between ANNs and BNNs in human brain based on naturalistic functional magnetic resonance imaging (nfMRI) data. With this approach, we are able to semantically annotate the neurons in ANNs with biologically meaningful description derived from human brain imaging for the first time. We evaluated the Sync-ACT framework on two publicly available movie-watching nfMRI datasets. The experiments demonstrate a) the significant correlation and similarity of the semantics between the visual representations in FBNs and those in a variety of convolutional neural networks (CNNs) models; b) the close relationship between CNN's visual representation similarity to BNNs and its performance in image classification tasks. Overall, our study introduces a general and effective paradigm to couple the ANNs and BNNs and provides novel insights for future studies such as brain-inspired artificial intelligence.



### A Feature Memory Rearrangement Network for Visual Inspection of Textured Surface Defects Toward Edge Intelligent Manufacturing
- **Arxiv ID**: http://arxiv.org/abs/2206.10830v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.10830v1)
- **Published**: 2022-06-22 04:05:13+00:00
- **Updated**: 2022-06-22 04:05:13+00:00
- **Authors**: Haiming Yao, Wenyong Yu, Xue Wang
- **Comment**: Revision to IEEE transactions on automation science and engineering
- **Journal**: None
- **Summary**: Recent advances in the industrial inspection of textured surfaces-in the form of visual inspection-have made such inspections possible for efficient, flexible manufacturing systems. We propose an unsupervised feature memory rearrangement network (FMR-Net) to accurately detect various textural defects simultaneously. Consistent with mainstream methods, we adopt the idea of background reconstruction; however, we innovatively utilize artificial synthetic defects to enable the model to recognize anomalies, while traditional wisdom relies only on defect-free samples. First, we employ an encoding module to obtain multiscale features of the textured surface. Subsequently, a contrastive-learning-based memory feature module (CMFM) is proposed to obtain discriminative representations and construct a normal feature memory bank in the latent space, which can be employed as a substitute for defects and fast anomaly scores at the patch level. Next, a novel global feature rearrangement module (GFRM) is proposed to further suppress the reconstruction of residual defects. Finally, a decoding module utilizes the restored features to reconstruct the normal texture background. In addition, to improve inspection performance, a two-phase training strategy is utilized for accurate defect restoration refinement, and we exploit a multimodal inspection method to achieve noise-robust defect localization. We verify our method through extensive experiments and test its practical deployment in collaborative edge--cloud intelligent manufacturing scenarios by means of a multilevel detection method, demonstrating that FMR-Net exhibits state-of-the-art inspection accuracy and shows great potential for use in edge-computing-enabled smart industries.



### MultiEarth 2022 Deforestation Challenge -- ForestGump
- **Arxiv ID**: http://arxiv.org/abs/2206.10831v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.10831v1)
- **Published**: 2022-06-22 04:10:07+00:00
- **Updated**: 2022-06-22 04:10:07+00:00
- **Authors**: Dongoo Lee, Yeonju Choi
- **Comment**: CVPR 2022, MultiEarth 2022, Deforestation Estimation Challenge
- **Journal**: None
- **Summary**: The estimation of deforestation in the Amazon Forest is challenge task because of the vast size of the area and the difficulty of direct human access. However, it is a crucial problem in that deforestation results in serious environmental problems such as global climate change, reduced biodiversity, etc. In order to effectively solve the problems, satellite imagery would be a good alternative to estimate the deforestation of the Amazon. With a combination of optical images and Synthetic aperture radar (SAR) images, observation of such a massive area regardless of weather conditions become possible. In this paper, we present an accurate deforestation estimation method with conventional UNet and comprehensive data processing. The diverse channels of Sentinel-1, Sentinel-2 and Landsat 8 are carefully selected and utilized to train deep neural networks. With the proposed method, deforestation status for novel queries are successfully estimated with high accuracy.



### Few-shot Long-Tailed Bird Audio Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.11260v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2206.11260v2)
- **Published**: 2022-06-22 04:14:25+00:00
- **Updated**: 2022-07-04 11:23:43+00:00
- **Authors**: Marcos V. Conde, Ui-Jin Choi
- **Comment**: LifeCLEF2022 (best paper award)
- **Journal**: None
- **Summary**: It is easier to hear birds than see them. However, they still play an essential role in nature and are excellent indicators of deteriorating environmental quality and pollution. Recent advances in Deep Neural Networks allow us to process audio data to detect and classify birds. This technology can assist researchers in monitoring bird populations and biodiversity. We propose a sound detection and classification pipeline to analyze complex soundscape recordings and identify birdcalls in the background. Our method learns from weak labels and few data and acoustically recognizes the bird species. Our solution achieved 18th place of 807 teams at the BirdCLEF 2022 Challenge hosted on Kaggle.



### Learning Debiased Classifier with Biased Committee
- **Arxiv ID**: http://arxiv.org/abs/2206.10843v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.10843v5)
- **Published**: 2022-06-22 04:50:28+00:00
- **Updated**: 2023-05-01 09:53:12+00:00
- **Authors**: Nayeong Kim, Sehyun Hwang, Sungsoo Ahn, Jaesik Park, Suha Kwak
- **Comment**: Conference on Neural Information Processing Systems (NeurIPS), New
  Orleans, 2022
- **Journal**: None
- **Summary**: Neural networks are prone to be biased towards spurious correlations between classes and latent attributes exhibited in a major portion of training data, which ruins their generalization capability. We propose a new method for training debiased classifiers with no spurious attribute label. The key idea is to employ a committee of classifiers as an auxiliary module that identifies bias-conflicting data, i.e., data without spurious correlation, and assigns large weights to them when training the main classifier. The committee is learned as a bootstrapped ensemble so that a majority of its classifiers are biased as well as being diverse, and intentionally fail to predict classes of bias-conflicting data accordingly. The consensus within the committee on prediction difficulty thus provides a reliable cue for identifying and weighting bias-conflicting data. Moreover, the committee is also trained with knowledge transferred from the main classifier so that it gradually becomes debiased along with the main classifier and emphasizes more difficult data as training progresses. On five real-world datasets, our method outperforms prior arts using no spurious attribute label like ours and even surpasses those relying on bias labels occasionally.



### Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.10845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.10845v1)
- **Published**: 2022-06-22 05:12:59+00:00
- **Updated**: 2022-06-22 05:12:59+00:00
- **Authors**: Ming Li, Jie Wu, Jinhang Cai, Jie Qin, Yuxi Ren, Xuefeng Xiao, Min Zheng, Rui Wang, Xin Pan
- **Comment**: The solution of 1st Place in AVA Accessibility Vision and Autonomy
  Challenge on CVPR 2022 workshop. Website: https://accessibility-cv.github.io/
- **Journal**: None
- **Summary**: Recently, Synthetic data-based Instance Segmentation has become an exceedingly favorable optimization paradigm since it leverages simulation rendering and physics to generate high-quality image-annotation pairs. In this paper, we propose a Parallel Pre-trained Transformers (PPT) framework to accomplish the synthetic data-based Instance Segmentation task. Specifically, we leverage the off-the-shelf pre-trained vision Transformers to alleviate the gap between natural and synthetic data, which helps to provide good generalization in the downstream synthetic data scene with few samples. Swin-B-based CBNet V2, SwinL-based CBNet V2 and Swin-L-based Uniformer are employed for parallel feature learning, and the results of these three models are fused by pixel-level Non-maximum Suppression (NMS) algorithm to obtain more robust results. The experimental results reveal that PPT ranks first in the CVPR2022 AVA Accessibility Vision and Autonomy Challenge, with a 65.155% mAP.



### UniCon+: ICTCAS-UCAS Submission to the AVA-ActiveSpeaker Task at ActivityNet Challenge 2022
- **Arxiv ID**: http://arxiv.org/abs/2206.10861v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2206.10861v1)
- **Published**: 2022-06-22 06:11:07+00:00
- **Updated**: 2022-06-22 06:11:07+00:00
- **Authors**: Yuanhang Zhang, Susan Liang, Shuang Yang, Shiguang Shan
- **Comment**: 5 pages, 3 figures; technical report for AVA Challenge (see
  https://research.google.com/ava/challenge.html) at the International
  Challenge on Activity Recognition (ActivityNet), CVPR 2022
- **Journal**: None
- **Summary**: This report presents a brief description of our winning solution to the AVA Active Speaker Detection (ASD) task at ActivityNet Challenge 2022. Our underlying model UniCon+ continues to build on our previous work, the Unified Context Network (UniCon) and Extended UniCon which are designed for robust scene-level ASD. We augment the architecture with a simple GRU-based module that allows information of recurring identities to flow across scenes through read and update operations. We report a best result of 94.47% mAP on the AVA-ActiveSpeaker test set, which continues to rank first on this year's challenge leaderboard and significantly pushes the state-of-the-art.



### NVIDIA-UNIBZ Submission for EPIC-KITCHENS-100 Action Anticipation Challenge 2022
- **Arxiv ID**: http://arxiv.org/abs/2206.10869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.10869v1)
- **Published**: 2022-06-22 06:34:58+00:00
- **Updated**: 2022-06-22 06:34:58+00:00
- **Authors**: Tsung-Ming Tai, Oswald Lanz, Giuseppe Fiameni, Yi-Kwan Wong, Sze-Sen Poon, Cheng-Kuang Lee, Ka-Chun Cheung, Simon See
- **Comment**: None
- **Journal**: None
- **Summary**: In this report, we describe the technical details of our submission for the EPIC-Kitchen-100 action anticipation challenge. Our modelings, the higher-order recurrent space-time transformer and the message-passing neural network with edge learning, are both recurrent-based architectures which observe only 2.5 seconds inference context to form the action anticipation prediction. By averaging the prediction scores from a set of models compiled with our proposed training pipeline, we achieved strong performance on the test set, which is 19.61% overall mean top-5 recall, recorded as second place on the public leaderboard.



### Feature Re-calibration based Multiple Instance Learning for Whole Slide Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.10878v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.10878v2)
- **Published**: 2022-06-22 07:00:39+00:00
- **Updated**: 2022-07-22 01:25:53+00:00
- **Authors**: Philip Chikontwe, Soo Jeong Nam, Heounjeong Go, Meejeong Kim, Hyun Jung Sung, Sang Hyun Park
- **Comment**: MICCAI 2022
- **Journal**: None
- **Summary**: Whole slide image (WSI) classification is a fundamental task for the diagnosis and treatment of diseases; but, curation of accurate labels is time-consuming and limits the application of fully-supervised methods. To address this, multiple instance learning (MIL) is a popular method that poses classification as a weakly supervised learning task with slide-level labels only. While current MIL methods apply variants of the attention mechanism to re-weight instance features with stronger models, scant attention is paid to the properties of the data distribution. In this work, we propose to re-calibrate the distribution of a WSI bag (instances) by using the statistics of the max-instance (critical) feature. We assume that in binary MIL, positive bags have larger feature magnitudes than negatives, thus we can enforce the model to maximize the discrepancy between bags with a metric feature loss that models positive bags as out-of-distribution. To achieve this, unlike existing MIL methods that use single-batch training modes, we propose balanced-batch sampling to effectively use the feature loss i.e., (+/-) bags simultaneously. Further, we employ a position encoding module (PEM) to model spatial/morphological information, and perform pooling by multi-head self-attention (PSMA) with a Transformer encoder. Experimental results on existing benchmark datasets show our approach is effective and improves over state-of-the-art MIL methods.



### Symmetric Network with Spatial Relationship Modeling for Natural Language-based Vehicle Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2206.10879v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.10879v1)
- **Published**: 2022-06-22 07:02:04+00:00
- **Updated**: 2022-06-22 07:02:04+00:00
- **Authors**: Chuyang Zhao, Haobo Chen, Wenyuan Zhang, Junru Chen, Sipeng Zhang, Yadong Li, Boxun Li
- **Comment**: 8 pages, 3 figures, publised to CVPRW
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition. 2022: 3226-3233
- **Summary**: Natural language (NL) based vehicle retrieval aims to search specific vehicle given text description. Different from the image-based vehicle retrieval, NL-based vehicle retrieval requires considering not only vehicle appearance, but also surrounding environment and temporal relations. In this paper, we propose a Symmetric Network with Spatial Relationship Modeling (SSM) method for NL-based vehicle retrieval. Specifically, we design a symmetric network to learn the unified cross-modal representations between text descriptions and vehicle images, where vehicle appearance details and vehicle trajectory global information are preserved. Besides, to make better use of location information, we propose a spatial relationship modeling methods to take surrounding environment and mutual relationship between vehicles into consideration. The qualitative and quantitative experiments verify the effectiveness of the proposed method. We achieve 43.92% MRR accuracy on the test set of the 6th AI City Challenge on natural language-based vehicle retrieval track, yielding the 1st place among all valid submissions on the public leaderboard. The code is available at https://github.com/hbchen121/AICITY2022_Track2_SSM.



### KiloNeuS: A Versatile Neural Implicit Surface Representation for Real-Time Rendering
- **Arxiv ID**: http://arxiv.org/abs/2206.10885v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, I.2.10; I.3.7; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2206.10885v2)
- **Published**: 2022-06-22 07:33:26+00:00
- **Updated**: 2022-11-21 10:13:29+00:00
- **Authors**: Stefano Esposito, Daniele Baieri, Stefan Zellmann, André Hinkenjann, Emanuele Rodolà
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: NeRF-based techniques fit wide and deep multi-layer perceptrons (MLPs) to a continuous radiance field that can be rendered from any unseen viewpoint. However, the lack of surface and normals definition and high rendering times limit their usage in typical computer graphics applications. Such limitations have recently been overcome separately, but solving them together remains an open problem. We present KiloNeuS, a neural representation reconstructing an implicit surface represented as a signed distance function (SDF) from multi-view images and enabling real-time rendering by partitioning the space into thousands of tiny MLPs fast to inference. As we learn the implicit surface locally using independent models, resulting in a globally coherent geometry is non-trivial and needs to be addressed during training. We evaluate rendering performance on a GPU-accelerated ray-caster with in-shader neural network inference, resulting in an average of 46 FPS at high resolution, proving a satisfying tradeoff between storage costs and rendering quality. In fact, our evaluation for rendering quality and surface recovery shows that KiloNeuS outperforms its single-MLP counterpart. Finally, to exhibit the versatility of KiloNeuS, we integrate it into an interactive path-tracer taking full advantage of its surface normals. We consider our work a crucial first step toward real-time rendering of implicit neural representations under global illumination.



### Optical Flow Regularization of Implicit Neural Representations for Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2206.10886v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.10886v1)
- **Published**: 2022-06-22 07:35:06+00:00
- **Updated**: 2022-06-22 07:35:06+00:00
- **Authors**: Weihao Zhuang, Tristan Hascoet, Ryoichi Takashima, Tetsuya Takiguchi
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works have shown the ability of Implicit Neural Representations (INR) to carry meaningful representations of signal derivatives. In this work, we leverage this property to perform Video Frame Interpolation (VFI) by explicitly constraining the derivatives of the INR to satisfy the optical flow constraint equation. We achieve state of the art VFI on limited motion ranges using only a target video and its optical flow, without learning the interpolation operator from additional training data. We further show that constraining the INR derivatives not only allows to better interpolate intermediate frames but also improves the ability of narrow networks to fit the observed frames, which suggests potential applications to video compression and INR optimization.



### I^2R-Net: Intra- and Inter-Human Relation Network for Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2206.10892v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.10892v2)
- **Published**: 2022-06-22 07:44:41+00:00
- **Updated**: 2022-06-27 11:20:17+00:00
- **Authors**: Yiwei Ding, Wenjin Deng, Yinglin Zheng, Pengfei Liu, Meihong Wang, Xuan Cheng, Jianmin Bao, Dong Chen, Ming Zeng
- **Comment**: Accepected by IJCAI 2022
- **Journal**: None
- **Summary**: In this paper, we present the Intra- and Inter-Human Relation Networks (I^2R-Net) for Multi-Person Pose Estimation. It involves two basic modules. First, the Intra-Human Relation Module operates on a single person and aims to capture Intra-Human dependencies. Second, the Inter-Human Relation Module considers the relation between multiple instances and focuses on capturing Inter-Human interactions. The Inter-Human Relation Module can be designed very lightweight by reducing the resolution of feature map, yet learn useful relation information to significantly boost the performance of the Intra-Human Relation Module. Even without bells and whistles, our method can compete or outperform current competition winners. We conduct extensive experiments on COCO, CrowdPose, and OCHuman datasets. The results demonstrate that the proposed model surpasses all the state-of-the-art methods. Concretely, the proposed method achieves 77.4% AP on CrowPose dataset and 67.8% AP on OCHuman dataset respectively, outperforming existing methods by a large margin. Additionally, the ablation study and visualization analysis also prove the effectiveness of our model.



### S2TNet: Spatio-Temporal Transformer Networks for Trajectory Prediction in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2206.10902v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.10902v1)
- **Published**: 2022-06-22 08:12:31+00:00
- **Updated**: 2022-06-22 08:12:31+00:00
- **Authors**: Weihuang Chen, Fangfang Wang, Hongbin Sun
- **Comment**: Accepted by ACML2021
- **Journal**: None
- **Summary**: To safely and rationally participate in dense and heterogeneous traffic, autonomous vehicles require to sufficiently analyze the motion patterns of surrounding traffic-agents and accurately predict their future trajectories. This is challenging because the trajectories of traffic-agents are not only influenced by the traffic-agents themselves but also by spatial interaction with each other. Previous methods usually rely on the sequential step-by-step processing of Long Short-Term Memory networks (LSTMs) and merely extract the interactions between spatial neighbors for single type traffic-agents. We propose the Spatio-Temporal Transformer Networks (S2TNet), which models the spatio-temporal interactions by spatio-temporal Transformer and deals with the temporel sequences by temporal Transformer. We input additional category, shape and heading information into our networks to handle the heterogeneity of traffic-agents. The proposed methods outperforms state-of-the-art methods on ApolloScape Trajectory dataset by more than 7\% on both the weighted sum of Average and Final Displacement Error. Our code is available at https://github.com/chenghuang66/s2tnet.



### UniUD-FBK-UB-UniBZ Submission to the EPIC-Kitchens-100 Multi-Instance Retrieval Challenge 2022
- **Arxiv ID**: http://arxiv.org/abs/2206.10903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.10903v1)
- **Published**: 2022-06-22 08:16:04+00:00
- **Updated**: 2022-06-22 08:16:04+00:00
- **Authors**: Alex Falcon, Giuseppe Serra, Sergio Escalera, Oswald Lanz
- **Comment**: Ranked joint 1st place in the Multi-Instance Action Retrieval
  Challenge organized at EPIC@CVPR2022
- **Journal**: None
- **Summary**: This report presents the technical details of our submission to the EPIC-Kitchens-100 Multi-Instance Retrieval Challenge 2022. To participate in the challenge, we designed an ensemble consisting of different models trained with two recently developed relevance-augmented versions of the widely used triplet loss. Our submission, visible on the public leaderboard, obtains an average score of 61.02% nDCG and 49.77% mAP.



### SpA-Former: Transformer image shadow detection and removal via spatial attention
- **Arxiv ID**: http://arxiv.org/abs/2206.10910v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.10910v3)
- **Published**: 2022-06-22 08:30:22+00:00
- **Updated**: 2022-10-17 03:27:55+00:00
- **Authors**: Xiao Feng Zhang, Chao Chen Gu, Shan Ying Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an end-to-end SpA-Former to recover a shadow-free image from a single shaded image. Unlike traditional methods that require two steps for shadow detection and then shadow removal, the SpA-Former unifies these steps into one, which is a one-stage network capable of directly learning the mapping function between shadows and no shadows, it does not require a separate shadow detection. Thus, SpA-former is adaptable to real image de-shadowing for shadows projected on different semantic regions. SpA-Former consists of transformer layer and a series of joint Fourier transform residual blocks and two-wheel joint spatial attention. The network in this paper is able to handle the task while achieving a very fast processing efficiency.   Our code is relased on https://github.com/zhangbaijin/SpA-Former-shadow-removal



### Influence of uncertainty estimation techniques on false-positive reduction in liver lesion detection
- **Arxiv ID**: http://arxiv.org/abs/2206.10911v4
- **DOI**: 10.59275/j.melba.2022-5937
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.10911v4)
- **Published**: 2022-06-22 08:33:52+00:00
- **Updated**: 2022-12-15 09:21:22+00:00
- **Authors**: Ishaan Bhat, Josien P. W. Pluim, Max A. Viergever, Hugo J. Kuijf
- **Comment**: Accepted for publication in the Journal of Machine Learning for
  Biomedical Imaging (MELBA)
- **Journal**: https://www.melba-journal.org/papers/2022:030.html
- **Summary**: Deep learning techniques show success in detecting objects in medical images, but still suffer from false-positive predictions that may hinder accurate diagnosis. The estimated uncertainty of the neural network output has been used to flag incorrect predictions. We study the role played by features computed from neural network uncertainty estimates and shape-based features computed from binary predictions in reducing false positives in liver lesion detection by developing a classification-based post-processing step for different uncertainty estimation methods. We demonstrate an improvement in the lesion detection performance of the neural network (with respect to F1-score) for all uncertainty estimation methods on two datasets, comprising abdominal MR and CT images, respectively. We show that features computed from neural network uncertainty estimates tend not to contribute much toward reducing false positives. Our results show that factors like class imbalance (true over false positive ratio) and shape-based features extracted from uncertainty maps play an important role in distinguishing false positive from true positive predictions. Our code can be found at https://github.com/ishaanb92/FPCPipeline.



### AI-based software for lung nodule detection in chest X-rays -- Time for a second reader approach?
- **Arxiv ID**: http://arxiv.org/abs/2206.10912v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.10912v1)
- **Published**: 2022-06-22 08:35:04+00:00
- **Updated**: 2022-06-22 08:35:04+00:00
- **Authors**: Susanne Ohlmann-Knafo, Naglis Ramanauskas, Sebastian Huettinger, Emil Johnson Jeyakumar, Darius Barušauskas, Neringa Bielskienė, Vytautas Naujalis, Jonas Bialopetravičius, Jonas Ražanskas, Artūras Samuilis, Jūratė Dementavičienė, Dirk Pickuth
- **Comment**: This paper is in submission process to the European Radiology journal
- **Journal**: None
- **Summary**: Objectives: To compare artificial intelligence (AI) as a second reader in detecting lung nodules on chest X-rays (CXR) versus radiologists of two binational institutions, and to evaluate AI performance when using two different modes: automated versus assisted (additional remote radiologist review).   Methods: The CXR public database (n = 247) of the Japanese Society of Radiological Technology with various types and sizes of lung nodules was analyzed. Eight radiologists evaluated the CXR images with regard to the presence of lung nodules and nodule conspicuity. After radiologist review, the AI software processed and flagged the CXR with the highest probability of missed nodules. The calculated accuracy metrics were the area under the curve (AUC), sensitivity, specificity, F1 score, false negative case number (FN), and the effect of different AI modes (automated/assisted) on the accuracy of nodule detection.   Results: For radiologists, the average AUC value was 0.77 $\pm$ 0.07, while the average FN was 52.63 $\pm$ 17.53 (all studies) and 32 $\pm$ 11.59 (studies containing a nodule of malignant etiology = 32% rate of missed malignant nodules). Both AI modes -- automated and assisted -- produced an average increase in sensitivity (by 14% and 12%) and of F1-score (5% and 6%) and a decrease in specificity (by 10% and 3%, respectively).   Conclusions: Both AI modes flagged the pulmonary nodules missed by radiologists in a significant number of cases. AI as a second reader has a high potential to improve diagnostic accuracy and radiology workflow. AI might detect certain pulmonary nodules earlier than radiologists, with a potentially significant impact on patient outcomes.



### Localisation And Imaging Methods for Moving Target Ghost Imaging Radar Based On Correlation Intensity Weighting
- **Arxiv ID**: http://arxiv.org/abs/2207.07649v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2207.07649v1)
- **Published**: 2022-06-22 08:39:00+00:00
- **Updated**: 2022-06-22 08:39:00+00:00
- **Authors**: Yuliang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Ghost imaging radar is a new system of gaze imaging radar with high detection sensitivity, super-resolution and better anti-interference performance, but the relative motion between the radar system and the target will make the target imaging deteriorate. This paper proposes to perform absolute position localisation of a single target in the field of view by weighting the correlation strength of a single frame image of rough target, and to compensate translation of the reference arm speckle according to the localisation and tracking trajectory to accumulate the rough image into a high quality image. The proposed correlation intensity weighted localization and tracking imaging method has been verified by simulation to be able to locate and image targets in the field of view well.



### Understanding the effect of sparsity on neural networks robustness
- **Arxiv ID**: http://arxiv.org/abs/2206.10915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.10915v1)
- **Published**: 2022-06-22 08:51:40+00:00
- **Updated**: 2022-06-22 08:51:40+00:00
- **Authors**: Lukas Timpl, Rahim Entezari, Hanie Sedghi, Behnam Neyshabur, Olga Saukh
- **Comment**: None
- **Journal**: None
- **Summary**: This paper examines the impact of static sparsity on the robustness of a trained network to weight perturbations, data corruption, and adversarial examples. We show that, up to a certain sparsity achieved by increasing network width and depth while keeping the network capacity fixed, sparsified networks consistently match and often outperform their initially dense versions. Robustness and accuracy decline simultaneously for very high sparsity due to loose connectivity between network layers. Our findings show that a rapid robustness drop caused by network compression observed in the literature is due to a reduced network capacity rather than sparsity.



### A Study on the Evaluation of Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2206.10935v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.10935v1)
- **Published**: 2022-06-22 09:27:31+00:00
- **Updated**: 2022-06-22 09:27:31+00:00
- **Authors**: Eyal Betzalel, Coby Penso, Aviv Navon, Ethan Fetaya
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Implicit generative models, which do not return likelihood values, such as generative adversarial networks and diffusion models, have become prevalent in recent years. While it is true that these models have shown remarkable results, evaluating their performance is challenging. This issue is of vital importance to push research forward and identify meaningful gains from random noise. Currently, heuristic metrics such as the Inception score (IS) and Frechet Inception Distance (FID) are the most common evaluation metrics, but what they measure is not entirely clear. Additionally, there are questions regarding how meaningful their score actually is. In this work, we study the evaluation metrics of generative models by generating a high-quality synthetic dataset on which we can estimate classical metrics for comparison. Our study shows that while FID and IS do correlate to several f-divergences, their ranking of close models can vary considerably making them problematic when used for fain-grained comparison. We further used this experimental setting to study which evaluation metric best correlates with our probabilistic metrics. Lastly, we look into the base features used for metrics such as FID.



### Polar Parametrization for Vision-based Surround-View 3D Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.10965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.10965v1)
- **Published**: 2022-06-22 10:26:12+00:00
- **Updated**: 2022-06-22 10:26:12+00:00
- **Authors**: Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian Zhang, Chang Huang, Wenyu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: 3D detection based on surround-view camera system is a critical technique in autopilot. In this work, we present Polar Parametrization for 3D detection, which reformulates position parametrization, velocity decomposition, perception range, label assignment and loss function in polar coordinate system. Polar Parametrization establishes explicit associations between image patterns and prediction targets, exploiting the view symmetry of surround-view cameras as inductive bias to ease optimization and boost performance. Based on Polar Parametrization, we propose a surround-view 3D DEtection TRansformer, named PolarDETR. PolarDETR achieves promising performance-speed trade-off on different backbone configurations. Besides, PolarDETR ranks 1st on the leaderboard of nuScenes benchmark in terms of both 3D detection and 3D tracking at the submission time (Mar. 4th, 2022). Code will be released at \url{https://github.com/hustvl/PolarDETR}.



### Single Morphing Attack Detection using Siamese Network and Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.10969v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.10969v1)
- **Published**: 2022-06-22 10:37:13+00:00
- **Updated**: 2022-06-22 10:37:13+00:00
- **Authors**: Juan Tapia, Daniel Schulz, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: Face morphing attack detection is challenging and presents a concrete and severe threat for face verification systems. Reliable detection mechanisms for such attacks, which have been tested with a robust cross-database protocol and unknown morphing tools still is a research challenge. This paper proposes a framework following the Few-Shot-Learning approach that shares image information based on the siamese network using triplet-semi-hard-loss to tackle the morphing attack detection and boost the clustering classification process. This network compares a bona fide or potentially morphed image with triplets of morphing and bona fide face images. Our results show that this new network cluster the data points, and assigns them to classes in order to obtain a lower equal error rate in a cross-database scenario sharing only small image numbers from an unknown database. Few-shot learning helps to boost the learning process. Experimental results using a cross-datasets trained with FRGCv2 and tested with FERET and the AMSL open-access databases reduced the BPCER10 from 43% to 4.91% using ResNet50 and 5.50% for MobileNetV2.



### AdvSmo: Black-box Adversarial Attack by Smoothing Linear Structure of Texture
- **Arxiv ID**: http://arxiv.org/abs/2206.10988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.10988v1)
- **Published**: 2022-06-22 11:33:15+00:00
- **Updated**: 2022-06-22 11:33:15+00:00
- **Authors**: Hui Xia, Rui Zhang, Shuliang Jiang, Zi Kang
- **Comment**: 6 pages,3 figures
- **Journal**: None
- **Summary**: Black-box attacks usually face two problems: poor transferability and the inability to evade the adversarial defense. To overcome these shortcomings, we create an original approach to generate adversarial examples by smoothing the linear structure of the texture in the benign image, called AdvSmo. We construct the adversarial examples without relying on any internal information to the target model and design the imperceptible-high attack success rate constraint to guide the Gabor filter to select appropriate angles and scales to smooth the linear texture from the input images to generate adversarial examples. Benefiting from the above design concept, AdvSmo will generate adversarial examples with strong transferability and solid evasiveness. Finally, compared to the four advanced black-box adversarial attack methods, for the eight target models, the results show that AdvSmo improves the average attack success rate by 9% on the CIFAR-10 and 16% on the Tiny-ImageNet dataset compared to the best of these attack methods.



### Identity Documents Authentication based on Forgery Detection of Guilloche Pattern
- **Arxiv ID**: http://arxiv.org/abs/2206.10989v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2206.10989v1)
- **Published**: 2022-06-22 11:37:10+00:00
- **Updated**: 2022-06-22 11:37:10+00:00
- **Authors**: Musab Al-Ghadi, Zuheng Ming, Petra Gomez-Krämer, Jean-Christophe Burie
- **Comment**: None
- **Journal**: None
- **Summary**: In cases such as digital enrolment via mobile and online services, identity document verification is critical in order to efficiently detect forgery and therefore build user trust in the digital world. In this paper, an authentication model for identity documents based on forgery detection of guilloche patterns is proposed. The proposed approach is made up of two steps: feature extraction and similarity measure between a pair of feature vectors of identity documents. The feature extraction step involves learning the similarity between a pair of identity documents via a convolutional neural network (CNN) architecture and ends by extracting highly discriminative features between them. While, the similarity measure step is applied to decide if a given identity document is authentic or forged. In this work, these two steps are combined together to achieve two objectives: (i) extracted features should have good anticollision (discriminative) capabilities to distinguish between a pair of identity documents belonging to different classes, (ii) checking out the conformity of the guilloche pattern of a given identity document and its similarity to the guilloche pattern of an authentic version of the same country. Experiments are conducted in order to analyze and identify the most proper parameters to achieve higher authentication performance. The experimental results are performed on the MIDV-2020 dataset. The results show the ability of the proposed approach to extract the relevant characteristics of the processed pair of identity documents in order to model the guilloche patterns, and thus distinguish them correctly. The implementation code and the forged dataset are provided here (https://drive.google.com/id-FDGP-1)



### Prototypical Contrastive Language Image Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2206.10996v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.10996v2)
- **Published**: 2022-06-22 11:55:53+00:00
- **Updated**: 2022-08-11 05:15:15+00:00
- **Authors**: Delong Chen, Zhao Wu, Fan Liu, Zaiquan Yang, Yixiang Huang, Yiping Bao, Erjin Zhou
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Contrastive Language Image Pretraining (CLIP) received widespread attention since its learned representations can be transferred well to various downstream tasks. During CLIP training, the InfoNCE objective aims to align positive image-text pairs and separate negative ones. In this paper, we show a representation grouping effect during this process: the InfoNCE objective indirectly groups semantically similar representations together via randomly emerged within-modal anchors. We introduce Prototypical Contrastive Language Image Pretraining (ProtoCLIP) to enhance such grouping by boosting its efficiency and increasing its robustness against modality gap. Specifically, ProtoCLIP sets up prototype-level discrimination between image and text spaces, which efficiently transfers higher-level structural knowledge. We further propose Prototypical Back Translation (PBT) to decouple representation grouping from representation alignment, resulting in effective learning of meaningful representations under large modality gap. PBT also enables us to introduce additional external teachers with richer prior knowledge. ProtoCLIP is trained with an online episodic training strategy, which makes it can be scaled up to unlimited amounts of data. We train our ProtoCLIP on Conceptual Captions and achieved an +5.81% ImageNet linear probing improvement and an +2.01% ImageNet zero-shot classification improvement. On larger YFCC dataset, ProtoCLIP matches the performance of CLIP with 4$\times$fewer pretraining epochs. Codes are available at https://github.com/megvii-research/protoclip.



### Weakly-Supervised Temporal Action Localization by Progressive Complementary Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.11011v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.11011v2)
- **Published**: 2022-06-22 12:19:09+00:00
- **Updated**: 2022-11-14 15:21:46+00:00
- **Authors**: Jia-Run Du, Jia-Chang Feng, Kun-Yu Lin, Fa-Ting Hong, Xiao-Ming Wu, Zhongang Qi, Ying Shan, Wei-Shi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly Supervised Temporal Action Localization (WSTAL) aims to localize and classify action instances in long untrimmed videos with only video-level category labels. Due to the lack of snippet-level supervision for indicating action boundaries, previous methods typically assign pseudo labels for unlabeled snippets. However, since some action instances of different categories are visually similar, it is non-trivial to exactly label the (usually) one action category for a snippet, and incorrect pseudo labels would impair the localization performance. To address this problem, we propose a novel method from a category exclusion perspective, named Progressive Complementary Learning (ProCL), which gradually enhances the snippet-level supervision. Our method is inspired by the fact that video-level labels precisely indicate the categories that all snippets surely do not belong to, which is ignored by previous works. Accordingly, we first exclude these surely non-existent categories by a complementary learning loss. And then, we introduce the background-aware pseudo complementary labeling in order to exclude more categories for snippets of less ambiguity. Furthermore, for the remaining ambiguous snippets, we attempt to reduce the ambiguity by distinguishing foreground actions from the background. Extensive experimental results show that our method achieves new state-of-the-art performance on two popular benchmarks, namely THUMOS14 and ActivityNet1.3.



### Efficient fine-grained road segmentation using superpixel-based CNN and CRF models
- **Arxiv ID**: http://arxiv.org/abs/2207.02844v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2, I.4, I.5
- **Links**: [PDF](http://arxiv.org/pdf/2207.02844v1)
- **Published**: 2022-06-22 12:38:30+00:00
- **Updated**: 2022-06-22 12:38:30+00:00
- **Authors**: Farnoush Zohourian, Jan Siegemund, Mirko Meuter, Josef Pauli
- **Comment**: 7 pages,4 figures, Original language: English, Publication status:
  Published - 2018 Publisher: CENPARMI, Centre for Pattern Recognition and
  Machine Intelligence Concordia University, Montreal, Canada Pages:512-517,
  ISBN (Electronic): 978-1-895193-04-6 Conference: International Conference on
  Pattern Recognition and Artificial Intelligence ICPRAI 2018 - Montreal,
  Quebec, Canada
- **Journal**: None
- **Summary**: Towards a safe and comfortable driving, road scene segmentation is a rudimentary problem in camera-based advance driver assistance systems (ADAS). Despite of the great achievement of Convolutional Neural Networks (CNN) for semantic segmentation task, the high computational efforts of CNN based methods is still a challenging area. In recent work, we proposed a novel approach to utilise the advantages of CNNs for the task of road segmentation at reasonable computational effort. The runtime benefits from using irregular super pixels as basis for the input for the CNN rather than the image grid, which tremendously reduces the input size. Although, this method achieved remarkable low computational time in both training and testing phases, the lower resolution of the super pixel domain yields naturally lower accuracy compared to high cost state of the art methods. In this work, we focus on a refinement of the road segmentation utilising a Conditional Random Field (CRF).The refinement procedure is limited to the super pixels touching the predicted road boundary to keep the additional computational effort low. Reducing the input to the super pixel domain allows the CNNs structure to stay small and efficient to compute while keeping the advantage of convolutional layers and makes them eligible for ADAS. Applying CRF compensate the trade off between accuracy and computational efficiency. The proposed system obtained comparable performance among the top performing algorithms on the KITTI road benchmark and its fast inference makes it particularly suitable for realtime applications.



### Automated GI tract segmentation using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2206.11048v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.11048v4)
- **Published**: 2022-06-22 13:12:54+00:00
- **Updated**: 2022-08-08 08:07:01+00:00
- **Authors**: Manhar Sharma
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: The job of Radiation oncologists is to deliver x-ray beams pointed toward the tumor and at the same time avoid the stomach and intestines. With MR-Linacs (magnetic resonance imaging and linear accelerator systems), oncologists can visualize the position of the tumor and allow for precise dose according to tumor cell presence which can vary from day to day. The current job of outlining the position of the stomach and intestines to adjust the X-ray beams direction for the dose delivery to the tumor while avoiding the organs. This is a time-consuming and labor-intensive process that can easily prolong treatments from 15 minutes to an hour a day unless deep learning methods can automate the segmentation process. This paper discusses an automated segmentation process using deep learning to make this process faster and allow more patients to get effective treatment.



### Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer
- **Arxiv ID**: http://arxiv.org/abs/2206.11053v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.11053v2)
- **Published**: 2022-06-22 13:21:31+00:00
- **Updated**: 2022-06-26 13:26:20+00:00
- **Authors**: Lalithkumar Seenivasan, Mobarakol Islam, Adithya K Krishna, Hongliang Ren
- **Comment**: Code: https://github.com/lalithjets/Surgical_VQA.git
- **Journal**: None
- **Summary**: Visual question answering (VQA) in surgery is largely unexplored. Expert surgeons are scarce and are often overloaded with clinical and academic workloads. This overload often limits their time answering questionnaires from patients, medical students or junior residents related to surgical procedures. At times, students and junior residents also refrain from asking too many questions during classes to reduce disruption. While computer-aided simulators and recording of past surgical procedures have been made available for them to observe and improve their skills, they still hugely rely on medical experts to answer their questions. Having a Surgical-VQA system as a reliable 'second opinion' could act as a backup and ease the load on the medical experts in answering these questions. The lack of annotated medical data and the presence of domain-specific terms has limited the exploration of VQA for surgical procedures. In this work, we design a Surgical-VQA task that answers questionnaires on surgical procedures based on the surgical scene. Extending the MICCAI endoscopic vision challenge 2018 dataset and workflow recognition dataset further, we introduce two Surgical-VQA datasets with classification and sentence-based answers. To perform Surgical-VQA, we employ vision-text transformers models. We further introduce a residual MLP-based VisualBert encoder model that enforces interaction between visual and text tokens, improving performance in classification-based answering. Furthermore, we study the influence of the number of input image patches and temporal visual features on the model performance in both classification and sentence-based answering.



### Motion Gait: Gait Recognition via Motion Excitation
- **Arxiv ID**: http://arxiv.org/abs/2206.11080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.11080v1)
- **Published**: 2022-06-22 13:47:14+00:00
- **Updated**: 2022-06-22 13:47:14+00:00
- **Authors**: Yunpeng Zhang, Zhengyou Wang, Shanna Zhuang, Hui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Gait recognition, which can realize long-distance and contactless identification, is an important biometric technology. Recent gait recognition methods focus on learning the pattern of human movement or appearance during walking, and construct the corresponding spatio-temporal representations. However, different individuals have their own laws of movement patterns, simple spatial-temporal features are difficult to describe changes in motion of human parts, especially when confounding variables such as clothing and carrying are included, thus distinguishability of features is reduced. In this paper, we propose the Motion Excitation Module (MEM) to guide spatio-temporal features to focus on human parts with large dynamic changes, MEM learns the difference information between frames and intervals, so as to obtain the representation of temporal motion changes, it is worth mentioning that MEM can adapt to frame sequences with uncertain length, and it does not add any additional parameters. Furthermore, we present the Fine Feature Extractor (FFE), which independently learns the spatio-temporal representations of human body according to different horizontal parts of individuals. Benefiting from MEM and FFE, our method innovatively combines motion change information, significantly improving the performance of the model under cross appearance conditions. On the popular dataset CASIA-B, our proposed Motion Gait is better than the existing gait recognition methods.



### A High Resolution Multi-exposure Stereoscopic Image & Video Database of Natural Scenes
- **Arxiv ID**: http://arxiv.org/abs/2206.11095v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2206.11095v1)
- **Published**: 2022-06-22 13:54:02+00:00
- **Updated**: 2022-06-22 13:54:02+00:00
- **Authors**: Rohit Choudhary, Mansi Sharma, Aditya Wadaskar
- **Comment**: None
- **Journal**: None
- **Summary**: Immersive displays such as VR headsets, AR glasses, Multiview displays, Free point televisions have emerged as a new class of display technologies in recent years, offering a better visual experience and viewer engagement as compared to conventional displays. With the evolution of 3D video and display technologies, the consumer market for High Dynamic Range (HDR) cameras and displays is quickly growing. The lack of appropriate experimental data is a critical hindrance for the development of primary research efforts in the field of 3D HDR video technology. Also, the unavailability of sufficient real world multi-exposure experimental dataset is a major bottleneck for HDR imaging research, thereby limiting the quality of experience (QoE) for the viewers. In this paper, we introduce a diversified stereoscopic multi-exposure dataset captured within the campus of Indian Institute of Technology Madras, which is home to a diverse flora and fauna. The dataset is captured using ZED stereoscopic camera and provides intricate scenes of outdoor locations such as gardens, roadside views, festival venues, buildings and indoor locations such as academic and residential areas. The proposed dataset accommodates wide depth range, complex depth structure, complicate object movement, illumination variations, rich color dynamics, texture discrepancy in addition to significant randomness introduced by moving camera and background motion. The proposed dataset is made publicly available to the research community. Furthermore, the procedure for capturing, aligning and calibrating multi-exposure stereo videos and images is described in detail. Finally, we have discussed the progress, challenges, potential use cases and future research opportunities with respect to HDR imaging, depth estimation, consistent tone mapping and 3D HDR coding.



### ICC++: Explainable Image Retrieval for Art Historical Corpora using Image Composition Canvas
- **Arxiv ID**: http://arxiv.org/abs/2206.11115v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.11115v1)
- **Published**: 2022-06-22 14:06:29+00:00
- **Updated**: 2022-06-22 14:06:29+00:00
- **Authors**: Prathmesh Madhu, Tilman Marquart, Ronak Kosti, Dirk Suckow, Peter Bell, Andreas Maier, Vincent Christlein
- **Comment**: None
- **Journal**: None
- **Summary**: Image compositions are helpful in the study of image structures and assist in discovering the semantics of the underlying scene portrayed across art forms and styles. With the digitization of artworks in recent years, thousands of images of a particular scene or narrative could potentially be linked together. However, manually linking this data with consistent objectiveness can be a highly challenging and time-consuming task. In this work, we present a novel approach called Image Composition Canvas (ICC++) to compare and retrieve images having similar compositional elements. ICC++ is an improvement over ICC specializing in generating low and high-level features (compositional elements) motivated by Max Imdahl's work. To this end, we present a rigorous quantitative and qualitative comparison of our approach with traditional and state-of-the-art (SOTA) methods showing that our proposed method outperforms all of them. In combination with deep features, our method outperforms the best deep learning-based method, opening the research direction for explainable machine learning for digital humanities. We will release the code and the data post-publication.



### CNN-based fully automatic wrist cartilage volume quantification in MR Image
- **Arxiv ID**: http://arxiv.org/abs/2206.11127v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2206.11127v1)
- **Published**: 2022-06-22 14:19:06+00:00
- **Updated**: 2022-06-22 14:19:06+00:00
- **Authors**: Nikita Vladimirov, Ekaterina Brui, Anatoliy Levchuk, Vladimir Fokin, Aleksandr Efimtcev, David Bendahan
- **Comment**: 17 pages, 6 Figures, 6 Tables, 1 Suplementary
- **Journal**: None
- **Summary**: Detection of cartilage loss is crucial for the diagnosis of osteo- and rheumatoid arthritis. A large number of automatic segmentation tools have been reported so far for cartilage assessment in magnetic resonance images of large joints. As compared to knee or hip, wrist cartilage has a more complex structure so that automatic tools developed for large joints are not expected to be operational for wrist cartilage segmentation. In that respect, a fully automatic wrist cartilage segmentation method would be of high clinical interest. We assessed the performance of four optimized variants of the U-Net architecture with truncation of its depth and addition of attention layers (U-Net_AL). The corresponding results were compared to those from a patch-based convolutional neural network (CNN) we previously designed. The segmentation quality was assessed on the basis of a comparative analysis with manual segmentation using several morphological (2D DSC, 3D DSC, precision) and a volumetric metrics. The four networks outperformed the patch-based CNN in terms of segmentation homogeneity and quality. The median 3D DSC value computed with the U-Net_AL (0.817) was significantly larger than the corresponding DSC values computed with the other networks. In addition, the U-Net_AL CNN provided the lowest mean volume error (17%) and the highest Pearson correlation coefficient (0.765) with respect to the ground truth. Of interest, the reproducibility computed from using U-Net_AL was larger than the reproducibility of the manual segmentation. U-net convolutional neural network with additional attention layers provides the best wrist cartilage segmentation performance. In order to be used in clinical conditions, the trained network can be fine-tuned on a dataset representing a group of specific patients. The error of cartilage volume measurement should be assessed independently using a non-MRI method.



### Open Vocabulary Object Detection with Proposal Mining and Prediction Equalization
- **Arxiv ID**: http://arxiv.org/abs/2206.11134v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.11134v4)
- **Published**: 2022-06-22 14:30:41+00:00
- **Updated**: 2022-11-24 09:17:05+00:00
- **Authors**: Peixian Chen, Kekai Sheng, Mengdan Zhang, Mingbao Lin, Yunhang Shen, Shaohui Lin, Bo Ren, Ke Li
- **Comment**: None
- **Journal**: None
- **Summary**: Open-vocabulary object detection (OVD) aims to scale up vocabulary size to detect objects of novel categories beyond the training vocabulary. Recent work resorts to the rich knowledge in pre-trained vision-language models. However, existing methods are ineffective in proposal-level vision-language alignment. Meanwhile, the models usually suffer from confidence bias toward base categories and perform worse on novel ones. To overcome the challenges, we present MEDet, a novel and effective OVD framework with proposal mining and prediction equalization. First, we design an online proposal mining to refine the inherited vision-semantic knowledge from coarse to fine, allowing for proposal-level detection-oriented feature alignment. Second, based on causal inference theory, we introduce a class-wise backdoor adjustment to reinforce the predictions on novel categories to improve the overall OVD performance. Extensive experiments on COCO and LVIS benchmarks verify the superiority of MEDet over the competing approaches in detecting objects of novel categories, e.g., 32.6% AP50 on COCO and 22.4% mask mAP on LVIS.



### A Fast Text-Driven Approach for Generating Artistic Content
- **Arxiv ID**: http://arxiv.org/abs/2208.01748v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.01748v1)
- **Published**: 2022-06-22 14:34:59+00:00
- **Updated**: 2022-06-22 14:34:59+00:00
- **Authors**: Marian Lupascu, Ryan Murdock, Ionut Mironică, Yijun Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a complete framework that generates visual art. Unlike previous stylization methods that are not flexible with style parameters (i.e., they allow stylization with only one style image, a single stylization text or stylization of a content image from a certain domain), our method has no such restriction. In addition, we implement an improved version that can generate a wide range of results with varying degrees of detail, style and structure, with a boost in generation speed. To further enhance the results, we insert an artistic super-resolution module in the generative pipeline. This module will bring additional details such as patterns specific to painters, slight brush marks, and so on.



### Hybrid Physical Metric For 6-DoF Grasp Pose Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.11141v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.11141v1)
- **Published**: 2022-06-22 14:35:48+00:00
- **Updated**: 2022-06-22 14:35:48+00:00
- **Authors**: Yuhao Lu, Beixing Deng, Zhenyu Wang, Peiyuan Zhi, Yali Li, Shengjin Wang
- **Comment**: 7 pages, 7 figures, accepted by ICRA 2022
- **Journal**: None
- **Summary**: 6-DoF grasp pose detection of multi-grasp and multi-object is a challenge task in the field of intelligent robot. To imitate human reasoning ability for grasping objects, data driven methods are widely studied. With the introduction of large-scale datasets, we discover that a single physical metric usually generates several discrete levels of grasp confidence scores, which cannot finely distinguish millions of grasp poses and leads to inaccurate prediction results. In this paper, we propose a hybrid physical metric to solve this evaluation insufficiency. First, we define a novel metric is based on the force-closure metric, supplemented by the measurement of the object flatness, gravity and collision. Second, we leverage this hybrid physical metric to generate elaborate confidence scores. Third, to learn the new confidence scores effectively, we design a multi-resolution network called Flatness Gravity Collision GraspNet (FGC-GraspNet). FGC-GraspNet proposes a multi-resolution features learning architecture for multiple tasks and introduces a new joint loss function that enhances the average precision of the grasp detection. The network evaluation and adequate real robot experiments demonstrate the effectiveness of our hybrid physical metric and FGC-GraspNet. Our method achieves 90.5\% success rate in real-world cluttered scenes. Our code is available at https://github.com/luyh20/FGC-GraspNet.



### Optimal transport meets noisy label robust loss and MixUp regularization for domain adaptation
- **Arxiv ID**: http://arxiv.org/abs/2206.11180v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2206.11180v1)
- **Published**: 2022-06-22 15:40:52+00:00
- **Updated**: 2022-06-22 15:40:52+00:00
- **Authors**: Kilian Fatras, Hiroki Naganuma, Ioannis Mitliagkas
- **Comment**: None
- **Journal**: None
- **Summary**: It is common in computer vision to be confronted with domain shift: images which have the same class but different acquisition conditions. In domain adaptation (DA), one wants to classify unlabeled target images using source labeled images. Unfortunately, deep neural networks trained on a source training set perform poorly on target images which do not belong to the training domain. One strategy to improve these performances is to align the source and target image distributions in an embedded space using optimal transport (OT). However OT can cause negative transfer, i.e. aligning samples with different labels, which leads to overfitting especially in the presence of label shift between domains. In this work, we mitigate negative alignment by explaining it as a noisy label assignment to target images. We then mitigate its effect by appropriate regularization. We propose to couple the MixUp regularization \citep{zhang2018mixup} with a loss that is robust to noisy labels in order to improve domain adaptation performance. We show in an extensive ablation study that a combination of the two techniques is critical to achieve improved performance. Finally, we evaluate our method, called \textsc{mixunbot}, on several benchmarks and real-world DA problems.



### Independent evaluation of state-of-the-art deep networks for mammography
- **Arxiv ID**: http://arxiv.org/abs/2206.12407v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2206.12407v1)
- **Published**: 2022-06-22 15:56:17+00:00
- **Updated**: 2022-06-22 15:56:17+00:00
- **Authors**: Osvaldo Matias Velarde, Lucas Parra
- **Comment**: 17 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: Deep neural models have shown remarkable performance in image recognition tasks, whenever large datasets of labeled images are available. The largest datasets in radiology are available for screening mammography. Recent reports, including in high impact journals, document performance of deep models at or above that of trained radiologists. What is not yet known is whether performance of these trained models is robust and replicates across datasets. Here we evaluate performance of five published state-of-the-art models on four publicly available mammography datasets. The limited size of public datasets precludes retraining the model and so we are limited to evaluate those models that have been made available with pre-trained parameters. Where test data was available, we replicated published results. However, the trained models performed poorly on out-of-sample data, except when based on all four standard views of a mammographic exam. We conclude that future progress will depend on a concerted effort to make more diverse and larger mammography datasets publicly available. Meanwhile, results that are not accompanied by a release of trained models for independent validation should be judged cautiously.



### Facke: a Survey on Generative Models for Face Swapping
- **Arxiv ID**: http://arxiv.org/abs/2206.11203v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.11203v1)
- **Published**: 2022-06-22 16:41:17+00:00
- **Updated**: 2022-06-22 16:41:17+00:00
- **Authors**: Wei Jiang, Wentao Dong
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we investigate into the performance of mainstream neural generative models on the very task of swapping faces. We have experimented on CVAE, CGAN, CVAE-GAN, and conditioned diffusion models. Existing finely trained models have already managed to produce fake faces (Facke) indistinguishable to the naked eye as well as achieve high objective metrics. We perform a comparison among them and analyze their pros and cons. Furthermore, we proposed some promising tricks though they do not apply to this task.



### VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives
- **Arxiv ID**: http://arxiv.org/abs/2206.11212v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.11212v2)
- **Published**: 2022-06-22 17:02:01+00:00
- **Updated**: 2022-10-25 19:25:54+00:00
- **Authors**: Zhuofan Ying, Peter Hase, Mohit Bansal
- **Comment**: NeurIPS 2022 (first two authors contributed equally)
- **Journal**: None
- **Summary**: Many past works aim to improve visual reasoning in models by supervising feature importance (estimated by model explanation techniques) with human annotations such as highlights of important image regions. However, recent work has shown that performance gains from feature importance (FI) supervision for Visual Question Answering (VQA) tasks persist even with random supervision, suggesting that these methods do not meaningfully align model FI with human FI. In this paper, we show that model FI supervision can meaningfully improve VQA model accuracy as well as performance on several Right-for-the-Right-Reason (RRR) metrics by optimizing for four key model objectives: (1) accurate predictions given limited but sufficient information (Sufficiency); (2) max-entropy predictions given no important information (Uncertainty); (3) invariance of predictions to changes in unimportant features (Invariance); and (4) alignment between model FI explanations and human FI explanations (Plausibility). Our best performing method, Visual Feature Importance Supervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in terms of both in-distribution and out-of-distribution accuracy. While past work suggests that the mechanism for improved accuracy is through improved explanation plausibility, we show that this relationship depends crucially on explanation faithfulness (whether explanations truly represent the model's internal reasoning). Predictions are more accurate when explanations are plausible and faithful, and not when they are plausible but not faithful. Lastly, we show that, surprisingly, RRR metrics are not predictive of out-of-distribution model accuracy when controlling for a model's in-distribution accuracy, which calls into question the value of these metrics for evaluating model reasoning. All supporting code is available at https://github.com/zfying/visfis



### Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training
- **Arxiv ID**: http://arxiv.org/abs/2206.11215v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.11215v4)
- **Published**: 2022-06-22 17:06:39+00:00
- **Updated**: 2023-04-28 19:47:34+00:00
- **Authors**: Rajat Talak, Lisa Peng, Luca Carlone
- **Comment**: None
- **Journal**: None
- **Summary**: We consider a certifiable object pose estimation problem, where -- given a partial point cloud of an object -- the goal is to not only estimate the object pose, but also to provide a certificate of correctness for the resulting estimate. Our first contribution is a general theory of certification for end-to-end perception models. In particular, we introduce the notion of $\zeta$-correctness, which bounds the distance between an estimate and the ground truth. We show that $\zeta$-correctness can be assessed by implementing two certificates: (i) a certificate of observable correctness, that asserts if the model output is consistent with the input data and prior information, (ii) a certificate of non-degeneracy, that asserts whether the input data is sufficient to compute a unique estimate. Our second contribution is to apply this theory and design a new learning-based certifiable pose estimator. We propose C-3PO, a semantic-keypoint-based pose estimation model, augmented with the two certificates, to solve the certifiable pose estimation problem. C-3PO also includes a keypoint corrector, implemented as a differentiable optimization layer, that can correct large detection errors (e.g. due to the sim-to-real gap). Our third contribution is a novel self-supervised training approach that uses our certificate of observable correctness to provide the supervisory signal to C-3PO during training. In it, the model trains only on the observably correct input-output pairs, in each training iteration. As training progresses, we see that the observably correct input-output pairs grow, eventually reaching near 100% in many cases. Our experiments show that (i) standard semantic-keypoint-based methods outperform more recent alternatives, (ii) C-3PO further improves performance and significantly outperforms all the baselines, and (iii) C-3PO's certificates are able to discern correct pose estimates.



### Depth-aware Glass Surface Detection with Cross-modal Context Mining
- **Arxiv ID**: http://arxiv.org/abs/2206.11250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.11250v1)
- **Published**: 2022-06-22 17:56:09+00:00
- **Updated**: 2022-06-22 17:56:09+00:00
- **Authors**: Jiaying Lin, Yuen Hei Yeung, Rynson W. H. Lau
- **Comment**: None
- **Journal**: None
- **Summary**: Glass surfaces are becoming increasingly ubiquitous as modern buildings tend to use a lot of glass panels. This however poses substantial challenges on the operations of autonomous systems such as robots, self-driving cars and drones, as the glass panels can become transparent obstacles to the navigation.Existing works attempt to exploit various cues, including glass boundary context or reflections, as a prior. However, they are all based on input RGB images.We observe that the transmission of 3D depth sensor light through glass surfaces often produces blank regions in the depth maps, which can offer additional insights to complement the RGB image features for glass surface detection. In this paper, we propose a novel framework for glass surface detection by incorporating RGB-D information, with two novel modules: (1) a cross-modal context mining (CCM) module to adaptively learn individual and mutual context features from RGB and depth information, and (2) a depth-missing aware attention (DAA) module to explicitly exploit spatial locations where missing depths occur to help detect the presence of glass surfaces. In addition, we propose a large-scale RGB-D glass surface detection dataset, called \textit{RGB-D GSD}, for RGB-D glass surface detection. Our dataset comprises 3,009 real-world RGB-D glass surface images with precise annotations. Extensive experimental results show that our proposed model outperforms state-of-the-art methods.



### Behavior Transformers: Cloning $k$ modes with one stone
- **Arxiv ID**: http://arxiv.org/abs/2206.11251v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.11251v2)
- **Published**: 2022-06-22 17:57:08+00:00
- **Updated**: 2022-10-11 22:49:49+00:00
- **Authors**: Nur Muhammad Mahi Shafiullah, Zichen Jeff Cui, Ariuntuya Altanzaya, Lerrel Pinto
- **Comment**: Code and data available at https://github.com/notmahi/bet
- **Journal**: None
- **Summary**: While behavior learning has made impressive progress in recent times, it lags behind computer vision and natural language processing due to its inability to leverage large, human-generated datasets. Human behaviors have wide variance, multiple modes, and human demonstrations typically do not come with reward labels. These properties limit the applicability of current methods in Offline RL and Behavioral Cloning to learn from large, pre-collected datasets. In this work, we present Behavior Transformer (BeT), a new technique to model unlabeled demonstration data with multiple modes. BeT retrofits standard transformer architectures with action discretization coupled with a multi-task action correction inspired by offset prediction in object detection. This allows us to leverage the multi-modal modeling ability of modern transformers to predict multi-modal continuous actions. We experimentally evaluate BeT on a variety of robotic manipulation and self-driving behavior datasets. We show that BeT significantly improves over prior state-of-the-art work on solving demonstrated tasks while capturing the major modes present in the pre-collected datasets. Finally, through an extensive ablation study, we analyze the importance of every crucial component in BeT. Videos of behavior generated by BeT are available at https://notmahi.github.io/bet



### Towards Robust Blind Face Restoration with Codebook Lookup Transformer
- **Arxiv ID**: http://arxiv.org/abs/2206.11253v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.11253v2)
- **Published**: 2022-06-22 17:58:01+00:00
- **Updated**: 2022-11-01 03:19:29+00:00
- **Authors**: Shangchen Zhou, Kelvin C. K. Chan, Chongyi Li, Chen Change Loy
- **Comment**: Accepted by NeurIPS 2022. Code: https://github.com/sczhou/CodeFormer
- **Journal**: None
- **Summary**: Blind face restoration is a highly ill-posed problem that often requires auxiliary guidance to 1) improve the mapping from degraded inputs to desired outputs, or 2) complement high-quality details lost in the inputs. In this paper, we demonstrate that a learned discrete codebook prior in a small proxy space largely reduces the uncertainty and ambiguity of restoration mapping by casting blind face restoration as a code prediction task, while providing rich visual atoms for generating high-quality faces. Under this paradigm, we propose a Transformer-based prediction network, named CodeFormer, to model the global composition and context of the low-quality faces for code prediction, enabling the discovery of natural faces that closely approximate the target faces even when the inputs are severely degraded. To enhance the adaptiveness for different degradation, we also propose a controllable feature transformation module that allows a flexible trade-off between fidelity and quality. Thanks to the expressive codebook prior and global modeling, CodeFormer outperforms the state of the arts in both quality and fidelity, showing superior robustness to degradation. Extensive experimental results on synthetic and real-world datasets verify the effectiveness of our method.



### Brain tumor detection using artificial convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2207.11248v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.11248v1)
- **Published**: 2022-06-22 18:45:37+00:00
- **Updated**: 2022-06-22 18:45:37+00:00
- **Authors**: Javier Melchor, Balam Sotelo, Jorge Vera, Horacio Corral
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: In this paper, a convolutional neural network (CNN) was used to classify NMR images of human brains with 4 different types of tumors: meningioma, glioma and pituitary gland tumors. During the training phase of this project, an accuracy of 100% was obtained, meanwhile, in the evaluation phase the precision was 96%.



### Mitigating Presentation Attack using DCGAN and Deep CNN
- **Arxiv ID**: http://arxiv.org/abs/2207.00161v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.00161v1)
- **Published**: 2022-06-22 19:40:08+00:00
- **Updated**: 2022-06-22 19:40:08+00:00
- **Authors**: Nyle Siddiqui, Rushit Dave
- **Comment**: None
- **Journal**: None
- **Summary**: Biometric based authentication is currently playing an essential role over conventional authentication system; however, the risk of presentation attacks subsequently rising. Our research aims at identifying the areas where presentation attack can be prevented even though adequate biometric image samples of users are limited. Our work focusses on generating photorealistic synthetic images from the real image sets by implementing Deep Convolution Generative Adversarial Net (DCGAN). We have implemented the temporal and spatial augmentation during the fake image generation. Our work detects the presentation attacks on facial and iris images using our deep CNN, inspired by VGGNet [1]. We applied the deep neural net techniques on three different biometric image datasets, namely MICHE I [2], VISOB [3], and UBIPr [4]. The datasets, used in this research, contain images that are captured both in controlled and uncontrolled environment along with different resolutions and sizes. We obtained the best test accuracy of 97% on UBI-Pr [4] Iris datasets. For MICHE-I [2] and VISOB [3] datasets, we achieved the test accuracies of 95% and 96% respectively.



### Doubly Reparameterized Importance Weighted Structure Learning for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2206.11352v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.11352v1)
- **Published**: 2022-06-22 20:00:25+00:00
- **Updated**: 2022-06-22 20:00:25+00:00
- **Authors**: Daqi Liu, Miroslaw Bober, Josef Kittler
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2205.07017
- **Journal**: None
- **Summary**: As a structured prediction task, scene graph generation, given an input image, aims to explicitly model objects and their relationships by constructing a visually-grounded scene graph. In the current literature, such task is universally solved via a message passing neural network based mean field variational Bayesian methodology. The classical loose evidence lower bound is generally chosen as the variational inference objective, which could induce oversimplified variational approximation and thus underestimate the underlying complex posterior. In this paper, we propose a novel doubly reparameterized importance weighted structure learning method, which employs a tighter importance weighted lower bound as the variational inference objective. It is computed from multiple samples drawn from a reparameterizable Gumbel-Softmax sampler and the resulting constrained variational inference task is solved by a generic entropic mirror descent algorithm. The resulting doubly reparameterized gradient estimator reduces the variance of the corresponding derivatives with a beneficial impact on learning. The proposed method achieves the state-of-the-art performance on various popular scene graph generation benchmarks.



### Monocular Spherical Depth Estimation with Explicitly Connected Weak Layout Cues
- **Arxiv ID**: http://arxiv.org/abs/2206.11358v1
- **DOI**: 10.1016/j.isprsjprs.2021.10.016
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.11358v1)
- **Published**: 2022-06-22 20:10:45+00:00
- **Updated**: 2022-06-22 20:10:45+00:00
- **Authors**: Nikolaos Zioulis, Federico Alvarez, Dimitrios Zarpalas, Petros Daras
- **Comment**: Project page at https://vcl3d.github.io/ExplicitLayoutDepth/
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, Volume 183,
  January 2022, Pages 269-285
- **Summary**: Spherical cameras capture scenes in a holistic manner and have been used for room layout estimation. Recently, with the availability of appropriate datasets, there has also been progress in depth estimation from a single omnidirectional image. While these two tasks are complementary, few works have been able to explore them in parallel to advance indoor geometric perception, and those that have done so either relied on synthetic data, or used small scale datasets, as few options are available that include both layout annotations and dense depth maps in real scenes. This is partly due to the necessity of manual annotations for room layouts. In this work, we move beyond this limitation and generate a 360 geometric vision (360V) dataset that includes multiple modalities, multi-view stereo data and automatically generated weak layout cues. We also explore an explicit coupling between the two tasks to integrate them into a singleshot trained model. We rely on depth-based layout reconstruction and layout-based depth attention, demonstrating increased performance across both tasks. By using single 360 cameras to scan rooms, the opportunity for facile and quick building-scale 3D scanning arises.



### Real-Time Online Skeleton Extraction and Gesture Recognition on Pepper
- **Arxiv ID**: http://arxiv.org/abs/2206.11376v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.11376v1)
- **Published**: 2022-06-22 20:55:46+00:00
- **Updated**: 2022-06-22 20:55:46+00:00
- **Authors**: Axel Lefrant, Jean-Marc Montanier
- **Comment**: None
- **Journal**: None
- **Summary**: We present a multi-stage pipeline for simple gesture recognition. The novelty of our approach is the association of different technologies, resulting in the first real-time system as of now to conjointly extract skeletons and recognise gesture on a Pepper robot. For this task, Pepper has been augmented with an embedded GPU for running deep CNNs and a fish-eye camera to capture whole scene interaction. We show in this article that real-case scenarios are challenging, and the state-of-the-art approaches hardly deal with unknown human gestures. We present here a way to handle such cases.



### The ArtBench Dataset: Benchmarking Generative Models with Artworks
- **Arxiv ID**: http://arxiv.org/abs/2206.11404v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.11404v1)
- **Published**: 2022-06-22 22:10:18+00:00
- **Updated**: 2022-06-22 22:10:18+00:00
- **Authors**: Peiyuan Liao, Xiuyu Li, Xihui Liu, Kurt Keutzer
- **Comment**: The first two authors contributed equally to this work. The code and
  data are available at https://github.com/liaopeiyuan/artbench
- **Journal**: None
- **Summary**: We introduce ArtBench-10, the first class-balanced, high-quality, cleanly annotated, and standardized dataset for benchmarking artwork generation. It comprises 60,000 images of artwork from 10 distinctive artistic styles, with 5,000 training images and 1,000 testing images per style. ArtBench-10 has several advantages over previous artwork datasets. Firstly, it is class-balanced while most previous artwork datasets suffer from the long tail class distributions. Secondly, the images are of high quality with clean annotations. Thirdly, ArtBench-10 is created with standardized data collection, annotation, filtering, and preprocessing procedures. We provide three versions of the dataset with different resolutions ($32\times32$, $256\times256$, and original image size), formatted in a way that is easy to be incorporated by popular machine learning frameworks. We also conduct extensive benchmarking experiments using representative image synthesis models with ArtBench-10 and present in-depth analysis. The dataset is available at https://github.com/liaopeiyuan/artbench under a Fair Use license.



