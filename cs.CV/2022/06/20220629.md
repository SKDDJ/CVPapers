# Arxiv Papers in cs.CV on 2022-06-29
### A New Adjacency Matrix Configuration in GCN-based Models for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.14344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14344v1)
- **Published**: 2022-06-29 01:08:37+00:00
- **Updated**: 2022-06-29 01:08:37+00:00
- **Authors**: Zheng Fang, Xiongwei Zhang, Tieyong Cao, Yunfei Zheng, Meng Sun
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: Human skeleton data has received increasing attention in action recognition due to its background robustness and high efficiency. In skeleton-based action recognition, graph convolutional network (GCN) has become the mainstream method. This paper analyzes the fundamental factor for GCN-based models -- the adjacency matrix. We notice that most GCN-based methods conduct their adjacency matrix based on the human natural skeleton structure. Based on our former work and analysis, we propose that the human natural skeleton structure adjacency matrix is not proper for skeleton-based action recognition. We propose a new adjacency matrix that abandons all rigid neighbor connections but lets the model adaptively learn the relationships of joints. We conduct extensive experiments and analysis with a validation model on two skeleton-based action recognition datasets (NTURGBD60 and FineGYM). Comprehensive experimental results and analysis reveals that 1) the most widely used human natural skeleton structure adjacency matrix is unsuitable in skeleton-based action recognition; 2) The proposed adjacency matrix is superior in model performance, noise robustness and transferability.



### Convolutional Neural Network Based Partial Face Detection
- **Arxiv ID**: http://arxiv.org/abs/2206.14350v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.14350v1)
- **Published**: 2022-06-29 01:26:40+00:00
- **Updated**: 2022-06-29 01:26:40+00:00
- **Authors**: Md. Towfiqul Islam, Tanzim Ahmed, A. B. M. Raihanur Rashid, Taminul Islam, Md. Sadekur Rahman, Md. Tarek Habib
- **Comment**: Accepted in 7th International Conference for Convergence in
  Technology (I2CT), 2022, 6 pages, 7 figures
- **Journal**: None
- **Summary**: Due to the massive explanation of artificial intelligence, machine learning technology is being used in various areas of our day-to-day life. In the world, there are a lot of scenarios where a simple crime can be prevented before it may even happen or find the person responsible for it. A face is one distinctive feature that we have and can differentiate easily among many other species. But not just different species, it also plays a significant role in determining someone from the same species as us, humans. Regarding this critical feature, a single problem occurs most often nowadays. When the camera is pointed, it cannot detect a person's face, and it becomes a poor image. On the other hand, where there was a robbery and a security camera installed, the robber's identity is almost indistinguishable due to the low-quality camera. But just making an excellent algorithm to work and detecting a face reduces the cost of hardware, and it doesn't cost that much to focus on that area. Facial recognition, widget control, and such can be done by detecting the face correctly. This study aims to create and enhance a machine learning model that correctly recognizes faces. Total 627 Data have been collected from different Bangladeshi people's faces on four angels. In this work, CNN, Harr Cascade, Cascaded CNN, Deep CNN & MTCNN are these five machine learning approaches implemented to get the best accuracy of our dataset. After creating and running the model, Multi-Task Convolutional Neural Network (MTCNN) achieved 96.2% best model accuracy with training data rather than other machine learning models.



### EBMs vs. CL: Exploring Self-Supervised Visual Pretraining for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2206.14355v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.14355v1)
- **Published**: 2022-06-29 01:44:23+00:00
- **Updated**: 2022-06-29 01:44:23+00:00
- **Authors**: Violetta Shevchenko, Ehsan Abbasnejad, Anthony Dick, Anton van den Hengel, Damien Teney
- **Comment**: None
- **Journal**: None
- **Summary**: The availability of clean and diverse labeled data is a major roadblock for training models on complex tasks such as visual question answering (VQA). The extensive work on large vision-and-language models has shown that self-supervised learning is effective for pretraining multimodal interactions. In this technical report, we focus on visual representations. We review and evaluate self-supervised methods to leverage unlabeled images and pretrain a model, which we then fine-tune on a custom VQA task that allows controlled evaluation and diagnosis. We compare energy-based models (EBMs) with contrastive learning (CL). While EBMs are growing in popularity, they lack an evaluation on downstream tasks. We find that both EBMs and CL can learn representations from unlabeled images that enable training a VQA model on very little annotated data. In a simple setting similar to CLEVR, we find that CL representations also improve systematic generalization, and even match the performance of representations from a larger, supervised, ImageNet-pretrained model. However, we find EBMs to be difficult to train because of instabilities and high variability in their results. Although EBMs prove useful for OOD detection, other results on supervised energy-based training and uncertainty calibration are largely negative. Overall, CL currently seems a preferable option over EBMs.



### Formalizing and Evaluating Requirements of Perception Systems for Automated Vehicles using Spatio-Temporal Perception Logic
- **Arxiv ID**: http://arxiv.org/abs/2206.14372v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.FL, 14D15, 03B44, 68T40, F.4.3; I.2.9; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2206.14372v1)
- **Published**: 2022-06-29 02:36:53+00:00
- **Updated**: 2022-06-29 02:36:53+00:00
- **Authors**: Mohammad Hekmatnejad, Bardh Hoxha, Jyotirmoy V. Deshmukh, Yezhou Yang, Georgios Fainekos
- **Comment**: 27 pages, 12 figures, 4 tables, 5 algorithms, 3 appendixes
- **Journal**: None
- **Summary**: Automated vehicles (AV) heavily depend on robust perception systems. Current methods for evaluating vision systems focus mainly on frame-by-frame performance. Such evaluation methods appear to be inadequate in assessing the performance of a perception subsystem when used within an AV. In this paper, we present a logic -- referred to as Spatio-Temporal Perception Logic (STPL) -- which utilizes both spatial and temporal modalities. STPL enables reasoning over perception data using spatial and temporal relations. One major advantage of STPL is that it facilitates basic sanity checks on the real-time performance of the perception system, even without ground-truth data in some cases. We identify a fragment of STPL which is efficiently monitorable offline in polynomial time. Finally, we present a range of specifications for AV perception systems to highlight the types of requirements that can be expressed and analyzed through offline monitoring with STPL.



### Exploiting Semantic Role Contextualized Video Features for Multi-Instance Text-Video Retrieval EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022
- **Arxiv ID**: http://arxiv.org/abs/2206.14381v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.14381v1)
- **Published**: 2022-06-29 03:24:43+00:00
- **Updated**: 2022-06-29 03:24:43+00:00
- **Authors**: Burak Satar, Hongyuan Zhu, Hanwang Zhang, Joo Hwee Lim
- **Comment**: Ranked joint 3rd place in the Multi-Instance Retrieval Challenge at
  EPIC@CVPR2022
- **Journal**: None
- **Summary**: In this report, we present our approach for EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022. We first parse sentences into semantic roles corresponding to verbs and nouns; then utilize self-attentions to exploit semantic role contextualized video features along with textual features via triplet losses in multiple embedding spaces. Our method overpasses the strong baseline in normalized Discounted Cumulative Gain (nDCG), which is more valuable for semantic similarity. Our submission is ranked 3rd for nDCG and ranked 4th for mAP.



### BATFormer: Towards Boundary-Aware Lightweight Transformer for Efficient Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.14409v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.14409v3)
- **Published**: 2022-06-29 05:37:16+00:00
- **Updated**: 2023-04-19 02:43:34+00:00
- **Authors**: Xian Lin, Li Yu, Kwang-Ting Cheng, Zengqiang Yan
- **Comment**: Accepted by IEEE Journal of Biomedical and Health Informatics The
  source code is publicly available at https://github.com/xianlin7/BATFormer
- **Journal**: None
- **Summary**: Objective: Transformers, born to remedy the inadequate receptive fields of CNNs, have drawn explosive attention recently. However, the daunting computational complexity of global representation learning, together with rigid window partitioning, hinders their deployment in medical image segmentation. This work aims to address the above two issues in transformers for better medical image segmentation. Methods: We propose a boundary-aware lightweight transformer (BATFormer) that can build cross-scale global interaction with lower computational complexity and generate windows flexibly under the guidance of entropy. Specifically, to fully explore the benefits of transformers in long-range dependency establishment, a cross-scale global transformer (CGT) module is introduced to jointly utilize multiple small-scale feature maps for richer global features with lower computational complexity. Given the importance of shape modeling in medical image segmentation, a boundary-aware local transformer (BLT) module is constructed. Different from rigid window partitioning in vanilla transformers which would produce boundary distortion, BLT adopts an adaptive window partitioning scheme under the guidance of entropy for both computational complexity reduction and shape preservation. Results: BATFormer achieves the best performance in Dice of 92.84%, 91.97%, 90.26%, and 96.30% for the average, right ventricle, myocardium, and left ventricle respectively on the ACDC dataset and the best performance in Dice, IoU, and ACC of 90.76%, 84.64%, and 96.76% respectively on the ISIC 2018 dataset. More importantly, BATFormer requires the least amount of model parameters and the lowest computational complexity compared to the state-of-the-art approaches. Conclusion and Significance: Our results demonstrate the necessity of developing customized transformers for efficient and better medical image segmentation.



### The Lighter The Better: Rethinking Transformers in Medical Image Segmentation Through Adaptive Pruning
- **Arxiv ID**: http://arxiv.org/abs/2206.14413v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.14413v2)
- **Published**: 2022-06-29 05:49:36+00:00
- **Updated**: 2022-07-12 12:54:24+00:00
- **Authors**: Xian Lin, Li Yu, Kwang-Ting Cheng, Zengqiang Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Vision transformers have recently set off a new wave in the field of medical image analysis due to their remarkable performance on various computer vision tasks. However, recent hybrid-/transformer-based approaches mainly focus on the benefits of transformers in capturing long-range dependency while ignoring the issues of their daunting computational complexity, high training costs, and redundant dependency. In this paper, we propose to employ adaptive pruning to transformers for medical image segmentation and propose a lightweight and effective hybrid network APFormer. To our best knowledge, this is the first work on transformer pruning for medical image analysis tasks. The key features of APFormer mainly are self-supervised self-attention (SSA) to improve the convergence of dependency establishment, Gaussian-prior relative position embedding (GRPE) to foster the learning of position information, and adaptive pruning to eliminate redundant computations and perception information. Specifically, SSA and GRPE consider the well-converged dependency distribution and the Gaussian heatmap distribution separately as the prior knowledge of self-attention and position embedding to ease the training of transformers and lay a solid foundation for the following pruning operation. Then, adaptive transformer pruning, both query-wise and dependency-wise, is performed by adjusting the gate control parameters for both complexity reduction and performance improvement. Extensive experiments on two widely-used datasets demonstrate the prominent segmentation performance of APFormer against the state-of-the-art methods with much fewer parameters and lower GFLOPs. More importantly, we prove, through ablation studies, that adaptive pruning can work as a plug-n-play module for performance improvement on other hybrid-/transformer-based methods. Code is available at https://github.com/xianlin7/APFormer.



### MaNi: Maximizing Mutual Information for Nuclei Cross-Domain Unsupervised Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.14437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14437v1)
- **Published**: 2022-06-29 07:24:02+00:00
- **Updated**: 2022-06-29 07:24:02+00:00
- **Authors**: Yash Sharma, Sana Syed, Donald E. Brown
- **Comment**: Accepted at MICCAI 2022
- **Journal**: None
- **Summary**: In this work, we propose a mutual information (MI) based unsupervised domain adaptation (UDA) method for the cross-domain nuclei segmentation. Nuclei vary substantially in structure and appearances across different cancer types, leading to a drop in performance of deep learning models when trained on one cancer type and tested on another. This domain shift becomes even more critical as accurate segmentation and quantification of nuclei is an essential histopathology task for the diagnosis/ prognosis of patients and annotating nuclei at the pixel level for new cancer types demands extensive effort by medical experts. To address this problem, we maximize the MI between labeled source cancer type data and unlabeled target cancer type data for transferring nuclei segmentation knowledge across domains. We use the Jensen-Shanon divergence bound, requiring only one negative pair per positive pair for MI maximization. We evaluate our set-up for multiple modeling frameworks and on different datasets comprising of over 20 cancer-type domain shifts and demonstrate competitive performance. All the recently proposed approaches consist of multiple components for improving the domain adaptation, whereas our proposed module is light and can be easily incorporated into other methods (Implementation: https://github.com/YashSharma/MaNi ).



### SRCN3D: Sparse R-CNN 3D for Compact Convolutional Multi-View 3D Object Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2206.14451v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14451v3)
- **Published**: 2022-06-29 07:58:39+00:00
- **Updated**: 2023-07-02 01:11:12+00:00
- **Authors**: Yining Shi, Jingyan Shen, Yifan Sun, Yunlong Wang, Jiaxin Li, Shiqi Sun, Kun Jiang, Diange Yang
- **Comment**: Accepted to Vision-centric Autonomous Driving(VCAD) Workshop at
  CVPR2023, For more details refer to http://vcad.site/#/papers
- **Journal**: None
- **Summary**: Detection and tracking of moving objects is an essential component in environmental perception for autonomous driving. In the flourishing field of multi-view 3D camera-based detectors, different transformer-based pipelines are designed to learn queries in 3D space from 2D feature maps of perspective views, but the dominant dense BEV query mechanism is computationally inefficient. This paper proposes Sparse R-CNN 3D (SRCN3D), a novel two-stage fully-sparse detector that incorporates sparse queries, sparse attention with box-wise sampling, and sparse prediction. SRCN3D adopts a cascade structure with the twin-track update of both a fixed number of query boxes and latent query features. Our novel sparse feature sampling module only utilizes local 2D region of interest (RoI) features calculated by the projection of 3D query boxes for further box refinement, leading to a fully-convolutional and deployment-friendly pipeline. For multi-object tracking, motion features, query features and RoI features are comprehensively utilized in multi-hypotheses data association. Extensive experiments on nuScenes dataset demonstrate that SRCN3D achieves competitive performance in both 3D object detection and multi-object tracking tasks, while also exhibiting superior efficiency compared to transformer-based methods. Code and models are available at https://github.com/synsin0/SRCN3D.



### Single-domain Generalization in Medical Image Segmentation via Test-time Adaptation from Shape Dictionary
- **Arxiv ID**: http://arxiv.org/abs/2206.14467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14467v1)
- **Published**: 2022-06-29 08:46:27+00:00
- **Updated**: 2022-06-29 08:46:27+00:00
- **Authors**: Quande Liu, Cheng Chen, Qi Dou, Pheng-Ann Heng
- **Comment**: Accepted to AAAI 2022
- **Journal**: None
- **Summary**: Domain generalization typically requires data from multiple source domains for model learning. However, such strong assumption may not always hold in practice, especially in medical field where the data sharing is highly concerned and sometimes prohibitive due to privacy issue. This paper studies the important yet challenging single domain generalization problem, in which a model is learned under the worst-case scenario with only one source domain to directly generalize to different unseen target domains. We present a novel approach to address this problem in medical image segmentation, which extracts and integrates the semantic shape prior information of segmentation that are invariant across domains and can be well-captured even from single domain data to facilitate segmentation under distribution shifts. Besides, a test-time adaptation strategy with dual-consistency regularization is further devised to promote dynamic incorporation of these shape priors under each unseen domain to improve model generalizability. Extensive experiments on two medical image segmentation tasks demonstrate the consistent improvements of our method across various unseen domains, as well as its superiority over state-of-the-art approaches in addressing domain generalization under the worst-case scenario.



### Feature-selected Graph Spatial Attention Network for Addictive Brain-Networks Identification
- **Arxiv ID**: http://arxiv.org/abs/2207.00583v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2207.00583v2)
- **Published**: 2022-06-29 08:50:54+00:00
- **Updated**: 2022-07-05 08:12:47+00:00
- **Authors**: Changwei Gong, Changhong Jing, Junren Pan, Shuqiang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Functional alterations in the relevant neural circuits occur from drug addiction over a certain period. And these significant alterations are also revealed by analyzing fMRI. However, because of fMRI's high dimensionality and poor signal-to-noise ratio, it is challenging to encode efficient and robust brain regional embeddings for both graph-level identification and region-level biomarkers detection tasks between nicotine addiction (NA) and healthy control (HC) groups. In this work, we represent the fMRI of the rat brain as a graph with biological attributes and propose a novel feature-selected graph spatial attention network(FGSAN) to extract the biomarkers of addiction and identify from these brain networks. Specially, a graph spatial attention encoder is employed to capture the features of spatiotemporal brain networks with spatial information. The method simultaneously adopts a Bayesian feature selection strategy to optimize the model and improve classification task by constraining features. Experiments on an addiction-related neural imaging dataset show that the proposed model can obtain superior performance and detect interpretable biomarkers associated with addiction-relevant neural circuits.



### Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.14475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14475v1)
- **Published**: 2022-06-29 09:02:35+00:00
- **Updated**: 2022-06-29 09:02:35+00:00
- **Authors**: Xiangyu Li, Xu Yang, Kun Wei, Cheng Deng, Muli Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositions formed from seen state and object during training. Since the same state may be various in the visual appearance while entangled with different objects, CZSL is still a challenging task. Some methods recognize state and object with two trained classifiers, ignoring the impact of the interaction between object and state; the other methods try to learn the joint representation of the state-object compositions, leading to the domain gap between seen and unseen composition sets. In this paper, we propose a novel Siamese Contrastive Embedding Network (SCEN) (Code: https://github.com/XDUxyLi/SCEN-master) for unseen composition recognition. Considering the entanglement between state and object, we embed the visual feature into a Siamese Contrastive Space to capture prototypes of them separately, alleviating the interaction between state and object. In addition, we design a State Transition Module (STM) to increase the diversity of training compositions, improving the robustness of the recognition model. Extensive experiments indicate that our method significantly outperforms the state-of-the-art approaches on three challenging benchmark datasets, including the recent proposed C-QGA dataset.



### Beyond neural scaling laws: beating power law scaling via data pruning
- **Arxiv ID**: http://arxiv.org/abs/2206.14486v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2206.14486v6)
- **Published**: 2022-06-29 09:20:47+00:00
- **Updated**: 2023-04-21 20:14:07+00:00
- **Authors**: Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, Ari S. Morcos
- **Comment**: Outstanding Paper Award @ NeurIPS 2022. Added github link to metric
  scores
- **Journal**: None
- **Summary**: Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on ResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.



### RegMixup: Mixup as a Regularizer Can Surprisingly Improve Accuracy and Out Distribution Robustness
- **Arxiv ID**: http://arxiv.org/abs/2206.14502v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.4.0; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2206.14502v2)
- **Published**: 2022-06-29 09:44:33+00:00
- **Updated**: 2023-02-06 23:56:02+00:00
- **Authors**: Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H. S. Torr, Puneet K. Dokania
- **Comment**: 22 pages, 18 figures
- **Journal**: None
- **Summary**: We show that the effectiveness of the well celebrated Mixup [Zhang et al., 2018] can be further improved if instead of using it as the sole learning objective, it is utilized as an additional regularizer to the standard cross-entropy loss. This simple change not only provides much improved accuracy but also significantly improves the quality of the predictive uncertainty estimation of Mixup in most cases under various forms of covariate shifts and out-of-distribution detection experiments. In fact, we observe that Mixup yields much degraded performance on detecting out-of-distribution samples possibly, as we show empirically, because of its tendency to learn models that exhibit high-entropy throughout; making it difficult to differentiate in-distribution samples from out-distribution ones. To show the efficacy of our approach (RegMixup), we provide thorough analyses and experiments on vision datasets (ImageNet & CIFAR-10/100) and compare it with a suite of recent approaches for reliable uncertainty estimation.



### Procrustes Analysis with Deformations: A Closed-Form Solution by Eigenvalue Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2206.14528v1
- **DOI**: 10.1007/s11263-021-01571-8
- **Categories**: **cs.RO**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2206.14528v1)
- **Published**: 2022-06-29 10:52:44+00:00
- **Updated**: 2022-06-29 10:52:44+00:00
- **Authors**: Fang Bai, Adrien Bartoli
- **Comment**: Published on International journal of computer vision (IJCV) 2022
- **Journal**: International Journal of Computer Vision 130, no. 2 (2022):
  567-593
- **Summary**: Generalized Procrustes Analysis (GPA) is the problem of bringing multiple shapes into a common reference by estimating transformations. GPA has been extensively studied for the Euclidean and affine transformations. We introduce GPA with deformable transformations, which forms a much wider and difficult problem. We specifically study a class of transformations called the Linear Basis Warps (LBWs), which contains the affine transformation and most of the usual deformation models, such as the Thin-Plate Spline (TPS). GPA with deformations is a nonconvex underconstrained problem. We resolve the fundamental ambiguities of deformable GPA using two shape constraints requiring the eigenvalues of the shape covariance. These eigenvalues can be computed independently as a prior or posterior. We give a closed-form and optimal solution to deformable GPA based on an eigenvalue decomposition. This solution handles regularization, favoring smooth deformation fields. It requires the transformation model to satisfy a fundamental property of free-translations, which asserts that the model can implement any translation. We show that this property fortunately holds true for most common transformation models, including the affine and TPS models. For the other models, we give another closed-form solution to GPA, which agrees exactly with the first solution for models with free-translation. We give pseudo-code for computing our solution, leading to the proposed DefGPA method, which is fast, globally optimal and widely applicable. We validate our method and compare it to previous work on six diverse 2D and 3D datasets, with special care taken to choose the hyperparameters from cross-validation.



### vMFNet: Compositionality Meets Domain-generalised Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.14538v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14538v1)
- **Published**: 2022-06-29 11:31:23+00:00
- **Updated**: 2022-06-29 11:31:23+00:00
- **Authors**: Xiao Liu, Spyridon Thermos, Pedro Sanchez, Alison Q. O'Neil, Sotirios A. Tsaftaris
- **Comment**: Accepted by MICCAI 2022
- **Journal**: None
- **Summary**: Training medical image segmentation models usually requires a large amount of labeled data. By contrast, humans can quickly learn to accurately recognise anatomy of interest from medical (e.g. MRI and CT) images with some limited guidance. Such recognition ability can easily generalise to new images from different clinical centres. This rapid and generalisable learning ability is mostly due to the compositional structure of image patterns in the human brain, which is less incorporated in medical image segmentation. In this paper, we model the compositional components (i.e. patterns) of human anatomy as learnable von-Mises-Fisher (vMF) kernels, which are robust to images collected from different domains (e.g. clinical centres). The image features can be decomposed to (or composed by) the components with the composing operations, i.e. the vMF likelihoods. The vMF likelihoods tell how likely each anatomical part is at each position of the image. Hence, the segmentation mask can be predicted based on the vMF likelihoods. Moreover, with a reconstruction module, unlabeled data can also be used to learn the vMF kernels and likelihoods by recombining them to reconstruct the input image. Extensive experiments show that the proposed vMFNet achieves improved generalisation performance on two benchmarks, especially when annotations are limited. Code is publicly available at: https://github.com/vios-s/vMFNet.



### Why patient data cannot be easily forgotten?
- **Arxiv ID**: http://arxiv.org/abs/2206.14541v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.14541v1)
- **Published**: 2022-06-29 11:36:49+00:00
- **Updated**: 2022-06-29 11:36:49+00:00
- **Authors**: Ruolin Su, Xiao Liu, Sotirios A. Tsaftaris
- **Comment**: Ruolin Su and Xiao Liu contributed equally. Accepted by MICCAI 2022
- **Journal**: None
- **Summary**: Rights provisioned within data protection regulations, permit patients to request that knowledge about their information be eliminated by data holders. With the advent of AI learned on data, one can imagine that such rights can extent to requests for forgetting knowledge of patient's data within AI models. However, forgetting patients' imaging data from AI models, is still an under-explored problem. In this paper, we study the influence of patient data on model performance and formulate two hypotheses for a patient's data: either they are common and similar to other patients or form edge cases, i.e. unique and rare cases. We show that it is not possible to easily forget patient data. We propose a targeted forgetting approach to perform patient-wise forgetting. Extensive experiments on the benchmark Automated Cardiac Diagnosis Challenge dataset showcase the improved performance of the proposed targeted forgetting approach as opposed to a state-of-the-art method.



### Uncertainty-aware Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.14554v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.14554v3)
- **Published**: 2022-06-29 12:07:21+00:00
- **Updated**: 2022-12-24 17:07:26+00:00
- **Authors**: Kshitij Sirohi, Sajad Marvi, Daniel Büscher, Wolfram Burgard
- **Comment**: None
- **Journal**: None
- **Summary**: Reliable scene understanding is indispensable for modern autonomous systems. Current learning-based methods typically try to maximize their performance based on segmentation metrics that only consider the quality of the segmentation. However, for the safe operation of a system in the real world it is crucial to consider the uncertainty in the prediction as well. In this work, we introduce the novel task of uncertainty-aware panoptic segmentation, which aims to predict per-pixel semantic and instance segmentations, together with per-pixel uncertainty estimates. We define two novel metrics to facilitate its quantitative analysis, the uncertainty-aware Panoptic Quality (uPQ) and the panoptic Expected Calibration Error (pECE). We further propose the novel top-down Evidential Panoptic Segmentation Network (EvPSNet) to solve this task. Our architecture employs a simple yet effective panoptic fusion module that leverages the predicted uncertainties. Furthermore, we provide several strong baselines combining state-of-the-art panoptic segmentation networks with sampling-free uncertainty estimation techniques. Extensive evaluations show that our EvPSNet achieves the new state-of-the-art for the standard Panoptic Quality (PQ), as well as for our uncertainty-aware panoptic metrics. We make the code available at: \url{https://github.com/kshitij3112/EvPSNet}



### Technical Report for CVPR 2022 LOVEU AQTC Challenge
- **Arxiv ID**: http://arxiv.org/abs/2206.14555v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.14555v1)
- **Published**: 2022-06-29 12:07:43+00:00
- **Updated**: 2022-06-29 12:07:43+00:00
- **Authors**: Hyeonyu Kim, Jongeun Kim, Jeonghun Kang, Sanguk Park, Dongchan Park, Taehwan Kim
- **Comment**: 4 pages, 3 figures, technical report for track3 of CVPR 2022 LOVEU
  challenge
- **Journal**: None
- **Summary**: This technical report presents the 2nd winning model for AQTC, a task newly introduced in CVPR 2022 LOng-form VidEo Understanding (LOVEU) challenges. This challenge faces difficulties with multi-step answers, multi-modal, and diverse and changing button representations in video. We address this problem by proposing a new context ground module attention mechanism for more effective feature mapping. In addition, we also perform the analysis over the number of buttons and ablation study of different step networks and video features. As a result, we achieved the overall 2nd place in LOVEU competition track 3, specifically the 1st place in two out of four evaluation metrics. Our code is available at https://github.com/jaykim9870/ CVPR-22_LOVEU_unipyler.



### Turbo: Opportunistic Enhancement for Edge Video Analytics
- **Arxiv ID**: http://arxiv.org/abs/2207.00172v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2207.00172v1)
- **Published**: 2022-06-29 12:13:30+00:00
- **Updated**: 2022-06-29 12:13:30+00:00
- **Authors**: Yan Lu, Shiqi Jiang, Ting Cao, Yuanchao Shu
- **Comment**: None
- **Journal**: None
- **Summary**: Edge computing is being widely used for video analytics. To alleviate the inherent tension between accuracy and cost, various video analytics pipelines have been proposed to optimize the usage of GPU on edge nodes. Nonetheless, we find that GPU compute resources provisioned for edge nodes are commonly under-utilized due to video content variations, subsampling and filtering at different places of a pipeline. As opposed to model and pipeline optimization, in this work, we study the problem of opportunistic data enhancement using the non-deterministic and fragmented idle GPU resources. In specific, we propose a task-specific discrimination and enhancement module and a model-aware adversarial training mechanism, providing a way to identify and transform low-quality images that are specific to a video pipeline in an accurate and efficient manner. A multi-exit model structure and a resource-aware scheduler is further developed to make online enhancement decisions and fine-grained inference execution under latency and GPU resource constraints. Experiments across multiple video analytics pipelines and datasets reveal that by judiciously allocating a small amount of idle resources on frames that tend to yield greater marginal benefits from enhancement, our system boosts DNN object detection accuracy by $7.3-11.3\%$ without incurring any latency costs.



### BoT-SORT: Robust Associations Multi-Pedestrian Tracking
- **Arxiv ID**: http://arxiv.org/abs/2206.14651v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14651v2)
- **Published**: 2022-06-29 13:45:03+00:00
- **Updated**: 2022-07-07 15:36:49+00:00
- **Authors**: Nir Aharon, Roy Orfaig, Ben-Zion Bobrovsky
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of multi-object tracking (MOT) is detecting and tracking all the objects in a scene, while keeping a unique identifier for each object. In this paper, we present a new robust state-of-the-art tracker, which can combine the advantages of motion and appearance information, along with camera-motion compensation, and a more accurate Kalman filter state vector. Our new trackers BoT-SORT, and BoT-SORT-ReID rank first in the datasets of MOTChallenge [29, 11] on both MOT17 and MOT20 test sets, in terms of all the main MOT metrics: MOTA, IDF1, and HOTA. For MOT17: 80.5 MOTA, 80.2 IDF1, and 65.0 HOTA are achieved. The source code and the pre-trained models are available at https://github.com/NirAharon/BOT-SORT



### Cut Inner Layers: A Structured Pruning Strategy for Efficient U-Net GANs
- **Arxiv ID**: http://arxiv.org/abs/2206.14658v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.14658v1)
- **Published**: 2022-06-29 13:55:36+00:00
- **Updated**: 2022-06-29 13:55:36+00:00
- **Authors**: Bo-Kyeong Kim, Shinkook Choi, Hancheol Park
- **Comment**: ICML Workshop on Hardware Aware Efficient Training, 2022
- **Journal**: None
- **Summary**: Pruning effectively compresses overparameterized models. Despite the success of pruning methods for discriminative models, applying them for generative models has been relatively rarely approached. This study conducts structured pruning on U-Net generators of conditional GANs. A per-layer sensitivity analysis confirms that many unnecessary filters exist in the innermost layers near the bottleneck and can be substantially pruned. Based on this observation, we prune these filters from multiple inner layers or suggest alternative architectures by completely eliminating the layers. We evaluate our approach with Pix2Pix for image-to-image translation and Wav2Lip for speech-driven talking face generation. Our method outperforms global pruning baselines, demonstrating the importance of properly considering where to prune for U-Net generators.



### Ensemble CNN models for Covid-19 Recognition and Severity Perdition From 3D CT-scan
- **Arxiv ID**: http://arxiv.org/abs/2206.15431v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.15431v1)
- **Published**: 2022-06-29 14:20:23+00:00
- **Updated**: 2022-06-29 14:20:23+00:00
- **Authors**: Fares Bougourzi, Cosimo Distante, Fadi Dornaika, Abdelmalik Taleb-Ahmed
- **Comment**: None
- **Journal**: None
- **Summary**: Since the appearance of Covid-19 in late 2019, Covid-19 has become an active research topic for the artificial intelligence (AI) community. One of the most interesting AI topics is Covid-19 analysis of medical imaging. CT-scan imaging is the most informative tool about this disease. This work is part of the 2nd COV19D competition, where two challenges are set: Covid-19 Detection and Covid-19 Severity Detection from the CT-scans. For Covid-19 detection from CT-scans, we proposed an ensemble of 2D Convolution blocks with Densenet-161 models. Here, each 2D convolutional block with Densenet-161 architecture is trained separately and in testing phase, the ensemble model is based on the average of their probabilities. On the other hand, we proposed an ensemble of Convolutional Layers with Inception models for Covid-19 severity detection. In addition to the Convolutional Layers, three Inception variants were used, namely Inception-v3, Inception-v4 and Inception-Resnet. Our proposed approaches outperformed the baseline approach in the validation data of the 2nd COV19D competition by 11% and 16% for Covid-19 detection and Covid-19 severity detection, respectively.



### BiometryNet: Landmark-based Fetal Biometry Estimation from Standard Ultrasound Planes
- **Arxiv ID**: http://arxiv.org/abs/2206.14678v1
- **DOI**: 10.1007/978-3-031-16440-8_27
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.14678v1)
- **Published**: 2022-06-29 14:32:32+00:00
- **Updated**: 2022-06-29 14:32:32+00:00
- **Authors**: Netanell Avisdris, Leo Joskowicz, Brian Dromey, Anna L. David, Donald M. Peebles, Danail Stoyanov, Dafna Ben Bashat, Sophia Bano
- **Comment**: 13 pages, 6 figures, Accepted to MICCAI 2022
- **Journal**: None
- **Summary**: Fetal growth assessment from ultrasound is based on a few biometric measurements that are performed manually and assessed relative to the expected gestational age. Reliable biometry estimation depends on the precise detection of landmarks in standard ultrasound planes. Manual annotation can be time-consuming and operator dependent task, and may results in high measurements variability. Existing methods for automatic fetal biometry rely on initial automatic fetal structure segmentation followed by geometric landmark detection. However, segmentation annotations are time-consuming and may be inaccurate, and landmark detection requires developing measurement-specific geometric methods. This paper describes BiometryNet, an end-to-end landmark regression framework for fetal biometry estimation that overcomes these limitations. It includes a novel Dynamic Orientation Determination (DOD) method for enforcing measurement-specific orientation consistency during network training. DOD reduces variabilities in network training, increases landmark localization accuracy, thus yields accurate and robust biometric measurements. To validate our method, we assembled a dataset of 3,398 ultrasound images from 1,829 subjects acquired in three clinical sites with seven different ultrasound devices. Comparison and cross-validation of three different biometric measurements on two independent datasets shows that BiometryNet is robust and yields accurate measurements whose errors are lower than the clinically permissible errors, outperforming other existing automated biometry estimation methods. Code is available at https://github.com/netanellavisdris/fetalbiometry.



### Multi-scale Physical Representations for Approximating PDE Solutions with Graph Neural Operators
- **Arxiv ID**: http://arxiv.org/abs/2206.14687v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.14687v1)
- **Published**: 2022-06-29 14:42:03+00:00
- **Updated**: 2022-06-29 14:42:03+00:00
- **Authors**: Léon Migus, Yuan Yin, Jocelyn Ahmed Mazari, Patrick Gallinari
- **Comment**: ICLR 2022 Workshop on Geometrical and Topological Representation
  Learning
- **Journal**: ICLR 2022 Workshop on Geometrical and Topological Representation
  Learning
- **Summary**: Representing physical signals at different scales is among the most challenging problems in engineering. Several multi-scale modeling tools have been developed to describe physical systems governed by \emph{Partial Differential Equations} (PDEs). These tools are at the crossroad of principled physical models and numerical schema. Recently, data-driven models have been introduced to speed-up the approximation of PDE solutions compared to numerical solvers. Among these recent data-driven methods, neural integral operators are a class that learn a mapping between function spaces. These functions are discretized on graphs (meshes) which are appropriate for modeling interactions in physical phenomena. In this work, we study three multi-resolution schema with integral kernel operators that can be approximated with \emph{Message Passing Graph Neural Networks} (MPGNNs). To validate our study, we make extensive MPGNNs experiments with well-chosen metrics considering steady and unsteady PDEs.



### Interventional Contrastive Learning with Meta Semantic Regularizer
- **Arxiv ID**: http://arxiv.org/abs/2206.14702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14702v1)
- **Published**: 2022-06-29 15:02:38+00:00
- **Updated**: 2022-06-29 15:02:38+00:00
- **Authors**: Wenwen Qiang, Jiangmeng Li, Changwen Zheng, Bing Su, Hui Xiong
- **Comment**: Accepted by ICML 2022
- **Journal**: None
- **Summary**: Contrastive learning (CL)-based self-supervised learning models learn visual representations in a pairwise manner. Although the prevailing CL model has achieved great progress, in this paper, we uncover an ever-overlooked phenomenon: When the CL model is trained with full images, the performance tested in full images is better than that in foreground areas; when the CL model is trained with foreground areas, the performance tested in full images is worse than that in foreground areas. This observation reveals that backgrounds in images may interfere with the model learning semantic information and their influence has not been fully eliminated. To tackle this issue, we build a Structural Causal Model (SCM) to model the background as a confounder. We propose a backdoor adjustment-based regularization method, namely Interventional Contrastive Learning with Meta Semantic Regularizer (ICL-MSR), to perform causal intervention towards the proposed SCM. ICL-MSR can be incorporated into any existing CL methods to alleviate background distractions from representation learning. Theoretically, we prove that ICL-MSR achieves a tighter error bound. Empirically, our experiments on multiple benchmark datasets demonstrate that ICL-MSR is able to improve the performances of different state-of-the-art CL methods.



### An extensible Benchmarking Graph-Mesh dataset for studying Steady-State Incompressible Navier-Stokes Equations
- **Arxiv ID**: http://arxiv.org/abs/2206.14709v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2206.14709v1)
- **Published**: 2022-06-29 15:18:30+00:00
- **Updated**: 2022-06-29 15:18:30+00:00
- **Authors**: Florent Bonnet, Jocelyn Ahmed Mazari, Thibaut Munzer, Pierre Yser, Patrick Gallinari
- **Comment**: ICLR 2022 Workshop on Geometrical and Topological Representation
  Learning
- **Journal**: ICLR 2022 Workshop on Geometrical and Topological Representation
  Learning
- **Summary**: Recent progress in \emph{Geometric Deep Learning} (GDL) has shown its potential to provide powerful data-driven models. This gives momentum to explore new methods for learning physical systems governed by \emph{Partial Differential Equations} (PDEs) from Graph-Mesh data. However, despite the efforts and recent achievements, several research directions remain unexplored and progress is still far from satisfying the physical requirements of real-world phenomena. One of the major impediments is the absence of benchmarking datasets and common physics evaluation protocols. In this paper, we propose a 2-D graph-mesh dataset to study the airflow over airfoils at high Reynolds regime (from $10^6$ and beyond). We also introduce metrics on the stress forces over the airfoil in order to evaluate GDL models on important physical quantities. Moreover, we provide extensive GDL baselines.



### CONVIQT: Contrastive Video Quality Estimator
- **Arxiv ID**: http://arxiv.org/abs/2206.14713v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2206.14713v1)
- **Published**: 2022-06-29 15:22:01+00:00
- **Updated**: 2022-06-29 15:22:01+00:00
- **Authors**: Pavan C. Madhusudana, Neil Birkbeck, Yilin Wang, Balu Adsumilli, Alan C. Bovik
- **Comment**: None
- **Journal**: None
- **Summary**: Perceptual video quality assessment (VQA) is an integral component of many streaming and video sharing platforms. Here we consider the problem of learning perceptually relevant video quality representations in a self-supervised manner. Distortion type identification and degradation level determination is employed as an auxiliary task to train a deep learning model containing a deep Convolutional Neural Network (CNN) that extracts spatial features, as well as a recurrent unit that captures temporal information. The model is trained using a contrastive loss and we therefore refer to this training framework and resulting model as CONtrastive VIdeo Quality EstimaTor (CONVIQT). During testing, the weights of the trained model are frozen, and a linear regressor maps the learned features to quality scores in a no-reference (NR) setting. We conduct comprehensive evaluations of the proposed model on multiple VQA databases by analyzing the correlations between model predictions and ground-truth quality ratings, and achieve competitive performance when compared to state-of-the-art NR-VQA models, even though it is not trained on those databases. Our ablation experiments demonstrate that the learned representations are highly robust and generalize well across synthetic and realistic distortions. Our results indicate that compelling representations with perceptual bearing can be obtained using self-supervised learning. The implementations used in this work have been made available at https://github.com/pavancm/CONVIQT.



### LViT: Language meets Vision Transformer in Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.14718v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14718v4)
- **Published**: 2022-06-29 15:36:02+00:00
- **Updated**: 2023-06-27 01:43:10+00:00
- **Authors**: Zihan Li, Yunxiang Li, Qingde Li, Puyang Wang, Dazhou Guo, Le Lu, Dakai Jin, You Zhang, Qingqi Hong
- **Comment**: Accepted by IEEE Transactions on Medical Imaging (TMI)
- **Journal**: None
- **Summary**: Deep learning has been widely used in medical image segmentation and other aspects. However, the performance of existing medical image segmentation models has been limited by the challenge of obtaining sufficient high-quality labeled data due to the prohibitive data annotation cost. To alleviate this limitation, we propose a new text-augmented medical image segmentation model LViT (Language meets Vision Transformer). In our LViT model, medical text annotation is incorporated to compensate for the quality deficiency in image data. In addition, the text information can guide to generate pseudo labels of improved quality in the semi-supervised learning. We also propose an Exponential Pseudo label Iteration mechanism (EPI) to help the Pixel-Level Attention Module (PLAM) preserve local image features in semi-supervised LViT setting. In our model, LV (Language-Vision) loss is designed to supervise the training of unlabeled images using text information directly. For evaluation, we construct three multimodal medical segmentation datasets (image + text) containing X-rays and CT images. Experimental results show that our proposed LViT has superior segmentation performance in both fully-supervised and semi-supervised setting. The code and datasets are available at https://github.com/HUANGLIZI/LViT.



### GO-Surf: Neural Feature Grid Optimization for Fast, High-Fidelity RGB-D Surface Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2206.14735v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14735v2)
- **Published**: 2022-06-29 15:59:23+00:00
- **Updated**: 2022-09-17 23:03:07+00:00
- **Authors**: Jingwen Wang, Tymoteusz Bleja, Lourdes Agapito
- **Comment**: 3DV2022 (Oral), first two authors contributed equally. Project page:
  https://jingwenwang95.github.io/go_surf/
- **Journal**: None
- **Summary**: We present GO-Surf, a direct feature grid optimization method for accurate and fast surface reconstruction from RGB-D sequences. We model the underlying scene with a learned hierarchical feature voxel grid that encapsulates multi-level geometric and appearance local information. Feature vectors are directly optimized such that after being tri-linearly interpolated, decoded by two shallow MLPs into signed distance and radiance values, and rendered via surface volume rendering, the discrepancy between synthesized and observed RGB/depth values is minimized. Our supervision signals -- RGB, depth and approximate SDF -- can be obtained directly from input images without any need for fusion or post-processing. We formulate a novel SDF gradient regularization term that encourages surface smoothness and hole filling while maintaining high frequency details. GO-Surf can optimize sequences of $1$-$2$K frames in $15$-$45$ minutes, a speedup of $\times60$ over NeuralRGB-D, the most related approach based on an MLP representation, while maintaining on par performance on standard benchmarks. Project page: https://jingwenwang95.github.io/go_surf/



### Placenta Segmentation in Ultrasound Imaging: Addressing Sources of Uncertainty and Limited Field-of-View
- **Arxiv ID**: http://arxiv.org/abs/2206.14746v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.14746v1)
- **Published**: 2022-06-29 16:18:55+00:00
- **Updated**: 2022-06-29 16:18:55+00:00
- **Authors**: Veronika A. Zimmer, Alberto Gomez, Emily Skelton, Robert Wright, Gavin Wheeler, Shujie Deng, Nooshin Ghavami, Karen Lloyd, Jacqueline Matthew, Bernhard Kainz, Daniel Rueckert, Joseph V. Hajnal, Julia A. Schnabel
- **Comment**: 21 pages (18 + appendix), 13 figures (9 + appendix)
- **Journal**: None
- **Summary**: Automatic segmentation of the placenta in fetal ultrasound (US) is challenging due to the (i) high diversity of placenta appearance, (ii) the restricted quality in US resulting in highly variable reference annotations, and (iii) the limited field-of-view of US prohibiting whole placenta assessment at late gestation. In this work, we address these three challenges with a multi-task learning approach that combines the classification of placental location (e.g., anterior, posterior) and semantic placenta segmentation in a single convolutional neural network. Through the classification task the model can learn from larger and more diverse datasets while improving the accuracy of the segmentation task in particular in limited training set conditions. With this approach we investigate the variability in annotations from multiple raters and show that our automatic segmentations (Dice of 0.86 for anterior and 0.83 for posterior placentas) achieve human-level performance as compared to intra- and inter-observer variability. Lastly, our approach can deliver whole placenta segmentation using a multi-view US acquisition pipeline consisting of three stages: multi-probe image acquisition, image fusion and image segmentation. This results in high quality segmentation of larger structures such as the placenta in US with reduced image artifacts which are beyond the field-of-view of single probes.



### 3D-Aware Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2206.14797v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.14797v4)
- **Published**: 2022-06-29 17:56:03+00:00
- **Updated**: 2023-08-09 07:34:03+00:00
- **Authors**: Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou, Hao Tang, Gordon Wetzstein, Leonidas Guibas, Luc Van Gool, Radu Timofte
- **Comment**: TMLR 2023; Project page: https://sherwinbahmani.github.io/3dvidgen
- **Journal**: None
- **Summary**: Generative models have emerged as an essential building block for many image synthesis and editing tasks. Recent advances in this field have also enabled high-quality 3D or video content to be generated that exhibits either multi-view or temporal consistency. With our work, we explore 4D generative adversarial networks (GANs) that learn unconditional generation of 3D-aware videos. By combining neural implicit representations with time-aware discriminator, we develop a GAN framework that synthesizes 3D video supervised only with monocular videos. We show that our method learns a rich embedding of decomposable 3D structures and motions that enables new visual effects of spatio-temporal renderings while producing imagery with quality comparable to that of existing 3D or video GANs.



### Strong Lensing Source Reconstruction Using Continuous Neural Fields
- **Arxiv ID**: http://arxiv.org/abs/2206.14820v2
- **DOI**: None
- **Categories**: **astro-ph.CO**, astro-ph.IM, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.14820v2)
- **Published**: 2022-06-29 18:00:01+00:00
- **Updated**: 2022-10-15 22:37:06+00:00
- **Authors**: Siddharth Mishra-Sharma, Ge Yang
- **Comment**: 9+2 pages, 3+2 figures, Spotlight at the Machine Learning for
  Astrophysics Workshop at ICML 2022; v2, references added
- **Journal**: None
- **Summary**: From the nature of dark matter to the rate of expansion of our Universe, observations of distant galaxies distorted through strong gravitational lensing have the potential to answer some of the major open questions in astrophysics. Modeling galaxy-galaxy strong lensing observations presents a number of challenges as the exact configuration of both the background source and foreground lens galaxy is unknown. A timely call, prompted by a number of upcoming surveys anticipating high-resolution lensing images, demands methods that can efficiently model lenses at their full complexity. In this work, we introduce a method that uses continuous neural fields to non-parametrically reconstruct the complex morphology of a source galaxy while simultaneously inferring a distribution over foreground lens galaxy configurations. We demonstrate the efficacy of our method through experiments on simulated data targeting high-resolution lensing images similar to those anticipated in near-future astrophysical surveys.



### Causality for Inherently Explainable Transformers: CAT-XPLAIN
- **Arxiv ID**: http://arxiv.org/abs/2206.14841v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.14841v1)
- **Published**: 2022-06-29 18:11:01+00:00
- **Updated**: 2022-06-29 18:11:01+00:00
- **Authors**: Subash Khanal, Benjamin Brodie, Xin Xing, Ai-Ling Lin, Nathan Jacobs
- **Comment**: Accepted for spotlight presentation at the Explainable Artificial
  Intelligence for Computer Vision Workshop at CVPR 2022
- **Journal**: None
- **Summary**: There have been several post-hoc explanation approaches developed to explain pre-trained black-box neural networks. However, there is still a gap in research efforts toward designing neural networks that are inherently explainable. In this paper, we utilize a recently proposed instance-wise post-hoc causal explanation method to make an existing transformer architecture inherently explainable. Once trained, our model provides an explanation in the form of top-$k$ regions in the input space of the given instance contributing to its decision. We evaluate our method on binary classification tasks using three image datasets: MNIST, FMNIST, and CIFAR. Our results demonstrate that compared to the causality-based post-hoc explainer model, our inherently explainable model achieves better explainability results while eliminating the need of training a separate explainer model. Our code is available at https://github.com/mvrl/CAT-XPLAIN.



### Deep Reinforcement Learning for Small Bowel Path Tracking using Different Types of Annotations
- **Arxiv ID**: http://arxiv.org/abs/2206.14847v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.14847v1)
- **Published**: 2022-06-29 18:18:20+00:00
- **Updated**: 2022-06-29 18:18:20+00:00
- **Authors**: Seung Yeon Shin, Ronald M. Summers
- **Comment**: Accepted to MICCAI 2022
- **Journal**: None
- **Summary**: Small bowel path tracking is a challenging problem considering its many folds and contact along its course. For the same reason, it is very costly to achieve the ground-truth (GT) path of the small bowel in 3D. In this work, we propose to train a deep reinforcement learning tracker using datasets with different types of annotations. Specifically, we utilize CT scans that have only GT small bowel segmentation as well as ones with the GT path. It is enabled by designing a unique environment that is compatible for both, including a reward definable even without the GT path. The performed experiments proved the validity of the proposed method. The proposed method holds a high degree of usability in this problem by being able to utilize the scans with weak annotations, and thus by possibly reducing the required annotation cost.



### Neural Motion Fields: Encoding Grasp Trajectories as Implicit Value Functions
- **Arxiv ID**: http://arxiv.org/abs/2206.14854v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.14854v1)
- **Published**: 2022-06-29 18:47:05+00:00
- **Updated**: 2022-06-29 18:47:05+00:00
- **Authors**: Yun-Chun Chen, Adithyavairavan Murali, Balakumar Sundaralingam, Wei Yang, Animesh Garg, Dieter Fox
- **Comment**: RSS 2022 Workshop on Implicit Representations for Robotic
  Manipulation
- **Journal**: None
- **Summary**: The pipeline of current robotic pick-and-place methods typically consists of several stages: grasp pose detection, finding inverse kinematic solutions for the detected poses, planning a collision-free trajectory, and then executing the open-loop trajectory to the grasp pose with a low-level tracking controller. While these grasping methods have shown good performance on grasping static objects on a table-top, the problem of grasping dynamic objects in constrained environments remains an open problem. We present Neural Motion Fields, a novel object representation which encodes both object point clouds and the relative task trajectories as an implicit value function parameterized by a neural network. This object-centric representation models a continuous distribution over the SE(3) space and allows us to perform grasping reactively by leveraging sampling-based MPC to optimize this value function.



### Two-Stage COVID19 Classification Using BERT Features
- **Arxiv ID**: http://arxiv.org/abs/2206.14861v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.14861v1)
- **Published**: 2022-06-29 19:01:37+00:00
- **Updated**: 2022-06-29 19:01:37+00:00
- **Authors**: Weijun Tan, Qi Yao, Jingfeng Liu
- **Comment**: arXiv admin note: text overlap with arXiv:2106.14403
- **Journal**: None
- **Summary**: We propose an automatic COVID1-19 diagnosis framework from lung CT-scan slice images using double BERT feature extraction. In the first BERT feature extraction, A 3D-CNN is first used to extract CNN internal feature maps. Instead of using the global average pooling, a late BERT temporal pooing is used to aggregate the temporal information in these feature maps, followed by a classification layer. This 3D-CNN-BERT classification network is first trained on sampled fixed number of slice images from every original CT scan volume. In the second stage, the 3D-CNN-BERT embedding features are extracted on all slice images of every CT scan volume, and these features are averaged into a fixed number of segments. Then another BERT network is used to aggregate these multiple features into a single feature followed by another classification layer. The classification results of both stages are combined to generate final outputs. On the validation dataset, we achieve macro F1 score of 0.9164.



### Teach me how to Interpolate a Myriad of Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2206.14868v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.14868v1)
- **Published**: 2022-06-29 19:16:48+00:00
- **Updated**: 2022-06-29 19:16:48+00:00
- **Authors**: Shashanka Venkataramanan, Ewa Kijak, Laurent Amsaleg, Yannis Avrithis
- **Comment**: None
- **Journal**: None
- **Summary**: Mixup refers to interpolation-based data augmentation, originally motivated as a way to go beyond empirical risk minimization (ERM). Yet, its extensions focus on the definition of interpolation and the space where it takes place, while the augmentation itself is less studied: For a mini-batch of size $m$, most methods interpolate between $m$ pairs with a single scalar interpolation factor $\lambda$.   In this work, we make progress in this direction by introducing MultiMix, which interpolates an arbitrary number $n$ of tuples, each of length $m$, with one vector $\lambda$ per tuple. On sequence data, we further extend to dense interpolation and loss computation over all spatial positions. Overall, we increase the number of tuples per mini-batch by orders of magnitude at little additional cost. This is possible by interpolating at the very last layer before the classifier. Finally, to address inconsistencies due to linear target interpolation, we introduce a self-distillation approach to generate and interpolate synthetic targets.   We empirically show that our contributions result in significant improvement over state-of-the-art mixup methods on four benchmarks. By analyzing the embedding space, we observe that the classes are more tightly clustered and uniformly spread over the embedding space, thereby explaining the improved behavior.



### Semantic Unfolding of StyleGAN Latent Space
- **Arxiv ID**: http://arxiv.org/abs/2206.14892v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.14892v1)
- **Published**: 2022-06-29 20:22:10+00:00
- **Updated**: 2022-06-29 20:22:10+00:00
- **Authors**: Mustafa Shukor, Xu Yao, Bharath Bushan Damodaran, Pierre Hellier
- **Comment**: Accepted at ICIP22
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have proven to be surprisingly efficient for image editing by inverting and manipulating the latent code corresponding to an input real image. This editing property emerges from the disentangled nature of the latent space. In this paper, we identify that the facial attribute disentanglement is not optimal, thus facial editing relying on linear attribute separation is flawed. We thus propose to improve semantic disentanglement with supervision. Our method consists in learning a proxy latent representation using normalizing flows, and we show that this leads to a more efficient space for face image editing.



### CIRDataset: A large-scale Dataset for Clinically-Interpretable lung nodule Radiomics and malignancy prediction
- **Arxiv ID**: http://arxiv.org/abs/2206.14903v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.14903v1)
- **Published**: 2022-06-29 20:46:12+00:00
- **Updated**: 2022-06-29 20:46:12+00:00
- **Authors**: Wookjin Choi, Navdeep Dahiya, Saad Nadeem
- **Comment**: MICCAI 2022
- **Journal**: None
- **Summary**: Spiculations/lobulations, sharp/curved spikes on the surface of lung nodules, are good predictors of lung cancer malignancy and hence, are routinely assessed and reported by radiologists as part of the standardized Lung-RADS clinical scoring criteria. Given the 3D geometry of the nodule and 2D slice-by-slice assessment by radiologists, manual spiculation/lobulation annotation is a tedious task and thus no public datasets exist to date for probing the importance of these clinically-reported features in the SOTA malignancy prediction algorithms. As part of this paper, we release a large-scale Clinically-Interpretable Radiomics Dataset, CIRDataset, containing 956 radiologist QA/QC'ed spiculation/lobulation annotations on segmented lung nodules from two public datasets, LIDC-IDRI (N=883) and LUNGx (N=73). We also present an end-to-end deep learning model based on multi-class Voxel2Mesh extension to segment nodules (while preserving spikes), classify spikes (sharp/spiculation and curved/lobulation), and perform malignancy prediction. Previous methods have performed malignancy prediction for LIDC and LUNGx datasets but without robust attribution to any clinically reported/actionable features (due to known hyperparameter sensitivity issues with general attribution schemes). With the release of this comprehensively-annotated CIRDataset and end-to-end deep learning baseline, we hope that malignancy prediction methods can validate their explanations, benchmark against our baseline, and provide clinically-actionable insights. Dataset, code, pretrained models, and docker containers are available at https://github.com/nadeemlab/CIR.



### Identifying and Combating Bias in Segmentation Networks by leveraging multiple resolutions
- **Arxiv ID**: http://arxiv.org/abs/2206.14919v1
- **DOI**: 10.1007/978-3-031-16443-9_34
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.14919v1)
- **Published**: 2022-06-29 21:22:21+00:00
- **Updated**: 2022-06-29 21:22:21+00:00
- **Authors**: Leonie Henschel, David Kügler, Derek S Andrews, Christine W Nordahl, Martin Reuter
- **Comment**: None
- **Journal**: None
- **Summary**: Exploration of bias has significant impact on the transparency and applicability of deep learning pipelines in medical settings, yet is so far woefully understudied. In this paper, we consider two separate groups for which training data is only available at differing image resolutions. For group H, available images and labels are at the preferred high resolution while for group L only deprecated lower resolution data exist. We analyse how this resolution-bias in the data distribution propagates to systematically biased predictions for group L at higher resolutions. Our results demonstrate that single-resolution training settings result in significant loss of volumetric group differences that translate to erroneous segmentations as measured by DSC and subsequent classification failures on the low resolution group. We further explore how training data across resolutions can be used to combat this systematic bias. Specifically, we investigate the effect of image resampling, scale augmentation and resolution independence and demonstrate that biases can effectively be reduced with multi-resolution approaches.



### On Non-Random Missing Labels in Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.14923v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.14923v1)
- **Published**: 2022-06-29 22:01:29+00:00
- **Updated**: 2022-06-29 22:01:29+00:00
- **Authors**: Xinting Hu, Yulei Niu, Chunyan Miao, Xian-Sheng Hua, Hanwang Zhang
- **Comment**: None
- **Journal**: ICLR 2022
- **Summary**: Semi-Supervised Learning (SSL) is fundamentally a missing label problem, in which the label Missing Not At Random (MNAR) problem is more realistic and challenging, compared to the widely-adopted yet naive Missing Completely At Random assumption where both labeled and unlabeled data share the same class distribution. Different from existing SSL solutions that overlook the role of "class" in causing the non-randomness, e.g., users are more likely to label popular classes, we explicitly incorporate "class" into SSL. Our method is three-fold: 1) We propose Class-Aware Propensity (CAP) that exploits the unlabeled data to train an improved classifier using the biased labeled data. 2) To encourage rare class training, whose model is low-recall but high-precision that discards too many pseudo-labeled data, we propose Class-Aware Imputation (CAI) that dynamically decreases (or increases) the pseudo-label assignment threshold for rare (or frequent) classes. 3) Overall, we integrate CAP and CAI into a Class-Aware Doubly Robust (CADR) estimator for training an unbiased SSL model. Under various MNAR settings and ablations, our method not only significantly outperforms existing baselines but also surpasses other label bias removal SSL methods. Please check our code at: https://github.com/JoyHuYY1412/CADR-FixMatch.



### Regularization of NeRFs using differential geometry
- **Arxiv ID**: http://arxiv.org/abs/2206.14938v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.14938v2)
- **Published**: 2022-06-29 22:45:34+00:00
- **Updated**: 2022-11-30 18:05:11+00:00
- **Authors**: Thibaud Ehret, Roger Marí, Gabriele Facciolo
- **Comment**: None
- **Journal**: None
- **Summary**: Neural radiance fields, or NeRF, represent a breakthrough in the field of novel view synthesis and 3D modeling of complex scenes from multi-view image collections. Numerous recent works have shown the importance of making NeRF models more robust, by means of regularization, in order to train with possibly inconsistent and/or very sparse data. In this work, we explore how differential geometry can provide elegant regularization tools for robustly training NeRF-like models, which are modified so as to represent continuous and infinitely differentiable functions. In particular, we present a generic framework for regularizing different types of NeRFs observations to improve the performance in challenging conditions. We also show how the same formalism can also be used to natively encourage the regularity of surfaces by means of Gaussian or mean curvatures.



### CLTS-GAN: Color-Lighting-Texture-Specular Reflection Augmentation for Colonoscopy
- **Arxiv ID**: http://arxiv.org/abs/2206.14951v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.14951v1)
- **Published**: 2022-06-29 23:51:16+00:00
- **Updated**: 2022-06-29 23:51:16+00:00
- **Authors**: Shawn Mathew, Saad Nadeem, Arie Kaufman
- **Comment**: MICCAI 2022. **First two authors contributed equally
- **Journal**: None
- **Summary**: Automated analysis of optical colonoscopy (OC) video frames (to assist endoscopists during OC) is challenging due to variations in color, lighting, texture, and specular reflections. Previous methods either remove some of these variations via preprocessing (making pipelines cumbersome) or add diverse training data with annotations (but expensive and time-consuming). We present CLTS-GAN, a new deep learning model that gives fine control over color, lighting, texture, and specular reflection synthesis for OC video frames. We show that adding these colonoscopy-specific augmentations to the training data can improve state-of-the-art polyp detection/segmentation methods as well as drive next generation of OC simulators for training medical students. The code and pre-trained models for CLTS-GAN are available on Computational Endoscopy Platform GitHub (https://github.com/nadeemlab/CEP).



