# Arxiv Papers in cs.CV on 2022-06-10
### Superresolution and Segmentation of OCT scans using Multi-Stage adversarial Guided Attention Training
- **Arxiv ID**: http://arxiv.org/abs/2206.05277v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05277v1)
- **Published**: 2022-06-10 00:26:55+00:00
- **Updated**: 2022-06-10 00:26:55+00:00
- **Authors**: Paria Jeihouni, Omid Dehzangi, Annahita Amireskandari, Ali Dabouei, Ali Rezai, Nasser M. Nasrabadi
- **Comment**: 5 pages,conference
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) is one of the non-invasive and easy-to-acquire biomarkers (the thickness of the retinal layers, which is detectable within OCT scans) being investigated to diagnose Alzheimer's disease (AD). This work aims to segment the OCT images automatically; however, it is a challenging task due to various issues such as the speckle noise, small target region, and unfavorable imaging conditions. In our previous work, we have proposed the multi-stage & multi-discriminatory generative adversarial network (MultiSDGAN) to translate OCT scans in high-resolution segmentation labels. In this investigation, we aim to evaluate and compare various combinations of channel and spatial attention to the MultiSDGAN architecture to extract more powerful feature maps by capturing rich contextual relationships to improve segmentation performance. Moreover, we developed and evaluated a guided mutli-stage attention framework where we incorporated a guided attention mechanism by forcing an L-1 loss between a specifically designed binary mask and the generated attention maps. Our ablation study results on the WVU-OCT data-set in five-fold cross-validation (5-CV) suggest that the proposed MultiSDGAN with a serial attention module provides the most competitive performance, and guiding the spatial attention feature maps by binary masks further improves the performance in our proposed network. Comparing the baseline model with adding the guided-attention, our results demonstrated relative improvements of 21.44% and 19.45% on the Dice coefficient and SSIM, respectively.



### R4D: Utilizing Reference Objects for Long-Range Distance Estimation
- **Arxiv ID**: http://arxiv.org/abs/2206.04831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.04831v1)
- **Published**: 2022-06-10 01:27:16+00:00
- **Updated**: 2022-06-10 01:27:16+00:00
- **Authors**: Yingwei Li, Tiffany Chen, Maya Kabkab, Ruichi Yu, Longlong Jing, Yurong You, Hang Zhao
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: Estimating the distance of objects is a safety-critical task for autonomous driving. Focusing on short-range objects, existing methods and datasets neglect the equally important long-range objects. In this paper, we introduce a challenging and under-explored task, which we refer to as Long-Range Distance Estimation, as well as two datasets to validate new methods developed for this task. We then proposeR4D, the first framework to accurately estimate the distance of long-range objects by using references with known distances in the scene. Drawing inspiration from human perception, R4D builds a graph by connecting a target object to all references. An edge in the graph encodes the relative distance information between a pair of target and reference objects. An attention module is then used to weigh the importance of reference objects and combine them into one target object distance prediction. Experiments on the two proposed datasets demonstrate the effectiveness and robustness of R4D by showing significant improvements compared to existing baselines. We are looking to make the proposed dataset, Waymo OpenDataset - Long-Range Labels, available publicly at waymo.com/open/download.



### Dual-Branch Squeeze-Fusion-Excitation Module for Cross-Modality Registration of Cardiac SPECT and CT
- **Arxiv ID**: http://arxiv.org/abs/2206.05278v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05278v1)
- **Published**: 2022-06-10 01:44:06+00:00
- **Updated**: 2022-06-10 01:44:06+00:00
- **Authors**: Xiongchao Chen, Bo Zhou, Huidong Xie, Xueqi Guo, Jiazhen Zhang, Albert J. Sinusas, John A. Onofrey, Chi liu
- **Comment**: 10 pages, 4 figures, accepted at MICCAI 2022
- **Journal**: None
- **Summary**: Single-photon emission computed tomography (SPECT) is a widely applied imaging approach for diagnosis of coronary artery diseases. Attenuation maps (u-maps) derived from computed tomography (CT) are utilized for attenuation correction (AC) to improve diagnostic accuracy of cardiac SPECT. However, SPECT and CT are obtained sequentially in clinical practice, which potentially induces misregistration between the two scans. Convolutional neural networks (CNN) are powerful tools for medical image registration. Previous CNN-based methods for cross-modality registration either directly concatenated two input modalities as an early feature fusion or extracted image features using two separate CNN modules for a late fusion. These methods do not fully extract or fuse the cross-modality information. Besides, deep-learning-based rigid registration of cardiac SPECT and CT-derived u-maps has not been investigated before. In this paper, we propose a Dual-Branch Squeeze-Fusion-Excitation (DuSFE) module for the registration of cardiac SPECT and CT-derived u-maps. DuSFE fuses the knowledge from multiple modalities to recalibrate both channel-wise and spatial features for each modality. DuSFE can be embedded at multiple convolutional layers to enable feature fusion at different spatial dimensions. Our studies using clinical data demonstrated that a network embedded with DuSFE generated substantial lower registration errors and therefore more accurate AC SPECT images than previous methods.



### Masked Autoencoders are Robust Data Augmentors
- **Arxiv ID**: http://arxiv.org/abs/2206.04846v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.04846v1)
- **Published**: 2022-06-10 02:41:48+00:00
- **Updated**: 2022-06-10 02:41:48+00:00
- **Authors**: Haohang Xu, Shuangrui Ding, Xiaopeng Zhang, Hongkai Xiong, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are capable of learning powerful representations to tackle complex vision tasks but expose undesirable properties like the over-fitting issue. To this end, regularization techniques like image augmentation are necessary for deep neural networks to generalize well. Nevertheless, most prevalent image augmentation recipes confine themselves to off-the-shelf linear transformations like scale, flip, and colorjitter. Due to their hand-crafted property, these augmentations are insufficient to generate truly hard augmented examples. In this paper, we propose a novel perspective of augmentation to regularize the training process. Inspired by the recent success of applying masked image modeling to self-supervised learning, we adopt the self-supervised masked autoencoder to generate the distorted view of the input images. We show that utilizing such model-based nonlinear transformation as data augmentation can improve high-level recognition tasks. We term the proposed method as \textbf{M}ask-\textbf{R}econstruct \textbf{A}ugmentation (MRA). The extensive experiments on various image classification benchmarks verify the effectiveness of the proposed augmentation. Specifically, MRA consistently enhances the performance on supervised, semi-supervised as well as few-shot classification. The code will be available at \url{https://github.com/haohang96/MRA}.



### PILC: Practical Image Lossless Compression with an End-to-end GPU Oriented Neural Framework
- **Arxiv ID**: http://arxiv.org/abs/2206.05279v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2206.05279v1)
- **Published**: 2022-06-10 03:00:10+00:00
- **Updated**: 2022-06-10 03:00:10+00:00
- **Authors**: Ning Kang, Shanzhao Qiu, Shifeng Zhang, Zhenguo Li, Shutao Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Generative model based image lossless compression algorithms have seen a great success in improving compression ratio. However, the throughput for most of them is less than 1 MB/s even with the most advanced AI accelerated chips, preventing them from most real-world applications, which often require 100 MB/s. In this paper, we propose PILC, an end-to-end image lossless compression framework that achieves 200 MB/s for both compression and decompression with a single NVIDIA Tesla V100 GPU, 10 times faster than the most efficient one before. To obtain this result, we first develop an AI codec that combines auto-regressive model and VQ-VAE which performs well in lightweight setting, then we design a low complexity entropy coder that works well with our codec. Experiments show that our framework compresses better than PNG by a margin of 30% in multiple datasets. We believe this is an important step to bring AI compression forward to commercial use.



### Heterogeneous Face Recognition via Face Synthesis with Identity-Attribute Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2206.04854v1
- **DOI**: 10.1109/TIFS.2022.3160595
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.04854v1)
- **Published**: 2022-06-10 03:01:33+00:00
- **Updated**: 2022-06-10 03:01:33+00:00
- **Authors**: Ziming Yang, Jian Liang, Chaoyou Fu, Mandi Luo, Xiao-Yu Zhang
- **Comment**: Accepted for publication in IEEE Transactions on Information
  Forensics and Security (TIFS)
- **Journal**: IEEE Transactions on Information Forensics and Security, vol. 17,
  pp. 1344-1358, 2022
- **Summary**: Heterogeneous Face Recognition (HFR) aims to match faces across different domains (e.g., visible to near-infrared images), which has been widely applied in authentication and forensics scenarios. However, HFR is a challenging problem because of the large cross-domain discrepancy, limited heterogeneous data pairs, and large variation of facial attributes. To address these challenges, we propose a new HFR method from the perspective of heterogeneous data augmentation, named Face Synthesis with Identity-Attribute Disentanglement (FSIAD). Firstly, the identity-attribute disentanglement (IAD) decouples face images into identity-related representations and identity-unrelated representations (called attributes), and then decreases the correlation between identities and attributes. Secondly, we devise a face synthesis module (FSM) to generate a large number of images with stochastic combinations of disentangled identities and attributes for enriching the attribute diversity of synthetic images. Both the original images and the synthetic ones are utilized to train the HFR network for tackling the challenges and improving the performance of HFR. Extensive experiments on five HFR databases validate that FSIAD obtains superior performance than previous HFR approaches. Particularly, FSIAD obtains 4.8% improvement over state of the art in terms of VR@FAR=0.01% on LAMP-HQ, the largest HFR database so far.



### Symbolic image detection using scene and knowledge graphs
- **Arxiv ID**: http://arxiv.org/abs/2206.04863v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.04863v1)
- **Published**: 2022-06-10 04:06:28+00:00
- **Updated**: 2022-06-10 04:06:28+00:00
- **Authors**: Nasrin Kalanat, Adriana Kovashka
- **Comment**: None
- **Journal**: None
- **Summary**: Sometimes the meaning conveyed by images goes beyond the list of objects they contain; instead, images may express a powerful message to affect the viewers' minds. Inferring this message requires reasoning about the relationships between the objects, and general common-sense knowledge about the components. In this paper, we use a scene graph, a graph representation of an image, to capture visual components. In addition, we generate a knowledge graph using facts extracted from ConceptNet to reason about objects and attributes. To detect the symbols, we propose a neural network framework named SKG-Sym. The framework first generates the representations of the scene graph of the image and its knowledge graph using Graph Convolution Network. The framework then fuses the representations and uses an MLP to classify them. We extend the network further to use an attention mechanism which learn the importance of the graph representations. We evaluate our methods on a dataset of advertisements, and compare it with baseline symbolism classification methods (ResNet and VGG). Results show that our methods outperform ResNet in terms of F-score and the attention-based mechanism is competitive with VGG while it has much lower model complexity.



### The Gender Gap in Face Recognition Accuracy Is a Hairy Problem
- **Arxiv ID**: http://arxiv.org/abs/2206.04867v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.04867v1)
- **Published**: 2022-06-10 04:32:47+00:00
- **Updated**: 2022-06-10 04:32:47+00:00
- **Authors**: Aman Bhatta, Vítor Albiero, Kevin W. Bowyer, Michael C. King
- **Comment**: None
- **Journal**: None
- **Summary**: It is broadly accepted that there is a "gender gap" in face recognition accuracy, with females having higher false match and false non-match rates. However, relatively little is known about the cause(s) of this gender gap. Even the recent NIST report on demographic effects lists "analyze cause and effect" under "what we did not do". We first demonstrate that female and male hairstyles have important differences that impact face recognition accuracy. In particular, compared to females, male facial hair contributes to creating a greater average difference in appearance between different male faces. We then demonstrate that when the data used to estimate recognition accuracy is balanced across gender for how hairstyles occlude the face, the initially observed gender gap in accuracy largely disappears. We show this result for two different matchers, and analyzing images of Caucasians and of African-Americans. These results suggest that future research on demographic variation in accuracy should include a check for balanced quality of the test data as part of the problem formulation. To promote reproducible research, matchers, attribute classifiers, and datasets used in this research are/will be publicly available.



### The 1st Data Science for Pavements Challenge
- **Arxiv ID**: http://arxiv.org/abs/2206.04874v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.04874v1)
- **Published**: 2022-06-10 05:02:31+00:00
- **Updated**: 2022-06-10 05:02:31+00:00
- **Authors**: Ashkan Behzadian, Tanner Wambui Muturi, Tianjie Zhang, Hongmin Kim, Amanda Mullins, Yang Lu, Neema Jasika Owor, Yaw Adu-Gyamfi, William Buttlar, Majidifard Hamed, Armstrong Aboah, David Mensching, Spragg Robert, Matthew Corrigan, Jack Youtchef, Dave Eshan
- **Comment**: None
- **Journal**: None
- **Summary**: The Data Science for Pavement Challenge (DSPC) seeks to accelerate the research and development of automated vision systems for pavement condition monitoring and evaluation by providing a platform with benchmarked datasets and codes for teams to innovate and develop machine learning algorithms that are practice-ready for use by industry. The first edition of the competition attracted 22 teams from 8 countries. Participants were required to automatically detect and classify different types of pavement distresses present in images captured from multiple sources, and under different conditions. The competition was data-centric: teams were tasked to increase the accuracy of a predefined model architecture by utilizing various data modification methods such as cleaning, labeling and augmentation. A real-time, online evaluation system was developed to rank teams based on the F1 score. Leaderboard results showed the promise and challenges of machine for advancing automation in pavement monitoring and evaluation. This paper summarizes the solutions from the top 5 teams. These teams proposed innovations in the areas of data cleaning, annotation, augmentation, and detection parameter tuning. The F1 score for the top-ranked team was approximately 0.9. The paper concludes with a review of different experiments that worked well for the current challenge and those that did not yield any significant improvement in model accuracy.



### Efficient Per-Shot Convex Hull Prediction By Recurrent Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.04877v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.04877v1)
- **Published**: 2022-06-10 05:11:02+00:00
- **Updated**: 2022-06-10 05:11:02+00:00
- **Authors**: Somdyuti Paul, Andrey Norkin, Alan C. Bovik
- **Comment**: None
- **Journal**: None
- **Summary**: Adaptive video streaming relies on the construction of efficient bitrate ladders to deliver the best possible visual quality to viewers under bandwidth constraints. The traditional method of content dependent bitrate ladder selection requires a video shot to be pre-encoded with multiple encoding parameters to find the optimal operating points given by the convex hull of the resulting rate-quality curves. However, this pre-encoding step is equivalent to an exhaustive search process over the space of possible encoding parameters, which causes significant overhead in terms of both computation and time expenditure. To reduce this overhead, we propose a deep learning based method of content aware convex hull prediction. We employ a recurrent convolutional network (RCN) to implicitly analyze the spatiotemporal complexity of video shots in order to predict their convex hulls. A two-step transfer learning scheme is adopted to train our proposed RCN-Hull model, which ensures sufficient content diversity to analyze scene complexity, while also making it possible capture the scene statistics of pristine source videos. Our experimental results reveal that our proposed model yields better approximations of the optimal convex hulls, and offers competitive time savings as compared to existing approaches. On average, the pre-encoding time was reduced by 58.0% by our method, while the average Bjontegaard delta bitrate (BD-rate) of the predicted convex hulls against ground truth was 0.08%, while the mean absolute deviation of the BD-rate distribution was 0.44%



### Unsupervised Foggy Scene Understanding via Self Spatial-Temporal Label Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2206.04879v1
- **DOI**: 10.1109/TIP.2022.3172208
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.04879v1)
- **Published**: 2022-06-10 05:16:50+00:00
- **Updated**: 2022-06-10 05:16:50+00:00
- **Authors**: Liang Liao, Wenyi Chen, Jing Xiao, Zheng Wang, Chia-Wen Lin, Shin'ichi Satoh
- **Comment**: IEEE Transactions on Image Processing 2022
- **Journal**: None
- **Summary**: Understanding foggy image sequence in the driving scenes is critical for autonomous driving, but it remains a challenging task due to the difficulty in collecting and annotating real-world images of adverse weather. Recently, the self-training strategy has been considered a powerful solution for unsupervised domain adaptation, which iteratively adapts the model from the source domain to the target domain by generating target pseudo labels and re-training the model. However, the selection of confident pseudo labels inevitably suffers from the conflict between sparsity and accuracy, both of which will lead to suboptimal models. To tackle this problem, we exploit the characteristics of the foggy image sequence of driving scenes to densify the confident pseudo labels. Specifically, based on the two discoveries of local spatial similarity and adjacent temporal correspondence of the sequential image data, we propose a novel Target-Domain driven pseudo label Diffusion (TDo-Dif) scheme. It employs superpixels and optical flows to identify the spatial similarity and temporal correspondence, respectively and then diffuses the confident but sparse pseudo labels within a superpixel or a temporal corresponding pair linked by the flow. Moreover, to ensure the feature similarity of the diffused pixels, we introduce local spatial similarity loss and temporal contrastive loss in the model re-training stage. Experimental results show that our TDo-Dif scheme helps the adaptive model achieve 51.92% and 53.84% mean intersection-over-union (mIoU) on two publicly available natural foggy datasets (Foggy Zurich and Foggy Driving), which exceeds the state-of-the-art unsupervised domain adaptive semantic segmentation methods. Models and data can be found at https://github.com/velor2012/TDo-Dif.



### Enhancing Clean Label Backdoor Attack with Two-phase Specific Triggers
- **Arxiv ID**: http://arxiv.org/abs/2206.04881v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.04881v1)
- **Published**: 2022-06-10 05:34:06+00:00
- **Updated**: 2022-06-10 05:34:06+00:00
- **Authors**: Nan Luo, Yuanzhang Li, Yajie Wang, Shangbo Wu, Yu-an Tan, Quanxin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Backdoor attacks threaten Deep Neural Networks (DNNs). Towards stealthiness, researchers propose clean-label backdoor attacks, which require the adversaries not to alter the labels of the poisoned training datasets. Clean-label settings make the attack more stealthy due to the correct image-label pairs, but some problems still exist: first, traditional methods for poisoning training data are ineffective; second, traditional triggers are not stealthy which are still perceptible. To solve these problems, we propose a two-phase and image-specific triggers generation method to enhance clean-label backdoor attacks. Our methods are (1) powerful: our triggers can both promote the two phases (i.e., the backdoor implantation and activation phase) in backdoor attacks simultaneously; (2) stealthy: our triggers are generated from each image. They are image-specific instead of fixed triggers. Extensive experiments demonstrate that our approach can achieve a fantastic attack success rate~(98.98%) with low poisoning rate~(5%), high stealthiness under many evaluation metrics and is resistant to backdoor defense methods.



### AntPivot: Livestream Highlight Detection via Hierarchical Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2206.04888v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.04888v1)
- **Published**: 2022-06-10 05:58:11+00:00
- **Updated**: 2022-06-10 05:58:11+00:00
- **Authors**: Yang Zhao, Xuan Lin, Wenqiang Xu, Maozong Zheng, Zhengyong Liu, Zhou Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: In recent days, streaming technology has greatly promoted the development in the field of livestream. Due to the excessive length of livestream records, it's quite essential to extract highlight segments with the aim of effective reproduction and redistribution. Although there are lots of approaches proven to be effective in the highlight detection for other modals, the challenges existing in livestream processing, such as the extreme durations, large topic shifts, much irrelevant information and so forth, heavily hamper the adaptation and compatibility of these methods. In this paper, we formulate a new task Livestream Highlight Detection, discuss and analyze the difficulties listed above and propose a novel architecture AntPivot to solve this problem. Concretely, we first encode the original data into multiple views and model their temporal relations to capture clues in a hierarchical attention mechanism. Afterwards, we try to convert the detection of highlight clips into the search for optimal decision sequences and use the fully integrated representations to predict the final results in a dynamic-programming mechanism. Furthermore, we construct a fully-annotated dataset AntHighlight to instantiate this task and evaluate the performance of our model. The extensive experiments indicate the effectiveness and validity of our proposed method.



### NeRF-In: Free-Form NeRF Inpainting with RGB-D Priors
- **Arxiv ID**: http://arxiv.org/abs/2206.04901v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2206.04901v1)
- **Published**: 2022-06-10 06:54:22+00:00
- **Updated**: 2022-06-10 06:54:22+00:00
- **Authors**: Hao-Kang Liu, I-Chao Shen, Bing-Yu Chen
- **Comment**: Hao-Kang Liu and I-Chao Shen contributed equally to the paper.
  Project page: https://jdily.github.io/proj_site/nerfin_proj.html
- **Journal**: None
- **Summary**: Though Neural Radiance Field (NeRF) demonstrates compelling novel view synthesis results, it is still unintuitive to edit a pre-trained NeRF because the neural network's parameters and the scene geometry/appearance are often not explicitly associated. In this paper, we introduce the first framework that enables users to remove unwanted objects or retouch undesired regions in a 3D scene represented by a pre-trained NeRF without any category-specific data and training. The user first draws a free-form mask to specify a region containing unwanted objects over a rendered view from the pre-trained NeRF. Our framework first transfers the user-provided mask to other rendered views and estimates guiding color and depth images within these transferred masked regions. Next, we formulate an optimization problem that jointly inpaints the image content in all masked regions across multiple views by updating the NeRF model's parameters. We demonstrate our framework on diverse scenes and show it obtained visual plausible and structurally consistent results across multiple views using shorter time and less user manual efforts.



### Less Is More: Linear Layers on CLIP Features as Powerful VizWiz Model
- **Arxiv ID**: http://arxiv.org/abs/2206.05281v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05281v1)
- **Published**: 2022-06-10 07:03:52+00:00
- **Updated**: 2022-06-10 07:03:52+00:00
- **Authors**: Fabian Deuser, Konrad Habel, Philipp J. Rösch, Norbert Oswald
- **Comment**: VizWiz Grand Challenge: Describing Images and Videos Taken by Blind
  People (CVPR Workshop 2022)
- **Journal**: None
- **Summary**: Current architectures for multi-modality tasks such as visual question answering suffer from their high complexity. As a result, these architectures are difficult to train and require high computational resources. To address these problems we present a CLIP-based architecture that does not require any fine-tuning of the feature extractors. A simple linear classifier is used on the concatenated features of the image and text encoder. During training an auxiliary loss is added which operates on the answer types. The resulting classification is then used as an attention gate on the answer class selection. On the VizWiz 2022 Visual Question Answering Challenge we achieve 60.15 % accuracy on Task 1: Predict Answer to a Visual Question and AP score of 83.78 % on Task 2: Predict Answerability of a Visual Question.



### Out of Sight, Out of Mind: A Source-View-Wise Feature Aggregation for Multi-View Image-Based Rendering
- **Arxiv ID**: http://arxiv.org/abs/2206.04906v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.04906v1)
- **Published**: 2022-06-10 07:06:05+00:00
- **Updated**: 2022-06-10 07:06:05+00:00
- **Authors**: Geonho Cha, Chaehun Shin, Sungroh Yoon, Dongyoon Wee
- **Comment**: None
- **Journal**: None
- **Summary**: To estimate the volume density and color of a 3D point in the multi-view image-based rendering, a common approach is to inspect the consensus existence among the given source image features, which is one of the informative cues for the estimation procedure. To this end, most of the previous methods utilize equally-weighted aggregation features. However, this could make it hard to check the consensus existence when some outliers, which frequently occur by occlusions, are included in the source image feature set. In this paper, we propose a novel source-view-wise feature aggregation method, which facilitates us to find out the consensus in a robust way by leveraging local structures in the feature set. We first calculate the source-view-wise distance distribution for each source feature for the proposed aggregation. After that, the distance distribution is converted to several similarity distributions with the proposed learnable similarity mapping functions. Finally, for each element in the feature set, the aggregation features are extracted by calculating the weighted means and variances, where the weights are derived from the similarity distributions. In experiments, we validate the proposed method on various benchmark datasets, including synthetic and real image scenes. The experimental results demonstrate that incorporating the proposed features improves the performance by a large margin, resulting in the state-of-the-art performance.



### Learning to Estimate Shapley Values with Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2206.05282v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05282v3)
- **Published**: 2022-06-10 07:09:28+00:00
- **Updated**: 2023-03-01 20:24:58+00:00
- **Authors**: Ian Covert, Chanwoo Kim, Su-In Lee
- **Comment**: ICLR 2023 camera-ready
- **Journal**: None
- **Summary**: Transformers have become a default architecture in computer vision, but understanding what drives their predictions remains a challenging problem. Current explanation approaches rely on attention values or input gradients, but these provide a limited view of a model's dependencies. Shapley values offer a theoretically sound alternative, but their computational cost makes them impractical for large, high-dimensional models. In this work, we aim to make Shapley values practical for vision transformers (ViTs). To do so, we first leverage an attention masking approach to evaluate ViTs with partial information, and we then develop a procedure to generate Shapley value explanations via a separate, learned explainer model. Our experiments compare Shapley values to many baseline methods (e.g., attention rollout, GradCAM, LRP), and we find that our approach provides more accurate explanations than existing methods for ViTs.



### Poissonian Blurred Image Deconvolution by Framelet based Local Minimal Prior
- **Arxiv ID**: http://arxiv.org/abs/2206.05283v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2206.05283v1)
- **Published**: 2022-06-10 07:23:33+00:00
- **Updated**: 2022-06-10 07:23:33+00:00
- **Authors**: Reza Parvaz
- **Comment**: None
- **Journal**: None
- **Summary**: Image production tools do not always create a clear image, noisy and blurry images are sometimes created. Among these cases, Poissonian noise is one of the most famous noises that appear in medical images and images taken in astronomy. Blurred image with Poissonian noise obscures important details that are of great importance in medicine or astronomy. Therefore, studying and increasing the quality of images that are affected by this type of noise is always considered by researchers. In this paper, in the first step, based on framelet transform, a local minimal prior is introduced, and in the next step, this tool together with fractional calculation is used for Poissonian blurred image deconvolution. In the following, the model is generalized to the blind case. To evaluate the performance of the presented model, several images such as real images have been investigated.



### PatchComplete: Learning Multi-Resolution Patch Priors for 3D Shape Completion on Unseen Categories
- **Arxiv ID**: http://arxiv.org/abs/2206.04916v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.04916v2)
- **Published**: 2022-06-10 07:34:10+00:00
- **Updated**: 2022-10-12 11:50:45+00:00
- **Authors**: Yuchen Rao, Yinyu Nie, Angela Dai
- **Comment**: Video link: https://www.youtube.com/watch?v=Ch1rvw2D_Kc ; Project
  page: https://yuchenrao.github.io/projects/patchComplete/patchComplete.html ;
  Accepted to NeurIPS'22
- **Journal**: None
- **Summary**: While 3D shape representations enable powerful reasoning in many visual and perception applications, learning 3D shape priors tends to be constrained to the specific categories trained on, leading to an inefficient learning process, particularly for general applications with unseen categories. Thus, we propose PatchComplete, which learns effective shape priors based on multi-resolution local patches, which are often more general than full shapes (e.g., chairs and tables often both share legs) and thus enable geometric reasoning about unseen class categories. To learn these shared substructures, we learn multi-resolution patch priors across all train categories, which are then associated to input partial shape observations by attention across the patch priors, and finally decoded into a complete shape reconstruction. Such patch-based priors avoid overfitting to specific train categories and enable reconstruction on entirely unseen categories at test time. We demonstrate the effectiveness of our approach on synthetic ShapeNet data as well as challenging real-scanned objects from ScanNet, which include noise and clutter, improving over state of the art in novel-category shape completion by 19.3% in chamfer distance on ShapeNet, and 9.0% for ScanNet.



### Ego2HandsPose: A Dataset for Egocentric Two-hand 3D Global Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2206.04927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.04927v1)
- **Published**: 2022-06-10 07:50:45+00:00
- **Updated**: 2022-06-10 07:50:45+00:00
- **Authors**: Fanqing Lin, Tony Martinez
- **Comment**: None
- **Journal**: None
- **Summary**: Color-based two-hand 3D pose estimation in the global coordinate system is essential in many applications. However, there are very few datasets dedicated to this task and no existing dataset supports estimation in a non-laboratory environment. This is largely attributed to the sophisticated data collection process required for 3D hand pose annotations, which also leads to difficulty in obtaining instances with the level of visual diversity needed for estimation in the wild. Progressing towards this goal, a large-scale dataset Ego2Hands was recently proposed to address the task of two-hand segmentation and detection in the wild. The proposed composition-based data generation technique can create two-hand instances with quality, quantity and diversity that generalize well to unseen domains. In this work, we present Ego2HandsPose, an extension of Ego2Hands that contains 3D hand pose annotation and is the first dataset that enables color-based two-hand 3D tracking in unseen domains. To this end, we develop a set of parametric fitting algorithms to enable 1) 3D hand pose annotation using a single image, 2) automatic conversion from 2D to 3D hand poses and 3) accurate two-hand tracking with temporal consistency. We provide incremental quantitative analysis on the multi-stage pipeline and show that training on our dataset achieves state-of-the-art results that significantly outperforms other datasets for the task of egocentric two-hand global 3D pose estimation.



### Neural Template: Topology-aware Reconstruction and Disentangled Generation of 3D Meshes
- **Arxiv ID**: http://arxiv.org/abs/2206.04942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.04942v1)
- **Published**: 2022-06-10 08:32:57+00:00
- **Updated**: 2022-06-10 08:32:57+00:00
- **Authors**: Ka-Hei Hui, Ruihui Li, Jingyu Hu, Chi-Wing Fu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel framework called DTNet for 3D mesh reconstruction and generation via Disentangled Topology. Beyond previous works, we learn a topology-aware neural template specific to each input then deform the template to reconstruct a detailed mesh while preserving the learned topology. One key insight is to decouple the complex mesh reconstruction into two sub-tasks: topology formulation and shape deformation. Thanks to the decoupling, DT-Net implicitly learns a disentangled representation for the topology and shape in the latent space. Hence, it can enable novel disentangled controls for supporting various shape generation applications, e.g., remix the topologies of 3D objects, that are not achievable by previous reconstruction works. Extensive experimental results demonstrate that our method is able to produce high-quality meshes, particularly with diverse topologies, as compared with the state-of-the-art methods.



### Decoupling Predictions in Distributed Learning for Multi-Center Left Atrial MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.05284v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05284v1)
- **Published**: 2022-06-10 08:35:42+00:00
- **Updated**: 2022-06-10 08:35:42+00:00
- **Authors**: Zheyao Gao, Lei Li, Fuping Wu, Sihan Wang, Xiahai Zhuang
- **Comment**: Accepted by MICCAI 2022
- **Journal**: None
- **Summary**: Distributed learning has shown great potential in medical image analysis. It allows to use multi-center training data with privacy protection. However, data distributions in local centers can vary from each other due to different imaging vendors, and annotation protocols. Such variation degrades the performance of learning-based methods. To mitigate the influence, two groups of methods have been proposed for different aims, i.e., the global methods and the personalized methods. The former are aimed to improve the performance of a single global model for all test data from unseen centers (known as generic data); while the latter target multiple models for each center (denoted as local data). However, little has been researched to achieve both goals simultaneously. In this work, we propose a new framework of distributed learning that bridges the gap between two groups, and improves the performance for both generic and local data. Specifically, our method decouples the predictions for generic data and local data, via distribution-conditioned adaptation matrices. Results on multi-center left atrial (LA) MRI segmentation showed that our method demonstrated superior performance over existing methods on both generic and local data. Our code is available at https://github.com/key1589745/decouple_predict



### Deep Multi-View Semi-Supervised Clustering with Sample Pairwise Constraints
- **Arxiv ID**: http://arxiv.org/abs/2206.04949v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.04949v2)
- **Published**: 2022-06-10 08:51:56+00:00
- **Updated**: 2023-05-05 06:43:34+00:00
- **Authors**: Rui Chen, Yongqiang Tang, Wensheng Zhang, Wenlong Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-view clustering has attracted much attention thanks to the capacity of multi-source information integration. Although numerous advanced methods have been proposed in past decades, most of them generally overlook the significance of weakly-supervised information and fail to preserve the feature properties of multiple views, thus resulting in unsatisfactory clustering performance. To address these issues, in this paper, we propose a novel Deep Multi-view Semi-supervised Clustering (DMSC) method, which jointly optimizes three kinds of losses during networks finetuning, including multi-view clustering loss, semi-supervised pairwise constraint loss and multiple autoencoders reconstruction loss. Specifically, a KL divergence based multi-view clustering loss is imposed on the common representation of multi-view data to perform heterogeneous feature optimization, multi-view weighting and clustering prediction simultaneously. Then, we innovatively propose to integrate pairwise constraints into the process of multi-view clustering by enforcing the learned multi-view representation of must-link samples (cannot-link samples) to be similar (dissimilar), such that the formed clustering architecture can be more credible. Moreover, unlike existing rivals that only preserve the encoders for each heterogeneous branch during networks finetuning, we further propose to tune the intact autoencoders frame that contains both encoders and decoders. In this way, the issue of serious corruption of view-specific and view-shared feature space could be alleviated, making the whole training procedure more stable. Through comprehensive experiments on eight popular image datasets, we demonstrate that our proposed approach performs better than the state-of-the-art multi-view and single-view competitors.



### Self-Supervised Deep Subspace Clustering with Entropy-norm
- **Arxiv ID**: http://arxiv.org/abs/2206.04958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.04958v1)
- **Published**: 2022-06-10 09:15:33+00:00
- **Updated**: 2022-06-10 09:15:33+00:00
- **Authors**: Guangyi Zhao, Simin Kou, Xuesong Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Auto-Encoder based deep subspace clustering (DSC) is widely used in computer vision, motion segmentation and image processing. However, it suffers from the following three issues in the self-expressive matrix learning process: the first one is less useful information for learning self-expressive weights due to the simple reconstruction loss; the second one is that the construction of the self-expression layer associated with the sample size requires high-computational cost; and the last one is the limited connectivity of the existing regularization terms. In order to address these issues, in this paper we propose a novel model named Self-Supervised deep Subspace Clustering with Entropy-norm (S$^{3}$CE). Specifically, S$^{3}$CE exploits a self-supervised contrastive network to gain a more effetive feature vector. The local structure and dense connectivity of the original data benefit from the self-expressive layer and additional entropy-norm constraint. Moreover, a new module with data enhancement is designed to help S$^{3}$CE focus on the key information of data, and improve the clustering performance of positive and negative instances through spectral clustering. Extensive experimental results demonstrate the superior performance of S$^{3}$CE in comparison to the state-of-the-art approaches.



### NR-DFERNet: Noise-Robust Network for Dynamic Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.04975v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.04975v1)
- **Published**: 2022-06-10 10:17:30+00:00
- **Updated**: 2022-06-10 10:17:30+00:00
- **Authors**: Hanting Li, Mingzhe Sui, Zhaoqing Zhu, Feng zhao
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Dynamic facial expression recognition (DFER) in the wild is an extremely challenging task, due to a large number of noisy frames in the video sequences. Previous works focus on extracting more discriminative features, but ignore distinguishing the key frames from the noisy frames. To tackle this problem, we propose a noise-robust dynamic facial expression recognition network (NR-DFERNet), which can effectively reduce the interference of noisy frames on the DFER task. Specifically, at the spatial stage, we devise a dynamic-static fusion module (DSF) that introduces dynamic features to static features for learning more discriminative spatial features. To suppress the impact of target irrelevant frames, we introduce a novel dynamic class token (DCT) for the transformer at the temporal stage. Moreover, we design a snippet-based filter (SF) at the decision stage to reduce the effect of too many neutral frames on non-neutral sequence classification. Extensive experimental results demonstrate that our NR-DFERNet outperforms the state-of-the-art methods on both the DFEW and AFEW benchmarks.



### Convolutional Layers are Equivariant to Discrete Shifts But Not Continuous Translations
- **Arxiv ID**: http://arxiv.org/abs/2206.04979v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.04979v2)
- **Published**: 2022-06-10 10:28:55+00:00
- **Updated**: 2022-07-02 22:48:41+00:00
- **Authors**: Nick McGreivy, Ammar Hakim
- **Comment**: None
- **Journal**: None
- **Summary**: The purpose of this short and simple note is to clarify a common misconception about convolutional neural networks (CNNs). CNNs are made up of convolutional layers which are shift equivariant due to weight sharing. However, convolutional layers are not translation equivariant, even when boundary effects are ignored and when pooling and subsampling are absent. This is because shift equivariance is a discrete symmetry while translation equivariance is a continuous symmetry. This fact is well known among researchers in equivariant machine learning, but is usually overlooked among non-experts. To minimize confusion, we suggest using the term `shift equivariance' to refer to discrete shifts in pixels and `translation equivariance' to refer to continuous translations.



### Positional Label for Self-Supervised Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2206.04981v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.04981v3)
- **Published**: 2022-06-10 10:29:20+00:00
- **Updated**: 2023-02-16 04:20:35+00:00
- **Authors**: Zhemin Zhang, Xun Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Positional encoding is important for vision transformer (ViT) to capture the spatial structure of the input image. General effectiveness has been proven in ViT. In our work we propose to train ViT to recognize the positional label of patches of the input image, this apparently simple task actually yields a meaningful self-supervisory task. Based on previous work on ViT positional encoding, we propose two positional labels dedicated to 2D images including absolute position and relative position. Our positional labels can be easily plugged into various current ViT variants. It can work in two ways: (a) As an auxiliary training target for vanilla ViT (e.g., ViT-B and Swin-B) for better performance. (b) Combine the self-supervised ViT (e.g., MAE) to provide a more powerful self-supervised signal for semantic feature learning. Experiments demonstrate that with the proposed self-supervised methods, ViT-B and Swin-B gain improvements of 1.20% (top-1 Acc) and 0.74% (top-1 Acc) on ImageNet, respectively, and 6.15% and 1.14% improvement on Mini-ImageNet.



### Subjective Quality Assessment for Images Generated by Computer Graphics
- **Arxiv ID**: http://arxiv.org/abs/2206.05008v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05008v1)
- **Published**: 2022-06-10 11:48:24+00:00
- **Updated**: 2022-06-10 11:48:24+00:00
- **Authors**: Tao Wang, Zicheng Zhang, Wei Sun, Xiongkuo Min, Wei Lu, Guangtao Zhai
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of rendering techniques, computer graphics generated images (CGIs) have been widely used in practical application scenarios such as architecture design, video games, simulators, movies, etc. Different from natural scene images (NSIs), the distortions of CGIs are usually caused by poor rending settings and limited computation resources. What's more, some CGIs may also suffer from compression distortions in transmission systems like cloud gaming and stream media. However, limited work has been put forward to tackle the problem of computer graphics generated images' quality assessment (CG-IQA). Therefore, in this paper, we establish a large-scale subjective CG-IQA database to deal with the challenge of CG-IQA tasks. We collect 25,454 in-the-wild CGIs through previous databases and personal collection. After data cleaning, we carefully select 1,200 CGIs to conduct the subjective experiment. Several popular no-reference image quality assessment (NR-IQA) methods are tested on our database. The experimental results show that the handcrafted-based methods achieve low correlation with subjective judgment and deep learning based methods obtain relatively better performance, which demonstrates that the current NR-IQA models are not suitable for CG-IQA tasks and more effective models are urgently needed.



### Image Generation with Multimodal Priors using Denoising Diffusion Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/2206.05039v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05039v1)
- **Published**: 2022-06-10 12:23:05+00:00
- **Updated**: 2022-06-10 12:23:05+00:00
- **Authors**: Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, Vishal M Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Image synthesis under multi-modal priors is a useful and challenging task that has received increasing attention in recent years. A major challenge in using generative models to accomplish this task is the lack of paired data containing all modalities (i.e. priors) and corresponding outputs. In recent work, a variational auto-encoder (VAE) model was trained in a weakly supervised manner to address this challenge. Since the generative power of VAEs is usually limited, it is difficult for this method to synthesize images belonging to complex distributions. To this end, we propose a solution based on a denoising diffusion probabilistic models to synthesise images under multi-model priors. Based on the fact that the distribution over each time step in the diffusion model is Gaussian, in this work we show that there exists a closed-form expression to the generate the image corresponds to the given modalities. The proposed solution does not require explicit retraining for all modalities and can leverage the outputs of individual modalities to generate realistic images according to different constraints. We conduct studies on two real-world datasets to demonstrate the effectiveness of our approach



### From Labels to Priors in Capsule Endoscopy: A Prior Guided Approach for Improving Generalization with Few Labels
- **Arxiv ID**: http://arxiv.org/abs/2206.05288v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05288v1)
- **Published**: 2022-06-10 12:35:49+00:00
- **Updated**: 2022-06-10 12:35:49+00:00
- **Authors**: Anuja Vats, Ahmed Mohammed, Marius Pedersen
- **Comment**: None
- **Journal**: None
- **Summary**: The lack of generalizability of deep learning approaches for the automated diagnosis of pathologies in Wireless Capsule Endoscopy (WCE) has prevented any significant advantages from trickling down to real clinical practices. As a result, disease management using WCE continues to depend on exhaustive manual investigations by medical experts. This explains its limited use despite several advantages. Prior works have considered using higher quality and quantity of labels as a way of tackling the lack of generalization, however this is hardly scalable considering pathology diversity not to mention that labeling large datasets encumbers the medical staff additionally. We propose using freely available domain knowledge as priors to learn more robust and generalizable representations. We experimentally show that domain priors can benefit representations by acting in proxy of labels, thereby significantly reducing the labeling requirement while still enabling fully unsupervised yet pathology-aware learning. We use the contrastive objective along with prior-guided views during pretraining, where the view choices inspire sensitivity to pathological information. Extensive experiments on three datasets show that our method performs better than (or closes gap with) the state-of-the-art in the domain, establishing a new benchmark in pathology classification and cross-dataset generalization, as well as scaling to unseen pathology categories.



### Learning self-calibrated optic disc and cup segmentation from multi-rater annotations
- **Arxiv ID**: http://arxiv.org/abs/2206.05092v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05092v2)
- **Published**: 2022-06-10 13:35:07+00:00
- **Updated**: 2022-06-14 06:32:11+00:00
- **Authors**: Junde Wu, Huihui Fang, Fangxin Shang, Zhaowei Wang, Dalu Yang, Wenshuo Zhou, Yehui Yang, Yanwu Xu
- **Comment**: None
- **Journal**: None
- **Summary**: The segmentation of optic disc(OD) and optic cup(OC) from fundus images is an important fundamental task for glaucoma diagnosis. In the clinical practice, it is often necessary to collect opinions from multiple experts to obtain the final OD/OC annotation. This clinical routine helps to mitigate the individual bias. But when data is multiply annotated, standard deep learning models will be inapplicable. In this paper, we propose a novel neural network framework to learn OD/OC segmentation from multi-rater annotations. The segmentation results are self-calibrated through the iterative optimization of multi-rater expertness estimation and calibrated OD/OC segmentation. In this way, the proposed method can realize a mutual improvement of both tasks and finally obtain a refined segmentation result. Specifically, we propose Diverging Model(DivM) and Converging Model(ConM) to process the two tasks respectively. ConM segments the raw image based on the multi-rater expertness map provided by DivM. DivM generates multi-rater expertness map from the segmentation mask provided by ConM. The experiment results show that by recurrently running ConM and DivM, the results can be self-calibrated so as to outperform a range of state-of-the-art(SOTA) multi-rater segmentation methods.



### Federated Momentum Contrastive Clustering
- **Arxiv ID**: http://arxiv.org/abs/2206.05093v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2206.05093v1)
- **Published**: 2022-06-10 13:37:15+00:00
- **Updated**: 2022-06-10 13:37:15+00:00
- **Authors**: Runxuan Miao, Erdem Koyuncu
- **Comment**: Originally submitted March 2022
- **Journal**: None
- **Summary**: We present federated momentum contrastive clustering (FedMCC), a learning framework that can not only extract discriminative representations over distributed local data but also perform data clustering. In FedMCC, a transformed data pair passes through both the online and target networks, resulting in four representations over which the losses are determined. The resulting high-quality representations generated by FedMCC can outperform several existing self-supervised learning methods for linear evaluation and semi-supervised learning tasks. FedMCC can easily be adapted to ordinary centralized clustering through what we call momentum contrastive clustering (MCC). We show that MCC achieves state-of-the-art clustering accuracy results in certain datasets such as STL-10 and ImageNet-10. We also present a method to reduce the memory footprint of our clustering schemes.



### Saccade Mechanisms for Image Classification, Object Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2206.05102v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2206.05102v1)
- **Published**: 2022-06-10 13:50:34+00:00
- **Updated**: 2022-06-10 13:50:34+00:00
- **Authors**: Saurabh Farkya, Zachary Daniels, Aswin Nadamuni Raghavan, David Zhang, Michael Piacentino
- **Comment**: 4 Pages, 6 figures, will be presented at CVPR2022-NeuroVision
  workshop as a Lightning talk
- **Journal**: None
- **Summary**: We examine how the saccade mechanism from biological vision can be used to make deep neural networks more efficient for classification and object detection problems. Our proposed approach is based on the ideas of attention-driven visual processing and saccades, miniature eye movements influenced by attention. We conduct experiments by analyzing: i) the robustness of different deep neural network (DNN) feature extractors to partially-sensed images for image classification and object detection, and ii) the utility of saccades in masking image patches for image classification and object tracking. Experiments with convolutional nets (ResNet-18) and transformer-based models (ViT, DETR, TransTrack) are conducted on several datasets (CIFAR-10, DAVSOD, MSCOCO, and MOT17). Our experiments show intelligent data reduction via learning to mimic human saccades when used in conjunction with state-of-the-art DNNs for classification, detection, and tracking tasks. We observed minimal drop in performance for the classification and detection tasks while only using about 30\% of the original sensor data. We discuss how the saccade mechanism can inform hardware design via ``in-pixel'' processing.



### Globally-Optimal Contrast Maximisation for Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/2206.05127v1
- **DOI**: 10.1109/TPAMI.2021.3053243
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05127v1)
- **Published**: 2022-06-10 14:06:46+00:00
- **Updated**: 2022-06-10 14:06:46+00:00
- **Authors**: Xin Peng, Ling Gao, Yifu Wang, Laurent Kneip
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2203.03914
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2022
- **Summary**: Event cameras are bio-inspired sensors that perform well in challenging illumination conditions and have high temporal resolution. However, their concept is fundamentally different from traditional frame-based cameras. The pixels of an event camera operate independently and asynchronously. They measure changes of the logarithmic brightness and return them in the highly discretised form of time-stamped events indicating a relative change of a certain quantity since the last event. New models and algorithms are needed to process this kind of measurements. The present work looks at several motion estimation problems with event cameras. The flow of the events is modelled by a general homographic warping in a space-time volume, and the objective is formulated as a maximisation of contrast within the image of warped events. Our core contribution consists of deriving globally optimal solutions to these generally non-convex problems, which removes the dependency on a good initial guess plaguing existing methods. Our methods rely on branch-and-bound optimisation and employ novel and efficient, recursive upper and lower bounds derived for six different contrast estimation functions. The practical validity of our approach is demonstrated by a successful application to three different event camera motion estimation problems.



### Real-time Hyper-Dimensional Reconfiguration at the Edge using Hardware Accelerators
- **Arxiv ID**: http://arxiv.org/abs/2206.05128v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/2206.05128v1)
- **Published**: 2022-06-10 14:08:41+00:00
- **Updated**: 2022-06-10 14:08:41+00:00
- **Authors**: Indhumathi Kandaswamy, Saurabh Farkya, Zachary Daniels, Gooitzen van der Wal, Aswin Raghavan, Yuzheng Zhang, Jun Hu, Michael Lomnitz, Michael Isnardi, David Zhang, Michael Piacentino
- **Comment**: 9 pages, 15 figures. Will be presented in Embedded Vision Workshop at
  CVPR2022
- **Journal**: None
- **Summary**: In this paper we present Hyper-Dimensional Reconfigurable Analytics at the Tactical Edge (HyDRATE) using low-SWaP embedded hardware that can perform real-time reconfiguration at the edge leveraging non-MAC (free of floating-point MultiplyACcumulate operations) deep neural nets (DNN) combined with hyperdimensional (HD) computing accelerators. We describe the algorithm, trained quantized model generation, and simulated performance of a feature extractor free of multiply-accumulates feeding a hyperdimensional logic-based classifier. Then we show how performance increases with the number of hyperdimensions. We describe the realized low-SWaP FPGA hardware and embedded software system compared to traditional DNNs and detail the implemented hardware accelerators. We discuss the measured system latency and power, noise robustness due to use of learnable quantization and HD computing, actual versus simulated system performance for a video activity classification task and demonstration of reconfiguration on this same dataset. We show that reconfigurability in the field is achieved by retraining only the feed-forward HD classifier without gradient descent backpropagation (gradient-free), using few-shot learning of new classes at the edge. Initial work performed used LRCN DNN and is currently extended to use Two-stream DNN with improved performance.



### Weakly-supervised segmentation using inherently-explainable classification models and their application to brain tumour classification
- **Arxiv ID**: http://arxiv.org/abs/2206.05148v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05148v1)
- **Published**: 2022-06-10 14:44:05+00:00
- **Updated**: 2022-06-10 14:44:05+00:00
- **Authors**: Soumick Chatterjee, Hadya Yassin, Florian Dubost, Andreas Nürnberger, Oliver Speck
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have shown their potential for several applications. However, most of the models are opaque and difficult to trust due to their complex reasoning - commonly known as the black-box problem. Some fields, such as medicine, require a high degree of transparency to accept and adopt such technologies. Consequently, creating explainable/interpretable models or applying post-hoc methods on classifiers to build trust in deep learning models are required. Moreover, deep learning methods can be used for segmentation tasks, which typically require hard-to-obtain, time-consuming manually-annotated segmentation labels for training. This paper introduces three inherently-explainable classifiers to tackle both of these problems as one. The localisation heatmaps provided by the networks -- representing the models' focus areas and being used in classification decision-making -- can be directly interpreted, without requiring any post-hoc methods to derive information for model explanation. The models are trained by using the input image and only the classification labels as ground-truth in a supervised fashion - without using any information about the location of the region of interest (i.e. the segmentation labels), making the segmentation training of the models weakly-supervised through classification labels. The final segmentation is obtained by thresholding these heatmaps. The models were employed for the task of multi-class brain tumour classification using two different datasets, resulting in the best F1-score of 0.93 for the supervised classification task while securing a median Dice score of 0.67$\pm$0.08 for the weakly-supervised segmentation task. Furthermore, the obtained accuracy on a subset of tumour-only images outperformed the state-of-the-art glioma tumour grading binary classifiers with the best model achieving 98.7\% accuracy.



### Referring Image Matting
- **Arxiv ID**: http://arxiv.org/abs/2206.05149v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.05149v3)
- **Published**: 2022-06-10 14:44:43+00:00
- **Updated**: 2023-03-22 03:47:41+00:00
- **Authors**: Jizhizi Li, Jing Zhang, Dacheng Tao
- **Comment**: Accepted to CVPR2023. The dataset, code and models are available at
  https://github.com/JizhiziLi/RIM
- **Journal**: None
- **Summary**: Different from conventional image matting, which either requires user-defined scribbles/trimap to extract a specific foreground object or directly extracts all the foreground objects in the image indiscriminately, we introduce a new task named Referring Image Matting (RIM) in this paper, which aims to extract the meticulous alpha matte of the specific object that best matches the given natural language description, thus enabling a more natural and simpler instruction for image matting. First, we establish a large-scale challenging dataset RefMatte by designing a comprehensive image composition and expression generation engine to automatically produce high-quality images along with diverse text attributes based on public datasets. RefMatte consists of 230 object categories, 47,500 images, 118,749 expression-region entities, and 474,996 expressions. Additionally, we construct a real-world test set with 100 high-resolution natural images and manually annotate complex phrases to evaluate the out-of-domain generalization abilities of RIM methods. Furthermore, we present a novel baseline method CLIPMat for RIM, including a context-embedded prompt, a text-driven semantic pop-up, and a multi-level details extractor. Extensive experiments on RefMatte in both keyword and expression settings validate the superiority of CLIPMat over representative methods. We hope this work could provide novel insights into image matting and encourage more follow-up studies. The dataset, code and models are available at https://github.com/JizhiziLi/RIM.



### MEAT: Maneuver Extraction from Agent Trajectories
- **Arxiv ID**: http://arxiv.org/abs/2206.05158v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.05158v1)
- **Published**: 2022-06-10 14:56:32+00:00
- **Updated**: 2022-06-10 14:56:32+00:00
- **Authors**: Julian Schmidt, Julian Jordan, David Raba, Tobias Welz, Klaus Dietmayer
- **Comment**: Accepted at IEEE Intelligent Vehicles Symposium (IV) 2022 2nd
  Workshop on Autonomy@Scale
- **Journal**: None
- **Summary**: Advances in learning-based trajectory prediction are enabled by large-scale datasets. However, in-depth analysis of such datasets is limited. Moreover, the evaluation of prediction models is limited to metrics averaged over all samples in the dataset. We propose an automated methodology that allows to extract maneuvers (e.g., left turn, lane change) from agent trajectories in such datasets. The methodology considers information about the agent dynamics and information about the lane segments the agent traveled along. Although it is possible to use the resulting maneuvers for training classification networks, we exemplary use them for extensive trajectory dataset analysis and maneuver-specific evaluation of multiple state-of-the-art trajectory prediction models. Additionally, an analysis of the datasets and an evaluation of the prediction models based on the agent dynamics is provided.



### An Image Processing Pipeline for Camera Trap Time-Lapse Recordings
- **Arxiv ID**: http://arxiv.org/abs/2206.05159v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, J.3
- **Links**: [PDF](http://arxiv.org/pdf/2206.05159v1)
- **Published**: 2022-06-10 14:58:47+00:00
- **Updated**: 2022-06-10 14:58:47+00:00
- **Authors**: Michael L. Hilton, Mark T. Yamane, Leah M. Knezevich
- **Comment**: 5 pages, 2 figures, presented at the CV4Animals workshop of CVIP2022
- **Journal**: None
- **Summary**: A new open-source image processing pipeline for analyzing camera trap time-lapse recordings is described. This pipeline includes machine learning models to assist human-in-the-loop video segmentation and animal re-identification. We present some performance results and observations on the utility of this pipeline after using it in a year-long project studying the spatial ecology and social behavior of the gopher tortoise.



### Localized adversarial artifacts for compressed sensing MRI
- **Arxiv ID**: http://arxiv.org/abs/2206.05289v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05289v1)
- **Published**: 2022-06-10 15:20:52+00:00
- **Updated**: 2022-06-10 15:20:52+00:00
- **Authors**: Rima Alaifari, Giovanni S. Alberti, Tandri Gauksson
- **Comment**: 14 pages, 7 figures
- **Journal**: None
- **Summary**: As interest in deep neural networks (DNNs) for image reconstruction tasks grows, their reliability has been called into question (Antun et al., 2020; Gottschling et al., 2020). However, recent work has shown that compared to total variation (TV) minimization, they show similar robustness to adversarial noise in terms of $\ell^2$-reconstruction error (Genzel et al., 2022). We consider a different notion of robustness, using the $\ell^\infty$-norm, and argue that localized reconstruction artifacts are a more relevant defect than the $\ell^2$-error. We create adversarial perturbations to undersampled MRI measurements which induce severe localized artifacts in the TV-regularized reconstruction. The same attack method is not as effective against DNN based reconstruction. Finally, we show that this phenomenon is inherent to reconstruction methods for which exact recovery can be guaranteed, as with compressed sensing reconstructions with $\ell^1$- or TV-minimization.



### SERE: Exploring Feature Self-relation for Self-supervised Transformer
- **Arxiv ID**: http://arxiv.org/abs/2206.05184v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05184v2)
- **Published**: 2022-06-10 15:25:00+00:00
- **Updated**: 2023-08-23 01:14:30+00:00
- **Authors**: Zhong-Yu Li, Shanghua Gao, Ming-Ming Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Learning representations with self-supervision for convolutional networks (CNN) has been validated to be effective for vision tasks. As an alternative to CNN, vision transformers (ViT) have strong representation ability with spatial self-attention and channel-level feedforward networks. Recent works reveal that self-supervised learning helps unleash the great potential of ViT. Still, most works follow self-supervised strategies designed for CNN, e.g., instance-level discrimination of samples, but they ignore the properties of ViT. We observe that relational modeling on spatial and channel dimensions distinguishes ViT from other networks. To enforce this property, we explore the feature SElf-RElation (SERE) for training self-supervised ViT. Specifically, instead of conducting self-supervised learning solely on feature embeddings from multiple views, we utilize the feature self-relations, i.e., spatial/channel self-relations, for self-supervised learning. Self-relation based learning further enhances the relation modeling ability of ViT, resulting in stronger representations that stably improve performance on multiple downstream tasks. Our source code will be made publicly available.



### Learning the Space of Deep Models
- **Arxiv ID**: http://arxiv.org/abs/2206.05194v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05194v1)
- **Published**: 2022-06-10 15:53:35+00:00
- **Updated**: 2022-06-10 15:53:35+00:00
- **Authors**: Gianluca Berardi, Luca De Luigi, Samuele Salti, Luigi Di Stefano
- **Comment**: Accepted at ICPR2022
- **Journal**: None
- **Summary**: Embedding of large but redundant data, such as images or text, in a hierarchy of lower-dimensional spaces is one of the key features of representation learning approaches, which nowadays provide state-of-the-art solutions to problems once believed hard or impossible to solve. In this work, in a plot twist with a strong meta aftertaste, we show how trained deep models are as redundant as the data they are optimized to process, and how it is therefore possible to use deep learning models to embed deep learning models. In particular, we show that it is possible to use representation learning to learn a fixed-size, low-dimensional embedding space of trained deep models and that such space can be explored by interpolation or optimization to attain ready-to-use models. We find that it is possible to learn an embedding space of multiple instances of the same architecture and of multiple architectures. We address image classification and neural representation of signals, showing how our embedding space can be learnt so as to capture the notions of performance and 3D shape, respectively. In the Multi-Architecture setting we also show how an embedding trained only on a subset of architectures can learn to generate already-trained instances of architectures it never sees instantiated at training time.



### ProActive: Self-Attentive Temporal Point Process Flows for Activity Sequences
- **Arxiv ID**: http://arxiv.org/abs/2206.05291v1
- **DOI**: 10.1145/3534678.3539477
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05291v1)
- **Published**: 2022-06-10 16:30:55+00:00
- **Updated**: 2022-06-10 16:30:55+00:00
- **Authors**: Vinayak Gupta, Srikanta Bedathur
- **Comment**: KDD 2022
- **Journal**: None
- **Summary**: Any human activity can be represented as a temporal sequence of actions performed to achieve a certain goal. Unlike machine-made time series, these action sequences are highly disparate as the time taken to finish a similar action might vary between different persons. Therefore, understanding the dynamics of these sequences is essential for many downstream tasks such as activity length prediction, goal prediction, etc. Existing neural approaches that model an activity sequence are either limited to visual data or are task specific, i.e., limited to next action or goal prediction. In this paper, we present ProActive, a neural marked temporal point process (MTPP) framework for modeling the continuous-time distribution of actions in an activity sequence while simultaneously addressing three high-impact problems -- next action prediction, sequence-goal prediction, and end-to-end sequence generation. Specifically, we utilize a self-attention module with temporal normalizing flows to model the influence and the inter-arrival times between actions in a sequence. Moreover, for time-sensitive prediction, we perform an early detection of sequence goal via a constrained margin-based optimization procedure. This in-turn allows ProActive to predict the sequence goal using a limited number of actions. Extensive experiments on sequences derived from three activity recognition datasets show the significant accuracy boost of ProActive over the state-of-the-art in terms of action and goal prediction, and the first-ever application of end-to-end action sequence generation.



### ClamNet: Using contrastive learning with variable depth Unets for medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2206.05225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05225v1)
- **Published**: 2022-06-10 16:55:45+00:00
- **Updated**: 2022-06-10 16:55:45+00:00
- **Authors**: Samayan Bhattacharya, Sk Shahnawaz, Avigyan Bhattacharya
- **Comment**: None
- **Journal**: None
- **Summary**: Unets have become the standard method for semantic segmentation of medical images, along with fully convolutional networks (FCN). Unet++ was introduced as a variant of Unet, in order to solve some of the problems facing Unet and FCNs. Unet++ provided networks with an ensemble of variable depth Unets, hence eliminating the need for professionals estimating the best suitable depth for a task. While Unet and all its variants, including Unet++ aimed at providing networks that were able to train well without requiring large quantities of annotated data, none of them attempted to eliminate the need for pixel-wise annotated data altogether. Obtaining such data for each disease to be diagnosed comes at a high cost. Hence such data is scarce. In this paper we use contrastive learning to train Unet++ for semantic segmentation of medical images using medical images from various sources including magnetic resonance imaging (MRI) and computed tomography (CT), without the need for pixel-wise annotations. Here we describe the architecture of the proposed model and the training method used. This is still a work in progress and so we abstain from including results in this paper. The results and the trained model would be made available upon publication or in subsequent versions of this paper on arxiv.



### Optical Diffraction Tomography based on 3D Physics-Inspired Neural Network (PINN)
- **Arxiv ID**: http://arxiv.org/abs/2206.05236v1
- **DOI**: None
- **Categories**: **physics.optics**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05236v1)
- **Published**: 2022-06-10 17:19:04+00:00
- **Updated**: 2022-06-10 17:19:04+00:00
- **Authors**: Ahmed B. Ayoub, Amirhossein Saba, Carlo Gigli, Demetri Psaltis
- **Comment**: None
- **Journal**: None
- **Summary**: Optical diffraction tomography (ODT) is an emerging 3D imaging technique that is used for the 3D reconstruction of the refractive index (RI) for semi-transparent samples. Various inverse models have been proposed to reconstruct the 3D RI based on the holographic detection of different samples such as the Born and the Rytov approximations. However, such approximations usually suffer from the so-called missing-cone problem that results in an elongation of the final reconstruction along the optical axis. Different iterative schemes have been proposed to solve the missing cone problem relying on physical forward models and an error function that aims at filling in the k-space and thus eliminating the missing-cone problem and reaching better reconstruction accuracy. In this paper, we propose a different approach where a 3D neural network (NN) is employed. The NN is trained with a cost function derived from a physical model based on the physics of optical wave propagation. The 3D NN starts with an initial guess for the 3D RI reconstruction (i.e. Born, or Rytov) and aims at reconstructing better 3D reconstruction based on an error function. With this technique, the NN can be trained without any examples of the relation between the ill-posed reconstruction (Born or Rytov) and the ground truth (true shape).



### Lost in Transmission: On the Impact of Networking Corruptions on Video Machine Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2206.05252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05252v1)
- **Published**: 2022-06-10 17:50:50+00:00
- **Updated**: 2022-06-10 17:50:50+00:00
- **Authors**: Trenton Chang, Daniel Y. Fu
- **Comment**: 12 pages, 12 figures (with supplemental: 34 pages)
- **Journal**: None
- **Summary**: We study how networking corruptions--data corruptions caused by networking errors--affect video machine learning (ML) models. We discover apparent networking corruptions in Kinetics-400, a benchmark video ML dataset. In a simulation study, we investigate (1) what artifacts networking corruptions cause, (2) how such artifacts affect ML models, and (3) whether standard robustness methods can mitigate their negative effects. We find that networking corruptions cause visual and temporal artifacts (i.e., smeared colors or frame drops). These networking corruptions degrade performance on a variety of video ML tasks, but effects vary by task and dataset, depending on how much temporal context the tasks require. Lastly, we evaluate data augmentation--a standard defense for data corruptions--but find that it does not recover performance.



### Rethinking Spatial Invariance of Convolutional Networks for Object Counting
- **Arxiv ID**: http://arxiv.org/abs/2206.05253v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2206.05253v2)
- **Published**: 2022-06-10 17:51:25+00:00
- **Updated**: 2022-08-18 14:43:06+00:00
- **Authors**: Zhi-Qi Cheng, Qi Dai, Hong Li, JingKuan Song, Xiao Wu, Alexander G. Hauptmann
- **Comment**: Accepted to CVPR 2022, Code:
  https://github.com/zhiqic/Rethinking-Counting
- **Journal**: None
- **Summary**: Previous work generally believes that improving the spatial invariance of convolutional networks is the key to object counting. However, after verifying several mainstream counting networks, we surprisingly found too strict pixel-level spatial invariance would cause overfit noise in the density map generation. In this paper, we try to use locally connected Gaussian kernels to replace the original convolution filter to estimate the spatial position in the density map. The purpose of this is to allow the feature extraction process to potentially stimulate the density map generation process to overcome the annotation noise. Inspired by previous work, we propose a low-rank approximation accompanied with translation invariance to favorably implement the approximation of massive Gaussian convolution. Our work points a new direction for follow-up research, which should investigate how to properly relax the overly strict pixel-level spatial invariance for object counting. We evaluate our methods on 4 mainstream object counting networks (i.e., MCNN, CSRNet, SANet, and ResNet-50). Extensive experiments were conducted on 7 popular benchmarks for 3 applications (i.e., crowd, vehicle, and plant counting). Experimental results show that our methods significantly outperform other state-of-the-art methods and achieve promising learning of the spatial position of objects.



### Explaining Image Classifiers Using Contrastive Counterfactuals in Generative Latent Spaces
- **Arxiv ID**: http://arxiv.org/abs/2206.05257v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.05257v1)
- **Published**: 2022-06-10 17:54:46+00:00
- **Updated**: 2022-06-10 17:54:46+00:00
- **Authors**: Kamran Alipour, Aditya Lahiri, Ehsan Adeli, Babak Salimi, Michael Pazzani
- **Comment**: None
- **Journal**: None
- **Summary**: Despite their high accuracies, modern complex image classifiers cannot be trusted for sensitive tasks due to their unknown decision-making process and potential biases. Counterfactual explanations are very effective in providing transparency for these black-box algorithms. Nevertheless, generating counterfactuals that can have a consistent impact on classifier outputs and yet expose interpretable feature changes is a very challenging task. We introduce a novel method to generate causal and yet interpretable counterfactual explanations for image classifiers using pretrained generative models without any re-training or conditioning. The generative models in this technique are not bound to be trained on the same data as the target classifier. We use this framework to obtain contrastive and causal sufficiency and necessity scores as global explanations for black-box classifiers. On the task of face attribute classification, we show how different attributes influence the classifier output by providing both causal and contrastive feature attributions, and the corresponding counterfactual images.



### Is Self-Supervised Learning More Robust Than Supervised Learning?
- **Arxiv ID**: http://arxiv.org/abs/2206.05259v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05259v1)
- **Published**: 2022-06-10 17:58:00+00:00
- **Updated**: 2022-06-10 17:58:00+00:00
- **Authors**: Yuanyi Zhong, Haoran Tang, Junkun Chen, Jian Peng, Yu-Xiong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised contrastive learning is a powerful tool to learn visual representation without labels. Prior work has primarily focused on evaluating the recognition accuracy of various pre-training algorithms, but has overlooked other behavioral aspects. In addition to accuracy, distributional robustness plays a critical role in the reliability of machine learning models. We design and conduct a series of robustness tests to quantify the behavioral differences between contrastive learning and supervised learning to downstream or pre-training data distribution changes. These tests leverage data corruptions at multiple levels, ranging from pixel-level gamma distortion to patch-level shuffling and to dataset-level distribution shift. Our tests unveil intriguing robustness behaviors of contrastive and supervised learning. On the one hand, under downstream corruptions, we generally observe that contrastive learning is surprisingly more robust than supervised learning. On the other hand, under pre-training corruptions, we find contrastive learning vulnerable to patch shuffling and pixel intensity change, yet less sensitive to dataset-level distribution change. We attempt to explain these results through the role of data augmentation and feature space properties. Our insight has implications in improving the downstream robustness of supervised learning.



### Balanced Product of Calibrated Experts for Long-Tailed Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.05260v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05260v3)
- **Published**: 2022-06-10 17:59:02+00:00
- **Updated**: 2023-06-07 17:52:01+00:00
- **Authors**: Emanuel Sanchez Aimar, Arvi Jonnarth, Michael Felsberg, Marco Kuhlmann
- **Comment**: Accepted at CVPR 2023, 19 pages
- **Journal**: None
- **Summary**: Many real-world recognition problems are characterized by long-tailed label distributions. These distributions make representation learning highly challenging due to limited generalization over the tail classes. If the test distribution differs from the training distribution, e.g. uniform versus long-tailed, the problem of the distribution shift needs to be addressed. A recent line of work proposes learning multiple diverse experts to tackle this issue. Ensemble diversity is encouraged by various techniques, e.g. by specializing different experts in the head and the tail classes. In this work, we take an analytical approach and extend the notion of logit adjustment to ensembles to form a Balanced Product of Experts (BalPoE). BalPoE combines a family of experts with different test-time target distributions, generalizing several previous approaches. We show how to properly define these distributions and combine the experts in order to achieve unbiased predictions, by proving that the ensemble is Fisher-consistent for minimizing the balanced error. Our theoretical analysis shows that our balanced ensemble requires calibrated experts, which we achieve in practice using mixup. We conduct extensive experiments and our method obtains new state-of-the-art results on three long-tailed datasets: CIFAR-100-LT, ImageNet-LT, and iNaturalist-2018. Our code is available at https://github.com/emasa/BalPoE-CalibratedLT.



### Causal Balancing for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2206.05263v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05263v4)
- **Published**: 2022-06-10 17:59:11+00:00
- **Updated**: 2023-02-19 23:47:57+00:00
- **Authors**: Xinyi Wang, Michael Saxon, Jiachen Li, Hongyang Zhang, Kun Zhang, William Yang Wang
- **Comment**: Published at ICLR 2023
- **Journal**: None
- **Summary**: While machine learning models rapidly advance the state-of-the-art on various real-world tasks, out-of-domain (OOD) generalization remains a challenging problem given the vulnerability of these models to spurious correlations. We propose a balanced mini-batch sampling strategy to transform a biased data distribution into a spurious-free balanced distribution, based on the invariance of the underlying causal mechanisms for the data generation process. We argue that the Bayes optimal classifiers trained on such balanced distribution are minimax optimal across a diverse enough environment space. We also provide an identifiability guarantee of the latent variable model of the proposed data generation process, when utilizing enough train environments. Experiments are conducted on DomainBed, demonstrating empirically that our method obtains the best performance across 20 baselines reported on the benchmark.



### Does Self-supervised Learning Really Improve Reinforcement Learning from Pixels?
- **Arxiv ID**: http://arxiv.org/abs/2206.05266v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2206.05266v4)
- **Published**: 2022-06-10 17:59:30+00:00
- **Updated**: 2023-01-13 21:14:45+00:00
- **Authors**: Xiang Li, Jinghuan Shang, Srijan Das, Michael S. Ryoo
- **Comment**: NeurIPS 2022. Code for ELo-SACv3 is at
  https://github.com/LostXine/elo-sac and code for ELo-Rainbow is at
  https://github.com/LostXine/elo-rainbow
- **Journal**: None
- **Summary**: We investigate whether self-supervised learning (SSL) can improve online reinforcement learning (RL) from pixels. We extend the contrastive reinforcement learning framework (e.g., CURL) that jointly optimizes SSL and RL losses and conduct an extensive amount of experiments with various self-supervised losses. Our observations suggest that the existing SSL framework for RL fails to bring meaningful improvement over the baselines only taking advantage of image augmentation when the same amount of data and augmentation is used. We further perform evolutionary searches to find the optimal combination of multiple self-supervised losses for RL, but find that even such a loss combination fails to meaningfully outperform the methods that only utilize carefully designed image augmentations. After evaluating these approaches together in multiple different environments including a real-world robot environment, we confirm that no single self-supervised loss or image augmentation method can dominate all environments and that the current framework for joint optimization of SSL and RL is limited. Finally, we conduct the ablation study on multiple factors and demonstrate the properties of representations learned with different approaches.



### EigenFairing: 3D Model Fairing using Image Coherence
- **Arxiv ID**: http://arxiv.org/abs/2206.05309v1
- **DOI**: 10.5244/C.18.4
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2206.05309v1)
- **Published**: 2022-06-10 18:13:19+00:00
- **Updated**: 2022-06-10 18:13:19+00:00
- **Authors**: Pragyana Mishra, Omead Amidi, Takeo Kanade
- **Comment**: British Machine Vision Conference, BMVC 2004, Kingston, UK, September
  7-9, 2004
- **Journal**: Proceedings of the British Machine Conference, pages 1-10, BMVA
  Press, September 2004
- **Summary**: A surface is often modeled as a triangulated mesh of 3D points and textures associated with faces of the mesh. The 3D points could be either sampled from range data or derived from a set of images using a stereo or Structure-from-Motion algorithm. When the points do not lie at critical points of maximum curvature or discontinuities of the real surface, faces of the mesh do not lie close to the modeled surface. This results in textural artifacts, and the model is not perfectly coherent with a set of actual images -- the ones that are used to texture-map its mesh. This paper presents a technique for perfecting the 3D surface model by repositioning its vertices so that it is coherent with a set of observed images of the object. The textural artifacts and incoherence with images are due to the non-planarity of a surface patch being approximated by a planar face, as observed from multiple viewpoints. Image areas from the viewpoints are used to represent texture for the patch in Eigenspace. The Eigenspace representation captures variations of texture, which we seek to minimize. A coherence measure based on the difference between the face textures reconstructed from Eigenspace and the actual images is used to reposition the vertices so that the model is improved or faired. We refer to this technique of model refinement as EigenFairing, by which the model is faired, both geometrically and texturally, to better approximate the real surface.



### Object Instance Identification in Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/2206.05319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05319v1)
- **Published**: 2022-06-10 18:38:10+00:00
- **Updated**: 2022-06-10 18:38:10+00:00
- **Authors**: Takuma Yagi, Md Tasnimul Hasan, Yoichi Sato
- **Comment**: Joint 1st Ego4D and 10th EPIC Workshop (EPIC@CVPR2022) Extended
  Abstract
- **Journal**: None
- **Summary**: We study the problem of identifying object instances in a dynamic environment where people interact with the objects. In such an environment, objects' appearance changes dynamically by interaction with other entities, occlusion by hands, background change, etc. This leads to a larger intra-instance variation of appearance than in static environments. To discover the challenges in this setting, we newly built a benchmark of more than 1,500 instances built on the EPIC-KITCHENS dataset which includes natural activities and conducted an extensive analysis of it. Experimental results suggest that (i) robustness against instance-specific appearance change (ii) integration of low-level (e.g., color, texture) and high-level (e.g., object category) features (iii) foreground feature selection on overlapping objects are required for further improvement.



### Memory Classifiers: Two-stage Classification for Robustness in Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.05323v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05323v2)
- **Published**: 2022-06-10 18:44:45+00:00
- **Updated**: 2022-08-29 19:49:49+00:00
- **Authors**: Souradeep Dutta, Yahan Yang, Elena Bernardis, Edgar Dobriban, Insup Lee
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of machine learning models can significantly degrade under distribution shifts of the data. We propose a new method for classification which can improve robustness to distribution shifts, by combining expert knowledge about the ``high-level" structure of the data with standard classifiers. Specifically, we introduce two-stage classifiers called memory classifiers. First, these identify prototypical data points -- memories -- to cluster the training data. This step is based on features designed with expert guidance; for instance, for image data they can be extracted using digital image processing algorithms. Then, within each cluster, we learn local classifiers based on finer discriminating features, via standard models like deep neural networks. We establish generalization bounds for memory classifiers. We illustrate in experiments that they can improve generalization and robustness to distribution shifts on image datasets. We show improvements which push beyond standard data augmentation techniques.



### Differentiable Rendering of Neural SDFs through Reparameterization
- **Arxiv ID**: http://arxiv.org/abs/2206.05344v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.05344v1)
- **Published**: 2022-06-10 20:30:26+00:00
- **Updated**: 2022-06-10 20:30:26+00:00
- **Authors**: Sai Praveen Bangaru, Michaël Gharbi, Tzu-Mao Li, Fujun Luan, Kalyan Sunkavalli, Miloš Hašan, Sai Bi, Zexiang Xu, Gilbert Bernstein, Frédo Durand
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method to automatically compute correct gradients with respect to geometric scene parameters in neural SDF renderers. Recent physically-based differentiable rendering techniques for meshes have used edge-sampling to handle discontinuities, particularly at object silhouettes, but SDFs do not have a simple parametric form amenable to sampling. Instead, our approach builds on area-sampling techniques and develops a continuous warping function for SDFs to account for these discontinuities. Our method leverages the distance to surface encoded in an SDF and uses quadrature on sphere tracer points to compute this warping function. We further show that this can be done by subsampling the points to make the method tractable for neural SDFs. Our differentiable renderer can be used to optimize neural shapes from multi-view images and produces comparable 3D reconstructions to recent SDF-based inverse rendering methods, without the need for 2D segmentation masks to guide the geometry optimization and no volumetric approximations to the geometry.



### Object Detection, Recognition, Deep Learning, and the Universal Law of Generalization
- **Arxiv ID**: http://arxiv.org/abs/2206.05365v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2206.05365v1)
- **Published**: 2022-06-10 22:26:29+00:00
- **Updated**: 2022-06-10 22:26:29+00:00
- **Authors**: Faris B. Rustom, Haluk Öğmen, Arash Yazdanbakhsh
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection and recognition are fundamental functions underlying the success of species. Because the appearance of an object exhibits a large variability, the brain has to group these different stimuli under the same object identity, a process of generalization. Does the process of generalization follow some general principles or is it an ad-hoc "bag-of-tricks"? The Universal Law of Generalization provided evidence that generalization follows similar properties across a variety of species and tasks. Here we test the hypothesis that the internal representations underlying generalization reflect the natural properties of object detection and recognition in our environment rather than the specifics of the system solving these problems. By training a deep-neural-network with images of "clear" and "camouflaged" animals, we found that with a proper choice of category prototypes, the generalization functions are monotone decreasing, similar to the generalization functions of biological systems. Our findings support the hypothesis of the study.



### Generalizable Neural Radiance Fields for Novel View Synthesis with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2206.05375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.05375v1)
- **Published**: 2022-06-10 23:16:43+00:00
- **Updated**: 2022-06-10 23:16:43+00:00
- **Authors**: Dan Wang, Xinrui Cui, Septimiu Salcudean, Z. Jane Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a Transformer-based NeRF (TransNeRF) to learn a generic neural radiance field conditioned on observed-view images for the novel view synthesis task. By contrast, existing MLP-based NeRFs are not able to directly receive observed views with an arbitrary number and require an auxiliary pooling-based operation to fuse source-view information, resulting in the missing of complicated relationships between source views and the target rendering view. Furthermore, current approaches process each 3D point individually and ignore the local consistency of a radiance field scene representation. These limitations potentially can reduce their performance in challenging real-world applications where large differences between source views and a novel rendering view may exist. To address these challenges, our TransNeRF utilizes the attention mechanism to naturally decode deep associations of an arbitrary number of source views into a coordinate-based scene representation. Local consistency of shape and appearance are considered in the ray-cast space and the surrounding-view space within a unified Transformer network. Experiments demonstrate that our TransNeRF, trained on a wide variety of scenes, can achieve better performance in comparison to state-of-the-art image-based neural rendering methods in both scene-agnostic and per-scene finetuning scenarios especially when there is a considerable gap between source views and a rendering view.



### Fast building segmentation from satellite imagery and few local labels
- **Arxiv ID**: http://arxiv.org/abs/2206.05377v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.05377v1)
- **Published**: 2022-06-10 23:39:21+00:00
- **Updated**: 2022-06-10 23:39:21+00:00
- **Authors**: Caleb Robinson, Anthony Ortiz, Hogeun Park, Nancy Lozano Gracia, Jon Kher Kaw, Tina Sederholm, Rahul Dodhia, Juan M. Lavista Ferres
- **Comment**: Accepted at EarthVision 2022
- **Journal**: None
- **Summary**: Innovations in computer vision algorithms for satellite image analysis can enable us to explore global challenges such as urbanization and land use change at the planetary level. However, domain shift problems are a common occurrence when trying to replicate models that drive these analyses to new areas, particularly in the developing world. If a model is trained with imagery and labels from one location, then it usually will not generalize well to new locations where the content of the imagery and data distributions are different. In this work, we consider the setting in which we have a single large satellite imagery scene over which we want to solve an applied problem -- building footprint segmentation. Here, we do not necessarily need to worry about creating a model that generalizes past the borders of our scene but can instead train a local model. We show that surprisingly few labels are needed to solve the building segmentation problem with very high-resolution (0.5m/px) satellite imagery with this setting in mind. Our best model trained with just 527 sparse polygon annotations (an equivalent of 1500 x 1500 densely labeled pixels) has a recall of 0.87 over held out footprints and a R2 of 0.93 on the task of counting the number of buildings in 200 x 200-meter windows. We apply our models over high-resolution imagery in Amman, Jordan in a case study on urban change detection.



