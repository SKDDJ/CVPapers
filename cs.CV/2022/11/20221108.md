# Arxiv Papers in cs.CV on 2022-11-08
### ShaSTA: Modeling Shape and Spatio-Temporal Affinities for 3D Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2211.03919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.03919v2)
- **Published**: 2022-11-08 00:20:38+00:00
- **Updated**: 2023-02-07 00:55:23+00:00
- **Authors**: Tara Sadjadpour, Jie Li, Rares Ambrus, Jeannette Bohg
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: Multi-object tracking is a cornerstone capability of any robotic system. The quality of tracking is largely dependent on the quality of the detector used. In many applications, such as autonomous vehicles, it is preferable to over-detect objects to avoid catastrophic outcomes due to missed detections. As a result, current state-of-the-art 3D detectors produce high rates of false-positives to ensure a low number of false-negatives. This can negatively affect tracking by making data association and track lifecycle management more challenging. Additionally, occasional false-negative detections due to difficult scenarios like occlusions can harm tracking performance. To address these issues in a unified framework, we propose to learn shape and spatio-temporal affinities between tracks and detections in consecutive frames. Our affinity provides a probabilistic matching that leads to robust data association, track lifecycle management, false-positive elimination, false-negative propagation, and sequential track confidence refinement. Though past 3D MOT approaches address a subset of components in this problem domain, we offer the first self-contained framework that addresses all these aspects of the 3D MOT problem. We quantitatively evaluate our method on the nuScenes tracking benchmark where we achieve 1st place amongst LiDAR-only trackers using CenterPoint detections. Our method estimates accurate and precise tracks, while decreasing the overall number of false-positive and false-negative tracks and increasing the number of true-positive tracks. We analyze our performance with 5 metrics, giving a comprehensive overview of our approach to indicate how our tracking framework may impact the ultimate goal of an autonomous mobile agent. We also present ablative experiments and qualitative results that demonstrate our framework's capabilities in complex scenarios.



### Automatic Error Detection in Integrated Circuits Image Segmentation: A Data-driven Approach
- **Arxiv ID**: http://arxiv.org/abs/2211.03927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.03927v1)
- **Published**: 2022-11-08 00:58:10+00:00
- **Updated**: 2022-11-08 00:58:10+00:00
- **Authors**: Zhikang Zhang, Bruno Machado Trindade, Michael Green, Zifan Yu, Christopher Pawlowicz, Fengbo Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the complicated nanoscale structures of current integrated circuits(IC) builds and low error tolerance of IC image segmentation tasks, most existing automated IC image segmentation approaches require human experts for visual inspection to ensure correctness, which is one of the major bottlenecks in large-scale industrial applications. In this paper, we present the first data-driven automatic error detection approach targeting two types of IC segmentation errors: wire errors and via errors. On an IC image dataset collected from real industry, we demonstrate that, by adapting existing CNN-based approaches of image classification and image translation with additional pre-processing and post-processing techniques, we are able to achieve recall/precision of 0.92/0.93 in wire error detection and 0.96/0.90 in via error detection, respectively.



### Editable Indoor Lighting Estimation
- **Arxiv ID**: http://arxiv.org/abs/2211.03928v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.03928v2)
- **Published**: 2022-11-08 00:58:29+00:00
- **Updated**: 2022-11-09 21:19:05+00:00
- **Authors**: Henrique Weber, Mathieu Garon, Jean-Fran√ßois Lalonde
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: We present a method for estimating lighting from a single perspective image of an indoor scene. Previous methods for predicting indoor illumination usually focus on either simple, parametric lighting that lack realism, or on richer representations that are difficult or even impossible to understand or modify after prediction. We propose a pipeline that estimates a parametric light that is easy to edit and allows renderings with strong shadows, alongside with a non-parametric texture with high-frequency information necessary for realistic rendering of specular objects. Once estimated, the predictions obtained with our model are interpretable and can easily be modified by an artist/user with a few mouse clicks. Quantitative and qualitative results show that our approach makes indoor lighting estimation easier to handle by a casual user, while still producing competitive results.



### ReLoc: A Restoration-Assisted Framework for Robust Image Tampering Localization
- **Arxiv ID**: http://arxiv.org/abs/2211.03930v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.03930v1)
- **Published**: 2022-11-08 01:00:00+00:00
- **Updated**: 2022-11-08 01:00:00+00:00
- **Authors**: Peiyu Zhuang, Haodong Li, Rui Yang, Jiwu Huang
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: With the spread of tampered images, locating the tampered regions in digital images has drawn increasing attention. The existing image tampering localization methods, however, suffer from severe performance degradation when the tampered images are subjected to some post-processing, as the tampering traces would be distorted by the post-processing operations. The poor robustness against post-processing has become a bottleneck for the practical applications of image tampering localization techniques. In order to address this issue, this paper proposes a novel restoration-assisted framework for image tampering localization (ReLoc). The ReLoc framework mainly consists of an image restoration module and a tampering localization module. The key idea of ReLoc is to use the restoration module to recover a high-quality counterpart of the distorted tampered image, such that the distorted tampering traces can be re-enhanced, facilitating the tampering localization module to identify the tampered regions. To achieve this, the restoration module is optimized not only with the conventional constraints on image visual quality but also with a forensics-oriented objective function. Furthermore, the restoration module and the localization module are trained alternately, which can stabilize the training process and is beneficial for improving the performance. The proposed framework is evaluated by fighting against JPEG compression, the most commonly used post-processing. Extensive experimental results show that ReLoc can significantly improve the robustness against JPEG compression. The restoration module in a well-trained ReLoc model is transferable. Namely, it is still effective when being directly deployed with another tampering localization module.



### Enhanced Low-resolution LiDAR-Camera Calibration Via Depth Interpolation and Supervised Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2211.03932v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2211.03932v1)
- **Published**: 2022-11-08 01:12:08+00:00
- **Updated**: 2022-11-08 01:12:08+00:00
- **Authors**: Zhikang Zhang, Zifan Yu, Suya You, Raghuveer Rao, Sanjeev Agarwal, Fengbo Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Motivated by the increasing application of low-resolution LiDAR recently, we target the problem of low-resolution LiDAR-camera calibration in this work. The main challenges are two-fold: sparsity and noise in point clouds. To address the problem, we propose to apply depth interpolation to increase the point density and supervised contrastive learning to learn noise-resistant features. The experiments on RELLIS-3D demonstrate that our approach achieves an average mean absolute rotation/translation errors of 0.15cm/0.33\textdegree on 32-channel LiDAR point cloud data, which significantly outperforms all reference methods.



### From fat droplets to floating forests: cross-domain transfer learning using a PatchGAN-based segmentation model
- **Arxiv ID**: http://arxiv.org/abs/2211.03937v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.03937v1)
- **Published**: 2022-11-08 01:19:15+00:00
- **Updated**: 2022-11-08 01:19:15+00:00
- **Authors**: Kameswara Bharadwaj Mantha, Ramanakumar Sankar, Yuping Zheng, Lucy Fortson, Thomas Pengo, Douglas Mashek, Mark Sanders, Trace Christensen, Jeffrey Salisbury, Laura Trouille, Jarrett E. K. Byrnes, Isaac Rosenthal, Henry Houskeeper, Kyle Cavanaugh
- **Comment**: 5 pages, 4 figures, accepted for publication at the Proceedings of
  the ACM/CIKM 2022 (Human-in-the-loop Data Curation Workshop)
- **Journal**: None
- **Summary**: Many scientific domains gather sufficient labels to train machine algorithms through human-in-the-loop techniques provided by the Zooniverse.org citizen science platform. As the range of projects, task types and data rates increase, acceleration of model training is of paramount concern to focus volunteer effort where most needed. The application of Transfer Learning (TL) between Zooniverse projects holds promise as a solution. However, understanding the effectiveness of TL approaches that pretrain on large-scale generic image sets vs. images with similar characteristics possibly from similar tasks is an open challenge. We apply a generative segmentation model on two Zooniverse project-based data sets: (1) to identify fat droplets in liver cells (FatChecker; FC) and (2) the identification of kelp beds in satellite images (Floating Forests; FF) through transfer learning from the first project. We compare and contrast its performance with a TL model based on the COCO image set, and subsequently with baseline counterparts. We find that both the FC and COCO TL models perform better than the baseline cases when using >75% of the original training sample size. The COCO-based TL model generally performs better than the FC-based one, likely due to its generalized features. Our investigations provide important insights into usage of TL approaches on multi-domain data hosted across different Zooniverse projects, enabling future projects to accelerate task completion.



### Understanding the Role of Mixup in Knowledge Distillation: An Empirical Study
- **Arxiv ID**: http://arxiv.org/abs/2211.03946v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.03946v2)
- **Published**: 2022-11-08 01:43:14+00:00
- **Updated**: 2022-11-09 01:53:34+00:00
- **Authors**: Hongjun Choi, Eun Som Jeon, Ankita Shukla, Pavan Turaga
- **Comment**: To be presented at WACV 2023
- **Journal**: None
- **Summary**: Mixup is a popular data augmentation technique based on creating new samples by linear interpolation between two given data samples, to improve both the generalization and robustness of the trained model. Knowledge distillation (KD), on the other hand, is widely used for model compression and transfer learning, which involves using a larger network's implicit knowledge to guide the learning of a smaller network. At first glance, these two techniques seem very different, however, we found that "smoothness" is the connecting link between the two and is also a crucial attribute in understanding KD's interplay with mixup. Although many mixup variants and distillation methods have been proposed, much remains to be understood regarding the role of a mixup in knowledge distillation. In this paper, we present a detailed empirical study on various important dimensions of compatibility between mixup and knowledge distillation. We also scrutinize the behavior of the networks trained with a mixup in the light of knowledge distillation through extensive analysis, visualizations, and comprehensive experiments on image classification. Finally, based on our findings, we suggest improved strategies to guide the student network to enhance its effectiveness. Additionally, the findings of this study provide insightful suggestions to researchers and practitioners that commonly use techniques from KD. Our code is available at https://github.com/hchoi71/MIX-KD.



### $BT^2$: Backward-compatible Training with Basis Transformation
- **Arxiv ID**: http://arxiv.org/abs/2211.03989v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.03989v3)
- **Published**: 2022-11-08 04:00:23+00:00
- **Updated**: 2023-08-28 05:56:47+00:00
- **Authors**: Yifei Zhou, Zilu Li, Abhinav Shrivastava, Hengshuang Zhao, Antonio Torralba, Taipeng Tian, Ser-Nam Lim
- **Comment**: iccv2023 camera ready
- **Journal**: None
- **Summary**: Modern retrieval system often requires recomputing the representation of every piece of data in the gallery when updating to a better representation model. This process is known as backfilling and can be especially costly in the real world where the gallery often contains billions of samples. Recently, researchers have proposed the idea of Backward Compatible Training (BCT) where the new representation model can be trained with an auxiliary loss to make it backward compatible with the old representation. In this way, the new representation can be directly compared with the old representation, in principle avoiding the need for any backfilling. However, followup work shows that there is an inherent tradeoff where a backward compatible representation model cannot simultaneously maintain the performance of the new model itself. This paper reports our ``not-so-surprising'' finding that adding extra dimensions to the representation can help here. However, we also found that naively increasing the dimension of the representation did not work. To deal with this, we propose Backward-compatible Training with a novel Basis Transformation ($BT^2$). A basis transformation (BT) is basically a learnable set of parameters that applies an orthonormal transformation. Such a transformation possesses an important property whereby the original information contained in its input is retained in its output. We show in this paper how a BT can be utilized to add only the necessary amount of additional dimensions. We empirically verify the advantage of $BT^2$ over other state-of-the-art methods in a wide range of settings. We then further extend $BT^2$ to other challenging yet more practical settings, including significant change in model architecture (CNN to Transformers), modality change, and even a series of updates in the model architecture mimicking the evolution of deep learning models.



### SimOn: A Simple Framework for Online Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2211.04905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04905v1)
- **Published**: 2022-11-08 04:50:54+00:00
- **Updated**: 2022-11-08 04:50:54+00:00
- **Authors**: Tuan N. Tang, Jungin Park, Kwonyoung Kim, Kwanghoon Sohn
- **Comment**: None
- **Journal**: None
- **Summary**: Online Temporal Action Localization (On-TAL) aims to immediately provide action instances from untrimmed streaming videos. The model is not allowed to utilize future frames and any processing techniques to modify past predictions, making On-TAL much more challenging. In this paper, we propose a simple yet effective framework, termed SimOn, that learns to predict action instances using the popular Transformer architecture in an end-to-end manner. Specifically, the model takes the current frame feature as a query and a set of past context information as keys and values of the Transformer. Different from the prior work that uses a set of outputs of the model as past contexts, we leverage the past visual context and the learnable context embedding for the current query. Experimental results on the THUMOS14 and ActivityNet1.3 datasets show that our model remarkably outperforms the previous methods, achieving a new state-of-the-art On-TAL performance. In addition, the evaluation for Online Detection of Action Start (ODAS) demonstrates the effectiveness and robustness of our method in the online setting. The code is available at https://github.com/TuanTNG/SimOn



### An Incremental Phase Mapping Approach for X-ray Diffraction Patterns using Binary Peak Representations
- **Arxiv ID**: http://arxiv.org/abs/2211.04011v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04011v1)
- **Published**: 2022-11-08 05:05:21+00:00
- **Updated**: 2022-11-08 05:05:21+00:00
- **Authors**: Dipendra Jha, K. V. L. V. Narayanachari, Ruifeng Zhang, Justin Liao, Denis T. Keane, Wei-keng Liao, Alok Choudhary, Yip-Wah Chung, Michael Bedzyk, Ankit Agrawal
- **Comment**: Accepted and presented at the International Workshop on Domain-Driven
  Data Mining (DDDM) as a part of the SIAM International Conference on Data
  Mining (SDM 2021). Contains 11 pages and 5 figures
- **Journal**: None
- **Summary**: Despite the huge advancement in knowledge discovery and data mining techniques, the X-ray diffraction (XRD) analysis process has mostly remained untouched and still involves manual investigation, comparison, and verification. Due to the large volume of XRD samples from high-throughput XRD experiments, it has become impossible for domain scientists to process them manually. Recently, they have started leveraging standard clustering techniques, to reduce the XRD pattern representations requiring manual efforts for labeling and verification. Nevertheless, these standard clustering techniques do not handle problem-specific aspects such as peak shifting, adjacent peaks, background noise, and mixed phases; hence, resulting in incorrect composition-phase diagrams that complicate further steps. Here, we leverage data mining techniques along with domain expertise to handle these issues. In this paper, we introduce an incremental phase mapping approach based on binary peak representations using a new threshold based fuzzy dissimilarity measure. The proposed approach first applies an incremental phase computation algorithm on discrete binary peak representation of XRD samples, followed by hierarchical clustering or manual merging of similar pure phases to obtain the final composition-phase diagram. We evaluate our method on the composition space of two ternary alloy systems- Co-Ni-Ta and Co-Ti-Ta. Our results are verified by domain scientists and closely resembles the manually computed ground-truth composition-phase diagrams. The proposed approach takes us closer towards achieving the goal of complete end-to-end automated XRD analysis.



### Hilbert Distillation for Cross-Dimensionality Networks
- **Arxiv ID**: http://arxiv.org/abs/2211.04031v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.04031v1)
- **Published**: 2022-11-08 06:25:06+00:00
- **Updated**: 2022-11-08 06:25:06+00:00
- **Authors**: Dian Qin, Haishuai Wang, Zhe Liu, Hongjia Xu, Sheng Zhou, Jiajun Bu
- **Comment**: Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: 3D convolutional neural networks have revealed superior performance in processing volumetric data such as video and medical imaging. However, the competitive performance by leveraging 3D networks results in huge computational costs, which are far beyond that of 2D networks. In this paper, we propose a novel Hilbert curve-based cross-dimensionality distillation approach that facilitates the knowledge of 3D networks to improve the performance of 2D networks. The proposed Hilbert Distillation (HD) method preserves the structural information via the Hilbert curve, which maps high-dimensional (>=2) representations to one-dimensional continuous space-filling curves. Since the distilled 2D networks are supervised by the curves converted from dimensionally heterogeneous 3D features, the 2D networks are given an informative view in terms of learning structural information embedded in well-trained high-dimensional representations. We further propose a Variable-length Hilbert Distillation (VHD) method to dynamically shorten the walking stride of the Hilbert curve in activation feature areas and lengthen the stride in context feature areas, forcing the 2D networks to pay more attention to learning from activation features. The proposed algorithm outperforms the current state-of-the-art distillation techniques adapted to cross-dimensionality distillation on two classification tasks. Moreover, the distilled 2D networks by the proposed method achieve competitive performance with the original 3D networks, indicating the lightweight distilled 2D networks could potentially be the substitution of cumbersome 3D networks in the real-world scenario.



### Fine-grained Population Mapping from Coarse Census Counts and Open Geodata
- **Arxiv ID**: http://arxiv.org/abs/2211.04039v1
- **DOI**: 10.1038/s41598-022-24495-w
- **Categories**: **cs.LG**, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2211.04039v1)
- **Published**: 2022-11-08 06:43:52+00:00
- **Updated**: 2022-11-08 06:43:52+00:00
- **Authors**: Nando Metzger, John E. Vargas-Mu√±oz, Rodrigo C. Daudt, Benjamin Kellenberger, Thao Ton-That Whelan, Ferda Ofli, Muhammad Imran, Konrad Schindler, Devis Tuia
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained population maps are needed in several domains, like urban planning, environmental monitoring, public health, and humanitarian operations. Unfortunately, in many countries only aggregate census counts over large spatial units are collected, moreover, these are not always up-to-date. We present POMELO, a deep learning model that employs coarse census counts and open geodata to estimate fine-grained population maps with 100m ground sampling distance. Moreover, the model can also estimate population numbers when no census counts at all are available, by generalizing across countries. In a series of experiments for several countries in sub-Saharan Africa, the maps produced with POMELOare in good agreement with the most detailed available reference counts: disaggregation of coarse census counts reaches R2 values of 85-89%; unconstrained prediction in the absence of any counts reaches 48-69%.



### ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2211.04041v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2211.04041v4)
- **Published**: 2022-11-08 06:50:10+00:00
- **Updated**: 2023-03-24 05:57:41+00:00
- **Authors**: Jad Abou-Chakra, Feras Dayoub, Niko S√ºnderhauf
- **Comment**: None
- **Journal**: None
- **Summary**: While existing Neural Radiance Fields (NeRFs) for dynamic scenes are offline methods with an emphasis on visual fidelity, our paper addresses the online use case that prioritises real-time adaptability. We present ParticleNeRF, a new approach that dynamically adapts to changes in the scene geometry by learning an up-to-date representation online, every 200ms. ParticleNeRF achieves this using a novel particle-based parametric encoding. We couple features to particles in space and backpropagate the photometric reconstruction loss into the particles' position gradients, which are then interpreted as velocity vectors. Governed by a lightweight physics system to handle collisions, this lets the features move freely with the changing scene geometry. We demonstrate ParticleNeRF on various dynamic scenes containing translating, rotating, articulated, and deformable objects. ParticleNeRF is the first online dynamic NeRF and achieves fast adaptability with better visual fidelity than brute-force online InstantNGP and other baseline approaches on dynamic scenes with online constraints. Videos of our system can be found at our project website https://sites.google.com/view/particlenerf.



### DNN Filter for Bias Reduction in Distribution-to-Distribution Scan Matching
- **Arxiv ID**: http://arxiv.org/abs/2211.04047v3
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2211.04047v3)
- **Published**: 2022-11-08 07:08:08+00:00
- **Updated**: 2022-11-18 19:36:48+00:00
- **Authors**: Matthew McDermott, Jason Rife
- **Comment**: None
- **Journal**: None
- **Summary**: Distribution-to-distribution (D2D) point cloud registration techniques such as the Normal Distributions Transform (NDT) can align point clouds sampled from unstructured scenes and provide accurate bounds of their own solution error covariance -- an important feature for safety-of-life navigation tasks. D2D methods rely on the assumption of a static scene and are therefore susceptible to bias from range-shadowing, self-occlusion, moving objects, and distortion artifacts as the recording device moves between frames. Deep Learning-based approaches can achieve higher accuracy in dynamic scenes by relaxing these constraints, however, DNNs produce uninterpretable solutions which can be problematic from a safety perspective. In this paper, we propose a method of down-sampling LIDAR point clouds to exclude voxels that violate the assumption of a static scene and introduce error to the D2D scan matching process. Our approach uses a solution consistency filter -- identifying and suppressing voxels where D2D contributions disagree with local estimates from a PointNet-based registration network. Our results show that this technique provides significant benefits in registration accuracy, and is particularly useful in scenes containing dense foliage.



### Does an ensemble of GANs lead to better performance when training segmentation networks with synthetic images?
- **Arxiv ID**: http://arxiv.org/abs/2211.04086v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04086v2)
- **Published**: 2022-11-08 08:35:15+00:00
- **Updated**: 2023-03-12 13:42:25+00:00
- **Authors**: M√•ns Larsson, Muhammad Usman Akbar, Anders Eklund
- **Comment**: 5 pages, submitted to ISBI 2023
- **Journal**: None
- **Summary**: Large annotated datasets are required to train segmentation networks. In medical imaging, it is often difficult, time consuming and expensive to create such datasets, and it may also be difficult to share these datasets with other researchers. Different AI models can today generate very realistic synthetic images, which can potentially be openly shared as they do not belong to specific persons. However, recent work has shown that using synthetic images for training deep networks often leads to worse performance compared to using real images. Here we demonstrate that using synthetic images and annotations from an ensemble of 20 GANs, instead of from a single GAN, increases the Dice score on real test images with 4.7 % to 14.0 % on specific classes.



### Determining Accessible Sidewalk Width by Extracting Obstacle Information from Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2211.04108v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2211.04108v1)
- **Published**: 2022-11-08 09:19:16+00:00
- **Updated**: 2022-11-08 09:19:16+00:00
- **Authors**: Cl√°udia Fonseca Pinh√£o, Chris Eijgenstein, Iva Gornishka, Shayla Jansen, Diederik M. Roijers, Daan Bloembergen
- **Comment**: 4 pages, 9 figures. Presented at the workshop on "The Future of Urban
  Accessibility" at ACM ASSETS'22. Code for this paper is available at
  https://github.com/Amsterdam-AI-Team/Urban_PointCloud_Sidewalk_Width
- **Journal**: None
- **Summary**: Obstacles on the sidewalk often block the path, limiting passage and resulting in frustration and wasted time, especially for citizens and visitors who use assistive devices (wheelchairs, walkers, strollers, canes, etc). To enable equal participation and use of the city, all citizens should be able to perform and complete their daily activities in a similar amount of time and effort. Therefore, we aim to offer accessibility information regarding sidewalks, so that citizens can better plan their routes, and to help city officials identify the location of bottlenecks and act on them. In this paper we propose a novel pipeline to estimate obstacle-free sidewalk widths based on 3D point cloud data of the city of Amsterdam, as the first step to offer a more complete set of information regarding sidewalk accessibility.



### Cross-view Graph Contrastive Representation Learning on Partially Aligned Multi-view Data
- **Arxiv ID**: http://arxiv.org/abs/2211.04906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04906v1)
- **Published**: 2022-11-08 09:19:32+00:00
- **Updated**: 2022-11-08 09:19:32+00:00
- **Authors**: Yiming Wang, Dongxia Chang, Zhiqiang Fu, Jie Wen, Yao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-view representation learning has developed rapidly over the past decades and has been applied in many fields. However, most previous works assumed that each view is complete and aligned. This leads to an inevitable deterioration in their performance when encountering practical problems such as missing or unaligned views. To address the challenge of representation learning on partially aligned multi-view data, we propose a new cross-view graph contrastive learning framework, which integrates multi-view information to align data and learn latent representations. Compared with current approaches, the proposed method has the following merits: (1) our model is an end-to-end framework that simultaneously performs view-specific representation learning via view-specific autoencoders and cluster-level data aligning by combining multi-view information with the cross-view graph contrastive learning; (2) it is easy to apply our model to explore information from three or more modalities/sources as the cross-view graph contrastive learning is devised. Extensive experiments conducted on several real datasets demonstrate the effectiveness of the proposed method on the clustering and classification tasks.



### Privacy Meets Explainability: A Comprehensive Impact Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2211.04110v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04110v1)
- **Published**: 2022-11-08 09:20:28+00:00
- **Updated**: 2022-11-08 09:20:28+00:00
- **Authors**: Saifullah Saifullah, Dominique Mercier, Adriano Lucieri, Andreas Dengel, Sheraz Ahmed
- **Comment**: Under Submission
- **Journal**: None
- **Summary**: Since the mid-10s, the era of Deep Learning (DL) has continued to this day, bringing forth new superlatives and innovations each year. Nevertheless, the speed with which these innovations translate into real applications lags behind this fast pace. Safety-critical applications, in particular, underlie strict regulatory and ethical requirements which need to be taken care of and are still active areas of debate. eXplainable AI (XAI) and privacy-preserving machine learning (PPML) are both crucial research fields, aiming at mitigating some of the drawbacks of prevailing data-hungry black-box models in DL. Despite brisk research activity in the respective fields, no attention has yet been paid to their interaction. This work is the first to investigate the impact of private learning techniques on generated explanations for DL-based models. In an extensive experimental analysis covering various image and time series datasets from multiple domains, as well as varying privacy techniques, XAI methods, and model architectures, the effects of private training on generated explanations are studied. The findings suggest non-negligible changes in explanations through the introduction of privacy. Apart from reporting individual effects of PPML on XAI, the paper gives clear recommendations for the choice of techniques in real applications. By unveiling the interdependencies of these pivotal technologies, this work is a first step towards overcoming the remaining hurdles for practically applicable AI in safety-critical domains.



### A kinetic approach to consensus-based segmentation of biomedical images
- **Arxiv ID**: http://arxiv.org/abs/2211.05226v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.05226v1)
- **Published**: 2022-11-08 09:54:34+00:00
- **Updated**: 2022-11-08 09:54:34+00:00
- **Authors**: Raffaella Fiamma Cabini, Anna Pichiecchio, Alessandro Lascialfari, Silvia Figini, Mattia Zanella
- **Comment**: 25 pages, 16 figures
- **Journal**: None
- **Summary**: In this work, we apply a kinetic version of a bounded confidence consensus model to biomedical segmentation problems. In the presented approach, time-dependent information on the microscopic state of each particle/pixel includes its space position and a feature representing a static characteristic of the system, i.e. the gray level of each pixel. From the introduced microscopic model we derive a kinetic formulation of the model. The large time behavior of the system is then computed with the aid of a surrogate Fokker-Planck approach that can be obtained in the quasi-invariant scaling. We exploit the computational efficiency of direct simulation Monte Carlo methods for the obtained Boltzmann-type description of the problem for parameter identification tasks. Based on a suitable loss function measuring the distance between the ground truth segmentation mask and the evaluated mask, we minimize the introduced segmentation metric for a relevant set of 2D gray-scale images. Applications to biomedical segmentation concentrate on different imaging research contexts.



### Theoretical analysis and experimental validation of volume bias of soft Dice optimized segmentation maps in the context of inherent uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2211.04161v1
- **DOI**: 10.1016/j.media.2020.101833
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.04161v1)
- **Published**: 2022-11-08 11:04:52+00:00
- **Updated**: 2022-11-08 11:04:52+00:00
- **Authors**: Jeroen Bertels, David Robben, Dirk Vandermeulen, Paul Suetens
- **Comment**: 18 pages, 7 figures, 3 tables, published in Elsevier Medical Image
  Analysis (2021)
- **Journal**: Elsevier Medical Image Analysis, Volume 61, January 2021, 101833
- **Summary**: The clinical interest is often to measure the volume of a structure, which is typically derived from a segmentation. In order to evaluate and compare segmentation methods, the similarity between a segmentation and a predefined ground truth is measured using popular discrete metrics, such as the Dice score. Recent segmentation methods use a differentiable surrogate metric, such as soft Dice, as part of the loss function during the learning phase. In this work, we first briefly describe how to derive volume estimates from a segmentation that is, potentially, inherently uncertain or ambiguous. This is followed by a theoretical analysis and an experimental validation linking the inherent uncertainty to common loss functions for training CNNs, namely cross-entropy and soft Dice. We find that, even though soft Dice optimization leads to an improved performance with respect to the Dice score and other measures, it may introduce a volume bias for tasks with high inherent uncertainty. These findings indicate some of the method's clinical limitations and suggest doing a closer ad-hoc volume analysis with an optional re-calibration step.



### Dynamic loss balancing and sequential enhancement for road-safety assessment and traffic scene classification
- **Arxiv ID**: http://arxiv.org/abs/2211.04165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04165v1)
- **Published**: 2022-11-08 11:10:07+00:00
- **Updated**: 2022-11-08 11:10:07+00:00
- **Authors**: Marin Kaƒçan, Marko ≈†evroviƒá, Sini≈°a ≈†egviƒá
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Road-safety inspection is an indispensable instrument for reducing road-accident fatalities contributed to road infrastructure. Recent work formalizes road-safety assessment in terms of carefully selected risk factors that are also known as road-safety attributes. In current practice, these attributes are manually annotated in geo-referenced monocular video for each road segment. We propose to reduce dependency on tedious human labor by automating recognition with a two-stage neural architecture. The first stage predicts more than forty road-safety attributes by observing a local spatio-temporal context. Our design leverages an efficient convolutional pipeline, which benefits from pre-training on semantic segmentation of street scenes. The second stage enhances predictions through sequential integration across a larger temporal window. Our design leverages per-attribute instances of a lightweight bidirectional LSTM architecture. Both stages alleviate extreme class imbalance by incorporating a multi-task variant of recall-based dynamic loss weighting. We perform experiments on the iRAP-BH dataset, which involves fully labeled geo-referenced video along 2,300 km of public roads in Bosnia and Herzegovina. We also validate our approach by comparing it with the related work on two road-scene classification datasets from the literature: Honda Scenes and FM3m. Experimental evaluation confirms the value of our contributions on all three datasets.



### Learning advisor networks for noisy image classification
- **Arxiv ID**: http://arxiv.org/abs/2211.04177v1
- **DOI**: 10.1007/978-3-031-06430-2_37
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.04177v1)
- **Published**: 2022-11-08 11:44:08+00:00
- **Updated**: 2022-11-08 11:44:08+00:00
- **Authors**: Simone Ricci, Tiberio Uricchio, Alberto Del Bimbo
- **Comment**: Paper published as Poster at ICIAP21
- **Journal**: ICIAP 2022
- **Summary**: In this paper, we introduced the novel concept of advisor network to address the problem of noisy labels in image classification. Deep neural networks (DNN) are prone to performance reduction and overfitting problems on training data with noisy annotations. Weighting loss methods aim to mitigate the influence of noisy labels during the training, completely removing their contribution. This discarding process prevents DNNs from learning wrong associations between images and their correct labels but reduces the amount of data used, especially when most of the samples have noisy labels. Differently, our method weighs the feature extracted directly from the classifier without altering the loss value of each data. The advisor helps to focus only on some part of the information present in mislabeled examples, allowing the classifier to leverage that data as well. We trained it with a meta-learning strategy so that it can adapt throughout the training of the main model. We tested our method on CIFAR10 and CIFAR100 with synthetic noise, and on Clothing1M which contains real-world noise, reporting state-of-the-art results.



### Exploiting segmentation labels and representation learning to forecast therapy response of PDAC patients
- **Arxiv ID**: http://arxiv.org/abs/2211.04180v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04180v2)
- **Published**: 2022-11-08 11:50:31+00:00
- **Updated**: 2023-03-30 08:07:01+00:00
- **Authors**: Alexander Ziller, Ayhan Can Erdur, Friederike Jungmann, Daniel Rueckert, Rickmer Braren, Georgios Kaissis
- **Comment**: None
- **Journal**: None
- **Summary**: The prediction of pancreatic ductal adenocarcinoma therapy response is a clinically challenging and important task in this high-mortality tumour entity. The training of neural networks able to tackle this challenge is impeded by a lack of large datasets and the difficult anatomical localisation of the pancreas. Here, we propose a hybrid deep neural network pipeline to predict tumour response to initial chemotherapy which is based on the Response Evaluation Criteria in Solid Tumors (RECIST) score, a standardised method for cancer response evaluation by clinicians as well as tumour markers, and clinical evaluation of the patients. We leverage a combination of representation transfer from segmentation to classification, as well as localisation and representation learning. Our approach yields a remarkably data-efficient method able to predict treatment response with a ROC-AUC of 63.7% using only 477 datasets in total.



### DepthFormer: Multimodal Positional Encodings and Cross-Input Attention for Transformer-Based Segmentation Networks
- **Arxiv ID**: http://arxiv.org/abs/2211.04188v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04188v2)
- **Published**: 2022-11-08 12:01:31+00:00
- **Updated**: 2023-03-27 12:54:49+00:00
- **Authors**: Francesco Barbato, Giulia Rizzoli, Pietro Zanuttigh
- **Comment**: Accepted at ICASSP 2023
- **Journal**: None
- **Summary**: Most approaches for semantic segmentation use only information from color cameras to parse the scenes, yet recent advancements show that using depth data allows to further improve performances. In this work, we focus on transformer-based deep learning architectures, that have achieved state-of-the-art performances on the segmentation task, and we propose to employ depth information by embedding it in the positional encoding. Effectively, we extend the network to multimodal data without adding any parameters and in a natural way that makes use of the strength of transformers' self-attention modules. We also investigate the idea of performing cross-modality operations inside the attention module, swapping the key inputs between the depth and color branches. Our approach consistently improves performances on the Cityscapes benchmark.



### RRSR:Reciprocal Reference-based Image Super-Resolution with Progressive Feature Alignment and Selection
- **Arxiv ID**: http://arxiv.org/abs/2211.04203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04203v1)
- **Published**: 2022-11-08 12:39:35+00:00
- **Updated**: 2022-11-08 12:39:35+00:00
- **Authors**: Lin Zhang, Xin Li, Dongliang He, Fu Li, Yili Wang, Zhaoxiang Zhang
- **Comment**: 8 figures, 17 pages
- **Journal**: None
- **Summary**: Reference-based image super-resolution (RefSR) is a promising SR branch and has shown great potential in overcoming the limitations of single image super-resolution. While previous state-of-the-art RefSR methods mainly focus on improving the efficacy and robustness of reference feature transfer, it is generally overlooked that a well reconstructed SR image should enable better SR reconstruction for its similar LR images when it is referred to as. Therefore, in this work, we propose a reciprocal learning framework that can appropriately leverage such a fact to reinforce the learning of a RefSR network. Besides, we deliberately design a progressive feature alignment and selection module for further improving the RefSR task. The newly proposed module aligns reference-input images at multi-scale feature spaces and performs reference-aware feature selection in a progressive manner, thus more precise reference features can be transferred into the input features and the network capability is enhanced. Our reciprocal learning paradigm is model-agnostic and it can be applied to arbitrary RefSR models. We empirically show that multiple recent state-of-the-art RefSR models can be consistently improved with our reciprocal learning paradigm. Furthermore, our proposed model together with the reciprocal learning strategy sets new state-of-the-art performances on multiple benchmarks.



### Generative Adversarial Networks for anonymous Acneic face dataset generation
- **Arxiv ID**: http://arxiv.org/abs/2211.04214v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.04214v1)
- **Published**: 2022-11-08 12:59:41+00:00
- **Updated**: 2022-11-08 12:59:41+00:00
- **Authors**: Hazem Zein, Samer Chantaf, R√©gis Fournier, Amine Nait-Ali
- **Comment**: None
- **Journal**: None
- **Summary**: It is well known that the performance of any classification model is effective if the dataset used for the training process and the test process satisfy some specific requirements. In other words, the more the dataset size is large, balanced, and representative, the more one can trust the proposed model's effectiveness and, consequently, the obtained results. Unfortunately, large-size anonymous datasets are generally not publicly available in biomedical applications, especially those dealing with pathological human face images. This concern makes using deep-learning-based approaches challenging to deploy and difficult to reproduce or verify some published results. In this paper, we suggest an efficient method to generate a realistic anonymous synthetic dataset of human faces with the attributes of acne disorders corresponding to three levels of severity (i.e. Mild, Moderate and Severe). Therefore, a specific hierarchy StyleGAN-based algorithm trained at distinct levels is considered. To evaluate the performance of the proposed scheme, we consider a CNN-based classification system, trained using the generated synthetic acneic face images and tested using authentic face images. Consequently, we show that an accuracy of 97,6\% is achieved using InceptionResNetv2. As a result, this work allows the scientific community to employ the generated synthetic dataset for any data processing application without restrictions on legal or ethical concerns. Moreover, this approach can also be extended to other applications requiring the generation of synthetic medical images. We can make the code and the generated dataset accessible for the scientific community.



### Learning Spatio-Temporal Model of Disease Progression with NeuralODEs from Longitudinal Volumetric Data
- **Arxiv ID**: http://arxiv.org/abs/2211.04234v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.04234v1)
- **Published**: 2022-11-08 13:28:26+00:00
- **Updated**: 2022-11-08 13:28:26+00:00
- **Authors**: Dmitrii Lachinov, Arunava Chakravarty, Christoph Grechenig, Ursula Schmidt-Erfurth, Hrvoje Bogunovic
- **Comment**: None
- **Journal**: None
- **Summary**: Robust forecasting of the future anatomical changes inflicted by an ongoing disease is an extremely challenging task that is out of grasp even for experienced healthcare professionals. Such a capability, however, is of great importance since it can improve patient management by providing information on the speed of disease progression already at the admission stage, or it can enrich the clinical trials with fast progressors and avoid the need for control arms by the means of digital twins. In this work, we develop a deep learning method that models the evolution of age-related disease by processing a single medical scan and providing a segmentation of the target anatomy at a requested future point in time. Our method represents a time-invariant physical process and solves a large-scale problem of modeling temporal pixel-level changes utilizing NeuralODEs. In addition, we demonstrate the approaches to incorporate the prior domain-specific constraints into our method and define temporal Dice loss for learning temporal objectives. To evaluate the applicability of our approach across different age-related diseases and imaging modalities, we developed and tested the proposed method on the datasets with 967 retinal OCT volumes of 100 patients with Geographic Atrophy, and 2823 brain MRI volumes of 633 patients with Alzheimer's Disease. For Geographic Atrophy, the proposed method outperformed the related baseline models in the atrophy growth prediction. For Alzheimer's Disease, the proposed method demonstrated remarkable performance in predicting the brain ventricle changes induced by the disease, achieving the state-of-the-art result on TADPOLE challenge.



### Contaminated Images Recovery by Implementing Non-negative Matrix Factorisation
- **Arxiv ID**: http://arxiv.org/abs/2211.04247v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04247v4)
- **Published**: 2022-11-08 13:50:27+00:00
- **Updated**: 2023-05-01 11:01:32+00:00
- **Authors**: Pengwei Yang, Chongyangzi Teng, Jack George Mangos
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Non-negative matrix factorisation (NMF) has been extensively applied to the problem of corrupted image data. Standard NMF approach minimises Euclidean distance between data matrix and factorised approximation. The traditional NMF technique is sensitive to outliers since it utilises the squared error of each data point, despite the fact that this method has proven effective. In this study, we theoretically examine the robustness of the traditional NMF, HCNMF, and L2,1-NMF algorithms and execute sets of experiments to demonstrate the robustness on ORL and Extended YaleB datasets. Our research indicates that each algorithm requires a different number of iterations to converge. Due to the computational cost of these approaches, our final models, such as the HCNMF and L2,1-NMF model, fail to converge within the iteration parameters of this work. Nonetheless, the experimental results illustrate, to some extent, the robustness of the aforementioned techniques.



### Eat-Radar: Continuous Fine-Grained Eating Gesture Detection Using FMCW Radar and 3D Temporal Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2211.04253v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2211.04253v1)
- **Published**: 2022-11-08 14:03:44+00:00
- **Updated**: 2022-11-08 14:03:44+00:00
- **Authors**: Chunzhuo Wang, T. Sunil Kumar, Walter De Raedt, Guido Camps, Hans Hallez, Bart Vanrumste
- **Comment**: None
- **Journal**: None
- **Summary**: Unhealthy dietary habits are considered as the primary cause of multiple chronic diseases such as obesity and diabetes. The automatic food intake monitoring system has the potential to improve the quality of life (QoF) of people with dietary related diseases through dietary assessment. In this work, we propose a novel contact-less radar-based food intake monitoring approach. Specifically, a Frequency Modulated Continuous Wave (FMCW) radar sensor is employed to recognize fine-grained eating and drinking gestures. The fine-grained eating/drinking gesture contains a series of movement from raising the hand to the mouth until putting away the hand from the mouth. A 3D temporal convolutional network (3D-TCN) is developed to detect and segment eating and drinking gestures in meal sessions by processing the Range-Doppler Cube (RD Cube). Unlike previous radar-based research, this work collects data in continuous meal sessions. We create a public dataset that contains 48 meal sessions (3121 eating gestures and 608 drinking gestures) from 48 participants with a total duration of 783 minutes. Four eating styles (fork & knife, chopsticks, spoon, hand) are included in this dataset. To validate the performance of the proposed approach, 8-fold cross validation method is applied. Experimental results show that our proposed 3D-TCN outperforms the model that combines a convolutional neural network and a long-short-term-memory network (CNN-LSTM), and also the CNN-Bidirectional LSTM model (CNN-BiLSTM) in eating and drinking gesture detection. The 3D-TCN model achieves a segmental F1-score of 0.887 and 0.844 for eating and drinking gestures, respectively. The results of the proposed approach indicate the feasibility of using radar for fine-grained eating and drinking gesture detection and segmentation in meal sessions.



### Two-stream Multi-dimensional Convolutional Network for Real-time Violence Detection
- **Arxiv ID**: http://arxiv.org/abs/2211.04255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04255v1)
- **Published**: 2022-11-08 14:04:47+00:00
- **Updated**: 2022-11-08 14:04:47+00:00
- **Authors**: Dipon Kumar Ghosh, Amitabha Chakrabarty
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: The increasing number of surveillance cameras and security concerns have made automatic violent activity detection from surveillance footage an active area for research. Modern deep learning methods have achieved good accuracy in violence detection and proved to be successful because of their applicability in intelligent surveillance systems. However, the models are computationally expensive and large in size because of their inefficient methods for feature extraction. This work presents a novel architecture for violence detection called Two-stream Multi-dimensional Convolutional Network (2s-MDCN), which uses RGB frames and optical flow to detect violence. Our proposed method extracts temporal and spatial information independently by 1D, 2D, and 3D convolutions. Despite combining multi-dimensional convolutional networks, our models are lightweight and efficient due to reduced channel capacity, yet they learn to extract meaningful spatial and temporal information. Additionally, combining RGB frames and optical flow yields 2.2% more accuracy than a single RGB stream. Regardless of having less complexity, our models obtained state-of-the-art accuracy of 89.7% on the largest violence detection benchmark dataset.



### Detecting Shortcuts in Medical Images -- A Case Study in Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2211.04279v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04279v2)
- **Published**: 2022-11-08 14:36:33+00:00
- **Updated**: 2022-11-09 12:06:12+00:00
- **Authors**: Amelia Jim√©nez-S√°nchez, Dovile Juodelyte, Bethany Chamberlain, Veronika Cheplygina
- **Comment**: Submitted to ISBI 2023
- **Journal**: None
- **Summary**: The availability of large public datasets and the increased amount of computing power have shifted the interest of the medical community to high-performance algorithms. However, little attention is paid to the quality of the data and their annotations. High performance on benchmark datasets may be reported without considering possible shortcuts or artifacts in the data, besides, models are not tested on subpopulation groups. With this work, we aim to raise awareness about shortcuts problems. We validate previous findings, and present a case study on chest X-rays using two publicly available datasets. We share annotations for a subset of pneumothorax images with drains. We conclude with general recommendations for medical image classification.



### Evaluation of Color Anomaly Detection in Multispectral Images For Synthetic Aperture Sensing
- **Arxiv ID**: http://arxiv.org/abs/2211.04293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04293v1)
- **Published**: 2022-11-08 15:01:14+00:00
- **Updated**: 2022-11-08 15:01:14+00:00
- **Authors**: Francis Seits, Indrajit Kurmi, Oliver Bimber
- **Comment**: 12 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: In this article, we evaluate unsupervised anomaly detection methods in multispectral images obtained with a wavelength-independent synthetic aperture sensing technique, called Airborne Optical Sectioning (AOS). With a focus on search and rescue missions that apply drones to locate missing or injured persons in dense forest and require real-time operation, we evaluate runtime vs. quality of these methods. Furthermore, we show that color anomaly detection methods that normally operate in the visual range always benefit from an additional far infrared (thermal) channel. We also show that, even without additional thermal bands, the choice of color space in the visual range already has an impact on the detection results. Color spaces like HSV and HLS have the potential to outperform the widely used RGB color space, especially when color anomaly detection is used for forest-like environments.



### Multi-Stage Based Feature Fusion of Multi-Modal Data for Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2211.04331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04331v1)
- **Published**: 2022-11-08 15:48:44+00:00
- **Updated**: 2022-11-08 15:48:44+00:00
- **Authors**: Hyeongju Choi, Apoorva Beedu, Harish Haresamudram, Irfan Essa
- **Comment**: None
- **Journal**: None
- **Summary**: To properly assist humans in their needs, human activity recognition (HAR) systems need the ability to fuse information from multiple modalities. Our hypothesis is that multimodal sensors, visual and non-visual tend to provide complementary information, addressing the limitations of other modalities. In this work, we propose a multi-modal framework that learns to effectively combine features from RGB Video and IMU sensors, and show its robustness for MMAct and UTD-MHAD datasets. Our model is trained in two-stage, where in the first stage, each input encoder learns to effectively extract features, and in the second stage, learns to combine these individual features. We show significant improvements of 22% and 11% compared to video only and IMU only setup on UTD-MHAD dataset, and 20% and 12% on MMAct datasets. Through extensive experimentation, we show the robustness of our model on zero shot setting, and limited annotated data setting. We further compare with state-of-the-art methods that use more input modalities and show that our method outperforms significantly on the more difficult MMact dataset, and performs comparably in UTD-MHAD dataset.



### Calibrated Perception Uncertainty Across Objects and Regions in Bird's-Eye-View
- **Arxiv ID**: http://arxiv.org/abs/2211.04340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04340v1)
- **Published**: 2022-11-08 16:01:17+00:00
- **Updated**: 2022-11-08 16:01:17+00:00
- **Authors**: Markus K√§ngsepp, Meelis Kull
- **Comment**: None
- **Journal**: None
- **Summary**: In driving scenarios with poor visibility or occlusions, it is important that the autonomous vehicle would take into account all the uncertainties when making driving decisions, including choice of a safe speed. The grid-based perception outputs, such as occupancy grids, and object-based outputs, such as lists of detected objects, must then be accompanied by well-calibrated uncertainty estimates. We highlight limitations in the state-of-the-art and propose a more complete set of uncertainties to be reported, particularly including undetected-object-ahead probabilities. We suggest a novel way to get these probabilistic outputs from bird's-eye-view probabilistic semantic segmentation, in the example of the FIERY model. We demonstrate that the obtained probabilities are not calibrated out-of-the-box and propose methods to achieve well-calibrated uncertainties.



### When & How to Transfer with Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2211.04347v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.04347v1)
- **Published**: 2022-11-08 16:12:41+00:00
- **Updated**: 2022-11-08 16:12:41+00:00
- **Authors**: Adrian Tormos, Dario Garcia-Gasulla, Victor Gimenez-Abalos, Sergio Alvarez-Napagao
- **Comment**: None
- **Journal**: None
- **Summary**: In deep learning, transfer learning (TL) has become the de facto approach when dealing with image related tasks. Visual features learnt for one task have been shown to be reusable for other tasks, improving performance significantly. By reusing deep representations, TL enables the use of deep models in domains with limited data availability, limited computational resources and/or limited access to human experts. Domains which include the vast majority of real-life applications. This paper conducts an experimental evaluation of TL, exploring its trade-offs with respect to performance, environmental footprint, human hours and computational requirements. Results highlight the cases were a cheap feature extraction approach is preferable, and the situations where an expensive fine-tuning effort may be worth the added cost. Finally, a set of guidelines on the use of TL are proposed.



### Infant hip screening using multi-class ultrasound scan segmentation
- **Arxiv ID**: http://arxiv.org/abs/2211.04350v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2211.04350v1)
- **Published**: 2022-11-08 16:16:23+00:00
- **Updated**: 2022-11-08 16:16:23+00:00
- **Authors**: Andrew Stamper, Abhinav Singh, James McCouat, Irina Voiculescu
- **Comment**: Four page paper
- **Journal**: None
- **Summary**: Developmental dysplasia of the hip (DDH) is a condition in infants where the femoral head is incorrectly located in the hip joint. We propose a deep learning algorithm for segmenting key structures within ultrasound images, employing this to calculate Femoral Head Coverage (FHC) and provide a screening diagnosis for DDH. To our knowledge, this is the first study to automate FHC calculation for DDH screening. Our algorithm outperforms the international state of the art, agreeing with expert clinicians on 89.8% of our test images.



### Much Easier Said Than Done: Falsifying the Causal Relevance of Linear Decoding Methods
- **Arxiv ID**: http://arxiv.org/abs/2211.04367v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04367v1)
- **Published**: 2022-11-08 16:43:02+00:00
- **Updated**: 2022-11-08 16:43:02+00:00
- **Authors**: Lucas Hayne, Abhijit Suresh, Hunar Jain, Rahul Kumar, R. McKell Carter
- **Comment**: 6 pages, 3 figures, to be published in I Can't Believe It's Note
  Better Workshop at NeurIPS 2022
- **Journal**: None
- **Summary**: Linear classifier probes are frequently utilized to better understand how neural networks function. Researchers have approached the problem of determining unit importance in neural networks by probing their learned, internal representations. Linear classifier probes identify highly selective units as the most important for network function. Whether or not a network actually relies on high selectivity units can be tested by removing them from the network using ablation. Surprisingly, when highly selective units are ablated they only produce small performance deficits, and even then only in some cases. In spite of the absence of ablation effects for selective neurons, linear decoding methods can be effectively used to interpret network function, leaving their effectiveness a mystery. To falsify the exclusive role of selectivity in network function and resolve this contradiction, we systematically ablate groups of units in subregions of activation space. Here, we find a weak relationship between neurons identified by probes and those identified by ablation. More specifically, we find that an interaction between selectivity and the average activity of the unit better predicts ablation performance deficits for groups of units in AlexNet, VGG16, MobileNetV2, and ResNet101. Linear decoders are likely somewhat effective because they overlap with those units that are causally important for network function. Interpretability methods could be improved by focusing on causally important units.



### A Multimodal Approach for Dementia Detection from Spontaneous Speech with Tensor Fusion Layer
- **Arxiv ID**: http://arxiv.org/abs/2211.04368v1
- **DOI**: 10.1109/BHI56158.2022.9926818
- **Categories**: **cs.CL**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2211.04368v1)
- **Published**: 2022-11-08 16:43:58+00:00
- **Updated**: 2022-11-08 16:43:58+00:00
- **Authors**: Loukas Ilias, Dimitris Askounis, John Psarras
- **Comment**: 2022 IEEE-EMBS International Conference on Biomedical and Health
  Informatics (BHI) - Oral Presentation
- **Journal**: None
- **Summary**: Alzheimer's disease (AD) is a progressive neurological disorder, meaning that the symptoms develop gradually throughout the years. It is also the main cause of dementia, which affects memory, thinking skills, and mental abilities. Nowadays, researchers have moved their interest towards AD detection from spontaneous speech, since it constitutes a time-effective procedure. However, existing state-of-the-art works proposing multimodal approaches do not take into consideration the inter- and intra-modal interactions and propose early and late fusion approaches. To tackle these limitations, we propose deep neural networks, which can be trained in an end-to-end trainable way and capture the inter- and intra-modal interactions. Firstly, each audio file is converted to an image consisting of three channels, i.e., log-Mel spectrogram, delta, and delta-delta. Next, each transcript is passed through a BERT model followed by a gated self-attention layer. Similarly, each image is passed through a Swin Transformer followed by an independent gated self-attention layer. Acoustic features are extracted also from each audio file. Finally, the representation vectors from the different modalities are fed to a tensor fusion layer for capturing the inter-modal interactions. Extensive experiments conducted on the ADReSS Challenge dataset indicate that our introduced approaches obtain valuable advantages over existing research initiatives reaching Accuracy and F1-score up to 86.25% and 85.48% respectively.



### PyNet-V2 Mobile: Efficient On-Device Photo Processing With Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2211.06263v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.06263v1)
- **Published**: 2022-11-08 17:18:01+00:00
- **Updated**: 2022-11-08 17:18:01+00:00
- **Authors**: Andrey Ignatov, Grigory Malivenko, Radu Timofte, Yu Tseng, Yu-Syuan Xu, Po-Hsiang Yu, Cheng-Ming Chiang, Hsien-Kai Kuo, Min-Hung Chen, Chia-Ming Cheng, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: The increased importance of mobile photography created a need for fast and performant RAW image processing pipelines capable of producing good visual results in spite of the mobile camera sensor limitations. While deep learning-based approaches can efficiently solve this problem, their computational requirements usually remain too large for high-resolution on-device image processing. To address this limitation, we propose a novel PyNET-V2 Mobile CNN architecture designed specifically for edge devices, being able to process RAW 12MP photos directly on mobile phones under 1.5 second and producing high perceptual photo quality. To train and to evaluate the performance of the proposed solution, we use the real-world Fujifilm UltraISP dataset consisting on thousands of RAW-RGB image pairs captured with a professional medium-format 102MP Fujifilm camera and a popular Sony mobile camera sensor. The results demonstrate that the PyNET-V2 Mobile model can substantially surpass the quality of tradition ISP pipelines, while outperforming the previously introduced neural network-based solutions designed for fast image processing. Furthermore, we show that the proposed architecture is also compatible with the latest mobile AI accelerators such as NPUs or APUs that can be used to further reduce the latency of the model to as little as 0.5 second. The dataset, code and pre-trained models used in this paper are available on the project website: https://github.com/gmalivenko/PyNET-v2



### Normalization Perturbation: A Simple Domain Generalization Method for Real-World Domain Shifts
- **Arxiv ID**: http://arxiv.org/abs/2211.04393v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04393v2)
- **Published**: 2022-11-08 17:36:49+00:00
- **Updated**: 2022-11-09 02:53:31+00:00
- **Authors**: Qi Fan, Mattia Segu, Yu-Wing Tai, Fisher Yu, Chi-Keung Tang, Bernt Schiele, Dengxin Dai
- **Comment**: None
- **Journal**: None
- **Summary**: Improving model's generalizability against domain shifts is crucial, especially for safety-critical applications such as autonomous driving. Real-world domain styles can vary substantially due to environment changes and sensor noises, but deep models only know the training domain style. Such domain style gap impedes model generalization on diverse real-world domains. Our proposed Normalization Perturbation (NP) can effectively overcome this domain style overfitting problem. We observe that this problem is mainly caused by the biased distribution of low-level features learned in shallow CNN layers. Thus, we propose to perturb the channel statistics of source domain features to synthesize various latent styles, so that the trained deep model can perceive diverse potential domains and generalizes well even without observations of target domain data in training. We further explore the style-sensitive channels for effective style synthesis. Normalization Perturbation only relies on a single source domain and is surprisingly effective and extremely easy to implement. Extensive experiments verify the effectiveness of our method for generalizing models under real-world domain shifts.



### Expressing linear equality constraints in feedforward neural networks
- **Arxiv ID**: http://arxiv.org/abs/2211.04395v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2211.04395v2)
- **Published**: 2022-11-08 17:39:05+00:00
- **Updated**: 2023-01-07 18:53:34+00:00
- **Authors**: Anand Rangarajan, Pan He, Jaemoon Lee, Tania Banerjee, Sanjay Ranka
- **Comment**: None
- **Journal**: None
- **Summary**: We seek to impose linear, equality constraints in feedforward neural networks. As top layer predictors are usually nonlinear, this is a difficult task if we seek to deploy standard convex optimization methods and strong duality. To overcome this, we introduce a new saddle-point Lagrangian with auxiliary predictor variables on which constraints are imposed. Elimination of the auxiliary variables leads to a dual minimization problem on the Lagrange multipliers introduced to satisfy the linear constraints. This minimization problem is combined with the standard learning problem on the weight matrices. From this theoretical line of development, we obtain the surprising interpretation of Lagrange parameters as additional, penultimate layer hidden units with fixed weights stemming from the constraints. Consequently, standard minimization approaches can be used despite the inclusion of Lagrange parameters -- a very satisfying, albeit unexpected, discovery. Examples ranging from multi-label classification to constrained autoencoders are envisaged in the future. The code has been made available at https://github.com/anandrajan0/smartalec



### MicroISP: Processing 32MP Photos on Mobile Devices with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2211.06770v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.06770v1)
- **Published**: 2022-11-08 17:40:50+00:00
- **Updated**: 2022-11-08 17:40:50+00:00
- **Authors**: Andrey Ignatov, Anastasia Sycheva, Radu Timofte, Yu Tseng, Yu-Syuan Xu, Po-Hsiang Yu, Cheng-Ming Chiang, Hsien-Kai Kuo, Min-Hung Chen, Chia-Ming Cheng, Luc Van Gool
- **Comment**: arXiv admin note: text overlap with arXiv:2211.06263
- **Journal**: None
- **Summary**: While neural networks-based photo processing solutions can provide a better image quality compared to the traditional ISP systems, their application to mobile devices is still very limited due to their very high computational complexity. In this paper, we present a novel MicroISP model designed specifically for edge devices, taking into account their computational and memory limitations. The proposed solution is capable of processing up to 32MP photos on recent smartphones using the standard mobile ML libraries and requiring less than 1 second to perform the inference, while for FullHD images it achieves real-time performance. The architecture of the model is flexible, allowing to adjust its complexity to devices of different computational power. To evaluate the performance of the model, we collected a novel Fujifilm UltraISP dataset consisting of thousands of paired photos captured with a normal mobile camera sensor and a professional 102MP medium-format FujiFilm GFX100 camera. The experiments demonstrated that, despite its compact size, the MicroISP model is able to provide comparable or better visual results than the traditional mobile ISP systems, while outperforming the previously proposed efficient deep learning based solutions. Finally, this model is also compatible with the latest mobile AI accelerators, achieving good runtime and low power consumption on smartphone NPUs and APUs. The code, dataset and pre-trained models are available on the project website: https://people.ee.ethz.ch/~ihnatova/microisp.html



### Harmonizing the object recognition strategies of deep neural networks with humans
- **Arxiv ID**: http://arxiv.org/abs/2211.04533v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.04533v2)
- **Published**: 2022-11-08 20:03:49+00:00
- **Updated**: 2022-11-11 12:43:38+00:00
- **Authors**: Thomas Fel, Ivan Felipe, Drew Linsley, Thomas Serre
- **Comment**: Published at NeurIPS 2022
- **Journal**: None
- **Summary**: The many successes of deep neural networks (DNNs) over the past decade have largely been driven by computational scale rather than insights from biological intelligence. Here, we explore if these trends have also carried concomitant improvements in explaining the visual strategies humans rely on for object recognition. We do this by comparing two related but distinct properties of visual strategies in humans and DNNs: where they believe important visual features are in images and how they use those features to categorize objects. Across 84 different DNNs trained on ImageNet and three independent datasets measuring the where and the how of human visual strategies for object recognition on those images, we find a systematic trade-off between DNN categorization accuracy and alignment with human visual strategies for object recognition. State-of-the-art DNNs are progressively becoming less aligned with humans as their accuracy improves. We rectify this growing issue with our neural harmonizer: a general-purpose training routine that both aligns DNN and human visual strategies and improves categorization accuracy. Our work represents the first demonstration that the scaling laws that are guiding the design of DNNs today have also produced worse models of human vision. We release our code and data at https://serre-lab.github.io/Harmonization to help the field build more human-like DNNs.



### Going for GOAL: A Resource for Grounded Football Commentaries
- **Arxiv ID**: http://arxiv.org/abs/2211.04534v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2211.04534v1)
- **Published**: 2022-11-08 20:04:27+00:00
- **Updated**: 2022-11-08 20:04:27+00:00
- **Authors**: Alessandro Suglia, Jos√© Lopes, Emanuele Bastianelli, Andrea Vanzo, Shubham Agarwal, Malvina Nikandrou, Lu Yu, Ioannis Konstas, Verena Rieser
- **Comment**: Preprint formatted using the ACM Multimedia template (8 pages +
  appendix)
- **Journal**: None
- **Summary**: Recent video+language datasets cover domains where the interaction is highly structured, such as instructional videos, or where the interaction is scripted, such as TV shows. Both of these properties can lead to spurious cues to be exploited by models rather than learning to ground language. In this paper, we present GrOunded footbAlL commentaries (GOAL), a novel dataset of football (or `soccer') highlights videos with transcribed live commentaries in English. As the course of a game is unpredictable, so are commentaries, which makes them a unique resource to investigate dynamic language grounding. We also provide state-of-the-art baselines for the following tasks: frame reordering, moment retrieval, live commentary retrieval and play-by-play live commentary generation. Results show that SOTA models perform reasonably well in most tasks. We discuss the implications of these results and suggest new tasks for which GOAL can be used. Our codebase is available at: https://gitlab.com/grounded-sport-convai/goal-baselines.



### Estimation of Appearance and Occupancy Information in Birds Eye View from Surround Monocular Images
- **Arxiv ID**: http://arxiv.org/abs/2211.04557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2211.04557v1)
- **Published**: 2022-11-08 20:57:56+00:00
- **Updated**: 2022-11-08 20:57:56+00:00
- **Authors**: Sarthak Sharma, Unnikrishnan R. Nair, Udit Singh Parihar, Midhun Menon S, Srikanth Vidapanakal
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous driving requires efficient reasoning about the location and appearance of the different agents in the scene, which aids in downstream tasks such as object detection, object tracking, and path planning. The past few years have witnessed a surge in approaches that combine the different taskbased modules of the classic self-driving stack into an End-toEnd(E2E) trainable learning system. These approaches replace perception, prediction, and sensor fusion modules with a single contiguous module with shared latent space embedding, from which one extracts a human-interpretable representation of the scene. One of the most popular representations is the Birds-eye View (BEV), which expresses the location of different traffic participants in the ego vehicle frame from a top-down view. However, a BEV does not capture the chromatic appearance information of the participants. To overcome this limitation, we propose a novel representation that captures various traffic participants appearance and occupancy information from an array of monocular cameras covering 360 deg field of view (FOV). We use a learned image embedding of all camera images to generate a BEV of the scene at any instant that captures both appearance and occupancy of the scene, which can aid in downstream tasks such as object tracking and executing language-based commands. We test the efficacy of our approach on synthetic dataset generated from CARLA. The code, data set, and results can be found at https://rebrand.ly/APP OCC-results.



### Classification of Colorectal Cancer Polyps via Transfer Learning and Vision-Based Tactile Sensing
- **Arxiv ID**: http://arxiv.org/abs/2211.04573v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2211.04573v1)
- **Published**: 2022-11-08 21:47:36+00:00
- **Updated**: 2022-11-08 21:47:36+00:00
- **Authors**: Nethra Venkatayogi, Ozdemir Can Kara, Jeff Bonyun, Naruhiko Ikoma, Farshid Alambeigi
- **Comment**: Accepted to IEEE Sensors 2022 Conference
- **Journal**: None
- **Summary**: In this study, to address the current high earlydetection miss rate of colorectal cancer (CRC) polyps, we explore the potentials of utilizing transfer learning and machine learning (ML) classifiers to precisely and sensitively classify the type of CRC polyps. Instead of using the common colonoscopic images, we applied three different ML algorithms on the 3D textural image outputs of a unique vision-based surface tactile sensor (VS-TS). To collect realistic textural images of CRC polyps for training the utilized ML classifiers and evaluating their performance, we first designed and additively manufactured 48 types of realistic polyp phantoms with different hardness, type, and textures. Next, the performance of the used three ML algorithms in classifying the type of fabricated polyps was quantitatively evaluated using various statistical metrics.



### StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects
- **Arxiv ID**: http://arxiv.org/abs/2211.04604v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.04604v2)
- **Published**: 2022-11-08 23:04:49+00:00
- **Updated**: 2023-04-25 15:59:47+00:00
- **Authors**: Weiyu Liu, Yilun Du, Tucker Hermans, Sonia Chernova, Chris Paxton
- **Comment**: Accepted to Robotics: Science and Systems (RSS) 2023. The previous
  version appeared in CoRL Workshop on Language and Robot Learning 2022
- **Journal**: None
- **Summary**: Robots operating in human environments must be able to rearrange objects into semantically-meaningful configurations, even if these objects are previously unseen. In this work, we focus on the problem of building physically-valid structures without step-by-step instructions. We propose StructDiffusion, which combines a diffusion model and an object-centric transformer to construct structures given partial-view point clouds and high-level language goals, such as "set the table". Our method can perform multiple challenging language-conditioned multi-step 3D planning tasks using one model. StructDiffusion even improves the success rate of assembling physically-valid structures out of unseen objects by on average 16% over an existing multi-modal transformer model trained on specific structures. We show experiments on held-out objects in both simulation and on real-world rearrangement tasks. Importantly, we show how integrating both a diffusion model and a collision-discriminator model allows for improved generalization over other methods when rearranging previously-unseen objects. For videos and additional results, see our website: https://structdiffusion.github.io/.



