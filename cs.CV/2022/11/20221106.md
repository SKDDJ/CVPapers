# Arxiv Papers in cs.CV on 2022-11-06
### Distilling Representations from GAN Generator via Squeeze and Span
- **Arxiv ID**: http://arxiv.org/abs/2211.03000v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.03000v1)
- **Published**: 2022-11-06 01:10:28+00:00
- **Updated**: 2022-11-06 01:10:28+00:00
- **Authors**: Yu Yang, Xiaotian Cheng, Chang Liu, Hakan Bilen, Xiangyang Ji
- **Comment**: 16 pages, NeurIPS 2022
- **Journal**: None
- **Summary**: In recent years, generative adversarial networks (GANs) have been an actively studied topic and shown to successfully produce high-quality realistic images in various domains. The controllable synthesis ability of GAN generators suggests that they maintain informative, disentangled, and explainable image representations, but leveraging and transferring their representations to downstream tasks is largely unexplored. In this paper, we propose to distill knowledge from GAN generators by squeezing and spanning their representations. We squeeze the generator features into representations that are invariant to semantic-preserving transformations through a network before they are distilled into the student network. We span the distilled representation of the synthetic domain to the real domain by also using real training data to remedy the mode collapse of GANs and boost the student network performance in a real domain. Experiments justify the efficacy of our method and reveal its great significance in self-supervised representation learning. Code is available at https://github.com/yangyu12/squeeze-and-span.



### Learning to Annotate Part Segmentation with Gradient Matching
- **Arxiv ID**: http://arxiv.org/abs/2211.03003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.03003v1)
- **Published**: 2022-11-06 01:29:22+00:00
- **Updated**: 2022-11-06 01:29:22+00:00
- **Authors**: Yu Yang, Xiaotian Cheng, Hakan Bilen, Xiangyang Ji
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: The success of state-of-the-art deep neural networks heavily relies on the presence of large-scale labelled datasets, which are extremely expensive and time-consuming to annotate. This paper focuses on tackling semi-supervised part segmentation tasks by generating high-quality images with a pre-trained GAN and labelling the generated images with an automatic annotator. In particular, we formulate the annotator learning as a learning-to-learn problem. Given a pre-trained GAN, the annotator learns to label object parts in a set of randomly generated images such that a part segmentation model trained on these synthetic images with their predicted labels obtains low segmentation error on a small validation set of manually labelled images. We further reduce this nested-loop optimization problem to a simple gradient matching problem and efficiently solve it with an iterative algorithm. We show that our method can learn annotators from a broad range of labelled images including real images, generated images, and even analytically rendered images. Our method is evaluated with semi-supervised part segmentation tasks and significantly outperforms other semi-supervised competitors when the amount of labelled examples is extremely limited.



### Bringing Online Egocentric Action Recognition into the wild
- **Arxiv ID**: http://arxiv.org/abs/2211.03004v2
- **DOI**: 10.1109/LRA.2023.3251843
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.03004v2)
- **Published**: 2022-11-06 01:41:02+00:00
- **Updated**: 2023-03-09 19:27:59+00:00
- **Authors**: Gabriele Goletto, Mirco Planamente, Barbara Caputo, Giuseppe Averta
- **Comment**: Accepted to RA-L, for associated video, see
  https://www.youtube.com/watch?v=7rtynmoYnuw&t=9s
- **Journal**: None
- **Summary**: To enable a safe and effective human-robot cooperation, it is crucial to develop models for the identification of human activities. Egocentric vision seems to be a viable solution to solve this problem, and therefore many works provide deep learning solutions to infer human actions from first person videos. However, although very promising, most of these do not consider the major challenges that comes with a realistic deployment, such as the portability of the model, the need for real-time inference, and the robustness with respect to the novel domains (i.e., new spaces, users, tasks). With this paper, we set the boundaries that egocentric vision models should consider for realistic applications, defining a novel setting of egocentric action recognition in the wild, which encourages researchers to develop novel, applications-aware solutions. We also present a new model-agnostic technique that enables the rapid repurposing of existing architectures in this new context, demonstrating the feasibility to deploy a model on a tiny device (Jetson Nano) and to perform the task directly on the edge with very low energy consumption (2.4W on average at 50 fps). The code is publicly available at: https://github.com/EgocentricVision/EgoWild.



### A Geometrically Constrained Point Matching based on View-invariant Cross-ratios, and Homography
- **Arxiv ID**: http://arxiv.org/abs/2211.03007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.03007v1)
- **Published**: 2022-11-06 01:55:35+00:00
- **Updated**: 2022-11-06 01:55:35+00:00
- **Authors**: Yueh-Cheng Huang, Ching-Huai Yang, Chen-Tao Hsu, Jen-Hui Chuang
- **Comment**: None
- **Journal**: None
- **Summary**: In computer vision, finding point correspondence among images plays an important role in many applications, such as image stitching, image retrieval, visual localization, etc. Most of the research worksfocus on the matching of local feature before a sampling method is employed, such as RANSAC, to verify initial matching results via repeated fitting of certain global transformation among the images. However, incorrect matches may still exist, while careful examination of such problems is often skipped. Accordingly, a geometrically constrained algorithm is proposed in this work to verify the correctness of initially matched SIFT keypoints based on view-invariant cross-ratios (CRs). By randomly forming pentagons from these keypoints and matching their shape and location among images with CRs, robust planar region estimation can be achieved efficiently for the above verification, while correct and incorrect matches of keypoints can be examined easily with respect to those shape and location matched pentagons. Experimental results show that satisfactory results can be obtained for various scenes with single as well as multiple planar regions.



### Learning-based Inverse Rendering of Complex Indoor Scenes with Differentiable Monte Carlo Raytracing
- **Arxiv ID**: http://arxiv.org/abs/2211.03017v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2211.03017v2)
- **Published**: 2022-11-06 03:34:26+00:00
- **Updated**: 2022-11-23 08:08:39+00:00
- **Authors**: Jingsen Zhu, Fujun Luan, Yuchi Huo, Zihao Lin, Zhihua Zhong, Dianbing Xi, Jiaxiang Zheng, Rui Tang, Hujun Bao, Rui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Indoor scenes typically exhibit complex, spatially-varying appearance from global illumination, making inverse rendering a challenging ill-posed problem. This work presents an end-to-end, learning-based inverse rendering framework incorporating differentiable Monte Carlo raytracing with importance sampling. The framework takes a single image as input to jointly recover the underlying geometry, spatially-varying lighting, and photorealistic materials. Specifically, we introduce a physically-based differentiable rendering layer with screen-space ray tracing, resulting in more realistic specular reflections that match the input photo. In addition, we create a large-scale, photorealistic indoor scene dataset with significantly richer details like complex furniture and dedicated decorations. Further, we design a novel out-of-view lighting network with uncertainty-aware refinement leveraging hypernetwork-based neural radiance fields to predict lighting outside the view of the input photo. Through extensive evaluations on common benchmark datasets, we demonstrate superior inverse rendering quality of our method compared to state-of-the-art baselines, enabling various applications such as complex object insertion and material editing with high fidelity. Code and data will be made available at \url{https://jingsenzhu.github.io/invrend}.



### Hear The Flow: Optical Flow-Based Self-Supervised Visual Sound Source Localization
- **Arxiv ID**: http://arxiv.org/abs/2211.03019v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.03019v1)
- **Published**: 2022-11-06 03:48:45+00:00
- **Updated**: 2022-11-06 03:48:45+00:00
- **Authors**: Dennis Fedorishin, Deen Dayal Mohan, Bhavin Jawade, Srirangaraj Setlur, Venu Govindaraju
- **Comment**: Accepted to WACV 2023
- **Journal**: None
- **Summary**: Learning to localize the sound source in videos without explicit annotations is a novel area of audio-visual research. Existing work in this area focuses on creating attention maps to capture the correlation between the two modalities to localize the source of the sound. In a video, oftentimes, the objects exhibiting movement are the ones generating the sound. In this work, we capture this characteristic by modeling the optical flow in a video as a prior to better aid in localizing the sound source. We further demonstrate that the addition of flow-based attention substantially improves visual sound source localization. Finally, we benchmark our method on standard sound source localization datasets and achieve state-of-the-art performance on the Soundnet Flickr and VGG Sound Source datasets. Code: https://github.com/denfed/heartheflow.



### High-Fidelity Simulation and Novel Data Analysis of the Bubble Creation and Sound Generation Processes in Breaking Waves
- **Arxiv ID**: http://arxiv.org/abs/2211.03024v1
- **DOI**: None
- **Categories**: **physics.flu-dyn**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.03024v1)
- **Published**: 2022-11-06 04:29:26+00:00
- **Updated**: 2022-11-06 04:29:26+00:00
- **Authors**: Qiang Gao, Grant B. Deane, Saswata Basak, Umberto Bitencourt, Lian Shen
- **Comment**: conference
- **Journal**: 34th Symposium on Naval Hydrodynamics (2022)
- **Summary**: Recent increases in computing power have enabled the numerical simulation of many complex flow problems that are of practical and strategic interest for naval applications. A noticeable area of advancement is the computation of turbulent, two-phase flows resulting from wave breaking and other multiphase flow processes such as cavitation that can generate underwater sound and entrain bubbles in ship wakes, among other effects. Although advanced flow solvers are sophisticated and are capable of simulating high Reynolds number flows on large numbers of grid points, challenges in data analysis remain. Specifically, there is a critical need to transform highly resolved flow fields described on fine grids at discrete time steps into physically resolved features for which the flow dynamics can be understood and utilized in naval applications. This paper presents our recent efforts in this field. In previous works, we developed a novel algorithm to track bubbles in breaking wave simulations and to interpret their dynamical behavior over time (Gao et al., 2021a). We also discovered a new physical mechanism driving bubble production within breaking wave crests (Gao et al., 2021b) and developed a model to relate bubble behaviors to underwater sound generation (Gao et al., 2021c). In this work, we applied our bubble tracking algorithm to the breaking waves simulations and investigated the bubble trajectories, bubble creation mechanisms, and bubble acoustics based on our previous works.



### Learning Dual-Fused Modality-Aware Representations for RGBD Tracking
- **Arxiv ID**: http://arxiv.org/abs/2211.03055v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.03055v2)
- **Published**: 2022-11-06 07:59:07+00:00
- **Updated**: 2022-11-15 16:02:48+00:00
- **Authors**: Shang Gao, Jinyu Yang, Zhe Li, Feng Zheng, Aleš Leonardis, Jingkuan Song
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of depth sensors in recent years, RGBD object tracking has received significant attention. Compared with the traditional RGB object tracking, the addition of the depth modality can effectively solve the target and background interference. However, some existing RGBD trackers use the two modalities separately and thus some particularly useful shared information between them is ignored. On the other hand, some methods attempt to fuse the two modalities by treating them equally, resulting in the missing of modality-specific features. To tackle these limitations, we propose a novel Dual-fused Modality-aware Tracker (termed DMTracker) which aims to learn informative and discriminative representations of the target objects for robust RGBD tracking. The first fusion module focuses on extracting the shared information between modalities based on cross-modal attention. The second aims at integrating the RGB-specific and depth-specific information to enhance the fused features. By fusing both the modality-shared and modality-specific information in a modality-aware scheme, our DMTracker can learn discriminative representations in complex tracking scenes. Experiments show that our proposed tracker achieves very promising results on challenging RGBD benchmarks.



### Towards Real World HDRTV Reconstruction: A Data Synthesis-based Approach
- **Arxiv ID**: http://arxiv.org/abs/2211.03058v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.03058v1)
- **Published**: 2022-11-06 08:27:37+00:00
- **Updated**: 2022-11-06 08:27:37+00:00
- **Authors**: Zhen Cheng, Tao Wang, Yong Li, Fenglong Song, Chang Chen, Zhiwei Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Existing deep learning based HDRTV reconstruction methods assume one kind of tone mapping operators (TMOs) as the degradation procedure to synthesize SDRTV-HDRTV pairs for supervised training. In this paper, we argue that, although traditional TMOs exploit efficient dynamic range compression priors, they have several drawbacks on modeling the realistic degradation: information over-preservation, color bias and possible artifacts, making the trained reconstruction networks hard to generalize well to real-world cases. To solve this problem, we propose a learning-based data synthesis approach to learn the properties of real-world SDRTVs by integrating several tone mapping priors into both network structures and loss functions. In specific, we design a conditioned two-stream network with prior tone mapping results as a guidance to synthesize SDRTVs by both global and local transformations. To train the data synthesis network, we form a novel self-supervised content loss to constraint different aspects of the synthesized SDRTVs at regions with different brightness distributions and an adversarial loss to emphasize the details to be more realistic. To validate the effectiveness of our approach, we synthesize SDRTV-HDRTV pairs with our method and use them to train several HDRTV reconstruction networks. Then we collect two inference datasets containing both labeled and unlabeled real-world SDRTVs, respectively. Experimental results demonstrate that, the networks trained with our synthesized data generalize significantly better to these two real-world datasets than existing solutions.



### MyoPS-Net: Myocardial Pathology Segmentation with Flexible Combination of Multi-Sequence CMR Images
- **Arxiv ID**: http://arxiv.org/abs/2211.03062v1
- **DOI**: 10.1016/j.media.2022.102694
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.03062v1)
- **Published**: 2022-11-06 08:46:24+00:00
- **Updated**: 2022-11-06 08:46:24+00:00
- **Authors**: Junyi Qiu, Lei Li, Sihan Wang, Ke Zhang, Yinyin Chen, Shan Yang, Xiahai Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: Myocardial pathology segmentation (MyoPS) can be a prerequisite for the accurate diagnosis and treatment planning of myocardial infarction. However, achieving this segmentation is challenging, mainly due to the inadequate and indistinct information from an image. In this work, we develop an end-to-end deep neural network, referred to as MyoPS-Net, to flexibly combine five-sequence cardiac magnetic resonance (CMR) images for MyoPS. To extract precise and adequate information, we design an effective yet flexible architecture to extract and fuse cross-modal features. This architecture can tackle different numbers of CMR images and complex combinations of modalities, with output branches targeting specific pathologies. To impose anatomical knowledge on the segmentation results, we first propose a module to regularize myocardium consistency and localize the pathologies, and then introduce an inclusiveness loss to utilize relations between myocardial scars and edema. We evaluated the proposed MyoPS-Net on two datasets, i.e., a private one consisting of 50 paired multi-sequence CMR images and a public one from MICCAI2020 MyoPS Challenge. Experimental results showed that MyoPS-Net could achieve state-of-the-art performance in various scenarios. Note that in practical clinics, the subjects may not have full sequences, such as missing LGE CMR or mapping CMR scans. We therefore conducted extensive experiments to investigate the performance of the proposed method in dealing with such complex combinations of different CMR sequences. Results proved the superiority and generalizability of MyoPS-Net, and more importantly, indicated a practical clinical application.



### ViT-CX: Causal Explanation of Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2211.03064v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.03064v3)
- **Published**: 2022-11-06 09:06:16+00:00
- **Updated**: 2023-06-09 08:32:06+00:00
- **Authors**: Weiyan Xie, Xiao-Hui Li, Caleb Chen Cao, Nevin L. Zhang
- **Comment**: IJCAI2023 Camera-ready
- **Journal**: None
- **Summary**: Despite the popularity of Vision Transformers (ViTs) and eXplainable AI (XAI), only a few explanation methods have been designed specially for ViTs thus far. They mostly use attention weights of the [CLS] token on patch embeddings and often produce unsatisfactory saliency maps. This paper proposes a novel method for explaining ViTs called ViT-CX. It is based on patch embeddings, rather than attentions paid to them, and their causal impacts on the model output. Other characteristics of ViTs such as causal overdetermination are also considered in the design of ViT-CX. The empirical results show that ViT-CX produces more meaningful saliency maps and does a better job revealing all important evidence for the predictions than previous methods. The explanation generated by ViT-CX also shows significantly better faithfulness to the model. The codes and appendix are available at https://github.com/vaynexie/CausalX-ViT.



### Sequential Transformer for End-to-End Person Search
- **Arxiv ID**: http://arxiv.org/abs/2211.04323v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04323v2)
- **Published**: 2022-11-06 09:32:30+00:00
- **Updated**: 2022-11-13 05:11:49+00:00
- **Authors**: Long Chen, Jinhua Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Person Search aims to simultaneously localize and recognize a target person from realistic and uncropped gallery images. One major challenge of person search comes from the contradictory goals of the two sub-tasks, i.e., person detection focuses on finding the commonness of all persons so as to distinguish persons from the background, while person re-identification (re-ID) focuses on the differences among different persons. In this paper, we propose a novel Sequential Transformer (SeqTR) for end-to-end person search to deal with this challenge. Our SeqTR contains a detection transformer and a novel re-ID transformer that sequentially addresses detection and re-ID tasks. The re-ID transformer comprises the self-attention layer that utilizes contextual information and the cross-attention layer that learns local fine-grained discriminative features of the human body. Moreover, the re-ID transformer is shared and supervised by multi-scale features to improve the robustness of learned person representations. Extensive experiments on two widely-used person search benchmarks, CUHK-SYSU and PRW, show that our proposed SeqTR not only outperforms all existing person search methods with a 59.3% mAP on PRW but also achieves comparable performance to the state-of-the-art results with an mAP of 94.8% on CUHK-SYSU.



### BriFiSeg: a deep learning-based method for semantic and instance segmentation of nuclei in brightfield images
- **Arxiv ID**: http://arxiv.org/abs/2211.03072v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.03072v1)
- **Published**: 2022-11-06 10:03:04+00:00
- **Updated**: 2022-11-06 10:03:04+00:00
- **Authors**: Gendarme Mathieu, Lambert Annika M., El Debs Bachir
- **Comment**: None
- **Journal**: None
- **Summary**: Generally, microscopy image analysis in biology relies on the segmentation of individual nuclei, using a dedicated stained image, to identify individual cells. However stained nuclei have drawbacks like the need for sample preparation, and specific equipment on the microscope but most importantly, and as it is in most cases, the nuclear stain is not relevant to the biological questions of interest but is solely used for the segmentation task. In this study, we used non-stained brightfield images for nuclei segmentation with the advantage that they can be acquired on any microscope from both live or fixed samples and do not necessitate specific sample preparation. Nuclei semantic segmentation from brightfield images was obtained, on four distinct cell lines with U-Net-based architectures. We tested systematically deep pre-trained encoders to identify the best performing in combination with the different neural network architectures used. Additionally, two distinct and effective strategies were employed for instance segmentation, followed by thorough instance evaluation. We obtained effective semantic and instance segmentation of nuclei in brightfield images from standard test sets as well as from very diverse biological contexts triggered upon treatment with various small molecule inhibitor. The code used in this study was made public to allow further use by the community.



### Contrastive Weighted Learning for Near-Infrared Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/2211.03073v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.03073v2)
- **Published**: 2022-11-06 10:03:23+00:00
- **Updated**: 2022-12-08 14:03:52+00:00
- **Authors**: Adam Lee
- **Comment**: There were deficiencies in the experiment process for validation
- **Journal**: None
- **Summary**: Appearance-based gaze estimation has been very successful with the use of deep learning. Many following works improved domain generalization for gaze estimation. However, even though there has been much progress in domain generalization for gaze estimation, most of the recent work have been focused on cross-dataset performance -- accounting for different distributions in illuminations, head pose, and lighting. Although improving gaze estimation in different distributions of RGB images is important, near-infrared image based gaze estimation is also critical for gaze estimation in dark settings. Also there are inherent limitations relying solely on supervised learning for regression tasks. This paper contributes to solving these problems and proposes GazeCWL, a novel framework for gaze estimation with near-infrared images using contrastive learning. This leverages adversarial attack techniques for data augmentation and a novel contrastive loss function specifically for regression tasks that effectively clusters the features of different samples in the latent space. Our model outperforms previous domain generalization models in infrared image based gaze estimation and outperforms the baseline by 45.6\% while improving the state-of-the-art by 8.6\%, we demonstrate the efficacy of our method.



### A Sequence Agnostic Multimodal Preprocessing for Clogged Blood Vessel Detection in Alzheimer's Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2211.03109v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.03109v1)
- **Published**: 2022-11-06 13:22:05+00:00
- **Updated**: 2022-11-06 13:22:05+00:00
- **Authors**: Partho Ghosh, Md. Abrar Istiak, Mir Sayeed Mohammad, Swapnil Saha, Uday Kamal
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Successful identification of blood vessel blockage is a crucial step for Alzheimer's disease diagnosis. These blocks can be identified from the spatial and time-depth variable Two-Photon Excitation Microscopy (TPEF) images of the brain blood vessels using machine learning methods. In this study, we propose several preprocessing schemes to improve the performance of these methods. Our method includes 3D-point cloud data extraction from image modality and their feature-space fusion to leverage complementary information inherent in different modalities. We also enforce the learned representation to be sequence-order invariant by utilizing bi-direction dataflow. Experimental results on The Clog Loss dataset show that our proposed method consistently outperforms the state-of-the-art preprocessing methods in stalled and non-stalled vessel classification.



### StuArt: Individualized Classroom Observation of Students with Automatic Behavior Recognition and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2211.03127v3
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.03127v3)
- **Published**: 2022-11-06 14:08:04+00:00
- **Updated**: 2023-03-13 14:09:06+00:00
- **Authors**: Huayi Zhou, Fei Jiang, Jiaxin Si, Lili Xiong, Hongtao Lu
- **Comment**: accepted by ICASSP2023. Novel pedagogical approaches in signal
  processing for K-12 education
- **Journal**: None
- **Summary**: Each student matters, but it is hardly for instructors to observe all the students during the courses and provide helps to the needed ones immediately. In this paper, we present StuArt, a novel automatic system designed for the individualized classroom observation, which empowers instructors to concern the learning status of each student. StuArt can recognize five representative student behaviors (hand-raising, standing, sleeping, yawning, and smiling) that are highly related to the engagement and track their variation trends during the course. To protect the privacy of students, all the variation trends are indexed by the seat numbers without any personal identification information. Furthermore, StuArt adopts various user-friendly visualization designs to help instructors quickly understand the individual and whole learning status. Experimental results on real classroom videos have demonstrated the superiority and robustness of the embedded algorithms. We expect our system promoting the development of large-scale individualized guidance of students. More information is in https://github.com/hnuzhy/StuArt.



### MSMG-Net: Multi-scale Multi-grained Supervised Metworks for Multi-task Image Manipulation Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/2211.03140v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.03140v1)
- **Published**: 2022-11-06 14:58:21+00:00
- **Updated**: 2022-11-06 14:58:21+00:00
- **Authors**: Fengsheng Wang, Leyi Wei
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid advances of image editing techniques in recent years, image manipulation detection has attracted considerable attention since the increasing security risks posed by tampered images. To address these challenges, a novel multi-scale multi-grained deep network (MSMG-Net) is proposed to automatically identify manipulated regions. In our MSMG-Net, a parallel multi-scale feature extraction structure is used to extract multi-scale features. Then the multi-grained feature learning is utilized to perceive object-level semantics relation of multi-scale features by introducing the shunted self-attention. To fuse multi-scale multi-grained features, global and local feature fusion block are designed for manipulated region segmentation by a bottom-up approach and multi-level feature aggregation block is designed for edge artifacts detection by a top-down approach. Thus, MSMG-Net can effectively perceive the object-level semantics and encode the edge artifact. Experimental results on five benchmark datasets justify the superior performance of the proposed method, outperforming state-of-the-art manipulation detection and localization methods. Extensive ablation experiments and feature visualization demonstrate the multi-scale multi-grained learning can present effective visual representations of manipulated regions. In addition, MSMG-Net shows better robustness when various post-processing methods further manipulate images.



### MiddleGAN: Generate Domain Agnostic Samples for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2211.03144v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.03144v1)
- **Published**: 2022-11-06 15:09:36+00:00
- **Updated**: 2022-11-06 15:09:36+00:00
- **Authors**: Ye Gao, Zhendong Chu, Hongning Wang, John Stankovic
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, machine learning has achieved impressive results across different application areas. However, machine learning algorithms do not necessarily perform well on a new domain with a different distribution than its training set. Domain Adaptation (DA) is used to mitigate this problem. One approach of existing DA algorithms is to find domain invariant features whose distributions in the source domain are the same as their distribution in the target domain. In this paper, we propose to let the classifier that performs the final classification task on the target domain learn implicitly the invariant features to perform classification. It is achieved via feeding the classifier during training generated fake samples that are similar to samples from both the source and target domains. We call these generated samples domain-agnostic samples. To accomplish this we propose a novel variation of generative adversarial networks (GAN), called the MiddleGAN, that generates fake samples that are similar to samples from both the source and target domains, using two discriminators and one generator. We extend the theory of GAN to show that there exist optimal solutions for the parameters of the two discriminators and one generator in MiddleGAN, and empirically show that the samples generated by the MiddleGAN are similar to both samples from the source domain and samples from the target domain. We conducted extensive evaluations using 24 benchmarks; on the 24 benchmarks, we compare MiddleGAN against various state-of-the-art algorithms and outperform the state-of-the-art by up to 20.1\% on certain benchmarks.



### UATTA-ENS: Uncertainty Aware Test Time Augmented Ensemble for PIRC Diabetic Retinopathy Detection
- **Arxiv ID**: http://arxiv.org/abs/2211.03148v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.03148v2)
- **Published**: 2022-11-06 15:22:06+00:00
- **Updated**: 2022-11-08 18:42:00+00:00
- **Authors**: Pratinav Seth, Adil Khan, Ananya Gupta, Saurabh Kumar Mishra, Akshat Bhandari
- **Comment**: To Appear at Medical Imaging meets NeurIPS Workshop 2022
- **Journal**: None
- **Summary**: Deep Ensemble Convolutional Neural Networks has become a methodology of choice for analyzing medical images with a diagnostic performance comparable to a physician, including the diagnosis of Diabetic Retinopathy. However, commonly used techniques are deterministic and are therefore unable to provide any estimate of predictive uncertainty. Quantifying model uncertainty is crucial for reducing the risk of misdiagnosis. A reliable architecture should be well-calibrated to avoid over-confident predictions. To address this, we propose a UATTA-ENS: Uncertainty-Aware Test-Time Augmented Ensemble Technique for 5 Class PIRC Diabetic Retinopathy Classification to produce reliable and well-calibrated predictions.



### LG-Hand: Advancing 3D Hand Pose Estimation with Locally and Globally Kinematic Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2211.03151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.03151v1)
- **Published**: 2022-11-06 15:26:32+00:00
- **Updated**: 2022-11-06 15:26:32+00:00
- **Authors**: Tu Le-Xuan, Trung Tran-Quang, Thi Ngoc Hien Doan, Thanh-Hai Tran
- **Comment**: None
- **Journal**: None
- **Summary**: 3D hand pose estimation from RGB images suffers from the difficulty of obtaining the depth information. Therefore, a great deal of attention has been spent on estimating 3D hand pose from 2D hand joints. In this paper, we leverage the advantage of spatial-temporal Graph Convolutional Neural Networks and propose LG-Hand, a powerful method for 3D hand pose estimation. Our method incorporates both spatial and temporal dependencies into a single process. We argue that kinematic information plays an important role, contributing to the performance of 3D hand pose estimation. We thereby introduce two new objective functions, Angle and Direction loss, to take the hand structure into account. While Angle loss covers locally kinematic information, Direction loss handles globally kinematic one. Our LG-Hand achieves promising results on the First-Person Hand Action Benchmark (FPHAB) dataset. We also perform an ablation study to show the efficacy of the two proposed objective functions.



### ProtoX: Explaining a Reinforcement Learning Agent via Prototyping
- **Arxiv ID**: http://arxiv.org/abs/2211.03162v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.03162v1)
- **Published**: 2022-11-06 16:05:39+00:00
- **Updated**: 2022-11-06 16:05:39+00:00
- **Authors**: Ronilo J. Ragodos, Tong Wang, Qihang Lin, Xun Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: While deep reinforcement learning has proven to be successful in solving control tasks, the "black-box" nature of an agent has received increasing concerns. We propose a prototype-based post-hoc policy explainer, ProtoX, that explains a blackbox agent by prototyping the agent's behaviors into scenarios, each represented by a prototypical state. When learning prototypes, ProtoX considers both visual similarity and scenario similarity. The latter is unique to the reinforcement learning context, since it explains why the same action is taken in visually different states. To teach ProtoX about visual similarity, we pre-train an encoder using contrastive learning via self-supervised learning to recognize states as similar if they occur close together in time and receive the same action from the black-box agent. We then add an isometry layer to allow ProtoX to adapt scenario similarity to the downstream task. ProtoX is trained via imitation learning using behavior cloning, and thus requires no access to the environment or agent. In addition to explanation fidelity, we design different prototype shaping terms in the objective function to encourage better interpretability. We conduct various experiments to test ProtoX. Results show that ProtoX achieved high fidelity to the original black-box agent while providing meaningful and understandable explanations.



### Motion Style Transfer: Modular Low-Rank Adaptation for Deep Motion Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2211.03165v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2211.03165v1)
- **Published**: 2022-11-06 16:14:17+00:00
- **Updated**: 2022-11-06 16:14:17+00:00
- **Authors**: Parth Kothari, Danya Li, Yuejiang Liu, Alexandre Alahi
- **Comment**: CoRL 2022
- **Journal**: None
- **Summary**: Deep motion forecasting models have achieved great success when trained on a massive amount of data. Yet, they often perform poorly when training data is limited. To address this challenge, we propose a transfer learning approach for efficiently adapting pre-trained forecasting models to new domains, such as unseen agent types and scene contexts. Unlike the conventional fine-tuning approach that updates the whole encoder, our main idea is to reduce the amount of tunable parameters that can precisely account for the target domain-specific motion style. To this end, we introduce two components that exploit our prior knowledge of motion style shifts: (i) a low-rank motion style adapter that projects and adjusts the style features at a low-dimensional bottleneck; and (ii) a modular adapter strategy that disentangles the features of scene context and motion history to facilitate a fine-grained choice of adaptation layers. Through extensive experimentation, we show that our proposed adapter design, coined MoSA, outperforms prior methods on several forecasting benchmarks.



### Measurement-Consistent Networks via a Deep Implicit Layer for Solving Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/2211.03177v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.03177v2)
- **Published**: 2022-11-06 17:05:04+00:00
- **Updated**: 2023-03-13 07:06:55+00:00
- **Authors**: Rahul Mourya, João F. C. Mota
- **Comment**: None
- **Journal**: None
- **Summary**: End-to-end deep neural networks (DNNs) have become the state-of-the-art (SOTA) for solving inverse problems. Despite their outstanding performance, during deployment, such networks are sensitive to minor variations in the testing pipeline and often fail to reconstruct small but important details, a feature critical in medical imaging, astronomy, or defence. Such instabilities in DNNs can be explained by the fact that they ignore the forward measurement model during deployment, and thus fail to enforce consistency between their output and the input measurements. To overcome this, we propose a framework that transforms any DNN for inverse problems into a measurement-consistent one. This is done by appending to it an implicit layer (or deep equilibrium network) designed to solve a model-based optimization problem. The implicit layer consists of a shallow learnable network that can be integrated into the end-to-end training while keeping the SOTA DNN fixed. Experiments on single-image super-resolution show that the proposed framework leads to significant improvements in reconstruction quality and robustness over the SOTA DNNs.



### Understanding the properties and limitations of contrastive learning for Out-of-Distribution detection
- **Arxiv ID**: http://arxiv.org/abs/2211.03183v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.03183v1)
- **Published**: 2022-11-06 17:33:29+00:00
- **Updated**: 2022-11-06 17:33:29+00:00
- **Authors**: Nawid Keshtmand, Raul Santos-Rodriguez, Jonathan Lawry
- **Comment**: None
- **Journal**: None
- **Summary**: A recent popular approach to out-of-distribution (OOD) detection is based on a self-supervised learning technique referred to as contrastive learning. There are two main variants of contrastive learning, namely instance and class discrimination, targeting features that can discriminate between different instances for the former, and different classes for the latter.   In this paper, we aim to understand the effectiveness and limitation of existing contrastive learning methods for OOD detection. We approach this in 3 ways. First, we systematically study the performance difference between the instance discrimination and supervised contrastive learning variants in different OOD detection settings. Second, we study which in-distribution (ID) classes OOD data tend to be classified into. Finally, we study the spectral decay property of the different contrastive learning approaches and examine how it correlates with OOD detection performance. In scenarios where the ID and OOD datasets are sufficiently different from one another, we see that instance discrimination, in the absence of fine-tuning, is competitive with supervised approaches in OOD detection. We see that OOD samples tend to be classified into classes that have a distribution similar to the distribution of the entire dataset. Furthermore, we show that contrastive learning learns a feature space that contains singular vectors containing several directions with a high variance which can be detrimental or beneficial to OOD detection depending on the inference approach used.



### A Deep-Unfolded Spatiotemporal RPCA Network For L+S Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2211.03184v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.03184v1)
- **Published**: 2022-11-06 17:33:30+00:00
- **Updated**: 2022-11-06 17:33:30+00:00
- **Authors**: Shoaib Imran, Muhammad Tahir, Zubair Khalid, Momin Uppal
- **Comment**: None
- **Journal**: None
- **Summary**: Low-rank and sparse decomposition based methods find their use in many applications involving background modeling such as clutter suppression and object tracking. While Robust Principal Component Analysis (RPCA) has achieved great success in performing this task, it can take hundreds of iterations to converge and its performance decreases in the presence of different phenomena such as occlusion, jitter and fast motion. The recently proposed deep unfolded networks, on the other hand, have demonstrated better accuracy and improved convergence over both their iterative equivalents as well as over other neural network architectures. In this work, we propose a novel deep unfolded spatiotemporal RPCA (DUST-RPCA) network, which explicitly takes advantage of the spatial and temporal continuity in the low-rank component. Our experimental results on the moving MNIST dataset indicate that DUST-RPCA gives better accuracy when compared with the existing state of the art deep unfolded RPCA networks.



### Momentum-based Weight Interpolation of Strong Zero-Shot Models for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2211.03186v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.03186v1)
- **Published**: 2022-11-06 17:41:39+00:00
- **Updated**: 2022-11-06 17:41:39+00:00
- **Authors**: Zafir Stojanovski, Karsten Roth, Zeynep Akata
- **Comment**: First Workshop on Interpolation Regularizers and Beyond, NeurIPS 2022
  (Spotlight) and Workshop on Distribution Shifts, NeurIPS 2022
- **Journal**: None
- **Summary**: Large pre-trained, zero-shot capable models have shown considerable success both for standard transfer and adaptation tasks, with particular robustness towards distribution shifts. In addition, subsequent fine-tuning can considerably improve performance on a selected downstream task. However, through naive fine-tuning, these zero-shot models lose their generalizability and robustness towards distribution shifts. This is a particular problem for tasks such as Continual Learning (CL), where continuous adaptation has to be performed as new task distributions are introduced sequentially. In this work, we showcase that where fine-tuning falls short to adapt such zero-shot capable models, simple momentum-based weight interpolation can provide consistent improvements for CL tasks in both memory-free and memory-based settings. In particular, we find improvements of over $+4\%$ on standard CL benchmarks, while reducing the error to the upper limit of jointly training on all tasks at once in parts by more than half, allowing the continual learner to inch closer to the joint training limits.



### KGTN-ens: Few-Shot Image Classification with Knowledge Graph Ensembles
- **Arxiv ID**: http://arxiv.org/abs/2211.03199v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.4.0; I.2.10; I.2.4
- **Links**: [PDF](http://arxiv.org/pdf/2211.03199v1)
- **Published**: 2022-11-06 18:44:04+00:00
- **Updated**: 2022-11-06 18:44:04+00:00
- **Authors**: Dominik Filipiak, Anna Fensel, Agata Filipowska
- **Comment**: None
- **Journal**: None
- **Summary**: We propose KGTN-ens, a framework extending the recent Knowledge Graph Transfer Network (KGTN) in order to incorporate multiple knowledge graph embeddings at a small cost. We evaluate it with different combinations of embeddings in a few-shot image classification task. We also construct a new knowledge source - Wikidata embeddings - and evaluate it with KGTN and KGTN-ens. Our approach outperforms KGTN in terms of the top-5 accuracy on the ImageNet-FS dataset for the majority of tested settings.



### Towards real-time 6D pose estimation of objects in single-view cone-beam X-ray
- **Arxiv ID**: http://arxiv.org/abs/2211.03211v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.03211v1)
- **Published**: 2022-11-06 20:06:28+00:00
- **Updated**: 2022-11-06 20:06:28+00:00
- **Authors**: Christiaan G. A. Viviers, Joel de Bruijn, Lena Filatova, Peter H. N. de With, Fons van der Sommen
- **Comment**: Published at SPIE Medical Imaging 2022
- **Journal**: None
- **Summary**: Deep learning-based pose estimation algorithms can successfully estimate the pose of objects in an image, especially in the field of color images. 6D Object pose estimation based on deep learning models for X-ray images often use custom architectures that employ extensive CAD models and simulated data for training purposes. Recent RGB-based methods opt to solve pose estimation problems using small datasets, making them more attractive for the X-ray domain where medical data is scarcely available. We refine an existing RGB-based model (SingleShotPose) to estimate the 6D pose of a marked cube from grayscale X-ray images by creating a generic solution trained on only real X-ray data and adjusted for X-ray acquisition geometry. The model regresses 2D control points and calculates the pose through 2D/3D correspondences using Perspective-n-Point(PnP), allowing a single trained model to be used across all supporting cone-beam-based X-ray geometries. Since modern X-ray systems continuously adjust acquisition parameters during a procedure, it is essential for such a pose estimation network to consider these parameters in order to be deployed successfully and find a real use case. With a 5-cm/5-degree accuracy of 93% and an average 3D rotation error of 2.2 degrees, the results of the proposed approach are comparable with state-of-the-art alternatives, while requiring significantly less real training examples and being applicable in real-time applications.



### Cementron: Machine Learning the Constituent Phases in Cement Clinker from Optical Images
- **Arxiv ID**: http://arxiv.org/abs/2211.03223v1
- **DOI**: None
- **Categories**: **cs.CV**, cond-mat.mtrl-sci, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.03223v1)
- **Published**: 2022-11-06 21:45:30+00:00
- **Updated**: 2022-11-06 21:45:30+00:00
- **Authors**: Mohd Zaki, Siddhant Sharma, Sunil Kumar Gurjar, Raju Goyal, Jayadeva, N. M. Anoop Krishnan
- **Comment**: None
- **Journal**: None
- **Summary**: Cement is the most used construction material. The performance of cement hydrate depends on the constituent phases, viz. alite, belite, aluminate, and ferrites present in the cement clinker, both qualitatively and quantitatively. Traditionally, clinker phases are analyzed from optical images relying on a domain expert and simple image processing techniques. However, the non-uniformity of the images, variations in the geometry and size of the phases, and variabilities in the experimental approaches and imaging methods make it challenging to obtain the phases. Here, we present a machine learning (ML) approach to detect clinker microstructure phases automatically. To this extent, we create the first annotated dataset of cement clinker by segmenting alite and belite particles. Further, we use supervised ML methods to train models for identifying alite and belite regions. Specifically, we finetune the image detection and segmentation model Detectron-2 on the cement microstructure to develop a model for detecting the cement phases, namely, Cementron. We demonstrate that Cementron, trained only on literature data, works remarkably well on new images obtained from our experiments, demonstrating its generalizability. We make Cementron available for public use.



