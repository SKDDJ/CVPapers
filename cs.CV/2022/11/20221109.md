# Arxiv Papers in cs.CV on 2022-11-09
### Cold Start Streaming Learning for Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2211.04624v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, 68T07, I.2.6; I.2.10; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2211.04624v1)
- **Published**: 2022-11-09 00:53:19+00:00
- **Updated**: 2022-11-09 00:53:19+00:00
- **Authors**: Cameron R. Wolfe, Anastasios Kyrillidis
- **Comment**: 52 pages, 7 figures, pre-print
- **Journal**: None
- **Summary**: The ability to dynamically adapt neural networks to newly-available data without performance deterioration would revolutionize deep learning applications. Streaming learning (i.e., learning from one data example at a time) has the potential to enable such real-time adaptation, but current approaches i) freeze a majority of network parameters during streaming and ii) are dependent upon offline, base initialization procedures over large subsets of data, which damages performance and limits applicability. To mitigate these shortcomings, we propose Cold Start Streaming Learning (CSSL), a simple, end-to-end approach for streaming learning with deep networks that uses a combination of replay and data augmentation to avoid catastrophic forgetting.   Because CSSL updates all model parameters during streaming, the algorithm is capable of beginning streaming from a random initialization, making base initialization optional. Going further, the algorithm's simplicity allows theoretical convergence guarantees to be derived using analysis of the Neural Tangent Random Feature (NTRF). In experiments, we find that CSSL outperforms existing baselines for streaming learning in experiments on CIFAR100, ImageNet, and Core50 datasets. Additionally, we propose a novel multi-task streaming learning setting and show that CSSL performs favorably in this domain. Put simply, CSSL performs well and demonstrates that the complicated, multi-step training pipelines adopted by most streaming methodologies can be replaced with a simple, end-to-end learning approach without sacrificing performance.



### Soft Augmentation for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2211.04625v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04625v1)
- **Published**: 2022-11-09 01:04:06+00:00
- **Updated**: 2022-11-09 01:04:06+00:00
- **Authors**: Yang Liu, Shen Yan, Laura Leal-Taixé, James Hays, Deva Ramanan
- **Comment**: None
- **Journal**: None
- **Summary**: Modern neural networks are over-parameterized and thus rely on strong regularization such as data augmentation and weight decay to reduce overfitting and improve generalization. The dominant form of data augmentation applies invariant transforms, where the learning target of a sample is invariant to the transform applied to that sample. We draw inspiration from human visual classification studies and propose generalizing augmentation with invariant transforms to soft augmentation where the learning target softens non-linearly as a function of the degree of the transform applied to the sample: e.g., more aggressive image crop augmentations produce less confident learning targets. We demonstrate that soft targets allow for more aggressive data augmentation, offer more robust performance boosts, work with other augmentation policies, and interestingly, produce better calibrated models (since they are trained to be less confident on aggressively cropped/occluded examples). Combined with existing aggressive augmentation strategies, soft target 1) doubles the top-1 accuracy boost across Cifar-10, Cifar-100, ImageNet-1K, and ImageNet-V2, 2) improves model occlusion performance by up to $4\times$, and 3) halves the expected calibration error (ECE). Finally, we show that soft augmentation generalizes to self-supervised classification tasks.



### Gold-standard of HER2 breast cancer biopsies using supervised learning based on multiple pathologist annotations
- **Arxiv ID**: http://arxiv.org/abs/2211.04649v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04649v1)
- **Published**: 2022-11-09 02:55:20+00:00
- **Updated**: 2022-11-09 02:55:20+00:00
- **Authors**: Benjamín Hernández, Violeta Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer is one of the most common cancer in women around the world. For diagnosis, pathologists evaluate biomarkers such as HER2 protein using immunohistochemistry over tissue extracted by a biopsy. Through microscopic inspection, this assessment estimates the intensity and integrity of the membrane cells' staining and scores the sample as 0, 1+, 2+, or 3+: a subjective decision that depends on the interpretation of the pathologist. This paper presents the preliminary data analysis of the annotations of three pathologists over the same set of samples obtained using 20x magnification and including $1,252$ non-overlapping biopsy patches. We evaluate the intra- and inter-expert variability achieving substantial and moderate agreement, respectively, according to Fleiss' Kappa coefficient, as a previous stage towards a generation of a HER2 breast cancer biopsy gold-standard using supervised learning from multiple pathologist annotations.



### MEVID: Multi-view Extended Videos with Identities for Video Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2211.04656v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04656v2)
- **Published**: 2022-11-09 03:07:31+00:00
- **Updated**: 2022-11-10 14:35:24+00:00
- **Authors**: Daniel Davila, Dawei Du, Bryon Lewis, Christopher Funk, Joseph Van Pelt, Roderick Collins, Kellie Corona, Matt Brown, Scott McCloskey, Anthony Hoogs, Brian Clipp
- **Comment**: This paper was accepted to WACV 2023
- **Journal**: None
- **Summary**: In this paper, we present the Multi-view Extended Videos with Identities (MEVID) dataset for large-scale, video person re-identification (ReID) in the wild. To our knowledge, MEVID represents the most-varied video person ReID dataset, spanning an extensive indoor and outdoor environment across nine unique dates in a 73-day window, various camera viewpoints, and entity clothing changes. Specifically, we label the identities of 158 unique people wearing 598 outfits taken from 8, 092 tracklets, average length of about 590 frames, seen in 33 camera views from the very large-scale MEVA person activities dataset. While other datasets have more unique identities, MEVID emphasizes a richer set of information about each individual, such as: 4 outfits/identity vs. 2 outfits/identity in CCVID, 33 viewpoints across 17 locations vs. 6 in 5 simulated locations for MTA, and 10 million frames vs. 3 million for LS-VID. Being based on the MEVA video dataset, we also inherit data that is intentionally demographically balanced to the continental United States. To accelerate the annotation process, we developed a semi-automatic annotation framework and GUI that combines state-of-the-art real-time models for object detection, pose estimation, person ReID, and multi-object tracking. We evaluate several state-of-the-art methods on MEVID challenge problems and comprehensively quantify their robustness in terms of changes of outfit, scale, and background location. Our quantitative analysis on the realistic, unique aspects of MEVID shows that there are significant remaining challenges in video person ReID and indicates important directions for future research.



### SUPRA: Superpixel Guided Loss for Improved Multi-modal Segmentation in Endoscopy
- **Arxiv ID**: http://arxiv.org/abs/2211.04658v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2211.04658v3)
- **Published**: 2022-11-09 03:13:59+00:00
- **Updated**: 2023-04-09 18:30:47+00:00
- **Authors**: Rafael Martinez-Garcia-Peña, Mansoor Ali Teevno, Gilberto Ochoa-Ruiz, Sharib Ali
- **Comment**: This work has been accepted at the LatinX in Computer Vision Research
  Workshop at CVPR 2023
- **Journal**: None
- **Summary**: Domain shift is a well-known problem in the medical imaging community. In particular, for endoscopic image analysis where the data can have different modalities the performance of deep learning (DL) methods gets adversely affected. In other words, methods developed on one modality cannot be used for a different modality. However, in real clinical settings, endoscopists switch between modalities for better mucosal visualisation. In this paper, we explore the domain generalisation technique to enable DL methods to be used in such scenarios. To this extend, we propose to use super pixels generated with Simple Linear Iterative Clustering (SLIC) which we refer to as "SUPRA" for SUPeRpixel Augmented method. SUPRA first generates a preliminary segmentation mask making use of our new loss "SLICLoss" that encourages both an accurate and color-consistent segmentation. We demonstrate that SLICLoss when combined with Binary Cross Entropy loss (BCE) can improve the model's generalisability with data that presents significant domain shift. We validate this novel compound loss on a vanilla U-Net using the EndoUDA dataset, which contains images for Barret's Esophagus and polyps from two modalities. We show that our method yields an improvement of nearly 20% in the target domain set compared to the baseline.



### Combination of multiple neural networks using transfer learning and extensive geometric data augmentation for assessing cellularity scores in histopathology images
- **Arxiv ID**: http://arxiv.org/abs/2211.04675v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2211.04675v1)
- **Published**: 2022-11-09 04:29:15+00:00
- **Updated**: 2022-11-09 04:29:15+00:00
- **Authors**: Jacob D. Beckmann, Kosta Popovic
- **Comment**: 7 pages (includes a cover page), 5 figures, 1 table, work addresses
  the BreastPathQ challenge
- **Journal**: None
- **Summary**: Classification of cancer cellularity within tissue samples is currently a manual process performed by pathologists. This process of correctly determining cancer cellularity can be time intensive. Deep Learning (DL) techniques in particular have become increasingly more popular for this purpose, due to the accuracy and performance they exhibit, which can be comparable to the pathologists. This work investigates the capabilities of two DL approaches to assess cancer cellularity in whole slide images (WSI) in the SPIE-AAPM-NCI BreastPathQ challenge dataset. The effects of training on augmented data via rotations, and combinations of multiple architectures into a single network were analyzed using a modified Kendall Tau-b prediction probability metric known as the average prediction probability PK. A deep, transfer learned, Convolutional Neural Network (CNN) InceptionV3 was used as a baseline, achieving an average PK value of 0.884, showing improvement from the average PK value of 0.83 achieved by pathologists. The network was then trained on additional training datasets which were rotated between 1 and 360 degrees, which saw a peak increase of PK up to 4.2%. An additional architecture consisting of the InceptionV3 network and VGG16, a shallow, transfer learned CNN, was combined in a parallel architecture. This parallel architecture achieved a baseline average PK value of 0.907, a statistically significantly improvement over either of the architectures' performances separately (p<0.0001 by unpaired t-test).



### Deep Learning based Computer Vision Methods for Complex Traffic Environments Perception: A Review
- **Arxiv ID**: http://arxiv.org/abs/2211.05120v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.05120v1)
- **Published**: 2022-11-09 05:16:01+00:00
- **Updated**: 2022-11-09 05:16:01+00:00
- **Authors**: Talha Azfar, Jinlong Li, Hongkai Yu, Ruey Long Cheu, Yisheng Lv, Ruimin Ke
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision applications in intelligent transportation systems (ITS) and autonomous driving (AD) have gravitated towards deep neural network architectures in recent years. While performance seems to be improving on benchmark datasets, many real-world challenges are yet to be adequately considered in research. This paper conducted an extensive literature review on the applications of computer vision in ITS and AD, and discusses challenges related to data, models, and complex urban environments. The data challenges are associated with the collection and labeling of training data and its relevance to real world conditions, bias inherent in datasets, the high volume of data needed to be processed, and privacy concerns. Deep learning (DL) models are commonly too complex for real-time processing on embedded hardware, lack explainability and generalizability, and are hard to test in real-world settings. Complex urban traffic environments have irregular lighting and occlusions, and surveillance cameras can be mounted at a variety of angles, gather dirt, shake in the wind, while the traffic conditions are highly heterogeneous, with violation of rules and complex interactions in crowded scenarios. Some representative applications that suffer from these problems are traffic flow estimation, congestion detection, autonomous driving perception, vehicle interaction, and edge computing for practical deployment. The possible ways of dealing with the challenges are also explored while prioritizing practical deployment.



### Lightweight network towards real-time image denoising on mobile devices
- **Arxiv ID**: http://arxiv.org/abs/2211.04687v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04687v2)
- **Published**: 2022-11-09 05:19:26+00:00
- **Updated**: 2023-05-25 15:23:27+00:00
- **Authors**: Zhuoqun Liu, Meiguang Jin, Ying Chen, Huaida Liu, Canqian Yang, Hongkai Xiong
- **Comment**: Under review at the 2023 IEEE International Conference on Image
  Processing (ICIP 2023)
- **Journal**: None
- **Summary**: Deep convolutional neural networks have achieved great progress in image denoising tasks. However, their complicated architectures and heavy computational cost hinder their deployments on mobile devices. Some recent efforts in designing lightweight denoising networks focus on reducing either FLOPs (floating-point operations) or the number of parameters. However, these metrics are not directly correlated with the on-device latency. In this paper, we identify the real bottlenecks that affect the CNN-based models' run-time performance on mobile devices: memory access cost and NPU-incompatible operations, and build the model based on these. To further improve the denoising performance, the mobile-friendly attention module MFA and the model reparameterization module RepConv are proposed, which enjoy both low latency and excellent denoising performance. To this end, we propose a mobile-friendly denoising network, namely MFDNet. The experiments show that MFDNet achieves state-of-the-art performance on real-world denoising benchmarks SIDD and DND under real-time latency on mobile devices. The code and pre-trained models will be released.



### A Solution for a Fundamental Problem of 3D Inference based on 2D Representations
- **Arxiv ID**: http://arxiv.org/abs/2211.04691v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04691v1)
- **Published**: 2022-11-09 05:37:01+00:00
- **Updated**: 2022-11-09 05:37:01+00:00
- **Authors**: Thien An L. Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: 3D inference from monocular vision using neural networks is an important research area of computer vision. Applications of the research area are various with many proposed solutions and have shown remarkable performance. Although many efforts have been invested, there are still unanswered questions, some of which are fundamental. In this paper, I discuss a problem that I hope will come to be known as a generalization of the Blind Perspective-n-Point (Blind PnP) problem for object-driven 3D inference based on 2D representations. The vital difference between the fundamental problem and the Blind PnP problem is that 3D inference parameters in the fundamental problem are attached directly to 3D points and the camera concept will be represented through the sharing of the parameters of these points. By providing an explainable and robust gradient-decent solution based on 2D representations for an important special case of the problem, the paper opens up a new approach for using available information-based learning methods to solve problems related to 3D object pose estimation from 2D images.



### Robust Point Cloud Registration Framework Based on Deep Graph Matching(TPAMI Version)
- **Arxiv ID**: http://arxiv.org/abs/2211.04696v1
- **DOI**: 10.1109/TPAMI.2022.3204713
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04696v1)
- **Published**: 2022-11-09 06:05:25+00:00
- **Updated**: 2022-11-09 06:05:25+00:00
- **Authors**: Kexue Fu, Jiazheng Luo, Xiaoyuan Luo, Shaolei Liu, Chenxi Zhang, Manning Wang
- **Comment**: accepted by TPAMI 2022. arXiv admin note: substantial text overlap
  with arXiv:2103.04256
- **Journal**: None
- **Summary**: 3D point cloud registration is a fundamental problem in computer vision and robotics. Recently, learning-based point cloud registration methods have made great progress. However, these methods are sensitive to outliers, which lead to more incorrect correspondences. In this paper, we propose a novel deep graph matching-based framework for point cloud registration. Specifically, we first transform point clouds into graphs and extract deep features for each point. Then, we develop a module based on deep graph matching to calculate a soft correspondence matrix. By using graph matching, not only the local geometry of each point but also its structure and topology in a larger range are considered in establishing correspondences, so that more correct correspondences are found. We train the network with a loss directly defined on the correspondences, and in the test stage the soft correspondences are transformed into hard one-to-one correspondences so that registration can be performed by a correspondence-based solver. Furthermore, we introduce a transformer-based method to generate edges for graph construction, which further improves the quality of the correspondences. Extensive experiments on object-level and scene-level benchmark datasets show that the proposed method achieves state-of-the-art performance. The code is available at: \href{https://github.com/fukexue/RGM}{https://github.com/fukexue/RGM}.



### NoiSER: Noise is All You Need for Low-Light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2211.04700v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04700v2)
- **Published**: 2022-11-09 06:18:18+00:00
- **Updated**: 2022-11-27 14:54:18+00:00
- **Authors**: Zhao Zhang, Suiyi Zhao, Xiaojie Jin, Mingliang Xu, Yi Yang, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an embarrassingly simple yet effective solution to a seemingly impossible mission, low-light image enhancement (LLIE) without access to any task-related data. The proposed solution, Noise SElf-Regression (NoiSER), simply learns a convolutional neural network equipped with a instance-normalization layer by taking a random noise image, $\mathcal{N}(0,\sigma^2)$ for each pixel, as both input and output for each training pair, and then the low-light image is fed to the learned network for predicting the normal-light image. Technically, an intuitive explanation for its effectiveness is as follows: 1) the self-regression reconstructs the contrast between adjacent pixels of the input image, 2) the instance-normalization layers may naturally remediate the overall magnitude/lighting of the input image, and 3) the $\mathcal{N}(0,\sigma^2)$ assumption for each pixel enforces the output image to follow the well-known gray-world hypothesis \cite{Gary-world_Hypothesis} when the image size is big enough, namely, the averages of three RGB components of an image converge to the same value. Compared to existing SOTA LLIE methods with access to different task-related data, NoiSER is surprisingly highly competitive in enhancement quality, yet with a much smaller model size, and much lower training and inference cost. With only $\sim$ 1K parameters, NoiSER realizes about 1 minute for training and 1.2 ms for inference with 600x400 resolution on RTX 2080 Ti. As a bonus, NoiSER possesses automated over-exposure suppression ability and shows excellent performance on over-exposed photos.



### Automated MRI Field of View Prescription from Region of Interest Prediction by Intra-stack Attention Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2211.04703v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04703v1)
- **Published**: 2022-11-09 06:40:18+00:00
- **Updated**: 2022-11-09 06:40:18+00:00
- **Authors**: Ke Lei, Ali B. Syed, Xucheng Zhu, John M. Pauly, Shreyas S. Vasanawala
- **Comment**: None
- **Journal**: None
- **Summary**: Manual prescription of the field of view (FOV) by MRI technologists is variable and prolongs the scanning process. Often, the FOV is too large or crops critical anatomy. We propose a deep-learning framework, trained by radiologists' supervision, for automating FOV prescription. An intra-stack shared feature extraction network and an attention network are used to process a stack of 2D image inputs to generate output scalars defining the location of a rectangular region of interest (ROI). The attention mechanism is used to make the model focus on the small number of informative slices in a stack. Then the smallest FOV that makes the neural network predicted ROI free of aliasing is calculated by an algebraic operation derived from MR sampling theory. We retrospectively collected 595 cases between February 2018 and February 2022. The framework's performance is examined quantitatively with intersection over union (IoU) and pixel error on position, and qualitatively with a reader study. We use the t-test for comparing quantitative results from all models and a radiologist. The proposed model achieves an average IoU of 0.867 and average ROI position error of 9.06 out of 512 pixels on 80 test cases, significantly better (P<0.05) than two baseline models and not significantly different from a radiologist (P>0.12). Finally, the FOV given by the proposed framework achieves an acceptance rate of 92% from an experienced radiologist.



### Efficient Joint Detection and Multiple Object Tracking with Spatially Aware Transformer
- **Arxiv ID**: http://arxiv.org/abs/2211.05654v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.05654v1)
- **Published**: 2022-11-09 07:19:33+00:00
- **Updated**: 2022-11-09 07:19:33+00:00
- **Authors**: Siddharth Sagar Nijhawan, Leo Hoshikawa, Atsushi Irie, Masakazu Yoshimura, Junji Otsuka, Takeshi Ohashi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a light-weight and highly efficient Joint Detection and Tracking pipeline for the task of Multi-Object Tracking using a fully-transformer architecture. It is a modified version of TransTrack, which overcomes the computational bottleneck associated with its design, and at the same time, achieves state-of-the-art MOTA score of 73.20%. The model design is driven by a transformer based backbone instead of CNN, which is highly scalable with the input resolution. We also propose a drop-in replacement for Feed Forward Network of transformer encoder layer, by using Butterfly Transform Operation to perform channel fusion and depth-wise convolution to learn spatial context within the feature maps, otherwise missing within the attention maps of the transformer. As a result of our modifications, we reduce the overall model size of TransTrack by 58.73% and the complexity by 78.72%. Therefore, we expect our design to provide novel perspectives for architecture optimization in future research related to multi-object tracking.



### Efficient Neural Mapping for Localisation of Unmanned Ground Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2211.04718v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04718v1)
- **Published**: 2022-11-09 07:23:28+00:00
- **Updated**: 2022-11-09 07:23:28+00:00
- **Authors**: Christopher J. Holder, Muhammad Shafique
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Global localisation from visual data is a challenging problem applicable to many robotics domains. Prior works have shown that neural networks can be trained to map images of an environment to absolute camera pose within that environment, learning an implicit neural mapping in the process. In this work we evaluate the applicability of such an approach to real-world robotics scenarios, demonstrating that by constraining the problem to 2-dimensions and significantly increasing the quantity of training data, a compact model capable of real-time inference on embedded platforms can be used to achieve localisation accuracy of several centimetres. We deploy our trained model onboard a UGV platform, demonstrating its effectiveness in a waypoint navigation task. Along with this work we will release a novel localisation dataset comprising simulated and real environments, each with training samples numbering in the tens of thousands.



### ReFu: Refine and Fuse the Unobserved View for Detail-Preserving Single-Image 3D Human Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2211.04753v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04753v1)
- **Published**: 2022-11-09 09:14:11+00:00
- **Updated**: 2022-11-09 09:14:11+00:00
- **Authors**: Gyumin Shim, Minsoo Lee, Jaegul Choo
- **Comment**: Accepted at ACM MM 2022
- **Journal**: None
- **Summary**: Single-image 3D human reconstruction aims to reconstruct the 3D textured surface of the human body given a single image. While implicit function-based methods recently achieved reasonable reconstruction performance, they still bear limitations showing degraded quality in both surface geometry and texture from an unobserved view. In response, to generate a realistic textured surface, we propose ReFu, a coarse-to-fine approach that refines the projected backside view image and fuses the refined image to predict the final human body. To suppress the diffused occupancy that causes noise in projection images and reconstructed meshes, we propose to train occupancy probability by simultaneously utilizing 2D and 3D supervisions with occupancy-based volume rendering. We also introduce a refinement architecture that generates detail-preserving backside-view images with front-to-back warping. Extensive experiments demonstrate that our method achieves state-of-the-art performance in 3D human reconstruction from a single image, showing enhanced geometry and texture quality from an unobserved view.



### Towards Global Crop Maps with Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2211.04755v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.04755v2)
- **Published**: 2022-11-09 09:17:42+00:00
- **Updated**: 2022-11-10 10:33:16+00:00
- **Authors**: Hyun-Woo Jo, Alkiviadis Koukos, Vasileios Sitokonstantinou, Woo-Kyun Lee, Charalampos Kontoes
- **Comment**: Accepted for publication at Tackling Climate Change with Machine
  Learning: workshop at NeurIPS 2022
- **Journal**: None
- **Summary**: The continuous increase in global population and the impact of climate change on crop production are expected to affect the food sector significantly. In this context, there is need for timely, large-scale and precise mapping of crops for evidence-based decision making. A key enabler towards this direction are new satellite missions that freely offer big remote sensing data of high spatio-temporal resolution and global coverage. During the previous decade and because of this surge of big Earth observations, deep learning methods have dominated the remote sensing and crop mapping literature. Nevertheless, deep learning models require large amounts of annotated data that are scarce and hard-to-acquire. To address this problem, transfer learning methods can be used to exploit available annotations and enable crop mapping for other regions, crop types and years of inspection. In this work, we have developed and trained a deep learning model for paddy rice detection in South Korea using Sentinel-1 VH time-series. We then fine-tune the model for i) paddy rice detection in France and Spain and ii) barley detection in the Netherlands. Additionally, we propose a modification in the pre-trained weights in order to incorporate extra input features (Sentinel-1 VV). Our approach shows excellent performance when transferring in different areas for the same crop type and rather promising results when transferring in a different area and crop type.



### Resource-Aware Heterogeneous Federated Learning using Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2211.05716v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.05716v1)
- **Published**: 2022-11-09 09:38:57+00:00
- **Updated**: 2022-11-09 09:38:57+00:00
- **Authors**: Sixing Yu, Phuong Nguyen, Waqwoya Abebe, Justin Stanley, Pablo Munoz, Ali Jannesari
- **Comment**: None
- **Journal**: None
- **Summary**: Federated Learning (FL) is extensively used to train AI/ML models in distributed and privacy-preserving settings. Participant edge devices in FL systems typically contain non-independent and identically distributed~(Non-IID) private data and unevenly distributed computational resources. Preserving user data privacy while optimizing AI/ML models in a heterogeneous federated network requires us to address data heterogeneity and system/resource heterogeneity. Hence, we propose \underline{R}esource-\underline{a}ware \underline{F}ederated \underline{L}earning~(RaFL) to address these challenges. RaFL allocates resource-aware models to edge devices using Neural Architecture Search~(NAS) and allows heterogeneous model architecture deployment by knowledge extraction and fusion. Integrating NAS into FL enables on-demand customized model deployment for resource-diverse edge devices. Furthermore, we propose a multi-model architecture fusion scheme allowing the aggregation of the distributed learning results. Results demonstrate RaFL's superior resource efficiency compared to SoTA.



### Interpretable Explainability in Facial Emotion Recognition and Gamification for Data Collection
- **Arxiv ID**: http://arxiv.org/abs/2211.04769v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2211.04769v1)
- **Published**: 2022-11-09 09:53:48+00:00
- **Updated**: 2022-11-09 09:53:48+00:00
- **Authors**: Krist Shingjergji, Deniz Iren, Felix Bottger, Corrie Urlings, Roland Klemke
- **Comment**: 8 pages, 8 figures, 2022 10th International Conference on Affective
  Computing and Intelligent Interaction (ACII)
- **Journal**: None
- **Summary**: Training facial emotion recognition models requires large sets of data and costly annotation processes. To alleviate this problem, we developed a gamified method of acquiring annotated facial emotion data without an explicit labeling effort by humans. The game, which we named Facegame, challenges the players to imitate a displayed image of a face that portrays a particular basic emotion. Every round played by the player creates new data that consists of a set of facial features and landmarks, already annotated with the emotion label of the target facial expression. Such an approach effectively creates a robust, sustainable, and continuous machine learning training process. We evaluated Facegame with an experiment that revealed several contributions to the field of affective computing. First, the gamified data collection approach allowed us to access a rich variation of facial expressions of each basic emotion due to the natural variations in the players' facial expressions and their expressive abilities. We report improved accuracy when the collected data were used to enrich well-known in-the-wild facial emotion datasets and consecutively used for training facial emotion recognition models. Second, the natural language prescription method used by the Facegame constitutes a novel approach for interpretable explainability that can be applied to any facial emotion recognition model. Finally, we observed significant improvements in the facial emotion perception and expression skills of the players through repeated game play.



### SG-Shuffle: Multi-aspect Shuffle Transformer for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2211.04773v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04773v1)
- **Published**: 2022-11-09 10:00:45+00:00
- **Updated**: 2022-11-09 10:00:45+00:00
- **Authors**: Anh Duc Bui, Soyeon Caren Han, Josiah Poon
- **Comment**: None
- **Journal**: None
- **Summary**: Scene Graph Generation (SGG) serves a comprehensive representation of the images for human understanding as well as visual understanding tasks. Due to the long tail bias problem of the object and predicate labels in the available annotated data, the scene graph generated from current methodologies can be biased toward common, non-informative relationship labels. Relationship can sometimes be non-mutually exclusive, which can be described from multiple perspectives like geometrical relationships or semantic relationships, making it even more challenging to predict the most suitable relationship label. In this work, we proposed the SG-Shuffle pipeline for scene graph generation with 3 components: 1) Parallel Transformer Encoder, which learns to predict object relationships in a more exclusive manner by grouping relationship labels into groups of similar purpose; 2) Shuffle Transformer, which learns to select the final relationship labels from the category-specific feature generated in the previous step; and 3) Weighted CE loss, used to alleviate the training bias caused by the imbalanced dataset.



### IRNet: Iterative Refinement Network for Noisy Partial Label Learning
- **Arxiv ID**: http://arxiv.org/abs/2211.04774v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.04774v5)
- **Published**: 2022-11-09 10:01:25+00:00
- **Updated**: 2023-03-08 05:18:21+00:00
- **Authors**: Zheng Lian, Mingyu Xu, Lan Chen, Licai Sun, Bin Liu, Jianhua Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Partial label learning (PLL) is a typical weakly supervised learning, where each sample is associated with a set of candidate labels. The basic assumption of PLL is that the ground-truth label must reside in the candidate set. However, this assumption may not be satisfied due to the unprofessional judgment of the annotators, thus limiting the practical application of PLL. In this paper, we relax this assumption and focus on a more general problem, noisy PLL, where the ground-truth label may not exist in the candidate set. To address this challenging problem, we propose a novel framework called "Iterative Refinement Network (IRNet)". It aims to purify the noisy samples by two key modules, i.e., noisy sample detection and label correction. Ideally, we can convert noisy PLL into traditional PLL if all noisy samples are corrected. To guarantee the performance of these modules, we start with warm-up training and exploit data augmentation to reduce prediction errors. Through theoretical analysis, we prove that IRNet is able to reduce the noise level of the dataset and eventually approximate the Bayes optimal classifier. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our method. IRNet is superior to existing state-of-the-art approaches on noisy PLL.



### On the Robustness of Explanations of Deep Neural Network Models: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2211.04780v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04780v1)
- **Published**: 2022-11-09 10:14:21+00:00
- **Updated**: 2022-11-09 10:14:21+00:00
- **Authors**: Amlan Jyoti, Karthik Balaji Ganesh, Manoj Gayala, Nandita Lakshmi Tunuguntla, Sandesh Kamath, Vineeth N Balasubramanian
- **Comment**: Under Review ACM Computing Surveys "Special Issue on Trustworthy AI"
- **Journal**: None
- **Summary**: Explainability has been widely stated as a cornerstone of the responsible and trustworthy use of machine learning models. With the ubiquitous use of Deep Neural Network (DNN) models expanding to risk-sensitive and safety-critical domains, many methods have been proposed to explain the decisions of these models. Recent years have also seen concerted efforts that have shown how such explanations can be distorted (attacked) by minor input perturbations. While there have been many surveys that review explainability methods themselves, there has been no effort hitherto to assimilate the different methods and metrics proposed to study the robustness of explanations of DNN models. In this work, we present a comprehensive survey of methods that study, understand, attack, and defend explanations of DNN models. We also present a detailed review of different metrics used to evaluate explanation methods, as well as describe attributional attack and defense methods. We conclude with lessons and take-aways for the community towards ensuring robust explanations of DNN model predictions.



### Profiling Obese Subgroups in National Health and Nutritional Status Survey Data using Machine Learning Techniques: A Case Study from Brunei Darussalam
- **Arxiv ID**: http://arxiv.org/abs/2211.04781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04781v1)
- **Published**: 2022-11-09 10:15:58+00:00
- **Updated**: 2022-11-09 10:15:58+00:00
- **Authors**: Usman Khalil, Owais Ahmed Malik, Daphne Teck Ching Lai, Ong Sok King
- **Comment**: A Case study of Obese Subgroups from Brunei Darussalam: 15 Pages, 4
  figures, journal
- **Journal**: None
- **Summary**: National Health and Nutritional Status Survey (NHANSS) is conducted annually by the Ministry of Health in Negara Brunei Darussalam to assess the population health and nutritional patterns and characteristics. The main aim of this study was to discover meaningful patterns (groups) from the obese sample of NHANSS data by applying data reduction and interpretation techniques. The mixed nature of the variables (qualitative and quantitative) in the data set added novelty to the study. Accordingly, the Categorical Principal Component (CATPCA) technique was chosen to interpret the meaningful results. The relationships between obesity and the lifestyle factors like demography, socioeconomic status, physical activity, dietary behavior, history of blood pressure, diabetes, etc., were determined based on the principal components generated by CATPCA. The results were validated with the help of the split method technique to counter verify the authenticity of the generated groups. Based on the analysis and results, two subgroups were found in the data set, and the salient features of these subgroups have been reported. These results can be proposed for the betterment of the healthcare industry.



### Masked Vision-Language Transformers for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2211.04785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04785v1)
- **Published**: 2022-11-09 10:28:23+00:00
- **Updated**: 2022-11-09 10:28:23+00:00
- **Authors**: Jie Wu, Ying Peng, Shengming Zhang, Weigang Qi, Jian Zhang
- **Comment**: The paper is accepted by the 33rd British Machine Vision Conference
  (BMVC 2022)
- **Journal**: None
- **Summary**: Scene text recognition (STR) enables computers to recognize and read the text in various real-world scenes. Recent STR models benefit from taking linguistic information in addition to visual cues into consideration. We propose a novel Masked Vision-Language Transformers (MVLT) to capture both the explicit and the implicit linguistic information. Our encoder is a Vision Transformer, and our decoder is a multi-modal Transformer. MVLT is trained in two stages: in the first stage, we design a STR-tailored pretraining method based on a masking strategy; in the second stage, we fine-tune our model and adopt an iterative correction method to improve the performance. MVLT attains superior results compared to state-of-the-art STR models on several benchmarks. Our code and model are available at https://github.com/onealwj/MVLT.



### RadFormer: Transformers with Global-Local Attention for Interpretable and Accurate Gallbladder Cancer Detection
- **Arxiv ID**: http://arxiv.org/abs/2211.04793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04793v1)
- **Published**: 2022-11-09 10:40:35+00:00
- **Updated**: 2022-11-09 10:40:35+00:00
- **Authors**: Soumen Basu, Mayank Gupta, Pratyaksha Rana, Pankaj Gupta, Chetan Arora
- **Comment**: To Appear in Elsevier Medical Image Analysis
- **Journal**: None
- **Summary**: We propose a novel deep neural network architecture to learn interpretable representation for medical image analysis. Our architecture generates a global attention for region of interest, and then learns bag of words style deep feature embeddings with local attention. The global, and local feature maps are combined using a contemporary transformer architecture for highly accurate Gallbladder Cancer (GBC) detection from Ultrasound (USG) images. Our experiments indicate that the detection accuracy of our model beats even human radiologists, and advocates its use as the second reader for GBC diagnosis. Bag of words embeddings allow our model to be probed for generating interpretable explanations for GBC detection consistent with the ones reported in medical literature. We show that the proposed model not only helps understand decisions of neural network models but also aids in discovery of new visual features relevant to the diagnosis of GBC. Source-code and model will be available at https://github.com/sbasu276/RadFormer



### On the use of learning-based forecasting methods for ameliorating fashion business processes: A position paper
- **Arxiv ID**: http://arxiv.org/abs/2211.04798v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.04798v1)
- **Published**: 2022-11-09 10:44:51+00:00
- **Updated**: 2022-11-09 10:44:51+00:00
- **Authors**: Geri Skenderi, Christian Joppi, Matteo Denitto, Marco Cristani
- **Comment**: 2nd International Workshop on Industrial Machine Learning @ ICPR 2022
- **Journal**: None
- **Summary**: The fashion industry is one of the most active and competitive markets in the world, manufacturing millions of products and reaching large audiences every year. A plethora of business processes are involved in this large-scale industry, but due to the generally short life-cycle of clothing items, supply-chain management and retailing strategies are crucial for good market performance. Correctly understanding the wants and needs of clients, managing logistic issues and marketing the correct products are high-level problems with a lot of uncertainty associated to them given the number of influencing factors, but most importantly due to the unpredictability often associated with the future. It is therefore straightforward that forecasting methods, which generate predictions of the future, are indispensable in order to ameliorate all the various business processes that deal with the true purpose and meaning of fashion: having a lot of people wear a particular product or style, rendering these items, people and consequently brands fashionable. In this paper, we provide an overview of three concrete forecasting tasks that any fashion company can apply in order to improve their industrial and market impact. We underline advances and issues in all three tasks and argue about their importance and the impact they can have at an industrial level. Finally, we highlight issues and directions of future work, reflecting on how learning-based forecasting methods can further aid the fashion industry.



### Bit-depth enhancement detection for compressed video
- **Arxiv ID**: http://arxiv.org/abs/2211.04799v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04799v1)
- **Published**: 2022-11-09 10:46:34+00:00
- **Updated**: 2022-11-09 10:46:34+00:00
- **Authors**: Nickolay Safonov, Dmitriy Vatolin
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, display intensity and contrast have increased considerably. Many displays support high dynamic range (HDR) and 10-bit color depth. Since high bit-depth is an emerging technology, video content is still largely shot and transmitted with a bit depth of 8 bits or less per color component. Insufficient bit-depths produce distortions called false contours or banding, and they are visible on high contrast screens. To deal with such distortions, researchers have proposed algorithms for bit-depth enhancement (dequantization). Such techniques convert videos with low bit-depth (LBD) to videos with high bit-depth (HBD). The quality of converted LBD video, however, is usually lower than that of the original HBD video, and many consumers prefer to keep the original HBD versions. In this paper, we propose an algorithm to determine whether a video has undergone conversion before compression. This problem is complex; it involves detecting outcomes of different dequantization algorithms in the presence of compression that strongly affects the least-significant bits (LSBs) in the video frames. Our algorithm can detect bit-depth enhancement and demonstrates good generalization capability, as it is able to determine whether a video has undergone processing by dequantization algorithms absent from the training dataset.



### Designing Network Design Strategies Through Gradient Path Analysis
- **Arxiv ID**: http://arxiv.org/abs/2211.04800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04800v1)
- **Published**: 2022-11-09 10:51:57+00:00
- **Updated**: 2022-11-09 10:51:57+00:00
- **Authors**: Chien-Yao Wang, Hong-Yuan Mark Liao, I-Hau Yeh
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: Designing a high-efficiency and high-quality expressive network architecture has always been the most important research topic in the field of deep learning. Most of today's network design strategies focus on how to integrate features extracted from different layers, and how to design computing units to effectively extract these features, thereby enhancing the expressiveness of the network. This paper proposes a new network design strategy, i.e., to design the network architecture based on gradient path analysis. On the whole, most of today's mainstream network design strategies are based on feed forward path, that is, the network architecture is designed based on the data path. In this paper, we hope to enhance the expressive ability of the trained model by improving the network learning ability. Due to the mechanism driving the network parameter learning is the backward propagation algorithm, we design network design strategies based on back propagation path. We propose the gradient path design strategies for the layer-level, the stage-level, and the network-level, and the design strategies are proved to be superior and feasible from theoretical analysis and experiments.



### Novel structural-scale uncertainty measures and error retention curves: application to multiple sclerosis
- **Arxiv ID**: http://arxiv.org/abs/2211.04825v2
- **DOI**: 10.48550/ARXIV.2211.04825
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04825v2)
- **Published**: 2022-11-09 11:53:29+00:00
- **Updated**: 2022-11-11 08:41:31+00:00
- **Authors**: Nataliia Molchanova, Vatsal Raina, Andrey Malinin, Francesco La Rosa, Henning Muller, Mark Gales, Cristina Granziera, Mara Graziani, Meritxell Bach Cuadra
- **Comment**: 4 pages, 2 figures, 3 tables, ISBI preprint
- **Journal**: None
- **Summary**: This paper focuses on the uncertainty estimation for white matter lesions (WML) segmentation in magnetic resonance imaging (MRI). On one side, voxel-scale segmentation errors cause the erroneous delineation of the lesions; on the other side, lesion-scale detection errors lead to wrong lesion counts. Both of these factors are clinically relevant for the assessment of multiple sclerosis patients. This work aims to compare the ability of different voxel- and lesion-scale uncertainty measures to capture errors related to segmentation and lesion detection, respectively. Our main contributions are (i) proposing new measures of lesion-scale uncertainty that do not utilise voxel-scale uncertainties; (ii) extending an error retention curves analysis framework for evaluation of lesion-scale uncertainty measures. Our results obtained on the multi-center testing set of 58 patients demonstrate that the proposed lesion-scale measure achieves the best performance among the analysed measures. All code implementations are provided at https://github.com/NataliiaMolch/MS_WML_uncs



### 3DFill:Reference-guided Image Inpainting by Self-supervised 3D Image Alignment
- **Arxiv ID**: http://arxiv.org/abs/2211.04831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04831v1)
- **Published**: 2022-11-09 12:09:03+00:00
- **Updated**: 2022-11-09 12:09:03+00:00
- **Authors**: Liang Zhao, Xinyuan Zhao, Hailong Ma, Xinyu Zhang, Long Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing image inpainting algorithms are based on a single view, struggling with large holes or the holes containing complicated scenes. Some reference-guided algorithms fill the hole by referring to another viewpoint image and use 2D image alignment. Due to the camera imaging process, simple 2D transformation is difficult to achieve a satisfactory result. In this paper, we propose 3DFill, a simple and efficient method for reference-guided image inpainting. Given a target image with arbitrary hole regions and a reference image from another viewpoint, the 3DFill first aligns the two images by a two-stage method: 3D projection + 2D transformation, which has better results than 2D image alignment. The 3D projection is an overall alignment between images and the 2D transformation is a local alignment focused on the hole region. The entire process of image alignment is self-supervised. We then fill the hole in the target image with the contents of the aligned image. Finally, we use a conditional generation network to refine the filled image to obtain the inpainting result. 3DFill achieves state-of-the-art performance on image inpainting across a variety of wide view shifts and has a faster inference speed than other inpainting models.



### Final infarct prediction in acute ischemic stroke
- **Arxiv ID**: http://arxiv.org/abs/2211.04850v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.04850v1)
- **Published**: 2022-11-09 12:37:20+00:00
- **Updated**: 2022-11-09 12:37:20+00:00
- **Authors**: Jeroen Bertels, David Robben, Dirk Vandermeulen, Robin Lemmens
- **Comment**: 17 pages, 5 figures, part of PhD thesis KU Leuven 2022 "Understanding
  Final Infarct Prediction in Acute Ischemic Stroke Using Convolutional Neural
  Networks"
- **Journal**: None
- **Summary**: This article focuses on the control center of each human body: the brain. We will point out the pivotal role of the cerebral vasculature and how its complex mechanisms may vary between subjects. We then emphasize a specific acute pathological state, i.e., acute ischemic stroke, and show how medical imaging and its analysis can be used to define the treatment. We show how the core-penumbra concept is used in practice using mismatch criteria and how machine learning can be used to make predictions of the final infarct, either via deconvolution or convolutional neural networks.



### ERNIE-UniX2: A Unified Cross-lingual Cross-modal Framework for Understanding and Generation
- **Arxiv ID**: http://arxiv.org/abs/2211.04861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04861v1)
- **Published**: 2022-11-09 13:06:58+00:00
- **Updated**: 2022-11-09 13:06:58+00:00
- **Authors**: Bin Shan, Yaqian Han, Weichong Yin, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang
- **Comment**: 13 pages, 2 figures
- **Journal**: None
- **Summary**: Recent cross-lingual cross-modal works attempt to extend Vision-Language Pre-training (VLP) models to non-English inputs and achieve impressive performance. However, these models focus only on understanding tasks utilizing encoder-only architecture. In this paper, we propose ERNIE-UniX2, a unified cross-lingual cross-modal pre-training framework for both generation and understanding tasks. ERNIE-UniX2 integrates multiple pre-training paradigms (e.g., contrastive learning and language modeling) based on encoder-decoder architecture and attempts to learn a better joint representation across languages and modalities. Furthermore, ERNIE-UniX2 can be seamlessly fine-tuned for varieties of generation and understanding downstream tasks. Pre-trained on both multilingual text-only and image-text datasets, ERNIE-UniX2 achieves SOTA results on various cross-lingual cross-modal generation and understanding tasks such as multimodal machine translation and multilingual visual question answering.



### Domain-incremental Cardiac Image Segmentation with Style-oriented Replay and Domain-sensitive Feature Whitening
- **Arxiv ID**: http://arxiv.org/abs/2211.04862v1
- **DOI**: 10.1109/TMI.2022.3211195
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04862v1)
- **Published**: 2022-11-09 13:07:36+00:00
- **Updated**: 2022-11-09 13:07:36+00:00
- **Authors**: Kang Li, Lequan Yu, Pheng-Ann Heng
- **Comment**: Accepted to IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Contemporary methods have shown promising results on cardiac image segmentation, but merely in static learning, i.e., optimizing the network once for all, ignoring potential needs for model updating. In real-world scenarios, new data continues to be gathered from multiple institutions over time and new demands keep growing to pursue more satisfying performance. The desired model should incrementally learn from each incoming dataset and progressively update with improved functionality as time goes by. As the datasets sequentially delivered from multiple sites are normally heterogenous with domain discrepancy, each updated model should not catastrophically forget previously learned domains while well generalizing to currently arrived domains or even unseen domains. In medical scenarios, this is particularly challenging as accessing or storing past data is commonly not allowed due to data privacy. To this end, we propose a novel domain-incremental learning framework to recover past domain inputs first and then regularly replay them during model optimization. Particularly, we first present a style-oriented replay module to enable structure-realistic and memory-efficient reproduction of past data, and then incorporate the replayed past data to jointly optimize the model with current data to alleviate catastrophic forgetting. During optimization, we additionally perform domain-sensitive feature whitening to suppress model's dependency on features that are sensitive to domain changes (e.g., domain-distinctive style features) to assist domain-invariant feature exploration and gradually improve the generalization performance of the network. We have extensively evaluated our approach with the M&Ms Dataset in single-domain and compound-domain incremental learning settings with improved performance over other comparison approaches.



### Trackerless freehand ultrasound with sequence modelling and auxiliary transformation over past and future frames
- **Arxiv ID**: http://arxiv.org/abs/2211.04867v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04867v2)
- **Published**: 2022-11-09 13:18:35+00:00
- **Updated**: 2023-02-04 17:08:45+00:00
- **Authors**: Qi Li, Ziyi Shen, Qian Li, Dean C Barratt, Thomas Dowrick, Matthew J Clarkson, Tom Vercauteren, Yipeng Hu
- **Comment**: Accepted to IEEE International Symposium on Biomedical Imaging (ISBI)
  2023
- **Journal**: None
- **Summary**: Three-dimensional (3D) freehand ultrasound (US) reconstruction without a tracker can be advantageous over its two-dimensional or tracked counterparts in many clinical applications. In this paper, we propose to estimate 3D spatial transformation between US frames from both past and future 2D images, using feed-forward and recurrent neural networks (RNNs). With the temporally available frames, a further multi-task learning algorithm is proposed to utilise a large number of auxiliary transformation-predicting tasks between them. Using more than 40,000 US frames acquired from 228 scans on 38 forearms of 19 volunteers in a volunteer study, the hold-out test performance is quantified by frame prediction accuracy, volume reconstruction overlap, accumulated tracking error and final drift, based on ground-truth from an optical tracker. The results show the importance of modelling the temporal-spatially correlated input frames as well as output transformations, with further improvement owing to additional past and/or future frames. The best performing model was associated with predicting transformation between moderately-spaced frames, with an interval of less than ten frames at 20 frames per second (fps). Little benefit was observed by adding frames more than one second away from the predicted transformation, with or without LSTM-based RNNs. Interestingly, with the proposed approach, explicit within-sequence loss that encourages consistency in composing transformations or minimises accumulated error may no longer be required. The implementation code and volunteer data will be made publicly available ensuring reproducibility and further research.



### Visual Named Entity Linking: A New Dataset and A Baseline
- **Arxiv ID**: http://arxiv.org/abs/2211.04872v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.04872v1)
- **Published**: 2022-11-09 13:27:50+00:00
- **Updated**: 2022-11-09 13:27:50+00:00
- **Authors**: Wenxiang Sun, Yixing Fan, Jiafeng Guo, Ruqing Zhang, Xueqi Cheng
- **Comment**: 13 pages, 11 figures, published to EMNLP 2022(findings)
- **Journal**: None
- **Summary**: Visual Entity Linking (VEL) is a task to link regions of images with their corresponding entities in Knowledge Bases (KBs), which is beneficial for many computer vision tasks such as image retrieval, image caption, and visual question answering. While existing tasks in VEL either rely on textual data to complement a multi-modal linking or only link objects with general entities, which fails to perform named entity linking on large amounts of image data. In this paper, we consider a purely Visual-based Named Entity Linking (VNEL) task, where the input only consists of an image. The task is to identify objects of interest (i.e., visual entity mentions) in images and link them to corresponding named entities in KBs. Since each entity often contains rich visual and textual information in KBs, we thus propose three different sub-tasks, i.e., visual to visual entity linking (V2VEL), visual to textual entity linking (V2TEL), and visual to visual-textual entity linking (V2VTEL). In addition, we present a high-quality human-annotated visual person linking dataset, named WIKIPerson. Based on WIKIPerson, we establish a series of baseline algorithms for the solution of each sub-task, and conduct experiments to verify the quality of proposed datasets and the effectiveness of baseline methods. We envision this work to be helpful for soliciting more works regarding VNEL in the future. The codes and datasets are publicly available at https://github.com/ict-bigdatalab/VNEL.



### Interactive Feature Embedding for Infrared and Visible Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/2211.04877v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04877v1)
- **Published**: 2022-11-09 13:34:42+00:00
- **Updated**: 2022-11-09 13:34:42+00:00
- **Authors**: Fan Zhao, Wenda Zhao, Huchuan Lu
- **Comment**: None
- **Journal**: None
- **Summary**: General deep learning-based methods for infrared and visible image fusion rely on the unsupervised mechanism for vital information retention by utilizing elaborately designed loss functions. However, the unsupervised mechanism depends on a well designed loss function, which cannot guarantee that all vital information of source images is sufficiently extracted. In this work, we propose a novel interactive feature embedding in self-supervised learning framework for infrared and visible image fusion, attempting to overcome the issue of vital information degradation. With the help of self-supervised learning framework, hierarchical representations of source images can be efficiently extracted. In particular, interactive feature embedding models are tactfully designed to build a bridge between the self-supervised learning and infrared and visible image fusion learning, achieving vital information retention. Qualitative and quantitative evaluations exhibit that the proposed method performs favorably against state-of-the-art methods.



### Composite Fixed-Length Ordered Features for Palmprint Template Protection with Diminished Performance Loss
- **Arxiv ID**: http://arxiv.org/abs/2211.04884v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04884v1)
- **Published**: 2022-11-09 13:40:04+00:00
- **Updated**: 2022-11-09 13:40:04+00:00
- **Authors**: Weiqiang Zhao, Heng Zhao, Zhicheng Cao, Liaojun Pang
- **Comment**: None
- **Journal**: None
- **Summary**: Palmprint recognition has become more and more popular due to its advantages over other biometric modalities such as fingerprint, in that it is larger in area, richer in information and able to work at a distance. However, the issue of palmprint privacy and security (especially palmprint template protection) remains under-studied. Among the very few research works, most of them only use the directional and orientation features of the palmprint with transformation processing, yielding unsatisfactory protection and identification performance. Thus, this paper proposes a palmprint template protection-oriented operator that has a fixed length and is ordered in nature, by fusing point features and orientation features. Firstly, double orientations are extracted with more accuracy based on MFRAT. Then key points of SURF are extracted and converted to be fixed-length and ordered features. Finally, composite features that fuse up the double orientations and SURF points are transformed using the irreversible transformation of IOM to generate the revocable palmprint template. Experiments show that the EER after irreversible transformation on the PolyU and CASIA databases are 0.17% and 0.19% respectively, and the absolute precision loss is 0.08% and 0.07%, respectively, which proves the advantage of our method.



### Extending Temporal Data Augmentation for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2211.04888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04888v1)
- **Published**: 2022-11-09 13:49:38+00:00
- **Updated**: 2022-11-09 13:49:38+00:00
- **Authors**: Artjoms Gorpincenko, Michal Mackiewicz
- **Comment**: None
- **Journal**: None
- **Summary**: Pixel space augmentation has grown in popularity in many Deep Learning areas, due to its effectiveness, simplicity, and low computational cost. Data augmentation for videos, however, still remains an under-explored research topic, as most works have been treating inputs as stacks of static images rather than temporally linked series of data. Recently, it has been shown that involving the time dimension when designing augmentations can be superior to its spatial-only variants for video action recognition. In this paper, we propose several novel enhancements to these techniques to strengthen the relationship between the spatial and temporal domains and achieve a deeper level of perturbations. The video action recognition results of our techniques outperform their respective variants in Top-1 and Top-5 settings on the UCF-101 and the HMDB-51 datasets.



### Exploring Video Quality Assessment on User Generated Contents from Aesthetic and Technical Perspectives
- **Arxiv ID**: http://arxiv.org/abs/2211.04894v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04894v3)
- **Published**: 2022-11-09 13:55:50+00:00
- **Updated**: 2023-03-07 08:25:24+00:00
- **Authors**: Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, Weisi Lin
- **Comment**: None
- **Journal**: None
- **Summary**: The rapid increase in user-generated-content (UGC) videos calls for the development of effective video quality assessment (VQA) algorithms. However, the objective of the UGC-VQA problem is still ambiguous and can be viewed from two perspectives: the technical perspective, measuring the perception of distortions; and the aesthetic perspective, which relates to preference and recommendation on contents. To understand how these two perspectives affect overall subjective opinions in UGC-VQA, we conduct a large-scale subjective study to collect human quality opinions on overall quality of videos as well as perceptions from aesthetic and technical perspectives. The collected Disentangled Video Quality Database (DIVIDE-3k) confirms that human quality opinions on UGC videos are universally and inevitably affected by both aesthetic and technical perspectives. In light of this, we propose the Disentangled Objective Video Quality Evaluator (DOVER) to learn the quality of UGC videos based on the two perspectives. The DOVER proves state-of-the-art performance in UGC-VQA under very high efficiency. With perspective opinions in DIVIDE-3k, we further propose DOVER++, the first approach to provide reliable clear-cut quality evaluations from a single aesthetic or technical perspective. Code at https://github.com/VQAssessment/DOVER.



### Optimized Global Perturbation Attacks For Brain Tumour ROI Extraction From Binary Classification Models
- **Arxiv ID**: http://arxiv.org/abs/2211.04926v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04926v1)
- **Published**: 2022-11-09 14:52:36+00:00
- **Updated**: 2022-11-09 14:52:36+00:00
- **Authors**: Sajith Rajapaksa, Farzad Khalvati
- **Comment**: Accepted in Medical Imaging meets NeurIPS Workshop NeurIPS 2022
- **Journal**: None
- **Summary**: Deep learning techniques have greatly benefited computer-aided diagnostic systems. However, unlike other fields, in medical imaging, acquiring large fine-grained annotated datasets such as 3D tumour segmentation is challenging due to the high cost of manual annotation and privacy regulations. This has given interest to weakly-supervise methods to utilize the weakly labelled data for tumour segmentation. In this work, we propose a weakly supervised approach to obtain regions of interest using binary class labels. Furthermore, we propose a novel objective function to train the generator model based on a pretrained binary classification model. Finally, we apply our method to the brain tumour segmentation problem in MRI.



### From Distance to Dependency: A Paradigm Shift of Full-reference Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2211.04927v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04927v1)
- **Published**: 2022-11-09 14:57:27+00:00
- **Updated**: 2022-11-09 14:57:27+00:00
- **Authors**: Hanwei Zhu, Baoliang Chen, Lingyu Zhu, Shiqi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based full-reference image quality assessment (FR-IQA) models typically rely on the feature distance between the reference and distorted images. However, the underlying assumption of these models that the distance in the deep feature domain could quantify the quality degradation does not scientifically align with the invariant texture perception, especially when the images are generated artificially by neural networks. In this paper, we bring a radical shift in inferring the quality with learned features and propose the Deep Image Dependency (DID) based FR-IQA model. The feature dependency facilitates the comparisons of deep learning features in a high-order manner with Brownian distance covariance, which is characterized by the joint distribution of the features from reference and test images, as well as their marginal distributions. This enables the quantification of the feature dependency against nonlinear transformation, which is far beyond the computation of the numerical errors in the feature space. Experiments on image quality prediction, texture image similarity, and geometric invariance validate the superior performance of our proposed measure.



### Pure Transformer with Integrated Experts for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2211.04963v1
- **DOI**: 10.1007/978-3-031-19815-1_28
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.04963v1)
- **Published**: 2022-11-09 15:26:59+00:00
- **Updated**: 2022-11-09 15:26:59+00:00
- **Authors**: Yew Lee Tan, Adams Wai-kin Kong, Jung-Jae Kim
- **Comment**: Accepted in ECCV2022
- **Journal**: None
- **Summary**: Scene text recognition (STR) involves the task of reading text in cropped images of natural scenes. Conventional models in STR employ convolutional neural network (CNN) followed by recurrent neural network in an encoder-decoder framework. In recent times, the transformer architecture is being widely adopted in STR as it shows strong capability in capturing long-term dependency which appears to be prominent in scene text images. Many researchers utilized transformer as part of a hybrid CNN-transformer encoder, often followed by a transformer decoder. However, such methods only make use of the long-term dependency mid-way through the encoding process. Although the vision transformer (ViT) is able to capture such dependency at an early stage, its utilization remains largely unexploited in STR. This work proposes the use of a transformer-only model as a simple baseline which outperforms hybrid CNN-transformer models. Furthermore, two key areas for improvement were identified. Firstly, the first decoded character has the lowest prediction accuracy. Secondly, images of different original aspect ratios react differently to the patch resolutions while ViT only employ one fixed patch resolution. To explore these areas, Pure Transformer with Integrated Experts (PTIE) is proposed. PTIE is a transformer model that can process multiple patch resolutions and decode in both the original and reverse character orders. It is examined on 7 commonly used benchmarks and compared with over 20 state-of-the-art methods. The experimental results show that the proposed method outperforms them and obtains state-of-the-art results in most benchmarks.



### Understanding Cross-modal Interactions in V&L Models that Generate Scene Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2211.04971v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.04971v2)
- **Published**: 2022-11-09 15:33:51+00:00
- **Updated**: 2022-11-10 16:49:37+00:00
- **Authors**: Michele Cafagna, Kees van Deemter, Albert Gatt
- **Comment**: None
- **Journal**: None
- **Summary**: Image captioning models tend to describe images in an object-centric way, emphasising visible objects. But image descriptions can also abstract away from objects and describe the type of scene depicted. In this paper, we explore the potential of a state-of-the-art Vision and Language model, VinVL, to caption images at the scene level using (1) a novel dataset which pairs images with both object-centric and scene descriptions. Through (2) an in-depth analysis of the effect of the fine-tuning, we show (3) that a small amount of curated data suffices to generate scene descriptions without losing the capability to identify object-level concepts in the scene; the model acquires a more holistic view of the image compared to when object-centric descriptions are generated. We discuss the parallels between these results and insights from computational and cognitive science research on scene perception.



### ParGAN: Learning Real Parametrizable Transformations
- **Arxiv ID**: http://arxiv.org/abs/2211.04996v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.04996v1)
- **Published**: 2022-11-09 16:16:06+00:00
- **Updated**: 2022-11-09 16:16:06+00:00
- **Authors**: Diego Martin Arroyo, Alessio Tonioni, Federico Tombari
- **Comment**: None
- **Journal**: None
- **Summary**: Current methods for image-to-image translation produce compelling results, however, the applied transformation is difficult to control, since existing mechanisms are often limited and non-intuitive. We propose ParGAN, a generalization of the cycle-consistent GAN framework to learn image transformations with simple and intuitive controls. The proposed generator takes as input both an image and a parametrization of the transformation. We train this network to preserve the content of the input image while ensuring that the result is consistent with the given parametrization. Our approach does not require paired data and can learn transformations across several tasks and datasets. We show how, with disjoint image domains with no annotated parametrization, our framework can create smooth interpolations as well as learn multiple transformations simultaneously.



### Similarity among the 2D-shapes and the analysis of dissimilarity scores
- **Arxiv ID**: http://arxiv.org/abs/2211.04998v1
- **DOI**: None
- **Categories**: **cs.CV**, G.1; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2211.04998v1)
- **Published**: 2022-11-09 16:22:34+00:00
- **Updated**: 2022-11-09 16:22:34+00:00
- **Authors**: Karel Zimmermann
- **Comment**: None
- **Journal**: None
- **Summary**: We present a conceptually simple and intuitive method to calculate and to measure the dissimilarities among 2D shapes. Several methods to interpret and to visualize the resulting dissimilarity matrix are presented and compared.



### The Best of Both Worlds: a Framework for Combining Degradation Prediction with High Performance Super-Resolution Networks
- **Arxiv ID**: http://arxiv.org/abs/2211.05018v1
- **DOI**: 10.3390/s23010419
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.05018v1)
- **Published**: 2022-11-09 16:49:35+00:00
- **Updated**: 2022-11-09 16:49:35+00:00
- **Authors**: Matthew Aquilina, Keith George Ciantar, Christian Galea, Kenneth P. Camilleri, Reuben A. Farrugia, John Abela
- **Comment**: None
- **Journal**: None
- **Summary**: To date, the best-performing blind super-resolution (SR) techniques follow one of two paradigms: A) generate and train a standard SR network on synthetic low-resolution - high-resolution (LR - HR) pairs or B) attempt to predict the degradations an LR image has suffered and use these to inform a customised SR network. Despite significant progress, subscribers to the former miss out on useful degradation information that could be used to improve the SR process. On the other hand, followers of the latter rely on weaker SR networks, which are significantly outperformed by the latest architectural advancements. In this work, we present a framework for combining any blind SR prediction mechanism with any deep SR network, using a metadata insertion block to insert prediction vectors into SR network feature maps. Through comprehensive testing, we prove that state-of-the-art contrastive and iterative prediction schemes can be successfully combined with high-performance SR networks such as RCAN and HAN within our framework. We show that our hybrid models consistently achieve stronger SR performance than both their non-blind and blind counterparts. Furthermore, we demonstrate our framework's robustness by predicting degradations and super-resolving images from a complex pipeline of blurring, noise and compression.



### Portmanteauing Features for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2211.05036v1
- **DOI**: 10.1109/ICPR56361.2022.9956468
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.05036v1)
- **Published**: 2022-11-09 17:14:14+00:00
- **Updated**: 2022-11-09 17:14:14+00:00
- **Authors**: Yew Lee Tan, Ernest Yu Kai Chew, Adams Wai-Kin Kong, Jung-Jae Kim, Joo Hwee Lim
- **Comment**: Accepted in ICPR 2022
- **Journal**: None
- **Summary**: Scene text images have different shapes and are subjected to various distortions, e.g. perspective distortions. To handle these challenges, the state-of-the-art methods rely on a rectification network, which is connected to the text recognition network. They form a linear pipeline which uses text rectification on all input images, even for images that can be recognized without it. Undoubtedly, the rectification network improves the overall text recognition performance. However, in some cases, the rectification network generates unnecessary distortions on images, resulting in incorrect predictions in images that would have otherwise been correct without it. In order to alleviate the unnecessary distortions, the portmanteauing of features is proposed. The portmanteau feature, inspired by the portmanteau word, is a feature containing information from both the original text image and the rectified image. To generate the portmanteau feature, a non-linear input pipeline with a block matrix initialization is presented. In this work, the transformer is chosen as the recognition network due to its utilization of attention and inherent parallelism, which can effectively handle the portmanteau feature. The proposed method is examined on 6 benchmarks and compared with 13 state-of-the-art methods. The experimental results show that the proposed method outperforms the state-of-the-art methods on various of the benchmarks.



### Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task
- **Arxiv ID**: http://arxiv.org/abs/2211.05039v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2211.05039v2)
- **Published**: 2022-11-09 17:16:36+00:00
- **Updated**: 2023-07-03 14:47:18+00:00
- **Authors**: Jannik Kossen, Cătălina Cangea, Eszter Vértes, Andrew Jaegle, Viorica Patraucean, Ira Ktena, Nenad Tomasev, Danielle Belgrave
- **Comment**: Published in Transactions on Machine Learning Research. Previous
  version accepted to Foundation Models for Decision Making Workshop at NeurIPS
  2022
- **Journal**: None
- **Summary**: We introduce a challenging decision-making task that we call active acquisition for multimodal temporal data (A2MT). In many real-world scenarios, input features are not readily available at test time and must instead be acquired at significant cost. With A2MT, we aim to learn agents that actively select which modalities of an input to acquire, trading off acquisition cost and predictive performance. A2MT extends a previous task called active feature acquisition to temporal decision making about high-dimensional inputs. We propose a method based on the Perceiver IO architecture to address A2MT in practice. Our agents are able to solve a novel synthetic scenario requiring practically relevant cross-modal reasoning skills. On two large-scale, real-world datasets, Kinetics-700 and AudioSet, our agents successfully learn cost-reactive acquisition behavior. However, an ablation reveals they are unable to learn adaptive acquisition strategies, emphasizing the difficulty of the task even for state-of-the-art models. Applications of A2MT may be impactful in domains like medicine, robotics, or finance, where modalities differ in acquisition cost and informativeness.



### In-memory factorization of holographic perceptual representations
- **Arxiv ID**: http://arxiv.org/abs/2211.05052v2
- **DOI**: 10.1038/s41565-023-01357-8
- **Categories**: **cs.ET**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2211.05052v2)
- **Published**: 2022-11-09 17:36:06+00:00
- **Updated**: 2023-02-16 21:37:12+00:00
- **Authors**: Jovin Langenegger, Geethan Karunaratne, Michael Hersche, Luca Benini, Abu Sebastian, Abbas Rahimi
- **Comment**: 23 pages, 4 figures, 1 extended data figure, 3 supplementary notes, 2
  supplementary figures and 3 supplementary tables
- **Journal**: None
- **Summary**: Disentanglement of constituent factors of a sensory signal is central to perception and cognition and hence is a critical task for future artificial intelligence systems. In this paper, we present a compute engine capable of efficiently factorizing holographic perceptual representations by exploiting the computation-in-superposition capability of brain-inspired hyperdimensional computing and the intrinsic stochasticity associated with analog in-memory computing based on nanoscale memristive devices. Such an iterative in-memory factorizer is shown to solve at least five orders of magnitude larger problems that cannot be solved otherwise, while also significantly lowering the computational time and space complexity. We present a large-scale experimental demonstration of the factorizer by employing two in-memory compute chips based on phase-change memristive devices. The dominant matrix-vector multiply operations are executed at O(1) thus reducing the computational time complexity to merely the number of iterations. Moreover, we experimentally demonstrate the ability to factorize visual perceptual representations reliably and efficiently.



### Prompting Large Pre-trained Vision-Language Models For Compositional Concept Learning
- **Arxiv ID**: http://arxiv.org/abs/2211.05077v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.05077v1)
- **Published**: 2022-11-09 18:08:53+00:00
- **Updated**: 2022-11-09 18:08:53+00:00
- **Authors**: Guangyue Xu, Parisa Kordjamshidi, Joyce Chai
- **Comment**: None
- **Journal**: None
- **Summary**: This work explores the zero-shot compositional learning ability of large pre-trained vision-language models(VLMs) within the prompt-based learning framework and propose a model (\textit{PromptCompVL}) to solve the compositonal zero-shot learning (CZSL) problem. \textit{PromptCompVL} makes two design choices: first, it uses a soft-prompting instead of hard-prompting to inject learnable parameters to reprogram VLMs for compositional learning. Second, to address the compositional challenge, it uses the soft-embedding layer to learn primitive concepts in different combinations. By combining both soft-embedding and soft-prompting, \textit{PromptCompVL} achieves state-of-the-art performance on the MIT-States dataset. Furthermore, our proposed model achieves consistent improvement compared to other CLIP-based methods which shows the effectiveness of the proposed prompting strategies for CZSL.



### Clinical Contrastive Learning for Biomarker Detection
- **Arxiv ID**: http://arxiv.org/abs/2211.05092v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.05092v1)
- **Published**: 2022-11-09 18:29:56+00:00
- **Updated**: 2022-11-09 18:29:56+00:00
- **Authors**: Kiran Kokilepersaud, Mohit Prabhushankar, Ghassan AlRegib
- **Comment**: arXiv admin note: text overlap with arXiv:2209.11195
- **Journal**: NeurIPS 2022 Workshop: Self-Supervised Learning - Theory and
  Practice
- **Summary**: This paper presents a novel positive and negative set selection strategy for contrastive learning of medical images based on labels that can be extracted from clinical data. In the medical field, there exists a variety of labels for data that serve different purposes at different stages of a diagnostic and treatment process. Clinical labels and biomarker labels are two examples. In general, clinical labels are easier to obtain in larger quantities because they are regularly collected during routine clinical care, while biomarker labels require expert analysis and interpretation to obtain. Within the field of ophthalmology, previous work has shown that clinical values exhibit correlations with biomarker structures that manifest within optical coherence tomography (OCT) scans. We exploit this relationship between clinical and biomarker data to improve performance for biomarker classification. This is accomplished by leveraging the larger amount of clinical data as pseudo-labels for our data without biomarker labels in order to choose positive and negative instances for training a backbone network with a supervised contrastive loss. In this way, a backbone network learns a representation space that aligns with the clinical data distribution available. Afterwards, we fine-tune the network trained in this manner with the smaller amount of biomarker labeled data with a cross-entropy loss in order to classify these key indicators of disease directly from OCT scans. Our method is shown to outperform state of the art self-supervised methods by as much as 5% in terms of accuracy on individual biomarker detection.



### 3D Scene Inference from Transient Histograms
- **Arxiv ID**: http://arxiv.org/abs/2211.05094v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.05094v1)
- **Published**: 2022-11-09 18:31:50+00:00
- **Updated**: 2022-11-09 18:31:50+00:00
- **Authors**: Sacha Jungerman, Atul Ingle, Yin Li, Mohit Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Time-resolved image sensors that capture light at pico-to-nanosecond timescales were once limited to niche applications but are now rapidly becoming mainstream in consumer devices. We propose low-cost and low-power imaging modalities that capture scene information from minimal time-resolved image sensors with as few as one pixel. The key idea is to flood illuminate large scene patches (or the entire scene) with a pulsed light source and measure the time-resolved reflected light by integrating over the entire illuminated area. The one-dimensional measured temporal waveform, called \emph{transient}, encodes both distances and albedoes at all visible scene points and as such is an aggregate proxy for the scene's 3D geometry. We explore the viability and limitations of the transient waveforms by themselves for recovering scene information, and also when combined with traditional RGB cameras. We show that plane estimation can be performed from a single transient and that using only a few more it is possible to recover a depth map of the whole scene. We also show two proof-of-concept hardware prototypes that demonstrate the feasibility of our approach for compact, mobile, and budget-limited applications.



### Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2211.05105v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.05105v4)
- **Published**: 2022-11-09 18:54:25+00:00
- **Updated**: 2023-04-26 11:47:44+00:00
- **Authors**: Patrick Schramowski, Manuel Brack, Björn Deiseroth, Kristian Kersting
- **Comment**: Proceedings of the 22nd IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), 2023
- **Journal**: None
- **Summary**: Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment.



### ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention
- **Arxiv ID**: http://arxiv.org/abs/2211.05109v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.05109v1)
- **Published**: 2022-11-09 18:58:21+00:00
- **Updated**: 2022-11-09 18:58:21+00:00
- **Authors**: Jyotikrishna Dass, Shang Wu, Huihong Shi, Chaojian Li, Zhifan Ye, Zhongfeng Wang, Yingyan Lin
- **Comment**: 14 pages, 15 figures, Accepted to IEEE HPCA 2023
- **Journal**: None
- **Summary**: Vision Transformer (ViT) has emerged as a competitive alternative to convolutional neural networks for various computer vision applications. Specifically, ViT multi-head attention layers make it possible to embed information globally across the overall image. Nevertheless, computing and storing such attention matrices incurs a quadratic cost dependency on the number of patches, limiting its achievable efficiency and scalability and prohibiting more extensive real-world ViT applications on resource-constrained devices. Sparse attention has been shown to be a promising direction for improving hardware acceleration efficiency for NLP models. However, a systematic counterpart approach is still missing for accelerating ViT models. To close the above gap, we propose a first-of-its-kind algorithm-hardware codesigned framework, dubbed ViTALiTy, for boosting the inference efficiency of ViTs. Unlike sparsity-based Transformer accelerators for NLP, ViTALiTy unifies both low-rank and sparse components of the attention in ViTs. At the algorithm level, we approximate the dot-product softmax operation via first-order Taylor attention with row-mean centering as the low-rank component to linearize the cost of attention blocks and further boost the accuracy by incorporating a sparsity-based regularization. At the hardware level, we develop a dedicated accelerator to better leverage the resulting workload and pipeline from ViTALiTy's linear Taylor attention which requires the execution of only the low-rank component, to further boost the hardware efficiency. Extensive experiments and ablation studies validate that ViTALiTy offers boosted end-to-end efficiency (e.g., $3\times$ faster and $3\times$ energy-efficient) under comparable accuracy, with respect to the state-of-the-art solution.



### An Empirical Study on Clustering Pretrained Embeddings: Is Deep Strictly Better?
- **Arxiv ID**: http://arxiv.org/abs/2211.05183v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.05183v1)
- **Published**: 2022-11-09 20:26:40+00:00
- **Updated**: 2022-11-09 20:26:40+00:00
- **Authors**: Tyler R. Scott, Ting Liu, Michael C. Mozer, Andrew C. Gallagher
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research in clustering face embeddings has found that unsupervised, shallow, heuristic-based methods -- including $k$-means and hierarchical agglomerative clustering -- underperform supervised, deep, inductive methods. While the reported improvements are indeed impressive, experiments are mostly limited to face datasets, where the clustered embeddings are highly discriminative or well-separated by class (Recall@1 above 90% and often nearing ceiling), and the experimental methodology seemingly favors the deep methods. We conduct a large-scale empirical study of 17 clustering methods across three datasets and obtain several robust findings. Notably, deep methods are surprisingly fragile for embeddings with more uncertainty, where they match or even perform worse than shallow, heuristic-based methods. When embeddings are highly discriminative, deep methods do outperform the baselines, consistent with past results, but the margin between methods is much smaller than previously reported. We believe our benchmarks broaden the scope of supervised clustering methods beyond the face domain and can serve as a foundation on which these methods could be improved. To enable reproducibility, we include all necessary details in the appendices, and plan to release the code.



### Training a Vision Transformer from scratch in less than 24 hours with 1 GPU
- **Arxiv ID**: http://arxiv.org/abs/2211.05187v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2211.05187v1)
- **Published**: 2022-11-09 20:36:46+00:00
- **Updated**: 2022-11-09 20:36:46+00:00
- **Authors**: Saghar Irandoust, Thibaut Durand, Yunduz Rakhmangulova, Wenjie Zi, Hossein Hajimirsadeghi
- **Comment**: 7 pages, 2 figures, 1 table, published in "Has it Trained Yet?
  Workshop at the Conference on Neural Information Processing Systems (NeurIPS
  2022)"
- **Journal**: None
- **Summary**: Transformers have become central to recent advances in computer vision. However, training a vision Transformer (ViT) model from scratch can be resource intensive and time consuming. In this paper, we aim to explore approaches to reduce the training costs of ViT models. We introduce some algorithmic improvements to enable training a ViT model from scratch with limited hardware (1 GPU) and time (24 hours) resources. First, we propose an efficient approach to add locality to the ViT architecture. Second, we develop a new image size curriculum learning strategy, which allows to reduce the number of patches extracted from each image at the beginning of the training. Finally, we propose a new variant of the popular ImageNet1k benchmark by adding hardware and time constraints. We evaluate our contributions on this benchmark, and show they can significantly improve performances given the proposed training budget. We will share the code in https://github.com/BorealisAI/efficient-vit-training.



### Affordance detection with Dynamic-Tree Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/2211.05200v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2211.05200v1)
- **Published**: 2022-11-09 21:14:08+00:00
- **Updated**: 2022-11-09 21:14:08+00:00
- **Authors**: Antonio Rodríguez-Sánchez, Simon Haller-Seeber, David Peer, Chris Engelhardt, Jakob Mittelberger, Matteo Saveriano
- **Comment**: IEEE-RAS International Conference on Humanoid Robots (Humanoids 2022)
- **Journal**: None
- **Summary**: Affordance detection from visual input is a fundamental step in autonomous robotic manipulation. Existing solutions to the problem of affordance detection rely on convolutional neural networks. However, these networks do not consider the spatial arrangement of the input data and miss parts-to-whole relationships. Therefore, they fall short when confronted with novel, previously unseen object instances or new viewpoints. One solution to overcome such limitations can be to resort to capsule networks. In this paper, we introduce the first affordance detection network based on dynamic tree-structured capsules for sparse 3D point clouds. We show that our capsule-based network outperforms current state-of-the-art models on viewpoint invariance and parts-segmentation of new object instances through a novel dataset we only used for evaluation and it is publicly available from github.com/gipfelen/DTCG-Net. In the experimental evaluation we will show that our algorithm is superior to current affordance detection methods when faced with grasping previously unseen objects thanks to our Capsule Network enforcing a parts-to-whole representation.



### Interpretable Machine Learning System to EEG Patterns on the Ictal-Interictal-Injury Continuum
- **Arxiv ID**: http://arxiv.org/abs/2211.05207v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2.6; I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2211.05207v4)
- **Published**: 2022-11-09 21:33:40+00:00
- **Updated**: 2023-04-11 18:50:03+00:00
- **Authors**: Alina Jade Barnett, Zhicheng Guo, Jin Jing, Wendong Ge, Cynthia Rudin, M. Brandon Westover
- **Comment**: 20 pages including appendices, 7 figures, submitted for peer review
- **Journal**: None
- **Summary**: In intensive care units (ICUs), critically ill patients are monitored with electroencephalograms (EEGs) to prevent serious brain injury. The number of patients who can be monitored is constrained by the availability of trained physicians to read EEGs, and EEG interpretation can be subjective and prone to inter-observer variability. Automated deep learning systems for EEG could reduce human bias and accelerate the diagnostic process. However, black box deep learning models are untrustworthy, difficult to troubleshoot, and lack accountability in real-world applications, leading to a lack of trust and adoption by clinicians. To address these challenges, we propose a novel interpretable deep learning model that not only predicts the presence of harmful brainwave patterns but also provides high-quality case-based explanations of its decisions. Our model performs better than the corresponding black box model, despite being constrained to be interpretable. The learned 2D embedded space provides the first global overview of the structure of ictal-interictal-injury continuum brainwave patterns. The ability to understand how our model arrived at its decisions will not only help clinicians to diagnose and treat harmful brain activities more accurately but also increase their trust and adoption of machine learning models in clinical practice; this could be an integral component of the ICU neurologists' standard workflow.



### Content-Diverse Comparisons improve IQA
- **Arxiv ID**: http://arxiv.org/abs/2211.05215v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.05215v1)
- **Published**: 2022-11-09 21:53:13+00:00
- **Updated**: 2022-11-09 21:53:13+00:00
- **Authors**: William Thong, Jose Costa Pereira, Sarah Parisot, Ales Leonardis, Steven McDonagh
- **Comment**: Accepted at British Machine Vision Conference (BMVC) 2022
- **Journal**: None
- **Summary**: Image quality assessment (IQA) forms a natural and often straightforward undertaking for humans, yet effective automation of the task remains highly challenging. Recent metrics from the deep learning community commonly compare image pairs during training to improve upon traditional metrics such as PSNR or SSIM. However, current comparisons ignore the fact that image content affects quality assessment as comparisons only occur between images of similar content. This restricts the diversity and number of image pairs that the model is exposed to during training. In this paper, we strive to enrich these comparisons with content diversity. Firstly, we relax comparison constraints, and compare pairs of images with differing content. This increases the variety of available comparisons. Secondly, we introduce listwise comparisons to provide a holistic view to the model. By including differentiable regularizers, derived from correlation coefficients, models can better adjust predicted scores relative to one another. Evaluation on multiple benchmarks, covering a wide range of distortions and image content, shows the effectiveness of our learning scheme for training image quality assessment models.



### Reproducibility in medical image radiomic studies: contribution of dynamic histogram binning
- **Arxiv ID**: http://arxiv.org/abs/2211.05241v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.05241v1)
- **Published**: 2022-11-09 22:23:32+00:00
- **Updated**: 2022-11-09 22:23:32+00:00
- **Authors**: Darryl E. Wright, Cole Cook, Jason Klug, Panagiotis Korfiatis, Timothy L. Kline
- **Comment**: 5 pages, 1 figure, 2 tables
- **Journal**: None
- **Summary**: The de facto standard of dynamic histogram binning for radiomic feature extraction leads to an elevated sensitivity to fluctuations in annotated regions. This may impact the majority of radiomic studies published recently and contribute to issues regarding poor reproducibility of radiomic-based machine learning that has led to significant efforts for data harmonization; however, we believe the issues highlighted here are comparatively neglected, but often remedied by choosing static binning.   The field of radiomics has improved through the development of community standards and open-source libraries such as PyRadiomics. But differences in image acquisition, systematic differences between observers' annotations, and preprocessing steps still pose challenges. These can change the distribution of voxels altering extracted features and can be exacerbated with dynamic binning.



### Integrating machine learning concepts into undergraduate classes
- **Arxiv ID**: http://arxiv.org/abs/2211.06491v1
- **DOI**: 10.1109/FIE49875.2021.9637283
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.06491v1)
- **Published**: 2022-11-09 23:28:45+00:00
- **Updated**: 2022-11-09 23:28:45+00:00
- **Authors**: Chinmay Sahu, Blaine Ayotte, Mahesh K. Banavar
- **Comment**: This paper has been accepted to 2021 IEEE Frontiers in Education
  Conference (FIE)
- **Journal**: None
- **Summary**: In this innovative practice work-in-progress paper, we compare two different methods to teach machine learning concepts to undergraduate students in Electrical Engineering. While machine learning is now being offered as a senior-level elective in several curricula, this does not mean all students are exposed to it. Exposure to the concepts and practical applications of machine learning will assist in the creation of a workforce ready to tackle problems related to machine learning, currently a hot topic in industry. Preliminary assessments indicate that this approach promotes student learning. While students prefer the proposed side-by-side teaching approach, numerical comparisons show that the workshop approach may be more effective for student learning, indicating that further work in this area is required.



