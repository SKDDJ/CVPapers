# Arxiv Papers in cs.CV on 2022-11-18
### Potential Auto-driving Threat: Universal Rain-removal Attack
- **Arxiv ID**: http://arxiv.org/abs/2211.09959v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.09959v1)
- **Published**: 2022-11-18 00:35:05+00:00
- **Updated**: 2022-11-18 00:35:05+00:00
- **Authors**: Jinchegn Hu, Jihao Li, Zhuoran Hou, Jingjing Jiang, Cunjia Liu, Yuanjian Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of robustness in adverse weather conditions is considered a significant challenge for computer vision algorithms in the applicants of autonomous driving. Image rain removal algorithms are a general solution to this problem. They find a deep connection between raindrops/rain-streaks and images by mining the hidden features and restoring information about the rain-free environment based on the powerful representation capabilities of neural networks. However, previous research has focused on architecture innovations and has yet to consider the vulnerability issues that already exist in neural networks. This research gap hints at a potential security threat geared toward the intelligent perception of autonomous driving in the rain. In this paper, we propose a universal rain-removal attack (URA) on the vulnerability of image rain-removal algorithms by generating a non-additive spatial perturbation that significantly reduces the similarity and image quality of scene restoration. Notably, this perturbation is difficult to recognise by humans and is also the same for different target images. Thus, URA could be considered a critical tool for the vulnerability detection of image rain-removal algorithms. It also could be developed as a real-world artificial intelligence attack method. Experimental results show that URA can reduce the scene repair capability by 39.5% and the image generation quality by 26.4%, targeting the state-of-the-art (SOTA) single-image rain-removal algorithms currently available.



### Ask4Help: Learning to Leverage an Expert for Embodied Tasks
- **Arxiv ID**: http://arxiv.org/abs/2211.09960v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.09960v1)
- **Published**: 2022-11-18 00:38:51+00:00
- **Updated**: 2022-11-18 00:38:51+00:00
- **Authors**: Kunal Pratap Singh, Luca Weihs, Alvaro Herrasti, Jonghyun Choi, Aniruddha Kemhavi, Roozbeh Mottaghi
- **Comment**: Accepted at NeurIPS, 2022
- **Journal**: None
- **Summary**: Embodied AI agents continue to become more capable every year with the advent of new models, environments, and benchmarks, but are still far away from being performant and reliable enough to be deployed in real, user-facing, applications. In this paper, we ask: can we bridge this gap by enabling agents to ask for assistance from an expert such as a human being? To this end, we propose the Ask4Help policy that augments agents with the ability to request, and then use expert assistance. Ask4Help policies can be efficiently trained without modifying the original agent's parameters and learn a desirable trade-off between task performance and the amount of requested help, thereby reducing the cost of querying the expert. We evaluate Ask4Help on two different tasks -- object goal navigation and room rearrangement and see substantial improvements in performance using minimal help. On object navigation, an agent that achieves a $52\%$ success rate is raised to $86\%$ with $13\%$ help and for rearrangement, the state-of-the-art model with a $7\%$ success rate is dramatically improved to $90.4\%$ using $39\%$ help. Human trials with Ask4Help demonstrate the efficacy of our approach in practical scenarios. We release the code for Ask4Help here: https://github.com/allenai/ask4help.



### AVATAR submission to the Ego4D AV Transcription Challenge
- **Arxiv ID**: http://arxiv.org/abs/2211.09966v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.09966v1)
- **Published**: 2022-11-18 01:03:30+00:00
- **Updated**: 2022-11-18 01:03:30+00:00
- **Authors**: Paul Hongsuck Seo, Arsha Nagrani, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: In this report, we describe our submission to the Ego4D AudioVisual (AV) Speech Transcription Challenge 2022. Our pipeline is based on AVATAR, a state of the art encoder-decoder model for AV-ASR that performs early fusion of spectrograms and RGB images. We describe the datasets, experimental settings and ablations. Our final method achieves a WER of 68.40 on the challenge test set, outperforming the baseline by 43.7%, and winning the challenge.



### The Runner-up Solution for YouTube-VIS Long Video Challenge 2022
- **Arxiv ID**: http://arxiv.org/abs/2211.09973v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.09973v1)
- **Published**: 2022-11-18 01:40:59+00:00
- **Updated**: 2022-11-18 01:40:59+00:00
- **Authors**: Junfeng Wu, Yi Jiang, Qihao Liu, Xiang Bai, Song Bai
- **Comment**: The Runner-up Solution for YouTube-VIS Long Video Challenge 2022,
  ECCV 2022 Workshop. arXiv admin note: text overlap with arXiv:2207.10661
- **Journal**: None
- **Summary**: This technical report describes our 2nd-place solution for the ECCV 2022 YouTube-VIS Long Video Challenge. We adopt the previously proposed online video instance segmentation method IDOL for this challenge. In addition, we use pseudo labels to further help contrastive learning, so as to obtain more temporally consistent instance embedding to improve tracking performance between frames. The proposed method obtains 40.2 AP on the YouTube-VIS 2022 long video dataset and was ranked second place in this challenge. We hope our simple and effective method could benefit further research.



### Comparison between EM and FCM algorithms in skin tone extraction
- **Arxiv ID**: http://arxiv.org/abs/2211.09979v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.09979v1)
- **Published**: 2022-11-18 01:53:51+00:00
- **Updated**: 2022-11-18 01:53:51+00:00
- **Authors**: Elham Ravanbakhsh, Mosab Rezaei, Ehsan Namjoo, Padideh Choobdar
- **Comment**: 2016 1st International Conference on New Research Achievements in
  Electrical and Computer Engineering (ICNRAECE)
- **Journal**: None
- **Summary**: This study aims to investigate implementing EM and FCM algorithms for skin color extraction. The capabilities of three well-known color spaces, namely, RGB, HSV, and YCbCr for skin-tone extraction are assessed by using statistical modeling of skin tones using EM and FCM algorithms. The results show that utilizing a Gaussian mixture model for parametric modeling of skin tones using EM algorithm works well in HSV color space when all three components of the color vector are used. In spite of discarding the luminance components in YCbCr and HSV color spaces, EM algorithm provides the best results. The results of the detailed comparisons are explained in the conclusion.



### Contrastive Positive Sample Propagation along the Audio-Visual Event Line
- **Arxiv ID**: http://arxiv.org/abs/2211.09980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.09980v1)
- **Published**: 2022-11-18 01:55:45+00:00
- **Updated**: 2022-11-18 01:55:45+00:00
- **Authors**: Jinxing Zhou, Dan Guo, Meng Wang
- **Comment**: Accepted to TPAMI; Dataset and Code are available at
  https://github.com/jasongief/CPSP. arXiv admin note: substantial text overlap
  with arXiv:2104.00239
- **Journal**: None
- **Summary**: Visual and audio signals often coexist in natural environments, forming audio-visual events (AVEs). Given a video, we aim to localize video segments containing an AVE and identify its category. It is pivotal to learn the discriminative features for each video segment. Unlike existing work focusing on audio-visual feature fusion, in this paper, we propose a new contrastive positive sample propagation (CPSP) method for better deep feature representation learning. The contribution of CPSP is to introduce the available full or weak label as a prior that constructs the exact positive-negative samples for contrastive learning. Specifically, the CPSP involves comprehensive contrastive constraints: pair-level positive sample propagation (PSP), segment-level and video-level positive sample activation (PSA$_S$ and PSA$_V$). Three new contrastive objectives are proposed (\emph{i.e.}, $\mathcal{L}_{\text{avpsp}}$, $\mathcal{L}_\text{spsa}$, and $\mathcal{L}_\text{vpsa}$) and introduced into both the fully and weakly supervised AVE localization. To draw a complete picture of the contrastive learning in AVE localization, we also study the self-supervised positive sample propagation (SSPSP). As a result, CPSP is more helpful to obtain the refined audio-visual features that are distinguishable from the negatives, thus benefiting the classifier prediction. Extensive experiments on the AVE and the newly collected VGGSound-AVEL100k datasets verify the effectiveness and generalization ability of our method.



### Look More but Care Less in Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2211.09992v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.09992v1)
- **Published**: 2022-11-18 02:39:56+00:00
- **Updated**: 2022-11-18 02:39:56+00:00
- **Authors**: Yitian Zhang, Yue Bai, Huan Wang, Yi Xu, Yun Fu
- **Comment**: Accepted by NeurIPS 2022
- **Journal**: None
- **Summary**: Existing action recognition methods typically sample a few frames to represent each video to avoid the enormous computation, which often limits the recognition performance. To tackle this problem, we propose Ample and Focal Network (AFNet), which is composed of two branches to utilize more frames but with less computation. Specifically, the Ample Branch takes all input frames to obtain abundant information with condensed computation and provides the guidance for Focal Branch by the proposed Navigation Module; the Focal Branch squeezes the temporal size to only focus on the salient frames at each convolution block; in the end, the results of two branches are adaptively fused to prevent the loss of information. With this design, we can introduce more frames to the network but cost less computation. Besides, we demonstrate AFNet can utilize fewer frames while achieving higher accuracy as the dynamic selection in intermediate features enforces implicit temporal modeling. Further, we show that our method can be extended to reduce spatial redundancy with even less cost. Extensive experiments on five datasets demonstrate the effectiveness and efficiency of our method.



### 3d human motion generation from the text via gesture action classification and the autoregressive model
- **Arxiv ID**: http://arxiv.org/abs/2211.10003v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2211.10003v1)
- **Published**: 2022-11-18 03:05:49+00:00
- **Updated**: 2022-11-18 03:05:49+00:00
- **Authors**: Gwantae Kim, Youngsuk Ryu, Junyeop Lee, David K. Han, Jeongmin Bae, Hanseok Ko
- **Comment**: 5 pages, 3 figures, ICIP 2022
- **Journal**: None
- **Summary**: In this paper, a deep learning-based model for 3D human motion generation from the text is proposed via gesture action classification and an autoregressive model. The model focuses on generating special gestures that express human thinking, such as waving and nodding. To achieve the goal, the proposed method predicts expression from the sentences using a text classification model based on a pretrained language model and generates gestures using the gate recurrent unit-based autoregressive model. Especially, we proposed the loss for the embedding space for restoring raw motions and generating intermediate motions well. Moreover, the novel data augmentation method and stop token are proposed to generate variable length motions. To evaluate the text classification model and 3D human motion generation model, a gesture action classification dataset and action-based gesture dataset are collected. With several experiments, the proposed method successfully generates perceptually natural and realistic 3D human motion from the text. Moreover, we verified the effectiveness of the proposed method using a public-available action recognition dataset to evaluate cross-dataset generalization performance.



### Step Counting with Attention-based LSTM
- **Arxiv ID**: http://arxiv.org/abs/2211.13114v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2211.13114v1)
- **Published**: 2022-11-18 03:33:57+00:00
- **Updated**: 2022-11-18 03:33:57+00:00
- **Authors**: Shehroz S. Khan, Ali Abedi
- **Comment**: None
- **Journal**: None
- **Summary**: Physical activity is recognized as an essential component of overall health. One measure of physical activity, the step count, is well known as a predictor of long-term morbidity and mortality. Step Counting (SC) is the automated counting of the number of steps an individual takes over a specified period of time and space. Due to the ubiquity of smartphones and smartwatches, most current SC approaches rely on the built-in accelerometer sensors on these devices. The sensor signals are analyzed as multivariate time series, and the number of steps is calculated through a variety of approaches, such as time-domain, frequency-domain, machine-learning, and deep-learning approaches. Most of the existing approaches rely on dividing the input signal into windows, detecting steps in each window, and summing the detected steps. However, these approaches require the determination of multiple parameters, including the window size. Furthermore, most of the existing deep-learning SC approaches require ground-truth labels for every single step, which can be arduous and time-consuming to annotate. To circumvent these requirements, we present a novel SC approach utilizing many-to-one attention-based LSTM. With the proposed LSTM network, SC is solved as a regression problem, taking the entire sensor signal as input and the step count as the output. The analysis shows that the attention-based LSTM automatically learned the pattern of steps even in the absence of ground-truth labels. The experimental results on three publicly available SC datasets demonstrate that the proposed method successfully counts the number of steps with low values of mean absolute error and high values of SC accuracy.



### LiSnowNet: Real-time Snow Removal for LiDAR Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2211.10023v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.10023v1)
- **Published**: 2022-11-18 04:19:05+00:00
- **Updated**: 2022-11-18 04:19:05+00:00
- **Authors**: Ming-Yuan Yu, Ram Vasudevan, Matthew Johnson-Roberson
- **Comment**: The paper has been accepted for the 2022 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2022)
- **Journal**: None
- **Summary**: LiDARs have been widely adopted to modern self-driving vehicles, providing 3D information of the scene and surrounding objects. However, adverser weather conditions still pose significant challenges to LiDARs since point clouds captured during snowfall can easily be corrupted. The resulting noisy point clouds degrade downstream tasks such as mapping. Existing works in de-noising point clouds corrupted by snow are based on nearest-neighbor search, and thus do not scale well with modern LiDARs which usually capture $100k$ or more points at 10Hz. In this paper, we introduce an unsupervised de-noising algorithm, LiSnowNet, running 52$\times$ faster than the state-of-the-art methods while achieving superior performance in de-noising. Unlike previous methods, the proposed algorithm is based on a deep convolutional neural network and can be easily deployed to hardware accelerators such as GPUs. In addition, we demonstrate how to use the proposed method for mapping even with corrupted point clouds.



### DGD-cGAN: A Dual Generator for Image Dewatering and Restoration
- **Arxiv ID**: http://arxiv.org/abs/2211.10026v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.10026v1)
- **Published**: 2022-11-18 04:42:52+00:00
- **Updated**: 2022-11-18 04:42:52+00:00
- **Authors**: Salma Gonzalez-Sabbagh, Antonio Robles-Kelly, Shang Gao
- **Comment**: 12 pages and 61 images
- **Journal**: None
- **Summary**: Underwater images are usually covered with a blue-greenish colour cast, making them distorted, blurry or low in contrast. This phenomenon occurs due to the light attenuation given by the scattering and absorption in the water column. In this paper, we present an image enhancement approach for dewatering which employs a conditional generative adversarial network (cGAN) with two generators. Our Dual Generator Dewatering cGAN (DGD-cGAN) removes the haze and colour cast induced by the water column and restores the true colours of underwater scenes whereby the effects of various attenuation and scattering phenomena that occur in underwater images are tackled by the two generators. The first generator takes at input the underwater image and predicts the dewatered scene, while the second generator learns the underwater image formation process by implementing a custom loss function based upon the transmission and the veiling light components of the image formation model. Our experiments show that DGD-cGAN consistently delivers a margin of improvement as compared with the state-of-the-art methods on several widely available datasets.



### Vision Transformers in Medical Imaging: A Review
- **Arxiv ID**: http://arxiv.org/abs/2211.10043v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.10043v1)
- **Published**: 2022-11-18 05:52:37+00:00
- **Updated**: 2022-11-18 05:52:37+00:00
- **Authors**: Emerald U. Henry, Onyeka Emebob, Conrad Asotie Omonhinmin
- **Comment**: None
- **Journal**: None
- **Summary**: Transformer, a model comprising attention-based encoder-decoder architecture, have gained prevalence in the field of natural language processing (NLP) and recently influenced the computer vision (CV) space. The similarities between computer vision and medical imaging, reviewed the question among researchers if the impact of transformers on computer vision be translated to medical imaging? In this paper, we attempt to provide a comprehensive and recent review on the application of transformers in medical imaging by; describing the transformer model comparing it with a diversity of convolutional neural networks (CNNs), detailing the transformer based approaches for medical image classification, segmentation, registration and reconstruction with a focus on the image modality, comparing the performance of state-of-the-art transformer architectures to best performing CNNs on standard medical datasets.



### Pedestrian Spatio-Temporal Information Fusion For Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2211.10052v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10052v2)
- **Published**: 2022-11-18 06:41:02+00:00
- **Updated**: 2022-11-28 13:02:01+00:00
- **Authors**: Chao Hu, Liqiang Zhu
- **Comment**: International Conference on Intelligent Media, Big Data and Knowledge
  Mining
- **Journal**: IMBDKM 2023
- **Summary**: Aiming at the problem that the current video anomaly detection cannot fully use the temporal information and ignore the diversity of normal behavior, an anomaly detection method is proposed to integrate the spatiotemporal information of pedestrians. Based on the convolutional autoencoder, the input frame is compressed and restored through the encoder and decoder. Anomaly detection is realized according to the difference between the output frame and the true value. In order to strengthen the characteristic information connection between continuous video frames, the residual temporal shift module and the residual channel attention module are introduced to improve the modeling ability of the network on temporal information and channel information, respectively. Due to the excessive generalization of convolutional neural networks, in the memory enhancement modules, the hopping connections of each codec layer are added to limit autoencoders' ability to represent abnormal frames too vigorously and improve the anomaly detection accuracy of the network. In addition, the objective function is modified by a feature discretization loss, which effectively distinguishes different normal behavior patterns. The experimental results on the CUHK Avenue and ShanghaiTech datasets show that the proposed method is superior to the current mainstream video anomaly detection methods while meeting the real-time requirements.



### Contrastive Losses Are Natural Criteria for Unsupervised Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/2211.10056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10056v1)
- **Published**: 2022-11-18 07:01:28+00:00
- **Updated**: 2022-11-18 07:01:28+00:00
- **Authors**: Zongshang Pang, Yuta Nakashima, Mayu Otani, Hajime Nagahara
- **Comment**: To appear in WACV2023
- **Journal**: None
- **Summary**: Video summarization aims to select the most informative subset of frames in a video to facilitate efficient video browsing. Unsupervised methods usually rely on heuristic training objectives such as diversity and representativeness. However, such methods need to bootstrap the online-generated summaries to compute the objectives for importance score regression. We consider such a pipeline inefficient and seek to directly quantify the frame-level importance with the help of contrastive losses in the representation learning literature. Leveraging the contrastive losses, we propose three metrics featuring a desirable key frame: local dissimilarity, global consistency, and uniqueness. With features pre-trained on the image classification task, the metrics can already yield high-quality importance scores, demonstrating competitive or better performance than past heavily-trained methods. We show that by refining the pre-trained features with a lightweight contrastively learned projection module, the frame-level importance scores can be further improved, and the model can also leverage a large number of random videos and generalize to test videos with decent performance. Code available at https://github.com/pangzss/pytorch-CTVSUM.



### Normal Reference Attention and Defective Feature Perception Network for Surface Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/2211.10060v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.10060v2)
- **Published**: 2022-11-18 07:13:55+00:00
- **Updated**: 2023-01-13 09:33:00+00:00
- **Authors**: Wei Luo, Haiming Yao, Wenyong Yu
- **Comment**: 15pages
- **Journal**: None
- **Summary**: Visual anomaly detection plays a significant role in the development of industrial automatic product quality inspection. As a result of the utmost imbalance in the amount of normal and abnormal data, growing attention has been given to unsupervised methods for defect detection. Although existing reconstruction-based methods have been widely studied recently, establishing a robust reconstruction model for various textured surface defect detection remains a challenging task due to homogeneous and nonregular surface textures. In this paper, we propose a novel unsupervised reconstruction-based method called the normal reference attention and defective feature perception network (NDP-Net) to accurately inspect a variety of textured defects. Unlike most reconstruction-based methods, our NDP-Net first employs an encoding module that extracts multi scale discriminative features of the surface textures, which is augmented with the defect discriminative ability by the proposed artificial defects and the novel pixel-level defect perception loss. Subsequently, a novel reference-based attention module (RBAM) is proposed to leverage the normal features of the fixed reference image to repair the defective features and restrain the reconstruction of the defects. Next, the repaired features are fed into a decoding module to reconstruct the normal textured background. Finally, the novel multi scale defect segmentation module (MSDSM) is introduced for precise defect detection and segmentation. In addition, a two-stage training strategy is utilized to enhance the inspection performance.



### Detect Only What You Specify : Object Detection with Linguistic Target
- **Arxiv ID**: http://arxiv.org/abs/2211.11572v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.11572v1)
- **Published**: 2022-11-18 07:28:47+00:00
- **Updated**: 2022-11-18 07:28:47+00:00
- **Authors**: Moyuru Yamada
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is a computer vision task of predicting a set of bounding boxes and category labels for each object of interest in a given image. The category is related to a linguistic symbol such as 'dog' or 'person' and there should be relationships among them. However the object detector only learns to classify the categories and does not treat them as the linguistic symbols. Multi-modal models often use the pre-trained object detector to extract object features from the image, but the models are separated from the detector and the extracted visual features does not change with their linguistic input. We rethink the object detection as a vision-and-language reasoning task. We then propose targeted detection task, where detection targets are given by a natural language and the goal of the task is to detect only all the target objects in a given image. There are no detection if the target is not given. Commonly used modern object detectors have many hand-designed components like anchor and it is difficult to fuse the textual inputs into the complex pipeline. We thus propose Language-Targeted Detector (LTD) for the targeted detection based on a recently proposed Transformer-based detector. LTD is a encoder-decoder architecture and our conditional decoder allows the model to reason about the encoded image with the textual input as the linguistic context. We evaluate detection performances of LTD on COCO object detection dataset and also show that our model improves the detection results with the textual input grounding to the visual object.



### Explanation on Pretraining Bias of Finetuned Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2211.15428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.15428v1)
- **Published**: 2022-11-18 07:57:38+00:00
- **Updated**: 2022-11-18 07:57:38+00:00
- **Authors**: Bumjin Park, Jaesik Choi
- **Comment**: None
- **Journal**: None
- **Summary**: As the number of fine tuning of pretrained models increased, understanding the bias of pretrained model is essential. However, there is little tool to analyse transformer architecture and the interpretation of the attention maps is still challenging. To tackle the interpretability, we propose Input-Attribution and Attention Score Vector (IAV) which measures the similarity between attention map and input-attribution and shows the general trend of interpretable attention patterns. We empirically explain the pretraining bias of supervised and unsupervised pretrained ViT models, and show that each head in ViT has a specific range of agreement on the decision of the classification. We show that generalization, robustness and entropy of attention maps are not property of pretraining types. On the other hand, IAV trend can separate the pretraining types.



### UnconFuse: Avatar Reconstruction from Unconstrained Images
- **Arxiv ID**: http://arxiv.org/abs/2211.10098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10098v1)
- **Published**: 2022-11-18 08:54:33+00:00
- **Updated**: 2022-11-18 08:54:33+00:00
- **Authors**: Han Huang, Liliang Chen, Xihao Wang
- **Comment**: Accepted to ECCV 2022 Workshop
- **Journal**: None
- **Summary**: The report proposes an effective solution about 3D human body reconstruction from multiple unconstrained frames for ECCV 2022 WCPA Challenge: From Face, Body and Fashion to 3D Virtual avatars I (track1: Multi-View Based 3D Human Body Reconstruction). We reproduce the reconstruction method presented in MVP-Human as our baseline, and make some improvements for the particularity of this challenge. We finally achieve the score 0.93 on the official testing set, getting the 1st place on the leaderboard.



### Let's Enhance: A Deep Learning Approach to Extreme Deblurring of Text Images
- **Arxiv ID**: http://arxiv.org/abs/2211.10103v2
- **DOI**: 10.3934/ipi.2023019
- **Categories**: **cs.CV**, cs.LG, cs.NA, math.NA, 94A08, 68T07, 68T20
- **Links**: [PDF](http://arxiv.org/pdf/2211.10103v2)
- **Published**: 2022-11-18 09:06:56+00:00
- **Updated**: 2023-04-23 11:44:02+00:00
- **Authors**: Theophil Trippe, Martin Genzel, Jan Macdonald, Maximilian März
- **Comment**: This article has been published in a revised form in Inverse Problems
  and Imaging
- **Journal**: Inverse Probl. Imaging 17:5 (2023) 1041-1068
- **Summary**: This work presents a novel deep-learning-based pipeline for the inverse problem of image deblurring, leveraging augmentation and pre-training with synthetic data. Our results build on our winning submission to the recent Helsinki Deblur Challenge 2021, whose goal was to explore the limits of state-of-the-art deblurring algorithms in a real-world data setting. The task of the challenge was to deblur out-of-focus images of random text, thereby in a downstream task, maximizing an optical-character-recognition-based score function. A key step of our solution is the data-driven estimation of the physical forward model describing the blur process. This enables a stream of synthetic data, generating pairs of ground-truth and blurry images on-the-fly, which is used for an extensive augmentation of the small amount of challenge data provided. The actual deblurring pipeline consists of an approximate inversion of the radial lens distortion (determined by the estimated forward model) and a U-Net architecture, which is trained end-to-end. Our algorithm was the only one passing the hardest challenge level, achieving over $70\%$ character recognition accuracy. Our findings are well in line with the paradigm of data-centric machine learning, and we demonstrate its effectiveness in the context of inverse problems. Apart from a detailed presentation of our methodology, we also analyze the importance of several design choices in a series of ablation studies. The code of our challenge submission is available under https://github.com/theophil-trippe/HDC_TUBerlin_version_1.



### Stereo Image Rain Removal via Dual-View Mutual Attention
- **Arxiv ID**: http://arxiv.org/abs/2211.10104v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10104v2)
- **Published**: 2022-11-18 09:07:01+00:00
- **Updated**: 2022-12-20 02:30:13+00:00
- **Authors**: Yanyan Wei, Zhao Zhang, Zhongqiu Zhao, Yang Zhao, Richang Hong, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Stereo images, containing left and right view images with disparity, are utilized in solving low-vision tasks recently, e.g., rain removal and super-resolution. Stereo image restoration methods usually obtain better performance than monocular methods by learning the disparity between dual views either implicitly or explicitly. However, existing stereo rain removal methods still cannot make full use of the complementary information between two views, and we find it is because: 1) the rain streaks have more complex distributions in directions and densities, which severely damage the complementary information and pose greater challenges; 2) the disparity estimation is not accurate enough due to the imperfect fusion mechanism for the features between two views. To overcome such limitations, we propose a new \underline{Stereo} \underline{I}mage \underline{R}ain \underline{R}emoval method (StereoIRR) via sufficient interaction between two views, which incorporates: 1) a new Dual-view Mutual Attention (DMA) mechanism which generates mutual attention maps by taking left and right views as key information for each other to facilitate cross-view feature fusion; 2) a long-range and cross-view interaction, which is constructed with basic blocks and dual-view mutual attention, can alleviate the adverse effect of rain on complementary information to help the features of stereo images to get long-range and cross-view interaction and fusion. Notably, StereoIRR outperforms other related monocular and stereo image rain removal methods on several datasets. Our codes and datasets will be released.



### $α$ DARTS Once More: Enhancing Differentiable Architecture Search by Masked Image Modeling
- **Arxiv ID**: http://arxiv.org/abs/2211.10105v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10105v1)
- **Published**: 2022-11-18 09:07:19+00:00
- **Updated**: 2022-11-18 09:07:19+00:00
- **Authors**: Bicheng Guo, Shuxuan Guo, Miaojing Shi, Peng Chen, Shibo He, Jiming Chen, Kaicheng Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Differentiable architecture search (DARTS) has been a mainstream direction in automatic machine learning. Since the discovery that original DARTS will inevitably converge to poor architectures, recent works alleviate this by either designing rule-based architecture selection techniques or incorporating complex regularization techniques, abandoning the simplicity of the original DARTS that selects architectures based on the largest parametric value, namely $\alpha$. Moreover, we find that all the previous attempts only rely on classification labels, hence learning only single modal information and limiting the representation power of the shared network. To this end, we propose to additionally inject semantic information by formulating a patch recovery approach. Specifically, we exploit the recent trending masked image modeling and do not abandon the guidance from the downstream tasks during the search phase. Our method surpasses all previous DARTS variants and achieves state-of-the-art results on CIFAR-10, CIFAR-100, and ImageNet without complex manual-designed strategies.



### Mixture Domain Adaptation to Improve Semantic Segmentation in Real-World Surveillance
- **Arxiv ID**: http://arxiv.org/abs/2211.10119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10119v1)
- **Published**: 2022-11-18 09:44:41+00:00
- **Updated**: 2022-11-18 09:44:41+00:00
- **Authors**: Sébastien Piérard, Anthony Cioppa, Anaïs Halin, Renaud Vandeghen, Maxime Zanella, Benoît Macq, Saïd Mahmoudi, Marc Van Droogenbroeck
- **Comment**: None
- **Journal**: None
- **Summary**: Various tasks encountered in real-world surveillance can be addressed by determining posteriors (e.g. by Bayesian inference or machine learning), based on which critical decisions must be taken. However, the surveillance domain (acquisition device, operating conditions, etc.) is often unknown, which prevents any possibility of scene-specific optimization. In this paper, we define a probabilistic framework and present a formal proof of an algorithm for the unsupervised many-to-infinity domain adaptation of posteriors. Our proposed algorithm is applicable when the probability measure associated with the target domain is a convex combination of the probability measures of the source domains. It makes use of source models and a domain discriminator model trained off-line to compute posteriors adapted on the fly to the target domain. Finally, we show the effectiveness of our algorithm for the task of semantic segmentation in real-world surveillance. The code is publicly available at https://github.com/rvandeghen/MDA.



### Spatio-Temporal Feedback Control of Small Target Motion Detection Visual System
- **Arxiv ID**: http://arxiv.org/abs/2211.10128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10128v1)
- **Published**: 2022-11-18 10:10:48+00:00
- **Updated**: 2022-11-18 10:10:48+00:00
- **Authors**: Hongxin Wang, Zhiyan Zhong, Fang Lei, Xiaohua Jing, Jigen Peng, Shigang Yue
- **Comment**: None
- **Journal**: None
- **Summary**: Feedback is crucial to motion perception in animals' visual systems where its spatial and temporal dynamics are often shaped by movement patterns of surrounding environments. However, such spatio-temporal feedback has not been deeply explored in designing neural networks to detect small moving targets that cover only one or a few pixels in image while presenting extremely limited visual features. In this paper, we address small target motion detection problem by developing a visual system with spatio-temporal feedback loop, and further reveal its important roles in suppressing false positive background movement while enhancing network responses to small targets. Specifically, the proposed visual system is composed of two complementary subnetworks. The first subnetwork is designed to extract spatial and temporal motion patterns of cluttered backgrounds by neuronal ensemble coding. The second subnetwork is developed to capture small target motion information and integrate the spatio-temporal feedback signal from the first subnetwork to inhibit background false positives. Experimental results demonstrate that the proposed spatio-temporal feedback visual system is more competitive than existing methods in discriminating small moving targets from complex dynamic environment.



### Joint nnU-Net and Radiomics Approaches for Segmentation and Prognosis of Head and Neck Cancers with PET/CT images
- **Arxiv ID**: http://arxiv.org/abs/2211.10138v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.10138v1)
- **Published**: 2022-11-18 10:31:26+00:00
- **Updated**: 2022-11-18 10:31:26+00:00
- **Authors**: Hui Xu, Yihao Li, Wei Zhao, Gwenolé Quellec, Lijun Lu, Mathieu Hatt
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic segmentation of head and neck cancer (HNC) tumors and lymph nodes plays a crucial role in the optimization treatment strategy and prognosis analysis. This study aims to employ nnU-Net for automatic segmentation and radiomics for recurrence-free survival (RFS) prediction using pretreatment PET/CT images in multi-center HNC cohort. A multi-center HNC dataset with 883 patients (524 patients for training, 359 for testing) was provided in HECKTOR 2022. A bounding box of the extended oropharyngeal region was retrieved for each patient with fixed size of 224 x 224 x 224 $mm^{3}$. Then 3D nnU-Net architecture was adopted to automatic segmentation of primary tumor and lymph nodes synchronously.Based on predicted segmentation, ten conventional features and 346 standardized radiomics features were extracted for each patient. Three prognostic models were constructed containing conventional and radiomics features alone, and their combinations by multivariate CoxPH modelling. The statistical harmonization method, ComBat, was explored towards reducing multicenter variations. Dice score and C-index were used as evaluation metrics for segmentation and prognosis task, respectively. For segmentation task, we achieved mean dice score around 0.701 for primary tumor and lymph nodes by 3D nnU-Net. For prognostic task, conventional and radiomics models obtained the C-index of 0.658 and 0.645 in the test set, respectively, while the combined model did not improve the prognostic performance with the C-index of 0.648.



### 2CET-GAN: Pixel-Level GAN Model for Human Facial Expression Transfer
- **Arxiv ID**: http://arxiv.org/abs/2211.11570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.11570v1)
- **Published**: 2022-11-18 11:10:44+00:00
- **Updated**: 2022-11-18 11:10:44+00:00
- **Authors**: Xiaohang Hu, Nuha Aldausari, Gelareh Mohammadi
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Recent studies have used GAN to transfer expressions between human faces. However, existing models have many flaws: relying on emotion labels, lacking continuous expressions, and failing to capture the expression details. To address these limitations, we propose a novel CycleGAN- and InfoGAN-based network called 2 Cycles Expression Transfer GAN (2CET-GAN), which can learn continuous expression transfer without using emotion labels. The experiment shows our network can generate diverse and high-quality expressions and can generalize to unknown identities. To the best of our knowledge, we are among the first to successfully use an unsupervised approach to disentangle expression representation from identities at the pixel level.



### Improving Pixel-Level Contrastive Learning by Leveraging Exogenous Depth Information
- **Arxiv ID**: http://arxiv.org/abs/2211.10177v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10177v1)
- **Published**: 2022-11-18 11:45:39+00:00
- **Updated**: 2022-11-18 11:45:39+00:00
- **Authors**: Ahmed Ben Saad, Kristina Prokopetc, Josselin Kherroubi, Axel Davy, Adrien Courtois, Gabriele Facciolo
- **Comment**: Accepted for WACV 2023
- **Journal**: None
- **Summary**: Self-supervised representation learning based on Contrastive Learning (CL) has been the subject of much attention in recent years. This is due to the excellent results obtained on a variety of subsequent tasks (in particular classification), without requiring a large amount of labeled samples. However, most reference CL algorithms (such as SimCLR and MoCo, but also BYOL and Barlow Twins) are not adapted to pixel-level downstream tasks. One existing solution known as PixPro proposes a pixel-level approach that is based on filtering of pairs of positive/negative image crops of the same image using the distance between the crops in the whole image. We argue that this idea can be further enhanced by incorporating semantic information provided by exogenous data as an additional selection filter, which can be used (at training time) to improve the selection of the pixel-level positive/negative samples. In this paper we will focus on the depth information, which can be obtained by using a depth estimation network or measured from available data (stereovision, parallax motion, LiDAR, etc.). Scene depth can provide meaningful cues to distinguish pixels belonging to different objects based on their depth. We show that using this exogenous information in the contrastive loss leads to improved results and that the learned representations better follow the shapes of objects. In addition, we introduce a multi-scale loss that alleviates the issue of finding the training parameters adapted to different object sizes. We demonstrate the effectiveness of our ideas on the Breakout Segmentation on Borehole Images where we achieve an improvement of 1.9\% over PixPro and nearly 5\% over the supervised baseline. We further validate our technique on the indoor scene segmentation tasks with ScanNet and outdoor scenes with CityScapes ( 1.6\% and 1.1\% improvement over PixPro respectively).



### LVOS: A Benchmark for Long-term Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2211.10181v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10181v2)
- **Published**: 2022-11-18 11:59:37+00:00
- **Updated**: 2023-08-18 12:35:59+00:00
- **Authors**: Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, Wenqiang Zhang
- **Comment**: Accepted by ICCV 2023. Project page:
  https://lingyihongfd.github.io/lvos.github.io/
- **Journal**: None
- **Summary**: Existing video object segmentation (VOS) benchmarks focus on short-term videos which just last about 3-5 seconds and where objects are visible most of the time. These videos are poorly representative of practical applications, and the absence of long-term datasets restricts further investigation of VOS on the application in realistic scenarios. So, in this paper, we present a new benchmark dataset named \textbf{LVOS}, which consists of 220 videos with a total duration of 421 minutes. To the best of our knowledge, LVOS is the first densely annotated long-term VOS dataset. The videos in our LVOS last 1.59 minutes on average, which is 20 times longer than videos in existing VOS datasets. Each video includes various attributes, especially challenges deriving from the wild, such as long-term reappearing and cross-temporal similar objeccts.Based on LVOS, we assess existing video object segmentation algorithms and propose a Diverse Dynamic Memory network (DDMemory) that consists of three complementary memory banks to exploit temporal information adequately. The experimental results demonstrate the strength and weaknesses of prior methods, pointing promising directions for further study. Data and code are available at https://lingyihongfd.github.io/lvos.github.io/.



### IEEE Big Data Cup 2022: Privacy Preserving Matching of Encrypted Images with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2211.11565v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2211.11565v1)
- **Published**: 2022-11-18 12:20:40+00:00
- **Updated**: 2022-11-18 12:20:40+00:00
- **Authors**: Vrizlynn L. L. Thing
- **Comment**: Keywords: privacy preservation, privacy enhancing, masking, encoding,
  homomorphic encryption, deep learning, convolutional neural networks
- **Journal**: IEEE International Conference on Big Data, IEEE BigData, 2022
- **Summary**: Smart sensors, devices and systems deployed in smart cities have brought improved physical protections to their citizens. Enhanced crime prevention, and fire and life safety protection are achieved through these technologies that perform motion detection, threat and actors profiling, and real-time alerts. However, an important requirement in these increasingly prevalent deployments is the preservation of privacy and enforcement of protection of personal identifiable information. Thus, strong encryption and anonymization techniques should be applied to the collected data. In this IEEE Big Data Cup 2022 challenge, different masking, encoding and homomorphic encryption techniques were applied to the images to protect the privacy of their contents. Participants are required to develop detection solutions to perform privacy preserving matching of these images. In this paper, we describe our solution which is based on state-of-the-art deep convolutional neural networks and various data augmentation techniques. Our solution achieved 1st place at the IEEE Big Data Cup 2022: Privacy Preserving Matching of Encrypted Images Challenge.



### Multi-view Inverse Rendering for Large-scale Real-world Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/2211.10206v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10206v4)
- **Published**: 2022-11-18 12:53:10+00:00
- **Updated**: 2023-03-21 07:50:56+00:00
- **Authors**: Zhen Li, Lingli Wang, Mofang Cheng, Cihui Pan, Jiaqi Yang
- **Comment**: Accepted to CVPR 2023. The project page is at:
  https://lzleejean.github.io/TexIR
- **Journal**: None
- **Summary**: We present a efficient multi-view inverse rendering method for large-scale real-world indoor scenes that reconstructs global illumination and physically-reasonable SVBRDFs. Unlike previous representations, where the global illumination of large scenes is simplified as multiple environment maps, we propose a compact representation called Texture-based Lighting (TBL). It consists of 3D mesh and HDR textures, and efficiently models direct and infinite-bounce indirect lighting of the entire large scene. Based on TBL, we further propose a hybrid lighting representation with precomputed irradiance, which significantly improves the efficiency and alleviates the rendering noise in the material optimization. To physically disentangle the ambiguity between materials, we propose a three-stage material optimization strategy based on the priors of semantic segmentation and room segmentation. Extensive experiments show that the proposed method outperforms the state-of-the-art quantitatively and qualitatively, and enables physically-reasonable mixed-reality applications such as material editing, editable novel view synthesis and relighting. The project page is at https://lzleejean.github.io/TexIR.



### Leveraging Multi-stream Information Fusion for Trajectory Prediction in Low-illumination Scenarios: A Multi-channel Graph Convolutional Approach
- **Arxiv ID**: http://arxiv.org/abs/2211.10226v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.10226v1)
- **Published**: 2022-11-18 13:25:15+00:00
- **Updated**: 2022-11-18 13:25:15+00:00
- **Authors**: Hailong Gong, Zirui Li, Chao Lu, Guodong Du, Jianwei Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Trajectory prediction is a fundamental problem and challenge for autonomous vehicles. Early works mainly focused on designing complicated architectures for deep-learning-based prediction models in normal-illumination environments, which fail in dealing with low-light conditions. This paper proposes a novel approach for trajectory prediction in low-illumination scenarios by leveraging multi-stream information fusion, which flexibly integrates image, optical flow, and object trajectory information. The image channel employs Convolutional Neural Network (CNN) and Long Short-term Memory (LSTM) networks to extract temporal information from the camera. The optical flow channel is applied to capture the pattern of relative motion between adjacent camera frames and modelled by Spatial-Temporal Graph Convolutional Network (ST-GCN). The trajectory channel is used to recognize high-level interactions between vehicles. Finally, information from all the three channels is effectively fused in the prediction module to generate future trajectories of surrounding vehicles in low-illumination conditions. The proposed multi-channel graph convolutional approach is validated on HEV-I and newly generated Dark-HEV-I, egocentric vision datasets that primarily focus on urban intersection scenarios. The results demonstrate that our method outperforms the baselines, in standard and low-illumination scenarios. Additionally, our approach is generic and applicable to scenarios with different types of perception data. The source code of the proposed approach is available at https://github.com/TommyGong08/MSIF}{https://github.com/TommyGong08/MSIF.



### Adversarial Detection by Approximation of Ensemble Boundary
- **Arxiv ID**: http://arxiv.org/abs/2211.10227v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.10227v3)
- **Published**: 2022-11-18 13:26:57+00:00
- **Updated**: 2022-12-13 16:03:26+00:00
- **Authors**: T. Windeatt
- **Comment**: 6 pages, 3 figures, 5 tables
- **Journal**: None
- **Summary**: A spectral approximation of a Boolean function is proposed for approximating the decision boundary of an ensemble of Deep Neural Networks (DNNs) solving two-class pattern recognition problems. The Walsh combination of relatively weak DNN classifiers is shown experimentally to be capable of detecting adversarial attacks. By observing the difference in Walsh coefficient approximation between clean and adversarial images, it appears that transferability of attack may be used for detection. Approximating the decision boundary may also aid in understanding the learning and transferability properties of DNNs. While the experiments here use images, the proposed approach of modelling two-class ensemble decision boundaries could in principle be applied to any application area. Code for this paper implementing Walsh Coefficient Examples of approximating artificial Boolean functions can be found at https://doi.org/10.24433/CO.3695905.v1



### Knowing What to Label for Few Shot Microscopy Image Cell Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2211.10244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10244v1)
- **Published**: 2022-11-18 14:03:49+00:00
- **Updated**: 2022-11-18 14:03:49+00:00
- **Authors**: Youssef Dawoud, Arij Bouazizi, Katharina Ernst, Gustavo Carneiro, Vasileios Belagiannis
- **Comment**: Accepted to WACV 2023
- **Journal**: None
- **Summary**: In microscopy image cell segmentation, it is common to train a deep neural network on source data, containing different types of microscopy images, and then fine-tune it using a support set comprising a few randomly selected and annotated training target images. In this paper, we argue that the random selection of unlabelled training target images to be annotated and included in the support set may not enable an effective fine-tuning process, so we propose a new approach to optimise this image selection process. Our approach involves a new scoring function to find informative unlabelled target images. In particular, we propose to measure the consistency in the model predictions on target images against specific data augmentations. However, we observe that the model trained with source datasets does not reliably evaluate consistency on target images. To alleviate this problem, we propose novel self-supervised pretext tasks to compute the scores of unlabelled target images. Finally, the top few images with the least consistency scores are added to the support set for oracle (i.e., expert) annotation and later used to fine-tune the model to the target images. In our evaluations that involve the segmentation of five different types of cell images, we demonstrate promising results on several target test sets compared to the random selection approach as well as other selection approaches, such as Shannon's entropy and Monte-Carlo dropout.



### Delving into Transformer for Incremental Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2211.10253v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10253v1)
- **Published**: 2022-11-18 14:16:04+00:00
- **Updated**: 2022-11-18 14:16:04+00:00
- **Authors**: Zekai Xu, Mingyi Zhang, Jiayue Hou, Xing Gong, Chuan Wen, Chengjie Wang, Junge Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Incremental semantic segmentation(ISS) is an emerging task where old model is updated by incrementally adding new classes. At present, methods based on convolutional neural networks are dominant in ISS. However, studies have shown that such methods have difficulty in learning new tasks while maintaining good performance on old ones (catastrophic forgetting). In contrast, a Transformer based method has a natural advantage in curbing catastrophic forgetting due to its ability to model both long-term and short-term tasks. In this work, we explore the reasons why Transformer based architecture are more suitable for ISS, and accordingly propose propose TISS, a Transformer based method for Incremental Semantic Segmentation. In addition, to better alleviate catastrophic forgetting while preserving transferability on ISS, we introduce two patch-wise contrastive losses to imitate similar features and enhance feature diversity respectively, which can further improve the performance of TISS. Under extensive experimental settings with Pascal-VOC 2012 and ADE20K datasets, our method significantly outperforms state-of-the-art incremental semantic segmentation methods.



### SolderNet: Towards Trustworthy Visual Inspection of Solder Joints in Electronics Manufacturing Using Explainable Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2211.10274v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10274v1)
- **Published**: 2022-11-18 15:02:59+00:00
- **Updated**: 2022-11-18 15:02:59+00:00
- **Authors**: Hayden Gunraj, Paul Guerrier, Sheldon Fernandez, Alexander Wong
- **Comment**: Accepted by IAAI-23, 7 pages
- **Journal**: None
- **Summary**: In electronics manufacturing, solder joint defects are a common problem affecting a variety of printed circuit board components. To identify and correct solder joint defects, the solder joints on a circuit board are typically inspected manually by trained human inspectors, which is a very time-consuming and error-prone process. To improve both inspection efficiency and accuracy, in this work we describe an explainable deep learning-based visual quality inspection system tailored for visual inspection of solder joints in electronics manufacturing environments. At the core of this system is an explainable solder joint defect identification system called SolderNet which we design and implement with trust and transparency in mind. While several challenges remain before the full system can be developed and deployed, this study presents important progress towards trustworthy visual inspection of solder joints in electronics manufacturing.



### Task Residual for Tuning Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2211.10277v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10277v2)
- **Published**: 2022-11-18 15:09:03+00:00
- **Updated**: 2023-03-24 17:22:32+00:00
- **Authors**: Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, Xinchao Wang
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: Large-scale vision-language models (VLMs) pre-trained on billion-level data have learned general visual representations and broad visual concepts. In principle, the well-learned knowledge structure of the VLMs should be inherited appropriately when being transferred to downstream tasks with limited data. However, most existing efficient transfer learning (ETL) approaches for VLMs either damage or are excessively biased towards the prior knowledge, e.g., prompt tuning (PT) discards the pre-trained text-based classifier and builds a new one while adapter-style tuning (AT) fully relies on the pre-trained features. To address this, we propose a new efficient tuning approach for VLMs named Task Residual Tuning (TaskRes), which performs directly on the text-based classifier and explicitly decouples the prior knowledge of the pre-trained models and new knowledge regarding a target task. Specifically, TaskRes keeps the original classifier weights from the VLMs frozen and obtains a new classifier for the target task by tuning a set of prior-independent parameters as a residual to the original one, which enables reliable prior knowledge preservation and flexible task-specific knowledge exploration. The proposed TaskRes is simple yet effective, which significantly outperforms previous ETL methods (e.g., PT and AT) on 11 benchmark datasets while requiring minimal effort for the implementation. Our code is available at https://github.com/geekyutao/TaskRes.



### Unsupervised 3D Pose Transfer with Cross Consistency and Dual Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2211.10278v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10278v3)
- **Published**: 2022-11-18 15:09:56+00:00
- **Updated**: 2023-03-16 11:55:47+00:00
- **Authors**: Chaoyue Song, Jiacheng Wei, Ruibo Li, Fayao Liu, Guosheng Lin
- **Comment**: arXiv admin note: text overlap with arXiv:2109.15025
- **Journal**: None
- **Summary**: The goal of 3D pose transfer is to transfer the pose from the source mesh to the target mesh while preserving the identity information (e.g., face, body shape) of the target mesh. Deep learning-based methods improved the efficiency and performance of 3D pose transfer. However, most of them are trained under the supervision of the ground truth, whose availability is limited in real-world scenarios. In this work, we present X-DualNet, a simple yet effective approach that enables unsupervised 3D pose transfer. In X-DualNet, we introduce a generator $G$ which contains correspondence learning and pose transfer modules to achieve 3D pose transfer. We learn the shape correspondence by solving an optimal transport problem without any key point annotations and generate high-quality meshes with our elastic instance normalization (ElaIN) in the pose transfer module. With $G$ as the basic component, we propose a cross consistency learning scheme and a dual reconstruction objective to learn the pose transfer without supervision. Besides that, we also adopt an as-rigid-as-possible deformer in the training process to fine-tune the body shape of the generated results. Extensive experiments on human and animal data demonstrate that our framework can successfully achieve comparable performance as the state-of-the-art supervised approaches.



### Estimating more camera poses for ego-centric videos is essential for VQ3D
- **Arxiv ID**: http://arxiv.org/abs/2211.10284v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10284v1)
- **Published**: 2022-11-18 15:16:49+00:00
- **Updated**: 2022-11-18 15:16:49+00:00
- **Authors**: Jinjie Mai, Chen Zhao, Abdullah Hamdi, Silvio Giancola, Bernard Ghanem
- **Comment**: Second International Ego4D Workshop at ECCV 2022
- **Journal**: None
- **Summary**: Visual queries 3D localization (VQ3D) is a task in the Ego4D Episodic Memory Benchmark. Given an egocentric video, the goal is to answer queries of the form "Where did I last see object X?", where the query object X is specified as a static image, and the answer should be a 3D displacement vector pointing to object X. However, current techniques use naive ways to estimate the camera poses of video frames, resulting in a low query with pose (QwP) ratio, thus a poor overall success rate. We design a new pipeline for the challenging egocentric video camera pose estimation problem in our work. Moreover, we revisit the current VQ3D framework and optimize it in terms of performance and efficiency. As a result, we get the top-1 overall success rate of 25.8% on VQ3D leaderboard, which is two times better than the 8.7% reported by the baseline.



### Just a Matter of Scale? Reevaluating Scale Equivariance in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2211.10288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10288v1)
- **Published**: 2022-11-18 15:27:05+00:00
- **Updated**: 2022-11-18 15:27:05+00:00
- **Authors**: Thomas Altstidl, An Nguyen, Leo Schwinn, Franz Köferl, Christopher Mutschler, Björn Eskofier, Dario Zanca
- **Comment**: None
- **Journal**: None
- **Summary**: The widespread success of convolutional neural networks may largely be attributed to their intrinsic property of translation equivariance. However, convolutions are not equivariant to variations in scale and fail to generalize to objects of different sizes. Despite recent advances in this field, it remains unclear how well current methods generalize to unobserved scales on real-world data and to what extent scale equivariance plays a role. To address this, we propose the novel Scaled and Translated Image Recognition (STIR) benchmark based on four different domains. Additionally, we introduce a new family of models that applies many re-scaled kernels with shared weights in parallel and then selects the most appropriate one. Our experimental results on STIR show that both the existing and proposed approaches can improve generalization across scales compared to standard convolutions. We also demonstrate that our family of models is able to generalize well towards larger scales and improve scale equivariance. Moreover, due to their unique design we can validate that kernel selection is consistent with input scale. Even so, none of the evaluated models maintain their performance for large differences in scale, demonstrating that a general understanding of how scale equivariance can improve generalization and robustness is still lacking.



### SeaTurtleID: A novel long-span dataset highlighting the importance of timestamps in wildlife re-identification
- **Arxiv ID**: http://arxiv.org/abs/2211.10307v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10307v2)
- **Published**: 2022-11-18 15:46:24+00:00
- **Updated**: 2023-03-20 11:30:49+00:00
- **Authors**: Kostas Papafitsoros, Lukáš Adam, Vojtěch Čermák, Lukáš Picek
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces SeaTurtleID, the first public large-scale, long-span dataset with sea turtle photographs captured in the wild. The dataset is suitable for benchmarking re-identification methods and evaluating several other computer vision tasks. The dataset consists of 7774 high-resolution photographs of 400 unique individuals collected within 12 years in 1081 encounters. Each photograph is accompanied by rich metadata, e.g., identity label, head segmentation mask, and encounter timestamp. The 12-year span of the dataset makes it the longest-spanned public wild animal dataset with timestamps. By exploiting this unique property, we show that timestamps are necessary for an unbiased evaluation of animal re-identification methods because they allow time-aware splits of the dataset into reference and query sets. We show that time-unaware (random) splits can lead to performance overestimation of more than 100% compared to the time-aware splits for both feature- and CNN-based re-identification methods. We also argue that time-aware splits correspond to more realistic re-identification pipelines than the time-unaware ones. We recommend that animal re-identification methods should only be tested on datasets with timestamps using time-aware splits, and we encourage dataset curators to include such information in the associated metadata.



### Masked Autoencoders for Egocentric Video Understanding @ Ego4D Challenge 2022
- **Arxiv ID**: http://arxiv.org/abs/2211.15286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.15286v1)
- **Published**: 2022-11-18 16:05:15+00:00
- **Updated**: 2022-11-18 16:05:15+00:00
- **Authors**: Jiachen Lei, Shuang Ma, Zhongjie Ba, Sai Vemprala, Ashish Kapoor, Kui Ren
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: In this report, we present our approach and empirical results of applying masked autoencoders in two egocentric video understanding tasks, namely, Object State Change Classification and PNR Temporal Localization, of Ego4D Challenge 2022. As team TheSSVL, we ranked 2nd place in both tasks. Our code will be made available.



### Deep learning based landslide density estimation on SAR data for rapid response
- **Arxiv ID**: http://arxiv.org/abs/2211.10338v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T07, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2211.10338v1)
- **Published**: 2022-11-18 16:50:02+00:00
- **Updated**: 2022-11-18 16:50:02+00:00
- **Authors**: Vanessa Boehm, Wei Ji Leong, Ragini Bal Mahesh, Ioannis Prapas, Edoardo Nemni, Freddie Kalaitzis, Siddha Ganju, Raul Ramos-Pollán
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: This work aims to produce landslide density estimates using Synthetic Aperture Radar (SAR) satellite imageries to prioritise emergency resources for rapid response. We use the United States Geological Survey (USGS) Landslide Inventory data annotated by experts after Hurricane Mar\'ia in Puerto Rico on Sept 20, 2017, and their subsequent susceptibility study which uses extensive additional information such as precipitation, soil moisture, geological terrain features, closeness to waterways and roads, etc. Since such data might not be available during other events or regions, we aimed to produce a landslide density map using only elevation and SAR data to be useful to decision-makers in rapid response scenarios.   The USGS Landslide Inventory contains the coordinates of 71,431 landslide heads (not their full extent) and was obtained by manual inspection of aerial and satellite imagery. It is estimated that around 45\% of the landslides are smaller than a Sentinel-1 typical pixel which is 10m $\times$ 10m, although many are long and thin, probably leaving traces across several pixels. Our method obtains 0.814 AUC in predicting the correct density estimation class at the chip level (128$\times$128 pixels, at Sentinel-1 resolution) using only elevation data and up to three SAR acquisitions pre- and post-hurricane, thus enabling rapid assessment after a disaster. The USGS Susceptibility Study reports a 0.87 AUC, but it is measured at the landslide level and uses additional information sources (such as proximity to fluvial channels, roads, precipitation, etc.) which might not regularly be available in an rapid response emergency scenario.



### Invariant Learning via Diffusion Dreamed Distribution Shifts
- **Arxiv ID**: http://arxiv.org/abs/2211.10370v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.10370v1)
- **Published**: 2022-11-18 17:07:43+00:00
- **Updated**: 2022-11-18 17:07:43+00:00
- **Authors**: Priyatham Kattakinda, Alexander Levine, Soheil Feizi
- **Comment**: 18 pages, 13 figures, 5 tables
- **Journal**: None
- **Summary**: Though the background is an important signal for image classification, over reliance on it can lead to incorrect predictions when spurious correlations between foreground and background are broken at test time. Training on a dataset where these correlations are unbiased would lead to more robust models. In this paper, we propose such a dataset called Diffusion Dreamed Distribution Shifts (D3S). D3S consists of synthetic images generated through StableDiffusion using text prompts and image guides obtained by pasting a sample foreground image onto a background template image. Using this scalable approach we generate 120K images of objects from all 1000 ImageNet classes in 10 diverse backgrounds. Due to the incredible photorealism of the diffusion model, our images are much closer to natural images than previous synthetic datasets. D3S contains a validation set of more than 17K images whose labels are human-verified in an MTurk study. Using the validation set, we evaluate several popular DNN image classifiers and find that the classification performance of models generally suffers on our background diverse images. Next, we leverage the foreground & background labels in D3S to learn a foreground (background) representation that is invariant to changes in background (foreground) by penalizing the mutual information between the foreground (background) features and the background (foreground) labels. Linear classifiers trained on these features to predict foreground (background) from foreground (background) have high accuracies at 82.9% (93.8%), while classifiers that predict these labels from background and foreground have a much lower accuracy of 2.4% and 45.6% respectively. This suggests that our foreground and background features are well disentangled. We further test the efficacy of these representations by training classifiers on a task with strong spurious correlations.



### Informative Sample-Aware Proxy for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2211.10382v1
- **DOI**: 10.1145/3551626.3564942
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2211.10382v1)
- **Published**: 2022-11-18 17:25:25+00:00
- **Updated**: 2022-11-18 17:25:25+00:00
- **Authors**: Aoyu Li, Ikuro Sato, Kohta Ishikawa, Rei Kawakami, Rio Yokota
- **Comment**: Accepted at ACM Multimedia Asia (MMAsia) 2022
- **Journal**: None
- **Summary**: Among various supervised deep metric learning methods proxy-based approaches have achieved high retrieval accuracies. Proxies, which are class-representative points in an embedding space, receive updates based on proxy-sample similarities in a similar manner to sample representations. In existing methods, a relatively small number of samples can produce large gradient magnitudes (ie, hard samples), and a relatively large number of samples can produce small gradient magnitudes (ie, easy samples); these can play a major part in updates. Assuming that acquiring too much sensitivity to such extreme sets of samples would deteriorate the generalizability of a method, we propose a novel proxy-based method called Informative Sample-Aware Proxy (Proxy-ISA), which directly modifies a gradient weighting factor for each sample using a scheduled threshold function, so that the model is more sensitive to the informative samples. Extensive experiments on the CUB-200-2011, Cars-196, Stanford Online Products and In-shop Clothes Retrieval datasets demonstrate the superiority of Proxy-ISA compared with the state-of-the-art methods.



### CroCo v2: Improved Cross-view Completion Pre-training for Stereo Matching and Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2211.10408v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10408v3)
- **Published**: 2022-11-18 18:18:53+00:00
- **Updated**: 2023-08-18 15:06:20+00:00
- **Authors**: Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy, Yohann Cabon, Vaibhav Arora, Romain Brégier, Gabriela Csurka, Leonid Antsfeld, Boris Chidlovskii, Jérôme Revaud
- **Comment**: ICCV 2023
- **Journal**: None
- **Summary**: Despite impressive performance for high-level downstream tasks, self-supervised pre-training methods have not yet fully delivered on dense geometric vision tasks such as stereo matching or optical flow. The application of self-supervised concepts, such as instance discrimination or masked image modeling, to geometric tasks is an active area of research. In this work, we build on the recent cross-view completion framework, a variation of masked image modeling that leverages a second view from the same scene which makes it well suited for binocular downstream tasks. The applicability of this concept has so far been limited in at least two ways: (a) by the difficulty of collecting real-world image pairs -- in practice only synthetic data have been used -- and (b) by the lack of generalization of vanilla transformers to dense downstream tasks for which relative position is more meaningful than absolute position. We explore three avenues of improvement. First, we introduce a method to collect suitable real-world image pairs at large scale. Second, we experiment with relative positional embeddings and show that they enable vision transformers to perform substantially better. Third, we scale up vision transformer based cross-completion architectures, which is made possible by the use of large amounts of data. With these improvements, we show for the first time that state-of-the-art results on stereo matching and optical flow can be reached without using any classical task-specific techniques like correlation volume, iterative estimation, image warping or multi-scale reasoning, thus paving the way towards universal vision models.



### CNeRV: Content-adaptive Neural Representation for Visual Data
- **Arxiv ID**: http://arxiv.org/abs/2211.10421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10421v1)
- **Published**: 2022-11-18 18:35:43+00:00
- **Updated**: 2022-11-18 18:35:43+00:00
- **Authors**: Hao Chen, Matt Gwilliam, Bo He, Ser-Nam Lim, Abhinav Shrivastava
- **Comment**: BMVC 2022 at https://haochen-rye.github.io/CNeRV
- **Journal**: None
- **Summary**: Compression and reconstruction of visual data have been widely studied in the computer vision community, even before the popularization of deep learning. More recently, some have used deep learning to improve or refine existing pipelines, while others have proposed end-to-end approaches, including autoencoders and implicit neural representations, such as SIREN and NeRV. In this work, we propose Neural Visual Representation with Content-adaptive Embedding (CNeRV), which combines the generalizability of autoencoders with the simplicity and compactness of implicit representation. We introduce a novel content-adaptive embedding that is unified, concise, and internally (within-video) generalizable, that compliments a powerful decoder with a single-layer encoder. We match the performance of NeRV, a state-of-the-art implicit neural representation, on the reconstruction task for frames seen during training while far surpassing for frames that are skipped during training (unseen images). To achieve similar reconstruction quality on unseen images, NeRV needs 120x more time to overfit per-frame due to its lack of internal generalization. With the same latent code length and similar model size, CNeRV outperforms autoencoders on reconstruction of both seen and unseen images. We also show promising results for visual data compression. More details can be found in the project pagehttps://haochen-rye.github.io/CNeRV/



### Visual Programming: Compositional visual reasoning without training
- **Arxiv ID**: http://arxiv.org/abs/2211.11559v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2211.11559v1)
- **Published**: 2022-11-18 18:50:09+00:00
- **Updated**: 2022-11-18 18:50:09+00:00
- **Authors**: Tanmay Gupta, Aniruddha Kembhavi
- **Comment**: None
- **Journal**: None
- **Summary**: We present VISPROG, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. VISPROG avoids the need for any task-specific training. Instead, it uses the in-context learning ability of large language models to generate python-like modular programs, which are then executed to get both the solution and a comprehensive and interpretable rationale. Each line of the generated program may invoke one of several off-the-shelf computer vision models, image processing routines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program. We demonstrate the flexibility of VISPROG on 4 diverse tasks - compositional visual question answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image editing. We believe neuro-symbolic approaches like VISPROG are an exciting avenue to easily and effectively expand the scope of AI systems to serve the long tail of complex tasks that people may wish to perform.



### A Structure-Guided Diffusion Model for Large-Hole Diverse Image Completion
- **Arxiv ID**: http://arxiv.org/abs/2211.10437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10437v1)
- **Published**: 2022-11-18 18:59:01+00:00
- **Updated**: 2022-11-18 18:59:01+00:00
- **Authors**: Daichi Horita, Jiaolong Yang, Dong Chen, Yuki Koyama, Kiyoharu Aizawa
- **Comment**: None
- **Journal**: None
- **Summary**: Diverse image completion, a problem of generating various ways of filling incomplete regions (i.e. holes) of an image, has made remarkable success. However, managing input images with large holes is still a challenging problem due to the corruption of semantically important structures. In this paper, we tackle this problem by incorporating explicit structural guidance. We propose a structure-guided diffusion model (SGDM) for the large-hole diverse completion problem. Our proposed SGDM consists of a structure generator and a texture generator, which are both diffusion probabilistic models (DMs). The structure generator generates an edge image representing a plausible structure within the holes, which is later used to guide the texture generation process. To jointly train these two generators, we design a strategy that combines optimal Bayesian denoising and a momentum framework. In addition to the quality improvement, auxiliary edge images generated by the structure generator can be manually edited to allow user-guided image editing. Our experiments using datasets of faces (CelebA-HQ) and natural scenes (Places) show that our method achieves a comparable or superior trade-off between visual quality and diversity compared to other state-of-the-art methods.



### BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision
- **Arxiv ID**: http://arxiv.org/abs/2211.10439v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10439v1)
- **Published**: 2022-11-18 18:59:48+00:00
- **Updated**: 2022-11-18 18:59:48+00:00
- **Authors**: Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou Zhu, Zhaoxiang Zhang, Gao Huang, Hongyang Li, Yu Qiao, Lewei Lu, Jie Zhou, Jifeng Dai
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel bird's-eye-view (BEV) detector with perspective supervision, which converges faster and better suits modern image backbones. Existing state-of-the-art BEV detectors are often tied to certain depth pre-trained backbones like VoVNet, hindering the synergy between booming image backbones and BEV detectors. To address this limitation, we prioritize easing the optimization of BEV detectors by introducing perspective space supervision. To this end, we propose a two-stage BEV detector, where proposals from the perspective head are fed into the bird's-eye-view head for final predictions. To evaluate the effectiveness of our model, we conduct extensive ablation studies focusing on the form of supervision and the generality of the proposed detector. The proposed method is verified with a wide spectrum of traditional and modern image backbones and achieves new SoTA results on the large-scale nuScenes dataset. The code shall be released soon.



### Magic3D: High-Resolution Text-to-3D Content Creation
- **Arxiv ID**: http://arxiv.org/abs/2211.10440v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.10440v2)
- **Published**: 2022-11-18 18:59:59+00:00
- **Updated**: 2023-03-25 17:32:25+00:00
- **Authors**: Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin
- **Comment**: Accepted to CVPR 2023 as highlight. Project website:
  https://research.nvidia.com/labs/dir/magic3d
- **Journal**: None
- **Summary**: DreamFusion has recently demonstrated the utility of a pre-trained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF), achieving remarkable text-to-3D synthesis results. However, the method has two inherent limitations: (a) extremely slow optimization of NeRF and (b) low-resolution image space supervision on NeRF, leading to low-quality 3D models with a long processing time. In this paper, we address these limitations by utilizing a two-stage optimization framework. First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure. Using the coarse representation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer interacting with a high-resolution latent diffusion model. Our method, dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is 2x faster than DreamFusion (reportedly taking 1.5 hours on average), while also achieving higher resolution. User studies show 61.7% raters to prefer our approach over DreamFusion. Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications.



### A mixed-reality dataset for category-level 6D pose and size estimation of hand-occluded containers
- **Arxiv ID**: http://arxiv.org/abs/2211.10470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10470v1)
- **Published**: 2022-11-18 19:14:52+00:00
- **Updated**: 2022-11-18 19:14:52+00:00
- **Authors**: Xavier Weber, Alessio Xompero, Andrea Cavallaro
- **Comment**: 5 pages, 4 figures, 1 table. Submitted to IEEE ICASSP 2023. Webpage
  at https://corsmal.eecs.qmul.ac.uk/pose.html
- **Journal**: None
- **Summary**: Estimating the 6D pose and size of household containers is challenging due to large intra-class variations in the object properties, such as shape, size, appearance, and transparency. The task is made more difficult when these objects are held and manipulated by a person due to varying degrees of hand occlusions caused by the type of grasps and by the viewpoint of the camera observing the person holding the object. In this paper, we present a mixed-reality dataset of hand-occluded containers for category-level 6D object pose and size estimation. The dataset consists of 138,240 images of rendered hands and forearms holding 48 synthetic objects, split into 3 grasp categories over 30 real backgrounds. We re-train and test an existing model for 6D object pose estimation on our mixed-reality dataset. We discuss the impact of the use of this dataset in improving the task of 6D pose and size estimation.



### Towards Automatic Prediction of Outcome in Treatment of Cerebral Aneurysms
- **Arxiv ID**: http://arxiv.org/abs/2211.11749v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2211.11749v1)
- **Published**: 2022-11-18 19:23:00+00:00
- **Updated**: 2022-11-18 19:23:00+00:00
- **Authors**: Ashutosh Jadhav, Satyananda Kashyap, Hakan Bulu, Ronak Dholakia, Amon Y. Liu, Tanveer Syeda-Mahmood, William R. Patterson, Hussain Rangwala, Mehdi Moradi
- **Comment**: 10 pages
- **Journal**: AMAI 2022 Annual Symposium
- **Summary**: Intrasaccular flow disruptors treat cerebral aneurysms by diverting the blood flow from the aneurysm sac. Residual flow into the sac after the intervention is a failure that could be due to the use of an undersized device, or to vascular anatomy and clinical condition of the patient. We report a machine learning model based on over 100 clinical and imaging features that predict the outcome of wide-neck bifurcation aneurysm treatment with an intravascular embolization device. We combine clinical features with a diverse set of common and novel imaging measurements within a random forest model. We also develop neural network segmentation algorithms in 2D and 3D to contour the sac in angiographic images and automatically calculate the imaging features. These deliver 90% overlap with manual contouring in 2D and 83% in 3D. Our predictive model classifies complete vs. partial occlusion outcomes with an accuracy of 75.31%, and weighted F1-score of 0.74.



### Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference
- **Arxiv ID**: http://arxiv.org/abs/2211.10526v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10526v3)
- **Published**: 2022-11-18 22:49:04+00:00
- **Updated**: 2023-06-12 16:11:55+00:00
- **Authors**: Haoran You, Yunyang Xiong, Xiaoliang Dai, Bichen Wu, Peizhao Zhang, Haoqi Fan, Peter Vajda, Yingyan, Lin
- **Comment**: CVPR 2023 Camera Ready
- **Journal**: None
- **Summary**: Vision Transformers (ViTs) have shown impressive performance but still require a high computation cost as compared to convolutional neural networks (CNNs), one reason is that ViTs' attention measures global similarities and thus has a quadratic complexity with the number of input tokens. Existing efficient ViTs adopt local attention (e.g., Swin) or linear attention (e.g., Performer), which sacrifice ViTs' capabilities of capturing either global or local context. In this work, we ask an important research question: Can ViTs learn both global and local context while being more efficient during inference? To this end, we propose a framework called Castling-ViT, which trains ViTs using both linear-angular attention and masked softmax-based quadratic attention, but then switches to having only linear angular attention during ViT inference. Our Castling-ViT leverages angular kernels to measure the similarities between queries and keys via spectral angles. And we further simplify it with two techniques: (1) a novel linear-angular attention mechanism: we decompose the angular kernels into linear terms and high-order residuals, and only keep the linear terms; and (2) we adopt two parameterized modules to approximate high-order residuals: a depthwise convolution and an auxiliary masked softmax attention to help learn both global and local information, where the masks for softmax attention are regularized to gradually become zeros and thus incur no overhead during ViT inference. Extensive experiments and ablation studies on three tasks consistently validate the effectiveness of the proposed Castling-ViT, e.g., achieving up to a 1.8% higher accuracy or 40% MACs reduction on ImageNet classification and 1.2 higher mAP on COCO detection under comparable FLOPs, as compared to ViTs with vanilla softmax-based attentions.



### Where is my Wallet? Modeling Object Proposal Sets for Egocentric Visual Query Localization
- **Arxiv ID**: http://arxiv.org/abs/2211.10528v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.10528v2)
- **Published**: 2022-11-18 22:50:50+00:00
- **Updated**: 2023-04-06 09:21:18+00:00
- **Authors**: Mengmeng Xu, Yanghao Li, Cheng-Yang Fu, Bernard Ghanem, Tao Xiang, Juan-Manuel Perez-Rua
- **Comment**: We ranked first and second in the VQ2D and VQ3D tasks in the 2nd
  Ego4D challenge
- **Journal**: None
- **Summary**: This paper deals with the problem of localizing objects in image and video datasets from visual exemplars. In particular, we focus on the challenging problem of egocentric visual query localization. We first identify grave implicit biases in current query-conditioned model design and visual query datasets. Then, we directly tackle such biases at both frame and object set levels. Concretely, our method solves these issues by expanding limited annotations and dynamically dropping object proposals during training. Additionally, we propose a novel transformer-based module that allows for object-proposal set context to be considered while incorporating query information. We name our module Conditioned Contextual Transformer or CocoFormer. Our experiments show the proposed adaptations improve egocentric query detection, leading to a better visual query localization system in both 2D and 3D configurations. Thus, we are able to improve frame-level detection performance from 26.28% to 31.26 in AP, which correspondingly improves the VQ2D and VQ3D localization scores by significant margins. Our improved context-aware query object detector ranked first and second in the VQ2D and VQ3D tasks in the 2nd Ego4D challenge. In addition to this, we showcase the relevance of our proposed model in the Few-Shot Detection (FSD) task, where we also achieve SOTA results. Our code is available at https://github.com/facebookresearch/vq2d_cvpr.



### Semantic Encoder Guided Generative Adversarial Face Ultra-Resolution Network
- **Arxiv ID**: http://arxiv.org/abs/2211.10532v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.10532v2)
- **Published**: 2022-11-18 23:16:57+00:00
- **Updated**: 2023-01-03 03:54:36+00:00
- **Authors**: Xiang Wang, Yimin Yang, Qixiang Pang, Xiao Lu, Yu Liu, Shan Du
- **Comment**: 11 pages,5 figures,3 tables
- **Journal**: None
- **Summary**: Face super-resolution is a domain-specific image super-resolution, which aims to generate High-Resolution (HR) face images from their Low-Resolution (LR) counterparts. In this paper, we propose a novel face super-resolution method, namely Semantic Encoder guided Generative Adversarial Face Ultra-Resolution Network (SEGA-FURN) to ultra-resolve an unaligned tiny LR face image to its HR counterpart with multiple ultra-upscaling factors (e.g., 4x and 8x). The proposed network is composed of a novel semantic encoder that has the ability to capture the embedded semantics to guide adversarial learning and a novel generator that uses a hierarchical architecture named Residual in Internal Dense Block (RIDB). Moreover, we propose a joint discriminator which discriminates both image data and embedded semantics. The joint discriminator learns the joint probability distribution of the image space and latent space. We also use a Relativistic average Least Squares loss (RaLS) as the adversarial loss to alleviate the gradient vanishing problem and enhance the stability of the training procedure. Extensive experiments on large face datasets have proved that the proposed method can achieve superior super-resolution results and significantly outperform other state-of-the-art methods in both qualitative and quantitative comparisons.



### Toward a Flexible Metadata Pipeline for Fish Specimen Images
- **Arxiv ID**: http://arxiv.org/abs/2211.15472v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, q-bio.OT
- **Links**: [PDF](http://arxiv.org/pdf/2211.15472v1)
- **Published**: 2022-11-18 23:25:17+00:00
- **Updated**: 2022-11-18 23:25:17+00:00
- **Authors**: Dom Jebbia, Xiaojun Wang, Yasin Bakis, Henry L. Bart Jr., Jane Greenberg
- **Comment**: 12 pages. 5 figures. Presented at the 16th International Conference
  on Metadata and Semantics Research. To be published in the conference
  proceedings of Metadata and Semantic Research: 16th International Conference,
  MTSR 2022, London, United Kingdom, November 8-10, 2022
- **Journal**: None
- **Summary**: Flexible metadata pipelines are crucial for supporting the FAIR data principles. Despite this need, researchers seldom report their approaches for identifying metadata standards and protocols that support optimal flexibility. This paper reports on an initiative targeting the development of a flexible metadata pipeline for a collection containing over 300,000 digital fish specimen images, harvested from multiple data repositories and fish collections. The images and their associated metadata are being used for AI-related scientific research involving automated species identification, segmentation and trait extraction. The paper provides contextual background, followed by the presentation of a four-phased approach involving: 1. Assessment of the Problem, 2. Investigation of Solutions, 3. Implementation, and 4. Refinement. The work is part of the NSF Harnessing the Data Revolution, Biology Guided Neural Networks (NSF/HDR-BGNN) project and the HDR Imageomics Institute. An RDF graph prototype pipeline is presented, followed by a discussion of research implications and conclusion summarizing the results.



