# Arxiv Papers in cs.CV on 2022-11-05
### Multi-Objective Evolutionary for Object Detection Mobile Architectures Search
- **Arxiv ID**: http://arxiv.org/abs/2211.02791v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.02791v1)
- **Published**: 2022-11-05 00:28:49+00:00
- **Updated**: 2022-11-05 00:28:49+00:00
- **Authors**: Haichao Zhang, Jiashi Li, Xin Xia, Kuangrong Hao, Xuefeng Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Neural architecture search has achieved great success on classification tasks for mobile devices. The backbone network for object detection is usually obtained on the image classification task. However, the architecture which is searched through the classification task is sub-optimal because of the gap between the task of image and object detection. As while work focuses on backbone network architecture search for mobile device object detection is limited, mainly because the backbone always requires expensive ImageNet pre-training. Accordingly, it is necessary to study the approach of network architecture search for mobile device object detection without expensive pre-training. In this work, we propose a mobile object detection backbone network architecture search algorithm which is a kind of evolutionary optimized method based on non-dominated sorting for NAS scenarios. It can quickly search to obtain the backbone network architecture within certain constraints. It better solves the problem of suboptimal linear combination accuracy and computational cost. The proposed approach can search the backbone networks with different depths, widths, or expansion sizes via a technique of weight mapping, making it possible to use NAS for mobile devices detection tasks a lot more efficiently. In our experiments, we verify the effectiveness of the proposed approach on YoloX-Lite, a lightweight version of the target detection framework. Under similar computational complexity, the accuracy of the backbone network architecture we search for is 2.0% mAP higher than MobileDet. Our improved backbone network can reduce the computational effort while improving the accuracy of the object detection network. To prove its effectiveness, a series of ablation studies have been carried out and the working mechanism has been analyzed in detail.



### Local Manifold Augmentation for Multiview Semantic Consistency
- **Arxiv ID**: http://arxiv.org/abs/2211.02798v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02798v1)
- **Published**: 2022-11-05 02:00:13+00:00
- **Updated**: 2022-11-05 02:00:13+00:00
- **Authors**: Yu Yang, Wing Yin Cheung, Chang Liu, Xiangyang Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Multiview self-supervised representation learning roots in exploring semantic consistency across data of complex intra-class variation. Such variation is not directly accessible and therefore simulated by data augmentations. However, commonly adopted augmentations are handcrafted and limited to simple geometrical and color changes, which are unable to cover the abundant intra-class variation. In this paper, we propose to extract the underlying data variation from datasets and construct a novel augmentation operator, named local manifold augmentation (LMA). LMA is achieved by training an instance-conditioned generator to fit the distribution on the local manifold of data and sampling multiview data using it. LMA shows the ability to create an infinite number of data views, preserve semantics, and simulate complicated variations in object pose, viewpoint, lighting condition, background etc. Experiments show that with LMA integrated, self-supervised learning methods such as MoCov2 and SimSiam gain consistent improvement on prevalent benchmarks including CIFAR10, CIFAR100, STL10, ImageNet100, and ImageNet. Furthermore, LMA leads to representations that obtain more significant invariance to the viewpoint, object pose, and illumination changes and stronger robustness to various real distribution shifts reflected by ImageNet-V2, ImageNet-R, ImageNet Sketch etc.



### Evaluating Novel Mask-RCNN Architectures for Ear Mask Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2211.02799v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.02799v1)
- **Published**: 2022-11-05 02:05:52+00:00
- **Updated**: 2022-11-05 02:05:52+00:00
- **Authors**: Saurav K. Aryal, Teanna Barrett, Gloria Washington
- **Comment**: Accepted into ICCBS 2022
- **Journal**: None
- **Summary**: The human ear is generally universal, collectible, distinct, and permanent. Ear-based biometric recognition is a niche and recent approach that is being explored. For any ear-based biometric algorithm to perform well, ear detection and segmentation need to be accurately performed. While significant work has been done in existing literature for bounding boxes, a lack of approaches output a segmentation mask for ears. This paper trains and compares three newer models to the state-of-the-art MaskRCNN (ResNet 101 +FPN) model across four different datasets. The Average Precision (AP) scores reported show that the newer models outperform the state-of-the-art but no one model performs the best over multiple datasets.



### KSS-ICP: Point Cloud Registration based on Kendall Shape Space
- **Arxiv ID**: http://arxiv.org/abs/2211.02807v1
- **DOI**: 10.1109/TIP.2023.3251021
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02807v1)
- **Published**: 2022-11-05 04:00:53+00:00
- **Updated**: 2022-11-05 04:00:53+00:00
- **Authors**: Chenlei Lv, Weisi Lin, Baoquan Zhao
- **Comment**: 13 pages, 20 figures
- **Journal**: None
- **Summary**: Point cloud registration is a popular topic which has been widely used in 3D model reconstruction, location, and retrieval. In this paper, we propose a new registration method, KSS-ICP, to address the rigid registration task in Kendall shape space (KSS) with Iterative Closest Point (ICP). The KSS is a quotient space that removes influences of translations, scales, and rotations for shape feature-based analysis. Such influences can be concluded as the similarity transformations that do not change the shape feature. The point cloud representation in KSS is invariant to similarity transformations. We utilize such property to design the KSS-ICP for point cloud registration. To tackle the difficulty to achieve the KSS representation in general, the proposed KSS-ICP formulates a practical solution that does not require complex feature analysis, data training, and optimization. With a simple implementation, KSS-ICP achieves more accurate registration from point clouds. It is robust to similarity transformation, non-uniform density, noise, and defective parts. Experiments show that KSS-ICP has better performance than the state of the art.



### A Robust and Low Complexity Deep Learning Model for Remote Sensing Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2211.02820v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.02820v2)
- **Published**: 2022-11-05 06:14:30+00:00
- **Updated**: 2022-12-12 14:58:45+00:00
- **Authors**: Cam Le, Lam Pham, Nghia NVN, Truong Nguyen, Le Hong Trang
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: In this paper, we present a robust and low complexity deep learning model for Remote Sensing Image Classification (RSIC), the task of identifying the scene of a remote sensing image. In particular, we firstly evaluate different low complexity and benchmark deep neural networks: MobileNetV1, MobileNetV2, NASNetMobile, and EfficientNetB0, which present the number of trainable parameters lower than 5 Million (M). After indicating best network architecture, we further improve the network performance by applying attention schemes to multiple feature maps extracted from middle layers of the network. To deal with the issue of increasing the model footprint as using attention schemes, we apply the quantization technique to satisfy the maximum of 20 MB memory occupation. By conducting extensive experiments on the benchmark datasets NWPU-RESISC45, we achieve a robust and low-complexity model, which is very competitive to the state-of-the-art systems and potential for real-life applications on edge devices.



### A Survey of Deep Face Restoration: Denoise, Super-Resolution, Deblur, Artifact Removal
- **Arxiv ID**: http://arxiv.org/abs/2211.02831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02831v1)
- **Published**: 2022-11-05 07:08:15+00:00
- **Updated**: 2022-11-05 07:08:15+00:00
- **Authors**: Tao Wang, Kaihao Zhang, Xuanxi Chen, Wenhan Luo, Jiankang Deng, Tong Lu, Xiaochun Cao, Wei Liu, Hongdong Li, Stefanos Zafeiriou
- **Comment**: 21 pages, 19 figures
- **Journal**: None
- **Summary**: Face Restoration (FR) aims to restore High-Quality (HQ) faces from Low-Quality (LQ) input images, which is a domain-specific image restoration problem in the low-level computer vision area. The early face restoration methods mainly use statistic priors and degradation models, which are difficult to meet the requirements of real-world applications in practice. In recent years, face restoration has witnessed great progress after stepping into the deep learning era. However, there are few works to study deep learning-based face restoration methods systematically. Thus, this paper comprehensively surveys recent advances in deep learning techniques for face restoration. Specifically, we first summarize different problem formulations and analyze the characteristic of the face image. Second, we discuss the challenges of face restoration. Concerning these challenges, we present a comprehensive review of existing FR methods, including prior based methods and deep learning-based methods. Then, we explore developed techniques in the task of FR covering network architectures, loss functions, and benchmark datasets. We also conduct a systematic benchmark evaluation on representative methods. Finally, we discuss future directions, including network designs, metrics, benchmark datasets, applications,etc. We also provide an open-source repository for all the discussed methods, which is available at https://github.com/TaoWangzj/Awesome-Face-Restoration.



### Lightweight 3D Convolutional Neural Network for Schizophrenia diagnosis using MRI Images and Ensemble Bagging Classifier
- **Arxiv ID**: http://arxiv.org/abs/2211.02868v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2211.02868v1)
- **Published**: 2022-11-05 10:27:37+00:00
- **Updated**: 2022-11-05 10:27:37+00:00
- **Authors**: P Supriya Patro, Tripti Goel, S A VaraPrasad, M Tanveer, R Murugan
- **Comment**: None
- **Journal**: None
- **Summary**: Structural alterations have been thoroughly investigated in the brain during the early onset of schizophrenia (SCZ) with the development of neuroimaging methods. The objective of the paper is an efficient classification of SCZ in 2 different classes: Cognitive Normal (CN), and SCZ using magnetic resonance imaging (MRI) images. This paper proposed a lightweight 3D convolutional neural network (CNN) based framework for SCZ diagnosis using MRI images. In the proposed model, lightweight 3D CNN is used to extract both spatial and spectral features simultaneously from 3D volume MRI scans, and classification is done using an ensemble bagging classifier. Ensemble bagging classifier contributes to preventing overfitting, reduces variance, and improves the model's accuracy. The proposed algorithm is tested on datasets taken from three benchmark databases available as open-source: MCICShare, COBRE, and fBRINPhase-II. These datasets have undergone preprocessing steps to register all the MRI images to the standard template and reduce the artifacts. The model achieves the highest accuracy 92.22%, sensitivity 94.44%, specificity 90%, precision 90.43%, recall 94.44%, F1-score 92.39% and G-mean 92.19% as compared to the current state-of-the-art techniques. The performance metrics evidenced the use of this model to assist the clinicians for automatic accurate diagnosis of SCZ.



### Deep Learning for Rapid Landslide Detection using Synthetic Aperture Radar (SAR) Datacubes
- **Arxiv ID**: http://arxiv.org/abs/2211.02869v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.02869v1)
- **Published**: 2022-11-05 10:31:57+00:00
- **Updated**: 2022-11-05 10:31:57+00:00
- **Authors**: Vanessa Boehm, Wei Ji Leong, Ragini Bal Mahesh, Ioannis Prapas, Edoardo Nemni, Freddie Kalaitzis, Siddha Ganju, Raul Ramos-Pollan
- **Comment**: Accepted in the NeurIPS 2022 workshop on Tackling Climate Change with
  Machine Learning. Authors Vanessa Boehm, Wei Ji Leong, Ragini Bal Mahesh,
  Ioannis Prapas contributed equally as researchers for the Frontier
  Development Lab (FDL) 2022
- **Journal**: None
- **Summary**: With climate change predicted to increase the likelihood of landslide events, there is a growing need for rapid landslide detection technologies that help inform emergency responses. Synthetic Aperture Radar (SAR) is a remote sensing technique that can provide measurements of affected areas independent of weather or lighting conditions. Usage of SAR, however, is hindered by domain knowledge that is necessary for the pre-processing steps and its interpretation requires expert knowledge. We provide simplified, pre-processed, machine-learning ready SAR datacubes for four globally located landslide events obtained from several Sentinel-1 satellite passes before and after a landslide triggering event together with segmentation maps of the landslides. From this dataset, using the Hokkaido, Japan datacube, we study the feasibility of SAR-based landslide detection with supervised deep learning (DL). Our results demonstrate that DL models can be used to detect landslides from SAR data, achieving an Area under the Precision-Recall curve exceeding 0.7. We find that additional satellite visits enhance detection performance, but that early detection is possible when SAR data is combined with terrain information from a digital elevation model. This can be especially useful for time-critical emergency interventions. Code is made publicly available at https://github.com/iprapas/landslide-sar-unet.



### SizeGAN: Improving Size Representation in Clothing Catalogs
- **Arxiv ID**: http://arxiv.org/abs/2211.02892v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02892v2)
- **Published**: 2022-11-05 12:20:01+00:00
- **Updated**: 2023-06-26 18:41:15+00:00
- **Authors**: Kathleen M. Lewis, John Guttag
- **Comment**: None
- **Journal**: None
- **Summary**: Online clothing catalogs lack diversity in body shape and garment size. Brands commonly display their garments on models of one or two sizes, rarely including plus-size models. To our knowledge, our paper presents the first method for generating images of garments and models in a new target size to tackle the size under-representation problem. Our primary technical contribution is a conditional generative adversarial network that learns deformation fields at multiple resolutions to realistically change the size of models and garments. Results from our two user studies show SizeGAN outperforms alternative methods along three dimensions -- realism, garment faithfulness, and size -- which are all important for real world use.



### Simple Primitives with Feasibility- and Contextuality-Dependence for Open-World Compositional Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2211.02895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02895v1)
- **Published**: 2022-11-05 12:57:06+00:00
- **Updated**: 2022-11-05 12:57:06+00:00
- **Authors**: Zhe Liu, Yun Li, Lina Yao, Xiaojun Chang, Wei Fang, Xiaojun Wu, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The task of Compositional Zero-Shot Learning (CZSL) is to recognize images of novel state-object compositions that are absent during the training stage. Previous methods of learning compositional embedding have shown effectiveness in closed-world CZSL. However, in Open-World CZSL (OW-CZSL), their performance tends to degrade significantly due to the large cardinality of possible compositions. Some recent works separately predict simple primitives (i.e., states and objects) to reduce cardinality. However, they consider simple primitives as independent probability distributions, ignoring the heavy dependence between states, objects, and compositions. In this paper, we model the dependence of compositions via feasibility and contextuality. Feasibility-dependence refers to the unequal feasibility relations between simple primitives, e.g., \textit{hairy} is more feasible with \textit{cat} than with \textit{building} in the real world. Contextuality-dependence represents the contextual variance in images, e.g., \textit{cat} shows diverse appearances under the state of \textit{dry} and \textit{wet}. We design Semantic Attention (SA) and generative Knowledge Disentanglement (KD) to learn the dependence of feasibility and contextuality, respectively. SA captures semantics in compositions to alleviate impossible predictions, driven by the visual similarity between simple primitives. KD disentangles images into unbiased feature representations, easing contextual bias in predictions. Moreover, we complement the current compositional probability model with feasibility and contextuality in a compatible format. Finally, we conduct comprehensive experiments to analyze and validate the superior or competitive performance of our model, Semantic Attention and knowledge Disentanglement guided Simple Primitives (SAD-SP), on three widely-used benchmark OW-CZSL datasets.



### Robust Reflection Removal with Flash-only Cues in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2211.02914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02914v1)
- **Published**: 2022-11-05 14:09:10+00:00
- **Updated**: 2022-11-05 14:09:10+00:00
- **Authors**: Chenyang Lei, Xudong Jiang, Qifeng Chen
- **Comment**: Extension of CVPR 2021 paper [arXiv:2103.04273], submitted to TPAMI.
  Our source code and dataset are publicly available at
  http://github.com/ChenyangLEI/flash-reflection-removal
- **Journal**: None
- **Summary**: We propose a simple yet effective reflection-free cue for robust reflection removal from a pair of flash and ambient (no-flash) images. The reflection-free cue exploits a flash-only image obtained by subtracting the ambient image from the corresponding flash image in raw data space. The flash-only image is equivalent to an image taken in a dark environment with only a flash on. This flash-only image is visually reflection-free and thus can provide robust cues to infer the reflection in the ambient image. Since the flash-only image usually has artifacts, we further propose a dedicated model that not only utilizes the reflection-free cue but also avoids introducing artifacts, which helps accurately estimate reflection and transmission. Our experiments on real-world images with various types of reflection demonstrate the effectiveness of our model with reflection-free flash-only cues: our model outperforms state-of-the-art reflection removal approaches by more than 5.23dB in PSNR. We extend our approach to handheld photography to address the misalignment between the flash and no-flash pair. With misaligned training data and the alignment module, our aligned model outperforms our previous version by more than 3.19dB in PSNR on a misaligned dataset. We also study using linear RGB images as training data. Our source code and dataset are publicly available at https://github.com/ChenyangLEI/flash-reflection-removal.



### ESKNet-An enhanced adaptive selection kernel convolution for breast tumors segmentation
- **Arxiv ID**: http://arxiv.org/abs/2211.02915v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.02915v1)
- **Published**: 2022-11-05 14:15:29+00:00
- **Updated**: 2022-11-05 14:15:29+00:00
- **Authors**: Gongping Chen, Jianxun Zhang, Yuming Liu, Jingjing Yin, Xiaotao Yin, Liang Cui, Yu Dai
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: Breast cancer is one of the common cancers that endanger the health of women globally. Accurate target lesion segmentation is essential for early clinical intervention and postoperative follow-up. Recently, many convolutional neural networks (CNNs) have been proposed to segment breast tumors from ultrasound images. However, the complex ultrasound pattern and the variable tumor shape and size bring challenges to the accurate segmentation of the breast lesion. Motivated by the selective kernel convolution, we introduce an enhanced selective kernel convolution for breast tumor segmentation, which integrates multiple feature map region representations and adaptively recalibrates the weights of these feature map regions from the channel and spatial dimensions. This region recalibration strategy enables the network to focus more on high-contributing region features and mitigate the perturbation of less useful regions. Finally, the enhanced selective kernel convolution is integrated into U-net with deep supervision constraints to adaptively capture the robust representation of breast tumors. Extensive experiments with twelve state-of-the-art deep learning segmentation methods on three public breast ultrasound datasets demonstrate that our method has a more competitive segmentation performance in breast ultrasound images.



### Prototypical quadruplet for few-shot class incremental learning
- **Arxiv ID**: http://arxiv.org/abs/2211.02947v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.02947v3)
- **Published**: 2022-11-05 17:19:14+00:00
- **Updated**: 2023-04-08 09:16:05+00:00
- **Authors**: Sanchar Palit, Biplab Banerjee, Subhasis Chaudhuri
- **Comment**: Submitted to IJCNN 2023
- **Journal**: None
- **Summary**: Scarcity of data and incremental learning of new tasks pose two major bottlenecks for many modern computer vision algorithms. The phenomenon of catastrophic forgetting, i.e., the model's inability to classify previously learned data after training with new batches of data, is a major challenge. Conventional methods address catastrophic forgetting while compromising the current session's training. Generative replay-based approaches, such as generative adversarial networks (GANs), have been proposed to mitigate catastrophic forgetting, but training GANs with few samples may lead to instability. To address these challenges, we propose a novel method that improves classification robustness by identifying a better embedding space using an improved contrasting loss. Our approach retains previously acquired knowledge in the embedding space, even when trained with new classes, by updating previous session class prototypes to represent the true class mean, which is crucial for our nearest class mean classification strategy. We demonstrate the effectiveness of our method by showing that the embedding space remains intact after training the model with new classes and outperforms existing state-of-the-art algorithms in terms of accuracy across different sessions.



### A Comparative Analysis of the Face Recognition Methods in Video Surveillance Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2211.02952v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.02952v1)
- **Published**: 2022-11-05 17:59:18+00:00
- **Updated**: 2022-11-05 17:59:18+00:00
- **Authors**: Eker Onur, Bal Murat
- **Comment**: None
- **Journal**: None
- **Summary**: Facial recognition is fundamental for a wide variety of security systems operating in real-time applications. In video surveillance based face recognition, face images are typically captured over multiple frames in uncontrolled conditions; where head pose, illumination, shadowing, motion blur and focus change over the sequence. We can generalize that the three fundamental operations involved in the facial recognition tasks: face detection, face alignment and face recognition. This study presents comparative benchmark tables for the state-of-art face recognition methods by testing them with same backbone architecture in order to focus only on the face recognition solution instead of network architecture. For this purpose, we constructed a video surveillance dataset of face IDs that has high age variance, intra-class variance (face make-up, beard, etc.) with native surveillance facial imagery data for evaluation. On the other hand, this work discovers the best recognition methods for different conditions like non-masked faces, masked faces, and faces with glasses.



### Can Ensemble of Classifiers Provide Better Recognition Results in Packaging Activity?
- **Arxiv ID**: http://arxiv.org/abs/2211.02965v1
- **DOI**: 10.1007/978-981-19-0361-8_10
- **Categories**: **cs.CV**, cs.LG, eess.SP, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2211.02965v1)
- **Published**: 2022-11-05 19:37:15+00:00
- **Updated**: 2022-11-05 19:37:15+00:00
- **Authors**: A. H. M. Nazmus Sakib, Promit Basak, Syed Doha Uddin, Shahamat Mustavi Tasin, Md Atiqur Rahman Ahad
- **Comment**: None
- **Journal**: None
- **Summary**: Skeleton-based Motion Capture (MoCap) systems have been widely used in the game and film industry for mimicking complex human actions for a long time. MoCap data has also proved its effectiveness in human activity recognition tasks. However, it is a quite challenging task for smaller datasets. The lack of such data for industrial activities further adds to the difficulties. In this work, we have proposed an ensemble-based machine learning methodology that is targeted to work better on MoCap datasets. The experiments have been performed on the MoCap data given in the Bento Packaging Activity Recognition Challenge 2021. Bento is a Japanese word that resembles lunch-box. Upon processing the raw MoCap data at first, we have achieved an astonishing accuracy of 98% on 10-fold Cross-Validation and 82% on Leave-One-Out-Cross-Validation by using the proposed ensemble model.



### Improved Kidney Stone Recognition Through Attention and Multi-View Feature Fusion Strategies
- **Arxiv ID**: http://arxiv.org/abs/2211.02967v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.02967v1)
- **Published**: 2022-11-05 19:49:03+00:00
- **Updated**: 2022-11-05 19:49:03+00:00
- **Authors**: Elias Villalvazo-Avila, Francisco Lopez-Tiro, Jonathan El-Beze, Jacques Hubert, Miguel Gonzalez-Mendoza, Gilberto Ochoa-Ruiz, Christian Daul
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: This contribution presents a deep learning method for the extraction and fusion of information relating to kidney stone fragments acquired from different viewpoints of the endoscope. Surface and section fragment images are jointly used during the training of the classifier to improve the discrimination power of the features by adding attention layers at the end of each convolutional block. This approach is specifically designed to mimic the morpho-constitutional analysis performed in ex-vivo by biologists to visually identify kidney stones by inspecting both views. The addition of attention mechanisms to the backbone improved the results of single view extraction backbones by 4% on average. Moreover, in comparison to the state-of-the-art, the fusion of the deep features improved the overall results up to 11% in terms of kidney stone classification accuracy.



### Inside Out: Transforming Images of Lab-Grown Plants for Machine Learning Applications in Agriculture
- **Arxiv ID**: http://arxiv.org/abs/2211.02972v1
- **DOI**: 10.3389/frai.2023.1200977
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.02972v1)
- **Published**: 2022-11-05 20:51:45+00:00
- **Updated**: 2022-11-05 20:51:45+00:00
- **Authors**: A. E. Krosney, P. Sotoodeh, C. J. Henry, M. A. Beck, C. P. Bidinosti
- **Comment**: 35 pages, 23 figures
- **Journal**: None
- **Summary**: Machine learning tasks often require a significant amount of training data for the resultant network to perform suitably for a given problem in any domain. In agriculture, dataset sizes are further limited by phenotypical differences between two plants of the same genotype, often as a result of differing growing conditions. Synthetically-augmented datasets have shown promise in improving existing models when real data is not available. In this paper, we employ a contrastive unpaired translation (CUT) generative adversarial network (GAN) and simple image processing techniques to translate indoor plant images to appear as field images. While we train our network to translate an image containing only a single plant, we show that our method is easily extendable to produce multiple-plant field images. Furthermore, we use our synthetic multi-plant images to train several YoloV5 nano object detection models to perform the task of plant detection and measure the accuracy of the model on real field data images. Including training data generated by the CUT-GAN leads to better plant detection performance compared to a network trained solely on real data.



### Mixture-Net: Low-Rank Deep Image Prior Inspired by Mixture Models for Spectral Image Recovery
- **Arxiv ID**: http://arxiv.org/abs/2211.02973v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.02973v1)
- **Published**: 2022-11-05 21:32:25+00:00
- **Updated**: 2022-11-05 21:32:25+00:00
- **Authors**: Tatiana Gelvez-Barrera, Jorge Bacca, Henry Arguello
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a non-data-driven deep neural network for spectral image recovery problems such as denoising, single hyperspectral image super-resolution, and compressive spectral imaging reconstruction. Unlike previous methods, the proposed approach, dubbed Mixture-Net, implicitly learns the prior information through the network. Mixture-Net consists of a deep generative model whose layers are inspired by the linear and non-linear low-rank mixture models, where the recovered image is composed of a weighted sum between the linear and non-linear decomposition. Mixture-Net also provides a low-rank decomposition interpreted as the spectral image abundances and endmembers, helpful in achieving remote sensing tasks without running additional routines. The experiments show the MixtureNet effectiveness outperforming state-of-the-art methods in recovery quality with the advantage of architecture interpretability.



### Disentangling Content and Motion for Text-Based Neural Video Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2211.02980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02980v1)
- **Published**: 2022-11-05 21:49:41+00:00
- **Updated**: 2022-11-05 21:49:41+00:00
- **Authors**: Levent Karacan, Tolga Kerimoğlu, İsmail İnan, Tolga Birdal, Erkut Erdem, Aykut Erdem
- **Comment**: None
- **Journal**: None
- **Summary**: Giving machines the ability to imagine possible new objects or scenes from linguistic descriptions and produce their realistic renderings is arguably one of the most challenging problems in computer vision. Recent advances in deep generative models have led to new approaches that give promising results towards this goal. In this paper, we introduce a new method called DiCoMoGAN for manipulating videos with natural language, aiming to perform local and semantic edits on a video clip to alter the appearances of an object of interest. Our GAN architecture allows for better utilization of multiple observations by disentangling content and motion to enable controllable semantic edits. To this end, we introduce two tightly coupled networks: (i) a representation network for constructing a concise understanding of motion dynamics and temporally invariant content, and (ii) a translation network that exploits the extracted latent content representation to actuate the manipulation according to the target description. Our qualitative and quantitative evaluations demonstrate that DiCoMoGAN significantly outperforms existing frame-based methods, producing temporally coherent and semantically more meaningful results.



### Event and Entity Extraction from Generated Video Captions
- **Arxiv ID**: http://arxiv.org/abs/2211.02982v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2211.02982v2)
- **Published**: 2022-11-05 22:06:50+00:00
- **Updated**: 2023-08-15 21:19:13+00:00
- **Authors**: Johannes Scherer, Ansgar Scherp, Deepayan Bhowmik
- **Comment**: Paper accepted at CD-MAKE 2023
- **Journal**: None
- **Summary**: Annotation of multimedia data by humans is time-consuming and costly, while reliable automatic generation of semantic metadata is a major challenge. We propose a framework to extract semantic metadata from automatically generated video captions. As metadata, we consider entities, the entities' properties, relations between entities, and the video category. We employ two state-of-the-art dense video captioning models with masked transformer (MT) and parallel decoding (PVDC) to generate captions for videos of the ActivityNet Captions dataset. Our experiments show that it is possible to extract entities, their properties, relations between entities, and the video category from the generated captions. We observe that the quality of the extracted information is mainly influenced by the quality of the event localization in the video as well as the performance of the event caption generation.



