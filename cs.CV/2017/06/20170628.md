# Arxiv Papers in cs.CV on 2017-06-28
### Super-Resolution via Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1706.09077v1
- **DOI**: 10.1016/j.dsp.2018.07.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09077v1)
- **Published**: 2017-06-28 00:02:18+00:00
- **Updated**: 2017-06-28 00:02:18+00:00
- **Authors**: Khizar Hayat
- **Comment**: None
- **Journal**: None
- **Summary**: The recent phenomenal interest in convolutional neural networks (CNNs) must have made it inevitable for the super-resolution (SR) community to explore its potential. The response has been immense and in the last three years, since the advent of the pioneering work, there appeared too many works not to warrant a comprehensive survey. This paper surveys the SR literature in the context of deep learning. We focus on the three important aspects of multimedia - namely image, video and multi-dimensions, especially depth maps. In each case, first relevant benchmarks are introduced in the form of datasets and state of the art SR methods, excluding deep learning. Next is a detailed analysis of the individual works, each including a short description of the method and a critique of the results with special reference to the benchmarking done. This is followed by minimum overall benchmarking in the form of comparison on some common dataset, while relying on the results reported in various works.



### Classification of Medical Images and Illustrations in the Biomedical Literature Using Synergic Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1706.09092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09092v1)
- **Published**: 2017-06-28 01:15:06+00:00
- **Updated**: 2017-06-28 01:15:06+00:00
- **Authors**: Jianpeng Zhang, Yong Xia, Qi Wu, Yutong Xie
- **Comment**: None
- **Journal**: None
- **Summary**: The Classification of medical images and illustrations in the literature aims to label a medical image according to the modality it was produced or label an illustration according to its production attributes. It is an essential and challenging research hotspot in the area of automated literature review, retrieval and mining. The significant intra-class variation and inter-class similarity caused by the diverse imaging modalities and various illustration types brings a great deal of difficulties to the problem. In this paper, we propose a synergic deep learning (SDL) model to address this issue. Specifically, a dual deep convolutional neural network with a synergic signal system is designed to mutually learn image representation. The synergic signal is used to verify whether the input image pair belongs to the same category and to give the corrective feedback if a synergic error exists. Our SDL model can be trained 'end to end'. In the test phase, the class label of an input can be predicted by averaging the likelihood probabilities obtained by two convolutional neural network components. Experimental results on the ImageCLEF2016 Subfigure Classification Challenge suggest that our proposed SDL model achieves the state-of-the art performance in this medical image classification problem and its accuracy is higher than that of the first place solution on the Challenge leader board so far.



### Robust Lane Tracking with Multi-mode Observation Model and Particle Filtering
- **Arxiv ID**: http://arxiv.org/abs/1706.09119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09119v1)
- **Published**: 2017-06-28 04:08:02+00:00
- **Updated**: 2017-06-28 04:08:02+00:00
- **Authors**: Jiawei Huang, Zhaowen Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic lane tracking involves estimating the underlying signal from a sequence of noisy signal observations. Many models and methods have been proposed for lane tracking, and dynamic targets tracking in general. The Kalman Filter is a widely used method that works well on linear Gaussian models. But this paper shows that Kalman Filter is not suitable for lane tracking, because its Gaussian observation model cannot faithfully represent the procured observations. We propose using a Particle Filter on top of a novel multiple mode observation model. Experiments show that our method produces superior performance to a conventional Kalman Filter.



### Perceptual Adversarial Networks for Image-to-Image Transformation
- **Arxiv ID**: http://arxiv.org/abs/1706.09138v2
- **DOI**: 10.1109/TIP.2018.2836316
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09138v2)
- **Published**: 2017-06-28 07:04:08+00:00
- **Updated**: 2019-04-03 02:44:53+00:00
- **Authors**: Chaoyue Wang, Chang Xu, Chaohui Wang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a principled Perceptual Adversarial Networks (PAN) for image-to-image transformation tasks. Unlike existing application-specific algorithms, PAN provides a generic framework of learning mapping relationship between paired images (Fig. 1), such as mapping a rainy image to its de-rained counterpart, object edges to its photo, semantic labels to a scenes image, etc. The proposed PAN consists of two feed-forward convolutional neural networks (CNNs), the image transformation network T and the discriminative network D. Through combining the generative adversarial loss and the proposed perceptual adversarial loss, these two networks can be trained alternately to solve image-to-image transformation tasks. Among them, the hidden layers and output of the discriminative network D are upgraded to continually and automatically discover the discrepancy between the transformed image and the corresponding ground-truth. Simultaneously, the image transformation network T is trained to minimize the discrepancy explored by the discriminative network D. Through the adversarial training process, the image transformation network T will continually narrow the gap between transformed images and ground-truth images. Experiments evaluated on several image-to-image transformation tasks (e.g., image de-raining, image inpainting, etc.) show that the proposed PAN outperforms many related state-of-the-art methods.



### Yes-Net: An effective Detector Based on Global Information
- **Arxiv ID**: http://arxiv.org/abs/1706.09180v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09180v2)
- **Published**: 2017-06-28 09:16:18+00:00
- **Updated**: 2017-06-30 07:14:40+00:00
- **Authors**: Liangzhuang Ma, Xin Kan, Qianjiang Xiao, Wenlong Liu, Peiqin Sun
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a new real-time object detection approach named Yes-Net. It realizes the prediction of bounding boxes and class via single neural network like YOLOv2 and SSD, but owns more efficient and outstanding features. It combines local information with global information by adding the RNN architecture as a packed unit in CNN model to form the basic feature extractor. Independent anchor boxes coming from full-dimension k-means is also applied in Yes-Net, it brings better average IOU than grid anchor box. In addition, instead of NMS, Yes-Net uses RNN as a filter to get the final boxes, which is more efficient. For 416 x 416 input, Yes-Net achieves 79.2% mAP on VOC2007 test at 39 FPS on an Nvidia Titan X Pascal.



### A Parameterized Approach to Personalized Variable Length Summarization of Soccer Matches
- **Arxiv ID**: http://arxiv.org/abs/1706.09193v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09193v1)
- **Published**: 2017-06-28 09:57:05+00:00
- **Updated**: 2017-06-28 09:57:05+00:00
- **Authors**: Mohak Sukhwani, Ravi Kothari
- **Comment**: None
- **Journal**: None
- **Summary**: We present a parameterized approach to produce personalized variable length summaries of soccer matches. Our approach is based on temporally segmenting the soccer video into 'plays', associating a user-specifiable 'utility' for each type of play and using 'bin-packing' to select a subset of the plays that add up to the desired length while maximizing the overall utility (volume in bin-packing terms). Our approach systematically allows a user to override the default weights assigned to each type of play with individual preferences and thus see a highly personalized variable length summarization of soccer matches. We demonstrate our approach based on the output of an end-to-end pipeline that we are building to produce such summaries. Though aspects of the overall end-to-end pipeline are human assisted at present, the results clearly show that the proposed approach is capable of producing semantically meaningful and compelling summaries. Besides the obvious use of producing summaries of superior league matches for news broadcasts, we anticipate our work to promote greater awareness of the local matches and junior leagues by producing consumable summaries of them.



### Hierarchical Attentive Recurrent Tracking
- **Arxiv ID**: http://arxiv.org/abs/1706.09262v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1706.09262v2)
- **Published**: 2017-06-28 13:00:14+00:00
- **Updated**: 2017-09-05 14:35:08+00:00
- **Authors**: Adam R. Kosiorek, Alex Bewley, Ingmar Posner
- **Comment**: Published as a conference paper at NIPS 2017. Code is available at
  https://github.com/akosiorek/hart and qualitative results are available at
  https://youtu.be/Vvkjm0FRGSs
- **Journal**: None
- **Summary**: Class-agnostic object tracking is particularly difficult in cluttered environments as target specific discriminative models cannot be learned a priori. Inspired by how the human visual cortex employs spatial attention and separate "where" and "what" processing pathways to actively suppress irrelevant visual features, this work develops a hierarchical attentive recurrent model for single object tracking in videos. The first layer of attention discards the majority of background by selecting a region containing the object of interest, while the subsequent layers tune in on visual features particular to the tracked object. This framework is fully differentiable and can be trained in a purely data driven fashion by gradient methods. To improve training convergence, we augment the loss function with terms for a number of auxiliary tasks relevant for tracking. Evaluation of the proposed model is performed on two datasets: pedestrian tracking on the KTH activity recognition dataset and the more difficult KITTI object tracking dataset.



### The YouTube-8M Kaggle Competition: Challenges and Methods
- **Arxiv ID**: http://arxiv.org/abs/1706.09274v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09274v2)
- **Published**: 2017-06-28 13:20:51+00:00
- **Updated**: 2017-07-13 05:30:37+00:00
- **Authors**: Haosheng Zou, Kun Xu, Jialian Li, Jun Zhu
- **Comment**: accepted to CVPR'17 Workshop on YouTube-8M Large-Scale Video
  Understanding (oral presentation); code is at
  https://github.com/taufikxu/youtube on branches kunxu and zhs
- **Journal**: None
- **Summary**: We took part in the YouTube-8M Video Understanding Challenge hosted on Kaggle, and achieved the 10th place within less than one month's time. In this paper, we present an extensive analysis and solution to the underlying machine-learning problem based on frame-level data, where major challenges are identified and corresponding preliminary methods are proposed. It's noteworthy that, with merely the proposed strategies and uniformly-averaging multi-crop ensemble was it sufficient for us to reach our ranking. We also report the methods we believe to be promising but didn't have enough time to train to convergence. We hope this paper could serve, to some extent, as a review and guideline of the YouTube-8M multi-label video classification benchmark, inspiring future attempts and research.



### Deep Learning Based Large-Scale Automatic Satellite Crosswalk Classification
- **Arxiv ID**: http://arxiv.org/abs/1706.09302v2
- **DOI**: 10.1109/LGRS.2017.2719863
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.09302v2)
- **Published**: 2017-06-28 14:06:24+00:00
- **Updated**: 2017-07-05 12:58:35+00:00
- **Authors**: Rodrigo F. Berriel, Andre Teixeira Lopes, Alberto F. de Souza, Thiago Oliveira-Santos
- **Comment**: 5 pages, 3 figures, accepted by IEEE Geoscience and Remote Sensing
  Letters
- **Journal**: None
- **Summary**: High-resolution satellite imagery have been increasingly used on remote sensing classification problems. One of the main factors is the availability of this kind of data. Even though, very little effort has been placed on the zebra crossing classification problem. In this letter, crowdsourcing systems are exploited in order to enable the automatic acquisition and annotation of a large-scale satellite imagery database for crosswalks related tasks. Then, this dataset is used to train deep-learning-based models in order to accurately classify satellite images that contains or not zebra crossings. A novel dataset with more than 240,000 images from 3 continents, 9 countries and more than 20 cities was used in the experiments. Experimental results showed that freely available crowdsourcing data can be used to accurately (97.11%) train robust models to perform crosswalk classification on a global scale.



### A New Urban Objects Detection Framework Using Weakly Annotated Sets
- **Arxiv ID**: http://arxiv.org/abs/1706.09308v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09308v2)
- **Published**: 2017-06-28 14:16:56+00:00
- **Updated**: 2017-07-14 14:38:55+00:00
- **Authors**: Eric Keiji, Gabriel Ferreira, Claudio Silva, Roberto M. Cesar Jr
- **Comment**: None
- **Journal**: None
- **Summary**: Urban informatics explore data science methods to address different urban issues intensively based on data. The large variety and quantity of data available should be explored but this brings important challenges. For instance, although there are powerful computer vision methods that may be explored, they may require large annotated datasets. In this work we propose a novel approach to automatically creating an object recognition system with minimal manual annotation. The basic idea behind the method is to use large input datasets using available online cameras on large cities. A off-the-shelf weak classifier is used to detect an initial set of urban elements of interest (e.g. cars, pedestrians, bikes, etc.). Such initial dataset undergoes a quality control procedure and it is subsequently used to fine tune a strong classifier. Quality control and comparative performance assessment are used as part of the pipeline. We evaluate the method for detecting cars based on monitoring cameras. Experimental results using real data show that despite losing generality, the final detector provides better detection rates tailored to the selected cameras. The programmed robot gathered 770 video hours from 24 online city cameras (\~300GB), which has been fed to the proposed system. Our approach has shown that the method nearly doubled the recall (93\%) with respect to state-of-the-art methods using off-the-shelf algorithms.



### Alternative Semantic Representations for Zero-Shot Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1706.09317v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1706.09317v1)
- **Published**: 2017-06-28 14:32:57+00:00
- **Updated**: 2017-06-28 14:32:57+00:00
- **Authors**: Qian Wang, Ke Chen
- **Comment**: Technical Report, School of Computer Science, The University of
  Manchester, Accepted to ECML-PKDD 2017
- **Journal**: None
- **Summary**: A proper semantic representation for encoding side information is key to the success of zero-shot learning. In this paper, we explore two alternative semantic representations especially for zero-shot human action recognition: textual descriptions of human actions and deep features extracted from still images relevant to human actions. Such side information are accessible on Web with little cost, which paves a new way in gaining side information for large-scale zero-shot human action recognition. We investigate different encoding methods to generate semantic representations for human actions from such side information. Based on our zero-shot visual recognition method, we conducted experiments on UCF101 and HMDB51 to evaluate two proposed semantic representations . The results suggest that our proposed text- and image-based semantic representations outperform traditional attributes and word vectors considerably for zero-shot human action recognition. In particular, the image-based semantic representations yield the favourable performance even though the representation is extracted from a small number of images per class.



### Retinal Vessel Segmentation in Fundoscopic Images with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.09318v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.09318v1)
- **Published**: 2017-06-28 14:33:22+00:00
- **Updated**: 2017-06-28 14:33:22+00:00
- **Authors**: Jaemin Son, Sang Jun Park, Kyu-Hwan Jung
- **Comment**: 9 pages, submitted to DLMIA 2017
- **Journal**: None
- **Summary**: Retinal vessel segmentation is an indispensable step for automatic detection of retinal diseases with fundoscopic images. Though many approaches have been proposed, existing methods tend to miss fine vessels or allow false positives at terminal branches. Let alone under-segmentation, over-segmentation is also problematic when quantitative studies need to measure the precise width of vessels. In this paper, we present a method that generates the precise map of retinal vessels using generative adversarial training. Our methods achieve dice coefficient of 0.829 on DRIVE dataset and 0.834 on STARE dataset which is the state-of-the-art performance on both datasets.



### Online Adaptation of Convolutional Neural Networks for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1706.09364v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09364v2)
- **Published**: 2017-06-28 17:02:39+00:00
- **Updated**: 2017-08-01 15:18:18+00:00
- **Authors**: Paul Voigtlaender, Bastian Leibe
- **Comment**: Accepted at BMVC 2017. This version contains minor changes for the
  camera ready version
- **Journal**: None
- **Summary**: We tackle the task of semi-supervised video object segmentation, i.e. segmenting the pixels belonging to an object in the video using the ground truth pixel mask for the first frame. We build on the recently introduced one-shot video object segmentation (OSVOS) approach which uses a pretrained network and fine-tunes it on the first frame. While achieving impressive performance, at test time OSVOS uses the fine-tuned network in unchanged form and is not able to adapt to large changes in object appearance. To overcome this limitation, we propose Online Adaptive Video Object Segmentation (OnAVOS) which updates the network online using training examples selected based on the confidence of the network and the spatial configuration. Additionally, we add a pretraining step based on objectness, which is learned on PASCAL. Our experiments show that both extensions are highly effective and improve the state of the art on DAVIS to an intersection-over-union score of 85.7%.



### Summarization of ICU Patient Motion from Multimodal Multiview Videos
- **Arxiv ID**: http://arxiv.org/abs/1706.09430v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09430v1)
- **Published**: 2017-06-28 18:08:37+00:00
- **Updated**: 2017-06-28 18:08:37+00:00
- **Authors**: Carlos Torres, Kenneth Rose, Jeffrey C. Fried, B. S. Manjunath
- **Comment**: Total of 17 pages: 1-12 (body), 13-16 (result figures), and
  17(references) Number of figures: 21
- **Journal**: None
- **Summary**: Clinical observations indicate that during critical care at the hospitals, patients sleep positioning and motion affect recovery. Unfortunately, there is no formal medical protocol to record, quantify, and analyze patient motion. There is a small number of clinical studies, which use manual analysis of sleep poses and motion recordings to support medical benefits of patient positioning and motion monitoring. Manual processes are not scalable, are prone to human errors, and strain an already taxed healthcare workforce. This study introduces DECU (Deep Eye-CU): an autonomous mulitmodal multiview system, which addresses these issues by autonomously monitoring healthcare environments and enabling the recording and analysis of patient sleep poses and motion. DECU uses three RGB-D cameras to monitor patient motion in a medical Intensive Care Unit (ICU). The algorithms in DECU estimate pose direction at different temporal resolutions and use keyframes to efficiently represent pose transition dynamics. DECU combines deep features computed from the data with a modified version of Hidden Markov Model to more flexibly model sleep pose duration, analyze pose patterns, and summarize patient motion. Extensive experimental results are presented. The performance of DECU is evaluated in ideal (BC: Bright and Clear/occlusion-free) and natural (DO: Dark and Occluded) scenarios at two motion resolutions in a mock-up and a real ICU. The results indicate that deep features allow DECU to match the classification performance of engineered features in BC scenes and increase the accuracy by up to 8% in DO scenes. In addition, the overall pose history summarization tracing accuracy shows an average detection rate of 85% in BC and of 76% in DO scenes. The proposed keyframe estimation algorithm allows DECU to reach an average 78% transition classification accuracy.



### You Are How You Walk: Uncooperative MoCap Gait Identification for Video Surveillance with Incomplete and Noisy Data
- **Arxiv ID**: http://arxiv.org/abs/1706.09443v4
- **DOI**: 10.1109/BTAS.2017.8272700
- **Categories**: **cs.CV**, 68T05, 68T10, I.5
- **Links**: [PDF](http://arxiv.org/pdf/1706.09443v4)
- **Published**: 2017-06-28 18:47:57+00:00
- **Updated**: 2022-12-07 22:16:31+00:00
- **Authors**: Michal Balazia, Petr Sojka
- **Comment**: Preprint. Full paper accepted at the IEEE/IAPR International Joint
  Conference on Biometrics (IJCB), Denver, USA, October 2017. 8 pages
- **Journal**: None
- **Summary**: This work offers a design of a video surveillance system based on a soft biometric -- gait identification from MoCap data. The main focus is on two substantial issues of the video surveillance scenario: (1) the walkers do not cooperate in providing learning data to establish their identities and (2) the data are often noisy or incomplete. We show that only a few examples of human gait cycles are required to learn a projection of raw MoCap data onto a low-dimensional sub-space where the identities are well separable. Latent features learned by Maximum Margin Criterion (MMC) method discriminate better than any collection of geometric features. The MMC method is also highly robust to noisy data and works properly even with only a fraction of joints tracked. The overall workflow of the design is directly applicable for a day-to-day operation based on the available MoCap technology and algorithms for gait analysis. In the concept we introduce, a walker's identity is represented by a cluster of gait data collected at their incidents within the surveillance system: They are how they walk.



### The application of deep convolutional neural networks to ultrasound for modelling of dynamic states within human skeletal muscle
- **Arxiv ID**: http://arxiv.org/abs/1706.09450v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09450v1)
- **Published**: 2017-06-28 19:18:04+00:00
- **Updated**: 2017-06-28 19:18:04+00:00
- **Authors**: Ryan J. Cunningham, Peter J. Harding, Ian D. Loram
- **Comment**: paper in preparation for submission to IEEE TMI
- **Journal**: None
- **Summary**: This paper concerns the fully automatic direct in vivo measurement of active and passive dynamic skeletal muscle states using ultrasound imaging. Despite the long standing medical need (myopathies, neuropathies, pain, injury, ageing), currently technology (electromyography, dynamometry, shear wave imaging) provides no general, non-invasive method for online estimation of skeletal intramuscular states. Ultrasound provides a technology in which static and dynamic muscle states can be observed non-invasively, yet current computational image understanding approaches are inadequate. We propose a new approach in which deep learning methods are used for understanding the content of ultrasound images of muscle in terms of its measured state. Ultrasound data synchronized with electromyography of the calf muscles, with measures of joint torque/angle were recorded from 19 healthy participants (6 female, ages: 30 +- 7.7). A segmentation algorithm previously developed by our group was applied to extract a region of interest of the medial gastrocnemius. Then a deep convolutional neural network was trained to predict the measured states (joint angle/torque, electromyography) directly from the segmented images. Results revealed for the first time that active and passive muscle states can be measured directly from standard b-mode ultrasound images, accurately predicting for a held out test participant changes in the joint angle, electromyography, and torque with as little error as 0.022{\deg}, 0.0001V, 0.256Nm (root mean square error) respectively.



### Real-time Distracted Driver Posture Classification
- **Arxiv ID**: http://arxiv.org/abs/1706.09498v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09498v3)
- **Published**: 2017-06-28 22:13:10+00:00
- **Updated**: 2018-11-29 21:50:16+00:00
- **Authors**: Yehya Abouelnaga, Hesham M. Eraqi, Mohamed N. Moustafa
- **Comment**: None
- **Journal**: 32nd Conference on Neural Information Processing Systems (NIPS
  2018), Workshop on Machine Learning for Intelligent Transportation Systems
- **Summary**: In this paper, we present a new dataset for "distracted driver" posture estimation. In addition, we propose a novel system that achieves 95.98% driving posture estimation classification accuracy. The system consists of a genetically-weighted ensemble of Convolutional Neural Networks (CNNs). We show that a weighted ensemble of classifiers using a genetic algorithm yields in better classification confidence. We also study the effect of different visual elements (i.e. hands and face) in distraction detection and classification by means of face and hand localizations. Finally, we present a thinned version of our ensemble that could achieve a 94.29% classification accuracy and operate in a realtime environment.



