# Arxiv Papers in cs.CV on 2017-06-23
### Multiresolution Match Kernels for Gesture Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1706.07530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07530v1)
- **Published**: 2017-06-23 00:23:32+00:00
- **Updated**: 2017-06-23 00:23:32+00:00
- **Authors**: Hemanth Venkateswara, Vineeth N. Balasubramanian, Prasanth Lade, Sethuraman Panchanathan
- **Comment**: ICME 2013 Conference
- **Journal**: None
- **Summary**: The emergence of depth imaging technologies like the Microsoft Kinect has renewed interest in computational methods for gesture classification based on videos. For several years now, researchers have used the Bag-of-Features (BoF) as a primary method for generation of feature vectors from video data for recognition of gestures. However, the BoF method is a coarse representation of the information in a video, which often leads to poor similarity measures between videos. Besides, when features extracted from different spatio-temporal locations in the video are pooled to create histogram vectors in the BoF method, there is an intrinsic loss of their original locations in space and time. In this paper, we propose a new Multiresolution Match Kernel (MMK) for video classification, which can be considered as a generalization of the BoF method. We apply this procedure to hand gesture classification based on RGB-D videos of the American Sign Language(ASL) hand gestures and our results show promise and usefulness of this new method.



### Listen to Your Face: Inferring Facial Action Units from Audio Channel
- **Arxiv ID**: http://arxiv.org/abs/1706.07536v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07536v2)
- **Published**: 2017-06-23 01:22:21+00:00
- **Updated**: 2017-09-19 14:27:00+00:00
- **Authors**: Zibo Meng, Shizhong Han, Yan Tong
- **Comment**: Accepted to IEEE Transactions on Affective Computing (TAFFC)
- **Journal**: None
- **Summary**: Extensive efforts have been devoted to recognizing facial action units (AUs). However, it is still challenging to recognize AUs from spontaneous facial displays especially when they are accompanied with speech. Different from all prior work that utilized visual observations for facial AU recognition, this paper presents a novel approach that recognizes speech-related AUs exclusively from audio signals based on the fact that facial activities are highly correlated with voice during speech. Specifically, dynamic and physiological relationships between AUs and phonemes are modeled through a continuous time Bayesian network (CTBN); then AU recognition is performed by probabilistic inference via the CTBN model.   A pilot audiovisual AU-coded database has been constructed to evaluate the proposed audio-based AU recognition framework. The database consists of a "clean" subset with frontal and neutral faces and a challenging subset collected with large head movements and occlusions. Experimental results on this database show that the proposed CTBN model achieves promising recognition performance for 7 speech-related AUs and outperforms the state-of-the-art visual-based methods especially for those AUs that are activated at low intensities or "hardly visible" in the visual channel. Furthermore, the CTBN model yields more impressive recognition performance on the challenging subset, where the visual-based approaches suffer significantly.



### Sampling Matters in Deep Embedding Learning
- **Arxiv ID**: http://arxiv.org/abs/1706.07567v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07567v2)
- **Published**: 2017-06-23 05:14:55+00:00
- **Updated**: 2018-01-16 16:54:27+00:00
- **Authors**: Chao-Yuan Wu, R. Manmatha, Alexander J. Smola, Philipp Krähenbühl
- **Comment**: Add supplementary material. Paper published in ICCV 2017
- **Journal**: None
- **Summary**: Deep embeddings answer one simple question: How similar are two images? Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search. The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss. While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role. We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches. In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions. We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification. Our method achieves state-of-the-art performance on all of them.



### Joint Prediction of Depths, Normals and Surface Curvature from RGB Images using CNNs
- **Arxiv ID**: http://arxiv.org/abs/1706.07593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07593v1)
- **Published**: 2017-06-23 08:18:44+00:00
- **Updated**: 2017-06-23 08:18:44+00:00
- **Authors**: Thanuja Dharmasiri, Andrew Spek, Tom Drummond
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the 3D structure of a scene is of vital importance, when it comes to developing fully autonomous robots. To this end, we present a novel deep learning based framework that estimates depth, surface normals and surface curvature by only using a single RGB image. To the best of our knowledge this is the first work to estimate surface curvature from colour using a machine learning approach. Additionally, we demonstrate that by tuning the network to infer well designed features, such as surface curvature, we can achieve improved performance at estimating depth and normals.This indicates that network guidance is still a useful aspect of designing and training a neural network. We run extensive experiments where the network is trained to infer different tasks while the model capacity is kept constant resulting in different feature maps based on the tasks at hand. We outperform the previous state-of-the-art benchmarks which jointly estimate depths and surface normals while predicting surface curvature in parallel.



### Computer-aided implant design for the restoration of cranial defects
- **Arxiv ID**: http://arxiv.org/abs/1706.07649v1
- **DOI**: 10.1038/s41598-017-04454-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07649v1)
- **Published**: 2017-06-23 11:55:46+00:00
- **Updated**: 2017-06-23 11:55:46+00:00
- **Authors**: Xiaojun Chen, Lu Xu, Xing Li, Jan Egger
- **Comment**: 10 pages, 12 figures, 20 References
- **Journal**: Scientific Reports 7, Article number: 4199 (2017)
- **Summary**: Patient-specific cranial implants are important and necessary in the surgery of cranial defect restoration. However, traditional methods of manual design of cranial implants are complicated and time-consuming. Our purpose is to develop a novel software named EasyCrania to design the cranial implants conveniently and efficiently. The process can be divided into five steps, which are mirroring model, clipping surface, surface fitting, the generation of the initial implant and the generation of the final implant. The main concept of our method is to use the geometry information of the mirrored model as the base to generate the final implant. The comparative studies demonstrated that the EasyCrania can improve the efficiency of cranial implant design significantly. And, the intra- and inter-rater reliability of the software were stable, which were 87.07+/-1.6% and 87.73+/-1.4% respectively.



### Training Adversarial Discriminators for Cross-channel Abnormal Event Detection in Crowds
- **Arxiv ID**: http://arxiv.org/abs/1706.07680v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07680v2)
- **Published**: 2017-06-23 13:06:33+00:00
- **Updated**: 2018-11-26 23:54:56+00:00
- **Authors**: Mahdyar Ravanbakhsh, Enver Sangineto, Moin Nabi, Nicu Sebe
- **Comment**: To appear at WACV 2019
- **Journal**: None
- **Summary**: Abnormal crowd behaviour detection attracts a large interest due to its importance in video surveillance scenarios. However, the ambiguity and the lack of sufficient abnormal ground truth data makes end-to-end training of large deep networks hard in this domain. In this paper we propose to use Generative Adversarial Nets (GANs), which are trained to generate only the normal distribution of the data. During the adversarial GAN training, a discriminator (D) is used as a supervisor for the generator network (G) and vice versa. At testing time we use D to solve our discriminative task (abnormality detection), where D has been trained without the need of manually-annotated abnormal data. Moreover, in order to prevent G learn a trivial identity function, we use a cross-channel approach, forcing G to transform raw-pixel data in motion information and vice versa. The quantitative results on standard benchmarks show that our method outperforms previous state-of-the-art methods in both the frame-level and the pixel-level evaluation.



