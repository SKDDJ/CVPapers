# Arxiv Papers in cs.CV on 2017-06-25
### Decomposing Motion and Content for Natural Video Sequence Prediction
- **Arxiv ID**: http://arxiv.org/abs/1706.08033v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08033v2)
- **Published**: 2017-06-25 04:18:12+00:00
- **Updated**: 2018-01-08 01:21:32+00:00
- **Authors**: Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, Honglak Lee
- **Comment**: International Conference on Learning Representations (ICLR) 2017
- **Journal**: None
- **Summary**: We propose a deep neural network for the prediction of future frames in natural video sequences. To effectively handle complex evolution of pixels in videos, we propose to decompose the motion and content, two key components generating dynamics in videos. Our model is built upon the Encoder-Decoder Convolutional Neural Network and Convolutional LSTM for pixel-level prediction, which independently capture the spatial layout of an image and the corresponding temporal dynamics. By independently modeling motion and content, predicting the next frame reduces to converting the extracted content features into the next frame content by the identified motion features, which simplifies the task of prediction. Our model is end-to-end trainable over multiple time steps, and naturally learns to decompose motion and content without separate training. We evaluate the proposed network architecture on human activity videos using KTH, Weizmann action, and UCF-101 datasets. We show state-of-the-art performance in comparison to recent approaches. To the best of our knowledge, this is the first end-to-end trainable network architecture with motion and content separation to model the spatiotemporal dynamics for pixel-level future prediction in natural videos.



### Obstacle detection test in real-word traffic contexts for the purposes of motorcycle autonomous emergency braking (MAEB)
- **Arxiv ID**: http://arxiv.org/abs/1707.03435v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1707.03435v2)
- **Published**: 2017-06-25 09:44:17+00:00
- **Updated**: 2017-10-02 20:57:23+00:00
- **Authors**: Giovanni Savino, Simone Piantini, Gustavo Gil, Marco Pierini
- **Comment**: 25th International Technical Conference on the Enhanced Safety of
  Vehicles (2017)
- **Journal**: None
- **Summary**: Research suggests that a Motorcycle Autonomous Emergency Braking system (MAEB) could influence 25% of the crashes involving powered two wheelers (PTWs). By automatically slowing down a host PTW of up to 10 km/h in inevitable collision scenarios, MAEB could potentially mitigate the crash severity for the riders. The feasibility of automatic decelerations of motorcycles was shown via field trials in controlled environment. However, the feasibility of correct MAEB triggering in the real traffic context is still unclear. In particular, MAEB requires an accurate obstacle detection, the feasibility of which from a single track vehicle has not been confirmed yet. To address this issue, our study presents obstacle detection tests in a real-world MAEB-sensitive crash scenario.



### Efficient and accurate monitoring of the depth information in a Wireless Multimedia Sensor Network based surveillance
- **Arxiv ID**: http://arxiv.org/abs/1706.08088v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08088v1)
- **Published**: 2017-06-25 12:24:30+00:00
- **Updated**: 2017-06-25 12:24:30+00:00
- **Authors**: Anthony Tannoury, Rony Darazi, Christophe Guyeux, Abdallah Makhoul
- **Comment**: None
- **Journal**: None
- **Summary**: Wireless Multimedia Sensor Network (WMSN) is a promising technology capturing rich multimedia data like audio and video, which can be useful to monitor an environment under surveillance. However, many scenarios in real time monitoring requires 3D depth information. In this research work, we propose to use the disparity map that is computed from two or multiple images, in order to monitor the depth information in an object or event under surveillance using WMSN. Our system is based on distributed wireless sensors allowing us to notably reduce the computational time needed for 3D depth reconstruction, thus permitting the success of real time solutions. Each pair of sensors will capture images for a targeted place/object and will operate a Stereo Matching in order to create a Disparity Map. Disparity maps will give us the ability to decrease traffic on the bandwidth, because they are of low size. This will increase WMSN lifetime. Any event can be detected after computing the depth value for the target object in the scene, and also 3D scene reconstruction can be achieved with a disparity map and some reference(s) image(s) taken by the node(s).



### Merging real and virtual worlds: An analysis of the state of the art and practical evaluation of Microsoft Hololens
- **Arxiv ID**: http://arxiv.org/abs/1706.08096v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.08096v1)
- **Published**: 2017-06-25 13:10:39+00:00
- **Updated**: 2017-06-25 13:10:39+00:00
- **Authors**: Adrien Coppens
- **Comment**: Submitted in fulfillment of the requirements for the degree of Master
  in Computer Science
- **Journal**: None
- **Summary**: Achieving a symbiotic blending between reality and virtuality is a dream that has been lying in the minds of many people for a long time. Advances in various domains constantly bring us closer to making that dream come true. Augmented reality as well as virtual reality are in fact trending terms and are expected to further progress in the years to come.   This master's thesis aims to explore these areas and starts by defining necessary terms such as augmented reality (AR) or virtual reality (VR). Usual taxonomies to classify and compare the corresponding experiences are then discussed.   In order to enable those applications, many technical challenges need to be tackled, such as accurate motion tracking with 6 degrees of freedom (positional and rotational), that is necessary for compelling experiences and to prevent user sickness. Additionally, augmented reality experiences typically rely on image processing to position the superimposed content. To do so, "paper" markers or features extracted from the environment are often employed. Both sets of techniques are explored and common solutions and algorithms are presented.   After investigating those technical aspects, I carry out an objective comparison of the existing state-of-the-art and state-of-the-practice in those domains, and I discuss present and potential applications in these areas. As a practical validation, I present the results of an application that I have developed using Microsoft HoloLens, one of the more advanced affordable technologies for augmented reality that is available today. Based on the experience and lessons learned during this development, I discuss the limitations of current technologies and present some avenues of future research.



### FReLU: Flexible Rectified Linear Units for Improving Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.08098v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08098v2)
- **Published**: 2017-06-25 13:28:27+00:00
- **Updated**: 2018-01-29 07:07:42+00:00
- **Authors**: Suo Qiu, Xiangmin Xu, Bolun Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Rectified linear unit (ReLU) is a widely used activation function for deep convolutional neural networks. However, because of the zero-hard rectification, ReLU networks miss the benefits from negative values. In this paper, we propose a novel activation function called \emph{flexible rectified linear unit (FReLU)} to further explore the effects of negative values. By redesigning the rectified point of ReLU as a learnable parameter, FReLU expands the states of the activation output. When the network is successfully trained, FReLU tends to converge to a negative value, which improves the expressiveness and thus the performance. Furthermore, FReLU is designed to be simple and effective without exponential functions to maintain low cost computation. For being able to easily used in various network architectures, FReLU does not rely on strict assumptions by self-adaption. We evaluate FReLU on three standard image classification datasets, including CIFAR-10, CIFAR-100, and ImageNet. Experimental results show that the proposed method achieves fast convergence and higher performances on both plain and residual networks.



### Detekcja upadku i wybranych akcji na sekwencjach obraz√≥w cyfrowych
- **Arxiv ID**: http://arxiv.org/abs/1706.08107v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08107v1)
- **Published**: 2017-06-25 13:55:55+00:00
- **Updated**: 2017-06-25 13:55:55+00:00
- **Authors**: Michal Kepski
- **Comment**: PhD Thesis (in Polish)
- **Journal**: None
- **Summary**: In recent years a growing interest on action recognition is observed, including detection of fall accident for the elderly. However, despite many efforts undertaken, the existing technology is not widely used by elderly, mainly because of its flaws like low precision, large number of false alarms, inadequate privacy preserving during data acquisition and processing. This research work meets these expectations. The work is empirical and it is situated in the field of computer vision systems. The main part of the work situates itself in the area of action and behavior recognition. Efficient algorithms for fall detection were developed, tested and implemented using image sequences and wireless inertial sensor worn by a monitored person. A set of descriptors for depth maps has been elaborated to permit classification of pose as well as the action of a person. Experimental research was carried out based on the prepared data repository consisting of synchronized depth and accelerometric data. The study was carried out in the scenario with a static camera facing the scene and an active camera observing the scene from above. The experimental results showed that the developed algorithms for fall detection have high sensitivity and specificity. The algorithm were designed with regard to low computational demands and possibility to run on ARM platforms. Several experiments including person detection, tracking and fall detection in real-time were carried out to show efficiency and reliability of the proposed solutions.



### Scalable multimodal convolutional networks for brain tumour segmentation
- **Arxiv ID**: http://arxiv.org/abs/1706.08124v1
- **DOI**: 10.1007/978-3-319-66179-7_33
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08124v1)
- **Published**: 2017-06-25 15:23:54+00:00
- **Updated**: 2017-06-25 15:23:54+00:00
- **Authors**: Lucas Fidon, Wenqi Li, Luis C. Garcia-Peraza-Herrera, Jinendra Ekanayake, Neil Kitchen, Sebastien Ourselin, Tom Vercauteren
- **Comment**: Paper accepted at MICCAI 2017
- **Journal**: None
- **Summary**: Brain tumour segmentation plays a key role in computer-assisted surgery. Deep neural networks have increased the accuracy of automatic segmentation significantly, however these models tend to generalise poorly to different imaging modalities than those for which they have been designed, thereby limiting their applications. For example, a network architecture initially designed for brain parcellation of monomodal T1 MRI can not be easily translated into an efficient tumour segmentation network that jointly utilises T1, T1c, Flair and T2 MRI. To tackle this, we propose a novel scalable multimodal deep learning architecture using new nested structures that explicitly leverage deep features within or across modalities. This aims at making the early layers of the architecture structured and sparse so that the final architecture becomes scalable to the number of modalities. We evaluate the scalable architecture for brain tumour segmentation and give evidence of its regularisation effect compared to the conventional concatenation approach.



### ToolNet: Holistically-Nested Real-Time Segmentation of Robotic Surgical Tools
- **Arxiv ID**: http://arxiv.org/abs/1706.08126v2
- **DOI**: 10.1109/IROS.2017.8206462
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08126v2)
- **Published**: 2017-06-25 15:41:25+00:00
- **Updated**: 2017-07-04 19:53:12+00:00
- **Authors**: Luis C. Garcia-Peraza-Herrera, Wenqi Li, Lucas Fidon, Caspar Gruijthuijsen, Alain Devreker, George Attilakos, Jan Deprest, Emmanuel Vander Poorten, Danail Stoyanov, Tom Vercauteren, Sebastien Ourselin
- **Comment**: Paper accepted at IROS 2017
- **Journal**: None
- **Summary**: Real-time tool segmentation from endoscopic videos is an essential part of many computer-assisted robotic surgical systems and of critical importance in robotic surgical data science. We propose two novel deep learning architectures for automatic segmentation of non-rigid surgical instruments. Both methods take advantage of automated deep-learning-based multi-scale feature extraction while trying to maintain an accurate segmentation quality at all resolutions. The two proposed methods encode the multi-scale constraint inside the network architecture. The first proposed architecture enforces it by cascaded aggregation of predictions and the second proposed network does it by means of a holistically-nested architecture where the loss at each scale is taken into account for the optimization process. As the proposed methods are for real-time semantic labeling, both present a reduced number of parameters. We propose the use of parametric rectified linear units for semantic labeling in these small architectures to increase the regularization ability of the design and maintain the segmentation accuracy without overfitting the training sets. We compare the proposed architectures against state-of-the-art fully convolutional networks. We validate our methods using existing benchmark datasets, including ex vivo cases with phantom tissue and different robotic surgical instruments present in the scene. Our results show a statistically significant improved Dice Similarity Coefficient over previous instrument segmentation methods. We analyze our design choices and discuss the key drivers for improving accuracy.



### Photometric Stereo by Hemispherical Metric Embedding
- **Arxiv ID**: http://arxiv.org/abs/1706.08153v1
- **DOI**: 10.1007/s10851-017-0748-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08153v1)
- **Published**: 2017-06-25 18:36:28+00:00
- **Updated**: 2017-06-25 18:36:28+00:00
- **Authors**: Ofer Bartal, Nati Ofir, Yaron Lipman, Ronen Basri
- **Comment**: None
- **Journal**: None
- **Summary**: Photometric Stereo methods seek to reconstruct the 3d shape of an object from motionless images obtained with varying illumination. Most existing methods solve a restricted problem where the physical reflectance model, such as Lambertian reflectance, is known in advance. In contrast, we do not restrict ourselves to a specific reflectance model. Instead, we offer a method that works on a wide variety of reflectances. Our approach uses a simple yet uncommonly used property of the problem - the sought after normals are points on a unit hemisphere. We present a novel embedding method that maps pixels to normals on the unit hemisphere. Our experiments demonstrate that this approach outperforms existing manifold learning methods for the task of hemisphere embedding. We further show successful reconstructions of objects from a wide variety of reflectances including smooth, rough, diffuse and specular surfaces, even in the presence of significant attached shadows. Finally, we empirically prove that under these challenging settings we obtain more accurate shape reconstructions than existing methods.



### Robust Video-Based Eye Tracking Using Recursive Estimation of Pupil Characteristics
- **Arxiv ID**: http://arxiv.org/abs/1706.08189v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08189v2)
- **Published**: 2017-06-25 23:22:02+00:00
- **Updated**: 2017-06-27 08:23:16+00:00
- **Authors**: Terence Brouns
- **Comment**: 29 pages, 16 figures; fixed hyperlink in abstract
- **Journal**: None
- **Summary**: Video-based eye tracking is a valuable technique in various research fields. Numerous open-source eye tracking algorithms have been developed in recent years, primarily designed for general application with many different camera types. These algorithms do not, however, capitalize on the high frame rate of eye tracking cameras often employed in psychophysical studies. We present a pupil detection method that utilizes this high-speed property to obtain reliable predictions through recursive estimation about certain pupil characteristics in successive camera frames. These predictions are subsequently used to carry out novel image segmentation and classification routines to improve pupil detection performance. Based on results from hand-labelled eye images, our approach was found to have a greater detection rate, accuracy and speed compared to other recently published open-source pupil detection algorithms. The program's source code, together with a graphical user interface, can be downloaded at https://github.com/tbrouns/eyestalker



