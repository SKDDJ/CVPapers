# Arxiv Papers in cs.CV on 2017-06-07
### Network Sketching: Exploiting Binary Structure in Deep CNNs
- **Arxiv ID**: http://arxiv.org/abs/1706.02021v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.02021v1)
- **Published**: 2017-06-07 01:53:44+00:00
- **Updated**: 2017-06-07 01:53:44+00:00
- **Authors**: Yiwen Guo, Anbang Yao, Hao Zhao, Yurong Chen
- **Comment**: To appear in CVPR2017
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) with deep architectures have substantially advanced the state-of-the-art in computer vision tasks. However, deep networks are typically resource-intensive and thus difficult to be deployed on mobile devices. Recently, CNNs with binary weights have shown compelling efficiency to the community, whereas the accuracy of such models is usually unsatisfactory in practice. In this paper, we introduce network sketching as a novel technique of pursuing binary-weight CNNs, targeting at more faithful inference and better trade-off for practical applications. Our basic idea is to exploit binary structure directly in pre-trained filter banks and produce binary-weight models via tensor expansion. The whole process can be treated as a coarse-to-fine model approximation, akin to the pencil drawing steps of outlining and shading. To further speedup the generated models, namely the sketches, we also propose an associative implementation of binary tensor convolutions. Experimental results demonstrate that a proper sketch of AlexNet (or ResNet) outperforms the existing binary-weight models by large margins on the ImageNet large scale classification task, while the committed memory for network parameters only exceeds a little.



### Imposing Hard Constraints on Deep Networks: Promises and Limitations
- **Arxiv ID**: http://arxiv.org/abs/1706.02025v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02025v1)
- **Published**: 2017-06-07 02:03:00+00:00
- **Updated**: 2017-06-07 02:03:00+00:00
- **Authors**: Pablo Márquez-Neila, Mathieu Salzmann, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Imposing constraints on the output of a Deep Neural Net is one way to improve the quality of its predictions while loosening the requirements for labeled training data. Such constraints are usually imposed as soft constraints by adding new terms to the loss function that is minimized during training. An alternative is to impose them as hard constraints, which has a number of theoretical benefits but has not been explored so far due to the perceived intractability of the problem.   In this paper, we show that imposing hard constraints can in fact be done in a computationally feasible way and delivers reasonable results. However, the theoretical benefits do not materialize and the resulting technique is no better than existing ones relying on soft constraints. We analyze the reasons for this and hope to spur other researchers into proposing better solutions.



### DeepSketch2Face: A Deep Learning Based Sketching System for 3D Face and Caricature Modeling
- **Arxiv ID**: http://arxiv.org/abs/1706.02042v1
- **DOI**: 10.1145/3072959.3073629
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.02042v1)
- **Published**: 2017-06-07 04:02:27+00:00
- **Updated**: 2017-06-07 04:02:27+00:00
- **Authors**: Xiaoguang Han, Chang Gao, Yizhou Yu
- **Comment**: 12 pages, 16 figures, to appear in SIGGRAPH 2017
- **Journal**: None
- **Summary**: Face modeling has been paid much attention in the field of visual computing. There exist many scenarios, including cartoon characters, avatars for social media, 3D face caricatures as well as face-related art and design, where low-cost interactive face modeling is a popular approach especially among amateur users. In this paper, we propose a deep learning based sketching system for 3D face and caricature modeling. This system has a labor-efficient sketching interface, that allows the user to draw freehand imprecise yet expressive 2D lines representing the contours of facial features. A novel CNN based deep regression network is designed for inferring 3D face models from 2D sketches. Our network fuses both CNN and shape based features of the input sketch, and has two independent branches of fully connected layers generating independent subsets of coefficients for a bilinear face representation. Our system also supports gesture based interactions for users to further manipulate initial face models. Both user studies and numerical results indicate that our sketching system can help users create face models quickly and effectively. A significantly expanded face database with diverse identities, expressions and levels of exaggeration is constructed to promote further research and evaluation of face modeling techniques.



### Automatic Emphysema Detection using Weakly Labeled HRCT Lung Images
- **Arxiv ID**: http://arxiv.org/abs/1706.02051v2
- **DOI**: 10.1371/journal.pone.0205397
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02051v2)
- **Published**: 2017-06-07 05:38:49+00:00
- **Updated**: 2018-10-01 12:48:31+00:00
- **Authors**: Isabel Pino Peña, Veronika Cheplygina, Sofia Paschaloudi, Morten Vuust, Jesper Carl, Ulla Møller Weinreich, Lasse Riis Østergaard, Marleen de Bruijne
- **Comment**: Accepted at PLoS ONE
- **Journal**: None
- **Summary**: A method for automatically quantifying emphysema regions using High-Resolution Computed Tomography (HRCT) scans of patients with chronic obstructive pulmonary disease (COPD) that does not require manually annotated scans for training is presented. HRCT scans of controls and of COPD patients with diverse disease severity are acquired at two different centers. Textural features from co-occurrence matrices and Gaussian filter banks are used to characterize the lung parenchyma in the scans. Two robust versions of multiple instance learning (MIL) classifiers, miSVM and MILES, are investigated. The classifiers are trained with the weak labels extracted from the forced expiratory volume in one minute (FEV$_1$) and diffusing capacity of the lungs for carbon monoxide (DLCO). At test time, the classifiers output a patient label indicating overall COPD diagnosis and local labels indicating the presence of emphysema. The classifier performance is compared with manual annotations by two radiologists, a classical density based method, and pulmonary function tests (PFTs). The miSVM classifier performed better than MILES on both patient and emphysema classification. The classifier has a stronger correlation with PFT than the density based method, the percentage of emphysema in the intersection of annotations from both radiologists, and the percentage of emphysema annotated by one of the radiologists. The correlation between the classifier and the PFT is only outperformed by the second radiologist. The method is therefore promising for facilitating assessment of emphysema and reducing inter-observer variability.



### Unsupervised Place Discovery for Place-Specific Change Classifier
- **Arxiv ID**: http://arxiv.org/abs/1706.02054v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02054v1)
- **Published**: 2017-06-07 06:00:49+00:00
- **Updated**: 2017-06-07 06:00:49+00:00
- **Authors**: Fei Xiaoxiao, Tanaka Kanji
- **Comment**: 6 pages, 6 figures, 2 tables, Technical report
- **Journal**: None
- **Summary**: In this study, we address the problem of supervised change detection for robotic map learning applications, in which the aim is to train a place-specific change classifier (e.g., support vector machine (SVM)) to predict changes from a robot's view image. An open question is the manner in which to partition a robot's workspace into places (e.g., SVMs) to maximize the overall performance of change classifiers. This is a chicken-or-egg problem: if we have a well-trained change classifier, partitioning the robot's workspace into places is rather easy. However, training a change classifier requires a set of place-specific training data. In this study, we address this novel problem, which we term unsupervised place discovery. In addition, we present a solution powered by convolutional-feature-based visual place recognition, and validate our approach by applying it to two place-specific change classifiers, namely, nuisance and anomaly predictors.



### Early Experiences with Crowdsourcing Airway Annotations in Chest CT
- **Arxiv ID**: http://arxiv.org/abs/1706.02055v1
- **DOI**: 10.1007/978-3-319-46976-8_22
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02055v1)
- **Published**: 2017-06-07 06:09:04+00:00
- **Updated**: 2017-06-07 06:09:04+00:00
- **Authors**: Veronika Cheplygina, Adria Perez-Rovira, Wieying Kuo, Harm A. W. M. Tiddens, Marleen de Bruijne
- **Comment**: None
- **Journal**: LABELS 2016, DLMIA 2016: Deep Learning and Data Labeling for
  Medical Applications pp 209-218
- **Summary**: Measuring airways in chest computed tomography (CT) images is important for characterizing diseases such as cystic fibrosis, yet very time-consuming to perform manually. Machine learning algorithms offer an alternative, but need large sets of annotated data to perform well. We investigate whether crowdsourcing can be used to gather airway annotations which can serve directly for measuring the airways, or as training data for the algorithms. We generate image slices at known locations of airways and request untrained crowd workers to outline the airway lumen and airway wall. Our results show that the workers are able to interpret the images, but that the instructions are too complex, leading to many unusable annotations. After excluding unusable annotations, quantitative results show medium to high correlations with expert measurements of the airways. Based on this positive experience, we describe a number of further research directions and provide insight into the challenges of crowdsourcing in medical images from the perspective of first-time users.



### DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data
- **Arxiv ID**: http://arxiv.org/abs/1706.02071v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02071v1)
- **Published**: 2017-06-07 07:27:20+00:00
- **Updated**: 2017-06-07 07:27:20+00:00
- **Authors**: Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, Venkatesh Babu Radhakrishnan
- **Comment**: Accepted at CVPR-2017. Code for training the GAN models and computing
  modified inception-scores can be found at https://github.com/val-iisc/deligan
- **Journal**: None
- **Summary**: A class of recent approaches for generating images, called Generative Adversarial Networks (GAN), have been used to generate impressively realistic images of objects, bedrooms, handwritten digits and a variety of other image modalities. However, typical GAN-based approaches require large amounts of training data to capture the diversity across the image modality. In this paper, we propose DeLiGAN -- a novel GAN-based architecture for diverse and limited training data scenarios. In our approach, we reparameterize the latent generative space as a mixture model and learn the mixture model's parameters along with those of GAN. This seemingly simple modification to the GAN framework is surprisingly effective and results in models which enable diversity in generated samples although trained with limited data. In our work, we show that DeLiGAN can generate images of handwritten digits, objects and hand-drawn sketches, all using limited amounts of data. To quantitatively characterize intra-class diversity of generated samples, we also introduce a modified version of "inception-score", a measure which has been found to correlate well with human assessment of generated samples.



### BiSeg: Simultaneous Instance Segmentation and Semantic Segmentation with Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.02135v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02135v2)
- **Published**: 2017-06-07 11:23:44+00:00
- **Updated**: 2017-07-18 02:28:21+00:00
- **Authors**: Viet-Quoc Pham, Satoshi Ito, Tatsuo Kozakaya
- **Comment**: BMVC2017
- **Journal**: None
- **Summary**: We present a simple and effective framework for simultaneous semantic segmentation and instance segmentation with Fully Convolutional Networks (FCNs). The method, called BiSeg, predicts instance segmentation as a posterior in Bayesian inference, where semantic segmentation is used as a prior. We extend the idea of position-sensitive score maps used in recent methods to a fusion of multiple score maps at different scales and partition modes, and adopt it as a robust likelihood for instance segmentation inference. As both Bayesian inference and map fusion are performed per pixel, BiSeg is a fully convolutional end-to-end solution that inherits all the advantages of FCNs. We demonstrate state-of-the-art instance segmentation accuracy on PASCAL VOC.



### Time Stretch Inspired Computational Imaging
- **Arxiv ID**: http://arxiv.org/abs/1706.07841v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.data-an, physics.optics, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.07841v1)
- **Published**: 2017-06-07 12:41:50+00:00
- **Updated**: 2017-06-07 12:41:50+00:00
- **Authors**: Bahram Jalali, Madhuri Suthar, Mohamad Asghari, Ata Mahjoubfar
- **Comment**: This work has been published in the PHOTOPTICS 2017 - 5th
  International Conference on Photonics, Optics and Laser Technology, Volume 1,
  ISBN 978-989-758-223-3, pages 340-345
- **Journal**: None
- **Summary**: We show that dispersive propagation of light followed by phase detection has properties that can be exploited for extracting features from the waveforms. This discovery is spearheading development of a new class of physics-inspired algorithms for feature extraction from digital images with unique properties and superior dynamic range compared to conventional algorithms. In certain cases, these algorithms have the potential to be an energy efficient and scalable substitute to synthetically fashioned computational techniques in practice today.



### Synthesizing Filamentary Structured Images with GANs
- **Arxiv ID**: http://arxiv.org/abs/1706.02185v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02185v1)
- **Published**: 2017-06-07 13:37:09+00:00
- **Updated**: 2017-06-07 13:37:09+00:00
- **Authors**: He Zhao, Huiqi Li, Li Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims at synthesizing filamentary structured images such as retinal fundus images and neuronal images, as follows: Given a ground-truth, to generate multiple realistic looking phantoms. A ground-truth could be a binary segmentation map containing the filamentary structured morphology, while the synthesized output image is of the same size as the ground-truth and has similar visual appearance to what have been presented in the training set. Our approach is inspired by the recent progresses in generative adversarial nets (GANs) as well as image style transfer. In particular, it is dedicated to our problem context with the following properties: Rather than large-scale dataset, it works well in the presence of as few as 10 training examples, which is common in medical image analysis; It is capable of synthesizing diverse images from the same ground-truth; Last and importantly, the synthetic images produced by our approach are demonstrated to be useful in boosting image analysis performance. Empirical examination over various benchmarks of fundus and neuronal images demonstrate the advantages of the proposed approach.



### Recurrent computations for visual pattern completion
- **Arxiv ID**: http://arxiv.org/abs/1706.02240v2
- **DOI**: 10.1073/pnas.1719397115
- **Categories**: **q-bio.NC**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.02240v2)
- **Published**: 2017-06-07 16:23:28+00:00
- **Updated**: 2018-04-06 12:29:22+00:00
- **Authors**: Hanlin Tang, Martin Schrimpf, Bill Lotter, Charlotte Moerman, Ana Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, Gabriel Kreiman
- **Comment**: None
- **Journal**: None
- **Summary**: Making inferences from partial information constitutes a critical aspect of cognition. During visual perception, pattern completion enables recognition of poorly visible or occluded objects. We combined psychophysics, physiology and computational models to test the hypothesis that pattern completion is implemented by recurrent computations and present three pieces of evidence that are consistent with this hypothesis. First, subjects robustly recognized objects even when rendered <15% visible, but recognition was largely impaired when processing was interrupted by backward masking. Second, invasive physiological responses along the human ventral cortex exhibited visually selective responses to partially visible objects that were delayed compared to whole objects, suggesting the need for additional computations. These physiological delays were correlated with the effects of backward masking. Third, state-of-the-art feed-forward computational architectures were not robust to partial visibility. However, recognition performance was recovered when the model was augmented with attractor-based recurrent connectivity. These results provide a strong argument of plausibility for the role of recurrent computations in making visual inferences from partial information.



### Comparative Analysis of Open Source Frameworks for Machine Learning with Use Case in Single-Threaded and Multi-Threaded Modes
- **Arxiv ID**: http://arxiv.org/abs/1706.02248v1
- **DOI**: 10.1109/STC-CSIT.2017.8098808
- **Categories**: **cs.LG**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1706.02248v1)
- **Published**: 2017-06-07 16:41:21+00:00
- **Updated**: 2017-06-07 16:41:21+00:00
- **Authors**: Yuriy Kochura, Sergii Stirenko, Anis Rojbi, Oleg Alienin, Michail Novotarskiy, Yuri Gordienko
- **Comment**: 4 pages, 6 figures, 4 tables; XIIth International Scientific and
  Technical Conference on Computer Sciences and Information Technologies (CSIT
  2017), Lviv, Ukraine
- **Journal**: Proceedings of 12th International Scientific and Technical
  Conference on Computer Sciences and Information Technologies (CSIT), 5-8
  Sept. 2017, (Lviv, Ukraine), vol.1, pp. 373-376, IEEE
- **Summary**: The basic features of some of the most versatile and popular open source frameworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are considered and compared. Their comparative analysis was performed and conclusions were made as to the advantages and disadvantages of these platforms. The performance tests for the de facto standard MNIST data set were carried out on H2O framework for deep learning algorithms designed for CPU and GPU platforms for single-threaded and multithreaded modes of operation.



### Driver Action Prediction Using Deep (Bidirectional) Recurrent Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1706.02257v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1706.02257v1)
- **Published**: 2017-06-07 17:00:08+00:00
- **Updated**: 2017-06-07 17:00:08+00:00
- **Authors**: Oluwatobi Olabiyi, Eric Martinson, Vijay Chintalapudi, Rui Guo
- **Comment**: ITSC'17
- **Journal**: None
- **Summary**: Advanced driver assistance systems (ADAS) can be significantly improved with effective driver action prediction (DAP). Predicting driver actions early and accurately can help mitigate the effects of potentially unsafe driving behaviors and avoid possible accidents. In this paper, we formulate driver action prediction as a timeseries anomaly prediction problem. While the anomaly (driver actions of interest) detection might be trivial in this context, finding patterns that consistently precede an anomaly requires searching for or extracting features across multi-modal sensory inputs. We present such a driver action prediction system, including a real-time data acquisition, processing and learning framework for predicting future or impending driver action. The proposed system incorporates camera-based knowledge of the driving environment and the driver themselves, in addition to traditional vehicle dynamics. It then uses a deep bidirectional recurrent neural network (DBRNN) to learn the correlation between sensory inputs and impending driver behavior achieving accurate and high horizon action prediction. The proposed system performs better than other existing systems on driver action prediction tasks and can accurately predict key driver actions including acceleration, braking, lane change and turning at durations of 5sec before the action is executed by the driver.



### CoMaL Tracking: Tracking Points at the Object Boundaries
- **Arxiv ID**: http://arxiv.org/abs/1706.02331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02331v1)
- **Published**: 2017-06-07 18:38:05+00:00
- **Updated**: 2017-06-07 18:38:05+00:00
- **Authors**: Santhosh K. Ramakrishnan, Swarna Kamlam Ravindran, Anurag Mittal
- **Comment**: 10 pages, 10 figures, to appear in 1st Joint BMTT-PETS Workshop on
  Tracking and Surveillance, CVPR 2017
- **Journal**: None
- **Summary**: Traditional point tracking algorithms such as the KLT use local 2D information aggregation for feature detection and tracking, due to which their performance degrades at the object boundaries that separate multiple objects. Recently, CoMaL Features have been proposed that handle such a case. However, they proposed a simple tracking framework where the points are re-detected in each frame and matched. This is inefficient and may also lose many points that are not re-detected in the next frame. We propose a novel tracking algorithm to accurately and efficiently track CoMaL points. For this, the level line segment associated with the CoMaL points is matched to MSER segments in the next frame using shape-based matching and the matches are further filtered using texture-based matching. Experiments show improvements over a simple re-detect-and-match framework as well as KLT in terms of speed/accuracy on different real-world applications, especially at the object boundaries.



### Low-shot learning with large-scale diffusion
- **Arxiv ID**: http://arxiv.org/abs/1706.02332v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.02332v3)
- **Published**: 2017-06-07 18:40:26+00:00
- **Updated**: 2018-06-15 15:31:37+00:00
- **Authors**: Matthijs Douze, Arthur Szlam, Bharath Hariharan, Hervé Jégou
- **Comment**: None
- **Journal**: None
- **Summary**: This paper considers the problem of inferring image labels from images when only a few annotated examples are available at training time. This setup is often referred to as low-shot learning, where a standard approach is to re-train the last few layers of a convolutional neural network learned on separate classes for which training examples are abundant. We consider a semi-supervised setting based on a large collection of images to support label propagation. This is possible by leveraging the recent advances on large-scale similarity graph construction.   We show that despite its conceptual simplicity, scaling label propagation up to hundred millions of images leads to state of the art accuracy in the low-shot learning regime.



### Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1706.02337v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.02337v1)
- **Published**: 2017-06-07 18:51:31+00:00
- **Updated**: 2017-06-07 18:51:31+00:00
- **Authors**: Xiao Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel Kifer, C. Lee Giles
- **Comment**: CVPR 2017 Spotlight
- **Journal**: None
- **Summary**: We present an end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images. We consider document semantic structure extraction as a pixel-wise segmentation task, and propose a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text. Moreover, we propose an efficient synthetic document generation process that we use to generate pretraining data for our network. Once the network is trained on a large set of synthetic documents, we fine-tune the network on unlabeled real documents using a semi-supervised approach. We systematically study the optimum network architecture and show that both our multimodal approach and the synthetic data pretraining significantly boost the performance.



### Active Learning for Structured Prediction from Partially Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/1706.02342v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02342v2)
- **Published**: 2017-06-07 19:01:25+00:00
- **Updated**: 2017-06-09 06:23:32+00:00
- **Authors**: Mehran Khodabandeh, Zhiwei Deng, Mostafa S. Ibrahim, Shinichi Satoh, Greg Mori
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a general purpose active learning algorithm for structured prediction, gathering labeled data for training a model that outputs a set of related labels for an image or video. Active learning starts with a limited initial training set, then iterates querying a user for labels on unlabeled data and retraining the model. We propose a novel algorithm for selecting data for labeling, choosing examples to maximize expected information gain based on belief propagation inference. This is a general purpose method and can be applied to a variety of tasks or models. As a specific example we demonstrate this framework for learning to recognize human actions and group activities in video sequences. Experiments show that our proposed algorithm outperforms previous active learning methods and can achieve accuracy comparable to fully supervised methods while utilizing significantly less labeled data.



### Training Quantized Nets: A Deeper Understanding
- **Arxiv ID**: http://arxiv.org/abs/1706.02379v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.02379v3)
- **Published**: 2017-06-07 21:01:15+00:00
- **Updated**: 2017-11-13 16:32:39+00:00
- **Authors**: Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, Tom Goldstein
- **Comment**: NIPS 2017
- **Journal**: None
- **Summary**: Currently, deep neural networks are deployed on low-power portable devices by first training a full-precision model using powerful hardware, and then deriving a corresponding low-precision model for efficient inference on such systems. However, training models directly with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. Numerous recent publications have studied methods for training quantized networks, but these studies have mostly been empirical. In this work, we investigate training methods for quantized neural networks from a theoretical viewpoint. We first explore accuracy guarantees for training methods under convexity assumptions. We then look at the behavior of these algorithms for non-convex problems, and show that training algorithms that exploit high-precision representations have an important greedy search phase that purely quantized training methods lack, which explains the difficulty of training using low-precision arithmetic.



### ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.02393v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1706.02393v1)
- **Published**: 2017-06-07 22:00:25+00:00
- **Updated**: 2017-06-07 22:00:25+00:00
- **Authors**: Denis A. Gudovskiy, Luca Rigazio
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce ShiftCNN, a generalized low-precision architecture for inference of multiplierless convolutional neural networks (CNNs). ShiftCNN is based on a power-of-two weight representation and, as a result, performs only shift and addition operations. Furthermore, ShiftCNN substantially reduces computational cost of convolutional layers by precomputing convolution terms. Such an optimization can be applied to any CNN architecture with a relatively small codebook of weights and allows to decrease the number of product operations by at least two orders of magnitude. The proposed architecture targets custom inference accelerators and can be realized on FPGAs or ASICs. Extensive evaluation on ImageNet shows that the state-of-the-art CNNs can be converted without retraining into ShiftCNN with less than 1% drop in accuracy when the proposed quantization algorithm is employed. RTL simulations, targeting modern FPGAs, show that power consumption of convolutional layers is reduced by a factor of 4 compared to conventional 8-bit fixed-point architectures.



### PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space
- **Arxiv ID**: http://arxiv.org/abs/1706.02413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02413v1)
- **Published**: 2017-06-07 23:37:44+00:00
- **Updated**: 2017-06-07 23:37:44+00:00
- **Authors**: Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.



