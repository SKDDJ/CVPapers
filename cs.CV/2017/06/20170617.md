# Arxiv Papers in cs.CV on 2017-06-17
### Truly Multi-modal YouTube-8M Video Classification with Video, Audio, and Text
- **Arxiv ID**: http://arxiv.org/abs/1706.05461v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05461v3)
- **Published**: 2017-06-17 00:39:04+00:00
- **Updated**: 2017-07-10 00:44:45+00:00
- **Authors**: Zhe Wang, Kingsley Kuan, Mathieu Ravaut, Gaurav Manek, Sibo Song, Yuan Fang, Seokhwan Kim, Nancy Chen, Luis Fernando D'Haro, Luu Anh Tuan, Hongyuan Zhu, Zeng Zeng, Ngai Man Cheung, Georgios Piliouras, Jie Lin, Vijay Chandrasekhar
- **Comment**: 8 pages, Accepted to CVPR'17 Workshop on YouTube-8M Large-Scale Video
  Understanding
- **Journal**: None
- **Summary**: The YouTube-8M video classification challenge requires teams to classify 0.7 million videos into one or more of 4,716 classes. In this Kaggle competition, we placed in the top 3% out of 650 participants using released video and audio features. Beyond that, we extend the original competition by including text information in the classification, making this a truly multi-modal approach with vision, audio and text. The newly introduced text data is termed as YouTube-8M-Text. We present a classification framework for the joint use of text, visual and audio features, and conduct an extensive set of experiments to quantify the benefit that this additional mode brings. The inclusion of text yields state-of-the-art results, e.g. 86.7% GAP on the YouTube-8M-Text validation dataset.



### Variants of RMSProp and Adagrad with Logarithmic Regret Bounds
- **Arxiv ID**: http://arxiv.org/abs/1706.05507v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.05507v2)
- **Published**: 2017-06-17 09:48:55+00:00
- **Updated**: 2017-11-28 18:47:37+00:00
- **Authors**: Mahesh Chandra Mukkamala, Matthias Hein
- **Comment**: ICML 2017, 16 pages, 23 figures
- **Journal**: None
- **Summary**: Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neural networks. In this paper we have analyzed RMSProp, originally proposed for the training of deep neural networks, in the context of online convex optimization and show $\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks.



### Place recognition: An Overview of Vision Perspective
- **Arxiv ID**: http://arxiv.org/abs/1707.03470v2
- **DOI**: 10.3390/app8112257
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03470v2)
- **Published**: 2017-06-17 11:34:34+00:00
- **Updated**: 2018-12-30 14:26:37+00:00
- **Authors**: Zhiqiang Zeng, Jian Zhang, Xiaodong Wang, Yuming Chen, Chaoyang Zhu
- **Comment**: Applied Sciences (2018)
- **Journal**: None
- **Summary**: Place recognition is one of the most fundamental topics in computer vision and robotics communities, where the task is to accurately and efficiently recognize the location of a given query image. Despite years of wisdom accumulated in this field, place recognition still remains an open problem due to the various ways in which the appearance of real-world places may differ. This paper presents an overview of the place recognition literature. Since condition invariant and viewpoint invariant features are essential factors to long-term robust visual place recognition system, We start with traditional image description methodology developed in the past, which exploit techniques from image retrieval field. Recently, the rapid advances of related fields such as object detection and image classification have inspired a new technique to improve visual place recognition system, i.e., convolutional neural networks (CNNs). Thus we then introduce recent progress of visual place recognition system based on CNNs to automatically learn better image representations for places. Eventually, we close with discussions and future work of place recognition.



### Rotation Invariance Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1706.05534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05534v1)
- **Published**: 2017-06-17 13:33:29+00:00
- **Updated**: 2017-06-17 13:33:29+00:00
- **Authors**: Shiyuan Li
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: Rotation invariance and translation invariance have great values in image recognition tasks. In this paper, we bring a new architecture in convolutional neural network (CNN) named cyclic convolutional layer to achieve rotation invariance in 2-D symbol recognition. We can also get the position and orientation of the 2-D symbol by the network to achieve detection purpose for multiple non-overlap target. Last but not least, this architecture can achieve one-shot learning in some cases using those invariance.



### Machine Learning in Appearance-based Robot Self-localization
- **Arxiv ID**: http://arxiv.org/abs/1707.03469v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1707.03469v2)
- **Published**: 2017-06-17 15:32:53+00:00
- **Updated**: 2017-10-04 22:17:24+00:00
- **Authors**: Alexander Kuleshov, Alexander Bernstein, Evgeny Burnaev, Yury Yanovich
- **Comment**: 7 pages, 3 figures, ICMLA 2017 conference
- **Journal**: None
- **Summary**: An appearance-based robot self-localization problem is considered in the machine learning framework. The appearance space is composed of all possible images, which can be captured by a robot's visual system under all robot localizations. Using recent manifold learning and deep learning techniques, we propose a new geometrically motivated solution based on training data consisting of a finite set of images captured in known locations of the robot. The solution includes estimation of the robot localization mapping from the appearance space to the robot localization space, as well as estimation of the inverse mapping for modeling visual image features. The latter allows solving the robot localization problem as the Kalman filtering problem.



### Rethinking Atrous Convolution for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1706.05587v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05587v3)
- **Published**: 2017-06-17 22:48:57+00:00
- **Updated**: 2017-12-05 18:06:21+00:00
- **Authors**: Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam
- **Comment**: Add more experimental results
- **Journal**: None
- **Summary**: In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.



