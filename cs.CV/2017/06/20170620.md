# Arxiv Papers in cs.CV on 2017-06-20
### A Bayesian algorithm for detecting identity matches and fraud in image databases
- **Arxiv ID**: http://arxiv.org/abs/1706.06230v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.06230v1)
- **Published**: 2017-06-20 00:43:22+00:00
- **Updated**: 2017-06-20 00:43:22+00:00
- **Authors**: Gaurav Thakur
- **Comment**: None
- **Journal**: None
- **Summary**: A statistical algorithm for categorizing different types of matches and fraud in image databases is presented. The approach is based on a generative model of a graph representing images and connections between pairs of identities, trained using properties of a matching algorithm between images.



### Individual Recognition in Schizophrenia using Deep Learning Methods with Random Forest and Voting Classifiers: Insights from Resting State EEG Streams
- **Arxiv ID**: http://arxiv.org/abs/1707.03467v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03467v2)
- **Published**: 2017-06-20 01:23:24+00:00
- **Updated**: 2018-01-17 13:06:39+00:00
- **Authors**: Lei Chu, Robert Qiu, Haichun Liu, Zenan Ling, Tianhong Zhang, Jijun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, there has been a growing interest in monitoring brain activity for individual recognition system. So far these works are mainly focussing on single channel data or fragment data collected by some advanced brain monitoring modalities. In this study we propose new individual recognition schemes based on spatio-temporal resting state Electroencephalography (EEG) data. Besides, instead of using features derived from artificially-designed procedures, modified deep learning architectures which aim to automatically extract an individual's unique features are developed to conduct classification. Our designed deep learning frameworks are proved of a small but consistent advantage of replacing the $softmax$ layer with Random Forest. Additionally, a voting layer is added at the top of designed neural networks in order to tackle the classification problem arisen from EEG streams. Lastly, various experiments are implemented to evaluate the performance of the designed deep learning architectures; Results indicate that the proposed EEG-based individual recognition scheme yields a high degree of classification accuracy: $81.6\%$ for characteristics in high risk (CHR) individuals, $96.7\%$ for clinically stable first episode patients with schizophrenia (FES) and $99.2\%$ for healthy controls (HC).



### Low Resolution Face Recognition Using a Two-Branch Deep Convolutional Neural Network Architecture
- **Arxiv ID**: http://arxiv.org/abs/1706.06247v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06247v1)
- **Published**: 2017-06-20 02:54:52+00:00
- **Updated**: 2017-06-20 02:54:52+00:00
- **Authors**: Erfan Zangeneh, Mohammad Rahmati, Yalda Mohsenzadeh
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: We propose a novel couple mappings method for low resolution face recognition using deep convolutional neural networks (DCNNs). The proposed architecture consists of two branches of DCNNs to map the high and low resolution face images into a common space with nonlinear transformations. The branch corresponding to transformation of high resolution images consists of 14 layers and the other branch which maps the low resolution face images to the common space includes a 5-layer super-resolution network connected to a 14-layer network. The distance between the features of corresponding high and low resolution images are backpropagated to train the networks. Our proposed method is evaluated on FERET data set and compared with state-of-the-art competing methods. Our extensive experimental results show that the proposed method significantly improves the recognition performance especially for very low resolution probe face images (11.4% improvement in recognition accuracy). Furthermore, it can reconstruct a high resolution image from its corresponding low resolution probe image which is comparable with state-of-the-art super-resolution methods in terms of visual quality.



### Learning-based Ensemble Average Propagator Estimation
- **Arxiv ID**: http://arxiv.org/abs/1706.06258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06258v1)
- **Published**: 2017-06-20 03:55:50+00:00
- **Updated**: 2017-06-20 03:55:50+00:00
- **Authors**: Chuyang Ye
- **Comment**: Accepted by MICCAI 2017
- **Journal**: None
- **Summary**: By capturing the anisotropic water diffusion in tissue, diffusion magnetic resonance imaging (dMRI) provides a unique tool for noninvasively probing the tissue microstructure and orientation in the human brain. The diffusion profile can be described by the ensemble average propagator (EAP), which is inferred from observed diffusion signals. However, accurate EAP estimation using the number of diffusion gradients that is clinically practical can be challenging. In this work, we propose a deep learning algorithm for EAP estimation, which is named learning-based ensemble average propagator estimation (LEAPE). The EAP is commonly represented by a basis and its associated coefficients, and here we choose the SHORE basis and design a deep network to estimate the coefficients. The network comprises two cascaded components. The first component is a multiple layer perceptron (MLP) that simultaneously predicts the unknown coefficients. However, typical training loss functions, such as mean squared errors, may not properly represent the geometry of the possibly non-Euclidean space of the coefficients, which in particular causes problems for the extraction of directional information from the EAP. Therefore, to regularize the training, in the second component we compute an auxiliary output of approximated fiber orientation (FO) errors with the aid of a second MLP that is trained separately. We performed experiments using dMRI data that resemble clinically achievable $q$-space sampling, and observed promising results compared with the conventional EAP estimation method.



### Multi-frame image super-resolution with fast upscaling technique
- **Arxiv ID**: http://arxiv.org/abs/1706.06266v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06266v2)
- **Published**: 2017-06-20 04:51:23+00:00
- **Updated**: 2017-10-13 08:27:36+00:00
- **Authors**: Longguang Wang, Zaiping Lin, Xinpu Deng, Wei An
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-frame image super-resolution (MISR) aims to fuse information in low-resolution (LR) image sequence to compose a high-resolution (HR) one, which is applied extensively in many areas recently. Different with single image super-resolution (SISR), sub-pixel transitions between multiple frames introduce additional information, attaching more significance to fusion operator to alleviate the ill-posedness of MISR. For reconstruction-based approaches, the inevitable projection of reconstruction errors from LR space to HR space is commonly tackled by an interpolation operator, however crude interpolation may not fit the natural image and generate annoying blurring artifacts, especially after fusion operator. In this paper, we propose an end-to-end fast upscaling technique to replace the interpolation operator, design upscaling filters in LR space for periodic sub-locations respectively and shuffle the filter results to derive the final reconstruction errors in HR space. The proposed fast upscaling technique not only reduce the computational complexity of the upscaling operation by utilizing shuffling operation to avoid complex operation in HR space, but also realize superior performance with fewer blurring artifacts. Extensive experimental results demonstrate the effectiveness and efficiency of the proposed technique, whilst, combining the proposed technique with bilateral total variation (BTV) regu-larization, the MISR approach outperforms state-of-the-art methods.



### Using Artificial Tokens to Control Languages for Multilingual Image Caption Generation
- **Arxiv ID**: http://arxiv.org/abs/1706.06275v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06275v1)
- **Published**: 2017-06-20 05:50:49+00:00
- **Updated**: 2017-06-20 05:50:49+00:00
- **Authors**: Satoshi Tsutsui, David Crandall
- **Comment**: This work appears as an Extended Abstract at the 2017 CVPR Language
  and Vision Workshop
- **Journal**: None
- **Summary**: Recent work in computer vision has yielded impressive results in automatically describing images with natural language. Most of these systems generate captions in a sin- gle language, requiring multiple language-specific models to build a multilingual captioning system. We propose a very simple technique to build a single unified model across languages, using artificial tokens to control the language, making the captioning system more compact. We evaluate our approach on generating English and Japanese captions, and show that a typical neural captioning architecture is capable of learning a single model that can switch between two different languages.



### SPLBoost: An Improved Robust Boosting Algorithm Based on Self-paced Learning
- **Arxiv ID**: http://arxiv.org/abs/1706.06341v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.06341v2)
- **Published**: 2017-06-20 09:31:30+00:00
- **Updated**: 2017-06-23 14:04:46+00:00
- **Authors**: Kaidong Wang, Yao Wang, Qian Zhao, Deyu Meng, Zongben Xu
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: It is known that Boosting can be interpreted as a gradient descent technique to minimize an underlying loss function. Specifically, the underlying loss being minimized by the traditional AdaBoost is the exponential loss, which is proved to be very sensitive to random noise/outliers. Therefore, several Boosting algorithms, e.g., LogitBoost and SavageBoost, have been proposed to improve the robustness of AdaBoost by replacing the exponential loss with some designed robust loss functions. In this work, we present a new way to robustify AdaBoost, i.e., incorporating the robust learning idea of Self-paced Learning (SPL) into Boosting framework. Specifically, we design a new robust Boosting algorithm based on SPL regime, i.e., SPLBoost, which can be easily implemented by slightly modifying off-the-shelf Boosting packages. Extensive experiments and a theoretical characterization are also carried out to illustrate the merits of the proposed SPLBoost.



### Clustering-Based Quantisation for PDE-Based Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1706.06347v1
- **DOI**: None
- **Categories**: **cs.CV**, 65N99, 90C25, 97N50
- **Links**: [PDF](http://arxiv.org/pdf/1706.06347v1)
- **Published**: 2017-06-20 10:02:05+00:00
- **Updated**: 2017-06-20 10:02:05+00:00
- **Authors**: Laurent Hoeltgen, Pascal Peter, Michael Breuß
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Finding optimal data for inpainting is a key problem in the context of partial differential equation based image compression. The data that yields the most accurate reconstruction is real-valued. Thus, quantisation models are mandatory to allow an efficient encoding. These can also be understood as challenging data clustering problems. Although clustering approaches are well suited for this kind of compression codecs, very few works actually consider them. Each pixel has a global impact on the reconstruction and optimal data locations are strongly correlated with their corresponding colour values. These facts make it hard to predict which feature works best.   In this paper we discuss quantisation strategies based on popular methods such as k-means. We are lead to the central question which kind of feature vectors are best suited for image compression. To this end we consider choices such as the pixel values, the histogram or the colour map.   Our findings show that the number of colours can be reduced significantly without impacting the reconstruction quality. Surprisingly, these benefits do not directly translate to a good image compression performance. The gains in the compression ratio are lost due to increased storage costs. This suggests that it is integral to evaluate the clustering on both, the reconstruction error and the final file size.



### Revisiting L21-norm Robustness with Vector Outlier Regularization
- **Arxiv ID**: http://arxiv.org/abs/1706.06409v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06409v2)
- **Published**: 2017-06-20 13:20:11+00:00
- **Updated**: 2020-01-09 07:45:27+00:00
- **Authors**: Bo Jiang, Chris Ding
- **Comment**: None
- **Journal**: None
- **Summary**: In many real-world applications, data usually contain outliers. One popular approach is to use L2,1 norm function as a robust error/loss function. However, the robustness of L2,1 norm function is not well understood so far. In this paper, we propose a new Vector Outlier Regularization (VOR) framework to understand and analyze the robustness of L2,1 norm function. Our VOR function defines a data point to be outlier if it is outside a threshold with respect to a theoretical prediction, and regularize it-pull it back to the threshold line. We then prove that L2,1 function is the limiting case of this VOR with the usual least square/L2 error function as the threshold shrinks to zero. One interesting property of VOR is that how far an outlier lies away from its theoretically predicted value does not affect the final regularization and analysis results. This VOR property unmasks one of the most peculiar property of L2,1 norm function: The effects of outliers seem to be independent of how outlying they are-if an outlier is moved further away from the intrinsic manifold/subspace, the final analysis results do not change. VOR provides a new way to understand and analyze the robustness of L2,1 norm function. Applying VOR to matrix factorization leads to a new VORPCA model. We give a comprehensive comparison with trace-norm based L21-norm PCA to demonstrate the advantages of VORPCA.



### Advanced Steel Microstructural Classification by Deep Learning Methods
- **Arxiv ID**: http://arxiv.org/abs/1706.06480v2
- **DOI**: 10.1038/s41598-018-20037-5
- **Categories**: **cs.CV**, cond-mat.mtrl-sci
- **Links**: [PDF](http://arxiv.org/pdf/1706.06480v2)
- **Published**: 2017-06-20 14:29:42+00:00
- **Updated**: 2018-02-15 14:30:16+00:00
- **Authors**: Seyed Majid Azimi, Dominik Britz, Michael Engstler, Mario Fritz, Frank Mücklich
- **Comment**: Published in Nature - Scientific Reports Journal
- **Journal**: None
- **Summary**: The inner structure of a material is called microstructure. It stores the genesis of a material and determines all its physical and chemical properties. While microstructural characterization is widely spread and well known, the microstructural classification is mostly done manually by human experts, which gives rise to uncertainties due to subjectivity. Since the microstructure could be a combination of different phases or constituents with complex substructures its automatic classification is very challenging and only a few prior studies exist. Prior works focused on designed and engineered features by experts and classified microstructures separately from the feature extraction step. Recently, Deep Learning methods have shown strong performance in vision applications by learning the features from data together with the classification step. In this work, we propose a Deep Learning method for microstructural classification in the examples of certain microstructural constituents of low carbon steel. This novel method employs pixel-wise segmentation via Fully Convolutional Neural Networks (FCNN) accompanied by a max-voting scheme. Our system achieves 93.94% classification accuracy, drastically outperforming the state-of-the-art method of 48.89% accuracy. Beyond the strong performance of our method, this line of research offers a more robust and first of all objective way for the difficult task of steel quality appreciation.



### Recovering Dense Tissue Multispectral Signal from in vivo RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1707.03468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03468v1)
- **Published**: 2017-06-20 15:38:44+00:00
- **Updated**: 2017-06-20 15:38:44+00:00
- **Authors**: Jianyu Lin, Neil T. Clancy, Daniel S. Elson
- **Comment**: accepted by Hamlyn Symposium 2017
- **Journal**: None
- **Summary**: Hyperspectral/multispectral imaging (HSI/MSI) contains rich information clinical applications, such as 1) narrow band imaging for vascular visualisation; 2) oxygen saturation for intraoperative perfusion monitoring and clinical decision making [1]; 3) tissue classification and identification of pathology [2]. The current systems which provide pixel-level HSI/MSI signal can be generally divided into two types: spatial scanning and spectral scanning. However, the trade-off between spatial/spectral resolution, the acquisition time, and the hardware complexity hampers implementation in real-world applications, especially intra-operatively. Acquiring high resolution images in real-time is important for HSI/MSI in intra-operative imaging, to alleviate the side effect caused by breathing, heartbeat, and other sources of motion. Therefore, we developed an algorithm to recover a pixel-level MSI stack using only the captured snapshot RGB images from a normal camera. We refer to this technique as "super-spectral-resolution". The proposed method enables recovery of pixel-level-dense MSI signals with 24 spectral bands at ~11 frames per second (FPS) on a GPU. Multispectral data captured from porcine bowel and sheep/rabbit uteri in vivo has been used for training, and the algorithm has been validated using unseen in vivo animal experiments.



### A comparative study of breast surface reconstruction for aesthetic outcome assessment
- **Arxiv ID**: http://arxiv.org/abs/1706.06531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06531v1)
- **Published**: 2017-06-20 16:10:30+00:00
- **Updated**: 2017-06-20 16:10:30+00:00
- **Authors**: Rene Lacher, Francisco Vasconcelos, David Bishop, Norman Williams, Mohammed Keshtgar, David Hawkes, John Hipwell, Danail Stoyanov
- **Comment**: This paper has been accepted to MICCAI2017
- **Journal**: None
- **Summary**: Breast cancer is the most prevalent cancer type in women, and while its survival rate is generally high the aesthetic outcome is an increasingly important factor when evaluating different treatment alternatives. 3D scanning and reconstruction techniques offer a flexible tool for building detailed and accurate 3D breast models that can be used both pre-operatively for surgical planning and post-operatively for aesthetic evaluation. This paper aims at comparing the accuracy of low-cost 3D scanning technologies with the significantly more expensive state-of-the-art 3D commercial scanners in the context of breast 3D reconstruction. We present results from 28 synthetic and clinical RGBD sequences, including 12 unique patients and an anthropomorphic phantom demonstrating the applicability of low-cost RGBD sensors to real clinical cases. Body deformation and homogeneous skin texture pose challenges to the studied reconstruction systems. Although these should be addressed appropriately if higher model quality is warranted, we observe that low-cost sensors are able to obtain valuable reconstructions comparable to the state-of-the-art within an error margin of 3 mm.



### Co-Fusion: Real-time Segmentation, Tracking and Fusion of Multiple Objects
- **Arxiv ID**: http://arxiv.org/abs/1706.06629v1
- **DOI**: 10.1109/ICRA.2017.7989518
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06629v1)
- **Published**: 2017-06-20 19:10:31+00:00
- **Updated**: 2017-06-20 19:10:31+00:00
- **Authors**: Martin Rünz, Lourdes Agapito
- **Comment**: International Conference on Robotics and Automation (ICRA) 2017,
  http://visual.cs.ucl.ac.uk/pubs/cofusion,
  https://github.com/martinruenz/co-fusion
- **Journal**: None
- **Summary**: In this paper we introduce Co-Fusion, a dense SLAM system that takes a live stream of RGB-D images as input and segments the scene into different objects (using either motion or semantic cues) while simultaneously tracking and reconstructing their 3D shape in real time. We use a multiple model fitting approach where each object can move independently from the background and still be effectively tracked and its shape fused over time using only the information from pixels associated with that object label. Previous attempts to deal with dynamic scenes have typically considered moving regions as outliers, and consequently do not model their shape or track their motion over time. In contrast, we enable the robot to maintain 3D models for each of the segmented objects and to improve them over time through fusion. As a result, our system can enable a robot to maintain a scene description at the object level which has the potential to allow interactions with its working environment; even in the case of dynamic scenes.



### Passive Classification of Source Printer using Text-line-level Geometric Distortion Signatures from Scanned Images of Printed Documents
- **Arxiv ID**: http://arxiv.org/abs/1706.06651v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.06651v1)
- **Published**: 2017-06-20 20:11:45+00:00
- **Updated**: 2017-06-20 20:11:45+00:00
- **Authors**: Hardik Jain, Gaurav Gupta, Sharad Joshi, Nitin Khanna
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: In this digital era, one thing that still holds the convention is a printed archive. Printed documents find their use in many critical domains such as contract papers, legal tenders and proof of identity documents. As more advanced printing, scanning and image editing techniques are becoming available, forgeries on these legal tenders pose a serious threat. Ability to easily and reliably identify source printer of a printed document can help a lot in reducing this menace. During printing procedure, printer hardware introduces certain distortions in printed characters' locations and shapes which are invisible to naked eyes. These distortions are referred as geometric distortions, their profile (or signature) is generally unique for each printer and can be used for printer classification purpose. This paper proposes a set of features for characterizing text-line-level geometric distortions, referred as geometric distortion signatures and presents a novel system to use them for identification of the origin of a printed document. Detailed experiments performed on a set of thirteen printers demonstrate that the proposed system achieves state of the art performance and gives much higher accuracy under small training size constraint. For four training and six test pages of three different fonts, the proposed method gives 99\% classification accuracy.



### Chemception: A Deep Neural Network with Minimal Chemistry Knowledge Matches the Performance of Expert-developed QSAR/QSPR Models
- **Arxiv ID**: http://arxiv.org/abs/1706.06689v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CE, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.06689v1)
- **Published**: 2017-06-20 22:25:57+00:00
- **Updated**: 2017-06-20 22:25:57+00:00
- **Authors**: Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas, Nathan Baker
- **Comment**: Submitted to a chemistry peer-reviewed journal
- **Journal**: None
- **Summary**: In the last few years, we have seen the transformative impact of deep learning in many applications, particularly in speech recognition and computer vision. Inspired by Google's Inception-ResNet deep convolutional neural network (CNN) for image classification, we have developed "Chemception", a deep CNN for the prediction of chemical properties, using just the images of 2D drawings of molecules. We develop Chemception without providing any additional explicit chemistry knowledge, such as basic concepts like periodicity, or advanced features like molecular descriptors and fingerprints. We then show how Chemception can serve as a general-purpose neural network architecture for predicting toxicity, activity, and solvation properties when trained on a modest database of 600 to 40,000 compounds. When compared to multi-layer perceptron (MLP) deep neural networks trained with ECFP fingerprints, Chemception slightly outperforms in activity and solvation prediction and slightly underperforms in toxicity prediction. Having matched the performance of expert-developed QSAR/QSPR deep learning models, our work demonstrates the plausibility of using deep neural networks to assist in computational chemistry research, where the feature engineering process is performed primarily by a deep learning algorithm.



### Highly curved image sensors: a practical approach for improved optical performance
- **Arxiv ID**: http://arxiv.org/abs/1706.07041v1
- **DOI**: 10.1364/OE.25.013010
- **Categories**: **physics.ins-det**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.07041v1)
- **Published**: 2017-06-20 22:37:17+00:00
- **Updated**: 2017-06-20 22:37:17+00:00
- **Authors**: Brian Guenter, Neel Joshi, Richard Stoakley, Andrew Keefe, Kevin Geary, Ryan Freeman, Jake Hundley, Pamela Patterson, David Hammon, Guillermo Herrera, Elena Sherman, Andrew Nowak, Randall Schubert, Peter Brewer, Louis Yang, Russell Mott, Geoff McKnight
- **Comment**: None
- **Journal**: Opt. Express 25, 13010-13023 (2017)
- **Summary**: The significant optical and size benefits of using a curved focal surface for imaging systems have been well studied yet never brought to market for lack of a high-quality, mass-producible, curved image sensor. In this work we demonstrate that commercial silicon CMOS image sensors can be thinned and formed into accurate, highly curved optical surfaces with undiminished functionality. Our key development is a pneumatic forming process that avoids rigid mechanical constraints and suppresses wrinkling instabilities. A combination of forming-mold design, pressure membrane elastic properties, and controlled friction forces enables us to gradually contact the die at the corners and smoothly press the sensor into a spherical shape. Allowing the die to slide into the concave target shape enables a threefold increase in the spherical curvature over prior approaches having mechanical constraints that resist deformation, and create a high-stress, stretch-dominated state. Our process creates a bridge between the high precision and low-cost but planar CMOS process, and ideal non-planar component shapes such as spherical imagers for improved optical systems. We demonstrate these curved sensors in prototype cameras with custom lenses, measuring exceptional resolution of 3220 line-widths per picture height at an aperture of f/1.2 and nearly 100% relative illumination across the field. Though we use a 1/2.3" format image sensor in this report, we also show this process is generally compatible with many state of the art imaging sensor formats. By example, we report photogrammetry test data for an APS-C sized silicon die formed to a 30$^\circ$ subtended spherical angle. These gains in sharpness and relative illumination enable a new generation of ultra-high performance, manufacturable, digital imaging systems for scientific, industrial, and artistic use.



### Recognition of Grasp Points for Clothes Manipulation under unconstrained Conditions
- **Arxiv ID**: http://arxiv.org/abs/1706.06694v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1706.06694v1)
- **Published**: 2017-06-20 22:51:39+00:00
- **Updated**: 2017-06-20 22:51:39+00:00
- **Authors**: Luz María Martínez, Javier Ruiz-del-Solar
- **Comment**: Accepted in the RoboCup Symposium 2017. Final version will be
  published at Springer
- **Journal**: None
- **Summary**: In this work a system for recognizing grasp points in RGB-D images is proposed. This system is intended to be used by a domestic robot when deploying clothes lying at a random position on a table. By taking into consideration that the grasp points are usually near key parts of clothing, such as the waist of pants or the neck of a shirt. The proposed system attempts to detect these key parts first, using a local multivariate contour that adapts its shape accordingly. Then, the proposed system applies the Vessel Enhancement filter to identify wrinkles in the clothes, allowing to compute a roughness index for the clothes. Finally, by mixing (i) the key part contours and (ii) the roughness information obtained by the vessel filter, the system is able to recognize grasp points for unfolding a piece of clothing. The recognition system is validated using realistic RGB-D images of different cloth types.



### Using Convolutional Neural Networks in Robots with Limited Computational Resources: Detecting NAO Robots while Playing Soccer
- **Arxiv ID**: http://arxiv.org/abs/1706.06702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06702v1)
- **Published**: 2017-06-20 23:29:49+00:00
- **Updated**: 2017-06-20 23:29:49+00:00
- **Authors**: Nicolás Cruz, Kenzo Lobos-Tsunekawa, Javier Ruiz-del-Solar
- **Comment**: Accepted in the RoboCup Symposium 2017. Final version will be
  published at Springer
- **Journal**: None
- **Summary**: The main goal of this paper is to analyze the general problem of using Convolutional Neural Networks (CNNs) in robots with limited computational capabilities, and to propose general design guidelines for their use. In addition, two different CNN based NAO robot detectors that are able to run in real-time while playing soccer are proposed. One of the detectors is based on the XNOR-Net and the other on the SqueezeNet. Each detector is able to process a robot object-proposal in ~1ms, with an average number of 1.5 proposals per frame obtained by the upper camera of the NAO. The obtained detection rate is ~97%.



### Compact Tensor Pooling for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1706.06706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06706v1)
- **Published**: 2017-06-20 23:55:32+00:00
- **Updated**: 2017-06-20 23:55:32+00:00
- **Authors**: Yang Shi, Tommaso Furlanello, Anima Anandkumar
- **Comment**: None
- **Journal**: None
- **Summary**: Performing high level cognitive tasks requires the integration of feature maps with drastically different structure. In Visual Question Answering (VQA) image descriptors have spatial structures, while lexical inputs inherently follow a temporal sequence. The recently proposed Multimodal Compact Bilinear pooling (MCB) forms the outer products, via count-sketch approximation, of the visual and textual representation at each spatial location. While this procedure preserves spatial information locally, outer-products are taken independently for each fiber of the activation tensor, and therefore do not include spatial context. In this work, we introduce multi-dimensional sketch ({MD-sketch}), a novel extension of count-sketch to tensors. Using this new formulation, we propose Multimodal Compact Tensor Pooling (MCT) to fully exploit the global spatial context during bilinear pooling operations. Contrarily to MCB, our approach preserves spatial context by directly convolving the MD-sketch from the visual tensor features with the text vector feature using higher order FFT. Furthermore we apply MCT incrementally at each step of the question embedding and accumulate the multi-modal vectors with a second LSTM layer before the final answer is chosen.



