# Arxiv Papers in cs.CV on 2017-06-22
### Balanced Quantization: An Effective and Efficient Approach to Quantized Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.07145v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.07145v1)
- **Published**: 2017-06-22 01:25:37+00:00
- **Updated**: 2017-06-22 01:25:37+00:00
- **Authors**: Shuchang Zhou, Yuzhi Wang, He Wen, Qinyao He, Yuheng Zou
- **Comment**: None
- **Journal**: None
- **Summary**: Quantized Neural Networks (QNNs), which use low bitwidth numbers for representing parameters and performing computations, have been proposed to reduce the computation complexity, storage size and memory usage. In QNNs, parameters and activations are uniformly quantized, such that the multiplications and additions can be accelerated by bitwise operations. However, distributions of parameters in Neural Networks are often imbalanced, such that the uniform quantization determined from extremal values may under utilize available bitwidth. In this paper, we propose a novel quantization method that can ensure the balance of distributions of quantized values. Our method first recursively partitions the parameters by percentiles into balanced bins, and then applies uniform quantization. We also introduce computationally cheaper approximations of percentiles to reduce the computation overhead introduced. Overall, our method improves the prediction accuracies of QNNs without introducing extra computation during inference, has negligible impact on training speed, and is applicable to both Convolutional Neural Networks and Recurrent Neural Networks. Experiments on standard datasets including ImageNet and Penn Treebank confirm the effectiveness of our method. On ImageNet, the top-5 error rate of our 4-bit quantized GoogLeNet model is 12.7\%, which is superior to the state-of-the-arts of QNNs.



### Personalized Automatic Estimation of Self-reported Pain Intensity from Facial Expressions
- **Arxiv ID**: http://arxiv.org/abs/1706.07154v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07154v2)
- **Published**: 2017-06-22 03:11:29+00:00
- **Updated**: 2017-06-24 00:04:06+00:00
- **Authors**: Daniel Lopez Martinez, Ognjen Rudovic, Rosalind Picard
- **Comment**: Computer Vision and Pattern Recognition Conference, The 1st
  International Workshop on Deep Affective Learning and Context Modeling
- **Journal**: None
- **Summary**: Pain is a personal, subjective experience that is commonly evaluated through visual analog scales (VAS). While this is often convenient and useful, automatic pain detection systems can reduce pain score acquisition efforts in large-scale studies by estimating it directly from the participants' facial expressions. In this paper, we propose a novel two-stage learning approach for VAS estimation: first, our algorithm employs Recurrent Neural Networks (RNNs) to automatically estimate Prkachin and Solomon Pain Intensity (PSPI) levels from face images. The estimated scores are then fed into the personalized Hidden Conditional Random Fields (HCRFs), used to estimate the VAS, provided by each person. Personalization of the model is performed using a newly introduced facial expressiveness score, unique for each person. To the best of our knowledge, this is the first approach to automatically estimate VAS from face images. We show the benefits of the proposed personalized over traditional non-personalized approach on a benchmark dataset for pain analysis from face images.



### Comparison of Time-Frequency Representations for Environmental Sound Classification using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.07156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07156v1)
- **Published**: 2017-06-22 03:23:09+00:00
- **Updated**: 2017-06-22 03:23:09+00:00
- **Authors**: M. Huzaifah
- **Comment**: None
- **Journal**: None
- **Summary**: Recent successful applications of convolutional neural networks (CNNs) to audio classification and speech recognition have motivated the search for better input representations for more efficient training. Visual displays of an audio signal, through various time-frequency representations such as spectrograms offer a rich representation of the temporal and spectral structure of the original signal. In this letter, we compare various popular signal processing methods to obtain this representation, such as short-time Fourier transform (STFT) with linear and Mel scales, constant-Q transform (CQT) and continuous Wavelet transform (CWT), and assess their impact on the classification performance of two environmental sound datasets using CNNs. This study supports the hypothesis that time-frequency representations are valuable in learning useful features for sound classification. Moreover, the actual transformation used is shown to impact the classification accuracy, with Mel-scaled STFT outperforming the other discussed methods slightly and baseline MFCC features to a large degree. Additionally, we observe that the optimal window size during transformation is dependent on the characteristics of the audio signal and architecturally, 2D convolution yielded better results in most cases compared to 1D.



### A Novel VHR Image Change Detection Algorithm Based on Image Fusion and Fuzzy C-Means Clustering
- **Arxiv ID**: http://arxiv.org/abs/1706.07157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07157v1)
- **Published**: 2017-06-22 03:25:48+00:00
- **Updated**: 2017-06-22 03:25:48+00:00
- **Authors**: Rongcui Dong, Haoxiang Wang
- **Comment**: 9 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: This thesis describes a study to perform change detection on Very High Resolution satellite images using image fusion based on 2D Discrete Wavelet Transform and Fuzzy C-Means clustering algorithm. Multiple other methods are also quantitatively and qualitatively compared in this study.



### Shape recognition of volcanic ash by simple convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1706.07178v1
- **DOI**: None
- **Categories**: **physics.geo-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.07178v1)
- **Published**: 2017-06-22 06:53:19+00:00
- **Updated**: 2017-06-22 06:53:19+00:00
- **Authors**: Daigo Shoji, Rina Noguchi
- **Comment**: 13 pages, 3 figures
- **Journal**: None
- **Summary**: Shape analyses of tephra grains result in understanding eruption mechanism of volcanoes. However, we have to define and select parameter set such as convexity for the precise discrimination of tephra grains. Selection of the best parameter set for the recognition of tephra shapes is complicated. Actually, many shape parameters have been suggested. Recently, neural network has made a great success in the field of machine learning. Convolutional neural network can recognize the shape of images without human bias and shape parameters. We applied the simple convolutional neural network developed for the handwritten digits to the recognition of tephra shapes. The network was trained by Morphologi tephra images, and it can recognize the tephra shapes with approximately 90% of accuracy.



### Synthesis of Near-regular Natural Textures
- **Arxiv ID**: http://arxiv.org/abs/1706.07198v1
- **DOI**: None
- **Categories**: **cs.CV**, 62-07, 93B50
- **Links**: [PDF](http://arxiv.org/pdf/1706.07198v1)
- **Published**: 2017-06-22 07:58:20+00:00
- **Updated**: 2017-06-22 07:58:20+00:00
- **Authors**: V. Asha
- **Comment**: 5 Pages, 10 Figures, IJCRD-5(1), 2016
- **Journal**: None
- **Summary**: Texture synthesis is widely used in the field of computer graphics, vision, and image processing. In the present paper, a texture synthesis algorithm is proposed for near-regular natural textures with the help of a representative periodic pattern extracted from the input textures using distance matching function. Local texture statistics is then analyzed against global texture statistics for non-overlapping windows of size same as periodic pattern size and a representative periodic pattern is extracted from the image and used for texture synthesis, while preserving the global regularity and visual appearance. Validation of the algorithm based on experiments with synthetic textures whose periodic pattern sizes are known and containing camouflages / defects proves the strength of the algorithm for texture synthesis and its application in detection of camouflages / defects in textures.



### A Self-Adaptive Proposal Model for Temporal Action Detection based on Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1706.07251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07251v1)
- **Published**: 2017-06-22 10:59:33+00:00
- **Updated**: 2017-06-22 10:59:33+00:00
- **Authors**: Jingjia Huang, Nannan Li, Tao Zhang, Ge Li
- **Comment**: Deep Reinforcement Learning, Action Temporal Detection, Temporal
  Location Regression
- **Journal**: None
- **Summary**: Existing action detection algorithms usually generate action proposals through an extensive search over the video at multiple temporal scales, which brings about huge computational overhead and deviates from the human perception procedure. We argue that the process of detecting actions should be naturally one of observation and refinement: observe the current window and refine the span of attended window to cover true action regions. In this paper, we propose an active action proposal model that learns to find actions through continuously adjusting the temporal bounds in a self-adaptive way. The whole process can be deemed as an agent, which is firstly placed at a position in the video at random, adopts a sequence of transformations on the current attended region to discover actions according to a learned policy. We utilize reinforcement learning, especially the Deep Q-learning algorithm to learn the agent's decision policy. In addition, we use temporal pooling operation to extract more effective feature representation for the long temporal window, and design a regression network to adjust the position offsets between predicted results and the ground truth. Experiment results on THUMOS 2014 validate the effectiveness of the proposed approach, which can achieve competitive performance with current action detection algorithms via much fewer proposals.



### Fast Estimation of Haemoglobin Concentration in Tissue Via Wavelet Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1706.07263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07263v1)
- **Published**: 2017-06-22 11:32:09+00:00
- **Updated**: 2017-06-22 11:32:09+00:00
- **Authors**: Geoffrey Jones, Neil T Clancy, Xiaofei Du, Maria Robu, Simon Arridge, Daniel S Elson, Danail Stoyanov
- **Comment**: None
- **Journal**: None
- **Summary**: Tissue oxygenation and perfusion can be an indicator for organ viability during minimally invasive surgery, for example allowing real-time assessment of tissue perfusion and oxygen saturation. Multispectral imaging is an optical modality that can inspect tissue perfusion in wide field images without contact. In this paper, we present a novel, fast method for using RGB images for MSI, which while limiting the spectral resolution of the modality allows normal laparoscopic systems to be used. We exploit the discrete Haar decomposition to separate individual video frames into low pass and directional coefficients and we utilise a different multispectral estimation technique on each. The increase in speed is achieved by using fast Tikhonov regularisation on the directional coefficients and more accurate Bayesian estimation on the low pass component. The pipeline is implemented using a graphics processing unit (GPU) architecture and achieves a frame rate of approximately 15Hz. We validate the method on animal models and on human data captured using a da Vinci stereo laparoscope.



### A Computer Vision Pipeline for Automated Determination of Cardiac Structure and Function and Detection of Disease by Two-Dimensional Echocardiography
- **Arxiv ID**: http://arxiv.org/abs/1706.07342v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07342v7)
- **Published**: 2017-06-22 14:39:49+00:00
- **Updated**: 2018-01-12 17:09:23+00:00
- **Authors**: Jeffrey Zhang, Sravani Gajjala, Pulkit Agrawal, Geoffrey H. Tison, Laura A. Hallock, Lauren Beussink-Nelson, Eugene Fan, Mandar A. Aras, ChaRandle Jordan, Kirsten E. Fleischmann, Michelle Melisko, Atif Qasim, Alexei Efros, Sanjiv J. Shah, Ruzena Bajcsy, Rahul C. Deo
- **Comment**: 9 figures, 2 tables
- **Journal**: None
- **Summary**: Automated cardiac image interpretation has the potential to transform clinical practice in multiple ways including enabling low-cost serial assessment of cardiac function in the primary care and rural setting. We hypothesized that advances in computer vision could enable building a fully automated, scalable analysis pipeline for echocardiogram (echo) interpretation. Our approach entailed: 1) preprocessing; 2) convolutional neural networks (CNN) for view identification, image segmentation, and phasing of the cardiac cycle; 3) quantification of chamber volumes and left ventricular mass; 4) particle tracking to compute longitudinal strain; and 5) targeted disease detection. CNNs accurately identified views (e.g. 99% for apical 4-chamber) and segmented individual cardiac chambers. Cardiac structure measurements agreed with study report values (e.g. mean absolute deviations (MAD) of 7.7 mL/kg/m2 for left ventricular diastolic volume index, 2918 studies). We computed automated ejection fraction and longitudinal strain measurements (within 2 cohorts), which agreed with commercial software-derived values [for ejection fraction, MAD=5.3%, N=3101 studies; for strain, MAD=1.5% (n=197) and 1.6% (n=110)], and demonstrated applicability to serial monitoring of breast cancer patients for trastuzumab cardiotoxicity. Overall, we found that, compared to manual measurements, automated measurements had superior performance across seven internal consistency metrics with an average increase in the Spearman correlation coefficient of 0.05 (p=0.02). Finally, we developed disease detection algorithms for hypertrophic cardiomyopathy and cardiac amyloidosis, with C-statistics of 0.93 and 0.84, respectively. Our pipeline lays the groundwork for using automated interpretation to support point-of-care handheld cardiac ultrasound and large-scale analysis of the millions of echos archived within healthcare systems.



### Deep Supervision for Pancreatic Cyst Segmentation in Abdominal CT Scans
- **Arxiv ID**: http://arxiv.org/abs/1706.07346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07346v1)
- **Published**: 2017-06-22 14:46:16+00:00
- **Updated**: 2017-06-22 14:46:16+00:00
- **Authors**: Yuyin Zhou, Lingxi Xie, Elliot K. Fishman, Alan L. Yuille
- **Comment**: Accepted to MICCAI 2017 (8 pages, 3 figures)
- **Journal**: None
- **Summary**: Automatic segmentation of an organ and its cystic region is a prerequisite of computer-aided diagnosis. In this paper, we focus on pancreatic cyst segmentation in abdominal CT scan. This task is important and very useful in clinical practice yet challenging due to the low contrast in boundary, the variability in location, shape and the different stages of the pancreatic cancer. Inspired by the high relevance between the location of a pancreas and its cystic region, we introduce extra deep supervision into the segmentation network, so that cyst segmentation can be improved with the help of relatively easier pancreas segmentation. Under a reasonable transformation function, our approach can be factorized into two stages, and each stage can be efficiently optimized via gradient back-propagation throughout the deep networks. We collect a new dataset with 131 pathological samples, which, to the best of our knowledge, is the largest set for pancreatic cyst segmentation. Without human assistance, our approach reports a 63.44% average accuracy, measured by the Dice-S{\o}rensen coefficient (DSC), which is higher than the number (60.46%) without deep supervision.



### Reconstructing the Forest of Lineage Trees of Diverse Bacterial Communities Using Bio-inspired Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1706.07359v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1706.07359v1)
- **Published**: 2017-06-22 15:14:08+00:00
- **Updated**: 2017-06-22 15:14:08+00:00
- **Authors**: Athanasios D. Balomenos, Elias S. Manolakos
- **Comment**: None
- **Journal**: None
- **Summary**: Cell segmentation and tracking allow us to extract a plethora of cell attributes from bacterial time-lapse cell movies, thus promoting computational modeling and simulation of biological processes down to the single-cell level. However, to analyze successfully complex cell movies, imaging multiple interacting bacterial clones as they grow and merge to generate overcrowded bacterial communities with thousands of cells in the field of view, segmentation results should be near perfect to warrant good tracking results. We introduce here a fully automated closed-loop bio-inspired computational strategy that exploits prior knowledge about the expected structure of a colony's lineage tree to locate and correct segmentation errors in analyzed movie frames. We show that this correction strategy is effective, resulting in improved cell tracking and consequently trustworthy deep colony lineage trees. Our image analysis approach has the unique capability to keep tracking cells even after clonal subpopulations merge in the movie. This enables the reconstruction of the complete Forest of Lineage Trees (FLT) representation of evolving multi-clonal bacterial communities. Moreover, the percentage of valid cell trajectories extracted from the image analysis almost doubles after segmentation correction. This plethora of trustworthy data extracted from a complex cell movie analysis enables single-cell analytics as a tool for addressing compelling questions for human health, such as understanding the role of single-cell stochasticity in antibiotics resistance without losing site of the inter-cellular interactions and microenvironment effects that may shape it.



### Tracking Single-Cells in Overcrowded Bacterial Colonies
- **Arxiv ID**: http://arxiv.org/abs/1706.07362v1
- **DOI**: 10.1109/EMBC.2015.7319875
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07362v1)
- **Published**: 2017-06-22 15:19:06+00:00
- **Updated**: 2017-06-22 15:19:06+00:00
- **Authors**: Athanasios D. Balomenos, Panagiotis Tsakanikas, Elias S. Manolakos
- **Comment**: None
- **Journal**: 37th Annual International Conference of the IEEE Engineering in
  Medicine and Biology Society (EMBC), 6473-6476 (2015)
- **Summary**: Cell tracking enables data extraction from time-lapse "cell movies" and promotes modeling biological processes at the single-cell level. We introduce a new fully automated computational strategy to track accurately cells across frames in time-lapse movies. Our method is based on a dynamic neighborhoods formation and matching approach, inspired by motion estimation algorithms for video compression. Moreover, it exploits "divide and conquer" opportunities to solve effectively the challenging cells tracking problem in overcrowded bacterial colonies. Using cell movies generated by different labs we demonstrate that the accuracy of the proposed method remains very high (exceeds 97%) even when analyzing large overcrowded microbial colonies.



### Pixels to Graphs by Associative Embedding
- **Arxiv ID**: http://arxiv.org/abs/1706.07365v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.07365v2)
- **Published**: 2017-06-22 15:20:25+00:00
- **Updated**: 2018-03-27 17:13:31+00:00
- **Authors**: Alejandro Newell, Jia Deng
- **Comment**: Updated numbers. Code and pretrained models available at
  https://github.com/umich-vl/px2graph
- **Journal**: Advances in Neural Information Processing Systems 30 (NIPS 2017)
- **Summary**: Graphs are a useful abstraction of image content. Not only can graphs represent details about individual objects in a scene but they can capture the interactions between pairs of objects. We present a method for training a convolutional neural network such that it takes in an input image and produces a full graph definition. This is done end-to-end in a single stage with the use of associative embeddings. The network learns to simultaneously identify all of the elements that make up a graph and piece them together. We benchmark on the Visual Genome dataset, and demonstrate state-of-the-art performance on the challenging task of scene graph generation.



### On Detection of Faint Edges in Noisy Images
- **Arxiv ID**: http://arxiv.org/abs/1706.07717v1
- **DOI**: 10.1109/TPAMI.2019.2892134
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07717v1)
- **Published**: 2017-06-22 16:37:22+00:00
- **Updated**: 2017-06-22 16:37:22+00:00
- **Authors**: Nati Ofir, Meirav Galun, Sharon Alpert, Achi Brandt, Boaz Nadler, Ronen Basri
- **Comment**: None
- **Journal**: None
- **Summary**: A fundamental question for edge detection in noisy images is how faint can an edge be and still be detected. In this paper we offer a formalism to study this question and subsequently introduce computationally efficient multiscale edge detection algorithms designed to detect faint edges in noisy images. In our formalism we view edge detection as a search in a discrete, though potentially large, set of feasible curves. First, we derive approximate expressions for the detection threshold as a function of curve length and the complexity of the search space. We then present two edge detection algorithms, one for straight edges, and the second for curved ones. Both algorithms efficiently search for edges in a large set of candidates by hierarchically constructing difference filters that match the curves traced by the sought edges. We demonstrate the utility of our algorithms in both simulations and applications involving challenging real images. Finally, based on these principles, we develop an algorithm for fiber detection and enhancement. We exemplify its utility to reveal and enhance nerve axons in light microscopy images.



### Fine-Grained Categorization via CNN-Based Automatic Extraction and Integration of Object-Level and Part-Level Features
- **Arxiv ID**: http://arxiv.org/abs/1706.07397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07397v1)
- **Published**: 2017-06-22 16:59:16+00:00
- **Updated**: 2017-06-22 16:59:16+00:00
- **Authors**: Ting Sun, Lin Sun, Dit-Yan Yeung
- **Comment**: 45 pages, 20 figures, accepted by Image and Vision Computing
- **Journal**: None
- **Summary**: Fine-grained categorization can benefit from part-based features which reveal subtle visual differences between object categories. Handcrafted features have been widely used for part detection and classification. Although a recent trend seeks to learn such features automatically using powerful deep learning models such as convolutional neural networks (CNN), their training and possibly also testing require manually provided annotations which are costly to obtain. To relax these requirements, we assume in this study a general problem setting in which the raw images are only provided with object-level class labels for model training with no other side information needed. Specifically, by extracting and interpreting the hierarchical hidden layer features learned by a CNN, we propose an elaborate CNN-based system for fine-grained categorization. When evaluated on the Caltech-UCSD Birds-200-2011, FGVC-Aircraft, Cars and Stanford dogs datasets under the setting that only object-level class labels are used for training and no other annotations are available for both training and testing, our method achieves impressive performance that is superior or comparable to the state of the art. Moreover, it sheds some light on ingenious use of the hierarchical features learned by CNN which has wide applicability well beyond the current fine-grained categorization task.



### Single Classifier-based Passive System for Source Printer Classification using Local Texture Features
- **Arxiv ID**: http://arxiv.org/abs/1706.07422v1
- **DOI**: 10.1109/TIFS.2017.2779441
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.07422v1)
- **Published**: 2017-06-22 17:53:57+00:00
- **Updated**: 2017-06-22 17:53:57+00:00
- **Authors**: Sharad Joshi, Nitin Khanna
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: An important aspect of examining printed documents for potential forgeries and copyright infringement is the identification of source printer as it can be helpful for ascertaining the leak and detecting forged documents. This paper proposes a system for classification of source printer from scanned images of printed documents using all the printed letters simultaneously. This system uses local texture patterns based features and a single classifier for classifying all the printed letters. Letters are extracted from scanned images using connected component analysis followed by morphological filtering without the need of using an OCR. Each letter is sub-divided into a flat region and an edge region, and local tetra patterns are estimated separately for these two regions. A strategically constructed pooling technique is used to extract the final feature vectors. The proposed method has been tested on both a publicly available dataset of 10 printers and a new dataset of 18 printers scanned at a resolution of 600 dpi as well as 300 dpi printed in four different fonts. The results indicate shape independence property in the proposed method as using a single classifier it outperforms existing handcrafted feature-based methods and needs much smaller number of training pages by using all the printed letters.



### Deep Transfer Learning: A new deep learning glitch classification method for advanced LIGO
- **Arxiv ID**: http://arxiv.org/abs/1706.07446v1
- **DOI**: 10.1103/PhysRevD.97.101501
- **Categories**: **gr-qc**, astro-ph.IM, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1706.07446v1)
- **Published**: 2017-06-22 18:11:13+00:00
- **Updated**: 2017-06-22 18:11:13+00:00
- **Authors**: Daniel George, Hongyu Shen, E. A. Huerta
- **Comment**: None
- **Journal**: None
- **Summary**: The exquisite sensitivity of the advanced LIGO detectors has enabled the detection of multiple gravitational wave signals. The sophisticated design of these detectors mitigates the effect of most types of noise. However, advanced LIGO data streams are contaminated by numerous artifacts known as glitches: non-Gaussian noise transients with complex morphologies. Given their high rate of occurrence, glitches can lead to false coincident detections, obscure and even mimic gravitational wave signals. Therefore, successfully characterizing and removing glitches from advanced LIGO data is of utmost importance. Here, we present the first application of Deep Transfer Learning for glitch classification, showing that knowledge from deep learning algorithms trained for real-world object recognition can be transferred for classifying glitches in time-series based on their spectrogram images. Using the Gravity Spy dataset, containing hand-labeled, multi-duration spectrograms obtained from real LIGO data, we demonstrate that this method enables optimal use of very deep convolutional neural networks for classification given small training datasets, significantly reduces the time for training the networks, and achieves state-of-the-art accuracy above 98.8%, with perfect precision-recall on 8 out of 22 classes. Furthermore, new types of glitches can be classified accurately given few labeled examples with this technique. Once trained via transfer learning, we show that the convolutional neural networks can be truncated and used as excellent feature extractors for unsupervised clustering methods to identify new classes based on their morphology, without any labeled examples. Therefore, this provides a new framework for dynamic glitch classification for gravitational wave detectors, which are expected to encounter new types of noise as they undergo gradual improvements to attain design sensitivity.



### Learning Spatial-Aware Regressions for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1706.07457v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07457v2)
- **Published**: 2017-06-22 18:55:35+00:00
- **Updated**: 2018-04-24 12:16:40+00:00
- **Authors**: Chong Sun, Dong Wang, Huchuan Lu, Ming-Hsuan Yang
- **Comment**: To appear in CVPR2018
- **Journal**: None
- **Summary**: In this paper, we analyze the spatial information of deep features, and propose two complementary regressions for robust visual tracking. First, we propose a kernelized ridge regression model wherein the kernel value is defined as the weighted sum of similarity scores of all pairs of patches between two samples. We show that this model can be formulated as a neural network and thus can be efficiently solved. Second, we propose a fully convolutional neural network with spatially regularized kernels, through which the filter kernel corresponding to each output channel is forced to focus on a specific region of the target. Distance transform pooling is further exploited to determine the effectiveness of each output channel of the convolution layer. The outputs from the kernelized ridge regression model and the fully convolutional neural network are combined to obtain the ultimate response. Experimental results on two benchmark datasets validate the effectiveness of the proposed method.



### Fractal dimension analysis for automatic morphological galaxy classification
- **Arxiv ID**: http://arxiv.org/abs/1706.07507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07507v1)
- **Published**: 2017-06-22 22:17:26+00:00
- **Updated**: 2017-06-22 22:17:26+00:00
- **Authors**: Jorge de la Calleja, Elsa M. de la Calleja, Hugo Jair Escalante
- **Comment**: None
- **Journal**: None
- **Summary**: In this report we present experimental results using \emph{Haussdorf-Besicovich} fractal dimension for performing morphological galaxy classification. The fractal dimension is a topological, structural and spatial property that give us information about the space were an object lives. We have calculated the fractal dimension value of the main types of galaxies: ellipticals, spirals and irregulars; and we use it as a feature for classifying them. Also, we have performed an image analysis process in order to standardize the galaxy images, and we have used principal component analysis to obtain the main attributes in the images. Galaxy classification was performed using machine learning algorithms: C4.5, k-nearest neighbors, random forest and support vector machines. Preliminary experimental results using 10-fold cross-validation show that fractal dimension helps to improve classification, with over 88 per cent accuracy for elliptical galaxies, 100 per cent accuracy for spiral galaxies and over 40 per cent for irregular galaxies.



### Comparing Neural and Attractiveness-based Visual Features for Artwork Recommendation
- **Arxiv ID**: http://arxiv.org/abs/1706.07515v2
- **DOI**: 10.1145/3125486.3125495
- **Categories**: **cs.IR**, cs.AI, cs.CV, cs.DL
- **Links**: [PDF](http://arxiv.org/pdf/1706.07515v2)
- **Published**: 2017-06-22 22:48:48+00:00
- **Updated**: 2017-07-21 22:17:48+00:00
- **Authors**: Vicente Dominguez, Pablo Messina, Denis Parra, Domingo Mery, Christoph Trattner, Alvaro Soto
- **Comment**: DLRS 2017 workshop, co-located at RecSys 2017
- **Journal**: None
- **Summary**: Advances in image processing and computer vision in the latest years have brought about the use of visual features in artwork recommendation. Recent works have shown that visual features obtained from pre-trained deep neural networks (DNNs) perform very well for recommending digital art. Other recent works have shown that explicit visual features (EVF) based on attractiveness can perform well in preference prediction tasks, but no previous work has compared DNN features versus specific attractiveness-based visual features (e.g. brightness, texture) in terms of recommendation performance. In this work, we study and compare the performance of DNN and EVF features for the purpose of physical artwork recommendation using transactional data from UGallery, an online store of physical paintings. In addition, we perform an exploratory analysis to understand if DNN embedded features have some relation with certain EVF. Our results show that DNN features outperform EVF, that certain EVF features are more suited for physical artwork recommendation and, finally, we show evidence that certain neurons in the DNN might be partially encoding visual features such as brightness, providing an opportunity for explaining recommendations based on visual neural models.



### Deep Hashing Network for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1706.07522v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1706.07522v1)
- **Published**: 2017-06-22 23:15:10+00:00
- **Updated**: 2017-06-22 23:15:10+00:00
- **Authors**: Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, Sethuraman Panchanathan
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: In recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application domains. However, training a deep neural network requires a large amount of labeled data, which is an expensive process in terms of time, labor and human expertise. Domain adaptation or transfer learning algorithms address this challenge by leveraging labeled data in a different, but related source domain, to develop a model for the target domain. Further, the explosive growth of digital data has posed a fundamental challenge concerning its storage and retrieval. Due to its storage and retrieval efficiency, recent years have witnessed a wide application of hashing in a variety of computer vision applications. In this paper, we first introduce a new dataset, Office-Home, to evaluate domain adaptation algorithms. The dataset contains images of a variety of everyday objects from multiple domains. We then propose a novel deep learning framework that can exploit labeled source data and unlabeled target data to learn informative hash codes, to accurately classify unseen target data. To the best of our knowledge, this is the first research effort to exploit the feature learning capabilities of deep neural networks to learn representative hash codes to address the domain adaptation problem. Our extensive empirical studies on multiple transfer tasks corroborate the usefulness of the framework in learning efficient hash codes which outperform existing competitive baselines for unsupervised domain adaptation.



### Nonlinear Embedding Transform for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1706.07524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07524v1)
- **Published**: 2017-06-22 23:42:27+00:00
- **Updated**: 2017-06-22 23:42:27+00:00
- **Authors**: Hemanth Venkateswara, Shayok Chakraborty, Sethuraman Panchanathan
- **Comment**: ECCV Workshops 2016
- **Journal**: None
- **Summary**: The problem of domain adaptation (DA) deals with adapting classifier models trained on one data distribution to different data distributions. In this paper, we introduce the Nonlinear Embedding Transform (NET) for unsupervised DA by combining domain alignment along with similarity-based embedding. We also introduce a validation procedure to estimate the model parameters for the NET algorithm using the source data. Comprehensive evaluations on multiple vision datasets demonstrate that the NET algorithm outperforms existing competitive procedures for unsupervised DA.



### Coupled Support Vector Machines for Supervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1706.07525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07525v1)
- **Published**: 2017-06-22 23:53:09+00:00
- **Updated**: 2017-06-22 23:53:09+00:00
- **Authors**: Hemanth Venkateswara, Prasanth Lade, Jieping Ye, Sethuraman Panchanathan
- **Comment**: ACM Multimedia Conference 2015
- **Journal**: None
- **Summary**: Popular domain adaptation (DA) techniques learn a classifier for the target domain by sampling relevant data points from the source and combining it with the target data. We present a Support Vector Machine (SVM) based supervised DA technique, where the similarity between source and target domains is modeled as the similarity between their SVM decision boundaries. We couple the source and target SVMs and reduce the model to a standard single SVM. We test the Coupled-SVM on multiple datasets and compare our results with other popular SVM based DA approaches.



