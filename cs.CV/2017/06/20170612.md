# Arxiv Papers in cs.CV on 2017-06-12
### Modeling Multi-Object Configurations via Medial/Skeletal Linking Structures
- **Arxiv ID**: http://arxiv.org/abs/1706.03431v1
- **DOI**: 10.1007/s11263-017-1019-5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03431v1)
- **Published**: 2017-06-12 01:16:36+00:00
- **Updated**: 2017-06-12 01:16:36+00:00
- **Authors**: James Damon, Ellen Gasparovic
- **Comment**: This paper presents material relevant for two and three dimensional
  images that builds on and references a previous paper by the authors,
  arXiv:1402.5517
- **Journal**: International Journal of Computer Vision, 2017
- **Summary**: We introduce a method for modeling a configuration of objects in 2D or 3D images using a mathematical "skeletal linking structure" which will simultaneously capture the individual shape features of the objects and their positional information relative to one another. The objects may either have smooth boundaries and be disjoint from the others or share common portions of their boundaries with other objects in a piecewise smooth manner. These structures include a special class of "Blum medial linking structures," which are intrinsically associated to the configuration and build upon the Blum medial axes of the individual objects. We give a classification of the properties of Blum linking structures for generic configurations. The skeletal linking structures add increased flexibility for modeling configurations of objects by relaxing the Blum conditions and they extend in a minimal way the individual "skeletal structures" which have been previously used for modeling individual objects and capturing their geometric properties. This allows for the mathematical methods introduced for single objects to be significantly extended to the entire configuration of objects. These methods not only capture the internal shape structures of the individual objects but also the external structure of the neighboring regions of the objects.



### Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model
- **Arxiv ID**: http://arxiv.org/abs/1706.03458v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03458v2)
- **Published**: 2017-06-12 04:02:03+00:00
- **Updated**: 2017-10-05 06:31:47+00:00
- **Authors**: Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, Wang-chun Woo
- **Comment**: NIPS 2017 Spotlight
- **Journal**: None
- **Summary**: With the goal of making high-resolution forecasts of regional rainfall, precipitation nowcasting has become an important and fundamental technology underlying various public services ranging from rainstorm warnings to flight safety. Recently, the Convolutional LSTM (ConvLSTM) model has been shown to outperform traditional optical flow based methods for precipitation nowcasting, suggesting that deep learning models have a huge potential for solving the problem. However, the convolutional recurrence structure in ConvLSTM-based models is location-invariant while natural motion and transformation (e.g., rotation) are location-variant in general. Furthermore, since deep-learning-based precipitation nowcasting is a newly emerging area, clear evaluation protocols have not yet been established. To address these problems, we propose both a new model and a benchmark for precipitation nowcasting. Specifically, we go beyond ConvLSTM and propose the Trajectory GRU (TrajGRU) model that can actively learn the location-variant structure for recurrent connections. Besides, we provide a benchmark that includes a real-world large-scale dataset from the Hong Kong Observatory, a new training loss, and a comprehensive evaluation protocol to facilitate future research and gauge the state of the art.



### Few-Shot Image Recognition by Predicting Parameters from Activations
- **Arxiv ID**: http://arxiv.org/abs/1706.03466v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03466v3)
- **Published**: 2017-06-12 04:57:29+00:00
- **Updated**: 2017-11-25 18:50:39+00:00
- **Authors**: Siyuan Qiao, Chenxi Liu, Wei Shen, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we are interested in the few-shot learning problem. In particular, we focus on a challenging scenario where the number of categories is large and the number of examples per novel category is very limited, e.g. 1, 2, or 3. Motivated by the close relationship between the parameters and the activations in a neural network associated with the same category, we propose a novel method that can adapt a pre-trained neural network to novel categories by directly predicting the parameters from the activations. Zero training is required in adaptation to novel categories, and fast inference is realized by a single forward pass. We evaluate our method by doing few-shot image recognition on the ImageNet dataset, which achieves the state-of-the-art classification accuracy on novel categories by a significant margin while keeping comparable performance on the large-scale categories. We also test our method on the MiniImageNet dataset and it strongly outperforms the previous state-of-the-art methods.



### A filter based approach for inbetweening
- **Arxiv ID**: http://arxiv.org/abs/1706.03497v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1706.03497v1)
- **Published**: 2017-06-12 08:04:42+00:00
- **Updated**: 2017-06-12 08:04:42+00:00
- **Authors**: Yuichi Yagi
- **Comment**: 10 pages, in Japanese
- **Journal**: None
- **Summary**: We present a filter based approach for inbetweening. We train a convolutional neural network to generate intermediate frames. This network aim to generate smooth animation of line drawings. Our method can process scanned images directly. Our method does not need to compute correspondence of lines and topological changes explicitly. We experiment our method with real animation production data. The results show that our method can generate intermediate frames partially.



### Exploring the similarity of medical imaging classification problems
- **Arxiv ID**: http://arxiv.org/abs/1706.03509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03509v1)
- **Published**: 2017-06-12 08:28:17+00:00
- **Updated**: 2017-06-12 08:28:17+00:00
- **Authors**: Veronika Cheplygina, Pim Moeskops, Mitko Veta, Behdad Dasht Bozorg, Josien Pluim
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised learning is ubiquitous in medical image analysis. In this paper we consider the problem of meta-learning -- predicting which methods will perform well in an unseen classification problem, given previous experience with other classification problems. We investigate the first step of such an approach: how to quantify the similarity of different classification problems. We characterize datasets sampled from six classification problems by performance ranks of simple classifiers, and define the similarity by the inverse of Euclidean distance in this meta-feature space. We visualize the similarities in a 2D space, where meaningful clusters start to emerge, and show that the proposed representation can be used to classify datasets according to their origin with 89.3\% accuracy. These findings, together with the observations of recent trends in machine learning, suggest that meta-learning could be a valuable tool for the medical imaging community.



### Enriched Deep Recurrent Visual Attention Model for Multiple Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1706.03581v1
- **DOI**: 10.1109/WACV.2017.113
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.03581v1)
- **Published**: 2017-06-12 11:55:35+00:00
- **Updated**: 2017-06-12 11:55:35+00:00
- **Authors**: Artsiom Ablavatski, Shijian Lu, Jianfei Cai
- **Comment**: None
- **Journal**: None
- **Summary**: We design an Enriched Deep Recurrent Visual Attention Model (EDRAM) - an improved attention-based architecture for multiple object recognition. The proposed model is a fully differentiable unit that can be optimized end-to-end by using Stochastic Gradient Descent (SGD). The Spatial Transformer (ST) was employed as visual attention mechanism which allows to learn the geometric transformation of objects within images. With the combination of the Spatial Transformer and the powerful recurrent architecture, the proposed EDRAM can localize and recognize objects simultaneously. EDRAM has been evaluated on two publicly available datasets including MNIST Cluttered (with 70K cluttered digits) and SVHN (with up to 250k real world images of house numbers). Experiments show that it obtains superior performance as compared with the state-of-the-art models.



### Point Linking Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1706.03646v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03646v2)
- **Published**: 2017-06-12 14:02:01+00:00
- **Updated**: 2017-06-13 05:04:37+00:00
- **Authors**: Xinggang Wang, Kaibing Chen, Zilong Huang, Cong Yao, Wenyu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is a core problem in computer vision. With the development of deep ConvNets, the performance of object detectors has been dramatically improved. The deep ConvNets based object detectors mainly focus on regressing the coordinates of bounding box, e.g., Faster-R-CNN, YOLO and SSD. Different from these methods that considering bounding box as a whole, we propose a novel object bounding box representation using points and links and implemented using deep ConvNets, termed as Point Linking Network (PLN). Specifically, we regress the corner/center points of bounding-box and their links using a fully convolutional network; then we map the corner points and their links back to multiple bounding boxes; finally an object detection result is obtained by fusing the multiple bounding boxes. PLN is naturally robust to object occlusion and flexible to object scale variation and aspect ratio variation. In the experiments, PLN with the Inception-v2 model achieves state-of-the-art single-model and single-scale results on the PASCAL VOC 2007, the PASCAL VOC 2012 and the COCO detection benchmarks without bells and whistles. The source code will be released.



### Image Crowd Counting Using Convolutional Neural Network and Markov Random Field
- **Arxiv ID**: http://arxiv.org/abs/1706.03686v3
- **DOI**: 10.20965/jaciii.2017.p0632(2017)
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03686v3)
- **Published**: 2017-06-12 15:29:48+00:00
- **Updated**: 2017-10-17 02:24:34+00:00
- **Authors**: Kang Han, Wanggen Wan, Haiyan Yao, Li Hou
- **Comment**: 6 pages, 6 figures, JACIII Vol.21 No.4
- **Journal**: JACIII Vol.21 No.4 2017 pp. 632-638
- **Summary**: In this paper, we propose a method called Convolutional Neural Network-Markov Random Field (CNN-MRF) to estimate the crowd count in a still image. We first divide the dense crowd visible image into overlapping patches and then use a deep convolutional neural network to extract features from each patch image, followed by a fully connected neural network to regress the local patch crowd count. Since the local patches have overlapping portions, the crowd count of the adjacent patches has a high correlation. We use this correlation and the Markov random field to smooth the counting results of the local patches. Experiments show that our approach significantly outperforms the state-of-the-art methods on UCF and Shanghaitech crowd counting datasets.



### Progressive and Multi-Path Holistically Nested Neural Networks for Pathological Lung Segmentation from CT Images
- **Arxiv ID**: http://arxiv.org/abs/1706.03702v1
- **DOI**: 10.1007/978-3-319-66179-7_71
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03702v1)
- **Published**: 2017-06-12 15:54:35+00:00
- **Updated**: 2017-06-12 15:54:35+00:00
- **Authors**: Adam P. Harrison, Ziyue Xu, Kevin George, Le Lu, Ronald M. Summers, Daniel J. Mollura
- **Comment**: 8 Pages, 4 figures, MICCAI 2007
- **Journal**: Proc. MICCAI (2017); pp 621-629
- **Summary**: Pathological lung segmentation (PLS) is an important, yet challenging, medical image application due to the wide variability of pathological lung appearance and shape. Because PLS is often a pre-requisite for other imaging analytics, methodological simplicity and generality are key factors in usability. Along those lines, we present a bottom-up deep-learning based approach that is expressive enough to handle variations in appearance, while remaining unaffected by any variations in shape. We incorporate the deeply supervised learning framework, but enhance it with a simple, yet effective, progressive multi-path scheme, which more reliably merges outputs from different network stages. The result is a deep model able to produce finer detailed masks, which we call progressive holistically-nested networks (P-HNNs). Using extensive cross-validation, our method is tested on multi-institutional datasets comprising 929 CT scans (848 publicly available), of pathological lungs, reporting mean dice scores of 0.985 and demonstrating significant qualitative and quantitative improvements over state-of-the art approaches.



### Transferring a Semantic Representation for Person Re-Identification and Search
- **Arxiv ID**: http://arxiv.org/abs/1706.03725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03725v1)
- **Published**: 2017-06-12 16:52:57+00:00
- **Updated**: 2017-06-12 16:52:57+00:00
- **Authors**: Zhiyuan Shi, Timothy M. Hospedales, Tao Xiang
- **Comment**: cvpr 2015
- **Journal**: None
- **Summary**: Learning semantic attributes for person re-identification and description-based person search has gained increasing interest due to attributes' great potential as a pose and view-invariant representation. However, existing attribute-centric approaches have thus far underperformed state-of-the-art conventional approaches. This is due to their non-scalable need for extensive domain (camera) specific annotation. In this paper we present a new semantic attribute learning approach for person re-identification and search. Our model is trained on existing fashion photography datasets -- either weakly or strongly labelled. It can then be transferred and adapted to provide a powerful semantic description of surveillance person detections, without requiring any surveillance domain supervision. The resulting representation is useful for both unsupervised and supervised person re-identification, achieving state-of-the-art and near state-of-the-art performance respectively. Furthermore, as a semantic representation it allows description-based person search to be integrated within the same framework.



### Channel-Recurrent Autoencoding for Image Modeling
- **Arxiv ID**: http://arxiv.org/abs/1706.03729v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.03729v2)
- **Published**: 2017-06-12 17:01:34+00:00
- **Updated**: 2018-03-11 09:00:51+00:00
- **Authors**: Wenling Shang, Kihyuk Sohn, Yuandong Tian
- **Comment**: Code: https://github.com/WendyShang/crVAE. Supplementary Materials:
  http://www-personal.umich.edu/~shangw/wacv18_supplementary_material.pdf
- **Journal**: None
- **Summary**: Despite recent successes in synthesizing faces and bedrooms, existing generative models struggle to capture more complex image types, potentially due to the oversimplification of their latent space constructions. To tackle this issue, building on Variational Autoencoders (VAEs), we integrate recurrent connections across channels to both inference and generation steps, allowing the high-level features to be captured in global-to-local, coarse-to-fine manners. Combined with adversarial loss, our channel-recurrent VAE-GAN (crVAE-GAN) outperforms VAE-GAN in generating a diverse spectrum of high resolution images while maintaining the same level of computational efficacy. Our model produces interpretable and expressive latent representations to benefit downstream tasks such as image completion. Moreover, we propose two novel regularizations, namely the KL objective weighting scheme over time steps and mutual information maximization between transformed latent variables and the outputs, to enhance the training.



### Large-Scale Plant Classification with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.03736v1
- **DOI**: 10.1145/3075564.3075590
- **Categories**: **cs.LG**, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1706.03736v1)
- **Published**: 2017-06-12 17:16:20+00:00
- **Updated**: 2017-06-12 17:16:20+00:00
- **Authors**: Ignacio Heredia
- **Comment**: 5 pages, 3 figures, 1 table. Published at Proocedings of ACM
  Computing Frontiers Conference 2017
- **Journal**: ACM CF'17 Proceedings of the Computing Frontiers Conference
  (2017), 259-262
- **Summary**: This paper discusses the potential of applying deep learning techniques for plant classification and its usage for citizen science in large-scale biodiversity monitoring. We show that plant classification using near state-of-the-art convolutional network architectures like ResNet50 achieves significant improvements in accuracy compared to the most widespread plant classification application in test sets composed of thousands of different species labels. We find that the predictions can be confidently used as a baseline classification in citizen science communities like iNaturalist (or its Spanish fork, Natusfera) which in turn can share their data with biodiversity portals like GBIF.



### Portable Trust: biometric-based authentication and blockchain storage for self-sovereign identity systems
- **Arxiv ID**: http://arxiv.org/abs/1706.03744v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.03744v1)
- **Published**: 2017-06-12 17:33:12+00:00
- **Updated**: 2017-06-12 17:33:12+00:00
- **Authors**: J. S. Hammudoglu, J. Sparreboom, J. I. Rauhamaa, J. K. Faber, L. C. Guerchi, I. P. Samiotis, S. P. Rao, J. A. Pouwelse
- **Comment**: Delft University of Technology student project report
- **Journal**: None
- **Summary**: We devised a mobile biometric-based authentication system only relying on local processing. Our Android open source solution explores the capability of current smartphones to acquire, process and match fingerprints using only its built-in hardware. Our architecture is specifically designed to run completely locally and autonomously, not requiring any cloud service, server, or permissioned access to fingerprint reader hardware. It involves three main stages, starting with the fingerprint acquisition using the smartphone camera, followed by a processing pipeline to obtain minutiae features and a final step for matching against other locally stored fingerprints, based on Oriented FAST and Rotated BRIEF (ORB) descriptors. We obtained a mean matching accuracy of 55%, with the highest value of 67% for thumb fingers. Our ability to capture and process a finger fingerprint in mere seconds using a smartphone makes this work usable in a wide range of scenarios, for instance, offline remote regions. This work is specifically designed to be a key building block for a self-sovereign identity solution and integrate with our permissionless blockchain for identity and key attestation.



### SmoothGrad: removing noise by adding noise
- **Arxiv ID**: http://arxiv.org/abs/1706.03825v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.03825v1)
- **Published**: 2017-06-12 19:53:30+00:00
- **Updated**: 2017-06-12 19:53:30+00:00
- **Authors**: Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, Martin Wattenberg
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.



### Subspace Clustering via Optimal Direction Search
- **Arxiv ID**: http://arxiv.org/abs/1706.03860v4
- **DOI**: 10.1109/LSP.2017.2757901
- **Categories**: **cs.CV**, cs.IR, cs.LG, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.03860v4)
- **Published**: 2017-06-12 21:52:57+00:00
- **Updated**: 2017-11-26 15:43:15+00:00
- **Authors**: Mostafa Rahmani, George Atia
- **Comment**: None
- **Journal**: IEEE Signal Processing Letters ( Volume: 24, Issue: 12, Dec. 2017
  )
- **Summary**: This letter presents a new spectral-clustering-based approach to the subspace clustering problem. Underpinning the proposed method is a convex program for optimal direction search, which for each data point d finds an optimal direction in the span of the data that has minimum projection on the other data points and non-vanishing projection on d. The obtained directions are subsequently leveraged to identify a neighborhood set for each data point. An alternating direction method of multipliers framework is provided to efficiently solve for the optimal directions. The proposed method is shown to notably outperform the existing subspace clustering methods, particularly for unwieldy scenarios involving high levels of noise and close subspaces, and yields the state-of-the-art results for the problem of face clustering using subspace segmentation.



### Criteria Sliders: Learning Continuous Database Criteria via Interactive Ranking
- **Arxiv ID**: http://arxiv.org/abs/1706.03863v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03863v1)
- **Published**: 2017-06-12 21:59:26+00:00
- **Updated**: 2017-06-12 21:59:26+00:00
- **Authors**: James Tompkin, Kwang In Kim, Hanspeter Pfister, Christian Theobalt
- **Comment**: None
- **Journal**: None
- **Summary**: Large databases are often organized by hand-labeled metadata, or criteria, which are expensive to collect. We can use unsupervised learning to model database variation, but these models are often high dimensional, complex to parameterize, or require expert knowledge. We learn low-dimensional continuous criteria via interactive ranking, so that the novice user need only describe the relative ordering of examples. This is formed as semi-supervised label propagation in which we maximize the information gained from a limited number of examples. Further, we actively suggest data points to the user to rank in a more informative way than existing work. Our efficient approach allows users to interactively organize thousands of data points along 1D and 2D continuous sliders. We experiment with datasets of imagery and geometry to demonstrate that our tool is useful for quickly assessing and organizing the content of large databases.



### Can We See Photosynthesis? Magnifying the Tiny Color Changes of Plant Green Leaves Using Eulerian Video Magnification
- **Arxiv ID**: http://arxiv.org/abs/1706.03867v3
- **DOI**: 10.1117/1.JEI.26.6.060501
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03867v3)
- **Published**: 2017-06-12 23:04:33+00:00
- **Updated**: 2017-08-29 16:50:37+00:00
- **Authors**: Islam A. T. F. Taj-Eddin, Mahmoud Afifi, Mostafa Korashy, Ali H. Ahmed, Ng Yoke Cheng, Evelyng Hernandez, Salma M. Abdel-latif
- **Comment**: 7 pages, 3 figures
- **Journal**: J. Electron. Imaging, 2017
- **Summary**: Plant aliveness is proven through laboratory experiments and special scientific instruments. In this paper, we aim to detect the degree of animation of plants based on the magnification of the small color changes in the plant's green leaves using the Eulerian video magnification. Capturing the video under a controlled environment, e.g., using a tripod and direct current (DC) light sources, reduces camera movements and minimizes light fluctuations; we aim to reduce the external factors as much as possible. The acquired video is then stabilized and a proposed algorithm used to reduce the illumination variations. Lastly, the Euler magnification is utilized to magnify the color changes on the light invariant video. The proposed system does not require any special purpose instruments as it uses a digital camera with a regular frame rate. The results of magnified color changes on both natural and plastic leaves show that the live green leaves have color changes in contrast to the plastic leaves. Hence, we can argue that the color changes of the leaves are due to biological operations, such as photosynthesis. To date, this is possibly the first work that focuses on interpreting visually, some biological operations of plants without any special purpose instruments.



