# Arxiv Papers in cs.CV on 2017-06-02
### Data Augmentation of Wearable Sensor Data for Parkinson's Disease Monitoring using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.00527v2
- **DOI**: 10.1145/3136755.3136817
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00527v2)
- **Published**: 2017-06-02 00:43:11+00:00
- **Updated**: 2017-11-08 20:01:02+00:00
- **Authors**: Terry Taewoong Um, Franz Michael Josef Pfister, Daniel Pichler, Satoshi Endo, Muriel Lang, Sandra Hirche, Urban Fietzek, Dana Kulić
- **Comment**: ICMI2017 (oral session)
- **Journal**: None
- **Summary**: While convolutional neural networks (CNNs) have been successfully applied to many challenging classification applications, they typically require large datasets for training. When the availability of labeled data is limited, data augmentation is a critical preprocessing step for CNNs. However, data augmentation for wearable sensor data has not been deeply investigated yet.   In this paper, various data augmentation methods for wearable sensor data are proposed. The proposed methods and CNNs are applied to the classification of the motor state of Parkinson's Disease patients, which is challenging due to small dataset size, noisy labels, and large intra-class variability. Appropriate augmentation improves the classification performance from 77.54\% to 86.88\%.



### Integrated Deep and Shallow Networks for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1706.00530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00530v1)
- **Published**: 2017-06-02 00:52:55+00:00
- **Updated**: 2017-06-02 00:52:55+00:00
- **Authors**: Jing Zhang, Bo Li, Yuchao Dai, Fatih Porikli, Mingyi He
- **Comment**: Accepted by IEEE International Conference on Image Processing (ICIP)
  2017
- **Journal**: None
- **Summary**: Deep convolutional neural network (CNN) based salient object detection methods have achieved state-of-the-art performance and outperform those unsupervised methods with a wide margin. In this paper, we propose to integrate deep and unsupervised saliency for salient object detection under a unified framework. Specifically, our method takes results of unsupervised saliency (Robust Background Detection, RBD) and normalized color images as inputs, and directly learns an end-to-end mapping between inputs and the corresponding saliency maps. The color images are fed into a Fully Convolutional Neural Networks (FCNN) adapted from semantic segmentation to exploit high-level semantic cues for salient object detection. Then the results from deep FCNN and RBD are concatenated to feed into a shallow network to map the concatenated feature maps to saliency maps. Finally, to obtain a spatially consistent saliency map with sharp object boundaries, we fuse superpixel level saliency map at multi-scale. Extensive experimental results on 8 benchmark datasets demonstrate that the proposed method outperforms the state-of-the-art approaches with a margin.



### SAR Image Despeckling Using a Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1706.00552v2
- **DOI**: 10.1109/LSP.2017.2758203
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00552v2)
- **Published**: 2017-06-02 04:31:43+00:00
- **Updated**: 2018-06-26 02:45:39+00:00
- **Authors**: Puyang Wang, He Zhang, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic Aperture Radar (SAR) images are often contaminated by a multiplicative noise known as speckle. Speckle makes the processing and interpretation of SAR images difficult. We propose a deep learning-based approach called, Image Despeckling Convolutional Neural Network (ID-CNN), for automatically removing speckle from the input noisy images. In particular, ID-CNN uses a set of convolutional layers along with batch normalization and rectified linear unit (ReLU) activation function and a component-wise division residual layer to estimate speckle and it is trained in an end-to-end fashion using a combination of Euclidean loss and Total Variation (TV) loss. Extensive experiments on synthetic and real SAR images show that the proposed method achieves significant improvements over the state-of-the-art speckle reduction methods.



### Rank Persistence: Assessing the Temporal Performance of Real-World Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1706.00553v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00553v2)
- **Published**: 2017-06-02 04:34:08+00:00
- **Updated**: 2017-06-05 02:00:52+00:00
- **Authors**: Srikrishna Karanam, Eric Lam, Richard J. Radke
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Designing useful person re-identification systems for real-world applications requires attention to operational aspects not typically considered in academic research. Here, we focus on the temporal aspect of re-identification; that is, instead of finding a match to a probe person of interest in a fixed candidate gallery, we consider the more realistic scenario in which the gallery is continuously populated by new candidates over a long time period. A key question of interest for an operator of such a system is: how long is a correct match to a probe likely to remain in a rank-k shortlist of possible candidates? We propose to distill this information into a Rank Persistence Curve (RPC), which allows different algorithms' temporal performance characteristics to be directly compared. We present examples to illustrate the RPC using a new long-term dataset with multiple candidate reappearances, and discuss considerations for future re-identification research that explicitly involves temporal aspects.



### r-BTN: Cross-domain Face Composite and Synthesis from Limited Facial Patches
- **Arxiv ID**: http://arxiv.org/abs/1706.00556v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00556v2)
- **Published**: 2017-06-02 05:07:37+00:00
- **Updated**: 2017-12-06 20:08:27+00:00
- **Authors**: Yang Song, Zhifei Zhang, Hairong Qi
- **Comment**: Accepted by AAAI 2018
- **Journal**: None
- **Summary**: We start by asking an interesting yet challenging question, "If an eyewitness can only recall the eye features of the suspect, such that the forensic artist can only produce a sketch of the eyes (e.g., the top-left sketch shown in Fig. 1), can advanced computer vision techniques help generate the whole face image?" A more generalized question is that if a large proportion (e.g., more than 50%) of the face/sketch is missing, can a realistic whole face sketch/image still be estimated. Existing face completion and generation methods either do not conduct domain transfer learning or can not handle large missing area. For example, the inpainting approach tends to blur the generated region when the missing area is large (i.e., more than 50%). In this paper, we exploit the potential of deep learning networks in filling large missing region (e.g., as high as 95% missing) and generating realistic faces with high-fidelity in cross domains. We propose the recursive generation by bidirectional transformation networks (r-BTN) that recursively generates a whole face/sketch from a small sketch/face patch. The large missing area and the cross domain challenge make it difficult to generate satisfactory results using a unidirectional cross-domain learning structure. On the other hand, a forward and backward bidirectional learning between the face and sketch domains would enable recursive estimation of the missing region in an incremental manner (Fig. 1) and yield appealing results. r-BTN also adopts an adversarial constraint to encourage the generation of realistic faces/sketches. Extensive experiments have been conducted to demonstrate the superior performance from r-BTN as compared to existing potential solutions.



### Image Restoration from Patch-based Compressed Sensing Measurement
- **Arxiv ID**: http://arxiv.org/abs/1706.00597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00597v1)
- **Published**: 2017-06-02 09:08:06+00:00
- **Updated**: 2017-06-02 09:08:06+00:00
- **Authors**: Guangtao Nie, Ying Fu, Yinqiang Zheng, Hua Huang
- **Comment**: None
- **Journal**: None
- **Summary**: A series of methods have been proposed to reconstruct an image from compressively sensed random measurement, but most of them have high time complexity and are inappropriate for patch-based compressed sensing capture, because of their serious blocky artifacts in the restoration results. In this paper, we present a non-iterative image reconstruction method from patch-based compressively sensed random measurement. Our method features two cascaded networks based on residual convolution neural network to learn the end-to-end full image restoration, which is capable of reconstructing image patches and removing the blocky effect with low time cost. Experimental results on synthetic and real data show that our method outperforms state-of-the-art compressive sensing (CS) reconstruction methods with patch-based CS measurement. To demonstrate the effectiveness of our method in more general setting, we apply the de-block process in our method to JPEG compression artifacts removal and achieve outstanding performance as well.



### Dynamic Steerable Blocks in Deep Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.00598v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.00598v2)
- **Published**: 2017-06-02 09:08:09+00:00
- **Updated**: 2017-07-19 15:12:52+00:00
- **Authors**: Jörn-Henrik Jacobsen, Bert de Brabandere, Arnold W. M. Smeulders
- **Comment**: None
- **Journal**: None
- **Summary**: Filters in convolutional networks are typically parameterized in a pixel basis, that does not take prior knowledge about the visual world into account. We investigate the generalized notion of frames designed with image properties in mind, as alternatives to this parametrization. We show that frame-based ResNets and Densenets can improve performance on Cifar-10+ consistently, while having additional pleasant properties like steerability. By exploiting these transformation properties explicitly, we arrive at dynamic steerable blocks. They are an extension of residual blocks, that are able to seamlessly transform filters under pre-defined transformations, conditioned on the input at training and inference time. Dynamic steerable blocks learn the degree of invariance from data and locally adapt filters, allowing them to apply a different geometrical variant of the same filter to each location of the feature map. When evaluated on the Berkeley Segmentation contour detection dataset, our approach outperforms all competing approaches that do not utilize pre-training. Our results highlight the benefits of image-based regularization to deep networks.



### Facies classification from well logs using an inception convolutional network
- **Arxiv ID**: http://arxiv.org/abs/1706.00613v1
- **DOI**: None
- **Categories**: **cs.CV**, 86-04
- **Links**: [PDF](http://arxiv.org/pdf/1706.00613v1)
- **Published**: 2017-06-02 10:13:28+00:00
- **Updated**: 2017-06-02 10:13:28+00:00
- **Authors**: Valentin Tschannen, Matthias Delescluse, Mathieu Rodriguez, Janis Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: The idea to use automated algorithms to determine geological facies from well logs is not new (see e.g Busch et al. (1987); Rabaute (1998)) but the recent and dramatic increase in research in the field of machine learning makes it a good time to revisit the topic. Following an exercise proposed by Dubois et al. (2007) and Hall (2016) we employ a modern type of deep convolutional network, called \textit{inception network} (Szegedy et al., 2015), to tackle the supervised classification task and we discuss the methodological limits of such problem as well as further research opportunities.



### Detection, Recognition and Tracking of Moving Objects from Real-time Video via Visual Vocabulary Model and Species Inspired PSO
- **Arxiv ID**: http://arxiv.org/abs/1707.05224v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1707.05224v1)
- **Published**: 2017-06-02 11:09:10+00:00
- **Updated**: 2017-06-02 11:09:10+00:00
- **Authors**: Kumar S. Ray, Anit Chakraborty, Sayandip Dutta
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the basic problem of recognizing moving objects in video images using Visual Vocabulary model and Bag of Words and track our object of interest in the subsequent video frames using species inspired PSO. Initially, the shadow free images are obtained by background modelling followed by foreground modeling to extract the blobs of our object of interest. Subsequently, we train a cubic SVM with human body datasets in accordance with our domain of interest for recognition and tracking. During training, using the principle of Bag of Words we extract necessary features of certain domains and objects for classification. Subsequently, matching these feature sets with those of the extracted object blobs that are obtained by subtracting the shadow free background from the foreground, we detect successfully our object of interest from the test domain. The performance of the classification by cubic SVM is satisfactorily represented by confusion matrix and ROC curve reflecting the accuracy of each module. After classification, our object of interest is tracked in the test domain using species inspired PSO. By combining the adaptive learning tools with the efficient classification of description, we achieve optimum accuracy in recognition of the moving objects. We evaluate our algorithm benchmark datasets: iLIDS, VIVID, Walking2, Woman. Comparative analysis of our algorithm against the existing state-of-the-art trackers shows very satisfactory and competitive results.



### Dual-reference Face Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1706.00631v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00631v2)
- **Published**: 2017-06-02 11:14:50+00:00
- **Updated**: 2017-11-22 11:15:16+00:00
- **Authors**: BingZhang Hu, Feng Zheng, Ling Shao
- **Comment**: Accepted at AAAI 2018
- **Journal**: None
- **Summary**: Face retrieval has received much attention over the past few decades, and many efforts have been made in retrieving face images against pose, illumination, and expression variations. However, the conventional works fail to meet the requirements of a potential and novel task --- retrieving a person's face image at a specific age, especially when the specific 'age' is not given as a numeral, i.e. 'retrieving someone's image at the similar age period shown by another person's image'. To tackle this problem, we propose a dual reference face retrieval framework in this paper, where the system takes two inputs: an identity reference image which indicates the target identity and an age reference image which reflects the target age. In our framework, the raw images are first projected on a joint manifold, which preserves both the age and identity locality. Then two similarity metrics of age and identity are exploited and optimized by utilizing our proposed quartet-based model. The experiments show promising results, outperforming hierarchical methods.



### Action Sets: Weakly Supervised Action Segmentation without Ordering Constraints
- **Arxiv ID**: http://arxiv.org/abs/1706.00699v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00699v2)
- **Published**: 2017-06-02 14:34:21+00:00
- **Updated**: 2018-05-17 17:30:28+00:00
- **Authors**: Alexander Richard, Hilde Kuehne, Juergen Gall
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Action detection and temporal segmentation of actions in videos are topics of increasing interest. While fully supervised systems have gained much attention lately, full annotation of each action within the video is costly and impractical for large amounts of video data. Thus, weakly supervised action detection and temporal segmentation methods are of great importance. While most works in this area assume an ordered sequence of occurring actions to be given, our approach only uses a set of actions. Such action sets provide much less supervision since neither action ordering nor the number of action occurrences are known. In exchange, they can be easily obtained, for instance, from meta-tags, while ordered sequences still require human annotation. We introduce a system that automatically learns to temporally segment and label actions in a video, where the only supervision that is used are action sets. An evaluation on three datasets shows that our method still achieves good results although the amount of supervision is significantly smaller than for other related methods.



### Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?
- **Arxiv ID**: http://arxiv.org/abs/1706.00712v1
- **DOI**: 10.1109/TMI.2016.2535302
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.00712v1)
- **Published**: 2017-06-02 15:04:43+00:00
- **Updated**: 2017-06-02 15:04:43+00:00
- **Authors**: Nima Tajbakhsh, Jae Y. Shin, Suryakanth R. Gurudu, R. Todd Hurst, Christopher B. Kendall, Michael B. Gotway, Jianming Liang
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging. 35(5):1299-1312 (2016)
- **Summary**: Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: \emph{Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch?} To address this question, we considered 4 distinct medical imaging applications in 3 specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from 3 different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that (1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; (2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; (3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and (4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.



### Automating Carotid Intima-Media Thickness Video Interpretation with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.00719v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.00719v1)
- **Published**: 2017-06-02 15:21:09+00:00
- **Updated**: 2017-06-02 15:21:09+00:00
- **Authors**: Jae Y. Shin, Nima Tajbakhsh, R. Todd Hurst, Christopher B. Kendall, Jianming Liang
- **Comment**: J. Y. Shin, N. Tajbakhsh, R. T. Hurst, C. B. Kendall, and J. Liang.
  Automating carotid intima-media thickness video interpretation with
  convolutional neural networks. CVPR 2016, pp 2526-2535; N. Tajbakhsh, J. Y.
  Shin, R. T. Hurst, C. B. Kendall, and J. Liang. Automatic interpretation of
  CIMT videos using convolutional neural networks. Deep Learning for Medical
  Image Analysis, Academic Press, 2017
- **Journal**: None
- **Summary**: Cardiovascular disease (CVD) is the leading cause of mortality yet largely preventable, but the key to prevention is to identify at-risk individuals before adverse events. For predicting individual CVD risk, carotid intima-media thickness (CIMT), a noninvasive ultrasound method, has proven to be valuable, offering several advantages over CT coronary artery calcium score. However, each CIMT examination includes several ultrasound videos, and interpreting each of these CIMT videos involves three operations: (1) select three end-diastolic ultrasound frames (EUF) in the video, (2) localize a region of interest (ROI) in each selected frame, and (3) trace the lumen-intima interface and the media-adventitia interface in each ROI to measure CIMT. These operations are tedious, laborious, and time consuming, a serious limitation that hinders the widespread utilization of CIMT in clinical practice. To overcome this limitation, this paper presents a new system to automate CIMT video interpretation. Our extensive experiments demonstrate that the suggested system significantly outperforms the state-of-the-art methods. The superior performance is attributable to our unified framework based on convolutional neural networks (CNNs) coupled with our informative image representation and effective post-processing of the CNN outputs, which are uniquely designed for each of the above three operations.



### Yeah, Right, Uh-Huh: A Deep Learning Backchannel Predictor
- **Arxiv ID**: http://arxiv.org/abs/1706.01340v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.HC, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1706.01340v1)
- **Published**: 2017-06-02 17:05:26+00:00
- **Updated**: 2017-06-02 17:05:26+00:00
- **Authors**: Robin Ruede, Markus Müller, Sebastian Stüker, Alex Waibel
- **Comment**: None
- **Journal**: None
- **Summary**: Using supporting backchannel (BC) cues can make human-computer interaction more social. BCs provide a feedback from the listener to the speaker indicating to the speaker that he is still listened to. BCs can be expressed in different ways, depending on the modality of the interaction, for example as gestures or acoustic cues. In this work, we only considered acoustic cues. We are proposing an approach towards detecting BC opportunities based on acoustic input features like power and pitch. While other works in the field rely on the use of a hand-written rule set or specialized features, we made use of artificial neural networks. They are capable of deriving higher order features from input features themselves. In our setup, we first used a fully connected feed-forward network to establish an updated baseline in comparison to our previously proposed setup. We also extended this setup by the use of Long Short-Term Memory (LSTM) networks which have shown to outperform feed-forward based setups on various tasks. Our best system achieved an F1-Score of 0.37 using power and pitch features. Adding linguistic information using word2vec, the score increased to 0.39.



### A watershed-based algorithm to segment and classify cells in fluorescence microscopy images
- **Arxiv ID**: http://arxiv.org/abs/1706.00815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00815v1)
- **Published**: 2017-06-02 18:38:10+00:00
- **Updated**: 2017-06-02 18:38:10+00:00
- **Authors**: Lena R. Bartell, Lawrence J. Bonassar, Itai Cohen
- **Comment**: 8 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: Imaging assays of cellular function, especially those using fluorescent stains, are ubiquitous in the biological and medical sciences. Despite advances in computer vision, such images are often analyzed using only manual or rudimentary automated processes. Watershed-based segmentation is an effective technique for identifying objects in images; it outperforms commonly used image analysis methods, but requires familiarity with computer-vision techniques to be applied successfully. In this report, we present and implement a watershed-based image analysis and classification algorithm in a GUI, enabling a broad set of users to easily understand the algorithm and adjust the parameters to their specific needs. As an example, we implement this algorithm to find and classify cells in a complex imaging assay for mitochondrial function. In a second example, we demonstrate a workflow using manual comparisons and receiver operator characteristics to optimize the algorithm parameters for finding live and dead cells in a standard viability assay. Overall, this watershed-based algorithm is more advanced than traditional thresholding and can produce optimized, automated results. By incorporating associated pre-processing steps in the GUI, the algorithm is also easily adjusted, rendering it user-friendly.



### One-Sided Unsupervised Domain Mapping
- **Arxiv ID**: http://arxiv.org/abs/1706.00826v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00826v2)
- **Published**: 2017-06-02 19:31:30+00:00
- **Updated**: 2017-11-18 17:15:56+00:00
- **Authors**: Sagie Benaim, Lior Wolf
- **Comment**: to be published in NIPS 2017
- **Journal**: None
- **Summary**: In unsupervised domain mapping, the learner is given two unmatched datasets $A$ and $B$. The goal is to learn a mapping $G_{AB}$ that translates a sample in $A$ to the analog sample in $B$. Recent approaches have shown that when learning simultaneously both $G_{AB}$ and the inverse mapping $G_{BA}$, convincing mappings are obtained. In this work, we present a method of learning $G_{AB}$ without learning $G_{BA}$. This is done by learning a mapping that maintains the distance between a pair of samples. Moreover, good mappings are obtained, even by maintaining the distance between different parts of the same sample before and after mapping. We present experimental results that the new method not only allows for one sided mapping learning, but also leads to preferable numerical results over the existing circularity-based constraint. Our entire code is made publicly available at https://github.com/sagiebenaim/DistanceGAN .



### Multi-Class Model Fitting by Energy Minimization and Mode-Seeking
- **Arxiv ID**: http://arxiv.org/abs/1706.00827v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00827v2)
- **Published**: 2017-06-02 19:32:07+00:00
- **Updated**: 2017-11-16 08:22:07+00:00
- **Authors**: Daniel Barath, Jiri Matas
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a general formulation, called Multi-X, for multi-class multi-instance model fitting - the problem of interpreting the input data as a mixture of noisy observations originating from multiple instances of multiple classes. We extend the commonly used alpha-expansion-based technique with a new move in the label space. The move replaces a set of labels with the corresponding density mode in the model parameter domain, thus achieving fast and robust optimization. Key optimization parameters like the bandwidth of the mode seeking are set automatically within the algorithm. Considering that a group of outliers may form spatially coherent structures in the data, we propose a cross-validation-based technique removing statistically insignificant instances. Multi-X outperforms significantly the state-of-the-art on publicly available datasets for diverse problems: multiple plane and rigid motion detection; motion segmentation; simultaneous plane and cylinder fitting; circle and line fitting.



### Neural Network-Based Automatic Liver Tumor Segmentation With Random Forest-Based Candidate Filtering
- **Arxiv ID**: http://arxiv.org/abs/1706.00842v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00842v3)
- **Published**: 2017-06-02 20:33:22+00:00
- **Updated**: 2017-06-27 07:07:20+00:00
- **Authors**: Grzegorz Chlebus, Hans Meine, Jan Hendrik Moltz, Andrea Schenk
- **Comment**: Submitted to the ISBI 2017 LiTS Challenge
- **Journal**: None
- **Summary**: We present a fully automatic method employing convolutional neural networks based on the 2D U-net architecture and random forest classifier to solve the automatic liver lesion segmentation problem of the ISBI 2017 Liver Tumor Segmentation Challenge (LiTS). In order to constrain the ROI in which the tumors could be located, a liver segmentation is performed first. For the organ segmentation, an ensemble of convolutional networks is trained to segment a liver using a set of 179 liver CT datasets from liver surgery planning. Inside of the liver ROI a neural network, trained using 127 challenge training datasets, identifies tumor candidates, which are subsequently filtered with a random forest classifier yielding the final tumor segmentation. The evaluation on the 70 challenge test cases resulted in a mean Dice coefficient of 0.65, ranking our method in the second place.



