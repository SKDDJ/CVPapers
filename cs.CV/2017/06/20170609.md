# Arxiv Papers in cs.CV on 2017-06-09
### TextureGAN: Controlling Deep Image Synthesis with Texture Patches
- **Arxiv ID**: http://arxiv.org/abs/1706.02823v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1706.02823v3)
- **Published**: 2017-06-09 03:35:08+00:00
- **Updated**: 2018-04-14 20:11:56+00:00
- **Authors**: Wenqi Xian, Patsorn Sangkloy, Varun Agrawal, Amit Raj, Jingwan Lu, Chen Fang, Fisher Yu, James Hays
- **Comment**: CVPR 2018 spotlight
- **Journal**: None
- **Summary**: In this paper, we investigate deep image synthesis guided by sketch, color, and texture. Previous image synthesis methods can be controlled by sketch and color strokes but we are the first to examine texture control. We allow a user to place a texture patch on a sketch at arbitrary locations and scales to control the desired output texture. Our generative network learns to synthesize objects consistent with these texture suggestions. To achieve this, we develop a local texture loss in addition to adversarial and content loss to train the generative network. We conduct experiments using sketches generated from real images and textures sampled from a separate texture database and results show that our proposed algorithm is able to generate plausible images that are faithful to user controls. Ablation studies show that our proposed pipeline can generate more realistic images than adapting existing methods directly.



### Weakly supervised training of deep convolutional neural networks for overhead pedestrian localization in depth fields
- **Arxiv ID**: http://arxiv.org/abs/1706.02850v1
- **DOI**: 10.1109/AVSS.2017.8078490
- **Categories**: **cs.CV**, cs.NE, physics.soc-ph
- **Links**: [PDF](http://arxiv.org/pdf/1706.02850v1)
- **Published**: 2017-06-09 07:14:08+00:00
- **Updated**: 2017-06-09 07:14:08+00:00
- **Authors**: Alessandro Corbetta, Vlado Menkovski, Federico Toschi
- **Comment**: None
- **Journal**: Advanced Video and Signal Based Surveillance (AVSS), 2017 14th
  IEEE International Conference on
- **Summary**: Overhead depth map measurements capture sufficient amount of information to enable human experts to track pedestrians accurately. However, fully automating this process using image analysis algorithms can be challenging. Even though hand-crafted image analysis algorithms are successful in many common cases, they fail frequently when there are complex interactions of multiple objects in the image. Many of the assumptions underpinning the hand-crafted solutions do not hold in these cases and the multitude of exceptions are hard to model precisely. Deep Learning (DL) algorithms, on the other hand, do not require hand crafted solutions and are the current state-of-the-art in object localization in images. However, they require exceeding amount of annotations to produce successful models. In the case of object localization these annotations are difficult and time consuming to produce. In this work we present an approach for developing pedestrian localization models using DL algorithms with efficient weak supervision from an expert. We circumvent the need for annotation of large corpus of data by annotating only small amount of patches and relying on synthetic data augmentation as a vehicle for injecting expert knowledge in the model training. This approach of weak supervision through expert selection of representative patches, suitable transformations and synthetic data augmentations enables us to successfully develop DL models for pedestrian localization efficiently.



### Face Detection through Scale-Friendly Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.02863v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02863v1)
- **Published**: 2017-06-09 08:20:56+00:00
- **Updated**: 2017-06-09 08:20:56+00:00
- **Authors**: Shuo Yang, Yuanjun Xiong, Chen Change Loy, Xiaoou Tang
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: In this paper, we share our experience in designing a convolutional network-based face detector that could handle faces of an extremely wide range of scales. We show that faces with different scales can be modeled through a specialized set of deep convolutional networks with different structures. These detectors can be seamlessly integrated into a single unified network that can be trained end-to-end. In contrast to existing deep models that are designed for wide scale range, our network does not require an image pyramid input and the model is of modest complexity. Our network, dubbed ScaleFace, achieves promising performance on WIDER FACE and FDDB datasets with practical runtime speed. Specifically, our method achieves 76.4 average precision on the challenging WIDER FACE dataset and 96% recall rate on the FDDB dataset with 7 frames per second (fps) for 900 * 1300 input image.



### Class-specific Poisson denoising by patch-based importance sampling
- **Arxiv ID**: http://arxiv.org/abs/1706.02867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02867v1)
- **Published**: 2017-06-09 08:47:26+00:00
- **Updated**: 2017-06-09 08:47:26+00:00
- **Authors**: Milad Niknejad, Jose M. Bioucas-Dias, Mario A. T. Figueiredo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of recovering images degraded by Poisson noise, where the image is known to belong to a specific class. In the proposed method, a dataset of clean patches from images of the class of interest is clustered using multivariate Gaussian distributions. In order to recover the noisy image, each noisy patch is assigned to one of these distributions, and the corresponding minimum mean squared error (MMSE) estimate is obtained. We propose to use a self-normalized importance sampling approach, which is a method of the Monte-Carlo family, for the both determining the most likely distribution and approximating the MMSE estimate of the clean patch. Experimental results shows that our proposed method outperforms other methods for Poisson denoising at a low SNR regime.



### Learning to Learn from Noisy Web Videos
- **Arxiv ID**: http://arxiv.org/abs/1706.02884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02884v1)
- **Published**: 2017-06-09 10:25:05+00:00
- **Updated**: 2017-06-09 10:25:05+00:00
- **Authors**: Serena Yeung, Vignesh Ramanathan, Olga Russakovsky, Liyue Shen, Greg Mori, Li Fei-Fei
- **Comment**: To appear in CVPR 2017
- **Journal**: None
- **Summary**: Understanding the simultaneously very diverse and intricately fine-grained set of possible human actions is a critical open problem in computer vision. Manually labeling training videos is feasible for some action classes but doesn't scale to the full long-tailed distribution of actions. A promising way to address this is to leverage noisy data from web queries to learn new actions, using semi-supervised or "webly-supervised" approaches. However, these methods typically do not learn domain-specific knowledge, or rely on iterative hand-tuned data labeling policies. In this work, we instead propose a reinforcement learning-based formulation for selecting the right examples for training a classifier from noisy web search results. Our method uses Q-learning to learn a data labeling policy on a small labeled training dataset, and then uses this to automatically label noisy web data for new visual concepts. Experiments on the challenging Sports-1M action recognition benchmark as well as on additional fine-grained and newly emerging action classes demonstrate that our method is able to learn good labeling policies for noisy data and use this to learn accurate visual concept classifiers.



### DCCO: Towards Deformable Continuous Convolution Operators
- **Arxiv ID**: http://arxiv.org/abs/1706.02888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02888v1)
- **Published**: 2017-06-09 10:39:32+00:00
- **Updated**: 2017-06-09 10:39:32+00:00
- **Authors**: Joakim Johnander, Martin Danelljan, Fahad Shahbaz Khan, Michael Felsberg
- **Comment**: None
- **Journal**: None
- **Summary**: Discriminative Correlation Filter (DCF) based methods have shown competitive performance on tracking benchmarks in recent years. Generally, DCF based trackers learn a rigid appearance model of the target. However, this reliance on a single rigid appearance model is insufficient in situations where the target undergoes non-rigid transformations. In this paper, we propose a unified formulation for learning a deformable convolution filter. In our framework, the deformable filter is represented as a linear combination of sub-filters. Both the sub-filter coefficients and their relative locations are inferred jointly in our formulation. Experiments are performed on three challenging tracking benchmarks: OTB- 2015, TempleColor and VOT2016. Our approach improves the baseline method, leading to performance comparable to state-of-the-art.



### MirBot: A collaborative object recognition system for smartphones using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1706.02889v3
- **DOI**: 10.1016/j.neucom.2018.03.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02889v3)
- **Published**: 2017-06-09 10:50:43+00:00
- **Updated**: 2018-03-24 08:30:28+00:00
- **Authors**: Antonio Pertusa, Antonio-Javier Gallego, Marisa Bernabeu
- **Comment**: Accepted in Neurocomputing, 2018
- **Journal**: Neurocomputing, vol 293, 2018, Pages 87-99
- **Summary**: MirBot is a collaborative application for smartphones that allows users to perform object recognition. This app can be used to take a photograph of an object, select the region of interest and obtain the most likely class (dog, chair, etc.) by means of similarity search using features extracted from a convolutional neural network (CNN). The answers provided by the system can be validated by the user so as to improve the results for future queries. All the images are stored together with a series of metadata, thus enabling a multimodal incremental dataset labeled with synset identifiers from the WordNet ontology. This dataset grows continuously thanks to the users' feedback, and is publicly available for research. This work details the MirBot object recognition system, analyzes the statistics gathered after more than four years of usage, describes the image classification methodology, and performs an exhaustive evaluation using handcrafted features, convolutional neural codes and different transfer learning techniques. After comparing various models and transformation methods, the results show that the CNN features maintain the accuracy of MirBot constant over time, despite the increasing number of new classes. The app is freely available at the Apple and Google Play stores.



### Multi-Modal Obstacle Detection in Unstructured Environments with Conditional Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1706.02908v2
- **DOI**: 10.1002/rob.21866
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.02908v2)
- **Published**: 2017-06-09 11:48:25+00:00
- **Updated**: 2019-03-13 14:04:28+00:00
- **Authors**: Mikkel Kragh, James Underwood
- **Comment**: This is the accepted version of the following article: Kragh M,
  Underwood J. Multimodal obstacle detection in unstructured environments with
  conditional random fields. J Field Robotics. 2019, 1-20., which has been
  published in final form at https://doi.org/10.1002/rob.21866
- **Journal**: None
- **Summary**: Reliable obstacle detection and classification in rough and unstructured terrain such as agricultural fields or orchards remains a challenging problem. These environments involve large variations in both geometry and appearance, challenging perception systems that rely on only a single sensor modality. Geometrically, tall grass, fallen leaves, or terrain roughness can mistakenly be perceived as nontraversable or might even obscure actual obstacles. Likewise, traversable grass or dirt roads and obstacles such as trees and bushes might be visually ambiguous. In this paper, we combine appearance- and geometry-based detection methods by probabilistically fusing lidar and camera sensing with semantic segmentation using a conditional random field. We apply a state-of-the-art multimodal fusion algorithm from the scene analysis domain and adjust it for obstacle detection in agriculture with moving ground vehicles. This involves explicitly handling sparse point cloud data and exploiting both spatial, temporal, and multimodal links between corresponding 2D and 3D regions. The proposed method was evaluated on a diverse data set, comprising a dairy paddock and different orchards gathered with a perception research robot in Australia. Results showed that for a two-class classification problem (ground and nonground), only the camera leveraged from information provided by the other modality with an increase in the mean classification score of 0.5%. However, as more classes were introduced (ground, sky, vegetation, and object), both modalities complemented each other with improvements of 1.4% in 2D and 7.9% in 3D. Finally, introducing temporal links between successive frames resulted in improvements of 0.2% in 2D and 1.5% in 3D.



### Unsupervised learning of object frames by dense equivariant image labelling
- **Arxiv ID**: http://arxiv.org/abs/1706.02932v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.02932v2)
- **Published**: 2017-06-09 12:49:36+00:00
- **Updated**: 2017-11-18 02:36:48+00:00
- **Authors**: James Thewlis, Hakan Bilen, Andrea Vedaldi
- **Comment**: NIPS 2017
- **Journal**: None
- **Summary**: One of the key challenges of visual perception is to extract abstract models of 3D objects and object categories from visual measurements, which are affected by complex nuisance factors such as viewpoint, occlusion, motion, and deformations. Starting from the recent idea of viewpoint factorization, we propose a new approach that, given a large number of images of an object and no other supervision, can extract a dense object-centric coordinate frame. This coordinate frame is invariant to deformations of the images and comes with a dense equivariant labelling neural network that can map image pixels to their corresponding object coordinates. We demonstrate the applicability of this method to simple articulated objects and deformable objects such as human faces, learning embeddings from random synthetic transformations or optical flow correspondences, all without any manual supervision.



### Enhancing SDO/HMI images using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1706.02933v2
- **DOI**: 10.1051/0004-6361/201731344
- **Categories**: **astro-ph.SR**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.02933v2)
- **Published**: 2017-06-09 12:54:03+00:00
- **Updated**: 2018-01-30 19:45:05+00:00
- **Authors**: C. J. Diaz Baso, A. Asensio Ramos
- **Comment**: 13 pages, 10 figures. Accepted for publication in Astronomy &
  Astrophysics
- **Journal**: A&A 614, A5 (2018)
- **Summary**: The Helioseismic and Magnetic Imager (HMI) provides continuum images and magnetograms with a cadence better than one per minute. It has been continuously observing the Sun 24 hours a day for the past 7 years. The obvious trade-off between full disk observations and spatial resolution makes HMI not enough to analyze the smallest-scale events in the solar atmosphere. Our aim is to develop a new method to enhance HMI data, simultaneously deconvolving and super-resolving images and magnetograms. The resulting images will mimic observations with a diffraction-limited telescope twice the diameter of HMI. Our method, which we call Enhance, is based on two deep fully convolutional neural networks that input patches of HMI observations and output deconvolved and super-resolved data. The neural networks are trained on synthetic data obtained from simulations of the emergence of solar active regions. We have obtained deconvolved and supper-resolved HMI images. To solve this ill-defined problem with infinite solutions we have used a neural network approach to add prior information from the simulations. We test Enhance against Hinode data that has been degraded to a 28 cm diameter telescope showing very good consistency. The code is open source.



### Phase-error estimation and image reconstruction from digital-holography data using a Bayesian framework
- **Arxiv ID**: http://arxiv.org/abs/1708.01142v1
- **DOI**: 10.1364/JOSAA.34.001659
- **Categories**: **physics.data-an**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.01142v1)
- **Published**: 2017-06-09 14:07:10+00:00
- **Updated**: 2017-06-09 14:07:10+00:00
- **Authors**: Casey J. Pellizzari, Mark F. Spencer, Charles A. Bouman
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: The estimation of phase errors from digital-holography data is critical for applications such as imaging or wave-front sensing. Conventional techniques require multiple i.i.d. data and perform poorly in the presence of high noise or large phase errors. In this paper we propose a method to estimate isoplanatic phase errors from a single data realization. We develop a model-based iterative reconstruction algorithm which computes the maximum a posteriori estimate of the phase and the speckle-free object reflectance. Using simulated data, we show that the algorithm is robust against high noise and strong phase errors.



### An Ensemble Deep Learning Based Approach for Red Lesion Detection in Fundus Images
- **Arxiv ID**: http://arxiv.org/abs/1706.03008v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03008v2)
- **Published**: 2017-06-09 15:47:11+00:00
- **Updated**: 2017-10-12 19:44:36+00:00
- **Authors**: José Ignacio Orlando, Elena Prokofyeva, Mariana del Fresno, Matthew B. Blaschko
- **Comment**: Accepted for publication in Computer Methods and Programs in
  Biomedicine
- **Journal**: None
- **Summary**: Diabetic retinopathy is one of the leading causes of preventable blindness in the world. Its earliest sign are red lesions, a general term that groups both microaneurysms and hemorrhages. In daily clinical practice, these lesions are manually detected by physicians using fundus photographs. However, this task is tedious and time consuming, and requires an intensive effort due to the small size of the lesions and their lack of contrast. Computer-assisted diagnosis of DR based on red lesion detection is being actively explored due to its improvement effects both in clinicians consistency and accuracy. Several methods for detecting red lesions have been proposed in the literature, most of them based on characterizing lesion candidates using hand crafted features, and classifying them into true or false positive detections. Deep learning based approaches, by contrast, are scarce in this domain due to the high expense of annotating the lesions manually. In this paper we propose a novel method for red lesion detection based on combining both deep learned and domain knowledge. Features learned by a CNN are augmented by incorporating hand crafted features. Such ensemble vector of descriptors is used afterwards to identify true lesion candidates using a Random Forest classifier. We empirically observed that combining both sources of information significantly improve results with respect to using each approach separately. Furthermore, our method reported the highest performance on a per-lesion basis on DIARETDB1 and e-ophtha, and for screening and need for referral on MESSIDOR compared to a second human expert. Results highlight the fact that integrating manually engineered approaches with deep learned features is relevant to improve results when the networks are trained from lesion-level annotated data. An open source implementation of our system is publicly available online.



### Manifold Regularized Slow Feature Analysis for Dynamic Texture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1706.03015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03015v1)
- **Published**: 2017-06-09 16:06:25+00:00
- **Updated**: 2017-06-09 16:06:25+00:00
- **Authors**: Jie Miao, Xiangmin Xu, Xiaofen Xing, Dacheng Tao
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Dynamic textures exist in various forms, e.g., fire, smoke, and traffic jams, but recognizing dynamic texture is challenging due to the complex temporal variations. In this paper, we present a novel approach stemmed from slow feature analysis (SFA) for dynamic texture recognition. SFA extracts slowly varying features from fast varying signals. Fortunately, SFA is capable to leach invariant representations from dynamic textures. However, complex temporal variations require high-level semantic representations to fully achieve temporal slowness, and thus it is impractical to learn a high-level representation from dynamic textures directly by SFA. In order to learn a robust low-level feature to resolve the complexity of dynamic textures, we propose manifold regularized SFA (MR-SFA) by exploring the neighbor relationship of the initial state of each temporal transition and retaining the locality of their variations. Therefore, the learned features are not only slowly varying, but also partly predictable. MR-SFA for dynamic texture recognition is proposed in the following steps: 1) learning feature extraction functions as convolution filters by MR-SFA, 2) extracting local features by convolution and pooling, and 3) employing Fisher vectors to form a video-level representation for classification. Experimental results on dynamic texture and dynamic scene recognition datasets validate the effectiveness of the proposed approach.



### Okutama-Action: An Aerial View Video Dataset for Concurrent Human Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1706.03038v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03038v2)
- **Published**: 2017-06-09 16:54:51+00:00
- **Updated**: 2017-06-15 16:04:01+00:00
- **Authors**: Mohammadamin Barekatain, Miquel Martí, Hsueh-Fu Shih, Samuel Murray, Kotaro Nakayama, Yutaka Matsuo, Helmut Prendinger
- **Comment**: Computer Vision and Pattern Recognition Workshops (CVPRW), Hawaii,
  USA, 2017
- **Journal**: None
- **Summary**: Despite significant progress in the development of human action detection datasets and algorithms, no current dataset is representative of real-world aerial view scenarios. We present Okutama-Action, a new video dataset for aerial view concurrent human action detection. It consists of 43 minute-long fully-annotated sequences with 12 action classes. Okutama-Action features many challenges missing in current datasets, including dynamic transition of actions, significant changes in scale and aspect ratio, abrupt camera movement, as well as multi-labeled actors. As a result, our dataset is more challenging than existing ones, and will help push the field forward to enable real-world applications.



### Unsupervised Adaptive Re-identification in Open World Dynamic Camera Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.03112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03112v1)
- **Published**: 2017-06-09 20:17:55+00:00
- **Updated**: 2017-06-09 20:17:55+00:00
- **Authors**: Rameswar Panda, Amran Bhuiyan, Vittorio Murino, Amit K. Roy-Chowdhury
- **Comment**: CVPR 2017 Spotlight
- **Journal**: None
- **Summary**: Person re-identification is an open and challenging problem in computer vision. Existing approaches have concentrated on either designing the best feature representation or learning optimal matching metrics in a static setting where the number of cameras are fixed in a network. Most approaches have neglected the dynamic and open world nature of the re-identification problem, where a new camera may be temporarily inserted into an existing system to get additional information. To address such a novel and very practical problem, we propose an unsupervised adaptation scheme for re-identification models in a dynamic camera network. First, we formulate a domain perceptive re-identification method based on geodesic flow kernel that can effectively find the best source camera (already installed) to adapt with a newly introduced target camera, without requiring a very expensive training phase. Second, we introduce a transitive inference algorithm for re-identification that can exploit the information from best source camera to improve the accuracy across other camera pairs in a network of multiple cameras. Extensive experiments on four benchmark datasets demonstrate that the proposed approach significantly outperforms the state-of-the-art unsupervised learning based alternatives whilst being extremely efficient to compute.



### Collaborative Summarization of Topic-Related Videos
- **Arxiv ID**: http://arxiv.org/abs/1706.03114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03114v1)
- **Published**: 2017-06-09 20:23:43+00:00
- **Updated**: 2017-06-09 20:23:43+00:00
- **Authors**: Rameswar Panda, Amit K. Roy-Chowdhury
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Large collections of videos are grouped into clusters by a topic keyword, such as Eiffel Tower or Surfing, with many important visual concepts repeating across them. Such a topically close set of videos have mutual influence on each other, which could be used to summarize one of them by exploiting information from others in the set. We build on this intuition to develop a novel approach to extract a summary that simultaneously captures both important particularities arising in the given video, as well as, generalities identified from the set of videos. The topic-related videos provide visual context to identify the important parts of the video being summarized. We achieve this by developing a collaborative sparse optimization method which can be efficiently solved by a half-quadratic minimization algorithm. Our work builds upon the idea of collaborative techniques from information retrieval and natural language processing, which typically use the attributes of other similar objects to predict the attribute of a given object. Experiments on two challenging and diverse datasets well demonstrate the efficacy of our approach over state-of-the-art methods.



### Multi-View Surveillance Video Summarization via Joint Embedding and Sparse Optimization
- **Arxiv ID**: http://arxiv.org/abs/1706.03121v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03121v1)
- **Published**: 2017-06-09 20:56:20+00:00
- **Updated**: 2017-06-09 20:56:20+00:00
- **Authors**: Rameswar Panda, Amit K. Roy-Chowdhury
- **Comment**: IEEE Trans. on Multimedia, 2017 (In Press)
- **Journal**: None
- **Summary**: Most traditional video summarization methods are designed to generate effective summaries for single-view videos, and thus they cannot fully exploit the complicated intra and inter-view correlations in summarizing multi-view videos in a camera network. In this paper, with the aim of summarizing multi-view videos, we introduce a novel unsupervised framework via joint embedding and sparse representative selection. The objective function is two-fold. The first is to capture the multi-view correlations via an embedding, which helps in extracting a diverse set of representatives. The second is to use a `2;1- norm to model the sparsity while selecting representative shots for the summary. We propose to jointly optimize both of the objectives, such that embedding can not only characterize the correlations, but also indicate the requirements of sparse representative selection. We present an efficient alternating algorithm based on half-quadratic minimization to solve the proposed non-smooth and non-convex objective with convergence analysis. A key advantage of the proposed approach with respect to the state-of-the-art is that it can summarize multi-view videos without assuming any prior correspondences/alignment between them, e.g., uncalibrated camera networks. Rigorous experiments on several multi-view datasets demonstrate that our approach clearly outperforms the state-of-the-art methods.



### Diversity-aware Multi-Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1706.03123v1
- **DOI**: 10.1109/TIP.2017.2708902
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03123v1)
- **Published**: 2017-06-09 20:58:07+00:00
- **Updated**: 2017-06-09 20:58:07+00:00
- **Authors**: Rameswar Panda, Niluthpol Chowdhury Mithun, Amit K. Roy-Chowdhury
- **Comment**: IEEE Trans. on Image Processing, 2017 (In Press)
- **Journal**: None
- **Summary**: Most video summarization approaches have focused on extracting a summary from a single video; we propose an unsupervised framework for summarizing a collection of videos. We observe that each video in the collection may contain some information that other videos do not have, and thus exploring the underlying complementarity could be beneficial in creating a diverse informative summary. We develop a novel diversity-aware sparse optimization method for multi-video summarization by exploring the complementarity within the videos. Our approach extracts a multi-video summary which is both interesting and representative in describing the whole video collection. To efficiently solve our optimization problem, we develop an alternating minimization algorithm that minimizes the overall objective function with respect to one video at a time while fixing the other videos. Moreover, we introduce a new benchmark dataset, Tour20, that contains 140 videos with multiple human created summaries, which were acquired in a controlled experiment. Finally, by extensive experiments on the new Tour20 dataset and several other multi-view datasets, we show that the proposed approach clearly outperforms the state-of-the-art methods on the two problems-topic-oriented video summarization and multi-view video summarization in a camera network.



### Measurement-Adaptive Sparse Image Sampling and Recovery
- **Arxiv ID**: http://arxiv.org/abs/1706.03129v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03129v2)
- **Published**: 2017-06-09 21:05:37+00:00
- **Updated**: 2017-11-23 19:51:17+00:00
- **Authors**: Ali Taimori, Farokh Marvasti
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an adaptive and intelligent sparse model for digital image sampling and recovery. In the proposed sampler, we adaptively determine the number of required samples for retrieving image based on space-frequency-gradient information content of image patches. By leveraging texture in space, sparsity locations in DCT domain, and directional decomposition of gradients, the sampler structure consists of a combination of uniform, random, and nonuniform sampling strategies. For reconstruction, we model the recovery problem as a two-state cellular automaton to iteratively restore image with scalable windows from generation to generation. We demonstrate the recovery algorithm quickly converges after a few generations for an image with arbitrary degree of texture. For a given number of measurements, extensive experiments on standard image-sets, infra-red, and mega-pixel range imaging devices show that the proposed measurement matrix considerably increases the overall recovery performance, or equivalently decreases the number of sampled pixels for a specific recovery quality compared to random sampling matrix and Gaussian linear combinations employed by the state-of-the-art compressive sensing methods. In practice, the proposed measurement-adaptive sampling/recovery framework includes various applications from intelligent compressive imaging-based acquisition devices to computer vision and graphics, and image processing technology. Simulation codes are available online for reproduction purposes.



### Deep Learning for Isotropic Super-Resolution from Non-Isotropic 3D Electron Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1706.03142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03142v1)
- **Published**: 2017-06-09 22:17:16+00:00
- **Updated**: 2017-06-09 22:17:16+00:00
- **Authors**: Larissa Heinrich, John A. Bogovic, Stephan Saalfeld
- **Comment**: None
- **Journal**: None
- **Summary**: The most sophisticated existing methods to generate 3D isotropic super-resolution (SR) from non-isotropic electron microscopy (EM) are based on learned dictionaries. Unfortunately, none of the existing methods generate practically satisfying results. For 2D natural images, recently developed super-resolution methods that use deep learning have been shown to significantly outperform the previous state of the art.   We have adapted one of the most successful architectures (FSRCNN) for 3D super-resolution, and compared its performance to a 3D U-Net architecture that has not been used previously to generate super-resolution.   We trained both architectures on artificially downscaled isotropic ground truth from focused ion beam milling scanning EM (FIB-SEM) and tested the performance for various hyperparameter settings.   Our results indicate that both architectures can successfully generate 3D isotropic super-resolution from non-isotropic EM, with the U-Net performing consistently better. We propose several promising directions for practical application.



