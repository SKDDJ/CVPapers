# Arxiv Papers in cs.CV on 2017-06-16
### The Monkeytyping Solution to the YouTube-8M Video Understanding Challenge
- **Arxiv ID**: http://arxiv.org/abs/1706.05150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05150v1)
- **Published**: 2017-06-16 05:39:53+00:00
- **Updated**: 2017-06-16 05:39:53+00:00
- **Authors**: He-Da Wang, Teng Zhang, Ji Wu
- **Comment**: Submitted to the CVPR 2017 Workshop on YouTube-8M Large-Scale Video
  Understanding
- **Journal**: None
- **Summary**: This article describes the final solution of team monkeytyping, who finished in second place in the YouTube-8M video understanding challenge. The dataset used in this challenge is a large-scale benchmark for multi-label video classification. We extend the work in [1] and propose several improvements for frame sequence modeling. We propose a network structure called Chaining that can better capture the interactions between labels. Also, we report our approaches in dealing with multi-scale information and attention pooling. In addition, We find that using the output of model ensemble as a side target in training can boost single model performance. We report our experiments in bagging, boosting, cascade, and stacking, and propose a stacking algorithm called attention weighted stacking. Our final submission is an ensemble that consists of 74 sub models, all of which are listed in the appendix.



### A Fully Trainable Network with RNN-based Pooling
- **Arxiv ID**: http://arxiv.org/abs/1706.05157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05157v1)
- **Published**: 2017-06-16 06:42:15+00:00
- **Updated**: 2017-06-16 06:42:15+00:00
- **Authors**: Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, Yanbo Gao
- **Comment**: 17 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Pooling is an important component in convolutional neural networks (CNNs) for aggregating features and reducing computational burden. Compared with other components such as convolutional layers and fully connected layers which are completely learned from data, the pooling component is still handcrafted such as max pooling and average pooling. This paper proposes a learnable pooling function using recurrent neural networks (RNN) so that the pooling can be fully adapted to data and other components of the network, leading to an improved performance. Such a network with learnable pooling function is referred to as a fully trainable network (FTN). Experimental results have demonstrated that the proposed RNN-based pooling can well approximate the existing pooling functions and improve the performance of the network. Especially for small networks, the proposed FTN can improve the performance by seven percentage points in terms of error rate on the CIFAR-10 dataset compared with the traditional CNN.



### Interactive 3D Modeling with a Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1706.05170v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1706.05170v2)
- **Published**: 2017-06-16 08:01:09+00:00
- **Updated**: 2018-01-07 08:55:45+00:00
- **Authors**: Jerry Liu, Fisher Yu, Thomas Funkhouser
- **Comment**: Published at International Conference on 3D Vision 2017
  (http://irc.cs.sdu.edu.cn/3dv/index.html)
- **Journal**: None
- **Summary**: This paper proposes the idea of using a generative adversarial network (GAN) to assist a novice user in designing real-world shapes with a simple interface. The user edits a voxel grid with a painting interface (like Minecraft). Yet, at any time, he/she can execute a SNAP command, which projects the current voxel grid onto a latent shape manifold with a learned projection operator and then generates a similar, but more realistic, shape using a learned generator network. Then the user can edit the resulting shape and snap again until he/she is satisfied with the result. The main advantage of this approach is that the projection and generation operators assist novice users to create 3D models characteristic of a background distribution of object shapes, but without having to specify all the details. The core new research idea is to use a GAN to support this application. 3D GANs have previously been used for shape generation, interpolation, and completion, but never for interactive modeling. The new challenge for this application is to learn a projection operator that takes an arbitrary 3D voxel model and produces a latent vector on the shape manifold from which a similar and realistic shape can be generated. We develop algorithms for this and other steps of the SNAP processing pipeline and integrate them into a simple modeling tool. Experiments with these algorithms and tool suggest that GANs provide a promising approach to computer-assisted interactive modeling.



### Two-pixel polarimetric camera by compressive sensing
- **Arxiv ID**: http://arxiv.org/abs/1707.03705v1
- **DOI**: 10.1364/ao.57.00b102
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1707.03705v1)
- **Published**: 2017-06-16 09:11:05+00:00
- **Updated**: 2017-06-16 09:11:05+00:00
- **Authors**: Julien Fade, Estéban Perrotin, Jérôme Bobin
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an original concept of compressive sensing (CS) polarimetric imaging based on a digital micro-mirror (DMD) array and two single-pixel detectors. The polarimetric sensitivity of the proposed setup is due to an experimental imperfection of reflecting mirrors which is exploited here to form an original reconstruction problem, including a CS problem and a source separation task. We show that a two-step approach tackling each problem successively is outperformed by a dedicated combined reconstruction method, which is explicited in this article and preferably implemented through a reweighted FISTA algorithm. The combined reconstruction approach is then further improved by including physical constraints specific to the polarimetric imaging context considered, which are implemented in an original constrained GFB algorithm. Numerical simulations demonstrate the efficiency of the 2-pixel CS polarimetric imaging setup to retrieve polarimetric contrast data with significant compression rate and good reconstruction quality. The influence of experimental imperfections of the DMD are also analyzed through numerical simulations, and 2D polarimetric imaging reconstruction results are finally presented.



### FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis
- **Arxiv ID**: http://arxiv.org/abs/1706.05206v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05206v2)
- **Published**: 2017-06-16 10:08:53+00:00
- **Updated**: 2018-03-28 13:27:39+00:00
- **Authors**: Nitika Verma, Edmond Boyer, Jakob Verbeek
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have massively impacted visual recognition in 2D images, and are now ubiquitous in state-of-the-art approaches. CNNs do not easily extend, however, to data that are not represented by regular grids, such as 3D shape meshes or other graph-structured data, to which traditional local convolution operators do not directly apply. To address this problem, we propose a novel graph-convolution operator to establish correspondences between filter weights and graph neighborhoods with arbitrary connectivity. The key novelty of our approach is that these correspondences are dynamically computed from features learned by the network, rather than relying on predefined static coordinates over the graph as in previous work. We obtain excellent experimental results that significantly improve over previous state-of-the-art shape correspondence results. This shows that our approach can learn effective shape representations from raw input coordinates, without relying on shape descriptors.



### Self-ensembling for visual domain adaptation
- **Arxiv ID**: http://arxiv.org/abs/1706.05208v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05208v4)
- **Published**: 2017-06-16 10:10:42+00:00
- **Updated**: 2018-09-23 05:50:44+00:00
- **Authors**: Geoffrey French, Michal Mackiewicz, Mark Fisher
- **Comment**: 20 pages, 3 figure, accepted as a poster at ICLR 2018
- **Journal**: None
- **Summary**: This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et al., 2017) of temporal ensembling (Laine et al;, 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.



### Multispectral and Hyperspectral Image Fusion Using a 3-D-Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1706.05249v1
- **DOI**: 10.1109/LGRS.2017.2668299
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.05249v1)
- **Published**: 2017-06-16 12:38:44+00:00
- **Updated**: 2017-06-16 12:38:44+00:00
- **Authors**: Frosti Palsson, Johannes R. Sveinsson, Magnus O. Ulfarsson
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a method using a three dimensional convolutional neural network (3-D-CNN) to fuse together multispectral (MS) and hyperspectral (HS) images to obtain a high resolution hyperspectral image. Dimensionality reduction of the hyperspectral image is performed prior to fusion in order to significantly reduce the computational time and make the method more robust to noise. Experiments are performed on a data set simulated using a real hyperspectral image. The results obtained show that the proposed approach is very promising when compared to conventional methods. This is especially true when the hyperspectral image is corrupted by additive noise.



### Perceptual Generative Adversarial Networks for Small Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1706.05274v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05274v2)
- **Published**: 2017-06-16 13:41:54+00:00
- **Updated**: 2017-06-20 14:38:43+00:00
- **Authors**: Jianan Li, Xiaodan Liang, Yunchao Wei, Tingfa Xu, Jiashi Feng, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting small objects is notoriously challenging due to their low resolution and noisy representation. Existing object detection pipelines usually detect small objects through learning representations of all the objects at multiple scales. However, the performance gain of such ad hoc architectures is usually limited to pay off the computational cost. In this work, we address the small object detection problem by developing a single architecture that internally lifts representations of small objects to "super-resolved" ones, achieving similar characteristics as large objects and thus more discriminative for detection. For this purpose, we propose a new Perceptual Generative Adversarial Network (Perceptual GAN) model that improves small object detection through narrowing representation difference of small objects from the large ones. Specifically, its generator learns to transfer perceived poor representations of the small objects to super-resolved ones that are similar enough to real large objects to fool a competing discriminator. Meanwhile its discriminator competes with the generator to identify the generated representation and imposes an additional perceptual requirement - generated representations of small objects must be beneficial for detection purpose - on the generator. Extensive evaluations on the challenging Tsinghua-Tencent 100K and the Caltech benchmark well demonstrate the superiority of Perceptual GAN in detecting small objects, including traffic signs and pedestrians, over well-established state-of-the-arts.



### Robotic Ironing with 3D Perception and Force/Torque Feedback in Household Environments
- **Arxiv ID**: http://arxiv.org/abs/1706.05340v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, I.2.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1706.05340v1)
- **Published**: 2017-06-16 16:37:54+00:00
- **Updated**: 2017-06-16 16:37:54+00:00
- **Authors**: David Estevez, Juan G. Victores, Raul Fernandez-Fernandez, Carlos Balaguer
- **Comment**: Accepted and to be published on the 2017 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2017) that will be held in
  Vancouver, Canada, September 24-28, 2017
- **Journal**: None
- **Summary**: As robotic systems become more popular in household environments, the complexity of required tasks also increases. In this work we focus on a domestic chore deemed dull by a majority of the population, the task of ironing. The presented algorithm improves on the limited number of previous works by joining 3D perception with force/torque sensing, with emphasis on finding a practical solution with a feasible implementation in a domestic setting. Our algorithm obtains a point cloud representation of the working environment. From this point cloud, the garment is segmented and a custom Wrinkleness Local Descriptor (WiLD) is computed to determine the location of the present wrinkles. Using this descriptor, the most suitable ironing path is computed and, based on it, the manipulation algorithm performs the force-controlled ironing operation. Experiments have been performed with a humanoid robot platform, proving that our algorithm is able to detect successfully wrinkles present in garments and iteratively reduce the wrinkleness using an unmodified iron.



### Block-Matching Optical Flow for Dynamic Vision Sensor- Algorithm and FPGA Implementation
- **Arxiv ID**: http://arxiv.org/abs/1706.05415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05415v1)
- **Published**: 2017-06-16 19:45:47+00:00
- **Updated**: 2017-06-16 19:45:47+00:00
- **Authors**: Min Liu, Tobi Delbruck
- **Comment**: Published in ISCAS 2017
- **Journal**: None
- **Summary**: Rapid and low power computation of optical flow (OF) is potentially useful in robotics. The dynamic vision sensor (DVS) event camera produces quick and sparse output, and has high dynamic range, but conventional OF algorithms are frame-based and cannot be directly used with event-based cameras. Previous DVS OF methods do not work well with dense textured input and are designed for implementation in logic circuits. This paper proposes a new block-matching based DVS OF algorithm which is inspired by motion estimation methods used for MPEG video compression. The algorithm was implemented both in software and on FPGA. For each event, it computes the motion direction as one of 9 directions. The speed of the motion is set by the sample interval. Results show that the Average Angular Error can be improved by 30\% compared with previous methods. The OF can be calculated on FPGA with 50\,MHz clock in 0.2\,us per event (11 clock cycles), 20 times faster than a Java software implementation running on a desktop PC. Sample data is shown that the method works on scenes dominated by edges, sparse features, and dense texture.



