# Arxiv Papers in cs.CV on 2017-06-29
### Flow-free Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1706.09544v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09544v1)
- **Published**: 2017-06-29 02:15:15+00:00
- **Updated**: 2017-06-29 02:15:15+00:00
- **Authors**: Aditya Vora, Shanmuganathan Raman
- **Comment**: None
- **Journal**: None
- **Summary**: Segmenting foreground object from a video is a challenging task because of the large deformations of the objects, occlusions, and background clutter. In this paper, we propose a frame-by-frame but computationally efficient approach for video object segmentation by clustering visually similar generic object segments throughout the video. Our algorithm segments various object instances appearing in the video and then perform clustering in order to group visually similar segments into one cluster. Since the object that needs to be segmented appears in most part of the video, we can retrieve the foreground segments from the cluster having maximum number of segments, thus filtering out noisy segments that do not represent any object. We then apply a track and fill approach in order to localize the objects in the frames where the object segmentation framework fails to segment any object. Our algorithm performs comparably to the recent automatic methods for video object segmentation when benchmarked on DAVIS dataset while being computationally much faster.



### Online Convolutional Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1706.09563v2
- **DOI**: 10.1109/ICIP.2017.8296573
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1706.09563v2)
- **Published**: 2017-06-29 03:25:32+00:00
- **Updated**: 2017-08-30 20:14:46+00:00
- **Authors**: Jialin Liu, Cristina Garcia-Cardona, Brendt Wohlberg, Wotao Yin
- **Comment**: Accepted to be presented at ICIP 2017
- **Journal**: Proceedings of IEEE International Conference on Image Processing
  (ICIP), 2017, pp. 1707-1711
- **Summary**: While a number of different algorithms have recently been proposed for convolutional dictionary learning, this remains an expensive problem. The single biggest impediment to learning from large training sets is the memory requirements, which grow at least linearly with the size of the training set since all existing methods are batch algorithms. The work reported here addresses this limitation by extending online dictionary learning ideas to the convolutional context.



### R2CNN: Rotational Region CNN for Orientation Robust Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1706.09579v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09579v2)
- **Published**: 2017-06-29 05:00:38+00:00
- **Updated**: 2017-06-30 13:01:52+00:00
- **Authors**: Yingying Jiang, Xiangyu Zhu, Xiaobing Wang, Shuli Yang, Wei Li, Hua Wang, Pei Fu, Zhenbo Luo
- **Comment**: 8 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: In this paper, we propose a novel method called Rotational Region CNN (R2CNN) for detecting arbitrary-oriented texts in natural scene images. The framework is based on Faster R-CNN [1] architecture. First, we use the Region Proposal Network (RPN) to generate axis-aligned bounding boxes that enclose the texts with different orientations. Second, for each axis-aligned text box proposed by RPN, we extract its pooled features with different pooled sizes and the concatenated features are used to simultaneously predict the text/non-text score, axis-aligned box and inclined minimum area box. At last, we use an inclined non-maximum suppression to get the detection results. Our approach achieves competitive results on text detection benchmarks: ICDAR 2015 and ICDAR 2013.



### CS591 Report: Application of siamesa network in 2D transformation
- **Arxiv ID**: http://arxiv.org/abs/1706.09598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09598v1)
- **Published**: 2017-06-29 07:13:42+00:00
- **Updated**: 2017-06-29 07:13:42+00:00
- **Authors**: Dorothy Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been extensively used various aspects of computer vision area. Deep learning separate itself from traditional neural network by having a much deeper and complicated network layers in its network structures. Traditionally, deep neural network is abundantly used in computer vision tasks including classification and detection and has achieve remarkable success and set up a new state of the art results in these fields. Instead of using neural network for vision recognition and detection. I will show the ability of neural network to do image registration, synthesis of images and image retrieval in this report.



### Actor-Critic Sequence Training for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1706.09601v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09601v2)
- **Published**: 2017-06-29 07:26:05+00:00
- **Updated**: 2017-11-28 01:32:37+00:00
- **Authors**: Li Zhang, Flood Sung, Feng Liu, Tao Xiang, Shaogang Gong, Yongxin Yang, Timothy M. Hospedales
- **Comment**: None
- **Journal**: None
- **Summary**: Generating natural language descriptions of images is an important capability for a robot or other visual-intelligence driven AI agent that may need to communicate with human users about what it is seeing. Such image captioning methods are typically trained by maximising the likelihood of ground-truth annotated caption given the image. While simple and easy to implement, this approach does not directly maximise the language quality metrics we care about such as CIDEr. In this paper we investigate training image captioning methods based on actor-critic reinforcement learning in order to directly optimise non-differentiable quality metrics of interest. By formulating a per-token advantage and value computation strategy in this novel reinforcement learning based captioning model, we show that it is possible to achieve the state of the art performance on the widely used MSCOCO benchmark.



### Weakly-supervised localization of diabetic retinopathy lesions in retinal fundus images
- **Arxiv ID**: http://arxiv.org/abs/1706.09634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09634v1)
- **Published**: 2017-06-29 09:15:56+00:00
- **Updated**: 2017-06-29 09:15:56+00:00
- **Authors**: Waleed M. Gondal, Jan M. Köhler, René Grzeszick, Gernot A. Fink, Michael Hirsch
- **Comment**: Accepted in Proc. IEEE International Conference on Image Processing
  (ICIP), 2017
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) show impressive performance for image classification and detection, extending heavily to the medical image domain. Nevertheless, medical experts are sceptical in these predictions as the nonlinear multilayer structure resulting in a classification outcome is not directly graspable. Recently, approaches have been shown which help the user to understand the discriminative regions within an image which are decisive for the CNN to conclude to a certain class. Although these approaches could help to build trust in the CNNs predictions, they are only slightly shown to work with medical image data which often poses a challenge as the decision for a class relies on different lesion areas scattered around the entire image. Using the DiaretDB1 dataset, we show that on retina images different lesion areas fundamental for diabetic retinopathy are detected on an image level with high accuracy, comparable or exceeding supervised methods. On lesion level, we achieve few false positives with high sensitivity, though, the network is solely trained on image-level labels which do not include information about existing lesions. Classifying between diseased and healthy images, we achieve an AUC of 0.954 on the DiaretDB1.



### Co-salient Object Detection Based on Deep Saliency Networks and Seed Propagation over an Integrated Graph
- **Arxiv ID**: http://arxiv.org/abs/1706.09650v1
- **DOI**: 10.1109/TIP.2018.2859752
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09650v1)
- **Published**: 2017-06-29 09:40:48+00:00
- **Updated**: 2017-06-29 09:40:48+00:00
- **Authors**: Dong-ju Jeong, Insung Hwang, Nam Ik Cho
- **Comment**: 13 pages, 10 figures, 3 tables
- **Journal**: None
- **Summary**: This paper presents a co-salient object detection method to find common salient regions in a set of images. We utilize deep saliency networks to transfer co-saliency prior knowledge and better capture high-level semantic information, and the resulting initial co-saliency maps are enhanced by seed propagation steps over an integrated graph. The deep saliency networks are trained in a supervised manner to avoid online weakly supervised learning and exploit them not only to extract high-level features but also to produce both intra- and inter-image saliency maps. Through a refinement step, the initial co-saliency maps can uniformly highlight co-salient regions and locate accurate object boundaries. To handle input image groups inconsistent in size, we propose to pool multi-regional descriptors including both within-segment and within-group information. In addition, the integrated multilayer graph is constructed to find the regions that the previous steps may not detect by seed propagation with low-level descriptors. In this work, we utilize the useful complementary components of high-, low-level information, and several learning-based steps. Our experiments have demonstrated that the proposed approach outperforms comparable co-saliency detection methods on widely used public databases and can also be directly applied to co-segmentation tasks.



### Tensor-based approach to accelerate deformable part models
- **Arxiv ID**: http://arxiv.org/abs/1707.03268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03268v1)
- **Published**: 2017-06-29 11:33:18+00:00
- **Updated**: 2017-06-29 11:33:18+00:00
- **Authors**: D. V. Parkhomenko, I. L. Mazurenko
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: This article provides next step towards solving speed bottleneck of any system that intensively uses convolutions operations (e.g. CNN). Method described in the article is applied on deformable part models (DPM) algorithm. Method described here is based on multidimensional tensors and provides efficient tradeoff between DPM performance and accuracy. Experiments on various databases, including Pascal VOC, show that the proposed method allows decreasing a number of convolutions up to 4.5 times compared with DPM v.5, while maintaining similar accuracy. If insignificant accuracy degradation is allowable, higher computational gain can be achieved. The method consists of filters tensor decomposition and convolutions shortening using the decomposed filter. Mathematical overview of the proposed method as well as simulation results are provided.



### Iterative Spectral Clustering for Unsupervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1706.09719v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09719v1)
- **Published**: 2017-06-29 12:37:44+00:00
- **Updated**: 2017-06-29 12:37:44+00:00
- **Authors**: Aditya Vora, Shanmuganathan Raman
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of unsupervised object localization in an image. Unlike previous supervised and weakly supervised algorithms that require bounding box or image level annotations for training classifiers in order to learn features representing the object, we propose a simple yet effective technique for localization using iterative spectral clustering. This iterative spectral clustering approach along with appropriate cluster selection strategy in each iteration naturally helps in searching of object region in the image. In order to estimate the final localization window, we group the proposals obtained from the iterative spectral clustering step based on the perceptual similarity, and average the coordinates of the proposals from the top scoring groups. We benchmark our algorithm on challenging datasets like Object Discovery and PASCAL VOC 2007, achieving an average CorLoc percentage of 51% and 35% respectively which is comparable to various other weakly supervised algorithms despite being completely unsupervised.



### Improving Speech Related Facial Action Unit Recognition by Audiovisual Information Fusion
- **Arxiv ID**: http://arxiv.org/abs/1706.10197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.10197v1)
- **Published**: 2017-06-29 14:36:07+00:00
- **Updated**: 2017-06-29 14:36:07+00:00
- **Authors**: Zibo Meng, Shizhong Han, Ping Liu, Yan Tong
- **Comment**: arXiv admin note: text overlap with arXiv:1706.07536
- **Journal**: None
- **Summary**: It is challenging to recognize facial action unit (AU) from spontaneous facial displays, especially when they are accompanied by speech. The major reason is that the information is extracted from a single source, i.e., the visual channel, in the current practice. However, facial activity is highly correlated with voice in natural human communications.   Instead of solely improving visual observations, this paper presents a novel audiovisual fusion framework, which makes the best use of visual and acoustic cues in recognizing speech-related facial AUs. In particular, a dynamic Bayesian network (DBN) is employed to explicitly model the semantic and dynamic physiological relationships between AUs and phonemes as well as measurement uncertainty. A pilot audiovisual AU-coded database has been collected to evaluate the proposed framework, which consists of a "clean" subset containing frontal faces under well controlled circumstances and a challenging subset with large head movements and occlusions. Experiments on this database have demonstrated that the proposed framework yields significant improvement in recognizing speech-related AUs compared to the state-of-the-art visual-based methods especially for those AUs whose visual observations are impaired during speech, and more importantly also outperforms feature-level fusion methods by explicitly modeling and exploiting physiological relationships between AUs and phonemes.



### Robust Face Tracking using Multiple Appearance Models and Graph Relational Learning
- **Arxiv ID**: http://arxiv.org/abs/1706.09806v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09806v2)
- **Published**: 2017-06-29 15:28:24+00:00
- **Updated**: 2018-08-30 16:26:36+00:00
- **Authors**: Tanushri Chakravorty, Guillaume-Alexandre Bilodeau, Eric Granger
- **Comment**: Paper is under consideration at CVIU
- **Journal**: None
- **Summary**: This paper addresses the problem of appearance matching across different challenges while doing visual face tracking in real-world scenarios. In this paper, FaceTrack is proposed that utilizes multiple appearance models with its long-term and short-term appearance memory for efficient face tracking. It demonstrates robustness to deformation, in-plane and out-of-plane rotation, scale, distractors and background clutter. It capitalizes on the advantages of the tracking-by-detection, by using a face detector that tackles drastic scale appearance change of a face. The detector also helps to reinitialize FaceTrack during drift. A weighted score-level fusion strategy is proposed to obtain the face tracking output having the highest fusion score by generating candidates around possible face locations. The tracker showcases impressive performance when initiated automatically by outperforming many state-of-the-art trackers, except Struck by a very minute margin: 0.001 in precision and 0.017 in success respectively.



### What's Mine is Yours: Pretrained CNNs for Limited Training Sonar ATR
- **Arxiv ID**: http://arxiv.org/abs/1706.09858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09858v1)
- **Published**: 2017-06-29 17:13:37+00:00
- **Updated**: 2017-06-29 17:13:37+00:00
- **Authors**: John McKay, Isaac Gerg, Vishal Monga, Raghu Raj
- **Comment**: Accepted to OCEANS 2017 - Anchorage (Conference)
- **Journal**: None
- **Summary**: Finding mines in Sonar imagery is a significant problem with a great deal of relevance for seafaring military and commercial endeavors. Unfortunately, the lack of enormous Sonar image data sets has prevented automatic target recognition (ATR) algorithms from some of the same advances seen in other computer vision fields. Namely, the boom in convolutional neural nets (CNNs) which have been able to achieve incredible results - even surpassing human actors - has not been an easily feasible route for many practitioners of Sonar ATR. We demonstrate the power of one avenue to incorporating CNNs into Sonar ATR: transfer learning. We first show how well a straightforward, flexible CNN feature-extraction strategy can be used to obtain impressive if not state-of-the-art results. Secondly, we propose a way to utilize the powerful transfer learning approach towards multiple instance target detection and identification within a provided synthetic aperture Sonar data set.



### Scale-Aware Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1706.09876v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09876v1)
- **Published**: 2017-06-29 17:40:31+00:00
- **Updated**: 2017-06-29 17:40:31+00:00
- **Authors**: Zekun Hao, Yu Liu, Hongwei Qin, Junjie Yan, Xiu Li, Xiaolin Hu
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: Convolutional neural network (CNN) based face detectors are inefficient in handling faces of diverse scales. They rely on either fitting a large single model to faces across a large scale range or multi-scale testing. Both are computationally expensive. We propose Scale-aware Face Detector (SAFD) to handle scale explicitly using CNN, and achieve better performance with less computation cost. Prior to detection, an efficient CNN predicts the scale distribution histogram of the faces. Then the scale histogram guides the zoom-in and zoom-out of the image. Since the faces will be approximately in uniform scale after zoom, they can be detected accurately even with much smaller CNN. Actually, more than 99% of the faces in AFW can be covered with less than two zooms per image. Extensive experiments on FDDB, MALF and AFW show advantages of SAFD.



### Automatic Face Image Quality Prediction
- **Arxiv ID**: http://arxiv.org/abs/1706.09887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.09887v1)
- **Published**: 2017-06-29 17:58:48+00:00
- **Updated**: 2017-06-29 17:58:48+00:00
- **Authors**: Lacey Best-Rowden, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Face image quality can be defined as a measure of the utility of a face image to automatic face recognition. In this work, we propose (and compare) two methods for automatic face image quality based on target face quality values from (i) human assessments of face image quality (matcher-independent), and (ii) quality values computed from similarity scores (matcher-dependent). A support vector regression model trained on face features extracted using a deep convolutional neural network (ConvNet) is used to predict the quality of a face image. The proposed methods are evaluated on two unconstrained face image databases, LFW and IJB-A, which both contain facial variations with multiple quality factors. Evaluation of the proposed automatic face image quality measures shows we are able to reduce the FNMR at 1% FMR by at least 13% for two face matchers (a COTS matcher and a ConvNet matcher) by using the proposed face quality to select subsets of face images and video frames for matching templates (i.e., multiple faces per subject) in the IJB-A protocol. To our knowledge, this is the first work to utilize human assessments of face image quality in designing a predictor of unconstrained face quality that is shown to be effective in cross-database evaluation.



