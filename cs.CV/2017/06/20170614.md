# Arxiv Papers in cs.CV on 2017-06-14
### When Image Denoising Meets High-Level Vision Tasks: A Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1706.04284v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04284v3)
- **Published**: 2017-06-14 00:04:56+00:00
- **Updated**: 2018-04-16 19:33:33+00:00
- **Authors**: Ding Liu, Bihan Wen, Xianming Liu, Zhangyang Wang, Thomas S. Huang
- **Comment**: the 27th International Joint Conference on Artificial Intelligence
  (2018)
- **Journal**: None
- **Summary**: Conventionally, image denoising and high-level vision tasks are handled separately in computer vision. In this paper, we cope with the two jointly and explore the mutual influence between them. First we propose a convolutional neural network for image denoising which achieves the state-of-the-art performance. Second we propose a deep neural network solution that cascades two modules for image denoising and various high-level tasks, respectively, and use the joint loss for updating only the denoising network via back-propagation. We demonstrate that on one hand, the proposed denoiser has the generality to overcome the performance degradation of different high-level vision tasks. On the other hand, with the guidance of high-level vision information, the denoising network can generate more visually appealing results. To the best of our knowledge, this is the first work investigating the benefit of exploiting image semantics simultaneously for image denoising and high-level vision tasks via deep learning. The code is available online https://github.com/Ding-Liu/DeepDenoising.



### Saliency detection by aggregating complementary background template with optimization framework
- **Arxiv ID**: http://arxiv.org/abs/1706.04285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04285v1)
- **Published**: 2017-06-14 00:06:02+00:00
- **Updated**: 2017-06-14 00:06:02+00:00
- **Authors**: Chenxing Xia, Hanling Zhang, Xiuju Gao
- **Comment**: 28 pages,10 figures
- **Journal**: None
- **Summary**: This paper proposes an unsupervised bottom-up saliency detection approach by aggregating complementary background template with refinement. Feature vectors are extracted from each superpixel to cover regional color, contrast and texture information. By using these features, a coarse detection for salient region is realized based on background template achieved by different combinations of boundary regions instead of only treating four boundaries as background. Then, by ranking the relevance of the image nodes with foreground cues extracted from the former saliency map, we obtain an improved result. Finally, smoothing operation is utilized to refine the foreground-based saliency map to improve the contrast between salient and non-salient regions until a close to binary saliency map is reached. Experimental results show that the proposed algorithm generates more accurate saliency maps and performs favorably against the state-off-the-art saliency detection methods on four publicly available datasets.



### Accurate Pulmonary Nodule Detection in Computed Tomography Images Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.04303v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04303v3)
- **Published**: 2017-06-14 03:31:04+00:00
- **Updated**: 2017-08-29 00:26:48+00:00
- **Authors**: Jia Ding, Aoxue Li, Zhiqiang Hu, Liwei Wang
- **Comment**: MICCAI 2017 accepted
- **Journal**: None
- **Summary**: Early detection of pulmonary cancer is the most promising way to enhance a patient's chance for survival. Accurate pulmonary nodule detection in computed tomography (CT) images is a crucial step in diagnosing pulmonary cancer. In this paper, inspired by the successful use of deep convolutional neural networks (DCNNs) in natural image recognition, we propose a novel pulmonary nodule detection approach based on DCNNs. We first introduce a deconvolutional structure to Faster Region-based Convolutional Neural Network (Faster R-CNN) for candidate detection on axial slices. Then, a three-dimensional DCNN is presented for the subsequent false positive reduction. Experimental results of the LUng Nodule Analysis 2016 (LUNA16) Challenge demonstrate the superior detection performance of the proposed approach on nodule detection(average FROC-score of 0.891, ranking the 1st place over all submitted results).



### Photo-realistic Facial Texture Transfer
- **Arxiv ID**: http://arxiv.org/abs/1706.04306v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04306v1)
- **Published**: 2017-06-14 03:45:50+00:00
- **Updated**: 2017-06-14 03:45:50+00:00
- **Authors**: Parneet Kaur, Hang Zhang, Kristin J. Dana
- **Comment**: None
- **Journal**: None
- **Summary**: Style transfer methods have achieved significant success in recent years with the use of convolutional neural networks. However, many of these methods concentrate on artistic style transfer with few constraints on the output image appearance. We address the challenging problem of transferring face texture from a style face image to a content face image in a photorealistic manner without changing the identity of the original content image. Our framework for face texture transfer (FaceTex) augments the prior work of MRF-CNN with a novel facial semantic regularization that incorporates a face prior regularization smoothly suppressing the changes around facial meso-structures (e.g eyes, nose and mouth) and a facial structure loss function which implicitly preserves the facial structure so that face texture can be transferred without changing the original identity. We demonstrate results on face images and compare our approach with recent state-of-the-art methods. Our results demonstrate superior texture transfer because of the ability to maintain the identity of the original face image.



### Teaching Compositionality to CNNs
- **Arxiv ID**: http://arxiv.org/abs/1706.04313v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.04313v1)
- **Published**: 2017-06-14 04:34:59+00:00
- **Updated**: 2017-06-14 04:34:59+00:00
- **Authors**: Austin Stone, Huayan Wang, Michael Stark, Yi Liu, D. Scott Phoenix, Dileep George
- **Comment**: Preprint appearing in CVPR 2017
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have shown great success in computer vision, approaching human-level performance when trained for specific tasks via application-specific loss functions. In this paper, we propose a method for augmenting and training CNNs so that their learned features are compositional. It encourages networks to form representations that disentangle objects from their surroundings and from each other, thereby promoting better generalization. Our method is agnostic to the specific details of the underlying CNN to which it is applied and can in principle be used with any CNN. As we show in our experiments, the learned representations lead to feature activations that are more localized and improve performance over non-compositional baselines in object recognition tasks.



### Hierarchical Gaussian Descriptors with Application to Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1706.04318v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04318v1)
- **Published**: 2017-06-14 05:16:16+00:00
- **Updated**: 2017-06-14 05:16:16+00:00
- **Authors**: Tetsu Matsukawa, Takahiro Okabe, Einoshin Suzuki, Yoichi Sato
- **Comment**: 14 pages, 12 figures, 4 tables
- **Journal**: None
- **Summary**: Describing the color and textural information of a person image is one of the most crucial aspects of person re-identification (re-id). In this paper, we present novel meta-descriptors based on a hierarchical distribution of pixel features. Although hierarchical covariance descriptors have been successfully applied to image classification, the mean information of pixel features, which is absent from the covariance, tends to be the major discriminative information for person re-id. To solve this problem, we describe a local region in an image via hierarchical Gaussian distribution in which both means and covariances are included in their parameters. More specifically, the region is modeled as a set of multiple Gaussian distributions in which each Gaussian represents the appearance of a local patch. The characteristics of the set of Gaussians are again described by another Gaussian distribution. In both steps, we embed the parameters of the Gaussian into a point of Symmetric Positive Definite (SPD) matrix manifold. By changing the way to handle mean information in this embedding, we develop two hierarchical Gaussian descriptors. Additionally, we develop feature norm normalization methods with the ability to alleviate the biased trends that exist on the descriptors. The experimental results conducted on five public datasets indicate that the proposed descriptors achieve remarkably high performance on person re-id.



### Zoom-in-Net: Deep Mining Lesions for Diabetic Retinopathy Detection
- **Arxiv ID**: http://arxiv.org/abs/1706.04372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04372v1)
- **Published**: 2017-06-14 09:13:52+00:00
- **Updated**: 2017-06-14 09:13:52+00:00
- **Authors**: Zhe Wang, Yanxin Yin, Jianping Shi, Wei Fang, Hongsheng Li, Xiaogang Wang
- **Comment**: accepted by MICCAI 2017
- **Journal**: None
- **Summary**: We propose a convolution neural network based algorithm for simultaneously diagnosing diabetic retinopathy and highlighting suspicious regions. Our contributions are two folds: 1) a network termed Zoom-in-Net which mimics the zoom-in process of a clinician to examine the retinal images. Trained with only image-level supervisions, Zoomin-Net can generate attention maps which highlight suspicious regions, and predicts the disease level accurately based on both the whole image and its high resolution suspicious patches. 2) Only four bounding boxes generated from the automatically learned attention maps are enough to cover 80% of the lesions labeled by an experienced ophthalmologist, which shows good localization ability of the attention maps. By clustering features at high response locations on the attention maps, we discover meaningful clusters which contain potential lesions in diabetic retinopathy. Experiments show that our algorithm outperform the state-of-the-art methods on two datasets, EyePACS and Messidor.



### Shape-Color Differential Moment Invariants under Affine Transformations
- **Arxiv ID**: http://arxiv.org/abs/1706.04382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04382v1)
- **Published**: 2017-06-14 09:42:16+00:00
- **Updated**: 2017-06-14 09:42:16+00:00
- **Authors**: Hanlin Mo, Shirui Li, You Hao, Hua Li
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: We propose the general construction formula of shape-color primitives by using partial differentials of each color channel in this paper. By using all kinds of shape-color primitives, shape-color differential moment invariants can be constructed very easily, which are invariant to the shape affine and color affine transforms. 50 instances of SCDMIs are obtained finally. In experiments, several commonly used color descriptors and SCDMIs are used in image classification and retrieval of color images, respectively. By comparing the experimental results, we find that SCDMIs get better results.



### Alignment Distances on Systems of Bags
- **Arxiv ID**: http://arxiv.org/abs/1706.04388v1
- **DOI**: 10.1109/TCSVT.2017.2715851
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04388v1)
- **Published**: 2017-06-14 09:58:32+00:00
- **Updated**: 2017-06-14 09:58:32+00:00
- **Authors**: Alexander Sagel, Martin Kleinsteuber
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research in image and video recognition indicates that many visual processes can be thought of as being generated by a time-varying generative model. A nearby descriptive model for visual processes is thus a statistical distribution that varies over time. Specifically, modeling visual processes as streams of histograms generated by a kernelized linear dynamic system turns out to be efficient. We refer to such a model as a System of Bags. In this work, we investigate Systems of Bags with special emphasis on dynamic scenes and dynamic textures. Parameters of linear dynamic systems suffer from ambiguities. In order to cope with these ambiguities in the kernelized setting, we develop a kernelized version of the alignment distance. For its computation, we use a Jacobi-type method and prove its convergence to a set of critical points. We employ it as a dissimilarity measure on Systems of Bags. As such, it outperforms other known dissimilarity measures for kernelized linear dynamic systems, in particular the Martin Distance and the Maximum Singular Value Distance, in every tested classification setting. A considerable margin can be observed in settings, where classification is performed with respect to an abstract mean of video sets. For this scenario, the presented approach can outperform state-of-the-art techniques, such as Dynamic Fractal Spectrum or Orthogonal Tensor Dictionary Learning.



### $ν$-net: Deep Learning for Generalized Biventricular Cardiac Mass and Function Parameters
- **Arxiv ID**: http://arxiv.org/abs/1706.04397v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.04397v1)
- **Published**: 2017-06-14 10:36:30+00:00
- **Updated**: 2017-06-14 10:36:30+00:00
- **Authors**: Hinrich B Winther, Christian Hundt, Bertil Schmidt, Christoph Czerner, Johann Bauersachs, Frank Wacker, Jens Vogel-Claussen
- **Comment**: None
- **Journal**: None
- **Summary**: Background: Cardiac MRI derived biventricular mass and function parameters, such as end-systolic volume (ESV), end-diastolic volume (EDV), ejection fraction (EF), stroke volume (SV), and ventricular mass (VM) are clinically well established. Image segmentation can be challenging and time-consuming, due to the complex anatomy of the human heart.   Objectives: This study introduces $\nu$-net (/nju:n$\varepsilon$t/) -- a deep learning approach allowing for fully-automated high quality segmentation of right (RV) and left ventricular (LV) endocardium and epicardium for extraction of cardiac function parameters.   Methods: A set consisting of 253 manually segmented cases has been used to train a deep neural network. Subsequently, the network has been evaluated on 4 different multicenter data sets with a total of over 1000 cases.   Results: For LV EF the intraclass correlation coefficient (ICC) is 98, 95, and 80 % (95 %), and for RV EF 96, and 87 % (80 %) on the respective data sets (human expert ICCs reported in parenthesis). The LV VM ICC is 95, and 94 % (84 %), and the RV VM ICC is 83, and 83 % (54 %). This study proposes a simple adjustment procedure, allowing for the adaptation to distinct segmentation philosophies. $\nu$-net exhibits state of-the-art performance in terms of dice coefficient.   Conclusions: Biventricular mass and function parameters can be determined reliably in high quality by applying a deep neural network for cardiac MRI segmentation, especially in the anatomically complex right ventricle. Adaption to individual segmentation styles by applying a simple adjustment procedure is viable, allowing for the processing of novel data without time-consuming additional training.



### Enhanced discrete particle swarm optimization path planning for UAV vision-based surface inspection
- **Arxiv ID**: http://arxiv.org/abs/1706.04399v1
- **DOI**: 10.1016/j.autcon.2017.04.013
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.04399v1)
- **Published**: 2017-06-14 10:40:19+00:00
- **Updated**: 2017-06-14 10:40:19+00:00
- **Authors**: Manh Duong Phung, Cong Hoang Quach, Tran Hiep Dinh, Quang Ha
- **Comment**: None
- **Journal**: Automation in Construction, Vol.81, pp.25-33 (2017)
- **Summary**: In built infrastructure monitoring, an efficient path planning algorithm is essential for robotic inspection of large surfaces using computer vision. In this work, we first formulate the inspection path planning problem as an extended travelling salesman problem (TSP) in which both the coverage and obstacle avoidance were taken into account. An enhanced discrete particle swarm optimization (DPSO) algorithm is then proposed to solve the TSP, with performance improvement by using deterministic initialization, random mutation, and edge exchange. Finally, we take advantage of parallel computing to implement the DPSO in a GPU-based framework so that the computation time can be significantly reduced while keeping the hardware requirement unchanged. To show the effectiveness of the proposed algorithm, experimental results are included for datasets obtained from UAV inspection of an office building and a bridge.



### SalProp: Salient object proposals via aggregated edge cues
- **Arxiv ID**: http://arxiv.org/abs/1706.04472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04472v1)
- **Published**: 2017-06-14 13:17:42+00:00
- **Updated**: 2017-06-14 13:17:42+00:00
- **Authors**: Prerana Mukherjee, Brejesh Lall, Sarvaswa Tandon
- **Comment**: 5 pages, 4 figures, accepted at ICIP 2017
- **Journal**: None
- **Summary**: In this paper, we propose a novel object proposal generation scheme by formulating a graph-based salient edge classification framework that utilizes the edge context. In the proposed method, we construct a Bayesian probabilistic edge map to assign a saliency value to the edgelets by exploiting low level edge features. A Conditional Random Field is then learned to effectively combine these features for edge classification with object/non-object label. We propose an objectness score for the generated windows by analyzing the salient edge density inside the bounding box. Extensive experiments on PASCAL VOC 2007 dataset demonstrate that the proposed method gives competitive performance against 10 popular generic object detection techniques while using fewer number of proposals.



### Large-Scale YouTube-8M Video Understanding with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.04488v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04488v1)
- **Published**: 2017-06-14 13:38:43+00:00
- **Updated**: 2017-06-14 13:38:43+00:00
- **Authors**: Manuk Akopyan, Eshsou Khashba
- **Comment**: 6 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: Video classification problem has been studied many years. The success of Convolutional Neural Networks (CNN) in image recognition tasks gives a powerful incentive for researchers to create more advanced video classification approaches. As video has a temporal content Long Short Term Memory (LSTM) networks become handy tool allowing to model long-term temporal clues. Both approaches need a large dataset of input data. In this paper three models provided to address video classification using recently announced YouTube-8M large-scale dataset. The first model is based on frame pooling approach. Two other models based on LSTM networks. Mixture of Experts intermediate layer is used in third model allowing to increase model capacity without dramatically increasing computations. The set of experiments for handling imbalanced training data has been conducted.



### Learning Local Shape Descriptors from Part Correspondences With Multi-view Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.04496v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1706.04496v2)
- **Published**: 2017-06-14 13:56:07+00:00
- **Updated**: 2017-09-05 03:35:20+00:00
- **Authors**: Haibin Huang, Evangelos Kalogerakis, Siddhartha Chaudhuri, Duygu Ceylan, Vladimir G. Kim, Ersin Yumer
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new local descriptor for 3D shapes, directly applicable to a wide range of shape analysis problems such as point correspondences, semantic segmentation, affordance prediction, and shape-to-scan matching. The descriptor is produced by a convolutional network that is trained to embed geometrically and semantically similar points close to one another in descriptor space. The network processes surface neighborhoods around points on a shape that are captured at multiple scales by a succession of progressively zoomed out views, taken from carefully selected camera positions. We leverage two extremely large sources of data to train our network. First, since our network processes rendered views in the form of 2D images, we repurpose architectures pre-trained on massive image datasets. Second, we automatically generate a synthetic dense point correspondence dataset by non-rigid alignment of corresponding shape parts in a large collection of segmented 3D models. As a result of these design choices, our network effectively encodes multi-scale local context and fine-grained surface detail. Our network can be trained to produce either category-specific descriptors or more generic descriptors by learning from multiple shape categories. Once trained, at test time, the network extracts local descriptors for shapes without requiring any part segmentation as input. Our method can produce effective local descriptors even for shapes whose category is unknown or different from the ones used while training. We demonstrate through several experiments that our learned local descriptors are more discriminative compared to state of the art alternatives, and are effective in a variety of shape analysis applications.



### Modeling Multimodal Clues in a Hybrid Deep Learning Framework for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1706.04508v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.04508v1)
- **Published**: 2017-06-14 14:23:08+00:00
- **Updated**: 2017-06-14 14:23:08+00:00
- **Authors**: Yu-Gang Jiang, Zuxuan Wu, Jinhui Tang, Zechao Li, Xiangyang Xue, Shih-Fu Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Videos are inherently multimodal. This paper studies the problem of how to fully exploit the abundant multimodal clues for improved video categorization. We introduce a hybrid deep learning framework that integrates useful clues from multiple modalities, including static spatial appearance information, motion patterns within a short time window, audio information as well as long-range temporal dynamics. More specifically, we utilize three Convolutional Neural Networks (CNNs) operating on appearance, motion and audio signals to extract their corresponding features. We then employ a feature fusion network to derive a unified representation with an aim to capture the relationships among features. Furthermore, to exploit the long-range temporal dynamics in videos, we apply two Long Short Term Memory networks with extracted appearance and motion features as inputs. Finally, we also propose to refine the prediction scores by leveraging contextual relationships among video semantics. The hybrid deep learning framework is able to exploit a comprehensive set of multimodal features for video classification. Through an extensive set of experiments, we demonstrate that (1) LSTM networks which model sequences in an explicitly recurrent manner are highly complementary with CNN models; (2) the feature fusion network which produces a fused representation through modeling feature relationships outperforms alternative fusion strategies; (3) the semantic context of video classes can help further refine the predictions for improved performance. Experimental results on two challenging benchmarks, the UCF-101 and the Columbia Consumer Videos (CCV), provide strong quantitative evidence that our framework achieves promising results: $93.1\%$ on the UCF-101 and $84.5\%$ on the CCV, outperforming competing methods with clear margins.



### SideEye: A Generative Neural Network Based Simulator of Human Peripheral Vision
- **Arxiv ID**: http://arxiv.org/abs/1706.04568v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1706.04568v2)
- **Published**: 2017-06-14 16:18:20+00:00
- **Updated**: 2017-10-23 03:40:03+00:00
- **Authors**: Lex Fridman, Benedikt Jenik, Shaiyan Keshvari, Bryan Reimer, Christoph Zetzsche, Ruth Rosenholtz
- **Comment**: None
- **Journal**: None
- **Summary**: Foveal vision makes up less than 1% of the visual field. The other 99% is peripheral vision. Precisely what human beings see in the periphery is both obvious and mysterious in that we see it with our own eyes but can't visualize what we see, except in controlled lab experiments. Degradation of information in the periphery is far more complex than what might be mimicked with a radial blur. Rather, behaviorally-validated models hypothesize that peripheral vision measures a large number of local texture statistics in pooling regions that overlap and grow with eccentricity. In this work, we develop a new method for peripheral vision simulation by training a generative neural network on a behaviorally-validated full-field synthesis model. By achieving a 21,000 fold reduction in running time, our approach is the first to combine realism and speed of peripheral vision simulation to a degree that provides a whole new way to approach visual design: through peripheral visualization.



### Deep Learning Methods for Efficient Large Scale Video Labeling
- **Arxiv ID**: http://arxiv.org/abs/1706.04572v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.04572v1)
- **Published**: 2017-06-14 16:24:18+00:00
- **Updated**: 2017-06-14 16:24:18+00:00
- **Authors**: Miha Skalic, Marcin Pekalski, Xingguo E. Pan
- **Comment**: 7 pages, 5 tables, 1 figure
- **Journal**: None
- **Summary**: We present a solution to "Google Cloud and YouTube-8M Video Understanding Challenge" that ranked 5th place. The proposed model is an ensemble of three model families, two frame level and one video level. The training was performed on augmented dataset, with cross validation.



### Learning without Prejudice: Avoiding Bias in Webly-Supervised Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1706.04589v1
- **DOI**: 10.1016/j.cviu.2017.08.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04589v1)
- **Published**: 2017-06-14 17:10:59+00:00
- **Updated**: 2017-06-14 17:10:59+00:00
- **Authors**: Christian Rupprecht, Ansh Kapil, Nan Liu, Lamberto Ballan, Federico Tombari
- **Comment**: Submitted to CVIU SI: Computer Vision and the Web
- **Journal**: None
- **Summary**: Webly-supervised learning has recently emerged as an alternative paradigm to traditional supervised learning based on large-scale datasets with manual annotations. The key idea is that models such as CNNs can be learned from the noisy visual data available on the web. In this work we aim to exploit web data for video understanding tasks such as action recognition and detection. One of the main problems in webly-supervised learning is cleaning the noisy labeled data from the web. The state-of-the-art paradigm relies on training a first classifier on noisy data that is then used to clean the remaining dataset. Our key insight is that this procedure biases the second classifier towards samples that the first one understands. Here we train two independent CNNs, a RGB network on web images and video frames and a second network using temporal information from optical flow. We show that training the networks independently is vastly superior to selecting the frames for the flow classifier by using our RGB network. Moreover, we show benefits in enriching the training set with different data sources from heterogeneous public web databases. We demonstrate that our framework outperforms all other webly-supervised methods on two public benchmarks, UCF-101 and Thumos'14.



### LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.03718v1
- **DOI**: 10.1109/VCIP.2017.8305148
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.03718v1)
- **Published**: 2017-06-14 20:37:17+00:00
- **Updated**: 2017-06-14 20:37:17+00:00
- **Authors**: Abhishek Chaurasia, Eugenio Culurciello
- **Comment**: 5 pages, 5 figures, GitHub: https://github.com/e-lab/LinkNet
- **Journal**: None
- **Summary**: Pixel-wise semantic segmentation for visual scene understanding not only needs to be accurate, but also efficient in order to find any use in real-time application. Existing algorithms even though are accurate but they do not focus on utilizing the parameters of neural network efficiently. As a result they are huge in terms of parameters and number of operations; hence slow too. In this paper, we propose a novel deep neural network architecture which allows it to learn without any significant increase in number of parameters. Our network uses only 11.5 million parameters and 21.2 GFLOPs for processing an image of resolution 3x640x360. It gives state-of-the-art performance on CamVid and comparable results on Cityscapes dataset. We also compare our networks processing time on NVIDIA GPU and embedded system device with existing state-of-the-art architectures for different image resolutions.



### Feature Enhancement in Visually Impaired Images
- **Arxiv ID**: http://arxiv.org/abs/1706.04671v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1706.04671v1)
- **Published**: 2017-06-14 21:24:26+00:00
- **Updated**: 2017-06-14 21:24:26+00:00
- **Authors**: Madhuri Suthar, Mohammad Asghari, Bahram Jalali
- **Comment**: Submitted to PLOS One Journal on November 3, 2016
- **Journal**: None
- **Summary**: One of the major open problems in computer vision is detection of features in visually impaired images. In this paper, we describe a potential solution using Phase Stretch Transform, a new computational approach for image analysis, edge detection and resolution enhancement that is inspired by the physics of the photonic time stretch technique. We mathematically derive the intrinsic nonlinear transfer function and demonstrate how it leads to (1) superior performance at low contrast levels and (2) a reconfigurable operator for hyper-dimensional classification. We prove that the Phase Stretch Transform equalizes the input image brightness across the range of intensities resulting in a high dynamic range in visually impaired images. We also show further improvement in the dynamic range by combining our method with the conventional techniques. Finally, our results show a method for computation of mathematical derivatives via group delay dispersion operations.



### A New Adaptive Video Super-Resolution Algorithm With Improved Robustness to Innovations
- **Arxiv ID**: http://arxiv.org/abs/1706.04695v3
- **DOI**: 10.1109/TIP.2018.2866181
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04695v3)
- **Published**: 2017-06-14 23:32:55+00:00
- **Updated**: 2018-08-17 19:54:58+00:00
- **Authors**: Ricardo Augusto Borsoi, Guilherme Holsbach Costa, José Carlos Moreira Bermudez
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a new video super-resolution reconstruction (SRR) method with improved robustness to outliers is proposed. Although the R-LMS is one of the SRR algorithms with the best reconstruction quality for its computational cost, and is naturally robust to registration inaccuracies, its performance is known to degrade severely in the presence of innovation outliers. By studying the proximal point cost function representation of the R-LMS iterative equation, a better understanding of its performance under different situations is attained. Using statistical properties of typical innovation outliers, a new cost function is then proposed and two new algorithms are derived, which present improved robustness to outliers while maintaining computational costs comparable to that of R-LMS. Monte Carlo simulation results illustrate that the proposed method outperforms the traditional and regularized versions of LMS, and is competitive with state-of-the-art SRR methods at a much smaller computational cost.



