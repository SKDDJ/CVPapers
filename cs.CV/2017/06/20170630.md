# Arxiv Papers in cs.CV on 2017-06-30
### Superpixel-based Semantic Segmentation Trained by Statistical Process Control
- **Arxiv ID**: http://arxiv.org/abs/1706.10071v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.10071v2)
- **Published**: 2017-06-30 09:13:24+00:00
- **Updated**: 2017-08-07 10:35:03+00:00
- **Authors**: Hyojin Park, Jisoo Jeong, Youngjoon Yoo, Nojun Kwak
- **Comment**: Accepted in British Machine Vision Conference (BMVC), 2017
- **Journal**: None
- **Summary**: Semantic segmentation, like other fields of computer vision, has seen a remarkable performance advance by the use of deep convolution neural networks. However, considering that neighboring pixels are heavily dependent on each other, both learning and testing of these methods have a lot of redundant operations. To resolve this problem, the proposed network is trained and tested with only 0.37% of total pixels by superpixel-based sampling and largely reduced the complexity of upsampling calculation. The hypercolumn feature maps are constructed by pyramid module in combination with the convolution layers of the base network. Since the proposed method uses a very small number of sampled pixels, the end-to-end learning of the entire network is difficult with a common learning rate for all the layers. In order to resolve this problem, the learning rate after sampling is controlled by statistical process control (SPC) of gradients in each layer. The proposed method performs better than or equal to the conventional methods that use much more samples on Pascal Context, SUN-RGBD dataset.



### Persistence Diagrams with Linear Machine Learning Models
- **Arxiv ID**: http://arxiv.org/abs/1706.10082v2
- **DOI**: None
- **Categories**: **math.AT**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.10082v2)
- **Published**: 2017-06-30 09:33:50+00:00
- **Updated**: 2017-07-06 07:19:30+00:00
- **Authors**: Ippei Obayashi, Yasuaki Hiraoka
- **Comment**: None
- **Journal**: None
- **Summary**: Persistence diagrams have been widely recognized as a compact descriptor for characterizing multiscale topological features in data. When many datasets are available, statistical features embedded in those persistence diagrams can be extracted by applying machine learnings. In particular, the ability for explicitly analyzing the inverse in the original data space from those statistical features of persistence diagrams is significantly important for practical applications. In this paper, we propose a unified method for the inverse analysis by combining linear machine learning models with persistence images. The method is applied to point clouds and cubical sets, showing the ability of the statistical inverse analysis and its advantages.



### SMC Faster R-CNN: Toward a scene-specialized multi-object detector
- **Arxiv ID**: http://arxiv.org/abs/1706.10217v1
- **DOI**: 10.1016/j.cviu.2017.06.008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.10217v1)
- **Published**: 2017-06-30 14:31:12+00:00
- **Updated**: 2017-06-30 14:31:12+00:00
- **Authors**: Ala Mhalla, Thierry Chateau, Houda Maamatou, Sami Gazzah, Najoua Essoukri Ben Amara
- **Comment**: None
- **Journal**: None
- **Summary**: Generally, the performance of a generic detector decreases significantly when it is tested on a specific scene due to the large variation between the source training dataset and the samples from the target scene. To solve this problem, we propose a new formalism of transfer learning based on the theory of a Sequential Monte Carlo (SMC) filter to automatically specialize a scene-specific Faster R-CNN detector. The suggested framework uses different strategies based on the SMC filter steps to approximate iteratively the target distribution as a set of samples in order to specialize the Faster R-CNN detector towards a target scene. Moreover, we put forward a likelihood function that combines spatio-temporal information extracted from the target video sequence and the confidence-score given by the output layer of the Faster R-CNN, to favor the selection of target samples associated with the right label. The effectiveness of the suggested framework is demonstrated through experiments on several public traffic datasets. Compared with the state-of-the-art specialization frameworks, the proposed framework presents encouraging results for both single and multi-traffic object detections.



### A selectional auto-encoder approach for document image binarization
- **Arxiv ID**: http://arxiv.org/abs/1706.10241v3
- **DOI**: 10.1016/j.patcog.2018.08.011
- **Categories**: **cs.CV**, cs.DL
- **Links**: [PDF](http://arxiv.org/pdf/1706.10241v3)
- **Published**: 2017-06-30 15:35:13+00:00
- **Updated**: 2018-09-06 09:32:09+00:00
- **Authors**: Jorge Calvo-Zaragoza, Antonio-Javier Gallego
- **Comment**: Published in Pattern Recognition
- **Journal**: None
- **Summary**: Binarization plays a key role in the automatic information retrieval from document images. This process is usually performed in the first stages of documents analysis systems, and serves as a basis for subsequent steps. Hence it has to be robust in order to allow the full analysis workflow to be successful. Several methods for document image binarization have been proposed so far, most of which are based on hand-crafted image processing strategies. Recently, Convolutional Neural Networks have shown an amazing performance in many disparate duties related to computer vision. In this paper we discuss the use of convolutional auto-encoders devoted to learning an end-to-end map from an input image to its selectional output, in which activations indicate the likelihood of pixels to be either foreground or background. Once trained, documents can therefore be binarized by parsing them through the model and applying a threshold. This approach has proven to outperform existing binarization strategies in a number of document domains.



### Color-opponent mechanisms for local hue encoding in a hierarchical framework
- **Arxiv ID**: http://arxiv.org/abs/1706.10266v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.8; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1706.10266v2)
- **Published**: 2017-06-30 16:47:01+00:00
- **Updated**: 2018-02-20 18:34:05+00:00
- **Authors**: Paria Mehrani, Andrei Mouraviev, Oscar J. Avella Gonzalez, John K. Tsotsos
- **Comment**: 16 pages, 15 figures, 3 tables
- **Journal**: None
- **Summary**: A biologically plausible computational model for color representation is introduced. We present a mechanistic hierarchical model of neurons that not only successfully encodes local hue, but also explicitly reveals how the contributions of each visual cortical layer participating in the process can lead to a hue representation. Our proposed model benefits from studies on the visual cortex and builds a network of single-opponent and hue-selective neurons. Local hue encoding is achieved through gradually increasing nonlinearity in terms of cone inputs to single-opponent cells. We demonstrate that our model's single-opponent neurons have wide tuning curves, while the hue-selective neurons in our model V4 layer exhibit narrower tunings, resembling those in V4 of the primate visual system. Our simulation experiments suggest that neurons in V4 or later layers have the capacity of encoding unique hues. Moreover, with a few examples, we present the possibility of spanning the infinite space of physical hues by combining the hue-selective neurons in our model.



### Failing to Learn: Autonomously Identifying Perception Failures for Self-driving Cars
- **Arxiv ID**: http://arxiv.org/abs/1707.00051v4
- **DOI**: 10.1109/LRA.2018.2857402
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1707.00051v4)
- **Published**: 2017-06-30 21:42:47+00:00
- **Updated**: 2018-07-26 19:41:39+00:00
- **Authors**: Manikandasriram Srinivasan Ramanagopal, Cyrus Anderson, Ram Vasudevan, Matthew Johnson-Roberson
- **Comment**: 8 pages, 4 figures and 4 tables. Accepted for publication in RA-L and
  will be presented in IROS 2018 in Madrid, Spain
- **Journal**: None
- **Summary**: One of the major open challenges in self-driving cars is the ability to detect cars and pedestrians to safely navigate in the world. Deep learning-based object detector approaches have enabled great advances in using camera imagery to detect and classify objects. But for a safety critical application, such as autonomous driving, the error rates of the current state of the art are still too high to enable safe operation. Moreover, the characterization of object detector performance is primarily limited to testing on prerecorded datasets. Errors that occur on novel data go undetected without additional human labels. In this letter, we propose an automated method to identify mistakes made by object detectors without ground truth labels. We show that inconsistencies in the object detector output between a pair of similar images can be used as hypotheses for false negatives (e.g., missed detections) and using a novel set of features for each hypothesis, an off-the-shelf binary classifier can be used to find valid errors. In particular, we study two distinct cues - temporal and stereo inconsistencies - using data that are readily available on most autonomous vehicles. Our method can be used with any camera-based object detector and we illustrate the technique on several sets of real world data. We show that a state-of-the-art detector, tracker, and our classifier trained only on synthetic data can identify valid errors on KITTI tracking dataset with an average precision of 0.94. We also release a new tracking dataset with 104 sequences totaling 80,655 labeled pairs of stereo images along with ground truth disparity from a game engine to facilitate further research. The dataset and code are available at https://fcav.engin.umich.edu/research/failing-to-learn



### Multiple VLAD encoding of CNNs for image classification
- **Arxiv ID**: http://arxiv.org/abs/1707.00058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00058v1)
- **Published**: 2017-06-30 22:22:00+00:00
- **Updated**: 2017-06-30 22:22:00+00:00
- **Authors**: Qing Li, Qiang Peng, Chuan Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the effectiveness of convolutional neural networks (CNNs) especially in image classification tasks, the effect of convolution features on learned representations is still limited. It mostly focuses on the salient object of the images, but ignores the variation information on clutter and local. In this paper, we propose a special framework, which is the multiple VLAD encoding method with the CNNs features for image classification. Furthermore, in order to improve the performance of the VLAD coding method, we explore the multiplicity of VLAD encoding with the extension of three kinds of encoding algorithms, which are the VLAD-SA method, the VLAD-LSA and the VLAD-LLC method. Finally, we equip the spatial pyramid patch (SPM) on VLAD encoding to add the spatial information of CNNs feature. In particular, the power of SPM leads our framework to yield better performance compared to the existing method.



### Adversarial Image Alignment and Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1707.00067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00067v1)
- **Published**: 2017-06-30 23:59:27+00:00
- **Updated**: 2017-06-30 23:59:27+00:00
- **Authors**: Viren Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Volumetric (3d) images are acquired for many scientific and biomedical purposes using imaging methods such as serial section microscopy, CT scans, and MRI. A frequent step in the analysis and reconstruction of such data is the alignment and registration of images that were acquired in succession along a spatial or temporal dimension. For example, in serial section electron microscopy, individual 2d sections are imaged via electron microscopy and then must be aligned to one another in order to produce a coherent 3d volume. State of the art approaches find image correspondences derived from patch matching and invariant feature detectors, and then solve optimization problems that rigidly or elastically deform series of images into an aligned volume. Here we show how fully convolutional neural networks trained with an adversarial loss function can be used for two tasks: (1) synthesis of missing or damaged image data from adjacent sections, and (2) fine-scale alignment of block-face electron microscopy data. Finally, we show how these two capabilities can be combined in order to produce artificial isotropic volumes from anisotropic image volumes using a super-resolution adversarial alignment and interpolation approach.



