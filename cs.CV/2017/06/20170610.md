# Arxiv Papers in cs.CV on 2017-06-10
### Visual Search at eBay
- **Arxiv ID**: http://arxiv.org/abs/1706.03154v2
- **DOI**: 10.1145/3097983.3098162
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1706.03154v2)
- **Published**: 2017-06-10 00:02:34+00:00
- **Updated**: 2017-07-07 17:21:23+00:00
- **Authors**: Fan Yang, Ajinkya Kale, Yury Bubnov, Leon Stein, Qiaosong Wang, Hadi Kiapour, Robinson Piramuthu
- **Comment**: To appear in 23rd SIGKDD Conference on Knowledge Discovery and Data
  Mining (KDD), 2017. A demonstration video can be found at
  https://youtu.be/iYtjs32vh4g
- **Journal**: None
- **Summary**: In this paper, we propose a novel end-to-end approach for scalable visual search infrastructure. We discuss the challenges we faced for a massive volatile inventory like at eBay and present our solution to overcome those. We harness the availability of large image collection of eBay listings and state-of-the-art deep learning techniques to perform visual search at scale. Supervised approach for optimized search limited to top predicted categories and also for compact binary signature are key to scale up without compromising accuracy and precision. Both use a common deep neural network requiring only a single forward inference. The system architecture is presented with in-depth discussions of its basic components and optimizations for a trade-off between search relevance and latency. This solution is currently deployed in a distributed cloud infrastructure and fuels visual search in eBay ShopBot and Close5. We show benchmark on ImageNet dataset on which our approach is faster and more accurate than several unsupervised baselines. We share our learnings with the hope that visual search becomes a first class citizen for all large scale search engines rather than an afterthought.



### Deep Adaptive Feature Embedding with Local Sample Distributions for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1706.03160v2
- **DOI**: 10.1016/j.patcog.2017.08.029
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03160v2)
- **Published**: 2017-06-10 00:49:43+00:00
- **Updated**: 2017-09-07 02:37:44+00:00
- **Authors**: Lin Wu, Yang Wang, Junbin Gao, Xue Li
- **Comment**: Published on Pattern Recognition
- **Journal**: None
- **Summary**: Person re-identification (re-id) aims to match pedestrians observed by disjoint camera views. It attracts increasing attention in computer vision due to its importance to surveillance system. To combat the major challenge of cross-view visual variations, deep embedding approaches are proposed by learning a compact feature space from images such that the Euclidean distances correspond to their cross-view similarity metric. However, the global Euclidean distance cannot faithfully characterize the ideal similarity in a complex visual feature space because features of pedestrian images exhibit unknown distributions due to large variations in poses, illumination and occlusion. Moreover, intra-personal training samples within a local range are robust to guide deep embedding against uncontrolled variations, which however, cannot be captured by a global Euclidean distance. In this paper, we study the problem of person re-id by proposing a novel sampling to mine suitable \textit{positives} (i.e. intra-class) within a local range to improve the deep embedding in the context of large intra-class variations. Our method is capable of learning a deep similarity metric adaptive to local sample structure by minimizing each sample's local distances while propagating through the relationship between samples to attain the whole intra-class minimization. To this end, a novel objective function is proposed to jointly optimize similarity metric learning, local positive mining and robust deep embedding. This yields local discriminations by selecting local-ranged positive samples, and the learned features are robust to dramatic intra-class variations. Experiments on benchmarks show state-of-the-art results achieved by our method.



### Direct detection of pixel-level myocardial infarction areas via a deep-learning algorithm
- **Arxiv ID**: http://arxiv.org/abs/1706.03182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03182v1)
- **Published**: 2017-06-10 05:03:38+00:00
- **Updated**: 2017-06-10 05:03:38+00:00
- **Authors**: Chenchu Xu, Lei Xu, Zhifan Gao, Shen zhao, Heye Zhang, Yanping Zhang, Xiuquan Du, Shu Zhao, Dhanjoo Ghista, Shuo Li
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate detection of the myocardial infarction (MI) area is crucial for early diagnosis planning and follow-up management. In this study, we propose an end-to-end deep-learning algorithm framework (OF-RNN ) to accurately detect the MI area at the pixel level. Our OF-RNN consists of three different function layers: the heart localization layers, which can accurately and automatically crop the region-of-interest (ROI) sequences, including the left ventricle, using the whole cardiac magnetic resonance image sequences; the motion statistical layers, which are used to build a time-series architecture to capture two types of motion features (at the pixel-level) by integrating the local motion features generated by long short-term memory-recurrent neural networks and the global motion features generated by deep optical flows from the whole ROI sequence, which can effectively characterize myocardial physiologic function; and the fully connected discriminate layers, which use stacked auto-encoders to further learn these features, and they use a softmax classifier to build the correspondences from the motion features to the tissue identities (infarction or not) for each pixel. Through the seamless connection of each layer, our OF-RNN can obtain the area, position, and shape of the MI for each patient. Our proposed framework yielded an overall classification accuracy of 94.35% at the pixel level, from 114 clinical subjects. These results indicate the potential of our proposed method in aiding standardized MI assessments.



### Image Matching via Loopy RNN
- **Arxiv ID**: http://arxiv.org/abs/1706.03190v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.03190v3)
- **Published**: 2017-06-10 06:48:16+00:00
- **Updated**: 2017-06-18 15:58:23+00:00
- **Authors**: Donghao Luo, Bingbing Ni, Yichao Yan, Xiaokang Yang
- **Comment**: 6 pages, 5 figures, International Joint Conference on Artificial
  Intelligence(IJCAI)
- **Journal**: None
- **Summary**: Most existing matching algorithms are one-off algorithms, i.e., they usually measure the distance between the two image feature representation vectors for only one time. In contrast, human's vision system achieves this task, i.e., image matching, by recursively looking at specific/related parts of both images and then making the final judgement. Towards this end, we propose a novel loopy recurrent neural network (Loopy RNN), which is capable of aggregating relationship information of two input images in a progressive/iterative manner and outputting the consolidated matching score in the final iteration. A Loopy RNN features two uniqueness. First, built on conventional long short-term memory (LSTM) nodes, it links the output gate of the tail node to the input gate of the head node, thus it brings up symmetry property required for matching. Second, a monotonous loss designed for the proposed network guarantees increasing confidence during the recursive matching process. Extensive experiments on several image matching benchmarks demonstrate the great potential of the proposed method.



### Deep Learning-Based Food Calorie Estimation Method in Dietary Assessment
- **Arxiv ID**: http://arxiv.org/abs/1706.04062v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04062v4)
- **Published**: 2017-06-10 09:42:28+00:00
- **Updated**: 2018-02-18 08:13:04+00:00
- **Authors**: Yanchao Liang, Jianhua Li
- **Comment**: 12 pages. arXiv admin note: substantial text overlap with
  arXiv:1705.07632
- **Journal**: None
- **Summary**: Obesity treatment requires obese patients to record all food intakes per day. Computer vision has been introduced to estimate calories from food images. In order to increase accuracy of detection and reduce the error of volume estimation in food calorie estimation, we present our calorie estimation method in this paper. To estimate calorie of food, a top view and side view is needed. Faster R-CNN is used to detect the food and calibration object. GrabCut algorithm is used to get each food's contour. Then the volume is estimated with the food and corresponding object. Finally we estimate each food's calorie. And the experiment results show our estimation method is effective.



### Exploring Convolutional Networks for End-to-End Visual Servoing
- **Arxiv ID**: http://arxiv.org/abs/1706.03220v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.03220v1)
- **Published**: 2017-06-10 10:57:44+00:00
- **Updated**: 2017-06-10 10:57:44+00:00
- **Authors**: Aseem Saxena, Harit Pandya, Gourav Kumar, Ayush Gaud, K. Madhava Krishna
- **Comment**: IEEE ICRA 2017
- **Journal**: None
- **Summary**: Present image based visual servoing approaches rely on extracting hand crafted visual features from an image. Choosing the right set of features is important as it directly affects the performance of any approach. Motivated by recent breakthroughs in performance of data driven methods on recognition and localization tasks, we aim to learn visual feature representations suitable for servoing tasks in unstructured and unknown environments. In this paper, we present an end-to-end learning based approach for visual servoing in diverse scenes where the knowledge of camera parameters and scene geometry is not available a priori. This is achieved by training a convolutional neural network over color images with synchronised camera poses. Through experiments performed in simulation and on a quadrotor, we demonstrate the efficacy and robustness of our approach for a wide range of camera poses in both indoor as well as outdoor environments.



### Generate Identity-Preserving Faces by Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.03227v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03227v2)
- **Published**: 2017-06-10 12:37:54+00:00
- **Updated**: 2017-06-25 14:58:05+00:00
- **Authors**: Zhigang Li, Yupin Luo
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Generating identity-preserving faces aims to generate various face images keeping the same identity given a target face image. Although considerable generative models have been developed in recent years, it is still challenging to simultaneously acquire high quality of facial images and preserve the identity. Here we propose a compelling method using generative adversarial networks (GAN). Concretely, we leverage the generator of trained GAN to generate plausible faces and FaceNet as an identity-similarity discriminator to ensure the identity. Experimental results show that our method is qualified to generate both plausible and identity-preserving faces with high quality. In addition, our method provides a universal framework which can be realized in various ways by combining different face generators and identity-similarity discriminator.



### A Bayesian Hyperprior Approach for Joint Image Denoising and Interpolation, with an Application to HDR Imaging
- **Arxiv ID**: http://arxiv.org/abs/1706.03261v1
- **DOI**: 10.1109/TCI.2017.2704439
- **Categories**: **cs.CV**, eess.IV, stat.ML, 62H35, 68U10, 62F15, 68Q32, I.4.1, I.4.4, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1706.03261v1)
- **Published**: 2017-06-10 17:37:01+00:00
- **Updated**: 2017-06-10 17:37:01+00:00
- **Authors**: Cecilia Aguerrebere, Andrés Almansa, Julie Delon, Yann Gousseau, Pablo Musé
- **Comment**: Some figures are reduced to comply with arxiv's size constraints.
  Full size images are available as HAL technical report hal-01107519v5, IEEE
  Transactions on Computational Imaging, 2017
- **Journal**: None
- **Summary**: Recently, impressive denoising results have been achieved by Bayesian approaches which assume Gaussian models for the image patches. This improvement in performance can be attributed to the use of per-patch models. Unfortunately such an approach is particularly unstable for most inverse problems beyond denoising. In this work, we propose the use of a hyperprior to model image patches, in order to stabilize the estimation procedure. There are two main advantages to the proposed restoration scheme: Firstly it is adapted to diagonal degradation matrices, and in particular to missing data problems (e.g. inpainting of missing pixels or zooming). Secondly it can deal with signal dependent noise models, particularly suited to digital cameras. As such, the scheme is especially adapted to computational photography. In order to illustrate this point, we provide an application to high dynamic range imaging from a single image taken with a modified sensor, which shows the effectiveness of the proposed scheme.



### Segmentation of nearly isotropic overlapped tracks in photomicrographs using successive erosions as watershed markers
- **Arxiv ID**: http://arxiv.org/abs/1706.03282v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/1706.03282v2)
- **Published**: 2017-06-10 21:18:25+00:00
- **Updated**: 2018-01-18 16:04:00+00:00
- **Authors**: Alexandre Fioravante de Siqueira, Wagner Massayuki Nakasuga, Sandro Guedes, Lothar Ratschbacher
- **Comment**: 31 pages, 15 figures
- **Journal**: None
- **Summary**: The major challenges of automatic track counting are distinguishing tracks and material defects, identifying small tracks and defects of similar size, and detecting overlapping tracks. Here we address the latter issue using WUSEM, an algorithm which combines the watershed transform, morphological erosions and labeling to separate regions in photomicrographs. WUSEM shows reliable results when used in photomicrographs presenting almost isotropic objects. We tested this method in two datasets of diallyl phthalate (DAP) photomicrographs and compared the results when counting manually and using the classic watershed. The mean automatic/manual efficiency ratio when using WUSEM in the test datasets is 0.97 +/- 0.11.



### Recovering 6D Object Pose: A Review and Multi-modal Analysis
- **Arxiv ID**: http://arxiv.org/abs/1706.03285v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03285v2)
- **Published**: 2017-06-10 21:37:00+00:00
- **Updated**: 2018-08-15 19:23:54+00:00
- **Authors**: Caner Sahin, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: A large number of studies analyse object detection and pose estimation at visual level in 2D, discussing the effects of challenges such as occlusion, clutter, texture, etc., on the performances of the methods, which work in the context of RGB modality. Interpreting the depth data, the study in this paper presents thorough multi-modal analyses. It discusses the above-mentioned challenges for full 6D object pose estimation in RGB-D images comparing the performances of several 6D detectors in order to answer the following questions: What is the current position of the computer vision community for maintaining "automation" in robotic manipulation? What next steps should the community take for improving "autonomy" in robotics while handling objects? Our findings include: (i) reasonably accurate results are obtained on textured-objects at varying viewpoints with cluttered backgrounds. (ii) Heavy existence of occlusion and clutter severely affects the detectors, and similar-looking distractors is the biggest challenge in recovering instances' 6D. (iii) Template-based methods and random forest-based learning algorithms underlie object detection and 6D pose estimation. Recent paradigm is to learn deep discriminative feature representations and to adopt CNNs taking RGB images as input. (iv) Depending on the availability of large-scale 6D annotated depth datasets, feature representations can be learnt on these datasets, and then the learnt representations can be customized for the 6D problem.



