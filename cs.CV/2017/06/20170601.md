# Arxiv Papers in cs.CV on 2017-06-01
### Teaching Machines to Describe Images via Natural Language Feedback
- **Arxiv ID**: http://arxiv.org/abs/1706.00130v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1706.00130v2)
- **Published**: 2017-06-01 00:24:55+00:00
- **Updated**: 2017-06-05 16:47:40+00:00
- **Authors**: Huan Ling, Sanja Fidler
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Robots will eventually be part of every household. It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. We argue that a descriptive sentence can provide a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts. We propose a hierarchical phrase-based captioning model trained with policy gradients, and design a feedback network that provides reward to the learner by conditioning on the human-provided feedback. We show that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions.



### Faster Spatially Regularized Correlation Filters for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1706.00140v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00140v1)
- **Published**: 2017-06-01 01:11:57+00:00
- **Updated**: 2017-06-01 01:11:57+00:00
- **Authors**: Xiaoxiang Hu, Yujiu Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Discriminatively learned correlation filters (DCF) have been widely used in online visual tracking filed due to its simplicity and efficiency. These methods utilize a periodic assumption of the training samples to construct a circulant data matrix, which implicitly increases the training samples and reduces both storage and computational complexity.The periodic assumption also introduces unwanted boundary effects. Recently, Spatially Regularized Correlation Filters (SRDCF) solved this issue by introducing penalization on correlation filter coefficients depending on their spatial location. However, SRDCF's efficiency dramatically decreased due to the breaking of circulant structure.   We propose Faster Spatially Regularized Discriminative Correlation Filters (FSRDCF) for tracking. The FSRDCF is constructed from Ridge Regression, the circulant structure of training samples in the spatial domain is fully used, more importantly, we further exploit the circulant structure of regularization function in the Fourier domain, which allows our problem to be solved more directly and efficiently. Experiments are conducted on three benchmark datasets: OTB-2013, OTB-2015 and VOT2016. Our approach achieves equivalent performance to the baseline tracker SRDCF on all three datasets. On OTB-2013 and OTB-2015 datasets, our approach obtains a more than twice faster running speed and a more than third times shorter start-up time than the SRDCF. For state-of-the-art comparison, our approach demonstrates superior performance compared to other non-spatial-regularization trackers.



### Shape and Positional Geometry of Multi-Object Configurations
- **Arxiv ID**: http://arxiv.org/abs/1706.00150v1
- **DOI**: None
- **Categories**: **cs.CV**, 68
- **Links**: [PDF](http://arxiv.org/pdf/1706.00150v1)
- **Published**: 2017-06-01 02:12:06+00:00
- **Updated**: 2017-06-01 02:12:06+00:00
- **Authors**: James Damon, Ellen Gasparovic
- **Comment**: This paper presents material relevant for two and three dimensional
  images that builds on and makes many references to a previous paper by the
  authors, arXiv:1402.5517
- **Journal**: None
- **Summary**: In previous work, we introduced a method for modeling a configuration of objects in 2D and 3D images using a mathematical "medial/skeletal linking structure." In this paper, we show how these structures allow us to capture positional properties of a multi-object configuration in addition to the shape properties of the individual objects. In particular, we introduce numerical invariants for positional properties which measure the closeness of neighboring objects, including identifying the parts of the objects which are close, and the "relative significance" of objects compared with the other objects in the configuration. Using these numerical measures, we introduce a hierarchical ordering and relations between the individual objects, and quantitative criteria for identifying subconfigurations. In addition, the invariants provide a "proximity matrix" which yields a unique set of weightings measuring overall proximity of objects in the configuration. Furthermore, we show that these invariants, which are volumetrically defined and involve external regions, may be computed via integral formulas in terms of "skeletal linking integrals" defined on the internal skeletal structures of the objects.



### Cross-modal Common Representation Learning by Hybrid Transfer Network
- **Arxiv ID**: http://arxiv.org/abs/1706.00153v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.00153v2)
- **Published**: 2017-06-01 02:53:57+00:00
- **Updated**: 2017-06-24 14:08:19+00:00
- **Authors**: Xin Huang, Yuxin Peng, Mingkuan Yuan
- **Comment**: To appear in the proceedings of 26th International Joint Conference
  on Artificial Intelligence (IJCAI), Melbourne, Australia, Aug. 19-25, 2017. 8
  pages, 2 figures
- **Journal**: None
- **Summary**: DNN-based cross-modal retrieval is a research hotspot to retrieve across different modalities as image and text, but existing methods often face the challenge of insufficient cross-modal training data. In single-modal scenario, similar problem is usually relieved by transferring knowledge from large-scale auxiliary datasets (as ImageNet). Knowledge from such single-modal datasets is also very useful for cross-modal retrieval, which can provide rich general semantic information that can be shared across different modalities. However, it is challenging to transfer useful knowledge from single-modal (as image) source domain to cross-modal (as image/text) target domain. Knowledge in source domain cannot be directly transferred to both two different modalities in target domain, and the inherent cross-modal correlation contained in target domain provides key hints for cross-modal retrieval which should be preserved during transfer process. This paper proposes Cross-modal Hybrid Transfer Network (CHTN) with two subnetworks: Modal-sharing transfer subnetwork utilizes the modality in both source and target domains as a bridge, for transferring knowledge to both two modalities simultaneously; Layer-sharing correlation subnetwork preserves the inherent cross-modal semantic correlation to further adapt to cross-modal retrieval task. Cross-modal data can be converted to common representation by CHTN for retrieval, and comprehensive experiment on 3 datasets shows its effectiveness.



### Depth Structure Preserving Scene Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1706.00212v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00212v2)
- **Published**: 2017-06-01 09:03:08+00:00
- **Updated**: 2017-11-22 03:56:53+00:00
- **Authors**: Wendong Zhang, Bingbing Ni, Yichao Yan, Jingwei Xu, Xiaokang Yang
- **Comment**: There is an error in the first paragraph in Section 4.4. Actually, we
  train and test another new CGAN model with the input in our model to evaluate
  the improvements. This error can lead readers misunderstand the improvements
  of our model and make the comparison unfair. Therefore, we request to
  withdraw the current submission and will submit a final version later
- **Journal**: None
- **Summary**: Key to automatically generate natural scene images is to properly arrange among various spatial elements, especially in the depth direction. To this end, we introduce a novel depth structure preserving scene image generation network (DSP-GAN), which favors a hierarchical and heterogeneous architecture, for the purpose of depth structure preserving scene generation. The main trunk of the proposed infrastructure is built on a Hawkes point process that models the spatial dependency between different depth layers. Within each layer generative adversarial sub-networks are trained collaboratively to generate realistic scene components, conditioned on the layer information produced by the point process. We experiment our model on a sub-set of SUNdataset with annotated scene images and demonstrate that our models are capable of generating depth-realistic natural scene image.



### Ranking to Learn and Learning to Rank: On the Role of Ranking in Pattern Recognition Applications
- **Arxiv ID**: http://arxiv.org/abs/1706.05933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05933v1)
- **Published**: 2017-06-01 09:15:35+00:00
- **Updated**: 2017-06-01 09:15:35+00:00
- **Authors**: Giorgio Roffo
- **Comment**: European PhD Thesis. arXiv admin note: text overlap with
  arXiv:1601.06615, arXiv:1505.06821, arXiv:1704.02665 by other authors
- **Journal**: None
- **Summary**: The last decade has seen a revolution in the theory and application of machine learning and pattern recognition. Through these advancements, variable ranking has emerged as an active and growing research area and it is now beginning to be applied to many new problems. The rationale behind this fact is that many pattern recognition problems are by nature ranking problems. The main objective of a ranking algorithm is to sort objects according to some criteria, so that, the most relevant items will appear early in the produced result list. Ranking methods can be analyzed from two different methodological perspectives: ranking to learn and learning to rank. The former aims at studying methods and techniques to sort objects for improving the accuracy of a machine learning model. Enhancing a model performance can be challenging at times. For example, in pattern classification tasks, different data representations can complicate and hide the different explanatory factors of variation behind the data. In particular, hand-crafted features contain many cues that are either redundant or irrelevant, which turn out to reduce the overall accuracy of the classifier. In such a case feature selection is used, that, by producing ranked lists of features, helps to filter out the unwanted information. Moreover, in real-time systems (e.g., visual trackers) ranking approaches are used as optimization procedures which improve the robustness of the system that deals with the high variability of the image streams that change over time. The other way around, learning to rank is necessary in the construction of ranking models for information retrieval, biometric authentication, re-identification, and recommender systems. In this context, the ranking model's purpose is to sort objects according to their degrees of relevance, importance, or preference as defined in the specific application.



### An Effective Approach for Point Clouds Registration Based on the Hard and Soft Assignments
- **Arxiv ID**: http://arxiv.org/abs/1706.00227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00227v1)
- **Published**: 2017-06-01 09:31:04+00:00
- **Updated**: 2017-06-01 09:31:04+00:00
- **Authors**: Congcong Jin, Jihua Zhu, Yaochen Li, Shaoyi Du, Zhongyu Li, Huimin Lu
- **Comment**: 23 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: For the registration of partially overlapping point clouds, this paper proposes an effective approach based on both the hard and soft assignments. Given two initially posed clouds, it firstly establishes the forward correspondence for each point in the data shape and calculates the value of binary variable, which can indicate whether this point correspondence is located in the overlapping areas or not. Then, it establishes the bilateral correspondence and computes bidirectional distances for each point in the overlapping areas. Based on the ratio of bidirectional distances, the exponential function is selected and utilized to calculate the probability value, which can indicate the reliability of the point correspondence. Subsequently, both the values of hard and soft assignments are embedded into the proposed objective function for registration of partially overlapping point clouds and a novel variant of ICP algorithm is proposed to obtain the optimal rigid transformation. The proposed approach can achieve good registration of point clouds, even when their overlap percentage is low. Experimental results tested on public data sets illustrate its superiority over previous approaches on accuracy and robustness.



### TransFlow: Unsupervised Motion Flow by Joint Geometric and Pixel-level Estimation
- **Arxiv ID**: http://arxiv.org/abs/1706.00322v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00322v3)
- **Published**: 2017-06-01 14:33:42+00:00
- **Updated**: 2017-10-30 09:34:47+00:00
- **Authors**: Stefano Alletto, Davide Abati, Simone Calderara, Rita Cucchiara, Luca Rigazio
- **Comment**: We have found a bug in the flow evaluation code compromising the
  experimental evaluation and the results provided in the paper are no longer
  correct. We are currently working on a new experimental campaign but we
  estimate that results will be available in a few weeks and will drastically
  change the paper, hence the withdraw request
- **Journal**: None
- **Summary**: We address unsupervised optical flow estimation for ego-centric motion. We argue that optical flow can be cast as a geometrical warping between two successive video frames and devise a deep architecture to estimate such transformation in two stages. First, a dense pixel-level flow is computed with a geometric prior imposing strong spatial constraints. Such prior is typical of driving scenes, where the point of view is coherent with the vehicle motion. We show how such global transformation can be approximated with an homography and how spatial transformer layers can be employed to compute the flow field implied by such transformation. The second stage then refines the prediction feeding a second deeper network. A final reconstruction loss compares the warping of frame X(t) with the subsequent frame X(t+1) and guides both estimates. The model, which we named TransFlow, performs favorably compared to other unsupervised algorithms, and shows better generalization compared to supervised methods with a 3x reduction in error on unseen data.



### Deep Mutual Learning
- **Arxiv ID**: http://arxiv.org/abs/1706.00384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00384v1)
- **Published**: 2017-06-01 16:57:15+00:00
- **Updated**: 2017-06-01 16:57:15+00:00
- **Authors**: Ying Zhang, Tao Xiang, Timothy M. Hospedales, Huchuan Lu
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, that is better suited to low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy where, rather than one way transfer between a static pre-defined teacher and a student, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on CIFAR-100 recognition and Market-1501 person re-identification benchmarks. Surprisingly, it is revealed that no prior powerful teacher network is necessary -- mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher.



### DiracNets: Training Very Deep Neural Networks Without Skip-Connections
- **Arxiv ID**: http://arxiv.org/abs/1706.00388v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00388v2)
- **Published**: 2017-06-01 17:02:11+00:00
- **Updated**: 2018-01-26 14:08:57+00:00
- **Authors**: Sergey Zagoruyko, Nikos Komodakis
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks with skip-connections, such as ResNet, show excellent performance in various image classification benchmarks. It is though observed that the initial motivation behind them - training deeper networks - does not actually hold true, and the benefits come from increased capacity, rather than from depth. Motivated by this, and inspired from ResNet, we propose a simple Dirac weight parameterization, which allows us to train very deep plain networks without explicit skip-connections, and achieve nearly the same performance. This parameterization has a minor computational cost at training time and no cost at all at inference, as both Dirac parameterization and batch normalization can be folded into convolutional filters, so that network becomes a simple chain of convolution-ReLU pairs. We are able to match ResNet-1001 accuracy on CIFAR-10 with 28-layer wider plain DiracNet, and closely match ResNets on ImageNet. Our parameterization also mostly eliminates the need of careful initialization in residual and non-residual networks. The code and models for our experiments are available at https://github.com/szagoruyko/diracnets



### Line Profile Based Segmentation Algorithm for Touching Corn Kernels
- **Arxiv ID**: http://arxiv.org/abs/1706.00396v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00396v3)
- **Published**: 2017-06-01 17:15:02+00:00
- **Updated**: 2017-08-04 00:27:43+00:00
- **Authors**: Ali Mahdi, Jun Qin
- **Comment**: We found some results in this paper may not be correct. Therefore, we
  require to withdraw this paper. Thanks
- **Journal**: None
- **Summary**: Image segmentation of touching objects plays a key role in providing accurate classification for computer vision technologies. A new line profile based imaging segmentation algorithm has been developed to provide a robust and accurate segmentation of a group of touching corns. The performance of the line profile based algorithm has been compared to a watershed based imaging segmentation algorithm. Both algorithms are tested on three different patterns of images, which are isolated corns, single-lines, and random distributed formations. The experimental results show that the algorithm can segment a large number of touching corn kernels efficiently and accurately.



### Fader Networks: Manipulating Images by Sliding Attributes
- **Arxiv ID**: http://arxiv.org/abs/1706.00409v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00409v2)
- **Published**: 2017-06-01 17:48:24+00:00
- **Updated**: 2018-01-28 16:12:14+00:00
- **Authors**: Guillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes, Ludovic Denoyer, Marc'Aurelio Ranzato
- **Comment**: NIPS 2017
- **Journal**: None
- **Summary**: This paper introduces a new encoder-decoder architecture that is trained to reconstruct images by disentangling the salient information of the image and the values of attributes directly in the latent space. As a result, after training, our model can generate different realistic versions of an input image by varying the attribute values. By using continuous attribute values, we can choose how much a specific attribute is perceivable in the generated image. This property could allow for applications where users can modify an image using sliding knobs, like faders on a mixing console, to change the facial expression of a portrait, or to update the color of some objects. Compared to the state-of-the-art which mostly relies on training adversarial networks in pixel space by altering attribute values at train time, our approach results in much simpler training schemes and nicely scales to multiple attributes. We present evidence that our model can significantly change the perceived value of the attributes while preserving the naturalness of images.



### Provenance Filtering for Multimedia Phylogeny
- **Arxiv ID**: http://arxiv.org/abs/1706.00447v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1706.00447v1)
- **Published**: 2017-06-01 18:12:57+00:00
- **Updated**: 2017-06-01 18:12:57+00:00
- **Authors**: Allan Pinto, Daniel Moreira, Aparna Bharati, Joel Brogan, Kevin Bowyer, Patrick Flynn, Walter Scheirer, Anderson Rocha
- **Comment**: 5 pages, Accepted in IEEE International Conference on Image
  Processing (ICIP), 2017
- **Journal**: None
- **Summary**: Departing from traditional digital forensics modeling, which seeks to analyze single objects in isolation, multimedia phylogeny analyzes the evolutionary processes that influence digital objects and collections over time. One of its integral pieces is provenance filtering, which consists of searching a potentially large pool of objects for the most related ones with respect to a given query, in terms of possible ancestors (donors or contributors) and descendants. In this paper, we propose a two-tiered provenance filtering approach to find all the potential images that might have contributed to the creation process of a given query $q$. In our solution, the first (coarse) tier aims to find the most likely "host" images --- the major donor or background --- contributing to a composite/doctored image. The search is then refined in the second tier, in which we search for more specific (potentially small) parts of the query that might have been extracted from other images and spliced into the query image. Experimental results with a dataset containing more than a million images show that the two-tiered solution underpinned by the context of the query is highly useful for solving this difficult task.



### Personalized Pancreatic Tumor Growth Prediction via Group Learning
- **Arxiv ID**: http://arxiv.org/abs/1706.00493v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.00493v1)
- **Published**: 2017-06-01 20:57:53+00:00
- **Updated**: 2017-06-01 20:57:53+00:00
- **Authors**: Ling Zhang, Le Lu, Ronald M. Summers, Electron Kebebew, Jianhua Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Tumor growth prediction, a highly challenging task, has long been viewed as a mathematical modeling problem, where the tumor growth pattern is personalized based on imaging and clinical data of a target patient. Though mathematical models yield promising results, their prediction accuracy may be limited by the absence of population trend data and personalized clinical characteristics. In this paper, we propose a statistical group learning approach to predict the tumor growth pattern that incorporates both the population trend and personalized data, in order to discover high-level features from multimodal imaging data. A deep convolutional neural network approach is developed to model the voxel-wise spatio-temporal tumor progression. The deep features are combined with the time intervals and the clinical factors to feed a process of feature selection. Our predictive model is pretrained on a group data set and personalized on the target patient data to estimate the future spatio-temporal progression of the patient's tumor. Multimodal imaging data at multiple time points are used in the learning, personalization and inference stages. Our method achieves a Dice coefficient of 86.8% +- 3.6% and RVD of 7.9% +- 5.4% on a pancreatic tumor data set, outperforming the DSC of 84.4% +- 4.0% and RVD 13.9% +- 9.8% obtained by a previous state-of-the-art model-based method.



### A Vision System for Multi-View Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1706.00510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00510v1)
- **Published**: 2017-06-01 22:10:31+00:00
- **Updated**: 2017-06-01 22:10:31+00:00
- **Authors**: M. Y. Shams, A. S. Tolba, S. H. Sarhan
- **Comment**: 7 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: Multimodal biometric identification has been grown a great attention in the most interests in the security fields. In the real world there exist modern system devices that are able to detect, recognize, and classify the human identities with reliable and fast recognition rates. Unfortunately most of these systems rely on one modality, and the reliability for two or more modalities are further decreased. The variations of face images with respect to different poses are considered as one of the important challenges in face recognition systems. In this paper, we propose a multimodal biometric system that able to detect the human face images that are not only one view face image, but also multi-view face images. Each subject entered to the system adjusted their face at front of the three cameras, and then the features of the face images are extracted based on Speeded Up Robust Features (SURF) algorithm. We utilize Multi-Layer Perceptron (MLP) and combined classifiers based on both Learning Vector Quantization (LVQ), and Radial Basis Function (RBF) for classification purposes. The proposed system has been tested using SDUMLA-HMT, and CASIA datasets. Furthermore, we collected a database of multi-view face images by which we take the additive white Gaussian noise into considerations. The results indicated the reliability, robustness of the proposed system with different poses and variations including noise images.



