# Arxiv Papers in cs.CV on 2017-06-27
### Dense Non-rigid Structure-from-Motion Made Easy - A Spatial-Temporal Smoothness based Solution
- **Arxiv ID**: http://arxiv.org/abs/1706.08629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08629v1)
- **Published**: 2017-06-27 00:18:55+00:00
- **Updated**: 2017-06-27 00:18:55+00:00
- **Authors**: Yuchao Dai, Huizhong Deng, Mingyi He
- **Comment**: Accepted by ICIP 2017
- **Journal**: None
- **Summary**: This paper proposes a simple spatial-temporal smoothness based method for solving dense non-rigid structure-from-motion (NRSfM). First, we revisit the temporal smoothness and demonstrate that it can be extended to dense case directly. Second, we propose to exploit the spatial smoothness by resorting to the Laplacian of the 3D non-rigid shape. Third, to handle real world noise and outliers in measurements, we robustify the data term by using the $L_1$ norm. In this way, our method could robustly exploit both spatial and temporal smoothness effectively and make dense non-rigid reconstruction easy. Our method is very easy to implement, which involves solving a series of least squares problems. Experimental results on both synthetic and real image dense NRSfM tasks show that the proposed method outperforms state-of-the-art dense non-rigid reconstruction methods.



### A Unified approach for Conventional Zero-shot, Generalized Zero-shot and Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1706.08653v2
- **DOI**: 10.1109/TIP.2018.2861573
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08653v2)
- **Published**: 2017-06-27 02:42:55+00:00
- **Updated**: 2017-10-26 22:55:39+00:00
- **Authors**: Shafin Rahman, Salman H. Khan, Fatih Porikli
- **Comment**: None
- **Journal**: None
- **Summary**: Prevalent techniques in zero-shot learning do not generalize well to other related problem scenarios. Here, we present a unified approach for conventional zero-shot, generalized zero-shot and few-shot learning problems. Our approach is based on a novel Class Adapting Principal Directions (CAPD) concept that allows multiple embeddings of image features into a semantic space. Given an image, our method produces one principal direction for each seen class. Then, it learns how to combine these directions to obtain the principal direction for each unseen class such that the CAPD of the test image is aligned with the semantic embedding of the true class, and opposite to the other classes. This allows efficient and class-adaptive information transfer from seen to unseen classes. In addition, we propose an automatic process for selection of the most useful seen classes for each unseen class to achieve robustness in zero-shot learning. Our method can update the unseen CAPD taking the advantages of few unseen images to work in a few-shot learning scenario. Furthermore, our method can generalize the seen CAPDs by estimating seen-unseen diversity that significantly improves the performance of generalized zero-shot learning. Our extensive evaluations demonstrate that the proposed approach consistently achieves superior performance in zero-shot, generalized zero-shot and few/one-shot learning problems.



### A region-growing approach for automatic outcrop fracture extraction from a three-dimensional point cloud
- **Arxiv ID**: http://arxiv.org/abs/1707.03266v1
- **DOI**: 10.1016/j.cageo.2016.11.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03266v1)
- **Published**: 2017-06-27 02:51:43+00:00
- **Updated**: 2017-06-27 02:51:43+00:00
- **Authors**: Xin Wang, Lejun Zou, Xiaohua Shen, Yupeng Ren, Yi Qin
- **Comment**: 25 pages, 7 figures
- **Journal**: Computers & Geosciences, 99, 100-106
- **Summary**: Conventional manual surveys of rock mass fractures usually require large amounts of time and labor; yet, they provide a relatively small set of data that cannot be considered representative of the study region. Terrestrial laser scanners are increasingly used for fracture surveys because they can efficiently acquire large area, high-resolution, three-dimensional (3D) point clouds from outcrops. However, extracting fractures and other planar surfaces from 3D outcrop point clouds is still a challenging task. No method has been reported that can be used to automatically extract the full extent of every individual fracture from a 3D outcrop point cloud. In this study, we propose a method using a region-growing approach to address this problem; the method also estimates the orientation of each fracture. In this method, criteria based on the local surface normal and curvature of the point cloud are used to initiate and control the growth of the fracture region. In tests using outcrop point cloud data, the proposed method identified and extracted the full extent of individual fractures with high accuracy. Compared with manually acquired field survey data, our method obtained better-quality fracture data, thereby demonstrating the high potential utility of the proposed method.



### Fast and accurate classification of echocardiograms using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1706.08658v1
- **DOI**: None
- **Categories**: **cs.CV**, H.2.1
- **Links**: [PDF](http://arxiv.org/pdf/1706.08658v1)
- **Published**: 2017-06-27 03:21:47+00:00
- **Updated**: 2017-06-27 03:21:47+00:00
- **Authors**: Ali Madani, Ramy Arnaout, Mohammad Mofrad, Rima Arnaout
- **Comment**: 31 pages, 8 figures
- **Journal**: None
- **Summary**: Echocardiography is essential to modern cardiology. However, human interpretation limits high throughput analysis, limiting echocardiography from reaching its full clinical and research potential for precision medicine. Deep learning is a cutting-edge machine-learning technique that has been useful in analyzing medical images but has not yet been widely applied to echocardiography, partly due to the complexity of echocardiograms' multi view, multi modality format. The essential first step toward comprehensive computer assisted echocardiographic interpretation is determining whether computers can learn to recognize standard views. To this end, we anonymized 834,267 transthoracic echocardiogram (TTE) images from 267 patients (20 to 96 years, 51 percent female, 26 percent obese) seen between 2000 and 2017 and labeled them according to standard views. Images covered a range of real world clinical variation. We built a multilayer convolutional neural network and used supervised learning to simultaneously classify 15 standard views. Eighty percent of data used was randomly chosen for training and 20 percent reserved for validation and testing on never seen echocardiograms. Using multiple images from each clip, the model classified among 12 video views with 97.8 percent overall test accuracy without overfitting. Even on single low resolution images, test accuracy among 15 views was 91.7 percent versus 70.2 to 83.5 percent for board-certified echocardiographers. Confusional matrices, occlusion experiments, and saliency mapping showed that the model finds recognizable similarities among related views and classifies using clinically relevant image features. In conclusion, deep neural networks can classify essential echocardiographic views simultaneously and with high accuracy. Our results provide a foundation for more complex deep learning assisted echocardiographic interpretation.



### Hierarchical Model for Long-term Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1706.08665v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08665v2)
- **Published**: 2017-06-27 04:11:41+00:00
- **Updated**: 2017-07-03 06:27:39+00:00
- **Authors**: Peter Wang, Zhongxia Yan, Jeff Zhang
- **Comment**: We have made some errors in the submission
- **Journal**: None
- **Summary**: Video prediction has been an active topic of research in the past few years. Many algorithms focus on pixel-level predictions, which generates results that blur and disintegrate within a few frames. In this project, we use a hierarchical approach for long-term video prediction. We aim at estimating high-level structure in the input frame first, then predict how that structure grows in the future. Finally, we use an image analogy network to recover a realistic image from the predicted structure. Our method is largely adopted from the work by Villegas et al. The method is built with a combination of LSTMs and analogy-based convolutional auto-encoder networks. Additionally, in order to generate more realistic frame predictions, we also adopt adversarial loss. We evaluate our method on the Penn Action dataset, and demonstrate good results on high-level long-term structure prediction.



### Material Recognition CNNs and Hierarchical Planning for Biped Robot Locomotion on Slippery Terrain
- **Arxiv ID**: http://arxiv.org/abs/1706.08685v1
- **DOI**: 10.1109/HUMANOIDS.2016.7803258
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.08685v1)
- **Published**: 2017-06-27 06:38:53+00:00
- **Updated**: 2017-06-27 06:38:53+00:00
- **Authors**: Martim Brandao, Yukitoshi Minami Shiguematsu, Kenji Hashimoto, Atsuo Takanishi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we tackle the problem of visually predicting surface friction for environments with diverse surfaces, and integrating this knowledge into biped robot locomotion planning. The problem is essential for autonomous robot locomotion since diverse surfaces with varying friction abound in the real world, from wood to ceramic tiles, grass or ice, which may cause difficulties or huge energy costs for robot locomotion if not considered. We propose to estimate friction and its uncertainty from visual estimation of material classes using convolutional neural networks, together with probability distribution functions of friction associated with each material. We then robustly integrate the friction predictions into a hierarchical (footstep and full-body) planning method using chance constraints, and optimize the same trajectory costs at both levels of the planning method for consistency. Our solution achieves fully autonomous perception and locomotion on slippery terrain, which considers not only friction and its uncertainty, but also collision, stability and trajectory cost. We show promising friction prediction results in real pictures of outdoor scenarios, and planning experiments on a real robot facing surfaces with different friction.



### Large-scale Datasets: Faces with Partial Occlusions and Pose Variations in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1706.08690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08690v1)
- **Published**: 2017-06-27 07:04:51+00:00
- **Updated**: 2017-06-27 07:04:51+00:00
- **Authors**: Tarik Alafif, Zeyad Hailat, Melih Aslan, Xuewen Chen
- **Comment**: 5 pages 8 figures 1 table
- **Journal**: None
- **Summary**: Face detection methods have relied on face datasets for training. However, existing face datasets tend to be in small scales for face learning in both constrained and unconstrained environments. In this paper, we first introduce our large-scale image datasets, Large-scale Labeled Face (LSLF) and noisy Large-scale Labeled Non-face (LSLNF). Our LSLF dataset consists of a large number of unconstrained multi-view and partially occluded faces. The faces have many variations in color and grayscale, image quality, image resolution, image illumination, image background, image illusion, human face, cartoon face, facial expression, light and severe partial facial occlusion, make up, gender, age, and race. Many of these faces are partially occluded with accessories such as tattoos, hats, glasses, sunglasses, hands, hair, beards, scarves, microphones, or other objects or persons. The LSLF dataset is currently the largest labeled face image dataset in the literature in terms of the number of labeled images and the number of individuals compared to other existing labeled face image datasets. Second, we introduce our CrowedFaces and CrowedNonFaces image datasets. The crowedFaces and CrowedNonFaces datasets include faces and non-faces images from crowed scenes. These datasets essentially aim for researchers to provide a large number of training examples with many variations for large scale face learning and face recognition tasks.



### Independent Motion Detection with Event-driven Cameras
- **Arxiv ID**: http://arxiv.org/abs/1706.08713v2
- **DOI**: 10.1109/ICAR.2017.8023661
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08713v2)
- **Published**: 2017-06-27 08:17:50+00:00
- **Updated**: 2017-07-04 19:48:45+00:00
- **Authors**: Valentina Vasco, Arren Glover, Elias Mueggler, Davide Scaramuzza, Lorenzo Natale, Chiara Bartolozzi
- **Comment**: 7 pages, 6 figures
- **Journal**: International Conference on Advanced Robotics (ICAR), 2017
- **Summary**: Unlike standard cameras that send intensity images at a constant frame rate, event-driven cameras asynchronously report pixel-level brightness changes, offering low latency and high temporal resolution (both in the order of micro-seconds). As such, they have great potential for fast and low power vision algorithms for robots. Visual tracking, for example, is easily achieved even for very fast stimuli, as only moving objects cause brightness changes. However, cameras mounted on a moving robot are typically non-stationary and the same tracking problem becomes confounded by background clutter events due to the robot ego-motion. In this paper, we propose a method for segmenting the motion of an independently moving object for event-driven cameras. Our method detects and tracks corners in the event stream and learns the statistics of their motion as a function of the robot's joint velocities when no independently moving objects are present. During robot operation, independently moving objects are identified by discrepancies between the predicted corner velocities from ego-motion and the measured corner velocities. We validate the algorithm on data collected from the neuromorphic iCub robot. We achieve a precision of ~ 90 % and show that the method is robust to changes in speed of both the head and the target.



### Topometric Localization with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1706.08775v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1706.08775v1)
- **Published**: 2017-06-27 11:03:31+00:00
- **Updated**: 2017-06-27 11:03:31+00:00
- **Authors**: Gabriel L. Oliveira, Noha Radwan, Wolfram Burgard, Thomas Brox
- **Comment**: 16 pages, 7 figures, ISRR 2017 submission
- **Journal**: None
- **Summary**: Compared to LiDAR-based localization methods, which provide high accuracy but rely on expensive sensors, visual localization approaches only require a camera and thus are more cost-effective while their accuracy and reliability typically is inferior to LiDAR-based methods. In this work, we propose a vision-based localization approach that learns from LiDAR-based localization methods by using their output as training data, thus combining a cheap, passive sensor with an accuracy that is on-par with LiDAR-based localization. The approach consists of two deep networks trained on visual odometry and topological localization, respectively, and a successive optimization to combine the predictions of these two networks. We evaluate the approach on a new challenging pedestrian-based dataset captured over the course of six months in varying weather conditions with a high degree of noise. The experiments demonstrate that the localization errors are up to 10 times smaller than with traditional vision-based localization methods.



### Auto-Encoder Guided GAN for Chinese Calligraphy Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1706.08789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08789v1)
- **Published**: 2017-06-27 11:35:31+00:00
- **Updated**: 2017-06-27 11:35:31+00:00
- **Authors**: Pengyuan Lyu, Xiang Bai, Cong Yao, Zhen Zhu, Tengteng Huang, Wenyu Liu
- **Comment**: submitted to ICADR2017
- **Journal**: None
- **Summary**: In this paper, we investigate the Chinese calligraphy synthesis problem: synthesizing Chinese calligraphy images with specified style from standard font(eg. Hei font) images (Fig. 1(a)). Recent works mostly follow the stroke extraction and assemble pipeline which is complex in the process and limited by the effect of stroke extraction. We treat the calligraphy synthesis problem as an image-to-image translation problem and propose a deep neural network based model which can generate calligraphy images from standard font images directly. Besides, we also construct a large scale benchmark that contains various styles for Chinese calligraphy synthesis. We evaluate our method as well as some baseline methods on the proposed dataset, and the experimental results demonstrate the effectiveness of our proposed model.



### Detecting Approximate Reflection Symmetry in a Point Set using Optimization on Manifold
- **Arxiv ID**: http://arxiv.org/abs/1706.08801v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08801v6)
- **Published**: 2017-06-27 12:03:39+00:00
- **Updated**: 2019-01-15 15:28:08+00:00
- **Authors**: Rajendra Nagar, Shanmuganathan Raman
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an algorithm to detect approximate reflection symmetry present in a set of volumetrically distributed points belonging to $\mathbb{R}^d$ containing a distorted reflection symmetry pattern. We pose the problem of detecting approximate reflection symmetry as the problem of establishing correspondences between the points which are reflections of each other and we determine the reflection symmetry transformation. We formulate an optimization framework in which the problem of establishing the correspondences amounts to solving a linear assignment problem and the problem of determining the reflection symmetry transformation amounts to solving an optimization problem on a smooth Riemannian product manifold. The proposed approach estimates the symmetry from the geometry of the points and is descriptor independent. We evaluate the performance of the proposed approach on the standard benchmark dataset and achieve the state-of-the-art performance. We further show the robustness of our approach by varying the amount of distortion in a perfect reflection symmetry pattern where we perturb each point by a different amount of perturbation. We demonstrate the effectiveness of the method by applying it to the problem of 2-D and 3-D reflection symmetry detection along with comparisons.



### Recurrent Residual Learning for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1706.08807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08807v1)
- **Published**: 2017-06-27 12:08:14+00:00
- **Updated**: 2017-06-27 12:08:14+00:00
- **Authors**: Ahsan Iqbal, Alexander Richard, Hilde Kuehne, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: Action recognition is a fundamental problem in computer vision with a lot of potential applications such as video surveillance, human computer interaction, and robot learning. Given pre-segmented videos, the task is to recognize actions happening within videos. Historically, hand crafted video features were used to address the task of action recognition. With the success of Deep ConvNets as an image analysis method, a lot of extensions of standard ConvNets were purposed to process variable length video data. In this work, we propose a novel recurrent ConvNet architecture called recurrent residual networks to address the task of action recognition. The approach extends ResNet, a state of the art model for image classification. While the original formulation of ResNet aims at learning spatial residuals in its layers, we extend the approach by introducing recurrent connections that allow to learn a spatio-temporal residual. In contrast to fully recurrent networks, our temporal connections only allow a limited range of preceding frames to contribute to the output for the current frame, enabling efficient training and inference as well as limiting the temporal context to a reasonable local range around each frame. On a large-scale action recognition dataset, we show that our model improves over both, the standard ResNet architecture and a ResNet extended by a fully recurrent layer.



### Reduced Electron Exposure for Energy-Dispersive Spectroscopy using Dynamic Sampling
- **Arxiv ID**: http://arxiv.org/abs/1707.03848v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.03848v1)
- **Published**: 2017-06-27 15:05:14+00:00
- **Updated**: 2017-06-27 15:05:14+00:00
- **Authors**: Yan Zhang, G. M. Dilshan Godaliyadda, Nicola Ferrier, Emine B. Gulsoy, Charles A. Bouman, Charudatta Phatak
- **Comment**: 22 pages, 8 figures
- **Journal**: None
- **Summary**: Analytical electron microscopy and spectroscopy of biological specimens, polymers, and other beam sensitive materials has been a challenging area due to irradiation damage. There is a pressing need to develop novel imaging and spectroscopic imaging methods that will minimize such sample damage as well as reduce the data acquisition time. The latter is useful for high-throughput analysis of materials structure and chemistry. In this work, we present a novel machine learning based method for dynamic sparse sampling of EDS data using a scanning electron microscope. Our method, based on the supervised learning approach for dynamic sampling algorithm and neural networks based classification of EDS data, allows a dramatic reduction in the total sampling of up to 90%, while maintaining the fidelity of the reconstructed elemental maps and spectroscopic data. We believe this approach will enable imaging and elemental mapping of materials that would otherwise be inaccessible to these analysis techniques.



### Cross-Country Skiing Gears Classification using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1706.08924v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08924v1)
- **Published**: 2017-06-27 16:14:00+00:00
- **Updated**: 2017-06-27 16:14:00+00:00
- **Authors**: Aliaa Rassem, Mohammed El-Beltagy, Mohamed Saleh
- **Comment**: 15 pages, 8 figures, 1 table
- **Journal**: None
- **Summary**: Human Activity Recognition has witnessed a significant progress in the last decade. Although a great deal of work in this field goes in recognizing normal human activities, few studies focused on identifying motion in sports. Recognizing human movements in different sports has high impact on understanding the different styles of humans in the play and on improving their performance. As deep learning models proved to have good results in many classification problems, this paper will utilize deep learning to classify cross-country skiing movements, known as gears, collected using a 3D accelerometer. It will also provide a comparison between different deep learning models such as convolutional and recurrent neural networks versus standard multi-layer perceptron. Results show that deep learning is more effective and has the highest classification accuracy.



### Training a Fully Convolutional Neural Network to Route Integrated Circuits
- **Arxiv ID**: http://arxiv.org/abs/1706.08948v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.08948v2)
- **Published**: 2017-06-27 17:20:21+00:00
- **Updated**: 2017-09-11 18:37:04+00:00
- **Authors**: Sambhav R. Jain, Kye Okabe
- **Comment**: Code released. 8 pages, 6 figures
- **Journal**: None
- **Summary**: We present a deep, fully convolutional neural network that learns to route a circuit layout net with appropriate choice of metal tracks and wire class combinations. Inputs to the network are the encoded layouts containing spatial location of pins to be routed. After 15 fully convolutional stages followed by a score comparator, the network outputs 8 layout layers (corresponding to 4 route layers, 3 via layers and an identity-mapped pin layer) which are then decoded to obtain the routed layouts. We formulate this as a binary segmentation problem on a per-pixel per-layer basis, where the network is trained to correctly classify pixels in each layout layer to be 'on' or 'off'. To demonstrate learnability of layout design rules, we train the network on a dataset of 50,000 train and 10,000 validation samples that we generate based on certain pre-defined layout constraints. Precision, recall and $F_1$ score metrics are used to track the training progress. Our network achieves $F_1\approx97\%$ on the train set and $F_1\approx92\%$ on the validation set. We use PyTorch for implementing our model. Code is made publicly available at https://github.com/sjain-stanford/deep-route .



