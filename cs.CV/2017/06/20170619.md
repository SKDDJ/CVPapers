# Arxiv Papers in cs.CV on 2017-06-19
### Rotational Rectification Network: Enabling Pedestrian Detection for Mobile Vision
- **Arxiv ID**: http://arxiv.org/abs/1706.08917v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08917v4)
- **Published**: 2017-06-19 00:08:38+00:00
- **Updated**: 2020-11-03 13:53:52+00:00
- **Authors**: Xinshuo Weng, Shangxuan Wu, Fares Beainy, Kris Kitani
- **Comment**: published in WACV 2018. Author's homepage is
  http://www.xinshuoweng.com/publications.html
- **Journal**: None
- **Summary**: Across a majority of pedestrian detection datasets, it is typically assumed that pedestrians will be standing upright with respect to the image coordinate system. This assumption, however, is not always valid for many vision-equipped mobile platforms such as mobile phones, UAVs or construction vehicles on rugged terrain. In these situations, the motion of the camera can cause images of pedestrians to be captured at extreme angles. This can lead to very poor pedestrian detection performance when using standard pedestrian detectors. To address this issue, we propose a Rotational Rectification Network (R2N) that can be inserted into any CNN-based pedestrian (or object) detector to adapt it to significant changes in camera rotation. The rotational rectification network uses a 2D rotation estimation module that passes rotational information to a spatial transformer network to undistort image features. To enable robust rotation estimation, we propose a Global Polar Pooling (GP-Pooling) operator to capture rotational shifts in convolutional features. Through our experiments, we show how our rotational rectification network can be used to improve the performance of the state-of-the-art pedestrian detector under heavy image rotation by up to 45%



### Exploring Content-based Artwork Recommendation with Metadata and Visual Features
- **Arxiv ID**: http://arxiv.org/abs/1706.05786v3
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.05786v3)
- **Published**: 2017-06-19 04:55:10+00:00
- **Updated**: 2017-10-23 19:23:14+00:00
- **Authors**: Pablo Messina, Vicente Dominguez, Denis Parra, Christoph Trattner, Alvaro Soto
- **Comment**: 1 figure, 1 table
- **Journal**: None
- **Summary**: Compared to other areas, artwork recommendation has received little attention, despite the continuous growth of the artwork market. Previous research has relied on ratings and metadata to make artwork recommendations, as well as visual features extracted with deep neural networks (DNN). However, these features have no direct interpretation to explicit visual features (e.g. brightness, texture) which might hinder explainability and user-acceptance. In this work, we study the impact of artwork metadata as well as visual features (DNN-based and attractiveness-based) for physical artwork recommendation, using images and transaction data from the UGallery online artwork store.   Our results indicate that: (i) visual features perform better than manually curated data, (ii) DNN-based visual features perform better than attractiveness-based ones, and (iii) a hybrid approach improves the performance further. Our research can inform the development of new artwork recommenders relying on diverse content data.



### An Entropy-based Pruning Method for CNN Compression
- **Arxiv ID**: http://arxiv.org/abs/1706.05791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05791v1)
- **Published**: 2017-06-19 05:29:29+00:00
- **Updated**: 2017-06-19 05:29:29+00:00
- **Authors**: Jian-Hao Luo, Jianxin Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims to simultaneously accelerate and compress off-the-shelf CNN models via filter pruning strategy. The importance of each filter is evaluated by the proposed entropy-based method first. Then several unimportant filters are discarded to get a smaller CNN model. Finally, fine-tuning is adopted to recover its generalization ability which is damaged during filter pruning. Our method can reduce the size of intermediate activations, which would dominate most memory footprint during model training stage but is less concerned in previous compression methods. Experiments on the ILSVRC-12 benchmark demonstrate the effectiveness of our method. Compared with previous filter importance evaluation criteria, our entropy-based method obtains better performance. We achieve 3.3x speed-up and 16.64x compression on VGG-16, 1.54x acceleration and 1.47x compression on ResNet-50, both with about 1% top-5 accuracy decrease.



### Rapid Probabilistic Interest Learning from Domain-Specific Pairwise Image Comparisons
- **Arxiv ID**: http://arxiv.org/abs/1706.05850v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1706.05850v3)
- **Published**: 2017-06-19 09:37:29+00:00
- **Updated**: 2020-05-22 08:12:37+00:00
- **Authors**: Michael Burke, Siyabonga Mbonambi, Purity Molala, Raesetje Sefala
- **Comment**: None
- **Journal**: None
- **Summary**: A great deal of work aims to discover large general purpose models of image interest or memorability for visual search and information retrieval. This paper argues that image interest is often domain and user specific, and that efficient mechanisms for learning about this domain-specific image interest as quickly as possible, while limiting the amount of data-labelling required, are often more useful to end-users. This work uses pairwise image comparisons to reduce the labelling burden on these users, and introduces an image interest estimation approach that performs similarly to recent data hungry deep learning approaches trained using pairwise ranking losses. Here, we use a Gaussian process model to interpolate image interest inferred using a Bayesian ranking approach over image features extracted using a pre-trained convolutional neural network. Results show that fitting a Gaussian process in high-dimensional image feature space is not only computationally feasible, but also effective across a broad range of domains. The proposed probabilistic interest estimation approach produces image interests paired with uncertainties that can be used to identify images for which additional labelling is required and measure inference convergence, allowing for sample efficient active model training. Importantly, the probabilistic formulation allows for effective visual search and information retrieval when limited labelling data is available.



### Histograms of Gaussian normal distribution for feature matching in clutter scenes
- **Arxiv ID**: http://arxiv.org/abs/1706.05864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05864v1)
- **Published**: 2017-06-19 10:23:14+00:00
- **Updated**: 2017-06-19 10:23:14+00:00
- **Authors**: Wei Zhou, Caiwen Ma, Arjan Kuijper
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: 3D feature descriptor provide information between corresponding models and scenes. 3D objection recognition in cluttered scenes, however, remains a largely unsolved problem. Practical applications impose several challenges which are not fully addressed by existing methods. Especially in cluttered scenes there are many feature mismatches between scenes and models. We therefore propose Histograms of Gaussian Normal Distribution (HGND) for extracting salient features on a local reference frame (LRF) that enables us to solve this problem. We propose a LRF on each local surface patches using the scatter matrix's eigenvectors. Then the HGND information of each salient point is calculated on the LRF, for which we use both the mesh and point data of the depth image. Experiments on 45 cluttered scenes of the Bologna Dataset and 50 cluttered scenes of the UWA Dataset are made to evaluate the robustness and descriptiveness of our HGND. Experiments carried out by us demonstrate that HGND obtains a more reliable matching rate than state-of-the-art approaches in cluttered situations.



### Deep learning with spatiotemporal consistency for nerve segmentation in ultrasound images
- **Arxiv ID**: http://arxiv.org/abs/1706.05870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05870v1)
- **Published**: 2017-06-19 10:36:39+00:00
- **Updated**: 2017-06-19 10:36:39+00:00
- **Authors**: Adel Hafiane, Pierre Vieyres, Alain Delbos
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound-Guided Regional Anesthesia (UGRA) has been gaining importance in the last few years, offering numerous advantages over alternative methods of nerve localization (neurostimulation or paraesthesia). However, nerve detection is one of the most tasks that anaesthetists can encounter in the UGRA procedure. Computer aided system that can detect automatically region of nerve, would help practitioner to concentrate more in anaesthetic delivery. In this paper we propose a new method based on deep learning combined with spatiotemporal information to robustly segment the nerve region. The proposed method is based on two phases, localisation and segmentation. The first phase, consists in using convolutional neural network combined with spatial and temporal consistency to detect the nerve zone. The second phase utilises active contour model to delineate the region of interest. Obtained results show the validity of the proposed approach and its robustness.



### Pedestrian Prediction by Planning using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.05904v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05904v2)
- **Published**: 2017-06-19 12:40:30+00:00
- **Updated**: 2017-06-20 07:25:49+00:00
- **Authors**: Eike Rehder, Florian Wirth, Martin Lauer, Christoph Stiller
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate traffic participant prediction is the prerequisite for collision avoidance of autonomous vehicles. In this work, we predict pedestrians by emulating their own motion planning. From online observations, we infer a mixture density function for possible destinations. We use this result as the goal states of a planning stage that performs motion prediction based on common behavior patterns. The entire system is modeled as one monolithic neural network and trained via inverse reinforcement learning. Experimental validation on real world data shows the system's ability to predict both, destinations and trajectories accurately.



### Bayesian Joint Modelling for Object Localisation in Weakly Labelled Images
- **Arxiv ID**: http://arxiv.org/abs/1706.05952v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05952v1)
- **Published**: 2017-06-19 13:59:48+00:00
- **Updated**: 2017-06-19 13:59:48+00:00
- **Authors**: Zhiyuan Shi, Timothy M. Hospedales, Tao Xiang
- **Comment**: Accepted in IEEE Transaction on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: We address the problem of localisation of objects as bounding boxes in images and videos with weak labels. This weakly supervised object localisation problem has been tackled in the past using discriminative models where each object class is localised independently from other classes. In this paper, a novel framework based on Bayesian joint topic modelling is proposed, which differs significantly from the existing ones in that: (1) All foreground object classes are modelled jointly in a single generative model that encodes multiple object co-existence so that "explaining away" inference can resolve ambiguity and lead to better learning and localisation. (2) Image backgrounds are shared across classes to better learn varying surroundings and "push out" objects of interest. (3) Our model can be learned with a mixture of weakly labelled and unlabelled data, allowing the large volume of unlabelled images on the Internet to be exploited for learning. Moreover, the Bayesian formulation enables the exploitation of various types of prior knowledge to compensate for the limited supervision offered by weakly labelled data, as well as Bayesian domain adaptation for transfer learning. Extensive experiments on the PASCAL VOC, ImageNet and YouTube-Object videos datasets demonstrate the effectiveness of our Bayesian joint model for weakly supervised object localisation.



### Visual Decoding of Targets During Visual Search From Human Eye Fixations
- **Arxiv ID**: http://arxiv.org/abs/1706.05993v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1706.05993v3)
- **Published**: 2017-06-19 14:52:30+00:00
- **Updated**: 2017-06-21 11:19:10+00:00
- **Authors**: Hosnieh Sattar, Mario Fritz, Andreas Bulling
- **Comment**: None
- **Journal**: None
- **Summary**: What does human gaze reveal about a users' intents and to which extend can these intents be inferred or even visualized? Gaze was proposed as an implicit source of information to predict the target of visual search and, more recently, to predict the object class and attributes of the search target. In this work, we go one step further and investigate the feasibility of combining recent advances in encoding human gaze information using deep convolutional neural networks with the power of generative image models to visually decode, i.e. create a visual representation of, the search target. Such visual decoding is challenging for two reasons: 1) the search target only resides in the user's mind as a subjective visual pattern, and can most often not even be described verbally by the person, and 2) it is, as of yet, unclear if gaze fixations contain sufficient information for this task at all. We show, for the first time, that visual representations of search targets can indeed be decoded only from human gaze fixations. We propose to first encode fixations into a semantic representation and then decode this representation into an image. We evaluate our method on a recent gaze dataset of 14 participants searching for clothing in image collages and validate the model's predictions using two human studies. Our results show that 62% (Chance level = 10%) of the time users were able to select the categories of the decoded image right. In our second studies we show the importance of a local gaze encoding for decoding visual search targets of user



### Using Transfer Learning for Image-Based Cassava Disease Detection
- **Arxiv ID**: http://arxiv.org/abs/1707.03717v2
- **DOI**: 10.3389/fpls.2017.01852
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1707.03717v2)
- **Published**: 2017-06-19 15:01:59+00:00
- **Updated**: 2017-08-01 19:29:43+00:00
- **Authors**: Amanda Ramcharan, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed, James Legg, David Hughes
- **Comment**: 10 pages, 4 figures
- **Journal**: Frontiers in Plant Science 2017 vol. 8 p. 1852
- **Summary**: Cassava is the third largest source of carbohydrates for human food in the world but is vulnerable to virus diseases, which threaten to destabilize food security in sub-Saharan Africa. Novel methods of cassava disease detection are needed to support improved control which will prevent this crisis. Image recognition offers both a cost effective and scalable technology for disease detection. New transfer learning methods offer an avenue for this technology to be easily deployed on mobile devices. Using a dataset of cassava disease images taken in the field in Tanzania, we applied transfer learning to train a deep convolutional neural network to identify three diseases and two types of pest damage (or lack thereof). The best trained model accuracies were 98% for brown leaf spot (BLS), 96% for red mite damage (RMD), 95% for green mite damage (GMD), 98% for cassava brown streak disease (CBSD), and 96% for cassava mosaic disease (CMD). The best model achieved an overall accuracy of 93% for data not used in the training process. Our results show that the transfer learning approach for image recognition of field images offers a fast, affordable, and easily deployable strategy for digital plant disease detection.



### Optimising the topological information of the $A_\infty$-persistence groups
- **Arxiv ID**: http://arxiv.org/abs/1706.06019v1
- **DOI**: 10.1007/s00454-019-00094-x
- **Categories**: **math.AT**, cs.CG, cs.CV, 16E45, 18G55, 55S30, 57M25, 57Q45, 18G40, 55P62, 55U99
- **Links**: [PDF](http://arxiv.org/pdf/1706.06019v1)
- **Published**: 2017-06-19 15:37:53+00:00
- **Updated**: 2017-06-19 15:37:53+00:00
- **Authors**: Francisco Belchí
- **Comment**: 26 pages, 3 figures
- **Journal**: Discrete and Computational Geometry, Volume 62(1) (2019), pages
  29-54
- **Summary**: Persistent homology typically studies the evolution of homology groups $H_p(X)$ (with coefficients in a field) along a filtration of topological spaces. $A_\infty$-persistence extends this theory by analysing the evolution of subspaces such as $V := \text{Ker}\, {\Delta_n}_{| H_p(X)} \subseteq H_p(X)$, where $\{\Delta_m\}_{m\geq1}$ denotes a structure of $A_\infty$-coalgebra on $H_*(X)$. In this paper we illustrate how $A_\infty$-persistence can be useful beyond persistent homology by discussing the topological meaning of $V$, which is the most basic form of $A_\infty$-persistence group. In addition, we explore how to choose $A_\infty$-coalgebras along a filtration to make the $A_\infty$-persistence groups carry more faithful information.



### The $\mathcal{E}$-Average Common Submatrix: Approximate Searching in a Restricted Neighborhood
- **Arxiv ID**: http://arxiv.org/abs/1706.06026v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DM, cs.IR, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1706.06026v1)
- **Published**: 2017-06-19 15:53:07+00:00
- **Updated**: 2017-06-19 15:53:07+00:00
- **Authors**: Alessia Amelio, Darko Brodić
- **Comment**: 4 pages, 18th International Workshop on Combinatorial Image Analysis
  (IWCIA 2017), Short Communication
- **Journal**: None
- **Summary**: This paper introduces a new (dis)similarity measure for 2D arrays, extending the Average Common Submatrix measure. This is accomplished by: (i) considering the frequency of matching patterns, (ii) restricting the pattern matching to a fixed-size neighborhood, and (iii) computing a distance-based approximate matching. This will achieve better performances with low execution time and larger information retrieval.



### Evaluating 35 Methods to Generate Structural Connectomes Using Pairwise Classification
- **Arxiv ID**: http://arxiv.org/abs/1706.06031v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.06031v1)
- **Published**: 2017-06-19 16:05:11+00:00
- **Updated**: 2017-06-19 16:05:11+00:00
- **Authors**: Dmitry Petrov, Alexander Ivanov, Joshua Faskowitz, Boris Gutman, Daniel Moyer, Julio Villalon, Neda Jahanshad, Paul Thompson
- **Comment**: Accepted for MICCAI 2017, 8 pages, 3 figures
- **Journal**: None
- **Summary**: There is no consensus on how to construct structural brain networks from diffusion MRI. How variations in pre-processing steps affect network reliability and its ability to distinguish subjects remains opaque. In this work, we address this issue by comparing 35 structural connectome-building pipelines. We vary diffusion reconstruction models, tractography algorithms and parcellations. Next, we classify structural connectome pairs as either belonging to the same individual or not. Connectome weights and eight topological derivative measures form our feature set. For experiments, we use three test-retest datasets from the Consortium for Reliability and Reproducibility (CoRR) comprised of a total of 105 individuals. We also compare pairwise classification results to a commonly used parametric test-retest measure, Intraclass Correlation Coefficient (ICC).



### Endoscopic Depth Measurement and Super-Spectral-Resolution Imaging
- **Arxiv ID**: http://arxiv.org/abs/1706.06081v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06081v2)
- **Published**: 2017-06-19 17:49:20+00:00
- **Updated**: 2017-06-21 13:13:04+00:00
- **Authors**: Jianyu Lin, Neil T. Clancy, Yang Hu, Ji Qi, Taran Tatla, Danail Stoyanov, Lena Maier-Hein, Daniel S. Elson
- **Comment**: accepted by MICCAI2017
- **Journal**: None
- **Summary**: Intra-operative measurements of tissue shape and multi/ hyperspectral information have the potential to provide surgical guidance and decision making support. We report an optical probe based system to combine sparse hyperspectral measurements and spectrally-encoded structured lighting (SL) for surface measurements. The system provides informative signals for navigation with a surgical interface. By rapidly switching between SL and white light (WL) modes, SL information is combined with structure-from-motion (SfM) from white light images, based on SURF feature detection and Lucas-Kanade (LK) optical flow to provide quasi-dense surface shape reconstruction with known scale in real-time. Furthermore, "super-spectral-resolution" was realized, whereby the RGB images and sparse hyperspectral data were integrated to recover dense pixel-level hyperspectral stacks, by using convolutional neural networks to upscale the wavelength dimension. Validation and demonstration of this system is reported on ex vivo/in vivo animal/ human experiments.



### Satellite Imagery Feature Detection using Deep Convolutional Neural Network: A Kaggle Competition
- **Arxiv ID**: http://arxiv.org/abs/1706.06169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06169v1)
- **Published**: 2017-06-19 20:41:42+00:00
- **Updated**: 2017-06-19 20:41:42+00:00
- **Authors**: Vladimir Iglovikov, Sergey Mushinskiy, Vladimir Osin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes our approach to the DSTL Satellite Imagery Feature Detection challenge run by Kaggle. The primary goal of this challenge is accurate semantic segmentation of different classes in satellite imagery. Our approach is based on an adaptation of fully convolutional neural network for multispectral data processing. In addition, we defined several modifications to the training objective and overall training pipeline, e.g. boundary effect estimation, also we discuss usage of data augmentation strategies and reflectance indices. Our solution scored third place out of 419 entries. Its accuracy is comparable to the first two places, but unlike those solutions, it doesn't rely on complex ensembling techniques and thus can be easily scaled for deployment in production as a part of automatic feature labeling systems for satellite imagery analysis.



### Multi-Target Tracking in Multiple Non-Overlapping Cameras using Constrained Dominant Sets
- **Arxiv ID**: http://arxiv.org/abs/1706.06196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06196v1)
- **Published**: 2017-06-19 22:34:52+00:00
- **Updated**: 2017-06-19 22:34:52+00:00
- **Authors**: Yonatan Tariku Tesfaye, Eyasu Zemene, Andrea Prati, Marcello Pelillo, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a unified three-layer hierarchical approach for solving tracking problems in multiple non-overlapping cameras is proposed. Given a video and a set of detections (obtained by any person detector), we first solve within-camera tracking employing the first two layers of our framework and, then, in the third layer, we solve across-camera tracking by merging tracks of the same person in all cameras in a simultaneous fashion. To best serve our purpose, a constrained dominant sets clustering (CDSC) technique, a parametrized version of standard quadratic optimization, is employed to solve both tracking tasks. The tracking problem is caste as finding constrained dominant sets from a graph. In addition to having a unified framework that simultaneously solves within- and across-camera tracking, the third layer helps link broken tracks of the same person occurring during within-camera tracking. In this work, we propose a fast algorithm, based on dynamics from evolutionary game theory, which is efficient and salable to large-scale real-world applications.



### meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting
- **Arxiv ID**: http://arxiv.org/abs/1706.06197v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.06197v5)
- **Published**: 2017-06-19 22:36:33+00:00
- **Updated**: 2019-03-11 02:57:03+00:00
- **Authors**: Xu Sun, Xuancheng Ren, Shuming Ma, Houfeng Wang
- **Comment**: Accepted by the 34th International Conference on Machine Learning
  (ICML 2017)
- **Journal**: None
- **Summary**: We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-$k$ elements (in terms of magnitude) are kept. As a result, only $k$ rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction ($k$ divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1-4% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The code is available at https://github.com/lancopku/meProp



### Using deep learning to reveal the neural code for images in primary visual cortex
- **Arxiv ID**: http://arxiv.org/abs/1706.06208v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.06208v1)
- **Published**: 2017-06-19 23:13:54+00:00
- **Updated**: 2017-06-19 23:13:54+00:00
- **Authors**: William F. Kindel, Elijah D. Christensen, Joel Zylberberg
- **Comment**: None
- **Journal**: None
- **Summary**: Primary visual cortex (V1) is the first stage of cortical image processing, and a major effort in systems neuroscience is devoted to understanding how it encodes information about visual stimuli. Within V1, many neurons respond selectively to edges of a given preferred orientation: these are known as simple or complex cells, and they are well-studied. Other neurons respond to localized center-surround image features. Still others respond selectively to certain image stimuli, but the specific features that excite them are unknown. Moreover, even for the simple and complex cells-- the best-understood V1 neurons-- it is challenging to predict how they will respond to natural image stimuli. Thus, there are important gaps in our understanding of how V1 encodes images. To fill this gap, we train deep convolutional neural networks to predict the firing rates of V1 neurons in response to natural image stimuli, and find that 15% of these neurons are within 10% of their theoretical limit of predictability. For these well predicted neurons, we invert the predictor network to identify the image features (receptive fields) that cause the V1 neurons to spike. In addition to those with previously-characterized receptive fields (Gabor wavelet and center-surround), we identify neurons that respond predictably to higher-level textural image features that are not localized to any particular region of the image.



### Dualing GANs
- **Arxiv ID**: http://arxiv.org/abs/1706.06216v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.06216v1)
- **Published**: 2017-06-19 23:28:49+00:00
- **Updated**: 2017-06-19 23:28:49+00:00
- **Authors**: Yujia Li, Alexander Schwing, Kuan-Chieh Wang, Richard Zemel
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial nets (GANs) are a promising technique for modeling a distribution from samples. It is however well known that GAN training suffers from instability due to the nature of its maximin formulation. In this paper, we explore ways to tackle the instability problem by dualizing the discriminator. We start from linear discriminators in which case conjugate duality provides a mechanism to reformulate the saddle point objective into a maximization problem, such that both the generator and the discriminator of this 'dualing GAN' act in concert. We then demonstrate how to extend this intuition to non-linear formulations. For GANs with linear discriminators our approach is able to remove the instability in training, while for GANs with nonlinear discriminators our approach provides an alternative to the commonly used GAN training algorithm.



