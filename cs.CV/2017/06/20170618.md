# Arxiv Papers in cs.CV on 2017-06-18
### Dimensionality Reduction using Similarity-induced Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1706.05692v3
- **DOI**: 10.1109/TNNLS.2017.2728818
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05692v3)
- **Published**: 2017-06-18 17:03:02+00:00
- **Updated**: 2017-08-18 07:41:27+00:00
- **Authors**: Nikolaos Passalis, Anastasios Tefas
- **Comment**: Accepted in IEEE Transactions on Neural Networks and Learning Systems
- **Journal**: None
- **Summary**: The vast majority of Dimensionality Reduction (DR) techniques rely on second-order statistics to define their optimization objective. Even though this provides adequate results in most cases, it comes with several shortcomings. The methods require carefully designed regularizers and they are usually prone to outliers. In this work, a new DR framework, that can directly model the target distribution using the notion of similarity instead of distance, is introduced. The proposed framework, called Similarity Embedding Framework, can overcome the aforementioned limitations and provides a conceptually simpler way to express optimization targets similar to existing DR techniques. Deriving a new DR technique using the Similarity Embedding Framework becomes simply a matter of choosing an appropriate target similarity matrix. A variety of classical tasks, such as performing supervised dimensionality reduction and providing out-of-of-sample extensions, as well as, new novel techniques, such as providing fast linear embeddings for complex techniques, are demonstrated in this paper using the proposed framework. Six datasets from a diverse range of domains are used to evaluate the proposed method and it is demonstrated that it can outperform many existing DR techniques.



### Tversky loss function for image segmentation using 3D fully convolutional deep networks
- **Arxiv ID**: http://arxiv.org/abs/1706.05721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05721v1)
- **Published**: 2017-06-18 20:35:27+00:00
- **Updated**: 2017-06-18 20:35:27+00:00
- **Authors**: Seyed Sadegh Mohseni Salehi, Deniz Erdogmus, Ali Gholipour
- **Comment**: None
- **Journal**: None
- **Summary**: Fully convolutional deep neural networks carry out excellent potential for fast and accurate image segmentation. One of the main challenges in training these networks is data imbalance, which is particularly problematic in medical imaging applications such as lesion segmentation where the number of lesion voxels is often much lower than the number of non-lesion voxels. Training with unbalanced data can lead to predictions that are severely biased towards high precision but low recall (sensitivity), which is undesired especially in medical applications where false negatives are much less tolerable than false positives. Several methods have been proposed to deal with this problem including balanced sampling, two step training, sample re-weighting, and similarity loss functions. In this paper, we propose a generalized loss function based on the Tversky index to address the issue of data imbalance and achieve much better trade-off between precision and recall in training 3D fully convolutional deep neural networks. Experimental results in multiple sclerosis lesion segmentation on magnetic resonance images show improved F2 score, Dice coefficient, and the area under the precision-recall curve in test data. Based on these results we suggest Tversky loss function as a generalized framework to effectively train deep neural networks.



### Using Deep Networks for Drone Detection
- **Arxiv ID**: http://arxiv.org/abs/1706.05726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05726v1)
- **Published**: 2017-06-18 20:50:56+00:00
- **Updated**: 2017-06-18 20:50:56+00:00
- **Authors**: Cemal Aker, Sinan Kalkan
- **Comment**: To appear in International Workshop on Small-Drone Surveillance,
  Detection and Counteraction Techniques organised within AVSS 2017
- **Journal**: None
- **Summary**: Drone detection is the problem of finding the smallest rectangle that encloses the drone(s) in a video sequence. In this study, we propose a solution using an end-to-end object detection model based on convolutional neural networks. To solve the scarce data problem for training the network, we propose an algorithm for creating an extensive artificial dataset by combining background-subtracted real images. With this approach, we can achieve precision and recall values both of which are high at the same time.



### 3D Convolutional Neural Networks for Cross Audio-Visual Matching Recognition
- **Arxiv ID**: http://arxiv.org/abs/1706.05739v5
- **DOI**: 10.1109/ACCESS.2017.2761539
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.05739v5)
- **Published**: 2017-06-18 22:33:32+00:00
- **Updated**: 2017-08-13 02:20:41+00:00
- **Authors**: Amirsina Torfi, Seyed Mehdi Iranmanesh, Nasser M. Nasrabadi, Jeremy Dawson
- **Comment**: None
- **Journal**: IEEE Access (Year: 2017, Volume: PP, Issue: 99 )
- **Summary**: Audio-visual recognition (AVR) has been considered as a solution for speech recognition tasks when the audio is corrupted, as well as a visual recognition method used for speaker verification in multi-speaker scenarios. The approach of AVR systems is to leverage the extracted information from one modality to improve the recognition ability of the other modality by complementing the missing information. The essential problem is to find the correspondence between the audio and visual streams, which is the goal of this work. We propose the use of a coupled 3D Convolutional Neural Network (3D-CNN) architecture that can map both modalities into a representation space to evaluate the correspondence of audio-visual streams using the learned multimodal features. The proposed architecture will incorporate both spatial and temporal information jointly to effectively find the correlation between temporal information for different modalities. By using a relatively small network architecture and much smaller dataset for training, our proposed method surpasses the performance of the existing similar methods for audio-visual matching which use 3D CNNs for feature representation. We also demonstrate that an effective pair selection method can significantly increase the performance. The proposed method achieves relative improvements over 20% on the Equal Error Rate (EER) and over 7% on the Average Precision (AP) in comparison to the state-of-the-art method.



