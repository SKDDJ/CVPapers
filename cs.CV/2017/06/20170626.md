# Arxiv Papers in cs.CV on 2017-06-26
### End-to-end Learning of Image based Lane-Change Decision
- **Arxiv ID**: http://arxiv.org/abs/1706.08211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08211v1)
- **Published**: 2017-06-26 02:59:56+00:00
- **Updated**: 2017-06-26 02:59:56+00:00
- **Authors**: Seong-Gyun Jeong, Jiwon Kim, Sujung Kim, Jaesik Min
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an image based end-to-end learning framework that helps lane-change decisions for human drivers and autonomous vehicles. The proposed system, Safe Lane-Change Aid Network (SLCAN), trains a deep convolutional neural network to classify the status of adjacent lanes from rear view images acquired by cameras mounted on both sides of the vehicle. Rather than depending on any explicit object detection or tracking scheme, SLCAN reads the whole input image and directly decides whether initiation of the lane-change at the moment is safe or not. We collected and annotated 77,273 rear side view images to train and test SLCAN. Experimental results show that the proposed framework achieves 96.98% classification accuracy although the test images are from unseen roadways. We also visualize the saliency map to understand which part of image SLCAN looks at for correct decisions.



### YoTube: Searching Action Proposal via Recurrent and Static Regression Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.08218v1
- **DOI**: 10.1109/TIP.2018.2806279
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08218v1)
- **Published**: 2017-06-26 03:52:15+00:00
- **Updated**: 2017-06-26 03:52:15+00:00
- **Authors**: Hongyuan Zhu, Romain Vial, Shijian Lu, Yonghong Tian, Xianbin Cao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present YoTube-a novel network fusion framework for searching action proposals in untrimmed videos, where each action proposal corresponds to a spatialtemporal video tube that potentially locates one human action. Our method consists of a recurrent YoTube detector and a static YoTube detector, where the recurrent YoTube explores the regression capability of RNN for candidate bounding boxes predictions using learnt temporal dynamics and the static YoTube produces the bounding boxes using rich appearance cues in a single frame. Both networks are trained using rgb and optical flow in order to fully exploit the rich appearance, motion and temporal context, and their outputs are fused to produce accurate and robust proposal boxes. Action proposals are finally constructed by linking these boxes using dynamic programming with a novel trimming method to handle the untrimmed video effectively and efficiently. Extensive experiments on the challenging UCF-101 and UCF-Sports datasets show that our proposed technique obtains superior performance compared with the state-of-the-art.



### Multi-level SVM Based CAD Tool for Classifying Structural MRIs
- **Arxiv ID**: http://arxiv.org/abs/1706.08227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08227v1)
- **Published**: 2017-06-26 04:30:36+00:00
- **Updated**: 2017-06-26 04:30:36+00:00
- **Authors**: Jerrin Thomas Panachakel, Jeena R. S.
- **Comment**: None
- **Journal**: None
- **Summary**: The revolutionary developments in the field of supervised machine learning have paved way to the development of CAD tools for assisting doctors in diagnosis. Recently, the former has been employed in the prediction of neurological disorders such as Alzheimer's disease. We propose a CAD (Computer Aided Diagnosis tool for differentiating neural lesions caused by CVA (Cerebrovascular Accident) from the lesions caused by other neural disorders by using Non-negative Matrix Factorisation (NMF) and Haralick features for feature extraction and SVM (Support Vector Machine) for pattern recognition. We also introduce a multi-level classification system that has better classification efficiency, sensitivity and specificity when compared to systems using NMF or Haralick features alone as features for classification. Cross-validation was performed using LOOCV (Leave-One-Out Cross Validation) method and our proposed system has a classification accuracy of over 86%.



### Few-Example Object Detection with Model Communication
- **Arxiv ID**: http://arxiv.org/abs/1706.08249v8
- **DOI**: 10.1109/TPAMI.2018.2844853
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08249v8)
- **Published**: 2017-06-26 07:00:30+00:00
- **Updated**: 2018-10-30 23:51:46+00:00
- **Authors**: Xuanyi Dong, Liang Zheng, Fan Ma, Yi Yang, Deyu Meng
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI) 2018
- **Journal**: None
- **Summary**: In this paper, we study object detection using a large pool of unlabeled images and only a few labeled images per category, named "few-example object detection". The key challenge consists in generating trustworthy training samples as many as possible from the pool. Using few training examples as seeds, our method iterates between model training and high-confidence sample selection. In training, easy samples are generated first and, then the poorly initialized model undergoes improvement. As the model becomes more discriminative, challenging but reliable samples are selected. After that, another round of model improvement takes place. To further improve the precision and recall of the generated training samples, we embed multiple detection models in our framework, which has proven to outperform the single model baseline and the model ensemble method. Experiments on PASCAL VOC'07, MS COCO'14, and ILSVRC'13 indicate that by using as few as three or four samples selected for each category, our method produces very competitive results when compared to the state-of-the-art weakly-supervised approaches using a large number of image-level labels.



### Deep Semantics-Aware Photo Adjustment
- **Arxiv ID**: http://arxiv.org/abs/1706.08260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08260v1)
- **Published**: 2017-06-26 07:35:07+00:00
- **Updated**: 2017-06-26 07:35:07+00:00
- **Authors**: Seonghyeon Nam, Seon Joo Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic photo adjustment is to mimic the photo retouching style of professional photographers and automatically adjust photos to the learned style. There have been many attempts to model the tone and the color adjustment globally with low-level color statistics. Also, spatially varying photo adjustment methods have been studied by exploiting high-level features and semantic label maps. Those methods are semantics-aware since the color mapping is dependent on the high-level semantic context. However, their performance is limited to the pre-computed hand-crafted features and it is hard to reflect user's preference to the adjustment. In this paper, we propose a deep neural network that models the semantics-aware photo adjustment. The proposed network exploits bilinear models that are the multiplicative interaction of the color and the contexual features. As the contextual features we propose the semantic adjustment map, which discovers the inherent photo retouching presets that are applied according to the scene context. The proposed method is trained using a robust loss with a scene parsing task. The experimental results show that the proposed method outperforms the existing method both quantitatively and qualitatively. The proposed method also provides users a way to retouch the photo by their own likings by giving customized adjustment maps.



### Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates
- **Arxiv ID**: http://arxiv.org/abs/1706.08276v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08276v1)
- **Published**: 2017-06-26 08:35:45+00:00
- **Updated**: 2017-06-26 08:35:45+00:00
- **Authors**: Jun Liu, Amir Shahroudy, Dong Xu, Alex C. Kot, Gang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Skeleton-based human action recognition has attracted a lot of research attention during the past few years. Recent works attempted to utilize recurrent neural networks to model the temporal dependencies between the 3D positional configurations of human body joints for better analysis of human activities in the skeletal data. The proposed work extends this idea to spatial domain as well as temporal domain to better analyze the hidden sources of action-related information within the human skeleton sequences in both of these domains simultaneously. Based on the pictorial structure of Kinect's skeletal data, an effective tree-structure based traversal framework is also proposed. In order to deal with the noise in the skeletal data, a new gating mechanism within LSTM module is introduced, with which the network can learn the reliability of the sequential data and accordingly adjust the effect of the input data on the updating procedure of the long-term context representation stored in the unit's memory cell. Moreover, we introduce a novel multi-modal feature fusion strategy within the LSTM unit in this paper. The comprehensive experimental results on seven challenging benchmark datasets for human action recognition demonstrate the effectiveness of the proposed method.



### Multi-Label Learning with Label Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1706.08323v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.08323v4)
- **Published**: 2017-06-26 11:15:04+00:00
- **Updated**: 2019-04-16 09:51:09+00:00
- **Authors**: Ruifeng Shao, Ning Xu, Xin Geng
- **Comment**: ICDM 2018
- **Journal**: None
- **Summary**: The task of multi-label learning is to predict a set of relevant labels for the unseen instance. Traditional multi-label learning algorithms treat each class label as a logical indicator of whether the corresponding label is relevant or irrelevant to the instance, i.e., +1 represents relevant to the instance and -1 represents irrelevant to the instance. Such label represented by -1 or +1 is called logical label. Logical label cannot reflect different label importance. However, for real-world multi-label learning problems, the importance of each possible label is generally different. For the real applications, it is difficult to obtain the label importance information directly. Thus we need a method to reconstruct the essential label importance from the logical multilabel data. To solve this problem, we assume that each multi-label instance is described by a vector of latent real-valued labels, which can reflect the importance of the corresponding labels. Such label is called numerical label. The process of reconstructing the numerical labels from the logical multi-label data via utilizing the logical label information and the topological structure in the feature space is called Label Enhancement. In this paper, we propose a novel multi-label learning framework called LEMLL, i.e., Label Enhanced Multi-Label Learning, which incorporates regression of the numerical labels and label enhancement into a unified framework. Extensive comparative studies validate that the performance of multi-label learning can be improved significantly with label enhancement and LEMLL can effectively reconstruct latent label importance information from logical multi-label data.



### Semantically Informed Multiview Surface Refinement
- **Arxiv ID**: http://arxiv.org/abs/1706.08336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08336v1)
- **Published**: 2017-06-26 12:19:25+00:00
- **Updated**: 2017-06-26 12:19:25+00:00
- **Authors**: Maros Blaha, Mathias Rothermel, Martin R. Oswald, Torsten Sattler, Audrey Richard, Jan D. Wegner, Marc Pollefeys, Konrad Schindler
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method to jointly refine the geometry and semantic segmentation of 3D surface meshes. Our method alternates between updating the shape and the semantic labels. In the geometry refinement step, the mesh is deformed with variational energy minimization, such that it simultaneously maximizes photo-consistency and the compatibility of the semantic segmentations across a set of calibrated images. Label-specific shape priors account for interactions between the geometry and the semantic labels in 3D. In the semantic segmentation step, the labels on the mesh are updated with MRF inference, such that they are compatible with the semantic segmentations in the input images. Also, this step includes prior assumptions about the surface shape of different semantic classes. The priors induce a tight coupling, where semantic information influences the shape update and vice versa. Specifically, we introduce priors that favor (i) adaptive smoothing, depending on the class label; (ii) straightness of class boundaries; and (iii) semantic labels that are consistent with the surface orientation. The novel mesh-based reconstruction is evaluated in a series of experiments with real and synthetic data. We compare both to state-of-the-art, voxel-based semantic 3D reconstruction, and to purely geometric mesh refinement, and demonstrate that the proposed scheme yields improved 3D geometry as well as an improved semantic segmentation.



### Deep Semantic Classification for 3D LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/1706.08355v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.08355v1)
- **Published**: 2017-06-26 13:16:57+00:00
- **Updated**: 2017-06-26 13:16:57+00:00
- **Authors**: Ayush Dewan, Gabriel L. Oliveira, Wolfram Burgard
- **Comment**: 8 pages to be published in IROS 2017
- **Journal**: None
- **Summary**: Robots are expected to operate autonomously in dynamic environments. Understanding the underlying dynamic characteristics of objects is a key enabler for achieving this goal. In this paper, we propose a method for pointwise semantic classification of 3D LiDAR data into three classes: non-movable, movable and dynamic. We concentrate on understanding these specific semantics because they characterize important information required for an autonomous system. Non-movable points in the scene belong to unchanging segments of the environment, whereas the remaining classes corresponds to the changing parts of the scene. The difference between the movable and dynamic class is their motion state. The dynamic points can be perceived as moving, whereas movable objects can move, but are perceived as static. To learn the distinction between movable and non-movable points in the environment, we introduce an approach based on deep neural network and for detecting the dynamic points, we estimate pointwise motion. We propose a Bayes filter framework for combining the learned semantic cues with the motion cues to infer the required semantic classification. In extensive experiments, we compare our approach with other methods on a standard benchmark dataset and report competitive results in comparison to the existing state-of-the-art. Furthermore, we show an improvement in the classification of points by combining the semantic cues retrieved from the neural network with the motion cues.



### Image Processing in Floriculture Using a robotic Mobile Platform
- **Arxiv ID**: http://arxiv.org/abs/1706.08436v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.08436v1)
- **Published**: 2017-06-26 15:21:44+00:00
- **Updated**: 2017-06-26 15:21:44+00:00
- **Authors**: Juan Garcia-Torres, Diana Caro-Prieto
- **Comment**: 4 Pages, Paper made at Fundacion Universitaria Agraria de Colombia
- **Journal**: None
- **Summary**: Colombia has a privileged geographical location which makes it a cornerstone and equidistant point to all regional markets. The country has a great ecological diversity and it is one of the largest suppliers of flowers for US. Colombian flower companies have made innovations in the marketing process, using methods to reach all conditions for final consumers. This article develops a monitoring system for floriculture industries. The system was implemented in a robotic platform. This device has the ability to be programmed in different programming languages. The robot takes the necessary environment information from its camera. The algorithm of the monitoring system was developed with the image processing toolbox on Matlab. The implemented algorithm acquires images through its camera, it performs a preprocessing of the image, noise filter, enhancing of the color and adjusting the dimension in order to increase processing speed. Then, the image is segmented by color and with the binarized version of the image using morphological operations (erosion and dilation), extract relevant features such as centroid, perimeter and area. The data obtained from the image processing helps the robot with the automatic identification of objectives, orientation and move towards them. Also, the results generate a diagnostic quality of each object scanned.



### Learning to Map Vehicles into Bird's Eye View
- **Arxiv ID**: http://arxiv.org/abs/1706.08442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08442v1)
- **Published**: 2017-06-26 15:39:53+00:00
- **Updated**: 2017-06-26 15:39:53+00:00
- **Authors**: Andrea Palazzi, Guido Borghi, Davide Abati, Simone Calderara, Rita Cucchiara
- **Comment**: Accepted to International Conference on Image Analysis and Processing
  (ICIAP) 2017
- **Journal**: None
- **Summary**: Awareness of the road scene is an essential component for both autonomous vehicles and Advances Driver Assistance Systems and is gaining importance both for the academia and car companies. This paper presents a way to learn a semantic-aware transformation which maps detections from a dashboard camera view onto a broader bird's eye occupancy map of the scene. To this end, a huge synthetic dataset featuring 1M couples of frames, taken from both car dashboard and bird's eye view, has been collected and automatically annotated. A deep-network is then trained to warp detections from the first to the second view. We demonstrate the effectiveness of our model against several baselines and observe that is able to generalize on real-world data despite having been trained solely on synthetic ones.



### Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention
- **Arxiv ID**: http://arxiv.org/abs/1706.08474v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08474v4)
- **Published**: 2017-06-26 16:45:57+00:00
- **Updated**: 2018-05-21 09:26:27+00:00
- **Authors**: Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, Rita Cucchiara
- **Comment**: ACM Transactions on Multimedia Computing, Communications and
  Applications, Vol. 14, No. 2, Article 48
- **Journal**: None
- **Summary**: Image captioning has been recently gaining a lot of attention thanks to the impressive achievements shown by deep captioning architectures, which combine Convolutional Neural Networks to extract image representations, and Recurrent Neural Networks to generate the corresponding captions. At the same time, a significant research effort has been dedicated to the development of saliency prediction models, which can predict human eye fixations. Even though saliency information could be useful to condition an image captioning architecture, by providing an indication of what is salient and what is not, research is still struggling to incorporate these two techniques. In this work, we propose an image captioning approach in which a generative recurrent neural network can focus on different parts of the input image during the generation of the caption, by exploiting the conditioning given by a saliency prediction model on which parts of the image are salient and which are contextual. We show, through extensive quantitative and qualitative experiments on large scale datasets, that our model achieves superior performances with respect to captioning baselines with and without saliency, and to different state of the art approaches combining saliency and captioning.



### Deep Network Flow for Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1706.08482v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08482v1)
- **Published**: 2017-06-26 17:08:45+00:00
- **Updated**: 2017-06-26 17:08:45+00:00
- **Authors**: Samuel Schulter, Paul Vernaza, Wongun Choi, Manmohan Chandraker
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: Data association problems are an important component of many computer vision applications, with multi-object tracking being one of the most prominent examples. A typical approach to data association involves finding a graph matching or network flow that minimizes a sum of pairwise association costs, which are often either hand-crafted or learned as linear functions of fixed features. In this work, we demonstrate that it is possible to learn features for network-flow-based data association via backpropagation, by expressing the optimum of a smoothed network flow problem as a differentiable function of the pairwise association costs. We apply this approach to multi-object tracking with a network flow formulation. Our experiments demonstrate that we are able to successfully learn all cost functions for the association problem in an end-to-end fashion, which outperform hand-crafted costs in all settings. The integration and combination of various sources of inputs becomes easy and the cost functions can be learned entirely from data, alleviating tedious hand-designing of costs.



### Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog
- **Arxiv ID**: http://arxiv.org/abs/1706.08502v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.08502v3)
- **Published**: 2017-06-26 17:47:46+00:00
- **Updated**: 2017-08-20 04:41:15+00:00
- **Authors**: Satwik Kottur, José M. F. Moura, Stefan Lee, Dhruv Batra
- **Comment**: 9 pages, 7 figures, 2 tables, accepted at EMNLP 2017 as short paper
- **Journal**: None
- **Summary**: A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision!   In this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of 'negative' results culminating in a 'positive' one -- showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional.   In essence, we find that natural language does not emerge 'naturally', despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.



### Illuminating Pedestrians via Simultaneous Detection & Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1706.08564v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08564v1)
- **Published**: 2017-06-26 19:05:52+00:00
- **Updated**: 2017-06-26 19:05:52+00:00
- **Authors**: Garrick Brazil, Xi Yin, Xiaoming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian detection is a critical problem in computer vision with significant impact on safety in urban autonomous driving. In this work, we explore how semantic segmentation can be used to boost pedestrian detection accuracy while having little to no impact on network efficiency. We propose a segmentation infusion network to enable joint supervision on semantic segmentation and pedestrian detection. When placed properly, the additional supervision helps guide features in shared layers to become more sophisticated and helpful for the downstream pedestrian detector. Using this approach, we find weakly annotated boxes to be sufficient for considerable performance gains. We provide an in-depth analysis to demonstrate how shared layers are shaped by the segmentation supervision. In doing so, we show that the resulting feature maps become more semantically meaningful and robust to shape and occlusion. Overall, our simultaneous detection and segmentation framework achieves a considerable gain over the state-of-the-art on the Caltech pedestrian dataset, competitive performance on KITTI, and executes 2x faster than competitive methods.



### Detecting Small Signs from Large Images
- **Arxiv ID**: http://arxiv.org/abs/1706.08574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08574v1)
- **Published**: 2017-06-26 19:42:33+00:00
- **Updated**: 2017-06-26 19:42:33+00:00
- **Authors**: Zibo Meng, Xiaochuan Fan, Xin Chen, Min Chen, Yan Tong
- **Comment**: 8 pages, 6 figures, accepted by IEEE Conference on Information Reuse
  and Integration (IRI) 2017 as an oral presentation
- **Journal**: None
- **Summary**: In the past decade, Convolutional Neural Networks (CNNs) have been demonstrated successful for object detections. However, the size of network input is limited by the amount of memory available on GPUs. Moreover, performance degrades when detecting small objects. To alleviate the memory usage and improve the performance of detecting small traffic signs, we proposed an approach for detecting small traffic signs from large images under real world conditions. In particular, large images are broken into small patches as input to a Small-Object-Sensitive-CNN (SOS-CNN) modified from a Single Shot Multibox Detector (SSD) framework with a VGG-16 network as the base network to produce patch-level object detection results. Scale invariance is achieved by applying the SOS-CNN on an image pyramid. Then, image-level object detection is obtained by projecting all the patch-level detection results to the image at the original scale. Experimental results on a real-world conditioned traffic sign dataset have demonstrated the effectiveness of the proposed method in terms of detection accuracy and recall, especially for those with small sizes.



### Using Frame Theoretic Convolutional Gridding for Robust Synthetic Aperture Sonar Imaging
- **Arxiv ID**: http://arxiv.org/abs/1706.08575v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.08575v1)
- **Published**: 2017-06-26 19:42:59+00:00
- **Updated**: 2017-06-26 19:42:59+00:00
- **Authors**: John McKay, Anne Gelb, Vishal Monga, Raghu Raj
- **Comment**: Accepted to OCEANS 2017 - Anchorage (Conference)
- **Journal**: None
- **Summary**: Recent progress in synthetic aperture sonar (SAS) technology and processing has led to significant advances in underwater imaging, outperforming previously common approaches in both accuracy and efficiency. There are, however, inherent limitations to current SAS reconstruction methodology. In particular, popular and efficient Fourier domain SAS methods require a 2D interpolation which is often ill conditioned and inaccurate, inevitably reducing robustness with regard to speckle and inaccurate sound-speed estimation. To overcome these issues, we propose using the frame theoretic convolution gridding (FTCG) algorithm to handle the non-uniform Fourier data. FTCG extends upon non-uniform fast Fourier transform (NUFFT) algorithms by casting the NUFFT as an approximation problem given Fourier frame data. The FTCG has been show to yield improved accuracy at little more computational cost. Using simulated data, we outline how the FTCG can be used to enhance current SAS processing.



### Robust Sonar ATR Through Bayesian Pose Corrected Sparse Classification
- **Arxiv ID**: http://arxiv.org/abs/1706.08590v1
- **DOI**: 10.1109/TGRS.2017.2710040
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08590v1)
- **Published**: 2017-06-26 20:53:32+00:00
- **Updated**: 2017-06-26 20:53:32+00:00
- **Authors**: John McKay, Vishal Monga, Raghu G. Raj
- **Comment**: 14 Pages, 16 Figures, Accepted TGARS
- **Journal**: None
- **Summary**: Sonar imaging has seen vast improvements over the last few decades due in part to advances in synthetic aperture Sonar (SAS). Sophisticated classification techniques can now be used in Sonar automatic target recognition (ATR) to locate mines and other threatening objects. Among the most promising of these methods is sparse reconstruction-based classification (SRC) which has shown an impressive resiliency to noise, blur, and occlusion. We present a coherent strategy for expanding upon SRC for Sonar ATR that retains SRC's robustness while also being able to handle targets with diverse geometric arrangements, bothersome Rayleigh noise, and unavoidable background clutter. Our method, pose corrected sparsity (PCS), incorporates a novel interpretation of a spike and slab probability distribution towards use as a Bayesian prior for class-specific discrimination in combination with a dictionary learning scheme for localized patch extractions. Additionally, PCS offers the potential for anomaly detection in order to avoid false identifications of tested objects from outside the training set with no additional training required. Compelling results are shown using a database provided by the United States Naval Surface Warfare Center.



### Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study
- **Arxiv ID**: http://arxiv.org/abs/1706.08606v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.08606v2)
- **Published**: 2017-06-26 21:31:18+00:00
- **Updated**: 2017-06-29 17:52:55+00:00
- **Authors**: Samuel Ritter, David G. T. Barrett, Adam Santoro, Matt M. Botvinick
- **Comment**: ICML 2017
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have achieved unprecedented performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. This has caused a recent surge of interest in methods for rendering modern neural systems more interpretable. In this work, we propose to address the interpretability problem in modern DNNs using the rich history of problem descriptions, theories and experimental methods developed by cognitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.



### Do Deep Neural Networks Suffer from Crowding?
- **Arxiv ID**: http://arxiv.org/abs/1706.08616v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.08616v1)
- **Published**: 2017-06-26 22:25:56+00:00
- **Updated**: 2017-06-26 22:25:56+00:00
- **Authors**: Anna Volokitin, Gemma Roig, Tomaso Poggio
- **Comment**: CBMM memo
- **Journal**: None
- **Summary**: Crowding is a visual effect suffered by humans, in which an object that can be recognized in isolation can no longer be recognized when other objects, called flankers, are placed close to it. In this work, we study the effect of crowding in artificial Deep Neural Networks for object recognition. We analyze both standard deep convolutional neural networks (DCNNs) as well as a new version of DCNNs which is 1) multi-scale and 2) with size of the convolution filters change depending on the eccentricity wrt to the center of fixation. Such networks, that we call eccentricity-dependent, are a computational model of the feedforward path of the primate visual cortex. Our results reveal that the eccentricity-dependent model, trained on target objects in isolation, can recognize such targets in the presence of flankers, if the targets are near the center of the image, whereas DCNNs cannot. Also, for all tested networks, when trained on targets in isolation, we find that recognition accuracy of the networks decreases the closer the flankers are to the target and the more flankers there are. We find that visual similarity between the target and flankers also plays a role and that pooling in early layers of the network leads to more crowding. Additionally, we show that incorporating the flankers into the images of the training set does not improve performance with crowding.



