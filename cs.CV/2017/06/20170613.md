# Arxiv Papers in cs.CV on 2017-06-13
### Contrast Enhancement Estimation for Digital Image Forensics
- **Arxiv ID**: http://arxiv.org/abs/1706.03875v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03875v1)
- **Published**: 2017-06-13 00:13:53+00:00
- **Updated**: 2017-06-13 00:13:53+00:00
- **Authors**: Longyin Wen, Honggang Qi, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: Inconsistency in contrast enhancement can be used to expose image forgeries. In this work, we describe a new method to estimate contrast enhancement from a single image. Our method takes advantage of the nature of contrast enhancement as a mapping between pixel values, and the distinct characteristics it introduces to the image pixel histogram. Our method recovers the original pixel histogram and the contrast enhancement simultaneously from a single image with an iterative algorithm. Unlike previous methods, our method is robust in the presence of additive noise perturbations that are used to hide the traces of contrast enhancement. Furthermore, we also develop an e effective method to to detect image regions undergone contrast enhancement transformations that are different from the rest of the image, and use this method to detect composite images. We perform extensive experimental evaluations to demonstrate the efficacy and efficiency of our method method.



### Deep Control - a simple automatic gain control for memory efficient and high performance training of deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1706.03907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03907v1)
- **Published**: 2017-06-13 05:29:05+00:00
- **Updated**: 2017-06-13 05:29:05+00:00
- **Authors**: Brendan Ruff
- **Comment**: Submitted to BMVC 2017 on 2nd May 2017
- **Journal**: None
- **Summary**: Training a deep convolutional neural net typically starts with a random initialisation of all filters in all layers which severely reduces the forward signal and back-propagated error and leads to slow and sub-optimal training. Techniques that counter that focus on either increasing the signal or increasing the gradients adaptively but the model behaves very differently at the beginning of training compared to later when stable pathways through the net have been established. To compound this problem the effective minibatch size varies greatly between layers at different depths and between individual filters as activation sparsity typically increases with depth leading to a reduction in effective learning rate since gradients may superpose rather than add and this further compounds the covariate shift problem as deeper neurons are less able to adapt to upstream shift.   Proposed here is a method of automatic gain control of the signal built into each convolutional neuron that achieves equivalent or superior performance than batch normalisation and is compatible with single sample or minibatch gradient descent. The same model is used both for training and inference.   The technique comprises a scaled per sample map mean subtraction from the raw convolutional filter output followed by scaling of the difference.



### SEP-Nets: Small and Effective Pattern Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.03912v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.03912v1)
- **Published**: 2017-06-13 06:07:26+00:00
- **Updated**: 2017-06-13 06:07:26+00:00
- **Authors**: Zhe Li, Xiaoyu Wang, Xutao Lv, Tianbao Yang
- **Comment**: None
- **Journal**: None
- **Summary**: While going deeper has been witnessed to improve the performance of convolutional neural networks (CNN), going smaller for CNN has received increasing attention recently due to its attractiveness for mobile/embedded applications. It remains an active and important topic how to design a small network while retaining the performance of large and deep CNNs (e.g., Inception Nets, ResNets). Albeit there are already intensive studies on compressing the size of CNNs, the considerable drop of performance is still a key concern in many designs. This paper addresses this concern with several new contributions. First, we propose a simple yet powerful method for compressing the size of deep CNNs based on parameter binarization. The striking difference from most previous work on parameter binarization/quantization lies at different treatments of $1\times 1$ convolutions and $k\times k$ convolutions ($k>1$), where we only binarize $k\times k$ convolutions into binary patterns. The resulting networks are referred to as pattern networks. By doing this, we show that previous deep CNNs such as GoogLeNet and Inception-type Nets can be compressed dramatically with marginal drop in performance. Second, in light of the different functionalities of $1\times 1$ (data projection/transformation) and $k\times k$ convolutions (pattern extraction), we propose a new block structure codenamed the pattern residual block that adds transformed feature maps generated by $1\times 1$ convolutions to the pattern feature maps generated by $k\times k$ convolutions, based on which we design a small network with $\sim 1$ million parameters. Combining with our parameter binarization, we achieve better performance on ImageNet than using similar sized networks including recently released Google MobileNets.



### Long-Term Video Interpolation with Bidirectional Predictive Network
- **Arxiv ID**: http://arxiv.org/abs/1706.03947v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.03947v1)
- **Published**: 2017-06-13 08:15:32+00:00
- **Updated**: 2017-06-13 08:15:32+00:00
- **Authors**: Xiongtao Chen, Wenmin Wang, Jinzhuo Wang, Weimian Li, Baoyang Chen
- **Comment**: 5 pages, 7 figures
- **Journal**: None
- **Summary**: This paper considers the challenging task of long-term video interpolation. Unlike most existing methods that only generate few intermediate frames between existing adjacent ones, we attempt to speculate or imagine the procedure of an episode and further generate multiple frames between two non-consecutive frames in videos. In this paper, we present a novel deep architecture called bidirectional predictive network (BiPN) that predicts intermediate frames from two opposite directions. The bidirectional architecture allows the model to learn scene transformation with time as well as generate longer video sequences. Besides, our model can be extended to predict multiple possible procedures by sampling different noise vectors. A joint loss composed of clues in image and feature spaces and adversarial loss is designed to train our model. We demonstrate the advantages of BiPN on two benchmarks Moving 2D Shapes and UCF101 and report competitive results to recent approaches.



### Image Forgery Localization Based on Multi-Scale Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.07842v4
- **DOI**: 10.1109/TGRS.2018.2848473
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1706.07842v4)
- **Published**: 2017-06-13 09:40:27+00:00
- **Updated**: 2018-02-07 10:45:45+00:00
- **Authors**: Yaqi Liu, Qingxiao Guan, Xianfeng Zhao, Yun Cao
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper, we propose to utilize Convolutional Neural Networks (CNNs) and the segmentation-based multi-scale analysis to locate tampered areas in digital images. First, to deal with color input sliding windows of different scales, a unified CNN architecture is designed. Then, we elaborately design the training procedures of CNNs on sampled training patches. With a set of robust multi-scale tampering detectors based on CNNs, complementary tampering possibility maps can be generated. Last but not least, a segmentation-based method is proposed to fuse the maps and generate the final decision map. By exploiting the benefits of both the small-scale and large-scale analyses, the segmentation-based multi-scale analysis can lead to a performance leap in forgery localization of CNNs. Numerous experiments are conducted to demonstrate the effectiveness and efficiency of our method.



### Recurrent Inference Machines for Solving Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/1706.04008v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.04008v1)
- **Published**: 2017-06-13 11:24:41+00:00
- **Updated**: 2017-06-13 11:24:41+00:00
- **Authors**: Patrick Putzky, Max Welling
- **Comment**: None
- **Journal**: None
- **Summary**: Much of the recent research on solving iterative inference problems focuses on moving away from hand-chosen inference algorithms and towards learned inference. In the latter, the inference process is unrolled in time and interpreted as a recurrent neural network (RNN) which allows for joint learning of model and inference parameters with back-propagation through time. In this framework, the RNN architecture is directly derived from a hand-chosen inference algorithm, effectively limiting its capabilities. We propose a learning framework, called Recurrent Inference Machines (RIM), in which we turn algorithm construction the other way round: Given data and a task, train an RNN to learn an inference algorithm. Because RNNs are Turing complete [1, 2] they are capable to implement any inference algorithm. The framework allows for an abstraction which removes the need for domain knowledge. We demonstrate in several image restoration experiments that this abstraction is effective, allowing us to achieve state-of-the-art performance on image denoising and super-resolution tasks and superior across-task generalization.



### Probabilistic RGB-D Odometry based on Points, Lines and Planes Under Depth Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1706.04034v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1706.04034v3)
- **Published**: 2017-06-13 13:03:05+00:00
- **Updated**: 2018-01-18 17:07:59+00:00
- **Authors**: Pedro F. Proenca, Yang Gao
- **Comment**: Major update: more results, depth filter released as opensource, 34
  pages
- **Journal**: None
- **Summary**: This work proposes a robust visual odometry method for structured environments that combines point features with line and plane segments, extracted through an RGB-D camera. Noisy depth maps are processed by a probabilistic depth fusion framework based on Mixtures of Gaussians to denoise and derive the depth uncertainty, which is then propagated throughout the visual odometry pipeline. Probabilistic 3D plane and line fitting solutions are used to model the uncertainties of the feature parameters and pose is estimated by combining the three types of primitives based on their uncertainties. Performance evaluation on RGB-D sequences collected in this work and two public RGB-D datasets: TUM and ICL-NUIM show the benefit of using the proposed depth fusion framework and combining the three feature-types, particularly in scenes with low-textured surfaces, dynamic objects and missing depth measurements.



### Indirect Image Registration with Large Diffeomorphic Deformations
- **Arxiv ID**: http://arxiv.org/abs/1706.04048v3
- **DOI**: 10.1137/17M1134627
- **Categories**: **math.NA**, cs.CV, math.DS, math.FA, math.OC, 65F22, 65R32, 65R30, 65D18, 94A12, 94A08, 92C55, 54C56, 57N25, 47A52
- **Links**: [PDF](http://arxiv.org/pdf/1706.04048v3)
- **Published**: 2017-06-13 13:24:13+00:00
- **Updated**: 2017-10-11 07:48:53+00:00
- **Authors**: Chong Chen, Ozan Öktem
- **Comment**: 43 pages, 4 figures, 1 table; revised
- **Journal**: SIAM Journal on Imaging Sciences 2018
- **Summary**: The paper adapts the large deformation diffeomorphic metric mapping framework for image registration to the indirect setting where a template is registered against a target that is given through indirect noisy observations. The registration uses diffeomorphisms that transform the template through a (group) action. These diffeomorphisms are generated by solving a flow equation that is defined by a velocity field with certain regularity. The theoretical analysis includes a proof that indirect image registration has solutions (existence) that are stable and that converge as the data error tends so zero, so it becomes a well-defined regularization method. The paper concludes with examples of indirect image registration in 2D tomography with very sparse and/or highly noisy data.



### Joint Max Margin and Semantic Features for Continuous Event Detection in Complex Scenes
- **Arxiv ID**: http://arxiv.org/abs/1706.04122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04122v1)
- **Published**: 2017-06-13 15:30:16+00:00
- **Updated**: 2017-06-13 15:30:16+00:00
- **Authors**: Iman Abbasnejad, Sridha Sridharan, Simon Denman, Clinton Fookes, Simon Lucey
- **Comment**: submit to journal of Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: In this paper the problem of complex event detection in the continuous domain (i.e. events with unknown starting and ending locations) is addressed. Existing event detection methods are limited to features that are extracted from the local spatial or spatio-temporal patches from the videos. However, this makes the model vulnerable to the events with similar concepts e.g. "Open drawer" and "Open cupboard". In this work, in order to address the aforementioned limitations we present a novel model based on the combination of semantic and temporal features extracted from video frames. We train a max-margin classifier on top of the extracted features in an adaptive framework that is able to detect the events with unknown starting and ending locations. Our model is based on the Bidirectional Region Neural Network and large margin Structural Output SVM. The generality of our model allows it to be simply applied to different labeled and unlabeled datasets. We finally test our algorithm on three challenging datasets, "UCF 101-Action Recognition", "MPII Cooking Activities" and "Hollywood", and we report state-of-the-art performance.



### Video Imagination from a Single Image with Transformation Generation
- **Arxiv ID**: http://arxiv.org/abs/1706.04124v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04124v2)
- **Published**: 2017-06-13 15:31:10+00:00
- **Updated**: 2017-06-15 07:51:22+00:00
- **Authors**: Baoyang Chen, Wenmin Wang, Jinzhuo Wang, Xiongtao Chen
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: In this work, we focus on a challenging task: synthesizing multiple imaginary videos given a single image. Major problems come from high dimensionality of pixel space and the ambiguity of potential motions. To overcome those problems, we propose a new framework that produce imaginary videos by transformation generation. The generated transformations are applied to the original image in a novel volumetric merge network to reconstruct frames in imaginary video. Through sampling different latent variables, our method can output different imaginary video samples. The framework is trained in an adversarial way with unsupervised learning. For evaluation, we propose a new assessment metric $RIQA$. In experiments, we test on 3 datasets varying from synthetic data to natural scene. Our framework achieves promising performance in image quality assessment. The visual inspection indicates that it can successfully generate diverse five-frame videos in acceptable perceptual quality.



### Contour and Centreline Tracking of Vessels from Angiograms using the Classical Image Processing Techniques
- **Arxiv ID**: http://arxiv.org/abs/1707.03710v1
- **DOI**: None
- **Categories**: **cs.CV**, 68Uxx
- **Links**: [PDF](http://arxiv.org/pdf/1707.03710v1)
- **Published**: 2017-06-13 16:39:58+00:00
- **Updated**: 2017-06-13 16:39:58+00:00
- **Authors**: Tache Irina Andra
- **Comment**: 12 pages, 1 figures, 2 tables with figures, journal
- **Journal**: Buletinul Institutului Politehnic din Iasi, Sectia Automatica si
  Calculatoare, Vol. 62(66), No. 3, pp. 49-60, July 2016, ISSN 1220-2169
- **Summary**: This article deals with the problem of vessel edge and centerline detection using classical image processing techniques due to their simpleness and easiness to be implemented. The method is divided into four steps: the vessel enhancement which implies a non-linear filtering proposed by Frangi, the thresholding using Otsu method and the contour detection using the Canny edge detector due to its good performances for the small vessels and the morphological skeletonisation. The algorithms are tested on real data collected from a cardiac catheterism laboratory and it is accurate for images with good spatial resolution (512*512). The output image can be used for further processing in order to find the vessel length or its radius.



### Identifying Spatial Relations in Images using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.04215v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1706.04215v1)
- **Published**: 2017-06-13 18:24:11+00:00
- **Updated**: 2017-06-13 18:24:11+00:00
- **Authors**: Mandar Haldekar, Ashwinkumar Ganesan, Tim Oates
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional approaches to building a large scale knowledge graph have usually relied on extracting information (entities, their properties, and relations between them) from unstructured text (e.g. Dbpedia). Recent advances in Convolutional Neural Networks (CNN) allow us to shift our focus to learning entities and relations from images, as they build robust models that require little or no pre-processing of the images. In this paper, we present an approach to identify and extract spatial relations (e.g., The girl is standing behind the table) from images using CNNs. Our research addresses two specific challenges: providing insight into how spatial relations are learned by the network and which parts of the image are used to predict these relations. We use the pre-trained network VGGNet to extract features from an image and train a Multi-layer Perceptron (MLP) on a set of synthetic images and the sun09 dataset to extract spatial relations. The MLP predicts spatial relations without a bounding box around the objects or the space in the image depicting the relation. To understand how the spatial relations are represented in the network, a heatmap is overlayed on the image to show the regions that are deemed important by the network. Also, we analyze the MLP to show the relationship between the activation of consistent groups of nodes and the prediction of a spatial relation. We show how the loss of these groups affects the networks ability to identify relations.



### Automatic Localization of Deep Stimulation Electrodes Using Trajectory-based Segmentation Approach
- **Arxiv ID**: http://arxiv.org/abs/1706.04254v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1706.04254v1)
- **Published**: 2017-06-13 21:06:35+00:00
- **Updated**: 2017-06-13 21:06:35+00:00
- **Authors**: Roger Gomez Nieto, Andres Marino Alvarez Meza, Julian David Echeverry Correa, Alvaro Angel Orozco Gutierrez
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: Parkinson's disease (PD) is a degenerative condition of the nervous system, which manifests itself primarily as muscle stiffness, hypokinesia, bradykinesia, and tremor. In patients suffering from advanced stages of PD, Deep Brain Stimulation neurosurgery (DBS) is the best alternative to medical treatment, especially when they become tolerant to the drugs. This surgery produces a neuronal activity, a result from electrical stimulation, whose quantification is known as Volume of Tissue Activated (VTA). To locate correctly the VTA in the cerebral volume space, one should be aware exactly the location of the tip of the DBS electrodes, as well as their spatial projection.   In this paper, we automatically locate DBS electrodes using a threshold-based medical imaging segmentation methodology, determining the optimal value of this threshold adaptively. The proposed methodology allows the localization of DBS electrodes in Computed Tomography (CT) images, with high noise tolerance, using automatic threshold detection methods.



### Online Convolutional Dictionary Learning for Multimodal Imaging
- **Arxiv ID**: http://arxiv.org/abs/1706.04256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04256v1)
- **Published**: 2017-06-13 21:08:33+00:00
- **Updated**: 2017-06-13 21:08:33+00:00
- **Authors**: Kevin Degraux, Ulugbek S. Kamilov, Petros T. Boufounos, Dehong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Computational imaging methods that can exploit multiple modalities have the potential to enhance the capabilities of traditional sensing systems. In this paper, we propose a new method that reconstructs multimodal images from their linear measurements by exploiting redundancies across different modalities. Our method combines a convolutional group-sparse representation of images with total variation (TV) regularization for high-quality multimodal imaging. We develop an online algorithm that enables the unsupervised learning of convolutional dictionaries on large-scale datasets that are typical in such applications. We illustrate the benefit of our approach in the context of joint intensity-depth imaging.



### The "something something" video database for learning and evaluating visual common sense
- **Arxiv ID**: http://arxiv.org/abs/1706.04261v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04261v2)
- **Published**: 2017-06-13 21:26:19+00:00
- **Updated**: 2017-06-15 21:15:13+00:00
- **Authors**: Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzyńska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, Roland Memisevic
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks trained on datasets such as ImageNet have led to major advances in visual object classification. One obstacle that prevents networks from reasoning more deeply about complex scenes and situations, and from integrating visual knowledge with natural language, like humans do, is their lack of common sense knowledge about the physical world. Videos, unlike still images, contain a wealth of detailed information about the physical world. However, most labelled video datasets represent high-level concepts rather than detailed physical aspects about actions and scenes. In this work, we describe our ongoing collection of the "something-something" database of video prediction tasks whose solutions require a common sense understanding of the depicted situation. The database currently contains more than 100,000 videos across 174 classes, which are defined as caption-templates. We also describe the challenges in crowd-sourcing this data at scale.



### von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification
- **Arxiv ID**: http://arxiv.org/abs/1706.04264v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04264v2)
- **Published**: 2017-06-13 21:43:05+00:00
- **Updated**: 2017-12-31 22:24:58+00:00
- **Authors**: Md. Abul Hasnat, Julien Bohné, Jonathan Milgram, Stéphane Gentric, Liming Chen
- **Comment**: Under review
- **Journal**: None
- **Summary**: A number of pattern recognition tasks, \textit{e.g.}, face verification, can be boiled down to classification or clustering of unit length directional feature vectors whose distance can be simply computed by their angle. In this paper, we propose the von Mises-Fisher (vMF) mixture model as the theoretical foundation for an effective deep-learning of such directional features and derive a novel vMF Mixture Loss and its corresponding vMF deep features. The proposed vMF feature learning achieves the characteristics of discriminative learning, \textit{i.e.}, compacting the instances of the same class while increasing the distance of instances from different classes. Moreover, it subsumes a number of popular loss functions as well as an effective method in deep learning, namely normalization. We conduct extensive experiments on face verification using 4 different challenging face datasets, \textit{i.e.}, LFW, YouTube faces, CACD and IJB-A. Results show the effectiveness and excellent generalization ability of the proposed approach as it achieves state-of-the-art results on the LFW, YouTube faces and CACD datasets and competitive results on the IJB-A dataset.



### Action Search: Spotting Actions in Videos and Its Application to Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1706.04269v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04269v2)
- **Published**: 2017-06-13 22:15:09+00:00
- **Updated**: 2018-07-27 16:27:25+00:00
- **Authors**: Humam Alwassel, Fabian Caba Heilbron, Bernard Ghanem
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: State-of-the-art temporal action detectors inefficiently search the entire video for specific actions. Despite the encouraging progress these methods achieve, it is crucial to design automated approaches that only explore parts of the video which are the most relevant to the actions being searched for. To address this need, we propose the new problem of action spotting in video, which we define as finding a specific action in a video while observing a small portion of that video. Inspired by the observation that humans are extremely efficient and accurate in spotting and finding action instances in video, we propose Action Search, a novel Recurrent Neural Network approach that mimics the way humans spot actions. Moreover, to address the absence of data recording the behavior of human annotators, we put forward the Human Searches dataset, which compiles the search sequences employed by human annotators spotting actions in the AVA and THUMOS14 datasets. We consider temporal action localization as an application of the action spotting problem. Experiments on the THUMOS14 dataset reveal that our model is not only able to explore the video efficiently (observing on average 17.3% of the video) but it also accurately finds human activities with 30.8% mAP.



### AFIF4: Deep Gender Classification based on AdaBoost-based Fusion of Isolated Facial Features and Foggy Faces
- **Arxiv ID**: http://arxiv.org/abs/1706.04277v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.04277v5)
- **Published**: 2017-06-13 23:15:14+00:00
- **Updated**: 2017-11-18 02:26:50+00:00
- **Authors**: Mahmoud Afifi, Abdelrahman Abdelhamed
- **Comment**: 26 pages, 7 figures, 7 tables
- **Journal**: None
- **Summary**: Gender classification aims at recognizing a person's gender. Despite the high accuracy achieved by state-of-the-art methods for this task, there is still room for improvement in generalized and unrestricted datasets. In this paper, we advocate a new strategy inspired by the behavior of humans in gender recognition. Instead of dealing with the face image as a sole feature, we rely on the combination of isolated facial features and a holistic feature which we call the foggy face. Then, we use these features to train deep convolutional neural networks followed by an AdaBoost-based score fusion to infer the final gender class. We evaluate our method on four challenging datasets to demonstrate its efficacy in achieving better or on-par accuracy with state-of-the-art methods. In addition, we present a new face dataset that intensifies the challenges of occluded faces and illumination changes, which we believe to be a much-needed resource for gender classification research.



