# Arxiv Papers in cs.CV on 2017-11-16
### Occlusion Aware Unsupervised Learning of Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1711.05890v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05890v2)
- **Published**: 2017-11-16 02:02:42+00:00
- **Updated**: 2018-04-04 00:29:12+00:00
- **Authors**: Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, Peng Wang, Wei Xu
- **Comment**: CVPR 2018 Camera-ready
- **Journal**: None
- **Summary**: It has been recently shown that a convolutional neural network can learn optical flow estimation with unsupervised learning. However, the performance of the unsupervised methods still has a relatively large gap compared to its supervised counterpart. Occlusion and large motion are some of the major factors that limit the current unsupervised learning of optical flow methods. In this work we introduce a new method which models occlusion explicitly and a new warping way that facilitates the learning of large motion. Our method shows promising results on Flying Chairs, MPI-Sintel and KITTI benchmark datasets. Especially on KITTI dataset where abundant unlabeled samples exist, our unsupervised method outperforms its counterpart trained with supervised learning.



### NISP: Pruning Networks using Neuron Importance Score Propagation
- **Arxiv ID**: http://arxiv.org/abs/1711.05908v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05908v3)
- **Published**: 2017-11-16 03:49:02+00:00
- **Updated**: 2018-03-21 19:26:43+00:00
- **Authors**: Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I. Morariu, Xintong Han, Mingfei Gao, Ching-Yung Lin, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: To reduce the significant redundancy in deep Convolutional Neural Networks (CNNs), most existing methods prune neurons by only considering statistics of an individual layer or two consecutive layers (e.g., prune one layer to minimize the reconstruction error of the next layer), ignoring the effect of error propagation in deep networks. In contrast, we argue that it is essential to prune neurons in the entire neuron network jointly based on a unified goal: minimizing the reconstruction error of important responses in the "final response layer" (FRL), which is the second-to-last layer before classification, for a pruned network to retrain its predictive power. Specifically, we apply feature ranking techniques to measure the importance of each neuron in the FRL, and formulate network pruning as a binary integer optimization problem and derive a closed-form solution to it for pruning neurons in earlier layers. Based on our theoretical analysis, we propose the Neuron Importance Score Propagation (NISP) algorithm to propagate the importance scores of final responses to every neuron in the network. The CNN is pruned by removing neurons with least importance, and then fine-tuned to retain its predictive power. NISP is evaluated on several datasets with multiple CNN models and demonstrated to achieve significant acceleration and compression with negligible accuracy loss.



### Priming Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.05918v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.05918v2)
- **Published**: 2017-11-16 04:21:14+00:00
- **Updated**: 2017-11-17 01:50:00+00:00
- **Authors**: Amir Rosenfeld, Mahdi Biparva, John K. Tsotsos
- **Comment**: fixed error in author name
- **Journal**: None
- **Summary**: Visual priming is known to affect the human visual system to allow detection of scene elements, even those that may have been near unnoticeable before, such as the presence of camouflaged animals. This process has been shown to be an effect of top-down signaling in the visual system triggered by the said cue. In this paper, we propose a mechanism to mimic the process of priming in the context of object detection and segmentation. We view priming as having a modulatory, cue dependent effect on layers of features within a network. Our results show how such a process can be complementary to, and at times more effective than simple post-processing applied to the output of the network, notably so in cases where the object is hard to detect such as in severe noise. Moreover, we find the effects of priming are sometimes stronger when early visual layers are affected. Overall, our experiments confirm that top-down signals can go a long way in improving object detection and segmentation.



### Learning Deeply Supervised Good Features to Match for Dense Monocular Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1711.05919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05919v2)
- **Published**: 2017-11-16 04:22:49+00:00
- **Updated**: 2018-11-20 06:12:22+00:00
- **Authors**: Chamara Saroj Weerasekera, Ravi Garg, Yasir Latif, Ian Reid
- **Comment**: ACCV 2018
- **Journal**: None
- **Summary**: Visual SLAM (Simultaneous Localization and Mapping) methods typically rely on handcrafted visual features or raw RGB values for establishing correspondences between images. These features, while suitable for sparse mapping, often lead to ambiguous matches in texture-less regions when performing dense reconstruction due to the aperture problem. In this work, we explore the use of learned features for the matching task in dense monocular reconstruction. We propose a novel convolutional neural network (CNN) architecture along with a deeply supervised feature learning scheme for pixel-wise regression of visual descriptors from an image which are best suited for dense monocular SLAM. In particular, our learning scheme minimizes a multi-view matching cost-volume loss with respect to the regressed features at multiple stages within the network, for explicitly learning contextual features that are suitable for dense matching between images captured by a moving monocular camera along the epipolar line. We integrate the learned features from our model for depth estimation inside a real-time dense monocular SLAM framework, where photometric error is replaced by our learned descriptor error. Our extensive evaluation on several challenging indoor datasets demonstrate greatly improved accuracy in dense reconstructions of the well celebrated dense SLAM systems like DTAM, without compromising their real-time performance.



### Defense against Universal Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1711.05929v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05929v3)
- **Published**: 2017-11-16 05:08:49+00:00
- **Updated**: 2018-02-28 05:45:34+00:00
- **Authors**: Naveed Akhtar, Jian Liu, Ajmal Mian
- **Comment**: Accepted in IEEE CVPR 2018
- **Journal**: None
- **Summary**: Recent advances in Deep Learning show the existence of image-agnostic quasi-imperceptible perturbations that when applied to `any' image can fool a state-of-the-art network classifier to change its prediction about the image label. These `Universal Adversarial Perturbations' pose a serious threat to the success of Deep Learning in practice. We present the first dedicated framework to effectively defend the networks against such perturbations. Our approach learns a Perturbation Rectifying Network (PRN) as `pre-input' layers to a targeted model, such that the targeted model needs no modification. The PRN is learned from real and synthetic image-agnostic perturbations, where an efficient method to compute the latter is also proposed. A perturbation detector is separately trained on the Discrete Cosine Transform of the input-output difference of the PRN. A query image is first passed through the PRN and verified by the detector. If a perturbation is detected, the output of the PRN is used for label prediction instead of the actual image. A rigorous evaluation shows that our framework can defend the network classifiers against unseen adversarial perturbations in the real-world scenarios with up to 97.5% success rate. The PRN also generalizes well in the sense that training for one targeted network defends another network with a comparable success rate.



### Enhanced Attacks on Defensively Distilled Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.05934v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.05934v1)
- **Published**: 2017-11-16 05:37:14+00:00
- **Updated**: 2017-11-16 05:37:14+00:00
- **Authors**: Yujia Liu, Weiming Zhang, Shaohua Li, Nenghai Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have achieved tremendous success in many tasks of machine learning, such as the image classification. Unfortunately, researchers have shown that DNNs are easily attacked by adversarial examples, slightly perturbed images which can mislead DNNs to give incorrect classification results. Such attack has seriously hampered the deployment of DNN systems in areas where security or safety requirements are strict, such as autonomous cars, face recognition, malware detection. Defensive distillation is a mechanism aimed at training a robust DNN which significantly reduces the effectiveness of adversarial examples generation. However, the state-of-the-art attack can be successful on distilled networks with 100% probability. But it is a white-box attack which needs to know the inner information of DNN. Whereas, the black-box scenario is more general. In this paper, we first propose the epsilon-neighborhood attack, which can fool the defensively distilled networks with 100% success rate in the white-box setting, and it is fast to generate adversarial examples with good visual quality. On the basis of this attack, we further propose the region-based attack against defensively distilled DNNs in the black-box setting. And we also perform the bypass attack to indirectly break the distillation defense as a complementary method. The experimental results show that our black-box attacks have a considerable success rate on defensively distilled networks.



### Skepxels: Spatio-temporal Image Representation of Human Skeleton Joints for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.05941v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05941v4)
- **Published**: 2017-11-16 06:03:52+00:00
- **Updated**: 2018-08-03 06:57:39+00:00
- **Authors**: Jian Liu, Naveed Akhtar, Ajmal Mian
- **Comment**: Submitted to a journal
- **Journal**: None
- **Summary**: Human skeleton joints are popular for action analysis since they can be easily extracted from videos to discard background noises. However, current skeleton representations do not fully benefit from machine learning with CNNs. We propose "Skepxels" a spatio-temporal representation for skeleton sequences to fully exploit the "local" correlations between joints using the 2D convolution kernels of CNN. We transform skeleton videos into images of flexible dimensions using Skepxels and develop a CNN-based framework for effective human action recognition using the resulting images. Skepxels encode rich spatio-temporal information about the skeleton joints in the frames by maximizing a unique distance metric, defined collaboratively over the distinct joint arrangements used in the skeletal image. Moreover, they are flexible in encoding compound semantic notions such as location and speed of the joints. The proposed action recognition exploits the representation in a hierarchical manner by first capturing the micro-temporal relations between the skeleton joints with the Skepxels and then exploiting their macro-temporal relations by computing the Fourier Temporal Pyramids over the CNN features of the skeletal images. We extend the Inception-ResNet CNN architecture with the proposed method and improve the state-of-the-art accuracy by 4.4% on the large scale NTU human activity dataset. On the medium-sized N-UCLA and UTH-MHAD datasets, our method outperforms the existing results by 5.7% and 9.3% respectively.



### Learning from Millions of 3D Scans for Large-scale 3D Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.05942v3
- **DOI**: 10.1109/CVPR.2018.00203
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05942v3)
- **Published**: 2017-11-16 06:07:10+00:00
- **Updated**: 2018-07-05 07:24:59+00:00
- **Authors**: Syed Zulqarnain Gilani, Ajmal Mian
- **Comment**: 11 pages
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition, 2018
- **Summary**: Deep networks trained on millions of facial images are believed to be closely approaching human-level performance in face recognition. However, open world face recognition still remains a challenge. Although, 3D face recognition has an inherent edge over its 2D counterpart, it has not benefited from the recent developments in deep learning due to the unavailability of large training as well as large test datasets. Recognition accuracies have already saturated on existing 3D face datasets due to their small gallery sizes. Unlike 2D photographs, 3D facial scans cannot be sourced from the web causing a bottleneck in the development of deep 3D face recognition networks and datasets. In this backdrop, we propose a method for generating a large corpus of labeled 3D face identities and their multiple instances for training and a protocol for merging the most challenging existing 3D datasets for testing. We also propose the first deep CNN model designed specifically for 3D face recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our test dataset comprises 1,853 identities with a single 3D scan in the gallery and another 31K scans as probes, which is several orders of magnitude larger than existing ones. Without fine tuning on this dataset, our network already outperforms state of the art face recognition by over 10%. We fine tune our network on the gallery set to perform end-to-end large scale 3D face recognition which further improves accuracy. Finally, we show the efficacy of our method for the open world face recognition problem.



### HandSeg: An Automatically Labeled Dataset for Hand Segmentation from Depth Images
- **Arxiv ID**: http://arxiv.org/abs/1711.05944v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05944v4)
- **Published**: 2017-11-16 06:14:11+00:00
- **Updated**: 2018-08-02 20:17:08+00:00
- **Authors**: Abhishake Kumar Bojja, Franziska Mueller, Sri Raghu Malireddi, Markus Oberweger, Vincent Lepetit, Christian Theobalt, Kwang Moo Yi, Andrea Tagliasacchi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an automatic method for generating high-quality annotations for depth-based hand segmentation, and introduce a large-scale hand segmentation dataset. Existing datasets are typically limited to a single hand. By exploiting the visual cues given by an RGBD sensor and a pair of colored gloves, we automatically generate dense annotations for two hand segmentation. This lowers the cost/complexity of creating high quality datasets, and makes it easy to expand the dataset in the future. We further show that existing datasets, even with data augmentation, are not sufficient to train a hand segmentation algorithm that can distinguish two hands. Source and datasets will be made publicly available.



### 3D Face Reconstruction from Light Field Images: A Model-free Approach
- **Arxiv ID**: http://arxiv.org/abs/1711.05953v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05953v4)
- **Published**: 2017-11-16 06:50:19+00:00
- **Updated**: 2018-07-05 08:29:30+00:00
- **Authors**: Mingtao Feng, Syed Zulqarnain Gilani, Yaonan Wang, Ajmal Mian
- **Comment**: None
- **Journal**: European Conference on Computer Vision (ECCV), 2018
- **Summary**: Reconstructing 3D facial geometry from a single RGB image has recently instigated wide research interest. However, it is still an ill-posed problem and most methods rely on prior models hence undermining the accuracy of the recovered 3D faces. In this paper, we exploit the Epipolar Plane Images (EPI) obtained from light field cameras and learn CNN models that recover horizontal and vertical 3D facial curves from the respective horizontal and vertical EPIs. Our 3D face reconstruction network (FaceLFnet) comprises a densely connected architecture to learn accurate 3D facial curves from low resolution EPIs. To train the proposed FaceLFnets from scratch, we synthesize photo-realistic light field images from 3D facial scans. The curve by curve 3D face estimation approach allows the networks to learn from only 14K images of 80 identities, which still comprises over 11 Million EPIs/curves. The estimated facial curves are merged into a single pointcloud to which a surface is fitted to get the final 3D face. Our method is model-free, requires only a few training samples to learn FaceLFnet and can reconstruct 3D faces with high accuracy from single light field images under varying poses, expressions and lighting conditions. Comparison on the BU-3DFE and BU-4DFE datasets show that our method reduces reconstruction errors by over 20% compared to recent state of the art.



### Zero-Annotation Object Detection with Web Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/1711.05954v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05954v2)
- **Published**: 2017-11-16 06:51:40+00:00
- **Updated**: 2018-08-01 07:44:52+00:00
- **Authors**: Qingyi Tao, Hao Yang, Jianfei Cai
- **Comment**: Accepted in ECCV 2018
- **Journal**: None
- **Summary**: Object detection is one of the major problems in computer vision, and has been extensively studied. Most of the existing detection works rely on labor-intensive supervision, such as ground truth bounding boxes of objects or at least image-level annotations. On the contrary, we propose an object detection method that does not require any form of human annotation on target tasks, by exploiting freely available web images. In order to facilitate effective knowledge transfer from web images, we introduce a multi-instance multi-label domain adaption learning framework with two key innovations. First of all, we propose an instance-level adversarial domain adaptation network with attention on foreground objects to transfer the object appearances from web domain to target domain. Second, to preserve the class-specific semantic structure of transferred object features, we propose a simultaneous transfer mechanism to transfer the supervision across domains through pseudo strong label generation. With our end-to-end framework that simultaneously learns a weakly supervised detector and transfers knowledge across domains, we achieved significant improvements over baseline methods on the benchmark datasets.



### Less-forgetful Learning for Domain Expansion in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.05959v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.05959v1)
- **Published**: 2017-11-16 07:04:51+00:00
- **Updated**: 2017-11-16 07:04:51+00:00
- **Authors**: Heechul Jung, Jeongwoo Ju, Minju Jung, Junmo Kim
- **Comment**: 8 pages, accepted to AAAI 2018
- **Journal**: None
- **Summary**: Expanding the domain that deep neural network has already learned without accessing old domain data is a challenging task because deep neural networks forget previously learned information when learning new data from a new domain. In this paper, we propose a less-forgetful learning method for the domain expansion scenario. While existing domain adaptation techniques solely focused on adapting to new domains, the proposed technique focuses on working well with both old and new domains without needing to know whether the input is from the old or new domain. First, we present two naive approaches which will be problematic, then we provide a new method using two proposed properties for less-forgetful learning. Finally, we prove the effectiveness of our method through experiments on image classification tasks. All datasets used in the paper, will be released on our website for someone's follow-up study.



### Learning to Find Good Correspondences
- **Arxiv ID**: http://arxiv.org/abs/1711.05971v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05971v2)
- **Published**: 2017-11-16 07:54:57+00:00
- **Updated**: 2018-05-21 14:26:17+00:00
- **Authors**: Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit, Mathieu Salzmann, Pascal Fua
- **Comment**: CVPR 2018 (Oral)
- **Journal**: None
- **Summary**: We develop a deep architecture to learn to find good correspondences for wide-baseline stereo. Given a set of putative sparse matches and the camera intrinsics, we train our network in an end-to-end fashion to label the correspondences as inliers or outliers, while simultaneously using them to recover the relative pose, as encoded by the essential matrix. Our architecture is based on a multi-layer perceptron operating on pixel coordinates rather than directly on the image, and is thus simple and small. We introduce a novel normalization technique, called Context Normalization, which allows us to process each data point separately while imbuing it with global information, and also makes the network invariant to the order of the correspondences. Our experiments on multiple challenging datasets demonstrate that our method is able to drastically improve the state of the art with little training data.



### Efficient Diverse Ensemble for Discriminative Co-Tracking
- **Arxiv ID**: http://arxiv.org/abs/1711.06564v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06564v2)
- **Published**: 2017-11-16 08:34:42+00:00
- **Updated**: 2018-06-07 05:46:02+00:00
- **Authors**: Kourosh Meshgi, Shigeyuki Oba, Shin Ishii
- **Comment**: CVPR 2018 Submission
- **Journal**: None
- **Summary**: Ensemble discriminative tracking utilizes a committee of classifiers, to label data samples, which are in turn, used for retraining the tracker to localize the target using the collective knowledge of the committee. Committee members could vary in their features, memory update schemes, or training data, however, it is inevitable to have committee members that excessively agree because of large overlaps in their version space. To remove this redundancy and have an effective ensemble learning, it is critical for the committee to include consistent hypotheses that differ from one-another, covering the version space with minimum overlaps. In this study, we propose an online ensemble tracker that directly generates a diverse committee by generating an efficient set of artificial training. The artificial data is sampled from the empirical distribution of the samples taken from both target and background, whereas the process is governed by query-by-committee to shrink the overlap between classifiers. The experimental results demonstrate that the proposed scheme outperforms conventional ensemble trackers on public benchmarks.



### Minimizing Supervision for Free-space Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.05998v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05998v3)
- **Published**: 2017-11-16 09:24:15+00:00
- **Updated**: 2018-12-08 20:17:34+00:00
- **Authors**: Satoshi Tsutsui, Tommi Kerola, Shunta Saito, David J. Crandall
- **Comment**: Link to source code added; Typo fixed from the version published in
  CVPR 2018 Workshop on Autonomous Driving (WAD)
- **Journal**: None
- **Summary**: Identifying "free-space," or safely driveable regions in the scene ahead, is a fundamental task for autonomous navigation. While this task can be addressed using semantic segmentation, the manual labor involved in creating pixelwise annotations to train the segmentation model is very costly. Although weakly supervised segmentation addresses this issue, most methods are not designed for free-space. In this paper, we observe that homogeneous texture and location are two key characteristics of free-space, and develop a novel, practical framework for free-space segmentation with minimal human supervision. Our experiments show that our framework performs better than other weakly supervised methods while using less supervision. Our work demonstrates the potential for performing free-space segmentation without tedious and costly manual annotation, which will be important for adapting autonomous driving systems to different types of vehicles and environments



### DIMAL: Deep Isometric Manifold Learning Using Sparse Geodesic Sampling
- **Arxiv ID**: http://arxiv.org/abs/1711.06011v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06011v2)
- **Published**: 2017-11-16 10:28:43+00:00
- **Updated**: 2018-11-13 15:52:33+00:00
- **Authors**: Gautam Pai, Ronen Talmon, Alex Bronstein, Ron Kimmel
- **Comment**: 10 pages, 11 Figures
- **Journal**: None
- **Summary**: This paper explores a fully unsupervised deep learning approach for computing distance-preserving maps that generate low-dimensional embeddings for a certain class of manifolds. We use the Siamese configuration to train a neural network to solve the problem of least squares multidimensional scaling for generating maps that approximately preserve geodesic distances. By training with only a few landmarks, we show a significantly improved local and nonlocal generalization of the isometric mapping as compared to analogous non-parametric counterparts. Importantly, the combination of a deep-learning framework with a multidimensional scaling objective enables a numerical analysis of network architectures to aid in understanding their representation power. This provides a geometric perspective to the generalizability of deep learning.



### A Revisit on Deep Hashings for Large-scale Content Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1711.06016v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06016v1)
- **Published**: 2017-11-16 10:45:39+00:00
- **Updated**: 2017-11-16 10:45:39+00:00
- **Authors**: Deng Cai, Xiuye Gu, Chaoqi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: There is a growing trend in studying deep hashing methods for content-based image retrieval (CBIR), where hash functions and binary codes are learnt using deep convolutional neural networks and then the binary codes can be used to do approximate nearest neighbor (ANN) search. All the existing deep hashing papers report their methods' superior performance over the traditional hashing methods according to their experimental results. However, there are serious flaws in the evaluations of existing deep hashing papers: (1) The datasets they used are too small and simple to simulate the real CBIR situation. (2) They did not correctly include the search time in their evaluation criteria, while the search time is crucial in real CBIR systems. (3) The performance of some unsupervised hashing algorithms (e.g., LSH) can easily be boosted if one uses multiple hash tables, which is an important factor should be considered in the evaluation while most of the deep hashing papers failed to do so.   We re-evaluate several state-of-the-art deep hashing methods with a carefully designed experimental setting. Empirical results reveal that the performance of these deep hashing methods are inferior to multi-table IsoH, a very simple unsupervised hashing method. Thus, the conclusions in all the deep hashing papers should be carefully re-examined.



### Global versus Localized Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1711.06020v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06020v2)
- **Published**: 2017-11-16 10:53:53+00:00
- **Updated**: 2018-03-27 20:47:34+00:00
- **Authors**: Guo-Jun Qi, Liheng Zhang, Hao Hu, Marzieh Edraki, Jingdong Wang, Xian-Sheng Hua
- **Comment**: None
- **Journal**: Proceedings of of IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2018), Salt Lake City, Utah, June 18th - June 22nd, 2018
- **Summary**: In this paper, we present a novel localized Generative Adversarial Net (GAN) to learn on the manifold of real data. Compared with the classic GAN that {\em globally} parameterizes a manifold, the Localized GAN (LGAN) uses local coordinate charts to parameterize distinct local geometry of how data points can transform at different locations on the manifold. Specifically, around each point there exists a {\em local} generator that can produce data following diverse patterns of transformations on the manifold. The locality nature of LGAN enables local generators to adapt to and directly access the local geometry without need to invert the generator in a global GAN. Furthermore, it can prevent the manifold from being locally collapsed to a dimensionally deficient tangent subspace by imposing an orthonormality prior between tangents. This provides a geometric approach to alleviating mode collapse at least locally on the manifold by imposing independence between data transformations in different tangent directions. We will also demonstrate the LGAN can be applied to train a robust classifier that prefers locally consistent classification decisions on the manifold, and the resultant regularizer is closely related with the Laplace-Beltrami operator. Our experiments show that the proposed LGANs can not only produce diverse image transformations, but also deliver superior classification performances.



### Learning to Compare: Relation Network for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.06025v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06025v2)
- **Published**: 2017-11-16 11:01:51+00:00
- **Updated**: 2018-03-27 10:55:50+00:00
- **Authors**: Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H. S. Torr, Timothy M. Hospedales
- **Comment**: To appear in CVPR2018
- **Journal**: None
- **Summary**: We present a conceptually simple, flexible, and general framework for few-shot learning, where a classifier must learn to recognise new classes given only few examples from each. Our method, called the Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Besides providing improved performance on few-shot learning, our framework is easily extended to zero-shot learning. Extensive experiments on five benchmarks demonstrate that our simple approach provides a unified and effective approach for both of these two tasks.



### Natural Language Guided Visual Relationship Detection
- **Arxiv ID**: http://arxiv.org/abs/1711.06032v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06032v2)
- **Published**: 2017-11-16 11:26:19+00:00
- **Updated**: 2017-11-21 10:51:31+00:00
- **Authors**: Wentong Liao, Lin Shuai, Bodo Rosenhahn, Michael Ying Yang
- **Comment**: added supplementary material
- **Journal**: None
- **Summary**: Reasoning about the relationships between object pairs in images is a crucial task for holistic scene understanding. Most of the existing works treat this task as a pure visual classification task: each type of relationship or phrase is classified as a relation category based on the extracted visual features. However, each kind of relationships has a wide variety of object combination and each pair of objects has diverse interactions. Obtaining sufficient training samples for all possible relationship categories is difficult and expensive. In this work, we propose a natural language guided framework to tackle this problem. We propose to use a generic bi-directional recurrent neural network to predict the semantic connection between the participating objects in the relationship from the aspect of natural language. The proposed simple method achieves the state-of-the-art on the Visual Relationship Detection (VRD) and Visual Genome datasets, especially when predicting unseen relationships (e.g. recall improved from 76.42% to 89.79% on VRD zero-shot testing set).



### Frame Interpolation with Multi-Scale Deep Loss Functions and Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.06045v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06045v2)
- **Published**: 2017-11-16 11:46:16+00:00
- **Updated**: 2019-02-26 14:58:05+00:00
- **Authors**: Joost van Amersfoort, Wenzhe Shi, Alejandro Acosta, Francisco Massa, Johannes Totz, Zehan Wang, Jose Caballero
- **Comment**: None
- **Journal**: None
- **Summary**: Frame interpolation attempts to synthesise frames given one or more consecutive video frames. In recent years, deep learning approaches, and notably convolutional neural networks, have succeeded at tackling low- and high-level computer vision problems including frame interpolation. These techniques often tackle two problems, namely algorithm efficiency and reconstruction quality. In this paper, we present a multi-scale generative adversarial network for frame interpolation (\mbox{FIGAN}). To maximise the efficiency of our network, we propose a novel multi-scale residual estimation module where the predicted flow and synthesised frame are constructed in a coarse-to-fine fashion. To improve the quality of synthesised intermediate video frames, our network is jointly supervised at different levels with a perceptual loss function that consists of an adversarial and two content losses. We evaluate the proposed approach using a collection of 60fps videos from YouTube-8m. Our results improve the state-of-the-art accuracy and provide subjective visual quality comparable to the best performing interpolation method at x47 faster runtime.



### Deep Matching Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1711.06047v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.06047v1)
- **Published**: 2017-11-16 11:50:41+00:00
- **Updated**: 2017-11-16 11:50:41+00:00
- **Authors**: Tanmoy Mukherjee, Makoto Yamada, Timothy M. Hospedales
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Increasingly many real world tasks involve data in multiple modalities or views. This has motivated the development of many effective algorithms for learning a common latent space to relate multiple domains. However, most existing cross-view learning algorithms assume access to paired data for training. Their applicability is thus limited as the paired data assumption is often violated in practice: many tasks have only a small subset of data available with pairing annotation, or even no paired data at all. In this paper we introduce Deep Matching Autoencoders (DMAE), which learn a common latent space and pairing from unpaired multi-modal data. Specifically we formulate this as a cross-domain representation learning and object matching problem. We simultaneously optimise parameters of representation learning auto-encoders and the pairing of unpaired multi-modal data. This framework elegantly spans the full regime from fully supervised, semi-supervised, and unsupervised (no paired data) multi-modal learning. We show promising results in image captioning, and on a new task that is uniquely enabled by our methodology: unsupervised classifier learning.



### Integrated Face Analytics Networks through Cross-Dataset Hybrid Training
- **Arxiv ID**: http://arxiv.org/abs/1711.06055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06055v1)
- **Published**: 2017-11-16 12:09:40+00:00
- **Updated**: 2017-11-16 12:09:40+00:00
- **Authors**: Jianshu Li, Shengtao Xiao, Fang Zhao, Jian Zhao, Jianan Li, Jiashi Feng, Shuicheng Yan, Terence Sim
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Face analytics benefits many multimedia applications. It consists of a number of tasks, such as facial emotion recognition and face parsing, and most existing approaches generally treat these tasks independently, which limits their deployment in real scenarios. In this paper we propose an integrated Face Analytics Network (iFAN), which is able to perform multiple tasks jointly for face analytics with a novel carefully designed network architecture to fully facilitate the informative interaction among different tasks. The proposed integrated network explicitly models the interactions between tasks so that the correlations between tasks can be fully exploited for performance boost. In addition, to solve the bottleneck of the absence of datasets with comprehensive training data for various tasks, we propose a novel cross-dataset hybrid training strategy. It allows "plug-in and play" of multiple datasets annotated for different tasks without the requirement of a fully labeled common dataset for all the tasks. We experimentally show that the proposed iFAN achieves state-of-the-art performance on multiple face analytics tasks using a single integrated model. Specifically, iFAN achieves an overall F-score of 91.15% on the Helen dataset for face parsing, a normalized mean error of 5.81% on the MTFL dataset for facial landmark localization and an accuracy of 45.73% on the BNU dataset for emotion recognition with a single model.



### The Perception-Distortion Tradeoff
- **Arxiv ID**: http://arxiv.org/abs/1711.06077v4
- **DOI**: 10.1109/CVPR.2018.00652
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06077v4)
- **Published**: 2017-11-16 13:22:30+00:00
- **Updated**: 2020-10-25 07:29:01+00:00
- **Authors**: Yochai Blau, Tomer Michaeli
- **Comment**: CVPR 2018 (long oral presentation), see talk at:
  https://youtu.be/_aXbGqdEkjk?t=39m43s
- **Journal**: Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition, pp. 6228-6237, 2018
- **Summary**: Image restoration algorithms are typically evaluated by some distortion measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify perceived perceptual quality. In this paper, we prove mathematically that distortion and perceptual quality are at odds with each other. Specifically, we study the optimal probability for correctly discriminating the outputs of an image restoration algorithm from real images. We show that as the mean distortion decreases, this probability must increase (indicating worse perceptual quality). As opposed to the common belief, this result holds true for any distortion measure, and is not only a problem of the PSNR or SSIM criteria. We also show that generative-adversarial-nets (GANs) provide a principled way to approach the perception-distortion bound. This constitutes theoretical support to their observed success in low-level vision tasks. Based on our analysis, we propose a new methodology for evaluating image restoration methods, and use it to perform an extensive comparison between recent super-resolution algorithms.



### Two Birds with One Stone: Transforming and Generating Facial Images with Iterative GAN
- **Arxiv ID**: http://arxiv.org/abs/1711.06078v2
- **DOI**: 10.1016/j.neucom.2018.10.093
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06078v2)
- **Published**: 2017-11-16 13:23:20+00:00
- **Updated**: 2018-03-07 02:03:26+00:00
- **Authors**: Dan Ma, Bin Liu, Zhao Kang, Jiayu Zhou, Jianke Zhu, Zenglin Xu
- **Comment**: None
- **Journal**: Neurocomputing 2020
- **Summary**: Generating high fidelity identity-preserving faces with different facial attributes has a wide range of applications. Although a number of generative models have been developed to tackle this problem, there is still much room for further improvement.In paticular, the current solutions usually ignore the perceptual information of images, which we argue that it benefits the output of a high-quality image while preserving the identity information, especially in facial attributes learning area.To this end, we propose to train GAN iteratively via regularizing the min-max process with an integrated loss, which includes not only the per-pixel loss but also the perceptual loss. In contrast to the existing methods only deal with either image generation or transformation, our proposed iterative architecture can achieve both of them. Experiments on the multi-label facial dataset CelebA demonstrate that the proposed model has excellent performance on recognizing multiple attributes, generating a high-quality image, and transforming image with controllable attributes.



### Improving Consistency and Correctness of Sequence Inpainting using Semantically Guided Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1711.06106v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06106v2)
- **Published**: 2017-11-16 14:27:57+00:00
- **Updated**: 2017-11-17 08:10:08+00:00
- **Authors**: Avisek Lahiri, Arnav Jain, Prabir Kumar Biswas, Pabitra Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: Contemporary benchmark methods for image inpainting are based on deep generative models and specifically leverage adversarial loss for yielding realistic reconstructions. However, these models cannot be directly applied on image/video sequences because of an intrinsic drawback- the reconstructions might be independently realistic, but, when visualized as a sequence, often lacks fidelity to the original uncorrupted sequence. The fundamental reason is that these methods try to find the best matching latent space representation near to natural image manifold without any explicit distance based loss. In this paper, we present a semantically conditioned Generative Adversarial Network (GAN) for sequence inpainting. The conditional information constrains the GAN to map a latent representation to a point in image manifold respecting the underlying pose and semantics of the scene. To the best of our knowledge, this is the first work which simultaneously addresses consistency and correctness of generative model based inpainting. We show that our generative model learns to disentangle pose and appearance information; this independence is exploited by our model to generate highly consistent reconstructions. The conditional information also aids the generator network in GAN to produce sharper images compared to the original GAN formulation. This helps in achieving more appealing inpainting performance. Though generic, our algorithm was targeted for inpainting on faces. When applied on CelebA and Youtube Faces datasets, the proposed method results in a significant improvement over the current benchmark, both in terms of quantitative evaluation (Peak Signal to Noise Ratio) and human visual scoring over diversified combinations of resolutions and deformations.



### SUPRA: Open Source Software Defined Ultrasound Processing for Real-Time Applications
- **Arxiv ID**: http://arxiv.org/abs/1711.06127v3
- **DOI**: 10.1007/s11548-018-1750-6
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1711.06127v3)
- **Published**: 2017-11-16 15:12:35+00:00
- **Updated**: 2018-05-10 15:45:57+00:00
- **Authors**: Rüdiger Göbl, Nassir Navab, Christoph Hennersperger
- **Comment**: This is a pre-print of an article published in the International
  Journal of Computer Assisted Radiology and Surgery. The final authenticated
  version is available online at: https://doi.org/10.1007/s11548-018-1750-6
- **Journal**: G\"obl, R., Navab, N. & Hennersperger, C. , "SUPRA: Open Source
  Software Defined Ultrasound Processing for Real-Time Applications" Int J CARS
  (2018)
- **Summary**: Research in ultrasound imaging is limited in reproducibility by two factors: First, many existing ultrasound pipelines are protected by intellectual property, rendering exchange of code difficult. Second, most pipelines are implemented in special hardware, resulting in limited flexibility of implemented processing steps on such platforms.   Methods: With SUPRA we propose an open-source pipeline for fully Software Defined Ultrasound Processing for Real-time Applications to alleviate these problems. Covering all steps from beamforming to output of B-mode images, SUPRA can help improve the reproducibility of results and make modifications to the image acquisition mode accessible to the research community. We evaluate the pipeline qualitatively, quantitatively, and regarding its run-time.   Results: The pipeline shows image quality comparable to a clinical system and backed by point-spread function measurements a comparable resolution. Including all processing stages of a usual ultrasound pipeline, the run-time analysis shows that it can be executed in 2D and 3D on consumer GPUs in real-time.   Conclusions: Our software ultrasound pipeline opens up the research in image acquisition. Given access to ultrasound data from early stages (raw channel data, radiofrequency data) it simplifies the development in imaging. Furthermore, it tackles the reproducibility of research results, as code can be shared easily and even be executed without dedicated ultrasound hardware.



### Hybrid Approach of Relation Network and Localized Graph Convolutional Filtering for Breast Cancer Subtype Classification
- **Arxiv ID**: http://arxiv.org/abs/1711.05859v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.05859v3)
- **Published**: 2017-11-16 15:15:31+00:00
- **Updated**: 2018-06-15 05:29:23+00:00
- **Authors**: Sungmin Rhee, Seokjun Seo, Sun Kim
- **Comment**: 8 pages, To be published in proceeding of IJCAI 2018
- **Journal**: None
- **Summary**: Network biology has been successfully used to help reveal complex mechanisms of disease, especially cancer. On the other hand, network biology requires in-depth knowledge to construct disease-specific networks, but our current knowledge is very limited even with the recent advances in human cancer biology. Deep learning has shown a great potential to address the difficult situation like this. However, deep learning technologies conventionally use grid-like structured data, thus application of deep learning technologies to the classification of human disease subtypes is yet to be explored. Recently, graph based deep learning techniques have emerged, which becomes an opportunity to leverage analyses in network biology. In this paper, we proposed a hybrid model, which integrates two key components 1) graph convolution neural network (graph CNN) and 2) relation network (RN). We utilize graph CNN as a component to learn expression patterns of cooperative gene community, and RN as a component to learn associations between learned patterns. The proposed model is applied to the PAM50 breast cancer subtype classification task, the standard breast cancer subtype classification of clinical utility. In experiments of both subtype classification and patient survival analysis, our proposed method achieved significantly better performances than existing methods. We believe that this work is an important starting point to realize the upcoming personalized medicine.



### 3D Trajectory Reconstruction of Dynamic Objects Using Planarity Constraints
- **Arxiv ID**: http://arxiv.org/abs/1711.06136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06136v1)
- **Published**: 2017-11-16 15:21:36+00:00
- **Updated**: 2017-11-16 15:21:36+00:00
- **Authors**: Sebastian Bullinger, Christoph Bodensteiner, Michael Arens, Rainer Stiefelhagen
- **Comment**: 9 Pages, under review
- **Journal**: None
- **Summary**: We present a method to reconstruct the three-dimensional trajectory of a moving instance of a known object category in monocular video data. We track the two-dimensional shape of objects on pixel level exploiting instance-aware semantic segmentation techniques and optical flow cues. We apply Structure from Motion techniques to object and background images to determine for each frame camera poses relative to object instances and background structures. By combining object and background camera pose information, we restrict the object trajectory to a one-parameter family of possible solutions. We compute a ground representation by fusing background structures and corresponding semantic segmentations. This allows us to determine an object trajectory consistent to image observations and reconstructed environment model. Our method is robust to occlusion and handles temporarily stationary objects. We show qualitative results using drone imagery. Due to the lack of suitable benchmark datasets we present a new dataset to evaluate the quality of reconstructed three-dimensional object trajectories. The video sequences contain vehicles in urban areas and are rendered using the path-tracing render engine Cycles to achieve realistic results. We perform a quantitative evaluation of the presented approach using this dataset. Our algorithm achieves an average reconstruction-to-ground-truth distance of 0.31 meter.



### Learning Compositional Visual Concepts with Mutual Consistency
- **Arxiv ID**: http://arxiv.org/abs/1711.06148v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.06148v2)
- **Published**: 2017-11-16 15:41:34+00:00
- **Updated**: 2018-03-28 15:22:53+00:00
- **Authors**: Yunye Gong, Srikrishna Karanam, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, Peter C. Doerschuk
- **Comment**: 10 pages, 8 figures, 4 tables, CVPR 2018
- **Journal**: None
- **Summary**: Compositionality of semantic concepts in image synthesis and analysis is appealing as it can help in decomposing known and generatively recomposing unknown data. For instance, we may learn concepts of changing illumination, geometry or albedo of a scene, and try to recombine them to generate physically meaningful, but unseen data for training and testing. In practice however we often do not have samples from the joint concept space available: We may have data on illumination change in one data set and on geometric change in another one without complete overlap. We pose the following question: How can we learn two or more concepts jointly from different data sets with mutual consistency where we do not have samples from the full joint space? We present a novel answer in this paper based on cyclic consistency over multiple concepts, represented individually by generative adversarial networks (GANs). Our method, ConceptGAN, can be understood as a drop in for data augmentation to improve resilience for real world applications. Qualitative and quantitative evaluations demonstrate its efficacy in generating semantically meaningful images, as well as one shot face verification as an example application.



### Zero-Shot Learning via Category-Specific Visual-Semantic Mapping
- **Arxiv ID**: http://arxiv.org/abs/1711.06167v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06167v2)
- **Published**: 2017-11-16 16:03:15+00:00
- **Updated**: 2017-12-13 03:46:07+00:00
- **Authors**: Li Niu, Jianfei Cai, Ashok Veeraraghavan
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-Shot Learning (ZSL) aims to classify a test instance from an unseen category based on the training instances from seen categories, in which the gap between seen categories and unseen categories is generally bridged via visual-semantic mapping between the low-level visual feature space and the intermediate semantic space. However, the visual-semantic mapping learnt based on seen categories may not generalize well to unseen categories because the data distributions between seen categories and unseen categories are considerably different, which is known as the projection domain shift problem in ZSL. To address this domain shift issue, we propose a method named Adaptive Embedding ZSL (AEZSL) to learn an adaptive visual-semantic mapping for each unseen category based on the similarities between each unseen category and all the seen categories. Then, we further make two extensions based on our AEZSL method. Firstly, in order to utilize the unlabeled test instances from unseen categories, we extend our AEZSL to a semi-supervised approach named AEZSL with Label Refinement (AEZSL_LR), in which a progressive approach is developed to update the visual classifiers and refine the predicted test labels alternatively based on the similarities among test instances and among unseen categories. Secondly, to avoid learning visual-semantic mapping for each unseen category in the large-scale classification task, we extend our AEZSL to a deep adaptive embedding model named Deep AEZSL (DAEZSL) sharing the similar idea (i.e., visual-semantic mapping should be category-specific and related to the semantic space) with AEZSL, which only needs to be trained once, but can be applied to arbitrary number of unseen categories. Extensive experiments demonstrate that our proposed methods achieve the state-of-the-art results for image classification on four benchmark datasets.



### A Forward-Backward Approach for Visualizing Information Flow in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.06221v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.06221v1)
- **Published**: 2017-11-16 18:00:24+00:00
- **Updated**: 2017-11-16 18:00:24+00:00
- **Authors**: Aditya Balu, Thanh V. Nguyen, Apurva Kokate, Chinmay Hegde, Soumik Sarkar
- **Comment**: Presented at NIPS 2017 Symposium on Interpretable Machine Learning
- **Journal**: None
- **Summary**: We introduce a new, systematic framework for visualizing information flow in deep networks. Specifically, given any trained deep convolutional network model and a given test image, our method produces a compact support in the image domain that corresponds to a (high-resolution) feature that contributes to the given explanation. Our method is both computationally efficient as well as numerically robust. We present several preliminary numerical results that support the benefits of our framework over existing methods.



### A Novel Framework for Robustness Analysis of Visual QA Models
- **Arxiv ID**: http://arxiv.org/abs/1711.06232v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1711.06232v3)
- **Published**: 2017-11-16 18:27:49+00:00
- **Updated**: 2018-12-25 04:08:27+00:00
- **Authors**: Jia-Hong Huang, Cuong Duc Dao, Modar Alfadly, Bernard Ghanem
- **Comment**: Accepted by the Thirty-Third AAAI Conference on Artificial
  Intelligence, (AAAI-19), as an oral paper
- **Journal**: None
- **Summary**: Deep neural networks have been playing an essential role in many computer vision tasks including Visual Question Answering (VQA). Until recently, the study of their accuracy was the main focus of research but now there is a trend toward assessing the robustness of these models against adversarial attacks by evaluating their tolerance to varying noise levels. In VQA, adversarial attacks can target the image and/or the proposed main question and yet there is a lack of proper analysis of the later. In this work, we propose a flexible framework that focuses on the language part of VQA that uses semantically relevant questions, dubbed basic questions, acting as controllable noise to evaluate the robustness of VQA models. We hypothesize that the level of noise is positively correlated to the similarity of a basic question to the main question. Hence, to apply noise on any given main question, we rank a pool of basic questions based on their similarity by casting this ranking task as a LASSO optimization problem. Then, we propose a novel robustness measure, R_score, and two large-scale basic question datasets (BQDs) in order to standardize robustness analysis for VQA models.



### LDMNet: Low Dimensional Manifold Regularized Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.06246v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06246v1)
- **Published**: 2017-11-16 18:48:01+00:00
- **Updated**: 2017-11-16 18:48:01+00:00
- **Authors**: Wei Zhu, Qiang Qiu, Jiaji Huang, Robert Calderbank, Guillermo Sapiro, Ingrid Daubechies
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have proved very successful on archetypal tasks for which large training sets are available, but when the training data are scarce, their performance suffers from overfitting. Many existing methods of reducing overfitting are data-independent, and their efficacy is often limited when the training set is very small. Data-dependent regularizations are mostly motivated by the observation that data of interest lie close to a manifold, which is typically hard to parametrize explicitly and often requires human input of tangent vectors. These methods typically only focus on the geometry of the input data, and do not necessarily encourage the networks to produce geometrically meaningful features. To resolve this, we propose a new framework, the Low-Dimensional-Manifold-regularized neural Network (LDMNet), which incorporates a feature regularization method that focuses on the geometry of both the input data and the output features. In LDMNet, we regularize the network by encouraging the combination of the input data and the output features to sample a collection of low dimensional manifolds, which are searched efficiently without explicit parametrization. To achieve this, we directly use the manifold dimension as a regularization term in a variational functional. The resulting Euler-Lagrange equation is a Laplace-Beltrami equation over a point cloud, which is solved by the point integral method without increasing the computational complexity. We demonstrate two benefits of LDMNet in the experiments. First, we show that LDMNet significantly outperforms widely-used network regularizers such as weight decay and DropOut. Second, we show that LDMNet can be designed to extract common features of an object imaged via different modalities, which proves to be very useful in real-world applications such as cross-spectral face recognition.



### Language-Based Image Editing with Recurrent Attentive Models
- **Arxiv ID**: http://arxiv.org/abs/1711.06288v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.06288v2)
- **Published**: 2017-11-16 19:10:21+00:00
- **Updated**: 2018-06-10 04:04:30+00:00
- **Authors**: Jianbo Chen, Yelong Shen, Jianfeng Gao, Jingjing Liu, Xiaodong Liu
- **Comment**: Accepted to CVPR 2018 as a Spotlight
- **Journal**: None
- **Summary**: We investigate the problem of Language-Based Image Editing (LBIE). Given a source image and a natural language description, we want to generate a target image by editing the source image based on the description. We propose a generic modeling framework for two sub-tasks of LBIE: language-based image segmentation and image colorization. The framework uses recurrent attentive models to fuse image and language features. Instead of using a fixed step size, we introduce for each region of the image a termination gate to dynamically determine after each inference step whether to continue extrapolating additional information from the textual description. The effectiveness of the framework is validated on three datasets. First, we introduce a synthetic dataset, called CoSaL, to evaluate the end-to-end performance of our LBIE system. Second, we show that the framework leads to state-of-the-art performance on image segmentation on the ReferIt dataset. Third, we present the first language-based colorization result on the Oxford-102 Flowers dataset.



### Grammatical facial expression recognition using customized deep neural network architecture
- **Arxiv ID**: http://arxiv.org/abs/1711.06303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06303v1)
- **Published**: 2017-11-16 19:50:46+00:00
- **Updated**: 2017-11-16 19:50:46+00:00
- **Authors**: Devesh Walawalkar
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes to expand the visual understanding capacity of computers by helping it recognize human sign language more efficiently. This is carried out through recognition of facial expressions, which accompany the hand signs used in this language. This paper specially focuses on the popular Brazilian sign language (LIBRAS). While classifying different hand signs into their respective word meanings has already seen much literature dedicated to it, the emotions or intention with which the words are expressed haven't primarily been taken into consideration. As from our normal human experience, words expressed with different emotions or mood can have completely different meanings attached to it. Lending computers the ability of classifying these facial expressions, can help add another level of deep understanding of what the deaf person exactly wants to communicate. The proposed idea is implemented through a deep neural network having a customized architecture. This helps learning specific patterns in individual expressions much better as compared to a generic approach. With an overall accuracy of 98.04%, the implemented deep network performs excellently well and thus is fit to be used in any given practical scenario.



### Attend and Interact: Higher-Order Object Interactions for Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/1711.06330v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06330v2)
- **Published**: 2017-11-16 22:14:52+00:00
- **Updated**: 2018-03-20 21:22:42+00:00
- **Authors**: Chih-Yao Ma, Asim Kadav, Iain Melvin, Zsolt Kira, Ghassan AlRegib, Hans Peter Graf
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Human actions often involve complex interactions across several inter-related objects in the scene. However, existing approaches to fine-grained video understanding or visual relationship detection often rely on single object representation or pairwise object relationships. Furthermore, learning interactions across multiple objects in hundreds of frames for video is computationally infeasible and performance may suffer since a large combinatorial space has to be modeled. In this paper, we propose to efficiently learn higher-order interactions between arbitrary subgroups of objects for fine-grained video understanding. We demonstrate that modeling object interactions significantly improves accuracy for both action recognition and video captioning, while saving more than 3-times the computation over traditional pairwise relationships. The proposed method is validated on two large-scale datasets: Kinetics and ActivityNet Captions. Our SINet and SINet-Caption achieve state-of-the-art performances on both datasets even though the videos are sampled at a maximum of 1 FPS. To the best of our knowledge, this is the first work modeling object interactions on open domain large-scale video datasets, and we additionally model higher-order object interactions which improves the performance with low computational costs.



### Grounded Objects and Interactions for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1711.06354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06354v1)
- **Published**: 2017-11-16 23:39:08+00:00
- **Updated**: 2017-11-16 23:39:08+00:00
- **Authors**: Chih-Yao Ma, Asim Kadav, Iain Melvin, Zsolt Kira, Ghassan AlRegib, Hans Peter Graf
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1711.06330
- **Journal**: None
- **Summary**: We address the problem of video captioning by grounding language generation on object interactions in the video. Existing work mostly focuses on overall scene understanding with often limited or no emphasis on object interactions to address the problem of video understanding. In this paper, we propose SINet-Caption that learns to generate captions grounded over higher-order interactions between arbitrary groups of objects for fine-grained video understanding. We discuss the challenges and benefits of such an approach. We further demonstrate state-of-the-art results on the ActivityNet Captions dataset using our model, SINet-Caption based on this approach.



