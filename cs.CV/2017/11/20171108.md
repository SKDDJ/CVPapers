# Arxiv Papers in cs.CV on 2017-11-08
### A New Hybrid-parameter Recurrent Neural Networks for Online Handwritten Chinese Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.02809v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.02809v2)
- **Published**: 2017-11-08 03:04:41+00:00
- **Updated**: 2018-07-30 03:03:44+00:00
- **Authors**: Haiqing Ren, Weiqiang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The recurrent neural network (RNN) is appropriate for dealing with temporal sequences. In this paper, we present a deep RNN with new features and apply it for online handwritten Chinese character recognition. Compared with the existing RNN models, three innovations are involved in the proposed system. First, a new hidden layer function for RNN is proposed for learning temporal information better. we call it Memory Pool Unit (MPU). The proposed MPU has a simple architecture. Second, a new RNN architecture with hybrid parameter is presented, in order to increasing the expression capacity of RNN. The proposed hybrid-parameter RNN has parameter changes when calculating the iteration at temporal dimension. Third, we make a adaptation that all the outputs of each layer are stacked as the output of network. Stacked hidden layer states combine all the hidden layer states for increasing the expression capacity. Experiments are carried out on the IAHCC-UCAS2016 dataset and the CASIA-OLHWDB1.1 dataset. The experimental results show that the hybrid-parameter RNN obtain a better recognition performance with higher efficiency (fewer parameters and faster speed). And the proposed Memory Pool Unit is proved to be a simple hidden layer function and obtains a competitive recognition results.



### Multi-label Image Recognition by Recurrently Discovering Attentional Regions
- **Arxiv ID**: http://arxiv.org/abs/1711.02816v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.02816v1)
- **Published**: 2017-11-08 03:43:51+00:00
- **Updated**: 2017-11-08 03:43:51+00:00
- **Authors**: Zhouxia Wang, Tianshui Chen, Guanbin Li, Ruijia Xu, Liang Lin
- **Comment**: Accepted at ICCV 2017
- **Journal**: None
- **Summary**: This paper proposes a novel deep architecture to address multi-label image recognition, a fundamental and practical task towards general visual understanding. Current solutions for this task usually rely on an extra step of extracting hypothesis regions (i.e., region proposals), resulting in redundant computation and sub-optimal performance. In this work, we achieve the interpretable and contextualized multi-label image classification by developing a recurrent memorized-attention module. This module consists of two alternately performed components: i) a spatial transformer layer to locate attentional regions from the convolutional feature maps in a region-proposal-free way and ii) an LSTM (Long-Short Term Memory) sub-network to sequentially predict semantic labeling scores on the located regions while capturing the global dependencies of these regions. The LSTM also output the parameters for computing the spatial transformer. On large-scale benchmarks of multi-label image classification (e.g., MS-COCO and PASCAL VOC 07), our approach demonstrates superior performances over other existing state-of-the-arts in both accuracy and efficiency.



### Heuristic Search for Structural Constraints in Data Association
- **Arxiv ID**: http://arxiv.org/abs/1711.02823v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.02823v1)
- **Published**: 2017-11-08 04:26:35+00:00
- **Updated**: 2017-11-08 04:26:35+00:00
- **Authors**: Xiao Zhou, Peilin Jiang, Fei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The research on multi-object tracking (MOT) is essentially to solve for the data association assignment, the core of which is to design the association cost as discriminative as possible. Generally speaking, the match ambiguities caused by similar appearances of objects and the moving cameras make the data association perplexing and challenging. In this paper, we propose a new heuristic method to search for structural constraints (HSSC) of multiple targets when solving the problem of online multi-object tracking. We believe that the internal structure among multiple targets in the adjacent frames could remain constant and stable even though the video sequences are captured by a moving camera. As a result, the structural constraints are able to cut down the match ambiguities caused by the moving cameras as well as similar appearances of the tracked objects. The proposed heuristic method aims to obtain a maximum match set under the minimum structural cost for each available match pair, which can be integrated with the raw association costs and make them more elaborate and discriminative compared with other approaches. In addition, this paper presents a new method to recover missing targets by minimizing the cost function generated from both motion and structure cues. Our online multi-object tracking (MOT) algorithm based on HSSC has achieved the multi-object tracking accuracy (MOTA) of 25.0 on the public dataset 2DMOT2015[1].



### SIMILARnet: Simultaneous Intelligent Localization and Recognition Network
- **Arxiv ID**: http://arxiv.org/abs/1711.02831v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1711.02831v1)
- **Published**: 2017-11-08 05:08:26+00:00
- **Updated**: 2017-11-08 05:08:26+00:00
- **Authors**: Arna Ghosh, Biswarup Bhattacharya, Somnath Basu Roy Chowdhury
- **Comment**: 5 pages; 2 figures; 2 tables; All authors have equal contribution
- **Journal**: None
- **Summary**: Global Average Pooling (GAP) [4] has been used previously to generate class activation for image classification tasks. The motivation behind SIMILARnet comes from the fact that the convolutional filters possess position information of the essential features and hence, combination of the feature maps could help us locate the class instances in an image. We propose a biologically inspired model that is free of differential connections and doesn't require separate training thereby reducing computation overhead. Our novel architecture generates promising results and unlike existing methods, the model is not sensitive to the input image size, thus promising wider application. Codes for the experiment and illustrations can be found at: https://github.com/brcsomnath/Advanced-GAP .



### Revealing structure components of the retina by deep learning networks
- **Arxiv ID**: http://arxiv.org/abs/1711.02837v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1711.02837v1)
- **Published**: 2017-11-08 05:35:27+00:00
- **Updated**: 2017-11-08 05:35:27+00:00
- **Authors**: Qi Yan, Zhaofei Yu, Feng Chen, Jian K. Liu
- **Comment**: Presented at NIPS 2017 Symposium on Interpretable Machine Learning
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have demonstrated impressive performance on visual object classification tasks. In addition, it is a useful model for predication of neuronal responses recorded in visual system. However, there is still no clear understanding of what CNNs learn in terms of visual neuronal circuits. Visualizing CNN's features to obtain possible connections to neuronscience underpinnings is not easy due to highly complex circuits from the retina to higher visual cortex. Here we address this issue by focusing on single retinal ganglion cells with a simple model and electrophysiological recordings from salamanders. By training CNNs with white noise images to predicate neural responses, we found that convolutional filters learned in the end are resembling to biological components of the retinal circuit. Features represented by these filters tile the space of conventional receptive field of retinal ganglion cells. These results suggest that CNN could be used to reveal structure components of neuronal circuits.



### Transductive Zero-Shot Hashing via Coarse-to-Fine Similarity Mining
- **Arxiv ID**: http://arxiv.org/abs/1711.02856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.02856v1)
- **Published**: 2017-11-08 07:46:47+00:00
- **Updated**: 2017-11-08 07:46:47+00:00
- **Authors**: Hanjiang Lai, Yan Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot Hashing (ZSH) is to learn hashing models for novel/target classes without training data, which is an important and challenging problem. Most existing ZSH approaches exploit transfer learning via an intermediate shared semantic representations between the seen/source classes and novel/target classes. However, due to having disjoint, the hash functions learned from the source dataset are biased when applied directly to the target classes. In this paper, we study the transductive ZSH, i.e., we have unlabeled data for novel classes. We put forward a simple yet efficient joint learning approach via coarse-to-fine similarity mining which transfers knowledges from source data to target data. It mainly consists of two building blocks in the proposed deep architecture: 1) a shared two-streams network, which the first stream operates on the source data and the second stream operates on the unlabeled data, to learn the effective common image representations, and 2) a coarse-to-fine module, which begins with finding the most representative images from target classes and then further detect similarities among these images, to transfer the similarities of the source data to the target data in a greedy fashion. Extensive evaluation results on several benchmark datasets demonstrate that the proposed hashing method achieves significant improvement over the state-of-the-art methods.



### Learning Sparse Visual Representations with Leaky Capped Norm Regularizers
- **Arxiv ID**: http://arxiv.org/abs/1711.02857v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.02857v1)
- **Published**: 2017-11-08 07:54:41+00:00
- **Updated**: 2017-11-08 07:54:41+00:00
- **Authors**: Jianqiao Wangni, Dahua Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Sparsity inducing regularization is an important part for learning over-complete visual representations. Despite the popularity of $\ell_1$ regularization, in this paper, we investigate the usage of non-convex regularizations in this problem. Our contribution consists of three parts. First, we propose the leaky capped norm regularization (LCNR), which allows model weights below a certain threshold to be regularized more strongly as opposed to those above, therefore imposes strong sparsity and only introduces controllable estimation bias. We propose a majorization-minimization algorithm to optimize the joint objective function. Second, our study over monocular 3D shape recovery and neural networks with LCNR outperforms $\ell_1$ and other non-convex regularizations, achieving state-of-the-art performance and faster convergence. Third, we prove a theoretical global convergence speed on the 3D recovery problem. To the best of our knowledge, this is the first convergence analysis of the 3D recovery problem.



### Generalization of Deep Neural Networks for Chest Pathology Classification in X-Rays Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.01636v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1712.01636v2)
- **Published**: 2017-11-08 14:26:01+00:00
- **Updated**: 2018-02-12 18:25:28+00:00
- **Authors**: Hojjat Salehinejad, Shahrokh Valaee, Tim Dowdell, Errol Colak, Joseph Barfett
- **Comment**: This paper is accepted for presentation at IEEE International
  Conference on Acoustics, Speech and Signal Processing (IEEE ICASSP), 2018
- **Journal**: None
- **Summary**: Medical datasets are often highly imbalanced with over-representation of common medical problems and a paucity of data from rare conditions. We propose simulation of pathology in images to overcome the above limitations. Using chest X-rays as a model medical image, we implement a generative adversarial network (GAN) to create artificial images based upon a modest sized labeled dataset. We employ a combination of real and artificial images to train a deep convolutional neural network (DCNN) to detect pathology across five classes of chest X-rays. Furthermore, we demonstrate that augmenting the original imbalanced dataset with GAN generated images improves performance of chest pathology classification using the proposed DCNN in comparison to the same DCNN trained with the original dataset alone. This improved performance is largely attributed to balancing of the dataset using GAN generated images, where image classes that are lacking in example images are preferentially augmented.



### Offline signature authenticity verification through unambiguously connected skeleton segments
- **Arxiv ID**: http://arxiv.org/abs/1711.03082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03082v1)
- **Published**: 2017-11-08 18:25:31+00:00
- **Updated**: 2017-11-08 18:25:31+00:00
- **Authors**: Jugurta Montalvão, Luiz Miranda, Jânio Canuto
- **Comment**: None
- **Journal**: None
- **Summary**: A method for offline signature verification is presented in this paper. It is based on the segmentation of the signature skeleton (through standard image skeletonization) into unambiguous sequences of points, or unambiguously connected skeleton segments corresponding to vectorial representations of signature portions. These segments are assumed to be the fundamental carriers of useful information for authenticity verification, and are compactly encoded as sets of 9 scalars (4 sampled coordinates and 1 length measure). Thus signature authenticity is inferred through Euclidean distance based comparisons between pairs of such compact representations. The average performance of this method is evaluated through experiments with offline versions of signatures from the MCYT-100 database. For comparison purposes, three other approaches are applied to the same set of signatures, namely: (1) a straightforward approach based on Dynamic Time Warping distances between segments, (2) a published method by [shanker2007], also based on DTW, and (3) the average human performance under equivalent experimental protocol. Results suggest that if human performance is taken as a goal for automatic verification, then we should discard signature shape details to approach this goal. Moreover, our best result -- close to human performance -- was obtained by the simplest strategy, where equal weights were given to segment shape and length.



### MarrNet: 3D Shape Reconstruction via 2.5D Sketches
- **Arxiv ID**: http://arxiv.org/abs/1711.03129v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.03129v1)
- **Published**: 2017-11-08 19:29:01+00:00
- **Updated**: 2017-11-08 19:29:01+00:00
- **Authors**: Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, William T Freeman, Joshua B Tenenbaum
- **Comment**: NIPS 2017. The first two authors contributed equally to this paper.
  Project page: http://marrnet.csail.mit.edu
- **Journal**: None
- **Summary**: 3D object reconstruction from a single image is a highly under-determined problem, requiring strong prior knowledge of plausible 3D shapes. This introduces challenges for learning-based approaches, as 3D object annotations are scarce in real images. Previous work chose to train on synthetic data with ground truth 3D information, but suffered from domain adaptation when tested on real data. In this work, we propose MarrNet, an end-to-end trainable model that sequentially estimates 2.5D sketches and 3D object shape. Our disentangled, two-step formulation has three advantages. First, compared to full 3D shape, 2.5D sketches are much easier to be recovered from a 2D image; models that recover 2.5D sketches are also more likely to transfer from synthetic to real data. Second, for 3D reconstruction from 2.5D sketches, systems can learn purely from synthetic data. This is because we can easily render realistic 2.5D sketches without modeling object appearance variations in real images, including lighting, texture, etc. This further relieves the domain adaptation problem. Third, we derive differentiable projective functions from 3D shape to 2.5D sketches; the framework is therefore end-to-end trainable on real images, requiring no human annotations. Our model achieves state-of-the-art performance on 3D shape reconstruction.



### Picasso, Matisse, or a Fake? Automated Analysis of Drawings at the Stroke Level for Attribution and Authentication
- **Arxiv ID**: http://arxiv.org/abs/1711.03536v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.03536v1)
- **Published**: 2017-11-08 20:26:40+00:00
- **Updated**: 2017-11-08 20:26:40+00:00
- **Authors**: Ahmed Elgammal, Yan Kang, Milko Den Leeuw
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a computational approach for analysis of strokes in line drawings by artists. We aim at developing an AI methodology that facilitates attribution of drawings of unknown authors in a way that is not easy to be deceived by forged art. The methodology used is based on quantifying the characteristics of individual strokes in drawings. We propose a novel algorithm for segmenting individual strokes. We designed and compared different hand-crafted and learned features for the task of quantifying stroke characteristics. We also propose and compare different classification methods at the drawing level. We experimented with a dataset of 300 digitized drawings with over 80 thousands strokes. The collection mainly consisted of drawings of Pablo Picasso, Henry Matisse, and Egon Schiele, besides a small number of representative works of other artists. The experiments shows that the proposed methodology can classify individual strokes with accuracy 70%-90%, and aggregate over drawings with accuracy above 80%, while being robust to be deceived by fakes (with accuracy 100% for detecting fakes in most settings).



### Curve Reconstruction via the Global Statistics of Natural Curves
- **Arxiv ID**: http://arxiv.org/abs/1711.03172v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03172v3)
- **Published**: 2017-11-08 21:20:53+00:00
- **Updated**: 2018-06-13 06:22:52+00:00
- **Authors**: Ehud Barnea, Ohad Ben-Shahar
- **Comment**: CVPR version
- **Journal**: None
- **Summary**: Reconstructing the missing parts of a curve has been the subject of much computational research, with applications in image inpainting, object synthesis, etc. Different approaches for solving that problem are typically based on processes that seek visually pleasing or perceptually plausible completions. In this work we focus on reconstructing the underlying physically likely shape by utilizing the global statistics of natural curves. More specifically, we develop a reconstruction model that seeks the mean physical curve for a given inducer configuration. This simple model is both straightforward to compute and it is receptive to diverse additional information, but it requires enough samples for all curve configurations, a practical requirement that limits its effective utilization. To address this practical issue we explore and exploit statistical geometrical properties of natural curves, and in particular, we show that in many cases the mean curve is scale invariant and oftentimes it is extensible. This, in turn, allows to boost the number of examples and thus the robustness of the statistics and its applicability. The reconstruction results are not only more physically plausible but they also lead to important insights on the reconstruction problem, including an elegant explanation why certain inducer configurations are more likely to yield consistent perceptual completions than others.



### Multi-stage Suture Detection for Robot Assisted Anastomosis based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.03179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03179v1)
- **Published**: 2017-11-08 21:44:14+00:00
- **Updated**: 2017-11-08 21:44:14+00:00
- **Authors**: Yang Hu, Yun Gu, Jie Yang, Guang-Zhong Yang
- **Comment**: Submitted to ICRA 2018
- **Journal**: None
- **Summary**: In robotic surgery, task automation and learning from demonstration combined with human supervision is an emerging trend for many new surgical robot platforms. One such task is automated anastomosis, which requires bimanual needle handling and suture detection. Due to the complexity of the surgical environment and varying patient anatomies, reliable suture detection is difficult, which is further complicated by occlusion and thread topologies. In this paper, we propose a multi-stage framework for suture thread detection based on deep learning. Fully convolutional neural networks are used to obtain the initial detection and the overlapping status of suture thread, which are later fused with the original image to learn a gradient road map of the thread. Based on the gradient road map, multiple segments of the thread are extracted and linked to form the whole thread using a curvilinear structure detector. Experiments on two different types of sutures demonstrate the accuracy of the proposed framework.



### Deep Hyperspherical Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.03189v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.03189v5)
- **Published**: 2017-11-08 22:21:21+00:00
- **Updated**: 2018-01-30 16:00:14+00:00
- **Authors**: Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, Le Song
- **Comment**: NIPS 2017 (Spotlight)
- **Journal**: None
- **Summary**: Convolution as inner product has been the founding basis of convolutional neural networks (CNNs) and the key to end-to-end visual representation learning. Benefiting from deeper architectures, recent CNNs have demonstrated increasingly strong representation abilities. Despite such improvement, the increased depth and larger parameter space have also led to challenges in properly training a network. In light of such challenges, we propose hyperspherical convolution (SphereConv), a novel learning framework that gives angular representations on hyperspheres. We introduce SphereNet, deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks. In particular, SphereNet adopts SphereConv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under SphereConv. We show that SphereNet can effectively encode discriminative representation and alleviate training difficulty, leading to easier optimization, faster convergence and comparable (even better) classification accuracy over convolutional counterparts. We also provide some theoretical insights for the advantages of learning on hyperspheres. In addition, we introduce the learnable SphereConv, i.e., a natural improvement over prefixed SphereConv, and SphereNorm, i.e., hyperspherical learning as a normalization method. Experiments have verified our conclusions.



### CyCADA: Cycle-Consistent Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1711.03213v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03213v3)
- **Published**: 2017-11-08 23:54:52+00:00
- **Updated**: 2017-12-29 05:00:37+00:00
- **Authors**: Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A. Efros, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs. We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs. Our model can be applied in a variety of visual recognition and prediction settings. We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.



