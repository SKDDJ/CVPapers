# Arxiv Papers in cs.CV on 2017-11-19
### A novel total variation model based on kernel functions and its application
- **Arxiv ID**: http://arxiv.org/abs/1711.06948v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06948v1)
- **Published**: 2017-11-19 01:30:44+00:00
- **Updated**: 2017-11-19 01:30:44+00:00
- **Authors**: Zhizheng Liang, Lei Zhang, Jin Liu, Yong Zhou
- **Comment**: 22 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: The total variation (TV) model and its related variants have already been proposed for image processing in previous literature. In this paper a novel total variation model based on kernel functions is proposed. In this novel model, we first map each pixel value of an image into a Hilbert space by using a nonlinear map, and then define a coupled image of an original image in order to construct a kernel function. Finally, the proposed model is solved in a kernel function space instead of in the projecting space from a nonlinear map. For the proposed model, we theoretically show under what conditions the mapping image is in the space of bounded variation when the original image is in the space of bounded variation. It is also found that the proposed model further extends the generalized TV model and the information from three different channels of color images can be fused by adopting various kernel functions. A series of experiments on some gray and color images are carried out to demonstrate the effectiveness of the proposed model.



### BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning
- **Arxiv ID**: http://arxiv.org/abs/1711.06959v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.06959v1)
- **Published**: 2017-11-19 02:44:31+00:00
- **Updated**: 2017-11-19 02:44:31+00:00
- **Authors**: Ziming Zhang, Yuanwei Wu, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the global optimality in deep learning (DL) has been attracting more and more attention recently. Conventional DL solvers, however, have not been developed intentionally to seek for such global optimality. In this paper we propose a novel approximation algorithm, BPGrad, towards optimizing deep models globally via branch and pruning. Our BPGrad algorithm is based on the assumption of Lipschitz continuity in DL, and as a result it can adaptively determine the step size for current gradient given the history of previous updates, wherein theoretically no smaller steps can achieve the global optimality. We prove that, by repeating such branch-and-pruning procedure, we can locate the global optimality within finite iterations. Empirically an efficient solver based on BPGrad for DL is proposed as well, and it outperforms conventional DL solvers such as Adagrad, Adadelta, RMSProp, and Adam in the tasks of object recognition, detection, and segmentation.



### Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.06969v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.06969v2)
- **Published**: 2017-11-19 05:25:24+00:00
- **Updated**: 2018-04-01 21:48:18+00:00
- **Authors**: Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser Nam Lim, Rama Chellappa
- **Comment**: Accepted as spotlight talk at CVPR 2018. Code available here:
  https://github.com/swamiviv/LSD-seg
- **Journal**: None
- **Summary**: Visual Domain Adaptation is a problem of immense importance in computer vision. Previous approaches showcase the inability of even deep neural networks to learn informative representations across domain shift. This problem is more severe for tasks where acquiring hand labeled data is extremely hard and tedious. In this work, we focus on adapting the representations learned by segmentation networks across synthetic and real domains. Contrary to previous approaches that use a simple adversarial objective or superpixel information to aid the process, we propose an approach based on Generative Adversarial Networks (GANs) that brings the embeddings closer in the learned feature space. To showcase the generality and scalability of our approach, we show that we can achieve state of the art results on two challenging scenarios of synthetic to real domain adaptation. Additional exploratory experiments show that our approach: (1) generalizes to unseen domains and (2) results in improved alignment of source and target distributions.



### Color Face Recognition using High-Dimension Quaternion-based Adaptive Representation
- **Arxiv ID**: http://arxiv.org/abs/1712.01642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01642v1)
- **Published**: 2017-11-19 06:15:59+00:00
- **Updated**: 2017-11-19 06:15:59+00:00
- **Authors**: Qingxiang Feng, Yicong Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, quaternion collaborative representation-based classification (QCRC) and quaternion sparse representation-based classification (QSRC) have been proposed for color face recognition. They can obtain correlation information among different color channels. However, their performance is unstable in different conditions. For example, QSRC performs better than than QCRC on some situations but worse on other situations. To benefit from quaternion-based $e_2$-norm minimization in QCRC and quaternion-based $e_1$-norm minimization in QSRC, we propose the quaternion-based adaptive representation (QAR) that uses a quaternion-based $e_p$-norm minimization ($1 \le p \le 2$) for color face recognition. To obtain the high dimension correlation information among different color channels, we further propose the high-dimension quaternion-based adaptive representation (HD-QAR). The experimental results demonstrate that the proposed QAR and HD-QAR achieve better recognition rates than QCRC, QSRC and several state-of-the-art methods.



### Lung Nodule Classification by the Combination of Fusion Classifier and Cascaded Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.02198v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.02198v2)
- **Published**: 2017-11-19 06:22:20+00:00
- **Updated**: 2017-12-18 01:37:37+00:00
- **Authors**: Masaharu Sakamoto, Hiroki Nakano, Kun Zhao, Taro Sekiyama
- **Comment**: Draft of ISBI2018. arXiv admin note: text overlap with
  arXiv:1703.00311
- **Journal**: None
- **Summary**: Lung nodule classification is a class imbalanced problem, as nodules are found with much lower frequency than non-nodules. In the class imbalanced problem, conventional classifiers tend to be overwhelmed by the majority class and ignore the minority class. We showed that cascaded convolutional neural networks can classify the nodule candidates precisely for a class imbalanced nodule candidate data set in our previous study. In this paper, we propose Fusion classifier in conjunction with the cascaded convolutional neural network models. To fuse the models, nodule probabilities are calculated by using the convolutional neural network models at first. Then, Fusion classifier is trained and tested by the nodule probabilities. The proposed method achieved the sensitivity of 94.4% and 95.9% at 4 and 8 false positives per scan in Free Receiver Operating Characteristics (FROC) curve analysis, respectively.



### Discriminant Projection Representation-based Classification for Vision Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.01643v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1712.01643v1)
- **Published**: 2017-11-19 06:25:17+00:00
- **Updated**: 2017-11-19 06:25:17+00:00
- **Authors**: Qingxiang Feng, Yicong Zhou
- **Comment**: Accepted by the Thirty-Second AAAI Conference on Artificial
  Intelligence (AAAI-18)
- **Journal**: None
- **Summary**: Representation-based classification methods such as sparse representation-based classification (SRC) and linear regression classification (LRC) have attracted a lot of attentions. In order to obtain the better representation, a novel method called projection representation-based classification (PRC) is proposed for image recognition in this paper. PRC is based on a new mathematical model. This model denotes that the 'ideal projection' of a sample point $x$ on the hyper-space $H$ may be gained by iteratively computing the projection of $x$ on a line of hyper-space $H$ with the proper strategy. Therefore, PRC is able to iteratively approximate the 'ideal representation' of each subject for classification. Moreover, the discriminant PRC (DPRC) is further proposed, which obtains the discriminant information by maximizing the ratio of the between-class reconstruction error over the within-class reconstruction error. Experimental results on five typical databases show that the proposed PRC and DPRC are effective and outperform other state-of-the-art methods on several vision recognition tasks.



### MIT Advanced Vehicle Technology Study: Large-Scale Naturalistic Driving Study of Driver Behavior and Interaction with Automation
- **Arxiv ID**: http://arxiv.org/abs/1711.06976v4
- **DOI**: 10.1109/ACCESS.2019.2926040
- **Categories**: **cs.CY**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1711.06976v4)
- **Published**: 2017-11-19 06:46:21+00:00
- **Updated**: 2019-08-14 11:17:00+00:00
- **Authors**: Lex Fridman, Daniel E. Brown, Michael Glazer, William Angell, Spencer Dodd, Benedikt Jenik, Jack Terwilliger, Aleksandr Patsekin, Julia Kindelsberger, Li Ding, Sean Seaman, Alea Mehler, Andrew Sipperley, Anthony Pettinato, Bobbie Seppelt, Linda Angell, Bruce Mehler, Bryan Reimer
- **Comment**: None
- **Journal**: IEEE Access, vol. 7, pp. 102021-102038, 2019
- **Summary**: For the foreseeble future, human beings will likely remain an integral part of the driving task, monitoring the AI system as it performs anywhere from just over 0% to just under 100% of the driving. The governing objectives of the MIT Autonomous Vehicle Technology (MIT-AVT) study are to (1) undertake large-scale real-world driving data collection that includes high-definition video to fuel the development of deep learning based internal and external perception systems, (2) gain a holistic understanding of how human beings interact with vehicle automation technology by integrating video data with vehicle state data, driver characteristics, mental models, and self-reported experiences with technology, and (3) identify how technology and other factors related to automation adoption and use can be improved in ways that save lives. In pursuing these objectives, we have instrumented 23 Tesla Model S and Model X vehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6 vehicles for both long-term (over a year per driver) and medium term (one month per driver) naturalistic driving data collection. Furthermore, we are continually developing new methods for analysis of the massive-scale dataset collected from the instrumented vehicle fleet. The recorded data streams include IMU, GPS, CAN messages, and high-definition video streams of the driver face, the driver cabin, the forward roadway, and the instrument cluster (on select vehicles). The study is on-going and growing. To date, we have 122 participants, 15,610 days of participation, 511,638 miles, and 7.1 billion video frames. This paper presents the design of the study, the data collection hardware, the processing of the data, and the computer vision algorithms currently being used to extract actionable knowledge from the data.



### Kill Two Birds with One Stone: Weakly-Supervised Neural Network for Image Annotation and Tag Refinement
- **Arxiv ID**: http://arxiv.org/abs/1711.06998v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06998v1)
- **Published**: 2017-11-19 10:47:39+00:00
- **Updated**: 2017-11-19 10:47:39+00:00
- **Authors**: Junjie Zhang, Qi Wu, Jian Zhang, Chunhua Shen, Jianfeng Lu
- **Comment**: AAAI-2018
- **Journal**: None
- **Summary**: The number of social images has exploded by the wide adoption of social networks, and people like to share their comments about them. These comments can be a description of the image, or some objects, attributes, scenes in it, which are normally used as the user-provided tags. However, it is well-known that user-provided tags are incomplete and imprecise to some extent. Directly using them can damage the performance of related applications, such as the image annotation and retrieval. In this paper, we propose to learn an image annotation model and refine the user-provided tags simultaneously in a weakly-supervised manner. The deep neural network is utilized as the image feature learning and backbone annotation model, while visual consistency, semantic dependency, and user-error sparsity are introduced as the constraints at the batch level to alleviate the tag noise. Therefore, our model is highly flexible and stable to handle large-scale image sets. Experimental results on two benchmark datasets indicate that our proposed model achieves the best performance compared to the state-of-the-art methods.



### MicroExpNet: An Extremely Small and Fast Model For Expression Recognition From Face Images
- **Arxiv ID**: http://arxiv.org/abs/1711.07011v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07011v4)
- **Published**: 2017-11-19 12:31:09+00:00
- **Updated**: 2019-12-24 10:44:15+00:00
- **Authors**: İlke Çuğu, Eren Şener, Emre Akbaş
- **Comment**: International Conference on Image Processing Theory, Tools and
  Applications (IPTA) 2019 camera ready version. Codes are available at:
  https://github.com/cuguilke/microexpnet
- **Journal**: None
- **Summary**: This paper is aimed at creating extremely small and fast convolutional neural networks (CNN) for the problem of facial expression recognition (FER) from frontal face images. To this end, we employed the popular knowledge distillation (KD) method and identified two major shortcomings with its use: 1) a fine-grained grid search is needed for tuning the temperature hyperparameter and 2) to find the optimal size-accuracy balance, one needs to search for the final network size (or the compression rate). On the other hand, KD is proved to be useful for model compression for the FER problem, and we discovered that its effects gets more and more significant with the decreasing model size. In addition, we hypothesized that translation invariance achieved using max-pooling layers would not be useful for the FER problem as the expressions are sensitive to small, pixel-wise changes around the eye and the mouth. However, we have found an intriguing improvement on generalization when max-pooling is used. We conducted experiments on two widely-used FER datasets, CK+ and Oulu-CASIA. Our smallest model (MicroExpNet), obtained using knowledge distillation, is less than 1MB in size and works at 1851 frames per second on an Intel i7 CPU. Despite being less accurate than the state-of-the-art, MicroExpNet still provides significant insights for designing a microarchitecture for the FER problem.



### Vision Recognition using Discriminant Sparse Optimization Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.01645v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01645v1)
- **Published**: 2017-11-19 13:39:49+00:00
- **Updated**: 2017-11-19 13:39:49+00:00
- **Authors**: Qingxiang Feng, Yicong Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: To better select the correct training sample and obtain the robust representation of the query sample, this paper proposes a discriminant-based sparse optimization learning model. This learning model integrates discriminant and sparsity together. Based on this model, we then propose a classifier called locality-based discriminant sparse representation (LDSR). Because discriminant can help to increase the difference of samples in different classes and to decrease the difference of samples within the same class, LDSR can obtain better sparse coefficients and constitute a better sparse representation for classification. In order to take advantages of kernel techniques, discriminant and sparsity, we further propose a nonlinear classifier called kernel locality-based discriminant sparse representation (KLDSR). Experiments on several well-known databases prove that the performance of LDSR and KLDSR is better than that of several state-of-the-art methods including deep learning based methods.



### Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1711.07027v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07027v3)
- **Published**: 2017-11-19 15:05:17+00:00
- **Updated**: 2018-05-15 13:13:37+00:00
- **Authors**: Weijian Deng, Liang Zheng, Qixiang Ye, Guoliang Kang, Yi Yang, Jianbin Jiao
- **Comment**: Accepted in CVPR 2018
- **Journal**: None
- **Summary**: Person re-identification (re-ID) models trained on one domain often fail to generalize well to another. In our attempt, we present a "learning via translation" framework. In the baseline, we translate the labeled images from source to target domain in an unsupervised manner. We then train re-ID models with the translated images by supervised methods. Yet, being an essential part of this framework, unsupervised image-image translation suffers from the information loss of source-domain labels during translation.   Our motivation is two-fold. First, for each image, the discriminative cues contained in its ID label should be maintained after translation. Second, given the fact that two domains have entirely different persons, a translated image should be dissimilar to any of the target IDs. To this end, we propose to preserve two types of unsupervised similarities, 1) self-similarity of an image before and after translation, and 2) domain-dissimilarity of a translated source image and a target image. Both constraints are implemented in the similarity preserving generative adversarial network (SPGAN) which consists of an Siamese network and a CycleGAN. Through domain adaptation experiment, we show that images generated by SPGAN are more suitable for domain adaptation and yield consistent and competitive re-ID accuracy on two large-scale datasets.



### DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.07064v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07064v4)
- **Published**: 2017-11-19 19:46:18+00:00
- **Updated**: 2018-04-03 10:28:19+00:00
- **Authors**: Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, Jiri Matas
- **Comment**: CVPR 2018 camera-ready
- **Journal**: None
- **Summary**: We present DeblurGAN, an end-to-end learned method for motion deblurring. The learning is based on a conditional GAN and the content loss . DeblurGAN achieves state-of-the art performance both in the structural similarity measure and visual appearance. The quality of the deblurring model is also evaluated in a novel way on a real-world problem -- object detection on (de-)blurred images. The method is 5 times faster than the closest competitor -- DeepDeblur. We also introduce a novel method for generating synthetic motion blurred images from sharp ones, allowing realistic dataset augmentation.   The model, code and the dataset are available at https://github.com/KupynOrest/DeblurGAN



### Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space
- **Arxiv ID**: http://arxiv.org/abs/1711.07068v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07068v1)
- **Published**: 2017-11-19 20:12:43+00:00
- **Updated**: 2017-11-19 20:12:43+00:00
- **Authors**: Liwei Wang, Alexander G. Schwing, Svetlana Lazebnik
- **Comment**: None
- **Journal**: None
- **Summary**: This paper explores image caption generation using conditional variational auto-encoders (CVAEs). Standard CVAEs with a fixed Gaussian prior yield descriptions with too little variability. Instead, we propose two models that explicitly structure the latent space around $K$ components corresponding to different types of image content, and combine components to create priors for images that contain multiple types of content simultaneously (e.g., several kinds of objects). Our first model uses a Gaussian Mixture model (GMM) prior, while the second one defines a novel Additive Gaussian (AG) prior that linearly combines component means. We show that both models produce captions that are more diverse and more accurate than a strong LSTM baseline or a "vanilla" CVAE with a fixed Gaussian prior, with AG-CVAE showing particular promise.



