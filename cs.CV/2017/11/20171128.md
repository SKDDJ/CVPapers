# Arxiv Papers in cs.CV on 2017-11-28
### CAR-Net: Clairvoyant Attentive Recurrent Network
- **Arxiv ID**: http://arxiv.org/abs/1711.10061v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10061v3)
- **Published**: 2017-11-28 00:22:09+00:00
- **Updated**: 2018-07-31 06:25:03+00:00
- **Authors**: Amir Sadeghian, Ferdinand Legros, Maxime Voisin, Ricky Vesel, Alexandre Alahi, Silvio Savarese
- **Comment**: The 2nd and 3rd authors contributed equally
- **Journal**: ECCV 2018
- **Summary**: We present an interpretable framework for path prediction that leverages dependencies between agents' behaviors and their spatial navigation environment. We exploit two sources of information: the past motion trajectory of the agent of interest and a wide top-view image of the navigation scene. We propose a Clairvoyant Attentive Recurrent Network (CAR-Net) that learns where to look in a large image of the scene when solving the path prediction task. Our method can attend to any area, or combination of areas, within the raw image (e.g., road intersections) when predicting the trajectory of the agent. This allows us to visualize fine-grained semantic elements of navigation scenes that influence the prediction of trajectories. To study the impact of space on agents' trajectories, we build a new dataset made of top-view images of hundreds of scenes (Formula One racing tracks) where agents' behaviors are heavily influenced by known areas in the images (e.g., upcoming turns). CAR-Net successfully attends to these salient regions. Additionally, CAR-Net reaches state-of-the-art accuracy on the standard trajectory forecasting benchmark, Stanford Drone Dataset (SDD). Finally, we show CAR-Net's ability to generalize to unseen scenes.



### WSNet: Compact and Efficient Networks Through Weight Sampling
- **Arxiv ID**: http://arxiv.org/abs/1711.10067v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1711.10067v3)
- **Published**: 2017-11-28 00:43:20+00:00
- **Updated**: 2018-05-22 13:41:19+00:00
- **Authors**: Xiaojie Jin, Yingzhen Yang, Ning Xu, Jianchao Yang, Nebojsa Jojic, Jiashi Feng, Shuicheng Yan
- **Comment**: To appear at ICML 2018
- **Journal**: None
- **Summary**: We present a new approach and a novel architecture, termed WSNet, for learning compact and efficient deep neural networks. Existing approaches conventionally learn full model parameters independently and then compress them via ad hoc processing such as model pruning or filter factorization. Alternatively, WSNet proposes learning model parameters by sampling from a compact set of learnable parameters, which naturally enforces {parameter sharing} throughout the learning process. We demonstrate that such a novel weight sampling approach (and induced WSNet) promotes both weights and computation sharing favorably. By employing this method, we can more efficiently learn much smaller networks with competitive performance compared to baseline networks with equal numbers of convolution filters. Specifically, we consider learning compact and efficient 1D convolutional neural networks for audio classification. Extensive experiments on multiple audio classification datasets verify the effectiveness of WSNet. Combined with weight quantization, the resulted models are up to 180 times smaller and theoretically up to 16 times faster than the well-established baselines, without noticeable performance drop.



### Particle filter re-detection for visual tracking via correlation filters
- **Arxiv ID**: http://arxiv.org/abs/1711.10069v3
- **DOI**: 10.1007/s11042-018-6800-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10069v3)
- **Published**: 2017-11-28 00:44:40+00:00
- **Updated**: 2021-01-11 07:05:45+00:00
- **Authors**: Di Yuan, Xiaohuan Lu, Donghao Li, Yingyi Liang, Xinming Zhang
- **Comment**: Multimedia Tools and Applications
- **Journal**: [J]. Multimedia Tools and Applications, 2019, 78(11): 14277-14301
- **Summary**: Most of the correlation filter based tracking algorithms can achieve good performance and maintain fast computational speed. However, in some complicated tracking scenes, there is a fatal defect that causes the object to be located inaccurately. In order to address this problem, we propose a particle filter redetection based tracking approach for accurate object localization. During the tracking process, the kernelized correlation filter (KCF) based tracker locates the object by relying on the maximum response value of the response map; when the response map becomes ambiguous, the KCF tracking result becomes unreliable. Our method can provide more candidates by particle resampling to detect the object accordingly. Additionally, we give a new object scale evaluation mechanism, which merely considers the differences between the maximum response values in consecutive frames. Extensive experiments on OTB2013 and OTB2015 datasets demonstrate that the proposed tracker performs favorably in relation to the state-of-the-art methods.



### Attentive Generative Adversarial Network for Raindrop Removal from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1711.10098v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10098v4)
- **Published**: 2017-11-28 03:15:55+00:00
- **Updated**: 2018-05-06 06:41:10+00:00
- **Authors**: Rui Qian, Robby T. Tan, Wenhan Yang, Jiajun Su, Jiaying Liu
- **Comment**: CVPR2018 Spotlight
- **Journal**: None
- **Summary**: Raindrops adhered to a glass window or camera lens can severely hamper the visibility of a background scene and degrade an image considerably. In this paper, we address the problem by visually removing raindrops, and thus transforming a raindrop degraded image into a clean one. The problem is intractable, since first the regions occluded by raindrops are not given. Second, the information about the background scene of the occluded regions is completely lost for most part. To resolve the problem, we apply an attentive generative network using adversarial training. Our main idea is to inject visual attention into both the generative and discriminative networks. During the training, our visual attention learns about raindrop regions and their surroundings. Hence, by injecting this information, the generative network will pay more attention to the raindrop regions and the surrounding structures, and the discriminative network will be able to assess the local consistency of the restored regions. This injection of visual attention to both generative and discriminative networks is the main contribution of this paper. Our experiments show the effectiveness of our approach, which outperforms the state of the art methods quantitatively and qualitatively.



### Learning Channel Inter-dependencies at Multiple Scales on Dense Networks for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.10103v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10103v2)
- **Published**: 2017-11-28 03:32:21+00:00
- **Updated**: 2019-03-18 19:35:40+00:00
- **Authors**: Qiangchang Wang, Guodong Guo, Mohammad Iqbal Nouyed
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: We propose a new deep network structure for unconstrained face recognition. The proposed network integrates several key components together in order to characterize complex data distributions, such as in unconstrained face images. Inspired by recent progress in deep networks, we consider some important concepts, including multi-scale feature learning, dense connections of network layers, and weighting different network flows, for building our deep network structure. The developed network is evaluated in unconstrained face matching, showing the capability of learning complex data distributions caused by face images with various qualities.



### 3D-A-Nets: 3D Deep Dense Descriptor for Volumetric Shapes with Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.10108v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10108v1)
- **Published**: 2017-11-28 04:01:20+00:00
- **Updated**: 2017-11-28 04:01:20+00:00
- **Authors**: Mengwei Ren, Liang Niu, Yi Fang
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Recently researchers have been shifting their focus towards learned 3D shape descriptors from hand-craft ones to better address challenging issues of the deformation and structural variation inherently present in 3D objects. 3D geometric data are often transformed to 3D Voxel grids with regular format in order to be better fed to a deep neural net architecture. However, the computational intractability of direct application of 3D convolutional nets to 3D volumetric data severely limits the efficiency (i.e. slow processing) and effectiveness (i.e. unsatisfied accuracy) in processing 3D geometric data. In this paper, powered with a novel design of adversarial networks (3D-A-Nets), we have developed a novel 3D deep dense shape descriptor (3D-DDSD) to address the challenging issues of efficient and effective 3D volumetric data processing. We developed new definition of 2D multilayer dense representation (MDR) of 3D volumetric data to extract concise but geometrically informative shape description and a novel design of adversarial networks that jointly train a set of convolution neural network (CNN), recurrent neural network (RNN) and an adversarial discriminator. More specifically, the generator network produces 3D shape features that encourages the clustering of samples from the same category with correct class label, whereas the discriminator network discourages the clustering by assigning them misleading adversarial class labels. By addressing the challenges posed by the computational inefficiency of direct application of CNN to 3D volumetric data, 3D-A-Nets can learn high-quality 3D-DSDD which demonstrates superior performance on 3D shape classification and retrieval over other state-of-the-art techniques by a great margin.



### Learning to cluster in order to transfer across domains and tasks
- **Arxiv ID**: http://arxiv.org/abs/1711.10125v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.10125v3)
- **Published**: 2017-11-28 04:59:58+00:00
- **Updated**: 2018-03-17 04:42:49+00:00
- **Authors**: Yen-Chang Hsu, Zhaoyang Lv, Zsolt Kira
- **Comment**: ICLR 2018
- **Journal**: None
- **Summary**: This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not. This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets. Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.



### A fatal point concept and a low-sensitivity quantitative measure for traffic safety analytics
- **Arxiv ID**: http://arxiv.org/abs/1711.10131v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.10131v1)
- **Published**: 2017-11-28 05:37:37+00:00
- **Updated**: 2017-11-28 05:37:37+00:00
- **Authors**: Shan Suthaharan
- **Comment**: None
- **Journal**: None
- **Summary**: The variability of the clusters generated by clustering techniques in the domain of latitude and longitude variables of fatal crash data are significantly unpredictable. This unpredictability, caused by the randomness of fatal crash incidents, reduces the accuracy of crash frequency (i.e., counts of fatal crashes per cluster) which is used to measure traffic safety in practice. In this paper, a quantitative measure of traffic safety that is not significantly affected by the aforementioned variability is proposed. It introduces a fatal point -- a segment with the highest frequency of fatality -- concept based on cluster characteristics and detects them by imposing rounding errors to the hundredth decimal place of the longitude. The frequencies of the cluster and the cluster's fatal point are combined to construct a low-sensitive quantitative measure of traffic safety for the cluster. The performance of the proposed measure of traffic safety is then studied by varying the parameter k of k-means clustering with the expectation that other clustering techniques can be adopted in a similar fashion. The 2015 North Carolina fatal crash dataset of Fatality Analysis Reporting System (FARS) is used to evaluate the proposed fatal point concept and perform experimental analysis to determine the effectiveness of the proposed measure. The empirical study shows that the average traffic safety, measured by the proposed quantitative measure over several clusters, is not significantly affected by the variability, compared to that of the standard crash frequency.



### Revisiting hand-crafted feature for action recognition: a set of improved dense trajectories
- **Arxiv ID**: http://arxiv.org/abs/1711.10143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10143v1)
- **Published**: 2017-11-28 06:38:02+00:00
- **Updated**: 2017-11-28 06:38:02+00:00
- **Authors**: Kenji Matsui, Toru Tamaki, Gwladys Auffret, Bisser Raytchev, Kazufumi Kaneda
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: We propose a feature for action recognition called Trajectory-Set (TS), on top of the improved Dense Trajectory (iDT). The TS feature encodes only trajectories around densely sampled interest points, without any appearance features. Experimental results on the UCF50, UCF101, and HMDB51 action datasets demonstrate that TS is comparable to state-of-the-arts, and outperforms many other methods; for HMDB the accuracy of 85.4%, compared to the best accuracy of 80.2% obtained by a deep method. Our code is available on-line at https://github.com/Gauffret/TrajectorySet .



### Recurrent Segmentation for Variable Computational Budgets
- **Arxiv ID**: http://arxiv.org/abs/1711.10151v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10151v2)
- **Published**: 2017-11-28 06:55:19+00:00
- **Updated**: 2018-03-15 00:30:26+00:00
- **Authors**: Lane McIntosh, Niru Maheswaranathan, David Sussillo, Jonathon Shlens
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art systems for semantic image segmentation use feed-forward pipelines with fixed computational costs. Building an image segmentation system that works across a range of computational budgets is challenging and time-intensive as new architectures must be designed and trained for every computational setting. To address this problem we develop a recurrent neural network that successively improves prediction quality with each iteration. Importantly, the RNN may be deployed across a range of computational budgets by merely running the model for a variable number of iterations. We find that this architecture is uniquely suited for efficiently segmenting videos. By exploiting the segmentation of past frames, the RNN can perform video segmentation at similar quality but reduced computational cost compared to state-of-the-art image segmentation methods. When applied to static images in the PASCAL VOC 2012 and Cityscapes segmentation datasets, the RNN traces out a speed-accuracy curve that saturates near the performance of state-of-the-art segmentation methods.



### Restricting Greed in Training of Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1711.10152v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10152v2)
- **Published**: 2017-11-28 06:55:59+00:00
- **Updated**: 2018-09-06 12:37:56+00:00
- **Authors**: Haoxuan You, Zhicheng Jiao, Haojun Xu, Jie Li, Ying Wang, Xinbo Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial network (GAN) has gotten wide re-search interest in the field of deep learning. Variations of GAN have achieved competitive results on specific tasks. However, the stability of training and diversity of generated instances are still worth studying further. Training of GAN can be thought of as a greedy procedure, in which the generative net tries to make the locally optimal choice (minimizing loss function of discriminator) in each iteration. Unfortunately, this often makes generated data resemble only a few modes of real data and rotate between modes. To alleviate these problems, we propose a novel training strategy to restrict greed in training of GAN. With help of our method, the generated samples can cover more instance modes with more stable training process. Evaluating our method on several representative datasets, we demonstrate superiority of improved training strategy on typical GAN models with different distance metrics.



### Deformation estimation of an elastic object by partial observation using a neural network
- **Arxiv ID**: http://arxiv.org/abs/1711.10157v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.10157v1)
- **Published**: 2017-11-28 07:28:48+00:00
- **Updated**: 2017-11-28 07:28:48+00:00
- **Authors**: Utako Yamamoto, Megumi Nakao, Masayuki Ohzeki, Tetsuya Matsuda
- **Comment**: 12 pages, 12 figures, 1 table
- **Journal**: None
- **Summary**: Deformation estimation of elastic object assuming an internal organ is important for the computer navigation of surgery. The aim of this study is to estimate the deformation of an entire three-dimensional elastic object using displacement information of very few observation points. A learning approach with a neural network was introduced to estimate the entire deformation of an object. We applied our method to two elastic objects; a rectangular parallelepiped model, and a human liver model reconstructed from computed tomography data. The average estimation error for the human liver model was 0.041 mm when the object was deformed up to 66.4 mm, from only around 3 % observations. These results indicate that the deformation of an entire elastic object can be estimated with an acceptable level of error from limited observations by applying a trained neural network to a new deformation.



### Guaranteed Outlier Removal for Point Cloud Registration with Correspondences
- **Arxiv ID**: http://arxiv.org/abs/1711.10209v1
- **DOI**: 10.1109/TPAMI.2017.2773482
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10209v1)
- **Published**: 2017-11-28 10:02:18+00:00
- **Updated**: 2017-11-28 10:02:18+00:00
- **Authors**: Álvaro Parra Bustos, Tat-Jun Chin
- **Comment**: TPAMI accepted version (Nov, 2017) 14 pages, 10 figures
- **Journal**: None
- **Summary**: An established approach for 3D point cloud registration is to estimate the registration function from 3D keypoint correspondences. Typically, a robust technique is required to conduct the estimation, since there are false correspondences or outliers. Current 3D keypoint techniques are much less accurate than their 2D counterparts, thus they tend to produce extremely high outlier rates. A large number of putative correspondences must thus be extracted to ensure that sufficient good correspondences are available. Both factors (high outlier rates, large data sizes) however cause existing robust techniques to require very high computational cost. In this paper, we present a novel preprocessing method called \emph{guaranteed outlier removal} for point cloud registration. Our method reduces the input to a smaller set, in a way that any rejected correspondence is guaranteed to not exist in the globally optimal solution. The reduction is performed using purely geometric operations which are deterministic and fast. Our method significantly reduces the population of outliers, such that further optimization can be performed quickly. Further, since only true outliers are removed, the globally optimal solution is preserved. On various synthetic and real data experiments, we demonstrate the effectiveness of our preprocessing method. Demo code is available as supplementary material.



### Multi-stream 3D FCN with Multi-scale Deep Supervision for Multi-modality Isointense Infant Brain MR Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.10212v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10212v2)
- **Published**: 2017-11-28 10:11:41+00:00
- **Updated**: 2019-03-09 12:51:37+00:00
- **Authors**: Guodong Zeng, Guoyan Zheng
- **Comment**: 5 pages, 3 figures, submitted to ISBI 2018
- **Journal**: None
- **Summary**: We present a method to address the challenging problem of segmentation of multi-modality isointense infant brain MR images into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Our method is based on context-guided, multi-stream fully convolutional networks (FCN), which after training, can directly map a whole volumetric data to its volume-wise labels. In order to alleviate the poten-tial gradient vanishing problem during training, we designed multi-scale deep supervision. Furthermore, context infor-mation was used to further improve the performance of our method. Validated on the test data of the MICCAI 2017 Grand Challenge on 6-month infant brain MRI segmentation (iSeg-2017), our method achieved an average Dice Overlap Coefficient of 95.4%, 91.6% and 89.6% for CSF, GM and WM, respectively.



### Tracking for Half an Hour
- **Arxiv ID**: http://arxiv.org/abs/1711.10217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10217v1)
- **Published**: 2017-11-28 10:34:37+00:00
- **Updated**: 2017-11-28 10:34:37+00:00
- **Authors**: Ran Tao, Efstratios Gavves, Arnold W. M. Smeulders
- **Comment**: tech report
- **Journal**: None
- **Summary**: Long-term tracking requires extreme stability to the multitude of model updates and robustness to the disappearance and loss of the target as such will inevitably happen. For motivation, we have taken 10 randomly selected OTB-sequences, doubled each by attaching a reversed version and repeated each double sequence 20 times. On most of these repetitive videos, the best current tracker performs worse on each loop. This illustrates the difference between optimization for short-term versus long-term tracking. In a long-term tracker a combined global and local search strategy is beneficial, allowing for recovery from failures and disappearance. Most importantly, the proposed tracker also employs cautious updating, guided by self-quality assessment. The proposed tracker is still among the best on the 20-sec OTB-videos while achieving state-of-the-art on the 100-sec UAV20L benchmark. On 10 new half-an-hour videos with city bicycling, sport games etc, the proposed tracker outperforms others by a large margin where the 2010 TLD tracker comes second.



### Learning Less is More - 6D Camera Localization via 3D Surface Regression
- **Arxiv ID**: http://arxiv.org/abs/1711.10228v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10228v2)
- **Published**: 2017-11-28 11:11:03+00:00
- **Updated**: 2018-03-27 13:32:47+00:00
- **Authors**: Eric Brachmann, Carsten Rother
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Popular research areas like autonomous driving and augmented reality have renewed the interest in image-based camera localization. In this work, we address the task of predicting the 6D camera pose from a single RGB image in a given 3D environment. With the advent of neural networks, previous works have either learned the entire camera localization process, or multiple components of a camera localization pipeline. Our key contribution is to demonstrate and explain that learning a single component of this pipeline is sufficient. This component is a fully convolutional neural network for densely regressing so-called scene coordinates, defining the correspondence between the input image and the 3D scene space. The neural network is prepended to a new end-to-end trainable pipeline. Our system is efficient, highly accurate, robust in training, and exhibits outstanding generalization capabilities. It exceeds state-of-the-art consistently on indoor and outdoor datasets. Interestingly, our approach surpasses existing techniques even without utilizing a 3D model of the scene during training, since the network is able to discover 3D scene geometry automatically, solely from single-view constraints.



### Differential Generative Adversarial Networks: Synthesizing Non-linear Facial Variations with Limited Number of Training Data
- **Arxiv ID**: http://arxiv.org/abs/1711.10267v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10267v4)
- **Published**: 2017-11-28 13:02:51+00:00
- **Updated**: 2017-12-29 00:53:45+00:00
- **Authors**: Geonmo Gu, Seong Tae Kim, Kihyun Kim, Wissam J. Baddar, Yong Man Ro
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: In face-related applications with a public available dataset, synthesizing non-linear facial variations (e.g., facial expression, head-pose, illumination, etc.) through a generative model is helpful in addressing the lack of training data. In reality, however, there is insufficient data to even train the generative model for face synthesis. In this paper, we propose Differential Generative Adversarial Networks (D-GAN) that can perform photo-realistic face synthesis even when training data is small. Two discriminators are devised to ensure the generator to approximate a face manifold, which can express face changes as it wants. Experimental results demonstrate that the proposed method is robust to the amount of training data and synthesized images are useful to improve the performance of a face expression classifier.



### 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.10275v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10275v1)
- **Published**: 2017-11-28 13:21:58+00:00
- **Updated**: 2017-11-28 13:21:58+00:00
- **Authors**: Benjamin Graham, Martin Engelcke, Laurens van der Maaten
- **Comment**: arXiv admin note: text overlap with arXiv:1706.01307
- **Journal**: None
- **Summary**: Convolutional networks are the de-facto standard for analyzing spatio-temporal data such as images, videos, and 3D shapes. Whilst some of this data is naturally dense (e.g., photos), many other data sources are inherently sparse. Examples include 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard "dense" implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce new sparse convolutional operations that are designed to process spatially-sparse data more efficiently, and use them to develop spatially-sparse convolutional networks. We demonstrate the strong performance of the resulting models, called submanifold sparse convolutional networks (SSCNs), on two tasks involving semantic segmentation of 3D point clouds. In particular, our models outperform all prior state-of-the-art on the test set of a recent semantic segmentation competition.



### Between-class Learning for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1711.10284v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.10284v2)
- **Published**: 2017-11-28 13:31:14+00:00
- **Updated**: 2018-04-08 08:50:09+00:00
- **Authors**: Yuji Tokozume, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: 11 pages, 8 figures, published as a conference paper at CVPR 2018
- **Journal**: None
- **Summary**: In this paper, we propose a novel learning method for image classification called Between-Class learning (BC learning). We generate between-class images by mixing two images belonging to different classes with a random ratio. We then input the mixed image to the model and train the model to output the mixing ratio. BC learning has the ability to impose constraints on the shape of the feature distributions, and thus the generalization ability is improved. BC learning is originally a method developed for sounds, which can be digitally mixed. Mixing two image data does not appear to make sense; however, we argue that because convolutional neural networks have an aspect of treating input data as waveforms, what works on sounds must also work on images. First, we propose a simple mixing method using internal divisions, which surprisingly proves to significantly improve performance. Second, we propose a mixing method that treats the images as waveforms, which leads to a further improvement in performance. As a result, we achieved 19.4% and 2.26% top-1 errors on ImageNet-1K and CIFAR-10, respectively.



### Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1711.10288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10288v1)
- **Published**: 2017-11-28 13:39:10+00:00
- **Updated**: 2017-11-28 13:39:10+00:00
- **Authors**: Pietro Morerio, Jacopo Cavazza, Vittorio Murino
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages on our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains. We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics. Our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion. We provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks.



### Scalable and Compact 3D Action Recognition with Approximated RBF Kernel Machines
- **Arxiv ID**: http://arxiv.org/abs/1711.10290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10290v1)
- **Published**: 2017-11-28 13:46:05+00:00
- **Updated**: 2017-11-28 13:46:05+00:00
- **Authors**: Jacopo Cavazza, Pietro Morerio, Vittorio Murino
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the recent deep learning (DL) revolution, kernel machines still remain powerful methods for action recognition. DL has brought the use of large datasets and this is typically a problem for kernel approaches, which are not scaling up efficiently due to kernel Gram matrices. Nevertheless, kernel methods are still attractive and more generally applicable since they can equally manage different sizes of the datasets, also in cases where DL techniques show some limitations. This work investigates these issues by proposing an explicit approximated representation that, together with a linear model, is an equivalent, yet scalable, implementation of a kernel machine. Our approximation is directly inspired by the exact feature map that is induced by an RBF Gaussian kernel but, unlike the latter, it is finite dimensional and very compact. We justify the soundness of our idea with a theoretical analysis which proves the unbiasedness of the approximation, and provides a vanishing bound for its variance, which is shown to decrease much rapidly than in alternative methods in the literature. In a broad experimental validation, we assess the superiority of our approximation in terms of 1) ease and speed of training, 2) compactness of the model, and 3) improvements with respect to the state-of-the-art performance.



### Camera Style Adaptation for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1711.10295v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10295v2)
- **Published**: 2017-11-28 14:01:30+00:00
- **Updated**: 2018-04-10 12:03:43+00:00
- **Authors**: Zhun Zhong, Liang Zheng, Zhedong Zheng, Shaozi Li, Yi Yang
- **Comment**: To appear in CVPR 2018
- **Journal**: None
- **Summary**: Being a cross-camera retrieval task, person re-identification suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle) adaptation. CamStyle can serve as a data augmentation approach that smooths the camera style disparities. Specifically, with CycleGAN, labeled training images can be style-transferred to each camera, and, along with the original training samples, form the augmented training set. This method, while increasing data diversity against over-fitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few-camera systems in which over-fitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of over-fitting. We also report competitive accuracy compared with the state of the art.



### Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.10305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10305v1)
- **Published**: 2017-11-28 14:24:18+00:00
- **Updated**: 2017-11-28 14:24:18+00:00
- **Authors**: Zhaofan Qiu, Ting Yao, Tao Mei
- **Comment**: ICCV 2017; The codes and model of our P3D ResNet are publicly
  available at: https://github.com/ZhaofanQiu/pseudo-3d-residual-networks
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for image recognition problems. Nevertheless, it is not trivial when utilizing a CNN for learning spatio-temporal video representation. A few studies have shown that performing 3D convolutions is a rewarding approach to capture both spatial and temporal dimensions in videos. However, the development of a very deep 3D CNN from scratch results in expensive computational cost and memory demand. A valid question is why not recycle off-the-shelf 2D networks for a 3D CNN. In this paper, we devise multiple variants of bottleneck building blocks in a residual learning framework by simulating $3\times3\times3$ convolutions with $1\times3\times3$ convolutional filters on spatial domain (equivalent to 2D CNN) plus $3\times1\times1$ convolutions to construct temporal connections on adjacent feature maps in time. Furthermore, we propose a new architecture, named Pseudo-3D Residual Net (P3D ResNet), that exploits all the variants of blocks but composes each in different placement of ResNet, following the philosophy that enhancing structural diversity with going deep could improve the power of neural networks. Our P3D ResNet achieves clear improvements on Sports-1M video classification dataset against 3D CNN and frame-based 2D CNN by 5.3% and 1.8%, respectively. We further examine the generalization performance of video representation produced by our pre-trained P3D ResNet on five different benchmarks and three different tasks, demonstrating superior performances over several state-of-the-art techniques.



### Super-Resolution for Overhead Imagery Using DenseNets and Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.10312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10312v1)
- **Published**: 2017-11-28 14:37:48+00:00
- **Updated**: 2017-11-28 14:37:48+00:00
- **Authors**: Marc Bosch, Christopher M. Gifford, Pedro A. Rodriguez
- **Comment**: 9 pages, 9 figures, WACV 2018 submission
- **Journal**: None
- **Summary**: Recent advances in Generative Adversarial Learning allow for new modalities of image super-resolution by learning low to high resolution mappings. In this paper we present our work using Generative Adversarial Networks (GANs) with applications to overhead and satellite imagery. We have experimented with several state-of-the-art architectures. We propose a GAN-based architecture using densely connected convolutional neural networks (DenseNets) to be able to super-resolve overhead imagery with a factor of up to 8x. We have also investigated resolution limits of these networks. We report results on several publicly available datasets, including SpaceNet data and IARPA Multi-View Stereo Challenge, and compare performance with other state-of-the-art architectures.



### Learning Face Age Progression: A Pyramid Architecture of GANs
- **Arxiv ID**: http://arxiv.org/abs/1711.10352v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10352v4)
- **Published**: 2017-11-28 15:42:46+00:00
- **Updated**: 2019-01-10 05:33:29+00:00
- **Authors**: Hongyu Yang, Di Huang, Yunhong Wang, Anil K. Jain
- **Comment**: CVPR 2018. V4 and V2 are the same, i.e. the conference version; V3 is
  a related but different work, which is mistakenly submitted and will be
  submitted as a new arXiv paper
- **Journal**: None
- **Summary**: The two underlying requirements of face age progression, i.e. aging accuracy and identity permanence, are not well studied in the literature. In this paper, we present a novel generative adversarial network based approach. It separately models the constraints for the intrinsic subject-specific characteristics and the age-specific facial changes with respect to the elapsed time, ensuring that the generated faces present desired aging effects while simultaneously keeping personalized properties stable. Further, to generate more lifelike facial details, high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales, which simulates the aging effects in a finer manner. The proposed method is applicable to diverse face samples in the presence of variations in pose, expression, makeup, etc., and remarkably vivid aging effects are achieved. Both visual fidelity and quantitative evaluations show that the approach advances the state-of-the-art.



### Learning to Segment Every Thing
- **Arxiv ID**: http://arxiv.org/abs/1711.10370v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10370v2)
- **Published**: 2017-11-28 16:05:24+00:00
- **Updated**: 2018-03-27 13:45:52+00:00
- **Authors**: Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, Ross Girshick
- **Comment**: None
- **Journal**: None
- **Summary**: Most methods for object instance segmentation require all training examples to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to ~100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world.



### A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking
- **Arxiv ID**: http://arxiv.org/abs/1711.10378v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10378v2)
- **Published**: 2017-11-28 16:25:26+00:00
- **Updated**: 2018-04-02 11:38:23+00:00
- **Authors**: M. Saquib Sarfraz, Arne Schumann, Andreas Eberle, Rainer Stiefelhagen
- **Comment**: CVPR 2018: v2 (fixes, added new results on PRW dataset)
- **Journal**: None
- **Summary**: Person re identification is a challenging retrieval task that requires matching a person's acquired image across non overlapping camera views. In this paper we propose an effective approach that incorporates both the fine and coarse pose information of the person to learn a discriminative embedding. In contrast to the recent direction of explicitly modeling body parts or correcting for misalignment based on these, we show that a rather straightforward inclusion of acquired camera view and/or the detected joint locations into a convolutional neural network helps to learn a very effective representation. To increase retrieval performance, re-ranking techniques based on computed distances have recently gained much attention. We propose a new unsupervised and automatic re-ranking framework that achieves state-of-the-art re-ranking performance. We show that in contrast to the current state-of-the-art re-ranking methods our approach does not require to compute new rank lists for each image pair (e.g., based on reciprocal neighbors) and performs well by using simple direct rank list based comparison or even by just using the already computed euclidean distances between the images. We show that both our learned representation and our re-ranking method achieve state-of-the-art performance on a number of challenging surveillance image and video datasets.   The code is available online at: https://github.com/pse-ecn/pose-sensitive-embedding



### Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram Completion
- **Arxiv ID**: http://arxiv.org/abs/1711.10388v3
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.10388v3)
- **Published**: 2017-11-28 16:37:14+00:00
- **Updated**: 2018-07-11 17:20:08+00:00
- **Authors**: Rushil Anirudh, Hyojin Kim, Jayaraman J. Thiagarajan, K. Aditya Mohan, Kyle Champley, Timo Bremer
- **Comment**: Spotlight presentation at CVPR 2018
- **Journal**: None
- **Summary**: Computed Tomography (CT) reconstruction is a fundamental component to a wide variety of applications ranging from security, to healthcare. The classical techniques require measuring projections, called sinograms, from a full 180$^\circ$ view of the object. This is impractical in a limited angle scenario, when the viewing angle is less than 180$^\circ$, which can occur due to different factors including restrictions on scanning time, limited flexibility of scanner rotation, etc. The sinograms obtained as a result, cause existing techniques to produce highly artifact-laden reconstructions. In this paper, we propose to address this problem through implicit sinogram completion, on a challenging real world dataset containing scans of common checked-in luggage. We propose a system, consisting of 1D and 2D convolutional neural networks, that operates on a limited angle sinogram to directly produce the best estimate of a reconstruction. Next, we use the x-ray transform on this reconstruction to obtain a "completed" sinogram, as if it came from a full 180$^\circ$ measurement. We feed this to standard analytical and iterative reconstruction techniques to obtain the final reconstruction. We show with extensive experimentation that this combined strategy outperforms many competitive baselines. We also propose a measure of confidence for the reconstruction that enables a practitioner to gauge the reliability of a prediction made by our network. We show that this measure is a strong indicator of quality as measured by the PSNR, while not requiring ground truth at test time. Finally, using a segmentation experiment, we show that our reconstruction preserves the 3D structure of objects effectively.



### Exposing Computer Generated Images by Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.10394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10394v1)
- **Published**: 2017-11-28 16:51:22+00:00
- **Updated**: 2017-11-28 16:51:22+00:00
- **Authors**: Edmar R. S. de Rezende, Guilherme C. S. Ruppert, Antonio Theophilo, Tiago Carvalho
- **Comment**: None
- **Journal**: None
- **Summary**: The recent computer graphics developments have upraised the quality of the generated digital content, astonishing the most skeptical viewer. Games and movies have taken advantage of this fact but, at the same time, these advances have brought serious negative impacts like the ones yielded by fakeimages produced with malicious intents. Digital artists can compose artificial images capable of deceiving the great majority of people, turning this into a very dangerous weapon in a timespan currently know as Fake News/Post-Truth" Era. In this work, we propose a new approach for dealing with the problem of detecting computer generated images, through the application of deep convolutional networks and transfer learning techniques. We start from Residual Networks and develop different models adapted to the binary problem of identifying if an image was or not computer generated. Differently from the current state-of-the-art approaches, we don't rely on hand-crafted features, but provide to the model the raw pixel information, achieving the same 0.97 of state-of-the-art methods with two main advantages: our methods show more stable results (depicted by lower variance) and eliminate the laborious and manual step of specialized features extraction and selection.



### DOTA: A Large-scale Dataset for Object Detection in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/1711.10398v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10398v3)
- **Published**: 2017-11-28 16:52:44+00:00
- **Updated**: 2019-05-19 08:07:35+00:00
- **Authors**: Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, Liangpei Zhang
- **Comment**: Accepted to CVPR 2018
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition, 2018
- **Summary**: Object detection is an important and challenging problem in computer vision. Although the past decade has witnessed major advances in object detection in natural scenes, such successes have been slow to aerial imagery, not only because of the huge variation in the scale, orientation and shape of the object instances on the earth's surface, but also due to the scarcity of well-annotated datasets of objects in aerial scenes. To advance object detection research in Earth Vision, also known as Earth Observation and Remote Sensing, we introduce a large-scale Dataset for Object deTection in Aerial images (DOTA). To this end, we collect $2806$ aerial images from different sensors and platforms. Each image is of the size about 4000-by-4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using $15$ common object categories. The fully annotated DOTA images contains $188,282$ instances, each of which is labeled by an arbitrary (8 d.o.f.) quadrilateral To build a baseline for object detection in Earth Vision, we evaluate state-of-the-art object detection algorithms on DOTA. Experiments demonstrate that DOTA well represents real Earth Vision applications and are quite challenging.



### An Adversarial Neuro-Tensorial Approach For Learning Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/1711.10402v2
- **DOI**: 10.1007/s11263-019-01163-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10402v2)
- **Published**: 2017-11-28 16:57:21+00:00
- **Updated**: 2018-02-24 22:46:45+00:00
- **Authors**: Mengjiao Wang, Zhixin Shu, Shiyang Cheng, Yannis Panagakis, Dimitris Samaras, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: International Journal of Computer Vision, 2019
- **Summary**: Several factors contribute to the appearance of an object in a visual scene, including pose, illumination, and deformation, among others. Each factor accounts for a source of variability in the data, while the multiplicative interactions of these factors emulate the entangled variability, giving rise to the rich structure of visual object appearance. Disentangling such unobserved factors from visual data is a challenging task, especially when the data have been captured in uncontrolled recording conditions (also referred to as "in-the-wild") and label information is not available.   In this paper, we propose the first unsupervised deep learning method (with pseudo-supervision) for disentangling multiple latent factors of variation in face images captured in-the-wild. To this end, we propose a deep latent variable model, where the multiplicative interactions of multiple latent factors of variation are explicitly modelled by means of multilinear (tensor) structure. We demonstrate that the proposed approach indeed learns disentangled representations of facial expressions and pose, which can be used in various applications, including face editing, as well as 3D face reconstruction and classification of facial expression, identity and pose.



### Entropy-difference based stereo error detection
- **Arxiv ID**: http://arxiv.org/abs/1711.10412v1
- **DOI**: 10.1109/IVMSPW.2016.7528177
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10412v1)
- **Published**: 2017-11-28 17:11:57+00:00
- **Updated**: 2017-11-28 17:11:57+00:00
- **Authors**: Subhayan Mukherjee, Irene Cheng, Ram Mohana Reddy Guddeti, Anup Basu
- **Comment**: None
- **Journal**: None
- **Summary**: Stereo depth estimation is error-prone; hence, effective error detection methods are desirable. Most such existing methods depend on characteristics of the stereo matching cost curve, making them unduly dependent on functional details of the matching algorithm. As a remedy, we propose a novel error detection approach based solely on the input image and its depth map. Our assumption is that, entropy of any point on an image will be significantly higher than the entropy of its corresponding point on the image's depth map. In this paper, we propose a confidence measure, Entropy-Difference (ED) for stereo depth estimates and a binary classification method to identify incorrect depths. Experiments on the Middlebury dataset show the effectiveness of our method. Our proposed stereo confidence measure outperforms 17 existing measures in all aspects except occlusion detection. Established metrics such as precision, accuracy, recall, and area-under-curve are used to demonstrate the effectiveness of our method.



### DFUNet: Convolutional Neural Networks for Diabetic Foot Ulcer Classification
- **Arxiv ID**: http://arxiv.org/abs/1711.10448v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10448v2)
- **Published**: 2017-11-28 18:21:22+00:00
- **Updated**: 2017-12-10 15:14:49+00:00
- **Authors**: Manu Goyal, Neil D. Reeves, Adrian K. Davison, Satyan Rajbhandari, Jennifer Spragg, Moi Hoon Yap
- **Comment**: Submitted to IEEE Access Journal
- **Journal**: None
- **Summary**: Globally, in 2016, one out of eleven adults suffered from Diabetes Mellitus. Diabetic Foot Ulcers (DFU) are a major complication of this disease, which if not managed properly can lead to amputation. Current clinical approaches to DFU treatment rely on patient and clinician vigilance, which has significant limitations such as the high cost involved in the diagnosis, treatment and lengthy care of the DFU. We collected an extensive dataset of foot images, which contain DFU from different patients. In this paper, we have proposed the use of traditional computer vision features for detecting foot ulcers among diabetic patients, which represent a cost-effective, remote and convenient healthcare solution. Furthermore, we used Convolutional Neural Networks (CNNs) for the first time in DFU classification. We have proposed a novel convolutional neural network architecture, DFUNet, with better feature extraction to identify the feature differences between healthy skin and the DFU. Using 10-fold cross-validation, DFUNet achieved an AUC score of 0.962. This outperformed both the machine learning and deep learning classifiers we have tested. Here we present the development of a novel and highly sensitive DFUNet for objectively detecting the presence of DFUs. This novel approach has the potential to deliver a paradigm shift in diabetic foot care.



### Multi-class Semantic Segmentation of Skin Lesions via Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.10449v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10449v2)
- **Published**: 2017-11-28 18:24:15+00:00
- **Updated**: 2020-03-13 20:34:31+00:00
- **Authors**: Manu Goyal, Moi Hoon Yap, Saeed Hassanpour
- **Comment**: Comp2clinic workshop at Biostec 2020
- **Journal**: None
- **Summary**: Melanoma is clinically difficult to distinguish from common benign skin lesions, particularly melanocytic naevus and seborrhoeic keratosis. The dermoscopic appearance of these lesions has huge intra-class variations and high inter-class visual similarities. Most current research is focusing on single-class segmentation irrespective of classes of skin lesions. In this work, we evaluate the performance of deep learning on multi-class segmentation of ISIC-2017 challenge dataset, which consists of 2,750 dermoscopic images. We propose an end-to-end solution using fully convolutional networks (FCNs) for multi-class semantic segmentation to automatically segment the melanoma, seborrhoeic keratosis and naevus. To improve the performance of FCNs, transfer learning and a hybrid loss function are used. We evaluate the performance of the deep learning segmentation methods for multi-class segmentation and lesion diagnosis (with post-processing method) on the testing set of the ISIC-2017 challenge dataset. The results showed that the two-tier level transfer learning FCN-8s achieved the overall best result with \textit{Dice} score of 78.5% in a naevus category, 65.3% in melanoma, and 55.7% in seborrhoeic keratosis in multi-class segmentation and Accuracy of 84.62% for recognition of melanoma in lesion diagnosis.



### AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.10485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10485v1)
- **Published**: 2017-11-28 18:59:50+00:00
- **Updated**: 2017-11-28 18:59:50+00:00
- **Authors**: Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize fine-grained details at different subregions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the first time shows that the layered attentional GAN is able to automatically select the condition at the word level for generating different parts of the image.



### Highlighting objects of interest in an image by integrating saliency and depth
- **Arxiv ID**: http://arxiv.org/abs/1711.10515v1
- **DOI**: 10.1109/ICIP.2016.7532308
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10515v1)
- **Published**: 2017-11-28 19:12:49+00:00
- **Updated**: 2017-11-28 19:12:49+00:00
- **Authors**: Subhayan Mukherjee, Irene Cheng, Anup Basu
- **Comment**: None
- **Journal**: None
- **Summary**: Stereo images have been captured primarily for 3D reconstruction in the past. However, the depth information acquired from stereo can also be used along with saliency to highlight certain objects in a scene. This approach can be used to make still images more interesting to look at, and highlight objects of interest in the scene. We introduce this novel direction in this paper, and discuss the theoretical framework behind the approach. Even though we use depth from stereo in this work, our approach is applicable to depth data acquired from any sensor modality. Experimental results on both indoor and outdoor scenes demonstrate the benefits of our algorithm.



### Learning from Longitudinal Face Demonstration - Where Tractable Deep Modeling Meets Inverse Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.10520v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10520v4)
- **Published**: 2017-11-28 19:24:20+00:00
- **Updated**: 2019-02-02 17:22:16+00:00
- **Authors**: Chi Nhan Duong, Kha Gia Quach, Khoa Luu, T. Hoang Ngan Le, Marios Savvides, Tien D. Bui
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel Subject-dependent Deep Aging Path (SDAP), which inherits the merits of both Generative Probabilistic Modeling and Inverse Reinforcement Learning to model the facial structures and the longitudinal face aging process of a given subject. The proposed SDAP is optimized using tractable log-likelihood objective functions with Convolutional Neural Networks (CNNs) based deep feature extraction. Instead of applying a fixed aging development path for all input faces and subjects, SDAP is able to provide the most appropriate aging development path for individual subject that optimizes the reward aging formulation. Unlike previous methods that can take only one image as the input, SDAP further allows multiple images as inputs, i.e. all information of a subject at either the same or different ages, to produce the optimal aging path for the given subject. Finally, SDAP allows efficiently synthesizing in-the-wild aging faces. The proposed model is experimented in both tasks of face aging synthesis and cross-age face verification. The experimental results consistently show SDAP achieves the state-of-the-art performance on numerous face aging databases, i.e. FG-NET, MORPH, AginG Faces in the Wild (AGFW), and Cross-Age Celebrity Dataset (CACD). Furthermore, we also evaluate the performance of SDAP on large-scale Megaface challenge to demonstrate the advantages of the proposed solution.



### A Recursive Bayesian Approach To Describe Retinal Vasculature Geometry
- **Arxiv ID**: http://arxiv.org/abs/1711.10521v1
- **DOI**: 10.1016/j.patcog.2018.10.017
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1711.10521v1)
- **Published**: 2017-11-28 19:28:27+00:00
- **Updated**: 2017-11-28 19:28:27+00:00
- **Authors**: Fatmatulzehra Uslu, Anil Anthony Bharath
- **Comment**: 26 pages,13 figures, journal paper
- **Journal**: None
- **Summary**: Demographic studies suggest that changes in the retinal vasculature geometry, especially in vessel width, are associated with the incidence or progression of eye-related or systemic diseases. To date, the main information source for width estimation from fundus images has been the intensity profile between vessel edges. However, there are many factors affecting the intensity profile: pathologies, the central light reflex and local illumination levels, to name a few. In this study, we introduce three information sources for width estimation. These are the probability profiles of vessel interior, centreline and edge locations generated by a deep network. The probability profiles provide direct access to vessel geometry and are used in the likelihood calculation for a Bayesian method, particle filtering. We also introduce a geometric model which can handle non-ideal conditions of the probability profiles. Our experiments conducted on the REVIEW dataset yielded consistent estimates of vessel width, even in cases when one of the vessel edges is difficult to identify. Moreover, our results suggest that the method is better than human observers at locating edges of low contrast vessels.



### Deep Lesion Graphs in the Wild: Relationship Learning and Organization of Significant Radiology Image Findings in a Diverse Large-scale Lesion Database
- **Arxiv ID**: http://arxiv.org/abs/1711.10535v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10535v3)
- **Published**: 2017-11-28 20:06:10+00:00
- **Updated**: 2018-07-28 20:51:09+00:00
- **Authors**: Ke Yan, Xiaosong Wang, Le Lu, Ling Zhang, Adam Harrison, Mohammadhad Bagheri, Ronald Summers
- **Comment**: Accepted by CVPR2018. DeepLesion url added
- **Journal**: None
- **Summary**: Radiologists in their daily work routinely find and annotate significant abnormalities on a large number of radiology images. Such abnormalities, or lesions, have collected over years and stored in hospitals' picture archiving and communication systems. However, they are basically unsorted and lack semantic annotations like type and location. In this paper, we aim to organize and explore them by learning a deep feature representation for each lesion. A large-scale and comprehensive dataset, DeepLesion, is introduced for this task. DeepLesion contains bounding boxes and size measurements of over 32K lesions. To model their similarity relationship, we leverage multiple supervision information including types, self-supervised location coordinates and sizes. They require little manual annotation effort but describe useful attributes of the lesions. Then, a triplet network is utilized to learn lesion embeddings with a sequential sampling strategy to depict their hierarchical similarity structure. Experiments show promising qualitative and quantitative results on lesion retrieval, clustering, and classification. The learned embeddings can be further employed to build a lesion graph for various clinically useful applications. We propose algorithms for intra-patient lesion matching and missing annotation mining. Experimental results validate their effectiveness.



### FearNet: Brain-Inspired Model for Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.10563v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.10563v2)
- **Published**: 2017-11-28 21:26:15+00:00
- **Updated**: 2018-02-23 20:32:27+00:00
- **Authors**: Ronald Kemker, Christopher Kanan
- **Comment**: To appear in ICLR 2018
- **Journal**: None
- **Summary**: Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall. FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.



### Image Registration Techniques: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1712.07540v1
- **DOI**: 10.17605/OSF.IO/RV65C
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1712.07540v1)
- **Published**: 2017-11-28 21:44:28+00:00
- **Updated**: 2017-11-28 21:44:28+00:00
- **Authors**: Sayan Nag
- **Comment**: None
- **Journal**: None
- **Summary**: Image Registration is the process of aligning two or more images of the same scene with reference to a particular image. The images are captured from various sensors at different times and at multiple view-points. Thus to get a better picture of any change of a scene or object over a considerable period of time image registration is important. Image registration finds application in medical sciences, remote sensing and in computer vision. This paper presents a detailed review of several approaches which are classified accordingly along with their contributions and drawbacks. The main steps of an image registration procedure are also discussed. Different performance measures are presented that determine the registration quality and accuracy. The scope for the future research are presented as well.



### Deep learning analysis of breast MRIs for prediction of occult invasive disease in ductal carcinoma in situ
- **Arxiv ID**: http://arxiv.org/abs/1711.10577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10577v1)
- **Published**: 2017-11-28 21:52:49+00:00
- **Updated**: 2017-11-28 21:52:49+00:00
- **Authors**: Zhe Zhu, Michael Harowicz, Jun Zhang, Ashirbani Saha, Lars J. Grimm, E. Shelley Hwang, Maciej A. Mazurowski
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To determine whether deep learning-based algorithms applied to breast MR images can aid in the prediction of occult invasive disease following the di- agnosis of ductal carcinoma in situ (DCIS) by core needle biopsy. Material and Methods: In this institutional review board-approved study, we analyzed dynamic contrast-enhanced fat-saturated T1-weighted MRI sequences of 131 patients at our institution with a core needle biopsy-confirmed diagnosis of DCIS. The patients had no preoperative therapy before breast MRI and no prior history of breast cancer. We explored two different deep learning approaches to predict whether there was a hidden (occult) invasive component in the analyzed tumors that was ultimately detected at surgical excision. In the first approach, we adopted the transfer learning strategy, in which a network pre-trained on a large dataset of natural images is fine-tuned with our DCIS images. Specifically, we used the GoogleNet model pre-trained on the ImageNet dataset. In the second approach, we used a pre-trained network to extract deep features, and a support vector machine (SVM) that utilizes these features to predict the upstaging of the DCIS. We used 10-fold cross validation and the area under the ROC curve (AUC) to estimate the performance of the predictive models. Results: The best classification performance was obtained using the deep features approach with GoogleNet model pre-trained on ImageNet as the feature extractor and a polynomial kernel SVM used as the classifier (AUC = 0.70, 95% CI: 0.58- 0.79). For the transfer learning based approach, the highest AUC obtained was 0.53 (95% CI: 0.41-0.62). Conclusion: Convolutional neural networks could potentially be used to identify occult invasive disease in patients diagnosed with DCIS at the initial core needle biopsy.



