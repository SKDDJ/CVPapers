# Arxiv Papers in cs.CV on 2017-11-27
### Depth Map Completion by Jointly Exploiting Blurry Color Images and Sparse Depth Maps
- **Arxiv ID**: http://arxiv.org/abs/1711.09501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09501v1)
- **Published**: 2017-11-27 01:25:43+00:00
- **Updated**: 2017-11-27 01:25:43+00:00
- **Authors**: Liyuan Pan, Yuchao Dai, Miaomiao Liu, Fatih Porikli
- **Comment**: Accepted by WACV 2018
- **Journal**: None
- **Summary**: We aim at predicting a complete and high-resolution depth map from incomplete, sparse and noisy depth measurements. Existing methods handle this problem either by exploiting various regularizations on the depth maps directly or resorting to learning based methods. When the corresponding color images are available, the correlation between the depth maps and the color images are used to improve the completion performance, assuming the color images are clean and sharp. However, in real world dynamic scenes, color images are often blurry due to the camera motion and the moving objects in the scene. In this paper, we propose to tackle the problem of depth map completion by jointly exploiting the blurry color image sequences and the sparse depth map measurements, and present an energy minimization based formulation to simultaneously complete the depth maps, estimate the scene flow and deblur the color images. Our experimental evaluations on both outdoor and indoor scenarios demonstrate the state-of-the-art performance of our approach.



### Discriminative Learning of Open-Vocabulary Object Retrieval and Localization by Negative Phrase Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.09509v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09509v2)
- **Published**: 2017-11-27 02:25:34+00:00
- **Updated**: 2018-09-04 06:50:38+00:00
- **Authors**: Ryota Hinami, Shin'ichi Satoh
- **Comment**: Accepted to EMNLP 2018
- **Journal**: None
- **Summary**: Thanks to the success of object detection technology, we can retrieve objects of the specified classes even from huge image collections. However, the current state-of-the-art object detectors (such as Faster R-CNN) can only handle pre-specified classes. In addition, large amounts of positive and negative visual samples are required for training. In this paper, we address the problem of open-vocabulary object retrieval and localization, where the target object is specified by a textual query (e.g., a word or phrase). We first propose Query-Adaptive R-CNN, a simple extension of Faster R-CNN adapted to open-vocabulary queries, by transforming the text embedding vector into an object classifier and localization regressor. Then, for discriminative training, we then propose negative phrase augmentation (NPA) to mine hard negative samples which are visually similar to the query and at the same time semantically mutually exclusive of the query. The proposed method can retrieve and localize objects specified by a textual query from one million images in only 0.5 seconds with high precision.



### Structure propagation for zero-shot learning
- **Arxiv ID**: http://arxiv.org/abs/1711.09513v1
- **DOI**: 10.1007/978-3-030-03338-5_39
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09513v1)
- **Published**: 2017-11-27 02:51:58+00:00
- **Updated**: 2017-11-27 02:51:58+00:00
- **Authors**: Guangfeng Lin, Yajun Chen, Fan Zhao
- **Comment**: None
- **Journal**: Chinese Conference on Pattern Recognition and Computer Vision
  (PRCV) 2018
- **Summary**: The key of zero-shot learning (ZSL) is how to find the information transfer model for bridging the gap between images and semantic information (texts or attributes). Existing ZSL methods usually construct the compatibility function between images and class labels with the consideration of the relevance on the semantic classes (the manifold structure of semantic classes). However, the relationship of image classes (the manifold structure of image classes) is also very important for the compatibility model construction. It is difficult to capture the relationship among image classes due to unseen classes, so that the manifold structure of image classes often is ignored in ZSL. To complement each other between the manifold structure of image classes and that of semantic classes information, we propose structure propagation (SP) for improving the performance of ZSL for classification. SP can jointly consider the manifold structure of image classes and that of semantic classes for approximating to the intrinsic structure of object classes. Moreover, the SP can describe the constrain condition between the compatibility function and these manifold structures for balancing the influence of the structure propagation iteration. The SP solution provides not only unseen class labels but also the relationship of two manifold structures that encode the positive transfer in structure propagation. Experimental results demonstrate that SP can attain the promising results on the AwA, CUB, Dogs and SUN databases.



### DeepDeblur: Fast one-step blurry face images restoration
- **Arxiv ID**: http://arxiv.org/abs/1711.09515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09515v1)
- **Published**: 2017-11-27 02:52:38+00:00
- **Updated**: 2017-11-27 02:52:38+00:00
- **Authors**: Lingxiao Wang, Yali Li, Shengjin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a very fast and effective one-step restoring method for blurry face images. In the last decades, many blind deblurring algorithms have been proposed to restore latent sharp images. However, these algorithms run slowly because of involving two steps: kernel estimation and following non-blind deconvolution or latent image estimation. Also they cannot handle face images in small size. Our proposed method restores sharp face images directly in one step using Convolutional Neural Network. Unlike previous deep learning involved methods that can only handle a single blur kernel at one time, our network is trained on totally random and numerous training sample pairs to deal with the variances due to different blur kernels in practice. A smoothness regularization as well as a facial regularization are added to keep facial identity information which is the key to face image applications. Comprehensive experiments demonstrate that our proposed method can handle various blur kenels and achieve state-of-the-art results for small size blurry face images restoration. Moreover, the proposed method shows significant improvement in face recognition accuracy along with increasing running speed by more than 100 times.



### Dynamic Graph Generation Network: Generating Relational Knowledge from Diagrams
- **Arxiv ID**: http://arxiv.org/abs/1711.09528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09528v1)
- **Published**: 2017-11-27 04:32:21+00:00
- **Updated**: 2017-11-27 04:32:21+00:00
- **Authors**: Daesik Kim, Youngjoon Yoo, Jeesoo Kim, Sangkuk Lee, Nojun Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we introduce a new algorithm for analyzing a diagram, which contains visual and textual information in an abstract and integrated way. Whereas diagrams contain richer information compared with individual image-based or language-based data, proper solutions for automatically understanding them have not been proposed due to their innate characteristics of multi-modality and arbitrariness of layouts. To tackle this problem, we propose a unified diagram-parsing network for generating knowledge from diagrams based on an object detector and a recurrent neural network designed for a graphical structure. Specifically, we propose a dynamic graph-generation network that is based on dynamic memory and graph theory. We explore the dynamics of information in a diagram with activation of gates in gated recurrent unit (GRU) cells. On publicly available diagram datasets, our model demonstrates a state-of-the-art result that outperforms other baselines. Moreover, further experiments on question answering shows potentials of the proposed method for various applications.



### Pulsar Candidate Identification with Artificial Intelligence Techniques
- **Arxiv ID**: http://arxiv.org/abs/1711.10339v2
- **DOI**: None
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.10339v2)
- **Published**: 2017-11-27 05:14:19+00:00
- **Updated**: 2019-10-23 05:07:55+00:00
- **Authors**: Ping Guo, Fuqing Duan, Pei Wang, Yao Yao, Qian Yin, Xin Xin
- **Comment**: The title may be changed
- **Journal**: None
- **Summary**: Discovering pulsars is a significant and meaningful research topic in the field of radio astronomy. With the advent of astronomical instruments such as he Five-hundred-meter Aperture Spherical Telescope (FAST) in China, data volumes and data rates are exponentially growing. This fact necessitates a focus on artificial intelligence (AI) technologies that can perform the automatic pulsar candidate identification to mine large astronomical data sets. Automatic pulsar candidate identification can be considered as a task of determining potential candidates for further investigation and eliminating noises of radio frequency interferences or other non-pulsar signals. It is very hard to raise the performance of DCNN-based pulsar identification because the limited training samples restrict network structure to be designed deep enough for learning good features as well as the crucial class imbalance problem due to very limited number of real pulsar samples. To address these problems, we proposed a framework which combines deep convolution generative adversarial network (DCGAN) with support vector machine (SVM) to deal with imbalance class problem and to improve pulsar identification accuracy. DCGAN is used as sample generation and feature learning model, and SVM is adopted as the classifier for predicting candidate's labels in the inference stage. The proposed framework is a novel technique which not only can solve imbalance class problem but also can learn discriminative feature representations of pulsar candidates instead of computing hand-crafted features in preprocessing steps too, which makes it more accurate for automatic pulsar candidate selection. Experiments on two pulsar datasets verify the effectiveness and efficiency of our proposed method.



### Hierarchical Spatial-aware Siamese Network for Thermal Infrared Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1711.09539v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09539v2)
- **Published**: 2017-11-27 05:15:59+00:00
- **Updated**: 2018-11-27 09:44:40+00:00
- **Authors**: Xin Li, Qiao Liu, Nana Fan, Zhenyu He, Hongzhi Wang
- **Comment**: 20 pages, 7 figures
- **Journal**: None
- **Summary**: Most thermal infrared (TIR) tracking methods are discriminative, treating the tracking problem as a classification task. However, the objective of the classifier (label prediction) is not coupled to the objective of the tracker (location estimation). The classification task focuses on the between-class difference of the arbitrary objects, while the tracking task mainly deals with the within-class difference of the same objects. In this paper, we cast the TIR tracking problem as a similarity verification task, which is coupled well to the objective of the tracking task. We propose a TIR tracker via a Hierarchical Spatial-aware Siamese Convolutional Neural Network (CNN), named HSSNet. To obtain both spatial and semantic features of the TIR object, we design a Siamese CNN that coalesces the multiple hierarchical convolutional layers. Then, we propose a spatial-aware network to enhance the discriminative ability of the coalesced hierarchical feature. Subsequently, we train this network end to end on a large visible video detection dataset to learn the similarity between paired objects before we transfer the network into the TIR domain. Next, this pre-trained Siamese network is used to evaluate the similarity between the target template and target candidates. Finally, we locate the candidate that is most similar to the tracked target. Extensive experimental results on the benchmarks VOT-TIR 2015 and VOT-TIR 2016 show that our proposed method achieves favourable performance compared to the state-of-the-art methods.



### Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1711.09550v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.09550v1)
- **Published**: 2017-11-27 06:16:14+00:00
- **Updated**: 2017-11-27 06:16:14+00:00
- **Authors**: Xiang Long, Chuang Gan, Gerard de Melo, Jiajun Wu, Xiao Liu, Shilei Wen
- **Comment**: The backbone of the winner solution at ActivityNet Kinetics Challenge
  2017
- **Journal**: None
- **Summary**: Recently, substantial research effort has focused on how to apply CNNs or RNNs to better extract temporal patterns from videos, so as to improve the accuracy of video classification. In this paper, however, we show that temporal information, especially longer-term patterns, may not be necessary to achieve competitive results on common video classification datasets. We investigate the potential of a purely attention based local feature integration. Accounting for the characteristics of such features in video classification, we propose a local feature integration framework based on attention clusters, and introduce a shifting operation to capture more diverse signals. We carefully analyze and compare the effect of different attention mechanisms, cluster sizes, and the use of the shifting operation, and also investigate the combination of attention clusters for multimodal integration. We demonstrate the effectiveness of our framework on three real-world video classification datasets. Our model achieves competitive results across all of these. In particular, on the large-scale Kinetics dataset, our framework obtains an excellent single model accuracy of 79.4% in terms of the top-1 and 94.0% in terms of the top-5 accuracy on the validation set. The attention clusters are the backbone of our winner solution at ActivityNet Kinetics Challenge 2017. Code and models will be released soon.



### Accessible Melanoma Detection using Smartphones and Mobile Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1711.09553v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09553v2)
- **Published**: 2017-11-27 06:31:23+00:00
- **Updated**: 2018-02-26 11:29:58+00:00
- **Authors**: T. -T. Do, T. Hoang, V. Pomponiu, Y. Zhou, Z. Chen, N. -M. Cheung, D. Koh, A. Tan, S. -H. Tan
- **Comment**: Accepted to IEEE Trans. on Multimedia, 2018
- **Journal**: None
- **Summary**: We investigate the design of an entire mobile imaging system for early detection of melanoma. Different from previous work, we focus on smartphone-captured visible light images. Our design addresses two major challenges. First, images acquired using a smartphone under loosely-controlled environmental conditions may be subject to various distortions, and this makes melanoma detection more difficult. Second, processing performed on a smartphone is subject to stringent computation and memory constraints. In our work, we propose a detection system that is optimized to run entirely on the resource-constrained smartphone. Our system intends to localize the skin lesion by combining a lightweight method for skin detection with a hierarchical segmentation approach using two fast segmentation methods. Moreover, we study an extensive set of image features and propose new numerical features to characterize a skin lesion. Furthermore, we propose an improved feature selection algorithm to determine a small set of discriminative features used by the final lightweight system. In addition, we study the human-computer interface (HCI) design to understand the usability and acceptance issues of the proposed system.



### Discriminative Region Proposal Adversarial Networks for High-Quality Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1711.09554v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09554v3)
- **Published**: 2017-11-27 06:31:24+00:00
- **Updated**: 2018-08-06 15:26:44+00:00
- **Authors**: Chao Wang, Haiyong Zheng, Zhibin Yu, Ziqiang Zheng, Zhaorui Gu, Bing Zheng
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Image-to-image translation has been made much progress with embracing Generative Adversarial Networks (GANs). However, it's still very challenging for translation tasks that require high quality, especially at high-resolution and photorealism. In this paper, we present Discriminative Region Proposal Adversarial Networks (DRPAN) for high-quality image-to-image translation. We decompose the procedure of image-to-image translation task into three iterated steps, first is to generate an image with global structure but some local artifacts (via GAN), second is using our DRPnet to propose the most fake region from the generated image, and third is to implement "image inpainting" on the most fake region for more realistic result through a reviser, so that the system (DRPAN) can be gradually optimized to synthesize images with more attention on the most artifact local part. Experiments on a variety of image-to-image translation tasks and datasets validate that our method outperforms state-of-the-arts for producing high-quality translation results in terms of both human perceptual studies and automatic quantitative measures.



### HP-GAN: Probabilistic 3D human motion prediction via GAN
- **Arxiv ID**: http://arxiv.org/abs/1711.09561v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.09561v1)
- **Published**: 2017-11-27 07:07:11+00:00
- **Updated**: 2017-11-27 07:07:11+00:00
- **Authors**: Emad Barsoum, John Kender, Zicheng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting and understanding human motion dynamics has many applications, such as motion synthesis, augmented reality, security, and autonomous vehicles. Due to the recent success of generative adversarial networks (GAN), there has been much interest in probabilistic estimation and synthetic data generation using deep neural network architectures and learning algorithms.   We propose a novel sequence-to-sequence model for probabilistic human motion prediction, trained with a modified version of improved Wasserstein generative adversarial networks (WGAN-GP), in which we use a custom loss function designed for human motion prediction. Our model, which we call HP-GAN, learns a probability density function of future human poses conditioned on previous poses. It predicts multiple sequences of possible future human poses, each from the same input sequence but a different vector z drawn from a random distribution. Furthermore, to quantify the quality of the non-deterministic predictions, we simultaneously train a motion-quality-assessment model that learns the probability that a given skeleton sequence is a real human motion.   We test our algorithm on two of the largest skeleton datasets: NTURGB-D and Human3.6M. We train our model on both single and multiple action types. Its predictive power for long-term motion estimation is demonstrated by generating multiple plausible futures of more than 30 frames from just 10 frames of input. We show that most sequences generated from the same input have more than 50\% probabilities of being judged as a real human sequence. We will release all the code used in this paper to Github.



### Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?
- **Arxiv ID**: http://arxiv.org/abs/1711.09577v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09577v2)
- **Published**: 2017-11-27 08:29:06+00:00
- **Updated**: 2018-04-02 01:49:37+00:00
- **Authors**: Kensho Hara, Hirokatsu Kataoka, Yutaka Satoh
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: The purpose of this study is to determine whether current video datasets have sufficient data for training very deep convolutional neural networks (CNNs) with spatio-temporal three-dimensional (3D) kernels. Recently, the performance levels of 3D CNNs in the field of action recognition have improved significantly. However, to date, conventional research has only explored relatively shallow 3D architectures. We examine the architectures of various 3D CNNs from relatively shallow to very deep ones on current video datasets. Based on the results of those experiments, the following conclusions could be obtained: (i) ResNet-18 training resulted in significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics. (ii) The Kinetics dataset has sufficient data for training of deep 3D CNNs, and enables training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet. ResNeXt-101 achieved 78.4% average accuracy on the Kinetics test set. (iii) Kinetics pretrained simple 3D architectures outperforms complex 2D architectures, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% on UCF-101 and HMDB-51, respectively. The use of 2D CNNs trained on ImageNet has produced significant progress in various tasks in image. We believe that using deep 3D CNNs together with Kinetics will retrace the successful history of 2D CNNs and ImageNet, and stimulate advances in computer vision for videos. The codes and pretrained models used in this study are publicly available. https://github.com/kenshohara/3D-ResNets-PyTorch



### Joint Cuts and Matching of Partitions in One Graph
- **Arxiv ID**: http://arxiv.org/abs/1711.09584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09584v1)
- **Published**: 2017-11-27 08:52:19+00:00
- **Updated**: 2017-11-27 08:52:19+00:00
- **Authors**: Tianshu Yu, Junchi Yan, Jieyi Zhao, Baoxin Li
- **Comment**: None
- **Journal**: None
- **Summary**: As two fundamental problems, graph cuts and graph matching have been investigated over decades, resulting in vast literature in these two topics respectively. However the way of jointly applying and solving graph cuts and matching receives few attention. In this paper, we first formalize the problem of simultaneously cutting a graph into two partitions i.e. graph cuts and establishing their correspondence i.e. graph matching. Then we develop an optimization algorithm by updating matching and cutting alternatively, provided with theoretical analysis. The efficacy of our algorithm is verified on both synthetic dataset and real-world images containing similar regions or structures.



### FuCoLoT -- A Fully-Correlational Long-Term Tracker
- **Arxiv ID**: http://arxiv.org/abs/1711.09594v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09594v2)
- **Published**: 2017-11-27 09:31:05+00:00
- **Updated**: 2019-01-14 09:32:02+00:00
- **Authors**: Alan Lukežič, Luka Čehovin Zajc, Tomáš Vojíř, Jiří Matas, Matej Kristan
- **Comment**: None
- **Journal**: None
- **Summary**: We propose FuCoLoT -- a Fully Correlational Long-term Tracker. It exploits the novel DCF constrained filter learning method to design a detector that is able to re-detect the target in the whole image efficiently. FuCoLoT maintains several correlation filters trained on different time scales that act as the detector components. A novel mechanism based on the correlation response is used for tracking failure estimation. FuCoLoT achieves state-of-the-art results on standard short-term benchmarks and it outperforms the current best-performing tracker on the long-term UAV20L benchmark by over 19%. It has an order of magnitude smaller memory footprint than its best-performing competitors and runs at 15fps in a single CPU thread.



### Memory Aware Synapses: Learning what (not) to forget
- **Arxiv ID**: http://arxiv.org/abs/1711.09601v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.09601v4)
- **Published**: 2017-11-27 09:48:44+00:00
- **Updated**: 2018-10-05 08:40:30+00:00
- **Authors**: Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, Tinne Tuytelaars
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Humans can learn in a continuous manner. Old rarely utilized knowledge can be overwritten by new incoming information while important, frequently used knowledge is prevented from being erased. In artificial learning systems, lifelong learning so far has focused mainly on accumulating knowledge over tasks and overcoming catastrophic forgetting. In this paper, we argue that, given the limited model capacity and the unlimited new information to be learned, knowledge has to be preserved or erased selectively. Inspired by neuroplasticity, we propose a novel approach for lifelong learning, coined Memory Aware Synapses (MAS). It computes the importance of the parameters of a neural network in an unsupervised and online manner. Given a new sample which is fed to the network, MAS accumulates an importance measure for each parameter of the network, based on how sensitive the predicted output function is to a change in this parameter. When learning a new task, changes to important parameters can then be penalized, effectively preventing important knowledge related to previous tasks from being overwritten. Further, we show an interesting connection between a local version of our method and Hebb's rule,which is a model for the learning process in the brain. We test our method on a sequence of object recognition tasks and on the challenging problem of learning an embedding for predicting $<$subject, predicate, object$>$ triplets. We show state-of-the-art performance and, for the first time, the ability to adapt the importance of the parameters based on unlabeled data towards what the network needs (not) to forget, which may vary depending on test conditions.



### Hierarchical Video Generation from Orthogonal Information: Optical Flow and Texture
- **Arxiv ID**: http://arxiv.org/abs/1711.09618v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09618v2)
- **Published**: 2017-11-27 11:03:51+00:00
- **Updated**: 2017-12-01 08:23:09+00:00
- **Authors**: Katsunori Ohnishi, Shohei Yamamoto, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: Our supplemental material is available on
  http://www.mi.t.u-tokyo.ac.jp/assets/publication/hierarchical_video_generation_sup/
  Accepted to AAAI2018
- **Journal**: None
- **Summary**: Learning to represent and generate videos from unlabeled data is a very challenging problem. To generate realistic videos, it is important not only to ensure that the appearance of each frame is real, but also to ensure the plausibility of a video motion and consistency of a video appearance in the time direction. The process of video generation should be divided according to these intrinsic difficulties. In this study, we focus on the motion and appearance information as two important orthogonal components of a video, and propose Flow-and-Texture-Generative Adversarial Networks (FTGAN) consisting of FlowGAN and TextureGAN. In order to avoid a huge annotation cost, we have to explore a way to learn from unlabeled data. Thus, we employ optical flow as motion information to generate videos. FlowGAN generates optical flow, which contains only the edge and motion of the videos to be begerated. On the other hand, TextureGAN specializes in giving a texture to optical flow generated by FlowGAN. This hierarchical approach brings more realistic videos with plausible motion and appearance consistency. Our experiments show that our model generates more plausible motion videos and also achieves significantly improved performance for unsupervised action classification in comparison to previous GAN works. In addition, because our model generates videos from two independent information, our model can generate new combinations of motion and attribute that are not seen in training data, such as a video in which a person is doing sit-up in a baseball ground.



### Transfer Learning in CNNs Using Filter-Trees
- **Arxiv ID**: http://arxiv.org/abs/1711.09648v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09648v1)
- **Published**: 2017-11-27 12:29:44+00:00
- **Updated**: 2017-11-27 12:29:44+00:00
- **Authors**: Suresh Kirthi Kumaraswamy, PS Sastry, KR Ramakrishnan
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are very effective for many pattern recognition tasks. However, training deep CNNs needs extensive computation and large training data. In this paper we propose Bank of Filter-Trees (BFT) as a trans- fer learning mechanism for improving efficiency of learning CNNs. A filter-tree corresponding to a filter in k^{th} convolu- tional layer of a CNN is a subnetwork consisting of the filter along with all its connections to filters in all preceding layers. An ensemble of such filter-trees created from the k^{th} layers of many CNNs learnt on different but related tasks, forms the BFT. To learn a new CNN, we sample from the BFT to select a set of filter trees. This fixes the target net up to the k th layer and only the remaining network would be learnt using train- ing data of new task. Through simulations we demonstrate the effectiveness of this idea of BFT. This method constitutes a novel transfer learning technique where transfer is at a sub- network level; transfer can be effected from multiple source networks; and, with no finetuning of the transferred weights, the performance achieved is on par with networks that are trained from scratch.



### DeepBrain: Functional Representation of Neural In-Situ Hybridization Images for Gene Ontology Classification Using Deep Convolutional Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1711.09663v1
- **DOI**: 10.1007/978-3-319-68612-7_33
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.09663v1)
- **Published**: 2017-11-27 13:00:03+00:00
- **Updated**: 2017-11-27 13:00:03+00:00
- **Authors**: Ido Cohen, Eli David, Nathan S. Netanyahu, Noa Liscovitch, Gal Chechik
- **Comment**: None
- **Journal**: International Conference on Artificial Neural Networks (ICANN),
  Springer LNCS, Vol. 10614, pp. 287-296, Alghero, Italy, September, 2017
- **Summary**: This paper presents a novel deep learning-based method for learning a functional representation of mammalian neural images. The method uses a deep convolutional denoising autoencoder (CDAE) for generating an invariant, compact representation of in situ hybridization (ISH) images. While most existing methods for bio-imaging analysis were not developed to handle images with highly complex anatomical structures, the results presented in this paper show that functional representation extracted by CDAE can help learn features of functional gene ontology categories for their classification in a highly accurate manner. Using this CDAE representation, our method outperforms the previous state-of-the-art classification rate, by improving the average AUC from 0.92 to 0.98, i.e., achieving 75% reduction in error. The method operates on input images that were downsampled significantly with respect to the original ones to make it computationally feasible.



### Improving OCR Accuracy on Early Printed Books by utilizing Cross Fold Training and Voting
- **Arxiv ID**: http://arxiv.org/abs/1711.09670v1
- **DOI**: 10.1109/DAS.2018.30
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09670v1)
- **Published**: 2017-11-27 13:15:52+00:00
- **Updated**: 2017-11-27 13:15:52+00:00
- **Authors**: Christian Reul, Uwe Springmann, Christoph Wick, Frank Puppe
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce a method that significantly reduces the character error rates for OCR text obtained from OCRopus models trained on early printed books. The method uses a combination of cross fold training and confidence based voting. After allocating the available ground truth in different subsets several training processes are performed, each resulting in a specific OCR model. The OCR text generated by these models then gets voted to determine the final output by taking the recognized characters, their alternatives, and the confidence values assigned to each character into consideration. Experiments on seven early printed books show that the proposed method outperforms the standard approach considerably by reducing the amount of errors by up to 50% and more.



### Exploiting the potential of unlabeled endoscopic video data with self-supervised learning
- **Arxiv ID**: http://arxiv.org/abs/1711.09726v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09726v3)
- **Published**: 2017-11-27 14:56:38+00:00
- **Updated**: 2018-01-31 13:59:45+00:00
- **Authors**: Tobias Ross, David Zimmerer, Anant Vemuri, Fabian Isensee, Manuel Wiesenfarth, Sebastian Bodenstedt, Fabian Both, Philip Kessler, Martin Wagner, Beat Müller, Hannes Kenngott, Stefanie Speidel, Annette Kopp-Schneider, Klaus Maier-Hein, Lena Maier-Hein
- **Comment**: None
- **Journal**: None
- **Summary**: Surgical data science is a new research field that aims to observe all aspects of the patient treatment process in order to provide the right assistance at the right time. Due to the breakthrough successes of deep learning-based solutions for automatic image annotation, the availability of reference annotations for algorithm training is becoming a major bottleneck in the field. The purpose of this paper was to investigate the concept of self-supervised learning to address this issue.   Our approach is guided by the hypothesis that unlabeled video data can be used to learn a representation of the target domain that boosts the performance of state-of-the-art machine learning algorithms when used for pre-training. Core of the method is an auxiliary task based on raw endoscopic video data of the target domain that is used to initialize the convolutional neural network (CNN) for the target task. In this paper, we propose the re-colorization of medical images with a generative adversarial network (GAN)-based architecture as auxiliary task. A variant of the method involves a second pre-training step based on labeled data for the target task from a related domain. We validate both variants using medical instrument segmentation as target task.   The proposed approach can be used to radically reduce the manual annotation effort involved in training CNNs. Compared to the baseline approach of generating annotated data from scratch, our method decreases exploratively the number of labeled images by up to 75% without sacrificing performance. Our method also outperforms alternative methods for CNN pre-training, such as pre-training on publicly available non-medical or medical data using the target task (in this instance: segmentation).   As it makes efficient use of available (non-)public and (un-)labeled data, the approach has the potential to become a valuable tool for CNN (pre-)training.



### GazeGAN - Unpaired Adversarial Image Generation for Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/1711.09767v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1711.09767v1)
- **Published**: 2017-11-27 15:32:36+00:00
- **Updated**: 2017-11-27 15:32:36+00:00
- **Authors**: Matan Sela, Pingmei Xu, Junfeng He, Vidhya Navalpakkam, Dmitry Lagun
- **Comment**: Project was done when the first author was at Google Research
- **Journal**: None
- **Summary**: Recent research has demonstrated the ability to estimate gaze on mobile devices by performing inference on the image from the phone's front-facing camera, and without requiring specialized hardware. While this offers wide potential applications such as in human-computer interaction, medical diagnosis and accessibility (e.g., hands free gaze as input for patients with motor disorders), current methods are limited as they rely on collecting data from real users, which is a tedious and expensive process that is hard to scale across devices. There have been some attempts to synthesize eye region data using 3D models that can simulate various head poses and camera settings, however these lack in realism.   In this paper, we improve upon a recently suggested method, and propose a generative adversarial framework to generate a large dataset of high resolution colorful images with high diversity (e.g., in subjects, head pose, camera settings) and realism, while simultaneously preserving the accuracy of gaze labels. The proposed approach operates on extended regions of the eye, and even completes missing parts of the image. Using this rich synthesized dataset, and without using any additional training data from real users, we demonstrate improvements over state-of-the-art for estimating 2D gaze position on mobile devices. We further demonstrate cross-device generalization of model performance, as well as improved robustness to diverse head pose, blur and distance.



### Scalable Object Detection for Stylized Objects
- **Arxiv ID**: http://arxiv.org/abs/1711.09822v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.09822v2)
- **Published**: 2017-11-27 16:46:09+00:00
- **Updated**: 2017-11-29 10:04:56+00:00
- **Authors**: Aayush Garg, Thilo Will, William Darling, Willi Richert, Clemens Marschner
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Following recent breakthroughs in convolutional neural networks and monolithic model architectures, state-of-the-art object detection models can reliably and accurately scale into the realm of up to thousands of classes. Things quickly break down, however, when scaling into the tens of thousands, or, eventually, to millions or billions of unique objects. Further, bounding box-trained end-to-end models require extensive training data. Even though - with some tricks using hierarchies - one can sometimes scale up to thousands of classes, the labor requirements for clean image annotations quickly get out of control. In this paper, we present a two-layer object detection method for brand logos and other stylized objects for which prototypical images exist. It can scale to large numbers of unique classes. Our first layer is a CNN from the Single Shot Multibox Detector family of models that learns to propose regions where some stylized object is likely to appear. The contents of a proposed bounding box is then run against an image index that is targeted for the retrieval task at hand. The proposed architecture scales to a large number of object classes, allows to continously add new classes without retraining, and exhibits state-of-the-art quality on a stylized object detection task such as logo recognition.



### Separating Self-Expression and Visual Content in Hashtag Supervision
- **Arxiv ID**: http://arxiv.org/abs/1711.09825v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.09825v1)
- **Published**: 2017-11-27 16:50:52+00:00
- **Updated**: 2017-11-27 16:50:52+00:00
- **Authors**: Andreas Veit, Maximilian Nickel, Serge Belongie, Laurens van der Maaten
- **Comment**: None
- **Journal**: None
- **Summary**: The variety, abundance, and structured nature of hashtags make them an interesting data source for training vision models. For instance, hashtags have the potential to significantly reduce the problem of manual supervision and annotation when learning vision models for a large number of concepts. However, a key challenge when learning from hashtags is that they are inherently subjective because they are provided by users as a form of self-expression. As a consequence, hashtags may have synonyms (different hashtags referring to the same visual content) and may be ambiguous (the same hashtag referring to different visual content). These challenges limit the effectiveness of approaches that simply treat hashtags as image-label pairs. This paper presents an approach that extends upon modeling simple image-label pairs by modeling the joint distribution of images, hashtags, and users. We demonstrate the efficacy of such approaches in image tagging and retrieval experiments, and show how the joint model can be used to perform user-conditional retrieval and tagging.



### On the Robustness of Semantic Segmentation Models to Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1711.09856v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09856v3)
- **Published**: 2017-11-27 17:59:50+00:00
- **Updated**: 2018-07-08 12:37:09+00:00
- **Authors**: Anurag Arnab, Ondrej Miksik, Philip H. S. Torr
- **Comment**: CVPR 2018 extended version
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have demonstrated exceptional performance on most recognition tasks such as image classification and segmentation. However, they have also been shown to be vulnerable to adversarial examples. This phenomenon has recently attracted a lot of attention but it has not been extensively studied on multiple, large-scale datasets and structured prediction tasks such as semantic segmentation which often require more specialised networks with additional components such as CRFs, dilated convolutions, skip-connections and multiscale processing. In this paper, we present what to our knowledge is the first rigorous evaluation of adversarial attacks on modern semantic segmentation models, using two large-scale datasets. We analyse the effect of different network architectures, model capacity and multiscale processing, and show that many observations made on the task of classification do not always transfer to this more complex task. Furthermore, we show how mean-field inference in deep structured models, multiscale processing (and more generally, input transformations) naturally implement recently proposed adversarial defenses. Our observations will aid future efforts in understanding and defending against adversarial examples. Moreover, in the shorter term, we show how to effectively benchmark robustness and show which segmentation models should currently be preferred in safety-critical applications due to their inherent robustness.



### Accelerated Optimization in the PDE Framework: Formulations for the Active Contour Case
- **Arxiv ID**: http://arxiv.org/abs/1711.09867v1
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.09867v1)
- **Published**: 2017-11-27 18:27:24+00:00
- **Updated**: 2017-11-27 18:27:24+00:00
- **Authors**: Anthony Yezzi, Ganesh Sundaramoorthi
- **Comment**: None
- **Journal**: None
- **Summary**: Following the seminal work of Nesterov, accelerated optimization methods have been used to powerfully boost the performance of first-order, gradient-based parameter estimation in scenarios where second-order optimization strategies are either inapplicable or impractical. Not only does accelerated gradient descent converge considerably faster than traditional gradient descent, but it also performs a more robust local search of the parameter space by initially overshooting and then oscillating back as it settles into a final configuration, thereby selecting only local minimizers with a basis of attraction large enough to contain the initial overshoot. This behavior has made accelerated and stochastic gradient search methods particularly popular within the machine learning community. In their recent PNAS 2016 paper, Wibisono, Wilson, and Jordan demonstrate how a broad class of accelerated schemes can be cast in a variational framework formulated around the Bregman divergence, leading to continuum limit ODE's. We show how their formulation may be further extended to infinite dimension manifolds (starting here with the geometric space of curves and surfaces) by substituting the Bregman divergence with inner products on the tangent space and explicitly introducing a distributed mass model which evolves in conjunction with the object of interest during the optimization process. The co-evolving mass model, which is introduced purely for the sake of endowing the optimization with helpful dynamics, also links the resulting class of accelerated PDE based optimization schemes to fluid dynamical formulations of optimal mass transport.



### Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs
- **Arxiv ID**: http://arxiv.org/abs/1711.09869v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.09869v2)
- **Published**: 2017-11-27 18:37:50+00:00
- **Updated**: 2018-03-28 09:01:33+00:00
- **Authors**: Loic Landrieu, Martin Simonovsky
- **Comment**: Accepted to CVPR 2018; camera ready version. Major updates to [v1]:
  Improved performance on S3DIS (from +5.8 to +12.4 mIoU) and extended ablation
  study in Appendix
- **Journal**: None
- **Summary**: We propose a novel deep learning-based framework to tackle the challenge of semantic segmentation of large-scale point clouds of millions of points. We argue that the organization of 3D point clouds can be efficiently captured by a structure called superpoint graph (SPG), derived from a partition of the scanned scene into geometrically homogeneous elements. SPGs offer a compact yet rich representation of contextual relationships between object parts, which is then exploited by a graph convolutional network. Our framework sets a new state of the art for segmenting outdoor LiDAR scans (+11.9 and +8.8 mIoU points for both Semantic3D test sets), as well as indoor scans (+12.4 mIoU points for the S3DIS dataset).



### Training Convolutional Neural Networks with Limited Training Data for Ear Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1711.09952v2
- **DOI**: 10.1109/FG.2017.123
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09952v2)
- **Published**: 2017-11-27 19:51:06+00:00
- **Updated**: 2019-02-01 08:19:35+00:00
- **Authors**: Žiga Emeršič, Dejan Štepec, Vitomir Štruc, Peter Peer
- **Comment**: None
- **Journal**: None
- **Summary**: Identity recognition from ear images is an active field of research within the biometric community. The ability to capture ear images from a distance and in a covert manner makes ear recognition technology an appealing choice for surveillance and security applications as well as related application domains. In contrast to other biometric modalities, where large datasets captured in uncontrolled settings are readily available, datasets of ear images are still limited in size and mostly of laboratory-like quality. As a consequence, ear recognition technology has not benefited yet from advances in deep learning and convolutional neural networks (CNNs) and is still lacking behind other modalities that experienced significant performance gains owing to deep recognition technology. In this paper we address this problem and aim at building a CNNbased ear recognition model. We explore different strategies towards model training with limited amounts of training data and show that by selecting an appropriate model architecture, using aggressive data augmentation and selective learning on existing (pre-trained) models, we are able to learn an effective CNN-based model using a little more than 1300 training images. The result of our work is the first CNN-based approach to ear recognition that is also made publicly available to the research community. With our model we are able to improve on the rank one recognition rate of the previous state-of-the-art by more than 25% on a challenging dataset of ear images captured from the web (a.k.a. in the wild).



### SSD-6D: Making RGB-based 3D detection and 6D pose estimation great again
- **Arxiv ID**: http://arxiv.org/abs/1711.10006v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10006v1)
- **Published**: 2017-11-27 21:17:51+00:00
- **Updated**: 2017-11-27 21:17:51+00:00
- **Authors**: Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan Ilic, Nassir Navab
- **Comment**: The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: We present a novel method for detecting 3D model instances and estimating their 6D poses from RGB data in a single shot. To this end, we extend the popular SSD paradigm to cover the full 6D pose space and train on synthetic model data only. Our approach competes or surpasses current state-of-the-art methods that leverage RGB-D data on multiple challenging datasets. Furthermore, our method produces these results at around 10Hz, which is many times faster than the related methods. For the sake of reproducibility, we make our trained networks and detection code publicly available.



