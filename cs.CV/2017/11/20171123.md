# Arxiv Papers in cs.CV on 2017-11-23
### Multiple component decomposition from millimeter single-channel data
- **Arxiv ID**: http://arxiv.org/abs/1711.08456v1
- **DOI**: 10.3847/1538-4365/aaa83c
- **Categories**: **astro-ph.IM**, astro-ph.GA, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.08456v1)
- **Published**: 2017-11-23 02:52:20+00:00
- **Updated**: 2017-11-23 02:52:20+00:00
- **Authors**: Iván Rodríguez-Montoya, David Sánchez-Argüelles, Itziar Aretxaga, Emanuele Bertone, Miguel Chávez-Dagostino, David H. Hughes, Alfredo Montaña, Grant W. Wilson, Milagros Zeballos
- **Comment**: Accepted in ApJS
- **Journal**: None
- **Summary**: We present an implementation of a blind source separation algorithm to remove foregrounds off millimeter surveys made by single-channel instruments. In order to make possible such a decomposition over single-wavelength data: we generate levels of artificial redundancy, then perform a blind decomposition, calibrate the resulting maps, and lastly measure physical information. We simulate the reduction pipeline using mock data: atmospheric fluctuations, extended astrophysical foregrounds, and point-like sources, but we apply the same methodology to the AzTEC/ASTE survey of the Great Observatories Origins Deep Survey-South (GOODS-S). In both applications, our technique robustly decomposes redundant maps into their underlying components, reducing flux bias, improving signal-to-noise, and minimizing information loss. In particular, the GOODS-S survey is decomposed into four independent physical components, one of them is the already known map of point sources, two are atmospheric and systematic foregrounds, and the fourth component is an extended emission that can be interpreted as the confusion background of faint sources.



### Adversarial Feature Augmentation for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1711.08561v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08561v2)
- **Published**: 2017-11-23 03:17:11+00:00
- **Updated**: 2018-05-04 11:03:09+00:00
- **Authors**: Riccardo Volpi, Pietro Morerio, Silvio Savarese, Vittorio Murino
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: Recent works showed that Generative Adversarial Networks (GANs) can be successfully applied in unsupervised domain adaptation, where, given a labeled source dataset and an unlabeled target dataset, the goal is to train powerful classifiers for the target samples. In particular, it was shown that a GAN objective function can be used to learn target features indistinguishable from the source ones. In this work, we extend this framework by (i) forcing the learned feature extractor to be domain-invariant, and (ii) training it through data augmentation in the feature space, namely performing feature augmentation. While data augmentation in the image space is a well established technique in deep learning, feature augmentation has not yet received the same level of attention. We accomplish it by means of a feature generator trained by playing the GAN minimax game against source features. Results show that both enforcing domain-invariance and performing feature augmentation lead to superior or comparable performance to state-of-the-art results in several unsupervised domain adaptation benchmarks.



### Person Transfer GAN to Bridge Domain Gap for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1711.08565v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08565v2)
- **Published**: 2017-11-23 03:23:21+00:00
- **Updated**: 2018-06-25 12:50:58+00:00
- **Authors**: Longhui Wei, Shiliang Zhang, Wen Gao, Qi Tian
- **Comment**: 10 pages, 9 figures; accepted in CVPR 2018
- **Journal**: None
- **Summary**: Although the performance of person Re-Identification (ReID) has been significantly boosted, many challenging issues in real scenarios have not been fully investigated, e.g., the complex scenes and lighting variations, viewpoint and pose changes, and the large number of identities in a camera network. To facilitate the research towards conquering those issues, this paper contributes a new dataset called MSMT17 with many important features, e.g., 1) the raw videos are taken by an 15-camera network deployed in both indoor and outdoor scenes, 2) the videos cover a long period of time and present complex lighting variations, and 3) it contains currently the largest number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe that, domain gap commonly exists between datasets, which essentially causes severe performance drop when training and testing on different datasets. This results in that available training data cannot be effectively leveraged for new testing domains. To relieve the expensive costs of annotating new training samples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to bridge the domain gap. Comprehensive experiments show that the domain gap could be substantially narrowed-down by the PTGAN.



### Geometric Cross-Modal Comparison of Heterogeneous Sensor Data
- **Arxiv ID**: http://arxiv.org/abs/1711.08569v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.4; I.4.9; J.2
- **Links**: [PDF](http://arxiv.org/pdf/1711.08569v1)
- **Published**: 2017-11-23 03:55:41+00:00
- **Updated**: 2017-11-23 03:55:41+00:00
- **Authors**: Christopher J. Tralie, Abraham Smith, Nathan Borggren, Jay Hineman, Paul Bendich, Peter Zulch, John Harer
- **Comment**: 10 pages, 13 figures, Proceedings of IEEE Aeroconf 2017
- **Journal**: None
- **Summary**: In this work, we address the problem of cross-modal comparison of aerial data streams. A variety of simulated automobile trajectories are sensed using two different modalities: full-motion video, and radio-frequency (RF) signals received by detectors at various locations. The information represented by the two modalities is compared using self-similarity matrices (SSMs) corresponding to time-ordered point clouds in feature spaces of each of these data sources; we note that these feature spaces can be of entirely different scale and dimensionality. Several metrics for comparing SSMs are explored, including a cutting-edge time-warping technique that can simultaneously handle local time warping and partial matches, while also controlling for the change in geometry between feature spaces of the two modalities. We note that this technique is quite general, and does not depend on the choice of modalities. In this particular setting, we demonstrate that the cross-modal distance between SSMs corresponding to the same trajectory type is smaller than the cross-modal distance between SSMs corresponding to distinct trajectory types, and we formalize this observation via precision-recall metrics in experiments. Finally, we comment on promising implications of these ideas for future integration into multiple-hypothesis tracking systems.



### 3D Anisotropic Hybrid Network: Transferring Convolutional Features from 2D Images to 3D Anisotropic Volumes
- **Arxiv ID**: http://arxiv.org/abs/1711.08580v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08580v2)
- **Published**: 2017-11-23 05:30:08+00:00
- **Updated**: 2017-12-04 01:54:49+00:00
- **Authors**: Siqi Liu, Daguang Xu, S. Kevin Zhou, Thomas Mertelmeier, Julia Wicklein, Anna Jerebko, Sasa Grbic, Olivier Pauly, Weidong Cai, Dorin Comaniciu
- **Comment**: None
- **Journal**: None
- **Summary**: While deep convolutional neural networks (CNN) have been successfully applied for 2D image analysis, it is still challenging to apply them to 3D anisotropic volumes, especially when the within-slice resolution is much higher than the between-slice resolution and when the amount of 3D volumes is relatively small. On one hand, direct learning of CNN with 3D convolution kernels suffers from the lack of data and likely ends up with poor generalization; insufficient GPU memory limits the model size or representational power. On the other hand, applying 2D CNN with generalizable features to 2D slices ignores between-slice information. Coupling 2D network with LSTM to further handle the between-slice information is not optimal due to the difficulty in LSTM learning. To overcome the above challenges, we propose a 3D Anisotropic Hybrid Network (AH-Net) that transfers convolutional features learned from 2D images to 3D anisotropic volumes. Such a transfer inherits the desired strong generalization capability for within-slice information while naturally exploiting between-slice information for more effective modelling. The focal loss is further utilized for more effective end-to-end learning. We experiment with the proposed 3D AH-Net on two different medical image analysis tasks, namely lesion detection from a Digital Breast Tomosynthesis volume, and liver and liver tumor segmentation from a Computed Tomography volume and obtain the state-of-the-art results.



### Exploiting temporal information for 3D pose estimation
- **Arxiv ID**: http://arxiv.org/abs/1711.08585v4
- **DOI**: 10.1007/978-3-030-01249-6_5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08585v4)
- **Published**: 2017-11-23 06:20:51+00:00
- **Updated**: 2018-09-12 05:15:11+00:00
- **Authors**: Mir Rayat Imtiaz Hossain, James J. Little
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we address the problem of 3D human pose estimation from a sequence of 2D human poses. Although the recent success of deep networks has led many state-of-the-art methods for 3D pose estimation to train deep networks end-to-end to predict from images directly, the top-performing approaches have shown the effectiveness of dividing the task of 3D pose estimation into two steps: using a state-of-the-art 2D pose estimator to estimate the 2D pose from images and then mapping them into 3D space. They also showed that a low-dimensional representation like 2D locations of a set of joints can be discriminative enough to estimate 3D pose with high accuracy. However, estimation of 3D pose for individual frames leads to temporally incoherent estimates due to independent error in each frame causing jitter. Therefore, in this work we utilize the temporal information across a sequence of 2D joint locations to estimate a sequence of 3D poses. We designed a sequence-to-sequence network composed of layer-normalized LSTM units with shortcut connections connecting the input to the output on the decoder side and imposed temporal smoothness constraint during training. We found that the knowledge of temporal consistency improves the best reported result on Human3.6M dataset by approximately $12.2\%$ and helps our network to recover temporally consistent 3D poses over a sequence of images even when the 2D pose detector fails.



### SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.08588v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08588v2)
- **Published**: 2017-11-23 06:37:45+00:00
- **Updated**: 2019-05-30 23:10:56+00:00
- **Authors**: Weiyue Wang, Ronald Yu, Qiangui Huang, Ulrich Neumann
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Similarity Group Proposal Network (SGPN), a simple and intuitive deep learning framework for 3D object instance segmentation on point clouds. SGPN uses a single network to predict point grouping proposals and a corresponding semantic class for each proposal, from which we can directly extract instance segmentation results. Important to the effectiveness of SGPN is its novel representation of 3D instance segmentation results in the form of a similarity matrix that indicates the similarity between each pair of points in embedded feature space, thus producing an accurate grouping proposal for each point. To the best of our knowledge, SGPN is the first framework to learn 3D instance-aware semantic segmentation on point clouds. Experimental results on various 3D scenes show the effectiveness of our method on 3D instance segmentation, and we also evaluate the capability of SGPN to improve 3D object detection and semantic segmentation results. We also demonstrate its flexibility by seamlessly incorporating 2D CNN features into the framework to boost performance.



### End-to-End Supervised Product Quantization for Image Search and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1711.08589v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.08589v2)
- **Published**: 2017-11-23 06:40:28+00:00
- **Updated**: 2020-01-17 22:56:50+00:00
- **Authors**: Benjamin Klein, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Product Quantization, a dictionary based hashing method, is one of the leading unsupervised hashing techniques. While it ignores the labels, it harnesses the features to construct look up tables that can approximate the feature space. In recent years, several works have achieved state of the art results on hashing benchmarks by learning binary representations in a supervised manner. This work presents Deep Product Quantization (DPQ), a technique that leads to more accurate retrieval and classification than the latest state of the art methods, while having similar computational complexity and memory footprint as the Product Quantization method. To our knowledge, this is the first work to introduce a dictionary-based representation that is inspired by Product Quantization and which is learned end-to-end, and thus benefits from the supervised signal. DPQ explicitly learns soft and hard representations to enable an efficient and accurate asymmetric search, by using a straight-through estimator. Our method obtains state of the art results on an extensive array of retrieval and classification experiments.



### Contextual-based Image Inpainting: Infer, Match, and Translate
- **Arxiv ID**: http://arxiv.org/abs/1711.08590v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08590v5)
- **Published**: 2017-11-23 06:44:43+00:00
- **Updated**: 2018-07-25 19:00:08+00:00
- **Authors**: Yuhang Song, Chao Yang, Zhe Lin, Xiaofeng Liu, Qin Huang, Hao Li, C. -C. Jay Kuo
- **Comment**: ECCV 2018 camera ready
- **Journal**: None
- **Summary**: We study the task of image inpainting, which is to fill in the missing region of an incomplete image with plausible contents. To this end, we propose a learning-based approach to generate visually coherent completion given a high-resolution image with missing components. In order to overcome the difficulty to directly learn the distribution of high-dimensional image data, we divide the task into inference and translation as two separate steps and model each step with a deep neural network. We also use simple heuristics to guide the propagation of local textures from the boundary to the hole. We show that, by using such techniques, inpainting reduces to the problem of learning two image-feature translation functions in much smaller space and hence easier to train. We evaluate our method on several public datasets and show that we generate results of better visual quality than previous state-of-the-art methods.



### Regularization of Deep Neural Networks with Spectral Dropout
- **Arxiv ID**: http://arxiv.org/abs/1711.08591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08591v1)
- **Published**: 2017-11-23 06:47:37+00:00
- **Updated**: 2017-11-23 06:47:37+00:00
- **Authors**: Salman Khan, Munawar Hayat, Fatih Porikli
- **Comment**: None
- **Journal**: None
- **Summary**: The big breakthrough on the ImageNet challenge in 2012 was partially due to the `dropout' technique used to avoid overfitting. Here, we introduce a new approach called `Spectral Dropout' to improve the generalization ability of deep neural networks. We cast the proposed approach in the form of regular Convolutional Neural Network (CNN) weight layers using a decorrelation transform with fixed basis functions. Our spectral dropout method prevents overfitting by eliminating weak and `noisy' Fourier domain coefficients of the neural network activations, leading to remarkably better results than the current regularization methods. Furthermore, the proposed is very efficient due to the fixed basis functions used for spectral transformation. In particular, compared to Dropout and Drop-Connect, our method significantly speeds up the network convergence rate during the training process (roughly x2), with considerably higher neuron pruning rates (an increase of ~ 30%). We demonstrate that the spectral dropout can also be used in conjunction with other regularization approaches resulting in additional performance gains.



### Unsupervised End-to-end Learning for Deformable Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1711.08608v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08608v2)
- **Published**: 2017-11-23 08:20:49+00:00
- **Updated**: 2018-01-20 02:48:31+00:00
- **Authors**: Siyuan Shan, Wen Yan, Xiaoqing Guo, Eric I-Chao Chang, Yubo Fan, Yan Xu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a registration algorithm for 2D CT/MRI medical images with a new unsupervised end-to-end strategy using convolutional neural networks. The contributions of our algorithm are threefold: (1) We transplant traditional image registration algorithms to an end-to-end convolutional neural network framework, while maintaining the unsupervised nature of image registration problems. The image-to-image integrated framework can simultaneously learn both image features and transformation matrix for registration. (2) Training with additional data without any label can further improve the registration performance by approximately 10 %. (3) The registration speed is 100x faster than traditional methods. The proposed network is easy to implement and can be trained efficiently. Experiments demonstrate that our system achieves state-of-the-art results on 2D brain registration and achieves comparable results on 2D liver registration. It can be extended to register other organs beyond liver and brain such as kidney, lung, and heart.



### Self-Reinforced Cascaded Regression for Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1711.08624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08624v1)
- **Published**: 2017-11-23 09:15:22+00:00
- **Updated**: 2017-11-23 09:15:22+00:00
- **Authors**: Xin Fan, Risheng Liu, Kang Huyan, Yuyao Feng, Zhongxuan Luo
- **Comment**: 8 pages, 11 figures, AAAI 18
- **Journal**: None
- **Summary**: Cascaded regression is prevailing in face alignment thanks to its accuracy and robustness, but typically demands manually annotated examples having low discrepancy between shape-indexed features and shape updates. In this paper, we propose a self-reinforced strategy that iteratively expands the quantity and improves the quality of training examples, thus upgrading the performance of cascaded regression itself. The reinforced term evaluates the example quality upon the consistence on both local appearance and global geometry of human faces, and constitutes the example evolution by the philosophy of "survival of the fittest". We train a set of discriminative classifiers, each associated with one landmark label, to prune those examples with inconsistent local appearance, and further validate the geometric relationship among groups of labeled landmarks against the common global geometry derived from a projective invariant. We embed this generic strategy into typical cascaded regressions, and the alignment results on several benchmark data sets demonstrate its effectiveness to predict good examples starting from a small subset.



### Robust Visual SLAM with Point and Line Features
- **Arxiv ID**: http://arxiv.org/abs/1711.08654v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1711.08654v1)
- **Published**: 2017-11-23 11:23:22+00:00
- **Updated**: 2017-11-23 11:23:22+00:00
- **Authors**: Xingxing Zuo, Xiaojia Xie, Yong Liu, Guoquan Huang
- **Comment**: 8 pages, Conference paper, IROS 2017
- **Journal**: None
- **Summary**: In this paper, we develop a robust efficient visual SLAM system that utilizes heterogeneous point and line features. By leveraging ORB-SLAM [1], the proposed system consists of stereo matching, frame tracking, local mapping, loop detection, and bundle adjustment of both point and line features. In particular, as the main theoretical contributions of this paper, we, for the first time, employ the orthonormal representation as the minimal parameterization to model line features along with point features in visual SLAM and analytically derive the Jacobians of the re-projection errors with respect to the line parameters, which significantly improves the SLAM solution. The proposed SLAM has been extensively tested in both synthetic and real-world experiments whose results demonstrate that the proposed system outperforms the state-of-the-art methods in various scenarios.



### Self-view Grounding Given a Narrated 360° Video
- **Arxiv ID**: http://arxiv.org/abs/1711.08664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08664v1)
- **Published**: 2017-11-23 12:06:20+00:00
- **Updated**: 2017-11-23 12:06:20+00:00
- **Authors**: Shih-Han Chou, Yi-Chun Chen, Kuo-Hao Zeng, Hou-Ning Hu, Jianlong Fu, Min Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Narrated 360{\deg} videos are typically provided in many touring scenarios to mimic real-world experience. However, previous work has shown that smart assistance (i.e., providing visual guidance) can significantly help users to follow the Normal Field of View (NFoV) corresponding to the narrative. In this project, we aim at automatically grounding the NFoVs of a 360{\deg} video given subtitles of the narrative (referred to as "NFoV-grounding"). We propose a novel Visual Grounding Model (VGM) to implicitly and efficiently predict the NFoVs given the video content and subtitles. Specifically, at each frame, we efficiently encode the panorama into feature map of candidate NFoVs using a Convolutional Neural Network (CNN) and the subtitles to the same hidden space using an RNN with Gated Recurrent Units (GRU). Then, we apply soft-attention on candidate NFoVs to trigger sentence decoder aiming to minimize the reconstruct loss between the generated and given sentence. Finally, we obtain the NFoV as the candidate NFoV with the maximum attention without any human supervision. To train VGM more robustly, we also generate a reverse sentence conditioning on one minus the soft-attention such that the attention focuses on candidate NFoVs less relevant to the given sentence. The negative log reconstruction loss of the reverse sentence (referred to as "irrelevant loss") is jointly minimized to encourage the reverse sentence to be different from the given sentence. To evaluate our method, we collect the first narrated 360{\deg} videos dataset and achieve state-of-the-art NFoV-grounding performance.



### Beyond RGB: Very High Resolution Urban Remote Sensing With Multimodal Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.08681v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.08681v1)
- **Published**: 2017-11-23 13:10:24+00:00
- **Updated**: 2017-11-23 13:10:24+00:00
- **Authors**: Nicolas Audebert, Bertrand Le Saux, Sébastien Lefèvre
- **Comment**: ISPRS Journal of Photogrammetry and Remote Sensing, Elsevier, A
  Para{\^i}tre
- **Journal**: None
- **Summary**: In this work, we investigate various methods to deal with semantic labeling of very high resolution multi-modal remote sensing data. Especially, we study how deep fully convolutional networks can be adapted to deal with multi-modal and multi-scale remote sensing data for semantic labeling. Our contributions are threefold: a) we present an efficient multi-scale approach to leverage both a large spatial context and the high resolution data, b) we investigate early and late fusion of Lidar and multispectral data, c) we validate our methods on two public datasets with state-of-the-art results. Our results indicate that late fusion make it possible to recover errors steaming from ambiguous data, while early fusion allows for better joint-feature learning but at the cost of higher sensitivity to missing data.



### Deep Video Generation, Prediction and Completion of Human Action Sequences
- **Arxiv ID**: http://arxiv.org/abs/1711.08682v3
- **DOI**: 10.1007/978-3-030-01216-8_23
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.08682v3)
- **Published**: 2017-11-23 13:10:34+00:00
- **Updated**: 2017-12-08 12:17:15+00:00
- **Authors**: Haoye Cai, Chunyan Bai, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: Under review for CVPR 2018. Haoye and Chunyan have equal contribution
- **Journal**: None
- **Summary**: Current deep learning results on video generation are limited while there are only a few first results on video prediction and no relevant significant results on video completion. This is due to the severe ill-posedness inherent in these three problems. In this paper, we focus on human action videos, and propose a general, two-stage deep framework to generate human action videos with no constraints or arbitrary number of constraints, which uniformly address the three problems: video generation given no input frames, video prediction given the first few frames, and video completion given the first and last frames. To make the problem tractable, in the first stage we train a deep generative model that generates a human pose sequence from random noise. In the second stage, a skeleton-to-image network is trained, which is used to generate a human action video given the complete human pose sequence generated in the first stage. By introducing the two-stage strategy, we sidestep the original ill-posed problems while producing for the first time high-quality video generation/prediction/completion results of much longer duration. We present quantitative and qualitative evaluation to show that our two-stage approach outperforms state-of-the-art methods in video generation, prediction and video completion. Our video result demonstration can be viewed at https://iamacewhite.github.io/supp/index.html



### Attended End-to-end Architecture for Age Estimation from Facial Expression Videos
- **Arxiv ID**: http://arxiv.org/abs/1711.08690v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1711.08690v2)
- **Published**: 2017-11-23 13:43:49+00:00
- **Updated**: 2019-11-30 15:46:37+00:00
- **Authors**: Wenjie Pei, Hamdi Dibeklioğlu, Tadas Baltrušaitis, David M. J. Tax
- **Comment**: Accepted by Transactions on Image Processing (TIP)
- **Journal**: None
- **Summary**: The main challenges of age estimation from facial expression videos lie not only in the modeling of the static facial appearance, but also in the capturing of the temporal facial dynamics. Traditional techniques to this problem focus on constructing handcrafted features to explore the discriminative information contained in facial appearance and dynamics separately. This relies on sophisticated feature-refinement and framework-design. In this paper, we present an end-to-end architecture for age estimation, called Spatially-Indexed Attention Model (SIAM), which is able to simultaneously learn both the appearance and dynamics of age from raw videos of facial expressions. Specifically, we employ convolutional neural networks to extract effective latent appearance representations and feed them into recurrent networks to model the temporal dynamics. More importantly, we propose to leverage attention models for salience detection in both the spatial domain for each single image and the temporal domain for the whole video as well. We design a specific spatially-indexed attention mechanism among the convolutional layers to extract the salient facial regions in each individual image, and a temporal attention layer to assign attention weights to each frame. This two-pronged approach not only improves the performance by allowing the model to focus on informative frames and facial areas, but it also offers an interpretable correspondence between the spatial facial regions as well as temporal frames, and the task of age estimation. We demonstrate the strong performance of our model in experiments on a large, gender-balanced database with 400 subjects with ages spanning from 8 to 76 years. Experiments reveal that our model exhibits significant superiority over the state-of-the-art methods given sufficient training data.



### Prediction of the progression of subcortical brain structures in Alzheimer's disease from baseline
- **Arxiv ID**: http://arxiv.org/abs/1711.08716v1
- **DOI**: 10.1007/978-3-319-67675-3_10
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.08716v1)
- **Published**: 2017-11-23 14:47:42+00:00
- **Updated**: 2017-11-23 14:47:42+00:00
- **Authors**: Alexandre Bône, Maxime Louis, Alexandre Routier, Jorge Samper, Michael Bacci, Benjamin Charlier, Olivier Colliot, Stanley Durrleman
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method to predict the subject-specific longitudinal progression of brain structures extracted from baseline MRI, and evaluate its performance on Alzheimer's disease data. The disease progression is modeled as a trajectory on a group of diffeomorphisms in the context of large deformation diffeomorphic metric mapping (LDDMM). We first exhibit the limited predictive abilities of geodesic regression extrapolation on this group. Building on the recent concept of parallel curves in shape manifolds, we then introduce a second predictive protocol which personalizes previously learned trajectories to new subjects, and investigate the relative performances of two parallel shifting paradigms. This design only requires the baseline imaging data. Finally, coefficients encoding the disease dynamics are obtained from longitudinal cognitive measurements for each subject, and exploited to refine our methodology which is demonstrated to successfully predict the follow-up visits.



### A Pitfall of Unsupervised Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/1712.01655v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.01655v3)
- **Published**: 2017-11-23 14:54:18+00:00
- **Updated**: 2017-12-17 20:23:24+00:00
- **Authors**: Michele Alberti, Mathias Seuret, Rolf Ingold, Marcus Liwicki
- **Comment**: This submission has been withdrawn by the author, it is a duplicate
  of arXiv:1703.04332
- **Journal**: Conference on Neural Information Processing Systems, Deep
  Learning: Bridging Theory and Practice, December 2017
- **Summary**: The point of this paper is to question typical assumptions in deep learning and suggest alternatives. A particular contribution is to prove that even if a Stacked Convolutional Auto-Encoder is good at reconstructing pictures, it is not necessarily good at discriminating their classes. When using Auto-Encoders, intuitively one assumes that features which are good for reconstruction will also lead to high classification accuracy. Indeed, it became research practice and is a suggested strategy by introductory books. However, we prove that this is not always the case. We thoroughly investigate the quality of features produced by Stacked Convolutional Auto-Encoders when trained to reconstruct their input. In particular, we analyze the relation between the reconstruction and classification capabilities of the network, if we were to use the same features for both tasks. Experimental results suggest that in fact, there is no correlation between the reconstruction score and the quality of features for a classification task. This means, more formally, that the sub-dimension representation space learned from the Stacked Convolutional Auto-Encoder (while being trained for input reconstruction) is not necessarily better separable than the initial input space. Furthermore, we show that the reconstruction error is not a good metric to assess the quality of features, because it is biased by the decoder quality. We do not question the usefulness of pre-training, but we conclude that aiming for the lowest reconstruction error is not necessarily a good idea if afterwards one performs a classification task.



### Parallel transport in shape analysis: a scalable numerical scheme
- **Arxiv ID**: http://arxiv.org/abs/1711.08725v1
- **DOI**: 10.1007/978-3-319-68445-1_4
- **Categories**: **cs.CV**, math.DG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.08725v1)
- **Published**: 2017-11-23 14:57:32+00:00
- **Updated**: 2017-11-23 14:57:32+00:00
- **Authors**: Maxime Louis, Alexandre Bône, Benjamin Charlier, Stanley Durrleman
- **Comment**: None
- **Journal**: None
- **Summary**: The analysis of manifold-valued data requires efficient tools from Riemannian geometry to cope with the computational complexity at stake. This complexity arises from the always-increasing dimension of the data, and the absence of closed-form expressions to basic operations such as the Riemannian logarithm. In this paper, we adapt a generic numerical scheme recently introduced for computing parallel transport along geodesics in a Riemannian manifold to finite-dimensional manifolds of diffeomorphisms. We provide a qualitative and quantitative analysis of its behavior on high-dimensional manifolds, and investigate an application with the prediction of brain structures progression.



### Open Evaluation Tool for Layout Analysis of Document Images
- **Arxiv ID**: http://arxiv.org/abs/1712.01656v1
- **DOI**: 10.1109/ICDAR.2017.311
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01656v1)
- **Published**: 2017-11-23 15:18:04+00:00
- **Updated**: 2017-11-23 15:18:04+00:00
- **Authors**: Michele Alberti, Manuel Bouillon, Rolf Ingold, Marcus Liwicki
- **Comment**: The 14th IAPR International Conference on Document Analysis and
  Recognition (ICDAR), HIP: 4th International Workshop on Historical Document
  Imaging and Processing, Kyoto, Japan, 2017
- **Journal**: ICDAR-OST 2017
- **Summary**: This paper presents an open tool for standardizing the evaluation process of the layout analysis task of document images at pixel level. We introduce a new evaluation tool that is both available as a standalone Java application and as a RESTful web service. This evaluation tool is free and open-source in order to be a common tool that anyone can use and contribute to. It aims at providing as many metrics as possible to investigate layout analysis predictions, and also provide an easy way of visualizing the results. This tool evaluates document segmentation at pixel level, and support multi-labeled pixel ground truth. Finally, this tool has been successfully used for the ICDAR2017 competition on Layout Analysis for Challenging Medieval Manuscripts.



### fpgaConvNet: A Toolflow for Mapping Diverse Convolutional Neural Networks on Embedded FPGAs
- **Arxiv ID**: http://arxiv.org/abs/1711.08740v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.08740v1)
- **Published**: 2017-11-23 15:37:21+00:00
- **Updated**: 2017-11-23 15:37:21+00:00
- **Authors**: Stylianos I. Venieris, Christos-Savvas Bouganis
- **Comment**: Accepted at NIPS 2017 Workshop on Machine Learning on the Phone and
  other Consumer Devices
- **Journal**: None
- **Summary**: In recent years, Convolutional Neural Networks (ConvNets) have become an enabling technology for a wide range of novel embedded Artificial Intelligence systems. Across the range of applications, the performance needs vary significantly, from high-throughput video surveillance to the very low-latency requirements of autonomous cars. In this context, FPGAs can provide a potential platform that can be optimally configured based on the different performance needs. However, the complexity of ConvNet models keeps increasing making their mapping to an FPGA device a challenging task. This work presents fpgaConvNet, an end-to-end framework for mapping ConvNets on FPGAs. The proposed framework employs an automated design methodology based on the Synchronous Dataflow (SDF) paradigm and defines a set of SDF transformations in order to efficiently explore the architectural design space. By selectively optimising for throughput, latency or multiobjective criteria, the presented tool is able to efficiently explore the design space and generate hardware designs from high-level ConvNet specifications, explicitly optimised for the performance metric of interest. Overall, our framework yields designs that improve the performance by up to 6.65x over highly optimised embedded GPU designs for the same power constraints in embedded environments.



### Deep Expander Networks: Efficient Deep Networks from Graph Theory
- **Arxiv ID**: http://arxiv.org/abs/1711.08757v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08757v3)
- **Published**: 2017-11-23 16:16:04+00:00
- **Updated**: 2018-07-26 07:31:24+00:00
- **Authors**: Ameya Prabhu, Girish Varma, Anoop Namboodiri
- **Comment**: ECCV'18
- **Journal**: None
- **Summary**: Efficient CNN designs like ResNets and DenseNet were proposed to improve accuracy vs efficiency trade-offs. They essentially increased the connectivity, allowing efficient information flow across layers. Inspired by these techniques, we propose to model connections between filters of a CNN using graphs which are simultaneously sparse and well connected. Sparsity results in efficiency while well connectedness can preserve the expressive power of the CNNs. We use a well-studied class of graphs from theoretical computer science that satisfies these properties known as Expander graphs. Expander graphs are used to model connections between filters in CNNs to design networks called X-Nets. We present two guarantees on the connectivity of X-Nets: Each node influences every node in a layer in logarithmic steps, and the number of paths between two sets of nodes is proportional to the product of their sizes. We also propose efficient training and inference algorithms, making it possible to train deeper and wider X-Nets effectively.   Expander based models give a 4% improvement in accuracy on MobileNet over grouped convolutions, a popular technique, which has the same sparsity but worse connectivity. X-Nets give better performance trade-offs than the original ResNet and DenseNet-BC architectures. We achieve model sizes comparable to state-of-the-art pruning techniques using our simple architecture design, without any pruning. We hope that this work motivates other approaches to utilize results from graph theory to develop efficient network architectures.



### Boosted Cascaded Convnets for Multilabel Classification of Thoracic Diseases in Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/1711.08760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08760v1)
- **Published**: 2017-11-23 16:25:29+00:00
- **Updated**: 2017-11-23 16:25:29+00:00
- **Authors**: Pulkit Kumar, Monika Grewal, Muktabh Mayank Srivastava
- **Comment**: Submitted to CVPR 2018
- **Journal**: None
- **Summary**: Chest X-ray is one of the most accessible medical imaging technique for diagnosis of multiple diseases. With the availability of ChestX-ray14, which is a massive dataset of chest X-ray images and provides annotations for 14 thoracic diseases; it is possible to train Deep Convolutional Neural Networks (DCNN) to build Computer Aided Diagnosis (CAD) systems. In this work, we experiment a set of deep learning models and present a cascaded deep neural network that can diagnose all 14 pathologies better than the baseline and is competitive with other published methods. Our work provides the quantitative results to answer following research questions for the dataset: 1) What loss functions to use for training DCNN from scratch on ChestX-ray14 dataset that demonstrates high class imbalance and label co occurrence? 2) How to use cascading to model label dependency and to improve accuracy of the deep learning model?



### DNN-Buddies: A Deep Neural Network-Based Estimation Metric for the Jigsaw Puzzle Problem
- **Arxiv ID**: http://arxiv.org/abs/1711.08762v1
- **DOI**: 10.1007/978-3-319-44781-0_21
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.08762v1)
- **Published**: 2017-11-23 16:32:57+00:00
- **Updated**: 2017-11-23 16:32:57+00:00
- **Authors**: Dror Sholomon, Eli David, Nathan S. Netanyahu
- **Comment**: None
- **Journal**: International Conference on Artificial Neural Networks (ICANN),
  Springer LNCS, Vol. 9887, pp. 170-178, Barcelona, Spain, September 2016
- **Summary**: This paper introduces the first deep neural network-based estimation metric for the jigsaw puzzle problem. Given two puzzle piece edges, the neural network predicts whether or not they should be adjacent in the correct assembly of the puzzle, using nothing but the pixels of each piece. The proposed metric exhibits an extremely high precision even though no manual feature extraction is performed. When incorporated into an existing puzzle solver, the solution's accuracy increases significantly, achieving thereby a new state-of-the-art standard.



### Online and Batch Supervised Background Estimation via L1 Regression
- **Arxiv ID**: http://arxiv.org/abs/1712.02249v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.02249v1)
- **Published**: 2017-11-23 16:34:20+00:00
- **Updated**: 2017-11-23 16:34:20+00:00
- **Authors**: Aritra Dutta, Peter Richtarik
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a surprisingly simple model for supervised video background estimation. Our model is based on $\ell_1$ regression. As existing methods for $\ell_1$ regression do not scale to high-resolution videos, we propose several simple and scalable methods for solving the problem, including iteratively reweighted least squares, a homotopy method, and stochastic gradient descent. We show through extensive experiments that our model and methods match or outperform the state-of-the-art online and batch methods in virtually all quantitative and qualitative measures.



### DeepPainter: Painter Classification Using Deep Convolutional Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1711.08763v1
- **DOI**: 10.1007/978-3-319-44781-0_3
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.08763v1)
- **Published**: 2017-11-23 16:36:28+00:00
- **Updated**: 2017-11-23 16:36:28+00:00
- **Authors**: Eli David, Nathan S. Netanyahu
- **Comment**: None
- **Journal**: International Conference on Artificial Neural Networks (ICANN),
  Springer LNCS, Vol. 9887, pp. 20-28, Barcelona, Spain, September 2016
- **Summary**: In this paper we describe the problem of painter classification, and propose a novel approach based on deep convolutional autoencoder neural networks. While previous approaches relied on image processing and manual feature extraction from paintings, our approach operates on the raw pixel level, without any preprocessing or manual feature extraction. We first train a deep convolutional autoencoder on a dataset of paintings, and subsequently use it to initialize a supervised convolutional neural network for the classification phase.   The proposed approach substantially outperforms previous methods, improving the previous state-of-the-art for the 3-painter classification problem from 90.44% accuracy (previous state-of-the-art) to 96.52% accuracy, i.e., a 63% reduction in error rate.



### Region-based Quality Estimation Network for Large-scale Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1711.08766v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08766v2)
- **Published**: 2017-11-23 16:40:51+00:00
- **Updated**: 2017-12-21 12:25:26+00:00
- **Authors**: Guanglu Song, Biao Leng, Yu Liu, Congrui Hetang, Shaofan Cai
- **Comment**: Accepted by AAAI 2018
- **Journal**: None
- **Summary**: One of the major restrictions on the performance of video-based person re-id is partial noise caused by occlusion, blur and illumination. Since different spatial regions of a single frame have various quality, and the quality of the same region also varies across frames in a tracklet, a good way to address the problem is to effectively aggregate complementary information from all frames in a sequence, using better regions from other frames to compensate the influence of an image region with poor quality. To achieve this, we propose a novel Region-based Quality Estimation Network (RQEN), in which an ingenious training mechanism enables the effective learning to extract the complementary region-based information between different frames. Compared with other feature extraction methods, we achieved comparable results of 92.4%, 76.1% and 77.83% on the PRID 2011, iLIDS-VID and MARS, respectively. In addition, to alleviate the lack of clean large-scale person re-id datasets for the community, this paper also contributes a new high-quality dataset, named "Labeled Pedestrian in the Wild (LPW)" which contains 7,694 tracklets with over 590,000 images. Despite its relatively large scale, the annotations also possess high cleanliness. Moreover, it's more challenging in the following aspects: the age of characters varies from childhood to elderhood; the postures of people are diverse, including running and cycling in addition to the normal walking state.



### 3D Based Landmark Tracker Using Superpixels Based Segmentation for Neuroscience and Biomechanics Studies
- **Arxiv ID**: http://arxiv.org/abs/1711.08785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08785v1)
- **Published**: 2017-11-23 17:33:16+00:00
- **Updated**: 2017-11-23 17:33:16+00:00
- **Authors**: Omid Haji Maghsoudi, Andrew Spence
- **Comment**: Submitted to CVPR 2018
- **Journal**: None
- **Summary**: Examining locomotion has improved our basic understanding of motor control and aided in treating motor impairment. Mice and rats are premier models of human disease and increasingly the model systems of choice for basic neuroscience. High frame rates (250 Hz) are needed to quantify the kinematics of these running rodents. Manual tracking, especially for multiple markers, becomes time-consuming and impossible for large sample sizes. Therefore, the need for automatic segmentation of these markers has grown in recent years. Here, we address this need by presenting a method to segment the markers using the SLIC superpixel method. The 2D coordinates on the image plane are projected to a 3D domain using direct linear transform (DLT) and a 3D Kalman filter has been used to predict the position of markers based on the speed and position of markers from the previous frames. Finally, a probabilistic function is used to find the best match among superpixels. The method is evaluated for different difficulties for tracking of the markers and it achieves 95% correct labeling of markers.



### Visual Speech Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1711.08789v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1711.08789v3)
- **Published**: 2017-11-23 17:51:46+00:00
- **Updated**: 2018-06-13 07:25:47+00:00
- **Authors**: Aviv Gabbay, Asaph Shamir, Shmuel Peleg
- **Comment**: Accepted to Interspeech 2018. Supplementary video:
  https://www.youtube.com/watch?v=nyYarDGpcYA
- **Journal**: None
- **Summary**: When video is shot in noisy environment, the voice of a speaker seen in the video can be enhanced using the visible mouth movements, reducing background noise. While most existing methods use audio-only inputs, improved performance is obtained with our visual speech enhancement, based on an audio-visual neural network. We include in the training data videos to which we added the voice of the target speaker as background noise. Since the audio input is not sufficient to separate the voice of a speaker from his own voice, the trained model better exploits the visual input and generalizes well to different noise types. The proposed model outperforms prior audio visual methods on two public lipreading datasets. It is also the first to be demonstrated on a dataset not designed for lipreading, such as the weekly addresses of Barack Obama.



### A Dictionary Approach to Identifying Transient RFI
- **Arxiv ID**: http://arxiv.org/abs/1711.08823v1
- **DOI**: 10.1029/2018RS006538
- **Categories**: **astro-ph.IM**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1711.08823v1)
- **Published**: 2017-11-23 20:19:26+00:00
- **Updated**: 2017-11-23 20:19:26+00:00
- **Authors**: Daniel Czech, Amit Mishra, Michael Inggs
- **Comment**: None
- **Journal**: None
- **Summary**: As radio telescopes become more sensitive, the damaging effects of radio frequency interference (RFI) become more apparent. Near radio telescope arrays, RFI sources are often easily removed or replaced; the challenge lies in identifying them. Transient (impulsive) RFI is particularly difficult to identify. We propose a novel dictionary-based approach to transient RFI identification. RFI events are treated as sequences of sub-events, drawn from particular labelled classes. We demonstrate an automated method of extracting and labelling sub-events using a dataset of transient RFI. A dictionary of labels may be used in conjunction with hidden Markov models to identify the sources of RFI events reliably. We attain improved classification accuracy over traditional approaches such as SVMs or a na\"ive kNN classifier. Finally, we investigate why transient RFI is difficult to classify. We show that cluster separation in the principal components domain is influenced by the mains supply phase for certain sources.



