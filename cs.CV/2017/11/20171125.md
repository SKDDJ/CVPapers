# Arxiv Papers in cs.CV on 2017-11-25
### Real-Time Capable Micro-Doppler Signature Decomposition of Walking Human Limbs
- **Arxiv ID**: http://arxiv.org/abs/1711.09175v1
- **DOI**: 10.1109/RADAR.2017.7944367
- **Categories**: **cs.CV**, eess.SP, 68T10 (Primary), 68T40 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/1711.09175v1)
- **Published**: 2017-11-25 01:28:00+00:00
- **Updated**: 2017-11-25 01:28:00+00:00
- **Authors**: Sherif Abdulatif, Fady Aziz, Bernhard Kleiner, Urs Schneider
- **Comment**: 6 pages, IEEE RadarConf 17
- **Journal**: IEEE Radar Conference 2017 1093 1098
- **Summary**: Unique micro-Doppler signature ($\boldsymbol{\mu}$-D) of a human body motion can be analyzed as the superposition of different body parts $\boldsymbol{\mu}$-D signatures. Extraction of human limbs $\boldsymbol{\mu}$-D signatures in real-time can be used to detect, classify and track human motion especially for safety application. In this paper, two methods are combined to simulate $\boldsymbol{\mu}$-D signatures of a walking human. Furthermore, a novel limbs $\mu$-D signature time independent decomposition feasibility study is presented based on features as $\mu$-D signatures and range profiles also known as micro-Range ($\mu$-R). Walking human body parts can be divided into four classes (base, arms, legs, feet) and a decision tree classifier is used. Validation is done and the classifier is able to decompose $\mu$-D signatures of limbs from a walking human signature on real-time basis.



### Micro-Doppler Based Human-Robot Classification Using Ensemble and Deep Learning Approaches
- **Arxiv ID**: http://arxiv.org/abs/1711.09177v3
- **DOI**: 10.1109/RADAR.2018.8378705
- **Categories**: **cs.CV**, eess.SP, 68T10 (Primary), 68T40 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/1711.09177v3)
- **Published**: 2017-11-25 01:38:03+00:00
- **Updated**: 2018-02-26 09:13:32+00:00
- **Authors**: Sherif Abdulatif, Qian Wei, Fady Aziz, Bernhard Kleiner, Urs Schneider
- **Comment**: 6 pages, accepted in IEEE Radar Conference 2018
- **Journal**: IEEE Radar Conference 2017 1043 1048
- **Summary**: Radar sensors can be used for analyzing the induced frequency shifts due to micro-motions in both range and velocity dimensions identified as micro-Doppler ($\boldsymbol{\mu}$-D) and micro-Range ($\boldsymbol{\mu}$-R), respectively. Different moving targets will have unique $\boldsymbol{\mu}$-D and $\boldsymbol{\mu}$-R signatures that can be used for target classification. Such classification can be used in numerous fields, such as gait recognition, safety and surveillance. In this paper, a 25 GHz FMCW Single-Input Single-Output (SISO) radar is used in industrial safety for real-time human-robot identification. Due to the real-time constraint, joint Range-Doppler (R-D) maps are directly analyzed for our classification problem. Furthermore, a comparison between the conventional classical learning approaches with handcrafted extracted features, ensemble classifiers and deep learning approaches is presented. For ensemble classifiers, restructured range and velocity profiles are passed directly to ensemble trees, such as gradient boosting and random forest without feature extraction. Finally, a Deep Convolutional Neural Network (DCNN) is used and raw R-D images are directly fed into the constructed network. DCNN shows a superior performance of 99\% accuracy in identifying humans from robots on a single R-D map.



### Multiple Instance Curriculum Learning for Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1711.09191v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09191v1)
- **Published**: 2017-11-25 05:08:09+00:00
- **Updated**: 2017-11-25 05:08:09+00:00
- **Authors**: Siyang Li, Xiangxin Zhu, Qin Huang, Hao Xu, C. -C. Jay Kuo
- **Comment**: Published in BMVC 2017
- **Journal**: None
- **Summary**: When supervising an object detector with weakly labeled data, most existing approaches are prone to trapping in the discriminative object parts, e.g., finding the face of a cat instead of the full body, due to lacking the supervision on the extent of full objects. To address this challenge, we incorporate object segmentation into the detector training, which guides the model to correctly localize the full objects. We propose the multiple instance curriculum learning (MICL) method, which injects curriculum learning (CL) into the multiple instance learning (MIL) framework. The MICL method starts by automatically picking the easy training examples, where the extent of the segmentation masks agree with detection bounding boxes. The training set is gradually expanded to include harder examples to train strong detectors that handle complex images. The proposed MICL method with segmentation in the loop outperforms the state-of-the-art weakly supervised object detectors by a substantial margin on the PASCAL VOC datasets.



### Stacked Kernel Network
- **Arxiv ID**: http://arxiv.org/abs/1711.09219v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.09219v1)
- **Published**: 2017-11-25 09:01:40+00:00
- **Updated**: 2017-11-25 09:01:40+00:00
- **Authors**: Shuai Zhang, Jianxin Li, Pengtao Xie, Yingchun Zhang, Minglai Shao, Haoyi Zhou, Mengyi Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Kernel methods are powerful tools to capture nonlinear patterns behind data. They implicitly learn high (even infinite) dimensional nonlinear features in the Reproducing Kernel Hilbert Space (RKHS) while making the computation tractable by leveraging the kernel trick. Classic kernel methods learn a single layer of nonlinear features, whose representational power may be limited. Motivated by recent success of deep neural networks (DNNs) that learn multi-layer hierarchical representations, we propose a Stacked Kernel Network (SKN) that learns a hierarchy of RKHS-based nonlinear features. SKN interleaves several layers of nonlinear transformations (from a linear space to a RKHS) and linear transformations (from a RKHS to a linear space). Similar to DNNs, a SKN is composed of multiple layers of hidden units, but each parameterized by a RKHS function rather than a finite-dimensional vector. We propose three ways to represent the RKHS functions in SKN: (1)nonparametric representation, (2)parametric representation and (3)random Fourier feature representation. Furthermore, we expand SKN into CNN architecture called Stacked Kernel Convolutional Network (SKCN). SKCN learning a hierarchy of RKHS-based nonlinear features by convolutional operation with each filter also parameterized by a RKHS function rather than a finite-dimensional matrix in CNN, which is suitable for image inputs. Experiments on various datasets demonstrate the effectiveness of SKN and SKCN, which outperform the competitive methods.



### CondenseNet: An Efficient DenseNet using Learned Group Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1711.09224v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09224v2)
- **Published**: 2017-11-25 09:34:12+00:00
- **Updated**: 2018-06-07 14:59:11+00:00
- **Authors**: Gao Huang, Shichen Liu, Laurens van der Maaten, Kilian Q. Weinberger
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are increasingly used on mobile devices, where computational resources are limited. In this paper we develop CondenseNet, a novel network architecture with unprecedented efficiency. It combines dense connectivity with a novel module called learned group convolution. The dense connectivity facilitates feature re-use in the network, whereas learned group convolutions remove connections between layers for which this feature re-use is superfluous. At test time, our model can be implemented using standard group convolutions, allowing for efficient computation in practice. Our experiments show that CondenseNets are far more efficient than state-of-the-art compact convolutional networks such as MobileNets and ShuffleNets.



### On the Relations of Correlation Filter Based Trackers and Struck
- **Arxiv ID**: http://arxiv.org/abs/1711.09243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09243v1)
- **Published**: 2017-11-25 14:44:29+00:00
- **Updated**: 2017-11-25 14:44:29+00:00
- **Authors**: Jinqiao Wang, Ming Tang, Linyu Zheng, Jiayi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, two types of trackers, namely correlation filter based tracker (CF tracker) and structured output tracker (Struck), have exhibited the state-of-the-art performance. However, there seems to be lack of analytic work on their relations in the computer vision community. In this paper, we investigate two state-of-the-art CF trackers, i.e., spatial regularization discriminative correlation filter (SRDCF) and correlation filter with limited boundaries (CFLB), and Struck, and reveal their relations. Specifically, after extending the CFLB to its multiple channel version we prove the relation between SRDCF and CFLB on the condition that the spatial regularization factor of SRDCF is replaced by the masking matrix of CFLB. We also prove the asymptotical approximate relation between SRDCF and Struck on the conditions that the spatial regularization factor of SRDCF is replaced by an indicator function of object bounding box, the weights of SRDCF in its loss item are replaced by those of Struck, the linear kernel is employed by Struck, and the search region tends to infinity. Extensive experiments on public benchmarks OTB50 and OTB100 are conducted to verify our theoretical results. Moreover, we explain how detailed differences among SRDCF, CFLB, and Struck would give rise to slightly different performances on visual sequences



### Learning 3D Human Pose from Structure and Motion
- **Arxiv ID**: http://arxiv.org/abs/1711.09250v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09250v2)
- **Published**: 2017-11-25 15:25:16+00:00
- **Updated**: 2018-07-03 20:52:54+00:00
- **Authors**: Rishabh Dabral, Anurag Mundhada, Uday Kusupati, Safeer Afaque, Abhishek Sharma, Arjun Jain
- **Comment**: ECCV 2018. Project page: https://www.cse.iitb.ac.in/~rdabral/3DPose/
- **Journal**: None
- **Summary**: 3D human pose estimation from a single image is a challenging problem, especially for in-the-wild settings due to the lack of 3D annotated data. We propose two anatomically inspired loss functions and use them with a weakly-supervised learning framework to jointly learn from large-scale in-the-wild 2D and indoor/synthetic 3D data. We also present a simple temporal network that exploits temporal and structural cues present in predicted pose sequences to temporally harmonize the pose estimations. We carefully analyze the proposed contributions through loss surface visualizations and sensitivity analysis to facilitate deeper understanding of their working mechanism. Our complete pipeline improves the state-of-the-art by 11.8% and 12% on Human3.6M and MPI-INF-3DHP, respectively, and runs at 30 FPS on a commodity graphics card.



### Predictive Learning: Using Future Representation Learning Variantial Autoencoder for Human Action Prediction
- **Arxiv ID**: http://arxiv.org/abs/1711.09265v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09265v2)
- **Published**: 2017-11-25 17:43:12+00:00
- **Updated**: 2017-12-12 11:22:18+00:00
- **Authors**: Yu Runsheng, Shi Zhenyu, Ma Qiongxiong, Qing Laiyun
- **Comment**: None
- **Journal**: None
- **Summary**: The unsupervised Pretraining method has been widely used in aiding human action recognition. However, existing methods focus on reconstructing the already present frames rather than generating frames which happen in future.In this paper, We propose an improved Variantial Autoencoder model to extract the features with a high connection to the coming scenarios, also known as Predictive Learning. Our framework lists as following: two steam 3D-convolution neural networks are used to extract both spatial and temporal information as latent variables. Then a resample method is introduced to create new normal distribution probabilistic latent variables and finally, the deconvolution neural network will use these latent variables generate next frames. Through this possess, we train the model to focus more on how to generate the future and thus it will extract the future high connected features. In the experiment stage, A large number of experiments on UT and UCF101 datasets reveal that future generation aids Prediction does improve the performance. Moreover, the Future Representation Learning Network reach a higher score than other methods when in half observation. This means that Future Representation Learning is better than the traditional Representation Learning and other state- of-the-art methods in solving the human action prediction problems to some extends.



### Gradually Updated Neural Networks for Large-Scale Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.09280v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09280v2)
- **Published**: 2017-11-25 20:17:54+00:00
- **Updated**: 2018-02-10 21:00:21+00:00
- **Authors**: Siyuan Qiao, Zhishuai Zhang, Wei Shen, Bo Wang, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Depth is one of the keys that make neural networks succeed in the task of large-scale image recognition. The state-of-the-art network architectures usually increase the depths by cascading convolutional layers or building blocks. In this paper, we present an alternative method to increase the depth. Our method is by introducing computation orderings to the channels within convolutional layers or blocks, based on which we gradually compute the outputs in a channel-wise manner. The added orderings not only increase the depths and the learning capacities of the networks without any additional computation costs, but also eliminate the overlap singularities so that the networks are able to converge faster and perform better. Experiments show that the networks based on our method achieve the state-of-the-art performances on CIFAR and ImageNet datasets.



