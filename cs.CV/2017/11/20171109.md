# Arxiv Papers in cs.CV on 2017-11-09
### Fingerprint Orientation Refinement through Iterative Smoothing
- **Arxiv ID**: http://arxiv.org/abs/1711.03214v1
- **DOI**: 10.5121/sipij.2017.8503
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03214v1)
- **Published**: 2017-11-09 00:06:27+00:00
- **Updated**: 2017-11-09 00:06:27+00:00
- **Authors**: Pierluigi Maponi, Riccardo Piergallini, Filippo Santarelli
- **Comment**: None
- **Journal**: Signal & Image Processing: An International Journal, 8(5), 29-43,
  2017
- **Summary**: We propose a new gradient-based method for the extraction of the orientation field associated to a fingerprint, and a regularisation procedure to improve the orientation field computed from noisy fingerprint images. The regularisation algorithm is based on three new integral operators, introduced and discussed in this paper. A pre-processing technique is also proposed to achieve better performances of the algorithm. The results of a numerical experiment are reported to give an evidence of the efficiency of the proposed algorithm.



### Predicting Scene Parsing and Motion Dynamics in the Future
- **Arxiv ID**: http://arxiv.org/abs/1711.03270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03270v1)
- **Published**: 2017-11-09 06:34:07+00:00
- **Updated**: 2017-11-09 06:34:07+00:00
- **Authors**: Xiaojie Jin, Huaxin Xiao, Xiaohui Shen, Jimei Yang, Zhe Lin, Yunpeng Chen, Zequn Jie, Jiashi Feng, Shuicheng Yan
- **Comment**: To appear in NIPS 2017
- **Journal**: None
- **Summary**: The ability of predicting the future is important for intelligent systems, e.g. autonomous vehicles and robots to plan early and make decisions accordingly. Future scene parsing and optical flow estimation are two key tasks that help agents better understand their environments as the former provides dense semantic information, i.e. what objects will be present and where they will appear, while the latter provides dense motion information, i.e. how the objects will move. In this paper, we propose a novel model to simultaneously predict scene parsing and optical flow in unobserved future video frames. To our best knowledge, this is the first attempt in jointly predicting scene parsing and motion dynamics. In particular, scene parsing enables structured motion prediction by decomposing optical flow into different groups while optical flow estimation brings reliable pixel-wise correspondence to scene parsing. By exploiting this mutually beneficial relationship, our model shows significantly better parsing and motion prediction results when compared to well-established baselines and individual prediction models on the large-scale Cityscapes dataset. In addition, we also demonstrate that our model can be used to predict the steering angle of the vehicles, which further verifies the ability of our model to learn latent representations of scene dynamics.



### Two-stream Collaborative Learning with Spatial-Temporal Attention for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1711.03273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03273v1)
- **Published**: 2017-11-09 06:49:21+00:00
- **Updated**: 2017-11-09 06:49:21+00:00
- **Authors**: Yuxin Peng, Yunzhen Zhao, Junchao Zhang
- **Comment**: 14 pages, accepted by IEEE Transactions on Circuits and Systems for
  Video Technology
- **Journal**: None
- **Summary**: Video classification is highly important with wide applications, such as video search and intelligent surveillance. Video naturally consists of static and motion information, which can be represented by frame and optical flow. Recently, researchers generally adopt the deep networks to capture the static and motion information \textbf{\emph{separately}}, which mainly has two limitations: (1) Ignoring the coexistence relationship between spatial and temporal attention, while they should be jointly modelled as the spatial and temporal evolutions of video, thus discriminative video features can be extracted.(2) Ignoring the strong complementarity between static and motion information coexisted in video, while they should be collaboratively learned to boost each other. For addressing the above two limitations, this paper proposes the approach of two-stream collaborative learning with spatial-temporal attention (TCLSTA), which consists of two models: (1) Spatial-temporal attention model: The spatial-level attention emphasizes the salient regions in frame, and the temporal-level attention exploits the discriminative frames in video. They are jointly learned and mutually boosted to learn the discriminative static and motion features for better classification performance. (2) Static-motion collaborative model: It not only achieves mutual guidance on static and motion information to boost the feature learning, but also adaptively learns the fusion weights of static and motion streams, so as to exploit the strong complementarity between static and motion information to promote video classification. Experiments on 4 widely-used datasets show that our TCLSTA approach achieves the best performance compared with more than 10 state-of-the-art methods.



### Feed Forward and Backward Run in Deep Convolution Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1711.03278v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03278v1)
- **Published**: 2017-11-09 07:32:30+00:00
- **Updated**: 2017-11-09 07:32:30+00:00
- **Authors**: Pushparaja Murugan
- **Comment**: 20 pages, 20th International Conference on Computer Vision and Image
  Processing
- **Journal**: None
- **Summary**: Convolution Neural Networks (CNN), known as ConvNets are widely used in many visual imagery application, object classification, speech recognition. After the implementation and demonstration of the deep convolution neural network in Imagenet classification in 2012 by krizhevsky, the architecture of deep Convolution Neural Network is attracted many researchers. This has led to the major development in Deep learning frameworks such as Tensorflow, caffe, keras, theno. Though the implementation of deep learning is quite possible by employing deep learning frameworks, mathematical theory and concepts are harder to understand for new learners and practitioners. This article is intended to provide an overview of ConvNets architecture and to explain the mathematical theory behind it including activation function, loss function, feedforward and backward propagation. In this article, grey scale image is taken as input information image, ReLU and Sigmoid activation function are considered for developing the architecture and cross-entropy loss function are used for computing the difference between predicted value and actual value. The architecture is developed in such a way that it can contain one convolution layer, one pooling layer, and multiple dense layers



### Keypoint-based object tracking and localization using networks of low-power embedded smart cameras
- **Arxiv ID**: http://arxiv.org/abs/1712.01635v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01635v1)
- **Published**: 2017-11-09 07:54:54+00:00
- **Updated**: 2017-11-09 07:54:54+00:00
- **Authors**: Ibrahim Abdelkader, Yasser El-Sonbaty, Mohamed El-Habrouk
- **Comment**: None
- **Journal**: International Conferences Computer Graphics, Visualization,
  Computer Vision and Image Processing 2017
- **Summary**: Object tracking and localization is a complex task that typically requires processing power beyond the capabilities of low-power embedded cameras. This paper presents a new approach to real-time object tracking and localization using multi-view binary keypoints descriptor. The proposed approach offers a compromise between processing power, accuracy and networking bandwidth and has been tested using multiple distributed low-power smart cameras. Additionally, multiple optimization techniques are presented to improve the performance of the keypoints descriptor for low-power embedded systems.



### Fast camera focus estimation for gaze-based focus control
- **Arxiv ID**: http://arxiv.org/abs/1711.03306v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.5; I.4.6; I.4.7; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1711.03306v1)
- **Published**: 2017-11-09 09:47:37+00:00
- **Updated**: 2017-11-09 09:47:37+00:00
- **Authors**: Wolfgang Fuhl, Thiago Santini, Enkelejda Kasneci
- **Comment**: None
- **Journal**: None
- **Summary**: Many cameras implement auto-focus functionality. However, they typically require the user to manually identify the location to be focused on. While such an approach works for temporally-sparse autofocusing functionality (e.g., photo shooting), it presents extreme usability problems when the focus must be quickly switched between multiple areas (and depths) of interest - e.g., in a gaze-based autofocus approach. This work introduces a novel, real-time auto-focus approach based on eye-tracking, which enables the user to shift the camera focus plane swiftly based solely on the gaze information. Moreover, the proposed approach builds a graph representation of the image to estimate depth plane surfaces and runs in real time (requiring ~20ms on a single i5 core), thus allowing for the depth map estimation to be performed dynamically. We evaluated our algorithm for gaze-based depth estimation against state-of-the-art approaches based on eight new data sets with flat, skewed, and round surfaces, as well as publicly available datasets.



### Frangi-Net: A Neural Network Approach to Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.03345v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03345v1)
- **Published**: 2017-11-09 12:15:33+00:00
- **Updated**: 2017-11-09 12:15:33+00:00
- **Authors**: Weilin Fu, Katharina Breininger, Tobias WÃ¼rfl, Nishant Ravikumar, Roman Schaffert, Andreas Maier
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we reformulate the conventional 2-D Frangi vesselness measure into a pre-weighted neural network ("Frangi-Net"), and illustrate that the Frangi-Net is equivalent to the original Frangi filter. Furthermore, we show that, as a neural network, Frangi-Net is trainable. We evaluate the proposed method on a set of 45 high resolution fundus images. After fine-tuning, we observe both qualitative and quantitative improvements in the segmentation quality compared to the original Frangi measure, with an increase up to $17\%$ in F1 score.



### Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz
- **Arxiv ID**: http://arxiv.org/abs/1711.03357v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, quant-ph
- **Links**: [PDF](http://arxiv.org/pdf/1711.03357v3)
- **Published**: 2017-11-09 12:55:59+00:00
- **Updated**: 2018-12-12 23:55:50+00:00
- **Authors**: Andrew Hallam, Edward Grant, Vid Stojevic, Simone Severini, Andrew G. Green
- **Comment**: 8 pages, 2 figures
- **Journal**: None
- **Summary**: This paper demonstrates a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA). We employ MERA as a replacement for the fully connected layers in a convolutional neural network and test this implementation on the CIFAR-10 and CIFAR-100 datasets. The proposed method outperforms factorization using tensor trains, providing greater compression for the same level of accuracy and greater accuracy for the same level of compression. We demonstrate MERA layers with 14000 times fewer parameters and a reduction in accuracy of less than 1% compared to the equivalent fully connected layers, scaling like O(N).



### One-pass Person Re-identification by Sketch Online Discriminant Analysis
- **Arxiv ID**: http://arxiv.org/abs/1711.03368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03368v1)
- **Published**: 2017-11-09 13:29:33+00:00
- **Updated**: 2017-11-09 13:29:33+00:00
- **Authors**: Wei-Hong Li, Zhuowei Zhong, Wei-Shi Zheng
- **Comment**: Online learning, Person re-identification, Discriminant feature
  extraction
- **Journal**: None
- **Summary**: Person re-identification (re-id) is to match people across disjoint camera views in a multi-camera system, and re-id has been an important technology applied in smart city in recent years. However, the majority of existing person re-id methods are not designed for processing sequential data in an online way. This ignores the real-world scenario that person images detected from multi-cameras system are coming sequentially. While there is a few work on discussing online re-id, most of them require considerable storage of all passed data samples that have been ever observed, and this could be unrealistic for processing data from a large camera network. In this work, we present an onepass person re-id model that adapts the re-id model based on each newly observed data and no passed data are directly used for each update. More specifically, we develop an Sketch online Discriminant Analysis (SoDA) by embedding sketch processing into Fisher discriminant analysis (FDA). SoDA can efficiently keep the main data variations of all passed samples in a low rank matrix when processing sequential data samples, and estimate the approximate within-class variance (i.e. within-class covariance matrix) from the sketch data information. We provide theoretical analysis on the effect of the estimated approximate within-class covariance matrix. In particular, we derive upper and lower bounds on the Fisher discriminant score (i.e. the quotient between between-class variation and within-class variation after feature transformation) in order to investigate how the optimal feature transformation learned by SoDA sequentially approximates the offline FDA that is learned on all observed data. Extensive experimental results have shown the effectiveness of our SoDA and empirically support our theoretical analysis.



### Material Classification in the Wild: Do Synthesized Training Data Generalise Better than Real-World Training Data?
- **Arxiv ID**: http://arxiv.org/abs/1711.03874v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03874v1)
- **Published**: 2017-11-09 15:48:26+00:00
- **Updated**: 2017-11-09 15:48:26+00:00
- **Authors**: Grigorios Kalliatakis, Anca Sticlaru, George Stamatiadis, Shoaib Ehsan, Ales Leonardis, Juergen Gall, Klaus D. McDonald-Maier
- **Comment**: accepted for publication in VISAPP 2018. arXiv admin note: text
  overlap with arXiv:1703.04101
- **Journal**: None
- **Summary**: We question the dominant role of real-world training images in the field of material classification by investigating whether synthesized data can generalise more effectively than real-world data. Experimental results on three challenging real-world material databases show that the best performing pre-trained convolutional neural network (CNN) architectures can achieve up to 91.03% mean average precision when classifying materials in cross-dataset scenarios. We demonstrate that synthesized data achieve an improvement on mean average precision when used as training data and in conjunction with pre-trained CNN architectures, which spans from ~ 5% to ~ 19% across three widely used material databases of real-world images.



### Making a long story short: A Multi-Importance fast-forwarding egocentric videos with the emphasis on relevant objects
- **Arxiv ID**: http://arxiv.org/abs/1711.03473v3
- **DOI**: 10.1016/j.jvcir.2018.02.013
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03473v3)
- **Published**: 2017-11-09 17:03:29+00:00
- **Updated**: 2018-03-07 17:59:11+00:00
- **Authors**: Michel Melo Silva, Washington Luis Souza Ramos, Felipe Cadar Chamone, JoÃ£o Pedro Klock Ferreira, Mario Fernando Montenegro Campos, Erickson Rangel Nascimento
- **Comment**: Accepted to publication in the Journal of Visual Communication and
  Image Representation (JVCI) 2018. Project website:
  https://www.verlab.dcc.ufmg.br/semantic-hyperlapse
- **Journal**: None
- **Summary**: The emergence of low-cost high-quality personal wearable cameras combined with the increasing storage capacity of video-sharing websites have evoked a growing interest in first-person videos, since most videos are composed of long-running unedited streams which are usually tedious and unpleasant to watch. State-of-the-art semantic fast-forward methods currently face the challenge of providing an adequate balance between smoothness in visual flow and the emphasis on the relevant parts. In this work, we present the Multi-Importance Fast-Forward (MIFF), a fully automatic methodology to fast-forward egocentric videos facing these challenges. The dilemma of defining what is the semantic information of a video is addressed by a learning process based on the preferences of the user. Results show that the proposed method keeps over $3$ times more semantic content than the state-of-the-art fast-forward. Finally, we discuss the need of a particular video stabilization technique for fast-forward egocentric videos.



### Learning Multi-Modal Word Representation Grounded in Visual Context
- **Arxiv ID**: http://arxiv.org/abs/1711.03483v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.03483v1)
- **Published**: 2017-11-09 17:28:07+00:00
- **Updated**: 2017-11-09 17:28:07+00:00
- **Authors**: Ãloi Zablocki, Benjamin Piwowarski, Laure Soulier, Patrick Gallinari
- **Comment**: None
- **Journal**: None
- **Summary**: Representing the semantics of words is a long-standing problem for the natural language processing community. Most methods compute word semantics given their textual context in large corpora. More recently, researchers attempted to integrate perceptual and visual features. Most of these works consider the visual appearance of objects to enhance word representations but they ignore the visual environment and context in which objects appear. We propose to unify text-based techniques with vision-based techniques by simultaneously leveraging textual and visual context to learn multimodal word embeddings. We explore various choices for what can serve as a visual context and present an end-to-end method to integrate visual context elements in a multimodal skip-gram model. We provide experiments and extensive analysis of the obtained results.



### Toward Depth Estimation Using Mask-Based Lensless Cameras
- **Arxiv ID**: http://arxiv.org/abs/1711.03527v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03527v1)
- **Published**: 2017-11-09 18:54:43+00:00
- **Updated**: 2017-11-09 18:54:43+00:00
- **Authors**: M. Salman Asif
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, coded masks have been used to demonstrate a thin form-factor lensless camera, FlatCam, in which a mask is placed immediately on top of a bare image sensor. In this paper, we present an imaging model and algorithm to jointly estimate depth and intensity information in the scene from a single or multiple FlatCams. We use a light field representation to model the mapping of 3D scene onto the sensor in which light rays from different depths yield different modulation patterns. We present a greedy depth pursuit algorithm to search the 3D volume and estimate the depth and intensity of each pixel within the camera field-of-view. We present simulation results to analyze the performance of our proposed model and algorithm with different FlatCam settings.



### Exploiting ConvNet Diversity for Flooding Identification
- **Arxiv ID**: http://arxiv.org/abs/1711.03564v2
- **DOI**: 10.1109/LGRS.2018.2845549
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03564v2)
- **Published**: 2017-11-09 19:19:00+00:00
- **Updated**: 2018-06-05 14:05:42+00:00
- **Authors**: Keiller Nogueira, Samuel G. Fadel, Ãcaro C. Dourado, Rafael de O. Werneck, Javier A. V. MuÃ±oz, OtÃ¡vio A. B. Penatti, Rodrigo T. Calumby, Lin Tzy Li, Jefersson A. dos Santos, Ricardo da S. Torres
- **Comment**: Work winner of the Flood-Detection in Satellite Images, a subtask of
  2017 Multimedia Satellite Task (MediaEval Benchmark) Accepted for publication
  in the Geoscience and Remote Sensing Letters (GRSL)
- **Journal**: None
- **Summary**: Flooding is the world's most costly type of natural disaster in terms of both economic losses and human causalities. A first and essential procedure towards flood monitoring is based on identifying the area most vulnerable to flooding, which gives authorities relevant regions to focus. In this work, we propose several methods to perform flooding identification in high-resolution remote sensing images using deep learning. Specifically, some proposed techniques are based upon unique networks, such as dilated and deconvolutional ones, while other was conceived to exploit diversity of distinct networks in order to extract the maximum performance of each classifier. Evaluation of the proposed algorithms were conducted in a high-resolution remote sensing dataset. Results show that the proposed algorithms outperformed several state-of-the-art baselines, providing improvements ranging from 1 to 4% in terms of the Jaccard Index.



