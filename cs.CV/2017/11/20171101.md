# Arxiv Papers in cs.CV on 2017-11-01
### Improving Object Localization with Fitness NMS and Bounded IoU Loss
- **Arxiv ID**: http://arxiv.org/abs/1711.00164v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00164v3)
- **Published**: 2017-11-01 02:02:23+00:00
- **Updated**: 2018-03-12 23:54:45+00:00
- **Authors**: Lachlan Tychsen-Smith, Lars Petersson
- **Comment**: CVPR2018 Main Conference (Poster)
- **Journal**: None
- **Summary**: We demonstrate that many detection methods are designed to identify only a sufficently accurate bounding box, rather than the best available one. To address this issue we propose a simple and fast modification to the existing methods called Fitness NMS. This method is tested with the DeNet model and obtains a significantly improved MAP at greater localization accuracies without a loss in evaluation rate, and can be used in conjunction with Soft NMS for additional improvements. Next we derive a novel bounding box regression loss based on a set of IoU upper bounds that better matches the goal of IoU maximization while still providing good convergence properties. Following these novelties we investigate RoI clustering schemes for improving evaluation rates for the DeNet wide model variants and provide an analysis of localization performance at various input image dimensions. We obtain a MAP of 33.6%@79Hz and 41.8%@5Hz for MSCOCO and a Titan X (Maxwell). Source code available from: https://github.com/lachlants/denet



### PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes
- **Arxiv ID**: http://arxiv.org/abs/1711.00199v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1711.00199v3)
- **Published**: 2017-11-01 04:10:58+00:00
- **Updated**: 2018-05-26 07:34:09+00:00
- **Authors**: Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, Dieter Fox
- **Comment**: Accepted to RSS 2018
- **Journal**: None
- **Summary**: Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation. PoseCNN estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables PoseCNN to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the YCB-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further refine the poses, our approach achieves state-of-the-art results on the challenging OccludedLINEMOD dataset. Our code and dataset are available at https://rse-lab.cs.washington.edu/projects/posecnn/.



### Towards Effective Low-bitwidth Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.00205v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00205v2)
- **Published**: 2017-11-01 04:41:29+00:00
- **Updated**: 2017-11-17 01:35:27+00:00
- **Authors**: Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, Ian Reid
- **Comment**: 11 pages. Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2018
- **Journal**: None
- **Summary**: This paper tackles the problem of training a deep convolutional neural network with both low-precision weights and low-bitwidth activations. Optimizing a low-precision network is very challenging since the training process can easily get trapped in a poor local minima, which results in substantial accuracy loss. To mitigate this problem, we propose three simple-yet-effective approaches to improve the network training. First, we propose to use a two-stage optimization strategy to progressively find good local minima. Specifically, we propose to first optimize a net with quantized weights and then quantized activations. This is in contrast to the traditional methods which optimize them simultaneously. Second, following a similar spirit of the first method, we propose another progressive optimization approach which progressively decreases the bit-width from high-precision to low-precision during the course of training. Third, we adopt a novel learning scheme to jointly train a full-precision model alongside the low-precision one. By doing so, the full-precision model provides hints to guide the low-precision model training. Extensive experiments on various datasets ( i.e., CIFAR-100 and ImageNet) show the effectiveness of the proposed methods. To highlight, using our methods to train a 4-bit precision network leads to no performance decrease in comparison with its full-precision counterpart with standard network architectures ( i.e., AlexNet and ResNet-50).



### Learning deep features for source color laser printer identification based on cascaded learning
- **Arxiv ID**: http://arxiv.org/abs/1711.00207v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1711.00207v1)
- **Published**: 2017-11-01 04:48:05+00:00
- **Updated**: 2017-11-01 04:48:05+00:00
- **Authors**: Do-Guk Kim, Jong-Uk Hou, Heung-Kyu Lee
- **Comment**: 11 pages, 16 figures, submitted to Signal Processing: Image
  Communication
- **Journal**: None
- **Summary**: Color laser printers have fast printing speed and high resolution, and forgeries using color laser printers can cause significant harm to society. A source printer identification technique can be employed as a countermeasure to those forgeries. This paper presents a color laser printer identification method based on cascaded learning of deep neural networks. The refiner network is trained by adversarial training to refine the synthetic dataset for halftone color decomposition. The halftone color decomposing ConvNet is trained with the refined dataset, and the trained knowledge is transferred to the printer identifying ConvNet to enhance the accuracy. The robustness about rotation and scaling is considered in training process, which is not considered in existing methods. Experiments are performed on eight color laser printers, and the performance is compared with several existing methods. The experimental results clearly show that the proposed method outperforms existing source color laser printer identification methods.



### 3D-SSD: Learning Hierarchical Features from RGB-D Images for Amodal 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1711.00238v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00238v2)
- **Published**: 2017-11-01 07:57:25+00:00
- **Updated**: 2018-02-21 09:06:33+00:00
- **Authors**: Qianhui Luo, Huifang Ma, Yue Wang, Li Tang, Rong Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims at developing a faster and a more accurate solution to the amodal 3D object detection problem for indoor scenes. It is achieved through a novel neural network that takes a pair of RGB-D images as the input and delivers oriented 3D bounding boxes as the output. The network, named 3D-SSD, composed of two parts: hierarchical feature fusion and multi-layer prediction. The hierarchical feature fusion combines appearance and geometric features from RGB-D images while the multi-layer prediction utilizes multi-scale features for object detection. As a result, the network can exploit 2.5D representations in a synergetic way to improve the accuracy and efficiency. The issue of object sizes is addressed by attaching a set of 3D anchor boxes with varying sizes to every location of the prediction layers. At the end stage, the category scores for 3D anchor boxes are generated with adjusted positions, sizes and orientations respectively, leading to the final detections using non-maximum suppression. In the training phase, the positive samples are identified with the aid of 2D ground truth to avoid the noisy estimation of depth from raw data, which guide to a better converged model. Experiments performed on the challenging SUN RGB-D dataset show that our algorithm outperforms the state-of-the-art Deep Sliding Shape by 10.2% mAP and 88x faster. Further, experiments also suggest our approach achieves comparable accuracy and is 386x faster than the state-of-art method on the NYUv2 dataset even with a smaller input image size.



### Query-free Clothing Retrieval via Implicit Relevance Feedback
- **Arxiv ID**: http://arxiv.org/abs/1711.00248v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1711.00248v1)
- **Published**: 2017-11-01 08:33:39+00:00
- **Updated**: 2017-11-01 08:33:39+00:00
- **Authors**: Zhuoxiang Chen, Zhe Xu, Ya Zhang, Xiao Gu
- **Comment**: 12 pages, under review at IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Image-based clothing retrieval is receiving increasing interest with the growth of online shopping. In practice, users may often have a desired piece of clothing in mind (e.g., either having seen it before on the street or requiring certain specific clothing attributes) but may be unable to supply an image as a query. We model this problem as a new type of image retrieval task in which the target image resides only in the user's mind (called "mental image retrieval" hereafter). Because of the absence of an explicit query image, we propose to solve this problem through relevance feedback. Specifically, a new Bayesian formulation is proposed that simultaneously models the retrieval target and its high-level representation in the mind of the user (called the "user metric" hereafter) as posterior distributions of pre-fetched shop images and heterogeneous features extracted from multiple clothing attributes, respectively. Requiring only clicks as user feedback, the proposed algorithm is able to account for the variability in human decision-making. Experiments with real users demonstrate the effectiveness of the proposed algorithm.



### Openmv: A Python powered, extensible machine vision camera
- **Arxiv ID**: http://arxiv.org/abs/1711.10464v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1711.10464v1)
- **Published**: 2017-11-01 08:52:12+00:00
- **Updated**: 2017-11-01 08:52:12+00:00
- **Authors**: Ibrahim Abdelkader, Yasser El-Sonbaty, Mohamed El-Habrouk
- **Comment**: None
- **Journal**: International Conferences Computer Graphics, Visualization,
  Computer Vision and Image Processing 2017
- **Summary**: Advances in semiconductor manufacturing processes and large scale integration keep pushing demanding applications further away from centralized processing, and closer to the edges of the network (i.e. Edge Computing). It has become possible to perform complex in-network image processing using low-power embedded smart cameras, enabling a multitude of new collaborative image processing applications. This paper introduces OpenMV, a new low-power smart camera that lends itself naturally to wireless sensor networks and machine vision applications. The uniqueness of this platform lies in running an embedded Python3 interpreter, allowing its peripherals and machine vision library to be scripted in Python. In addition, its hardware is extensible via modules that augment the platform with new capabilities, such as thermal imaging and networking modules.



### Adversarial Learning of Structure-Aware Fully Convolutional Networks for Landmark Localization
- **Arxiv ID**: http://arxiv.org/abs/1711.00253v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00253v5)
- **Published**: 2017-11-01 08:54:50+00:00
- **Updated**: 2019-02-24 01:24:02+00:00
- **Authors**: Yu Chen, Chunhua Shen, Hao Chen, Xiu-Shen Wei, Lingqiao Liu, Jian Yang
- **Comment**: 18 pages. Extended version of arXiv:1705.00389. Accepted to IEEE
  Trans. Pattern Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: Landmark/pose estimation in single monocular images have received much effort in computer vision due to its important applications. It remains a challenging task when input images severe occlusions caused by, e.g., adverse camera views. Under such circumstances, biologically implausible pose predictions may be produced. In contrast, human vision is able to predict poses by exploiting geometric constraints of landmark point inter-connectivity. To address the problem, by incorporating priors about the structure of pose components, we propose a novel structure-aware fully convolutional network to implicitly take such priors into account during training of the deep network. Explicit learning of such constraints is typically challenging. Instead, inspired by how human identifies implausible poses, we design discriminators to distinguish the real poses from the fake ones (such as biologically implausible ones). If the pose generator G generates results that the discriminator fails to distinguish from real ones, the network successfully learns the priors. Training of the network follows the strategy of conditional Generative Adversarial Networks (GANs). The effectiveness of the proposed network is evaluated on three pose-related tasks: 2D single human pose estimation, 2D facial landmark estimation and 3D single human pose estimation. The proposed approach significantly outperforms the state-of-the-art methods and almost always generates plausible pose predictions, demonstrating the usefulness of implicit learning of structures using GANs.



### Acquiring Target Stacking Skills by Goal-Parameterized Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.00267v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.00267v2)
- **Published**: 2017-11-01 10:04:29+00:00
- **Updated**: 2017-11-22 11:38:17+00:00
- **Authors**: Wenbin Li, Jeannette Bohg, Mario Fritz
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding physical phenomena is a key component of human intelligence and enables physical interaction with previously unseen environments. In this paper, we study how an artificial agent can autonomously acquire this intuition through interaction with the environment. We created a synthetic block stacking environment with physics simulation in which the agent can learn a policy end-to-end through trial and error. Thereby, we bypass to explicitly model physical knowledge within the policy. We are specifically interested in tasks that require the agent to reach a given goal state that may be different for every new trial. To this end, we propose a deep reinforcement learning framework that learns policies which are parametrized by a goal. We validated the model on a toy example navigating in a grid world with different target positions and in a block stacking task with different target structures of the final tower. In contrast to prior work, our policies show better generalization across different goals.



### Multi-View Data Generation Without View Supervision
- **Arxiv ID**: http://arxiv.org/abs/1711.00305v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00305v2)
- **Published**: 2017-11-01 12:18:26+00:00
- **Updated**: 2019-04-17 11:27:10+00:00
- **Authors**: Mickaël Chen, Ludovic Denoyer, Thierry Artières
- **Comment**: Published as a conference paper at ICLR 2018
- **Journal**: None
- **Summary**: The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks. Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate. We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views. We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object. Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space. This approach allows us to generate realistic samples corresponding to various objects in a high variety of views. Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content. Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations. Our model can be used on problems with a huge, potentially infinite, number of categories. We experiment it on four image datasets on which we demonstrate the effectiveness of the model and its ability to generalize.



### Robust Saliency Detection via Fusing Foreground and Background Priors
- **Arxiv ID**: http://arxiv.org/abs/1711.00322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00322v1)
- **Published**: 2017-11-01 12:56:42+00:00
- **Updated**: 2017-11-01 12:56:42+00:00
- **Authors**: Kan Huang, Chunbiao Zhu, Ge Li
- **Comment**: Project website: https://github.com/ChunbiaoZhu/FBP
- **Journal**: None
- **Summary**: Automatic Salient object detection has received tremendous attention from research community and has been an increasingly important tool in many computer vision tasks. This paper proposes a novel bottom-up salient object detection framework which considers both foreground and background cues. First, A series of background and foreground seeds are selected from an image reliably, and then used for calculation of saliency map separately. Next, a combination of foreground and background saliency map is performed. Last, a refinement step based on geodesic distance is utilized to enhance salient regions, thus deriving the final saliency map. Particularly we provide a robust scheme for seeds selection which contributes a lot to accuracy improvement in saliency detection. Extensive experimental evaluations demonstrate the effectiveness of our proposed method against other outstanding methods.



### Automatic calcium scoring in low-dose chest CT using deep neural networks with dilated convolutions
- **Arxiv ID**: http://arxiv.org/abs/1711.00349v2
- **DOI**: 10.1109/TMI.2017.2769839
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00349v2)
- **Published**: 2017-11-01 13:54:55+00:00
- **Updated**: 2018-02-01 21:56:31+00:00
- **Authors**: Nikolas Lessmann, Bram van Ginneken, Majd Zreik, Pim A. de Jong, Bob D. de Vos, Max A. Viergever, Ivana Išgum
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging 37(2), pp 615-625, 2018
- **Summary**: Heavy smokers undergoing screening with low-dose chest CT are affected by cardiovascular disease as much as by lung cancer. Low-dose chest CT scans acquired in screening enable quantification of atherosclerotic calcifications and thus enable identification of subjects at increased cardiovascular risk. This paper presents a method for automatic detection of coronary artery, thoracic aorta and cardiac valve calcifications in low-dose chest CT using two consecutive convolutional neural networks. The first network identifies and labels potential calcifications according to their anatomical location and the second network identifies true calcifications among the detected candidates. This method was trained and evaluated on a set of 1744 CT scans from the National Lung Screening Trial. To determine whether any reconstruction or only images reconstructed with soft tissue filters can be used for calcification detection, we evaluated the method on soft and medium/sharp filter reconstructions separately. On soft filter reconstructions, the method achieved F1 scores of 0.89, 0.89, 0.67, and 0.55 for coronary artery, thoracic aorta, aortic valve and mitral valve calcifications, respectively. On sharp filter reconstructions, the F1 scores were 0.84, 0.81, 0.64, and 0.66, respectively. Linearly weighted kappa coefficients for risk category assignment based on per subject coronary artery calcium were 0.91 and 0.90 for soft and sharp filter reconstructions, respectively. These results demonstrate that the presented method enables reliable automatic cardiovascular risk assessment in all low-dose chest CT scans acquired for lung cancer screening.



### Complex-valued image denosing based on group-wise complex-domain sparsity
- **Arxiv ID**: http://arxiv.org/abs/1711.00362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00362v1)
- **Published**: 2017-11-01 14:30:31+00:00
- **Updated**: 2017-11-01 14:30:31+00:00
- **Authors**: Vladimir Katkovnik, Mykola Ponomarenko, Karen Egiazarian
- **Comment**: Submitted to Signal Processing
- **Journal**: None
- **Summary**: Phase imaging and wavefront reconstruction from noisy observations of complex exponent is a topic of this paper. It is a highly non-linear problem because the exponent is a 2{\pi}-periodic function of phase. The reconstruction of phase and amplitude is difficult. Even with an additive Gaussian noise in observations distributions of noisy components in phase and amplitude are signal dependent and non-Gaussian. Additional difficulties follow from a prior unknown correlation of phase and amplitude in real life scenarios. In this paper, we propose a new class of non-iterative and iterative complex domain filters based on group-wise sparsity in complex domain. This sparsity is based on the techniques implemented in Block-Matching 3D filtering (BM3D) and 3D/4D High-Order Singular Decomposition (HOSVD) exploited for spectrum design, analysis and filtering. The introduced algorithms are a generalization of the ideas used in the CD-BM3D algorithms presented in our previous publications. The algorithms are implemented as a MATLAB Toolbox. The efficiency of the algorithms is demonstrated by simulation tests.



### Intelligent Parameter Tuning in Optimization-based Iterative CT Reconstruction via Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.00414v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.00414v1)
- **Published**: 2017-11-01 16:04:24+00:00
- **Updated**: 2017-11-01 16:04:24+00:00
- **Authors**: Chenyang Shen, Yesenia Gonzalez, Liyuan Chen, Steve B. Jiang, Xun Jia
- **Comment**: 8 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: A number of image-processing problems can be formulated as optimization problems. The objective function typically contains several terms specifically designed for different purposes. Parameters in front of these terms are used to control the relative weights among them. It is of critical importance to tune these parameters, as quality of the solution depends on their values. Tuning parameter is a relatively straightforward task for a human, as one can intelligently determine the direction of parameter adjustment based on the solution quality. Yet manual parameter tuning is not only tedious in many cases, but becomes impractical when a number of parameters exist in a problem. Aiming at solving this problem, this paper proposes an approach that employs deep reinforcement learning to train a system that can automatically adjust parameters in a human-like manner. We demonstrate our idea in an example problem of optimization-based iterative CT reconstruction with a pixel-wise total-variation regularization term. We set up a parameter tuning policy network (PTPN), which maps an CT image patch to an output that specifies the direction and amplitude by which the parameter at the patch center is adjusted. We train the PTPN via an end-to-end reinforcement learning procedure. We demonstrate that under the guidance of the trained PTPN for parameter tuning at each pixel, reconstructed CT images attain quality similar or better than in those reconstructed with manually tuned parameters.



### Hierarchical Representations for Efficient Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1711.00436v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.00436v2)
- **Published**: 2017-11-01 16:46:27+00:00
- **Updated**: 2018-02-22 22:31:30+00:00
- **Authors**: Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray Kavukcuoglu
- **Comment**: Accepted as a conference paper at ICLR 2018
- **Journal**: None
- **Summary**: We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.



### Data, Depth, and Design: Learning Reliable Models for Skin Lesion Analysis
- **Arxiv ID**: http://arxiv.org/abs/1711.00441v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00441v4)
- **Published**: 2017-11-01 17:02:35+00:00
- **Updated**: 2019-08-18 02:00:32+00:00
- **Authors**: Eduardo Valle, Michel Fornaciali, Afonso Menegola, Julia Tavares, Flávia Vasques Bittencourt, Lin Tzy Li, Sandra Avila
- **Comment**: 12 pages, 6 figures, 3 tables. Article accepted at Neurocomputing
- **Journal**: None
- **Summary**: Deep learning fostered a leap ahead in automated skin lesion analysis in the last two years. Those models are expensive to train and difficult to parameterize. Objective: We investigate methodological issues for designing and evaluating deep learning models for skin lesion analysis. We explore 10 choices faced by researchers: use of transfer learning, model architecture, train dataset, image resolution, type of data augmentation, input normalization, use of segmentation, duration of training, additional use of SVMs, and test data augmentation. Methods: We perform two full factorial experiments, for five different test datasets, resulting in 2560 exhaustive trials in our main experiment, and 1280 trials in our assessment of transfer learning. We analyze both with multi-way ANOVA. We use the exhaustive trials to simulate sequential decisions and ensembles, with and without the use of privileged information from the test set. Results -- main experiment: Amount of train data has disproportionate influence, explaining almost half the variation in performance. Of the other factors, test data augmentation and input resolution are the most influential. Deeper models, when combined, with extra data, also help. -- transfer experiment: Transfer learning is critical, its absence brings huge performance penalties. -- simulations: Ensembles of models are the best option to provide reliable results with limited resources, without using privileged information and sacrificing methodological rigor. Conclusions and Significance: Advancing research on automated skin lesion analysis requires curating larger public datasets. Indirect use of privileged information from the test set to design the models is a subtle, but frequent methodological mistake that leads to overoptimistic results. Ensembles of models are a cost-effective alternative to the expensive full-factorial and to the unstable sequential designs.



### Almost instant brain atlas segmentation for large-scale studies
- **Arxiv ID**: http://arxiv.org/abs/1711.00457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00457v1)
- **Published**: 2017-11-01 17:44:11+00:00
- **Updated**: 2017-11-01 17:44:11+00:00
- **Authors**: Alex Fedorov, Eswar Damaraju, Vince Calhoun, Sergey Plis
- **Comment**: None
- **Journal**: None
- **Summary**: Large scale studies of group differences in healthy controls and patients and screenings for early stage disease prevention programs require processing and analysis of extensive multisubject datasets. Complexity of the task increases even further when segmenting structural MRI of the brain into an atlas with more than 50 regions. Current automatic approaches are time-consuming and hardly scalable; they often involve many error prone intermediate steps and don't utilize other available modalities. To alleviate these problems, we propose a feedforward fully convolutional neural network trained on the output produced by the state of the art models. Incredible speed due to available powerful GPUs neural network makes this analysis much easier and faster (from $>10$ hours to a minute). The proposed model is more than two orders of magnitudes faster than the state of the art and yet as accurate. We have evaluated the network's performance by comparing it with the state of the art in the task of differentiating region volumes of healthy controls and patients with schizophrenia on a dataset with 311 subjects. This comparison provides a strong evidence that speed did not harm the accuracy. The overall quality may also be increased by utilizing multi-modal datasets (not an easy task for other models) by simple adding more modalities as an input. Our model will be useful in large-scale studies as well as in clinical care solutions, where it can significantly reduce delay between the patient screening and the result.



### Don't Decay the Learning Rate, Increase the Batch Size
- **Arxiv ID**: http://arxiv.org/abs/1711.00489v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.00489v2)
- **Published**: 2017-11-01 18:04:31+00:00
- **Updated**: 2018-02-24 00:16:12+00:00
- **Authors**: Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, Quoc V. Le
- **Comment**: 11 pages, 8 figures. Published as a conference paper at ICLR 2018
- **Journal**: None
- **Summary**: It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate $\epsilon$ and scaling the batch size $B \propto \epsilon$. Finally, one can increase the momentum coefficient $m$ and scale $B \propto 1/(1-m)$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to $76.1\%$ validation accuracy in under 30 minutes.



### Widening siamese architectures for stereo matching
- **Arxiv ID**: http://arxiv.org/abs/1711.00499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00499v1)
- **Published**: 2017-11-01 18:24:31+00:00
- **Updated**: 2017-11-01 18:24:31+00:00
- **Authors**: Patrick Brandao, Evangelos Mazomenos, Danail Stoyanov
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: Computational stereo is one of the classical problems in computer vision. Numerous algorithms and solutions have been reported in recent years focusing on developing methods for computing similarity, aggregating it to obtain spatial support and finally optimizing an energy function to find the final disparity. In this paper, we focus on the feature extraction component of stereo matching architecture and we show standard CNNs operation can be used to improve the quality of the features used to find point correspondences. Furthermore, we propose a simple space aggregation that hugely simplifies the correlation learning problem. Our results on benchmark data are compelling and show promising potential even without refining the solution.



### Beautiful and damned. Combined effect of content quality and social ties on user engagement
- **Arxiv ID**: http://arxiv.org/abs/1711.00536v1
- **DOI**: 10.1109/TKDE.2017.2747552
- **Categories**: **cs.SI**, cs.AI, cs.CV, cs.MM, physics.soc-ph
- **Links**: [PDF](http://arxiv.org/pdf/1711.00536v1)
- **Published**: 2017-11-01 20:48:30+00:00
- **Updated**: 2017-11-01 20:48:30+00:00
- **Authors**: Luca M. Aiello, Rossano Schifanella, Miriam Redi, Stacey Svetlichnaya, Frank Liu, Simon Osindero
- **Comment**: 13 pages, 12 figures, final version published in IEEE Transactions on
  Knowledge and Data Engineering (Volume: PP, Issue: 99)
- **Journal**: None
- **Summary**: User participation in online communities is driven by the intertwinement of the social network structure with the crowd-generated content that flows along its links. These aspects are rarely explored jointly and at scale. By looking at how users generate and access pictures of varying beauty on Flickr, we investigate how the production of quality impacts the dynamics of online social systems. We develop a deep learning computer vision model to score images according to their aesthetic value and we validate its output through crowdsourcing. By applying it to over 15B Flickr photos, we study for the first time how image beauty is distributed over a large-scale social system. Beautiful images are evenly distributed in the network, although only a small core of people get social recognition for them. To study the impact of exposure to quality on user engagement, we set up matching experiments aimed at detecting causality from observational data. Exposure to beauty is double-edged: following people who produce high-quality content increases one's probability of uploading better photos; however, an excessive imbalance between the quality generated by a user and the user's neighbors leads to a decline in engagement. Our analysis has practical implications for improving link recommender systems.



### Recognizing Textures with Mobile Cameras for Pedestrian Safety Applications
- **Arxiv ID**: http://arxiv.org/abs/1711.00558v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1711.00558v1)
- **Published**: 2017-11-01 23:09:22+00:00
- **Updated**: 2017-11-01 23:09:22+00:00
- **Authors**: Shubham Jain, Marco Gruteser
- **Comment**: None
- **Journal**: None
- **Summary**: As smartphone rooted distractions become commonplace, the lack of compelling safety measures has led to a rise in the number of injuries to distracted walkers. Various solutions address this problem by sensing a pedestrian's walking environment. Existing camera-based approaches have been largely limited to obstacle detection and other forms of object detection. Instead, we present TerraFirma, an approach that performs material recognition on the pedestrian's walking surface. We explore, first, how well commercial off-the-shelf smartphone cameras can learn texture to distinguish among paving materials in uncontrolled outdoor urban settings. Second, we aim at identifying when a distracted user is about to enter the street, which can be used to support safety functions such as warning the user to be cautious. To this end, we gather a unique dataset of street/sidewalk imagery from a pedestrian's perspective, that spans major cities like New York, Paris, and London. We demonstrate that modern phone cameras can be enabled to distinguish materials of walking surfaces in urban areas with more than 90% accuracy, and accurately identify when pedestrians transition from sidewalk to street.



