# Arxiv Papers in cs.CV on 2017-11-20
### CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise
- **Arxiv ID**: http://arxiv.org/abs/1711.07131v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.07131v2)
- **Published**: 2017-11-20 03:50:53+00:00
- **Updated**: 2018-03-25 23:07:58+00:00
- **Authors**: Kuang-Huei Lee, Xiaodong He, Lei Zhang, Linjun Yang
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: In this paper, we study the problem of learning image classification models with label noise. Existing approaches depending on human supervision are generally not scalable as manually identifying correct or incorrect labels is time-consuming, whereas approaches not relying on human supervision are scalable but less effective. To reduce the amount of human supervision for label noise cleaning, we introduce CleanNet, a joint neural embedding network, which only requires a fraction of the classes being manually verified to provide the knowledge of label noise that can be transferred to other classes. We further integrate CleanNet and conventional convolutional neural network classifier into one framework for image classification learning. We demonstrate the effectiveness of the proposed algorithm on both of the label noise detection task and the image classification on noisy data task on several large-scale datasets. Experimental results show that CleanNet can reduce label noise detection error rate on held-out classes where no human supervision available by 41.5% compared to current weakly supervised methods. It also achieves 47% of the performance gain of verifying all images with only 3.2% images verified on an image classification task. Source code and dataset will be available at kuanghuei.github.io/CleanNetProject.



### Non-line-of-sight Imaging with Partial Occluders and Surface Normals
- **Arxiv ID**: http://arxiv.org/abs/1711.07134v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07134v3)
- **Published**: 2017-11-20 04:12:30+00:00
- **Updated**: 2018-04-04 23:17:26+00:00
- **Authors**: Felix Heide, Matthew O'Toole, Kai Zang, David Lindell, Steven Diamond, Gordon Wetzstein
- **Comment**: None
- **Journal**: None
- **Summary**: Imaging objects obscured by occluders is a significant challenge for many applications. A camera that could "see around corners" could help improve navigation and mapping capabilities of autonomous vehicles or make search and rescue missions more effective. Time-resolved single-photon imaging systems have recently been demonstrated to record optical information of a scene that can lead to an estimation of the shape and reflectance of objects hidden from the line of sight of a camera. However, existing non-line-of-sight (NLOS) reconstruction algorithms have been constrained in the types of light transport effects they model for the hidden scene parts. We introduce a factored NLOS light transport representation that accounts for partial occlusions and surface normals. Based on this model, we develop a factorization approach for inverse time-resolved light transport and demonstrate high-fidelity NLOS reconstructions for challenging scenes both in simulation and with an experimental NLOS imaging system.



### Spectral-Spatial Feature Extraction and Classification by ANN Supervised with Center Loss in Hyperspectral Imagery
- **Arxiv ID**: http://arxiv.org/abs/1711.07141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07141v1)
- **Published**: 2017-11-20 04:46:45+00:00
- **Updated**: 2017-11-20 04:46:45+00:00
- **Authors**: Alan J. X. Guo, Fei Zhu
- **Comment**: 17 pages, 10 figures
- **Journal**: None
- **Summary**: In this paper, we propose a spectral-spatial feature extraction and classification framework based on artificial neuron network (ANN) in the context of hyperspectral imagery. With limited labeled samples, only spectral information is exploited for training and spatial context is integrated posteriorly at the testing stage. Taking advantage of recent advances in face recognition, a joint supervision symbol that combines softmax loss and center loss is adopted to train the proposed network, by which intra-class features are gathered while inter-class variations are enlarged. Based on the learned architecture, the extracted spectrum-based features are classified by a center classifier. Moreover, to fuse the spectral and spatial information, an adaptive spectral-spatial center classifier is developed, where multiscale neighborhoods are considered simultaneously, and the final label is determined using an adaptive voting strategy. Finally, experimental results on three well-known datasets validate the effectiveness of the proposed methods compared with the state-of-the-art approaches.



### Let Features Decide for Themselves: Feature Mask Network for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1711.07155v1
- **DOI**: 10.1016/j.patrec.2019.02.015
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07155v1)
- **Published**: 2017-11-20 05:44:29+00:00
- **Updated**: 2017-11-20 05:44:29+00:00
- **Authors**: Guodong Ding, Salman Khan, Zhenmin Tang, Fatih Porikli
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Person re-identification aims at establishing the identity of a pedestrian from a gallery that contains images of multiple people obtained from a multi-camera system. Many challenges such as occlusions, drastic lighting and pose variations across the camera views, indiscriminate visual appearances, cluttered backgrounds, imperfect detections, motion blur, and noise make this task highly challenging. While most approaches focus on learning features and metrics to derive better representations, we hypothesize that both local and global contextual cues are crucial for an accurate identity matching. To this end, we propose a Feature Mask Network (FMN) that takes advantage of ResNet high-level features to predict a feature map mask and then imposes it on the low-level features to dynamically reweight different object parts for a locally aware feature representation. This serves as an effective attention mechanism by allowing the network to focus on local details selectively. Given the resemblance of person re-identification with classification and retrieval tasks, we frame the network training as a multi-task objective optimization, which further improves the learned feature descriptions. We conduct experiments on Market-1501, DukeMTMC-reID and CUHK03 datasets, where the proposed approach respectively achieves significant improvements of $5.3\%$, $9.1\%$ and $10.7\%$ in mAP measure relative to the state-of-the-art.



### Parameter Reference Loss for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1711.07170v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1711.07170v2)
- **Published**: 2017-11-20 06:40:43+00:00
- **Updated**: 2017-12-05 10:19:34+00:00
- **Authors**: Jiren Jin, Richard G. Calland, Takeru Miyato, Brian K. Vogel, Hideki Nakayama
- **Comment**: Add experiments that compare parameter reference loss with existing
  methods using the same architecture
- **Journal**: None
- **Summary**: The success of deep learning in computer vision is mainly attributed to an abundance of data. However, collecting large-scale data is not always possible, especially for the supervised labels. Unsupervised domain adaptation (UDA) aims to utilize labeled data from a source domain to learn a model that generalizes to a target domain of unlabeled data. A large amount of existing work uses Siamese network-based models, where two streams of neural networks process the source and the target domain data respectively. Nevertheless, most of these approaches focus on minimizing the domain discrepancy, overlooking the importance of preserving the discriminative ability for target domain features. Another important problem in UDA research is how to evaluate the methods properly. Common evaluation procedures require target domain labels for hyper-parameter tuning and model selection, contradicting the definition of the UDA task. Hence we propose a more reasonable evaluation principle that avoids this contradiction by simply adopting the latest snapshot of a model for evaluation. This adds an extra requirement for UDA methods besides the main performance criteria: the stability during training. We design a novel method that connects the target domain stream to the source domain stream with a Parameter Reference Loss (PRL) to solve these problems simultaneously. Experiments on various datasets show that the proposed PRL not only improves the performance on the target domain, but also stabilizes the training procedure. As a result, PRL based models do not need the contradictory model selection, and thus are more suitable for practical applications.



### Adversarial Attacks Beyond the Image Space
- **Arxiv ID**: http://arxiv.org/abs/1711.07183v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07183v6)
- **Published**: 2017-11-20 08:05:24+00:00
- **Updated**: 2019-04-06 19:46:43+00:00
- **Authors**: Xiaohui Zeng, Chenxi Liu, Yu-Siang Wang, Weichao Qiu, Lingxi Xie, Yu-Wing Tai, Chi Keung Tang, Alan L. Yuille
- **Comment**: To appear in CVPR 2019 as oral
- **Journal**: None
- **Summary**: Generating adversarial examples is an intriguing problem and an important way of understanding the working mechanism of deep neural networks. Most existing approaches generated perturbations in the image space, i.e., each pixel can be modified independently. However, in this paper we pay special attention to the subset of adversarial examples that correspond to meaningful changes in 3D physical properties (like rotation and translation, illumination condition, etc.). These adversaries arguably pose a more serious concern, as they demonstrate the possibility of causing neural network failure by easy perturbations of real-world 3D objects and scenes.   In the contexts of object classification and visual question answering, we augment state-of-the-art deep neural networks that receive 2D input images with a rendering module (either differentiable or not) in front, so that a 3D scene (in the physical space) is rendered into a 2D image (in the image space), and then mapped to a prediction (in the output space). The adversarial perturbations can now go beyond the image space, and have clear meanings in the 3D physical world. Though image-space adversaries can be interpreted as per-pixel albedo change, we verify that they cannot be well explained along these physically meaningful dimensions, which often have a non-local effect. But it is still possible to successfully attack beyond the image space on the physical space, though this is more difficult than image-space attacks, reflected in lower success rates and heavier perturbations required.



### Tactics to Directly Map CNN graphs on Embedded FPGAs
- **Arxiv ID**: http://arxiv.org/abs/1712.04322v1
- **DOI**: 10.1109/LES.2017.2743247
- **Categories**: **cs.DC**, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.04322v1)
- **Published**: 2017-11-20 08:13:39+00:00
- **Updated**: 2017-11-20 08:13:39+00:00
- **Authors**: Kamel Abdelouahab, Maxime Pelcat, Jocelyn Sérot, Cédric Bourrasset, François Berry, Jocelyn Serot
- **Comment**: IEEE Embedded Systems Letters, Institute of Electrical and
  Electronics Engineers, A Para\^itre, pp.1 - 1. arXiv admin note: text overlap
  with arXiv:1705.04543
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNNs) are the state-of-the-art in image classification. Since CNN feed forward propagation involves highly regular parallel computation, it benefits from a significant speed-up when running on fine grain parallel programmable logic devices. As a consequence, several studies have proposed FPGA-based accelerators for CNNs. However, because of the large computationalpower required by CNNs, none of the previous studies has proposed a direct mapping of the CNN onto the physical resources of an FPGA, allocating each processing actor to its own hardware instance.In this paper, we demonstrate the feasibility of the so called direct hardware mapping (DHM) and discuss several tactics we explore to make DHM usable in practice. As a proof of concept, we introduce the HADDOC2 open source tool, that automatically transforms a CNN description into a synthesizable hardware description with platform-independent direct hardware mapping.



### Block-Cyclic Stochastic Coordinate Descent for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.07190v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07190v1)
- **Published**: 2017-11-20 08:20:35+00:00
- **Updated**: 2017-11-20 08:20:35+00:00
- **Authors**: Kensuke Nakamura, Stefano Soatto, Byung-Woo Hong
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We present a stochastic first-order optimization algorithm, named BCSC, that adds a cyclic constraint to stochastic block-coordinate descent. It uses different subsets of the data to update different subsets of the parameters, thus limiting the detrimental effect of outliers in the training set. Empirical tests in benchmark datasets show that our algorithm outperforms state-of-the-art optimization methods in both accuracy as well as convergence speed. The improvements are consistent across different architectures, and can be combined with other training techniques and regularization methods.



### End-to-end Trained CNN Encode-Decoder Networks for Image Steganography
- **Arxiv ID**: http://arxiv.org/abs/1711.07201v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.07201v1)
- **Published**: 2017-11-20 08:49:32+00:00
- **Updated**: 2017-11-20 08:49:32+00:00
- **Authors**: Atique ur Rehman, Rafia Rahim, M Shahroz Nadeem, Sibt ul Hussain
- **Comment**: None
- **Journal**: None
- **Summary**: All the existing image steganography methods use manually crafted features to hide binary payloads into cover images. This leads to small payload capacity and image distortion. Here we propose a convolutional neural network based encoder-decoder architecture for embedding of images as payload. To this end, we make following three major contributions: (i) we propose a deep learning based generic encoder-decoder architecture for image steganography; (ii) we introduce a new loss function that ensures joint end-to-end training of encoder-decoder networks; (iii) we perform extensive empirical evaluation of proposed architecture on a range of challenging publicly available datasets (MNIST, CIFAR10, PASCAL-VOC12, ImageNet, LFW) and report state-of-the-art payload capacity at high PSNR and SSIM values.



### Stochastic metamorphosis with template uncertainties
- **Arxiv ID**: http://arxiv.org/abs/1711.07231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07231v1)
- **Published**: 2017-11-20 09:55:15+00:00
- **Updated**: 2017-11-20 09:55:15+00:00
- **Authors**: Alexis Arnaudon, Darryl Holm, Stefan Sommer
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate two stochastic perturbations of the metamorphosis equations of image analysis, in the geometrical context of the Euler-Poincar\'e theory. In the metamorphosis of images, the Lie group of diffeomorphisms deforms a template image that is undergoing its own internal dynamics as it deforms. This type of deformation allows more freedom for image matching and has analogies with complex fluids when the template properties are regarded as order parameters (coset spaces of broken symmetries). The first stochastic perturbation we consider corresponds to uncertainty due to random errors in the reconstruction of the deformation map from its vector field. We also consider a second stochastic perturbation, which compounds the uncertainty in of the deformation map with the uncertainty in the reconstruction of the template position from its velocity field. We apply this general geometric theory to several classical examples, including landmarks, images, and closed curves, and we discuss its use for functional data analysis.



### Tracking in Aerial Hyperspectral Videos using Deep Kernelized Correlation Filters
- **Arxiv ID**: http://arxiv.org/abs/1711.07235v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07235v3)
- **Published**: 2017-11-20 10:06:07+00:00
- **Updated**: 2018-05-06 06:04:27+00:00
- **Authors**: Burak Uzkent, Aneesh Rangnekar, Matthew J. Hoffman
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral imaging holds enormous potential to improve the state-of-the-art in aerial vehicle tracking with low spatial and temporal resolutions. Recently, adaptive multi-modal hyperspectral sensors have attracted growing interest due to their ability to record extended data quickly from aerial platforms. In this study, we apply popular concepts from traditional object tracking, namely (1) Kernelized Correlation Filters (KCF) and (2) Deep Convolutional Neural Network (CNN) features to aerial tracking in hyperspectral domain. We propose the Deep Hyperspectral Kernelized Correlation Filter based tracker (DeepHKCF) to efficiently track aerial vehicles using an adaptive multi-modal hyperspectral sensor. We address low temporal resolution by designing a single KCF-in-multiple Regions-of-Interest (ROIs) approach to cover a reasonably large area. To increase the speed of deep convolutional features extraction from multiple ROIs, we design an effective ROI mapping strategy. The proposed tracker also provides flexibility to couple with the more advanced correlation filter trackers. The DeepHKCF tracker performs exceptionally well with deep features set up in a synthetic hyperspectral video generated by the Digital Imaging and Remote Sensing Image Generation (DIRSIG) software. Additionally, we generate a large, synthetic, single-channel dataset using DIRSIG to perform vehicle classification in the Wide Area Motion Imagery (WAMI) platform. This way, the high-fidelity of the DIRSIG software is proved and a large scale aerial vehicle classification dataset is released to support studies on vehicle detection and tracking in the WAMI platform.



### MegDet: A Large Mini-Batch Object Detector
- **Arxiv ID**: http://arxiv.org/abs/1711.07240v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07240v4)
- **Published**: 2017-11-20 10:16:33+00:00
- **Updated**: 2018-04-11 05:59:52+00:00
- **Authors**: Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia, Gang Yu, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: The improvements in recent CNN-based object detection works, from R-CNN [11], Fast/Faster R-CNN [10, 31] to recent Mask R-CNN [14] and RetinaNet [24], mainly come from new network, new framework, or novel loss design. But mini-batch size, a key factor in the training, has not been well studied. In this paper, we propose a Large MiniBatch Object Detector (MegDet) to enable the training with much larger mini-batch size than before (e.g. from 16 to 256), so that we can effectively utilize multiple GPUs (up to 128 in our experiments) to significantly shorten the training time. Technically, we suggest a learning rate policy and Cross-GPU Batch Normalization, which together allow us to successfully train a large mini-batch detector in much less time (e.g., from 33 hours to 4 hours), and achieve even better accuracy. The MegDet is the backbone of our submission (mmAP 52.5%) to COCO 2017 Challenge, where we won the 1st place of Detection task.



### Optical Character Recognition (OCR) for Telugu: Database, Algorithm and Application
- **Arxiv ID**: http://arxiv.org/abs/1711.07245v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07245v2)
- **Published**: 2017-11-20 10:33:29+00:00
- **Updated**: 2018-12-25 18:33:15+00:00
- **Authors**: Chandra Prakash Konkimalla, Manikanta Srikar Yellapragada, Trishal Gayam, Souraj Mandal, Sumohana S. Channappayya
- **Comment**: Accepted to IEEE International Conference on Image Processing 2018
- **Journal**: None
- **Summary**: Telugu is a Dravidian language spoken by more than 80 million people worldwide. The optical character recognition (OCR) of the Telugu script has wide ranging applications including education, health-care, administration etc. The beautiful Telugu script however is very different from Germanic scripts like English and German. This makes the use of transfer learning of Germanic OCR solutions to Telugu a non-trivial task. To address the challenge of OCR for Telugu, we make three contributions in this work: (i) a database of Telugu characters, (ii) a deep learning based OCR algorithm, and (iii) a client server solution for the online deployment of the algorithm. For the benefit of the Telugu people and the research community, we will make our code freely available at https://gayamtrishal.github.io/OCR_Telugu.github.io/



### Face Attention Network: An Effective Face Detector for the Occluded Faces
- **Arxiv ID**: http://arxiv.org/abs/1711.07246v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07246v2)
- **Published**: 2017-11-20 10:48:12+00:00
- **Updated**: 2017-11-22 07:21:17+00:00
- **Authors**: Jianfeng Wang, Ye Yuan, Gang Yu
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of face detection has been largely improved with the development of convolutional neural network. However, the occlusion issue due to mask and sunglasses, is still a challenging problem. The improvement on the recall of these occluded cases usually brings the risk of high false positives. In this paper, we present a novel face detector called Face Attention Network (FAN), which can significantly improve the recall of the face detection problem in the occluded case without compromising the speed. More specifically, we propose a new anchor-level attention, which will highlight the features from the face region. Integrated with our anchor assign strategy and data augmentation techniques, we obtain state-of-art results on public face detection benchmarks like WiderFace and MAFA. The code will be released for reproduction.



### Virtual Adversarial Ladder Networks For Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.07476v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.07476v2)
- **Published**: 2017-11-20 11:10:40+00:00
- **Updated**: 2017-12-12 11:23:01+00:00
- **Authors**: Saki Shinoda, Daniel E. Worrall, Gabriel J. Brostow
- **Comment**: Camera-ready version for NIPS 2017 workshop Learning with Limited
  Labeled Data
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) partially circumvents the high cost of labeling data by augmenting a small labeled dataset with a large and relatively cheap unlabeled dataset drawn from the same distribution. This paper offers a novel interpretation of two deep learning-based SSL approaches, ladder networks and virtual adversarial training (VAT), as applying distributional smoothing to their respective latent spaces. We propose a class of models that fuse these approaches. We achieve near-supervised accuracy with high consistency on the MNIST dataset using just 5 labels per class: our best model, ladder with layer-wise virtual adversarial noise (LVAN-LW), achieves 1.42% +/- 0.12 average error rate on the MNIST test set, in comparison with 1.62% +/- 0.65 reported for the ladder network. On adversarial examples generated with L2-normalized fast gradient method, LVAN-LW trained with 5 examples per class achieves average error rate 2.4% +/- 0.3 compared to 68.6% +/- 6.5 for the ladder network and 9.9% +/- 7.5 for VAT.



### Light-Head R-CNN: In Defense of Two-Stage Object Detector
- **Arxiv ID**: http://arxiv.org/abs/1711.07264v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07264v2)
- **Published**: 2017-11-20 11:50:19+00:00
- **Updated**: 2017-11-23 02:53:09+00:00
- **Authors**: Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we first investigate why typical two-stage methods are not as fast as single-stage, fast detectors like YOLO and SSD. We find that Faster R-CNN and R-FCN perform an intensive computation after or before RoI warping. Faster R-CNN involves two fully connected layers for RoI recognition, while R-FCN produces a large score maps. Thus, the speed of these networks is slow due to the heavy-head design in the architecture. Even if we significantly reduce the base model, the computation cost cannot be largely decreased accordingly.   We propose a new two-stage detector, Light-Head R-CNN, to address the shortcoming in current two-stage approaches. In our design, we make the head of network as light as possible, by using a thin feature map and a cheap R-CNN subnet (pooling and single fully-connected layer). Our ResNet-101 based light-head R-CNN outperforms state-of-art object detectors on COCO while keeping time efficiency. More importantly, simply replacing the backbone with a tiny network (e.g, Xception), our Light-Head R-CNN gets 30.7 mmAP at 102 FPS on COCO, significantly outperforming the single-stage, fast detectors like YOLO and SSD on both speed and accuracy. Code will be made publicly available.



### Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments
- **Arxiv ID**: http://arxiv.org/abs/1711.07280v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1711.07280v3)
- **Published**: 2017-11-20 12:17:47+00:00
- **Updated**: 2018-04-05 22:57:44+00:00
- **Authors**: Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton van den Hengel
- **Comment**: CVPR 2018 Spotlight presentation
- **Journal**: None
- **Summary**: A robot that can carry out a natural-language instruction has been a dream since before the Jetsons cartoon series imagined a life of leisure mediated by a fleet of attentive robot helpers. It is a dream that remains stubbornly distant. However, recent advances in vision and language methods have made incredible progress in closely related areas. This is significant because a robot interpreting a natural-language navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to Visual Question Answering. Both tasks can be interpreted as visually grounded sequence-to-sequence translation problems, and many of the same methods are applicable. To enable and encourage the application of vision and language methods to the problem of interpreting visually-grounded navigation instructions, we present the Matterport3D Simulator -- a large-scale reinforcement learning environment based on real imagery. Using this simulator, which can in future support a range of embodied vision and language tasks, we provide the first benchmark dataset for visually-grounded natural language navigation in real buildings -- the Room-to-Room (R2R) dataset.



### Learning Steerable Filters for Rotation Equivariant CNNs
- **Arxiv ID**: http://arxiv.org/abs/1711.07289v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.07289v3)
- **Published**: 2017-11-20 12:49:02+00:00
- **Updated**: 2018-03-19 17:10:52+00:00
- **Authors**: Maurice Weiler, Fred A. Hamprecht, Martin Storath
- **Comment**: Camera ready version, accepted for CVPR 2018
- **Journal**: None
- **Summary**: In many machine learning tasks it is desirable that a model's prediction transforms in an equivariant way under transformations of its input. Convolutional neural networks (CNNs) implement translational equivariance by construction; for other transformations, however, they are compelled to learn the proper mapping. In this work, we develop Steerable Filter CNNs (SFCNNs) which achieve joint equivariance under translations and rotations by design. The proposed architecture employs steerable filters to efficiently compute orientation dependent responses for many orientations without suffering interpolation artifacts from filter rotation. We utilize group convolutions which guarantee an equivariant mapping. In addition, we generalize He's weight initialization scheme to filters which are defined as a linear combination of a system of atomic filters. Numerical experiments show a substantial enhancement of the sample complexity with a growing number of sampled filter orientations and confirm that the network generalizes learned patterns over orientations. The proposed approach achieves state-of-the-art on the rotated MNIST benchmark and on the ISBI 2012 2D EM segmentation challenge.



### Zero-shot Learning via Shared-Reconstruction-Graph Pursuit
- **Arxiv ID**: http://arxiv.org/abs/1711.07302v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07302v1)
- **Published**: 2017-11-20 13:31:16+00:00
- **Updated**: 2017-11-20 13:31:16+00:00
- **Authors**: Bo Zhao, Xinwei Sun, Yuan Yao, Yizhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to recognize objects from novel unseen classes without any training data. Recently, structure-transfer based methods are proposed to implement ZSL by transferring structural knowledge from the semantic embedding space to image feature space to classify testing images. However, we observe that such a knowledge transfer framework may suffer from the problem of the geometric inconsistency between the data in the training and testing spaces. We call this problem as the space shift problem. In this paper, we propose a novel graph based method to alleviate this space shift problem. Specifically, a Shared Reconstruction Graph (SRG) is pursued to capture the common structure of data in the two spaces. With the learned SRG, each unseen class prototype (cluster center) in the image feature space can be synthesized by the linear combination of other class prototypes, so that testing instances can be classified based on the distance to these synthesized prototypes. The SRG bridges the image feature space and semantic embedding space. By applying spectral clustering on the learned SRG, many meaningful clusters can be discovered, which interprets ZSL performance on the datasets. Our method can be easily extended to the generalized zero-shot learning setting. Experiments on three popular datasets show that our method outperforms other methods on all datasets. Even with a small number of training samples, our method can achieve the state-of-the-art performance.



### Detection of Tooth caries in Bitewing Radiographs using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.07312v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07312v2)
- **Published**: 2017-11-20 14:12:32+00:00
- **Updated**: 2017-11-23 16:08:27+00:00
- **Authors**: Muktabh Mayank Srivastava, Pratyush Kumar, Lalit Pradhan, Srikrishna Varadarajan
- **Comment**: Accepted at NIPS 2017 workshop on Machine Learning for Health (NIPS
  2017 ML4H)
- **Journal**: None
- **Summary**: We develop a Computer Aided Diagnosis (CAD) system, which enhances the performance of dentists in detecting wide range of dental caries. The CAD System achieves this by acting as a second opinion for the dentists with way higher sensitivity on the task of detecting cavities than the dentists themselves. We develop annotated dataset of more than 3000 bitewing radiographs and utilize it for developing a system for automated diagnosis of dental caries. Our system consists of a deep fully convolutional neural network (FCNN) consisting 100+ layers, which is trained to mark caries on bitewing radiographs. We have compared the performance of our proposed system with three certified dentists for marking dental caries. We exceed the average performance of the dentists in both recall (sensitivity) and F1-Score (agreement with truth) by a very large margin. Working example of our system is shown in Figure 1.



### Cascaded Pyramid Network for Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1711.07319v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07319v2)
- **Published**: 2017-11-20 14:36:31+00:00
- **Updated**: 2018-04-08 12:00:58+00:00
- **Authors**: Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, Jian Sun
- **Comment**: 10 pages, accepted to CVPR 2018
- **Journal**: None
- **Summary**: The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these "hard" keypoints. More specifically, our algorithm includes two stages: GlobalNet and RefineNet. GlobalNet is a feature pyramid network which can successfully localize the "simple" keypoints like eyes and hands but may fail to precisely recognize the occluded or invisible keypoints. Our RefineNet tries explicitly handling the "hard" keypoints by integrating all levels of feature representations from the GlobalNet together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation problem, a top-down pipeline is adopted to first generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-of-art results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19% relative improvement compared with 60.5 from the COCO 2016 keypoint challenge.Code (https://github.com/chenyilun95/tf-cpn.git) and the detection results are publicly available for further research.



### Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.07354v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.07354v1)
- **Published**: 2017-11-20 15:04:45+00:00
- **Updated**: 2017-11-20 15:04:45+00:00
- **Authors**: Ziming Zhang, Matthew Brand
- **Comment**: NIPS 2017
- **Journal**: None
- **Summary**: By lifting the ReLU function into a higher dimensional space, we develop a smooth multi-convex formulation for training feed-forward deep neural networks (DNNs). This allows us to develop a block coordinate descent (BCD) training algorithm consisting of a sequence of numerically well-behaved convex optimizations. Using ideas from proximal point methods in convex analysis, we prove that this BCD algorithm will converge globally to a stationary point with R-linear convergence rate of order one. In experiments with the MNIST database, DNNs trained with this BCD algorithm consistently yielded better test-set error rates than identical DNN architectures trained via all the stochastic gradient descent (SGD) variants in the Caffe toolbox.



### Evaluating Robustness of Neural Networks with Mixed Integer Programming
- **Arxiv ID**: http://arxiv.org/abs/1711.07356v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.07356v3)
- **Published**: 2017-11-20 15:05:33+00:00
- **Updated**: 2019-02-18 04:39:10+00:00
- **Authors**: Vincent Tjeng, Kai Xiao, Russ Tedrake
- **Comment**: Accepted as a conference paper at ICLR 2019
- **Journal**: None
- **Summary**: Neural networks have demonstrated considerable success on a wide variety of real-world problems. However, networks trained only to optimize for training accuracy can often be fooled by adversarial examples - slightly perturbed inputs that are misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional networks with an order of magnitude more ReLUs than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded $l_\infty$ norm $\epsilon=0.1$: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness (to perturbations with bounded norm) for the remainder. Across all robust training procedures and network architectures considered, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.



### Pixel-wise object tracking
- **Arxiv ID**: http://arxiv.org/abs/1711.07377v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07377v2)
- **Published**: 2017-11-20 15:30:41+00:00
- **Updated**: 2018-07-03 03:19:52+00:00
- **Authors**: Yilin Song, Chenge Li, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel pixel-wise visual object tracking framework that can track any anonymous object in a noisy background. The framework consists of two submodels, a global attention model and a local segmentation model. The global model generates a region of interests (ROI) that the object may lie in the new frame based on the past object segmentation maps, while the local model segments the new image in the ROI. Each model uses a LSTM structure to model the temporal dynamics of the motion and appearance, respectively. To circumvent the dependency of the training data between the two models, we use an iterative update strategy. Once the models are trained, there is no need to refine them to track specific objects, making our method efficient compared to online learning approaches. We demonstrate our real time pixel-wise object tracking framework on a challenging VOT dataset



### V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map
- **Arxiv ID**: http://arxiv.org/abs/1711.07399v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07399v3)
- **Published**: 2017-11-20 16:41:13+00:00
- **Updated**: 2018-08-16 14:55:40+00:00
- **Authors**: Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee
- **Comment**: HANDS 2017 Challenge Frame-based 3D Hand Pose Estimation Winner (ICCV
  2017), Published at CVPR 2018
- **Journal**: None
- **Summary**: Most of the existing deep learning-based methods for 3D hand and human pose estimation from a single depth map are based on a common framework that takes a 2D depth map and directly regresses the 3D coordinates of keypoints, such as hand or human body joints, via 2D convolutional neural networks (CNNs). The first weakness of this approach is the presence of perspective distortion in the 2D depth map. While the depth map is intrinsically 3D data, many previous methods treat depth maps as 2D images that can distort the shape of the actual object through projection from 3D to 2D space. This compels the network to perform perspective distortion-invariant estimation. The second weakness of the conventional approach is that directly regressing 3D coordinates from a 2D image is a highly non-linear mapping, which causes difficulty in the learning procedure. To overcome these weaknesses, we firstly cast the 3D hand and human pose estimation problem from a single depth map into a voxel-to-voxel prediction that uses a 3D voxelized grid and estimates the per-voxel likelihood for each keypoint. We design our model as a 3D CNN that provides accurate estimates while running in real-time. Our system outperforms previous methods in almost all publicly available 3D hand and human pose estimation datasets and placed first in the HANDS 2017 frame-based 3D hand pose estimation challenge. The code is available in https://github.com/mks0601/V2V-PoseNet_RELEASE.



### Disentangling Factors of Variation by Mixing Them
- **Arxiv ID**: http://arxiv.org/abs/1711.07410v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07410v2)
- **Published**: 2017-11-20 17:00:08+00:00
- **Updated**: 2018-03-28 10:27:00+00:00
- **Authors**: Qiyang Hu, Attila Szabó, Tiziano Portenier, Matthias Zwicker, Paolo Favaro
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: We propose an approach to learn image representations that consist of disentangled factors of variation without exploiting any manual labeling or data domain knowledge. A factor of variation corresponds to an image attribute that can be discerned consistently across a set of images, such as the pose or color of objects. Our disentangled representation consists of a concatenation of feature chunks, each chunk representing a factor of variation. It supports applications such as transferring attributes from one image to another, by simply mixing and unmixing feature chunks, and classification or retrieval based on one or several attributes, by considering a user-specified subset of feature chunks. We learn our representation without any labeling or knowledge of the data domain, using an autoencoder architecture with two novel training objectives: first, we propose an invariance objective to encourage that encoding of each attribute, and decoding of each chunk, are invariant to changes in other attributes and chunks, respectively; second, we include a classification objective, which ensures that each chunk corresponds to a consistently discernible attribute in the represented image, hence avoiding degenerate feature mappings where some chunks are completely ignored. We demonstrate the effectiveness of our approach on the MNIST, Sprites, and CelebA datasets.



### Robust Seed Mask Generation for Interactive Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.07419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07419v1)
- **Published**: 2017-11-20 17:19:24+00:00
- **Updated**: 2017-11-20 17:19:24+00:00
- **Authors**: Mario Amrehn, Stefan Steidl, Markus Kowarschik, Andreas Maier
- **Comment**: Medical Imaging Conference (MIC) 2017
- **Journal**: None
- **Summary**: In interactive medical image segmentation, anatomical structures are extracted from reconstructed volumetric images. The first iterations of user interaction traditionally consist of drawing pictorial hints as an initial estimate of the object to extract. Only after this time consuming first phase, the efficient selective refinement of current segmentation results begins. Erroneously labeled seeds, especially near the border of the object, are challenging to detect and replace for a human and may substantially impact the overall segmentation quality. We propose an automatic seeding pipeline as well as a configuration based on saliency recognition, in order to skip the time-consuming initial interaction phase during segmentation. A median Dice score of 68.22% is reached before the first user interaction on the test data set with an error rate in seeding of only 0.088%.



### Convolutional Networks for Object Category and 3D Pose Estimation from 2D Images
- **Arxiv ID**: http://arxiv.org/abs/1711.07426v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07426v3)
- **Published**: 2017-11-20 17:31:27+00:00
- **Updated**: 2018-07-20 19:21:36+00:00
- **Authors**: Siddharth Mahendran, Haider Ali, Rene Vidal
- **Comment**: None
- **Journal**: None
- **Summary**: Current CNN-based algorithms for recovering the 3D pose of an object in an image assume knowledge about both the object category and its 2D localization in the image. In this paper, we relax one of these constraints and propose to solve the task of joint object category and 3D pose estimation from an image assuming known 2D localization. We design a new architecture for this task composed of a feature network that is shared between subtasks, an object categorization network built on top of the feature network, and a collection of category dependent pose regression networks. We also introduce suitable loss functions and a training method for the new architecture. Experiments on the challenging PASCAL3D+ dataset show state-of-the-art performance in the joint categorization and pose estimation task. Moreover, our performance on the joint task is comparable to the performance of state-of-the-art methods on the simpler 3D pose estimation with known object category task.



### Action Recognition with Coarse-to-Fine Deep Feature Integration and Asynchronous Fusion
- **Arxiv ID**: http://arxiv.org/abs/1711.07430v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1711.07430v1)
- **Published**: 2017-11-20 17:35:46+00:00
- **Updated**: 2017-11-20 17:35:46+00:00
- **Authors**: Weiyao Lin, Yang Mi, Jianxin Wu, Ke Lu, Hongkai Xiong
- **Comment**: accepted by AAAI 2018
- **Journal**: None
- **Summary**: Action recognition is an important yet challenging task in computer vision. In this paper, we propose a novel deep-based framework for action recognition, which improves the recognition accuracy by: 1) deriving more precise features for representing actions, and 2) reducing the asynchrony between different information streams. We first introduce a coarse-to-fine network which extracts shared deep features at different action class granularities and progressively integrates them to obtain a more accurate feature representation for input actions. We further introduce an asynchronous fusion network. It fuses information from different streams by asynchronously integrating stream-wise features at different time points, hence better leveraging the complementary information in different streams. Experimental results on action recognition benchmarks demonstrate that our approach achieves the state-of-the-art performance.



### Self-Similarity Based Time Warping
- **Arxiv ID**: http://arxiv.org/abs/1711.07513v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9; I.5.4; H.5.1; H.5.5
- **Links**: [PDF](http://arxiv.org/pdf/1711.07513v1)
- **Published**: 2017-11-20 19:43:28+00:00
- **Updated**: 2017-11-20 19:43:28+00:00
- **Authors**: Christopher J. Tralie
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: In this work, we explore the problem of aligning two time-ordered point clouds which are spatially transformed and re-parameterized versions of each other. This has a diverse array of applications such as cross modal time series synchronization (e.g. MOCAP to video) and alignment of discretized curves in images. Most other works that address this problem attempt to jointly uncover a spatial alignment and correspondences between the two point clouds, or to derive local invariants to spatial transformations such as curvature before computing correspondences. By contrast, we sidestep spatial alignment completely by using self-similarity matrices (SSMs) as a proxy to the time-ordered point clouds, since self-similarity matrices are blind to isometries and respect global geometry. Our algorithm, dubbed "Isometry Blind Dynamic Time Warping" (IBDTW), is simple and general, and we show that its associated dissimilarity measure lower bounds the L1 Gromov-Hausdorff distance between the two point sets when restricted to warping paths. We also present a local, partial alignment extension of IBDTW based on the Smith Waterman algorithm. This eliminates the need for tedious manual cropping of time series, which is ordinarily necessary for global alignment algorithms to function properly.



### Dropping Activation Outputs with Localized First-layer Deep Network for Enhancing User Privacy and Data Security
- **Arxiv ID**: http://arxiv.org/abs/1711.07520v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07520v1)
- **Published**: 2017-11-20 19:57:57+00:00
- **Updated**: 2017-11-20 19:57:57+00:00
- **Authors**: Hao Dong, Chao Wu, Zhen Wei, Yike Guo
- **Comment**: IEEE Trans. Information Forensics and Security (TIFS)
- **Journal**: None
- **Summary**: Deep learning methods can play a crucial role in anomaly detection, prediction, and supporting decision making for applications like personal health-care, pervasive body sensing, etc. However, current architecture of deep networks suffers the privacy issue that users need to give out their data to the model (typically hosted in a server or a cluster on Cloud) for training or prediction. This problem is getting more severe for those sensitive health-care or medical data (e.g fMRI or body sensors measures like EEG signals). In addition to this, there is also a security risk of leaking these data during the data transmission from user to the model (especially when it's through Internet). Targeting at these issues, in this paper we proposed a new architecture for deep network in which users don't reveal their original data to the model. In our method, feed-forward propagation and data encryption are combined into one process: we migrate the first layer of deep network to users' local devices, and apply the activation functions locally, and then use "dropping activation output" method to make the output non-invertible. The resulting approach is able to make model prediction without accessing users' sensitive raw data. Experiment conducted in this paper showed that our approach achieves the desirable privacy protection requirement, and demonstrated several advantages over the traditional approach with encryption / decryption



### Neural 3D Mesh Renderer
- **Arxiv ID**: http://arxiv.org/abs/1711.07566v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.07566v1)
- **Published**: 2017-11-20 22:12:23+00:00
- **Updated**: 2017-11-20 22:12:23+00:00
- **Authors**: Hiroharu Kato, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: For modeling the 3D world behind 2D images, which 3D representation is most appropriate? A polygon mesh is a promising candidate for its compactness and geometric properties. However, it is not straightforward to model a polygon mesh from 2D images using neural networks because the conversion from a mesh to an image, or rendering, involves a discrete operation called rasterization, which prevents back-propagation. Therefore, in this work, we propose an approximate gradient for rasterization that enables the integration of rendering into neural networks. Using this renderer, we perform single-image 3D mesh reconstruction with silhouette image supervision and our system outperforms the existing voxel-based approach. Additionally, we perform gradient-based 3D mesh editing operations, such as 2D-to-3D style transfer and 3D DeepDream, with 2D supervision for the first time. These applications demonstrate the potential of the integration of a mesh renderer into neural networks and the effectiveness of our proposed renderer.



### On Nearest Neighbors in Non Local Means Denoising
- **Arxiv ID**: http://arxiv.org/abs/1711.07568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07568v1)
- **Published**: 2017-11-20 22:31:27+00:00
- **Updated**: 2017-11-20 22:31:27+00:00
- **Authors**: Iuri Frosio, Jan Kautz
- **Comment**: This paper is accepted at the 2017 NIPS workshop "Nearest Neighbors
  for Modern Applications with Massive Data"
- **Journal**: None
- **Summary**: To denoise a reference patch, the Non-Local-Means denoising filter processes a set of neighbor patches. Few Nearest Neighbors (NN) are used to limit the computational burden of the algorithm. Here here we show analytically that the NN approach introduces a bias in the denoised patch, and we propose a different neighbors' collection criterion, named Statistical NN (SNN), to alleviate this issue. Our approach outperforms the traditional one in case of both white and colored noise: fewer SNNs generate images of higher quality, at a lower computational cost.



