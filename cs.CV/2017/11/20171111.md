# Arxiv Papers in cs.CV on 2017-11-11
### DeepKSPD: Learning Kernel-matrix-based SPD Representation for Fine-grained Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.04047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04047v1)
- **Published**: 2017-11-11 00:41:32+00:00
- **Updated**: 2017-11-11 00:41:32+00:00
- **Authors**: Melih Engin, Lei Wang, Luping Zhou, Xinwang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Being symmetric positive-definite (SPD), covariance matrix has traditionally been used to represent a set of local descriptors in visual recognition. Recent study shows that kernel matrix can give considerably better representation by modelling the nonlinearity in the local descriptor set. Nevertheless, neither the descriptors nor the kernel matrix is deeply learned. Worse, they are considered separately, hindering the pursuit of an optimal SPD representation. This work proposes a deep network that jointly learns local descriptors, kernel-matrix-based SPD representation, and the classifier via an end-to-end training process. We derive the derivatives for the mapping from a local descriptor set to the SPD representation to carry out backpropagation. Also, we exploit the Daleckii-Krein formula in operator theory to give a concise and unified result on differentiating SPD matrix functions, including the matrix logarithm to handle the Riemannian geometry of kernel matrix. Experiments not only show the superiority of kernel-matrix-based SPD representation with deep local descriptors, but also verify the advantage of the proposed deep network in pursuing better SPD representations for fine-grained image recognition tasks.



### CT-SRCNN: Cascade Trained and Trimmed Deep Convolutional Neural Networks for Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1711.04048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04048v1)
- **Published**: 2017-11-11 00:55:57+00:00
- **Updated**: 2017-11-11 00:55:57+00:00
- **Authors**: Haoyu Ren, Mostafa El-Khamy, Jungwon Lee
- **Comment**: Accepted to IEEE Winter Conf. on Applications of Computer Vision
  (WACV) 2018, Lake Tahoe, USA
- **Journal**: None
- **Summary**: We propose methodologies to train highly accurate and efficient deep convolutional neural networks (CNNs) for image super resolution (SR). A cascade training approach to deep learning is proposed to improve the accuracy of the neural networks while gradually increasing the number of network layers. Next, we explore how to improve the SR efficiency by making the network slimmer. Two methodologies, the one-shot trimming and the cascade trimming, are proposed. With the cascade trimming, the network's size is gradually reduced layer by layer, without significant loss on its discriminative ability. Experiments on benchmark image datasets show that our proposed SR network achieves the state-of-the-art super resolution accuracy, while being more than 4 times faster compared to existing deep super resolution networks.



### Going Further with Point Pair Features
- **Arxiv ID**: http://arxiv.org/abs/1711.04061v1
- **DOI**: 10.1007/978-3-319-46487-9_51
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04061v1)
- **Published**: 2017-11-11 02:24:58+00:00
- **Updated**: 2017-11-11 02:24:58+00:00
- **Authors**: Stefan Hinterstoisser, Vincent Lepetit, Naresh Rajkumar, Kurt Konolige
- **Comment**: Corrected post-print of manuscript accepted to the European
  Conference on Computer Vision (ECCV) 2016;
  https://link.springer.com/chapter/10.1007/978-3-319-46487-9_51
- **Journal**: None
- **Summary**: Point Pair Features is a widely used method to detect 3D objects in point clouds, however they are prone to fail in presence of sensor noise and background clutter. We introduce novel sampling and voting schemes that significantly reduces the influence of clutter and sensor noise. Our experiments show that with our improvements, PPFs become competitive against state-of-the-art methods as it outperforms them on several objects from challenging benchmarks, at a low computational cost.



### Towards ECDSA key derivation from deep embeddings for novel Blockchain applications
- **Arxiv ID**: http://arxiv.org/abs/1711.04069v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1711.04069v1)
- **Published**: 2017-11-11 03:07:34+00:00
- **Updated**: 2017-11-11 03:07:34+00:00
- **Authors**: Christian S. Perone
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: In this work, we propose a straightforward method to derive Elliptic Curve Digital Signature Algorithm (ECDSA) key pairs from embeddings created using Deep Learning and Metric Learning approaches. We also show that these keys allows the derivation of cryptocurrencies (such as Bitcoin) addresses that can be used to transfer and receive funds, allowing novel Blockchain-based applications that can be used to transfer funds or data directly to domains such as image, text, sound or any other domain where Deep Learning can extract high-quality embeddings; providing thus a novel integration between the properties of the Blockchain-based technologies such as trust minimization and decentralization together with the high-quality learned representations from Deep Learning techniques.



### Phrase-based Image Captioning with Hierarchical LSTM Model
- **Arxiv ID**: http://arxiv.org/abs/1711.05557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1711.05557v1)
- **Published**: 2017-11-11 10:48:59+00:00
- **Updated**: 2017-11-11 10:48:59+00:00
- **Authors**: Ying Hua Tan, Chee Seng Chan
- **Comment**: 17 pages, 12 figures, ACCV2016 extension, phrase-based image
  captioning
- **Journal**: None
- **Summary**: Automatic generation of caption to describe the content of an image has been gaining a lot of research interests recently, where most of the existing works treat the image caption as pure sequential data. Natural language, however possess a temporal hierarchy structure, with complex dependencies between each subsequence. In this paper, we propose a phrase-based hierarchical Long Short-Term Memory (phi-LSTM) model to generate image description. In contrast to the conventional solutions that generate caption in a pure sequential manner, our proposed model decodes image caption from phrase to sentence. It consists of a phrase decoder at the bottom hierarchy to decode noun phrases of variable length, and an abbreviated sentence decoder at the upper hierarchy to decode an abbreviated form of the image description. A complete image caption is formed by combining the generated phrases with sentence during the inference stage. Empirically, our proposed model shows a better or competitive result on the Flickr8k, Flickr30k and MS-COCO datasets in comparison to the state-of-the art models. We also show that our proposed model is able to generate more novel captions (not seen in the training data) which are richer in word contents in all these three datasets.



### Deep Residual Text Detection Network for Scene Text
- **Arxiv ID**: http://arxiv.org/abs/1711.04147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04147v1)
- **Published**: 2017-11-11 15:03:33+00:00
- **Updated**: 2017-11-11 15:03:33+00:00
- **Authors**: Xiangyu Zhu, Yingying Jiang, Shuli Yang, Xiaobing Wang, Wei Li, Pei Fu, Hua Wang, Zhenbo Luo
- **Comment**: IAPR International Conference on Document Analysis and Recognition
  (ICDAR) 2017
- **Journal**: None
- **Summary**: Scene text detection is a challenging problem in computer vision. In this paper, we propose a novel text detection network based on prevalent object detection frameworks. In order to obtain stronger semantic feature, we adopt ResNet as feature extraction layers and exploit multi-level feature by combining hierarchical convolutional networks. A vertical proposal mechanism is utilized to avoid proposal classification, while regression layer remains working to improve localization accuracy. Our approach evaluated on ICDAR2013 dataset achieves F-measure of 0.91, which outperforms previous state-of-the-art results in scene text detection.



### End-to-end Video-level Representation Learning for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.04161v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04161v7)
- **Published**: 2017-11-11 15:52:19+00:00
- **Updated**: 2018-04-21 15:03:17+00:00
- **Authors**: Jiagang Zhu, Wei Zou, Zheng Zhu
- **Comment**: 10 pages, 6 figures, 6 tables. The explanation for the batch size is
  added. Accepted by ICPR 2018
- **Journal**: None
- **Summary**: From the frame/clip-level feature learning to the video-level representation building, deep learning methods in action recognition have developed rapidly in recent years. However, current methods suffer from the confusion caused by partial observation training, or without end-to-end learning, or restricted to single temporal scale modeling and so on. In this paper, we build upon two-stream ConvNets and propose Deep networks with Temporal Pyramid Pooling (DTPP), an end-to-end video-level representation learning approach, to address these problems. Specifically, at first, RGB images and optical flow stacks are sparsely sampled across the whole video. Then a temporal pyramid pooling layer is used to aggregate the frame-level features which consist of spatial and temporal cues. Lastly, the trained model has compact video-level representation with multiple temporal scales, which is both global and sequence-aware. Experimental results show that DTPP achieves the state-of-the-art performance on two challenging video action datasets: UCF101 and HMDB51, either by ImageNet pre-training or Kinetics pre-training.



### 3D Randomized Connection Network with Graph-based Label Inference
- **Arxiv ID**: http://arxiv.org/abs/1711.04170v1
- **DOI**: 10.1109/TIP.2018.2829263
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04170v1)
- **Published**: 2017-11-11 16:50:42+00:00
- **Updated**: 2017-11-11 16:50:42+00:00
- **Authors**: Siqi Bao, Pei Wang, Tony C. W. Mok, Albert C. S. Chung
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a novel 3D deep learning network is proposed for brain MR image segmentation with randomized connection, which can decrease the dependency between layers and increase the network capacity. The convolutional LSTM and 3D convolution are employed as network units to capture the long-term and short-term 3D properties respectively. To assemble these two kinds of spatial-temporal information and refine the deep learning outcomes, we further introduce an efficient graph-based node selection and label inference method. Experiments have been carried out on two publicly available databases and results demonstrate that the proposed method can obtain competitive performances as compared with other state-of-the-art methods.



### CUR Decompositions, Similarity Matrices, and Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1711.04178v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.04178v3)
- **Published**: 2017-11-11 18:34:34+00:00
- **Updated**: 2018-12-11 20:53:22+00:00
- **Authors**: Akram Aldroubi, Keaton Hamm, Ahmet Bugra Koku, Ali Sekmen
- **Comment**: Approximately 30 pages. Current version contains improved algorithm
  and numerical experiments from the previous version
- **Journal**: None
- **Summary**: A general framework for solving the subspace clustering problem using the CUR decomposition is presented. The CUR decomposition provides a natural way to construct similarity matrices for data that come from a union of unknown subspaces $\mathscr{U}=\underset{i=1}{\overset{M}\bigcup}S_i$. The similarity matrices thus constructed give the exact clustering in the noise-free case. Additionally, this decomposition gives rise to many distinct similarity matrices from a given set of data, which allow enough flexibility to perform accurate clustering of noisy data. We also show that two known methods for subspace clustering can be derived from the CUR decomposition. An algorithm based on the theoretical construction of similarity matrices is presented, and experiments on synthetic and real data are presented to test the method.   Additionally, an adaptation of our CUR based similarity matrices is utilized to provide a heuristic algorithm for subspace clustering; this algorithm yields the best overall performance to date for clustering the Hopkins155 motion segmentation dataset.



### End-to-end 3D shape inverse rendering of different classes of objects from a single input image
- **Arxiv ID**: http://arxiv.org/abs/1711.05858v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1711.05858v1)
- **Published**: 2017-11-11 19:13:57+00:00
- **Updated**: 2017-11-11 19:13:57+00:00
- **Authors**: Shima Kamyab, S. Zohreh Azimifar
- **Comment**: 16 pages, 12 figures, 2 tables
- **Journal**: None
- **Summary**: In this paper a semi-supervised deep framework is proposed for the problem of 3D shape inverse rendering from a single 2D input image. The main structure of proposed framework consists of unsupervised pre-trained components which significantly reduce the need to labeled data for training the whole framework. using labeled data has the advantage of achieving to accurate results without the need to predefined assumptions about image formation process. Three main components are used in the proposed network: an encoder which maps 2D input image to a representation space, a 3D decoder which decodes a representation to a 3D structure and a mapping component in order to map 2D to 3D representation. The only part that needs label for training is the mapping part with not too many parameters. The other components in the network can be pre-trained unsupervised using only 2D images or 3D data in each case. The way of reconstructing 3D shapes in the decoder component, inspired by the model based methods for 3D reconstruction, maps a low dimensional representation to 3D shape space with the advantage of extracting the basis vectors of shape space from training data itself and is not restricted to a small set of examples as used in predefined models. Therefore, the proposed framework deals directly with coordinate values of the point cloud representation which leads to achieve dense 3D shapes in the output. The experimental results on several benchmark datasets of objects and human faces and comparing with recent similar methods shows the power of proposed network in recovering more details from single 2D images.



### Latent Constrained Correlation Filter
- **Arxiv ID**: http://arxiv.org/abs/1711.04192v1
- **DOI**: 10.1109/TIP.2017.2775060
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04192v1)
- **Published**: 2017-11-11 20:27:39+00:00
- **Updated**: 2017-11-11 20:27:39+00:00
- **Authors**: Baochang Zhang, Shangzhen Luan, Chen Chen, Jungong Han, Wei Wang, Alessandro Perina, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Correlation filters are special classifiers designed for shift-invariant object recognition, which are robust to pattern distortions. The recent literature shows that combining a set of sub-filters trained based on a single or a small group of images obtains the best performance. The idea is equivalent to estimating variable distribution based on the data sampling (bagging), which can be interpreted as finding solutions (variable distribution approximation) directly from sampled data space. However, this methodology fails to account for the variations existed in the data. In this paper, we introduce an intermediate step -- solution sampling -- after the data sampling step to form a subspace, in which an optimal solution can be estimated. More specifically, we propose a new method, named latent constrained correlation filters (LCCF), by mapping the correlation filters to a given latent subspace, and develop a new learning framework in the latent subspace that embeds distribution-related constraints into the original problem. To solve the optimization problem, we introduce a subspace based alternating direction method of multipliers (SADMM), which is proven to converge at the saddle point. Our approach is successfully applied to three different tasks, including eye localization, car detection and object tracking. Extensive experiments demonstrate that LCCF outperforms the state-of-the-art methods. The source code will be publicly available. https://github.com/bczhangbczhang/.



