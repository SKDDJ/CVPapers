# Arxiv Papers in cs.CV on 2017-11-02
### Random Subspace Two-dimensional LDA for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.00575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00575v1)
- **Published**: 2017-11-02 00:27:01+00:00
- **Updated**: 2017-11-02 00:27:01+00:00
- **Authors**: Garrett Bingham
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a novel technique named random subspace two-dimensional LDA (RS-2DLDA) is developed for face recognition. This approach offers a number of improvements over the random subspace two-dimensional PCA (RS2DPCA) framework introduced by Nguyen et al. [5]. Firstly, the eigenvectors from 2DLDA have more discriminative power than those from 2DPCA, resulting in higher accuracy for the RS-2DLDA method over RS-2DPCA. Various distance metrics are evaluated, and a weighting scheme is developed to further boost accuracy. A series of experiments on the MORPH-II and ORL datasets are conducted to demonstrate the effectiveness of this approach.



### Deep Learning from Noisy Image Labels with Quality Embedding
- **Arxiv ID**: http://arxiv.org/abs/1711.00583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00583v1)
- **Published**: 2017-11-02 01:19:25+00:00
- **Updated**: 2017-11-02 01:19:25+00:00
- **Authors**: Jiangchao Yao, Jiajie Wang, Ivor Tsang, Ya Zhang, Jun Sun, Chengqi Zhang, Rui Zhang
- **Comment**: Under review for Transactions on Image Processing
- **Journal**: None
- **Summary**: There is an emerging trend to leverage noisy image datasets in many visual recognition tasks. However, the label noise among the datasets severely degenerates the \mbox{performance of deep} learning approaches. Recently, one mainstream is to introduce the latent label to handle label noise, which has shown promising improvement in the network designs. Nevertheless, the mismatch between latent labels and noisy labels still affects the predictions in such methods. To address this issue, we propose a quality embedding model, which explicitly introduces a quality variable to represent the trustworthiness of noisy labels. Our key idea is to identify the mismatch between the latent and noisy labels by embedding the quality variables into different subspaces, which effectively minimizes the noise effect. At the same time, the high-quality labels is still able to be applied for training. To instantiate the model, we further propose a Contrastive-Additive Noise network (CAN), which consists of two important layers: (1) the contrastive layer estimates the quality variable in the embedding space to reduce noise effect; and (2) the additive layer aggregates the prior predictions and noisy labels as the posterior to train the classifier. Moreover, to tackle the optimization difficulty, we deduce an SGD algorithm with the reparameterization tricks, which makes our method scalable to big data. We conduct the experimental evaluation of the proposed method over a range of noisy image datasets. Comprehensive results have demonstrated CAN outperforms the state-of-the-art deep learning approaches.



### A Bio-Inspired Multi-Exposure Fusion Framework for Low-light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1711.00591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00591v1)
- **Published**: 2017-11-02 02:18:03+00:00
- **Updated**: 2017-11-02 02:18:03+00:00
- **Authors**: Zhenqiang Ying, Ge Li, Wen Gao
- **Comment**: Project website: https://baidut.github.io/BIMEF/
- **Journal**: None
- **Summary**: Low-light images are not conducive to human observation and computer vision algorithms due to their low visibility. Although many image enhancement techniques have been proposed to solve this problem, existing methods inevitably introduce contrast under- and over-enhancement. Inspired by human visual system, we design a multi-exposure fusion framework for low-light image enhancement. Based on the framework, we propose a dual-exposure fusion algorithm to provide an accurate contrast and lightness enhancement. Specifically, we first design the weight matrix for image fusion using illumination estimation techniques. Then we introduce our camera response model to synthesize multi-exposure images. Next, we find the best exposure ratio so that the synthetic image is well-exposed in the regions where the original image is under-exposed. Finally, the enhanced result is obtained by fusing the input image and the synthetic image according to the weight matrix. Experiments show that our method can obtain results with less contrast and lightness distortion compared to that of several state-of-the-art methods.



### Data Augmentation in Emotion Classification Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.00648v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00648v5)
- **Published**: 2017-11-02 08:35:07+00:00
- **Updated**: 2017-12-14 06:27:58+00:00
- **Authors**: Xinyue Zhu, Yifan Liu, Zengchang Qin, Jiahong Li
- **Comment**: None
- **Journal**: None
- **Summary**: It is a difficult task to classify images with multiple class labels using only a small number of labeled examples, especially when the label (class) distribution is imbalanced. Emotion classification is such an example of imbalanced label distribution, because some classes of emotions like \emph{disgusted} are relatively rare comparing to other labels like {\it happy or sad}. In this paper, we propose a data augmentation method using generative adversarial networks (GAN). It can complement and complete the data manifold and find better margins between neighboring classes. Specifically, we design a framework with a CNN model as the classifier and a cycle-consistent adversarial networks (CycleGAN) as the generator. In order to avoid gradient vanishing problem, we employ the least-squared loss as adversarial loss. We also propose several evaluation methods on three benchmark datasets to validate GAN's performance. Empirical results show that we can obtain 5%~10% increase in the classification accuracy after employing the GAN-based data augmentation techniques.



### Development and validation of a novel dementia of Alzheimer's type (DAT) score based on metabolism FDG-PET imaging
- **Arxiv ID**: http://arxiv.org/abs/1711.00671v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00671v1)
- **Published**: 2017-11-02 10:06:53+00:00
- **Updated**: 2017-11-02 10:06:53+00:00
- **Authors**: Karteek Popuri, Rakesh Balachandar, Kathryn Alpert, Donghuan Lu, Mahadev Bhalla, Ian Mackenzie, Robin Ging-Yuek Hsiung, Lei Wang, Mirza Faisal Beg, the Alzhemier's Disease Neuroimaging Initiative
- **Comment**: None
- **Journal**: None
- **Summary**: Fluorodeoxyglucose positron emission tomography (FDG-PET) imaging based 3D topographic brain glucose metabolism patterns from normal controls (NC) and individuals with dementia of Alzheimer's type (DAT) are used to train a novel multi-scale ensemble classification model. This ensemble model outputs a FDG-PET DAT score (FPDS) between 0 and 1 denoting the probability of a subject to be clinically diagnosed with DAT based on their metabolism profile. A novel 7 group image stratification scheme is devised that groups images not only based on their associated clinical diagnosis but also on past and future trajectories of the clinical diagnoses, yielding a more continuous representation of the different stages of DAT spectrum that mimics a real-world clinical setting. The potential for using FPDS as a DAT biomarker was validated on a large number of FDG-PET images (N=2984) obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database taken across the proposed stratification, and a good classification AUC (area under the curve) of 0.78 was achieved in distinguishing between images belonging to subjects on a DAT trajectory and those images taken from subjects not progressing to a DAT diagnosis. Further, the FPDS biomarker achieved state-of-the-art performance on the mild cognitive impairment (MCI) to DAT conversion prediction task with an AUC of 0.81, 0.80, 0.77 for the 2, 3, 5 years to conversion windows respectively.



### Understanding and Predicting The Attractiveness of Human Action Shot
- **Arxiv ID**: http://arxiv.org/abs/1711.00677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00677v1)
- **Published**: 2017-11-02 10:31:08+00:00
- **Updated**: 2017-11-02 10:31:08+00:00
- **Authors**: Bin Dai, Baoyuan Wang, Gang Hua
- **Comment**: None
- **Journal**: None
- **Summary**: Selecting attractive photos from a human action shot sequence is quite challenging, because of the subjective nature of the "attractiveness", which is mainly a combined factor of human pose in action and the background. Prior works have actively studied high-level image attributes including interestingness, memorability, popularity, and aesthetics. However, none of them has ever studied the "attractiveness" of human action shot. In this paper, we present the first study of the "attractiveness" of human action shots by taking a systematic data-driven approach. Specifically, we create a new action-shot dataset composed of about 8000 high quality action-shot photos. We further conduct rich crowd-sourced human judge studies on Amazon Mechanical Turk(AMT) in terms of global attractiveness of a single photo, and relative attractiveness of a pair of photos. A deep Siamese network with a novel hybrid distribution matching loss was further proposed to fully exploit both types of ratings. Extensive experiments reveal that (1) the property of action shot attractiveness is subjective but predicable (2) our proposed method is both efficient and effective for predicting the attractive human action shots.



### Statistical evaluation of visual quality metrics for image denoising
- **Arxiv ID**: http://arxiv.org/abs/1711.00693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00693v1)
- **Published**: 2017-11-02 11:36:51+00:00
- **Updated**: 2017-11-02 11:36:51+00:00
- **Authors**: Karen Egiazarian, Mykola Ponomarenko, Vladimir Lukin, Oleg Ieremeiem
- **Comment**: Submitted to ICASSP 2018
- **Journal**: None
- **Summary**: This paper studies the problem of full reference visual quality assessment of denoised images with a special emphasis on images with low contrast and noise-like texture. Denoising of such images together with noise removal often results in image details loss or smoothing. A new test image database, FLT, containing 75 noise-free "reference" images and 300 filtered ("distorted") images is developed. Each reference image, corrupted by an additive white Gaussian noise, is denoised by the BM3D filter with four different values of threshold parameter (four levels of noise suppression). After carrying out a perceptual quality assessment of distorted images, the mean opinion scores (MOS) are obtained and compared with the values of known full reference quality metrics. As a result, the Spearman Rank Order Correlation Coefficient (SROCC) between PSNR values and MOS has a value close to zero, and SROCC between values of known full-reference image visual quality metrics and MOS does not exceed 0.82 (which is reached by a new visual quality metric proposed in this paper). The FLT dataset is more complex than earlier datasets used for assessment of visual quality for image denoising. Thus, it can be effectively used to design new image visual quality metrics for image denoising.



### Variational Inference of Disentangled Latent Concepts from Unlabeled Observations
- **Arxiv ID**: http://arxiv.org/abs/1711.00848v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.00848v3)
- **Published**: 2017-11-02 17:57:43+00:00
- **Updated**: 2018-12-27 19:25:22+00:00
- **Authors**: Abhishek Kumar, Prasanna Sattigeri, Avinash Balakrishnan
- **Comment**: ICLR 2018 Version
- **Journal**: None
- **Summary**: Disentangled representations, where the higher level data generative factors are reflected in disjoint latent dimensions, offer several benefits such as ease of deriving invariant representations, transferability to other tasks, interpretability, etc. We consider the problem of unsupervised learning of disentangled representations from large pool of unlabeled observations, and propose a variational inference based approach to infer disentangled latent factors. We introduce a regularizer on the expectation of the approximate posterior over observed data that encourages the disentanglement. We also propose a new disentanglement metric which is better aligned with the qualitative disentanglement observed in the decoder's output. We empirically observe significant improvement over existing methods in terms of both disentanglement and data likelihood (reconstruction quality).



### Set-to-Set Hashing with Applications in Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.00888v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1711.00888v2)
- **Published**: 2017-11-02 18:57:43+00:00
- **Updated**: 2019-05-29 02:16:43+00:00
- **Authors**: I-Hong Jhuo, Jun Wang
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Visual data, such as an image or a sequence of video frames, is often naturally represented as a point set. In this paper, we consider the fundamental problem of finding a nearest set from a collection of sets, to a query set. This problem has obvious applications in large-scale visual retrieval and recognition, and also in applied fields beyond computer vision. One challenge stands out in solving the problem---set representation and measure of similarity. Particularly, the query set and the sets in dataset collection can have varying cardinalities. The training collection is large enough such that linear scan is impractical. We propose a simple representation scheme that encodes both statistical and structural information of the sets. The derived representations are integrated in a kernel framework for flexible similarity measurement. For the query set process, we adopt a learning-to-hash pipeline that turns the kernel representations into hash bits based on simple learners, using multiple kernel learning. Experiments on two visual retrieval datasets show unambiguously that our set-to-set hashing framework outperforms prior methods that do not take the set-to-set search setting.



### Structured Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.00889v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.00889v1)
- **Published**: 2017-11-02 19:00:56+00:00
- **Updated**: 2017-11-02 19:00:56+00:00
- **Authors**: Zhijie Deng, Hao Zhang, Xiaodan Liang, Luona Yang, Shizhen Xu, Jun Zhu, Eric P. Xing
- **Comment**: To appear in NIPS 2017
- **Journal**: None
- **Summary**: We study the problem of conditional generative modeling based on designated semantics or structures. Existing models that build conditional generators either require massive labeled instances as supervision or are unable to accurately control the semantics of generated samples. We propose structured generative adversarial networks (SGANs) for semi-supervised conditional generative modeling. SGAN assumes the data x is generated conditioned on two independent latent variables: y that encodes the designated semantics, and z that contains other factors of variation. To ensure disentangled semantics in y and z, SGAN builds two collaborative games in the hidden space to minimize the reconstruction error of y and z, respectively. Training SGAN also involves solving two adversarial games that have their equilibrium concentrating at the true joint data distributions p(x, z) and p(x, y), avoiding distributing the probability mass diffusely over data space that MLE-based methods may suffer. We assess SGAN by evaluating its trained networks, and its performance on downstream tasks. We show that SGAN delivers a highly controllable generator, and disentangled representations; it also establishes start-of-the-art results across multiple datasets when applied for semi-supervised image classification (1.27%, 5.73%, 17.26% error rates on MNIST, SVHN and CIFAR-10 using 50, 1000 and 4000 labels, respectively). Benefiting from the separate modeling of y and z, SGAN can generate images with high visual quality and strictly following the designated semantic, and can be extended to a wide spectrum of applications, such as style transfer.



### Automatic Query Image Disambiguation for Content-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1711.00953v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1711.00953v1)
- **Published**: 2017-11-02 21:55:11+00:00
- **Updated**: 2017-11-02 21:55:11+00:00
- **Authors**: Björn Barz, Joachim Denzler
- **Comment**: VISAPP 2018 paper, 8 pages, 5 figures. Source code:
  https://github.com/cvjena/aid
- **Journal**: None
- **Summary**: Query images presented to content-based image retrieval systems often have various different interpretations, making it difficult to identify the search objective pursued by the user. We propose a technique for overcoming this ambiguity, while keeping the amount of required user interaction at a minimum. To achieve this, the neighborhood of the query image is divided into coherent clusters from which the user may choose the relevant ones. A novel feedback integration technique is then employed to re-rank the entire database with regard to both the user feedback and the original query. We evaluate our approach on the publicly available MIRFLICKR-25K dataset, where it leads to a relative improvement of average precision by 23% over the baseline retrieval, which does not distinguish between different image senses.



### A Classification-Based Study of Covariate Shift in GAN Distributions
- **Arxiv ID**: http://arxiv.org/abs/1711.00970v7
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.00970v7)
- **Published**: 2017-11-02 23:13:39+00:00
- **Updated**: 2018-06-06 02:51:11+00:00
- **Authors**: Shibani Santurkar, Ludwig Schmidt, Aleksander Mądry
- **Comment**: None
- **Journal**: None
- **Summary**: A basic, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether they are truly able to capture all the fundamental characteristics of the distributions they are trained on. In particular, evaluating the diversity of GAN distributions is challenging and existing methods provide only a partial understanding of this issue. In this paper, we develop quantitative and scalable tools for assessing the diversity of GAN distributions. Specifically, we take a classification-based perspective and view loss of diversity as a form of covariate shift introduced by GANs. We examine two specific forms of such shift: mode collapse and boundary distortion. In contrast to prior work, our methods need only minimal human supervision and can be readily applied to state-of-the-art GANs on large, canonical datasets. Examining popular GANs using our tools indicates that these GANs have significant problems in reproducing the more distributional properties of their training dataset.



### The Achievement of Higher Flexibility in Multiple Choice-based Tests Using Image Classification Techniques
- **Arxiv ID**: http://arxiv.org/abs/1711.00972v9
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00972v9)
- **Published**: 2017-11-02 23:36:13+00:00
- **Updated**: 2019-01-12 03:34:13+00:00
- **Authors**: Mahmoud Afifi, Khaled F. Hussain
- **Comment**: None
- **Journal**: None
- **Summary**: In spite of the high accuracy of the existing optical mark reading (OMR) systems and devices, a few restrictions remain existent. In this work, we aim to reduce the restrictions of multiple choice questions (MCQ) within tests. We use an image registration technique to extract the answer boxes from answer sheets. Unlike other systems that rely on simple image processing steps to recognize the extracted answer boxes, we address the problem from another perspective by training a machine learning classifier to recognize the class of each answer box (i.e., confirmed, crossed out, or blank answer). This gives us the ability to deal with a variety of shading and mark patterns, and distinguish between chosen (i.e., confirmed) and canceled answers (i.e., crossed out). All existing machine learning techniques require a large number of examples in order to train a model for classification, therefore we present a dataset including six real MCQ assessments with different answer sheet templates. We evaluate two strategies of classification: a straight-forward approach and a two-stage classifier approach. We test two handcrafted feature methods and a convolutional neural network. In the end, we present an easy-to-use graphical user interface of the proposed system. Compared with existing OMR systems, the proposed system has the least constraints and achieves a high accuracy. We believe that the presented work will further direct the development of OMR systems towards reducing the restrictions of the MCQ tests.



