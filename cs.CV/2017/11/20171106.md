# Arxiv Papers in cs.CV on 2017-11-06
### End-to-End Video Classification with Knowledge Graphs
- **Arxiv ID**: http://arxiv.org/abs/1711.01714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.01714v1)
- **Published**: 2017-11-06 03:50:35+00:00
- **Updated**: 2017-11-06 03:50:35+00:00
- **Authors**: Fang Yuan, Zhe Wang, Jie Lin, Luis Fernando D'Haro, Kim Jung Jae, Zeng Zeng, Vijay Chandrasekhar
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Video understanding has attracted much research attention especially since the recent availability of large-scale video benchmarks. In this paper, we address the problem of multi-label video classification. We first observe that there exists a significant knowledge gap between how machines and humans learn. That is, while current machine learning approaches including deep neural networks largely focus on the representations of the given data, humans often look beyond the data at hand and leverage external knowledge to make better decisions. Towards narrowing the gap, we propose to incorporate external knowledge graphs into video classification. In particular, we unify traditional "knowledgeless" machine learning models and knowledge graphs in a novel end-to-end framework. The framework is flexible to work with most existing video classification algorithms including state-of-the-art deep models. Finally, we conduct extensive experiments on the largest public video dataset YouTube-8M. The results are promising across the board, improving mean average precision by up to 2.9%.



### Active Learning for Visual Question Answering: An Empirical Study
- **Arxiv ID**: http://arxiv.org/abs/1711.01732v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.01732v1)
- **Published**: 2017-11-06 05:28:38+00:00
- **Updated**: 2017-11-06 05:28:38+00:00
- **Authors**: Xiao Lin, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: We present an empirical study of active learning for Visual Question Answering, where a deep VQA model selects informative question-image pairs from a pool and queries an oracle for answers to maximally improve its performance under a limited query budget. Drawing analogies from human learning, we explore cramming (entropy), curiosity-driven (expected model change), and goal-driven (expected error reduction) active learning approaches, and propose a fast and effective goal-driven active learning scoring function to pick question-image pairs for deep VQA models under the Bayesian Neural Network framework. We find that deep VQA models need large amounts of training data before they can start asking informative questions. But once they do, all three approaches outperform the random selection baseline and achieve significant query savings. For the scenario where the model is allowed to ask generic questions about images but is evaluated only on specific questions (e.g., questions whose answer is either yes or no), our proposed goal-driven scoring function performs the best.



### Towards Reverse-Engineering Black-Box Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.01768v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.01768v3)
- **Published**: 2017-11-06 07:58:48+00:00
- **Updated**: 2018-02-14 18:50:53+00:00
- **Authors**: Seong Joon Oh, Max Augustin, Bernt Schiele, Mario Fritz
- **Comment**: 20 pages, 12 figures, to appear at ICLR'18. Code:
  https://goo.gl/MbYfsv
- **Journal**: None
- **Summary**: Many deployed learned models are black boxes: given input, returns output. Internal information about the model, such as the architecture, optimisation procedure, or training data, is not disclosed explicitly as it might contain proprietary information or make the system more vulnerable. This work shows that such attributes of neural networks can be exposed from a sequence of queries. This has multiple implications. On the one hand, our work exposes the vulnerability of black-box neural networks to different types of attacks -- we show that the revealed internal information helps generate more effective adversarial examples against the black box model. On the other hand, this technique can be used for better protection of private content from automatic recognition models using adversarial examples. Our paper suggests that it is actually hard to draw a line between white box and black box models.



### HyperNetworks with statistical filtering for defending adversarial examples
- **Arxiv ID**: http://arxiv.org/abs/1711.01791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.01791v1)
- **Published**: 2017-11-06 09:11:47+00:00
- **Updated**: 2017-11-06 09:11:47+00:00
- **Authors**: Zhun Sun, Mete Ozay, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning algorithms have been known to be vulnerable to adversarial perturbations in various tasks such as image classification. This problem was addressed by employing several defense methods for detection and rejection of particular types of attacks. However, training and manipulating networks according to particular defense schemes increases computational complexity of the learning algorithms. In this work, we propose a simple yet effective method to improve robustness of convolutional neural networks (CNNs) to adversarial attacks by using data dependent adaptive convolution kernels. To this end, we propose a new type of HyperNetwork in order to employ statistical properties of input data and features for computation of statistical adaptive maps. Then, we filter convolution weights of CNNs with the learned statistical maps to compute dynamic kernels. Thereby, weights and kernels are collectively optimized for learning of image classification models robust to adversarial attacks without employment of additional target detection and rejection algorithms. We empirically demonstrate that the proposed method enables CNNs to spontaneously defend against different types of attacks, e.g. attacks generated by Gaussian noise, fast gradient sign methods (Goodfellow et al., 2014) and a black-box attack(Narodytska & Kasiviswanathan, 2016).



### Optimal transport maps for distribution preserving operations on latent spaces of Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1711.01970v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.01970v2)
- **Published**: 2017-11-06 15:51:29+00:00
- **Updated**: 2018-01-24 21:55:52+00:00
- **Authors**: Eirikur Agustsson, Alexander Sage, Radu Timofte, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Generative models such as Variational Auto Encoders (VAEs) and Generative Adversarial Networks (GANs) are typically trained for a fixed prior distribution in the latent space, such as uniform or Gaussian. After a trained model is obtained, one can sample the Generator in various forms for exploration and understanding, such as interpolating between two samples, sampling in the vicinity of a sample or exploring differences between a pair of samples applied to a third sample. In this paper, we show that the latent space operations used in the literature so far induce a distribution mismatch between the resulting outputs and the prior distribution the model was trained on. To address this, we propose to use distribution matching transport maps to ensure that such latent space operations preserve the prior distribution, while minimally modifying the original operation. Our experimental results validate that the proposed operations give higher quality samples compared to the original operations.



### PersonRank: Detecting Important People in Images
- **Arxiv ID**: http://arxiv.org/abs/1711.01984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.01984v1)
- **Published**: 2017-11-06 16:09:50+00:00
- **Updated**: 2017-11-06 16:09:50+00:00
- **Authors**: Wei-Hong Li, Benchao Li, Wei-Shi Zheng
- **Comment**: 8 pages, conference
- **Journal**: None
- **Summary**: Always, some individuals in images are more important/attractive than others in some events such as presentation, basketball game or speech. However, it is challenging to find important people among all individuals in images directly based on their spatial or appearance information due to the existence of diverse variations of pose, action, appearance of persons and various changes of occasions. We overcome this difficulty by constructing a multiple Hyper-Interaction Graph to treat each individual in an image as a node and inferring the most active node referring to interactions estimated by various types of clews. We model pairwise interactions between persons as the edge message communicated between nodes, resulting in a bidirectional pairwise-interaction graph. To enrich the personperson interaction estimation, we further introduce a unidirectional hyper-interaction graph that models the consensus of interaction between a focal person and any person in a local region around. Finally, we modify the PageRank algorithm to infer the activeness of persons on the multiple Hybrid-Interaction Graph (HIG), the union of the pairwise-interaction and hyperinteraction graphs, and we call our algorithm the PersonRank. In order to provide publicable datasets for evaluation, we have contributed a new dataset called Multi-scene Important People Image Dataset and gathered a NCAA Basketball Image Dataset from sports game sequences. We have demonstrated that the proposed PersonRank outperforms related methods clearly and substantially.



### Mitigating Adversarial Effects Through Randomization
- **Arxiv ID**: http://arxiv.org/abs/1711.01991v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.01991v3)
- **Published**: 2017-11-06 16:22:54+00:00
- **Updated**: 2018-02-28 22:39:15+00:00
- **Authors**: Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan Yuille
- **Comment**: To appear in ICLR 2018, code available at
  https://github.com/cihangxie/NIPS2017_adv_challenge_defense
- **Journal**: None
- **Summary**: Convolutional neural networks have demonstrated high accuracy on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. For example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail. In this paper, we propose to utilize randomization at inference time to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method provides the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at https://github.com/cihangxie/NIPS2017_adv_challenge_defense.



### Artificial Generation of Big Data for Improving Image Classification: A Generative Adversarial Network Approach on SAR Data
- **Arxiv ID**: http://arxiv.org/abs/1711.02010v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.02010v1)
- **Published**: 2017-11-06 16:52:51+00:00
- **Updated**: 2017-11-06 16:52:51+00:00
- **Authors**: Dimitrios Marmanis, Wei Yao, Fathalrahman Adam, Mihai Datcu, Peter Reinartz, Konrad Schindler, Jan Dirk Wegner, Uwe Stilla
- **Comment**: Submitted for review in "Big Data from Space 2017" conference
- **Journal**: None
- **Summary**: Very High Spatial Resolution (VHSR) large-scale SAR image databases are still an unresolved issue in the Remote Sensing field. In this work, we propose such a dataset and use it to explore patch-based classification in urban and periurban areas, considering 7 distinct semantic classes. In this context, we investigate the accuracy of large CNN classification models and pre-trained networks for SAR imaging systems. Furthermore, we propose a Generative Adversarial Network (GAN) for SAR image generation and test, whether the synthetic data can actually improve classification accuracy.



### NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm
- **Arxiv ID**: http://arxiv.org/abs/1711.02017v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.02017v3)
- **Published**: 2017-11-06 17:03:39+00:00
- **Updated**: 2018-06-01 04:04:09+00:00
- **Authors**: Xiaoliang Dai, Hongxu Yin, Niraj K. Jha
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have begun to have a pervasive impact on various applications of machine learning. However, the problem of finding an optimal DNN architecture for large applications is challenging. Common approaches go for deeper and larger DNN architectures but may incur substantial redundancy. To address these problems, we introduce a network growth algorithm that complements network pruning to learn both weights and compact DNN architectures during training. We propose a DNN synthesis tool (NeST) that combines both methods to automate the generation of compact and accurate DNNs. NeST starts with a randomly initialized sparse network called the seed architecture. It iteratively tunes the architecture with gradient-based growth and magnitude-based pruning of neurons and connections. Our experimental results show that NeST yields accurate, yet very compact DNNs, with a wide range of seed architecture selection. For the LeNet-300-100 (LeNet-5) architecture, we reduce network parameters by 70.2x (74.3x) and floating-point operations (FLOPs) by 79.4x (43.7x). For the AlexNet and VGG-16 architectures, we reduce network parameters (FLOPs) by 15.7x (4.6x) and 30.2x (8.6x), respectively. NeST's grow-and-prune paradigm delivers significant additional parameter and FLOPs reduction relative to pruning-only methods.



### Randomized Nonnegative Matrix Factorization
- **Arxiv ID**: http://arxiv.org/abs/1711.02037v2
- **DOI**: 10.1016/j.patrec.2018.01.007
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.02037v2)
- **Published**: 2017-11-06 17:42:47+00:00
- **Updated**: 2018-04-30 19:20:32+00:00
- **Authors**: N. Benjamin Erichson, Ariana Mendible, Sophie Wihlborn, J. Nathan Kutz
- **Comment**: This is an extended and revised version of the paper which appeared
  in JPRL
- **Journal**: Pattern Recognition Letters, Volume 104, 2018, Pages 1-7
- **Summary**: Nonnegative matrix factorization (NMF) is a powerful tool for data mining. However, the emergence of `big data' has severely challenged our ability to compute this fundamental decomposition using deterministic algorithms. This paper presents a randomized hierarchical alternating least squares (HALS) algorithm to compute the NMF. By deriving a smaller matrix from the nonnegative input data, a more efficient nonnegative decomposition can be computed. Our algorithm scales to big data applications while attaining a near-optimal factorization. The proposed algorithm is evaluated using synthetic and real world data and shows substantial speedups compared to deterministic HALS.



### End-to-end Lung Nodule Detection in Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/1711.02074v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.02074v2)
- **Published**: 2017-11-06 18:41:18+00:00
- **Updated**: 2018-10-02 18:15:45+00:00
- **Authors**: Dufan Wu, Kyungsang Kim, Bin Dong, Georges El Fakhri, Quanzheng Li
- **Comment**: published at MLMI 2018
- **Journal**: None
- **Summary**: Computer aided diagnostic (CAD) system is crucial for modern med-ical imaging. But almost all CAD systems operate on reconstructed images, which were optimized for radiologists. Computer vision can capture features that is subtle to human observers, so it is desirable to design a CAD system op-erating on the raw data. In this paper, we proposed a deep-neural-network-based detection system for lung nodule detection in computed tomography (CT). A primal-dual-type deep reconstruction network was applied first to convert the raw data to the image space, followed by a 3-dimensional convolutional neural network (3D-CNN) for the nodule detection. For efficient network training, the deep reconstruction network and the CNN detector was trained sequentially first, then followed by one epoch of end-to-end fine tuning. The method was evaluated on the Lung Image Database Consortium image collection (LIDC-IDRI) with simulated forward projections. With 144 multi-slice fanbeam pro-jections, the proposed end-to-end detector could achieve comparable sensitivity with the reference detector, which was trained and applied on the fully-sampled image data. It also demonstrated superior detection performance compared to detectors trained on the reconstructed images. The proposed method is general and could be expanded to most detection tasks in medical imaging.



### A General Neural Network Hardware Architecture on FPGA
- **Arxiv ID**: http://arxiv.org/abs/1711.05860v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.05860v1)
- **Published**: 2017-11-06 19:17:58+00:00
- **Updated**: 2017-11-06 19:17:58+00:00
- **Authors**: Yufeng Hao
- **Comment**: None
- **Journal**: None
- **Summary**: Field Programmable Gate Arrays (FPGAs) plays an increasingly important role in data sampling and processing industries due to its highly parallel architecture, low power consumption, and flexibility in custom algorithms. Especially, in the artificial intelligence field, for training and implement the neural networks and machine learning algorithms, high energy efficiency hardware implement and massively parallel computing capacity are heavily demanded. Therefore, many global companies have applied FPGAs into AI and Machine learning fields such as autonomous driving and Automatic Spoken Language Recognition (Baidu) [1] [2] and Bing search (Microsoft) [3]. Considering the FPGAs great potential in these fields, we tend to implement a general neural network hardware architecture on XILINX ZU9CG System On Chip (SOC) platform [4], which contains abundant hardware resource and powerful processing capacity. The general neural network architecture on the FPGA SOC platform can perform forward and backward algorithms in deep neural networks (DNN) with high performance and easily be adjusted according to the type and scale of the neural networks.



### Characterizing Sparse Connectivity Patterns in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.02131v5
- **DOI**: 10.1109/ITA.2018.8502950
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.02131v5)
- **Published**: 2017-11-06 19:26:21+00:00
- **Updated**: 2019-04-25 23:42:23+00:00
- **Authors**: Sourya Dey, Kuan-Wen Huang, Peter A. Beerel, Keith M. Chugg
- **Comment**: Presented at the 2018 Information Theory and Applications Workshop,
  San Diego, California
- **Journal**: in 2018 Information Theory and Applications Workshop (ITA), pp.
  1--8, Feb 2018
- **Summary**: We propose a novel way of reducing the number of parameters in the storage-hungry fully connected layers of a neural network by using pre-defined sparsity, where the majority of connections are absent prior to starting training. Our results indicate that convolutional neural networks can operate without any loss of accuracy at less than half percent classification layer connection density, or less than 5 percent overall network connection density. We also investigate the effects of pre-defining the sparsity of networks with only fully connected layers. Based on our sparsifying technique, we introduce the `scatter' metric to characterize the quality of a particular connection pattern. As proof of concept, we show results on CIFAR, MNIST and a new dataset on classifying Morse code symbols, which highlights some interesting trends and limits of sparse connection patterns.



### A Joint 3D-2D based Method for Free Space Detection on Roads
- **Arxiv ID**: http://arxiv.org/abs/1711.02144v3
- **DOI**: 10.1109/WACV.2018.00076
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.02144v3)
- **Published**: 2017-11-06 20:04:20+00:00
- **Updated**: 2018-01-15 20:22:10+00:00
- **Authors**: Suvam Patra, Pranjal Maheshwari, Shashank Yadav, Chetan Arora, Subhashis Banerjee
- **Comment**: Accepted for publication at IEEE WACV 2018
- **Journal**: None
- **Summary**: In this paper, we address the problem of road segmentation and free space detection in the context of autonomous driving. Traditional methods either use 3-dimensional (3D) cues such as point clouds obtained from LIDAR, RADAR or stereo cameras or 2-dimensional (2D) cues such as lane markings, road boundaries and object detection. Typical 3D point clouds do not have enough resolution to detect fine differences in heights such as between road and pavement. Image based 2D cues fail when encountering uneven road textures such as due to shadows, potholes, lane markings or road restoration. We propose a novel free road space detection technique combining both 2D and 3D cues. In particular, we use CNN based road segmentation from 2D images and plane/box fitting on sparse depth data obtained from SLAM as priors to formulate an energy minimization using conditional random field (CRF), for road pixels classification. While the CNN learns the road texture and is unaffected by depth boundaries, the 3D information helps in overcoming texture based classification failures. Finally, we use the obtained road segmentation with the 3D depth data from monocular SLAM to detect the free space for the navigation purposes. Our experiments on KITTI odometry dataset, Camvid dataset, as well as videos captured by us, validate the superiority of the proposed approach over the state of the art.



### Image Segmentation of Multi-Shaped Overlapping Objects
- **Arxiv ID**: http://arxiv.org/abs/1711.02217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.02217v1)
- **Published**: 2017-11-06 23:14:46+00:00
- **Updated**: 2017-11-06 23:14:46+00:00
- **Authors**: Kumar Abhinav, Jaideep Singh Chauhan, Debasis Sarkar
- **Comment**: Accepted at VISAPP 2018
- **Journal**: None
- **Summary**: In this work, we propose a new segmentation algorithm for images containing convex objects present in multiple shapes with a high degree of overlap. The proposed algorithm is carried out in two steps, first we identify the visible contours, segment them using concave points and finally group the segments belonging to the same object. The next step is to assign a shape identity to these grouped contour segments. For images containing objects in multiple shapes we begin first by identifying shape classes of the contours followed by assigning a shape entity to these classes. We provide a comprehensive experimentation of our algorithm on two crystal image datasets. One dataset comprises of images containing objects in multiple shapes overlapping each other and the other dataset contains standard images with objects present in a single shape. We test our algorithm against two baselines, with our proposed algorithm outperforming both the baselines.



