# Arxiv Papers in cs.CV on 2017-11-22
### Integrating both Visual and Audio Cues for Enhanced Video Caption
- **Arxiv ID**: http://arxiv.org/abs/1711.08097v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08097v2)
- **Published**: 2017-11-22 01:12:00+00:00
- **Updated**: 2017-12-09 04:03:21+00:00
- **Authors**: Wangli Hao, Zhaoxiang Zhang, He Guan, Guibo Zhu
- **Comment**: Have some problems need to be handled
- **Journal**: None
- **Summary**: Video caption refers to generating a descriptive sentence for a specific short video clip automatically, which has achieved remarkable success recently. However, most of the existing methods focus more on visual information while ignoring the synchronized audio cues. We propose three multimodal deep fusion strategies to maximize the benefits of visual-audio resonance information. The first one explores the impact on cross-modalities feature fusion from low to high order. The second establishes the visual-audio short-term dependency by sharing weights of corresponding front-end networks. The third extends the temporal dependency to long-term through sharing multimodal memory across visual and audio modalities. Extensive experiments have validated the effectiveness of our three cross-modalities fusion strategies on two benchmark datasets, including Microsoft Research Video to Text (MSRVTT) and Microsoft Video Description (MSVD). It is worth mentioning that sharing weight can coordinate visual-audio feature fusion effectively and achieve the state-of-art performance on both BELU and METEOR metrics. Furthermore, we first propose a dynamic multimodal feature fusion framework to deal with the part modalities missing case. Experimental results demonstrate that even in the audio absence mode, we can still obtain comparable results with the aid of the additional audio modality inference module.



### CMCGAN: A Uniform Framework for Cross-Modal Visual-Audio Mutual Generation
- **Arxiv ID**: http://arxiv.org/abs/1711.08102v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08102v2)
- **Published**: 2017-11-22 01:36:20+00:00
- **Updated**: 2017-12-09 04:01:40+00:00
- **Authors**: Wangli Hao, Zhaoxiang Zhang, He Guan
- **Comment**: Have some problems need to be handled
- **Journal**: None
- **Summary**: Visual and audio modalities are two symbiotic modalities underlying videos, which contain both common and complementary information. If they can be mined and fused sufficiently, performances of related video tasks can be significantly enhanced. However, due to the environmental interference or sensor fault, sometimes, only one modality exists while the other is abandoned or missing. By recovering the missing modality from the existing one based on the common information shared between them and the prior information of the specific modality, great bonus will be gained for various vision tasks. In this paper, we propose a Cross-Modal Cycle Generative Adversarial Network (CMCGAN) to handle cross-modal visual-audio mutual generation. Specifically, CMCGAN is composed of four kinds of subnetworks: audio-to-visual, visual-to-audio, audio-to-audio and visual-to-visual subnetworks respectively, which are organized in a cycle architecture. CMCGAN has several remarkable advantages. Firstly, CMCGAN unifies visual-audio mutual generation into a common framework by a joint corresponding adversarial loss. Secondly, through introducing a latent vector with Gaussian distribution, CMCGAN can handle dimension and structure asymmetry over visual and audio modalities effectively. Thirdly, CMCGAN can be trained end-to-end to achieve better convenience. Benefiting from CMCGAN, we develop a dynamic multimodal classification network to handle the modality missing problem. Abundant experiments have been conducted and validate that CMCGAN obtains the state-of-the-art cross-modal visual-audio generation results. Furthermore, it is shown that the generated modality achieves comparable effects with those of original modality, which demonstrates the effectiveness and advantages of our proposed method.



### Visual Question Answering as a Meta Learning Task
- **Arxiv ID**: http://arxiv.org/abs/1711.08105v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08105v1)
- **Published**: 2017-11-22 02:04:31+00:00
- **Updated**: 2017-11-22 02:04:31+00:00
- **Authors**: Damien Teney, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: The predominant approach to Visual Question Answering (VQA) demands that the model represents within its weights all of the information required to answer any question about any image. Learning this information from any real training set seems unlikely, and representing it in a reasonable number of weights doubly so. We propose instead to approach VQA as a meta learning task, thus separating the question answering method from the information required. At test time, the method is provided with a support set of example questions/answers, over which it reasons to resolve the given question. The support set is not fixed and can be extended without retraining, thereby expanding the capabilities of the model. To exploit this dynamically provided information, we adapt a state-of-the-art VQA model with two techniques from the recent meta learning literature, namely prototypical networks and meta networks. Experiments demonstrate the capability of the system to learn to produce completely novel answers (i.e. never seen during training) from examples provided at test time. In comparison to the existing state of the art, the proposed method produces qualitatively distinct results with higher recall of rare answers, and a better sample efficiency that allows training with little initial data. More importantly, it represents an important step towards vision-and-language methods that can learn and reason on-the-fly.



### The Devil is in the Middle: Exploiting Mid-level Representations for Cross-Domain Instance Matching
- **Arxiv ID**: http://arxiv.org/abs/1711.08106v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08106v2)
- **Published**: 2017-11-22 02:10:03+00:00
- **Updated**: 2018-04-04 20:12:42+00:00
- **Authors**: Qian Yu, Xiaobin Chang, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales
- **Comment**: Reference updated
- **Journal**: None
- **Summary**: Many vision problems require matching images of object instances across different domains. These include fine-grained sketch-based image retrieval (FG-SBIR) and Person Re-identification (person ReID). Existing approaches attempt to learn a joint embedding space where images from different domains can be directly compared. In most cases, this space is defined by the output of the final layer of a deep neural network (DNN), which primarily contains features of a high semantic level. In this paper, we argue that both high and mid-level features are relevant for cross-domain instance matching (CDIM). Importantly, mid-level features already exist in earlier layers of the DNN. They just need to be extracted, represented, and fused properly with the final layer. Based on this simple but powerful idea, we propose a unified framework for CDIM. Instantiating our framework for FG-SBIR and ReID, we show that our simple models can easily beat the state-of-the-art models, which are often equipped with much more elaborate architectures.



### Dilated FCN for Multi-Agent 2D/3D Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1712.01651v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1712.01651v1)
- **Published**: 2017-11-22 03:22:17+00:00
- **Updated**: 2017-11-22 03:22:17+00:00
- **Authors**: Shun Miao, Sebastien Piat, Peter Fischer, Ahmet Tuysuzoglu, Philip Mewes, Tommaso Mansi, Rui Liao
- **Comment**: AAAI 2018
- **Journal**: None
- **Summary**: 2D/3D image registration to align a 3D volume and 2D X-ray images is a challenging problem due to its ill-posed nature and various artifacts presented in 2D X-ray images. In this paper, we propose a multi-agent system with an auto attention mechanism for robust and efficient 2D/3D image registration. Specifically, an individual agent is trained with dilated Fully Convolutional Network (FCN) to perform registration in a Markov Decision Process (MDP) by observing a local region, and the final action is then taken based on the proposals from multiple agents and weighted by their corresponding confidence levels. The contributions of this paper are threefold. First, we formulate 2D/3D registration as a MDP with observations, actions, and rewards properly defined with respect to X-ray imaging systems. Second, to handle various artifacts in 2D X-ray images, multiple local agents are employed efficiently via FCN-based structures, and an auto attention mechanism is proposed to favor the proposals from regions with more reliable visual cues. Third, a dilated FCN-based training mechanism is proposed to significantly reduce the Degree of Freedom in the simulation of registration environment, and drastically improve training efficiency by an order of magnitude compared to standard CNN-based training method. We demonstrate that the proposed method achieves high robustness on both spine cone beam Computed Tomography data with a low signal-to-noise ratio and data from minimally invasive spine surgery where severe image artifacts and occlusions are presented due to metal screws and guide wires, outperforming other state-of-the-art methods (single agent-based and optimization-based) by a large margin.



### Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1711.08141v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08141v2)
- **Published**: 2017-11-22 05:52:19+00:00
- **Updated**: 2017-12-03 07:07:21+00:00
- **Authors**: Bichen Wu, Alvin Wan, Xiangyu Yue, Peter Jin, Sicheng Zhao, Noah Golmant, Amir Gholaminejad, Joseph Gonzalez, Kurt Keutzer
- **Comment**: Source code will be released afterwards
- **Journal**: None
- **Summary**: Neural networks rely on convolutions to aggregate spatial information. However, spatial convolutions are expensive in terms of model size and computation, both of which grow quadratically with respect to kernel size. In this paper, we present a parameter-free, FLOP-free "shift" operation as an alternative to spatial convolutions. We fuse shifts and point-wise convolutions to construct end-to-end trainable shift-based modules, with a hyperparameter characterizing the tradeoff between accuracy and efficiency. To demonstrate the operation's efficacy, we replace ResNet's 3x3 convolutions with shift-based modules for improved CIFAR10 and CIFAR100 accuracy using 60% fewer parameters; we additionally demonstrate the operation's resilience to parameter reduction on ImageNet, outperforming ResNet family members. We finally show the shift operation's applicability across domains, achieving strong performance with fewer parameters on classification, face verification and style transfer.



### A Face Fairness Framework for 3D Meshes
- **Arxiv ID**: http://arxiv.org/abs/1711.08155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08155v1)
- **Published**: 2017-11-22 07:22:08+00:00
- **Updated**: 2017-11-22 07:22:08+00:00
- **Authors**: Sk. Mohammadul Haque, Venu Madhav Govindu
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: In this paper, we present a face fairness framework for 3D meshes that preserves the regular shape of faces and is applicable to a variety of 3D mesh restoration tasks. Specifically, we present a number of desirable properties for any mesh restoration method and show that our framework satisfies them. We then apply our framework to two different tasks --- mesh-denoising and mesh-refinement, and present comparative results for these two tasks showing improvement over other relevant methods in the literature.



### Weakly Supervised Object Discovery by Generative Adversarial & Ranking Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.08174v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08174v2)
- **Published**: 2017-11-22 08:36:39+00:00
- **Updated**: 2018-04-17 15:06:04+00:00
- **Authors**: Ali Diba, Vivek Sharma, Rainer Stiefelhagen, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: The deep generative adversarial networks (GAN) recently have been shown to be promising for different computer vision applications, like image edit- ing, synthesizing high resolution images, generating videos, etc. These networks and the corresponding learning scheme can handle various visual space map- pings. We approach GANs with a novel training method and learning objective, to discover multiple object instances for three cases: 1) synthesizing a picture of a specific object within a cluttered scene; 2) localizing different categories in images for weakly supervised object detection; and 3) improving object discov- ery in object detection pipelines. A crucial advantage of our method is that it learns a new deep similarity metric, to distinguish multiple objects in one im- age. We demonstrate that the network can act as an encoder-decoder generating parts of an image which contain an object, or as a modified deep CNN to rep- resent images for object detection in supervised and weakly supervised scheme. Our ranking GAN offers a novel way to search through images for object specific patterns. We have conducted experiments for different scenarios and demonstrate the method performance for object synthesizing and weakly supervised object detection and classification using the MS-COCO and PASCAL VOC datasets.



### Video Semantic Object Segmentation by Self-Adaptation of DCNN
- **Arxiv ID**: http://arxiv.org/abs/1711.08180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08180v1)
- **Published**: 2017-11-22 09:08:03+00:00
- **Updated**: 2017-11-22 09:08:03+00:00
- **Authors**: Seong-Jin Park, Ki-Sang Hong
- **Comment**: Submitted to Pattern Recognition Letters on August 2016
- **Journal**: None
- **Summary**: This paper proposes a new framework for semantic segmentation of objects in videos. We address the label inconsistency problem of deep convolutional neural networks (DCNNs) by exploiting the fact that videos have multiple frames; in a few frames the object is confidently-estimated (CE) and we use the information in them to improve labels of the other frames. Given the semantic segmentation results of each frame obtained from DCNN, we sample several CE frames to adapt the DCNN model to the input video by focusing on specific instances in the video rather than general objects in various circumstances. We propose offline and online approaches under different supervision levels. In experiments our method achieved great improvement over the original model and previous state-of-the-art methods.



### AlignedReID: Surpassing Human-Level Performance in Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1711.08184v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08184v2)
- **Published**: 2017-11-22 09:15:22+00:00
- **Updated**: 2018-01-31 11:08:29+00:00
- **Authors**: Xuan Zhang, Hao Luo, Xing Fan, Weilai Xiang, Yixiao Sun, Qiqi Xiao, Wei Jiang, Chi Zhang, Jian Sun
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: In this paper, we propose a novel method called AlignedReID that extracts a global feature which is jointly learned with local features. Global feature learning benefits greatly from local feature learning, which performs an alignment/matching by calculating the shortest path between two sets of local features, without requiring extra supervision. After the joint learning, we only keep the global feature to compute the similarities between images. Our method achieves rank-1 accuracy of 94.4% on Market1501 and 97.8% on CUHK03, outperforming state-of-the-art methods by a large margin. We also evaluate human-level performance and demonstrate that our method is the first to surpass human-level performance on Market1501 and CUHK03, two widely used Person ReID datasets.



### An Analysis of Scale Invariance in Object Detection - SNIP
- **Arxiv ID**: http://arxiv.org/abs/1711.08189v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08189v2)
- **Published**: 2017-11-22 09:30:06+00:00
- **Updated**: 2018-05-25 12:47:23+00:00
- **Authors**: Bharat Singh, Larry S. Davis
- **Comment**: CVPR 2018, camera ready version
- **Journal**: None
- **Summary**: An analysis of different techniques for recognizing and detecting objects under extreme scale variation is presented. Scale specific and scale invariant design of detectors are compared by training them with different configurations of input data. By evaluating the performance of different network architectures for classifying small objects on ImageNet, we show that CNNs are not robust to changes in scale. Based on this analysis, we propose to train and test detectors on the same scales of an image-pyramid. Since small and large objects are difficult to recognize at smaller and larger scales respectively, we present a novel training scheme called Scale Normalization for Image Pyramids (SNIP) which selectively back-propagates the gradients of object instances of different sizes as a function of the image scale. On the COCO dataset, our single model performance is 45.7% and an ensemble of 3 networks obtains an mAP of 48.3%. We use off-the-shelf ImageNet-1000 pre-trained models and only train with bounding box supervision. Our submission won the Best Student Entry in the COCO 2017 challenge. Code will be made available at \url{http://bit.ly/2yXVg4c}.



### On the Automatic Generation of Medical Imaging Reports
- **Arxiv ID**: http://arxiv.org/abs/1711.08195v3
- **DOI**: 10.18653/v1/P18-1240
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.08195v3)
- **Published**: 2017-11-22 09:45:51+00:00
- **Updated**: 2018-07-20 17:45:14+00:00
- **Authors**: Baoyu Jing, Pengtao Xie, Eric Xing
- **Comment**: ACL 2018
- **Journal**: None
- **Summary**: Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and time- consuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the re- ports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi-task learning framework which jointly performs the pre- diction of tags and the generation of para- graphs, (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available datasets.



### Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1711.08200v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08200v1)
- **Published**: 2017-11-22 10:01:37+00:00
- **Updated**: 2017-11-22 10:01:37+00:00
- **Authors**: Ali Diba, Mohsen Fayyaz, Vivek Sharma, Amir Hossein Karami, Mohammad Mahdi Arzani, Rahman Yousefzadeh, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: The work in this paper is driven by the question how to exploit the temporal cues available in videos for their accurate classification, and for human action recognition in particular? Thus far, the vision community has focused on spatio-temporal approaches with fixed temporal convolution kernel depths. We introduce a new temporal layer that models variable temporal convolution kernel depths. We embed this new temporal layer in our proposed 3D CNN. We extend the DenseNet architecture - which normally is 2D - with 3D filters and pooling kernels. We name our proposed video convolutional network `Temporal 3D ConvNet'~(T3D) and its new temporal layer `Temporal Transition Layer'~(TTL). Our experiments show that T3D outperforms the current state-of-the-art methods on the HMDB51, UCF101 and Kinetics datasets.   The other issue in training 3D ConvNets is about training them from scratch with a huge labeled dataset to get a reasonable performance. So the knowledge learned in 2D ConvNets is completely ignored. Another contribution in this work is a simple and effective technique to transfer knowledge from a pre-trained 2D CNN to a randomly initialized 3D CNN for a stable weight initialization. This allows us to significantly reduce the number of training samples for 3D CNNs. Thus, by finetuning this network, we beat the performance of generic and recent methods in 3D CNNs, which were trained on large video datasets, e.g. Sports-1M, and finetuned on the target datasets, e.g. HMDB51/UCF101. The T3D codes will be released



### Integral Human Pose Regression
- **Arxiv ID**: http://arxiv.org/abs/1711.08229v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08229v4)
- **Published**: 2017-11-22 11:15:06+00:00
- **Updated**: 2018-09-18 08:29:46+00:00
- **Authors**: Xiao Sun, Bin Xiao, Fangyin Wei, Shuang Liang, Yichen Wei
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art human pose estimation methods are based on heat map representation. In spite of the good performance, the representation has a few issues in nature, such as not differentiable and quantization error. This work shows that a simple integral operation relates and unifies the heat map representation and joint regression, thus avoiding the above issues. It is differentiable, efficient, and compatible with any heat map based methods. Its effectiveness is convincingly validated via comprehensive ablation experiments under various settings, specifically on 3D pose estimation, for the first time.



### Multi-Level Recurrent Residual Networks for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.08238v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08238v6)
- **Published**: 2017-11-22 11:40:29+00:00
- **Updated**: 2018-01-03 09:20:09+00:00
- **Authors**: Zhenxing Zheng, Gaoyun An, Qiuqi Ruan
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing Convolutional Neural Networks(CNNs) used for action recognition are either difficult to optimize or underuse crucial temporal information. Inspired by the fact that the recurrent model consistently makes breakthroughs in the task related to sequence, we propose a novel Multi-Level Recurrent Residual Networks(MRRN) which incorporates three recognition streams. Each stream consists of a Residual Networks(ResNets) and a recurrent model. The proposed model captures spatiotemporal information by employing both alternative ResNets to learn spatial representations from static frames and stacked Simple Recurrent Units(SRUs) to model temporal dynamics. Three distinct-level streams learned low-, mid-, high-level representations independently are fused by computing a weighted average of their softmax scores to obtain the complementary representations of the video. Unlike previous models which boost performance at the cost of time complexity and space complexity, our models have a lower complexity by employing shortcut connection and are trained end-to-end with greater efficiency. MRRN displays significant performance improvements compared to CNN-RNN framework baselines and obtains comparable performance with the state-of-the-art, achieving 51.3% on HMDB-51 dataset and 81.9% on UCF-101 dataset although no additional data.



### 3D Point Cloud Classification and Segmentation using 3D Modified Fisher Vector Representation for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.08241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08241v1)
- **Published**: 2017-11-22 11:47:42+00:00
- **Updated**: 2017-11-22 11:47:42+00:00
- **Authors**: Yizhak Ben-Shabat, Michael Lindenbaum, Anath Fischer
- **Comment**: None
- **Journal**: None
- **Summary**: The point cloud is gaining prominence as a method for representing 3D shapes, but its irregular format poses a challenge for deep learning methods. The common solution of transforming the data into a 3D voxel grid introduces its own challenges, mainly large memory size. In this paper we propose a novel 3D point cloud representation called 3D Modified Fisher Vectors (3DmFV). Our representation is hybrid as it combines the discrete structure of a grid with continuous generalization of Fisher vectors, in a compact and computationally efficient way. Using the grid enables us to design a new CNN architecture for point cloud classification and part segmentation. In a series of experiments we demonstrate competitive performance or even better than state-of-the-art on challenging benchmark datasets.



### TexT - Text Extractor Tool for Handwritten Document Transcription and Annotation
- **Arxiv ID**: http://arxiv.org/abs/1801.05367v1
- **DOI**: 10.1007/978-3-319-73165-0_8
- **Categories**: **cs.DL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.05367v1)
- **Published**: 2017-11-22 12:05:33+00:00
- **Updated**: 2017-11-22 12:05:33+00:00
- **Authors**: Anders Hast, Per Cullhed, Ekta Vats
- **Comment**: None
- **Journal**: Digital Libraries and Multimedia Archives. IRCDL 2018.
  Communications in Computer and Information Science, vol 806. Springer, Cham
- **Summary**: This paper presents a framework for semi-automatic transcription of large-scale historical handwritten documents and proposes a simple user-friendly text extractor tool, TexT for transcription. The proposed approach provides a quick and easy transcription of text using computer assisted interactive technique. The algorithm finds multiple occurrences of the marked text on-the-fly using a word spotting system. TexT is also capable of performing on-the-fly annotation of handwritten text with automatic generation of ground truth labels, and dynamic adjustment and correction of user generated bounding box annotations with the word being perfectly encapsulated. The user can view the document and the found words in the original form or with background noise removed for easier visualization of transcription results. The effectiveness of TexT is demonstrated on an archival manuscript collection from well-known publicly available dataset.



### Few-shot Learning by Exploiting Visual Concepts within CNNs
- **Arxiv ID**: http://arxiv.org/abs/1711.08277v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.08277v3)
- **Published**: 2017-11-22 13:44:44+00:00
- **Updated**: 2018-02-13 12:30:09+00:00
- **Authors**: Boyang Deng, Qing Liu, Siyuan Qiao, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are one of the driving forces for the advancement of computer vision. Despite their promising performances on many tasks, CNNs still face major obstacles on the road to achieving ideal machine intelligence. One is that CNNs are complex and hard to interpret. Another is that standard CNNs require large amounts of annotated data, which is sometimes hard to obtain, and it is desirable to learn to recognize objects from few examples. In this work, we address these limitations of CNNs by developing novel, flexible, and interpretable models for few-shot learning. Our models are based on the idea of encoding objects in terms of visual concepts (VCs), which are interpretable visual cues represented by the feature vectors within CNNs. We first adapt the learning of VCs to the few-shot setting, and then uncover two key properties of feature encoding using VCs, which we call category sensitivity and spatial pattern. Motivated by these properties, we present two intuitive models for the problem of few-shot learning. Experiments show that our models achieve competitive performances, while being more flexible and interpretable than alternative state-of-the-art few-shot learning methods. We conclude that using VCs helps expose the natural capability of CNNs for few-shot learning.



### Neuron-level Selective Context Aggregation for Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.08278v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08278v1)
- **Published**: 2017-11-22 13:45:06+00:00
- **Updated**: 2017-11-22 13:45:06+00:00
- **Authors**: Zhenhua Wang, Fanglin Gu, Dani Lischinski, Daniel Cohen-Or, Changhe Tu, Baoquan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Contextual information provides important cues for disambiguating visually similar pixels in scene segmentation. In this paper, we introduce a neuron-level Selective Context Aggregation (SCA) module for scene segmentation, comprised of a contextual dependency predictor and a context aggregation operator. The dependency predictor is implicitly trained to infer contextual dependencies between different image regions. The context aggregation operator augments local representations with global context, which is aggregated selectively at each neuron according to its on-the-fly predicted dependencies. The proposed mechanism enables data-driven inference of contextual dependencies, and facilitates context-aware feature learning. The proposed method improves strong baselines built upon VGG16 on challenging scene segmentation datasets, which demonstrates its effectiveness in modeling context information.



### Three-Stream Convolutional Networks for Video-based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1712.01652v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.01652v1)
- **Published**: 2017-11-22 15:05:58+00:00
- **Updated**: 2017-11-22 15:05:58+00:00
- **Authors**: Zeng Yu, Tianrui Li, Ning Yu, Xun Gong, Ke Chen, Yi Pan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims to develop a new architecture that can make full use of the feature maps of convolutional networks. To this end, we study a number of methods for video-based person re-identification and make the following findings: 1) Max-pooling only focuses on the maximum value of a receptive field, wasting a lot of information. 2) Networks with different streams even including the one with the worst performance work better than networks with same streams, where each one has the best performance alone. 3) A full connection layer at the end of convolutional networks is not necessary. Based on these studies, we propose a new convolutional architecture termed Three-Stream Convolutional Networks (TSCN). It first uses different streams to learn different aspects of feature maps for attentive spatio-temporal fusion of video, and then merges them together to study some union features. To further utilize the feature maps, two architectures are designed by using the strategies of multi-scale and upsampling. Comparative experiments on iLIDS-VID, PRID-2011 and MARS datasets illustrate that the proposed architectures are significantly better for feature extraction than the state-of-the-art models.



### Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky Noisy-or Network
- **Arxiv ID**: http://arxiv.org/abs/1711.08324v1
- **DOI**: 10.1109/TNNLS.2019.2892409
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08324v1)
- **Published**: 2017-11-22 15:14:10+00:00
- **Updated**: 2017-11-22 15:14:10+00:00
- **Authors**: Fangzhou Liao, Ming Liang, Zhe Li, Xiaolin Hu, Sen Song
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: Automatic diagnosing lung cancer from Computed Tomography (CT) scans involves two steps: detect all suspicious lesions (pulmonary nodules) and evaluate the whole-lung/pulmonary malignancy. Currently, there are many studies about the first step, but few about the second step. Since the existence of nodule does not definitely indicate cancer, and the morphology of nodule has a complicated relationship with cancer, the diagnosis of lung cancer demands careful investigations on every suspicious nodule and integration of information of all nodules. We propose a 3D deep neural network to solve this problem. The model consists of two modules. The first one is a 3D region proposal network for nodule detection, which outputs all suspicious nodules for a subject. The second one selects the top five nodules based on the detection confidence, evaluates their cancer probabilities and combines them with a leaky noisy-or gate to obtain the probability of lung cancer for the subject. The two modules share the same backbone network, a modified U-net. The over-fitting caused by the shortage of training data is alleviated by training the two modules alternately. The proposed model won the first place in the Data Science Bowl 2017 competition. The code has been made publicly available.



### ForestHash: Semantic Hashing With Shallow Random Forests and Tiny Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.08364v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.08364v2)
- **Published**: 2017-11-22 16:16:42+00:00
- **Updated**: 2018-07-28 00:06:37+00:00
- **Authors**: Qiang Qiu, Jose Lezama, Alex Bronstein, Guillermo Sapiro
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Hash codes are efficient data representations for coping with the ever growing amounts of data. In this paper, we introduce a random forest semantic hashing scheme that embeds tiny convolutional neural networks (CNN) into shallow random forests, with near-optimal information-theoretic code aggregation among trees. We start with a simple hashing scheme, where random trees in a forest act as hashing functions by setting `1' for the visited tree leaf, and `0' for the rest. We show that traditional random forests fail to generate hashes that preserve the underlying similarity between the trees, rendering the random forests approach to hashing challenging. To address this, we propose to first randomly group arriving classes at each tree split node into two groups, obtaining a significantly simplified two-class classification problem, which can be handled using a light-weight CNN weak learner. Such random class grouping scheme enables code uniqueness by enforcing each class to share its code with different classes in different trees. A non-conventional low-rank loss is further adopted for the CNN weak learners to encourage code consistency by minimizing intra-class variations and maximizing inter-class distance for the two random class groups. Finally, we introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code, producing a near-optimal unique hash for each class. The proposed approach significantly outperforms state-of-the-art hashing methods for image retrieval tasks on large-scale public datasets, while performing at the level of other state-of-the-art image classification techniques while utilizing a more compact and efficient scalable representation. This work proposes a principled and robust procedure to train and deploy in parallel an ensemble of light-weight CNNs, instead of simply going deeper.



### Conditional Image-Text Embedding Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.08389v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08389v4)
- **Published**: 2017-11-22 16:58:31+00:00
- **Updated**: 2018-07-28 16:15:53+00:00
- **Authors**: Bryan A. Plummer, Paige Kordas, M. Hadi Kiapour, Shuai Zheng, Robinson Piramuthu, Svetlana Lazebnik
- **Comment**: ECCV 2018 accepted paper
- **Journal**: None
- **Summary**: This paper presents an approach for grounding phrases in images which jointly learns multiple text-conditioned embeddings in a single end-to-end model. In order to differentiate text phrases into semantically distinct subspaces, we propose a concept weight branch that automatically assigns phrases to embeddings, whereas prior works predefine such assignments. Our proposed solution simplifies the representation requirements for individual embeddings and allows the underrepresented concepts to take advantage of the shared representations before feeding them into concept-specific layers. Comprehensive experiments verify the effectiveness of our approach across three phrase grounding datasets, Flickr30K Entities, ReferIt Game, and Visual Genome, where we obtain a (resp.) 4%, 3%, and 4% improvement in grounding performance over a strong region-phrase embedding baseline.



### BlockDrop: Dynamic Inference Paths in Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.08393v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.08393v4)
- **Published**: 2017-11-22 17:01:59+00:00
- **Updated**: 2019-01-28 16:36:44+00:00
- **Authors**: Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S. Davis, Kristen Grauman, Rogerio Feris
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications. We introduce BlockDrop, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy. Exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image. In particular, given a pretrained ResNet, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy. We conduct extensive experiments on CIFAR and ImageNet. The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information. Built upon a ResNet-101 model, our method achieves a speedup of 20\% on average, going as high as 36\% for some images, while maintaining the same 76.4\% top-1 accuracy on ImageNet.



### SolarisNet: A Deep Regression Network for Solar Radiation Prediction
- **Arxiv ID**: http://arxiv.org/abs/1711.08413v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.08413v2)
- **Published**: 2017-11-22 17:41:40+00:00
- **Updated**: 2017-12-10 18:56:29+00:00
- **Authors**: Subhadip Dey, Sawon Pratiher, Saon Banerjee, Chanchal Kumar Mukherjee
- **Comment**: None
- **Journal**: None
- **Summary**: Effective utilization of photovoltaic (PV) plants requires weather variability robust global solar radiation (GSR) forecasting models. Random weather turbulence phenomena coupled with assumptions of clear sky model as suggested by Hottel pose significant challenges to parametric & non-parametric models in GSR conversion rate estimation. Also, a decent GSR estimate requires costly high-tech radiometer and expert dependent instrument handling and measurements, which are subjective. As such, a computer aided monitoring (CAM) system to evaluate PV plant operation feasibility by employing smart grid past data analytics and deep learning is developed. Our algorithm, SolarisNet is a 6-layer deep neural network trained on data collected at two weather stations located near Kalyani metrological site, West Bengal, India. The daily GSR prediction performance using SolarisNet outperforms the existing state of art and its efficacy in inferring past GSR data insights to comprehend daily and seasonal GSR variability along with its competence for short term forecasting is discussed.



### VITON: An Image-based Virtual Try-on Network
- **Arxiv ID**: http://arxiv.org/abs/1711.08447v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08447v4)
- **Published**: 2017-11-22 18:48:54+00:00
- **Updated**: 2018-06-12 21:57:44+00:00
- **Authors**: Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: We present an image-based VIirtual Try-On Network (VITON) without using 3D information in any form, which seamlessly transfers a desired clothing item onto the corresponding region of a person using a coarse-to-fine strategy. Conditioned upon a new clothing-agnostic yet descriptive person representation, our framework first generates a coarse synthesized image with the target clothing item overlaid on that same person in the same pose. We further enhance the initial blurry clothing area with a refinement network. The network is trained to learn how much detail to utilize from the target clothing item, and where to apply to the person in order to synthesize a photo-realistic image in which the target item deforms naturally with clear visual patterns. Experiments on our newly collected Zalando dataset demonstrate its promise in the image-based virtual try-on task over state-of-the-art generative models.



### Frustum PointNets for 3D Object Detection from RGB-D Data
- **Arxiv ID**: http://arxiv.org/abs/1711.08488v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08488v2)
- **Published**: 2017-11-22 19:52:18+00:00
- **Updated**: 2018-04-13 00:30:24+00:00
- **Authors**: Charles R. Qi, Wei Liu, Chenxia Wu, Hao Su, Leonidas J. Guibas
- **Comment**: 15 pages, 12 figures, 14 tables
- **Journal**: None
- **Summary**: In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.



### Learning Deep Representations of Medical Images using Siamese CNNs with Application to Content-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1711.08490v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08490v2)
- **Published**: 2017-11-22 20:04:01+00:00
- **Updated**: 2017-12-27 20:19:26+00:00
- **Authors**: Yu-An Chung, Wei-Hung Weng
- **Comment**: Presented in NIPS 2017 Workshop on Machine Learning for Health
  (ML4H); add retrieval results; fix typos
- **Journal**: None
- **Summary**: Deep neural networks have been investigated in learning latent representations of medical images, yet most of the studies limit their approach in a single supervised convolutional neural network (CNN), which usually rely heavily on a large scale annotated dataset for training. To learn image representations with less supervision involved, we propose a deep Siamese CNN (SCNN) architecture that can be trained with only binary image pair information. We evaluated the learned image representations on a task of content-based medical image retrieval using a publicly available multiclass diabetic retinopathy fundus image dataset. The experimental results show that our proposed deep SCNN is comparable to the state-of-the-art single supervised CNN, and requires much less supervision for training.



### Temporal Relational Reasoning in Videos
- **Arxiv ID**: http://arxiv.org/abs/1711.08496v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08496v2)
- **Published**: 2017-11-22 20:31:19+00:00
- **Updated**: 2018-07-25 03:03:32+00:00
- **Authors**: Bolei Zhou, Alex Andonian, Aude Oliva, Antonio Torralba
- **Comment**: camera-ready version for ECCV'18
- **Journal**: None
- **Summary**: Temporal relational reasoning, the ability to link meaningful transformations of objects or entities over time, is a fundamental property of intelligent species. In this paper, we introduce an effective and interpretable network module, the Temporal Relation Network (TRN), designed to learn and reason about temporal dependencies between video frames at multiple time scales. We evaluate TRN-equipped networks on activity recognition tasks using three recent video datasets - Something-Something, Jester, and Charades - which fundamentally depend on temporal relational reasoning. Our results demonstrate that the proposed TRN gives convolutional neural networks a remarkable capacity to discover temporal relations in videos. Through only sparsely sampled video frames, TRN-equipped networks can accurately predict human-object interactions in the Something-Something dataset and identify various human gestures on the Jester dataset with very competitive performance. TRN-equipped networks also outperform two-stream networks and 3D convolution networks in recognizing daily activities in the Charades dataset. Further analyses show that the models learn intuitive and interpretable visual common sense knowledge in videos.



### Train, Diagnose and Fix: Interpretable Approach for Fine-grained Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.08502v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08502v1)
- **Published**: 2017-11-22 20:51:32+00:00
- **Updated**: 2017-11-22 20:51:32+00:00
- **Authors**: Jingxuan Hou, Tae Soo Kim, Austin Reiter
- **Comment**: 8 pages, 8 figures, CVPR18 submission
- **Journal**: None
- **Summary**: Despite the growing discriminative capabilities of modern deep learning methods for recognition tasks, the inner workings of the state-of-art models still remain mostly black-boxes. In this paper, we propose a systematic interpretation of model parameters and hidden representations of Residual Temporal Convolutional Networks (Res-TCN) for action recognition in time-series data. We also propose a Feature Map Decoder as part of the interpretation analysis, which outputs a representation of model's hidden variables in the same domain as the input. Such analysis empowers us to expose model's characteristic learning patterns in an interpretable way. For example, through the diagnosis analysis, we discovered that our model has learned to achieve view-point invariance by implicitly learning to perform rotational normalization of the input to a more discriminative view. Based on the findings from the model interpretation analysis, we propose a targeted refinement technique, which can generalize to various other recognition models. The proposed work introduces a three-stage paradigm for model learning: training, interpretable diagnosis and targeted refinement. We validate our approach on skeleton based 3D human action recognition benchmark of NTU RGB+D. We show that the proposed workflow is an effective model learning strategy and the resulting Multi-stream Residual Temporal Convolutional Network (MS-Res-TCN) achieves the state-of-the-art performance on NTU RGB+D.



### W-Net: A Deep Model for Fully Unsupervised Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.08506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08506v1)
- **Published**: 2017-11-22 21:06:13+00:00
- **Updated**: 2017-11-22 21:06:13+00:00
- **Authors**: Xide Xia, Brian Kulis
- **Comment**: None
- **Journal**: None
- **Summary**: While significant attention has been recently focused on designing supervised deep semantic segmentation algorithms for vision tasks, there are many domains in which sufficient supervised pixel-level labels are difficult to obtain. In this paper, we revisit the problem of purely unsupervised image segmentation and propose a novel deep architecture for this problem. We borrow recent ideas from supervised semantic segmentation methods, in particular by concatenating two fully convolutional networks together into an autoencoder--one for encoding and one for decoding. The encoding layer produces a k-way pixelwise prediction, and both the reconstruction error of the autoencoder as well as the normalized cut produced by the encoder are jointly minimized during training. When combined with suitable postprocessing involving conditional random field smoothing and hierarchical segmentation, our resulting algorithm achieves impressive results on the benchmark Berkeley Segmentation Data Set, outperforming a number of competing methods.



### Context Augmentation for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.01653v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.01653v2)
- **Published**: 2017-11-22 23:53:47+00:00
- **Updated**: 2017-12-12 01:11:35+00:00
- **Authors**: Aysegul Dundar, Ignacio Garcia-Dorado
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Recent enhancements of deep convolutional neural networks (ConvNets) empowered by enormous amounts of labeled data have closed the gap with human performance for many object recognition tasks. These impressive results have generated interest in understanding and visualization of ConvNets. In this work, we study the effect of background in the task of image classification. Our results show that changing the backgrounds of the training datasets can have drastic effects on testing accuracies. Furthermore, we enhance existing augmentation techniques with the foreground segmented objects. The findings of this work are important in increasing the accuracies when only a small dataset is available, in creating datasets, and creating synthetic images.



