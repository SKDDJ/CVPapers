# Arxiv Papers in cs.CV on 2017-11-10
### Poverty Prediction with Public Landsat 7 Satellite Imagery and Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.03654v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.03654v1)
- **Published**: 2017-11-10 00:21:54+00:00
- **Updated**: 2017-11-10 00:21:54+00:00
- **Authors**: Anthony Perez, Christopher Yeh, George Azzari, Marshall Burke, David Lobell, Stefano Ermon
- **Comment**: Presented at NIPS 2017 Workshop on Machine Learning for the
  Developing World
- **Journal**: None
- **Summary**: Obtaining detailed and reliable data about local economic livelihoods in developing countries is expensive, and data are consequently scarce. Previous work has shown that it is possible to measure local-level economic livelihoods using high-resolution satellite imagery. However, such imagery is relatively expensive to acquire, often not updated frequently, and is mainly available for recent years. We train CNN models on free and publicly available multispectral daytime satellite images of the African continent from the Landsat 7 satellite, which has collected imagery with global coverage for almost two decades. We show that despite these images' lower resolution, we can achieve accuracies that exceed previous benchmarks.



### Unsupervised Learning of Geometry with Edge-aware Depth-Normal Consistency
- **Arxiv ID**: http://arxiv.org/abs/1711.03665v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03665v1)
- **Published**: 2017-11-10 01:39:29+00:00
- **Updated**: 2017-11-10 01:39:29+00:00
- **Authors**: Zhenheng Yang, Peng Wang, Wei Xu, Liang Zhao, Ramakant Nevatia
- **Comment**: Accepted at AAAI 2018
- **Journal**: None
- **Summary**: Learning to reconstruct depths in a single image by watching unlabeled videos via deep convolutional network (DCN) is attracting significant attention in recent years. In this paper, we introduce a surface normal representation for unsupervised depth estimation framework. Our estimated depths are constrained to be compatible with predicted normals, yielding more robust geometry results. Specifically, we formulate an edge-aware depth-normal consistency term, and solve it by constructing a depth-to-normal layer and a normal-to-depth layer inside of the DCN. The depth-to-normal layer takes estimated depths as input, and computes normal directions using cross production based on neighboring pixels. Then given the estimated normals, the normal-to-depth layer outputs a regularized depth map through local planar smoothness. Both layers are computed with awareness of edges inside the image to help address the issue of depth/normal discontinuity and preserve sharp edges. Finally, to train the network, we apply the photometric error and gradient smoothness for both depth and normal predictions. We conducted experiments on both outdoor (KITTI) and indoor (NYUv2) datasets, and show that our algorithm vastly outperforms state of the art, which demonstrates the benefits from our approach.



### Breast density classification with deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1711.03674v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.03674v1)
- **Published**: 2017-11-10 02:50:46+00:00
- **Updated**: 2017-11-10 02:50:46+00:00
- **Authors**: Nan Wu, Krzysztof J. Geras, Yiqiu Shen, Jingyi Su, S. Gene Kim, Eric Kim, Stacey Wolfson, Linda Moy, Kyunghyun Cho
- **Comment**: None
- **Journal**: None
- **Summary**: Breast density classification is an essential part of breast cancer screening. Although a lot of prior work considered this problem as a task for learning algorithms, to our knowledge, all of them used small and not clinically realistic data both for training and evaluation of their models. In this work, we explore the limits of this task with a data set coming from over 200,000 breast cancer screening exams. We use this data to train and evaluate a strong convolutional neural network classifier. In a reader study, we find that our model can perform this task comparably to a human expert.



### Egocentric Hand Detection Via Dynamic Region Growing
- **Arxiv ID**: http://arxiv.org/abs/1711.03677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03677v1)
- **Published**: 2017-11-10 03:23:55+00:00
- **Updated**: 2017-11-10 03:23:55+00:00
- **Authors**: Shao Huang, Weiqiang Wang, Shengfeng He, Rynson W. H. Lau
- **Comment**: None
- **Journal**: None
- **Summary**: Egocentric videos, which mainly record the activities carried out by the users of the wearable cameras, have drawn much research attentions in recent years. Due to its lengthy content, a large number of ego-related applications have been developed to abstract the captured videos. As the users are accustomed to interacting with the target objects using their own hands while their hands usually appear within their visual fields during the interaction, an egocentric hand detection step is involved in tasks like gesture recognition, action recognition and social interaction understanding. In this work, we propose a dynamic region growing approach for hand region detection in egocentric videos, by jointly considering hand-related motion and egocentric cues. We first determine seed regions that most likely belong to the hand, by analyzing the motion patterns across successive frames. The hand regions can then be located by extending from the seed regions, according to the scores computed for the adjacent superpixels. These scores are derived from four egocentric cues: contrast, location, position consistency and appearance continuity. We discuss how to apply the proposed method in real-life scenarios, where multiple hands irregularly appear and disappear from the videos. Experimental results on public datasets show that the proposed method achieves superior performance compared with the state-of-the-art methods, especially in complicated scenarios.



### Self-Supervised Intrinsic Image Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1711.03678v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.03678v2)
- **Published**: 2017-11-10 03:31:27+00:00
- **Updated**: 2018-02-05 22:52:18+00:00
- **Authors**: Michael Janner, Jiajun Wu, Tejas D. Kulkarni, Ilker Yildirim, Joshua B. Tenenbaum
- **Comment**: NIPS 2017 camera-ready version, project page:
  http://rin.csail.mit.edu/
- **Journal**: None
- **Summary**: Intrinsic decomposition from a single image is a highly challenging task, due to its inherent ambiguity and the scarcity of training data. In contrast to traditional fully supervised learning approaches, in this paper we propose learning intrinsic image decomposition by explaining the input image. Our model, the Rendered Intrinsics Network (RIN), joins together an image decomposition pipeline, which predicts reflectance, shape, and lighting conditions given a single image, with a recombination function, a learned shading model used to recompose the original input based off of intrinsic image predictions. Our network can then use unsupervised reconstruction error as an additional signal to improve its intermediate representations. This allows large-scale unlabeled data to be useful during training, and also enables transferring learned knowledge to images of unseen object categories, lighting conditions, and shapes. Extensive experiments demonstrate that our method performs well on both intrinsic image decomposition and knowledge transfer.



### A Fully Convolutional Tri-branch Network (FCTN) for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1711.03694v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03694v2)
- **Published**: 2017-11-10 05:01:28+00:00
- **Updated**: 2018-02-27 01:43:35+00:00
- **Authors**: Junting Zhang, Chen Liang, C. -C. Jay Kuo
- **Comment**: Accepted by ICASSP 2018
- **Journal**: None
- **Summary**: A domain adaptation method for urban scene segmentation is proposed in this work. We develop a fully convolutional tri-branch network, where two branches assign pseudo labels to images in the unlabeled target domain while the third branch is trained with supervision based on images in the pseudo-labeled target domain. The re-labeling and re-training processes alternate. With this design, the tri-branch network learns target-specific discriminative representations progressively and, as a result, the cross-domain capability of the segmenter improves. We evaluate the proposed network on large-scale domain adaptation experiments using both synthetic (GTA) and real (Cityscapes) images. It is shown that our solution achieves the state-of-the-art performance and it outperforms previous methods by a significant margin.



### Saliency Prediction for Mobile User Interfaces
- **Arxiv ID**: http://arxiv.org/abs/1711.03726v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1711.03726v3)
- **Published**: 2017-11-10 08:38:44+00:00
- **Updated**: 2017-11-28 05:26:59+00:00
- **Authors**: Prakhar Gupta, Shubh Gupta, Ajaykrishnan Jayagopal, Sourav Pal, Ritwik Sinha
- **Comment**: Paper accepted at WACV 2018
- **Journal**: None
- **Summary**: We introduce models for saliency prediction for mobile user interfaces. A mobile interface may include elements like buttons, text, etc. in addition to natural images which enable performing a variety of tasks. Saliency in natural images is a well studied area. However, given the difference in what constitutes a mobile interface, and the usage context of these devices, we postulate that saliency prediction for mobile interface images requires a fresh approach. Mobile interface design involves operating on elements, the building blocks of the interface. We first collected eye-gaze data from mobile devices for free viewing task. Using this data, we develop a novel autoencoder based multi-scale deep learning model that provides saliency prediction at the mobile interface element level. Compared to saliency prediction approaches developed for natural images, we show that our approach performs significantly better on a range of established metrics.



### UCT: Learning Unified Convolutional Networks for Real-time Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1711.04661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04661v1)
- **Published**: 2017-11-10 09:18:06+00:00
- **Updated**: 2017-11-10 09:18:06+00:00
- **Authors**: Zheng Zhu, Guan Huang, Wei Zou, Dalong Du, Chang Huang
- **Comment**: ICCV2017 Workshops. arXiv admin note: text overlap with
  arXiv:1711.01124
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) based tracking approaches have shown favorable performance in recent benchmarks. Nonetheless, the chosen CNN features are always pre-trained in different task and individual components in tracking systems are learned separately, thus the achieved tracking performance may be suboptimal. Besides, most of these trackers are not designed towards real-time applications because of their time-consuming feature extraction and complex optimization details.In this paper, we propose an end-to-end framework to learn the convolutional features and perform the tracking process simultaneously, namely, a unified convolutional tracker (UCT). Specifically, The UCT treats feature extractor and tracking process both as convolution operation and trains them jointly, enabling learned CNN features are tightly coupled to tracking process. In online tracking, an efficient updating method is proposed by introducing peak-versus-noise ratio (PNR) criterion, and scale changes are handled efficiently by incorporating a scale branch into network. The proposed approach results in superior tracking performance, while maintaining real-time speed. The standard UCT and UCT-Lite can track generic objects at 41 FPS and 154 FPS without further optimization, respectively. Experiments are performed on four challenging benchmark tracking datasets: OTB2013, OTB2015, VOT2014 and VOT2015, and our method achieves state-of-the-art results on these benchmarks compared with other real-time trackers.



### Depression Severity Estimation from Multiple Modalities
- **Arxiv ID**: http://arxiv.org/abs/1711.06095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1711.06095v1)
- **Published**: 2017-11-10 12:47:52+00:00
- **Updated**: 2017-11-10 12:47:52+00:00
- **Authors**: Evgeny Stepanov, Stephane Lathuiliere, Shammur Absar Chowdhury, Arindam Ghosh, Radu-Laurentiu Vieriu, Nicu Sebe, Giuseppe Riccardi
- **Comment**: 8 pages, 1 figure
- **Journal**: None
- **Summary**: Depression is a major debilitating disorder which can affect people from all ages. With a continuous increase in the number of annual cases of depression, there is a need to develop automatic techniques for the detection of the presence and extent of depression. In this AVEC challenge we explore different modalities (speech, language and visual features extracted from face) to design and develop automatic methods for the detection of depression. In psychology literature, the PHQ-8 questionnaire is well established as a tool for measuring the severity of depression. In this paper we aim to automatically predict the PHQ-8 scores from features extracted from the different modalities. We show that visual features extracted from facial landmarks obtain the best performance in terms of estimating the PHQ-8 results with a mean absolute error (MAE) of 4.66 on the development set. Behavioral characteristics from speech provide an MAE of 4.73. Language features yield a slightly higher MAE of 5.17. When switching to the test set, our Turn Features derived from audio transcriptions achieve the best performance, scoring an MAE of 4.11 (corresponding to an RMSE of 4.94), which makes our system the winner of the AVEC 2017 depression sub-challenge.



### Object Referring in Visual Scene with Spoken Language
- **Arxiv ID**: http://arxiv.org/abs/1711.03800v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1711.03800v2)
- **Published**: 2017-11-10 13:04:55+00:00
- **Updated**: 2017-12-05 15:12:24+00:00
- **Authors**: Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool
- **Comment**: 10 pages, Submitted to WACV 2018
- **Journal**: None
- **Summary**: Object referring has important applications, especially for human-machine interaction. While having received great attention, the task is mainly attacked with written language (text) as input rather than spoken language (speech), which is more natural. This paper investigates Object Referring with Spoken Language (ORSpoken) by presenting two datasets and one novel approach. Objects are annotated with their locations in images, text descriptions and speech descriptions. This makes the datasets ideal for multi-modality learning. The approach is developed by carefully taking down ORSpoken problem into three sub-problems and introducing task-specific vision-language interactions at the corresponding levels. Experiments show that our method outperforms competing methods consistently and significantly. The approach is also evaluated in the presence of audio noise, showing the efficacy of the proposed vision-language interaction methods in counteracting background noise.



### Robotic Tactile Perception of Object Properties: A Review
- **Arxiv ID**: http://arxiv.org/abs/1711.03810v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.03810v1)
- **Published**: 2017-11-10 13:26:08+00:00
- **Updated**: 2017-11-10 13:26:08+00:00
- **Authors**: Shan Luo, Joao Bimbo, Ravinder Dahiya, Hongbin Liu
- **Comment**: 17 pages, 3 figures, 5 tables
- **Journal**: None
- **Summary**: Touch sensing can help robots understand their sur- rounding environment, and in particular the objects they interact with. To this end, roboticists have, in the last few decades, developed several tactile sensing solutions, extensively reported in the literature. Research into interpreting the conveyed tactile information has also started to attract increasing attention in recent years. However, a comprehensive study on this topic is yet to be reported. In an effort to collect and summarize the major scientific achievements in the area, this survey extensively reviews current trends in robot tactile perception of object properties. Available tactile sensing technologies are briefly presented before an extensive review on tactile recognition of object properties. The object properties that are targeted by this review are shape, surface material and object pose. The role of touch sensing in combination with other sensing sources is also discussed. In this review, open issues are identified and future directions for applying tactile sensing in different tasks are suggested.



### Arrhythmia Classification from the Abductive Interpretation of Short Single-Lead ECG Records
- **Arxiv ID**: http://arxiv.org/abs/1711.03892v1
- **DOI**: 10.22489/CinC.2017.166-054
- **Categories**: **cs.AI**, cs.CV, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1711.03892v1)
- **Published**: 2017-11-10 15:47:21+00:00
- **Updated**: 2017-11-10 15:47:21+00:00
- **Authors**: Tomás Teijeiro, Constantino A. García, Daniel Castro, Paulo Félix
- **Comment**: 4 pages, 3 figures. Presented in the Computing in Cardiology 2017
  conference
- **Journal**: Proceedings of the 2017 Computing in cardiology conference (CinC)
  (pp. 1-4). IEEE
- **Summary**: In this work we propose a new method for the rhythm classification of short single-lead ECG records, using a set of high-level and clinically meaningful features provided by the abductive interpretation of the records. These features include morphological and rhythm-related features that are used to build two classifiers: one that evaluates the record globally, using aggregated values for each feature; and another one that evaluates the record as a sequence, using a Recurrent Neural Network fed with the individual features for each detected heartbeat. The two classifiers are finally combined using the stacking technique, providing an answer by means of four target classes: Normal sinus rhythm, Atrial fibrillation, Other anomaly, and Noisy. The approach has been validated against the 2017 Physionet/CinC Challenge dataset, obtaining a final score of 0.83 and ranking first in the competition.



### CARLA: An Open Urban Driving Simulator
- **Arxiv ID**: http://arxiv.org/abs/1711.03938v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1711.03938v1)
- **Published**: 2017-11-10 17:54:40+00:00
- **Updated**: 2017-11-10 17:54:40+00:00
- **Authors**: Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun
- **Comment**: Published at the 1st Conference on Robot Learning (CoRL)
- **Journal**: None
- **Summary**: We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E



### EddyNet: A Deep Neural Network For Pixel-Wise Classification of Oceanic Eddies
- **Arxiv ID**: http://arxiv.org/abs/1711.03954v1
- **DOI**: 10.1109/IGARSS.2018.8518411
- **Categories**: **cs.CV**, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/1711.03954v1)
- **Published**: 2017-11-10 18:30:05+00:00
- **Updated**: 2017-11-10 18:30:05+00:00
- **Authors**: Redouane Lguensat, Miao Sun, Ronan Fablet, Evan Mason, Pierre Tandeo, Ge Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents EddyNet, a deep learning based architecture for automated eddy detection and classification from Sea Surface Height (SSH) maps provided by the Copernicus Marine and Environment Monitoring Service (CMEMS). EddyNet is a U-Net like network that consists of a convolutional encoder-decoder followed by a pixel-wise classification layer. The output is a map with the same size of the input where pixels have the following labels \{'0': Non eddy, '1': anticyclonic eddy, '2': cyclonic eddy\}. We investigate the use of SELU activation function instead of the classical ReLU+BN and we use an overlap based loss function instead of the cross entropy loss. Keras Python code, the training datasets and EddyNet weights files are open-source and freely available on https://github.com/redouanelg/EddyNet.



### Longitudinal Study of Child Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.03990v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.03990v1)
- **Published**: 2017-11-10 19:19:55+00:00
- **Updated**: 2017-11-10 19:19:55+00:00
- **Authors**: Debayan Deb, Neeta Nain, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: We present a longitudinal study of face recognition performance on Children Longitudinal Face (CLF) dataset containing 3,682 face images of 919 subjects, in the age group [2, 18] years. Each subject has at least four face images acquired over a time span of up to six years. Face comparison scores are obtained from (i) a state-of-the-art COTS matcher (COTS-A), (ii) an open-source matcher (FaceNet), and (iii) a simple sum fusion of scores obtained from COTS-A and FaceNet matchers. To improve the performance of the open-source FaceNet matcher for child face recognition, we were able to fine-tune it on an independent training set of 3,294 face images of 1,119 children in the age group [3, 18] years. Multilevel statistical models are fit to genuine comparison scores from the CLF dataset to determine the decrease in face recognition accuracy over time. Additionally, we analyze both the verification and open-set identification accuracies in order to evaluate state-of-the-art face recognition technology for tracing and identifying children lost at a young age as victims of child trafficking or abduction.



