# Arxiv Papers in cs.CV on 2017-11-14
### A Multiple Radar Approach for Automatic Target Recognition of Aircraft using Inverse Synthetic Aperture Radar
- **Arxiv ID**: http://arxiv.org/abs/1711.04901v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04901v2)
- **Published**: 2017-11-14 01:31:24+00:00
- **Updated**: 2018-03-12 17:10:24+00:00
- **Authors**: Carlos Pena-Caballero, Elifaleth Cantu, Jesus Rodriguez, Adolfo Gonzales, Osvaldo Castellanos, Angel Cantu, Megan Strait, Jae Son, Dongchul Kim
- **Comment**: 8 pages, 9 figures, International Conference for Data Intelligence
  and Security (ICDIS)
- **Journal**: None
- **Summary**: Along with the improvement of radar technologies, Automatic Target Recognition (ATR) using Synthetic Aperture Radar (SAR) and Inverse SAR (ISAR) has come to be an active research area. SAR/ISAR are radar techniques to generate a two-dimensional high-resolution image of a target. Unlike other similar experiments using Convolutional Neural Networks (CNN) to solve this problem, we utilize an unusual approach that leads to better performance and faster training times. Our CNN uses complex values generated by a simulation to train the network; additionally, we utilize a multi-radar approach to increase the accuracy of the training and testing processes, thus resulting in higher accuracies than the other papers working on SAR/ISAR ATR. We generated our dataset with 7 different aircraft models with a radar simulator we developed called RadarPixel; it is a Windows GUI program implemented using Matlab and Java programming, the simulator is capable of accurately replicating a real SAR/ISAR configurations. Our objective is to utilize our multi-radar technique and determine the optimal number of radars needed to detect and classify targets.



### Capturing Localized Image Artifacts through a CNN-based Hyper-image Representation
- **Arxiv ID**: http://arxiv.org/abs/1711.04945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04945v1)
- **Published**: 2017-11-14 04:32:16+00:00
- **Updated**: 2017-11-14 04:32:16+00:00
- **Authors**: Parag Shridhar Chandakkar, Baoxin Li
- **Comment**: Our work on No-reference Image Quality Estimation (NR-IQA) using deep
  neural networks
- **Journal**: None
- **Summary**: Training deep CNNs to capture localized image artifacts on a relatively small dataset is a challenging task. With enough images at hand, one can hope that a deep CNN characterizes localized artifacts over the entire data and their effect on the output. However, on smaller datasets, such deep CNNs may overfit and shallow ones find it hard to capture local artifacts. Thus some image-based small-data applications first train their framework on a collection of patches (instead of the entire image) to better learn the representation of localized artifacts. Then the output is obtained by averaging the patch-level results. Such an approach ignores the spatial correlation among patches and how various patch locations affect the output. It also fails in cases where few patches mainly contribute to the image label. To combat these scenarios, we develop the notion of hyper-image representations. Our CNN has two stages. The first stage is trained on patches. The second stage utilizes the last layer representation developed in the first stage to form a hyper-image, which is used to train the second stage. We show that this approach is able to develop a better mapping between the image and its output. We analyze additional properties of our approach and show its effectiveness on one synthetic and two real-world vision tasks - no-reference image quality estimation and image tampering detection - by its performance improvement over existing strong baselines.



### An optimized shape descriptor based on structural properties of networks
- **Arxiv ID**: http://arxiv.org/abs/1711.05104v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1711.05104v1)
- **Published**: 2017-11-14 14:26:19+00:00
- **Updated**: 2017-11-14 14:26:19+00:00
- **Authors**: Gisele H. B. Miranda, Jeaneth Machicao, Odemir M. Bruno
- **Comment**: 19 pages, 13 figures
- **Journal**: None
- **Summary**: The structural analysis of shape boundaries leads to the characterization of objects as well as to the understanding of shape properties. The literature on graphs and networks have contributed to the structural characterization of shapes with different theoretical approaches. We performed a study on the relationship between the shape architecture and the network topology constructed over the shape boundary. For that, we used a method for network modeling proposed in 2009. Firstly, together with curvature analysis, we evaluated the proposed approach for regular polygons. This way, it was possible to investigate how the network measurements vary according to some specific shape properties. Secondly, we evaluated the performance of the proposed shape descriptor in classification tasks for three datasets, accounting for both real-world and synthetic shapes. We demonstrated that not only degree related measurements are capable of distinguishing classes of objects. Yet, when using measurements that account for distinct properties of the network structure, the construction of the shape descriptor becomes more computationally efficient. Given the fact the network is dynamically constructed, the number of iterations can be reduced. The proposed approach accounts for a more robust set of structural measurements, that improved the discriminant power of the shape descriptors.



### Grab, Pay and Eat: Semantic Food Detection for Smart Restaurants
- **Arxiv ID**: http://arxiv.org/abs/1711.05128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05128v1)
- **Published**: 2017-11-14 14:49:13+00:00
- **Updated**: 2017-11-14 14:49:13+00:00
- **Authors**: Eduardo Aguilar, Beatriz Remeseiro, Marc Bolaños, Petia Radeva
- **Comment**: None
- **Journal**: None
- **Summary**: The increase in awareness of people towards their nutritional habits has drawn considerable attention to the field of automatic food analysis. Focusing on self-service restaurants environment, automatic food analysis is not only useful for extracting nutritional information from foods selected by customers, it is also of high interest to speed up the service solving the bottleneck produced at the cashiers in times of high demand. In this paper, we address the problem of automatic food tray analysis in canteens and restaurants environment, which consists in predicting multiple foods placed on a tray image. We propose a new approach for food analysis based on convolutional neural networks, we name Semantic Food Detection, which integrates in the same framework food localization, recognition and segmentation. We demonstrate that our method improves the state of the art food detection by a considerable margin on the public dataset UNIMIB2016 achieving about 90% in terms of F-measure, and thus provides a significant technological advance towards the automatic billing in restaurant environments.



### XGAN: Unsupervised Image-to-Image Translation for Many-to-Many Mappings
- **Arxiv ID**: http://arxiv.org/abs/1711.05139v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05139v6)
- **Published**: 2017-11-14 15:18:38+00:00
- **Updated**: 2018-07-10 17:25:59+00:00
- **Authors**: Amélie Royer, Konstantinos Bousmalis, Stephan Gouws, Fred Bertsch, Inbar Mosseri, Forrester Cole, Kevin Murphy
- **Comment**: Domain Adaptation for Visual Understanding at ICML'18
- **Journal**: None
- **Summary**: Style transfer usually refers to the task of applying color and texture information from a specific style image to a given content image while preserving the structure of the latter. Here we tackle the more generic problem of semantic style transfer: given two unpaired collections of images, we aim to learn a mapping between the corpus-level style of each collection, while preserving semantic content shared across the two domains. We introduce XGAN ("Cross-GAN"), a dual adversarial autoencoder, which captures a shared representation of the common domain semantic content in an unsupervised way, while jointly learning the domain-to-domain image translations in both directions. We exploit ideas from the domain adaptation literature and define a semantic consistency loss which encourages the model to preserve semantics in the learned embedding space. We report promising qualitative results for the task of face-to-cartoon translation. The cartoon dataset, CartoonSet, we collected for this purpose is publicly available at google.github.io/cartoonset/ as a new benchmark for semantic style transfer.



### Saliency-based Sequential Image Attention with Multiset Prediction
- **Arxiv ID**: http://arxiv.org/abs/1711.05165v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1711.05165v1)
- **Published**: 2017-11-14 16:16:36+00:00
- **Updated**: 2017-11-14 16:16:36+00:00
- **Authors**: Sean Welleck, Jialin Mao, Kyunghyun Cho, Zheng Zhang
- **Comment**: To appear in Advances in Neural Information Processing Systems 30
  (NIPS 2017)
- **Journal**: None
- **Summary**: Humans process visual scenes selectively and sequentially using attention. Central to models of human visual attention is the saliency map. We propose a hierarchical visual architecture that operates on a saliency map and uses a novel attention mechanism to sequentially focus on salient regions and take additional glimpses within those regions. The architecture is motivated by human visual attention, and is used for multi-label image classification on a novel multiset task, demonstrating that it achieves high precision and recall while localizing objects with its attention. Unlike conventional multi-label image classification models, the model supports multiset prediction due to a reinforcement-learning based training process that allows for arbitrary label permutation and multiple instances per label.



### Robust Keyframe-based Dense SLAM with an RGB-D Camera
- **Arxiv ID**: http://arxiv.org/abs/1711.05166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05166v1)
- **Published**: 2017-11-14 16:18:04+00:00
- **Updated**: 2017-11-14 16:18:04+00:00
- **Authors**: Haomin Liu, Chen Li, Guojun Chen, Guofeng Zhang, Michael Kaess, Hujun Bao
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: In this paper, we present RKD-SLAM, a robust keyframe-based dense SLAM approach for an RGB-D camera that can robustly handle fast motion and dense loop closure, and run without time limitation in a moderate size scene. It not only can be used to scan high-quality 3D models, but also can satisfy the demand of VR and AR applications. First, we combine color and depth information to construct a very fast keyframe-based tracking method on a CPU, which can work robustly in challenging cases (e.g.~fast camera motion and complex loops). For reducing accumulation error, we also introduce a very efficient incremental bundle adjustment (BA) algorithm, which can greatly save unnecessary computation and perform local and global BA in a unified optimization framework. An efficient keyframe-based depth representation and fusion method is proposed to generate and timely update the dense 3D surface with online correction according to the refined camera poses of keyframes through BA. The experimental results and comparisons on a variety of challenging datasets and TUM RGB-D benchmark demonstrate the effectiveness of the proposed system.



### Adversarial Information Factorization
- **Arxiv ID**: http://arxiv.org/abs/1711.05175v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05175v2)
- **Published**: 2017-11-14 16:25:09+00:00
- **Updated**: 2018-09-28 14:42:04+00:00
- **Authors**: Antonia Creswell, Yumnah Mohamied, Biswa Sengupta, Anil A Bharath
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel generative model architecture designed to learn representations for images that factor out a single attribute from the rest of the representation. A single object may have many attributes which when altered do not change the identity of the object itself. Consider the human face; the identity of a particular person is independent of whether or not they happen to be wearing glasses. The attribute of wearing glasses can be changed without changing the identity of the person. However, the ability to manipulate and alter image attributes without altering the object identity is not a trivial task. Here, we are interested in learning a representation of the image that separates the identity of an object (such as a human face) from an attribute (such as 'wearing glasses'). We demonstrate the success of our factorization approach by using the learned representation to synthesize the same face with and without a chosen attribute. We refer to this specific synthesis process as image attribute manipulation. We further demonstrate that our model achieves competitive scores, with state of the art, on a facial attribute classification task.



### Dynamic Zoom-in Network for Fast Object Detection in Large Images
- **Arxiv ID**: http://arxiv.org/abs/1711.05187v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05187v2)
- **Published**: 2017-11-14 16:52:50+00:00
- **Updated**: 2018-03-27 15:10:11+00:00
- **Authors**: Mingfei Gao, Ruichi Yu, Ang Li, Vlad I. Morariu, Larry S. Davis
- **Comment**: CVPR2018
- **Journal**: None
- **Summary**: We introduce a generic framework that reduces the computational cost of object detection while retaining accuracy for scenarios where objects with varied sizes appear in high resolution images. Detection progresses in a coarse-to-fine manner, first on a down-sampled version of the image and then on a sequence of higher resolution regions identified as likely to improve the detection accuracy. Built upon reinforcement learning, our approach consists of a model (R-net) that uses coarse detection results to predict the potential accuracy gain for analyzing a region at a higher resolution and another model (Q-net) that sequentially selects regions to zoom in. Experiments on the Caltech Pedestrians dataset show that our approach reduces the number of processed pixels by over 50% without a drop in detection accuracy. The merits of our approach become more significant on a high resolution test set collected from YFCC100M dataset, where our approach maintains high detection performance while reducing the number of processed pixels by about 70% and the detection time by over 50%.



### CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.05225v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.05225v3)
- **Published**: 2017-11-14 17:58:50+00:00
- **Updated**: 2017-12-25 11:09:06+00:00
- **Authors**: Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, Matthew P. Lungren, Andrew Y. Ng
- **Comment**: None
- **Journal**: None
- **Summary**: We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.



### Towards Interpretable R-CNN by Unfolding Latent Structures
- **Arxiv ID**: http://arxiv.org/abs/1711.05226v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05226v2)
- **Published**: 2017-11-14 17:59:01+00:00
- **Updated**: 2018-09-06 16:52:13+00:00
- **Authors**: Tianfu Wu, Wei Sun, Xilai Li, Xi Song, Bo Li
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: This paper first proposes a method of formulating model interpretability in visual understanding tasks based on the idea of unfolding latent structures. It then presents a case study in object detection using popular two-stage region-based convolutional network (i.e., R-CNN) detection systems. We focus on weakly-supervised extractive rationale generation, that is learning to unfold latent discriminative part configurations of object instances automatically and simultaneously in detection without using any supervision for part configurations. We utilize a top-down hierarchical and compositional grammar model embedded in a directed acyclic AND-OR Graph (AOG) to explore and unfold the space of latent part configurations of regions of interest (RoIs). We propose an AOGParsing operator to substitute the RoIPooling operator widely used in R-CNN. In detection, a bounding box is interpreted by the best parse tree derived from the AOG on-the-fly, which is treated as the qualitatively extractive rationale generated for interpreting detection. We propose a folding-unfolding method to train the AOG and convolutional networks end-to-end. In experiments, we build on R-FCN and test our method on the PASCAL VOC 2007 and 2012 datasets. We show that the method can unfold promising latent structures without hurting the performance.



### Loss Functions for Multiset Prediction
- **Arxiv ID**: http://arxiv.org/abs/1711.05246v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.05246v2)
- **Published**: 2017-11-14 18:43:22+00:00
- **Updated**: 2018-10-25 18:32:36+00:00
- **Authors**: Sean Welleck, Zixin Yao, Yu Gai, Jialin Mao, Zheng Zhang, Kyunghyun Cho
- **Comment**: NIPS 2018
- **Journal**: None
- **Summary**: We study the problem of multiset prediction. The goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items. Unlike existing problems in supervised learning, such as classification, ranking and sequence generation, there is no known order among items in a target multiset, and each item in the multiset may appear more than once, making this problem extremely challenging. In this paper, we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making. The proposed multiset loss function is empirically evaluated on two families of datasets, one synthetic and the other real, with varying levels of difficulty, against various baseline loss functions including reinforcement learning, sequence, and aggregated distribution matching loss functions. The experiments reveal the effectiveness of the proposed loss function over the others.



### C-WSL: Count-guided Weakly Supervised Localization
- **Arxiv ID**: http://arxiv.org/abs/1711.05282v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.05282v2)
- **Published**: 2017-11-14 19:08:23+00:00
- **Updated**: 2018-07-25 04:40:46+00:00
- **Authors**: Mingfei Gao, Ang Li, Ruichi Yu, Vlad I. Morariu, Larry S. Davis
- **Comment**: ECCV2018
- **Journal**: None
- **Summary**: We introduce count-guided weakly supervised localization (C-WSL), an approach that uses per-class object count as a new form of supervision to improve weakly supervised localization (WSL). C-WSL uses a simple count-based region selection algorithm to select high-quality regions, each of which covers a single object instance during training, and improves existing WSL methods by training with the selected regions. To demonstrate the effectiveness of C-WSL, we integrate it into two WSL architectures and conduct extensive experiments on VOC2007 and VOC2012. Experimental results show that C-WSL leads to large improvements in WSL and that the proposed approach significantly outperforms the state-of-the-art methods. The results of annotation experiments on VOC2007 suggest that a modest extra time is needed to obtain per-class object counts compared to labeling only object categories in an image. Furthermore, we reduce the annotation time by more than $2\times$ and $38\times$ compared to center-click and bounding-box annotations.



