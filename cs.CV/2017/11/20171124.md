# Arxiv Papers in cs.CV on 2017-11-24
### Real-Time Seamless Single Shot 6D Object Pose Prediction
- **Arxiv ID**: http://arxiv.org/abs/1711.08848v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08848v5)
- **Published**: 2017-11-24 00:56:37+00:00
- **Updated**: 2018-12-07 09:38:58+00:00
- **Authors**: Bugra Tekin, Sudipta N. Sinha, Pascal Fua
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: We propose a single-shot approach for simultaneously detecting an object in an RGB image and predicting its 6D pose without requiring multiple stages or having to examine multiple hypotheses. Unlike a recently proposed single-shot technique for this task (Kehl et al., ICCV'17) that only predicts an approximate 6D pose that must then be refined, ours is accurate enough not to require additional post-processing. As a result, it is much faster - 50 fps on a Titan X (Pascal) GPU - and more suitable for real-time processing. The key component of our method is a new CNN architecture inspired by the YOLO network design that directly predicts the 2D image locations of the projected vertices of the object's 3D bounding box. The object's 6D pose is then estimated using a PnP algorithm.   For single object and multiple object pose estimation on the LINEMOD and OCCLUSION datasets, our approach substantially outperforms other recent CNN-based approaches when they are all used without post-processing. During post-processing, a pose refinement step can be used to boost the accuracy of the existing methods, but at 10 fps or less, they are much slower than our method.



### Wasserstein Introspective Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.08875v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.08875v5)
- **Published**: 2017-11-24 06:04:02+00:00
- **Updated**: 2018-04-07 17:05:25+00:00
- **Authors**: Kwonjoon Lee, Weijian Xu, Fan Fan, Zhuowen Tu
- **Comment**: Accepted to CVPR 2018 (Oral)
- **Journal**: None
- **Summary**: We present Wasserstein introspective neural networks (WINN) that are both a generator and a discriminator within a single model. WINN provides a significant improvement over the recent introspective neural networks (INN) method by enhancing INN's generative modeling capability. WINN has three interesting properties: (1) A mathematical connection between the formulation of the INN algorithm and that of Wasserstein generative adversarial networks (WGAN) is made. (2) The explicit adoption of the Wasserstein distance into INN results in a large enhancement to INN, achieving compelling results even with a single classifier --- e.g., providing nearly a 20 times reduction in model size over INN for unsupervised generative modeling. (3) When applied to supervised classification, WINN also gives rise to improved robustness against adversarial examples in terms of the error reduction. In the experiments, we report encouraging results on unsupervised learning problems including texture, face, and object modeling, as well as a supervised classification task against adversarial attacks.



### Feature Selective Networks for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1711.08879v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08879v1)
- **Published**: 2017-11-24 06:39:49+00:00
- **Updated**: 2017-11-24 06:39:49+00:00
- **Authors**: Yao Zhai, Jingjing Fu, Yan Lu, Houqiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Objects for detection usually have distinct characteristics in different sub-regions and different aspect ratios. However, in prevalent two-stage object detection methods, Region-of-Interest (RoI) features are extracted by RoI pooling with little emphasis on these translation-variant feature components. We present feature selective networks to reform the feature representations of RoIs by exploiting their disparities among sub-regions and aspect ratios. Our network produces the sub-region attention bank and aspect ratio attention bank for the whole image. The RoI-based sub-region attention map and aspect ratio attention map are selectively pooled from the banks, and then used to refine the original RoI features for RoI classification. Equipped with a light-weight detection subnetwork, our network gets a consistent boost in detection performance based on general ConvNet backbones (ResNet-101, GoogLeNet and VGG-16). Without bells and whistles, our detectors equipped with ResNet-101 achieve more than 3% mAP improvement compared to counterparts on PASCAL VOC 2007, PASCAL VOC 2012 and MS COCO datasets.



### Constrained Manifold Learning for Hyperspectral Imagery Visualization
- **Arxiv ID**: http://arxiv.org/abs/1712.01657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01657v1)
- **Published**: 2017-11-24 07:04:04+00:00
- **Updated**: 2017-11-24 07:04:04+00:00
- **Authors**: Danping Liao, Yuntao Qian, Yuan Yan Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Displaying the large number of bands in a hyper- spectral image (HSI) on a trichromatic monitor is important for HSI processing and analysis system. The visualized image shall convey as much information as possible from the original HSI and meanwhile facilitate image interpretation. However, most existing methods display HSIs in false color, which contradicts with user experience and expectation. In this paper, we propose a visualization approach based on constrained manifold learning, whose goal is to learn a visualized image that not only preserves the manifold structure of the HSI but also has natural colors. Manifold learning preserves the image structure by forcing pixels with similar signatures to be displayed with similar colors. A composite kernel is applied in manifold learning to incorporate both the spatial and spectral information of HSI in the embedded space. The colors of the output image are constrained by a corresponding natural-looking RGB image, which can either be generated from the HSI itself (e.g., band selection from the visible wavelength) or be captured by a separate device. Our method can be done at instance-level and feature-level. Instance-level learning directly obtains the RGB coordinates for the pixels in the HSI while feature-level learning learns an explicit mapping function from the high dimensional spectral space to the RGB space. Experimental results demonstrate the advantage of the proposed method in information preservation and natural color visualization.



### Supervised Hashing with End-to-End Binary Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1711.08901v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08901v2)
- **Published**: 2017-11-24 09:10:32+00:00
- **Updated**: 2018-10-27 11:16:43+00:00
- **Authors**: Dang-Khoa Le Tan, Thanh-Toan Do, Ngai-Man Cheung
- **Comment**: Accepted to IEEE ICIP 2018
- **Journal**: None
- **Summary**: Image hashing is a popular technique applied to large scale content-based visual retrieval due to its compact and efficient binary codes. Our work proposes a new end-to-end deep network architecture for supervised hashing which directly learns binary codes from input images and maintains good properties over binary codes such as similarity preservation, independence, and balancing. Furthermore, we also propose a new learning scheme that can cope with the binary constrained loss function. The proposed algorithm not only is scalable for learning over large-scale datasets but also outperforms state-of-the-art supervised hashing methods, which are illustrated throughout extensive experiments from various image retrieval benchmarks.



### Adversarial Transfer Learning for Cross-domain Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.08904v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08904v2)
- **Published**: 2017-11-24 09:34:53+00:00
- **Updated**: 2019-12-24 08:47:10+00:00
- **Authors**: Shanshan Wang, Lei Zhang, JingRu Fu
- **Comment**: None
- **Journal**: None
- **Summary**: In many practical visual recognition scenarios, feature distribution in the source domain is generally different from that of the target domain, which results in the emergence of general cross-domain visual recognition problems. To address the problems of visual domain mismatch, we propose a novel semi-supervised adversarial transfer learning approach, which is called Coupled adversarial transfer Domain Adaptation (CatDA), for distribution alignment between two domains. The proposed CatDA approach is inspired by cycleGAN, but leveraging multiple shallow multilayer perceptrons (MLPs) instead of deep networks. Specifically, our CatDA comprises of two symmetric and slim sub-networks, such that the coupled adversarial learning framework is formulated. With such symmetry of two generators, the input data from source/target domain can be fed into the MLP network for target/source domain generation, supervised by two confrontation oriented coupled discriminators. Notably, in order to avoid the critical flaw of high-capacity of the feature extraction function during domain adversarial training, domain specific loss and domain knowledge fidelity loss are proposed in each generator, such that the effectiveness of the proposed transfer network is guaranteed. Additionally, the essential difference from cycleGAN is that our method aims to generate domain-agnostic and aligned features for domain adaptation and transfer learning rather than synthesize realistic images. We show experimentally on a number of benchmark datasets and the proposed approach achieves competitive performance over state-of-the-art domain adaptation and transfer learning approaches.



### Deep learning analysis of the myocardium in coronary CT angiography for identification of patients with functionally significant coronary artery stenosis
- **Arxiv ID**: http://arxiv.org/abs/1711.08917v2
- **DOI**: 10.1016/j.media.2017.11.008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08917v2)
- **Published**: 2017-11-24 10:27:36+00:00
- **Updated**: 2017-12-06 09:22:49+00:00
- **Authors**: Majd Zreik, Nikolas Lessmann, Robbert W. van Hamersvelt, Jelmer M. Wolterink, Michiel Voskuil, Max A. Viergever, Tim Leiner, Ivana Išgum
- **Comment**: This paper was submitted in April 2017 and accepted in November 2017
  for publication in Medical Image Analysis. Please cite as: Zreik et al.,
  Medical Image Analysis, 2018, vol. 44, pp. 72-85
- **Journal**: None
- **Summary**: In patients with coronary artery stenoses of intermediate severity, the functional significance needs to be determined. Fractional flow reserve (FFR) measurement, performed during invasive coronary angiography (ICA), is most often used in clinical practice. To reduce the number of ICA procedures, we present a method for automatic identification of patients with functionally significant coronary artery stenoses, employing deep learning analysis of the left ventricle (LV) myocardium in rest coronary CT angiography (CCTA). The study includes consecutively acquired CCTA scans of 166 patients with FFR measurements. To identify patients with a functionally significant coronary artery stenosis, analysis is performed in several stages. First, the LV myocardium is segmented using a multiscale convolutional neural network (CNN). To characterize the segmented LV myocardium, it is subsequently encoded using unsupervised convolutional autoencoder (CAE). Thereafter, patients are classified according to the presence of functionally significant stenosis using an SVM classifier based on the extracted and clustered encodings. Quantitative evaluation of LV myocardium segmentation in 20 images resulted in an average Dice coefficient of 0.91 and an average mean absolute distance between the segmented and reference LV boundaries of 0.7 mm. Classification of patients was evaluated in the remaining 126 CCTA scans in 50 10-fold cross-validation experiments and resulted in an area under the receiver operating characteristic curve of 0.74 +- 0.02. At sensitivity levels 0.60, 0.70 and 0.80, the corresponding specificity was 0.77, 0.71 and 0.59, respectively. The results demonstrate that automatic analysis of the LV myocardium in a single CCTA scan acquired at rest, without assessment of the anatomy of the coronary arteries, can be used to identify patients with functionally significant coronary artery stenosis.



### SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels
- **Arxiv ID**: http://arxiv.org/abs/1711.08920v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08920v2)
- **Published**: 2017-11-24 10:33:05+00:00
- **Updated**: 2018-05-23 08:57:29+00:00
- **Authors**: Matthias Fey, Jan Eric Lenssen, Frank Weichert, Heinrich Müller
- **Comment**: Presented at CVPR 2018
- **Journal**: None
- **Summary**: We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors. For validation, we apply our method on tasks from the fields of image graph classification, shape correspondence and graph node classification, and show that it outperforms or pars state-of-the-art approaches while being significantly faster and having favorable properties like domain-independence.



### Summarizing First-Person Videos from Third Persons' Points of Views
- **Arxiv ID**: http://arxiv.org/abs/1711.08922v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08922v2)
- **Published**: 2017-11-24 10:42:42+00:00
- **Updated**: 2018-07-26 13:28:51+00:00
- **Authors**: Hsuan-I Ho, Wei-Chen Chiu, Yu-Chiang Frank Wang
- **Comment**: 16+10 pages, ECCV 2018
- **Journal**: None
- **Summary**: Video highlight or summarization is among interesting topics in computer vision, which benefits a variety of applications like viewing, searching, or storage. However, most existing studies rely on training data of third-person videos, which cannot easily generalize to highlight the first-person ones. With the goal of deriving an effective model to summarize first-person videos, we propose a novel deep neural network architecture for describing and discriminating vital spatiotemporal information across videos with different points of view. Our proposed model is realized in a semi-supervised setting, in which fully annotated third-person videos, unlabeled first-person videos, and a small number of annotated first-person ones are presented during training. In our experiments, qualitative and quantitative evaluations on both benchmarks and our collected first-person video datasets are presented.



### A Gamut-Mapping Framework for Color-Accurate Reproduction of HDR Images
- **Arxiv ID**: http://arxiv.org/abs/1711.08925v1
- **DOI**: 10.1109/MCG.2015.116
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1711.08925v1)
- **Published**: 2017-11-24 11:06:07+00:00
- **Updated**: 2017-11-24 11:06:07+00:00
- **Authors**: E. Sikudova, T. Pouli, A. Artusi, A. O. Akyuz, F. Banterle, Z. M. Mazlumoglu, E. Reinhard
- **Comment**: None
- **Journal**: IEEE Computer Graphics and Applications, Volume 36, Issue 4,
  p.78-90, ISSN 0272-1716, July-August 2016
- **Summary**: Few tone mapping operators (TMOs) take color management into consideration, limiting compression to luminance values only. This may lead to changes in image chroma and hues which are typically managed with a post-processing step. However, current post-processing techniques for tone reproduction do not explicitly consider the target display gamut. Gamut mapping on the other hand, deals with mapping images from one color gamut to another, usually smaller, gamut but has traditionally focused on smaller scale, chromatic changes. In this context, we present a novel gamut and tone management framework for color-accurate reproduction of high dynamic range (HDR) images, which is conceptually and computationally simple, parameter-free, and compatible with existing TMOs. In the CIE LCh color space, we compress chroma to fit the gamut of the output color space. This prevents hue and luminance shifts while taking gamut boundaries into consideration. We also propose a compatible lightness compression scheme that minimizes the number of color space conversions. Our results show that our gamut management method effectively compresses the chroma of tone mapped images, respecting the target gamut and without reducing image quality.



### Deep High Dynamic Range Imaging with Large Foreground Motions
- **Arxiv ID**: http://arxiv.org/abs/1711.08937v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1711.08937v3)
- **Published**: 2017-11-24 12:12:01+00:00
- **Updated**: 2018-07-24 11:07:30+00:00
- **Authors**: Shangzhe Wu, Jiarui Xu, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: This paper proposes the first non-flow-based deep framework for high dynamic range (HDR) imaging of dynamic scenes with large-scale foreground motions. In state-of-the-art deep HDR imaging, input images are first aligned using optical flows before merging, which are still error-prone due to occlusion and large motions. In stark contrast to flow-based methods, we formulate HDR imaging as an image translation problem without optical flows. Moreover, our simple translation network can automatically hallucinate plausible HDR details in the presence of total occlusion, saturation and under-exposure, which are otherwise almost impossible to recover by conventional optimization approaches. Our framework can also be extended for different reference images. We performed extensive qualitative and quantitative comparisons to show that our approach produces excellent results where color artifacts and geometric distortions are significantly reduced compared to existing state-of-the-art methods, and is robust across various inputs, including images without radiometric calibration.



### Image Generation from Sketch Constraint Using Contextual GAN
- **Arxiv ID**: http://arxiv.org/abs/1711.08972v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08972v2)
- **Published**: 2017-11-24 14:19:25+00:00
- **Updated**: 2018-07-26 03:01:01+00:00
- **Authors**: Yongyi Lu, Shangzhe Wu, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: In this paper we investigate image generation guided by hand sketch. When the input sketch is badly drawn, the output of common image-to-image translation follows the input edges due to the hard condition imposed by the translation process. Instead, we propose to use sketch as weak constraint, where the output edges do not necessarily follow the input edges. We address this problem using a novel joint image completion approach, where the sketch provides the image context for completing, or generating the output image. We train a generated adversarial network, i.e, contextual GAN to learn the joint distribution of sketch and the corresponding image by using joint images. Our contextual GAN has several advantages. First, the simple joint image representation allows for simple and effective learning of joint distribution in the same image-sketch space, which avoids complicated issues in cross-domain learning. Second, while the output is related to its input overall, the generated features exhibit more freedom in appearance and do not strictly align with the input features as previous conditional GANs do. Third, from the joint image's point of view, image and sketch are of no difference, thus exactly the same deep joint image completion network can be used for image-to-sketch generation. Experiments evaluated on three different datasets show that our contextual GAN can generate more realistic images than state-of-the-art conditional GANs on challenging inputs and generalize well on common categories.



### Self-Supervised Vision-Based Detection of the Active Speaker as Support for Socially-Aware Language Acquisition
- **Arxiv ID**: http://arxiv.org/abs/1711.08992v2
- **DOI**: 10.1109/TCDS.2019.2927941
- **Categories**: **cs.CV**, cs.CL, cs.HC, cs.LG, stat.ML, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1711.08992v2)
- **Published**: 2017-11-24 14:45:06+00:00
- **Updated**: 2019-07-18 17:55:38+00:00
- **Authors**: Kalin Stefanov, Jonas Beskow, Giampiero Salvi
- **Comment**: 10 pages, IEEE Transactions on Cognitive and Developmental Systems
- **Journal**: None
- **Summary**: This paper presents a self-supervised method for visual detection of the active speaker in a multi-person spoken interaction scenario. Active speaker detection is a fundamental prerequisite for any artificial cognitive system attempting to acquire language in social settings. The proposed method is intended to complement the acoustic detection of the active speaker, thus improving the system robustness in noisy conditions. The method can detect an arbitrary number of possibly overlapping active speakers based exclusively on visual information about their face. Furthermore, the method does not rely on external annotations, thus complying with cognitive development. Instead, the method uses information from the auditory modality to support learning in the visual domain. This paper reports an extensive evaluation of the proposed method using a large multi-person face-to-face interaction dataset. The results show good performance in a speaker dependent setting. However, in a speaker independent setting the proposed method yields a significantly lower performance. We believe that the proposed method represents an essential component of any artificial cognitive system or robotic platform engaging in social interactions.



### Unsupervised Domain Adaptation with Similarity Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.08995v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08995v2)
- **Published**: 2017-11-24 14:49:25+00:00
- **Updated**: 2018-04-18 01:06:21+00:00
- **Authors**: Pedro O. Pinheiro
- **Comment**: None
- **Journal**: None
- **Summary**: The objective of unsupervised domain adaptation is to leverage features from a labeled source domain and learn a classifier for an unlabeled target domain, with a similar but different data distribution. Most deep learning approaches to domain adaptation consist of two steps: (i) learn features that preserve a low risk on labeled samples (source domain) and (ii) make the features from both domains to be as indistinguishable as possible, so that a classifier trained on the source can also be applied on the target domain. In general, the classifiers in step (i) consist of fully-connected layers applied directly on the indistinguishable features learned in (ii). In this paper, we propose a different way to do the classification, using similarity learning. The proposed method learns a pairwise similarity function in which classification can be performed by computing similarity between prototype representations of each category. The domain-invariant features and the categorical prototype representations are learned jointly and in an end-to-end fashion. At inference time, images from the target domain are compared to the prototypes and the label associated with the one that best matches the image is outputed. The approach is simple, scalable and effective. We show that our model achieves state-of-the-art performance in different unsupervised domain adaptation scenarios.



### Dense 3D Regression for Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1711.08996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08996v1)
- **Published**: 2017-11-24 14:50:23+00:00
- **Updated**: 2017-11-24 14:50:23+00:00
- **Authors**: Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple and effective method for 3D hand pose estimation from a single depth frame. As opposed to previous state-of-the-art methods based on holistic 3D regression, our method works on dense pixel-wise estimation. This is achieved by careful design choices in pose parameterization, which leverages both 2D and 3D properties of depth map. Specifically, we decompose the pose parameters into a set of per-pixel estimations, i.e., 2D heat maps, 3D heat maps and unit 3D directional vector fields. The 2D/3D joint heat maps and 3D joint offsets are estimated via multi-task network cascades, which is trained end-to-end. The pixel-wise estimations can be directly translated into a vote casting scheme. A variant of mean shift is then used to aggregate local votes while enforcing consensus between the the estimated 3D pose and the pixel-wise 2D and 3D estimations by design. Our method is efficient and highly accurate. On MSRA and NYU hand dataset, our method outperforms all previous state-of-the-art approaches by a large margin. On the ICVL hand dataset, our method achieves similar accuracy compared to the currently proposed nearly saturated result and outperforms various other proposed methods. Code is available $\href{"https://github.com/melonwan/denseReg"}{\text{online}}$.



### Visual Feature Attribution using Wasserstein GANs
- **Arxiv ID**: http://arxiv.org/abs/1711.08998v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08998v3)
- **Published**: 2017-11-24 14:52:08+00:00
- **Updated**: 2018-06-26 10:38:13+00:00
- **Authors**: Christian F. Baumgartner, Lisa M. Koch, Kerem Can Tezcan, Jia Xi Ang, Ender Konukoglu
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: Attributing the pixels of an input image to a certain category is an important and well-studied problem in computer vision, with applications ranging from weakly supervised localisation to understanding hidden effects in the data. In recent years, approaches based on interpreting a previously trained neural network classifier have become the de facto state-of-the-art and are commonly used on medical as well as natural image datasets. In this paper, we discuss a limitation of these approaches which may lead to only a subset of the category specific features being detected. To address this problem we develop a novel feature attribution technique based on Wasserstein Generative Adversarial Networks (WGAN), which does not suffer from this limitation. We show that our proposed method performs substantially better than the state-of-the-art for visual attribution on a synthetic dataset and on real 3D neuroimaging data from patients with mild cognitive impairment (MCI) and Alzheimer's disease (AD). For AD patients the method produces compellingly realistic disease effect maps which are very close to the observed effects.



### Natural and Effective Obfuscation by Head Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1711.09001v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.CY, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1711.09001v5)
- **Published**: 2017-11-24 15:00:53+00:00
- **Updated**: 2018-03-16 10:56:10+00:00
- **Authors**: Qianru Sun, Liqian Ma, Seong Joon Oh, Luc Van Gool, Bernt Schiele, Mario Fritz
- **Comment**: To appear in CVPR 2018
- **Journal**: None
- **Summary**: As more and more personal photos are shared online, being able to obfuscate identities in such photos is becoming a necessity for privacy protection. People have largely resorted to blacking out or blurring head regions, but they result in poor user experience while being surprisingly ineffective against state of the art person recognizers. In this work, we propose a novel head inpainting obfuscation technique. Generating a realistic head inpainting in social media photos is challenging because subjects appear in diverse activities and head orientations. We thus split the task into two sub-tasks: (1) facial landmark generation from image context (e.g. body pose) for seamless hypothesis of sensible head pose, and (2) facial landmark conditioned head inpainting. We verify that our inpainting method generates realistic person images, while achieving superior obfuscation performance against automatic person recognizers.



### MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/1711.09017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09017v1)
- **Published**: 2017-11-24 15:20:22+00:00
- **Updated**: 2017-11-24 15:20:22+00:00
- **Authors**: Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling
- **Comment**: None
- **Journal**: None
- **Summary**: Learning-based methods are believed to work well for unconstrained gaze estimation, i.e. gaze estimation from a monocular RGB camera without assumptions regarding user, environment, or camera. However, current gaze datasets were collected under laboratory conditions and methods were not evaluated across multiple datasets. Our work makes three contributions towards addressing these limitations. First, we present the MPIIGaze that contains 213,659 full face images and corresponding ground-truth gaze positions collected from 15 users during everyday laptop use over several months. An experience sampling approach ensured continuous gaze and head poses and realistic variation in eye appearance and illumination. To facilitate cross-dataset evaluations, 37,667 images were manually annotated with eye corners, mouth corners, and pupil centres. Second, we present an extensive evaluation of state-of-the-art gaze estimation methods on three current datasets, including MPIIGaze. We study key challenges including target gaze range, illumination conditions, and facial appearance variation. We show that image resolution and the use of both eyes affect gaze estimation performance while head pose and pupil centre information are less informative. Finally, we propose GazeNet, the first deep appearance-based gaze estimation method. GazeNet improves the state of the art by 22% percent (from a mean error of 13.9 degrees to 10.8 degrees) for the most challenging cross-dataset evaluation.



### StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1711.09020v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09020v3)
- **Published**: 2017-11-24 15:37:30+00:00
- **Updated**: 2018-09-21 08:17:49+00:00
- **Authors**: Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo
- **Comment**: Accepted to CVPR 2018 (Oral)
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2018, pp. 8789-8797
- **Summary**: Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.



### Long-Term On-Board Prediction of People in Traffic Scenes under Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1711.09026v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09026v2)
- **Published**: 2017-11-24 15:54:49+00:00
- **Updated**: 2018-06-20 15:04:58+00:00
- **Authors**: Apratim Bhattacharyya, Mario Fritz, Bernt Schiele
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Progress towards advanced systems for assisted and autonomous driving is leveraging recent advances in recognition and segmentation methods. Yet, we are still facing challenges in bringing reliable driving to inner cities, as those are composed of highly dynamic scenes observed from a moving platform at considerable speeds. Anticipation becomes a key element in order to react timely and prevent accidents. In this paper we argue that it is necessary to predict at least 1 second and we thus propose a new model that jointly predicts ego motion and people trajectories over such large time horizons. We pay particular attention to modeling the uncertainty of our estimates arising from the non-deterministic nature of natural traffic scenes. Our experimental results show that it is indeed possible to predict people trajectories at the desired time horizons and that our uncertainty estimates are informative of the prediction error. We also show that both sequence modeling of trajectories as well as our novel method of long term odometry prediction are essential for best performance.



### Interactive Robot Learning of Gestures, Language and Affordances
- **Arxiv ID**: http://arxiv.org/abs/1711.09055v1
- **DOI**: 10.21437/GLU.2017-17
- **Categories**: **cs.RO**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.09055v1)
- **Published**: 2017-11-24 17:34:32+00:00
- **Updated**: 2017-11-24 17:34:32+00:00
- **Authors**: Giovanni Saponaro, Lorenzo Jamone, Alexandre Bernardino, Giampiero Salvi
- **Comment**: code available at https://github.com/gsaponaro/glu-gestures
- **Journal**: International Workshop on Grounding Language Understanding (GLU),
  Satellite of Interspeech 2017
- **Summary**: A growing field in robotics and Artificial Intelligence (AI) research is human-robot collaboration, whose target is to enable effective teamwork between humans and robots. However, in many situations human teams are still superior to human-robot teams, primarily because human teams can easily agree on a common goal with language, and the individual members observe each other effectively, leveraging their shared motor repertoire and sensorimotor resources. This paper shows that for cognitive robots it is possible, and indeed fruitful, to combine knowledge acquired from interacting with elements of the environment (affordance exploration) with the probabilistic observation of another agent's actions.   We propose a model that unites (i) learning robot affordances and word descriptions with (ii) statistical recognition of human gestures with vision sensors. We discuss theoretical motivations, possible implementations, and we show initial results which highlight that, after having acquired knowledge of its surrounding environment, a humanoid robot can generalize this knowledge to the case when it observes another agent (human partner) performing the same motor actions previously executed during training.



### Distance to Center of Mass Encoding for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.09060v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09060v1)
- **Published**: 2017-11-24 17:53:15+00:00
- **Updated**: 2017-11-24 17:53:15+00:00
- **Authors**: Thomio Watanabe, Denis Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: The instance segmentation can be considered an extension of the object detection problem where bounding boxes are replaced by object contours. Strictly speaking the problem requires to identify each pixel instance and class independently of the artifice used for this mean. The advantage of instance segmentation over the usual object detection lies in the precise delineation of objects improving object localization. Additionally, object contours allow the evaluation of partial occlusion with basic image processing algorithms. This work approaches the instance segmentation problem as an annotation problem and presents a novel technique to encode and decode ground truth annotations. We propose a mathematical representation of instances that any deep semantic segmentation model can learn and generalize. Each individual instance is represented by a center of mass and a field of vectors pointing to it. This encoding technique has been denominated Distance to Center of Mass Encoding (DCME).



### Efficient and Invariant Convolutional Neural Networks for Dense Prediction
- **Arxiv ID**: http://arxiv.org/abs/1711.09064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09064v1)
- **Published**: 2017-11-24 18:19:08+00:00
- **Updated**: 2017-11-24 18:19:08+00:00
- **Authors**: Hongyang Gao, Shuiwang Ji
- **Comment**: 6 pages, ICDM2017
- **Journal**: Gao, Hongyang, and Shuiwang Ji. "Efficient and Invariant
  Convolutional Neural Networks for Dense Prediction." In Data Mining (ICDM),
  2017 IEEE International Conference on, pp. 871-876. IEEE, 2017
- **Summary**: Convolutional neural networks have shown great success on feature extraction from raw input data such as images. Although convolutional neural networks are invariant to translations on the inputs, they are not invariant to other transformations, including rotation and flip. Recent attempts have been made to incorporate more invariance in image recognition applications, but they are not applicable to dense prediction tasks, such as image segmentation. In this paper, we propose a set of methods based on kernel rotation and flip to enable rotation and flip invariance in convolutional neural networks. The kernel rotation can be achieved on kernels of 3 $\times$ 3, while kernel flip can be applied on kernels of any size. By rotating in eight or four angles, the convolutional layers could produce the corresponding number of feature maps based on eight or four different kernels. By using flip, the convolution layer can produce three feature maps. By combining produced feature maps using maxout, the resource requirement could be significantly reduced while still retain the invariance properties. Experimental results demonstrate that the proposed methods can achieve various invariance at reasonable resource requirements in terms of both memory and time.



### Video Enhancement with Task-Oriented Flow
- **Arxiv ID**: http://arxiv.org/abs/1711.09078v3
- **DOI**: 10.1007/s11263-018-01144-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09078v3)
- **Published**: 2017-11-24 18:48:36+00:00
- **Updated**: 2019-11-10 20:06:00+00:00
- **Authors**: Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, William T. Freeman
- **Comment**: IJCV 2019. Project page: http://toflow.csail.mit.edu
- **Journal**: International Journal of Computer Vision (IJCV), 127(8):1106-1125,
  2019
- **Summary**: Many video enhancement algorithms rely on optical flow to register frames in a video sequence. Precise flow estimation is however intractable; and optical flow itself is often a sub-optimal representation for particular video processing tasks. In this paper, we propose task-oriented flow (TOFlow), a motion representation learned in a self-supervised, task-specific manner. We design a neural network with a trainable motion estimation component and a video processing component, and train them jointly to learn the task-oriented flow. For evaluation, we build Vimeo-90K, a large-scale, high-quality video dataset for low-level video processing. TOFlow outperforms traditional optical flow on standard benchmarks as well as our Vimeo-90K dataset in three video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution.



### Deep Extreme Cut: From Extreme Points to Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.09081v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09081v2)
- **Published**: 2017-11-24 18:54:35+00:00
- **Updated**: 2018-03-27 11:47:16+00:00
- **Authors**: Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, Luc Van Gool
- **Comment**: CVPR 2018 camera ready. Project webpage and code:
  http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr/
- **Journal**: None
- **Summary**: This paper explores the use of extreme points in an object (left-most, right-most, top, bottom pixels) as input to obtain precise object segmentation for images and videos. We do so by adding an extra channel to the image in the input of a convolutional neural network (CNN), which contains a Gaussian centered in each of the extreme points. The CNN learns to transform this information into a segmentation of an object that matches those extreme points. We demonstrate the usefulness of this approach for guided segmentation (grabcut-style), interactive segmentation, video object segmentation, and dense segmentation annotation. We show that we obtain the most precise results to date, also with less user input, in an extensive and varied selection of benchmarks and datasets. All our models and code are publicly available on http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr/.



### Cross-Domain Self-supervised Multi-task Feature Learning using Synthetic Imagery
- **Arxiv ID**: http://arxiv.org/abs/1711.09082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09082v1)
- **Published**: 2017-11-24 18:55:01+00:00
- **Updated**: 2017-11-24 18:55:01+00:00
- **Authors**: Zhongzheng Ren, Yong Jae Lee
- **Comment**: None
- **Journal**: None
- **Summary**: In human learning, it is common to use multiple sources of information jointly. However, most existing feature learning approaches learn from only a single task. In this paper, we propose a novel multi-task deep network to learn generalizable high-level visual representations. Since multi-task learning requires annotations for multiple properties of the same training instance, we look to synthetic images to train our network. To overcome the domain difference between real and synthetic data, we employ an unsupervised feature space domain adaptation method based on adversarial learning. Given an input synthetic RGB image, our network simultaneously predicts its surface normal, depth, and instance contour, while also minimizing the feature space domain differences between real and synthetic data. Through extensive experiments, we demonstrate that our network learns more transferable representations compared to single-task baselines. Our learned representation produces state-of-the-art transfer learning results on PASCAL VOC 2007 classification and 2012 detection.



### Geometric robustness of deep networks: analysis and improvement
- **Arxiv ID**: http://arxiv.org/abs/1711.09115v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.09115v1)
- **Published**: 2017-11-24 19:32:57+00:00
- **Updated**: 2017-11-24 19:32:57+00:00
- **Authors**: Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks have been shown to be vulnerable to arbitrary geometric transformations. However, there is no systematic method to measure the invariance properties of deep networks to such transformations. We propose ManiFool as a simple yet scalable algorithm to measure the invariance of deep networks. In particular, our algorithm measures the robustness of deep networks to geometric transformations in a worst-case regime as they can be problematic for sensitive applications. Our extensive experimental results show that ManiFool can be used to measure the invariance of fairly complex networks on high dimensional datasets and these values can be used for analyzing the reasons for it. Furthermore, we build on Manifool to propose a new adversarial training scheme and we show its effectiveness on improving the invariance properties of deep neural networks.



### Appearance-and-Relation Networks for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1711.09125v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09125v2)
- **Published**: 2017-11-24 20:10:26+00:00
- **Updated**: 2018-05-06 13:44:34+00:00
- **Authors**: Limin Wang, Wei Li, Wen Li, Luc Van Gool
- **Comment**: CVPR18 camera-ready version. Code & models available at
  https://github.com/wanglimin/ARTNet
- **Journal**: None
- **Summary**: Spatiotemporal feature learning in videos is a fundamental problem in computer vision. This paper presents a new architecture, termed as Appearance-and-Relation Network (ARTNet), to learn video representation in an end-to-end manner. ARTNets are constructed by stacking multiple generic building blocks, called as SMART, whose goal is to simultaneously model appearance and relation from RGB input in a separate and explicit manner. Specifically, SMART blocks decouple the spatiotemporal learning module into an appearance branch for spatial modeling and a relation branch for temporal modeling. The appearance branch is implemented based on the linear combination of pixels or filter responses in each frame, while the relation branch is designed based on the multiplicative interactions between pixels or filter responses across multiple frames. We perform experiments on three action recognition benchmarks: Kinetics, UCF101, and HMDB51, demonstrating that SMART blocks obtain an evident improvement over 3D convolutions for spatiotemporal feature learning. Under the same training setting, ARTNets achieve superior performance on these three datasets to the existing state-of-the-art methods.



### Convolutional Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1711.09151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09151v1)
- **Published**: 2017-11-24 22:04:14+00:00
- **Updated**: 2017-11-24 22:04:14+00:00
- **Authors**: Jyoti Aneja, Aditya Deshpande, Alexander Schwing
- **Comment**: 11 pages, 9 Figures
- **Journal**: None
- **Summary**: Image captioning is an important but challenging task, applicable to virtual assistants, editing tools, image indexing, and support of the disabled. Its challenges are due to the variability and ambiguity of possible image descriptions. In recent years significant progress has been made in image captioning, using Recurrent Neural Networks powered by long-short-term-memory (LSTM) units. Despite mitigating the vanishing gradient problem, and despite their compelling ability to memorize dependencies, LSTM units are complex and inherently sequential across time. To address this issue, recent work has shown benefits of convolutional networks for machine translation and conditional image generation. Inspired by their success, in this paper, we develop a convolutional image captioning technique. We demonstrate its efficacy on the challenging MSCOCO dataset and demonstrate performance on par with the baseline, while having a faster training time per number of parameters. We also perform a detailed analysis, providing compelling reasons in favor of convolutional language generation approaches.



### Cost-Effective Active Learning for Melanoma Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.09168v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09168v2)
- **Published**: 2017-11-24 23:42:50+00:00
- **Updated**: 2017-11-28 09:17:35+00:00
- **Authors**: Marc Gorriz, Axel Carlier, Emmanuel Faure, Xavier Giro-i-Nieto
- **Comment**: NIPS ML4H 2017 workshop
- **Journal**: None
- **Summary**: We propose a novel Active Learning framework capable to train effectively a convolutional neural network for semantic segmentation of medical imaging, with a limited amount of training labeled data. Our contribution is a practical Cost-Effective Active Learning approach using dropout at test time as Monte Carlo sampling to model the pixel-wise uncertainty and to analyze the image information to improve the training performance. The source code of this project is available at https://marc-gorriz.github.io/CEAL-Medical-Image-Segmentation/ .



