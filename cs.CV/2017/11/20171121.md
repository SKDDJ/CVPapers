# Arxiv Papers in cs.CV on 2017-11-21
### Knowledge Concentration: Learning 100K Object Classifiers in a Single CNN
- **Arxiv ID**: http://arxiv.org/abs/1711.07607v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07607v2)
- **Published**: 2017-11-21 02:38:34+00:00
- **Updated**: 2017-11-23 05:30:30+00:00
- **Authors**: Jiyang Gao, Zijian, Guo, Zhen Li, Ram Nevatia
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained image labels are desirable for many computer vision applications, such as visual search or mobile AI assistant. These applications rely on image classification models that can produce hundreds of thousands (e.g. 100K) of diversified fine-grained image labels on input images. However, training a network at this vocabulary scale is challenging, and suffers from intolerable large model size and slow training speed, which leads to unsatisfying classification performance. A straightforward solution would be training separate expert networks (specialists), with each specialist focusing on learning one specific vertical (e.g. cars, birds...). However, deploying dozens of expert networks in a practical system would significantly increase system complexity and inference latency, and consumes large amounts of computational resources. To address these challenges, we propose a Knowledge Concentration method, which effectively transfers the knowledge from dozens of specialists (multiple teacher networks) into one single model (one student network) to classify 100K object categories. There are three salient aspects in our method: (1) a multi-teacher single-student knowledge distillation framework; (2) a self-paced learning mechanism to allow the student to learn from different teachers at various paces; (3) structurally connected layers to expand the student network capacity with limited extra parameters. We validate our method on OpenImage and a newly collected dataset, Entity-Foto-Tree (EFT), with 100K categories, and show that the proposed model performs significantly better than the baseline generalist model.



### Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.07613v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1711.07613v1)
- **Published**: 2017-11-21 03:11:49+00:00
- **Updated**: 2017-11-21 03:11:49+00:00
- **Authors**: Qi Wu, Peng Wang, Chunhua Shen, Ian Reid, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: The Visual Dialogue task requires an agent to engage in a conversation about an image with a human. It represents an extension of the Visual Question Answering task in that the agent needs to answer a question about an image, but it needs to do so in light of the previous dialogue that has taken place. The key challenge in Visual Dialogue is thus maintaining a consistent, and natural dialogue while continuing to answer questions correctly. We present a novel approach that combines Reinforcement Learning and Generative Adversarial Networks (GANs) to generate more human-like responses to questions. The GAN helps overcome the relative paucity of training data, and the tendency of the typical MLE-based approach to generate overly terse answers. Critically, the GAN is tightly integrated into the attention mechanism that generates human-interpretable reasons for each answer. This means that the discriminative model of the GAN has the task of assessing whether a candidate answer is generated by a human or not, given the provided reason. This is significant because it drives the generative model to produce high quality answers that are well supported by the associated reasoning. The method also generates the state-of-the-art results on the primary benchmark.



### Asking the Difficult Questions: Goal-Oriented Visual Question Generation via Intermediate Rewards
- **Arxiv ID**: http://arxiv.org/abs/1711.07614v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1711.07614v1)
- **Published**: 2017-11-21 03:15:30+00:00
- **Updated**: 2017-11-21 03:15:30+00:00
- **Authors**: Junjie Zhang, Qi Wu, Chunhua Shen, Jian Zhang, Jianfeng Lu, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: Despite significant progress in a variety of vision-and-language problems, developing a method capable of asking intelligent, goal-oriented questions about images is proven to be an inscrutable challenge. Towards this end, we propose a Deep Reinforcement Learning framework based on three new intermediate rewards, namely goal-achieved, progressive and informativeness that encourage the generation of succinct questions, which in turn uncover valuable information towards the overall goal. By directly optimizing for questions that work quickly towards fulfilling the overall goal, we avoid the tendency of existing methods to generate long series of insane queries that add little value. We evaluate our model on the GuessWhat?! dataset and show that the resulting questions can help a standard Guesser identify a specific object in an image at a much higher success rate.



### S4Net: Single Stage Salient-Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.07618v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07618v2)
- **Published**: 2017-11-21 03:47:36+00:00
- **Updated**: 2019-04-10 10:40:28+00:00
- **Authors**: Ruochen Fan, Ming-Ming Cheng, Qibin Hou, Tai-Jiang Mu, Jingdong Wang, Shi-Min Hu
- **Comment**: None
- **Journal**: IEEE CVPR 2019
- **Summary**: We consider an interesting problem-salient instance segmentation in this paper. Other than producing bounding boxes, our network also outputs high-quality instance-level segments. Taking into account the category-independent property of each target, we design a single stage salient instance segmentation framework, with a novel segmentation branch. Our new branch regards not only local context inside each detection window but also its surrounding context, enabling us to distinguish the instances in the same scope even with obstruction. Our network is end-to-end trainable and runs at a fast speed (40 fps when processing an image with resolution 320x320). We evaluate our approach on a publicly available benchmark and show that it outperforms other alternative solutions. We also provide a thorough analysis of the design choices to help readers better understand the functions of each part of our network. The source code can be found at \url{https://github.com/RuochenFan/S4Net}.



### A deep learning-based method for relative location prediction in CT scan images
- **Arxiv ID**: http://arxiv.org/abs/1711.07624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07624v1)
- **Published**: 2017-11-21 04:11:37+00:00
- **Updated**: 2017-11-21 04:11:37+00:00
- **Authors**: Jiajia Guo, Hongwei Du, Bensheng Qiu, Xiao Liang
- **Comment**: Accepted poster at NIPS 2017 Workshop on Machine Learning for Health
  (https://ml4health.github.io/2017/)
- **Journal**: None
- **Summary**: Relative location prediction in computed tomography (CT) scan images is a challenging problem. In this paper, a regression model based on one-dimensional convolutional neural networks is proposed to determine the relative location of a CT scan image both robustly and precisely. A public dataset is employed to validate the performance of the study's proposed method using a 5-fold cross validation. Experimental results demonstrate an excellent performance of the proposed model when compared with the state-of-the-art techniques, achieving a median absolute error of 1.04 cm and mean absolute error of 1.69 cm.



### Fully Convolutional Measurement Network for Compressive Sensing Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1712.01641v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.01641v2)
- **Published**: 2017-11-21 04:27:23+00:00
- **Updated**: 2018-05-29 04:20:25+00:00
- **Authors**: Jiang Du, Xuemei Xie, Chenye Wang, Guangming Shi, Xun Xu, Yuxiang Wang
- **Comment**: Accepted by neurocomputing in 2018
- **Journal**: None
- **Summary**: Recently, deep learning methods have made a significant improvement in compressive sensing image reconstruction task. In the existing methods, the scene is measured block by block due to the high computational complexity. This results in block-effect of the recovered images. In this paper, we propose a fully convolutional measurement network, where the scene is measured as a whole. The proposed method powerfully removes the block-effect since the structure information of scene images is preserved. To make the measure more flexible, the measurement and the recovery parts are jointly trained. From the experiments, it is shown that the results by the proposed method outperforms those by the existing methods in PSNR, SSIM, and visual effect.



### Multi-Image Semantic Matching by Mining Consistent Features
- **Arxiv ID**: http://arxiv.org/abs/1711.07641v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07641v2)
- **Published**: 2017-11-21 06:17:39+00:00
- **Updated**: 2018-05-01 08:06:25+00:00
- **Authors**: Qianqian Wang, Xiaowei Zhou, Kostas Daniilidis
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: This work proposes a multi-image matching method to estimate semantic correspondences across multiple images. In contrast to the previous methods that optimize all pairwise correspondences, the proposed method identifies and matches only a sparse set of reliable features in the image collection. In this way, the proposed method is able to prune nonrepeatable features and also highly scalable to handle thousands of images. We additionally propose a low-rank constraint to ensure the geometric consistency of feature correspondences over the whole image collection. Besides the competitive performance on multi-graph matching and semantic flow benchmarks, we also demonstrate the applicability of the proposed method for reconstructing object-class models and discovering object-class landmarks from images without using any annotation.



### Proximal Alternating Direction Network: A Globally Converged Deep Unrolling Framework
- **Arxiv ID**: http://arxiv.org/abs/1711.07653v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07653v2)
- **Published**: 2017-11-21 07:21:53+00:00
- **Updated**: 2017-12-15 11:32:21+00:00
- **Authors**: Risheng Liu, Xin Fan, Shichao Cheng, Xiangyu Wang, Zhongxuan Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have gained great success in many real-world applications. However, most existing networks are typically designed in heuristic manners, thus lack of rigorous mathematical principles and derivations. Several recent studies build deep structures by unrolling a particular optimization model that involves task information. Unfortunately, due to the dynamic nature of network parameters, their resultant deep propagation networks do \emph{not} possess the nice convergence property as the original optimization scheme does. This paper provides a novel proximal unrolling framework to establish deep models by integrating experimentally verified network architectures and rich cues of the tasks. More importantly, we \emph{prove in theory} that 1) the propagation generated by our unrolled deep model globally converges to a critical-point of a given variational energy, and 2) the proposed framework is still able to learn priors from training data to generate a convergent propagation even when task information is only partially available. Indeed, these theoretical results are the best we can ask for, unless stronger assumptions are enforced. Extensive experiments on various real-world applications verify the theoretical convergence and demonstrate the effectiveness of designed deep models.



### Design Automation for Binarized Neural Networks: A Quantum Leap Opportunity?
- **Arxiv ID**: http://arxiv.org/abs/1712.01743v1
- **DOI**: None
- **Categories**: **cs.OH**, cs.AR, cs.CV, cs.NE, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1712.01743v1)
- **Published**: 2017-11-21 09:54:37+00:00
- **Updated**: 2017-11-21 09:54:37+00:00
- **Authors**: Manuele Rusci, Lukas Cavigelli, Luca Benini
- **Comment**: None
- **Journal**: None
- **Summary**: Design automation in general, and in particular logic synthesis, can play a key role in enabling the design of application-specific Binarized Neural Networks (BNN). This paper presents the hardware design and synthesis of a purely combinational BNN for ultra-low power near-sensor processing. We leverage the major opportunities raised by BNN models, which consist mostly of logical bit-wise operations and integer counting and comparisons, for pushing ultra-low power deep learning circuits close to the sensor and coupling it with binarized mixed-signal image sensor data. We analyze area, power and energy metrics of BNNs synthesized as combinational networks. Our synthesis results in GlobalFoundries 22nm SOI technology shows a silicon area of 2.61mm2 for implementing a combinational BNN with 32x32 binary input sensor receptive field and weight parameters fixed at design time. This is 2.2x smaller than a synthesized network with re-configurable parameters. With respect to other comparable techniques for deep learning near-sensor processing, our approach features a 10x higher energy efficiency.



### Fully Convolutional Neural Networks for Page Segmentation of Historical Document Images
- **Arxiv ID**: http://arxiv.org/abs/1711.07695v2
- **DOI**: 10.1109/DAS.2018.39
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07695v2)
- **Published**: 2017-11-21 10:02:34+00:00
- **Updated**: 2018-02-15 12:52:33+00:00
- **Authors**: Christoph Wick, Frank Puppe
- **Comment**: 6 pages, 7 figures, conference
- **Journal**: None
- **Summary**: We propose a high-performance fully convolutional neural network (FCN) for historical document segmentation that is designed to process a single page in one step. The advantage of this model beside its speed is its ability to directly learn from raw pixels instead of using preprocessing steps e. g. feature computation or superpixel generation. We show that this network yields better results than existing methods on different public data sets. For evaluation of this model we introduce a novel metric that is independent of ambiguous ground truth called Foreground Pixel Accuracy (FgPA). This pixel based measure only counts foreground pixels in the binarized page, any background pixel is omitted. The major advantage of this metric is, that it enables researchers to compare different segmentation methods on their ability to successfully segment text or pictures and not on their ability to learn and possibly overfit the peculiarities of an ambiguous hand-made ground truth segmentation.



### Residual Parameter Transfer for Deep Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1711.07714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07714v1)
- **Published**: 2017-11-21 11:03:55+00:00
- **Updated**: 2017-11-21 11:03:55+00:00
- **Authors**: Artem Rozantsev, Mathieu Salzmann, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of Deep Domain Adaptation is to make it possible to use Deep Nets trained in one domain where there is enough annotated training data in another where there is little or none. Most current approaches have focused on learning feature representations that are invariant to the changes that occur when going from one domain to the other, which means using the same network parameters in both domains. While some recent algorithms explicitly model the changes by adapting the network parameters, they either severely restrict the possible domain changes, or significantly increase the number of model parameters.   By contrast, we introduce a network architecture that includes auxiliary residual networks, which we train to predict the parameters in the domain with little annotated data from those in the other one. This architecture enables us to flexibly preserve the similarities between domains where they exist and model the differences when necessary. We demonstrate that our approach yields higher accuracy than state-of-the-art methods without undue complexity.



### Total Variation-Based Dense Depth from Multi-Camera Array
- **Arxiv ID**: http://arxiv.org/abs/1711.07719v1
- **DOI**: 10.1117/1.OE.57.6.063105
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07719v1)
- **Published**: 2017-11-21 11:16:21+00:00
- **Updated**: 2017-11-21 11:16:21+00:00
- **Authors**: Hossein Javidnia, Peter Corcoran
- **Comment**: 21 pages, 13 figures
- **Journal**: Optical Engineering, 57(6), 063105 (2018)
- **Summary**: Multi-Camera arrays are increasingly employed in both consumer and industrial applications, and various passive techniques are documented to estimate depth from such camera arrays. Current depth estimation methods provide useful estimations of depth in an imaged scene but are often impractical due to significant computational requirements. This paper presents a novel framework that generates a high-quality continuous depth map from multi-camera array/light field cameras. The proposed framework utilizes analysis of the local Epipolar Plane Image (EPI) to initiate the depth estimation process. The estimated depth map is then processed using Total Variation (TV) minimization based on the Fenchel-Rockafellar duality. Evaluation of this method based on a well-known benchmark indicates that the proposed framework performs well in terms of accuracy when compared to the top-ranked depth estimation methods and a baseline algorithm. The test dataset includes both photorealistic and non-photorealistic scenes. Notably, the computational requirements required to achieve an equivalent accuracy are significantly reduced when compared to the top algorithms. As a consequence, the proposed framework is suitable for deployment in consumer and industrial applications.



### The Application of Preconditioned Alternating Direction Method of Multipliers in Depth from Focal Stack
- **Arxiv ID**: http://arxiv.org/abs/1711.07721v1
- **DOI**: 10.1117/1.JEI.27.2.023019
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07721v1)
- **Published**: 2017-11-21 11:22:55+00:00
- **Updated**: 2017-11-21 11:22:55+00:00
- **Authors**: Hossein Javidnia, Peter Corcoran
- **Comment**: 15 pages, 8 figures
- **Journal**: J. of Electronic Imaging, 27(2), 023019 (2018)
- **Summary**: Post capture refocusing effect in smartphone cameras is achievable by using focal stacks. However, the accuracy of this effect is totally dependent on the combination of the depth layers in the stack. The accuracy of the extended depth of field effect in this application can be improved significantly by computing an accurate depth map which has been an open issue for decades. To tackle this issue, in this paper, a framework is proposed based on Preconditioned Alternating Direction Method of Multipliers (PADMM) for depth from the focal stack and synthetic defocus application. In addition to its ability to provide high structural accuracy and occlusion handling, the optimization function of the proposed method can, in fact, converge faster and better than state of the art methods. The evaluation has been done on 21 sets of focal stacks and the optimization function has been compared against 5 other methods. Preliminary results indicate that the proposed method has a better performance in terms of structural accuracy and optimization in comparison to the current state of the art methods.



### Repulsion Loss: Detecting Pedestrians in a Crowd
- **Arxiv ID**: http://arxiv.org/abs/1711.07752v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07752v2)
- **Published**: 2017-11-21 12:38:41+00:00
- **Updated**: 2018-03-26 06:31:48+00:00
- **Authors**: Xinlong Wang, Tete Xiao, Yuning Jiang, Shuai Shao, Jian Sun, Chunhua Shen
- **Comment**: Accepted to IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2018
- **Journal**: None
- **Summary**: Detecting individual pedestrians in a crowd remains a challenging problem since the pedestrians often gather together and occlude each other in real-world scenarios. In this paper, we first explore how a state-of-the-art pedestrian detector is harmed by crowd occlusion via experimentation, providing insights into the crowd occlusion problem. Then, we propose a novel bounding box regression loss specifically designed for crowd scenes, termed repulsion loss. This loss is driven by two motivations: the attraction by target, and the repulsion by other surrounding objects. The repulsion term prevents the proposal from shifting to surrounding objects thus leading to more crowd-robust localization. Our detector trained by repulsion loss outperforms all the state-of-the-art methods with a significant improvement in occlusion cases.



### Receptive Field Block Net for Accurate and Fast Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1711.07767v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07767v3)
- **Published**: 2017-11-21 13:18:26+00:00
- **Updated**: 2018-07-26 08:37:02+00:00
- **Authors**: Songtao Liu, Di Huang, Yunhong Wang
- **Comment**: Accepted by ECCV 2018
- **Journal**: None
- **Summary**: Current top-performing object detectors depend on deep CNN backbones, such as ResNet-101 and Inception, benefiting from their powerful feature representations but suffering from high computational costs. Conversely, some lightweight model based detectors fulfil real time processing, while their accuracies are often criticized. In this paper, we explore an alternative to build a fast and accurate detector by strengthening lightweight features using a hand-crafted mechanism. Inspired by the structure of Receptive Fields (RFs) in human visual systems, we propose a novel RF Block (RFB) module, which takes the relationship between the size and eccentricity of RFs into account, to enhance the feature discriminability and robustness. We further assemble RFB to the top of SSD, constructing the RFB Net detector. To evaluate its effectiveness, experiments are conducted on two major benchmarks and the results show that RFB Net is able to reach the performance of advanced very deep detectors while keeping the real-time speed. Code is available at https://github.com/ruinmessi/RFBNet.



### Efficient Multi-Person Pose Estimation with Provable Guarantees
- **Arxiv ID**: http://arxiv.org/abs/1711.07794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07794v1)
- **Published**: 2017-11-21 14:13:41+00:00
- **Updated**: 2017-11-21 14:13:41+00:00
- **Authors**: Shaofei Wang, Konrad Paul Kording, Julian Yarkony
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-person pose estimation (MPPE) in natural images is key to the meaningful use of visual data in many fields including movement science, security, and rehabilitation. In this paper we tackle MPPE with a bottom-up approach, starting with candidate detections of body parts from a convolutional neural network (CNN) and grouping them into people. We formulate the grouping of body part detections into people as a minimum-weight set packing (MWSP) problem where the set of potential people is the power set of body part detections. We model the quality of a hypothesis of a person which is a set in the MWSP by an augmented tree-structured Markov random field where variables correspond to body-parts and their state-spaces correspond to the power set of the detections for that part.   We describe a novel algorithm that combines efficiency with provable bounds on this MWSP problem. We employ an implicit column generation strategy where the pricing problem is formulated as a dynamic program. To efficiently solve this dynamic program we exploit the problem structure utilizing a nested Bender's decomposition (NBD) exact inference strategy which we speed up by recycling Bender's rows between calls to the pricing problem.   We test our approach on the MPII-Multiperson dataset, showing that our approach obtains comparable results with the state-of-the-art algorithm for joint node labeling and grouping problems, and that NBD achieves considerable speed-ups relative to a naive dynamic programming approach. Typical algorithms that solve joint node labeling and grouping problems use heuristics and thus can not obtain proofs of optimality. Our approach, in contrast, proves that for over 99 percent of problem instances we find the globally optimal solution and otherwise provide upper/lower bounds.



### Visual and Textual Sentiment Analysis Using Deep Fusion Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.07798v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1711.07798v1)
- **Published**: 2017-11-21 14:19:48+00:00
- **Updated**: 2017-11-21 14:19:48+00:00
- **Authors**: Xingyue Chen, Yunhong Wang, Qingjie Liu
- **Comment**: Accepted as oral presentation by ICIP2017
- **Journal**: None
- **Summary**: Sentiment analysis is attracting more and more attentions and has become a very hot research topic due to its potential applications in personalized recommendation, opinion mining, etc. Most of the existing methods are based on either textual or visual data and can not achieve satisfactory results, as it is very hard to extract sufficient information from only one single modality data. Inspired by the observation that there exists strong semantic correlation between visual and textual data in social medias, we propose an end-to-end deep fusion convolutional neural network to jointly learn textual and visual sentiment representations from training examples. The two modality information are fused together in a pooling layer and fed into fully-connected layers to predict the sentiment polarity. We evaluate the proposed approach on two widely used data sets. Results show that our method achieves promising result compared with the state-of-the-art methods which clearly demonstrate its competency.



### Universal Denoising Networks : A Novel CNN Architecture for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1711.07807v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07807v2)
- **Published**: 2017-11-21 14:36:14+00:00
- **Updated**: 2018-03-24 18:24:08+00:00
- **Authors**: Stamatios Lefkimmiatis
- **Comment**: Camera ready paper to appear in the Proceedings of CVPR 2018
- **Journal**: None
- **Summary**: We design a novel network architecture for learning discriminative image models that are employed to efficiently tackle the problem of grayscale and color image denoising. Based on the proposed architecture, we introduce two different variants. The first network involves convolutional layers as a core component, while the second one relies instead on non-local filtering layers and thus it is able to exploit the inherent non-local self-similarity property of natural images. As opposed to most of the existing deep network approaches, which require the training of a specific model for each considered noise level, the proposed models are able to handle a wide range of noise levels using a single set of learned parameters, while they are very robust when the noise degrading the latent image does not match the statistics of the noise used during training. The latter argument is supported by results that we report on publicly available images corrupted by unknown noise and which we compare against solutions obtained by competing methods. At the same time the introduced networks achieve excellent results under additive white Gaussian noise (AWGN), which are comparable to those of the current state-of-the-art network, while they depend on a more shallow architecture with the number of trained parameters being one order of magnitude smaller. These properties make the proposed networks ideal candidates to serve as sub-solvers on restoration methods that deal with general inverse imaging problems such as deblurring, demosaicking, superresolution, etc.



### Efficient Implementation of a Recognition System Using the Cortex Ventral Stream Model
- **Arxiv ID**: http://arxiv.org/abs/1711.07827v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07827v1)
- **Published**: 2017-11-21 15:03:04+00:00
- **Updated**: 2017-11-21 15:03:04+00:00
- **Authors**: Ahmad W. Bitar, Mohammad M. Mansour, Ali Chehab
- **Comment**: 10 pages
- **Journal**: In Proceedings of the 10th International Conference on Computer
  Vision Theory and Applications (VISIGRAPP 2015)
- **Summary**: In this paper, an efficient implementation for a recognition system based on the original HMAX model of the visual cortex is proposed. Various optimizations targeted to increase accuracy at the so-called layers S1, C1, and S2 of the HMAX model are proposed. At layer S1, all unimportant information such as illumination and expression variations are eliminated from the images. Each image is then convolved with 64 separable Gabor filters in the spatial domain. At layer C1, the minimum scales values are exploited to be embedded into the maximum ones using the additive embedding space. At layer S2, the prototypes are generated in a more efficient way using Partitioning Around Medoid (PAM) clustering algorithm. The impact of these optimizations in terms of accuracy and computational complexity was evaluated on the Caltech101 database, and compared with the baseline performance using support vector machine (SVM) and nearest neighbor (NN) classifiers. The results show that our model provides significant improvement in accuracy at the S1 layer by more than 10% where the computational complexity is also reduced. The accuracy is slightly increased for both approximations at the C1 and S2 layers.



### A smartphone application to measure the quality of pest control spraying machines via image analysis
- **Arxiv ID**: http://arxiv.org/abs/1711.07828v3
- **DOI**: 10.1145/3167132.3167237
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07828v3)
- **Published**: 2017-11-21 15:04:07+00:00
- **Updated**: 2017-12-16 20:50:15+00:00
- **Authors**: Bruno B. Machado, Gabriel Spadon, Mauro S. Arruda, Wesley N. Goncalves, Andre C. P. L. F. Carvalho, Jose F. Rodrigues-Jr
- **Comment**: Paper to be published on the 33rd ACM/SIGAPP Symposium On Applied
  Computing (SAC), 2018
- **Journal**: None
- **Summary**: The need for higher agricultural productivity has demanded the intensive use of pesticides. However, their correct use depends on assessment methods that can accurately predict how well the pesticides' spraying covered the intended crop region. Some methods have been proposed in the literature, but their high cost and low portability harm their widespread use. This paper proposes and experimentally evaluates a new methodology based on the use of a smartphone-based mobile application, named DropLeaf. Experiments performed using DropLeaf showed that, in addition to its versatility, it can predict with high accuracy the pesticide spraying. DropLeaf is a five-fold image-processing methodology based on: (i) color space conversion, (ii) threshold noise removal, (iii) convolutional operations of dilation and erosion, (iv) detection of contour markers in the water-sensitive card, and, (v) identification of droplets via the marker-controlled watershed transformation. The authors performed successful experiments over two case studies, the first using a set of synthetic cards and the second using a real-world crop. The proposed tool can be broadly used by farmers equipped with conventional mobile phones, improving the use of pesticides with health, environmental and financial benefits.



### Discussion among Different Methods of Updating Model Filter in Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1711.07829v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07829v3)
- **Published**: 2017-11-21 15:09:29+00:00
- **Updated**: 2020-11-15 13:26:12+00:00
- **Authors**: Taihang Dong, Sheng Zhong
- **Comment**: 8 pages, 3 figures, SPIE 10th International Symposium on
  Multispectral Image Processing and Pattern Recognition
- **Journal**: None
- **Summary**: Discriminative correlation filters (DCF) have recently shown excellent performance in visual object tracking area. In this paper, we summarize the methods of updating model filter from discriminative correlation filter (DCF) based tracking algorithms and analyzes similarities and differences among these methods. We deduce the relationship between updating coefficient in high dimension (kernel trick), updating filter in frequency domain and updating filter in spatial domain, and analyze the difference among these different ways. We also analyze the difference between the updating filter directly and updating filter's numerator (object response power) with updating filter's denominator (filter's power). The experiments about comparing different updating methods and visualizing the template filters are used to prove our derivation.



### Robust Object Tracking Based on Self-adaptive Search Area
- **Arxiv ID**: http://arxiv.org/abs/1711.07835v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07835v3)
- **Published**: 2017-11-21 15:15:11+00:00
- **Updated**: 2020-11-15 13:06:43+00:00
- **Authors**: Taihang Dong, Sheng Zhong
- **Comment**: 10 pages, 4 figures, 3 tables, SPIE 10th International Symposium on
  Multispectral Image Processing and Pattern Recognition
- **Journal**: None
- **Summary**: Discriminative correlation filter (DCF) based trackers have recently achieved excellent performance with great computational efficiency. However, DCF based trackers suffer boundary effects, which result in unstable performance in challenging situations exhibiting fast motion. In this paper, we propose a novel method to mitigate this side-effect in DCF based trackers. We change the search area according to the prediction of target motion. When the object moves fast, broad search area could alleviate boundary effects and reserve the probability of locating the object. When the object moves slowly, narrow search area could prevent effect of useless background information and improve computational efficiency to attain real-time performance. This strategy can impressively soothe boundary effects in situations exhibiting fast motion and motion blur, and it can be used in almost all DCF based trackers. The experiments on OTB benchmark show that the proposed framework improves the performance compared with the baseline trackers.



### UnFlow: Unsupervised Learning of Optical Flow with a Bidirectional Census Loss
- **Arxiv ID**: http://arxiv.org/abs/1711.07837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07837v1)
- **Published**: 2017-11-21 15:19:26+00:00
- **Updated**: 2017-11-21 15:19:26+00:00
- **Authors**: Simon Meister, Junhwa Hur, Stefan Roth
- **Comment**: 9 pages, To appear in AAAI 2018
- **Journal**: None
- **Summary**: In the era of end-to-end deep learning, many advances in computer vision are driven by large amounts of labeled data. In the optical flow setting, however, obtaining dense per-pixel ground truth for real scenes is difficult and thus such data is rare. Therefore, recent end-to-end convolutional networks for optical flow rely on synthetic datasets for supervision, but the domain mismatch between training and test scenarios continues to be a challenge. Inspired by classical energy-based optical flow methods, we design an unsupervised loss based on occlusion-aware bidirectional flow estimation and the robust census transform to circumvent the need for ground truth flow. On the KITTI benchmarks, our unsupervised approach outperforms previous unsupervised deep networks by a large margin, and is even more accurate than similar supervised methods trained on synthetic datasets alone. By optionally fine-tuning on the KITTI training data, our method achieves competitive optical flow accuracy on the KITTI 2012 and 2015 benchmarks, thus in addition enabling generic pre-training of supervised networks for datasets with limited amounts of ground truth.



### Functional Map of the World
- **Arxiv ID**: http://arxiv.org/abs/1711.07846v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07846v3)
- **Published**: 2017-11-21 15:28:00+00:00
- **Updated**: 2018-04-13 19:03:50+00:00
- **Authors**: Gordon Christie, Neil Fendley, James Wilson, Ryan Mukherjee
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: We present a new dataset, Functional Map of the World (fMoW), which aims to inspire the development of machine learning models capable of predicting the functional purpose of buildings and land use from temporal sequences of satellite images and a rich set of metadata features. The metadata provided with each image enables reasoning about location, time, sun angles, physical sizes, and other features when making predictions about objects in the image. Our dataset consists of over 1 million images from over 200 countries. For each image, we provide at least one bounding box annotation containing one of 63 categories, including a "false detection" category. We present an analysis of the dataset along with baseline approaches that reason about metadata and temporal views. Our data, code, and pretrained models have been made publicly available.



### Autoencoder Node Saliency: Selecting Relevant Latent Representations
- **Arxiv ID**: http://arxiv.org/abs/1711.07871v2
- **DOI**: 10.1016/j.patcog.2018.12.015
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.07871v2)
- **Published**: 2017-11-21 16:17:14+00:00
- **Updated**: 2018-03-08 02:09:31+00:00
- **Authors**: Ya Ju Fan
- **Comment**: None
- **Journal**: Pattern Recognition, Volume 88, 2019, Pages 643-653
- **Summary**: The autoencoder is an artificial neural network model that learns hidden representations of unlabeled data. With a linear transfer function it is similar to the principal component analysis (PCA). While both methods use weight vectors for linear transformations, the autoencoder does not come with any indication similar to the eigenvalues in PCA that are paired with the eigenvectors. We propose a novel supervised node saliency (SNS) method that ranks the hidden nodes by comparing class distributions of latent representations against a fixed reference distribution. The latent representations of a hidden node can be described using a one-dimensional histogram. We apply normalized entropy difference (NED) to measure the "interestingness" of the histograms, and conclude a property for NED values to identify a good classifying node. By applying our methods to real data sets, we demonstrate the ability of SNS to explain what the trained autoencoders have learned.



### SilNet : Single- and Multi-View Reconstruction by Learning from Silhouettes
- **Arxiv ID**: http://arxiv.org/abs/1711.07888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07888v1)
- **Published**: 2017-11-21 16:33:18+00:00
- **Updated**: 2017-11-21 16:33:18+00:00
- **Authors**: Olivia Wiles, Andrew Zisserman
- **Comment**: BMVC 2017; Best Poster
- **Journal**: None
- **Summary**: The objective of this paper is 3D shape understanding from single and multiple images. To this end, we introduce a new deep-learning architecture and loss function, SilNet, that can handle multiple views in an order-agnostic manner. The architecture is fully convolutional, and for training we use a proxy task of silhouette prediction, rather than directly learning a mapping from 2D images to 3D shape as has been the target in most recent work.   We demonstrate that with the SilNet architecture there is generalisation over the number of views -- for example, SilNet trained on 2 views can be used with 3 or 4 views at test-time; and performance improves with more views.   We introduce two new synthetics datasets: a blobby object dataset useful for pre-training, and a challenging and realistic sculpture dataset; and demonstrate on these datasets that SilNet has indeed learnt 3D shape. Finally, we show that SilNet exceeds the state of the art on the ShapeNet benchmark dataset, and use SilNet to generate novel views of the sculpture dataset.



### Aperture Supervision for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1711.07933v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07933v2)
- **Published**: 2017-11-21 17:39:51+00:00
- **Updated**: 2018-03-29 17:26:00+00:00
- **Authors**: Pratul P. Srinivasan, Rahul Garg, Neal Wadhwa, Ren Ng, Jonathan T. Barron
- **Comment**: To appear at CVPR 2018 (updated to camera ready version)
- **Journal**: None
- **Summary**: We present a novel method to train machine learning algorithms to estimate scene depths from a single image, by using the information provided by a camera's aperture as supervision. Prior works use a depth sensor's outputs or images of the same scene from alternate viewpoints as supervision, while our method instead uses images from the same viewpoint taken with a varying camera aperture. To enable learning algorithms to use aperture effects as supervision, we introduce two differentiable aperture rendering functions that use the input image and predicted depths to simulate the depth-of-field effects caused by real camera apertures. We train a monocular depth estimation network end-to-end to predict the scene depths that best explain these finite aperture images as defocus-blurred renderings of the input all-in-focus image.



### Non-local Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.07971v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07971v3)
- **Published**: 2017-11-21 18:51:16+00:00
- **Updated**: 2018-04-13 06:40:44+00:00
- **Authors**: Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He
- **Comment**: CVPR 2018, code is available at:
  https://github.com/facebookresearch/video-nonlocal-net
- **Journal**: None
- **Summary**: Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code is available at https://github.com/facebookresearch/video-nonlocal-net .



### WAYLA - Generating Images from Eye Movements
- **Arxiv ID**: http://arxiv.org/abs/1711.07974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07974v1)
- **Published**: 2017-11-21 18:53:31+00:00
- **Updated**: 2017-11-21 18:53:31+00:00
- **Authors**: Bingqing Yu, James J. Clark
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for reconstructing images viewed by observers based only on their eye movements. By exploring the relationships between gaze patterns and image stimuli, the "What Are You Looking At?" (WAYLA) system learns to synthesize photo-realistic images that are similar to the original pictures being viewed. The WAYLA approach is based on the Conditional Generative Adversarial Network (Conditional GAN) image-to-image translation technique of Isola et al. We consider two specific applications - the first, of reconstructing newspaper images from gaze heat maps, and the second, of detailed reconstruction of images containing only text. The newspaper image reconstruction process is divided into two image-to-image translation operations, the first mapping gaze heat maps into image segmentations, and the second mapping the generated segmentation into a newspaper image. We validate the performance of our approach using various evaluation metrics, along with human visual inspection. All results confirm the ability of our network to perform image generation tasks using eye tracking data.



### Generating Analytic Insights on Human Behaviour using Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1711.07992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07992v1)
- **Published**: 2017-11-21 19:00:32+00:00
- **Updated**: 2017-11-21 19:00:32+00:00
- **Authors**: Namit Juneja, Rajesh Kumar Muthu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a method to track human figures in physical spaces and then utilizes this data to generate several data points such as footfall distribution, demographic analysis,heat maps as well as gender distribution. The proposed framework aims to establish this while utilizing minimum computational resources while remaining real time. It is often useful to have information such as what kind of people visit a certain place or what hour of the day experiences maximum activity, Such analysis can be used improve sales, manage huge number of people as well as predict future behaviour. The proposed framework is designed in a way such that it can take input streams from IP cameras and use that to generate relevant data points using open source tools such as OpenCV and raspberryPi.



### Deep Sparse Coding for Invariant Multimodal Halle Berry Neurons
- **Arxiv ID**: http://arxiv.org/abs/1711.07998v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07998v2)
- **Published**: 2017-11-21 19:06:04+00:00
- **Updated**: 2018-06-12 21:30:32+00:00
- **Authors**: Edward Kim, Darryl Hannan, Garrett Kenyon
- **Comment**: None
- **Journal**: None
- **Summary**: Deep feed-forward convolutional neural networks (CNNs) have become ubiquitous in virtually all machine learning and computer vision challenges; however, advancements in CNNs have arguably reached an engineering saturation point where incremental novelty results in minor performance gains. Although there is evidence that object classification has reached human levels on narrowly defined tasks, for general applications, the biological visual system is far superior to that of any computer. Research reveals there are numerous missing components in feed-forward deep neural networks that are critical in mammalian vision. The brain does not work solely in a feed-forward fashion, but rather all of the neurons are in competition with each other; neurons are integrating information in a bottom up and top down fashion and incorporating expectation and feedback in the modeling process. Furthermore, our visual cortex is working in tandem with our parietal lobe, integrating sensory information from various modalities.   In our work, we sought to improve upon the standard feed-forward deep learning model by augmenting them with biologically inspired concepts of sparsity, top-down feedback, and lateral inhibition. We define our model as a sparse coding problem using hierarchical layers. We solve the sparse coding problem with an additional top-down feedback error driving the dynamics of the neural network. While building and observing the behavior of our model, we were fascinated that multimodal, invariant neurons naturally emerged that mimicked, "Halle Berry neurons" found in the human brain. Furthermore, our sparse representation of multimodal signals demonstrates qualitative and quantitative superiority to the standard feed-forward joint embedding in common vision and machine learning tasks.



### Dynamic High Resolution Deformable Articulated Tracking
- **Arxiv ID**: http://arxiv.org/abs/1711.07999v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07999v1)
- **Published**: 2017-11-21 19:07:30+00:00
- **Updated**: 2017-11-21 19:07:30+00:00
- **Authors**: Aaron Walsman, Weilin Wan, Tanner Schmidt, Dieter Fox
- **Comment**: 10 pages, 8 figures, Presented at 3DV 2017
- **Journal**: None
- **Summary**: The last several years have seen significant progress in using depth cameras for tracking articulated objects such as human bodies, hands, and robotic manipulators. Most approaches focus on tracking skeletal parameters of a fixed shape model, which makes them insufficient for applications that require accurate estimates of deformable object surfaces. To overcome this limitation, we present a 3D model-based tracking system for articulated deformable objects. Our system is able to track human body pose and high resolution surface contours in real time using a commodity depth sensor and GPU hardware. We implement this as a joint optimization over a skeleton to account for changes in pose, and over the vertices of a high resolution mesh to track the subject's shape. Through experimental results we show that we are able to capture dynamic sub-centimeter surface detail such as folds and wrinkles in clothing. We also show that this shape estimation aids kinematic pose estimation by providing a more accurate target to match against the point cloud. The end result is highly accurate spatiotemporal and semantic information which is well suited for physical human robot interaction as well as virtual and augmented reality systems.



### Personalization of Saliency Estimation
- **Arxiv ID**: http://arxiv.org/abs/1711.08000v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08000v1)
- **Published**: 2017-11-21 19:10:44+00:00
- **Updated**: 2017-11-21 19:10:44+00:00
- **Authors**: Bingqing Yu, James J. Clark
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing saliency models use low-level features or task descriptions when generating attention predictions. However, the link between observer characteristics and gaze patterns is rarely investigated. We present a novel saliency prediction technique which takes viewers' identities and personal traits into consideration when modeling human attention. Instead of only computing image salience for average observers, we consider the interpersonal variation in the viewing behaviors of observers with different personal traits and backgrounds. We present an enriched derivative of the GAN network, which is able to generate personalized saliency predictions when fed with image stimuli and specific information about the observer. Our model contains a generator which generates grayscale saliency heat maps based on the image and an observer label. The generator is paired with an adversarial discriminator which learns to distinguish generated salience from ground truth salience. The discriminator also has the observer label as an input, which contributes to the personalization ability of our approach. We evaluate the performance of our personalized salience model by comparison with a benchmark model along with other un-personalized predictions, and illustrate improvements in prediction accuracy for all tested observer groups.



### Relating Input Concepts to Convolutional Neural Network Decisions
- **Arxiv ID**: http://arxiv.org/abs/1711.08006v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, I.2.10; I.4.m
- **Links**: [PDF](http://arxiv.org/pdf/1711.08006v1)
- **Published**: 2017-11-21 19:37:13+00:00
- **Updated**: 2017-11-21 19:37:13+00:00
- **Authors**: Ning Xie, Md Kamruzzaman Sarker, Derek Doran, Pascal Hitzler, Michael Raymer
- **Comment**: 10 pages (including references), 9 figures, paper accepted by NIPS
  IEVDL 2017
- **Journal**: None
- **Summary**: Many current methods to interpret convolutional neural networks (CNNs) use visualization techniques and words to highlight concepts of the input seemingly relevant to a CNN's decision. The methods hypothesize that the recognition of these concepts are instrumental in the decision a CNN reaches, but the nature of this relationship has not been well explored. To address this gap, this paper examines the quality of a concept's recognition by a CNN and the degree to which the recognitions are associated with CNN decisions. The study considers a CNN trained for scene recognition over the ADE20k dataset. It uses a novel approach to find and score the strength of minimally distributed representations of input concepts (defined by objects in scene images) across late stage feature maps. Subsequent analysis finds evidence that concept recognition impacts decision making. Strong recognition of concepts frequently-occurring in few scenes are indicative of correct decisions, but recognizing concepts common to many scenes may mislead the network.



### The Riemannian Geometry of Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1711.08014v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.08014v1)
- **Published**: 2017-11-21 19:59:24+00:00
- **Updated**: 2017-11-21 19:59:24+00:00
- **Authors**: Hang Shao, Abhishek Kumar, P. Thomas Fletcher
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Deep generative models learn a mapping from a low dimensional latent space to a high-dimensional data space. Under certain regularity conditions, these models parameterize nonlinear manifolds in the data space. In this paper, we investigate the Riemannian geometry of these generated manifolds. First, we develop efficient algorithms for computing geodesic curves, which provide an intrinsic notion of distance between points on the manifold. Second, we develop an algorithm for parallel translation of a tangent vector along a path on the manifold. We show how parallel translation can be used to generate analogies, i.e., to transport a change in one data point into a semantically similar change of another data point. Our experiments on real image data show that the manifolds learned by deep generative models, while nonlinear, are surprisingly close to zero curvature. The practical implication is that linear paths in the latent space closely approximate geodesics on the generated manifold. However, further investigation into this phenomenon is warranted, to identify if there are other architectures or datasets where curvature plays a more prominent role. We believe that exploring the Riemannian geometry of deep generative models, using the tools developed in this paper, will be an important step in understanding the high-dimensional, nonlinear spaces these models learn.



### Identifying Most Walkable Direction for Navigation in an Outdoor Environment
- **Arxiv ID**: http://arxiv.org/abs/1711.08040v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08040v2)
- **Published**: 2017-11-21 21:15:33+00:00
- **Updated**: 2017-12-01 03:18:52+00:00
- **Authors**: Sachin Mehta, Hannaneh Hajishirzi, Linda Shapiro
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach for identifying the most walkable direction for navigation using a hand-held camera. Our approach extracts semantically rich contextual information from the scene using a custom encoder-decoder architecture for semantic segmentation and models the spatial and temporal behavior of objects in the scene using a spatio-temporal graph. The system learns to minimize a cost function over the spatial and temporal object attributes to identify the most walkable direction. We construct a new annotated navigation dataset collected using a hand-held mobile camera in an unconstrained outdoor environment, which includes challenging settings such as highly dynamic scenes, occlusion between objects, and distortions. Our system achieves an accuracy of 84% on predicting a safe direction. We also show that our custom segmentation network is both fast and accurate, achieving mIOU (mean intersection over union) scores of 81 and 44.7 on the PASCAL VOC and the PASCAL Context datasets, respectively, while running at about 21 frames per second.



