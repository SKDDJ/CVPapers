# Arxiv Papers in cs.CV on 2017-11-17
### 3D Reconstruction of Incomplete Archaeological Objects Using a Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1711.06363v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1711.06363v2)
- **Published**: 2017-11-17 00:58:53+00:00
- **Updated**: 2018-03-10 18:12:27+00:00
- **Authors**: Renato Hermoza, Ivan Sipiran
- **Comment**: 6 pages, 10 figures
- **Journal**: None
- **Summary**: We introduce a data-driven approach to aid the repairing and conservation of archaeological objects: ORGAN, an object reconstruction generative adversarial network (GAN). By using an encoder-decoder 3D deep neural network on a GAN architecture, and combining two loss objectives: a completion loss and an Improved Wasserstein GAN loss, we can train a network to effectively predict the missing geometry of damaged objects. As archaeological objects can greatly differ between them, the network is conditioned on a variable, which can be a culture, a region or any metadata of the object. In our results, we show that our method can recover most of the information from damaged objects, even in cases where more than half of the voxels are missing, without producing many errors.



### Mobile Video Object Detection with Temporally-Aware Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/1711.06368v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06368v2)
- **Published**: 2017-11-17 01:40:12+00:00
- **Updated**: 2018-03-28 20:05:29+00:00
- **Authors**: Mason Liu, Menglong Zhu
- **Comment**: In CVPR 2018
- **Journal**: None
- **Summary**: This paper introduces an online model for object detection in videos designed to run in real-time on low-powered mobile and embedded devices. Our approach combines fast single-image object detection with convolutional long short term memory (LSTM) layers to create an interweaved recurrent-convolutional architecture. Additionally, we propose an efficient Bottleneck-LSTM layer that significantly reduces computational cost compared to regular LSTMs. Our network achieves temporal awareness by using Bottleneck-LSTMs to refine and propagate feature maps across frames. This approach is substantially faster than existing detection methods in video, outperforming the fastest single-frame models in model size and computational cost while attaining accuracy comparable to much more expensive single-frame models on the Imagenet VID 2015 dataset. Our model reaches a real-time inference speed of up to 15 FPS on a mobile CPU.



### Parallel Attention: A Unified Framework for Visual Object Discovery through Dialogs and Queries
- **Arxiv ID**: http://arxiv.org/abs/1711.06370v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06370v1)
- **Published**: 2017-11-17 01:46:48+00:00
- **Updated**: 2017-11-17 01:46:48+00:00
- **Authors**: Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, Anton van den Hengel
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Recognising objects according to a pre-defined fixed set of class labels has been well studied in the Computer Vision. There are a great many practical applications where the subjects that may be of interest are not known beforehand, or so easily delineated, however. In many of these cases natural language dialog is a natural way to specify the subject of interest, and the task achieving this capability (a.k.a, Referring Expression Comprehension) has recently attracted attention. To this end we propose a unified framework, the ParalleL AttentioN (PLAN) network, to discover the object in an image that is being referred to in variable length natural expression descriptions, from short phrases query to long multi-round dialogs. The PLAN network has two attention mechanisms that relate parts of the expressions to both the global visual content and also directly to object candidates. Furthermore, the attention mechanisms are recurrent, making the referring process visualizable and explainable. The attended information from these dual sources are combined to reason about the referred object. These two attention mechanisms can be trained in parallel and we find the combined system outperforms the state-of-art on several benchmarked datasets with different length language input, such as RefCOCO, RefCOCO+ and GuessWhat?!.



### Thoracic Disease Identification and Localization with Limited Supervision
- **Arxiv ID**: http://arxiv.org/abs/1711.06373v6
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.06373v6)
- **Published**: 2017-11-17 01:52:56+00:00
- **Updated**: 2018-06-20 23:24:24+00:00
- **Authors**: Zhe Li, Chong Wang, Mei Han, Yuan Xue, Wei Wei, Li-Jia Li, Li Fei-Fei
- **Comment**: Conference on Computer Vision and Pattern Recognition 2018 (CVPR
  2018). V1: CVPR submission; V2: +supplementary; V3: CVPR camera-ready; V4:
  correction, update reference baseline results according to their latest post;
  V5: minor correction; V6: Identification results using NIH data splits and
  various image models
- **Journal**: None
- **Summary**: Accurate identification and localization of abnormalities from radiology images play an integral part in clinical diagnosis and treatment planning. Building a highly accurate prediction model for these tasks usually requires a large number of images manually annotated with labels and finding sites of abnormalities. In reality, however, such annotated data are expensive to acquire, especially the ones with location annotations. We need methods that can work well with only a small amount of location annotations. To address this challenge, we present a unified approach that simultaneously performs disease identification and localization through the same underlying model for all images. We demonstrate that our approach can effectively leverage both class information as well as limited location annotation, and significantly outperforms the comparative reference baseline in both classification and localization tasks.



### Shape Inpainting using 3D Generative Adversarial Network and Recurrent Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.06375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06375v1)
- **Published**: 2017-11-17 02:01:11+00:00
- **Updated**: 2017-11-17 02:01:11+00:00
- **Authors**: Weiyue Wang, Qiangui Huang, Suya You, Chao Yang, Ulrich Neumann
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in convolutional neural networks have shown promising results in 3D shape completion. But due to GPU memory limitations, these methods can only produce low-resolution outputs. To inpaint 3D models with semantic plausibility and contextual details, we introduce a hybrid framework that combines a 3D Encoder-Decoder Generative Adversarial Network (3D-ED-GAN) and a Long-term Recurrent Convolutional Network (LRCN). The 3D-ED-GAN is a 3D convolutional neural network trained with a generative adversarial paradigm to fill missing 3D data in low-resolution. LRCN adopts a recurrent neural network architecture to minimize GPU memory usage and incorporates an Encoder-Decoder pair into a Long Short-term Memory Network. By handling the 3D model as a sequence of 2D slices, LRCN transforms a coarse 3D shape into a more complete and higher resolution volume. While 3D-ED-GAN captures global contextual structure of the 3D shape, LRCN localizes the fine-grained details. Experimental results on both real-world and synthetic data show reconstructions from corrupted models result in complete and high-resolution 3D objects.



### Improvements to context based self-supervised learning
- **Arxiv ID**: http://arxiv.org/abs/1711.06379v3
- **DOI**: 10.1109/CVPR.2018.00973
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.06379v3)
- **Published**: 2017-11-17 02:22:21+00:00
- **Updated**: 2018-03-28 22:14:26+00:00
- **Authors**: T. Nathan Mundhenk, Daniel Ho, Barry Y. Chen
- **Comment**: Accepted paper at CVPR 2018
- **Journal**: None
- **Summary**: We develop a set of methods to improve on the results of self-supervised learning using context. We start with a baseline of patch based arrangement context learning and go from there. Our methods address some overt problems such as chromatic aberration as well as other potential problems such as spatial skew and mid-level feature neglect. We prevent problems with testing generalization on common self-supervised benchmark tests by using different datasets during our development. The results of our methods combined yield top scores on all standard self-supervised benchmarks, including classification and detection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and "linear tests" on the ImageNet and CSAIL Places datasets. We obtain an improvement over our baseline method of between 4.0 to 7.1 percentage points on transfer learning classification tests. We also show results on different standard network architectures to demonstrate generalization as well as portability. All data, models and programs are available at: https://gdo-datasci.llnl.gov/selfsupervised/.



### Dimensionality Reduction on Grassmannian via Riemannian Optimization: A Generalized Perspective
- **Arxiv ID**: http://arxiv.org/abs/1711.06382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06382v1)
- **Published**: 2017-11-17 02:50:04+00:00
- **Updated**: 2017-11-17 02:50:04+00:00
- **Authors**: Tianci Liu, Zelin Shi, Yunpeng Liu
- **Comment**: 12 pages,5 figures
- **Journal**: None
- **Summary**: This paper proposes a generalized framework with joint normalization which learns lower-dimensional subspaces with maximum discriminative power by making use of the Riemannian geometry. In particular, we model the similarity/dissimilarity between subspaces using various metrics defined on Grassmannian and formulate dimen-sionality reduction as a non-linear constraint optimization problem considering the orthogonalization. To obtain the linear mapping, we derive the components required to per-form Riemannian optimization (e.g., Riemannian conju-gate gradient) from the original Grassmannian through an orthonormal projection. We respect the Riemannian ge-ometry of the Grassmann manifold and search for this projection directly from one Grassmann manifold to an-other face-to-face without any additional transformations. In this natural geometry-aware way, any metric on the Grassmann manifold can be resided in our model theoreti-cally. We have combined five metrics with our model and the learning process can be treated as an unconstrained optimization problem on a Grassmann manifold. Exper-iments on several datasets demonstrate that our approach leads to a significant accuracy gain over state-of-the-art methods.



### VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1711.06396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06396v1)
- **Published**: 2017-11-17 04:25:24+00:00
- **Updated**: 2017-11-17 04:25:24+00:00
- **Authors**: Yin Zhou, Oncel Tuzel
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.



### Predicting Driver Attention in Critical Situations
- **Arxiv ID**: http://arxiv.org/abs/1711.06406v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06406v3)
- **Published**: 2017-11-17 04:53:51+00:00
- **Updated**: 2018-12-05 06:40:01+00:00
- **Authors**: Ye Xia, Danqing Zhang, Jinkyu Kim, Ken Nakayama, Karl Zipser, David Whitney
- **Comment**: ACCV 2018
- **Journal**: None
- **Summary**: Robust driver attention prediction for critical situations is a challenging computer vision problem, yet essential for autonomous driving. Because critical driving moments are so rare, collecting enough data for these situations is difficult with the conventional in-car data collection protocol---tracking eye movements during driving. Here, we first propose a new in-lab driver attention collection protocol and introduce a new driver attention dataset, Berkeley DeepDrive Attention (BDD-A) dataset, which is built upon braking event videos selected from a large-scale, crowd-sourced driving video dataset. We further propose Human Weighted Sampling (HWS) method, which uses human gaze behavior to identify crucial frames of a driving dataset and weights them heavily during model training. With our dataset and HWS, we built a driver attention prediction model that outperforms the state-of-the-art and demonstrates sophisticated behaviors, like attending to crossing pedestrians but not giving false alarms to pedestrians safely walking on the sidewalk. Its prediction results are nearly indistinguishable from ground-truth to humans. Although only being trained with our in-lab attention data, the model also predicts in-car driver attention data of routine driving with state-of-the-art accuracy. This result not only demonstrates the performance of our model but also proves the validity and usefulness of our dataset and data collection protocol.



### Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1711.06420v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06420v2)
- **Published**: 2017-11-17 06:10:03+00:00
- **Updated**: 2018-06-13 08:56:56+00:00
- **Authors**: Jiuxiang Gu, Jianfei Cai, Shafiq Joty, Li Niu, Gang Wang
- **Comment**: 10 pages, 6 figures, Accepted as spotlight at CVPR 2018
- **Journal**: None
- **Summary**: Textual-visual cross-modal retrieval has been a hot research topic in both computer vision and natural language processing communities. Learning appropriate representations for multi-modal data is crucial for the cross-modal retrieval performance. Unlike existing image-text retrieval approaches that embed image-text pairs as single feature vectors in a common representational space, we propose to incorporate generative processes into the cross-modal feature embedding, through which we are able to learn not only the global abstract features but also the local grounded features. Extensive experiments show that our framework can well match images and sentences with complex content, and achieve the state-of-the-art cross-modal retrieval results on MSCOCO dataset.



### Vision Based Railway Track Monitoring using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.06423v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06423v2)
- **Published**: 2017-11-17 06:16:41+00:00
- **Updated**: 2018-11-25 14:25:54+00:00
- **Authors**: Shruti Mittal, Dattaraj Rao
- **Comment**: 6 pages, 7 figures
- **Journal**: None
- **Summary**: Computer vision based methods have been explored in the past for detection of railway track defects, but full automation has always been a challenge because both traditional image processing methods and deep learning classifiers trained from scratch fail to generalize that well to infinite novel scenarios seen in the real world, given limited amount of labeled data. Advancements have been made recently to make machine learning models utilize knowledge from a different but related domain. In this paper, we show that even though similar domain data is not available, transfer learning provides the model understanding of other real world objects and enables training production scale deep learning classifiers for uncontrolled real world data. Our models efficiently detect both track defects like sunkinks, loose ballast and railway assets like switches and signals. Models were validated with hours of track videos recorded in different continents resulting in different weather conditions, different ambience and surroundings. A track health index concept has also been proposed to monitor complete rail network.



### Action-Attending Graphic Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1711.06427v1
- **DOI**: 10.1109/TIP.2018.2815744
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06427v1)
- **Published**: 2017-11-17 06:32:09+00:00
- **Updated**: 2017-11-17 06:32:09+00:00
- **Authors**: Chaolong Li, Zhen Cui, Wenming Zheng, Chunyan Xu, Rongrong Ji, Jian Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The motion analysis of human skeletons is crucial for human action recognition, which is one of the most active topics in computer vision. In this paper, we propose a fully end-to-end action-attending graphic neural network (A$^2$GNN) for skeleton-based action recognition, in which each irregular skeleton is structured as an undirected attribute graph. To extract high-level semantic representation from skeletons, we perform the local spectral graph filtering on the constructed attribute graphs like the standard image convolution operation. Considering not all joints are informative for action analysis, we design an action-attending layer to detect those salient action units (AUs) by adaptively weighting skeletal joints. Herein the filtering responses are parameterized into a weighting function irrelevant to the order of input nodes. To further encode continuous motion variations, the deep features learnt from skeletal graphs are gathered along consecutive temporal slices and then fed into a recurrent gated network. Finally, the spectral graph filtering, action-attending and recurrent temporal encoding are integrated together to jointly train for the sake of robust action recognition as well as the intelligibility of human actions. To evaluate our A$^2$GNN, we conduct extensive experiments on four benchmark skeleton-based action datasets, including the large-scale challenging NTU RGB+D dataset. The experimental results demonstrate that our network achieves the state-of-the-art performances.



### Using KL-divergence to focus Deep Visual Explanation
- **Arxiv ID**: http://arxiv.org/abs/1711.06431v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.06431v2)
- **Published**: 2017-11-17 06:53:17+00:00
- **Updated**: 2018-01-25 06:18:18+00:00
- **Authors**: Housam Khalifa Bashier Babiker, Randy Goebel
- **Comment**: Presented at NIPS 2017 Symposium on Interpretable Machine Learning
- **Journal**: None
- **Summary**: We present a method for explaining the image classification predictions of deep convolution neural networks, by highlighting the pixels in the image which influence the final class prediction. Our method requires the identification of a heuristic method to select parameters hypothesized to be most relevant in this prediction, and here we use Kullback-Leibler divergence to provide this focus. Overall, our approach helps in understanding and interpreting deep network predictions and we hope contributes to a foundation for such understanding of deep learning networks. In this brief paper, our experiments evaluate the performance of two popular networks in this context of interpretability.



### Towards dense volumetric pancreas segmentation in CT using 3D fully convolutional networks
- **Arxiv ID**: http://arxiv.org/abs/1711.06439v2
- **DOI**: 10.1117/12.2293499
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06439v2)
- **Published**: 2017-11-17 07:26:39+00:00
- **Updated**: 2018-01-19 02:14:48+00:00
- **Authors**: Holger Roth, Masahiro Oda, Natsuki Shimizu, Hirohisa Oda, Yuichiro Hayashi, Takayuki Kitasaka, Michitaka Fujiwara, Kazunari Misawa, Kensaku Mori
- **Comment**: Accepted for oral presentation at SPIE Medical Imaging 2018, Houston,
  TX, USA Updated experiment in Fig. 4
- **Journal**: Medical Imaging 2018: Image Processing
- **Summary**: Pancreas segmentation in computed tomography imaging has been historically difficult for automated methods because of the large shape and size variations between patients. In this work, we describe a custom-build 3D fully convolutional network (FCN) that can process a 3D image including the whole pancreas and produce an automatic segmentation. We investigate two variations of the 3D FCN architecture; one with concatenation and one with summation skip connections to the decoder part of the network. We evaluate our methods on a dataset from a clinical trial with gastric cancer patients, including 147 contrast enhanced abdominal CT scans acquired in the portal venous phase. Using the summation architecture, we achieve an average Dice score of 89.7 $\pm$ 3.8 (range [79.8, 94.8]) % in testing, achieving the new state-of-the-art performance in pancreas segmentation on this dataset.



### xUnit: Learning a Spatial Activation Function for Efficient Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1711.06445v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.06445v3)
- **Published**: 2017-11-17 08:00:44+00:00
- **Updated**: 2018-03-25 08:49:43+00:00
- **Authors**: Idan Kligvasser, Tamar Rott Shaham, Tomer Michaeli
- **Comment**: Conference on Computer Vision and Pattern Recognition (CVPR), 2018
- **Journal**: None
- **Summary**: In recent years, deep neural networks (DNNs) achieved unprecedented performance in many low-level vision tasks. However, state-of-the-art results are typically achieved by very deep networks, which can reach tens of layers with tens of millions of parameters. To make DNNs implementable on platforms with limited resources, it is necessary to weaken the tradeoff between performance and efficiency. In this paper, we propose a new activation unit, which is particularly suitable for image restoration problems. In contrast to the widespread per-pixel activation units, like ReLUs and sigmoids, our unit implements a learnable nonlinear function with spatial connections. This enables the net to capture much more complex features, thus requiring a significantly smaller number of layers in order to reach the same performance. We illustrate the effectiveness of our units through experiments with state-of-the-art nets for denoising, de-raining, and super resolution, which are already considered to be very small. With our approach, we are able to further reduce these models by nearly 50% without incurring any degradation in performance.



### Chinese Typeface Transformation with Hierarchical Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1711.06448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06448v1)
- **Published**: 2017-11-17 08:05:49+00:00
- **Updated**: 2017-11-17 08:05:49+00:00
- **Authors**: Jie Chang, Yujun Gu, Ya Zhang
- **Comment**: 8 pages(exclude reference), 6 figures
- **Journal**: None
- **Summary**: In this paper, we explore automated typeface generation through image style transfer which has shown great promise in natural image generation. Existing style transfer methods for natural images generally assume that the source and target images share similar high-frequency features. However, this assumption is no longer true in typeface transformation. Inspired by the recent advancement in Generative Adversarial Networks (GANs), we propose a Hierarchical Adversarial Network (HAN) for typeface transformation. The proposed HAN consists of two sub-networks: a transfer network and a hierarchical adversarial discriminator. The transfer network maps characters from one typeface to another. A unique characteristic of typefaces is that the same radicals may have quite different appearances in different characters even under the same typeface. Hence, a stage-decoder is employed by the transfer network to leverage multiple feature layers, aiming to capture both the global and local features. The hierarchical adversarial discriminator implicitly measures data discrepancy between the generated domain and the target domain. To leverage the complementary discriminating capability of different feature layers, a hierarchical structure is proposed for the discriminator. We have experimentally demonstrated that HAN is an effective framework for typeface transfer and characters restoration.



### A Fusion-based Gender Recognition Method Using Facial Images
- **Arxiv ID**: http://arxiv.org/abs/1711.06451v1
- **DOI**: 10.1109/ICEE.2018.8472550
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06451v1)
- **Published**: 2017-11-17 08:16:55+00:00
- **Updated**: 2017-11-17 08:16:55+00:00
- **Authors**: Benyamin Ghojogh, Saeed Bagheri Shouraki, Hoda Mohammadzade, Ensieh Iranmehr
- **Comment**: 6 pages, 4 figures, 2 tables, key words: gender recognition, Gabor
  filter, local binary pattern, lower face, LDA, SVM, back propagation neural
  network, PCA
- **Journal**: None
- **Summary**: This paper proposes a fusion-based gender recognition method which uses facial images as input. Firstly, this paper utilizes pre-processing and a landmark detection method in order to find the important landmarks of faces. Thereafter, four different frameworks are proposed which are inspired by state-of-the-art gender recognition systems. The first framework extracts features using Local Binary Pattern (LBP) and Principal Component Analysis (PCA) and uses back propagation neural network. The second framework uses Gabor filters, PCA, and kernel Support Vector Machine (SVM). The third framework uses lower part of faces as input and classifies them using kernel SVM. The fourth framework uses Linear Discriminant Analysis (LDA) in order to classify the side outline landmarks of faces. Finally, the four decisions of frameworks are fused using weighted voting. This paper takes advantage of both texture and geometrical information, the two dominant types of information in facial gender recognition. Experimental results show the power and effectiveness of the proposed method. This method obtains recognition rate of 94% for neutral faces of FEI face dataset, which is equal to state-of-the-art rate for this dataset.



### Separating Style and Content for Generalized Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1711.06454v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06454v6)
- **Published**: 2017-11-17 08:26:12+00:00
- **Updated**: 2018-09-23 10:36:52+00:00
- **Authors**: Yexun Zhang, Ya Zhang, Wenbin Cai, Jie Chang
- **Comment**: Accepted by CVPR2018
- **Journal**: None
- **Summary**: Neural style transfer has drawn broad attention in recent years. However, most existing methods aim to explicitly model the transformation between different styles, and the learned model is thus not generalizable to new styles. We here attempt to separate the representations for styles and contents, and propose a generalized style transfer network consisting of style encoder, content encoder, mixer and decoder. The style encoder and content encoder are used to extract the style and content factors from the style reference images and content reference images, respectively. The mixer employs a bilinear model to integrate the above two factors and finally feeds it into a decoder to generate images with target style and content. To separate the style features and content features, we leverage the conditional dependence of styles and contents given an image. During training, the encoder network learns to extract styles and contents from two sets of reference images in limited size, one with shared style and the other with shared content. This learning framework allows simultaneous style transfer among multiple styles and can be deemed as a special `multi-task' learning scenario. The encoders are expected to capture the underlying features for different styles and contents which is generalizable to new styles and contents. For validation, we applied the proposed algorithm to the Chinese Typeface transfer problem. Extensive experiment results on character generation have demonstrated the effectiveness and robustness of our method.



### Fast Recurrent Fully Convolutional Networks for Direct Perception in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1711.06459v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06459v2)
- **Published**: 2017-11-17 08:49:35+00:00
- **Updated**: 2017-11-20 03:06:42+00:00
- **Authors**: Yiqi Hou, Sascha Hornauer, Karl Zipser
- **Comment**: CVPR 2018 Submission, 9 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have been shown to perform extremely well at a variety of tasks including subtasks of autonomous driving such as image segmentation and object classification. However, networks designed for these tasks typically require vast quantities of training data and long training periods to converge. We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive deep end-to-end neural network models that generate driving control signals directly from input images. In contrast to prior work that segments the autonomous driving task, our models take on a novel approach to the autonomous driving problem by utilizing deep and thin Fully Convolutional Nets (FCNs) with recurrent neural nets and low parameter counts to tackle a complex end-to-end regression task predicting both steering and acceleration commands. In addition, we include layers optimized for classification to allow the networks to implicitly learn image semantics. We show that the resulting networks use 3x fewer parameters than the most recent comparable end-to-end driving network and 500x fewer parameters than the AlexNet variations and converge both faster and to lower losses while maintaining robustness against overfitting.



### Grounding Visual Explanations (Extended Abstract)
- **Arxiv ID**: http://arxiv.org/abs/1711.06465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06465v1)
- **Published**: 2017-11-17 09:33:58+00:00
- **Updated**: 2017-11-17 09:33:58+00:00
- **Authors**: Lisa Anne Hendricks, Ronghang Hu, Trevor Darrell, Zeynep Akata
- **Comment**: Presented at NIPS 2017 Symposium on Interpretable Machine Learning
- **Journal**: None
- **Summary**: Existing models which generate textual explanations enforce task relevance through a discriminative term loss function, but such mechanisms only weakly constrain mentioned object parts to actually be present in the image. In this paper, a new model is proposed for generating explanations by utilizing localized grounding of constituent phrases in generated explanations to ensure image relevance. Specifically, we introduce a phrase-critic model to refine (re-score/re-rank) generated candidate explanations and employ a relative-attribute inspired ranking loss using "flipped" phrases as negative examples for training. At test time, our phrase-critic model takes an image and a candidate explanation as input and outputs a score indicating how well the candidate explanation is grounded in the image.



### AI Challenger : A Large-scale Dataset for Going Deeper in Image Understanding
- **Arxiv ID**: http://arxiv.org/abs/1711.06475v1
- **DOI**: 10.1109/ICME.2019.00256
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06475v1)
- **Published**: 2017-11-17 09:58:20+00:00
- **Updated**: 2017-11-17 09:58:20+00:00
- **Authors**: Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan, Rui Liang, Wenjia Wang, Shipei Zhou, Guosen Lin, Yanwei Fu, Yizhou Wang, Yonggang Wang
- **Comment**: None
- **Journal**: 2019 IEEE International Conference on Multimedia and Expo (ICME)
- **Summary**: Significant progress has been achieved in Computer Vision by leveraging large-scale image datasets. However, large-scale datasets for complex Computer Vision tasks beyond classification are still limited. This paper proposed a large-scale dataset named AIC (AI Challenger) with three sub-datasets, human keypoint detection (HKD), large-scale attribute dataset (LAD) and image Chinese captioning (ICC). In this dataset, we annotate class labels (LAD), keypoint coordinate (HKD), bounding box (HKD and LAD), attribute (LAD) and caption (ICC). These rich annotations bridge the semantic gap between low-level images and high-level concepts. The proposed dataset is an effective benchmark to evaluate and improve different computational methods. In addition, for related tasks, others can also use our dataset as a new resource to pre-train their models.



### High-resolution Deep Convolutional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.06491v18
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06491v18)
- **Published**: 2017-11-17 10:47:08+00:00
- **Updated**: 2020-04-17 16:59:30+00:00
- **Authors**: J. D. Curtó, I. C. Zarza, Fernando de la Torre, Irwin King, Michael R. Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) [Goodfellow et al. 2014] convergence in a high-resolution setting with a computational constrain of GPU memory capacity has been beset with difficulty due to the known lack of convergence rate stability. In order to boost network convergence of DCGAN (Deep Convolutional Generative Adversarial Networks) [Radford et al. 2016] and achieve good-looking high-resolution results we propose a new layered network, HDCGAN, that incorporates current state-of-the-art techniques for this effect. Glasses, a mechanism to arbitrarily improve the final GAN generated results by enlarging the input size by a telescope {\zeta} is also presented. A novel bias-free dataset, Curt\'o & Zarza, containing human faces from different ethnical groups in a wide variety of illumination conditions and image resolutions is introduced. Curt\'o is enhanced with HDCGAN synthetic images, thus being the first GAN augmented dataset of faces. We conduct extensive experiments on CelebA [Liu et al. 2015], CelebA-hq [Karras et al. 2018] and Curt\'o. HDCGAN is the current state-of-the-art in synthetic image generation on CelebA achieving a MS-SSIM of 0.1978 and a FR\'ECHET Inception Distance of 8.44.



### Pseudo-positive regularization for deep person re-identification
- **Arxiv ID**: http://arxiv.org/abs/1711.06500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06500v1)
- **Published**: 2017-11-17 11:33:14+00:00
- **Updated**: 2017-11-17 11:33:14+00:00
- **Authors**: Fuqing Zhu, Xiangwei Kong, Haiyan Fu, Qi Tian
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: An intrinsic challenge of person re-identification (re-ID) is the annotation difficulty. This typically means 1) few training samples per identity, and 2) thus the lack of diversity among the training samples. Consequently, we face high risk of over-fitting when training the convolutional neural network (CNN), a state-of-the-art method in person re-ID. To reduce the risk of over-fitting, this paper proposes a Pseudo Positive Regularization (PPR) method to enrich the diversity of the training data. Specifically, unlabeled data from an independent pedestrian database is retrieved using the target training data as query. A small proportion of these retrieved samples are randomly selected as the Pseudo Positive samples and added to the target training set for the supervised CNN training. The addition of Pseudo Positive samples is therefore a data augmentation method to reduce the risk of over-fitting during CNN training. We implement our idea in the identification CNN models (i.e., CaffeNet, VGGNet-16 and ResNet-50). On CUHK03 and Market-1501 datasets, experimental results demonstrate that the proposed method consistently improves the baseline and yields competitive performance to the state-of-the-art person re-ID methods.



### Detecting hip fractures with radiologist-level performance using deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1711.06504v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.06504v1)
- **Published**: 2017-11-17 11:56:07+00:00
- **Updated**: 2017-11-17 11:56:07+00:00
- **Authors**: William Gale, Luke Oakden-Rayner, Gustavo Carneiro, Andrew P. Bradley, Lyle J. Palmer
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: We developed an automated deep learning system to detect hip fractures from frontal pelvic x-rays, an important and common radiological task. Our system was trained on a decade of clinical x-rays (~53,000 studies) and can be applied to clinical data, automatically excluding inappropriate and technically unsatisfactory studies. We demonstrate diagnostic performance equivalent to a human radiologist and an area under the ROC curve of 0.994. Translated to clinical practice, such a system has the potential to increase the efficiency of diagnosis, reduce the need for expensive additional testing, expand access to expert level medical image interpretation, and improve overall patient outcomes.



### Image Matters: Visually modeling user behaviors using Advanced Model Server
- **Arxiv ID**: http://arxiv.org/abs/1711.06505v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06505v3)
- **Published**: 2017-11-17 11:57:13+00:00
- **Updated**: 2018-09-04 09:11:57+00:00
- **Authors**: Tiezheng Ge, Liqin Zhao, Guorui Zhou, Keyu Chen, Shuying Liu, Huimin Yi, Zelin Hu, Bochao Liu, Peng Sun, Haoyu Liu, Pengtao Yi, Sui Huang, Zhiqiang Zhang, Xiaoqiang Zhu, Yu Zhang, Kun Gai
- **Comment**: CIKM 2018
- **Journal**: None
- **Summary**: In Taobao, the largest e-commerce platform in China, billions of items are provided and typically displayed with their images. For better user experience and business effectiveness, Click Through Rate (CTR) prediction in online advertising system exploits abundant user historical behaviors to identify whether a user is interested in a candidate ad. Enhancing behavior representations with user behavior images will help understand user's visual preference and improve the accuracy of CTR prediction greatly. So we propose to model user preference jointly with user behavior ID features and behavior images. However, training with user behavior images brings tens to hundreds of images in one sample, giving rise to a great challenge in both communication and computation. To handle these challenges, we propose a novel and efficient distributed machine learning paradigm called Advanced Model Server (AMS). With the well known Parameter Server (PS) framework, each server node handles a separate part of parameters and updates them independently. AMS goes beyond this and is designed to be capable of learning a unified image descriptor model shared by all server nodes which embeds large images into low dimensional high level features before transmitting images to worker nodes. AMS thus dramatically reduces the communication load and enables the arduous joint training process. Based on AMS, the methods of effectively combining the images and ID features are carefully studied, and then we propose a Deep Image CTR Model. Our approach is shown to achieve significant improvements in both online and offline evaluations, and has been deployed in Taobao display advertising system serving the main traffic.



### Multi-Label Zero-Shot Learning with Structured Knowledge Graphs
- **Arxiv ID**: http://arxiv.org/abs/1711.06526v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06526v2)
- **Published**: 2017-11-17 13:31:57+00:00
- **Updated**: 2018-05-26 12:48:10+00:00
- **Authors**: Chung-Wei Lee, Wei Fang, Chih-Kuan Yeh, Yu-Chiang Frank Wang
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: In this paper, we propose a novel deep learning architecture for multi-label zero-shot learning (ML-ZSL), which is able to predict multiple unseen class labels for each input instance. Inspired by the way humans utilize semantic knowledge between objects of interests, we propose a framework that incorporates knowledge graphs for describing the relationships between multiple labels. Our model learns an information propagation mechanism from the semantic label space, which can be applied to model the interdependencies between seen and unseen class labels. With such investigation of structured knowledge graphs for visual reasoning, we show that our model can be applied for solving multi-label classification and ML-ZSL tasks. Compared to state-of-the-art approaches, comparable or improved performances can be achieved by our method.



### Learning a Robust Representation via a Deep Network on Symmetric Positive Definite Manifolds
- **Arxiv ID**: http://arxiv.org/abs/1711.06540v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06540v2)
- **Published**: 2017-11-17 14:04:12+00:00
- **Updated**: 2017-11-20 11:19:21+00:00
- **Authors**: Zhi Gao, Yuwei Wu, Xingyuan Bu, Yunde Jia
- **Comment**: 11 pages, 8figures
- **Journal**: None
- **Summary**: Recent studies have shown that aggregating convolutional features of a pre-trained Convolutional Neural Network (CNN) can obtain impressive performance for a variety of visual tasks. The symmetric Positive Definite (SPD) matrix becomes a powerful tool due to its remarkable ability to learn an appropriate statistic representation to characterize the underlying structure of visual features. In this paper, we propose to aggregate deep convolutional features into an SPD matrix representation through the SPD generation and the SPD transformation under an end-to-end deep network. To this end, several new layers are introduced in our network, including a nonlinear kernel aggregation layer, an SPD matrix transformation layer, and a vectorization layer. The nonlinear kernel aggregation layer is employed to aggregate the convolutional features into a real SPD matrix directly. The SPD matrix transformation layer is designed to construct a more compact and discriminative SPD representation. The vectorization and normalization operations are performed in the vectorization layer for reducing the redundancy and accelerating the convergence. The SPD matrix in our network can be considered as a mid-level representation bridging convolutional features and high-level semantic features. To demonstrate the effectiveness of our method, we conduct extensive experiments on visual classification. Experiment results show that our method notably outperforms state-of-the-art methods.



### Memory Based Online Learning of Deep Representations from Video Streams
- **Arxiv ID**: http://arxiv.org/abs/1711.07368v1
- **DOI**: 10.1109/CVPR.2018.00247
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07368v1)
- **Published**: 2017-11-17 14:51:58+00:00
- **Updated**: 2017-11-17 14:51:58+00:00
- **Authors**: Federico Pernici, Federico Bartoli, Matteo Bruni, Alberto Del Bimbo
- **Comment**: arXiv admin note: text overlap with arXiv:1708.03615
- **Journal**: None
- **Summary**: We present a novel online unsupervised method for face identity learning from video streams. The method exploits deep face descriptors together with a memory based learning mechanism that takes advantage of the temporal coherence of visual data. Specifically, we introduce a discriminative feature matching solution based on Reverse Nearest Neighbour and a feature forgetting strategy that detect redundant features and discard them appropriately while time progresses. It is shown that the proposed learning procedure is asymptotically stable and can be effectively used in relevant applications like multiple face identification and tracking from unconstrained video streams. Experimental results show that the proposed method achieves comparable results in the task of multiple face tracking and better performance in face identification with offline approaches exploiting future information. Code will be publicly available.



### Learning to Play Othello with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.06583v1
- **DOI**: 10.1109/TG.2018.2799997
- **Categories**: **cs.AI**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.06583v1)
- **Published**: 2017-11-17 15:14:20+00:00
- **Updated**: 2017-11-17 15:14:20+00:00
- **Authors**: Paweł Liskowski, Wojciech Jaśkowski, Krzysztof Krawiec
- **Comment**: None
- **Journal**: None
- **Summary**: Achieving superhuman playing level by AlphaGo corroborated the capabilities of convolutional neural architectures (CNNs) for capturing complex spatial patterns. This result was to a great extent due to several analogies between Go board states and 2D images CNNs have been designed for, in particular translational invariance and a relatively large board. In this paper, we verify whether CNN-based move predictors prove effective for Othello, a game with significantly different characteristics, including a much smaller board size and complete lack of translational invariance. We compare several CNN architectures and board encodings, augment them with state-of-the-art extensions, train on an extensive database of experts' moves, and examine them with respect to move prediction accuracy and playing strength. The empirical evaluation confirms high capabilities of neural move predictors and suggests a strong correlation between prediction accuracy and playing strength. The best CNNs not only surpass all other 1-ply Othello players proposed to date but defeat (2-ply) Edax, the best open-source Othello player.



### Dependent landmark drift: robust point set registration with a Gaussian mixture model and a statistical shape model
- **Arxiv ID**: http://arxiv.org/abs/1711.06588v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.06588v3)
- **Published**: 2017-11-17 15:24:17+00:00
- **Updated**: 2018-07-26 01:00:59+00:00
- **Authors**: Osamu Hirose
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of point set registration is to find point-by-point correspondences between point sets, each of which characterizes the shape of an object. Because local preservation of object geometry is assumed, prevalent algorithms in the area can often elegantly solve the problems without using geometric information specific to the objects. This means that registration performance can be further improved by using prior knowledge of object geometry. In this paper, we propose a novel point set registration method using the Gaussian mixture model with prior shape information encoded as a statistical shape model. Our transformation model is defined as a combination of the similar transformation, motion coherence, and the statistical shape model. Therefore, the proposed method works effectively if the target point set includes outliers and missing regions, or if it is rotated. The computational cost can be reduced to linear, and therefore the method is scalable to large point sets. The effectiveness of the method will be verified through comparisons with existing algorithms using datasets concerning human body shapes, hands, and faces.



### Deep Local Binary Patterns
- **Arxiv ID**: http://arxiv.org/abs/1711.06597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06597v1)
- **Published**: 2017-11-17 15:40:27+00:00
- **Updated**: 2017-11-17 15:40:27+00:00
- **Authors**: Kelwin Fernandes, Jaime S. Cardoso
- **Comment**: None
- **Journal**: None
- **Summary**: Local Binary Pattern (LBP) is a traditional descriptor for texture analysis that gained attention in the last decade. Being robust to several properties such as invariance to illumination translation and scaling, LBPs achieved state-of-the-art results in several applications. However, LBPs are not able to capture high-level features from the image, merely encoding features with low abstraction levels. In this work, we propose Deep LBP, which borrow ideas from the deep learning community to improve LBP expressiveness. By using parametrized data-driven LBP, we enable successive applications of the LBP operators with increasing abstraction levels. We validate the relevance of the proposed idea in several datasets from a wide range of applications. Deep LBP improved the performance of traditional and multiscale LBP in all cases.



### Unsupervised Reverse Domain Adaptation for Synthetic Medical Images via Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1711.06606v2
- **DOI**: 10.1109/TMI.2018.2842767
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06606v2)
- **Published**: 2017-11-17 16:02:37+00:00
- **Updated**: 2017-11-29 01:32:42+00:00
- **Authors**: Faisal Mahmood, Richard Chen, Nicholas J. Durr
- **Comment**: 10 pages, 8 figure
- **Journal**: None
- **Summary**: To realize the full potential of deep learning for medical imaging, large annotated datasets are required for training. Such datasets are difficult to acquire because labeled medical images are not usually available due to privacy issues, lack of experts available for annotation, underrepresentation of rare conditions and poor standardization. Lack of annotated data has been addressed in conventional vision applications using synthetic images refined via unsupervised adversarial training to look like real images. However, this approach is difficult to extend to general medical imaging because of the complex and diverse set of features found in real human tissues. We propose an alternative framework that uses a reverse flow, where adversarial training is used to make real medical images more like synthetic images, and hypothesize that clinically-relevant features can be preserved via self-regularization. These domain-adapted images can then be accurately interpreted by networks trained on large datasets of synthetic medical images. We test this approach for the notoriously difficult task of depth-estimation from endoscopy. We train a depth estimator on a large dataset of synthetic images generated using an accurate forward model of an endoscope and an anatomically-realistic colon. This network predicts significantly better depths when using synthetic-like domain-adapted images compared to the real images, confirming that the clinically-relevant features of depth are preserved.



### Superpixels Based Segmentation and SVM Based Classification Method to Distinguish Five Diseases from Normal Regions in Wireless Capsule Endoscopy
- **Arxiv ID**: http://arxiv.org/abs/1711.06616v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06616v1)
- **Published**: 2017-11-17 16:25:34+00:00
- **Updated**: 2017-11-17 16:25:34+00:00
- **Authors**: Omid Haji Maghsoudi
- **Comment**: None
- **Journal**: None
- **Summary**: Wireless Capsule Endoscopy (WCE) is relatively a new technology to examine the entire GI trace. During an examination, it captures more than 55,000 frames. Reviewing all these images is time-consuming and prone to human error. It has been a challenge to develop intelligent methods assisting physicians to review the frames. The WCE frames are captured in 8-bit color depths which provides enough a color range to detect abnormalities. Here, superpixel based methods are proposed to segment five diseases including: bleeding, Crohn's disease, Lymphangiectasia, Xanthoma, and Lymphoid hyperplasia. Two superpixels methods are compared to provide semantic segmentation of these prolific diseases: simple linear iterative clustering (SLIC) and quick shift (QS). The segmented superpixels were classified into two classes (normal and abnormal) by support vector machine (SVM) using texture and color features. For both superpixel methods, the accuracy, specificity, sensitivity, and precision (SLIC, QS) were around 92%, 93%, 93%, and 88%, respectively. However, SLIC was dramatically faster than QS.



### Depth Assisted Full Resolution Network for Single Image-based View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1711.06620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06620v1)
- **Published**: 2017-11-17 16:50:13+00:00
- **Updated**: 2017-11-17 16:50:13+00:00
- **Authors**: Xiaodong Cun, Feng Xu, Chi-Man Pun, Hao Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Researches in novel viewpoint synthesis majorly focus on interpolation from multi-view input images. In this paper, we focus on a more challenging and ill-posed problem that is to synthesize novel viewpoints from one single input image. To achieve this goal, we propose a novel deep learning-based technique. We design a full resolution network that extracts local image features with the same resolution of the input, which contributes to derive high resolution and prevent blurry artifacts in the final synthesized images. We also involve a pre-trained depth estimation network into our system, and thus 3D information is able to be utilized to infer the flow field between the input and the target image. Since the depth network is trained by depth order information between arbitrary pairs of points in the scene, global image features are also involved into our system. Finally, a synthesis layer is used to not only warp the observed pixels to the desired positions but also hallucinate the missing pixels with recorded pixels. Experiments show that our technique performs well on images of various scenes, and outperforms the state-of-the-art techniques.



### Driven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments
- **Arxiv ID**: http://arxiv.org/abs/1711.06623v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.06623v2)
- **Published**: 2017-11-17 16:54:40+00:00
- **Updated**: 2018-03-05 14:29:23+00:00
- **Authors**: Dan Barnes, Will Maddern, Geoffrey Pascoe, Ingmar Posner
- **Comment**: International Conference on Robotics and Automation (ICRA), 2018.
  Video summary: http://youtu.be/ebIrBn_nc-k
- **Journal**: None
- **Summary**: We present a self-supervised approach to ignoring "distractors" in camera images for the purposes of robustly estimating vehicle motion in cluttered urban environments. We leverage offline multi-session mapping approaches to automatically generate a per-pixel ephemerality mask and depth map for each input image, which we use to train a deep convolutional network. At run-time we use the predicted ephemerality and depth as an input to a monocular visual odometry (VO) pipeline, using either sparse features or dense photometric matching. Our approach yields metric-scale VO using only a single camera and can recover the correct egomotion even when 90% of the image is obscured by dynamic, independently moving objects. We evaluate our robust VO methods on more than 400km of driving from the Oxford RobotCar Dataset and demonstrate reduced odometry drift and significantly improved egomotion estimation in the presence of large moving vehicles in urban traffic.



### Segmenting Brain Tumors with Symmetry
- **Arxiv ID**: http://arxiv.org/abs/1711.06636v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06636v1)
- **Published**: 2017-11-17 17:22:30+00:00
- **Updated**: 2017-11-17 17:22:30+00:00
- **Authors**: Hejia Zhang, Xia Zhu, Theodore L. Willke
- **Comment**: NIPS ML4H Workshop 2017
- **Journal**: None
- **Summary**: We explore encoding brain symmetry into a neural network for a brain tumor segmentation task. A healthy human brain is symmetric at a high level of abstraction, and the high-level asymmetric parts are more likely to be tumor regions. Paying more attention to asymmetries has the potential to boost the performance in brain tumor segmentation. We propose a method to encode brain symmetry into existing neural networks and apply the method to a state-of-the-art neural network for medical imaging segmentation. We evaluate our symmetry-encoded network on the dataset from a brain tumor segmentation challenge and verify that the new model extracts information in the training images more efficiently than the original model.



### Neural Motifs: Scene Graph Parsing with Global Context
- **Arxiv ID**: http://arxiv.org/abs/1711.06640v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06640v2)
- **Published**: 2017-11-17 17:33:01+00:00
- **Updated**: 2018-03-29 16:17:53+00:00
- **Authors**: Rowan Zellers, Mark Yatskar, Sam Thomson, Yejin Choi
- **Comment**: CVPR 2018 camera ready
- **Journal**: None
- **Summary**: We investigate the problem of producing structured graph representations of visual scenes. Our work analyzes the role of motifs: regularly appearing substructures in scene graphs. We present new quantitative insights on such repeated structures in the Visual Genome dataset. Our analysis shows that object labels are highly predictive of relation labels but not vice-versa. We also find that there are recurring patterns even in larger subgraphs: more than 50% of graphs contain motifs involving at least two relations. Our analysis motivates a new baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set. This baseline improves on the previous state-of-the-art by an average of 3.6% relative improvement across evaluation settings. We then introduce Stacked Motif Networks, a new architecture designed to capture higher order motifs in scene graphs that further improves over our strong baseline by an average 7.1% relative gain. Our code is available at github.com/rowanz/neural-motifs.



### Attentive Explanations: Justifying Decisions and Pointing to the Evidence (Extended Abstract)
- **Arxiv ID**: http://arxiv.org/abs/1711.07373v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.07373v1)
- **Published**: 2017-11-17 18:36:14+00:00
- **Updated**: 2017-11-17 18:36:14+00:00
- **Authors**: Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, Marcus Rohrbach
- **Comment**: arXiv admin note: text overlap with arXiv:1612.04757
- **Journal**: None
- **Summary**: Deep models are the defacto standard in visual decision problems due to their impressive performance on a wide array of visual tasks. On the other hand, their opaqueness has led to a surge of interest in explainable systems. In this work, we emphasize the importance of model explanation in various forms such as visual pointing and textual justification. The lack of data with justification annotations is one of the bottlenecks of generating multimodal explanations. Thus, we propose two large-scale datasets with annotations that visually and textually justify a classification decision for various activities, i.e. ACT-X, and for question answering, i.e. VQA-X. We also introduce a multimodal methodology for generating visual and textual explanations simultaneously. We quantitatively show that training with the textual explanations not only yields better textual justification models, but also models that better localize the evidence that support their decision.



### Multiresolution and Hierarchical Analysis of Astronomical Spectroscopic Cubes using 3D Discrete Wavelet Transform
- **Arxiv ID**: http://arxiv.org/abs/1711.06663v2
- **DOI**: 10.1049/cp.2017.0145
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06663v2)
- **Published**: 2017-11-17 18:37:00+00:00
- **Updated**: 2017-11-25 20:51:45+00:00
- **Authors**: Martín Villanueva, Mauricio Araya
- **Comment**: Presented at the 8th International Conference on Pattern Recognition
  Systems, Madrid-Spain, 2017
- **Journal**: None
- **Summary**: The intrinsically hierarchical and blended structure of interstellar molecular clouds, plus the always increasing resolution of astronomical instruments, demand advanced and automated pattern recognition techniques for identifying and connecting source components in spectroscopic cubes. We extend the work done in multiresolution analysis using Wavelets for astronomical 2D images to 3D spectroscopic cubes, combining the results with the Dendrograms approach to offer a hierarchical representation of connections between sources at different scale levels. We test our approach in real data from the ALMA observatory, exploring different Wavelet families and assessing the main parameter for source identification (i.e., RMS) at each level. Our approach shows that is feasible to perform multiresolution analysis for the spatial and frequency domains simultaneously rather than analyzing each spectral channel independently.



### ADVISE: Symbolism and External Knowledge for Decoding Advertisements
- **Arxiv ID**: http://arxiv.org/abs/1711.06666v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06666v2)
- **Published**: 2017-11-17 18:47:25+00:00
- **Updated**: 2018-07-29 23:41:33+00:00
- **Authors**: Keren Ye, Adriana Kovashka
- **Comment**: To appear, Proceedings of the European Conference on Computer Vision
  (ECCV)
- **Journal**: None
- **Summary**: In order to convey the most content in their limited space, advertisements embed references to outside knowledge via symbolism. For example, a motorcycle stands for adventure (a positive property the ad wants associated with the product being sold), and a gun stands for danger (a negative property to dissuade viewers from undesirable behaviors). We show how to use symbolic references to better understand the meaning of an ad. We further show how anchoring ad understanding in general-purpose object recognition and image captioning improves results. We formulate the ad understanding task as matching the ad image to human-generated statements that describe the action that the ad prompts, and the rationale it provides for taking this action. Our proposed method outperforms the state of the art on this task, and on an alternative formulation of question-answering on ads. We show additional applications of our learned representations for matching ads to slogans, and clustering ads according to their topic, without extra training.



### Fusing Bird View LIDAR Point Cloud and Front View Camera Image for Deep Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1711.06703v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.06703v3)
- **Published**: 2017-11-17 19:36:49+00:00
- **Updated**: 2018-02-14 03:52:03+00:00
- **Authors**: Zining Wang, Wei Zhan, Masayoshi Tomizuka
- **Comment**: 10 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: We propose a new method for fusing a LIDAR point cloud and camera-captured images in the deep convolutional neural network (CNN). The proposed method constructs a new layer called non-homogeneous pooling layer to transform features between bird view map and front view map. The sparse LIDAR point cloud is used to construct the mapping between the two maps. The pooling layer allows efficient fusion of the bird view and front view features at any stage of the network. This is favorable for the 3D-object detection using camera-LIDAR fusion in autonomous driving scenarios. A corresponding deep CNN is designed and tested on the KITTI bird view object detection dataset, which produces 3D bounding boxes from the bird view map. The fusion method shows particular benefit for detection of pedestrians in the bird view compared to other fusion-based object detection networks.



### Repeatability Is Not Enough: Learning Affine Regions via Discriminability
- **Arxiv ID**: http://arxiv.org/abs/1711.06704v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.06704v4)
- **Published**: 2017-11-17 19:37:28+00:00
- **Updated**: 2018-08-28 10:01:42+00:00
- **Authors**: Dmytro Mishkin, Filip Radenovic, Jiri Matas
- **Comment**: ECCV 2018 camera ready
- **Journal**: None
- **Summary**: A method for learning local affine-covariant regions is presented. We show that maximizing geometric repeatability does not lead to local regions, a.k.a features,that are reliably matched and this necessitates descriptor-based learning. We explore factors that influence such learning and registration: the loss function, descriptor type, geometric parametrization and the trade-off between matchability and geometric accuracy and propose a novel hard negative-constant loss function for learning of affine regions. The affine shape estimator -- AffNet -- trained with the hard negative-constant loss outperforms the state-of-the-art in bag-of-words image retrieval and wide baseline stereo. The proposed training process does not require precisely geometrically aligned patches.The source codes and trained weights are available at https://github.com/ducha-aiki/affnet



### Optimal Combination of Image Denoisers
- **Arxiv ID**: http://arxiv.org/abs/1711.06712v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06712v4)
- **Published**: 2017-11-17 20:00:26+00:00
- **Updated**: 2019-02-28 18:41:53+00:00
- **Authors**: Joon Hee Choi, Omar Elgendy, Stanley H. Chan
- **Comment**: IEEE Transaction on Image Processing
- **Journal**: None
- **Summary**: Given a set of image denoisers, each having a different denoising capability, is there a provably optimal way of combining these denoisers to produce an overall better result? An answer to this question is fundamental to designing an ensemble of weak estimators for complex scenes. In this paper, we present an optimal combination scheme by leveraging deep neural networks and convex optimization. The proposed framework, called the Consensus Neural Network (CsNet), introduces three new concepts in image denoising: (1) A provably optimal procedure to combine the denoised outputs via convex optimization; (2) A deep neural network to estimate the mean squared error (MSE) of denoised images without needing the ground truths; (3) An image boosting procedure using a deep neural network to improve contrast and to recover lost details of the combined images. Experimental results show that CsNet can consistently improve denoising performance for both deterministic and neural network denoisers.



### Learning SO(3) Equivariant Representations with Spherical CNNs
- **Arxiv ID**: http://arxiv.org/abs/1711.06721v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06721v3)
- **Published**: 2017-11-17 20:49:28+00:00
- **Updated**: 2018-09-28 03:19:48+00:00
- **Authors**: Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, Kostas Daniilidis
- **Comment**: Camera-ready. Accepted to ECCV'18 as oral presentation
- **Journal**: None
- **Summary**: We address the problem of 3D rotation equivariance in convolutional neural networks. 3D rotations have been a challenging nuisance in 3D classification tasks requiring higher capacity and extended data augmentation in order to tackle it. We model 3D data with multi-valued spherical functions and we propose a novel spherical convolutional network that implements exact convolutions on the sphere by realizing them in the spherical harmonic domain. Resulting filters have local symmetry and are localized by enforcing smooth spectra. We apply a novel pooling on the spectral domain and our operations are independent of the underlying spherical resolution throughout the network. We show that networks with much lower capacity and without requiring data augmentation can exhibit performance comparable to the state of the art in standard retrieval and classification benchmarks.



### Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.06753v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06753v5)
- **Published**: 2017-11-17 22:26:55+00:00
- **Updated**: 2018-10-23 20:09:07+00:00
- **Authors**: Zhen-Hua Feng, Josef Kittler, Muhammad Awais, Patrik Huber, Xiao-Jun Wu
- **Comment**: 11 pages, 6 figures, 6 tables
- **Journal**: None
- **Summary**: We present a new loss function, namely Wing loss, for robust facial landmark localisation with Convolutional Neural Networks (CNNs). We first compare and analyse different loss functions including L2, L1 and smooth L1. The analysis of these loss functions suggests that, for the training of a CNN-based localisation model, more attention should be paid to small and medium range errors. To this end, we design a piece-wise loss function. The new loss amplifies the impact of errors from the interval (-w, w) by switching from L1 loss to a modified logarithm function.   To address the problem of under-representation of samples with large out-of-plane head rotations in the training set, we propose a simple but effective boosting strategy, referred to as pose-based data balancing. In particular, we deal with the data imbalance problem by duplicating the minority training samples and perturbing them by injecting random image rotation, bounding box translation and other data augmentation approaches. Last, the proposed approach is extended to create a two-stage framework for robust facial landmark localisation. The experimental results obtained on AFLW and 300W demonstrate the merits of the Wing loss function, and prove the superiority of the proposed method over the state-of-the-art approaches.



### Image Registration of Very Large Images via Genetic Programming
- **Arxiv ID**: http://arxiv.org/abs/1711.06764v2
- **DOI**: 10.1109/CVPRW.2014.56
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.06764v2)
- **Published**: 2017-11-17 23:12:26+00:00
- **Updated**: 2017-11-21 07:21:37+00:00
- **Authors**: Sarit Chicotay, Eli David, Nathan S. Netanyahu
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  Workshop on Registration of Very Large Images, pp. 323-328, Columbus, OH,
  June 2014
- **Summary**: Image registration (IR) is a fundamental task in image processing for matching two or more images of the same scene taken at different times, from different viewpoints and/or by different sensors. Due to the enormous diversity of IR applications, automatic IR remains a challenging problem to this day. A wide range of techniques has been developed for various data types and problems. However, they might not handle effectively very large images, which give rise usually to more complex transformations, e.g., deformations and various other distortions.   In this paper we present a genetic programming (GP)-based approach for IR, which could offer a significant advantage in dealing with very large images, as it does not make any prior assumptions about the transformation model. Thus, by incorporating certain generic building blocks into the proposed GP framework, we hope to realize a large set of specialized transformations that should yield accurate registration of very large images.



### A Two-Phase Genetic Algorithm for Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1711.06765v1
- **DOI**: 10.1145/3067695.3076017
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.06765v1)
- **Published**: 2017-11-17 23:15:19+00:00
- **Updated**: 2017-11-17 23:15:19+00:00
- **Authors**: Sarit Chicotay, Eli David, Nathan S. Netanyahu
- **Comment**: None
- **Journal**: ACM Genetic and Evolutionary Computation Conference (GECCO), pp.
  189-190, Berlin, Germany, July 2017
- **Summary**: Image Registration (IR) is the process of aligning two (or more) images of the same scene taken at different times, different viewpoints and/or by different sensors. It is an important, crucial step in various image analysis tasks where multiple data sources are integrated/fused, in order to extract high-level information.   Registration methods usually assume a relevant transformation model for a given problem domain. The goal is to search for the "optimal" instance of the transformation model assumed with respect to a similarity measure in question.   In this paper we present a novel genetic algorithm (GA)-based approach for IR. Since GA performs effective search in various optimization problems, it could prove useful also for IR. Indeed, various GAs have been proposed for IR. However, most of them assume certain constraints, which simplify the transformation model, restrict the search space or make additional preprocessing requirements. In contrast, we present a generalized GA-based solution for an almost fully affine transformation model, which achieves competitive results without such limitations using a two-phase method and a multi-objective optimization (MOO) approach.   We present good results for multiple dataset and demonstrate the robustness of our method in the presence of noisy data.



### Genetic Algorithm-Based Solver for Very Large Multiple Jigsaw Puzzles of Unknown Dimensions and Piece Orientation
- **Arxiv ID**: http://arxiv.org/abs/1711.06766v1
- **DOI**: 10.1145/2576768.2598289
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.06766v1)
- **Published**: 2017-11-17 23:17:20+00:00
- **Updated**: 2017-11-17 23:17:20+00:00
- **Authors**: Dror Sholomon, Eli David, Nathan S. Netanyahu
- **Comment**: None
- **Journal**: ACM Genetic and Evolutionary Computation Conference (GECCO), pages
  1191-1198, Vancouver, Canada, July 2014
- **Summary**: In this paper we propose the first genetic algorithm (GA)-based solver for jigsaw puzzles of unknown puzzle dimensions and unknown piece location and orientation. Our solver uses a novel crossover technique, and sets a new state-of-the-art in terms of the puzzle sizes solved and the accuracy obtained. The results are significantly improved, even when compared to previous solvers assuming known puzzle dimensions. Moreover, the solver successfully contends with a mixed bag of multiple puzzle pieces, assembling simultaneously all puzzles.



### An Automatic Solver for Very Large Jigsaw Puzzles Using Genetic Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1711.06767v1
- **DOI**: 10.1007/s10710-015-9258-0
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.06767v1)
- **Published**: 2017-11-17 23:17:23+00:00
- **Updated**: 2017-11-17 23:17:23+00:00
- **Authors**: Dror Sholomon, Eli David, Nathan S. Netanyahu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1711.06769
- **Journal**: Genetic Programming and Evolvable Machines, Vol. 17, No. 3, pp.
  291-313, September 2016
- **Summary**: In this paper we propose the first effective genetic algorithm (GA)-based jigsaw puzzle solver. We introduce a novel crossover procedure that merges two "parent" solutions to an improved "child" configuration by detecting, extracting, and combining correctly assembled puzzle segments. The solver proposed exhibits state-of-the-art performance, as far as handling previously attempted puzzles more accurately and efficiently, as well puzzle sizes that have not been attempted before. The extended experimental results provided in this paper include, among others, a thorough inspection of up to 30,745-piece puzzles (compared to previous attempts on 22,755-piece puzzles), using a considerably faster concurrent implementation of the algorithm. Furthermore, we explore the impact of different phases of the novel crossover operator by experimenting with several variants of the GA. Finally, we compare different fitness functions and their effect on the overall results of the GA-based solver.



### A Generalized Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles of Complex Types
- **Arxiv ID**: http://arxiv.org/abs/1711.06768v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.06768v1)
- **Published**: 2017-11-17 23:17:29+00:00
- **Updated**: 2017-11-17 23:17:29+00:00
- **Authors**: Dror Sholomon, Eli David, Nathan S. Netanyahu
- **Comment**: None
- **Journal**: AAAI Conference on Artificial Intelligence, pages 2839-2845,
  Quebec City, Canada, July 2014
- **Summary**: In this paper we introduce new types of square-piece jigsaw puzzles, where in addition to the unknown location and orientation of each piece, a piece might also need to be flipped. These puzzles, which are associated with a number of real world problems, are considerably harder, from a computational standpoint. Specifically, we present a novel generalized genetic algorithm (GA)-based solver that can handle puzzle pieces of unknown location and orientation (Type 2 puzzles) and (two-sided) puzzle pieces of unknown location, orientation, and face (Type 4 puzzles). To the best of our knowledge, our solver provides a new state-of-the-art, solving previously attempted puzzles faster and far more accurately, handling puzzle sizes that have never been attempted before, and assembling the newly introduced two-sided puzzles automatically and effectively. This paper also presents, among other results, the most extensive set of experimental results, compiled as of yet, on Type 2 puzzles.



### A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles
- **Arxiv ID**: http://arxiv.org/abs/1711.06769v1
- **DOI**: 10.1109/CVPR.2013.231
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.06769v1)
- **Published**: 2017-11-17 23:17:33+00:00
- **Updated**: 2017-11-17 23:17:33+00:00
- **Authors**: Dror Sholomon, Eli David, Nathan S. Netanyahu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1711.06767
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  pages 1767-1774, Portland, OR, June 2013
- **Summary**: In this paper we propose the first effective automated, genetic algorithm (GA)-based jigsaw puzzle solver. We introduce a novel procedure of merging two "parent" solutions to an improved "child" solution by detecting, extracting, and combining correctly assembled puzzle segments. The solver proposed exhibits state-of-the-art performance solving previously attempted puzzles faster and far more accurately, and also puzzles of size never before attempted. Other contributions include the creation of a benchmark of large images, previously unavailable. We share the data sets and all of our results for future testing and comparative evaluation of jigsaw puzzle solvers.



