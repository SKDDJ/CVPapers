# Arxiv Papers in cs.CV on 2017-11-26
### Unsupervised 3D Reconstruction from a Single Image via Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.09312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09312v1)
- **Published**: 2017-11-26 00:12:43+00:00
- **Updated**: 2017-11-26 00:12:43+00:00
- **Authors**: Lingjing Wang, Yi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements in deep learning opened new opportunities for learning a high-quality 3D model from a single 2D image given sufficient training on large-scale data sets. However, the significant imbalance between available amount of images and 3D models, and the limited availability of labeled 2D image data (i.e. manually annotated pairs between images and their corresponding 3D models), severely impacts the training of most supervised deep learning methods in practice. In this paper, driven by a novel design of adversarial networks, we have developed an unsupervised learning paradigm to reconstruct 3D models from a single 2D image, which is free of manually annotated pairwise input image and its associated 3D model. Particularly, the paradigm begins with training an adaption network via autoencoder with adversarial loss, which embeds unpaired 2D synthesized image domain with real world image domain to a shared latent vector space. Then, we jointly train a 3D deconvolutional network to transform the latent vector space to the 3D object space together with the embedding process. Our experiments verify our network's robust and superior performance in handling 3D volumetric object generation from a single 2D image.



### DeepRadiologyNet: Radiologist Level Pathology Detection in CT Head Images
- **Arxiv ID**: http://arxiv.org/abs/1711.09313v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09313v3)
- **Published**: 2017-11-26 00:30:45+00:00
- **Updated**: 2017-12-02 19:14:49+00:00
- **Authors**: Jameson Merkow, Robert Lufkin, Kim Nguyen, Stefano Soatto, Zhuowen Tu, Andrea Vedaldi
- **Comment**: 22 pages with references, 6 figures, 2 tables
- **Journal**: None
- **Summary**: We describe a system to automatically filter clinically significant findings from computerized tomography (CT) head scans, operating at performance levels exceeding that of practicing radiologists. Our system, named DeepRadiologyNet, builds on top of deep convolutional neural networks (CNNs) trained using approximately 3.5 million CT head images gathered from over 24,000 studies taken from January 1, 2015 to August 31, 2015 and January 1, 2016 to April 30 2016 in over 80 clinical sites. For our initial system, we identified 30 phenomenological traits to be recognized in the CT scans. To test the system, we designed a clinical trial using over 4.8 million CT head images (29,925 studies), completely disjoint from the training and validation set, interpreted by 35 US Board Certified radiologists with specialized CT head experience. We measured clinically significant error rates to ascertain whether the performance of DeepRadiologyNet was comparable to or better than that of US Board Certified radiologists. DeepRadiologyNet achieved a clinically significant miss rate of 0.0367% on automatically selected high-confidence studies. Thus, DeepRadiologyNet enables significant reduction in the workload of human radiologists by automatically filtering studies and reporting on the high-confidence ones at an operating point well below the literal error rate for US Board Certified radiologists, estimated at 0.82%.



### In2I : Unsupervised Multi-Image-to-Image Translation Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.09334v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09334v1)
- **Published**: 2017-11-26 04:57:24+00:00
- **Updated**: 2017-11-26 04:57:24+00:00
- **Authors**: Pramuditha Perera, Mahdi Abavisani, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: In unsupervised image-to-image translation, the goal is to learn the mapping between an input image and an output image using a set of unpaired training images. In this paper, we propose an extension of the unsupervised image-to-image translation problem to multiple input setting. Given a set of paired images from multiple modalities, a transformation is learned to translate the input into a specified domain. For this purpose, we introduce a Generative Adversarial Network (GAN) based framework along with a multi-modal generator structure and a new loss term, latent consistency loss. Through various experiments we show that leveraging multiple inputs generally improves the visual quality of the translated images. Moreover, we show that the proposed method outperforms current state-of-the-art unsupervised image-to-image translation methods.



### Semantically Consistent Image Completion with Fine-grained Details
- **Arxiv ID**: http://arxiv.org/abs/1711.09345v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09345v1)
- **Published**: 2017-11-26 07:42:17+00:00
- **Updated**: 2017-11-26 07:42:17+00:00
- **Authors**: Pengpeng Liu, Xiaojuan Qi, Pinjia He, Yikang Li, Michael R. Lyu, Irwin King
- **Comment**: 9 pages plus 2-page supplement
- **Journal**: None
- **Summary**: Image completion has achieved significant progress due to advances in generative adversarial networks (GANs). Albeit natural-looking, the synthesized contents still lack details, especially for scenes with complex structures or images with large holes. This is because there exists a gap between low-level reconstruction loss and high-level adversarial loss. To address this issue, we introduce a perceptual network to provide mid-level guidance, which measures the semantical similarity between the synthesized and original contents in a similarity-enhanced space. We conduct a detailed analysis on the effects of different losses and different levels of perceptual features in image completion, showing that there exist complementarity between adversarial training and perceptual features. By combining them together, our model can achieve nearly seamless fusion results in an end-to-end manner. Moreover, we design an effective lightweight generator architecture, which can achieve effective image inpainting with far less parameters. Evaluated on CelebA Face and Paris StreetView dataset, our proposed method significantly outperforms existing methods.



### HashGAN:Attention-aware Deep Adversarial Hashing for Cross Modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1711.09347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09347v1)
- **Published**: 2017-11-26 08:14:13+00:00
- **Updated**: 2017-11-26 08:14:13+00:00
- **Authors**: Xi Zhang, Siyu Zhou, Jiashi Feng, Hanjiang Lai, Bo Li, Yan Pan, Jian Yin, Shuicheng Yan
- **Comment**: 10 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: As the rapid growth of multi-modal data, hashing methods for cross-modal retrieval have received considerable attention. Deep-networks-based cross-modal hashing methods are appealing as they can integrate feature learning and hash coding into end-to-end trainable frameworks. However, it is still challenging to find content similarities between different modalities of data due to the heterogeneity gap. To further address this problem, we propose an adversarial hashing network with attention mechanism to enhance the measurement of content similarities by selectively focusing on informative parts of multi-modal data. The proposed new adversarial network, HashGAN, consists of three building blocks: 1) the feature learning module to obtain feature representations, 2) the generative attention module to generate an attention mask, which is used to obtain the attended (foreground) and the unattended (background) feature representations, 3) the discriminative hash coding module to learn hash functions that preserve the similarities between different modalities. In our framework, the generative module and the discriminative module are trained in an adversarial way: the generator is learned to make the discriminator cannot preserve the similarities of multi-modal data w.r.t. the background feature representations, while the discriminator aims to preserve the similarities of multi-modal data w.r.t. both the foreground and the background feature representations. Extensive evaluations on several benchmark datasets demonstrate that the proposed HashGAN brings substantial improvements over other state-of-the-art cross-modal hashing methods.



### Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline)
- **Arxiv ID**: http://arxiv.org/abs/1711.09349v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09349v3)
- **Published**: 2017-11-26 08:44:53+00:00
- **Updated**: 2018-01-09 07:11:53+00:00
- **Authors**: Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, Shengjin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Employing part-level features for pedestrian image description offers fine-grained information and has been verified as beneficial for person retrieval in very recent literature. A prerequisite of part discovery is that each part should be well located. Instead of using external cues, e.g., pose estimation, to directly locate parts, this paper lays emphasis on the content consistency within each part.   Specifically, we target at learning discriminative part-informed features for person retrieval and make two contributions. (i) A network named Part-based Convolutional Baseline (PCB). Given an image input, it outputs a convolutional descriptor consisting of several part-level features. With a uniform partition strategy, PCB achieves competitive results with the state-of-the-art methods, proving itself as a strong convolutional baseline for person retrieval.   (ii) A refined part pooling (RPP) method. Uniform partition inevitably incurs outliers in each part, which are in fact more similar to other parts. RPP re-assigns these outliers to the parts they are closest to, resulting in refined parts with enhanced within-part consistency. Experiment confirms that RPP allows PCB to gain another round of performance boost. For instance, on the Market-1501 dataset, we achieve (77.4+4.2)% mAP and (92.3+1.5)% rank-1 accuracy, surpassing the state of the art by a large margin.



### Automatic Color Image Segmentation Using a Square Elemental Region-Based Seeded Region Growing and Merging Method
- **Arxiv ID**: http://arxiv.org/abs/1711.09352v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09352v1)
- **Published**: 2017-11-26 09:19:05+00:00
- **Updated**: 2017-11-26 09:19:05+00:00
- **Authors**: Hisashi Shimodaira
- **Comment**: 14 pages with 9 figures and 3 tables
- **Journal**: None
- **Summary**: This paper presents an efficient automatic color image segmentation method using a seeded region growing and merging method based on square elemental regions. Our segmentation method consists of the three steps: generating seed regions, merging the regions, and applying a pixel-wise boundary determination algorithm to the resultant polygonal regions. The major features of our method are as follows: the use of square elemental regions instead of pixels as the processing unit, a seed generation method based on enhanced gradient values, a seed region growing method exploiting local gradient values, a region merging method using a similarity measure including a homogeneity distance based on Tsallis entropy, and a termination condition of region merging using an estimated desired number of regions. Using square regions as the processing unit substantially reduces the time complexity of the algorithm and makes the performance stable. The experimental results show that our method exhibits stable performance for a variety of natural images, including heavily textured areas, and produces good segmentation results using the same parameter values. The results of our method are fairly comparable to, and in some respects better than, those of existing algorithms.



### Feature Map Pooling for Cross-View Gait Recognition Based on Silhouette Sequence Images
- **Arxiv ID**: http://arxiv.org/abs/1711.09358v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09358v1)
- **Published**: 2017-11-26 09:48:39+00:00
- **Updated**: 2017-11-26 09:48:39+00:00
- **Authors**: Qiang Chen, Yunhong Wang, Zheng Liu, Qingjie Liu, Di Huang
- **Comment**: Accepted to IJCB2017
- **Journal**: None
- **Summary**: In this paper, we develop a novel convolutional neural network based approach to extract and aggregate useful information from gait silhouette sequence images instead of simply representing the gait process by averaging silhouette images. The network takes a pair of arbitrary length sequence images as inputs and extracts features for each silhouette independently. Then a feature map pooling strategy is adopted to aggregate sequence features. Subsequently, a network which is similar to Siamese network is designed to perform recognition. The proposed network is simple and easy to implement and can be trained in an end-to-end manner. Cross-view gait recognition experiments are conducted on OU-ISIR large population dataset. The results demonstrate that our network can extract and aggregate features from silhouette sequence effectively. It also achieves significant equal error rates and comparable identification rates when compared with the state of the art.



### Personalized and Occupational-aware Age Progression by Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.09368v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09368v2)
- **Published**: 2017-11-26 10:50:56+00:00
- **Updated**: 2017-12-01 06:58:03+00:00
- **Authors**: Siyu Zhou, Weiqiang Zhao, Jiashi Feng, Hanjiang Lai, Yan Pan, Jian Yin, Shuicheng Yan
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: Face age progression, which aims to predict the future looks, is important for various applications and has been received considerable attentions. Existing methods and datasets are limited in exploring the effects of occupations which may influence the personal appearances. In this paper, we firstly introduce an occupational face aging dataset for studying the influences of occupations on the appearances. It includes five occupations, which enables the development of new algorithms for age progression and facilitate future researches. Second, we propose a new occupational-aware adversarial face aging network, which learns human aging process under different occupations. Two factors are taken into consideration in our aging process: personality-preserving and visually plausible texture change for different occupations. We propose personalized network with personalized loss in deep autoencoder network for keeping personalized facial characteristics, and occupational-aware adversarial network with occupational-aware adversarial loss for obtaining more realistic texture changes. Experimental results well demonstrate the advantages of the proposed method by comparing with other state-of-the-arts age progression methods.



### Compressive Sensing of Color Images Using Nonlocal Higher Order Dictionary
- **Arxiv ID**: http://arxiv.org/abs/1711.09375v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.09375v1)
- **Published**: 2017-11-26 12:31:24+00:00
- **Updated**: 2017-11-26 12:31:24+00:00
- **Authors**: Khanh Quoc Dinh, Thuong Nguyen Canh, Byeungwoo Jeon
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: This paper addresses an ill-posed problem of recovering a color image from its compressively sensed measurement data. Differently from the typical 1D vector-based approach of the state-of-the-art methods, we exploit the nonlocal similarities inherently existing in images by treating each patch of a color image as a 3D tensor consisting of not only horizontal and vertical but also spectral dimensions. A group of nonlocal similar patches form a 4D tensor for which a nonlocal higher order dictionary is learned via higher order singular value decomposition. The multiple sub-dictionaries contained in the higher order dictionary decorrelate the group in each corresponding dimension, thus help the detail of color images to be reconstructed better. Furthermore, we promote sparsity of the final solution using a sparsity regularization based on a weight tensor. It can distinguish those coefficients of the sparse representation generated by the higher order dictionary which are expected to have large magnitude from the others in the optimization. Accordingly, in the iterative solution, it acts like a weighting process which is designed by approximating the minimum mean squared error filter for more faithful recovery. Experimental results confirm improvement by the proposed method over the state-of-the-art ones.



### Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients
- **Arxiv ID**: http://arxiv.org/abs/1711.09404v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.09404v1)
- **Published**: 2017-11-26 15:20:46+00:00
- **Updated**: 2017-11-26 15:20:46+00:00
- **Authors**: Andrew Slavin Ross, Finale Doshi-Velez
- **Comment**: To appear in AAAI 2018
- **Journal**: None
- **Summary**: Deep neural networks have proven remarkably effective at solving many classification problems, but have been criticized recently for two major weaknesses: the reasons behind their predictions are uninterpretable, and the predictions themselves can often be fooled by small adversarial perturbations. These problems pose major obstacles for the adoption of neural networks in domains that require security or transparency. In this work, we evaluate the effectiveness of defenses that differentiably penalize the degree to which small changes in inputs can alter model predictions. Across multiple attacks, architectures, defenses, and datasets, we find that neural networks trained with this input gradient regularization exhibit robustness to transferred adversarial examples generated to fool all of the other models. We also find that adversarial examples generated to fool gradient-regularized models fool all other models equally well, and actually lead to more "legitimate," interpretable misclassifications as rated by people (which we confirm in a human subject experiment). Finally, we demonstrate that regularizing input gradients makes them more naturally interpretable as rationales for model predictions. We conclude by discussing this relationship between interpretability and robustness in deep neural networks.



### Learning a Rotation Invariant Detector with Rotatable Bounding Box
- **Arxiv ID**: http://arxiv.org/abs/1711.09405v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09405v1)
- **Published**: 2017-11-26 15:20:52+00:00
- **Updated**: 2017-11-26 15:20:52+00:00
- **Authors**: Lei Liu, Zongxu Pan, Bin Lei
- **Comment**: None
- **Journal**: None
- **Summary**: Detection of arbitrarily rotated objects is a challenging task due to the difficulties of locating the multi-angle objects and separating them effectively from the background. The existing methods are not robust to angle varies of the objects because of the use of traditional bounding box, which is a rotation variant structure for locating rotated objects. In this article, a new detection method is proposed which applies the newly defined rotatable bounding box (RBox). The proposed detector (DRBox) can effectively handle the situation where the orientation angles of the objects are arbitrary. The training of DRBox forces the detection networks to learn the correct orientation angle of the objects, so that the rotation invariant property can be achieved. DRBox is tested to detect vehicles, ships and airplanes on satellite images, compared with Faster R-CNN and SSD, which are chosen as the benchmark of the traditional bounding box based methods. The results shows that DRBox performs much better than traditional bounding box based methods do on the given tasks, and is more robust against rotation of input image and target objects. Besides, results show that DRBox correctly outputs the orientation angles of the objects, which is very useful for locating multi-angle objects efficiently. The code and models are available at https://github.com/liulei01/DRBox.



### MAVOT: Memory-Augmented Video Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1711.09414v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.09414v1)
- **Published**: 2017-11-26 16:20:45+00:00
- **Updated**: 2017-11-26 16:20:45+00:00
- **Authors**: Boyu Liu, Yanzhao Wang, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: Submitted to CVPR2018
- **Journal**: None
- **Summary**: We introduce a one-shot learning approach for video object tracking. The proposed algorithm requires seeing the object to be tracked only once, and employs an external memory to store and remember the evolving features of the foreground object as well as backgrounds over time during tracking. With the relevant memory retrieved and updated in each tracking, our tracking model is capable of maintaining long-term memory of the object, and thus can naturally deal with hard tracking scenarios including partial and total occlusion, motion changes and large scale and shape variations. In our experiments we use the ImageNet ILSVRC2015 video detection dataset to train and use the VOT-2016 benchmark to test and compare our Memory-Augmented Video Object Tracking (MAVOT) model. From the results, we conclude that given its oneshot property and simplicity in design, MAVOT is an attractive approach in visual tracking because it shows good performance on VOT-2016 benchmark and is among the top 5 performers in accuracy and robustness in occlusion, motion changes and empty target.



### Local Jet Pattern: A Robust Descriptor for Texture Classification
- **Arxiv ID**: http://arxiv.org/abs/1711.10921v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10921v3)
- **Published**: 2017-11-26 17:18:46+00:00
- **Updated**: 2018-12-04 02:01:15+00:00
- **Authors**: Swalpa Kumar Roy, Bhabatosh Chanda, Bidyut B. Chaudhuri, Dipak Kumar Ghosh, Shiv Ram Dubey
- **Comment**: Accepted in Multimedia Tools and Applications, Springer
- **Journal**: None
- **Summary**: Methods based on local image features have recently shown promise for texture classification tasks, especially in the presence of large intra-class variation due to illumination, scale, and viewpoint changes. Inspired by the theories of image structure analysis, this paper presents a simple, efficient, yet robust descriptor namely local jet pattern (LJP) for texture classification. In this approach, a jet space representation of a texture image is derived from a set of derivatives of Gaussian (DtGs) filter responses up to second order, so called local jet vectors (LJV), which also satisfy the Scale Space properties. The LJP is obtained by utilizing the relationship of center pixel with the local neighborhood information in jet space. Finally, the feature vector of a texture region is formed by concatenating the histogram of LJP for all elements of LJV. All DtGs responses up to second order together preserves the intrinsic local image structure, and achieves invariance to scale, rotation, and reflection. This allows us to develop a texture classification framework which is discriminative and robust. Extensive experiments on five standard texture image databases, employing nearest subspace classifier (NSC), the proposed descriptor achieves 100%, 99.92%, 99.75%, 99.16%, and 99.65% accuracy for Outex_TC-00010 (Outex_TC10), and Outex_TC-00012 (Outex_TC12), KTH-TIPS, Brodatz, CUReT, respectively, which are outperforms the state-of-the-art methods.



### Coplanar Repeats by Energy Minimization
- **Arxiv ID**: http://arxiv.org/abs/1711.09432v1
- **DOI**: 10.5244/C.30.107
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09432v1)
- **Published**: 2017-11-26 17:56:08+00:00
- **Updated**: 2017-11-26 17:56:08+00:00
- **Authors**: James Pritts, Denys Rozumnyi, M. Pawan Kumar, Ondrej Chum
- **Comment**: 14 pages with supplemental materials attached
- **Journal**: Proceedings of the British Machine Vision Conference (BMVC) 2016
- **Summary**: This paper proposes an automated method to detect, group and rectify arbitrarily-arranged coplanar repeated elements via energy minimization. The proposed energy functional combines several features that model how planes with coplanar repeats are projected into images and captures global interactions between different coplanar repeat groups and scene planes. An inference framework based on a recent variant of $\alpha$-expansion is described and fast convergence is demonstrated. We compare the proposed method to two widely-used geometric multi-model fitting methods using a new dataset of annotated images containing multiple scene planes with coplanar repeats in varied arrangements. The evaluation shows a significant improvement in the accuracy of rectifications computed from coplanar repeats detected with the proposed method versus those detected with the baseline methods.



### STAR-RT: Visual attention for real-time video game playing
- **Arxiv ID**: http://arxiv.org/abs/1711.09464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09464v1)
- **Published**: 2017-11-26 21:24:52+00:00
- **Updated**: 2017-11-26 21:24:52+00:00
- **Authors**: Iuliia Kotseruba, John K. Tsotsos
- **Comment**: 21 page, 13 figures
- **Journal**: None
- **Summary**: In this paper we present STAR-RT - the first working prototype of Selective Tuning Attention Reference (STAR) model and Cognitive Programs (CPs). The Selective Tuning (ST) model received substantial support through psychological and neurophysiological experiments. The STAR framework expands ST and applies it to practical visual tasks. In order to do so, similarly to many cognitive architectures, STAR combines the visual hierarchy (based on ST) with the executive controller, working and short-term memory components and fixation controller. CPs in turn enable the communication among all these elements for visual task execution. To test the relevance of the system in a realistic context, we implemented the necessary components of STAR and designed CPs for playing two closed-source video games - Canabaltand Robot Unicorn Attack. Since both games run in a browser window, our algorithm has the same amount of information and the same amount of time to react to the events on the screen as a human player would. STAR-RT plays both games in real time using only visual input and achieves scores comparable to human expert players. It thus provides an existence proof for the utility of the particular CP structure and primitives used and the potential for continued experimentation and verification of their utility in broader scenarios.



### An Introduction to Deep Visual Explanation
- **Arxiv ID**: http://arxiv.org/abs/1711.09482v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.09482v2)
- **Published**: 2017-11-26 22:54:18+00:00
- **Updated**: 2018-03-15 19:18:33+00:00
- **Authors**: Housam Khalifa Bashier Babiker, Randy Goebel
- **Comment**: Accepted at NIPS 2017 - Workshop Interpreting, Explaining and
  Visualizing Deep Learning
- **Journal**: None
- **Summary**: The practical impact of deep learning on complex supervised learning problems has been significant, so much so that almost every Artificial Intelligence problem, or at least a portion thereof, has been somehow recast as a deep learning problem. The applications appeal is significant, but this appeal is increasingly challenged by what some call the challenge of explainability, or more generally the more traditional challenge of debuggability: if the outcomes of a deep learning process produce unexpected results (e.g., less than expected performance of a classifier), then there is little available in the way of theories or tools to help investigate the potential causes of such unexpected behavior, especially when this behavior could impact people's lives. We describe a preliminary framework to help address this issue, which we call "deep visual explanation" (DVE). "Deep," because it is the development and performance of deep neural network models that we want to understand. "Visual," because we believe that the most rapid insight into a complex multi-dimensional model is provided by appropriate visualization techniques, and "Explanation," because in the spectrum from instrumentation by inserting print statements to the abductive inference of explanatory hypotheses, we believe that the key to understanding deep learning relies on the identification and exposure of hypotheses about the performance behavior of a learned deep model. In the exposition of our preliminary framework, we use relatively straightforward image classification examples and a variety of choices on initial configuration of a deep model building scenario. By careful but not complicated instrumentation, we expose classification outcomes of deep models using visualization, and also show initial results for one potential application of interpretability.



### SkipNet: Learning Dynamic Routing in Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.09485v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.09485v2)
- **Published**: 2017-11-26 23:03:37+00:00
- **Updated**: 2018-07-25 06:13:24+00:00
- **Authors**: Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, Joseph E. Gonzalez
- **Comment**: ECCV 2018 Camera ready version. Code is available at
  https://github.com/ucbdrive/skipnet
- **Journal**: None
- **Summary**: While deeper convolutional networks are needed to achieve maximum accuracy in visual perception tasks, for many inputs shallower networks are sufficient. We exploit this observation by learning to skip convolutional layers on a per-input basis. We introduce SkipNet, a modified residual network, that uses a gating network to selectively skip convolutional blocks based on the activations of the previous layer. We formulate the dynamic skipping problem in the context of sequential decision making and propose a hybrid learning algorithm that combines supervised learning and reinforcement learning to address the challenges of non-differentiable skipping decisions. We show SkipNet reduces computation by 30-90% while preserving the accuracy of the original model on four benchmark datasets and outperforms the state-of-the-art dynamic networks and static compression methods. We also qualitatively evaluate the gating policy to reveal a relationship between image scale and saliency and the number of layers skipped.



### Robust Subspace Learning: Robust PCA, Robust Subspace Tracking, and Robust Subspace Recovery
- **Arxiv ID**: http://arxiv.org/abs/1711.09492v4
- **DOI**: 10.1109/MSP.2018.2826566
- **Categories**: **cs.IT**, cs.CV, math.IT, stat.ME, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.09492v4)
- **Published**: 2017-11-26 23:52:53+00:00
- **Updated**: 2018-07-05 22:46:31+00:00
- **Authors**: Namrata Vaswani, Thierry Bouwmans, Sajid Javed, Praneeth Narayanamurthy
- **Comment**: To appear, IEEE Signal Processing Magazine, July 2018
- **Journal**: IEEE Signal Processing Magazine (Volume: 35, Issue: 4, July 2018)
- **Summary**: PCA is one of the most widely used dimension reduction techniques. A related easier problem is "subspace learning" or "subspace estimation". Given relatively clean data, both are easily solved via singular value decomposition (SVD). The problem of subspace learning or PCA in the presence of outliers is called robust subspace learning or robust PCA (RPCA). For long data sequences, if one tries to use a single lower dimensional subspace to represent the data, the required subspace dimension may end up being quite large. For such data, a better model is to assume that it lies in a low-dimensional subspace that can change over time, albeit gradually. The problem of tracking such data (and the subspaces) while being robust to outliers is called robust subspace tracking (RST). This article provides a magazine-style overview of the entire field of robust subspace learning and tracking. In particular solutions for three problems are discussed in detail: RPCA via sparse+low-rank matrix decomposition (S+LR), RST via S+LR, and "robust subspace recovery (RSR)". RSR assumes that an entire data vector is either an outlier or an inlier. The S+LR formulation instead assumes that outliers occur on only a few data vector indices and hence are well modeled as sparse corruptions.



