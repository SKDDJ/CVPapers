# Arxiv Papers in cs.CV on 2017-11-30
### Embedded Real-Time Fall Detection Using Deep Learning For Elderly Care
- **Arxiv ID**: http://arxiv.org/abs/1711.11200v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.11200v1)
- **Published**: 2017-11-30 03:07:14+00:00
- **Updated**: 2017-11-30 03:07:14+00:00
- **Authors**: Hyunwoo Lee, Jooyoung Kim, Dojun Yang, Joon-Ho Kim
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: This paper proposes a real-time embedded fall detection system using a DVS(Dynamic Vision Sensor) that has never been used for traditional fall detection, a dataset for fall detection using that, and a DVS-TN(DVS-Temporal Network). The first contribution is building a DVS Falls Dataset, which made our network to recognize a much greater variety of falls than the existing datasets that existed before and solved privacy issues using the DVS. Secondly, we introduce the DVS-TN : optimized deep learning network to detect falls using DVS. Finally, we implemented a fall detection system which can run on low-computing H/W with real-time, and tested on DVS Falls Dataset that takes into account various falls situations. Our approach achieved 95.5% on the F1-score and operates at 31.25 FPS on NVIDIA Jetson TX1 board.



### Future Person Localization in First-Person Videos
- **Arxiv ID**: http://arxiv.org/abs/1711.11217v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11217v2)
- **Published**: 2017-11-30 04:16:03+00:00
- **Updated**: 2018-03-28 01:29:15+00:00
- **Authors**: Takuma Yagi, Karttikeya Mangalam, Ryo Yonetani, Yoichi Sato
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: We present a new task that predicts future locations of people observed in first-person videos. Consider a first-person video stream continuously recorded by a wearable camera. Given a short clip of a person that is extracted from the complete stream, we aim to predict that person's location in future frames. To facilitate this future person localization ability, we make the following three key observations: a) First-person videos typically involve significant ego-motion which greatly affects the location of the target person in future frames; b) Scales of the target person act as a salient cue to estimate a perspective effect in first-person videos; c) First-person videos often capture people up-close, making it easier to leverage target poses (e.g., where they look) for predicting their future locations. We incorporate these three observations into a prediction framework with a multi-stream convolution-deconvolution architecture. Experimental results reveal our method to be effective on our new dataset as well as on a public social interaction dataset.



### Properties on n-dimensional convolution for image deconvolution
- **Arxiv ID**: http://arxiv.org/abs/1711.11224v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.11224v1)
- **Published**: 2017-11-30 04:45:41+00:00
- **Updated**: 2017-11-30 04:45:41+00:00
- **Authors**: Song Yizhi, Xu Cheng, Ding Daoxin, Zhou Hang, Quan Tingwei, Li Shiwei
- **Comment**: None
- **Journal**: None
- **Summary**: Convolution system is linear and time invariant, and can describe the optical imaging process. Based on convolution system, many deconvolution techniques have been developed for optical image analysis, such as boosting the space resolution of optical images, image denoising, image enhancement and so on. Here, we gave properties on N-dimensional convolution. By using these properties, we proposed image deconvolution method. This method uses a series of convolution operations to deconvolute image. We demonstrated that the method has the similar deconvolution results to the state-of-art method. The core calculation of the proposed method is image convolution, and thus our method can easily be integrated into GPU mode for large-scale image deconvolution.



### A Closer Look at Spatiotemporal Convolutions for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.11248v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11248v3)
- **Published**: 2017-11-30 06:28:20+00:00
- **Updated**: 2018-04-12 01:07:30+00:00
- **Authors**: Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, Manohar Paluri
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly advantages in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block "R(2+1)D" which gives rise to CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101 and HMDB51.



### ArbiText: Arbitrary-Oriented Text Detection in Unconstrained Scene
- **Arxiv ID**: http://arxiv.org/abs/1711.11249v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11249v1)
- **Published**: 2017-11-30 06:30:11+00:00
- **Updated**: 2017-11-30 06:30:11+00:00
- **Authors**: Daitao Xing, Zichen Li, Xin Chen, Yi Fang
- **Comment**: 10pages, 28 figures
- **Journal**: None
- **Summary**: Arbitrary-oriented text detection in the wild is a very challenging task, due to the aspect ratio, scale, orientation, and illumination variations. In this paper, we propose a novel method, namely Arbitrary-oriented Text (or ArbText for short) detector, for efficient text detection in unconstrained natural scene images. Specifically, we first adopt the circle anchors rather than the rectangular ones to represent bounding boxes, which is more robust to orientation variations. Subsequently, we incorporate a pyramid pooling module into the Single Shot MultiBox Detector framework, in order to simultaneously explore the local and global visual information, which can, therefore, generate more confidential detection results. Experiments on established scene-text datasets, such as the ICDAR 2015 and MSRA-TD500 datasets, have demonstrated the supe rior performance of the proposed method, compared to the state-of-the-art approaches.



### A novel graph structure for salient object detection based on divergence background and compact foreground
- **Arxiv ID**: http://arxiv.org/abs/1711.11266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11266v1)
- **Published**: 2017-11-30 08:41:26+00:00
- **Updated**: 2017-11-30 08:41:26+00:00
- **Authors**: Chenxing Xia, Hanling Zhang, Keqin Li
- **Comment**: 22 pages,16 figures, 2 tables
- **Journal**: None
- **Summary**: In this paper, we propose an efficient and discriminative model for salient object detection. Our method is carried out in a stepwise mechanism based on both divergence background and compact foreground cues. In order to effectively enhance the distinction between nodes along object boundaries and the similarity among object regions, a graph is constructed by introducing the concept of virtual node. To remove incorrect outputs, a scheme for selecting background seeds and a method for generating compactness foreground regions are introduced, respectively. Different from prior methods, we calculate the saliency value of each node based on the relationship between the corresponding node and the virtual node. In order to achieve significant performance improvement consistently, we propose an Extended Manifold Ranking (EMR) algorithm, which subtly combines suppressed / active nodes and mid-level information. Extensive experimental results demonstrate that the proposed algorithm performs favorably against the state-of-art saliency detection methods in terms of different evaluation metrics on several benchmark datasets.



### Unsupervised Learning for Cell-level Visual Representation in Histopathology Images with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.11317v4
- **DOI**: 10.1109/JBHI.2018.2852639
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11317v4)
- **Published**: 2017-11-30 10:53:56+00:00
- **Updated**: 2018-07-07 07:31:18+00:00
- **Authors**: Bo Hu, Ye Tang, Eric I-Chao Chang, Yubo Fan, Maode Lai, Yan Xu
- **Comment**: Accepted for publication in IEEE Journal of Biomedical and Health
  Informatics
- **Journal**: None
- **Summary**: The visual attributes of cells, such as the nuclear morphology and chromatin openness, are critical for histopathology image analysis. By learning cell-level visual representation, we can obtain a rich mix of features that are highly reusable for various tasks, such as cell-level classification, nuclei segmentation, and cell counting. In this paper, we propose a unified generative adversarial networks architecture with a new formulation of loss to perform robust cell-level visual representation learning in an unsupervised setting. Our model is not only label-free and easily trained but also capable of cell-level unsupervised classification with interpretable visualization, which achieves promising results in the unsupervised classification of bone marrow cellular components. Based on the proposed cell-level visual representation learning, we further develop a pipeline that exploits the varieties of cellular elements to perform histopathology image classification, the advantages of which are demonstrated on bone marrow datasets.



### High Dynamic Range Imaging Technology
- **Arxiv ID**: http://arxiv.org/abs/1711.11326v1
- **DOI**: 10.1109/MSP.2017.2716957
- **Categories**: **cs.GR**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1711.11326v1)
- **Published**: 2017-11-30 11:38:11+00:00
- **Updated**: 2017-11-30 11:38:11+00:00
- **Authors**: Alessandro Artusi, Thomas Richter, Touradj Ebrahimi, Rafal K. Mantiuk
- **Comment**: Lecture Notes
- **Journal**: IEEE Signal Processing Magazine ( Volume: 34, Issue: 5, Sept. 2017
  )
- **Summary**: In this lecture note, we describe high dynamic range (HDR) imaging systems; such systems are able to represent luminances of much larger brightness and, typically, also a larger range of colors than conventional standard dynamic range (SDR) imaging systems. The larger luminance range greatly improve the overall quality of visual content, making it appears much more realistic and appealing to observers. HDR is one of the key technologies of the future imaging pipeline, which will change the way the digital visual content is represented and manipulated today.



### Radially-Distorted Conjugate Translations
- **Arxiv ID**: http://arxiv.org/abs/1711.11339v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11339v3)
- **Published**: 2017-11-30 12:24:31+00:00
- **Updated**: 2018-06-21 12:47:57+00:00
- **Authors**: James Pritts, Zuzana Kukelova, Viktor Larsson, Ondrej Chum
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces the first minimal solvers that jointly solve for affine-rectification and radial lens distortion from coplanar repeated patterns. Even with imagery from moderately distorted lenses, plane rectification using the pinhole camera model is inaccurate or invalid. The proposed solvers incorporate lens distortion into the camera model and extend accurate rectification to wide-angle imagery, which is now common from consumer cameras. The solvers are derived from constraints induced by the conjugate translations of an imaged scene plane, which are integrated with the division model for radial lens distortion. The hidden-variable trick with ideal saturation is used to reformulate the constraints so that the solvers generated by the Grobner-basis method are stable, small and fast.   Rectification and lens distortion are recovered from either one conjugately translated affine-covariant feature or two independently translated similarity-covariant features. The proposed solvers are used in a \RANSAC-based estimator, which gives accurate rectifications after few iterations. The proposed solvers are evaluated against the state-of-the-art and demonstrate significantly better rectifications on noisy measurements. Qualitative results on diverse imagery demonstrate high-accuracy undistortions and rectifications. The source code is publicly available at https://github.com/prittjam/repeats.



### 3DContextNet: K-d Tree Guided Hierarchical Learning of Point Clouds Using Local and Global Contextual Cues
- **Arxiv ID**: http://arxiv.org/abs/1711.11379v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11379v3)
- **Published**: 2017-11-30 13:26:55+00:00
- **Updated**: 2018-12-04 13:45:45+00:00
- **Authors**: Wei Zeng, Theo Gevers
- **Comment**: 15 pages, 5 figures
- **Journal**: None
- **Summary**: Classification and segmentation of 3D point clouds are important tasks in computer vision. Because of the irregular nature of point clouds, most of the existing methods convert point clouds into regular 3D voxel grids before they are used as input for ConvNets. Unfortunately, voxel representations are highly insensitive to the geometrical nature of 3D data. More recent methods encode point clouds to higher dimensional features to cover the global 3D space. However, these models are not able to sufficiently capture the local structures of point clouds.   Therefore, in this paper, we propose a method that exploits both local and global contextual cues imposed by the k-d tree. The method is designed to learn representation vectors progressively along the tree structure. Experiments on challenging benchmarks show that the proposed model provides discriminative point set features. For the task of 3D scene semantic segmentation, our method significantly outperforms the state-of-the-art on the Stanford Large-Scale 3D Indoor Spaces Dataset (S3DIS).



### MR image reconstruction using deep density priors
- **Arxiv ID**: http://arxiv.org/abs/1711.11386v4
- **DOI**: 10.1109/TMI.2018.2887072
- **Categories**: **cs.CV**, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.11386v4)
- **Published**: 2017-11-30 13:36:58+00:00
- **Updated**: 2018-12-19 18:00:04+00:00
- **Authors**: Kerem C. Tezcan, Christian F. Baumgartner, Roger Luechinger, Klaas P. Pruessmann, Ender Konukoglu
- **Comment**: Published in IEEE TMI. Main text and supplementary material, 19 pages
  total
- **Journal**: IEEE Transactions on Medical Imaging, December 2018
- **Summary**: Algorithms for Magnetic Resonance (MR) image reconstruction from undersampled measurements exploit prior information to compensate for missing k-space data. Deep learning (DL) provides a powerful framework for extracting such information from existing image datasets, through learning, and then using it for reconstruction. Leveraging this, recent methods employed DL to learn mappings from undersampled to fully sampled images using paired datasets, including undersampled and corresponding fully sampled images, integrating prior knowledge implicitly. In this article, we propose an alternative approach that learns the probability distribution of fully sampled MR images using unsupervised DL, specifically Variational Autoencoders (VAE), and use this as an explicit prior term in reconstruction, completely decoupling the encoding operation from the prior. The resulting reconstruction algorithm enjoys a powerful image prior to compensate for missing k-space data without requiring paired datasets for training nor being prone to associated sensitivities, such as deviations in undersampling patterns used in training and test time or coil settings. We evaluated the proposed method with T1 weighted images from a publicly available dataset, multi-coil complex images acquired from healthy volunteers (N=8) and images with white matter lesions. The proposed algorithm, using the VAE prior, produced visually high quality reconstructions and achieved low RMSE values, outperforming most of the alternative methods on the same dataset. On multi-coil complex data, the algorithm yielded accurate magnitude and phase reconstruction results. In the experiments on images with white matter lesions, the method faithfully reconstructed the lesions.   Keywords: Reconstruction, MRI, prior probability, machine learning, deep learning, unsupervised learning, density estimation



### ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases
- **Arxiv ID**: http://arxiv.org/abs/1711.11443v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.CY, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.11443v2)
- **Published**: 2017-11-30 14:50:55+00:00
- **Updated**: 2018-07-20 16:57:30+00:00
- **Authors**: Pierre Stock, Moustapha Cisse
- **Comment**: ECCV 2018 camera-ready
- **Journal**: None
- **Summary**: ConvNets and Imagenet have driven the recent success of deep learning for image classification. However, the marked slowdown in performance improvement combined with the lack of robustness of neural networks to adversarial examples and their tendency to exhibit undesirable biases question the reliability of these methods. This work investigates these questions from the perspective of the end-user by using human subject studies and explanations. The contribution of this study is threefold. We first experimentally demonstrate that the accuracy and robustness of ConvNets measured on Imagenet are vastly underestimated. Next, we show that explanations can mitigate the impact of misclassified adversarial examples from the perspective of the end-user. We finally introduce a novel tool for uncovering the undesirable biases learned by a model. These contributions also show that explanations are a valuable tool both for improving our understanding of ConvNets' predictions and for designing more reliable models.



### Improving Video Generation for Multi-functional Applications
- **Arxiv ID**: http://arxiv.org/abs/1711.11453v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11453v2)
- **Published**: 2017-11-30 14:55:16+00:00
- **Updated**: 2018-03-14 20:56:50+00:00
- **Authors**: Bernhard Kratzwald, Zhiwu Huang, Danda Pani Paudel, Acharya Dinesh, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we aim to improve the state-of-the-art video generative adversarial networks (GANs) with a view towards multi-functional applications. Our improved video GAN model does not separate foreground from background nor dynamic from static patterns, but learns to generate the entire video clip conjointly. Our model can thus be trained to generate - and learn from - a broad set of videos with no restriction. This is achieved by designing a robust one-stream video generation architecture with an extension of the state-of-the-art Wasserstein GAN framework that allows for better convergence. The experimental results show that our improved video GAN model outperforms state-of-theart video generative models on multiple challenging datasets. Furthermore, we demonstrate the superiority of our model by successfully extending it to three challenging problems: video colorization, video inpainting, and future prediction. To the best of our knowledge, this is the first work using GANs to colorize and inpaint video clips.



### Spatially-Adaptive Filter Units for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.11473v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11473v2)
- **Published**: 2017-11-30 15:49:13+00:00
- **Updated**: 2018-03-15 16:30:46+00:00
- **Authors**: Domen Tabernik, Matej Kristan, Aleš Leonardis
- **Comment**: Accepted to Computer Vision and Pattern Recognition 2018
- **Journal**: None
- **Summary**: Classical deep convolutional networks increase receptive field size by either gradual resolution reduction or application of hand-crafted dilated convolutions to prevent increase in the number of parameters. In this paper we propose a novel displaced aggregation unit (DAU) that does not require hand-crafting. In contrast to classical filters with units (pixels) placed on a fixed regular grid, the displacement of the DAUs are learned, which enables filters to spatially-adapt their receptive field to a given problem. We extensively demonstrate the strength of DAUs on a classification and semantic segmentation tasks. Compared to ConvNets with regular filter, ConvNets with DAUs achieve comparable performance at faster convergence and up to 3-times reduction in parameters. Furthermore, DAUs allow us to study deep networks from novel perspectives. We study spatial distributions of DAU filters and analyze the number of parameters allocated for spatial coverage in a filter.



### Auxiliary Guided Autoregressive Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1711.11479v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11479v2)
- **Published**: 2017-11-30 15:57:24+00:00
- **Updated**: 2019-04-18 06:49:26+00:00
- **Authors**: Thomas Lucas, Jakob Verbeek
- **Comment**: Published as a conference paper at ECML-PKDD 2018
- **Journal**: None
- **Summary**: Generative modeling of high-dimensional data is a key problem in machine learning. Successful approaches include latent variable models and autoregressive models. The complementary strengths of these approaches, to model global and local image statistics respectively, suggest hybrid models that encode global image structure into latent variables while autoregressively modeling low level detail. Previous approaches to such hybrid models restrict the capacity of the autoregressive decoder to prevent degenerate models that ignore the latent variables and only rely on autoregressive modeling. Our contribution is a training procedure relying on an auxiliary loss function that controls which information is captured by the latent variables and what is left to the autoregressive decoder. Our approach can leverage arbitrarily powerful autoregressive decoders, achieves state-of-the art quantitative performance among models with latent variables, and generates qualitatively convincing samples.



### Convolutional Networks with Adaptive Inference Graphs
- **Arxiv ID**: http://arxiv.org/abs/1711.11503v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.11503v3)
- **Published**: 2017-11-30 16:45:25+00:00
- **Updated**: 2020-05-08 20:20:25+00:00
- **Authors**: Andreas Veit, Serge Belongie
- **Comment**: IJCV 2019
- **Journal**: None
- **Summary**: Do convolutional networks really need a fixed feed-forward structure? What if, after identifying the high-level concept of an image, a network could move directly to a layer that can distinguish fine-grained differences? Currently, a network would first need to execute sometimes hundreds of intermediate layers that specialize in unrelated aspects. Ideally, the more a network already knows about an image, the better it should be at deciding which layer to compute next. In this work, we propose convolutional networks with adaptive inference graphs (ConvNet-AIG) that adaptively define their network topology conditioned on the input image. Following a high-level structure similar to residual networks (ResNets), ConvNet-AIG decides for each input image on the fly which layers are needed. In experiments on ImageNet we show that ConvNet-AIG learns distinct inference graphs for different categories. Both ConvNet-AIG with 50 and 101 layers outperform their ResNet counterpart, while using 20% and 38% less computations respectively. By grouping parameters into layers for related classes and only executing relevant layers, ConvNet-AIG improves both efficiency and overall classification quality. Lastly, we also study the effect of adaptive inference graphs on the susceptibility towards adversarial examples. We observe that ConvNet-AIG shows a higher robustness than ResNets, complementing other known defense mechanisms.



### Single-epoch supernova classification with deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1711.11526v1
- **DOI**: 10.1109/ICDCSW.2017.47
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.11526v1)
- **Published**: 2017-11-30 17:30:39+00:00
- **Updated**: 2017-11-30 17:30:39+00:00
- **Authors**: Akisato Kimura, Ichiro Takahashi, Masaomi Tanaka, Naoki Yasuda, Naonori Ueda, Naoki Yoshida
- **Comment**: 7 pages, published as a workshop paper in ICDCS2017, in June 2017
- **Journal**: Published in: 2017 IEEE 37th International Conference on
  Distributed Computing Systems Workshops (ICDCSW)
- **Summary**: Supernovae Type-Ia (SNeIa) play a significant role in exploring the history of the expansion of the Universe, since they are the best-known standard candles with which we can accurately measure the distance to the objects. Finding large samples of SNeIa and investigating their detailed characteristics have become an important issue in cosmology and astronomy. Existing methods relied on a photometric approach that first measures the luminance of supernova candidates precisely and then fits the results to a parametric function of temporal changes in luminance. However, it inevitably requires multi-epoch observations and complex luminance measurements. In this work, we present a novel method for classifying SNeIa simply from single-epoch observation images without any complex measurements, by effectively integrating the state-of-the-art computer vision methodology into the standard photometric approach. Our method first builds a convolutional neural network for estimating the luminance of supernovae from telescope images, and then constructs another neural network for the classification, where the estimated luminance and observation dates are used as features for classification. Both of the neural networks are integrated into a single deep neural network to classify SNeIa directly from observation images. Experimental results show the effectiveness of the proposed method and reveal classification performance comparable to existing photometric methods with multi-epoch observations.



### Embodied Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1711.11543v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.11543v2)
- **Published**: 2017-11-30 18:06:47+00:00
- **Updated**: 2017-12-01 16:55:05+00:00
- **Authors**: Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra
- **Comment**: 20 pages, 13 figures, Webpage: https://embodiedqa.org/
- **Journal**: None
- **Summary**: We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where an agent is spawned at a random location in a 3D environment and asked a question ("What color is the car?"). In order to answer, the agent must first intelligently navigate to explore the environment, gather information through first-person (egocentric) vision, and then answer the question ("orange").   This challenging task requires a range of AI skills -- active perception, language understanding, goal-driven navigation, commonsense reasoning, and grounding of language into actions. In this work, we develop the environments, end-to-end-trained reinforcement learning agents, and evaluation protocols for EmbodiedQA.



### ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes
- **Arxiv ID**: http://arxiv.org/abs/1711.11556v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11556v2)
- **Published**: 2017-11-30 18:22:30+00:00
- **Updated**: 2018-04-07 16:21:11+00:00
- **Authors**: Yuhua Chen, Wen Li, Luc Van Gool
- **Comment**: Add experiments on SYNTHIA, CVPR 2018 camera-ready version
- **Journal**: None
- **Summary**: Exploiting synthetic data to learn deep models has attracted increasing attention in recent years. However, the intrinsic domain difference between synthetic and real images usually causes a significant performance drop when applying the learned model to real world scenarios. This is mainly due to two reasons: 1) the model overfits to synthetic images, making the convolutional filters incompetent to extract informative representation for real images; 2) there is a distribution difference between synthetic and real data, which is also known as the domain adaptation problem. To this end, we propose a new reality oriented adaptation approach for urban scene semantic segmentation by learning from synthetic data. First, we propose a target guided distillation approach to learn the real image style, which is achieved by training the segmentation model to imitate a pretrained real style model using real images. Second, we further take advantage of the intrinsic spatial structure presented in urban scene images, and propose a spatial-aware adaptation scheme to effectively align the distribution of two domains. These two modules can be readily integrated with existing state-of-the-art semantic segmentation networks to improve their generalizability when adapting from synthetic to real urban scenes. We evaluate the proposed method on Cityscapes dataset by adapting from GTAV and SYNTHIA datasets, where the results demonstrate the effectiveness of our method.



### Hybrid VAE: Improving Deep Generative Models using Partial Observations
- **Arxiv ID**: http://arxiv.org/abs/1711.11566v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.11566v1)
- **Published**: 2017-11-30 18:37:37+00:00
- **Updated**: 2017-11-30 18:37:37+00:00
- **Authors**: Sergey Tulyakov, Andrew Fitzgibbon, Sebastian Nowozin
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural network models trained on large labeled datasets are the state-of-the-art in a large variety of computer vision tasks. In many applications, however, labeled data is expensive to obtain or requires a time consuming manual annotation process. In contrast, unlabeled data is often abundant and available in large quantities. We present a principled framework to capitalize on unlabeled data by training deep generative models on both labeled and unlabeled data. We show that such a combination is beneficial because the unlabeled data acts as a data-driven form of regularization, allowing generative models trained on few labeled samples to reach the performance of fully-supervised generative models trained on much larger datasets. We call our method Hybrid VAE (H-VAE) as it contains both the generative and the discriminative parts. We validate H-VAE on three large-scale datasets of different modalities: two face datasets: (MultiPIE, CelebA) and a hand pose dataset (NYU Hand Pose). Our qualitative visualizations further support improvements achieved by using partial observations.



### Relation Networks for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1711.11575v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11575v2)
- **Published**: 2017-11-30 18:47:41+00:00
- **Updated**: 2018-06-14 11:21:57+00:00
- **Authors**: Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Although it is well believed for years that modeling relations between objects would help object recognition, there has not been evidence that the idea is working in the deep learning era. All state-of-the-art object detection systems still rely on recognizing object instances individually, without exploiting their relations during learning.   This work proposes an object relation module. It processes a set of objects simultaneously through interaction between their appearance feature and geometry, thus allowing modeling of their relations. It is lightweight and in-place. It does not require additional supervision and is easy to embed in existing networks. It is shown effective on improving object recognition and duplicate removal steps in the modern object detection pipeline. It verifies the efficacy of modeling object relations in CNN based detection. It gives rise to the first fully end-to-end object detector.



### Towards High Performance Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1711.11577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11577v1)
- **Published**: 2017-11-30 18:48:45+00:00
- **Updated**: 2017-11-30 18:48:45+00:00
- **Authors**: Xizhou Zhu, Jifeng Dai, Lu Yuan, Yichen Wei
- **Comment**: None
- **Journal**: None
- **Summary**: There has been significant progresses for image object detection in recent years. Nevertheless, video object detection has received little attention, although it is more challenging and more important in practical scenarios.   Built upon the recent works, this work proposes a unified approach based on the principle of multi-frame end-to-end learning of features and cross-frame motion. Our approach extends prior works with three new techniques and steadily pushes forward the performance envelope (speed-accuracy tradeoff), towards high performance video object detection.



### High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/1711.11585v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.11585v2)
- **Published**: 2017-11-30 18:57:21+00:00
- **Updated**: 2018-08-20 17:55:56+00:00
- **Authors**: Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro
- **Comment**: v2: CVPR camera ready, adding more results for edge-to-photo examples
- **Journal**: None
- **Summary**: We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.



### Toward Multimodal Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1711.11586v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.11586v4)
- **Published**: 2017-11-30 18:59:01+00:00
- **Updated**: 2018-10-24 00:29:43+00:00
- **Authors**: Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman
- **Comment**: NIPS 2017 Final paper. v4 updated acknowledgment. Website:
  https://junyanz.github.io/BicycleGAN/
- **Journal**: None
- **Summary**: Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a \emph{distribution} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.



### Paris-Lille-3D: a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification
- **Arxiv ID**: http://arxiv.org/abs/1712.00032v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.00032v2)
- **Published**: 2017-11-30 19:08:52+00:00
- **Updated**: 2018-04-10 15:53:58+00:00
- **Authors**: Xavier Roynard, Jean-Emmanuel Deschaud, François Goulette
- **Comment**: preprint
- **Journal**: None
- **Summary**: This paper introduces a new Urban Point Cloud Dataset for Automatic Segmentation and Classification acquired by Mobile Laser Scanning (MLS). We describe how the dataset is obtained from acquisition to post-processing and labeling. This dataset can be used to learn classification algorithm, however, given that a great attention has been paid to the split between the different objects, this dataset can also be used to learn the segmentation. The dataset consists of around 2km of MLS point cloud acquired in two cities. The number of points and range of classes make us consider that it can be used to train Deep-Learning methods. Besides we show some results of automatic segmentation and classification. The dataset is available at: http://caor-mines-paristech.fr/fr/paris-lille-3d-dataset/



### An End-to-end 3D Convolutional Neural Network for Action Detection and Segmentation in Videos
- **Arxiv ID**: http://arxiv.org/abs/1712.01111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01111v1)
- **Published**: 2017-11-30 19:26:49+00:00
- **Updated**: 2017-11-30 19:26:49+00:00
- **Authors**: Rui Hou, Chen Chen, Mubarak Shah
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1703.10664
- **Journal**: None
- **Summary**: In this paper, we propose an end-to-end 3D CNN for action detection and segmentation in videos. The proposed architecture is a unified deep network that is able to recognize and localize action based on 3D convolution features. A video is first divided into equal length clips and next for each clip a set of tube proposals are generated based on 3D CNN features. Finally, the tube proposals of different clips are linked together and spatio-temporal action detection is performed using these linked video proposals. This top-down action detection approach explicitly relies on a set of good tube proposals to perform well and training the bounding box regression usually requires a large number of annotated samples. To remedy this, we further extend the 3D CNN to an encoder-decoder structure and formulate the localization problem as action segmentation. The foreground regions (i.e. action regions) for each frame are segmented first then the segmented foreground maps are used to generate the bounding boxes. This bottom-up approach effectively avoids tube proposal generation by leveraging the pixel-wise annotations of segmentation. The segmentation framework also can be readily applied to a general problem of video object segmentation. Extensive experiments on several video datasets demonstrate the superior performance of our approach for action detection and video object segmentation compared to the state-of-the-arts.



### Multi-Channel CNN-based Object Detection for Enhanced Situation Awareness
- **Arxiv ID**: http://arxiv.org/abs/1712.00075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00075v1)
- **Published**: 2017-11-30 20:54:49+00:00
- **Updated**: 2017-11-30 20:54:49+00:00
- **Authors**: Shuo Liu, Zheng Liu
- **Comment**: Published at the Sensors & Electronics Technology (SET) panel
  Symposium SET-241 on 9th NATO Military Sensing Symposium
- **Journal**: None
- **Summary**: Object Detection is critical for automatic military operations. However, the performance of current object detection algorithms is deficient in terms of the requirements in military scenarios. This is mainly because the object presence is hard to detect due to the indistinguishable appearance and dramatic changes of object's size which is determined by the distance to the detection sensors. Recent advances in deep learning have achieved promising results in many challenging tasks. The state-of-the-art in object detection is represented by convolutional neural networks (CNNs), such as the fast R-CNN algorithm. These CNN-based methods improve the detection performance significantly on several public generic object detection datasets. However, their performance on detecting small objects or undistinguishable objects in visible spectrum images is still insufficient. In this study, we propose a novel detection algorithm for military objects by fusing multi-channel CNNs. We combine spatial, temporal and thermal information by generating a three-channel image, and they will be fused as CNN feature maps in an unsupervised manner. The backbone of our object detection framework is from the fast R-CNN algorithm, and we utilize cross-domain transfer learning technique to fine-tune the CNN model on generated multi-channel images. In the experiments, we validated the proposed method with the images from SENSIAC (Military Sensing Information Analysis Centre) database and compared it with the state-of-the-art. The experimental results demonstrated the effectiveness of the proposed method on both accuracy and computational efficiency.



### Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1712.00080v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00080v2)
- **Published**: 2017-11-30 21:05:15+00:00
- **Updated**: 2018-07-13 15:16:36+00:00
- **Authors**: Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik Learned-Miller, Jan Kautz
- **Comment**: CVPR 2018 version with minor revisions and supplementary material
  included. Project page http://jianghz.me/projects/superslomo
- **Journal**: CVPR 2018
- **Summary**: Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. We use 1,132 video clips with 240-fps, containing 300K individual video frames, to train our network. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods.



### Budget-Aware Activity Detection with A Recurrent Policy Network
- **Arxiv ID**: http://arxiv.org/abs/1712.00097v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00097v2)
- **Published**: 2017-11-30 21:52:27+00:00
- **Updated**: 2018-05-08 04:55:57+00:00
- **Authors**: Behrooz Mahasseni, Xiaodong Yang, Pavlo Molchanov, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the challenging problem of efficient temporal activity detection in untrimmed long videos. While most recent work has focused and advanced the detection accuracy, the inference time can take seconds to minutes in processing each single video, which is too slow to be useful in real-world settings. This motivates the proposed budget-aware framework, which learns to perform activity detection by intelligently selecting a small subset of frames according to a specified time budget. We formulate this problem as a Markov decision process, and adopt a recurrent network to model the frame selection policy. We derive a recurrent policy gradient based approach to approximate the gradient of the non-decomposable and non-differentiable objective defined in our problem. In the extensive experiments, we achieve competitive detection accuracy, and more importantly, our approach is able to substantially reduce computation time and detect multiple activities with only 0.35s for each untrimmed long video.



### Graph Distillation for Action Detection with Privileged Modalities
- **Arxiv ID**: http://arxiv.org/abs/1712.00108v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00108v2)
- **Published**: 2017-11-30 22:40:59+00:00
- **Updated**: 2018-07-27 22:03:03+00:00
- **Authors**: Zelun Luo, Jun-Ting Hsieh, Lu Jiang, Juan Carlos Niebles, Li Fei-Fei
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: We propose a technique that tackles action detection in multimodal videos under a realistic and challenging condition in which only limited training data and partially observed modalities are available. Common methods in transfer learning do not take advantage of the extra modalities potentially available in the source domain. On the other hand, previous work on multimodal learning only focuses on a single domain or task and does not handle the modality discrepancy between training and testing. In this work, we propose a method termed graph distillation that incorporates rich privileged information from a large-scale multimodal dataset in the source domain, and improves the learning in the target domain where training data and modalities are scarce. We evaluate our approach on action classification and detection tasks in multimodal videos, and show that our model outperforms the state-of-the-art by a large margin on the NTU RGB+D and PKU-MMD benchmarks. The code is released at http://alan.vision/eccv18_graph/.



### Semantic Photometric Bundle Adjustment on Natural Sequences
- **Arxiv ID**: http://arxiv.org/abs/1712.00110v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00110v1)
- **Published**: 2017-11-30 22:45:59+00:00
- **Updated**: 2017-11-30 22:45:59+00:00
- **Authors**: Rui Zhu, Chaoyang Wang, Chen-Hsuan Lin, Ziyan Wang, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of obtaining dense reconstruction of an object in a natural sequence of images has been long studied in computer vision. Classically this problem has been solved through the application of bundle adjustment (BA). More recently, excellent results have been attained through the application of photometric bundle adjustment (PBA) methods -- which directly minimize the photometric error across frames. A fundamental drawback to BA & PBA, however, is: (i) their reliance on having to view all points on the object, and (ii) for the object surface to be well textured. To circumvent these limitations we propose semantic PBA which incorporates a 3D object prior, obtained through deep learning, within the photometric bundle adjustment problem. We demonstrate state of the art performance in comparison to leading methods for object reconstruction across numerous natural sequences.



### Blind Gain and Phase Calibration via Sparse Spectral Methods
- **Arxiv ID**: http://arxiv.org/abs/1712.00111v1
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1712.00111v1)
- **Published**: 2017-11-30 22:46:14+00:00
- **Updated**: 2017-11-30 22:46:14+00:00
- **Authors**: Yanjun Li, Kiryung Lee, Yoram Bresler
- **Comment**: 28 pages, 11 figures
- **Journal**: None
- **Summary**: Blind gain and phase calibration (BGPC) is a bilinear inverse problem involving the determination of unknown gains and phases of the sensing system, and the unknown signal, jointly. BGPC arises in numerous applications, e.g., blind albedo estimation in inverse rendering, synthetic aperture radar autofocus, and sensor array auto-calibration. In some cases, sparse structure in the unknown signal alleviates the ill-posedness of BGPC. Recently there has been renewed interest in solutions to BGPC with careful analysis of error bounds. In this paper, we formulate BGPC as an eigenvalue/eigenvector problem, and propose to solve it via power iteration, or in the sparsity or joint sparsity case, via truncated power iteration. Under certain assumptions, the unknown gains, phases, and the unknown signal can be recovered simultaneously. Numerical experiments show that power iteration algorithms work not only in the regime predicted by our main results, but also in regimes where theoretical analysis is limited. We also show that our power iteration algorithms for BGPC compare favorably with competing algorithms in adversarial conditions, e.g., with noisy measurement or with a bad initial estimate.



### Label Efficient Learning of Transferable Representations across Domains and Tasks
- **Arxiv ID**: http://arxiv.org/abs/1712.00123v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.00123v1)
- **Published**: 2017-11-30 23:31:28+00:00
- **Updated**: 2017-11-30 23:31:28+00:00
- **Authors**: Zelun Luo, Yuliang Zou, Judy Hoffman, Li Fei-Fei
- **Comment**: NIPS 2017
- **Journal**: None
- **Summary**: We propose a framework that learns a representation transferable across different domains and tasks in a label efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition.



