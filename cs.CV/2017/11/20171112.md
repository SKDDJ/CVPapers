# Arxiv Papers in cs.CV on 2017-11-12
### AON: Towards Arbitrarily-Oriented Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.04226v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04226v2)
- **Published**: 2017-11-12 03:11:25+00:00
- **Updated**: 2018-03-22 06:15:45+00:00
- **Authors**: Zhanzhan Cheng, Yangliu Xu, Fan Bai, Yi Niu, Shiliang Pu, Shuigeng Zhou
- **Comment**: Accepted by CVPR2018
- **Journal**: None
- **Summary**: Recognizing text from natural images is a hot research topic in computer vision due to its various applications. Despite the enduring research of several decades on optical character recognition (OCR), recognizing texts from natural images is still a challenging task. This is because scene texts are often in irregular (e.g. curved, arbitrarily-oriented or seriously distorted) arrangements, which have not yet been well addressed in the literature. Existing methods on text recognition mainly work with regular (horizontal and frontal) texts and cannot be trivially generalized to handle irregular texts. In this paper, we develop the arbitrary orientation network (AON) to directly capture the deep features of irregular texts, which are combined into an attention-based decoder to generate character sequence. The whole network can be trained end-to-end by using only images and word-level annotations. Extensive experiments on various benchmarks, including the CUTE80, SVT-Perspective, IIIT5k, SVT and ICDAR datasets, show that the proposed AON-based method achieves the-state-of-the-art performance in irregular datasets, and is comparable to major existing methods in regular datasets.



### D-PCN: Parallel Convolutional Networks for Image Recognition via a Discriminator
- **Arxiv ID**: http://arxiv.org/abs/1711.04237v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.04237v3)
- **Published**: 2017-11-12 05:11:42+00:00
- **Updated**: 2018-03-14 13:55:43+00:00
- **Authors**: Shiqi Yang, Gang Peng
- **Comment**: 20 pages, 8 figures, 7 tables
- **Journal**: None
- **Summary**: In this paper, we introduce a simple but quite effective recognition framework dubbed D-PCN, aiming at enhancing feature extracting ability of CNN. The framework consists of two parallel CNNs, a discriminator and an extra classifier which takes integrated features from parallel networks and gives final prediction. The discriminator is core which drives parallel networks to focus on different regions and learn complementary representations. The corresponding joint training strategy is introduced which ensures the utilization of discriminator. We validate D-PCN with several CNN models on two benchmark datasets: CIFAR-100 and ImageNet32x32, D-PCN enhances all models. In particular it yields state of the art performance on CIFAR-100 compared with related works. We also conduct visualization experiment on fine-grained Stanford Dogs dataset and verify our motivation. Additionally, we apply D-PCN for segmentation on PASCAL VOC 2012 and also find promotion.



### Robust registration of medical images in the presence of spatially-varying noise
- **Arxiv ID**: http://arxiv.org/abs/1711.04247v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04247v2)
- **Published**: 2017-11-12 07:45:29+00:00
- **Updated**: 2020-06-17 04:55:41+00:00
- **Authors**: Reza Abbasi-Asl, Aboozar Ghaffari, Emad Fatemizadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Spatially-varying intensity noise is a common source of distortion in medical images. Bias field noise is one example of such a distortion that is often present in the magnetic resonance (MR) images or other modalities such as retina images. In this paper, we first show that the bias field noise can be considerably reduced using Empirical Mode Decomposition (EMD) technique. EMD is a multi-resolution tool that decomposes a signal into several principle patterns and residual components. We show that the spatially-varying noise is highly expressed in the residual component of the EMD and could be filtered out. Then, we propose two hierarchical multi-resolution EMD-based algorithms for robust registration of images in the presence of spatially varying noise. One algorithm (LR-EMD) is based on registration of EMD feature-maps from both floating and reference images in various resolution levels. In the second algorithm (AFR-EMD), we first extract an average feature-map based on EMD from both floating and reference images. Then, we use a simple hierarchical multi-resolution algorithm to register the average feature-maps. For the brain MR images, both algorithms achieve lower error rate and higher convergence percentage compared to the intensity-based hierarchical registration. Specifically, using mutual information as the similarity measure, AFR-EMD achieves 42% lower error rate in intensity and 52% lower error rate in transformation compared to intensity-based hierarchical registration. For LR-EMD, the error rate is 32% lower for the intensity and 41% lower for the transformation. Furthermore, we demonstrate that our proposed algorithms improve the registration of retina images in the presence of spatially varying noise.



### Feature Enhancement Network: A Refined Scene Text Detector
- **Arxiv ID**: http://arxiv.org/abs/1711.04249v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04249v1)
- **Published**: 2017-11-12 08:12:54+00:00
- **Updated**: 2017-11-12 08:12:54+00:00
- **Authors**: Sheng Zhang, Yuliang Liu, Lianwen Jin, Canjie Luo
- **Comment**: 8 pages, 5 figures, 2 tables. This paper is accepted to appear in
  AAAI 2018
- **Journal**: None
- **Summary**: In this paper, we propose a refined scene text detector with a \textit{novel} Feature Enhancement Network (FEN) for Region Proposal and Text Detection Refinement. Retrospectively, both region proposal with \textit{only} $3\times 3$ sliding-window feature and text detection refinement with \textit{single scale} high level feature are insufficient, especially for smaller scene text. Therefore, we design a new FEN network with \textit{task-specific}, \textit{low} and \textit{high} level semantic features fusion to improve the performance of text detection. Besides, since \textit{unitary} position-sensitive RoI pooling in general object detection is unreasonable for variable text regions, an \textit{adaptively weighted} position-sensitive RoI pooling layer is devised for further enhancing the detecting accuracy. To tackle the \textit{sample-imbalance} problem during the refinement stage, we also propose an effective \textit{positives mining} strategy for efficiently training our network. Experiments on ICDAR 2011 and 2013 robust text detection benchmarks demonstrate that our method can achieve state-of-the-art results, outperforming all reported methods in terms of F-measure.



### Unified Spectral Clustering with Optimal Graph
- **Arxiv ID**: http://arxiv.org/abs/1711.04258v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.04258v1)
- **Published**: 2017-11-12 09:20:25+00:00
- **Updated**: 2017-11-12 09:20:25+00:00
- **Authors**: Zhao Kang, Chong Peng, Qiang Cheng, Zenglin Xu
- **Comment**: Accepted by AAAI 2018
- **Journal**: None
- **Summary**: Spectral clustering has found extensive use in many areas. Most traditional spectral clustering algorithms work in three separate steps: similarity graph construction; continuous labels learning; discretizing the learned labels by k-means clustering. Such common practice has two potential flaws, which may lead to severe information loss and performance degradation. First, predefined similarity graph might not be optimal for subsequent clustering. It is well-accepted that similarity graph highly affects the clustering results. To this end, we propose to automatically learn similarity information from data and simultaneously consider the constraint that the similarity matrix has exact c connected components if there are c clusters. Second, the discrete solution may deviate from the spectral solution since k-means method is well-known as sensitive to the initialization of cluster centers. In this work, we transform the candidate solution into a new one that better approximates the discrete one. Finally, those three subtasks are integrated into a unified framework, with each subtask iteratively boosted by using the results of the others towards an overall optimal solution. It is known that the performance of a kernel method is largely determined by the choice of kernels. To tackle this practical problem of how to select the most suitable kernel for a particular data set, we further extend our model to incorporate multiple kernel learning ability. Extensive experiments demonstrate the superiority of our proposed method as compared to existing clustering approaches.



### Evaluation of trackers for Pan-Tilt-Zoom Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1711.04260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04260v1)
- **Published**: 2017-11-12 09:33:53+00:00
- **Updated**: 2017-11-12 09:33:53+00:00
- **Authors**: Yucao Tang, Guillaume-Alexandre Bilodeau
- **Comment**: 6 pages, 2 figures, International Conference on Pattern Recognition
  and Artificial Intelligence 2018
- **Journal**: ICPRAI (2018) 3-9
- **Summary**: Tracking with a Pan-Tilt-Zoom (PTZ) camera has been a research topic in computer vision for many years. Compared to tracking with a still camera, the images captured with a PTZ camera are highly dynamic in nature because the camera can perform large motion resulting in quickly changing capture conditions. Furthermore, tracking with a PTZ camera involves camera control to position the camera on the target. For successful tracking and camera control, the tracker must be fast enough, or has to be able to predict accurately the next position of the target. Therefore, standard benchmarks do not allow to assess properly the quality of a tracker for the PTZ scenario. In this work, we use a virtual PTZ framework to evaluate different tracking algorithms and compare their performances. We also extend the framework to add target position prediction for the next frame, accounting for camera motion and processing delays. By doing this, we can assess if predicting can make long-term tracking more robust as it may help slower algorithms for keeping the target in the field of view of the camera. Results confirm that both speed and robustness are required for tracking under the PTZ scenario.



### Hand Gesture Recognition with Leap Motion
- **Arxiv ID**: http://arxiv.org/abs/1711.04293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04293v1)
- **Published**: 2017-11-12 13:27:31+00:00
- **Updated**: 2017-11-12 13:27:31+00:00
- **Authors**: Youchen Du, Shenglan Liu, Lin Feng, Menghui Chen, Jie Wu
- **Comment**: 6 pages, 10 figures
- **Journal**: None
- **Summary**: The recent introduction of depth cameras like Leap Motion Controller allows researchers to exploit the depth information to recognize hand gesture more robustly. This paper proposes a novel hand gesture recognition system with Leap Motion Controller. A series of features are extracted from Leap Motion tracking data, we feed these features along with HOG feature extracted from sensor images into a multi-class SVM classifier to recognize performed gesture, dimension reduction and feature weighted fusion are also discussed. Our results show that our model is much more accurate than previous work.



### 11K Hands: Gender recognition and biometric identification using a large dataset of hand images
- **Arxiv ID**: http://arxiv.org/abs/1711.04322v9
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.04322v9)
- **Published**: 2017-11-12 17:22:20+00:00
- **Updated**: 2018-09-17 02:04:23+00:00
- **Authors**: Mahmoud Afifi
- **Comment**: None
- **Journal**: None
- **Summary**: The human hand possesses distinctive features which can reveal gender information. In addition, the hand is considered one of the primary biometric traits used to identify a person. In this work, we propose a large dataset of human hand images (dorsal and palmar sides) with detailed ground-truth information for gender recognition and biometric identification. Using this dataset, a convolutional neural network (CNN) can be trained effectively for the gender recognition task. Based on this, we design a two-stream CNN to tackle the gender recognition problem. This trained model is then used as a feature extractor to feed a set of support vector machine classifiers for the biometric identification task. We show that the dorsal side of hand images, captured by a regular digital camera, convey effective distinctive features similar to, if not better, those available in the palmar hand images. To facilitate access to the proposed dataset and replication of our experiments, the dataset, trained CNN models, and Matlab source code are available at (https://goo.gl/rQJndd).



### High-Order Attention Models for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1711.04323v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.04323v1)
- **Published**: 2017-11-12 17:30:05+00:00
- **Updated**: 2017-11-12 17:30:05+00:00
- **Authors**: Idan Schwartz, Alexander G. Schwing, Tamir Hazan
- **Comment**: 9 pages, 8 figures, NIPS 2017
- **Journal**: None
- **Summary**: The quest for algorithms that enable cognitive abilities is an important part of machine learning. A common trait in many recently investigated cognitive-like tasks is that they take into account different data modalities, such as visual and textual input. In this paper we propose a novel and generally applicable form of attention mechanism that learns high-order correlations between various data modalities. We show that high-order correlations effectively direct the appropriate attention to the relevant elements in the different data modalities that are required to solve the joint task. We demonstrate the effectiveness of our high-order attention mechanism on the task of visual question answering (VQA), where we achieve state-of-the-art performance on the standard VQA dataset.



### Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes
- **Arxiv ID**: http://arxiv.org/abs/1711.04325v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.04325v1)
- **Published**: 2017-11-12 17:36:46+00:00
- **Updated**: 2017-11-12 17:36:46+00:00
- **Authors**: Takuya Akiba, Shuji Suzuki, Keisuke Fukuda
- **Comment**: NIPS'17 Workshop: Deep Learning at Supercomputer Scale
- **Journal**: None
- **Summary**: We demonstrate that training ResNet-50 on ImageNet for 90 epochs can be achieved in 15 minutes with 1024 Tesla P100 GPUs. This was made possible by using a large minibatch size of 32k. To maintain accuracy with this large minibatch size, we employed several techniques such as RMSprop warm-up, batch normalization without moving averages, and a slow-start learning rate schedule. This paper also describes the details of the hardware and software of the system used to achieve the above performance.



### Data Augmentation Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.04340v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.04340v3)
- **Published**: 2017-11-12 19:17:57+00:00
- **Updated**: 2018-03-21 23:26:15+00:00
- **Authors**: Antreas Antoniou, Amos Storkey, Harrison Edwards
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13% increase in accuracy in the low-data regime experiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face (4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5% (from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).



### Aggregated Wasserstein Metric and State Registration for Hidden Markov Models
- **Arxiv ID**: http://arxiv.org/abs/1711.05792v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.05792v2)
- **Published**: 2017-11-12 22:43:22+00:00
- **Updated**: 2017-11-19 20:19:50+00:00
- **Authors**: Yukun Chen, Jianbo Ye, Jia Li
- **Comment**: Our manuscript is based on our conference paper [arXiv:1608.01747]
  published in 14th European Conference on Computer Vision (ECCV 2016,
  spotlight). It has been significantly extended and is now in journal
  submission
- **Journal**: None
- **Summary**: We propose a framework, named Aggregated Wasserstein, for computing a dissimilarity measure or distance between two Hidden Markov Models with state conditional distributions being Gaussian. For such HMMs, the marginal distribution at any time position follows a Gaussian mixture distribution, a fact exploited to softly match, aka register, the states in two HMMs. We refer to such HMMs as Gaussian mixture model-HMM (GMM-HMM). The registration of states is inspired by the intrinsic relationship of optimal transport and the Wasserstein metric between distributions. Specifically, the components of the marginal GMMs are matched by solving an optimal transport problem where the cost between components is the Wasserstein metric for Gaussian distributions. The solution of the optimization problem is a fast approximation to the Wasserstein metric between two GMMs. The new Aggregated Wasserstein distance is a semi-metric and can be computed without generating Monte Carlo samples. It is invariant to relabeling or permutation of states. The distance is defined meaningfully even for two HMMs that are estimated from data of different dimensionality, a situation that can arise due to missing variables. This distance quantifies the dissimilarity of GMM-HMMs by measuring both the difference between the two marginal GMMs and that between the two transition matrices. Our new distance is tested on tasks of retrieval, classification, and t-SNE visualization of time series. Experiments on both synthetic and real data have demonstrated its advantages in terms of accuracy as well as efficiency in comparison with existing distances based on the Kullback-Leibler divergence.



