# Arxiv Papers in cs.CV on 2017-12-28
### Sky detection and log illumination refinement for PDE-based hazy image contrast enhancement
- **Arxiv ID**: http://arxiv.org/abs/1712.09775v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09775v2)
- **Published**: 2017-12-28 06:30:26+00:00
- **Updated**: 2018-03-10 08:56:18+00:00
- **Authors**: Uche A. Nnolim
- **Comment**: 22 pages, 13 figures, 5 tables
- **Journal**: None
- **Summary**: This report presents the results of a sky detection technique used to improve the performance of a previously developed partial differential equation (PDE)-based hazy image enhancement algorithm. Additionally, a proposed alternative method utilizes a function for log illumination refinement to improve de-hazing results while avoiding over-enhancement of sky or homogeneous regions. The algorithms were tested with several benchmark and calibration images and compared with several standard algorithms from the literature. Results indicate that the algorithms yield mostly consistent results and surpasses several of the other algorithms in terms of colour and contrast enhancement in addition to improved edge visibility.



### Efficient Parallel Connected Components Labeling with a Coarse-to-fine Strategy
- **Arxiv ID**: http://arxiv.org/abs/1712.09789v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09789v2)
- **Published**: 2017-12-28 08:50:22+00:00
- **Updated**: 2018-01-26 08:18:10+00:00
- **Authors**: Jun Chen, Keisuke Nonaka, Ryosuke Watanabe, Hiroshi Sankoh, Houari Sabirin, Sei Naito
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a new parallel approach to solve connected components on a 2D binary image implemented with CUDA. We employ the following strategies to accelerate neighborhood exploration after dividing an input image into independent blocks. In the local labeling stage, a coarse-labeling algorithm, including row-column connection and label-equivalence list unification, is applied first to sort out the mess of an initialized local label map; a refinement algorithm is then introduced to merge separated sub-regions from a single component. In the block merge stage, we scan the pixels located on the boundary of each block instead of solving the connectivity of all the pixels. With the proposed method, the length of label-equivalence lists is compressed, and the number of memory accesses is reduced. Thus, the efficiency of connected components labeling is improved. Experimental results show that our method outperforms the other approaches between $29\%$ and $80\%$ on average.



### Siamese LSTM based Fiber Structural Similarity Network (FS2Net) for Rotation Invariant Brain Tractography Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1712.09792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09792v1)
- **Published**: 2017-12-28 09:09:28+00:00
- **Updated**: 2017-12-28 09:09:28+00:00
- **Authors**: Shreyas Malakarjun Patil, Aditya Nigam, Arnav Bhavsar, Chiranjoy Chattopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel deep learning architecture combining stacked Bi-directional LSTM and LSTMs with the Siamese network architecture for segmentation of brain fibers, obtained from tractography data, into anatomically meaningful clusters. The proposed network learns the structural difference between fibers of different classes, which enables it to classify fibers with high accuracy. Importantly, capturing such deep inter and intra class structural relationship also ensures that the segmentation is robust to relative rotation among test and training data, hence can be used with unregistered data. Our extensive experimentation over order of hundred-thousands of fibers show that the proposed model achieves state-of-the-art results, even in cases of large relative rotations between test and training data.



### A Multi-Scale and Multi-Depth Convolutional Neural Network for Remote Sensing Imagery Pan-Sharpening
- **Arxiv ID**: http://arxiv.org/abs/1712.09809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09809v1)
- **Published**: 2017-12-28 10:28:38+00:00
- **Updated**: 2017-12-28 10:28:38+00:00
- **Authors**: Qiangqiang Yuan, Yancong Wei, Xiangchao Meng, Huanfeng Shen, Liangpei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Pan-sharpening is a fundamental and significant task in the field of remote sensing imagery processing, in which high-resolution spatial details from panchromatic images are employed to enhance the spatial resolution of multi-spectral (MS) images. As the transformation from low spatial resolution MS image to high-resolution MS image is complex and highly non-linear, inspired by the powerful representation for non-linear relationships of deep neural networks, we introduce multi-scale feature extraction and residual learning into the basic convolutional neural network (CNN) architecture and propose the multi-scale and multi-depth convolutional neural network (MSDCNN) for the pan-sharpening of remote sensing imagery. Both the quantitative assessment results and the visual assessment confirm that the proposed network yields high-resolution MS images that are superior to the images produced by the compared state-of-the-art methods.



### Future Frame Prediction for Anomaly Detection -- A New Baseline
- **Arxiv ID**: http://arxiv.org/abs/1712.09867v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09867v3)
- **Published**: 2017-12-28 14:04:52+00:00
- **Updated**: 2018-03-13 13:25:36+00:00
- **Authors**: Wen Liu, Weixin Luo, Dongze Lian, Shenghua Gao
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition 2018
- **Journal**: None
- **Summary**: Anomaly detection in videos refers to the identification of events that do not conform to expected behavior. However, almost all existing methods tackle the problem by minimizing the reconstruction errors of training data, which cannot guarantee a larger reconstruction error for an abnormal event. In this paper, we propose to tackle the anomaly detection problem within a video prediction framework. To the best of our knowledge, this is the first work that leverages the difference between a predicted future frame and its ground truth to detect an abnormal event. To predict a future frame with higher quality for normal events, other than the commonly used appearance (spatial) constraints on intensity and gradient, we also introduce a motion (temporal) constraint in video prediction by enforcing the optical flow between predicted frames and ground truth frames to be consistent, and this is the first work that introduces a temporal constraint into the video prediction task. Such spatial and motion constraints facilitate the future frame prediction for normal events, and consequently facilitate to identify those abnormal events that do not conform the expectation. Extensive experiments on both a toy dataset and some publicly available datasets validate the effectiveness of our method in terms of robustness to the uncertainty in normal events and the sensitivity to abnormal events.



### Handwritten Bangla Character Recognition Using The State-of-Art Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.09872v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09872v3)
- **Published**: 2017-12-28 14:31:56+00:00
- **Updated**: 2018-02-10 18:40:54+00:00
- **Authors**: Md Zahangir Alom, Peheding Sidike, Mahmudul Hasan, Tark M. Taha, Vijayan K. Asari
- **Comment**: 12 pages,22 figures, 5 tables. arXiv admin note: text overlap with
  arXiv:1705.02680
- **Journal**: None
- **Summary**: In spite of advances in object recognition technology, Handwritten Bangla Character Recognition (HBCR) remains largely unsolved due to the presence of many ambiguous handwritten characters and excessively cursive Bangla handwritings. Even the best existing recognizers do not lead to satisfactory performance for practical applications related to Bangla character recognition and have much lower performance than those developed for English alpha-numeric characters. To improve the performance of HBCR, we herein present the application of the state-of-the-art Deep Convolutional Neural Networks (DCNN) including VGG Network, All Convolution Network (All-Conv Net), Network in Network (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep learning approaches have the advantage of extracting and using feature information, improving the recognition of 2D shapes with a high degree of invariance to translation, scaling and other distortions. We systematically evaluated the performance of DCNN models on publicly available Bangla handwritten character dataset called CMATERdb and achieved the superior recognition accuracy when using DCNN models. This improvement would help in building an automatic HBCR system for practical applications.



### Improved Inception-Residual Convolutional Neural Network for Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.09888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09888v1)
- **Published**: 2017-12-28 15:08:14+00:00
- **Updated**: 2017-12-28 15:08:14+00:00
- **Authors**: Md Zahangir Alom, Mahmudul Hasan, Chris Yakopcic, Tarek M. Taha, Vijayan K. Asari
- **Comment**: 17 pages, 15 figures, 4 tables
- **Journal**: None
- **Summary**: Machine learning and computer vision have driven many of the greatest advances in the modeling of Deep Convolutional Neural Networks (DCNNs). Nowadays, most of the research has been focused on improving recognition accuracy with better DCNN models and learning approaches. The recurrent convolutional approach is not applied very much, other than in a few DCNN architectures. On the other hand, Inception-v4 and Residual networks have promptly become popular among computer the vision community. In this paper, we introduce a new DCNN model called the Inception Recurrent Residual Convolutional Neural Network (IRRCNN), which utilizes the power of the Recurrent Convolutional Neural Network (RCNN), the Inception network, and the Residual network. This approach improves the recognition accuracy of the Inception-residual network with same number of network parameters. In addition, this proposed architecture generalizes the Inception network, the RCNN, and the Residual network with significantly improved training accuracy. We have empirically evaluated the performance of the IRRCNN model on different benchmarks including CIFAR-10, CIFAR-100, TinyImageNet-200, and CU3D-100. The experimental results show higher recognition accuracy against most of the popular DCNN models including the RCNN. We have also investigated the performance of the IRRCNN approach against the Equivalent Inception Network (EIN) and the Equivalent Inception Residual Network (EIRN) counterpart on the CIFAR-100 dataset. We report around 4.53%, 4.49% and 3.56% improvement in classification accuracy compared with the RCNN, EIN, and EIRN on the CIFAR-100 dataset respectively. Furthermore, the experiment has been conducted on the TinyImageNet-200 and CU3D-100 datasets where the IRRCNN provides better testing accuracy compared to the Inception Recurrent CNN (IRCNN), the EIN, and the EIRN.



### Visualizing the Loss Landscape of Neural Nets
- **Arxiv ID**: http://arxiv.org/abs/1712.09913v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.09913v3)
- **Published**: 2017-12-28 16:15:42+00:00
- **Updated**: 2018-11-07 06:25:20+00:00
- **Authors**: Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein
- **Comment**: NIPS 2018 (extended version, 10.5 pages), code is available at
  https://github.com/tomgoldstein/loss-landscape
- **Journal**: None
- **Summary**: Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.



### Discriminative and Geometry Aware Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1712.10042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.10042v1)
- **Published**: 2017-12-28 20:02:49+00:00
- **Updated**: 2017-12-28 20:02:49+00:00
- **Authors**: Lingkun Luo, Liming Chen, Shiqiang Hu, Ying Lu, Xiaofang Wang
- **Comment**: 18pages, 12figures
- **Journal**: None
- **Summary**: Domain adaptation (DA) aims to generalize a learning model across training and testing data despite the mismatch of their data distributions. In light of a theoretical estimation of upper error bound, we argue in this paper that an effective DA method should 1) search a shared feature subspace where source and target data are not only aligned in terms of distributions as most state of the art DA methods do, but also discriminative in that instances of different classes are well separated; 2) account for the geometric structure of the underlying data manifold when inferring data labels on the target domain. In comparison with a baseline DA method which only cares about data distribution alignment between source and target, we derive three different DA models, namely CDDA, GA-DA, and DGA-DA, to highlight the contribution of Close yet Discriminative DA(CDDA) based on 1), Geometry Aware DA (GA-DA) based on 2), and finally Discriminative and Geometry Aware DA (DGA-DA) implementing jointly 1) and 2). Using both synthetic and real data, we show the effectiveness of the proposed approach which consistently outperforms state of the art DA methods over 36 image classification DA tasks through 6 popular benchmarks. We further carry out in-depth analysis of the proposed DA method in quantifying the contribution of each term of our DA model and provide insights into the proposed DA methods in visualizing both real and synthetic data.



