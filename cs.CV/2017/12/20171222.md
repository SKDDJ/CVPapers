# Arxiv Papers in cs.CV on 2017-12-22
### Collision Selective Visual Neural Network Inspired by LGMD2 Neurons in Juvenile Locusts
- **Arxiv ID**: http://arxiv.org/abs/1801.06452v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1801.06452v1)
- **Published**: 2017-12-22 00:34:55+00:00
- **Updated**: 2017-12-22 00:34:55+00:00
- **Authors**: Qinbing Fu, Cheng Hu, Shigang Yue
- **Comment**: None
- **Journal**: None
- **Summary**: For autonomous robots in dynamic environments mixed with human, it is vital to detect impending collision quickly and robustly. The biological visual systems evolved over millions of years may provide us efficient solutions for collision detection in complex environments. In the cockpit of locusts, two Lobula Giant Movement Detectors, i.e. LGMD1 and LGMD2, have been identified which respond to looming objects rigorously with high firing rates. Compared to LGMD1, LGMD2 matures early in the juvenile locusts with specific selectivity to dark moving objects against bright background in depth while not responding to light objects embedded in dark background - a similar situation which ground vehicles and robots are facing with. However, little work has been done on modeling LGMD2, let alone its potential in robotics and other vision-based applications. In this article, we propose a novel way of modeling LGMD2 neuron, with biased ON and OFF pathways splitting visual streams into parallel channels encoding brightness increments and decrements separately to fulfill its selectivity. Moreover, we apply a biophysical mechanism of spike frequency adaptation to shape the looming selectivity in such a collision-detecting neuron model. The proposed visual neural network has been tested with systematic experiments, challenged against synthetic and real physical stimuli, as well as image streams from the sensor of a miniature robot. The results demonstrated this framework is able to detect looming dark objects embedded in bright backgrounds selectively, which make it ideal for ground mobile platforms. The robotic experiments also showed its robustness in collision detection - it performed well for near range navigation in an arena with many obstacles. Its enhanced collision selectivity to dark approaching objects versus receding and translating ones has also been verified via systematic experiments.



### Using LIP to Gloss Over Faces in Single-Stage Face Detection Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.08263v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08263v2)
- **Published**: 2017-12-22 00:42:42+00:00
- **Updated**: 2018-07-05 01:23:11+00:00
- **Authors**: Siqi Yang, Arnold Wiliem, Shaokang Chen, Brian C. Lovell
- **Comment**: to appear ECCV 2018 (accepted version)
- **Journal**: None
- **Summary**: This work shows that it is possible to fool/attack recent state-of-the-art face detectors which are based on the single-stage networks. Successfully attacking face detectors could be a serious malware vulnerability when deploying a smart surveillance system utilizing face detectors. We show that existing adversarial perturbation methods are not effective to perform such an attack, especially when there are multiple faces in the input image. This is because the adversarial perturbation specifically generated for one face may disrupt the adversarial perturbation for another face. In this paper, we call this problem the Instance Perturbation Interference (IPI) problem. This IPI problem is addressed by studying the relationship between the deep neural network receptive field and the adversarial perturbation. As such, we propose the Localized Instance Perturbation (LIP) that uses adversarial perturbation constrained to the Effective Receptive Field (ERF) of a target to perform the attack. Experiment results show the LIP method massively outperforms existing adversarial perturbation generation methods -- often by a factor of 2 to 10.



### Beyond saliency: understanding convolutional neural networks from saliency prediction on layer-wise relevance propagation
- **Arxiv ID**: http://arxiv.org/abs/1712.08268v5
- **DOI**: 10.1016/j.imavis.2019.02.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08268v5)
- **Published**: 2017-12-22 01:23:22+00:00
- **Updated**: 2019-03-30 22:16:42+00:00
- **Authors**: Heyi Li, Yunke Tian, Klaus Mueller, Xin Chen
- **Comment**: 35 pages, 15 figures
- **Journal**: Image and Vision Computing, 2019
- **Summary**: Despite the tremendous achievements of deep convolutional neural networks (CNNs) in many computer vision tasks, understanding how they actually work remains a significant challenge. In this paper, we propose a novel two-step understanding method, namely Salient Relevance (SR) map, which aims to shed light on how deep CNNs recognize images and learn features from areas, referred to as attention areas, therein. Our proposed method starts out with a layer-wise relevance propagation (LRP) step which estimates a pixel-wise relevance map over the input image. Following, we construct a context-aware saliency map, SR map, from the LRP-generated map which predicts areas close to the foci of attention instead of isolated pixels that LRP reveals. In human visual system, information of regions is more important than of pixels in recognition. Consequently, our proposed approach closely simulates human recognition. Experimental results using the ILSVRC2012 validation dataset in conjunction with two well-established deep CNN models, AlexNet and VGG-16, clearly demonstrate that our proposed approach concisely identifies not only key pixels but also attention areas that contribute to the underlying neural network's comprehension of the given images. As such, our proposed SR map constitutes a convenient visual interface which unveils the visual attention of the network and reveals which type of objects the model has learned to recognize after training. The source code is available at https://github.com/Hey1Li/Salient-Relevance-Propagation.



### Recurrent Pixel Embedding for Instance Grouping
- **Arxiv ID**: http://arxiv.org/abs/1712.08273v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1712.08273v1)
- **Published**: 2017-12-22 01:48:53+00:00
- **Updated**: 2017-12-22 01:48:53+00:00
- **Authors**: Shu Kong, Charless Fowlkes
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a differentiable, end-to-end trainable framework for solving pixel-level grouping problems such as instance segmentation consisting of two novel components. First, we regress pixels into a hyper-spherical embedding space so that pixels from the same group have high cosine similarity while those from different groups have similarity below a specified margin. We analyze the choice of embedding dimension and margin, relating them to theoretical results on the problem of distributing points uniformly on the sphere. Second, to group instances, we utilize a variant of mean-shift clustering, implemented as a recurrent neural network parameterized by kernel bandwidth. This recurrent grouping module is differentiable, enjoys convergent dynamics and probabilistic interpretability. Backpropagating the group-weighted loss through this module allows learning to focus on only correcting embedding errors that won't be resolved during subsequent clustering. Our framework, while conceptually simple and theoretically abundant, is also practically effective and computationally efficient. We demonstrate substantial improvements over state-of-the-art instance segmentation for object proposal generation, as well as demonstrating the benefits of grouping loss for classification tasks such as boundary detection and semantic segmentation.



### A Bidirectional Adaptive Bandwidth Mean Shift Strategy for Clustering
- **Arxiv ID**: http://arxiv.org/abs/1712.08283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08283v1)
- **Published**: 2017-12-22 02:28:46+00:00
- **Updated**: 2017-12-22 02:28:46+00:00
- **Authors**: Fanyang Meng, Hong Liu, Yongsheng Liang, Wei Liu, Jihong Pei
- **Comment**: Accepted by ICIP 2017
- **Journal**: None
- **Summary**: The bandwidth of a kernel function is a crucial parameter in the mean shift algorithm. This paper proposes a novel adaptive bandwidth strategy which contains three main contributions. (1) The differences among different adaptive bandwidth are analyzed. (2) A new mean shift vector based on bidirectional adaptive bandwidth is defined, which combines the advantages of different adaptive bandwidth strategies. (3) A bidirectional adaptive bandwidth mean shift (BAMS) strategy is proposed to improve the ability to escape from the local maximum density. Compared with contemporary adaptive bandwidth mean shift strategies, experiments demonstrate the effectiveness of the proposed strategy.



### CSGNet: Neural Shape Parser for Constructive Solid Geometry
- **Arxiv ID**: http://arxiv.org/abs/1712.08290v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1712.08290v2)
- **Published**: 2017-12-22 03:18:57+00:00
- **Updated**: 2018-03-31 18:03:22+00:00
- **Authors**: Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kalogerakis, Subhransu Maji
- **Comment**: Accepted at CVPR-2018
- **Journal**: None
- **Summary**: We present a neural architecture that takes as input a 2D or 3D shape and outputs a program that generates the shape. The instructions in our program are based on constructive solid geometry principles, i.e., a set of boolean operations on shape primitives defined recursively. Bottom-up techniques for this shape parsing task rely on primitive detection and are inherently slow since the search space over possible primitive combinations is large. In contrast, our model uses a recurrent neural network that parses the input shape in a top-down manner, which is significantly faster and yields a compact and easy-to-interpret sequence of modeling instructions. Our model is also more effective as a shape detector compared to existing state-of-the-art detection techniques. We finally demonstrate that our network can be trained on novel datasets without ground-truth program annotations through policy gradient techniques.



### SFCN-OPI: Detection and Fine-grained Classification of Nuclei Using Sibling FCN with Objectness Prior Interaction
- **Arxiv ID**: http://arxiv.org/abs/1712.08297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08297v1)
- **Published**: 2017-12-22 03:56:56+00:00
- **Updated**: 2017-12-22 03:56:56+00:00
- **Authors**: Yanning Zhou, Qi Dou, Hao Chen, Jing Qin, Pheng-Ann Heng
- **Comment**: Accepted at AAAI 2018
- **Journal**: None
- **Summary**: Cell nuclei detection and fine-grained classification have been fundamental yet challenging problems in histopathology image analysis. Due to the nuclei tiny size, significant inter-/intra-class variances, as well as the inferior image quality, previous automated methods would easily suffer from limited accuracy and robustness. In the meanwhile, existing approaches usually deal with these two tasks independently, which would neglect the close relatedness of them. In this paper, we present a novel method of sibling fully convolutional network with prior objectness interaction (called SFCN-OPI) to tackle the two tasks simultaneously and interactively using a unified end-to-end framework. Specifically, the sibling FCN branches share features in earlier layers while holding respective higher layers for specific tasks. More importantly, the detection branch outputs the objectness prior which dynamically interacts with the fine-grained classification sibling branch during the training and testing processes. With this mechanism, the fine-grained classification successfully focuses on regions with high confidence of nuclei existence and outputs the conditional probability, which in turn benefits the detection through back propagation. Extensive experiments on colon cancer histology images have validated the effectiveness of our proposed SFCN-OPI and our method has outperformed the state-of-the-art methods by a large margin.



### Benchmarking Decoupled Neural Interfaces with Synthetic Gradients
- **Arxiv ID**: http://arxiv.org/abs/1712.08314v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.08314v3)
- **Published**: 2017-12-22 06:28:28+00:00
- **Updated**: 2018-05-21 14:06:52+00:00
- **Authors**: Ekaba Bisong
- **Comment**: Serious issues with the content and not appropriate for high-level
  academic distribution
- **Journal**: None
- **Summary**: Artifical Neural Networks are a particular class of learning systems modeled after biological neural functions with an interesting penchant for Hebbian learning, that is "neurons that wire together, fire together". However, unlike their natural counterparts, artificial neural networks have a close and stringent coupling between the modules of neurons in the network. This coupling or locking imposes upon the network a strict and inflexible structure that prevent layers in the network from updating their weights until a full feed-forward and backward pass has occurred. Such a constraint though may have sufficed for a while, is now no longer feasible in the era of very-large-scale machine learning, coupled with the increased desire for parallelization of the learning process across multiple computing infrastructures. To solve this problem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are introduced as a viable alternative to the backpropagation algorithm. This paper performs a speed benchmark to compare the speed and accuracy capabilities of SG-DNI as opposed to a standard neural interface using multilayer perceptron MLP. SG-DNI shows good promise, in that it not only captures the learning problem, it is also over 3-fold faster due to it asynchronous learning capabilities.



### Deep Hashing with Category Mask for Fast Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1712.08315v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08315v2)
- **Published**: 2017-12-22 06:35:48+00:00
- **Updated**: 2018-05-24 07:20:26+00:00
- **Authors**: Xu Liu, Lili Zhao, Dajun Ding, Yajiao Dong
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an end-to-end deep hashing framework with category mask for fast video retrieval. We train our network in a supervised way by fully exploiting inter-class diversity and intra-class identity. Classification loss is optimized to maximize inter-class diversity, while intra-pair is introduced to learn representative intra-class identity. We investigate the binary bits distribution related to categories and find out that the effectiveness of binary bits is highly correlated with data categories, and some bits may degrade classification performance of some categories. We then design hash code generation scheme with category mask to filter out bits with negative contribution. Experimental results demonstrate the proposed method outperforms several state-of-the-arts under various evaluation metrics on public datasets.



### Towards dense object tracking in a 2D honeybee hive
- **Arxiv ID**: http://arxiv.org/abs/1712.08324v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.08324v1)
- **Published**: 2017-12-22 07:20:57+00:00
- **Updated**: 2017-12-22 07:20:57+00:00
- **Authors**: Katarzyna Bozek, Laetitia Hebert, Alexander S Mikheyev, Greg J Stephens
- **Comment**: 15 pages, including supplementary figures. 1 supplemental movie
  available as an ancillary file
- **Journal**: None
- **Summary**: From human crowds to cells in tissue, the detection and efficient tracking of multiple objects in dense configurations is an important and unsolved problem. In the past, limitations of image analysis have restricted studies of dense groups to tracking a single or subset of marked individuals, or to coarse-grained group-level dynamics, all of which yield incomplete information. Here, we combine convolutional neural networks (CNNs) with the model environment of a honeybee hive to automatically recognize all individuals in a dense group from raw image data. We create new, adapted individual labeling and use the segmentation architecture U-Net with a loss function dependent on both object identity and orientation. We additionally exploit temporal regularities of the video recording in a recurrent manner and achieve near human-level performance while reducing the network size by 94% compared to the original U-Net architecture. Given our novel application of CNNs, we generate extensive problem-specific image data in which labeled examples are produced through a custom interface with Amazon Mechanical Turk. This dataset contains over 375,000 labeled bee instances across 720 video frames at 2 FPS, representing an extensive resource for the development and testing of tracking methods. We correctly detect 96% of individuals with a location error of ~7% of a typical body dimension, and orientation error of 12 degrees, approximating the variability of human raters. Our results provide an important step towards efficient image-based dense object tracking by allowing for the accurate determination of object location and orientation across time-series image data efficiently within one network architecture.



### The ParallelEye Dataset: Constructing Large-Scale Artificial Scenes for Traffic Vision Research
- **Arxiv ID**: http://arxiv.org/abs/1712.08394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08394v1)
- **Published**: 2017-12-22 11:16:19+00:00
- **Updated**: 2017-12-22 11:16:19+00:00
- **Authors**: Xuan Li, Kunfeng Wang, Yonglin Tian, Lan Yan, Fei-Yue Wang
- **Comment**: To be published in IEEE ITSC 2017
- **Journal**: None
- **Summary**: Video image datasets are playing an essential role in design and evaluation of traffic vision algorithms. Nevertheless, a longstanding inconvenience concerning image datasets is that manually collecting and annotating large-scale diversified datasets from real scenes is time-consuming and prone to error. For that virtual datasets have begun to function as a proxy of real datasets. In this paper, we propose to construct large-scale artificial scenes for traffic vision research and generate a new virtual dataset called "ParallelEye". First of all, the street map data is used to build 3D scene model of Zhongguancun Area, Beijing. Then, the computer graphics, virtual reality, and rule modeling technologies are utilized to synthesize large-scale, realistic virtual urban traffic scenes, in which the fidelity and geography match the real world well. Furthermore, the Unity3D platform is used to render the artificial scenes and generate accurate ground-truth labels, e.g., semantic/instance segmentation, object bounding box, object tracking, optical flow, and depth. The environmental conditions in artificial scenes can be controlled completely. As a result, we present a viable implementation pipeline for constructing large-scale artificial scenes for traffic vision research. The experimental results demonstrate that this pipeline is able to generate photorealistic virtual datasets with low modeling time and high accuracy labeling.



### Detection and Tracking of General Movable Objects in Large 3D Maps
- **Arxiv ID**: http://arxiv.org/abs/1712.08409v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.08409v2)
- **Published**: 2017-12-22 11:53:52+00:00
- **Updated**: 2018-01-30 09:31:47+00:00
- **Authors**: Nils Bore, Johan Ekekrantz, Patric Jensfelt, John Folkesson
- **Comment**: Submitted for peer review
- **Journal**: None
- **Summary**: This paper studies the problem of detection and tracking of general objects with long-term dynamics, observed by a mobile robot moving in a large environment. A key problem is that due to the environment scale, it can only observe a subset of the objects at any given time. Since some time passes between observations of objects in different places, the objects might be moved when the robot is not there. We propose a model for this movement in which the objects typically only move locally, but with some small probability they jump longer distances, through what we call global motion. For filtering, we decompose the posterior over local and global movements into two linked processes. The posterior over the global movements and measurement associations is sampled, while we track the local movement analytically using Kalman filters. This novel filter is evaluated on point cloud data gathered autonomously by a mobile robot over an extended period of time. We show that tracking jumping objects is feasible, and that the proposed probabilistic treatment outperforms previous methods when applied to real world data. The key to efficient probabilistic tracking in this scenario is focused sampling of the object posteriors.



### On the Integration of Optical Flow and Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.08416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08416v1)
- **Published**: 2017-12-22 12:16:29+00:00
- **Updated**: 2017-12-22 12:16:29+00:00
- **Authors**: Laura Sevilla-Lara, Yiyi Liao, Fatma Guney, Varun Jampani, Andreas Geiger, Michael J. Black
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the top performing action recognition methods use optical flow as a "black box" input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: 1) optical flow is useful for action recognition because it is invariant to appearance, 2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, 3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, 4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and 5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities.



### Simple Methods for Scanner Drift Normalization Validated for Automatic Segmentation of Knee Magnetic Resonance Imaging - with data from the Osteoarthritis Initiative
- **Arxiv ID**: http://arxiv.org/abs/1712.08425v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08425v1)
- **Published**: 2017-12-22 12:58:22+00:00
- **Updated**: 2017-12-22 12:58:22+00:00
- **Authors**: Erik B Dam
- **Comment**: None
- **Journal**: None
- **Summary**: Scanner drift is a well-known magnetic resonance imaging (MRI) artifact characterized by gradual signal degradation and scan intensity changes over time. In addition, hardware and software updates may imply abrupt changes in signal. The combined effects are particularly challenging for automatic image analysis methods used in longitudinal studies. The implication is increased measurement variation and a risk of bias in the estimations (e.g. in the volume change for a structure). We proposed two quite different approaches for scanner drift normalization and demonstrated the performance for segmentation of knee MRI using the fully automatic KneeIQ framework. The validation included a total of 1975 scans from both high-field and low-field MRI. The results demonstrated that the pre-processing method denoted Atlas Affine Normalization significantly removed scanner drift effects and ensured that the cartilage volume change quantifications became consistent with manual expert scores.



### Taking Visual Motion Prediction To New Heightfields
- **Arxiv ID**: http://arxiv.org/abs/1712.09448v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09448v2)
- **Published**: 2017-12-22 13:22:14+00:00
- **Updated**: 2021-12-10 19:01:23+00:00
- **Authors**: Sebastien Ehrhardt, Aron Monszpart, Niloy Mitra, Andrea Vedaldi
- **Comment**: arXiv admin note: text overlap with arXiv:1706.02179
- **Journal**: None
- **Summary**: While the basic laws of Newtonian mechanics are well understood, explaining a physical scenario still requires manually modeling the problem with suitable equations and estimating the associated parameters. In order to be able to leverage the approximation capabilities of artificial intelligence techniques in such physics related contexts, researchers have handcrafted the relevant states, and then used neural networks to learn the state transitions using simulation runs as training data. Unfortunately, such approaches are unsuited for modeling complex real-world scenarios, where manually authoring relevant state spaces tend to be tedious and challenging. In this work, we investigate if neural networks can implicitly learn physical states of real-world mechanical processes only based on visual data while internally modeling non-homogeneous environment and in the process enable long-term physical extrapolation. We develop a recurrent neural network architecture for this task and also characterize resultant uncertainties in the form of evolving variance estimates. We evaluate our setup to extrapolate motion of rolling ball(s) on bowls of varying shape and orientation, and on arbitrary heightfields using only images as input. We report significant improvements over existing image-based methods both in terms of accuracy of predictions and complexity of scenarios; and report competitive performance with approaches that, unlike us, assume access to internal physical states.



### Training and Testing Object Detectors with Virtual Images
- **Arxiv ID**: http://arxiv.org/abs/1712.08470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08470v1)
- **Published**: 2017-12-22 14:45:53+00:00
- **Updated**: 2017-12-22 14:45:53+00:00
- **Authors**: Yonglin Tian, Xuan Li, Kunfeng Wang, Fei-Yue Wang
- **Comment**: To be published in IEEE/CAA Journal of Automatica Sinica
- **Journal**: None
- **Summary**: In the area of computer vision, deep learning has produced a variety of state-of-the-art models that rely on massive labeled data. However, collecting and annotating images from the real world has a great demand for labor and money investments and is usually too passive to build datasets with specific characteristics, such as small area of objects and high occlusion level. Under the framework of Parallel Vision, this paper presents a purposeful way to design artificial scenes and automatically generate virtual images with precise annotations. A virtual dataset named ParallelEye is built, which can be used for several computer vision tasks. Then, by training the DPM (Deformable Parts Model) and Faster R-CNN detectors, we prove that the performance of models can be significantly improved by combining ParallelEye with publicly available real-world datasets during the training phase. In addition, we investigate the potential of testing the trained models from a specific aspect using intentionally designed virtual datasets, in order to discover the flaws of trained models. From the experimental results, we conclude that our virtual dataset is viable to train and test the object detectors.



### An Incremental Self-Organizing Architecture for Sensorimotor Learning and Prediction
- **Arxiv ID**: http://arxiv.org/abs/1712.08521v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1712.08521v2)
- **Published**: 2017-12-22 15:34:19+00:00
- **Updated**: 2018-03-09 15:21:24+00:00
- **Authors**: Luiza Mici, German I. Parisi, Stefan Wermter
- **Comment**: None
- **Journal**: None
- **Summary**: During visuomotor tasks, robots must compensate for temporal delays inherent in their sensorimotor processing systems. Delay compensation becomes crucial in a dynamic environment where the visual input is constantly changing, e.g., during the interacting with a human demonstrator. For this purpose, the robot must be equipped with a prediction mechanism for using the acquired perceptual experience to estimate possible future motor commands. In this paper, we present a novel neural network architecture that learns prototypical visuomotor representations and provides reliable predictions on the basis of the visual input. These predictions are used to compensate for the delayed motor behavior in an online manner. We investigate the performance of our method with a set of experiments comprising a humanoid robot that has to learn and generate visually perceived arm motion trajectories. We evaluate the accuracy in terms of mean prediction error and analyze the response of the network to novel movement demonstrations. Additionally, we report experiments with incomplete data sequences, showing the robustness of the proposed architecture in the case of a noisy and faulty visual sensor.



### Evaluation of PPG Biometrics for Authentication in different states
- **Arxiv ID**: http://arxiv.org/abs/1712.08583v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1712.08583v1)
- **Published**: 2017-12-22 17:29:31+00:00
- **Updated**: 2017-12-22 17:29:31+00:00
- **Authors**: Umang Yadav, Sherif N Abbas, Dimitrios Hatzinakos
- **Comment**: Accepted at 11th IAPR/IEEE International Conference on Biometrics,
  2018. 6 pages, 6 figures
- **Journal**: None
- **Summary**: Amongst all medical biometric traits, Photoplethysmograph (PPG) is the easiest to acquire. PPG records the blood volume change with just combination of Light Emitting Diode and Photodiode from any part of the body. With IoT and smart homes' penetration, PPG recording can easily be integrated with other vital wearable devices. PPG represents peculiarity of hemodynamics and cardiovascular system for each individual. This paper presents non-fiducial method for PPG based biometric authentication. Being a physiological signal, PPG signal alters with physical/mental stress and time. For robustness, these variations cannot be ignored. While, most of the previous works focused only on single session, this paper demonstrates extensive performance evaluation of PPG biometrics against single session data, different emotions, physical exercise and time-lapse using Continuous Wavelet Transform (CWT) and Direct Linear Discriminant Analysis (DLDA). When evaluated on different states and datasets, equal error rate (EER) of $0.5\%$-$6\%$ was achieved for $45$-$60$s average training time. Our CWT/DLDA based technique outperformed all other dimensionality reduction techniques and previous work.



### Denoising of image gradients and total generalized variation denoising
- **Arxiv ID**: http://arxiv.org/abs/1712.08585v3
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1712.08585v3)
- **Published**: 2017-12-22 17:33:16+00:00
- **Updated**: 2018-04-04 09:30:12+00:00
- **Authors**: Birgit Komander, Dirk A. Lorenz, Lena Vestweber
- **Comment**: None
- **Journal**: None
- **Summary**: We revisit total variation denoising and study an augmented model where we assume that an estimate of the image gradient is available. We show that this increases the image reconstruction quality and derive that the resulting model resembles the total generalized variation denoising method, thus providing a new motivation for this model. Further, we propose to use a constraint denoising model and develop a variational denoising model that is basically parameter free, i.e. all model parameters are estimated directly from the noisy image.   Moreover, we use Chambolle-Pock's primal dual method as well as the Douglas-Rachford method for the new models. For the latter one has to solve large discretizations of partial differential equations. We propose to do this in an inexact manner using the preconditioned conjugate gradients method and derive preconditioners for this. Numerical experiments show that the resulting method has good denoising properties and also that preconditioning does increase convergence speed significantly. Finally we analyze the duality gap of different formulations of the TGV denoising problem and derive a simple stopping criterion.



### Automated Surgical Skill Assessment in RMIS Training
- **Arxiv ID**: http://arxiv.org/abs/1712.08604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08604v1)
- **Published**: 2017-12-22 18:25:00+00:00
- **Updated**: 2017-12-22 18:25:00+00:00
- **Authors**: Aneeq Zia, Irfan Essa
- **Comment**: Accepted at IPCAI 2018
- **Journal**: None
- **Summary**: Purpose: Manual feedback in basic RMIS training can consume a significant amount of time from expert surgeons' schedule and is prone to subjectivity. While VR-based training tasks can generate automated score reports, there is no mechanism of generating automated feedback for surgeons performing basic surgical tasks in RMIS training. In this paper, we explore the usage of different holistic features for automated skill assessment using only robot kinematic data and propose a weighted feature fusion technique for improving score prediction performance.   Methods: We perform our experiments on the publicly available JIGSAWS dataset and evaluate four different types of holistic features from robot kinematic data - Sequential Motion Texture (SMT), Discrete Fourier Transform (DFT), Discrete Cosine Transform (DCT) and Approximate Entropy (ApEn). The features are then used for skill classification and exact skill score prediction. Along with using these features individually, we also evaluate the performance using our proposed weighted combination technique.   Results: Our results demonstrate that these holistic features outperform all previous HMM based state-of-the-art methods for skill classification on the JIGSAWS dataset. Also, our proposed feature fusion strategy significantly improves performance for skill score predictions achieving up to 0.61 average spearman correlation coefficient.   Conclusions: Holistic features capturing global information from robot kinematic data can successfully be used for evaluating surgeon skill in basic surgical tasks on the da Vinci robot. Using the framework presented can potentially allow for real time score feedback in RMIS training.



### Boundary-sensitive Network for Portrait Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1712.08675v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1712.08675v2)
- **Published**: 2017-12-22 22:32:38+00:00
- **Updated**: 2018-04-09 18:26:29+00:00
- **Authors**: Xianzhi Du, Xiaolong Wang, Dawei Li, Jingwen Zhu, Serafettin Tasci, Cameron Upright, Stephen Walsh, Larry Davis
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Compared to the general semantic segmentation problem, portrait segmentation has higher precision requirement on boundary area. However, this problem has not been well studied in previous works. In this paper, we propose a boundary-sensitive deep neural network (BSN) for portrait segmentation. BSN introduces three novel techniques. First, an individual boundary-sensitive kernel is proposed by dilating the contour line and assigning the boundary pixels with multi-class labels. Second, a global boundary-sensitive kernel is employed as a position sensitive prior to further constrain the overall shape of the segmentation map. Third, we train a boundary-sensitive attribute classifier jointly with the segmentation network to reinforce the network with semantic boundary shape information. We have evaluated BSN on the current largest public portrait segmentation dataset, i.e, the PFCN dataset, as well as the portrait images collected from other three popular image segmentation datasets: COCO, COCO-Stuff, and PASCAL VOC. Our method achieves the superior quantitative and qualitative performance over state-of-the-arts on all the datasets, especially on the boundary area.



