# Arxiv Papers in cs.CV on 2017-12-27
### Exploring the Space of Black-box Attacks on Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.09491v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.09491v1)
- **Published**: 2017-12-27 04:39:02+00:00
- **Updated**: 2017-12-27 04:39:02+00:00
- **Authors**: Arjun Nitin Bhagoji, Warren He, Bo Li, Dawn Song
- **Comment**: 25 pages, 7 figures, 10 tables
- **Journal**: None
- **Summary**: Existing black-box attacks on deep neural networks (DNNs) so far have largely focused on transferability, where an adversarial instance generated for a locally trained model can "transfer" to attack other learning models. In this paper, we propose novel Gradient Estimation black-box attacks for adversaries with query access to the target model's class probabilities, which do not rely on transferability. We also propose strategies to decouple the number of queries required to generate each adversarial sample from the dimensionality of the input. An iterative variant of our attack achieves close to 100% adversarial success rates for both targeted and untargeted attacks on DNNs. We carry out extensive experiments for a thorough comparative evaluation of black-box attacks and show that the proposed Gradient Estimation attacks outperform all transferability based black-box attacks we tested on both MNIST and CIFAR-10 datasets, achieving adversarial success rates similar to well known, state-of-the-art white-box attacks. We also apply the Gradient Estimation attacks successfully against a real-world Content Moderation classifier hosted by Clarifai. Furthermore, we evaluate black-box attacks against state-of-the-art defenses. We show that the Gradient Estimation attacks are very effective even against these defenses.



### Multi-Target, Multi-Camera Tracking by Hierarchical Clustering: Recent Progress on DukeMTMC Project
- **Arxiv ID**: http://arxiv.org/abs/1712.09531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09531v1)
- **Published**: 2017-12-27 09:32:49+00:00
- **Updated**: 2017-12-27 09:32:49+00:00
- **Authors**: Zhimeng Zhang, Jianan Wu, Xuan Zhang, Chi Zhang
- **Comment**: 4 pages, 1 figure
- **Journal**: None
- **Summary**: Although many methods perform well in single camera tracking, multi-camera tracking remains a challenging problem with less attention. DukeMTMC is a large-scale, well-annotated multi-camera tracking benchmark which makes great progress in this field. This report is dedicated to briefly introduce our method on DukeMTMC and show that simple hierarchical clustering with well-trained person re-identification features can get good results on this dataset.



### Consensus-based Sequence Training for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1712.09532v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09532v1)
- **Published**: 2017-12-27 09:38:52+00:00
- **Updated**: 2017-12-27 09:38:52+00:00
- **Authors**: Sang Phan, Gustav Eje Henter, Yusuke Miyao, Shin'ichi Satoh
- **Comment**: 11 pages, 4 figures, 5 tables. Github repo at
  https://github.com/mynlp/cst_captioning
- **Journal**: None
- **Summary**: Captioning models are typically trained using the cross-entropy loss. However, their performance is evaluated on other metrics designed to better correlate with human assessments. Recently, it has been shown that reinforcement learning (RL) can directly optimize these metrics in tasks such as captioning. However, this is computationally costly and requires specifying a baseline reward at each step to make training converge. We propose a fast approach to optimize one's objective of interest through the REINFORCE algorithm. First we show that, by replacing model samples with ground-truth sentences, RL training can be seen as a form of weighted cross-entropy loss, giving a fast, RL-based pre-training algorithm. Second, we propose to use the consensus among ground-truth captions of the same video as the baseline reward. This can be computed very efficiently. We call the complete proposal Consensus-based Sequence Training (CST). Applied to the MSRVTT video captioning benchmark, our proposals train significantly faster than comparable methods and establish a new state-of-the-art on the task, improving the CIDEr score from 47.3 to 54.2.



### Memory-Efficient Deep Salient Object Segmentation Networks on Gridized Superpixels
- **Arxiv ID**: http://arxiv.org/abs/1712.09558v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09558v2)
- **Published**: 2017-12-27 12:20:13+00:00
- **Updated**: 2018-05-22 10:11:36+00:00
- **Authors**: Caglar Aytekin, Xingyang Ni, Francesco Cricri, Lixin Fan, Emre Aksu
- **Comment**: 6 pages, submitted to MMSP 2018
- **Journal**: None
- **Summary**: Computer vision algorithms with pixel-wise labeling tasks, such as semantic segmentation and salient object detection, have gone through a significant accuracy increase with the incorporation of deep learning. Deep segmentation methods slightly modify and fine-tune pre-trained networks that have hundreds of millions of parameters. In this work, we question the need to have such memory demanding networks for the specific task of salient object segmentation. To this end, we propose a way to learn a memory-efficient network from scratch by training it only on salient object detection datasets. Our method encodes images to gridized superpixels that preserve both the object boundaries and the connectivity rules of regular pixels. This representation allows us to use convolutional neural networks that operate on regular grids. By using these encoded images, we train a memory-efficient network using only 0.048\% of the number of parameters that other deep salient object detection networks have. Our method shows comparable accuracy with the state-of-the-art deep salient object detection methods and provides a faster and a much more memory-efficient alternative to them. Due to its easy deployment, such a network is preferable for applications in memory limited devices such as mobile phones and IoT devices.



### Adversarial Patch
- **Arxiv ID**: http://arxiv.org/abs/1712.09665v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09665v2)
- **Published**: 2017-12-27 20:03:51+00:00
- **Updated**: 2018-05-17 01:44:59+00:00
- **Authors**: Tom B. Brown, Dandelion Mané, Aurko Roy, Martín Abadi, Justin Gilmer
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method to create universal, robust, targeted adversarial image patches in the real world. The patches are universal because they can be used to attack any scene, robust because they work under a wide variety of transformations, and targeted because they can cause a classifier to output any target class. These adversarial patches can be printed, added to any scene, photographed, and presented to image classifiers; even when the patches are small, they cause the classifiers to ignore the other items in the scene and report a chosen target class.   To reproduce the results from the paper, our code is available at https://github.com/tensorflow/cleverhans/tree/master/examples/adversarial_patch



### Geometry Processing of Conventionally Produced Mouse Brain Slice Images
- **Arxiv ID**: http://arxiv.org/abs/1712.09684v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.09684v1)
- **Published**: 2017-12-27 21:05:06+00:00
- **Updated**: 2017-12-27 21:05:06+00:00
- **Authors**: Nitin Agarwal, Xiangmin Xu, Gopi Meenakshisundaram
- **Comment**: 14 pages, 11 figures
- **Journal**: None
- **Summary**: Brain mapping research in most neuroanatomical laboratories relies on conventional processing techniques, which often introduce histological artifacts such as tissue tears and tissue loss. In this paper we present techniques and algorithms for automatic registration and 3D reconstruction of conventionally produced mouse brain slices in a standardized atlas space. This is achieved first by constructing a virtual 3D mouse brain model from annotated slices of Allen Reference Atlas (ARA). Virtual re-slicing of the reconstructed model generates ARA-based slice images corresponding to the microscopic images of histological brain sections. These image pairs are aligned using a geometric approach through contour images. Histological artifacts in the microscopic images are detected and removed using Constrained Delaunay Triangulation before performing global alignment. Finally, non-linear registration is performed by solving Laplace's equation with Dirichlet boundary conditions. Our methods provide significant improvements over previously reported registration techniques for the tested slices in 3D space, especially on slices with significant histological artifacts. Further, as an application we count the number of neurons in various anatomical regions using a dataset of 51 microscopic slices from a single mouse brain. This work represents a significant contribution to this subfield of neuroscience as it provides tools to neuroanatomist for analyzing and processing histological data.



### Learning More Universal Representations for Transfer-Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.09708v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.09708v5)
- **Published**: 2017-12-27 23:14:46+00:00
- **Updated**: 2018-09-03 01:03:21+00:00
- **Authors**: Youssef Tamaazousti, Hervé Le Borgne, Céline Hudelot, Mohamed El Amine Seddik, Mohamed Tamaazousti
- **Comment**: Submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)
- **Journal**: None
- **Summary**: A representation is supposed universal if it encodes any element of the visual world (e.g., objects, scenes) in any configuration (e.g., scale, context). While not expecting pure universal representations, the goal in the literature is to improve the universality level, starting from a representation with a certain level. To do so, the state-of-the-art consists in learning CNN-based representations on a diversified training problem (e.g., ImageNet modified by adding annotated data). While it effectively increases universality, such approach still requires a large amount of efforts to satisfy the needs in annotated data. In this work, we propose two methods to improve universality, but pay special attention to limit the need of annotated data. We also propose a unified framework of the methods based on the diversifying of the training problem. Finally, to better match Atkinson's cognitive study about universal human representations, we proposed to rely on the transfer-learning scheme as well as a new metric to evaluate universality. This latter, aims us to demonstrates the interest of our methods on 10 target-problems, relating to the classification task and a variety of visual domains.



### Report: Dynamic Eye Movement Matching and Visualization Tool in Neuro Gesture
- **Arxiv ID**: http://arxiv.org/abs/1712.09709v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.09709v2)
- **Published**: 2017-12-27 23:26:30+00:00
- **Updated**: 2018-01-07 18:36:29+00:00
- **Authors**: Qiangeng Xu, John Kender
- **Comment**: 21 pages
- **Journal**: None
- **Summary**: In the research of the impact of gestures using by a lecturer, one challenging task is to infer the attention of a group of audiences. Two important measurements that can help infer the level of attention are eye movement data and Electroencephalography (EEG) data. Under the fundamental assumption that a group of people would look at the same place if they all pay attention at the same time, we apply a method, "Time Warp Edit Distance", to calculate the similarity of their eye movement trajectories. Moreover, we also cluster eye movement pattern of audiences based on these pair-wised similarity metrics. Besides, since we don't have a direct metric for the "attention" ground truth, a visual assessment would be beneficial to evaluate the gesture-attention relationship. Thus we also implement a visualization tool.



### Extrapolating Expected Accuracies for Large Multi-Class Problems
- **Arxiv ID**: http://arxiv.org/abs/1712.09713v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.09713v1)
- **Published**: 2017-12-27 23:49:39+00:00
- **Updated**: 2017-12-27 23:49:39+00:00
- **Authors**: Charles Zheng, Rakesh Achanta, Yuval Benjamini
- **Comment**: Submitted to JMLR
- **Journal**: None
- **Summary**: The difficulty of multi-class classification generally increases with the number of classes. Using data from a subset of the classes, can we predict how well a classifier will scale with an increased number of classes? Under the assumptions that the classes are sampled identically and independently from a population, and that the classifier is based on independently learned scoring functions, we show that the expected accuracy when the classifier is trained on k classes is the (k-1)st moment of a certain distribution that can be estimated from data. We present an unbiased estimation method based on the theory, and demonstrate its application on a facial recognition example.



