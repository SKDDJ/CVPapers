# Arxiv Papers in cs.CV on 2017-12-19
### PixelBNN: Augmenting the PixelCNN with batch normalization and the presentation of a fast architecture for retinal vessel segmentation
- **Arxiv ID**: http://arxiv.org/abs/1712.06742v1
- **DOI**: 10.3390/jimaging5020026
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06742v1)
- **Published**: 2017-12-19 01:30:11+00:00
- **Updated**: 2017-12-19 01:30:11+00:00
- **Authors**: Henry A Leopold, Jeff Orchard, John S Zelek, Vasudevan Lakshminarayanan
- **Comment**: Manuscript accepted into SPIE Journal of Medical Imaging special
  section on Radiomics and Deep Learning
- **Journal**: None
- **Summary**: Analysis of retinal fundus images is essential for eye-care physicians in the diagnosis, care and treatment of patients. Accurate fundus and/or retinal vessel maps give rise to longitudinal studies able to utilize multimedia image registration and disease/condition status measurements, as well as applications in surgery preparation and biometrics. The segmentation of retinal morphology has numerous applications in assessing ophthalmologic and cardiovascular disease pathologies. The early detection of many such conditions is often the most effective method for reducing patient risk. Computer aided segmentation of the vasculature has proven to be a challenge, mainly due to inconsistencies such as noise and variations in hue and brightness that can greatly reduce the quality of fundus images. This paper presents PixelBNN, a highly efficient deep method for automating the segmentation of fundus morphologies. The model was trained, tested and cross tested on the DRIVE, STARE and CHASE\_DB1 retinal vessel segmentation datasets. Performance was evaluated using G-mean, Mathews Correlation Coefficient and F1-score. The network was 8.5 times faster than the current state-of-the-art at test time and performed comparatively well, considering a 5 to 19 times reduction in information from resizing images during preprocessing.



### Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling
- **Arxiv ID**: http://arxiv.org/abs/1712.06760v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06760v2)
- **Published**: 2017-12-19 03:06:42+00:00
- **Updated**: 2018-04-03 21:23:26+00:00
- **Authors**: Yiru Shen, Chen Feng, Yaoqing Yang, Dong Tian
- **Comment**: Accepted in CVPR'18. *indicates equal contribution
- **Journal**: None
- **Summary**: Unlike on images, semantic learning on 3D point clouds using a deep network is challenging due to the naturally unordered data structure. Among existing works, PointNet has achieved promising results by directly learning on point sets. However, it does not take full advantage of a point's local neighborhood that contains fine-grained structural information which turns out to be helpful towards better semantic learning. In this regard, we present two new operations to improve PointNet with a more efficient exploitation of local structures. The first one focuses on local 3D geometric structures. In analogy to a convolution kernel for images, we define a point-set kernel as a set of learnable 3D points that jointly respond to a set of neighboring data points according to their geometric affinities measured by kernel correlation, adapted from a similar technique for point cloud registration. The second one exploits local high-dimensional feature structures by recursive feature aggregation on a nearest-neighbor-graph computed from 3D positions. Experiments show that our network can efficiently capture local information and robustly achieve better performances on major datasets. Our code is available at http://www.merl.com/research/license#KCNet



### MovieGraphs: Towards Understanding Human-Centric Situations from Videos
- **Arxiv ID**: http://arxiv.org/abs/1712.06761v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06761v2)
- **Published**: 2017-12-19 03:08:25+00:00
- **Updated**: 2018-04-15 18:59:49+00:00
- **Authors**: Paul Vicol, Makarand Tapaswi, Lluis Castrejon, Sanja Fidler
- **Comment**: Spotlight at CVPR 2018. Webpage: http://moviegraphs.cs.toronto.edu
- **Journal**: None
- **Summary**: There is growing interest in artificial intelligence to build socially intelligent robots. This requires machines to have the ability to "read" people's emotions, motivations, and other factors that affect behavior. Towards this goal, we introduce a novel dataset called MovieGraphs which provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions. In addition, most interactions and many attributes are grounded in the video with time stamps. We provide a thorough analysis of our dataset, showing interesting common-sense correlations between different social aspects of scenes, as well as across scenes over time. We propose a method for querying videos and text with graphs, and show that: 1) our graphs contain rich and sufficient information to summarize and localize each scene; and 2) subgraphs allow us to describe situations at an abstract level and retrieve multiple semantically relevant situations. We also propose methods for interaction understanding via ordering, and reason understanding. MovieGraphs is the first benchmark to focus on inferred properties of human-centric situations, and opens up an exciting avenue towards socially-intelligent AI agents.



### Tracking objects using 3D object proposals
- **Arxiv ID**: http://arxiv.org/abs/1712.06780v1
- **DOI**: 10.1109/APSIPA.2017.8282298
- **Categories**: **cs.RO**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.06780v1)
- **Published**: 2017-12-19 04:52:34+00:00
- **Updated**: 2017-12-19 04:52:34+00:00
- **Authors**: Ramanpreet Singh Pahwa, Tian Tsong Ng, Minh N. Do
- **Comment**: 4 pages, 4 figures, published in APSIPA 2017
- **Journal**: 2017 Asia-Pacific Signal and Information Processing Association
  Annual Summit and Conference (APSIPA ASC)
- **Summary**: 3D object proposals, quickly detected regions in a 3D scene that likely contain an object of interest, are an effective approach to improve the computational efficiency and accuracy of the object detection framework. In this work, we propose a novel online method that uses our previously developed 3D object proposals, in a RGB-D video sequence, to match and track static objects in the scene using shape matching. Our main observation is that depth images provide important information about the geometry of the scene that is often ignored in object matching techniques. Our method takes less than a second in MATLAB on the UW-RGBD scene dataset on a single thread CPU and thus, has potential to be used in low-power chips in Unmanned Aerial Vehicles (UAVs), quadcopters, and drones.



### Hierarchical Cross Network for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1712.06820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06820v1)
- **Published**: 2017-12-19 08:36:47+00:00
- **Updated**: 2017-12-19 08:36:47+00:00
- **Authors**: Huan-Cheng Hsu, Ching-Hang Chen, Hsiao-Rong Tyan, Hong-Yuan Mark Liao
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (person re-ID) aims at matching target person(s) grabbed from different and non-overlapping camera views. It plays an important role for public safety and has application in various tasks such as, human retrieval, human tracking, and activity analysis. In this paper, we propose a new network architecture called Hierarchical Cross Network (HCN) to perform person re-ID. In addition to the backbone model of a conventional CNN, HCN is equipped with two additional maps called hierarchical cross feature maps. The maps of an HCN are formed by merging layers with different resolutions and semantic levels. With the hierarchical cross feature maps, an HCN can effectively uncover additional semantic features which could not be discovered by a conventional CNN. Although the proposed HCN can discover features with higher semantics, its representation power is still limited. To derive more general representations, we augment the data during the training process by combining multiple datasets. Experiment results show that the proposed method outperformed several state-of-the-art methods.



### Single Image Deraining using Scale-Aware Multi-Stage Recurrent Network
- **Arxiv ID**: http://arxiv.org/abs/1712.06830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06830v1)
- **Published**: 2017-12-19 09:12:39+00:00
- **Updated**: 2017-12-19 09:12:39+00:00
- **Authors**: Ruoteng Li, Loong-Fah Cheong, Robby T. Tan
- **Comment**: 9 pages, CVPR 2018
- **Journal**: None
- **Summary**: Given a single input rainy image, our goal is to visually remove rain streaks and the veiling effect caused by scattering and transmission of rain streaks and rain droplets. We are particularly concerned with heavy rain, where rain streaks of various sizes and directions can overlap each other and the veiling effect reduces contrast severely. To achieve our goal, we introduce a scale-aware multi-stage convolutional neural network. Our main idea here is that different sizes of rain-streaks visually degrade the scene in different ways. Large nearby streaks obstruct larger regions and are likely to reflect specular highlights more prominently than smaller distant streaks. These different effects of different streaks have their own characteristics in their image features, and thus need to be treated differently. To realize this, we create parallel sub-networks that are trained and made aware of these different scales of rain streaks. To our knowledge, this idea of parallel sub-networks that treats the same class of objects according to their unique sub-classes is novel, particularly in the context of rain removal. To verify our idea, we conducted experiments on both synthetic and real images, and found that our method is effective and outperforms the state-of-the-art methods.



### Flexible Stereo: Constrained, Non-rigid, Wide-baseline Stereo Vision for Fixed-wing Aerial Platforms
- **Arxiv ID**: http://arxiv.org/abs/1712.06837v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.06837v2)
- **Published**: 2017-12-19 09:34:46+00:00
- **Updated**: 2018-02-26 09:40:37+00:00
- **Authors**: Timo Hinzmann, Tim Taubner, Roland Siegwart
- **Comment**: Accepted for publication in IEEE International Conference on Robotics
  and Automation (ICRA), 2018, Brisbane
- **Journal**: None
- **Summary**: This paper proposes a computationally efficient method to estimate the time-varying relative pose between two visual-inertial sensor rigs mounted on the flexible wings of a fixed-wing unmanned aerial vehicle (UAV). The estimated relative poses are used to generate highly accurate depth maps in real-time and can be employed for obstacle avoidance in low-altitude flights or landing maneuvers. The approach is structured as follows: Initially, a wing model is identified by fitting a probability density function to measured deviations from the nominal relative baseline transformation. At run-time, the prior knowledge about the wing model is fused in an Extended Kalman filter~(EKF) together with relative pose measurements obtained from solving a relative perspective N-point problem (PNP), and the linear accelerations and angular velocities measured by the two inertial measurement units (IMU) which are rigidly attached to the cameras. Results obtained from extensive synthetic experiments demonstrate that our proposed framework is able to estimate highly accurate baseline transformations and depth maps.



### End-to-end weakly-supervised semantic alignment
- **Arxiv ID**: http://arxiv.org/abs/1712.06861v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.06861v2)
- **Published**: 2017-12-19 10:52:22+00:00
- **Updated**: 2018-04-24 15:09:04+00:00
- **Authors**: Ignacio Rocco, Relja Arandjelović, Josef Sivic
- **Comment**: In 2018 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR 2018)
- **Journal**: None
- **Summary**: We tackle the task of semantic alignment where the goal is to compute dense semantic correspondence aligning two images depicting objects of the same category. This is a challenging task due to large intra-class variation, changes in viewpoint and background clutter. We present the following three principal contributions. First, we develop a convolutional neural network architecture for semantic alignment that is trainable in an end-to-end manner from weak image-level supervision in the form of matching image pairs. The outcome is that parameters are learnt from rich appearance variation present in different but semantically related images without the need for tedious manual annotation of correspondences at training time. Second, the main component of this architecture is a differentiable soft inlier scoring module, inspired by the RANSAC inlier scoring procedure, that computes the quality of the alignment based on only geometrically consistent correspondences thereby reducing the effect of background clutter. Third, we demonstrate that the proposed approach achieves state-of-the-art performance on multiple standard benchmarks for semantic alignment.



### Comparison of fingerprint authentication algorithms for small imaging sensors
- **Arxiv ID**: http://arxiv.org/abs/1712.06882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06882v1)
- **Published**: 2017-12-19 11:51:10+00:00
- **Updated**: 2017-12-19 11:51:10+00:00
- **Authors**: Mathilde Bourjot, Regis Perrier, Jean François Mainguet
- **Comment**: On going work which will be improved with more experimental results
- **Journal**: None
- **Summary**: The demand for biometric systems has been increasing with the growth of the smartphone market. Biometric devices allow the user to authenticate easily while securing its private data without the need to remember any access code. Amongst them, fingerprint sensors are the most widespread because they seem to provide a good balance between reliability, cost and ease of use. According to smartphone manufacturers, the security level is guaranteed to be high. However, the size of those sensors, which is only a few millimeters squared, prevents the use of minutiae algorithms. To the best of our knowledge, very few studies shed light onto this problem, though many pattern recognition algorithms already exist as well as commercial solutions which are supposedly robust. In this article we try to provide insights on how to tackle this problem by analyzing the performance of three algorithms dedicated to pattern recognition.



### Learning Fixation Point Strategy for Object Detection and Classification
- **Arxiv ID**: http://arxiv.org/abs/1712.06897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06897v1)
- **Published**: 2017-12-19 12:28:01+00:00
- **Updated**: 2017-12-19 12:28:01+00:00
- **Authors**: Jie Lyu, Zejian Yuan, Dapeng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel recurrent attentional structure to localize and recognize objects jointly. The network can learn to extract a sequence of local observations with detailed appearance and rough context, instead of sliding windows or convolutions on the entire image. Meanwhile, those observations are fused to complete detection and classification tasks. On training, we present a hybrid loss function to learn the parameters of the multi-task network end-to-end. Particularly, the combination of stochastic and object-awareness strategy, named SA, can select more abundant context and ensure the last fixation close to the object. In addition, we build a real-world dataset to verify the capacity of our method in detecting the object of interest including those small ones. Our method can predict a precise bounding box on an image, and achieve high speed on large images without pooling operations. Experimental results indicate that the proposed method can mine effective context by several local observations. Moreover, the precision and speed are easily improved by changing the number of recurrent steps. Finally, we will open the source code of our proposed approach.



### On the Evaluation of Video Keyframe Summaries using User Ground Truth
- **Arxiv ID**: http://arxiv.org/abs/1712.06899v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.9; H.3.1
- **Links**: [PDF](http://arxiv.org/pdf/1712.06899v1)
- **Published**: 2017-12-19 12:37:02+00:00
- **Updated**: 2017-12-19 12:37:02+00:00
- **Authors**: Ludmila I. Kuncheva, Paria Yousefi, Iain A. D. Gunn
- **Comment**: 12 pages, 10 figures, 2 tables
- **Journal**: None
- **Summary**: Given the great interest in creating keyframe summaries from video, it is surprising how little has been done to formalise their evaluation and comparison. User studies are often carried out to demonstrate that a proposed method generates a more appealing summary than one or two rival methods. But larger comparison studies cannot feasibly use such user surveys. Here we propose a discrimination capacity measure as a formal way to quantify the improvement over the uniform baseline, assuming that one or more ground truth summaries are available. Using the VSUMM video collection, we examine 10 video feature types, including CNN and SURF, and 6 methods for matching frames from two summaries. Our results indicate that a simple frame representation through hue histograms suffices for the purposes of comparing keyframe summaries. We subsequently propose a formal protocol for comparing summaries when ground truth is available.



### Cross-language Framework for Word Recognition and Spotting of Indic Scripts
- **Arxiv ID**: http://arxiv.org/abs/1712.06908v2
- **DOI**: 10.1016/j.patcog.2018.01.034
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06908v2)
- **Published**: 2017-12-19 13:12:29+00:00
- **Updated**: 2018-01-28 15:10:48+00:00
- **Authors**: Ayan Kumar Bhunia, Partha Pratim Roy, Akash Mohta, Umapada Pal
- **Comment**: Accepted in Pattern Recognition, Elsevier(2018)
- **Journal**: None
- **Summary**: Handwritten word recognition and spotting of low-resource scripts are difficult as sufficient training data is not available and it is often expensive for collecting data of such scripts. This paper presents a novel cross language platform for handwritten word recognition and spotting for such low-resource scripts where training is performed with a sufficiently large dataset of an available script (considered as source script) and testing is done on other scripts (considered as target script). Training with one source script and testing with another script to have a reasonable result is not easy in handwriting domain due to the complex nature of handwriting variability among scripts. Also it is difficult in mapping between source and target characters when they appear in cursive word images. The proposed Indic cross language framework exploits a large resource of dataset for training and uses it for recognizing and spotting text of other target scripts where sufficient amount of training data is not available. Since, Indic scripts are mostly written in 3 zones, namely, upper, middle and lower, we employ zone-wise character (or component) mapping for efficient learning purpose. The performance of our cross-language framework depends on the extent of similarity between the source and target scripts. Hence, we devise an entropy based script similarity score using source to target character mapping that will provide a feasibility of cross language transcription. We have tested our approach in three Indic scripts, namely, Bangla, Devanagari and Gurumukhi, and the corresponding results are reported.



### ComboGAN: Unrestrained Scalability for Image Domain Translation
- **Arxiv ID**: http://arxiv.org/abs/1712.06909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06909v1)
- **Published**: 2017-12-19 13:18:42+00:00
- **Updated**: 2017-12-19 13:18:42+00:00
- **Authors**: Asha Anoosheh, Eirikur Agustsson, Radu Timofte, Luc Van Gool
- **Comment**: Source code provided here: https://github.com/AAnoosheh/ComboGAN
- **Journal**: None
- **Summary**: This year alone has seen unprecedented leaps in the area of learning-based image translation, namely CycleGAN, by Zhu et al. But experiments so far have been tailored to merely two domains at a time, and scaling them to more would require an quadratic number of models to be trained. And with two-domain models taking days to train on current hardware, the number of domains quickly becomes limited by the time and resources required to process them. In this paper, we propose a multi-component image translation model and training scheme which scales linearly - both in resource consumption and time required - with the number of domains. We demonstrate its capabilities on a dataset of paintings by 14 different artists and on images of the four different seasons in the Alps. Note that 14 data groups would need (14 choose 2) = 91 different CycleGAN models: a total of 182 generator/discriminator pairs; whereas our model requires only 14 generator/discriminator pairs.



### Bipartite Graph Matching for Keyframe Summary Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1712.06914v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10, I.4.9, G.2.2
- **Links**: [PDF](http://arxiv.org/pdf/1712.06914v1)
- **Published**: 2017-12-19 13:27:26+00:00
- **Updated**: 2017-12-19 13:27:26+00:00
- **Authors**: Iain A. D. Gunn, Ludmila I. Kuncheva, Paria Yousefi
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: A keyframe summary, or "static storyboard", is a collection of frames from a video designed to summarise its semantic content. Many algorithms have been proposed to extract such summaries automatically. How best to evaluate these outputs is an important but little-discussed question. We review the current methods for matching frames between two summaries in the formalism of graph theory. Our analysis revealed different behaviours of these methods, which we illustrate with a number of case studies. Based on the results, we recommend a greedy matching algorithm due to Kannappan et al.



### Scale-Space Anisotropic Total Variation for Limited Angle Tomography
- **Arxiv ID**: http://arxiv.org/abs/1712.06930v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06930v2)
- **Published**: 2017-12-19 13:59:10+00:00
- **Updated**: 2018-01-29 14:25:50+00:00
- **Authors**: Yixing Huang, Oliver Taubmann, Xiaolin Huang, Viktor Haase, Guenter Lauritsch, Andreas Maier
- **Comment**: 8 pages, 12 figures (48 subfigrues in total)
- **Journal**: None
- **Summary**: This paper addresses streak reduction in limited angle tomography. Although the iterative reweighted total variation (wTV) algorithm reduces small streaks well, it is rather inept at eliminating large ones since total variation (TV) regularization is scale-dependent and may regard these streaks as homogeneous areas. Hence, the main purpose of this paper is to reduce streak artifacts at various scales. We propose the scale-space anisotropic total variation (ssaTV) algorithm in two different implementations. The first implementation (ssaTV-1) utilizes an anisotropic gradient-like operator which uses 2s neighboring pixels along the streaks' normal direction at each scale s. The second implementation (ssaTV-2) makes use of anisotropic down-sampling and up-sampling operations, similarly oriented along the streaks' normal direction, to apply TV regularization at various scales. Experiments on numerical and clinical data demonstrate that both ssaTV algorithms reduce streak artifacts more effectively and efficiently than wTV, particularly when using multiple scales.



### Automatic Renal Segmentation in DCE-MRI using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.07022v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.07022v1)
- **Published**: 2017-12-19 16:13:01+00:00
- **Updated**: 2017-12-19 16:13:01+00:00
- **Authors**: Marzieh Haghighi, Simon K. Warfield, Sila Kurugol
- **Comment**: None
- **Journal**: None
- **Summary**: Kidney function evaluation using dynamic contrast-enhanced MRI (DCE-MRI) images could help in diagnosis and treatment of kidney diseases of children. Automatic segmentation of renal parenchyma is an important step in this process. In this paper, we propose a time and memory efficient fully automated segmentation method which achieves high segmentation accuracy with running time in the order of seconds in both normal kidneys and kidneys with hydronephrosis. The proposed method is based on a cascaded application of two 3D convolutional neural networks that employs spatial and temporal information at the same time in order to learn the tasks of localization and segmentation of kidneys, respectively. Segmentation performance is evaluated on both normal and abnormal kidneys with varying levels of hydronephrosis. We achieved a mean dice coefficient of 91.4 and 83.6 for normal and abnormal kidneys of pediatric patients, respectively.



### Adversarial Examples: Attacks and Defenses for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.07107v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.07107v3)
- **Published**: 2017-12-19 18:44:07+00:00
- **Updated**: 2018-07-07 02:32:57+00:00
- **Authors**: Xiaoyong Yuan, Pan He, Qile Zhu, Xiaolin Li
- **Comment**: Github: https://github.com/chbrian/awesome-adversarial-examples-dl
- **Journal**: None
- **Summary**: With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called adversarial examples. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples and explore the challenges and the potential solutions.



### Real-time 3D Reconstruction on Construction Site using Visual SLAM and UAV
- **Arxiv ID**: http://arxiv.org/abs/1712.07122v1
- **DOI**: 10.1061/9780784481264.030
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.07122v1)
- **Published**: 2017-12-19 18:46:46+00:00
- **Updated**: 2017-12-19 18:46:46+00:00
- **Authors**: Zhexiong Shang, Zhigang Shen
- **Comment**: 10 pages, 7 figures, conference paper submitted to CRC 2018
- **Journal**: None
- **Summary**: 3D reconstruction can be used as a platform to monitor the performance of activities on construction site, such as construction progress monitoring, structure inspection and post-disaster rescue. Comparing to other sensors, RGB image has the advantages of low-cost, texture rich and easy to implement that has been used as the primary method for 3D reconstruction in construction industry. However, the image-based 3D reconstruction always requires extended time to acquire and/or to process the image data, which limits its application on time critical projects. Recent progress in Visual Simultaneous Localization and Mapping (SLAM) make it possible to reconstruct a 3D map of construction site in real-time. Integrated with Unmanned Aerial Vehicle (UAV), the obstacles areas that are inaccessible for the ground equipment can also be sensed. Despite these advantages of visual SLAM and UAV, until now, such technique has not been fully investigated on construction site. Therefore, the objective of this research is to present a pilot study of using visual SLAM and UAV for real-time construction site reconstruction. The system architecture and the experimental setup are introduced, and the preliminary results and the potential applications using Visual SLAM and UAV on construction site are discussed.



### Query-Efficient Black-box Adversarial Examples (superceded)
- **Arxiv ID**: http://arxiv.org/abs/1712.07113v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.07113v2)
- **Published**: 2017-12-19 18:58:10+00:00
- **Updated**: 2018-04-06 17:20:27+00:00
- **Authors**: Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin
- **Comment**: Superceded by "Black-Box Adversarial Attacks with Limited Queries and
  Information."
- **Journal**: None
- **Summary**: Note that this paper is superceded by "Black-Box Adversarial Attacks with Limited Queries and Information."   Current neural network-based image classifiers are susceptible to adversarial examples, even in the black-box setting, where the attacker is limited to query access without access to gradients. Previous methods --- substitute networks and coordinate-based finite-difference methods --- are either unreliable or query-inefficient, making these methods impractical for certain problems.   We introduce a new method for reliably generating adversarial examples under more restricted, practical black-box threat models. First, we apply natural evolution strategies to perform black-box attacks using two to three orders of magnitude fewer queries than previous methods. Second, we introduce a new algorithm to perform targeted adversarial attacks in the partial-information setting, where the attacker only has access to a limited number of target classes. Using these techniques, we successfully perform the first targeted adversarial attack against a commercially deployed machine learning system, the Google Cloud Vision API, in the partial information setting.



### Low-Shot Learning with Imprinted Weights
- **Arxiv ID**: http://arxiv.org/abs/1712.07136v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07136v2)
- **Published**: 2017-12-19 19:00:08+00:00
- **Updated**: 2018-04-06 18:00:01+00:00
- **Authors**: Hang Qi, Matthew Brown, David G. Lowe
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Human vision is able to immediately recognize novel visual categories after seeing just one or a few training examples. We describe how to add a similar capability to ConvNet classifiers by directly setting the final layer weights from novel training examples during low-shot learning. We call this process weight imprinting as it directly sets weights for a new category based on an appropriately scaled copy of the embedding layer activations for that training example. The imprinting process provides a valuable complement to training with stochastic gradient descent, as it provides immediate good classification performance and an initialization for any further fine-tuning in the future. We show how this imprinting process is related to proxy-based embeddings. However, it differs in that only a single imprinted weight vector is learned for each novel category, rather than relying on a nearest-neighbor distance to training instances as typically used with embedding methods. Our experiments show that using averaging of imprinted weights provides better generalization than using nearest-neighbor instance embeddings.



### Real-time deep hair matting on mobile devices
- **Arxiv ID**: http://arxiv.org/abs/1712.07168v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07168v2)
- **Published**: 2017-12-19 19:36:08+00:00
- **Updated**: 2018-01-10 20:48:35+00:00
- **Authors**: Alex Levinshtein, Cheng Chang, Edmund Phung, Irina Kezele, Wenzhangzhi Guo, Parham Aarabi
- **Comment**: 7 pages, 7 figures, submitted to CRV 2018
- **Journal**: None
- **Summary**: Augmented reality is an emerging technology in many application domains. Among them is the beauty industry, where live virtual try-on of beauty products is of great importance. In this paper, we address the problem of live hair color augmentation. To achieve this goal, hair needs to be segmented quickly and accurately. We show how a modified MobileNet CNN architecture can be used to segment the hair in real-time. Instead of training this network using large amounts of accurate segmentation data, which is difficult to obtain, we use crowd sourced hair segmentation data. While such data is much simpler to obtain, the segmentations there are noisy and coarse. Despite this, we show how our system can produce accurate and fine-detailed hair mattes, while running at over 30 fps on an iPad Pro tablet.



### Y-net: 3D intracranial artery segmentation using a convolutional autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1712.07194v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.07194v1)
- **Published**: 2017-12-19 20:38:51+00:00
- **Updated**: 2017-12-19 20:38:51+00:00
- **Authors**: Li Chen, Yanjun Xie, Jie Sun, Niranjan Balu, Mahmud Mossa-Basha, Kristi Pimentel, Thomas S. Hatsukami, Jenq-Neng Hwang, Chun Yuan
- **Comment**: 5 pages, 4 figures, an improved version after accepted by IEEE
  International Conference on Bioinformatics and Biomedicine, Kansas City, MO,
  USA, November 13 - 16, 2017
- **Journal**: None
- **Summary**: Automated segmentation of intracranial arteries on magnetic resonance angiography (MRA) allows for quantification of cerebrovascular features, which provides tools for understanding aging and pathophysiological adaptations of the cerebrovascular system. Using a convolutional autoencoder (CAE) for segmentation is promising as it takes advantage of the autoencoder structure in effective noise reduction and feature extraction by representing high dimensional information with low dimensional latent variables. In this report, an optimized CAE model (Y-net) was trained to learn a 3D segmentation model of intracranial arteries from 49 cases of MRA data. The trained model was shown to perform better than the three traditional segmentation methods in both binary classification and visual evaluation.



### Deep Regression Forests for Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/1712.07195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07195v1)
- **Published**: 2017-12-19 20:42:15+00:00
- **Updated**: 2017-12-19 20:42:15+00:00
- **Authors**: Wei Shen, Yilu Guo, Yan Wang, Kai Zhao, Bo Wang, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Age estimation from facial images is typically cast as a nonlinear regression problem. The main challenge of this problem is the facial feature space w.r.t. ages is heterogeneous, due to the large variation in facial appearance across different persons of the same age and the non-stationary property of aging patterns. In this paper, we propose Deep Regression Forests (DRFs), an end-to-end model, for age estimation. DRFs connect the split nodes to a fully connected layer of a convolutional neural network (CNN) and deal with heterogeneous data by jointly learning input-dependant data partitions at the split nodes and data abstractions at the leaf nodes. This joint learning follows an alternating strategy: First, by fixing the leaf nodes, the split nodes as well as the CNN parameters are optimized by Back-propagation; Then, by fixing the split nodes, the leaf nodes are optimized by iterating a step-size free and fast-converging update rule derived from Variational Bounding. We verify the proposed DRFs on three standard age estimation benchmarks and achieve state-of-the-art results on all of them.



### Hyperparameters Optimization in Deep Convolutional Neural Network / Bayesian Approach with Gaussian Process Prior
- **Arxiv ID**: http://arxiv.org/abs/1712.07233v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.07233v1)
- **Published**: 2017-12-19 21:48:56+00:00
- **Updated**: 2017-12-19 21:48:56+00:00
- **Authors**: Pushparaja Murugan
- **Comment**: 10 Pages
- **Journal**: None
- **Summary**: Convolutional Neural Network is known as ConvNet have been extensively used in many complex machine learning tasks. However, hyperparameters optimization is one of a crucial step in developing ConvNet architectures, since the accuracy and performance are reliant on the hyperparameters. This multilayered architecture parameterized by a set of hyperparameters such as the number of convolutional layers, number of fully connected dense layers & neurons, the probability of dropout implementation, learning rate. Hence the searching the hyperparameter over the hyperparameter space are highly difficult to build such complex hierarchical architecture. Many methods have been proposed over the decade to explore the hyperparameter space and find the optimum set of hyperparameter values. Reportedly, Gird search and Random search are said to be inefficient and extremely expensive, due to a large number of hyperparameters of the architecture. Hence, Sequential model-based Bayesian Optimization is a promising alternative technique to address the extreme of the unknown cost function. The recent study on Bayesian Optimization by Snoek in nine convolutional network parameters is achieved the lowerest error report in the CIFAR-10 benchmark. This article is intended to provide the overview of the mathematical concept behind the Bayesian Optimization over a Gaussian prior.



### Multi-shot Pedestrian Re-identification via Sequential Decision Making
- **Arxiv ID**: http://arxiv.org/abs/1712.07257v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1712.07257v2)
- **Published**: 2017-12-19 23:24:04+00:00
- **Updated**: 2018-05-09 02:53:06+00:00
- **Authors**: Jianfu Zhang, Naiyan Wang, Liqing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-shot pedestrian re-identification problem is at the core of surveillance video analysis. It matches two tracks of pedestrians from different cameras. In contrary to existing works that aggregate single frames features by time series model such as recurrent neural network, in this paper, we propose an interpretable reinforcement learning based approach to this problem. Particularly, we train an agent to verify a pair of images at each time. The agent could choose to output the result (same or different) or request another pair of images to verify (unsure). By this way, our model implicitly learns the difficulty of image pairs, and postpone the decision when the model does not accumulate enough evidence. Moreover, by adjusting the reward for unsure action, we can easily trade off between speed and accuracy. In three open benchmarks, our method are competitive with the state-of-the-art methods while only using 3% to 6% images. These promising results demonstrate that our method is favorable in both efficiency and performance.



### Audio to Body Dynamics
- **Arxiv ID**: http://arxiv.org/abs/1712.09382v1
- **DOI**: 10.1109/CVPR.2018.00790
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1712.09382v1)
- **Published**: 2017-12-19 23:45:00+00:00
- **Updated**: 2017-12-19 23:45:00+00:00
- **Authors**: Eli Shlizerman, Lucio M. Dery, Hayden Schoen, Ira Kemelmacher-Shlizerman
- **Comment**: Link with videos https://arviolin.github.io/AudioBodyDynamics/
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2018
- **Summary**: We present a method that gets as input an audio of violin or piano playing, and outputs a video of skeleton predictions which are further used to animate an avatar. The key idea is to create an animation of an avatar that moves their hands similarly to how a pianist or violinist would do, just from audio. Aiming for a fully detailed correct arms and fingers motion is a goal, however, it's not clear if body movement can be predicted from music at all. In this paper, we present the first result that shows that natural body dynamics can be predicted at all. We built an LSTM network that is trained on violin and piano recital videos uploaded to the Internet. The predicted points are applied onto a rigged avatar to create the animation.



### FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation
- **Arxiv ID**: http://arxiv.org/abs/1712.07262v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07262v2)
- **Published**: 2017-12-19 23:49:25+00:00
- **Updated**: 2018-04-03 00:46:03+00:00
- **Authors**: Yaoqing Yang, Chen Feng, Yiru Shen, Dong Tian
- **Comment**: Accepted as a spotlight paper in CVPR'18
- **Journal**: None
- **Summary**: Recent deep networks that directly handle points in a point set, e.g., PointNet, have been state-of-the-art for supervised learning tasks on point clouds such as classification and segmentation. In this work, a novel end-to-end deep auto-encoder is proposed to address unsupervised learning challenges on point clouds. On the encoder side, a graph-based enhancement is enforced to promote local structures on top of PointNet. Then, a novel folding-based decoder deforms a canonical 2D grid onto the underlying 3D object surface of a point cloud, achieving low reconstruction errors even for objects with delicate structures. The proposed decoder only uses about 7% parameters of a decoder with fully-connected neural networks, yet leads to a more discriminative representation that achieves higher linear SVM classification accuracy than the benchmark. In addition, the proposed decoder structure is shown, in theory, to be a generic architecture that is able to reconstruct an arbitrary point cloud from a 2D grid. Our code is available at http://www.merl.com/research/license#FoldingNet



