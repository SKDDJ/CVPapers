# Arxiv Papers in cs.CV on 2017-12-14
### MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels
- **Arxiv ID**: http://arxiv.org/abs/1712.05055v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05055v2)
- **Published**: 2017-12-14 00:02:37+00:00
- **Updated**: 2018-08-13 21:27:39+00:00
- **Authors**: Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, Li Fei-Fei
- **Comment**: None
- **Journal**: published at ICML 2018
- **Summary**: Recent deep networks are capable of memorizing the entire data even when the labels are completely random. To overcome the overfitting on corrupted labels, we propose a novel technique of learning another neural network, called MentorNet, to supervise the training of the base deep networks, namely, StudentNet. During training, MentorNet provides a curriculum (sample weighting scheme) for StudentNet to focus on the sample the label of which is probably correct. Unlike the existing curriculum that is usually predefined by human experts, MentorNet learns a data-driven curriculum dynamically with StudentNet. Experimental results demonstrate that our approach can significantly improve the generalization performance of deep networks trained on corrupted training data. Notably, to the best of our knowledge, we achieve the best-published result on WebVision, a large benchmark containing 2.2 million images of real-world noisy labels. The code are at https://github.com/google/mentornet



### Weakly Supervised Action Localization by Sparse Temporal Pooling Network
- **Arxiv ID**: http://arxiv.org/abs/1712.05080v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05080v2)
- **Published**: 2017-12-14 03:34:03+00:00
- **Updated**: 2018-04-03 17:23:46+00:00
- **Authors**: Phuc Nguyen, Ting Liu, Gautam Prasad, Bohyung Han
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: We propose a weakly supervised temporal action localization algorithm on untrimmed videos using convolutional neural networks. Our algorithm learns from video-level class labels and predicts temporal intervals of human actions with no requirement of temporal localization annotations. We design our network to identify a sparse subset of key segments associated with target actions in a video using an attention module and fuse the key segments through adaptive temporal pooling. Our loss function is comprised of two terms that minimize the video-level action classification error and enforce the sparsity of the segment selection. At inference time, we extract and score temporal proposals using temporal class activations and class-agnostic attentions to estimate the time intervals that correspond to target actions. The proposed algorithm attains state-of-the-art results on the THUMOS14 dataset and outstanding performance on ActivityNet1.3 even with its weak supervision.



### Extreme 3D Face Reconstruction: Seeing Through Occlusions
- **Arxiv ID**: http://arxiv.org/abs/1712.05083v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05083v2)
- **Published**: 2017-12-14 03:53:52+00:00
- **Updated**: 2018-03-29 23:22:21+00:00
- **Authors**: Anh Tuan Tran, Tal Hassner, Iacopo Masi, Eran Paz, Yuval Nirkin, Gerard Medioni
- **Comment**: Accepted to CVPR'18. Previously titled: "Extreme 3D Face
  Reconstruction: Looking Past Occlusions"
- **Journal**: None
- **Summary**: Existing single view, 3D face reconstruction methods can produce beautifully detailed 3D results, but typically only for near frontal, unobstructed viewpoints. We describe a system designed to provide detailed 3D reconstructions of faces viewed under extreme conditions, out of plane rotations, and occlusions. Motivated by the concept of bump mapping, we propose a layered approach which decouples estimation of a global shape from its mid-level details (e.g., wrinkles). We estimate a coarse 3D face shape which acts as a foundation and then separately layer this foundation with details represented by a bump map. We show how a deep convolutional encoder-decoder can be used to estimate such bump maps. We further show how this approach naturally extends to generate plausible details for occluded facial regions. We test our approach and its components extensively, quantitatively demonstrating the invariance of our estimated facial details. We further provide numerous qualitative examples showing that our method produces detailed 3D face shapes in viewing conditions where existing state of the art often break down.



### Learning to Navigate by Growing Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.05084v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, 68T40
- **Links**: [PDF](http://arxiv.org/pdf/1712.05084v1)
- **Published**: 2017-12-14 03:58:23+00:00
- **Updated**: 2017-12-14 03:58:23+00:00
- **Authors**: Thushan Ganegedara, Lionel Ott, Fabio Ramos
- **Comment**: 10 pages, Australasian Conference on Robotics and Automation, 2017
- **Journal**: None
- **Summary**: Adaptability is central to autonomy. Intuitively, for high-dimensional learning problems such as navigating based on vision, internal models with higher complexity allow to accurately encode the information available. However, most learning methods rely on models with a fixed structure and complexity. In this paper, we present a self-supervised framework for robots to learn to navigate, without any prior knowledge of the environment, by incrementally building the structure of a deep network as new data becomes available. Our framework captures images from a monocular camera and self labels the images to continuously train and predict actions from a computationally efficient adaptive deep architecture based on Autoencoders (AE), in a self-supervised fashion. The deep architecture, named Reinforced Adaptive Denoising Autoencoders (RA-DAE), uses reinforcement learning to dynamically change the network structure by adding or removing neurons. Experiments were conducted in simulation and real-world indoor and outdoor environments to assess the potential of self-supervised navigation. RA-DAE demonstrates better performance than equivalent non-adaptive deep learning alternatives and can continue to expand its knowledge, trading-off past and present information.



### Learning Binary Residual Representations for Domain-specific Video Streaming
- **Arxiv ID**: http://arxiv.org/abs/1712.05087v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1712.05087v1)
- **Published**: 2017-12-14 04:06:33+00:00
- **Updated**: 2017-12-14 04:06:33+00:00
- **Authors**: Yi-Hsuan Tsai, Ming-Yu Liu, Deqing Sun, Ming-Hsuan Yang, Jan Kautz
- **Comment**: Accepted in AAAI'18. Project website at
  https://research.nvidia.com/publication/2018-02_Learning-Binary-Residual
- **Journal**: None
- **Summary**: We study domain-specific video streaming. Specifically, we target a streaming setting where the videos to be streamed from a server to a client are all in the same domain and they have to be compressed to a small size for low-latency transmission. Several popular video streaming services, such as the video game streaming services of GeForce Now and Twitch, fall in this category. While conventional video compression standards such as H.264 are commonly used for this task, we hypothesize that one can leverage the property that the videos are all in the same domain to achieve better video quality. Based on this hypothesis, we propose a novel video compression pipeline. Specifically, we first apply H.264 to compress domain-specific videos. We then train a novel binary autoencoder to encode the leftover domain-specific residual information frame-by-frame into binary representations. These binary representations are then compressed and sent to the client together with the H.264 stream. In our experiments, we show that our pipeline yields consistent gains over standard H.264 compression across several benchmark datasets while using the same channel bandwidth.



### Detection and Attention: Diagnosing Pulmonary Lung Cancer from CT by Imitating Physicians
- **Arxiv ID**: http://arxiv.org/abs/1712.05114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05114v1)
- **Published**: 2017-12-14 07:34:32+00:00
- **Updated**: 2017-12-14 07:34:32+00:00
- **Authors**: Ning Li, Haopeng Liu, Bin Qiu, Wei Guo, Shijun Zhao, Kungang Li, Jie He
- **Comment**: 8 pages, 8 figures, 5 tables
- **Journal**: None
- **Summary**: This paper proposes a novel and efficient method to build a Computer-Aided Diagnoses (CAD) system for lung nodule detection based on Computed Tomography (CT). This task was treated as an Object Detection on Video (VID) problem by imitating how a radiologist reads CT scans. A lung nodule detector was trained to automatically learn nodule features from still images to detect lung nodule candidates with both high recall and accuracy. Unlike previous work which used 3-dimensional information around the nodule to reduce false positives, we propose two simple but efficient methods, Multi-slice propagation (MSP) and Motionless-guide suppression (MLGS), which analyze sequence information of CT scans to reduce false negatives and suppress false positives. We evaluated our method in open-source LUNA16 dataset which contains 888 CT scans, and obtained state-of-the-art result (Free-Response Receiver Operating Characteristic score of 0.892) with detection speed (end to end within 20 seconds per patient on a single NVidia GTX 1080) much higher than existing methods.



### Multi-appearance Segmentation and Extended 0-1 Program for Dense Small Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1712.05116v1
- **DOI**: 10.1371/journal.pone.0206168
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05116v1)
- **Published**: 2017-12-14 07:47:50+00:00
- **Updated**: 2017-12-14 07:47:50+00:00
- **Authors**: Longtao Chen, Jing Lou, Wei Zhu, Qingyuan Xia, Mingwu Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Aiming to address the fast multi-object tracking for dense small object in the cluster background, we review track orientated multi-hypothesis tracking(TOMHT) with consideration of batch optimization. Employing autocorrelation based motion score test and staged hypotheses merging approach, we build our homologous hypothesis generation and management method. A new one-to-many constraint is proposed and applied to tackle the track exclusions during complex occlusions. Besides, to achieve better results, we develop a multi-appearance segmentation for detection, which exploits tree-like topological information and realizes one threshold for one object. Experimental results verify the strength of our methods, indicating speed and performance advantages of our tracker.



### Semi-Automatic Algorithm for Breast MRI Lesion Segmentation Using Marker-Controlled Watershed Transformation
- **Arxiv ID**: http://arxiv.org/abs/1712.05200v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05200v1)
- **Published**: 2017-12-14 12:45:08+00:00
- **Updated**: 2017-12-14 12:45:08+00:00
- **Authors**: Sulaiman Vesal, Andres Diaz-Pinto, Nishant Ravikumar, Stephan Ellmann, Amirabbas Davari, Andreas Maier
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) is an effective imaging modality for identifying and localizing breast lesions in women. Accurate and precise lesion segmentation using a computer-aided-diagnosis (CAD) system, is a crucial step in evaluating tumor volume and in the quantification of tumor characteristics. However, this is a challenging task, since breast lesions have sophisticated shape, topological structure, and high variance in their intensity distribution across patients. In this paper, we propose a novel marker-controlled watershed transformation-based approach, which uses the brightest pixels in a region of interest (determined by experts) as markers to overcome this challenge, and accurately segment lesions in breast MRI. The proposed approach was evaluated on 106 lesions, which includes 64 malignant and 42 benign cases. Segmentation results were quantified by comparison with ground truth labels, using the Dice similarity coefficient (DSC) and Jaccard index (JI) metrics. The proposed method achieved an average Dice coefficient of 0.7808$\pm$0.1729 and Jaccard index of 0.6704$\pm$0.2167. These results illustrate that the proposed method shows promise for future work related to the segmentation and classification of benign and malignant breast lesions.



### SEE: Towards Semi-Supervised End-to-End Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.05404v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05404v1)
- **Published**: 2017-12-14 12:59:26+00:00
- **Updated**: 2017-12-14 12:59:26+00:00
- **Authors**: Christian Bartz, Haojin Yang, Christoph Meinel
- **Comment**: AAAI-18. arXiv admin note: substantial text overlap with
  arXiv:1707.08831
- **Journal**: None
- **Summary**: Detecting and recognizing text in natural scene images is a challenging, yet not completely solved task. In recent years several new systems that try to solve at least one of the two sub-tasks (text detection and text recognition) have been proposed. In this paper we present SEE, a step towards semi-supervised neural networks for scene text detection and recognition, that can be optimized end-to-end. Most existing works consist of multiple deep neural networks and several pre-processing steps. In contrast to this, we propose to use a single deep neural network, that learns to detect and recognize text from natural images, in a semi-supervised way. SEE is a network that integrates and jointly learns a spatial transformer network, which can learn to detect text regions in an image, and a text recognition network that takes the identified text regions and recognizes their textual content. We introduce the idea behind our novel approach and show its feasibility, by performing a range of experiments on standard benchmark datasets, where we achieve competitive results.



### Adaptive kNN using Expected Accuracy for Classification of Geo-Spatial Data
- **Arxiv ID**: http://arxiv.org/abs/1801.01453v1
- **DOI**: 10.1145/3167132.3167226
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.01453v1)
- **Published**: 2017-12-14 14:09:06+00:00
- **Updated**: 2017-12-14 14:09:06+00:00
- **Authors**: Mark Kibanov, Martin Becker, Juergen Mueller, Martin Atzmueller, Andreas Hotho, Gerd Stumme
- **Comment**: None
- **Journal**: None
- **Summary**: The k-Nearest Neighbor (kNN) classification approach is conceptually simple - yet widely applied since it often performs well in practical applications. However, using a global constant k does not always provide an optimal solution, e.g., for datasets with an irregular density distribution of data points. This paper proposes an adaptive kNN classifier where k is chosen dynamically for each instance (point) to be classified, such that the expected accuracy of classification is maximized. We define the expected accuracy as the accuracy of a set of structurally similar observations. An arbitrary similarity function can be used to find these observations. We introduce and evaluate different similarity functions. For the evaluation, we use five different classification tasks based on geo-spatial data. Each classification task consists of (tens of) thousands of items. We demonstrate, that the presented expected accuracy measures can be a good estimator for kNN performance, and the proposed adaptive kNN classifier outperforms common kNN and previously introduced adaptive kNN algorithms. Also, we show that the range of considered k can be significantly reduced to speed up the algorithm without negative influence on classification accuracy.



### Robust Estimation of Similarity Transformation for Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1712.05231v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05231v2)
- **Published**: 2017-12-14 14:09:36+00:00
- **Updated**: 2018-11-07 09:01:52+00:00
- **Authors**: Yang Li, Jianke Zhu, Steven C. H. Hoi, Wenjie Song, Zhefeng Wang, Hantang Liu
- **Comment**: Accepted by AAAI 2019
- **Journal**: None
- **Summary**: Most of existing correlation filter-based tracking approaches only estimate simple axis-aligned bounding boxes, and very few of them is capable of recovering the underlying similarity transformation. To tackle this challenging problem, in this paper, we propose a new correlation filter-based tracker with a novel robust estimation of similarity transformation on the large displacements. In order to efficiently search in such a large 4-DoF space in real-time, we formulate the problem into two 2-DoF sub-problems and apply an efficient Block Coordinates Descent solver to optimize the estimation result. Specifically, we employ an efficient phase correlation scheme to deal with both scale and rotation changes simultaneously in log-polar coordinates. Moreover, a variant of correlation filter is used to predict the translational motion individually. Our experimental results demonstrate that the proposed tracker achieves very promising prediction performance compared with the state-of-the-art visual object tracking methods while still retaining the advantages of high efficiency and simplicity in conventional correlation filter-based tracking methods.



### Pointwise Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.05245v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.05245v2)
- **Published**: 2017-12-14 14:25:52+00:00
- **Updated**: 2018-03-29 04:55:01+00:00
- **Authors**: Binh-Son Hua, Minh-Khoi Tran, Sai-Kit Yeung
- **Comment**: 10 pages, 6 figures, 10 tables. Paper accepted to CVPR 2018
- **Journal**: None
- **Summary**: Deep learning with 3D data such as reconstructed point clouds and CAD models has received great research interests recently. However, the capability of using point clouds with convolutional neural network has been so far not fully explored. In this paper, we present a convolutional neural network for semantic segmentation and object recognition with 3D point clouds. At the core of our network is pointwise convolution, a new convolution operator that can be applied at each point of a point cloud. Our fully convolutional network design, while being surprisingly simple to implement, can yield competitive accuracy in both semantic segmentation and object recognition task.



### Image Super-resolution via Feature-augmented Random Forest
- **Arxiv ID**: http://arxiv.org/abs/1712.05248v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05248v1)
- **Published**: 2017-12-14 14:27:39+00:00
- **Updated**: 2017-12-14 14:27:39+00:00
- **Authors**: Hailiang Li, Kin-Man Lam, Miaohui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent random-forest (RF)-based image super-resolution approaches inherit some properties from dictionary-learning-based algorithms, but the effectiveness of the properties in RF is overlooked in the literature. In this paper, we present a novel feature-augmented random forest (FARF) for image super-resolution, where the conventional gradient-based features are augmented with gradient magnitudes and different feature recipes are formulated on different stages in an RF. The advantages of our method are that, firstly, the dictionary-learning-based features are enhanced by adding gradient magnitudes, based on the observation that the non-linear gradient magnitude are with highly discriminative property. Secondly, generalized locality-sensitive hashing (LSH) is used to replace principal component analysis (PCA) for feature dimensionality reduction and original high-dimensional features are employed, instead of the compressed ones, for the leaf-nodes' regressors, since regressors can benefit from higher dimensional features. This original-compressed coupled feature sets scheme unifies the unsupervised LSH evaluation on both image super-resolution and content-based image retrieval (CBIR). Finally, we present a generalized weighted ridge regression (GWRR) model for the leaf-nodes' regressors. Experiment results on several public benchmark datasets show that our FARF method can achieve an average gain of about 0.3 dB, compared to traditional RF-based methods. Furthermore, a fine-tuned FARF model can compare to or (in many cases) outperform some recent stateof-the-art deep-learning-based algorithms.



### A Performance Evaluation of Local Features for Image Based 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1712.05271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05271v1)
- **Published**: 2017-12-14 15:04:58+00:00
- **Updated**: 2017-12-14 15:04:58+00:00
- **Authors**: Bin Fan, Qingqun Kong, Xinchao Wang, Zhiheng Wang, Shiming Xiang, Chunhong Pan, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: This paper performs a comprehensive and comparative evaluation of the state of the art local features for the task of image based 3D reconstruction. The evaluated local features cover the recently developed ones by using powerful machine learning techniques and the elaborately designed handcrafted features. To obtain a comprehensive evaluation, we choose to include both float type features and binary ones. Meanwhile, two kinds of datasets have been used in this evaluation. One is a dataset of many different scene types with groundtruth 3D points, containing images of different scenes captured at fixed positions, for quantitative performance evaluation of different local features in the controlled image capturing situations. The other dataset contains Internet scale image sets of several landmarks with a lot of unrelated images, which is used for qualitative performance evaluation of different local features in the free image collection situations. Our experimental results show that binary features are competent to reconstruct scenes from controlled image sequences with only a fraction of processing time compared to use float type features. However, for the case of large scale image set with many distracting images, float type features show a clear advantage over binary ones.



### Deep CNN ensembles and suggestive annotations for infant brain MRI segmentation
- **Arxiv ID**: http://arxiv.org/abs/1712.05319v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05319v2)
- **Published**: 2017-12-14 16:27:42+00:00
- **Updated**: 2017-12-19 16:54:19+00:00
- **Authors**: Jose Dolz, Christian Desrosiers, Li Wang, Jing Yuan, Dinggang Shen, Ismail Ben Ayed
- **Comment**: None
- **Journal**: None
- **Summary**: Precise 3D segmentation of infant brain tissues is an essential step towards comprehensive volumetric studies and quantitative analysis of early brain developement. However, computing such segmentations is very challenging, especially for 6-month infant brain, due to the poor image quality, among other difficulties inherent to infant brain MRI, e.g., the isointense contrast between white and gray matter and the severe partial volume effect due to small brain sizes. This study investigates the problem with an ensemble of semi-dense fully convolutional neural networks (CNNs), which employs T1-weighted and T2-weighted MR images as input. We demonstrate that the ensemble agreement is highly correlated with the segmentation errors. Therefore, our method provides measures that can guide local user corrections. To the best of our knowledge, this work is the first ensemble of 3D CNNs for suggesting annotations within images. Furthermore, inspired by the very recent success of dense networks, we propose a novel architecture, SemiDenseNet, which connects all convolutional layers directly to the end of the network. Our architecture allows the efficient propagation of gradients during training, while limiting the number of parameters, requiring one order of magnitude less parameters than popular medical image segmentation networks such as 3D U-Net. Another contribution of our work is the study of the impact that early or late fusions of multiple image modalities might have on the performances of deep architectures. We report evaluations of our method on the public data of the MICCAI iSEG-2017 Challenge on 6-month infant brain MRI segmentation, and show very competitive results among 21 teams, ranking first or second in most metrics.



### RAN4IQA: Restorative Adversarial Nets for No-Reference Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1712.05444v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05444v1)
- **Published**: 2017-12-14 20:37:49+00:00
- **Updated**: 2017-12-14 20:37:49+00:00
- **Authors**: Hongyu Ren, Diqi Chen, Yizhou Wang
- **Comment**: AAAI'18
- **Journal**: None
- **Summary**: Inspired by the free-energy brain theory, which implies that human visual system (HVS) tends to reduce uncertainty and restore perceptual details upon seeing a distorted image, we propose restorative adversarial net (RAN), a GAN-based model for no-reference image quality assessment (NR-IQA). RAN, which mimics the process of HVS, consists of three components: a restorator, a discriminator and an evaluator. The restorator restores and reconstructs input distorted image patches, while the discriminator distinguishes the reconstructed patches from the pristine distortion-free patches. After restoration, we observe that the perceptual distance between the restored and the distorted patches is monotonic with respect to the distortion level. We further define Gain of Restoration (GoR) based on this phenomenon. The evaluator predicts perceptual score by extracting feature representations from the distorted and restored patches to measure GoR. Eventually, the quality score of an input image is estimated by weighted sum of the patch scores. Experimental results on Waterloo Exploration, LIVE and TID2013 show the effectiveness and generalization ability of RAN compared to the state-of-the-art NR-IQA models.



### AI2-THOR: An Interactive 3D Environment for Visual AI
- **Arxiv ID**: http://arxiv.org/abs/1712.05474v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.05474v4)
- **Published**: 2017-12-14 23:17:24+00:00
- **Updated**: 2022-08-26 17:12:17+00:00
- **Authors**: Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, Aniruddha Kembhavi, Abhinav Gupta, Ali Farhadi
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at http://ai2thor.allenai.org. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.



