# Arxiv Papers in cs.CV on 2017-12-21
### Automatic Estimation of Ice Bottom Surfaces from Radar Imagery
- **Arxiv ID**: http://arxiv.org/abs/1712.07758v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07758v1)
- **Published**: 2017-12-21 00:56:47+00:00
- **Updated**: 2017-12-21 00:56:47+00:00
- **Authors**: Mingze Xu, David J Crandall, Geoffrey C Fox, John D Paden
- **Comment**: 5 pages, 3 figures, published in ICIP 2017
- **Journal**: None
- **Summary**: Ground-penetrating radar on planes and satellites now makes it practical to collect 3D observations of the subsurface structure of the polar ice sheets, providing crucial data for understanding and tracking global climate change. But converting these noisy readings into useful observations is generally done by hand, which is impractical at a continental scale. In this paper, we propose a computer vision-based technique for extracting 3D ice-bottom surfaces by viewing the task as an inference problem on a probabilistic graphical model. We first generate a seed surface subject to a set of constraints, and then incorporate additional sources of evidence to refine it via discrete energy minimization. We evaluate the performance of the tracking algorithm on 7 topographic sequences (each with over 3000 radar images) collected from the Canadian Arctic Archipelago with respect to human-labeled ground truth.



### Context-Aware Semantic Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1712.07778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07778v1)
- **Published**: 2017-12-21 03:19:00+00:00
- **Updated**: 2017-12-21 03:19:00+00:00
- **Authors**: Haofeng Li, Guanbin Li, Liang Lin, Yizhou Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently image inpainting has witnessed rapid progress due to generative adversarial networks (GAN) that are able to synthesize realistic contents. However, most existing GAN-based methods for semantic inpainting apply an auto-encoder architecture with a fully connected layer, which cannot accurately maintain spatial information. In addition, the discriminator in existing GANs struggle to understand high-level semantics within the image context and yield semantically consistent content. Existing evaluation criteria are biased towards blurry results and cannot well characterize edge preservation and visual authenticity in the inpainting results. In this paper, we propose an improved generative adversarial network to overcome the aforementioned limitations. Our proposed GAN-based framework consists of a fully convolutional design for the generator which helps to better preserve spatial structures and a joint loss function with a revised perceptual loss to capture high-level semantics in the context. Furthermore, we also introduce two novel measures to better assess the quality of image inpainting results. Experimental results demonstrate that our method outperforms the state of the art under a wide range of criteria.



### Deep learning for predicting refractive error from retinal fundus images
- **Arxiv ID**: http://arxiv.org/abs/1712.07798v1
- **DOI**: 10.1167/iovs.18-23887
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07798v1)
- **Published**: 2017-12-21 05:27:56+00:00
- **Updated**: 2017-12-21 05:27:56+00:00
- **Authors**: Avinash V. Varadarajan, Ryan Poplin, Katy Blumer, Christof Angermueller, Joe Ledsam, Reena Chopra, Pearse A. Keane, Greg S. Corrado, Lily Peng, Dale R. Webster
- **Comment**: None
- **Journal**: Investigative Ophthalmology & Visual Science (2018)
- **Summary**: Refractive error, one of the leading cause of visual impairment, can be corrected by simple interventions like prescribing eyeglasses. We trained a deep learning algorithm to predict refractive error from the fundus photographs from participants in the UK Biobank cohort, which were 45 degree field of view images and the AREDS clinical trial, which contained 30 degree field of view images. Our model use the "attention" method to identify features that are correlated with refractive error. Mean absolute error (MAE) of the algorithm's prediction compared to the refractive error obtained in the AREDS and UK Biobank. The resulting algorithm had a MAE of 0.56 diopters (95% CI: 0.55-0.56) for estimating spherical equivalent on the UK Biobank dataset and 0.91 diopters (95% CI: 0.89-0.92) for the AREDS dataset. The baseline expected MAE (obtained by simply predicting the mean of this population) was 1.81 diopters (95% CI: 1.79-1.84) for UK Biobank and 1.63 (95% CI: 1.60-1.67) for AREDS. Attention maps suggested that the foveal region was one of the most important areas used by the algorithm to make this prediction, though other regions also contribute to the prediction. The ability to estimate refractive error with high accuracy from retinal fundus photos has not been previously known and demonstrates that deep learning can be applied to make novel predictions from medical images. Given that several groups have recently shown that it is feasible to obtain retinal fundus photos using mobile phones and inexpensive attachments, this work may be particularly relevant in regions of the world where autorefractors may not be readily available.



### Wolf in Sheep's Clothing - The Downscaling Attack Against Deep Learning Applications
- **Arxiv ID**: http://arxiv.org/abs/1712.07805v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.07805v1)
- **Published**: 2017-12-21 06:17:43+00:00
- **Updated**: 2017-12-21 06:17:43+00:00
- **Authors**: Qixue Xiao, Kang Li, Deyue Zhang, Yier Jin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper considers security risks buried in the data processing pipeline in common deep learning applications. Deep learning models usually assume a fixed scale for their training and input data. To allow deep learning applications to handle a wide range of input data, popular frameworks, such as Caffe, TensorFlow, and Torch, all provide data scaling functions to resize input to the dimensions used by deep learning models. Image scaling algorithms are intended to preserve the visual features of an image after scaling. However, common image scaling algorithms are not designed to handle human crafted images. Attackers can make the scaling outputs look dramatically different from the corresponding input images.   This paper presents a downscaling attack that targets the data scaling process in deep learning applications. By carefully crafting input data that mismatches with the dimension used by deep learning models, attackers can create deceiving effects. A deep learning application effectively consumes data that are not the same as those presented to users. The visual inconsistency enables practical evasion and data poisoning attacks to deep learning applications. This paper presents proof-of-concept attack samples to popular deep-learning-based image classification applications. To address the downscaling attacks, the paper also suggests multiple potential mitigation strategies.



### Exploring Models and Data for Remote Sensing Image Caption Generation
- **Arxiv ID**: http://arxiv.org/abs/1712.07835v1
- **DOI**: 10.1109/TGRS.2017.2776321
- **Categories**: **cs.CV**, 68
- **Links**: [PDF](http://arxiv.org/pdf/1712.07835v1)
- **Published**: 2017-12-21 08:45:37+00:00
- **Updated**: 2017-12-21 08:45:37+00:00
- **Authors**: Xiaoqiang Lu, Binqiang Wang, Xiangtao Zheng, Xuelong Li
- **Comment**: 14 pages, 8 figures
- **Journal**: None
- **Summary**: Inspired by recent development of artificial satellite, remote sensing images have attracted extensive attention. Recently, noticeable progress has been made in scene classification and target detection.However, it is still not clear how to describe the remote sensing image content with accurate and concise sentences. In this paper, we investigate to describe the remote sensing images with accurate and flexible sentences. First, some annotated instructions are presented to better describe the remote sensing images considering the special characteristics of remote sensing images. Second, in order to exhaustively exploit the contents of remote sensing images, a large-scale aerial image data set is constructed for remote sensing image caption. Finally, a comprehensive review is presented on the proposed data set to fully advance the task of remote sensing caption. Extensive experiments on the proposed data set demonstrate that the content of the remote sensing image can be completely described by generating language descriptions. The data set is available at https://github.com/201528014227051/RSICD_optimal



### Simulating Patho-realistic Ultrasound Images using Deep Generative Networks with Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.07881v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07881v2)
- **Published**: 2017-12-21 11:31:56+00:00
- **Updated**: 2018-01-08 19:23:03+00:00
- **Authors**: Francis Tom, Debdoot Sheet
- **Comment**: To appear in the Proceedings of the 2018 IEEE International Symposium
  on Biomedical Imaging (ISBI 2018)
- **Journal**: None
- **Summary**: Ultrasound imaging makes use of backscattering of waves during their interaction with scatterers present in biological tissues. Simulation of synthetic ultrasound images is a challenging problem on account of inability to accurately model various factors of which some include intra-/inter scanline interference, transducer to surface coupling, artifacts on transducer elements, inhomogeneous shadowing and nonlinear attenuation. Current approaches typically solve wave space equations making them computationally expensive and slow to operate. We propose a generative adversarial network (GAN) inspired approach for fast simulation of patho-realistic ultrasound images. We apply the framework to intravascular ultrasound (IVUS) simulation. A stage 0 simulation performed using pseudo B-mode ultrasound image simulator yields speckle mapping of a digitally defined phantom. The stage I GAN subsequently refines them to preserve tissue specific speckle intensities. The stage II GAN further refines them to generate high resolution images with patho-realistic speckle profiles. We evaluate patho-realism of simulated images with a visual Turing test indicating an equivocal confusion in discriminating simulated from real. We also quantify the shift in tissue specific intensity distributions of the real and simulated images to prove their similarity.



### Track, then Decide: Category-Agnostic Vision-based Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1712.07920v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07920v1)
- **Published**: 2017-12-21 13:05:06+00:00
- **Updated**: 2017-12-21 13:05:06+00:00
- **Authors**: Aljoša Ošep, Wolfgang Mehner, Paul Voigtlaender, Bastian Leibe
- **Comment**: ICRA'18 submission
- **Journal**: None
- **Summary**: The most common paradigm for vision-based multi-object tracking is tracking-by-detection, due to the availability of reliable detectors for several important object categories such as cars and pedestrians. However, future mobile systems will need a capability to cope with rich human-made environments, in which obtaining detectors for every possible object category would be infeasible. In this paper, we propose a model-free multi-object tracking approach that uses a category-agnostic image segmentation method to track objects. We present an efficient segmentation mask-based tracker which associates pixel-precise masks reported by the segmentation. Our approach can utilize semantic information whenever it is available for classifying objects at the track level, while retaining the capability to track generic unknown objects in the absence of such information. We demonstrate experimentally that our approach achieves performance comparable to state-of-the-art tracking-by-detection methods for popular object categories such as cars and pedestrians. Additionally, we show that the proposed method can discover and robustly track a large variety of other objects.



### Encoding CNN Activations for Writer Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.07923v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07923v2)
- **Published**: 2017-12-21 13:07:40+00:00
- **Updated**: 2018-01-13 00:27:34+00:00
- **Authors**: Vincent Christlein, Andreas Maier
- **Comment**: (revised) DAS2018 submission
- **Journal**: None
- **Summary**: The encoding of local features is an essential part for writer identification and writer retrieval. While CNN activations have already been used as local features in related works, the encoding of these features has attracted little attention so far. In this work, we compare the established VLAD encoding with triangulation embedding. We further investigate generalized max pooling as an alternative to sum pooling and the impact of decorrelation and Exemplar SVMs. With these techniques, we set new standards on two publicly available datasets (ICDAR13, KHATT).



### Siamese Neural Networks for One-shot detection of Railway Track Switches
- **Arxiv ID**: http://arxiv.org/abs/1712.08036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08036v1)
- **Published**: 2017-12-21 16:02:21+00:00
- **Updated**: 2017-12-21 16:02:21+00:00
- **Authors**: Dattaraj J Rao, Shruti Mittal, S. Ritika
- **Comment**: 6 pages and 7 figures
- **Journal**: None
- **Summary**: Deep Learning methods have been extensively used to analyze video data to extract valuable information by classifying image frames and detecting objects. We describe a unique approach for using video feed from a moving Locomotive to continuously monitor the Railway Track and detect significant assets like Switches on the Track. The technique used here is called Siamese Networks, which uses 2 identical networks to learn the similarity between of 2 images. Here we will use a Siamese network to continuously compare Track images and detect any significant difference in the Track. Switch will be one of those images that will be different and we will find a mapping that clearly distinguishes the Switch from other possible Track anomalies. The same method will then be extended to detect any abnormalities on the Railway Track. Railway Transportation is unique in the sense that is has wheeled vehicles, Trains pulled by Locomotives, running on guided Rails at very high speeds nearing 200 mph. Multiple Tracks on the Rail network are connected to each other using an equipment called Switch or a Turnout. Switch is either operated manually or automatically through command from a Control center and it governs the movement of Trains on different Tracks of the network. Accurate location of these Switches is very important for the railroad and getting a true picture of their state in field is important. Modern trains use high definition video cameras facing the Track that continuously record video from track. Using a Siamese network and comparing to benchmark images, we describe a method to monitor the Track and highlight anomalies.



### Note on Attacking Object Detectors with Adversarial Stickers
- **Arxiv ID**: http://arxiv.org/abs/1712.08062v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.08062v2)
- **Published**: 2017-12-21 16:33:01+00:00
- **Updated**: 2018-07-23 19:37:07+00:00
- **Authors**: Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Dawn Song, Tadayoshi Kohno, Amir Rahmati, Atul Prakash, Florian Tramer
- **Comment**: Short Note: The full version of this paper was accepted to USENIX
  WOOT 2018, and is available at arXiv:1807.07769
- **Journal**: None
- **Summary**: Deep learning has proven to be a powerful tool for computer vision and has seen widespread adoption for numerous tasks. However, deep learning algorithms are known to be vulnerable to adversarial examples. These adversarial inputs are created such that, when provided to a deep learning algorithm, they are very likely to be mislabeled. This can be problematic when deep learning is used to assist in safety critical decisions. Recent research has shown that classifiers can be attacked by physical adversarial examples under various physical conditions. Given the fact that state-of-the-art objection detection algorithms are harder to be fooled by the same set of adversarial examples, here we show that these detectors can also be attacked by physical adversarial examples. In this note, we briefly show both static and dynamic test results. We design an algorithm that produces physical adversarial inputs, which can fool the YOLO object detector and can also attack Faster-RCNN with relatively high success rate based on transferability. Furthermore, our algorithm can compress the size of the adversarial inputs to stickers that, when attached to the targeted object, result in the detector either mislabeling or not detecting the object a high percentage of the time. This note provides a small set of results. Our upcoming paper will contain a thorough evaluation on other object detectors, and will present the algorithm.



### AVEID: Automatic Video System for Measuring Engagement In Dementia
- **Arxiv ID**: http://arxiv.org/abs/1712.08084v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.08084v1)
- **Published**: 2017-12-21 16:57:35+00:00
- **Updated**: 2017-12-21 16:57:35+00:00
- **Authors**: Viral Parekh, Pin Sym Foong, Shendong Zhao, Ramanathan Subramanian
- **Comment**: None
- **Journal**: None
- **Summary**: Engagement in dementia is typically measured using behavior observational scales (BOS) that are tedious and involve intensive manual labor to annotate, and are therefore not easily scalable. We propose AVEID, a low cost and easy-to-use video-based engagement measurement tool to determine the engagement level of a person with dementia (PwD) during digital interaction. We show that the objective behavioral measures computed via AVEID correlate well with subjective expert impressions for the popular MPES and OME BOS, confirming its viability and effectiveness. Moreover, AVEID measures can be obtained for a variety of engagement designs, thereby facilitating large-scale studies with PwD populations.



### Learning Intelligent Dialogs for Bounding Box Annotation
- **Arxiv ID**: http://arxiv.org/abs/1712.08087v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08087v3)
- **Published**: 2017-12-21 17:07:01+00:00
- **Updated**: 2018-11-20 14:50:26+00:00
- **Authors**: Ksenia Konyushkova, Jasper Uijlings, Christoph Lampert, Vittorio Ferrari
- **Comment**: This paper appeared at CVPR 2018
- **Journal**: None
- **Summary**: We introduce Intelligent Annotation Dialogs for bounding box annotation. We train an agent to automatically choose a sequence of actions for a human annotator to produce a bounding box in a minimal amount of time. Specifically, we consider two actions: box verification, where the annotator verifies a box generated by an object detector, and manual box drawing. We explore two kinds of agents, one based on predicting the probability that a box will be positively verified, and the other based on reinforcement learning. We demonstrate that (1) our agents are able to learn efficient annotation strategies in several scenarios, automatically adapting to the image difficulty, the desired quality of the boxes, and the detector strength; (2) in all scenarios the resulting annotation dialogs speed up annotation compared to manual box drawing alone and box verification alone, while also outperforming any fixed combination of verification and drawing in most scenarios; (3) in a realistic scenario where the detector is iteratively re-trained, our agents evolve a series of strategies that reflect the shifting trade-off between verification and drawing as the detector grows stronger.



### A Deep Learning Interpretable Classifier for Diabetic Retinopathy Disease Grading
- **Arxiv ID**: http://arxiv.org/abs/1712.08107v1
- **DOI**: 10.1016/j.neucom.2018.07.102
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T10, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1712.08107v1)
- **Published**: 2017-12-21 17:40:32+00:00
- **Updated**: 2017-12-21 17:40:32+00:00
- **Authors**: Jordi de la Torre, Aida Valls, Domenec Puig
- **Comment**: Submitted to Elsevier
- **Journal**: None
- **Summary**: Deep neural network models have been proven to be very successful in image classification tasks, also for medical diagnosis, but their main concern is its lack of interpretability. They use to work as intuition machines with high statistical confidence but unable to give interpretable explanations about the reported results. The vast amount of parameters of these models make difficult to infer a rationale interpretation from them. In this paper we present a diabetic retinopathy interpretable classifier able to classify retine images into the different levels of disease severity and of explaining its results by assigning a score for every point in the hidden and input space, evaluating its contribution to the final classification in a linear way. The generated visual maps can be interpreted by an expert in order to compare its own knowledge with the interpretation given by the model.



### Unifying Map and Landmark Based Representations for Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/1712.08125v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1712.08125v1)
- **Published**: 2017-12-21 18:02:14+00:00
- **Updated**: 2017-12-21 18:02:14+00:00
- **Authors**: Saurabh Gupta, David Fouhey, Sergey Levine, Jitendra Malik
- **Comment**: Project page with videos: https://s-gupta.github.io/cmpl/
- **Journal**: None
- **Summary**: This works presents a formulation for visual navigation that unifies map based spatial reasoning and path planning, with landmark based robust plan execution in noisy environments. Our proposed formulation is learned from data and is thus able to leverage statistical regularities of the world. This allows it to efficiently navigate in novel environments given only a sparse set of registered images as input for building representations for space. Our formulation is based on three key ideas: a learned path planner that outputs path plans to reach the goal, a feature synthesis engine that predicts features for locations along the planned path, and a learned goal-driven closed loop controller that can follow plans given these synthesized features. We test our approach for goal-driven navigation in simulated real world environments and report performance gains over competitive baseline approaches.



### Smart, Sparse Contours to Represent and Edit Images
- **Arxiv ID**: http://arxiv.org/abs/1712.08232v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08232v2)
- **Published**: 2017-12-21 22:11:49+00:00
- **Updated**: 2018-04-09 18:35:47+00:00
- **Authors**: Tali Dekel, Chuang Gan, Dilip Krishnan, Ce Liu, William T. Freeman
- **Comment**: Accepted to CVPR'18; Project page: contour2im.github.io
- **Journal**: None
- **Summary**: We study the problem of reconstructing an image from information stored at contour locations. We show that high-quality reconstructions with high fidelity to the source image can be obtained from sparse input, e.g., comprising less than $6\%$ of image pixels. This is a significant improvement over existing contour-based reconstruction methods that require much denser input to capture subtle texture information and to ensure image quality. Our model, based on generative adversarial networks, synthesizes texture and details in regions where no input information is provided. The semantic knowledge encoded into our model and the sparsity of the input allows to use contours as an intuitive interface for semantically-aware image manipulation: local edits in contour domain translate to long-range and coherent changes in pixel space. We can perform complex structural changes such as changing facial expression by simple edits of contours. Our experiments demonstrate that humans as well as a face recognition system mostly cannot distinguish between our reconstructions and the source images.



