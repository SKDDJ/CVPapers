# Arxiv Papers in cs.CV on 2017-12-30
### Face Synthesis from Visual Attributes via Sketch using Conditional VAEs and GANs
- **Arxiv ID**: http://arxiv.org/abs/1801.00077v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00077v1)
- **Published**: 2017-12-30 02:57:09+00:00
- **Updated**: 2017-12-30 02:57:09+00:00
- **Authors**: Xing Di, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic synthesis of faces from visual attributes is an important problem in computer vision and has wide applications in law enforcement and entertainment. With the advent of deep generative convolutional neural networks (CNNs), attempts have been made to synthesize face images from attributes and text descriptions. In this paper, we take a different approach, where we formulate the original problem as a stage-wise learning problem. We first synthesize the facial sketch corresponding to the visual attributes and then we reconstruct the face image based on the synthesized sketch. The proposed Attribute2Sketch2Face framework, which is based on a combination of deep Conditional Variational Autoencoder (CVAE) and Generative Adversarial Networks (GANs), consists of three stages: (1) Synthesis of facial sketch from attributes using a CVAE architecture, (2) Enhancement of coarse sketches to produce sharper sketches using a GAN-based framework, and (3) Synthesis of face from sketch using another GAN-based network. Extensive experiments and comparison with recent methods are performed to verify the effectiveness of the proposed attribute-based three stage face synthesis method.



### Towards automated patient data cleaning using deep learning: A feasibility study on the standardization of organ labeling
- **Arxiv ID**: http://arxiv.org/abs/1801.00096v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1801.00096v1)
- **Published**: 2017-12-30 07:56:46+00:00
- **Updated**: 2017-12-30 07:56:46+00:00
- **Authors**: Timothy Rozario, Troy Long, Mingli Chen, Weiguo Lu, Steve Jiang
- **Comment**: 17 pages, 7 figures, 3 tables, 39 references
- **Journal**: None
- **Summary**: Data cleaning consumes about 80% of the time spent on data analysis for clinical research projects. This is a much bigger problem in the era of big data and machine learning in the field of medicine where large volumes of data are being generated. We report an initial effort towards automated patient data cleaning using deep learning: the standardization of organ labeling in radiation therapy. Organs are often labeled inconsistently at different institutions (sometimes even within the same institution) and at different time periods, which poses a problem for clinical research, especially for multi-institutional collaborative clinical research where the acquired patient data is not being used effectively. We developed a convolutional neural network (CNN) to automatically identify each organ in the CT image and then label it with the standardized nomenclature presented at AAPM Task Group 263. We tested this model on the CT images of 54 patients with prostate and 100 patients with head and neck cancer who previously received radiation therapy. The model achieved 100% accuracy in detecting organs and assigning standardized labels for the patients tested. This work shows the feasibility of using deep learning in patient data cleaning that enables standardized datasets to be generated for effective intra- and interinstitutional collaborative clinical research.



### A PDE-based log-agnostic illumination correction algorithm
- **Arxiv ID**: http://arxiv.org/abs/1801.00098v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1801.00098v2)
- **Published**: 2017-12-30 08:02:56+00:00
- **Updated**: 2018-01-21 07:52:27+00:00
- **Authors**: U. A. Nnolim
- **Comment**: 22 pages, 9 figures, 4 tables
- **Journal**: None
- **Summary**: This report presents the results of a partial differential equation (PDE)-based image enhancement algorithm, for dynamic range compression and illumination correction in the absence of the logarithmic function. The proposed algorithm combines forward and reverse flows in a PDE-based formulation. The experimental results are compared with algorithms from the literature and indicate comparable performance in most cases.



### A Real-time and Registration-free Framework for Dynamic Shape Instantiation
- **Arxiv ID**: http://arxiv.org/abs/1801.00182v1
- **DOI**: 10.1016/j.media.2017.11.009
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00182v1)
- **Published**: 2017-12-30 19:25:09+00:00
- **Updated**: 2017-12-30 19:25:09+00:00
- **Authors**: Xiao-Yun Zhou, Guang-Zhong Yang, Su-Lin Lee
- **Comment**: 35 Pages, 11 figures
- **Journal**: Medical Image Analysis 44 (2017): 86-97
- **Summary**: Real-time 3D navigation during minimally invasive procedures is an essential yet challenging task, especially when considerable tissue motion is involved. To balance image acquisition speed and resolution, only 2D images or low-resolution 3D volumes can be used clinically. In this paper, a real-time and registration-free framework for dynamic shape instantiation, generalizable to multiple anatomical applications, is proposed to instantiate high-resolution 3D shapes of an organ from a single 2D image intra-operatively. Firstly, an approximate optimal scan plane was determined by analyzing the pre-operative 3D statistical shape model (SSM) of the anatomy with sparse principal component analysis (SPCA) and considering practical constraints . Secondly, kernel partial least squares regression (KPLSR) was used to learn the relationship between the pre-operative 3D SSM and a synchronized 2D SSM constructed from 2D images obtained at the approximate optimal scan plane. Finally, the derived relationship was applied to the new intra-operative 2D image obtained at the same scan plane to predict the high-resolution 3D shape intra-operatively. A major feature of the proposed framework is that no extra registration between the pre-operative 3D SSM and the synchronized 2D SSM is required. Detailed validation was performed on studies including the liver and right ventricle (RV) of the heart. The derived results (mean accuracy of 2.19mm on patients and computation speed of 1ms) demonstrate its potential clinical value for real-time, high-resolution, dynamic and 3D interventional guidance.



### Fractional Local Neighborhood Intensity Pattern for Image Retrieval using Genetic Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1801.00187v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00187v3)
- **Published**: 2017-12-30 20:18:32+00:00
- **Updated**: 2019-11-20 19:58:40+00:00
- **Authors**: Shuvozit Ghose, Abhirup Das, Ayan Kumar Bhunia, Partha Pratim Roy
- **Comment**: MTAP, Springer(Minor Revision)
- **Journal**: None
- **Summary**: In this paper, a new texture descriptor named "Fractional Local Neighborhood Intensity Pattern" (FLNIP) has been proposed for content based image retrieval (CBIR). It is an extension of the Local Neighborhood Intensity Pattern (LNIP)[1]. FLNIP calculates the relative intensity difference between a particular pixel and the center pixel of a 3x3 window by considering the relationship with adjacent neighbors. In this work, the fractional change in the local neighborhood involving the adjacent neighbors has been calculated first with respect to one of the eight neighbors of the center pixel of a 3x3 window. Next, the fractional change has been calculated with respect to the center itself. The two values of fractional change are next compared to generate a binary bit pattern. Both sign and magnitude information are encoded in a single descriptor as it deals with the relative change in magnitude in the adjacent neighborhood i.e., the comparison of the fractional change. The descriptor is applied on four multi-resolution images -- one being the raw image and the other three being filtered gaussian images obtained by applying gaussian filters of different standard deviations on the raw image to signify the importance of exploring texture information at different resolutions in an image. The four sets of distances obtained between the query and the target image are then combined with a genetic algorithm based approach to improve the retrieval performance by minimizing the distance between similar class images. The performance of the method has been tested for image retrieval on four popular databases. The precision and recall values observed on these databases have been compared with recent state-of-art local patterns. The proposed method has shown a significant improvement over many other existing methods.



### A Unified Method for First and Third Person Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.00192v2
- **DOI**: 10.1109/ICEE.2018.8472580
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00192v2)
- **Published**: 2017-12-30 21:03:13+00:00
- **Updated**: 2018-04-08 14:42:37+00:00
- **Authors**: Ali Javidani, Ahmad Mahmoudi-Aznaveh
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: In this paper, a new video classification methodology is proposed which can be applied in both first and third person videos. The main idea behind the proposed strategy is to capture complementary information of appearance and motion efficiently by performing two independent streams on the videos. The first stream is aimed to capture long-term motions from shorter ones by keeping track of how elements in optical flow images have changed over time. Optical flow images are described by pre-trained networks that have been trained on large scale image datasets. A set of multi-channel time series are obtained by aligning descriptions beside each other. For extracting motion features from these time series, PoT representation method plus a novel pooling operator is followed due to several advantages. The second stream is accomplished to extract appearance features which are vital in the case of video classification. The proposed method has been evaluated on both first and third-person datasets and results present that the proposed methodology reaches the state of the art successfully.



