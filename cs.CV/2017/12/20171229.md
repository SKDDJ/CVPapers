# Arxiv Papers in cs.CV on 2017-12-29
### Learning Deep and Compact Models for Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.10136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.10136v1)
- **Published**: 2017-12-29 07:38:43+00:00
- **Updated**: 2017-12-29 07:38:43+00:00
- **Authors**: Koustav Mullick, Anoop M. Namboodiri
- **Comment**: Accepted at 2017 IEEE International Conference on Image Processing
  (ICIP 2017)
- **Journal**: None
- **Summary**: We look at the problem of developing a compact and accurate model for gesture recognition from videos in a deep-learning framework. Towards this we propose a joint 3DCNN-LSTM model that is end-to-end trainable and is shown to be better suited to capture the dynamic information in actions. The solution achieves close to state-of-the-art accuracy on the ChaLearn dataset, with only half the model size. We also explore ways to derive a much more compact representation in a knowledge distillation framework followed by model compression. The final model is less than $1~MB$ in size, which is less than one hundredth of our initial model, with a drop of $7\%$ in accuracy, and is suitable for real-time gesture recognition on mobile devices.



### Significance of Softmax-based Features in Comparison to Distance Metric Learning-based Features
- **Arxiv ID**: http://arxiv.org/abs/1712.10151v2
- **DOI**: 10.1109/TPAMI.2019.2911075
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.10151v2)
- **Published**: 2017-12-29 09:01:09+00:00
- **Updated**: 2019-04-13 04:05:27+00:00
- **Authors**: Shota Horiguchi, Daiki Ikami, Kiyoharu Aizawa
- **Comment**: 6 pages
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2019
- **Summary**: The extraction of useful deep features is important for many computer vision tasks. Deep features extracted from classification networks have proved to perform well in those tasks. To obtain features of greater usefulness, end-to-end distance metric learning (DML) has been applied to train the feature extractor directly. However, in these DML studies, there were no equitable comparisons between features extracted from a DML-based network and those from a softmax-based network. In this paper, by presenting objective comparisons between these two approaches under the same network architecture, we show that the softmax-based features perform competitive, or even better, to the state-of-the-art DML features when the size of the dataset, that is, the number of training samples per class, is large. The results suggest that softmax-based features should be properly taken into account when evaluating the performance of deep features.



### Exploring the significance of using perceptually relevant image decolorization method for scene classification
- **Arxiv ID**: http://arxiv.org/abs/1712.10152v1
- **DOI**: 10.1117/1.JEI.26.6.063019
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.10152v1)
- **Published**: 2017-12-29 09:04:02+00:00
- **Updated**: 2017-12-29 09:04:02+00:00
- **Authors**: V. Sowmya, D. Govind, K. P. Soman
- **Comment**: This article is accepted in SPIE J.of Electronic Imaging with title:
  Significance of perceptually relevant image decolorization for scene
  classification
- **Journal**: Sowmya Viswanathan, Govind Divakaran, Kutti Padanyl Soman,
  Significance of perceptually relevant image decolorization for scene
  classification, J. Electron. Imaging 26(6), 063019 (2017)
- **Summary**: A color image contains luminance and chrominance components representing the intensity and color information respectively. The objective of the work presented in this paper is to show the significance of incorporating the chrominance information for the task of scene classification. An improved color-to-grayscale image conversion algorithm by effectively incorporating the chrominance information is proposed using color-to-gay structure similarity index (C2G-SSIM) and singular value decomposition (SVD) to improve the perceptual quality of the converted grayscale images. The experimental result analysis based on the image quality assessment for image decolorization called C2G-SSIM and success rate (Cadik and COLOR250 datasets) shows that the proposed image decolorization technique performs better than 8 existing benchmark algorithms for image decolorization. In the second part of the paper, the effectiveness of incorporating the chrominance component in scene classification task is demonstrated using the deep belief network (DBN) based image classification system developed using dense scale invariant feature transform (SIFT) as features. The levels of chrominance information incorporated by the proposed image decolorization technique is confirmed by the improvement in the overall scene classification accuracy . Also, the overall scene classification performance is improved by the combination of models obtained using the proposed and the conventional decolorization methods.



### Polyp detection inside the capsule endoscopy: an approach for power consumption reduction
- **Arxiv ID**: http://arxiv.org/abs/1712.10164v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.10164v1)
- **Published**: 2017-12-29 09:54:08+00:00
- **Updated**: 2017-12-29 09:54:08+00:00
- **Authors**: Mohammad Amin Khorsandi, Nader Karimi, Shadrokh Samavi
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule endoscopy is a novel and non-invasive method for diagnosis, which assists gastroenterologists to monitor the digestive track. Although this new technology has many advantages over the conventional endoscopy, there are weaknesses that limits the usage of this technology. Some weaknesses are due to using small-size batteries. Radio transmitter consumes the largest portion of energy; consequently, a simple way to reduce the power consumption is to reduce the data to be transmitted. Many works are proposed to reduce the amount of data to be transmitted consist of specific compression methods and reduction in video resolution and frame rate. We proposed a system inside the capsule for detecting informative frames and sending these frames instead of several non-informative frames. In this work, we specifically focused on hardware friendly algorithm (with capability of parallelism and pipeline) for implementation of polyp detection. Two features of positive contrast and customized edges of polyps are exploited to define whether the frame consists of polyp or not. The proposed method is devoid of complex and iterative structure to save power and reduce the response time. Experimental results indicate acceptable rate of detection of our work.



### A Resilient Image Matching Method with an Affine Invariant Feature Detector and Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1802.09623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.09623v1)
- **Published**: 2017-12-29 11:36:58+00:00
- **Updated**: 2017-12-29 11:36:58+00:00
- **Authors**: Biao Zhao, Shigang Yue
- **Comment**: None
- **Journal**: None
- **Summary**: Image feature matching is to seek, localize and identify the similarities across the images. The matched local features between different images can indicate the similarities of their content. Resilience of image feature matching to large view point changes is challenging for a lot of applications such as 3D object reconstruction, object recognition and navigation, etc, which need accurate and robust feature matching from quite different view points. In this paper we propose a novel image feature matching algorithm, integrating our previous proposed Affine Invariant Feature Detector (AIFD) and new proposed Affine Invariant Feature Descriptor (AIFDd). Both stages of this new proposed algorithm can provide sufficient resilience to view point changes. With systematic experiments, we can prove that the proposed method of feature detector and descriptor outperforms other state-of-the-art feature matching algorithms especially on view points robustness. It also performs well under other conditions such as the change of illumination, rotation and compression, etc.



### Dense Pooling layers in Fully Convolutional Network for Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1712.10207v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.10207v4)
- **Published**: 2017-12-29 12:46:52+00:00
- **Updated**: 2019-08-31 09:34:49+00:00
- **Authors**: Ebrahim Nasr-Esfahani, Shima Rafiei, Mohammad H. Jafari, Nader Karimi, James S. Wrobel, S. M. Reza Soroushmehr, Shadrokh Samavi, Kayvan Najarian
- **Comment**: None
- **Journal**: None
- **Summary**: One of the essential tasks in medical image analysis is segmentation and accurate detection of borders. Lesion segmentation in skin images is an essential step in the computerized detection of skin cancer. However, many of the state-of-the-art segmentation methods have deficiencies in their border detection phase. In this paper, a new class of fully convolutional network is proposed, with new dense pooling layers for segmentation of lesion regions in skin images. This network leads to highly accurate segmentation of lesions on skin lesion datasets which outperforms state-of-the-art algorithms in the skin lesion segmentation.



### Learning Deep Similarity Models with Focus Ranking for Fabric Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1712.10211v1
- **DOI**: 10.1016/j.imavis.2017.12.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.10211v1)
- **Published**: 2017-12-29 13:02:12+00:00
- **Updated**: 2017-12-29 13:02:12+00:00
- **Authors**: Daiguo Deng, Ruomei Wang, Hefeng Wu, Huayong He, Qi Li, Xiaonan Luo
- **Comment**: 11 pages, 9 figures, accepted by Image and Vision Computing
- **Journal**: None
- **Summary**: Fabric image retrieval is beneficial to many applications including clothing searching, online shopping and cloth modeling. Learning pairwise image similarity is of great importance to an image retrieval task. With the resurgence of Convolutional Neural Networks (CNNs), recent works have achieved significant progresses via deep representation learning with metric embedding, which drives similar examples close to each other in a feature space, and dissimilar ones apart from each other. In this paper, we propose a novel embedding method termed focus ranking that can be easily unified into a CNN for jointly learning image representations and metrics in the context of fine-grained fabric image retrieval. Focus ranking aims to rank similar examples higher than all dissimilar ones by penalizing ranking disorders via the minimization of the overall cost attributed to similar samples being ranked below dissimilar ones. At the training stage, training samples are organized into focus ranking units for efficient optimization. We build a large-scale fabric image retrieval dataset (FIRD) with about 25,000 images of 4,300 fabrics, and test the proposed model on the FIRD dataset. Experimental results show the superiority of the proposed model over existing metric embedding models.



### ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans
- **Arxiv ID**: http://arxiv.org/abs/1712.10215v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.10215v2)
- **Published**: 2017-12-29 13:17:06+00:00
- **Updated**: 2018-03-28 01:51:41+00:00
- **Authors**: Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, Jürgen Sturm, Matthias Nießner
- **Comment**: Video: https://youtu.be/5s5s8iH0NF8
- **Journal**: None
- **Summary**: We introduce ScanComplete, a novel data-driven approach for taking an incomplete 3D scan of a scene as input and predicting a complete 3D model along with per-voxel semantic labels. The key contribution of our method is its ability to handle large scenes with varying spatial extent, managing the cubic growth in data size as scene size increases. To this end, we devise a fully-convolutional generative 3D CNN model whose filter kernels are invariant to the overall scene size. The model can be trained on scene subvolumes but deployed on arbitrarily large scenes at test time. In addition, we propose a coarse-to-fine inference strategy in order to produce high-resolution output while also leveraging large input context sizes. In an extensive series of experiments, we carefully evaluate different model design choices, considering both deterministic and probabilistic models for completion and semantic inference. Our results show that we outperform other methods not only in the size of the environments handled and processing efficiency, but also with regard to completion quality and semantic segmentation performance by a significant margin.



### Deep Learning Interior Tomography for Region-of-Interest Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1712.10248v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.10248v2)
- **Published**: 2017-12-29 14:59:41+00:00
- **Updated**: 2018-01-03 15:54:24+00:00
- **Authors**: Yoseob Han, Jawook Gu, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Interior tomography for the region-of-interest (ROI) imaging has advantages of using a small detector and reducing X-ray radiation dose. However, standard analytic reconstruction suffers from severe cupping artifacts due to existence of null space in the truncated Radon transform. Existing penalized reconstruction methods may address this problem but they require extensive computations due to the iterative reconstruction. Inspired by the recent deep learning approaches to low-dose and sparse view CT, here we propose a deep learning architecture that removes null space signals from the FBP reconstruction. Experimental results have shown that the proposed method provides near-perfect reconstruction with about 7-10 dB improvement in PSNR over existing methods in spite of significantly reduced run-time complexity.



### Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward
- **Arxiv ID**: http://arxiv.org/abs/1801.00054v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00054v3)
- **Published**: 2017-12-29 22:51:36+00:00
- **Updated**: 2018-02-13 19:34:43+00:00
- **Authors**: Kaiyang Zhou, Yu Qiao, Tao Xiang
- **Comment**: AAAI 2018
- **Journal**: None
- **Summary**: Video summarization aims to facilitate large-scale video browsing by producing short, concise summaries that are diverse and representative of original videos. In this paper, we formulate video summarization as a sequential decision-making process and develop a deep summarization network (DSN) to summarize videos. DSN predicts for each video frame a probability, which indicates how likely a frame is selected, and then takes actions based on the probability distributions to select frames, forming video summaries. To train our DSN, we propose an end-to-end, reinforcement learning-based framework, where we design a novel reward function that jointly accounts for diversity and representativeness of generated summaries and does not rely on labels or user interactions at all. During training, the reward function judges how diverse and representative the generated summaries are, while DSN strives for earning higher rewards by learning to produce more diverse and more representative summaries. Since labels are not required, our method can be fully unsupervised. Extensive experiments on two benchmark datasets show that our unsupervised method not only outperforms other state-of-the-art unsupervised methods, but also is comparable to or even superior than most of published supervised approaches.



### Deformable GANs for Pose-based Human Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1801.00055v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00055v2)
- **Published**: 2017-12-29 22:58:31+00:00
- **Updated**: 2018-04-06 16:19:17+00:00
- **Authors**: Aliaksandr Siarohin, Enver Sangineto, Stephane Lathuiliere, Nicu Sebe
- **Comment**: CVPR 2018 version
- **Journal**: None
- **Summary**: In this paper we address the problem of generating person images conditioned on a given pose. Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with pixel-to-pixel misalignments caused by the pose differences, we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. We test our approach using photos of persons in different poses and we compare our method with previous work in this area showing state-of-the-art results in two benchmarks. Our method can be applied to the wider field of deformable object generation, provided that the pose of the articulated object can be extracted using a keypoint detector.



