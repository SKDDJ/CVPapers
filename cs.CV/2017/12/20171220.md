# Arxiv Papers in cs.CV on 2017-12-20
### Learning Sight from Sound: Ambient Sound Provides Supervision for Visual Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.07271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07271v1)
- **Published**: 2017-12-20 00:10:40+00:00
- **Updated**: 2017-12-20 00:10:40+00:00
- **Authors**: Andrew Owens, Jiajun Wu, Josh H. McDermott, William T. Freeman, Antonio Torralba
- **Comment**: Journal preprint of arXiv:1608.07017 (unpublished submission to IJCV)
- **Journal**: None
- **Summary**: The sound of crashing waves, the roar of fast-moving cars -- sound conveys important information about the objects in our surroundings. In this work, we show that ambient sounds can be used as a supervisory signal for learning visual models. To demonstrate this, we train a convolutional neural network to predict a statistical summary of the sound associated with a video frame. We show that, through this process, the network learns a representation that conveys information about objects and scenes. We evaluate this representation on several recognition tasks, finding that its performance is comparable to that of other state-of-the-art unsupervised learning methods. Finally, we show through visualizations that the network learns units that are selective to objects that are often associated with characteristic sounds. This paper extends an earlier conference paper, Owens et al. 2016, with additional experiments and discussion.



### LVreID: Person Re-Identification with Long Sequence Videos
- **Arxiv ID**: http://arxiv.org/abs/1712.07286v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07286v4)
- **Published**: 2017-12-20 01:38:53+00:00
- **Updated**: 2018-11-19 02:16:17+00:00
- **Authors**: Jianing Li, Shiliang Zhang, Jingdong Wang, Wen Gao, Qi Tian
- **Comment**: There is experimental error in secction 5.7
- **Journal**: None
- **Summary**: This paper mainly establishes a large-scale Long sequence Video database for person re-IDentification (LVreID).



### Analysis of supervised and semi-supervised GrowCut applied to segmentation of masses in mammography images
- **Arxiv ID**: http://arxiv.org/abs/1712.07312v1
- **DOI**: 10.1080/21681163.2015.1127775
- **Categories**: **cs.CV**, cs.AI, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1712.07312v1)
- **Published**: 2017-12-20 03:50:15+00:00
- **Updated**: 2017-12-20 03:50:15+00:00
- **Authors**: Filipe Rolim Cordeiro, Wellington Pinheiro dos Santos, Abel Guilhermino da Silva Filho
- **Comment**: None
- **Journal**: Computer Methods in Biomechanics and Biomedical Engineering:
  Imaging & Visualization, v. 5, p. 1-19, 2017
- **Summary**: Breast cancer is already one of the most common form of cancer worldwide. Mammography image analysis is still the most effective diagnostic method to promote the early detection of breast cancer. Accurately segmenting tumors in digital mammography images is important to improve diagnosis capabilities of health specialists and avoid misdiagnosis. In this work, we evaluate the feasibility of applying GrowCut to segment regions of tumor and we propose two GrowCut semi-supervised versions. All the analysis was performed by evaluating the application of segmentation techniques to a set of images obtained from the Mini-MIAS mammography image database. GrowCut segmentation was compared to Region Growing, Active Contours, Random Walks and Graph Cut techniques. Experiments showed that GrowCut, when compared to the other techniques, was able to acquire better results for the metrics analyzed. Moreover, the proposed semi-supervised versions of GrowCut was proved to have a clinically satisfactory quality of segmentation.



### Detection and classification of masses in mammographic images in a multi-kernel approach
- **Arxiv ID**: http://arxiv.org/abs/1712.07116v1
- **DOI**: 10.1016/j.cmpb.2016.04.029
- **Categories**: **cs.CV**, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1712.07116v1)
- **Published**: 2017-12-20 03:57:39+00:00
- **Updated**: 2017-12-20 03:57:39+00:00
- **Authors**: Sidney Marlon Lopes de Lima, Abel Guilhermino da Silva Filho, Wellington Pinheiro dos Santos
- **Comment**: None
- **Journal**: Computer Methods and Programs in Biomedicine, 134 (2016), 11-29
- **Summary**: According to the World Health Organization, breast cancer is the main cause of cancer death among adult women in the world. Although breast cancer occurs indiscriminately in countries with several degrees of social and economic development, among developing and underdevelopment countries mortality rates are still high, due to low availability of early detection technologies. From the clinical point of view, mammography is still the most effective diagnostic technology, given the wide diffusion of the use and interpretation of these images. Herein this work we propose a method to detect and classify mammographic lesions using the regions of interest of images. Our proposal consists in decomposing each image using multi-resolution wavelets. Zernike moments are extracted from each wavelet component. Using this approach we can combine both texture and shape features, which can be applied both to the detection and classification of mammary lesions. We used 355 images of fatty breast tissue of IRMA database, with 233 normal instances (no lesion), 72 benign, and 83 malignant cases. Classification was performed by using SVM and ELM networks with modified kernels, in order to optimize accuracy rates, reaching 94.11%. Considering both accuracy rates and training times, we defined the ration between average percentage accuracy and average training time in a reverse order. Our proposal was 50 times higher than the ratio obtained using the best method of the state-of-the-art. As our proposed model can combine high accuracy rate with low learning time, whenever a new data is received, our work will be able to save a lot of time, hours, in learning process in relation to the best method of the state-of-the-art.



### Lost in Time: Temporal Analytics for Long-Term Video Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1712.07322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07322v1)
- **Published**: 2017-12-20 05:07:48+00:00
- **Updated**: 2017-12-20 05:07:48+00:00
- **Authors**: Huai-Qian Khor, John See
- **Comment**: To Appear in Springer LNEE
- **Journal**: None
- **Summary**: Video surveillance is a well researched area of study with substantial work done in the aspects of object detection, tracking and behavior analysis. With the abundance of video data captured over a long period of time, we can understand patterns in human behavior and scene dynamics through data-driven temporal analytics. In this work, we propose two schemes to perform descriptive and predictive analytics on long-term video surveillance data. We generate heatmap and footmap visualizations to describe spatially pooled trajectory patterns with respect to time and location. We also present two approaches for anomaly prediction at the day-level granularity: a trajectory-based statistical approach, and a time-series based approach. Experimentation with one year data from a single camera demonstrates the ability to uncover interesting insights about the scene and to predict anomalies reasonably well.



### On the Diversity of Realistic Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1712.07329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07329v1)
- **Published**: 2017-12-20 05:52:52+00:00
- **Updated**: 2017-12-20 05:52:52+00:00
- **Authors**: Zichen Yang, Haifeng Liu, Deng Cai
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: Many image processing tasks can be formulated as translating images between two image domains, such as colorization, super resolution and conditional image synthesis. In most of these tasks, an input image may correspond to multiple outputs. However, current existing approaches only show very minor diversity of the outputs. In this paper, we present a novel approach to synthesize diverse realistic images corresponding to a semantic layout. We introduce a diversity loss objective, which maximizes the distance between synthesized image pairs and links the input noise to the semantic segments in the synthesized images. Thus, our approach can not only produce diverse images, but also allow users to manipulate the output images by adjusting the noise manually. Experimental results show that images synthesized by our approach are significantly more diverse than that of the current existing works and equipping our diversity loss does not degrade the reality of the base networks.



### DeepFuse: A Deep Unsupervised Approach for Exposure Fusion with Extreme Exposure Image Pairs
- **Arxiv ID**: http://arxiv.org/abs/1712.07384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07384v1)
- **Published**: 2017-12-20 09:47:51+00:00
- **Updated**: 2017-12-20 09:47:51+00:00
- **Authors**: K. Ram Prabhakar, V. Sai Srikar, R. Venkatesh Babu
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: We present a novel deep learning architecture for fusing static multi-exposure images. Current multi-exposure fusion (MEF) approaches use hand-crafted features to fuse input sequence. However, the weak hand-crafted representations are not robust to varying input conditions. Moreover, they perform poorly for extreme exposure image pairs. Thus, it is highly desirable to have a method that is robust to varying input conditions and capable of handling extreme exposure without artifacts. Deep representations have known to be robust to input conditions and have shown phenomenal performance in a supervised setting. However, the stumbling block in using deep learning for MEF was the lack of sufficient training data and an oracle to provide the ground-truth for supervision. To address the above issues, we have gathered a large dataset of multi-exposure image stacks for training and to circumvent the need for ground truth images, we propose an unsupervised deep learning framework for MEF utilizing a no-reference quality metric as loss function. The proposed approach uses a novel CNN architecture trained to learn the fusion operation without reference ground truth image. The model fuses a set of common low level features extracted from each image to generate artifact-free perceptually pleasing results. We perform extensive quantitative and qualitative evaluation and show that the proposed technique outperforms existing state-of-the-art approaches for a variety of natural images.



### Light Field Segmentation From Super-pixel Graph Representation
- **Arxiv ID**: http://arxiv.org/abs/1712.07394v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1712.07394v1)
- **Published**: 2017-12-20 10:20:12+00:00
- **Updated**: 2017-12-20 10:20:12+00:00
- **Authors**: Xianqiang Lv, Hao Zhu, Qing Wang
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: Efficient and accurate segmentation of light field is an important task in computer vision and graphics. The large volume of input data and the redundancy of light field make it an open challenge. In the paper, we propose a novel graph representation for interactive light field segmentation based on light field super-pixel (LFSP). The LFSP not only maintains light field redundancy, but also greatly reduces the graph size. These advantages make LFSP useful to improve segmentation efficiency. Based on LFSP graph structure, we present an efficient light field segmentation algorithm using graph-cuts. Experimental results on both synthetic and real dataset demonstrate that our method is superior to previous light field segmentation algorithms with respect to accuracy and efficiency.



### Finding Competitive Network Architectures Within a Day Using UCT
- **Arxiv ID**: http://arxiv.org/abs/1712.07420v2
- **DOI**: 10.1109/DSAA.2018.00037
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.07420v2)
- **Published**: 2017-12-20 11:24:50+00:00
- **Updated**: 2018-07-23 13:57:50+00:00
- **Authors**: Martin Wistuba
- **Comment**: None
- **Journal**: Proceedings of the 5th IEEE International Conference on Data
  Science and Advanced Analytics, pages 263-272, 2018
- **Summary**: The design of neural network architectures for a new data set is a laborious task which requires human deep learning expertise. In order to make deep learning available for a broader audience, automated methods for finding a neural network architecture are vital. Recently proposed methods can already achieve human expert level performances. However, these methods have run times of months or even years of GPU computing time, ignoring hardware constraints as faced by many researchers and companies. We propose the use of Monte Carlo planning in combination with two different UCT (upper confidence bound applied to trees) derivations to search for network architectures. We adapt the UCT algorithm to the needs of network architecture search by proposing two ways of sharing information between different branches of the search tree. In an empirical study we are able to demonstrate that this method is able to find competitive networks for MNIST, SVHN and CIFAR-10 in just a single GPU day. Extending the search time to five GPU days, we are able to outperform human architectures and our competitors which consider the same types of layers.



### Incremental Adversarial Domain Adaptation for Continually Changing Environments
- **Arxiv ID**: http://arxiv.org/abs/1712.07436v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1712.07436v2)
- **Published**: 2017-12-20 12:08:04+00:00
- **Updated**: 2018-02-24 16:05:02+00:00
- **Authors**: Markus Wulfmeier, Alex Bewley, Ingmar Posner
- **Comment**: International Conference on Robotics and Automation 2018
- **Journal**: None
- **Summary**: Continuous appearance shifts such as changes in weather and lighting conditions can impact the performance of deployed machine learning models. While unsupervised domain adaptation aims to address this challenge, current approaches do not utilise the continuity of the occurring shifts. In particular, many robotics applications exhibit these conditions and thus facilitate the potential to incrementally adapt a learnt model over minor shifts which integrate to massive differences over time. Our work presents an adversarial approach for lifelong, incremental domain adaptation which benefits from unsupervised alignment to a series of intermediate domains which successively diverge from the labelled source domain. We empirically demonstrate that our incremental approach improves handling of large appearance changes, e.g. day to night, on a traversable-path segmentation task compared with a direct, single alignment step approach. Furthermore, by approximating the feature distribution for the source domain with a generative adversarial network, the deployment module can be rendered fully independent of retaining potentially large amounts of the related source training data for only a minor reduction in performance.



### Human Action Recognition: Pose-based Attention draws focus to Hands
- **Arxiv ID**: http://arxiv.org/abs/1712.08002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08002v1)
- **Published**: 2017-12-20 12:58:46+00:00
- **Updated**: 2017-12-20 12:58:46+00:00
- **Authors**: Fabien Baradel, Christian Wolf, Julien Mille
- **Comment**: ICCV 2017 Workshop "Hands in action". arXiv admin note: text overlap
  with arXiv:1703.10106
- **Journal**: ICCV 2017
- **Summary**: We propose a new spatio-temporal attention based mechanism for human action recognition able to automatically attend to the hands most involved into the studied action and detect the most discriminative moments in an action. Attention is handled in a recurrent manner employing Recurrent Neural Network (RNN) and is fully-differentiable. In contrast to standard soft-attention based mechanisms, our approach does not use the hidden RNN state as input to the attention model. Instead, attention distributions are extracted using external information: human articulated pose. We performed an extensive ablation study to show the strengths of this approach and we particularly studied the conditioning aspect of the attention mechanism. We evaluate the method on the largest currently available human action recognition dataset, NTU-RGB+D, and report state-of-the-art results. Other advantages of our model are certain aspects of explanability, as the spatial and temporal attention distributions at test time allow to study and verify on which parts of the input data the method focuses.



### Recurrent Attentional Reinforcement Learning for Multi-label Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.07465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07465v1)
- **Published**: 2017-12-20 13:14:46+00:00
- **Updated**: 2017-12-20 13:14:46+00:00
- **Authors**: Tianshui Chen, Zhouxia Wang, Guanbin Li, Liang Lin
- **Comment**: Accepted at AAAI 2018
- **Journal**: None
- **Summary**: Recognizing multiple labels of images is a fundamental but challenging task in computer vision, and remarkable progress has been attained by localizing semantic-aware image regions and predicting their labels with deep convolutional neural networks. The step of hypothesis regions (region proposals) localization in these existing multi-label image recognition pipelines, however, usually takes redundant computation cost, e.g., generating hundreds of meaningless proposals with non-discriminative information and extracting their features, and the spatial contextual dependency modeling among the localized regions are often ignored or over-simplified. To resolve these issues, this paper proposes a recurrent attention reinforcement learning framework to iteratively discover a sequence of attentional and informative regions that are related to different semantic objects and further predict label scores conditioned on these regions. Besides, our method explicitly models long-term dependencies among these attentional regions that help to capture semantic label co-occurrence and thus facilitate multi-label recognition. Extensive experiments and comparisons on two large-scale benchmarks (i.e., PASCAL VOC and MS-COCO) show that our model achieves superior performance over existing state-of-the-art methods in both performance and efficiency as well as explicitly identifying image-level semantic labels to specific object regions.



### Accurate 3D Reconstruction of Dynamic Scenes from Monocular Image Sequences with Severe Occlusions
- **Arxiv ID**: http://arxiv.org/abs/1712.07472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07472v1)
- **Published**: 2017-12-20 13:27:39+00:00
- **Updated**: 2017-12-20 13:27:39+00:00
- **Authors**: Vladislav Golyanik, Torben Fetzer, Didier Stricker
- **Comment**: None
- **Journal**: None
- **Summary**: The paper introduces an accurate solution to dense orthographic Non-Rigid Structure from Motion (NRSfM) in scenarios with severe occlusions or, likewise, inaccurate correspondences. We integrate a shape prior term into variational optimisation framework. It allows to penalize irregularities of the time-varying structure on the per-pixel level if correspondence quality indicator such as an occlusion tensor is available. We make a realistic assumption that several non-occluded views of the scene are sufficient to estimate an initial shape prior, though the entire observed scene may exhibit non-rigid deformations. Experiments on synthetic and real image data show that the proposed framework significantly outperforms state of the art methods for correspondence establishment in combination with the state of the art NRSfM methods. Together with the profound insights into optimisation methods, implementation details for heterogeneous platforms are provided.



### Attribute CNNs for Word Spotting in Handwritten Documents
- **Arxiv ID**: http://arxiv.org/abs/1712.07487v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07487v1)
- **Published**: 2017-12-20 14:11:27+00:00
- **Updated**: 2017-12-20 14:11:27+00:00
- **Authors**: Sebastian Sudholt, Gernot Fink
- **Comment**: under review at IJDAR
- **Journal**: None
- **Summary**: Word spotting has become a field of strong research interest in document image analysis over the last years. Recently, AttributeSVMs were proposed which predict a binary attribute representation. At their time, this influential method defined the state-of-the-art in segmentation-based word spotting. In this work, we present an approach for learning attribute representations with Convolutional Neural Networks (CNNs). By taking a probabilistic perspective on training CNNs, we derive two different loss functions for binary and real-valued word string embeddings. In addition, we propose two different CNN architectures, specifically designed for word spotting. These architectures are able to be trained in an end-to-end fashion. In a number of experiments, we investigate the influence of different word string embeddings and optimization strategies. We show our Attribute CNNs to achieve state-of-the-art results for segmentation-based word spotting on a large variety of data sets.



### Partial Labeled Gastric Tumor Segmentation via patch-based Reiterative Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.07488v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07488v1)
- **Published**: 2017-12-20 14:12:13+00:00
- **Updated**: 2017-12-20 14:12:13+00:00
- **Authors**: Yang Nan, Gianmarc Coppola, Qiaokang Liang, Kunglin Zou, Wei Sun, Dan Zhang, Yaonan Wang, Guanzhen Yu
- **Comment**: 16 pages,9 figures
- **Journal**: None
- **Summary**: Gastric cancer is the second leading cause of cancer-related deaths worldwide, and the major hurdle in biomedical image analysis is the determination of the cancer extent. This assignment has high clinical relevance and would generally require vast microscopic assessment by pathologists. Recent advances in deep learning have produced inspiring results on biomedical image segmentation, while its outcome is reliant on comprehensive annotation. This requires plenty of labor costs, for the ground truth must be annotated meticulously by pathologists. In this paper, a reiterative learning framework was presented to train our network on partial annotated biomedical images, and superior performance was achieved without any pre-trained or further manual annotation. We eliminate the boundary error of patch-based model through our overlapped region forecast algorithm. Through these advisable methods, a mean intersection over union coefficient (IOU) of 0.883 and mean accuracy of 91.09% on the partial labeled dataset was achieved, which made us win the 2017 China Big Data & Artificial Intelligence Innovation and Entrepreneurship Competitions.



### Learning a Wavelet-like Auto-Encoder to Accelerate Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.07493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07493v1)
- **Published**: 2017-12-20 14:24:00+00:00
- **Updated**: 2017-12-20 14:24:00+00:00
- **Authors**: Tianshui Chen, Liang Lin, Wangmeng Zuo, Xiaonan Luo, Lei Zhang
- **Comment**: Accepted at AAAI 2018
- **Journal**: None
- **Summary**: Accelerating deep neural networks (DNNs) has been attracting increasing attention as it can benefit a wide range of applications, e.g., enabling mobile systems with limited computing resources to own powerful visual recognition ability. A practical strategy to this goal usually relies on a two-stage process: operating on the trained DNNs (e.g., approximating the convolutional filters with tensor decomposition) and fine-tuning the amended network, leading to difficulty in balancing the trade-off between acceleration and maintaining recognition performance. In this work, aiming at a general and comprehensive way for neural network acceleration, we develop a Wavelet-like Auto-Encoder (WAE) that decomposes the original input image into two low-resolution channels (sub-images) and incorporate the WAE into the classification neural networks for joint training. The two decomposed channels, in particular, are encoded to carry the low-frequency information (e.g., image profiles) and high-frequency (e.g., image details or noises), respectively, and enable reconstructing the original input image through the decoding process. Then, we feed the low-frequency channel into a standard classification network such as VGG or ResNet and employ a very lightweight network to fuse with the high-frequency channel to obtain the classification result. Compared to existing DNN acceleration solutions, our framework has the following advantages: i) it is tolerant to any existing convolutional neural networks for classification without amending their structures; ii) the WAE provides an interpretable way to preserve the main components of the input image for classification.



### Learning to Act Properly: Predicting and Explaining Affordances from Images
- **Arxiv ID**: http://arxiv.org/abs/1712.07576v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07576v2)
- **Published**: 2017-12-20 16:54:09+00:00
- **Updated**: 2018-06-15 05:26:46+00:00
- **Authors**: Ching-Yao Chuang, Jiaman Li, Antonio Torralba, Sanja Fidler
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of affordance reasoning in diverse scenes that appear in the real world. Affordances relate the agent's actions to their effects when taken on the surrounding objects. In our work, we take the egocentric view of the scene, and aim to reason about action-object affordances that respect both the physical world as well as the social norms imposed by the society. We also aim to teach artificial agents why some actions should not be taken in certain situations, and what would likely happen if these actions would be taken. We collect a new dataset that builds upon ADE20k, referred to as ADE-Affordance, which contains annotations enabling such rich visual reasoning. We propose a model that exploits Graph Neural Networks to propagate contextual information from the scene in order to perform detailed affordance reasoning about each object. Our model is showcased through various ablation studies, pointing to successes and challenges in this complex task.



### SuperPoint: Self-Supervised Interest Point Detection and Description
- **Arxiv ID**: http://arxiv.org/abs/1712.07629v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07629v4)
- **Published**: 2017-12-20 18:38:35+00:00
- **Updated**: 2018-04-19 15:59:15+00:00
- **Authors**: Daniel DeTone, Tomasz Malisiewicz, Andrew Rabinovich
- **Comment**: Camera-ready version for CVPR 2018 Deep Learning for Visual SLAM
  Workshop (DL4VSLAM2018)
- **Journal**: None
- **Summary**: This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a large number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our fully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and associated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multi-homography approach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on the MS-COCO generic image dataset using Homographic Adaptation, is able to repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other traditional corner detector. The final system gives rise to state-of-the-art homography estimation results on HPatches when compared to LIFT, SIFT and ORB.



### Deep Learning with Lung Segmentation and Bone Shadow Exclusion Techniques for Chest X-Ray Analysis of Lung Cancer
- **Arxiv ID**: http://arxiv.org/abs/1712.07632v1
- **DOI**: 10.1007/978-3-319-91008-6_63
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.07632v1)
- **Published**: 2017-12-20 18:40:49+00:00
- **Updated**: 2017-12-20 18:40:49+00:00
- **Authors**: Yu. Gordienko, Peng Gang, Jiang Hui, Wei Zeng, Yu. Kochura, O. Alienin, O. Rokovyi, S. Stirenko
- **Comment**: 10 pages, 7 figures; The First International Conference on Computer
  Science, Engineering and Education Applications (ICCSEEA2018)
  (www.uacnconf.org/iccseea2018) (accepted)
- **Journal**: In: Hu Z., Petoukhov S., Dychka I., He M. (eds) Advances in
  Computer Science for Engineering and Education. ICCSEEA 2018. Advances in
  Intelligent Systems and Computing, vol 754, p. 638-647. Springer, Cham
- **Summary**: The recent progress of computing, machine learning, and especially deep learning, for image recognition brings a meaningful effect for automatic detection of various diseases from chest X-ray images (CXRs). Here efficiency of lung segmentation and bone shadow exclusion techniques is demonstrated for analysis of 2D CXRs by deep learning approach to help radiologists identify suspicious lesions and nodules in lung cancer patients. Training and validation was performed on the original JSRT dataset (dataset #01), BSE-JSRT dataset, i.e. the same JSRT dataset, but without clavicle and rib shadows (dataset #02), original JSRT dataset after segmentation (dataset #03), and BSE-JSRT dataset after segmentation (dataset #04). The results demonstrate the high efficiency and usefulness of the considered pre-processing techniques in the simplified configuration even. The pre-processed dataset without bones (dataset #02) demonstrates the much better accuracy and loss results in comparison to the other pre-processed datasets after lung segmentation (datasets #02 and #03).



### Image Segmentation to Distinguish Between Overlapping Human Chromosomes
- **Arxiv ID**: http://arxiv.org/abs/1712.07639v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.07639v1)
- **Published**: 2017-12-20 18:48:41+00:00
- **Updated**: 2017-12-20 18:48:41+00:00
- **Authors**: R. Lily Hu, Jeremy Karnowski, Ross Fadely, Jean-Patrick Pommier
- **Comment**: Presented at NIPS 2017 Machine Learning for Health
- **Journal**: None
- **Summary**: In medicine, visualizing chromosomes is important for medical diagnostics, drug development, and biomedical research. Unfortunately, chromosomes often overlap and it is necessary to identify and distinguish between the overlapping chromosomes. A segmentation solution that is fast and automated will enable scaling of cost effective medicine and biomedical research. We apply neural network-based image segmentation to the problem of distinguishing between partially overlapping DNA chromosomes. A convolutional neural network is customized for this problem. The results achieved intersection over union (IOU) scores of 94.7% for the overlapping region and 88-94% on the non-overlapping chromosome regions.



### Sim2Real View Invariant Visual Servoing by Recurrent Control
- **Arxiv ID**: http://arxiv.org/abs/1712.07642v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1712.07642v1)
- **Published**: 2017-12-20 18:54:29+00:00
- **Updated**: 2017-12-20 18:54:29+00:00
- **Authors**: Fereshteh Sadeghi, Alexander Toshev, Eric Jang, Sergey Levine
- **Comment**: Supplementary video:
  https://fsadeghi.github.io/Sim2RealViewInvariantServo
- **Journal**: None
- **Summary**: Humans are remarkably proficient at controlling their limbs and tools from a wide range of viewpoints and angles, even in the presence of optical distortions. In robotics, this ability is referred to as visual servoing: moving a tool or end-point to a desired location using primarily visual feedback. In this paper, we study how viewpoint-invariant visual servoing skills can be learned automatically in a robotic manipulation scenario. To this end, we train a deep recurrent controller that can automatically determine which actions move the end-point of a robotic arm to a desired object. The problem that must be solved by this controller is fundamentally ambiguous: under severe variation in viewpoint, it may be impossible to determine the actions in a single feedforward operation. Instead, our visual servoing system must use its memory of past movements to understand how the actions affect the robot motion from the current viewpoint, correcting mistakes and gradually moving closer to the target. This ability is in stark contrast to most visual servoing methods, which either assume known dynamics or require a calibration phase. We show how we can learn this recurrent controller using simulated data and a reinforcement learning objective. We then describe how the resulting model can be transferred to a real-world robot by disentangling perception from control and only adapting the visual layers. The adapted model can servo to previously unseen objects from novel viewpoints on a real-world Kuka IIWA robotic arm. For supplementary videos, see: https://fsadeghi.github.io/Sim2RealViewInvariantServo



### Adversarial Synthesis Learning Enables Segmentation Without Target Modality Ground Truth
- **Arxiv ID**: http://arxiv.org/abs/1712.07695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07695v1)
- **Published**: 2017-12-20 20:22:01+00:00
- **Updated**: 2017-12-20 20:22:01+00:00
- **Authors**: Yuankai Huo, Zhoubing Xu, Shunxing Bao, Albert Assad, Richard G. Abramson, Bennett A. Landman
- **Comment**: IEEE International Symposium on Biomedical Imaging (ISBI) 2018
- **Journal**: None
- **Summary**: A lack of generalizability is one key limitation of deep learning based segmentation. Typically, one manually labels new training images when segmenting organs in different imaging modalities or segmenting abnormal organs from distinct disease cohorts. The manual efforts can be alleviated if one is able to reuse manual labels from one modality (e.g., MRI) to train a segmentation network for a new modality (e.g., CT). Previously, two stage methods have been proposed to use cycle generative adversarial networks (CycleGAN) to synthesize training images for a target modality. Then, these efforts trained a segmentation network independently using synthetic images. However, these two independent stages did not use the complementary information between synthesis and segmentation. Herein, we proposed a novel end-to-end synthesis and segmentation network (EssNet) to achieve the unpaired MRI to CT image synthesis and CT splenomegaly segmentation simultaneously without using manual labels on CT. The end-to-end EssNet achieved significantly higher median Dice similarity coefficient (0.9188) than the two stages strategy (0.8801), and even higher than canonical multi-atlas segmentation (0.9125) and ResNet method (0.9107), which used the CT manual labels.



### An Order Preserving Bilinear Model for Person Detection in Multi-Modal Data
- **Arxiv ID**: http://arxiv.org/abs/1712.07721v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07721v2)
- **Published**: 2017-12-20 21:44:35+00:00
- **Updated**: 2018-01-11 21:24:27+00:00
- **Authors**: Oytun Ulutan, Benjamin S. Riggan, Nasser M. Nasrabadi, B. S. Manjunath
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new order preserving bilinear framework that exploits low-resolution video for person detection in a multi-modal setting using deep neural networks. In this setting cameras are strategically placed such that less robust sensors, e.g. geophones that monitor seismic activity, are located within the field of views (FOVs) of cameras. The primary challenge is being able to leverage sufficient information from videos where there are less than 40 pixels on targets, while also taking advantage of less discriminative information from other modalities, e.g. seismic. Unlike state-of-the-art methods, our bilinear framework retains spatio-temporal order when computing the vector outer products between pairs of features. Despite the high dimensionality of these outer products, we demonstrate that our order preserving bilinear framework yields better performance than recent orderless bilinear models and alternative fusion methods.



### Enhance Visual Recognition under Adverse Conditions via Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.07732v2
- **DOI**: 10.1109/TIP.2019.2908802
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.07732v2)
- **Published**: 2017-12-20 22:19:12+00:00
- **Updated**: 2019-04-02 23:46:02+00:00
- **Authors**: Ding Liu, Bowen Cheng, Zhangyang Wang, Haichao Zhang, Thomas S. Huang
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing 2019
- **Summary**: Visual recognition under adverse conditions is a very important and challenging problem of high practical value, due to the ubiquitous existence of quality distortions during image acquisition, transmission, or storage. While deep neural networks have been extensively exploited in the techniques of low-quality image restoration and high-quality image recognition tasks respectively, few studies have been done on the important problem of recognition from very low-quality images. This paper proposes a deep learning based framework for improving the performance of image and video recognition models under adverse conditions, using robust adverse pre-training or its aggressive variant. The robust adverse pre-training algorithms leverage the power of pre-training and generalizes conventional unsupervised pre-training and data augmentation methods. We further develop a transfer learning approach to cope with real-world datasets of unknown adverse conditions. The proposed framework is comprehensively evaluated on a number of image and video recognition benchmarks, and obtains significant performance improvements under various single or mixed adverse conditions. Our visualization and analysis further add to the explainability of results.



