# Arxiv Papers in cs.CV on 2017-12-18
### Visual Explanations from Hadamard Product in Multimodal Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.06228v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.06228v1)
- **Published**: 2017-12-18 02:37:20+00:00
- **Updated**: 2017-12-18 02:37:20+00:00
- **Authors**: Jin-Hwa Kim, Byoung-Tak Zhang
- **Comment**: 8 pages, 5 figures, including appendix, NIPS 2017 Workshop on
  Visually-Grounded Interaction and Language (ViGIL)
- **Journal**: None
- **Summary**: The visual explanation of learned representation of models helps to understand the fundamentals of learning. The attentional models of previous works used to visualize the attended regions over an image or text using their learned weights to confirm their intended mechanism. Kim et al. (2016) show that the Hadamard product in multimodal deep networks, which is well-known for the joint function of visual question answering tasks, implicitly performs an attentional mechanism for visual inputs. In this work, we extend their work to show that the Hadamard product in multimodal deep networks performs not only for visual inputs but also for textual inputs simultaneously using the proposed gradient-based visualization technique. The attentional effect of Hadamard product is visualized for both visual and textual inputs by analyzing the two inputs and an output of the Hadamard product with the proposed method and compared with learned attentional weights of a visual question answering model.



### Panoramic Robust PCA for Foreground-Background Separation on Noisy, Free-Motion Camera Video
- **Arxiv ID**: http://arxiv.org/abs/1712.06229v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.06229v3)
- **Published**: 2017-12-18 02:45:54+00:00
- **Updated**: 2019-01-04 03:42:51+00:00
- **Authors**: Brian E. Moore, Chen Gao, Raj Rao Nadakuditi
- **Comment**: IEEE TCI. Project webpage: https://gaochen315.github.io/pRPCA/ Code:
  https://github.com/gaochen315/panoramicRPCA
- **Journal**: None
- **Summary**: This work presents a new robust PCA method for foreground-background separation on freely moving camera video with possible dense and sparse corruptions. Our proposed method registers the frames of the corrupted video and then encodes the varying perspective arising from camera motion as missing data in a global model. This formulation allows our algorithm to produce a panoramic background component that automatically stitches together corrupted data from partially overlapping frames to reconstruct the full field of view. We model the registered video as the sum of a low-rank component that captures the background, a smooth component that captures the dynamic foreground of the scene, and a sparse component that isolates possible outliers and other sparse corruptions in the video. The low-rank portion of our model is based on a recent low-rank matrix estimator (OptShrink) that has been shown to yield superior low-rank subspace estimates in practice. To estimate the smooth foreground component of our model, we use a weighted total variation framework that enables our method to reliably decouple the true foreground of the video from sparse corruptions. We perform extensive numerical experiments on both static and moving camera video subject to a variety of dense and sparse corruptions. Our experiments demonstrate the state-of-the-art performance of our proposed method compared to existing methods both in terms of foreground and background estimation accuracy.



### Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.06302v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.06302v3)
- **Published**: 2017-12-18 09:17:44+00:00
- **Updated**: 2019-03-08 12:11:15+00:00
- **Authors**: Jose Oramas, Kaili Wang, Tinne Tuytelaars
- **Comment**: Accepted at International Conference on Learning Representations
  (ICLR) 2019. Project website:
  http://homes.esat.kuleuven.be/~joramas/projects/visualExplanationByInterpretation
- **Journal**: None
- **Summary**: Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them. In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations. We interpret the model through average visualizations of this reduced set of features. Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features. In addition, we propose a method to address the artifacts introduced by stridded operations in deconvNet-based visualizations. Moreover, we introduce an8Flower, a dataset specifically designed for objective quantitative evaluation of methods for visual explanation.Experiments on the MNIST,ILSVRC12,Fashion144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest



### LSTM Pose Machines
- **Arxiv ID**: http://arxiv.org/abs/1712.06316v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06316v4)
- **Published**: 2017-12-18 09:56:45+00:00
- **Updated**: 2018-03-09 07:44:38+00:00
- **Authors**: Yue Luo, Jimmy Ren, Zhouxia Wang, Wenxiu Sun, Jinshan Pan, Jianbo Liu, Jiahao Pang, Liang Lin
- **Comment**: Poster in IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2018
- **Journal**: None
- **Summary**: We observed that recent state-of-the-art results on single image human pose estimation were achieved by multi-stage Convolution Neural Networks (CNN). Notwithstanding the superior performance on static images, the application of these models on videos is not only computationally intensive, it also suffers from performance degeneration and flicking. Such suboptimal results are mainly attributed to the inability of imposing sequential geometric consistency, handling severe image quality degradation (e.g. motion blur and occlusion) as well as the inability of capturing the temporal correlation among video frames. In this paper, we proposed a novel recurrent network to tackle these problems. We showed that if we were to impose the weight sharing scheme to the multi-stage CNN, it could be re-written as a Recurrent Neural Network (RNN). This property decouples the relationship among multiple network stages and results in significantly faster speed in invoking the network for videos. It also enables the adoption of Long Short-Term Memory (LSTM) units between video frames. We found such memory augmented RNN is very effective in imposing geometric consistency among frames. It also well handles input quality degradation in videos while successfully stabilizes the sequential outputs. The experiments showed that our approach significantly outperformed current state-of-the-art methods on two large-scale video pose estimation benchmarks. We also explored the memory cells inside the LSTM and provided insights on why such mechanism would benefit the prediction for video-based pose estimations.



### Video Object Detection with an Aligned Spatial-Temporal Memory
- **Arxiv ID**: http://arxiv.org/abs/1712.06317v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06317v3)
- **Published**: 2017-12-18 10:02:23+00:00
- **Updated**: 2018-07-27 00:47:46+00:00
- **Authors**: Fanyi Xiao, Yong Jae Lee
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Spatial-Temporal Memory Networks for video object detection. At its core, a novel Spatial-Temporal Memory module (STMM) serves as the recurrent computation unit to model long-term temporal appearance and motion dynamics. The STMM's design enables full integration of pretrained backbone CNN weights, which we find to be critical for accurate detection. Furthermore, in order to tackle object motion in videos, we propose a novel MatchTrans module to align the spatial-temporal memory from frame to frame. Our method produces state-of-the-art results on the benchmark ImageNet VID dataset, and our ablative studies clearly demonstrate the contribution of our different design choices. We release our code and models at http://fanyix.cs.ucdavis.edu/project/stmn/project.html.



### Space-Filling Curve Indices as Acceleration Structure for Exemplar-Based Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1712.06326v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06326v2)
- **Published**: 2017-12-18 10:28:36+00:00
- **Updated**: 2020-01-22 08:57:58+00:00
- **Authors**: Tim Dahmen, Patrick Trampert, Pascal Peter, Pinak Bheed, Joachim Weickert, Philipp Slusallek
- **Comment**: submitted to Signal Processing: Image Communication
- **Journal**: None
- **Summary**: Exemplar-based inpainting is the process of reconstructing missing parts of an image by searching the remaining data for patches that fit seamlessly. The image is completed to a plausible-looking solution by repeatedly inserting the patch that is the best match according to some cost function. We present an acceleration structure that uses a multi-index scheme to accelerate this search procedure drastically, particularly in the case of very large datasets. The index scheme uses ideas such as dimensionality reduction and k-nearest neighbor search on space-filling curves that are well known in the field of multimedia databases. Our method has a theoretic runtime of O(log2 n) per iteration and reaches a speedup factor of up to 660 over the original method. The approach has the advantage of being agnostic to most modelbased parts of exemplar-based inpainting such as the order in which patches are processed and the cost function used to determine patch similarity. Thus, the acceleration structure can be used in conjunction with most exemplar-based inpainting algorithms.



### On the Effectiveness of Least Squares Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.06391v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.06391v2)
- **Published**: 2017-12-18 13:36:09+00:00
- **Updated**: 2018-09-21 07:48:53+00:00
- **Authors**: Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, Zhen Wang, Stephen Paul Smolley
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised learning with generative adversarial networks (GANs) has proven to be hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss for both the discriminator and the generator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\chi^2$ divergence. We also show that the derived objective function that yields minimizing the Pearson $\chi^2$ divergence performs better than the classical one of using least squares for classification. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stably during the learning process. For evaluating the image quality, we conduct both qualitative and quantitative experiments, and the experimental results show that LSGANs can generate higher quality images than regular GANs. Furthermore, we evaluate the stability of LSGANs in two groups. One is to compare between LSGANs and regular GANs without gradient penalty. We conduct three experiments, including Gaussian mixture distribution, difficult architectures, and a newly proposed method --- datasets with small variability, to illustrate the stability of LSGANs. The other one is to compare between LSGANs with gradient penalty (LSGANs-GP) and WGANs with gradient penalty (WGANs-GP). The experimental results show that LSGANs-GP succeed in training for all the difficult architectures used in WGANs-GP, including 101-layer ResNet.



### Automatic Classification of Functional Gait Disorders
- **Arxiv ID**: http://arxiv.org/abs/1712.06405v2
- **DOI**: 10.1109/JBHI.2017.2785682
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06405v2)
- **Published**: 2017-12-18 14:09:01+00:00
- **Updated**: 2017-12-24 10:00:52+00:00
- **Authors**: Djordje Slijepcevic, Matthias Zeppelzauer, Anna-Maria Gorgas, Caterine Schwab, Michael Schüller, Arnold Baca, Christian Breiteneder, Brian Horsak
- **Comment**: 9 pages, 3 figures, IEEE Journal of Biomedical and Health Informatics
- **Journal**: None
- **Summary**: This article proposes a comprehensive investigation of the automatic classification of functional gait disorders based solely on ground reaction force (GRF) measurements. The aim of the study is twofold: (1) to investigate the suitability of stateof-the-art GRF parameterization techniques (representations) for the discrimination of functional gait disorders; and (2) to provide a first performance baseline for the automated classification of functional gait disorders for a large-scale dataset. The utilized database comprises GRF measurements from 279 patients with gait disorders (GDs) and data from 161 healthy controls (N). Patients were manually classified into four classes with different functional impairments associated with the "hip", "knee", "ankle", and "calcaneus". Different parameterizations are investigated: GRF parameters, global principal component analysis (PCA)-based representations and a combined representation applying PCA on GRF parameters. The discriminative power of each parameterization for different classes is investigated by linear discriminant analysis (LDA). Based on this analysis, two classification experiments are pursued: (1) distinction between healthy and impaired gait (N vs. GD) and (2) multi-class classification between healthy gait and all four GD classes. Experiments show promising results and reveal among others that several factors, such as imbalanced class cardinalities and varying numbers of measurement sessions per patient have a strong impact on the classification accuracy and therefore need to be taken into account. The results represent a promising first step towards the automated classification of gait disorders and a first performance baseline for future developments in this direction.



### Automatic segmentation method of pelvic floor levator hiatus in ultrasound using a self-normalising neural network
- **Arxiv ID**: http://arxiv.org/abs/1712.06452v1
- **DOI**: 10.1117/1.JMI.5.2.021206
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06452v1)
- **Published**: 2017-12-18 15:04:21+00:00
- **Updated**: 2017-12-18 15:04:21+00:00
- **Authors**: Ester Bonmati, Yipeng Hu, Nikhil Sindhwani, Hans Peter Dietz, Jan D'hooge, Dean Barratt, Jan Deprest, Tom Vercauteren
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation of the levator hiatus in ultrasound allows to extract biometrics which are of importance for pelvic floor disorder assessment. In this work, we present a fully automatic method using a convolutional neural network (CNN) to outline the levator hiatus in a 2D image extracted from a 3D ultrasound volume. In particular, our method uses a recently developed scaled exponential linear unit (SELU) as a nonlinear self-normalising activation function, which for the first time has been applied in medical imaging with CNN. SELU has important advantages such as being parameter-free and mini-batch independent, which may help to overcome memory constraints during training. A dataset with 91 images from 35 patients during Valsalva, contraction and rest, all labelled by three operators, is used for training and evaluation in a leave-one-patient-out cross-validation. Results show a median Dice similarity coefficient of 0.90 with an interquartile range of 0.08, with equivalent performance to the three operators (with a Williams' index of 1.03), and outperforming a U-Net architecture without the need for batch normalisation. We conclude that the proposed fully automatic method achieved equivalent accuracy in segmenting the pelvic floor levator hiatus compared to a previous semi-automatic approach.



### Super-Resolution with Deep Adaptive Image Resampling
- **Arxiv ID**: http://arxiv.org/abs/1712.06463v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06463v1)
- **Published**: 2017-12-18 15:19:14+00:00
- **Updated**: 2017-12-18 15:19:14+00:00
- **Authors**: Xu Jia, Hong Chang, Tinne Tuytelaars
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning based methods have recently pushed the state-of-the-art on the problem of Single Image Super-Resolution (SISR). In this work, we revisit the more traditional interpolation-based methods, that were popular before, now with the help of deep learning. In particular, we propose to use a Convolutional Neural Network (CNN) to estimate spatially variant interpolation kernels and apply the estimated kernels adaptively to each position in the image. The whole model is trained in an end-to-end manner. We explore two ways to improve the results for the case of large upscaling factors, and propose a recursive extension of our basic model. This achieves results that are on par with state-of-the-art methods. We visualize the estimated adaptive interpolation kernels to gain more insight on the effectiveness of the proposed method. We also extend the method to the task of joint image filtering and again achieve state-of-the-art performance.



### Multi-modal Face Pose Estimation with Multi-task Manifold Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.06467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06467v1)
- **Published**: 2017-12-18 15:22:26+00:00
- **Updated**: 2017-12-18 15:22:26+00:00
- **Authors**: Chaoqun Hong, Jun Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Human face pose estimation aims at estimating the gazing direction or head postures with 2D images. It gives some very important information such as communicative gestures, saliency detection and so on, which attracts plenty of attention recently. However, it is challenging because of complex background, various orientations and face appearance visibility. Therefore, a descriptive representation of face images and mapping it to poses are critical. In this paper, we make use of multi-modal data and propose a novel face pose estimation method that uses a novel deep learning framework named Multi-task Manifold Deep Learning $M^2DL$. It is based on feature extraction with improved deep neural networks and multi-modal mapping relationship with multi-task learning. In the proposed deep learning based framework, Manifold Regularized Convolutional Layers (MRCL) improve traditional convolutional layers by learning the relationship among outputs of neurons. Besides, in the proposed mapping relationship learning method, different modals of face representations are naturally combined to learn the mapping function from face images to poses. In this way, the computed mapping model with multiple tasks is improved. Experimental results on three challenging benchmark datasets DPOSE, HPID and BKHPD demonstrate the outstanding performance of $M^2DL$.



### Guiding human gaze with convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1712.06492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06492v1)
- **Published**: 2017-12-18 16:09:37+00:00
- **Updated**: 2017-12-18 16:09:37+00:00
- **Authors**: Leon A. Gatys, Matthias Kümmerer, Thomas S. A. Wallis, Matthias Bethge
- **Comment**: None
- **Journal**: None
- **Summary**: The eye fixation patterns of human observers are a fundamental indicator of the aspects of an image to which humans attend. Thus, manipulating fixation patterns to guide human attention is an exciting challenge in digital image processing. Here, we present a new model for manipulating images to change the distribution of human fixations in a controlled fashion. We use the state-of-the-art model for fixation prediction to train a convolutional neural network to transform images so that they satisfy a given fixation distribution. For network training, we carefully design a loss function to achieve a perceptual effect while preserving naturalness of the transformed images. Finally, we evaluate the success of our model by measuring human fixations for a set of manipulated images. On our test images we can in-/decrease the probability to fixate on selected objects on average by 43/22% but show that the effectiveness of the model depends on the semantic content of the manipulated images.



### IMU2Face: Real-time Gesture-driven Facial Reenactment
- **Arxiv ID**: http://arxiv.org/abs/1801.01446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01446v1)
- **Published**: 2017-12-18 16:26:45+00:00
- **Updated**: 2017-12-18 16:26:45+00:00
- **Authors**: Justus Thies, Michael Zollhöfer, Matthias Nießner
- **Comment**: https://youtu.be/UXGodiDAqiE
- **Journal**: None
- **Summary**: We present IMU2Face, a gesture-driven facial reenactment system. To this end, we combine recent advances in facial motion capture and inertial measurement units (IMUs) to control the facial expressions of a person in a target video based on intuitive hand gestures. IMUs are omnipresent, since modern smart-phones, smart-watches and drones integrate such sensors, e.g., for changing the orientation of the screen content, counting steps, or for flight stabilization. Face tracking and reenactment is based on the state-of-the-art real-time Face2Face facial reenactment system. Instead of transferring facial expressions from a source to a target actor, we employ an IMU to track the hand gestures of a source actor and use its orientation to modify the target actor's expressions.



### Dynamic Weight Alignment for Temporal Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.06530v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1712.06530v6)
- **Published**: 2017-12-18 17:16:07+00:00
- **Updated**: 2019-02-07 08:10:19+00:00
- **Authors**: Brian Kenji Iwana, Seiichi Uchida
- **Comment**: Accepted to ICASSP 2019
- **Journal**: None
- **Summary**: In this paper, we propose a method of improving temporal Convolutional Neural Networks (CNN) by determining the optimal alignment of weights and inputs using dynamic programming. Conventional CNN convolutions linearly match the shared weights to a window of the input. However, it is possible that there exists a more optimal alignment of weights. Thus, we propose the use of Dynamic Time Warping (DTW) to dynamically align the weights to the input of the convolutional layer. Specifically, the dynamic alignment overcomes issues such as temporal distortion by finding the minimal distance matching of the weights and the inputs under constraints. We demonstrate the effectiveness of the proposed architecture on the Unipen online handwritten digit and character datasets, the UCI Spoken Arabic Digit dataset, and the UCI Activities of Daily Life dataset.



### Multi-point Vibration Measurement for Mode Identification of Bridge Structures using Video-based Motion Magnification
- **Arxiv ID**: http://arxiv.org/abs/1712.06566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06566v1)
- **Published**: 2017-12-18 18:21:44+00:00
- **Updated**: 2017-12-18 18:21:44+00:00
- **Authors**: Zhexiong Shang, Zhigang Shen
- **Comment**: 15 pages including 9 figures and references
- **Journal**: None
- **Summary**: Image-based vibration mode identification gained increased attentions in civil and construction communities. A recent video-based motion magnification method was developed to measure and visualize small structure motions. This new approach presents a potential for low-cost vibration measurement and mode shape identification. Pilot studies using this approach on simple rigid body structures was reported. Its validity on complex outdoor structures have not been investigated. The objective is to investigate the capacity of video-based motion magnification approach in measuring the modal frequency and visualizing the mode shapes of complex steel bridges. A novel method that increases the performance of the current motion magnification for efficient structure modal analysis is introduced. This method was tested in both indoor and outdoor environments for validation. The results of the investigation show that motion magnification can be an efficient tool for modal analysis on complex bridge structures. With the developed method, mode frequencies of multiple structures are simultaneously measured and mode shapes of each structure are automatically visualized.



### End-to-end Recovery of Human Shape and Pose
- **Arxiv ID**: http://arxiv.org/abs/1712.06584v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06584v2)
- **Published**: 2017-12-18 18:57:40+00:00
- **Updated**: 2018-06-23 23:12:25+00:00
- **Authors**: Angjoo Kanazawa, Michael J. Black, David W. Jacobs, Jitendra Malik
- **Comment**: CVPR 2018, Project page with code: https://akanazawa.github.io/hmr/
- **Journal**: None
- **Summary**: We describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allow our model to be trained using images in-the-wild that only have ground truth 2D annotations. However, the reprojection loss alone leaves the model highly under constrained. In this work we address this problem by introducing an adversary trained to tell whether a human body parameter is real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any paired 2D-to-3D supervision. We do not rely on intermediate 2D keypoint detections and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person. We demonstrate our approach on various images in-the-wild and out-perform previous optimization based methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation.



### Objects that Sound
- **Arxiv ID**: http://arxiv.org/abs/1712.06651v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1712.06651v2)
- **Published**: 2017-12-18 19:52:53+00:00
- **Updated**: 2018-07-25 16:26:15+00:00
- **Authors**: Relja Arandjelović, Andrew Zisserman
- **Comment**: Appears in: European Conference on Computer Vision (ECCV) 2018
- **Journal**: None
- **Summary**: In this paper our objectives are, first, networks that can embed audio and visual inputs into a common space that is suitable for cross-modal retrieval; and second, a network that can localize the object that sounds in an image, given the audio signal. We achieve both these objectives by training from unlabelled video using only audio-visual correspondence (AVC) as the objective function. This is a form of cross-modal self-supervision from video.   To this end, we design new network architectures that can be trained for cross-modal retrieval and localizing the sound source in an image, by using the AVC task. We make the following contributions: (i) show that audio and visual embeddings can be learnt that enable both within-mode (e.g. audio-to-audio) and between-mode retrieval; (ii) explore various architectures for the AVC task, including those for the visual stream that ingest a single image, or multiple images, or a single image and multi-frame optical flow; (iii) show that the semantic object that sounds within an image can be localized (using only the sound, no motion or flow information); and (iv) give a cautionary tale on how to avoid undesirable shortcuts in the data preparation.



### DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation
- **Arxiv ID**: http://arxiv.org/abs/1712.06679v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.06679v2)
- **Published**: 2017-12-18 21:17:35+00:00
- **Updated**: 2018-03-07 04:51:35+00:00
- **Authors**: Jiang Liu, Chenqiang Gao, Deyu Meng, Alexander G. Hauptmann
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: In real-world crowd counting applications, the crowd densities vary greatly in spatial and temporal domains. A detection based counting method will estimate crowds accurately in low density scenes, while its reliability in congested areas is downgraded. A regression based approach, on the other hand, captures the general density information in crowded regions. Without knowing the location of each person, it tends to overestimate the count in low density areas. Thus, exclusively using either one of them is not sufficient to handle all kinds of scenes with varying densities. To address this issue, a novel end-to-end crowd counting framework, named DecideNet (DEteCtIon and Density Estimation Network) is proposed. It can adaptively decide the appropriate counting mode for different locations on the image based on its real density conditions. DecideNet starts with estimating the crowd density by generating detection and regression based density maps separately. To capture inevitable variation in densities, it incorporates an attention module, meant to adaptively assess the reliability of the two types of estimations. The final crowd counts are obtained with the guidance of the attention module to adopt suitable estimations from the two kinds of density maps. Experimental results show that our method achieves state-of-the-art performance on three challenging crowd counting datasets.



### Synthesizing Novel Pairs of Image and Text
- **Arxiv ID**: http://arxiv.org/abs/1712.06682v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.06682v1)
- **Published**: 2017-12-18 21:25:37+00:00
- **Updated**: 2017-12-18 21:25:37+00:00
- **Authors**: Jason Xie, Tingwen Bao
- **Comment**: None
- **Journal**: None
- **Summary**: Generating novel pairs of image and text is a problem that combines computer vision and natural language processing. In this paper, we present strategies for generating novel image and caption pairs based on existing captioning datasets. The model takes advantage of recent advances in generative adversarial networks and sequence-to-sequence modeling. We make generalizations to generate paired samples from multiple domains. Furthermore, we study cycles -- generating from image to text then back to image and vise versa, as well as its connection with autoencoders.



### Deformable Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1712.06715v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.06715v1)
- **Published**: 2017-12-18 23:12:06+00:00
- **Updated**: 2017-12-18 23:12:06+00:00
- **Authors**: Jiajun Shen, Yali Amit
- **Comment**: None
- **Journal**: None
- **Summary**: Geometric variations of objects, which do not modify the object class, pose a major challenge for object recognition. These variations could be rigid as well as non-rigid transformations. In this paper, we design a framework for training deformable classifiers, where latent transformation variables are introduced, and a transformation of the object image to a reference instantiation is computed in terms of the classifier output, separately for each class. The classifier outputs for each class, after transformation, are compared to yield the final decision. As a by-product of the classification this yields a transformation of the input object to a reference pose, which can be used for downstream tasks such as the computation of object support. We apply a two-step training mechanism for our framework, which alternates between optimizing over the latent transformation variables and the classifier parameters to minimize the loss function. We show that multilayer perceptrons, also known as deep networks, are well suited for this approach and achieve state of the art results on the rotated MNIST and the Google Earth dataset, and produce competitive results on MNIST and CIFAR-10 when training on smaller subsets of training data.



