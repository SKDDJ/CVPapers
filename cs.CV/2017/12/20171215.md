# Arxiv Papers in cs.CV on 2017-12-15
### Visual Based Navigation of Mobile Robots
- **Arxiv ID**: http://arxiv.org/abs/1712.05482v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05482v1)
- **Published**: 2017-12-15 00:08:54+00:00
- **Updated**: 2017-12-15 00:08:54+00:00
- **Authors**: Shailja, Soumabh Bhowmick, Jayanta Mukhopadhyay
- **Comment**: Bachelor Thesis, Electrical Engineering Department, IIT Kharagpur,
  2016
- **Journal**: None
- **Summary**: We have developed an algorithm to generate a complete map of the traversable region for a personal assistant robot using monocular vision only. Using multiple taken by a simple webcam, obstacle detection and avoidance algorithms have been developed. Simple Linear Iterative Clustering (SLIC) has been used for segmentation to reduce the memory and computation cost. A simple mapping technique using inverse perspective mapping and occupancy grids, which is robust, and supports very fast updates has been used to create the map for indoor navigation.



### CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication
- **Arxiv ID**: http://arxiv.org/abs/1712.05558v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.05558v3)
- **Published**: 2017-12-15 06:38:15+00:00
- **Updated**: 2019-06-04 13:01:42+00:00
- **Authors**: Jin-Hwa Kim, Nikita Kitaev, Xinlei Chen, Marcus Rohrbach, Byoung-Tak Zhang, Yuandong Tian, Dhruv Batra, Devi Parikh
- **Comment**: ACL 2019
- **Journal**: None
- **Summary**: In this work, we propose a goal-driven collaborative task that combines language, perception, and action. Specifically, we develop a Collaborative image-Drawing game between two agents, called CoDraw. Our game is grounded in a virtual world that contains movable clip art objects. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. The two players communicate with each other using natural language. We collect the CoDraw dataset of ~10K dialogs consisting of ~138K messages exchanged between human players. We define protocols and metrics to evaluate learned agents in this testbed, highlighting the need for a novel "crosstalk" evaluation condition which pairs agents trained independently on disjoint subsets of the training data. We present models for our task and benchmark them using both fully automated evaluation and by having them play the game live with humans.



### The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions
- **Arxiv ID**: http://arxiv.org/abs/1712.05577v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.05577v4)
- **Published**: 2017-12-15 08:25:51+00:00
- **Updated**: 2018-04-06 21:32:29+00:00
- **Authors**: George Philipp, Dawn Song, Jaime G. Carbonell
- **Comment**: An earlier version of this paper was named "Gradients explode - Deep
  Networks are shallow - ResNet explained" and presented at the ICLR 2018
  workshop (https://openreview.net/forum?id=rJjcdFkPM)
- **Journal**: None
- **Summary**: Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities "solve" the exploding gradient problem, we show that this is not the case in general and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the *collapsing domain problem*, which can arise in architectures that avoid exploding gradients.   ResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks. We show this is a direct consequence of the Pythagorean equation. By noticing that *any neural network is a residual network*, we devise the *residual trick*, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.



### Transfer Learning for OCRopus Model Training on Early Printed Books
- **Arxiv ID**: http://arxiv.org/abs/1712.05586v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05586v2)
- **Published**: 2017-12-15 09:14:38+00:00
- **Updated**: 2017-12-21 11:32:12+00:00
- **Authors**: Christian Reul, Christoph Wick, Uwe Springmann, Frank Puppe
- **Comment**: None
- **Journal**: None
- **Summary**: A method is presented that significantly reduces the character error rates for OCR text obtained from OCRopus models trained on early printed books when only small amounts of diplomatic transcriptions are available. This is achieved by building from already existing models during training instead of starting from scratch. To overcome the discrepancies between the set of characters of the pretrained model and the additional ground truth the OCRopus code is adapted to allow for alphabet expansion or reduction. The character set is now capable of flexibly adding and deleting characters from the pretrained alphabet when an existing model is loaded. For our experiments we use a self-trained mixed model on early Latin prints and the two standard OCRopus models on modern English and German Fraktur texts. The evaluation on seven early printed books showed that training from the Latin mixed model reduces the average amount of errors by 43% and 26%, respectively compared to training from scratch with 60 and 150 lines of ground truth, respectively. Furthermore, it is shown that even building from mixed models trained on data unrelated to the newly added training and test data can lead to significantly improved recognition results.



### Fast Hough Transform and approximation properties of dyadic patterns
- **Arxiv ID**: http://arxiv.org/abs/1712.05615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05615v1)
- **Published**: 2017-12-15 11:02:01+00:00
- **Updated**: 2017-12-15 11:02:01+00:00
- **Authors**: E. I. Ershov, S. M. Karpenko
- **Comment**: in Russian
- **Journal**: None
- **Summary**: Hough transform is a popular low-level computer vision algorithm. Its computationally effective modification, Fast Hough transform (FHT), makes use of special subsets of image matrix to approximate geometric lines on it. Because of their special structure, these subset are called dyadic patterns.   In this paper various properties of dyadic patterns are investigated. Exact upper bounds on approximation error are derived. In a simplest case, this error proves to be equal to $\frac{1}{6} log(n)$ for $n \times n$ sized images, as was conjectured previously by Goetz et al.



### Automated Image Analysis Framework for the High-Throughput Determination of Grapevine Berry Sizes Using Conditional Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1712.05647v1
- **DOI**: 10.1016/j.compag.2013.11.008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05647v1)
- **Published**: 2017-12-15 12:51:37+00:00
- **Updated**: 2017-12-15 12:51:37+00:00
- **Authors**: Ribana Roscher, Katja Herzog, Annemarie Kunkel, Anna Kicherer, Reinhard Töpfer, Wolfgang Förstner
- **Comment**: None
- **Journal**: Computers and Electronics in Agriculture 100 (2014), 148--158
- **Summary**: The berry size is one of the most important fruit traits in grapevine breeding. Non-invasive, image-based phenotyping promises a fast and precise method for the monitoring of the grapevine berry size. In the present study an automated image analyzing framework was developed in order to estimate the size of grapevine berries from images in a high-throughput manner. The framework includes (i) the detection of circular structures which are potentially berries and (ii) the classification of these into the class 'berry' or 'non-berry' by utilizing a conditional random field. The approach used the concept of a one-class classification, since only the target class 'berry' is of interest and needs to be modeled. Moreover, the classification was carried out by using an automated active learning approach, i.e no user interaction is required during the classification process and in addition, the process adapts automatically to changing image conditions, e.g. illumination or berry color. The framework was tested on three datasets consisting in total of 139 images. The images were taken in an experimental vineyard at different stages of grapevine growth according to the BBCH scale. The mean berry size of a plant estimated by the framework correlates with the manually measured berry size by $0.88$.



### Pre-training Attention Mechanisms
- **Arxiv ID**: http://arxiv.org/abs/1712.05652v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.05652v1)
- **Published**: 2017-12-15 12:59:22+00:00
- **Updated**: 2017-12-15 12:59:22+00:00
- **Authors**: Jack Lindsey
- **Comment**: Presented at NIPS 2017 Workshop on Cognitively Informed Artificial
  Intelligence
- **Journal**: None
- **Summary**: Recurrent neural networks with differentiable attention mechanisms have had success in generative and classification tasks. We show that the classification performance of such models can be enhanced by guiding a randomly initialized model to attend to salient regions of the input in early training iterations. We further show that, if explicit heuristics for guidance are unavailable, a model that is pretrained on an unsupervised reconstruction task can discover good attention policies without supervision. We demonstrate that increased efficiency of the attention mechanism itself contributes to these performance improvements. Based on these insights, we introduce bootstrapped glimpse mimicking, a simple, theoretically task-general method of more effectively training attention models. Our work draws inspiration from and parallels results on human learning of attention.



### Lightweight Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.05695v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1712.05695v1)
- **Published**: 2017-12-15 14:56:05+00:00
- **Updated**: 2017-12-15 14:56:05+00:00
- **Authors**: Altaf H. Khan
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the weights in a Lightweight Neural Network have a value of zero, while the remaining ones are either +1 or -1. These universal approximators require approximately 1.1 bits/weight of storage, posses a quick forward pass and achieve classification accuracies similar to conventional continuous-weight networks. Their training regimen focuses on error reduction initially, but later emphasizes discretization of weights. They ignore insignificant inputs, remove unnecessary weights, and drop unneeded hidden neurons. We have successfully tested them on the MNIST, credit card fraud, and credit card defaults data sets using networks having 2 to 16 hidden layers and up to 4.4 million weights.



### Unsupervised Domain Adaptation for 3D Keypoint Estimation via View Consistency
- **Arxiv ID**: http://arxiv.org/abs/1712.05765v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05765v2)
- **Published**: 2017-12-15 17:46:13+00:00
- **Updated**: 2018-07-26 04:38:58+00:00
- **Authors**: Xingyi Zhou, Arjun Karpur, Chuang Gan, Linjie Luo, Qixing Huang
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: In this paper, we introduce a novel unsupervised domain adaptation technique for the task of 3D keypoint prediction from a single depth scan or image. Our key idea is to utilize the fact that predictions from different views of the same or similar objects should be consistent with each other. Such view consistency can provide effective regularization for keypoint prediction on unlabeled instances. In addition, we introduce a geometric alignment term to regularize predictions in the target domain. The resulting loss function can be effectively optimized via alternating minimization. We demonstrate the effectiveness of our approach on real datasets and present experimental results showing that our approach is superior to state-of-the-art general-purpose domain adaptation techniques.



### Semantic Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/1712.05773v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05773v2)
- **Published**: 2017-12-15 18:02:47+00:00
- **Updated**: 2018-04-16 10:47:51+00:00
- **Authors**: Johannes L. Schönberger, Marc Pollefeys, Andreas Geiger, Torsten Sattler
- **Comment**: None
- **Journal**: None
- **Summary**: Robust visual localization under a wide range of viewing conditions is a fundamental problem in computer vision. Handling the difficult cases of this problem is not only very challenging but also of high practical relevance, e.g., in the context of life-long localization for augmented reality or autonomous robots. In this paper, we propose a novel approach based on a joint 3D geometric and semantic understanding of the world, enabling it to succeed under conditions where previous approaches failed. Our method leverages a novel generative model for descriptor learning, trained on semantic scene completion as an auxiliary task. The resulting 3D descriptors are robust to missing observations by encoding high-level 3D geometric and semantic information. Experiments on several challenging large-scale localization datasets demonstrate reliable localization under extreme viewpoint, illumination, and geometry changes.



### Multi-Attribute Robust Component Analysis for Facial UV Maps
- **Arxiv ID**: http://arxiv.org/abs/1712.05799v1
- **DOI**: 10.1109/JSTSP.2018.2877108
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05799v1)
- **Published**: 2017-12-15 18:14:59+00:00
- **Updated**: 2017-12-15 18:14:59+00:00
- **Authors**: Stylianos Moschoglou, Evangelos Ververas, Yannis Panagakis, Mihalis Nicolaou, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, due to the collection of large scale 3D face models, as well as the advent of deep learning, a significant progress has been made in the field of 3D face alignment "in-the-wild". That is, many methods have been proposed that establish sparse or dense 3D correspondences between a 2D facial image and a 3D face model. The utilization of 3D face alignment introduces new challenges and research directions, especially on the analysis of facial texture images. In particular, texture does not suffer any more from warping effects (that occurred when 2D face alignment methods were used). Nevertheless, since facial images are commonly captured in arbitrary recording conditions, a considerable amount of missing information and gross outliers is observed (e.g., due to self-occlusion, or subjects wearing eye-glasses). Given that many annotated databases have been developed for face analysis tasks, it is evident that component analysis techniques need to be developed in order to alleviate issues arising from the aforementioned challenges. In this paper, we propose a novel component analysis technique that is suitable for facial UV maps containing a considerable amount of missing information and outliers, while additionally, incorporates knowledge from various attributes (such as age and identity). We evaluate the proposed Multi-Attribute Robust Component Analysis (MA-RCA) on problems such as UV completion and age progression, where the proposed method outperforms compared techniques. Finally, we demonstrate that MA-RCA method is powerful enough to provide weak annotations for training deep learning systems for various applications, such as illumination transfer.



### Deep Burst Denoising
- **Arxiv ID**: http://arxiv.org/abs/1712.05790v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.05790v1)
- **Published**: 2017-12-15 18:55:16+00:00
- **Updated**: 2017-12-15 18:55:16+00:00
- **Authors**: Clément Godard, Kevin Matzen, Matt Uyttendaele
- **Comment**: None
- **Journal**: None
- **Summary**: Noise is an inherent issue of low-light image capture, one which is exacerbated on mobile devices due to their narrow apertures and small sensors. One strategy for mitigating noise in a low-light situation is to increase the shutter time of the camera, thus allowing each photosite to integrate more light and decrease noise variance. However, there are two downsides of long exposures: (a) bright regions can exceed the sensor range, and (b) camera and scene motion will result in blurred images. Another way of gathering more light is to capture multiple short (thus noisy) frames in a "burst" and intelligently integrate the content, thus avoiding the above downsides. In this paper, we use the burst-capture strategy and implement the intelligent integration via a recurrent fully convolutional deep neural net (CNN). We build our novel, multiframe architecture to be a simple addition to any single frame denoising model, and design to handle an arbitrary number of noisy input frames. We show that it achieves state of the art denoising results on our burst dataset, improving on the best published multi-frame techniques, such as VBM4D and FlexISP. Finally, we explore other applications of image enhancement by integrating content from multiple frames and demonstrate that our DNN architecture generalizes well to image super-resolution.



### Reducing Deep Network Complexity with Fourier Transform Methods
- **Arxiv ID**: http://arxiv.org/abs/1801.01451v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.01451v2)
- **Published**: 2017-12-15 20:30:09+00:00
- **Updated**: 2018-06-07 12:09:37+00:00
- **Authors**: Andrew Kiruluta
- **Comment**: mistake in tensorflow code with test data leakage into training set
  leading to model over fitting
- **Journal**: None
- **Summary**: We propose a novel way that uses shallow densely connected neuron network architectures to achieve superior performance to convolution based neural networks (CNNs) approaches with the added benefits of lower computation burden requiring dramatically less training examples to achieve high prediction accuracy ($>98\%$). The advantages of our proposed method is demonstrated in results on benchmark datasets which show significant performance gain over existing state-of-the-art results on MNIST, CIFAR-10 and CIFAR-100. By Fourier transforming the inputs, each point in the training sample then has a representational energy of all the weighted information from every other point. The consequence of using this input is a reduced complexity neuron network, reduced computation load and the lifting of the requirement for a large number of training examples to achieve high classification accuracy.



### Mapping the world population one building at a time
- **Arxiv ID**: http://arxiv.org/abs/1712.05839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05839v1)
- **Published**: 2017-12-15 21:18:05+00:00
- **Updated**: 2017-12-15 21:18:05+00:00
- **Authors**: Tobias G. Tiecke, Xianming Liu, Amy Zhang, Andreas Gros, Nan Li, Gregory Yetman, Talip Kilic, Siobhan Murray, Brian Blankespoor, Espen B. Prydz, Hai-Anh H. Dang
- **Comment**: None
- **Journal**: None
- **Summary**: High resolution datasets of population density which accurately map sparsely-distributed human populations do not exist at a global scale. Typically, population data is obtained using censuses and statistical modeling. More recently, methods using remotely-sensed data have emerged, capable of effectively identifying urbanized areas. Obtaining high accuracy in estimation of population distribution in rural areas remains a very challenging task due to the simultaneous requirements of sufficient sensitivity and resolution to detect very sparse populations through remote sensing as well as reliable performance at a global scale. Here, we present a computer vision method based on machine learning to create population maps from satellite imagery at a global scale, with a spatial sensitivity corresponding to individual buildings and suitable for global deployment. By combining this settlement data with census data, we create population maps with ~30 meter resolution for 18 countries. We validate our method, and find that the building identification has an average precision and recall of 0.95 and 0.91, respectively and that the population estimates have a standard error of a factor ~2 or less. Based on our data, we analyze 29 percent of the world population, and show that 99 percent lives within 36 km of the nearest urban cluster. The resulting high-resolution population datasets have applications in infrastructure planning, vaccination campaign planning, disaster response efforts and risk analysis such as high accuracy flood risk analysis.



### Multi-dimensional imaging data recovery via minimizing the partial sum of tubal nuclear norm
- **Arxiv ID**: http://arxiv.org/abs/1712.05870v3
- **DOI**: 10.1016/j.cam.2019.112680
- **Categories**: **cs.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.05870v3)
- **Published**: 2017-12-15 22:51:13+00:00
- **Updated**: 2020-01-23 08:12:53+00:00
- **Authors**: Tai-Xiang Jiang, Ting-Zhu Huang, Xi-Le Zhao, Liang-Jian Deng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate tensor recovery problems within the tensor singular value decomposition (t-SVD) framework. We propose the partial sum of the tubal nuclear norm (PSTNN) of a tensor. The PSTNN is a surrogate of the tensor tubal multi-rank. We build two PSTNN-based minimization models for two typical tensor recovery problems, i.e., the tensor completion and the tensor principal component analysis. We give two algorithms based on the alternating direction method of multipliers (ADMM) to solve proposed PSTNN-based tensor recovery models. Experimental results on the synthetic data and real-world data reveal the superior of the proposed PSTNN.



