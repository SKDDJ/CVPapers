# Arxiv Papers in cs.CV on 2017-12-13
### Fusing Multiple Multiband Images
- **Arxiv ID**: http://arxiv.org/abs/1712.04575v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04575v2)
- **Published**: 2017-12-13 00:09:28+00:00
- **Updated**: 2018-07-18 00:39:25+00:00
- **Authors**: Reza Arablouei
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of fusing an arbitrary number of multiband, i.e., panchromatic, multispectral, or hyperspectral, images belonging to the same scene. We use the well-known forward observation and linear mixture models with Gaussian perturbations to formulate the maximum-likelihood estimator of the endmember abundance matrix of the fused image. We calculate the Fisher information matrix for this estimator and examine the conditions for the uniqueness of the estimator. We use a vector total-variation penalty term together with nonnegativity and sum-to-one constraints on the endmember abundances to regularize the derived maximum-likelihood estimation problem. The regularization facilitates exploiting the prior knowledge that natural images are mostly composed of piecewise smooth regions with limited abrupt changes, i.e., edges, as well as coping with potential ill-posedness of the fusion problem. We solve the resultant convex optimization problem using the alternating direction method of multipliers. We utilize the circular convolution theorem in conjunction with the fast Fourier transform to alleviate the computational complexity of the proposed algorithm. Experiments with multiband images constructed from real hyperspectral datasets reveal the superior performance of the proposed algorithm in comparison with the state-of-the-art algorithms, which need to be used in tandem to fuse more than two multiband images.



### Low-dose spectral CT reconstruction using L0 image gradient and tensor dictionary
- **Arxiv ID**: http://arxiv.org/abs/1801.01452v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01452v2)
- **Published**: 2017-12-13 00:24:04+00:00
- **Updated**: 2018-07-24 15:34:00+00:00
- **Authors**: Weiwen Wu, Yanbo Zhang, Qian Wang, Fenglin Liu, Peijun Chen, Hengyong Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Spectral computed tomography (CT) has a great superiority in lesion detection, tissue characterization and material decomposition. To further extend its potential clinical applications, in this work, we propose an improved tensor dictionary learning method for low-dose spectral CT reconstruction with a constraint of image gradient L0-norm, which is named as L0TDL. The L0TDL method inherits the advantages of tensor dictionary learning (TDL) by employing the similarity of spectral CT images. On the other hand, by introducing the L0-norm constraint in gradient image domain, the proposed method emphasizes the spatial sparsity to overcome the weakness of TDL on preserving edge information. The alternative direction minimization method (ADMM) is employed to solve the proposed method. Both numerical simulations and real mouse studies are perform to evaluate the proposed method. The results show that the proposed L0TDL method outperforms other competing methods, such as total variation (TV) minimization, TV with low rank (TV+LR), and TDL methods.



### Deep Quaternion Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.04604v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.04604v3)
- **Published**: 2017-12-13 04:19:24+00:00
- **Updated**: 2018-07-29 14:12:23+00:00
- **Authors**: Chase Gaudet, Anthony Maida
- **Comment**: IJCNN 2018, 8 pages, 1 figure
- **Journal**: None
- **Summary**: The field of deep learning has seen significant advancement in recent years. However, much of the existing work has been focused on real-valued numbers. Recent work has shown that a deep learning system using the complex numbers can be deeper for a fixed parameter budget compared to its real-valued counterpart. In this work, we explore the benefits of generalizing one step further into the hyper-complex numbers, quaternions specifically, and provide the architecture components needed to build deep quaternion networks. We develop the theoretical basis by reviewing quaternion convolutions, developing a novel quaternion weight initialization scheme, and developing novel algorithms for quaternion batch-normalization. These pieces are tested in a classification model by end-to-end training on the CIFAR-10 and CIFAR-100 data sets and a segmentation model by end-to-end training on the KITTI Road Segmentation data set. These quaternion networks show improved convergence compared to real-valued and complex-valued networks, especially on the segmentation task, while having fewer parameters



### Transfer Adversarial Hashing for Hamming Space Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1712.04616v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04616v1)
- **Published**: 2017-12-13 06:06:13+00:00
- **Updated**: 2017-12-13 06:06:13+00:00
- **Authors**: Zhangjie Cao, Mingsheng Long, Chao Huang, Jianmin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing is widely applied to large-scale image retrieval due to the storage and retrieval efficiency. Existing work on deep hashing assumes that the database in the target domain is identically distributed with the training set in the source domain. This paper relaxes this assumption to a transfer retrieval setting, which allows the database and the training set to come from different but relevant domains. However, the transfer retrieval setting will introduce two technical difficulties: first, the hash model trained on the source domain cannot work well on the target domain due to the large distribution gap; second, the domain gap makes it difficult to concentrate the database points to be within a small Hamming ball. As a consequence, transfer retrieval performance within Hamming Radius 2 degrades significantly in existing hashing methods. This paper presents Transfer Adversarial Hashing (TAH), a new hybrid deep architecture that incorporates a pairwise $t$-distribution cross-entropy loss to learn concentrated hash codes and an adversarial network to align the data distributions between the source and target domains. TAH can generate compact transfer hash codes for efficient image retrieval on both source and target domains. Comprehensive experiments validate that TAH yields state of the art Hamming space retrieval performance on standard datasets.



### The Effectiveness of Data Augmentation in Image Classification using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.04621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04621v1)
- **Published**: 2017-12-13 06:41:00+00:00
- **Updated**: 2017-12-13 06:41:00+00:00
- **Authors**: Luis Perez, Jason Wang
- **Comment**: 8 pages, 12 figures
- **Journal**: None
- **Summary**: In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classification. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and flipping input images. We artificially constrain our access to data to a small subset of the ImageNet dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with GANs to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classifier, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.



### Learning Disentangling and Fusing Networks for Face Completion Under Structured Occlusions
- **Arxiv ID**: http://arxiv.org/abs/1712.04646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04646v1)
- **Published**: 2017-12-13 08:05:43+00:00
- **Updated**: 2017-12-13 08:05:43+00:00
- **Authors**: Zhihang Li, Yibo Hu, Ran He
- **Comment**: Submitted to CVPR 2018
- **Journal**: None
- **Summary**: Face completion aims to generate semantically new pixels for missing facial components. It is a challenging generative task due to large variations of face appearance. This paper studies generative face completion under structured occlusions. We treat the face completion and corruption as disentangling and fusing processes of clean faces and occlusions, and propose a jointly disentangling and fusing Generative Adversarial Network (DF-GAN). First, three domains are constructed, corresponding to the distributions of occluded faces, clean faces and structured occlusions. The disentangling and fusing processes are formulated as the transformations between the three domains. Then the disentangling and fusing networks are built to learn the transformations from unpaired data, where the encoder-decoder structure is adopted and allows DF-GAN to simulate structure occlusions by modifying the latent representations. Finally, the disentangling and fusing processes are unified into a dual learning framework along with an adversarial strategy. The proposed method is evaluated on Meshface verification problem. Experimental results on four Meshface databases demonstrate the effectiveness of our proposed method for the face completion under structured occlusions.



### UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.04695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04695v1)
- **Published**: 2017-12-13 10:52:42+00:00
- **Updated**: 2017-12-13 10:52:42+00:00
- **Authors**: Jiankang Deng, Shiyang Cheng, Niannan Xue, Yuxiang Zhou, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Recently proposed robust 3D face alignment methods establish either dense or sparse correspondence between a 3D face model and a 2D facial image. The use of these methods presents new challenges as well as opportunities for facial texture analysis. In particular, by sampling the image using the fitted model, a facial UV can be created. Unfortunately, due to self-occlusion, such a UV map is always incomplete. In this paper, we propose a framework for training Deep Convolutional Neural Network (DCNN) to complete the facial UV map extracted from in-the-wild images. To this end, we first gather complete UV maps by fitting a 3D Morphable Model (3DMM) to various multiview image and video datasets, as well as leveraging on a new 3D dataset with over 3,000 identities. Second, we devise a meticulously designed architecture that combines local and global adversarial DCNNs to learn an identity-preserving facial UV completion model. We demonstrate that by attaching the completed UV to the fitted mesh and generating instances of arbitrary poses, we can increase pose variations for training deep face recognition/verification models, and minimise pose discrepancy during testing, which lead to better performance. Experiments on both controlled and in-the-wild UV datasets prove the effectiveness of our adversarial UV completion model. We achieve state-of-the-art verification accuracy, $94.05\%$, under the CFP frontal-profile protocol only by combining pose augmentation during training and pose discrepancy reduction during testing. We will release the first in-the-wild UV dataset (we refer as WildUV) that comprises of complete facial UV maps from 1,892 identities for research purposes.



### An Enhanced Hybrid MobileNet
- **Arxiv ID**: http://arxiv.org/abs/1712.04698v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04698v2)
- **Published**: 2017-12-13 10:54:54+00:00
- **Updated**: 2019-06-13 04:21:26+00:00
- **Authors**: Hong-Yen Chen, Chung-Yen Su
- **Comment**: None
- **Journal**: None
- **Summary**: Complicated and deep neural network models can achieve high accuracy for image recognition. However, they require a huge amount of computations and model parameters, which are not suitable for mobile and embedded devices. Therefore, MobileNet was proposed, which can reduce the number of parameters and computational cost dramatically. The main idea of MobileNet is to use a depthwise separable convolution. Two hyper-parameters, a width multiplier and a resolution multiplier are used to the trade-off between the accuracy and the latency. In this paper, we propose a new architecture to improve the MobileNet. Instead of using the resolution multiplier, we use a depth multiplier and combine with either Fractional Max Pooling or the max pooling. Experimental results on CIFAR database show that the proposed architecture can reduce the amount of computational cost and increase the accuracy simultaneously.



### Regularization and Optimization strategies in Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1712.04711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04711v1)
- **Published**: 2017-12-13 11:23:45+00:00
- **Updated**: 2017-12-13 11:23:45+00:00
- **Authors**: Pushparaja Murugan, Shanmugasundaram Durairaj
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Convolution Neural Networks, known as ConvNets exceptionally perform well in many complex machine learning tasks. The architecture of ConvNets demands the huge and rich amount of data and involves with a vast number of parameters that leads the learning takes to be computationally expensive, slow convergence towards the global minima, trap in local minima with poor predictions. In some cases, architecture overfits the data and make the architecture difficult to generalise for new samples that were not in the training set samples. To address these limitations, many regularization and optimization strategies are developed for the past few years. Also, studies suggested that these techniques significantly increase the performance of the networks as well as reducing the computational cost. In implementing these techniques, one must thoroughly understand the theoretical concept of how this technique works in increasing the expressive power of the networks. This article is intended to provide the theoretical concepts and mathematical formulation of the most commonly used strategies in developing a ConvNet architecture.



### Mathematics of Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.04741v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.04741v1)
- **Published**: 2017-12-13 12:44:46+00:00
- **Updated**: 2017-12-13 12:44:46+00:00
- **Authors**: Rene Vidal, Joan Bruna, Raja Giryes, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: Recently there has been a dramatic increase in the performance of recognition systems due to the introduction of deep architectures for representation learning and classification. However, the mathematical reasons for this success remain elusive. This tutorial will review recent work that aims to provide a mathematical justification for several properties of deep networks, such as global optimality, geometric stability, and invariance of the learned representations.



### GMM-Based Synthetic Samples for Classification of Hyperspectral Images With Limited Training Data
- **Arxiv ID**: http://arxiv.org/abs/1712.04778v1
- **DOI**: 10.1109/LGRS.2018.2817361
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04778v1)
- **Published**: 2017-12-13 14:12:06+00:00
- **Updated**: 2017-12-13 14:12:06+00:00
- **Authors**: AmirAbbas Davari, Erchan Aptoula, Berrin Yanikoglu, Andreas Maier, Christian Riess
- **Comment**: None
- **Journal**: None
- **Summary**: The amount of training data that is required to train a classifier scales with the dimensionality of the feature data. In hyperspectral remote sensing, feature data can potentially become very high dimensional. However, the amount of training data is oftentimes limited. Thus, one of the core challenges in hyperspectral remote sensing is how to perform multi-class classification using only relatively few training data points.   In this work, we address this issue by enriching the feature matrix with synthetically generated sample points. This synthetic data is sampled from a GMM fitted to each class of the limited training data. Although, the true distribution of features may not be perfectly modeled by the fitted GMM, we demonstrate that a moderate augmentation by these synthetic samples can effectively replace a part of the missing training samples. We show the efficacy of the proposed approach on two hyperspectral datasets. The median gain in classification performance is $5\%$. It is also encouraging that this performance gain is remarkably stable for large variations in the number of added samples, which makes it much easier to apply this method to real-world applications.



### Symbol detection in online handwritten graphics using Faster R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1712.04833v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04833v1)
- **Published**: 2017-12-13 16:05:13+00:00
- **Updated**: 2017-12-13 16:05:13+00:00
- **Authors**: Frank D. Julca-Aguilar, Nina S. T. Hirata
- **Comment**: Submitted to DAS-2018
- **Journal**: None
- **Summary**: Symbol detection techniques in online handwritten graphics (e.g. diagrams and mathematical expressions) consist of methods specifically designed for a single graphic type. In this work, we evaluate the Faster R-CNN object detection algorithm as a general method for detection of symbols in handwritten graphics. We evaluate different configurations of the Faster R-CNN method, and point out issues relative to the handwritten nature of the data. Considering the online recognition context, we evaluate efficiency and accuracy trade-offs of using Deep Neural Networks of different complexities as feature extractors. We evaluate the method on publicly available flowchart and mathematical expression (CROHME-2016) datasets. Results show that Faster R-CNN can be effectively used on both datasets, enabling the possibility of developing general methods for symbol detection, and furthermore, general graphic understanding methods that could be built on top of the algorithm.



### MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features
- **Arxiv ID**: http://arxiv.org/abs/1712.04837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04837v1)
- **Published**: 2017-12-13 16:09:55+00:00
- **Updated**: 2017-12-13 16:09:55+00:00
- **Authors**: Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff, Peng Wang, Hartwig Adam
- **Comment**: 10 pages including reference
- **Journal**: None
- **Summary**: In this work, we tackle the problem of instance segmentation, the task of simultaneously solving object detection and semantic segmentation. Towards this goal, we present a model, called MaskLab, which produces three outputs: box detection, semantic segmentation, and direction prediction. Building on top of the Faster-RCNN object detector, the predicted boxes provide accurate localization of object instances. Within each region of interest, MaskLab performs foreground/background segmentation by combining semantic and direction prediction. Semantic segmentation assists the model in distinguishing between objects of different semantic classes including background, while the direction prediction, estimating each pixel's direction towards its corresponding center, allows separating instances of the same semantic class. Moreover, we explore the effect of incorporating recent successful methods from both segmentation and detection (i.e. atrous convolution and hypercolumn). Our proposed model is evaluated on the COCO instance segmentation benchmark and shows comparable performance with other state-of-art models.



### Self-Supervised Relative Depth Learning for Urban Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1712.04850v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04850v2)
- **Published**: 2017-12-13 16:39:14+00:00
- **Updated**: 2018-04-02 15:39:03+00:00
- **Authors**: Huaizu Jiang, Erik Learned-Miller, Gustav Larsson, Michael Maire, Greg Shakhnarovich
- **Comment**: None
- **Journal**: None
- **Summary**: As an agent moves through the world, the apparent motion of scene elements is (usually) inversely proportional to their depth. It is natural for a learning agent to associate image patterns with the magnitude of their displacement over time: as the agent moves, faraway mountains don't move much; nearby trees move a lot. This natural relationship between the appearance of objects and their motion is a rich source of information about the world. In this work, we start by training a deep network, using fully automatic supervision, to predict relative scene depth from single images. The relative depth training images are automatically derived from simple videos of cars moving through a scene, using recent motion segmentation techniques, and no human-provided labels. This proxy task of predicting relative depth from a single image induces features in the network that result in large improvements in a set of downstream tasks including semantic segmentation, joint road segmentation and car detection, and monocular (absolute) depth estimation, over a network trained from scratch. The improvement on the semantic segmentation task is greater than those produced by any other automatically supervised methods. Moreover, for monocular depth estimation, our unsupervised pre-training method even outperforms supervised pre-training with ImageNet. In addition, we demonstrate benefits from learning to predict (unsupervised) relative depth in the specific videos associated with various downstream tasks. We adapt to the specific scenes in those tasks in an unsupervised manner to improve performance. In summary, for semantic segmentation, we present state-of-the-art results among methods that do not use supervised pre-training, and we even exceed the performance of supervised ImageNet pre-trained models for monocular depth estimation, achieving results that are comparable with state-of-the-art methods.



### Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1712.04851v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04851v2)
- **Published**: 2017-12-13 16:40:55+00:00
- **Updated**: 2018-07-27 03:20:56+00:00
- **Authors**: Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, Kevin Murphy
- **Comment**: ECCV 2018 camera ready
- **Journal**: None
- **Summary**: Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model/computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model/computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level semantic features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial/temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101-24).



### Multidimensional Data Tensor Sensing for RF Tomographic Imaging
- **Arxiv ID**: http://arxiv.org/abs/1712.04919v2
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, eess.SP, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1712.04919v2)
- **Published**: 2017-12-13 18:40:22+00:00
- **Updated**: 2017-12-16 15:38:35+00:00
- **Authors**: Tao Deng, Xiao-Yang Liu, Feng Qian, Anwar Walid
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: Radio-frequency (RF) tomographic imaging is a promising technique for inferring multi-dimensional physical space by processing RF signals traversed across a region of interest. However, conventional RF tomography schemes are generally based on vector compressed sensing, which ignores the geometric structures of the target spaces and leads to low recovery precision. The recently proposed transform-based tensor model is more appropriate for sensory data processing, as it helps exploit the geometric structures of the three-dimensional target and improve the recovery precision. In this paper, we propose a novel tensor sensing approach that achieves highly accurate estimation for real-world three-dimensional spaces. First, we use the transform-based tensor model to formulate a tensor sensing problem, and propose a fast alternating minimization algorithm called Alt-Min. Secondly, we drive an algorithm which is optimized to reduce memory and computation requirements. Finally, we present evaluation of our Alt-Min approach using IKEA 3D data and demonstrate significant improvement in recovery error and convergence speed compared to prior tensor-based compressed sensing.



### Real-time Egocentric Gesture Recognition on Mobile Head Mounted Displays
- **Arxiv ID**: http://arxiv.org/abs/1712.04961v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04961v1)
- **Published**: 2017-12-13 19:06:37+00:00
- **Updated**: 2017-12-13 19:06:37+00:00
- **Authors**: Rohit Pandey, Marie White, Pavel Pidlypenskyi, Xue Wang, Christine Kaeser-Chen
- **Comment**: Extended Abstract NIPS 2017 Machine Learning on the Phone and other
  Consumer Devices Workshop
- **Journal**: None
- **Summary**: Mobile virtual reality (VR) head mounted displays (HMD) have become popular among consumers in recent years. In this work, we demonstrate real-time egocentric hand gesture detection and localization on mobile HMDs. Our main contributions are: 1) A novel mixed-reality data collection tool to automatic annotate bounding boxes and gesture labels; 2) The largest-to-date egocentric hand gesture and bounding box dataset with more than 400,000 annotated frames; 3) A neural network that runs real time on modern mobile CPUs, and achieves higher than 76% precision on gesture recognition across 8 classes.



### Learning Low-shot facial representations via 2D warping
- **Arxiv ID**: http://arxiv.org/abs/1712.05015v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.05015v2)
- **Published**: 2017-12-13 21:39:52+00:00
- **Updated**: 2018-02-05 15:08:07+00:00
- **Authors**: Shen Yan
- **Comment**: The new version should update the table as well as add some new
  results. This paper is talking about one-shot learning, but the current
  version is not mainly focus on that point. I should restructure the article.
  After the page reduction I assume the new version would be 4 pages. The title
  name should also be changed to one-shot learning and there are more previous
  work should be cited
- **Journal**: None
- **Summary**: In this work, we mainly study the influence of the 2D warping module for one-shot face recognition.



### Unsupervised Histopathology Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1712.05021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05021v1)
- **Published**: 2017-12-13 21:52:12+00:00
- **Updated**: 2017-12-13 21:52:12+00:00
- **Authors**: Le Hou, Ayush Agarwal, Dimitris Samaras, Tahsin M. Kurc, Rajarsi R. Gupta, Joel H. Saltz
- **Comment**: None
- **Journal**: None
- **Summary**: Hematoxylin and Eosin stained histopathology image analysis is essential for the diagnosis and study of complicated diseases such as cancer. Existing state-of-the-art approaches demand extensive amount of supervised training data from trained pathologists. In this work we synthesize in an unsupervised manner, large histopathology image datasets, suitable for supervised training tasks. We propose a unified pipeline that: a) generates a set of initial synthetic histopathology images with paired information about the nuclei such as segmentation masks; b) refines the initial synthetic images through a Generative Adversarial Network (GAN) to reference styles; c) trains a task-specific CNN and boosts the performance of the task-specific CNN with on-the-fly generated adversarial examples. Our main contribution is that the synthetic images are not only realistic, but also representative (in reference styles) and relatively challenging for training task-specific CNNs. We test our method for nucleus segmentation using images from four cancer types. When no supervised data exists for a cancer type, our method without supervision cost significantly outperforms supervised methods which perform across-cancer generalization. Even when supervised data exists for all cancer types, our approach without supervision cost performs better than supervised methods.



### Pediatric Bone Age Assessment Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.05053v2
- **DOI**: 10.1007/978-3-030-00889-5_34
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05053v2)
- **Published**: 2017-12-13 23:56:15+00:00
- **Updated**: 2018-06-19 19:46:33+00:00
- **Authors**: Vladimir Iglovikov, Alexander Rakhlin, Alexandr Kalinin, Alexey Shvets
- **Comment**: 14 pages, 9 figures
- **Journal**: None
- **Summary**: Skeletal bone age assessment is a common clinical practice to diagnose endocrine and metabolic disorders in child development. In this paper, we describe a fully automated deep learning approach to the problem of bone age assessment using data from Pediatric Bone Age Challenge organized by RSNA 2017. The dataset for this competition is consisted of 12.6k radiological images of left hand labeled by the bone age and sex of patients. Our approach utilizes several deep learning architectures: U-Net, ResNet-50, and custom VGG-style neural networks trained end-to-end. We use images of whole hands as well as specific parts of a hand for both training and inference. This approach allows us to measure importance of specific hand bones for the automated bone age analysis. We further evaluate performance of the method in the context of skeletal development stages. Our approach outperforms other common methods for bone age assessment.



