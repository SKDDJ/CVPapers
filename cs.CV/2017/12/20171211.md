# Arxiv Papers in cs.CV on 2017-12-11
### Parallel Mapper
- **Arxiv ID**: http://arxiv.org/abs/1712.03660v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.DC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.03660v3)
- **Published**: 2017-12-11 07:02:06+00:00
- **Updated**: 2020-05-12 01:56:37+00:00
- **Authors**: Mustafa Hajij, Basem Assiri, Paul Rosen
- **Comment**: None
- **Journal**: None
- **Summary**: The construction of Mapper has emerged in the last decade as a powerful and effective topological data analysis tool that approximates and generalizes other topological summaries, such as the Reeb graph, the contour tree, split, and joint trees. In this paper, we study the parallel analysis of the construction of Mapper. We give a provably correct parallel algorithm to execute Mapper on multiple processors and discuss the performance results that compare our approach to a reference sequential Mapper implementation. We report the performance experiments that demonstrate the efficiency of our method.



### A practical guide and software for analysing pairwise comparison experiments
- **Arxiv ID**: http://arxiv.org/abs/1712.03686v2
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1712.03686v2)
- **Published**: 2017-12-11 09:21:36+00:00
- **Updated**: 2017-12-15 14:43:34+00:00
- **Authors**: Maria Perez-Ortiz, Rafal K. Mantiuk
- **Comment**: Code available at https://github.com/mantiuk/pwcmp
- **Journal**: None
- **Summary**: Most popular strategies to capture subjective judgments from humans involve the construction of a unidimensional relative measurement scale, representing order preferences or judgments about a set of objects or conditions. This information is generally captured by means of direct scoring, either in the form of a Likert or cardinal scale, or by comparative judgments in pairs or sets. In this sense, the use of pairwise comparisons is becoming increasingly popular because of the simplicity of this experimental procedure. However, this strategy requires non-trivial data analysis to aggregate the comparison ranks into a quality scale and analyse the results, in order to take full advantage of the collected data. This paper explains the process of translating pairwise comparison data into a measurement scale, discusses the benefits and limitations of such scaling methods and introduces a publicly available software in Matlab. We improve on existing scaling methods by introducing outlier analysis, providing methods for computing confidence intervals and statistical testing and introducing a prior, which reduces estimation error when the number of observers is low. Most of our examples focus on image quality assessment.



### FHEDN: A based on context modeling Feature Hierarchy Encoder-Decoder Network for face detection
- **Arxiv ID**: http://arxiv.org/abs/1712.03687v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03687v1)
- **Published**: 2017-12-11 09:27:14+00:00
- **Updated**: 2017-12-11 09:27:14+00:00
- **Authors**: Zexun Zhou, Zhongshi He, Ziyu Chen, Yuanyuan Jia, Haiyan Wang, Jinglong Du, Dingding Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Because of affected by weather conditions, camera pose and range, etc. Objects are usually small, blur, occluded and diverse pose in the images gathered from outdoor surveillance cameras or access control system. It is challenging and important to detect faces precisely for face recognition system in the field of public security. In this paper, we design a based on context modeling structure named Feature Hierarchy Encoder-Decoder Network for face detection(FHEDN), which can detect small, blur and occluded face with hierarchy by hierarchy from the end to the beginning likes encoder-decoder in a single network. The proposed network is consist of multiple context modeling and prediction modules, which are in order to detect small, blur, occluded and diverse pose faces. In addition, we analyse the influence of distribution of training set, scale of default box and receipt field size to detection performance in implement stage. Demonstrated by experiments, Our network achieves promising performance on WIDER FACE and FDDB benchmarks.



### The Effectiveness of Data Augmentation for Detection of Gastrointestinal Diseases from Endoscopical Images
- **Arxiv ID**: http://arxiv.org/abs/1712.03689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03689v1)
- **Published**: 2017-12-11 09:29:01+00:00
- **Updated**: 2017-12-11 09:29:01+00:00
- **Authors**: Andrea Asperti, Claudio Mastronardo
- **Comment**: None
- **Journal**: Proceedings of the 5th International Conference on Bioimaging,
  BIOIMAGING 2018, 19-21 January 2018, Funchal, Madeira - Portugal
- **Summary**: The lack, due to privacy concerns, of large public databases of medical pathologies is a well-known and major problem, substantially hindering the application of deep learning techniques in this field. In this article, we investigate the possibility to supply to the deficiency in the number of data by means of data augmentation techniques, working on the recent Kvasir dataset of endoscopical images of gastrointestinal diseases. The dataset comprises 4,000 colored images labeled and verified by medical endoscopists, covering a few common pathologies at different anatomical landmarks: Z-line, pylorus and cecum. We show how the application of data augmentation techniques allows to achieve sensible improvements of the classification with respect to previous approaches, both in terms of precision and recall.



### Can We Teach Computers to Understand Art? Domain Adaptation for Enhancing Deep Networks Capacity to De-Abstract Art
- **Arxiv ID**: http://arxiv.org/abs/1712.03727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03727v1)
- **Published**: 2017-12-11 11:40:08+00:00
- **Updated**: 2017-12-11 11:40:08+00:00
- **Authors**: Mihai Badea, Corneliu Florea, Laura Florea, Constantin Vertan
- **Comment**: 17 pages, 5 figures, 4 tables, preprint for journal
- **Journal**: None
- **Summary**: Humans comprehend a natural scene at a single glance; painters and other visual artists, through their abstract representations, stressed this capacity to the limit. The performance of computer vision solutions matched that of humans in many problems of visual recognition. In this paper we address the problem of recognizing the genre (subject) in digitized paintings using Convolutional Neural Networks (CNN) as part of the more general dealing with abstract and/or artistic representation of scenes. Initially we establish the state of the art performance by training a CNN from scratch. In the next level of evaluation, we identify aspects that hinder the CNNs' recognition, such as artistic abstraction. Further, we test various domain adaptation methods that could enhance the subject recognition capabilities of the CNNs. The evaluation is performed on a database of 80,000 annotated digitized paintings, which is tentatively extended with artistic photographs, either original or stylized, in order to emulate artistic representations. Surprisingly, the most efficient domain adaptation is not the neural style transfer. Finally, the paper provides an experiment-based assessment of the abstraction level that CNNs are able to achieve.



### Learning Surrogate Models of Document Image Quality Metrics for Automated Document Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1712.03738v1
- **DOI**: 10.1109/DAS.2018.14
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03738v1)
- **Published**: 2017-12-11 12:13:33+00:00
- **Updated**: 2017-12-11 12:13:33+00:00
- **Authors**: Prashant Singh, Ekta Vats, Anders Hast
- **Comment**: None
- **Journal**: None
- **Summary**: Computation of document image quality metrics often depends upon the availability of a ground truth image corresponding to the document. This limits the applicability of quality metrics in applications such as hyperparameter optimization of image processing algorithms that operate on-the-fly on unseen documents. This work proposes the use of surrogate models to learn the behavior of a given document quality metric on existing datasets where ground truth images are available. The trained surrogate model can later be used to predict the metric value on previously unseen document images without requiring access to ground truth images. The surrogate model is empirically evaluated on the Document Image Binarization Competition (DIBCO) and the Handwritten Document Image Binarization Competition (H-DIBCO) datasets.



### Domain Adaptation Using Adversarial Learning for Autonomous Navigation
- **Arxiv ID**: http://arxiv.org/abs/1712.03742v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1712.03742v6)
- **Published**: 2017-12-11 12:23:06+00:00
- **Updated**: 2018-05-22 02:33:38+00:00
- **Authors**: Jaeyoon Yoo, Yongjun Hong, YungKyun Noh, Sungroh Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous navigation has become an increasingly popular machine learning application. Recent advances in deep learning have also resulted in great improvements to autonomous navigation. However, prior outdoor autonomous navigation depends on various expensive sensors or large amounts of real labeled data which is difficult to acquire and sometimes erroneous. The objective of this study is to train an autonomous navigation model that uses a simulator (instead of real labeled data) and an inexpensive monocular camera. In order to exploit the simulator satisfactorily, our proposed method is based on domain adaptation with adversarial learning. Specifically, we propose our model with 1) a dilated residual block in the generator, 2) cycle loss, and 3) style loss to improve the adversarial learning performance for satisfactory domain adaptation. In addition, we perform a theoretical analysis that supports the justification of our proposed method. We present empirical results of navigation in outdoor courses with various intersections using a commercial radio controlled car. We observe that our proposed method allows us to learn a favorable navigation model by generating images with realistic textures. To the best of our knowledge, this is the first work to apply domain adaptation with adversarial learning to autonomous navigation in real outdoor environments. Our proposed method can also be applied to precise image generation or other robotic tasks.



### Deep convolutional neural networks for brain image analysis on magnetic resonance imaging: a review
- **Arxiv ID**: http://arxiv.org/abs/1712.03747v3
- **DOI**: 10.1016/j.artmed.2018.08.008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03747v3)
- **Published**: 2017-12-11 12:25:30+00:00
- **Updated**: 2018-06-11 16:47:32+00:00
- **Authors**: Jose Bernal, Kaisar Kushibar, Daniel S. Asfaw, Sergi Valverde, Arnau Oliver, Robert Martí, Xavier Lladó
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep convolutional neural networks (CNNs) have shown record-shattering performance in a variety of computer vision problems, such as visual object recognition, detection and segmentation. These methods have also been utilised in medical image analysis domain for lesion segmentation, anatomical segmentation and classification. We present an extensive literature review of CNN techniques applied in brain magnetic resonance imaging (MRI) analysis, focusing on the architectures, pre-processing, data-preparation and post-processing strategies available in these works. The aim of this study is three-fold. Our primary goal is to report how different CNN architectures have evolved, discuss state-of-the-art strategies, condense their results obtained using public datasets and examine their pros and cons. Second, this paper is intended to be a detailed reference of the research activity in deep CNN for brain MRI analysis. Finally, we present a perspective on the future of CNNs in which we hint some of the research directions in subsequent years.



### NestedNet: Learning Nested Sparse Structures in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.03781v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.03781v2)
- **Published**: 2017-12-11 14:09:06+00:00
- **Updated**: 2018-03-27 04:44:56+00:00
- **Authors**: Eunwoo Kim, Chanho Ahn, Songhwai Oh
- **Comment**: To appear in CVPR 2018. Spotlight Presentation
- **Journal**: None
- **Summary**: Recently, there have been increasing demands to construct compact deep architectures to remove unnecessary redundancy and to improve the inference speed. While many recent works focus on reducing the redundancy by eliminating unneeded weight parameters, it is not possible to apply a single deep architecture for multiple devices with different resources. When a new device or circumstantial condition requires a new deep architecture, it is necessary to construct and train a new network from scratch. In this work, we propose a novel deep learning framework, called a nested sparse network, which exploits an n-in-1-type nested structure in a neural network. A nested sparse network consists of multiple levels of networks with a different sparsity ratio associated with each level, and higher level networks share parameters with lower level networks to enable stable nested learning. The proposed framework realizes a resource-aware versatile architecture as the same network can meet diverse resource requirements. Moreover, the proposed nested network can learn different forms of knowledge in its internal networks at different levels, enabling multiple tasks using a single network, such as coarse-to-fine hierarchical classification. In order to train the proposed nested sparse network, we propose efficient weight connection learning and channel and layer scheduling strategies. We evaluate our network in multiple tasks, including adaptive deep compression, knowledge distillation, and learning class hierarchy, and demonstrate that nested sparse networks perform competitively, but more efficiently, compared to existing methods.



### Identifying the Mislabeled Training Samples of ECG Signals using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.03792v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.03792v1)
- **Published**: 2017-12-11 14:22:56+00:00
- **Updated**: 2017-12-11 14:22:56+00:00
- **Authors**: Yaoguang Li, Wei Cui, Cong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The classification accuracy of electrocardiogram signal is often affected by diverse factors in which mislabeled training samples issue is one of the most influential problems. In order to mitigate this negative effect, the method of cross validation is introduced to identify the mislabeled samples. The method utilizes the cooperative advantages of different classifiers to act as a filter for the training samples. The filter removes the mislabeled training samples and retains the correctly labeled ones with the help of 10-fold cross validation. Consequently, a new training set is provided to the final classifiers to acquire higher classification accuracies. Finally, we numerically show the effectiveness of the proposed method with the MIT-BIH arrhythmia database.



### Error Correction for Dense Semantic Image Labeling
- **Arxiv ID**: http://arxiv.org/abs/1712.03812v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03812v1)
- **Published**: 2017-12-11 15:11:01+00:00
- **Updated**: 2017-12-11 15:11:01+00:00
- **Authors**: Yu-Hui Huang, Xu Jia, Stamatios Georgoulis, Tinne Tuytelaars, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Pixelwise semantic image labeling is an important, yet challenging, task with many applications. Typical approaches to tackle this problem involve either the training of deep networks on vast amounts of images to directly infer the labels or the use of probabilistic graphical models to jointly model the dependencies of the input (i.e. images) and output (i.e. labels). Yet, the former approaches do not capture the structure of the output labels, which is crucial for the performance of dense labeling, and the latter rely on carefully hand-designed priors that require costly parameter tuning via optimization techniques, which in turn leads to long inference times. To alleviate these restrictions, we explore how to arrive at dense semantic pixel labels given both the input image and an initial estimate of the output labels. We propose a parallel architecture that: 1) exploits the context information through a LabelPropagation network to propagate correct labels from nearby pixels to improve the object boundaries, 2) uses a LabelReplacement network to directly replace possibly erroneous, initial labels with new ones, and 3) combines the different intermediate results via a Fusion network to obtain the final per-pixel label. We experimentally validate our approach on two different datasets for the semantic segmentation and face parsing tasks respectively, where we show improvements over the state-of-the-art. We also provide both a quantitative and qualitative analysis of the generated results.



### Unsupervised Feature Learning for Audio Analysis
- **Arxiv ID**: http://arxiv.org/abs/1712.03835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03835v1)
- **Published**: 2017-12-11 15:25:01+00:00
- **Updated**: 2017-12-11 15:25:01+00:00
- **Authors**: Matthias Meyer, Jan Beutel, Lothar Thiele
- **Comment**: Presented at the 5th International Conference on Learning
  Representations (ICLR) 2017, Workshop Track, Toulon, France
- **Journal**: None
- **Summary**: Identifying acoustic events from a continuously streaming audio source is of interest for many applications including environmental monitoring for basic research. In this scenario neither different event classes are known nor what distinguishes one class from another. Therefore, an unsupervised feature learning method for exploration of audio data is presented in this paper. It incorporates the two following novel contributions: First, an audio frame predictor based on a Convolutional LSTM autoencoder is demonstrated, which is used for unsupervised feature extraction. Second, a training method for autoencoders is presented, which leads to distinct features by amplifying event similarities. In comparison to standard approaches, the features extracted from the audio frame predictor trained with the novel approach show 13 % better results when used with a classifier and 36 % better results when used for clustering.



### Using a single RGB frame for real time 3D hand pose estimation in the wild
- **Arxiv ID**: http://arxiv.org/abs/1712.03866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03866v1)
- **Published**: 2017-12-11 16:16:05+00:00
- **Updated**: 2017-12-11 16:16:05+00:00
- **Authors**: Paschalis Panteleris, Iason Oikonomidis, Antonis Argyros
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for the real-time estimation of the full 3D pose of one or more human hands using a single commodity RGB camera. Recent work in the area has displayed impressive progress using RGBD input. However, since the introduction of RGBD sensors, there has been little progress for the case of monocular color input. We capitalize on the latest advancements of deep learning, combining them with the power of generative hand pose estimation techniques to achieve real-time monocular 3D hand pose estimation in unrestricted scenarios. More specifically, given an RGB image and the relevant camera calibration information, we employ a state-of-the-art detector to localize hands. Given a crop of a hand in the image, we run the pretrained network of OpenPose for hands to estimate the 2D location of hand joints. Finally, non-linear least-squares minimization fits a 3D model of the hand to the estimated 2D joint positions, recovering the 3D hand pose. Extensive experimental results provide comparison to the state of the art as well as qualitative assessment of the method in the wild.



### Generalized Zero-Shot Learning via Synthesized Examples
- **Arxiv ID**: http://arxiv.org/abs/1712.03878v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.03878v5)
- **Published**: 2017-12-11 16:44:12+00:00
- **Updated**: 2018-06-12 00:13:53+00:00
- **Authors**: Vinay Kumar Verma, Gundeep Arora, Ashish Mishra, Piyush Rai
- **Comment**: Accepted in CVPR'18
- **Journal**: None
- **Summary**: We present a generative framework for generalized zero-shot learning where the training and test classes are not necessarily disjoint. Built upon a variational autoencoder based architecture, consisting of a probabilistic encoder and a probabilistic conditional decoder, our model can generate novel exemplars from seen/unseen classes, given their respective class attributes. These exemplars can subsequently be used to train any off-the-shelf classification model. One of the key aspects of our encoder-decoder architecture is a feedback-driven mechanism in which a discriminator (a multivariate regressor) learns to map the generated exemplars to the corresponding class attribute vectors, leading to an improved generator. Our model's ability to generate and leverage examples from unseen classes to train the classification model naturally helps to mitigate the bias towards predicting seen classes in generalized zero-shot learning settings. Through a comprehensive set of experiments, we show that our model outperforms several state-of-the-art methods, on several benchmark datasets, for both standard as well as generalized zero-shot learning.



### Learning Modality-Invariant Representations for Speech and Images
- **Arxiv ID**: http://arxiv.org/abs/1712.03897v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.03897v1)
- **Published**: 2017-12-11 17:18:34+00:00
- **Updated**: 2017-12-11 17:18:34+00:00
- **Authors**: Kenneth Leidal, David Harwath, James Glass
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore the unsupervised learning of a semantic embedding space for co-occurring sensory inputs. Specifically, we focus on the task of learning a semantic vector space for both spoken and handwritten digits using the TIDIGITs and MNIST datasets. Current techniques encode image and audio/textual inputs directly to semantic embeddings. In contrast, our technique maps an input to the mean and log variance vectors of a diagonal Gaussian from which sample semantic embeddings are drawn. In addition to encouraging semantic similarity between co-occurring inputs,our loss function includes a regularization term borrowed from variational autoencoders (VAEs) which drives the posterior distributions over embeddings to be unit Gaussian. We can use this regularization term to filter out modality information while preserving semantic information. We speculate this technique may be more broadly applicable to other areas of cross-modality/domain information retrieval and transfer learning.



### Feature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images
- **Arxiv ID**: http://arxiv.org/abs/1712.03904v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03904v2)
- **Published**: 2017-12-11 17:27:49+00:00
- **Updated**: 2018-03-26 15:56:40+00:00
- **Authors**: Mahdi Rad, Markus Oberweger, Vincent Lepetit
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: We propose a simple and efficient method for exploiting synthetic images when training a Deep Network to predict a 3D pose from an image. The ability of using synthetic images for training a Deep Network is extremely valuable as it is easy to create a virtually infinite training set made of such images, while capturing and annotating real images can be very cumbersome. However, synthetic images do not resemble real images exactly, and using them for training can result in suboptimal performance. It was recently shown that for exemplar-based approaches, it is possible to learn a mapping from the exemplar representations of real images to the exemplar representations of synthetic images. In this paper, we show that this approach is more general, and that a network can also be applied after the mapping to infer a 3D pose: At run time, given a real image of the target object, we first compute the features for the image, map them to the feature space of synthetic images, and finally use the resulting features as input to another network which predicts the 3D pose. Since this network can be trained very effectively by using synthetic images, it performs very well in practice, and inference is faster and more accurate than with an exemplar-based approach. We demonstrate our approach on the LINEMOD dataset for 3D object pose estimation from color images, and the NYU dataset for 3D hand pose estimation from depth maps. We show that it allows us to outperform the state-of-the-art on both datasets.



### Depth-Based 3D Hand Pose Estimation: From Current Achievements to Future Goals
- **Arxiv ID**: http://arxiv.org/abs/1712.03917v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03917v2)
- **Published**: 2017-12-11 17:55:19+00:00
- **Updated**: 2018-03-29 14:31:37+00:00
- **Authors**: Shanxin Yuan, Guillermo Garcia-Hernando, Bjorn Stenger, Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee, Pavlo Molchanov, Jan Kautz, Sina Honari, Liuhao Ge, Junsong Yuan, Xinghao Chen, Guijin Wang, Fan Yang, Kai Akiyama, Yang Wu, Qingfu Wan, Meysam Madadi, Sergio Escalera, Shile Li, Dongheui Lee, Iason Oikonomidis, Antonis Argyros, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we strive to answer two questions: What is the current state of 3D hand pose estimation from depth images? And, what are the next challenges that need to be tackled? Following the successful Hands In the Million Challenge (HIM2017), we investigate the top 10 state-of-the-art methods on three tasks: single frame 3D pose estimation, 3D hand tracking, and hand pose estimation during object interaction. We analyze the performance of different CNN structures with regard to hand shape, joint visibility, view point and articulation distributions. Our findings include: (1) isolated 3D hand pose estimation achieves low mean errors (10 mm) in the view point range of [70, 120] degrees, but it is far from being solved for extreme view points; (2) 3D volumetric representations outperform 2D CNNs, better capturing the spatial structure of the depth data; (3) Discriminative methods still generalize poorly to unseen hand shapes; (4) While joint occlusions pose a challenge for most methods, explicit modeling of structure constraints can significantly narrow the gap between errors on visible and occluded joints.



### MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments
- **Arxiv ID**: http://arxiv.org/abs/1712.03931v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1712.03931v1)
- **Published**: 2017-12-11 18:24:58+00:00
- **Updated**: 2017-12-11 18:24:58+00:00
- **Authors**: Manolis Savva, Angel X. Chang, Alexey Dosovitskiy, Thomas Funkhouser, Vladlen Koltun
- **Comment**: MINOS is a simulator designed to support research on end-to-end
  navigation
- **Journal**: None
- **Summary**: We present MINOS, a simulator designed to support the development of multisensory models for goal-directed navigation in complex indoor environments. The simulator leverages large datasets of complex 3D environments and supports flexible configuration of multimodal sensor suites. We use MINOS to benchmark deep-learning-based navigation methods, to analyze the influence of environmental complexity on navigation performance, and to carry out a controlled study of multimodality in sensorimotor learning. The experiments show that current deep reinforcement learning approaches fail in large realistic environments. The experiments also indicate that multimodality is beneficial in learning to navigate cluttered scenes. MINOS is released open-source to the research community at http://minosworld.org . A video that shows MINOS can be found at https://youtu.be/c0mL9K64q84



### StrassenNets: Deep Learning with a Multiplication Budget
- **Arxiv ID**: http://arxiv.org/abs/1712.03942v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.03942v3)
- **Published**: 2017-12-11 18:49:07+00:00
- **Updated**: 2018-06-08 10:59:23+00:00
- **Authors**: Michael Tschannen, Aran Khanna, Anima Anandkumar
- **Comment**: ICML 2018. Code available at https://github.com/mitscha/strassennets
- **Journal**: None
- **Summary**: A large fraction of the arithmetic operations required to evaluate deep neural networks (DNNs) consists of matrix multiplications, in both convolution and fully connected layers. We perform end-to-end learning of low-cost approximations of matrix multiplications in DNN layers by casting matrix multiplications as 2-layer sum-product networks (SPNs) (arithmetic circuits) and learning their (ternary) edge weights from data. The SPNs disentangle multiplication and addition operations and enable us to impose a budget on the number of multiplication operations. Combining our method with knowledge distillation and applying it to image classification DNNs (trained on ImageNet) and language modeling DNNs (using LSTMs), we obtain a first-of-a-kind reduction in number of multiplications (over 99.5%) while maintaining the predictive performance of the full-precision models. Finally, we demonstrate that the proposed framework is able to rediscover Strassen's matrix multiplication algorithm, learning to multiply $2 \times 2$ matrices using only 7 multiplications instead of 8.



### Eye In-Painting with Exemplar Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.03999v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.03999v1)
- **Published**: 2017-12-11 19:40:55+00:00
- **Updated**: 2017-12-11 19:40:55+00:00
- **Authors**: Brian Dolhansky, Cristian Canton Ferrer
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel approach to in-painting where the identity of the object to remove or change is preserved and accounted for at inference time: Exemplar GANs (ExGANs). ExGANs are a type of conditional GAN that utilize exemplar information to produce high-quality, personalized in painting results. We propose using exemplar information in the form of a reference image of the region to in-paint, or a perceptual code describing that object. Unlike previous conditional GAN formulations, this extra information can be inserted at multiple points within the adversarial network, thus increasing its descriptive power. We show that ExGANs can produce photo-realistic personalized in-painting results that are both perceptually and semantically plausible by applying them to the task of closed to-open eye in-painting in natural pictures. A new benchmark dataset is also introduced for the task of eye in-painting for future comparisons.



### Training Ensembles to Detect Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1712.04006v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.04006v1)
- **Published**: 2017-12-11 20:30:11+00:00
- **Updated**: 2017-12-11 20:30:11+00:00
- **Authors**: Alexander Bagnall, Razvan Bunescu, Gordon Stewart
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new ensemble method for detecting and classifying adversarial examples generated by state-of-the-art attacks, including DeepFool and C&W. Our method works by training the members of an ensemble to have low classification error on random benign examples while simultaneously minimizing agreement on examples outside the training distribution. We evaluate on both MNIST and CIFAR-10, against oblivious and both white- and black-box adversaries.



### Investigating the Impact of Data Volume and Domain Similarity on Transfer Learning Applications
- **Arxiv ID**: http://arxiv.org/abs/1712.04008v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1712.04008v4)
- **Published**: 2017-12-11 20:30:44+00:00
- **Updated**: 2018-05-30 19:35:08+00:00
- **Authors**: Michael Bernico, Yuntao Li, Dingchao Zhang
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Transfer learning allows practitioners to recognize and apply knowledge learned in previous tasks (source task) to new tasks or new domains (target task), which share some commonality. The two important factors impacting the performance of transfer learning models are: (a) the size of the target dataset, and (b) the similarity in distribution between source and target domains. Thus far, there has been little investigation into just how important these factors are. In this paper, we investigate the impact of target dataset size and source/target domain similarity on model performance through a series of experiments. We find that more data is always beneficial, and model performance improves linearly with the log of data size, until we are out of data. As source/target domains differ, more data is required and fine tuning will render better performance than feature extraction. When source/target domains are similar and data size is small, fine tuning and feature extraction renders equivalent performance. Our hope is that by beginning this quantitative investigation on the effect of data volume and domain similarity in transfer learning we might inspire others to explore the significance of data in developing more accurate statistical models.



### Deep metric learning for multi-labelled radiographs
- **Arxiv ID**: http://arxiv.org/abs/1712.07682v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.07682v1)
- **Published**: 2017-12-11 20:55:08+00:00
- **Updated**: 2017-12-11 20:55:08+00:00
- **Authors**: Mauro Annarumma, Giovanni Montana
- **Comment**: SAC 2018
- **Journal**: None
- **Summary**: Many radiological studies can reveal the presence of several co-existing abnormalities, each one represented by a distinct visual pattern. In this article we address the problem of learning a distance metric for plain radiographs that captures a notion of "radiological similarity": two chest radiographs are considered to be similar if they share similar abnormalities. Deep convolutional neural networks (DCNs) are used to learn a low-dimensional embedding for the radiographs that is equipped with the desired metric. Two loss functions are proposed to deal with multi-labelled images and potentially noisy labels. We report on a large-scale study involving over 745,000 chest radiographs whose labels were automatically extracted from free-text radiological reports through a natural language processing system. Using 4,500 validated exams, we demonstrate that the methodology performs satisfactorily on clustering and image retrieval tasks. Remarkably, the learned metric separates normal exams from those having radiological abnormalities.



### Character-Based Handwritten Text Transcription with Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.04046v3
- **DOI**: 10.1007/s00521-021-05813-1
- **Categories**: **cs.CV**, cs.CL, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.04046v3)
- **Published**: 2017-12-11 21:57:03+00:00
- **Updated**: 2021-02-24 17:00:03+00:00
- **Authors**: Jason Poulos, Rafael Valle
- **Comment**: None
- **Journal**: Neural Comput. & Applic., 33(16), 10563-10573 (2021)
- **Summary**: The paper approaches the task of handwritten text recognition (HTR) with attentional encoder-decoder networks trained on sequences of characters, rather than words. We experiment on lines of text from popular handwriting datasets and compare different activation functions for the attention mechanism used for aligning image pixels and target characters. We find that softmax attention focuses heavily on individual characters, while sigmoid attention focuses on multiple characters at each step of the decoding. When the sequence alignment is one-to-one, softmax attention is able to learn a more precise alignment at each step of the decoding, whereas the alignment generated by sigmoid attention is much less precise. When a linear function is used to obtain attention weights, the model predicts a character by looking at the entire sequence of characters and performs poorly because it lacks a precise alignment between the source and target. Future research may explore HTR in natural scene images, since the model is capable of transcribing handwritten text without the need for producing segmentations or bounding boxes of text in images.



