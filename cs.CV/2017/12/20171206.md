# Arxiv Papers in cs.CV on 2017-12-06
### Deep Anticipation: Light Weight Intelligent Mobile Sensing in IoT by Recurrent Architecture
- **Arxiv ID**: http://arxiv.org/abs/1801.01444v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01444v2)
- **Published**: 2017-12-06 02:45:04+00:00
- **Updated**: 2018-10-15 20:34:00+00:00
- **Authors**: Guang Chen, Shu Liu, Kejia Ren, Zhongnan Qu, Changhong Fu, Gereon Hinz, Alois Knoll
- **Comment**: 7 pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: The rapid growth of IoT era is shaping the future of mobile services. Advanced communication technology enables a heterogeneous connectivity where mobile devices broadcast information to everything. Mobile applications such as robotics and vehicles connecting to cloud and surroundings transfer the short-range on-board sensor perception system to long-range mobile-sensing perception system. However, the mobile sensing perception brings new challenges for how to efficiently analyze and intelligently interpret the deluge of IoT data in mission- critical services. In this article, we model the challenges as latency, packet loss and measurement noise which severely deteriorate the reliability and quality of IoT data. We integrate the artificial intelligence into IoT to tackle these challenges. We propose a novel architecture that leverages recurrent neural networks (RNN) and Kalman filtering to anticipate motions and interac- tions between objects. The basic idea is to learn environment dynamics by recurrent networks. To improve the robustness of IoT communication, we use the idea of Kalman filtering and deploy a prediction and correction step. In this way, the architecture learns to develop a biased belief between prediction and measurement in the different situation. We demonstrate our approach with synthetic and real-world datasets with noise that mimics the challenges of IoT communications. Our method brings a new level of IoT intelligence. It is also lightweight compared to other state-of-the-art convolutional recurrent architecture and is ideally suitable for the resource-limited mobile applications.



### Towards Recovery of Conditional Vectors from Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.01833v1
- **DOI**: 10.1016/j.patrec.2019.02.020
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.01833v1)
- **Published**: 2017-12-06 03:40:31+00:00
- **Updated**: 2017-12-06 03:40:31+00:00
- **Authors**: Sihao Ding, Andreas Wallin
- **Comment**: Under consideration for Pattern Recognition Letters, 11 pages
- **Journal**: Pattern Recognition Letters, vol. 122, pp. 66-72, 1 May 2019
- **Summary**: A conditional Generative Adversarial Network allows for generating samples conditioned on certain external information. Being able to recover latent and conditional vectors from a condi- tional GAN can be potentially valuable in various applications, ranging from image manipulation for entertaining purposes to diagnosis of the neural networks for security purposes. In this work, we show that it is possible to recover both latent and conditional vectors from generated images given the generator of a conditional generative adversarial network. Such a recovery is not trivial due to the often multi-layered non-linearity of deep neural networks. Furthermore, the effect of such recovery applied on real natural images are investigated. We discovered that there exists a gap between the recovery performance on generated and real images, which we believe comes from the difference between generated data distribution and real data distribution. Experiments are conducted to evaluate the recovered conditional vectors and the reconstructed images from these recovered vectors quantitatively and qualitatively, showing promising results.



### AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.02029v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, stat.ML, 68T05,, I.2.6; I.5.0
- **Links**: [PDF](http://arxiv.org/pdf/1712.02029v2)
- **Published**: 2017-12-06 04:19:14+00:00
- **Updated**: 2018-02-14 04:26:45+00:00
- **Authors**: Aditya Devarakonda, Maxim Naumov, Michael Garland
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Training deep neural networks with Stochastic Gradient Descent, or its variants, requires careful choice of both learning rate and batch size. While smaller batch sizes generally converge in fewer training epochs, larger batch sizes offer more parallelism and hence better computational efficiency. We have developed a new training approach that, rather than statically choosing a single batch size for all epochs, adaptively increases the batch size during the training process. Our method delivers the convergence rate of small batch sizes while achieving performance similar to large batch sizes. We analyse our approach using the standard AlexNet, ResNet, and VGG networks operating on the popular CIFAR-10, CIFAR-100, and ImageNet datasets. Our results demonstrate that learning with adaptive batch sizes can improve performance by factors of up to 6.25 on 4 NVIDIA Tesla P100 GPUs while changing accuracy by less than 1% relative to training with fixed batch sizes.



### Learning Semantic Concepts and Order for Image and Sentence Matching
- **Arxiv ID**: http://arxiv.org/abs/1712.02036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02036v1)
- **Published**: 2017-12-06 04:36:40+00:00
- **Updated**: 2017-12-06 04:36:40+00:00
- **Authors**: Yan Huang, Qi Wu, Liang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Image and sentence matching has made great progress recently, but it remains challenging due to the large visual-semantic discrepancy. This mainly arises from that the representation of pixel-level image usually lacks of high-level semantic information as in its matched sentence. In this work, we propose a semantic-enhanced image and sentence matching model, which can improve the image representation by learning semantic concepts and then organizing them in a correct semantic order. Given an image, we first use a multi-regional multi-label CNN to predict its semantic concepts, including objects, properties, actions, etc. Then, considering that different orders of semantic concepts lead to diverse semantic meanings, we use a context-gated sentence generation scheme for semantic order learning. It simultaneously uses the image global context containing concept relations as reference and the groundtruth semantic order in the matched sentence as supervision. After obtaining the improved image representation, we learn the sentence representation with a conventional LSTM, and then jointly perform image and sentence matching and sentence generation for model learning. Extensive experiments demonstrate the effectiveness of our learned semantic concepts and order, by achieving the state-of-the-art results on two public benchmark datasets.



### Saliency Preservation in Low-Resolution Grayscale Images
- **Arxiv ID**: http://arxiv.org/abs/1712.02048v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02048v3)
- **Published**: 2017-12-06 05:39:13+00:00
- **Updated**: 2018-03-27 11:06:31+00:00
- **Authors**: Shivanthan A. C. Yohanandan, Adrian G. Dyer, Dacheng Tao, Andy Song
- **Comment**: 17 pages, 8 figures
- **Journal**: None
- **Summary**: Visual salience detection originated over 500 million years ago and is one of nature's most efficient mechanisms. In contrast, many state-of-the-art computational saliency models are complex and inefficient. Most saliency models process high-resolution color (HC) images; however, insights into the evolutionary origins of visual salience detection suggest that achromatic low-resolution vision is essential to its speed and efficiency. Previous studies showed that low-resolution color and high-resolution grayscale images preserve saliency information. However, to our knowledge, no one has investigated whether saliency is preserved in low-resolution grayscale (LG) images. In this study, we explain the biological and computational motivation for LG, and show, through a range of human eye-tracking and computational modeling experiments, that saliency information is preserved in LG images. Moreover, we show that using LG images leads to significant speedups in model training and detection times and conclude by proposing LG images for fast and efficient salience detection.



### Unsupervised Multi-Domain Image Translation with Domain-Specific Encoders/Decoders
- **Arxiv ID**: http://arxiv.org/abs/1712.02050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02050v1)
- **Published**: 2017-12-06 06:08:31+00:00
- **Updated**: 2017-12-06 06:08:31+00:00
- **Authors**: Le Hui, Xiang Li, Jiaxin Chen, Hongliang He, Chen gong, Jian Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised Image-to-Image Translation achieves spectacularly advanced developments nowadays. However, recent approaches mainly focus on one model with two domains, which may face heavy burdens with large cost of $O(n^2)$ training time and model parameters, under such a requirement that $n$ domains are freely transferred to each other in a general setting. To address this problem, we propose a novel and unified framework named Domain-Bank, which consists of a global shared auto-encoder and $n$ domain-specific encoders/decoders, assuming that a universal shared-latent sapce can be projected. Thus, we yield $O(n)$ complexity in model parameters along with a huge reduction of the time budgets. Besides the high efficiency, we show the comparable (or even better) image translation results over state-of-the-arts on various challenging unsupervised image translation tasks, including face image translation, fashion-clothes translation and painting style translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on digit benchmark datasets. Further, thanks to the explicit representation of the domain-specific decoders as well as the universal shared-latent space, it also enables us to conduct incremental learning to add a new domain encoder/decoder. Linear combination of different domains' representations is also obtained by fusing the corresponding decoders.



### Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1712.02051v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02051v2)
- **Published**: 2017-12-06 06:08:59+00:00
- **Updated**: 2018-05-22 01:56:51+00:00
- **Authors**: Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Cho-Jui Hsieh
- **Comment**: Accepted by 56th Annual Meeting of the Association for Computational
  Linguistics (ACL 2018). Hongge Chen and Huan Zhang contribute equally to this
  work
- **Journal**: None
- **Summary**: Visual language grounding is widely studied in modern neural image captioning systems, which typically adopts an encoder-decoder framework consisting of two principal components: a convolutional neural network (CNN) for image feature extraction and a recurrent neural network (RNN) for language caption generation. To study the robustness of language grounding to adversarial perturbations in machine vision and perception, we propose Show-and-Fool, a novel algorithm for crafting adversarial examples in neural image captioning. The proposed algorithm provides two evaluation approaches, which check whether neural image captioning systems can be mislead to output some randomly chosen captions or keywords. Our extensive experiments show that our algorithm can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords, and the adversarial examples can be made highly transferable to other image captioning systems. Consequently, our approach leads to new robustness implications of neural image captioning and novel insights in visual language grounding.



### Automatic Segmentation and Overall Survival Prediction in Gliomas using Fully Convolutional Neural Network and Texture Analysis
- **Arxiv ID**: http://arxiv.org/abs/1712.02066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02066v1)
- **Published**: 2017-12-06 07:45:38+00:00
- **Updated**: 2017-12-06 07:45:38+00:00
- **Authors**: Varghese Alex, Mohammed Safwan, Ganapathy Krishnamurthi
- **Comment**: 10 Pages, 6 Figures
- **Journal**: None
- **Summary**: In this paper, we use a fully convolutional neural network (FCNN) for the segmentation of gliomas from Magnetic Resonance Images (MRI). A fully automatic, voxel based classification was achieved by training a 23 layer deep FCNN on 2-D slices extracted from patient volumes. The network was trained on slices extracted from 130 patients and validated on 50 patients. For the task of survival prediction, texture and shape based features were extracted from T1 post contrast volume to train an XGBoost regressor. On BraTS 2017 validation set, the proposed scheme achieved a mean whole tumor, tumor core and active dice score of 0.83, 0.69 and 0.69 respectively and an accuracy of 52% for the overall survival prediction.



### Separating Reflection and Transmission Images in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1712.02099v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02099v2)
- **Published**: 2017-12-06 09:31:27+00:00
- **Updated**: 2018-08-16 14:12:06+00:00
- **Authors**: Patrick Wieschollek, Orazio Gallo, Jinwei Gu, Jan Kautz
- **Comment**: accepted at ECCV 2018
- **Journal**: None
- **Summary**: The reflections caused by common semi-reflectors, such as glass windows, can impact the performance of computer vision algorithms. State-of-the-art methods can remove reflections on synthetic data and in controlled scenarios. However, they are based on strong assumptions and do not generalize well to real-world images. Contrary to a common misconception, real-world images are challenging even when polarization information is used. We present a deep learning approach to separate the reflected and the transmitted components of the recorded irradiance, which explicitly uses the polarization properties of light. To train it, we introduce an accurate synthetic data generation pipeline, which simulates realistic reflections, including those generated by curved and non-ideal surfaces, non-static scenes, and high-dynamic-range scenes.



### Guided Labeling using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.02154v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.02154v1)
- **Published**: 2017-12-06 12:18:24+00:00
- **Updated**: 2017-12-06 12:18:24+00:00
- **Authors**: Sebastian Stabinger, Antonio Rodriguez-Sanchez
- **Comment**: Under review for CVPR2018
- **Journal**: None
- **Summary**: Over the last couple of years, deep learning and especially convolutional neural networks have become one of the work horses of computer vision. One limiting factor for the applicability of supervised deep learning to more areas is the need for large, manually labeled datasets. In this paper we propose an easy to implement method we call guided labeling, which automatically determines which samples from an unlabeled dataset should be labeled. We show that using this procedure, the amount of samples that need to be labeled is reduced considerably in comparison to labeling images arbitrarily.



### Detecting Curve Text in the Wild: New Dataset and New Solution
- **Arxiv ID**: http://arxiv.org/abs/1712.02170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02170v1)
- **Published**: 2017-12-06 13:02:43+00:00
- **Updated**: 2017-12-06 13:02:43+00:00
- **Authors**: Liu Yuliang, Jin Lianwen, Zhang Shuaitao, Zhang Sheng
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Scene text detection has been made great progress in recent years. The detection manners are evolving from axis-aligned rectangle to rotated rectangle and further to quadrangle. However, current datasets contain very little curve text, which can be widely observed in scene images such as signboard, product name and so on. To raise the concerns of reading curve text in the wild, in this paper, we construct a curve text dataset named CTW1500, which includes over 10k text annotations in 1,500 images (1000 for training and 500 for testing). Based on this dataset, we pioneering propose a polygon based curve text detector (CTD) which can directly detect curve text without empirical combination. Moreover, by seamlessly integrating the recurrent transverse and longitudinal offset connection (TLOC), the proposed method can be end-to-end trainable to learn the inherent connection among the position offsets. This allows the CTD to explore context information instead of predicting points independently, resulting in more smooth and accurate detection. We also propose two simple but effective post-processing methods named non-polygon suppress (NPS) and polygonal non-maximum suppression (PNMS) to further improve the detection accuracy. Furthermore, the proposed approach in this paper is designed in an universal manner, which can also be trained with rectangular or quadrilateral bounding boxes without extra efforts. Experimental results on CTW-1500 demonstrate our method with only a light backbone can outperform state-of-the-art methods with a large margin. By evaluating only in the curve or non-curve subset, the CTD + TLOC can still achieve the best results. Code is available at https://github.com/Yuliang-Liu/Curve-Text-Detector.



### Learning to Write Stylized Chinese Characters by Reading a Handful of Examples
- **Arxiv ID**: http://arxiv.org/abs/1712.06424v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.06424v3)
- **Published**: 2017-12-06 13:33:51+00:00
- **Updated**: 2018-06-18 11:17:09+00:00
- **Authors**: Danyang Sun, Tongzheng Ren, Chongxun Li, Hang Su, Jun Zhu
- **Comment**: Accepted by IJCAI 2018
- **Journal**: None
- **Summary**: Automatically writing stylized Chinese characters is an attractive yet challenging task due to its wide applicabilities. In this paper, we propose a novel framework named Style-Aware Variational Auto-Encoder (SA-VAE) to flexibly generate Chinese characters. Specifically, we propose to capture the different characteristics of a Chinese character by disentangling the latent features into content-related and style-related components. Considering of the complex shapes and structures, we incorporate the structure information as prior knowledge into our framework to guide the generation. Our framework shows a powerful one-shot/low-shot generalization ability by inferring the style component given a character with unseen style. To the best of our knowledge, this is the first attempt to learn to write new-style Chinese characters by observing only one or a few examples. Extensive experiments demonstrate its effectiveness in generating different stylized Chinese characters by fusing the feature vectors corresponding to different contents and styles, which is of significant importance in real-world applications.



### Beyond the Pixel-Wise Loss for Topology-Aware Delineation
- **Arxiv ID**: http://arxiv.org/abs/1712.02190v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02190v1)
- **Published**: 2017-12-06 14:03:51+00:00
- **Updated**: 2017-12-06 14:03:51+00:00
- **Authors**: Agata Mosinska, Pablo Marquez-Neila, Mateusz Kozinski, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Delineation of curvilinear structures is an important problem in Computer Vision with multiple practical applications. With the advent of Deep Learning, many current approaches on automatic delineation have focused on finding more powerful deep architectures, but have continued using the habitual pixel-wise losses such as binary cross-entropy. In this paper we claim that pixel-wise losses alone are unsuitable for this problem because of their inability to reflect the topological impact of mistakes in the final prediction. We propose a new loss term that is aware of the higher-order topological features of linear structures. We also introduce a refinement pipeline that iteratively applies the same model over the previous delineation to refine the predictions at each step while keeping the number of parameters and the complexity of the model constant.   When combined with the standard pixel-wise loss, both our new loss term and our iterative refinement boost the quality of the predicted delineations, in some cases almost doubling the accuracy as compared to the same classifier trained with the binary cross-entropy alone. We show that our approach outperforms state-of-the-art methods on a wide range of data, from microscopy to aerial images.



### Pose-Normalized Image Generation for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1712.02225v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.02225v6)
- **Published**: 2017-12-06 15:18:53+00:00
- **Updated**: 2018-04-25 05:57:05+00:00
- **Authors**: Xuelin Qian, Yanwei Fu, Tao Xiang, Wenxuan Wang, Jie Qiu, Yang Wu, Yu-Gang Jiang, Xiangyang Xue
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Person Re-identification (re-id) faces two major challenges: the lack of cross-view paired training data and learning discriminative identity-sensitive and view-invariant features in the presence of large pose variations. In this work, we address both problems by proposing a novel deep person image generation model for synthesizing realistic person images conditional on the pose. The model is based on a generative adversarial network (GAN) designed specifically for pose normalization in re-id, thus termed pose-normalization GAN (PN-GAN). With the synthesized images, we can learn a new type of deep re-id feature free of the influence of pose variations. We show that this feature is strong on its own and complementary to features learned with the original images. Importantly, under the transfer learning setting, we show that our model generalizes well to any new re-id dataset without the need for collecting any training data for model fine-tuning. The model thus has the potential to make re-id model truly scalable.



### ObamaNet: Photo-realistic lip-sync from text
- **Arxiv ID**: http://arxiv.org/abs/1801.01442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01442v1)
- **Published**: 2017-12-06 16:18:31+00:00
- **Updated**: 2017-12-06 16:18:31+00:00
- **Authors**: Rithesh Kumar, Jose Sotelo, Kundan Kumar, Alexandre de Brebisson, Yoshua Bengio
- **Comment**: None
- **Journal**: None
- **Summary**: We present ObamaNet, the first architecture that generates both audio and synchronized photo-realistic lip-sync videos from any new text. Contrary to other published lip-sync approaches, ours is only composed of fully trainable neural modules and does not rely on any traditional computer graphics methods. More precisely, we use three main modules: a text-to-speech network based on Char2Wav, a time-delayed LSTM to generate mouth-keypoints synced to the audio, and a network based on Pix2Pix to generate the video frames conditioned on the keypoints.



### Stretching Domain Adaptation: How far is too far?
- **Arxiv ID**: http://arxiv.org/abs/1712.02286v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02286v2)
- **Published**: 2017-12-06 17:03:07+00:00
- **Updated**: 2018-11-29 20:53:17+00:00
- **Authors**: Yunhan Zhao, Haider Ali, Rene Vidal
- **Comment**: None
- **Journal**: None
- **Summary**: While deep learning has led to significant advances in visual recognition over the past few years, such advances often require a lot of annotated data. Unsupervised domain adaptation has emerged as an alternative approach that does not require as much annotated data, prior evaluations of domain adaptation approaches have been limited to relatively similar datasets, e.g source and target domains are samples captured by different cameras. A new data suite is proposed that comprehensively evaluates cross-modality domain adaptation problems. This work pushes the limit of unsupervised domain adaptation through an in-depth evaluation of several state of the art methods on benchmark datasets and the new dataset suite. We also propose a new domain adaptation network called "Deep MagNet" that effectively transfers knowledge for cross-modality domain adaptation problems. Deep Magnet achieves state of the art performance on two benchmark datasets. More importantly, the proposed method shows consistent improvements in performance on the newly proposed dataset suite.



### Joint 3D Proposal Generation and Object Detection from View Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1712.02294v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02294v4)
- **Published**: 2017-12-06 17:20:21+00:00
- **Updated**: 2018-07-12 14:11:40+00:00
- **Authors**: Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, Steven Waslander
- **Comment**: For any inquiries contact aharakeh(at)uwaterloo(dot)ca
- **Journal**: None
- **Summary**: We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is at: https://github.com/kujason/avod



### From Lifestyle Vlogs to Everyday Interactions
- **Arxiv ID**: http://arxiv.org/abs/1712.02310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02310v1)
- **Published**: 2017-12-06 18:07:57+00:00
- **Updated**: 2017-12-06 18:07:57+00:00
- **Authors**: David F. Fouhey, Wei-cheng Kuo, Alexei A. Efros, Jitendra Malik
- **Comment**: Project page at: http://people.eecs.berkeley.edu/~dfouhey/2017/VLOG/
- **Journal**: None
- **Summary**: A major stumbling block to progress in understanding basic human interactions, such as getting out of bed or opening a refrigerator, is lack of good training data. Most past efforts have gathered this data explicitly: starting with a laundry list of action labels, and then querying search engines for videos tagged with each label. In this work, we do the reverse and search implicitly: we start with a large collection of interaction-rich video data and then annotate and analyze it. We use Internet Lifestyle Vlogs as the source of surprisingly large and diverse interaction data. We show that by collecting the data first, we are able to achieve greater scale and far greater diversity in terms of actions and actors. Additionally, our data exposes biases built into common explicitly gathered data. We make sense of our data by analyzing the central component of interaction -- hands. We benchmark two tasks: identifying semantic object contact at the video level and non-semantic contact state at the frame level. We additionally demonstrate future prediction of hands.



### Burst Denoising with Kernel Prediction Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.02327v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02327v2)
- **Published**: 2017-12-06 18:50:28+00:00
- **Updated**: 2018-03-29 17:56:32+00:00
- **Authors**: Ben Mildenhall, Jonathan T. Barron, Jiawen Chen, Dillon Sharlet, Ren Ng, Robert Carroll
- **Comment**: To appear in CVPR 2018 (spotlight). Project page:
  http://people.eecs.berkeley.edu/~bmild/kpn/
- **Journal**: None
- **Summary**: We present a technique for jointly denoising bursts of images taken from a handheld camera. In particular, we propose a convolutional neural network architecture for predicting spatially varying kernels that can both align and denoise frames, a synthetic data generation approach based on a realistic noise formation model, and an optimization guided by an annealed loss function to avoid undesirable local minima. Our model matches or outperforms the state-of-the-art across a wide range of noise levels on both real and synthetic data.



### Generative Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1712.02328v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.02328v3)
- **Published**: 2017-12-06 18:52:12+00:00
- **Updated**: 2018-07-06 06:50:03+00:00
- **Authors**: Omid Poursaeed, Isay Katsman, Bicheng Gao, Serge Belongie
- **Comment**: CVPR 2018, camera-ready version
- **Journal**: None
- **Summary**: In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for both targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time.



### Controllable Top-down Feature Transformer
- **Arxiv ID**: http://arxiv.org/abs/1712.02400v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02400v4)
- **Published**: 2017-12-06 20:20:27+00:00
- **Updated**: 2018-11-04 01:31:30+00:00
- **Authors**: Zhiwei Jia, Haoshen Hong, Siyang Wang, Kwonjoon Lee, Zhuowen Tu
- **Comment**: None
- **Journal**: None
- **Summary**: We study the intrinsic transformation of feature maps across convolutional network layers with explicit top-down control. To this end, we develop top-down feature transformer (TFT), under controllable parameters, that are able to account for the hidden layer transformation while maintaining the overall consistency across layers. The learned generators capture the underlying feature transformation processes that are independent of particular training images. Our proposed TFT framework brings insights to and helps the understanding of, an important problem of studying the CNN internal feature representation and transformation under the top-down processes. In the case of spatial transformations, we demonstrate the significant advantage of TFT over existing data-driven approaches in building data-independent transformations. We also show that it can be adopted in other applications such as data augmentation and image style transfer.



### Deep Regionlets for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1712.02408v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02408v3)
- **Published**: 2017-12-06 21:05:21+00:00
- **Updated**: 2018-08-23 01:35:53+00:00
- **Authors**: Hongyu Xu, Xutao Lv, Xiaoyu Wang, Zhou Ren, Navaneeth Bodla, Rama Chellappa
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: In this paper, we propose a novel object detection framework named "Deep Regionlets" by establishing a bridge between deep neural networks and conventional detection schema for accurate generic object detection. Motivated by the abilities of regionlets for modeling object deformation and multiple aspect ratios, we incorporate regionlets into an end-to-end trainable deep learning framework. The deep regionlets framework consists of a region selection network and a deep regionlet learning module. Specifically, given a detection bounding box proposal, the region selection network provides guidance on where to select regions to learn the features from. The regionlet learning module focuses on local feature selection and transformation to alleviate local variations. To this end, we first realize non-rectangular region selection within the detection framework to accommodate variations in object appearance. Moreover, we design a "gating network" within the regionlet leaning module to enable soft regionlet selection and pooling. The Deep Regionlets framework is trained end-to-end without additional efforts. We perform ablation studies and conduct extensive experiments on the PASCAL VOC and Microsoft COCO datasets. The proposed framework outperforms state-of-the-art algorithms, such as RetinaNet and Mask R-CNN, even without additional segmentation labels.



### Tomographic Reconstruction using Global Statistical Prior
- **Arxiv ID**: http://arxiv.org/abs/1712.02423v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02423v1)
- **Published**: 2017-12-06 22:15:48+00:00
- **Updated**: 2017-12-06 22:15:48+00:00
- **Authors**: Preeti Gopal, Ritwick Chaudhry, Sharat Chandran, Imants Svalbe, Ajit Rajwade
- **Comment**: Published in The International Conference on Digital Image Computing:
  Techniques and Applications (DICTA), Sydney, Australia, 2017. The conference
  proceedings are not out yet. But the result can be seen here:
  http://dicta2017.dictaconference.org/fullprogram.html
- **Journal**: None
- **Summary**: Recent research in tomographic reconstruction is motivated by the need to efficiently recover detailed anatomy from limited measurements. One of the ways to compensate for the increasingly sparse sets of measurements is to exploit the information from templates, i.e., prior data available in the form of already reconstructed, structurally similar images. Towards this, previous work has exploited using a set of global and patch based dictionary priors. In this paper, we propose a global prior to improve both the speed and quality of tomographic reconstruction within a Compressive Sensing framework.   We choose a set of potential representative 2D images referred to as templates, to build an eigenspace; this is subsequently used to guide the iterative reconstruction of a similar slice from sparse acquisition data. Our experiments across a diverse range of datasets show that reconstruction using an appropriate global prior, apart from being faster, gives a much lower reconstruction error when compared to the state of the art.



