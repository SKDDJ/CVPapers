# Arxiv Papers in cs.CV on 2017-12-09
### IQA: Visual Question Answering in Interactive Environments
- **Arxiv ID**: http://arxiv.org/abs/1712.03316v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03316v3)
- **Published**: 2017-12-09 00:13:59+00:00
- **Updated**: 2018-09-06 17:05:18+00:00
- **Authors**: Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi
- **Comment**: Published in CVPR 2018
- **Journal**: None
- **Summary**: We introduce Interactive Question Answering (IQA), the task of answering questions that require an autonomous agent to interact with a dynamic visual environment. IQA presents the agent with a scene and a question, like: "Are there any apples in the fridge?" The agent must navigate around the scene, acquire visual understanding of scene elements, interact with objects (e.g. open refrigerators) and plan for a series of actions conditioned on the question. Popular reinforcement learning approaches with a single controller perform poorly on IQA owing to the large and diverse state space. We propose the Hierarchical Interactive Memory Network (HIMN), consisting of a factorized set of controllers, allowing the system to operate at multiple levels of temporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new dataset built upon AI2-THOR, a simulated photo-realistic environment of configurable indoor scenes with interactive objects (code and dataset available at https://github.com/danielgordon10/thor-iqa-cvpr-2018). IQUAD V1 has 75,000 questions, each paired with a unique scene configuration. Our experiments show that our proposed model outperforms popular single controller based methods on IQUAD V1. For sample questions and results, please view our video: https://youtu.be/pXd3C-1jr98



### Fine-Grained Object Recognition and Zero-Shot Learning in Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/1712.03323v1
- **DOI**: 10.1109/TGRS.2017.2754648
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03323v1)
- **Published**: 2017-12-09 00:44:39+00:00
- **Updated**: 2017-12-09 00:44:39+00:00
- **Authors**: Gencer Sumbul, Ramazan Gokberk Cinbis, Selim Aksoy
- **Comment**: G. Sumbul, R. G. Cinbis, S. Aksoy, "Fine-Grained Object Recognition
  and Zero-Shot Learning in Remote Sensing Imagery", IEEE Transactions on
  Geoscience and Remote Sensing (TGRS), in press, 2017
- **Journal**: None
- **Summary**: Fine-grained object recognition that aims to identify the type of an object among a large number of subcategories is an emerging application with the increasing resolution that exposes new details in image data. Traditional fully supervised algorithms fail to handle this problem where there is low between-class variance and high within-class variance for the classes of interest with small sample sizes. We study an even more extreme scenario named zero-shot learning (ZSL) in which no training example exists for some of the classes. ZSL aims to build a recognition model for new unseen categories by relating them to seen classes that were previously learned. We establish this relation by learning a compatibility function between image features extracted via a convolutional neural network and auxiliary information that describes the semantics of the classes of interest by using training samples from the seen classes. Then, we show how knowledge transfer can be performed for the unseen classes by maximizing this function during inference. We introduce a new data set that contains 40 different types of street trees in 1-ft spatial resolution aerial data, and evaluate the performance of this model with manually annotated attributes, a natural language model, and a scientific taxonomy as auxiliary information. The experiments show that the proposed model achieves 14.3% recognition accuracy for the classes with no training examples, which is significantly better than a random guess accuracy of 6.3% for 16 test classes, and three other ZSL algorithms.



### Bayesian Joint Matrix Decomposition for Data Integration with Heterogeneous Noise
- **Arxiv ID**: http://arxiv.org/abs/1712.03337v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.03337v1)
- **Published**: 2017-12-09 02:54:17+00:00
- **Updated**: 2017-12-09 02:54:17+00:00
- **Authors**: Chihao Zhang, Shihua Zhang
- **Comment**: 14 pages, 7 figures, 8 tables
- **Journal**: None
- **Summary**: Matrix decomposition is a popular and fundamental approach in machine learning and data mining. It has been successfully applied into various fields. Most matrix decomposition methods focus on decomposing a data matrix from one single source. However, it is common that data are from different sources with heterogeneous noise. A few of matrix decomposition methods have been extended for such multi-view data integration and pattern discovery. While only few methods were designed to consider the heterogeneity of noise in such multi-view data for data integration explicitly. To this end, we propose a joint matrix decomposition framework (BJMD), which models the heterogeneity of noise by Gaussian distribution in a Bayesian framework. We develop two algorithms to solve this model: one is a variational Bayesian inference algorithm, which makes full use of the posterior distribution; and another is a maximum a posterior algorithm, which is more scalable and can be easily paralleled. Extensive experiments on synthetic and real-world datasets demonstrate that BJMD considering the heterogeneity of noise is superior or competitive to the state-of-the-art methods.



### Geometry-Aware Learning of Maps for Camera Localization
- **Arxiv ID**: http://arxiv.org/abs/1712.03342v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03342v3)
- **Published**: 2017-12-09 05:26:57+00:00
- **Updated**: 2018-04-02 02:31:33+00:00
- **Authors**: Samarth Brahmbhatt, Jinwei Gu, Kihwan Kim, James Hays, Jan Kautz
- **Comment**: CVPR 2018 camera ready paper + supplementary material
- **Journal**: None
- **Summary**: Maps are a key component in image-based camera localization and visual SLAM systems: they are used to establish geometric constraints between images, correct drift in relative pose estimation, and relocalize cameras after lost tracking. The exact definitions of maps, however, are often application-specific and hand-crafted for different scenarios (e.g. 3D landmarks, lines, planes, bags of visual words). We propose to represent maps as a deep neural net called MapNet, which enables learning a data-driven map representation. Unlike prior work on learning maps, MapNet exploits cheap and ubiquitous sensory inputs like visual odometry and GPS in addition to images and fuses them together for camera localization. Geometric constraints expressed by these inputs, which have traditionally been used in bundle adjustment or pose-graph optimization, are formulated as loss terms in MapNet training and also used during inference. In addition to directly improving localization accuracy, this allows us to update the MapNet (i.e., maps) in a self-supervised manner using additional unlabeled video sequences from the scene. We also propose a novel parameterization for camera rotation which is better suited for deep-learning based camera pose regression. Experimental results on both the indoor 7-Scenes dataset and the outdoor Oxford RobotCar dataset show significant performance improvement over prior work. The MapNet project webpage is https://goo.gl/mRB3Au.



### Peephole: Predicting Network Performance Before Training
- **Arxiv ID**: http://arxiv.org/abs/1712.03351v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.03351v1)
- **Published**: 2017-12-09 07:50:27+00:00
- **Updated**: 2017-12-09 07:50:27+00:00
- **Authors**: Boyang Deng, Junjie Yan, Dahua Lin
- **Comment**: None
- **Journal**: None
- **Summary**: The quest for performant networks has been a significant force that drives the advancements of deep learning in recent years. While rewarding, improving network design has never been an easy journey. The large design space combined with the tremendous cost required for network training poses a major obstacle to this endeavor. In this work, we propose a new approach to this problem, namely, predicting the performance of a network before training, based on its architecture. Specifically, we develop a unified way to encode individual layers into vectors and bring them together to form an integrated description via LSTM. Taking advantage of the recurrent network's strong expressive power, this method can reliably predict the performances of various network architectures. Our empirical studies showed that it not only achieved accurate predictions but also produced consistent rankings across datasets -- a key desideratum in performance prediction.



### A Deep Recurrent Framework for Cleaning Motion Capture Data
- **Arxiv ID**: http://arxiv.org/abs/1712.03380v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.03380v1)
- **Published**: 2017-12-09 12:03:53+00:00
- **Updated**: 2017-12-09 12:03:53+00:00
- **Authors**: Utkarsh Mall, G. Roshan Lal, Siddhartha Chaudhuri, Parag Chaudhuri
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep, bidirectional, recurrent framework for cleaning noisy and incomplete motion capture data. It exploits temporal coherence and joint correlations to infer adaptive filters for each joint in each frame. A single model can be trained to denoise a heterogeneous mix of action types, under substantial amounts of noise. A signal that has both noise and gaps is preprocessed with a second bidirectional network that synthesizes missing frames from surrounding context. The approach handles a wide variety of noise types and long gaps, does not rely on knowledge of the noise distribution, and operates in a streaming setting. We validate our approach through extensive evaluations on noise both in joint angles and in joint positions, and show that it improves upon various alternatives.



### Noise Level Estimation for Overcomplete Dictionary Learning Based on Tight Asymptotic Bounds
- **Arxiv ID**: http://arxiv.org/abs/1712.03381v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.03381v1)
- **Published**: 2017-12-09 12:42:29+00:00
- **Updated**: 2017-12-09 12:42:29+00:00
- **Authors**: Rui Chen, Changshui Yang, Huizhu Jia, Xiaodong Xie
- **Comment**: None
- **Journal**: None
- **Summary**: In this letter, we address the problem of estimating Gaussian noise level from the trained dictionaries in update stage. We first provide rigorous statistical analysis on the eigenvalue distributions of a sample covariance matrix. Then we propose an interval-bounded estimator for noise variance in high dimensional setting. To this end, an effective estimation method for noise level is devised based on the boundness and asymptotic behavior of noise eigenvalue spectrum. The estimation performance of our method has been guaranteed both theoretically and empirically. The analysis and experiment results have demonstrated that the proposed algorithm can reliably infer true noise levels, and outperforms the relevant existing methods.



### Visual aesthetic analysis using deep neural network: model and techniques to increase accuracy without transfer learning
- **Arxiv ID**: http://arxiv.org/abs/1712.03382v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03382v4)
- **Published**: 2017-12-09 12:56:24+00:00
- **Updated**: 2018-01-31 05:15:23+00:00
- **Authors**: Muktabh Mayank Srivastava, Sonaal Kant
- **Comment**: Accepted at IEEE's 3rd International Conference for Convergence in
  Technology (I2CT) Pune - 7-8 April 2018
- **Journal**: None
- **Summary**: We train a deep Convolutional Neural Network (CNN) from scratch for visual aesthetic analysis in images and discuss techniques we adopt to improve the accuracy. We avoid the prevalent best transfer learning approaches of using pretrained weights to perform the task and train a model from scratch to get accuracy of 78.7% on AVA2 Dataset close to the best models available (85.6%). We further show that accuracy increases to 81.48% on increasing the training set by incremental 10 percentile of entire AVA dataset showing our algorithm gets better with more data.



### NAG: Network for Adversary Generation
- **Arxiv ID**: http://arxiv.org/abs/1712.03390v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.03390v2)
- **Published**: 2017-12-09 14:27:49+00:00
- **Updated**: 2018-03-28 09:31:27+00:00
- **Authors**: Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg, R. Venkatesh Babu
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Adversarial perturbations can pose a serious threat for deploying machine learning systems. Recent works have shown existence of image-agnostic perturbations that can fool classifiers over most natural images. Existing methods present optimization approaches that solve for a fooling objective with an imperceptibility constraint to craft the perturbations. However, for a given classifier, they generate one perturbation at a time, which is a single instance from the manifold of adversarial perturbations. Also, in order to build robust models, it is essential to explore the manifold of adversarial perturbations. In this paper, we propose for the first time, a generative approach to model the distribution of adversarial perturbations. The architecture of the proposed model is inspired from that of GANs and is trained using fooling and diversity objectives. Our trained generator network attempts to capture the distribution of adversarial perturbations for a given classifier and readily generates a wide variety of such perturbations. Our experimental evaluation demonstrates that perturbations crafted by our model (i) achieve state-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver excellent cross model generalizability. Our work can be deemed as an important step in the process of inferring about the complex manifolds of adversarial perturbations.



### Deep Koalarization: Image Colorization using CNNs and Inception-ResNet-v2
- **Arxiv ID**: http://arxiv.org/abs/1712.03400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03400v1)
- **Published**: 2017-12-09 15:29:35+00:00
- **Updated**: 2017-12-09 15:29:35+00:00
- **Authors**: Federico Baldassarre, Diego González Morín, Lucas Rodés-Guirao
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: We review some of the most recent approaches to colorize gray-scale images using deep learning methods. Inspired by these, we propose a model which combines a deep Convolutional Neural Network trained from scratch with high-level features extracted from the Inception-ResNet-v2 pre-trained model. Thanks to its fully convolutional architecture, our encoder-decoder model can process images of any size and aspect ratio. Other than presenting the training results, we assess the "public acceptance" of the generated images by means of a user study. Finally, we present a carousel of applications on different types of images, such as historical photographs.



### CycleGAN Face-off
- **Arxiv ID**: http://arxiv.org/abs/1712.03451v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03451v5)
- **Published**: 2017-12-09 23:32:38+00:00
- **Updated**: 2018-07-04 05:38:55+00:00
- **Authors**: Xiaohan Jin, Ye Qi, Shangxuan Wu
- **Comment**: Github repo: https://github.com/ShangxuanWu/CycleGAN-Face-off
- **Journal**: None
- **Summary**: Face-off is an interesting case of style transfer where the facial expressions and attributes of one person could be fully transformed to another face. We are interested in the unsupervised training process which only requires two sequences of unaligned video frames from each person and learns what shared attributes to extract automatically. In this project, we explored various improvements for adversarial training (i.e. CycleGAN[Zhu et al., 2017]) to capture details in facial expressions and head poses and thus generate transformation videos of higher consistency and stability.



### SPP-Net: Deep Absolute Pose Regression with Synthetic Views
- **Arxiv ID**: http://arxiv.org/abs/1712.03452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03452v1)
- **Published**: 2017-12-09 23:45:03+00:00
- **Updated**: 2017-12-09 23:45:03+00:00
- **Authors**: Pulak Purkait, Cheng Zhao, Christopher Zach
- **Comment**: Submitted to CVPR 2018
- **Journal**: None
- **Summary**: Image based localization is one of the important problems in computer vision due to its wide applicability in robotics, augmented reality, and autonomous systems. There is a rich set of methods described in the literature how to geometrically register a 2D image w.r.t.\ a 3D model. Recently, methods based on deep (and convolutional) feedforward networks (CNNs) became popular for pose regression. However, these CNN-based methods are still less accurate than geometry based methods despite being fast and memory efficient. In this work we design a deep neural network architecture based on sparse feature descriptors to estimate the absolute pose of an image. Our choice of using sparse feature descriptors has two major advantages: first, our network is significantly smaller than the CNNs proposed in the literature for this task---thereby making our approach more efficient and scalable. Second---and more importantly---, usage of sparse features allows to augment the training data with synthetic viewpoints, which leads to substantial improvements in the generalization performance to unseen poses. Thus, our proposed method aims to combine the best of the two worlds---feature-based localization and CNN-based pose regression--to achieve state-of-the-art performance in the absolute pose estimation. A detailed analysis of the proposed architecture and a rigorous evaluation on the existing datasets are provided to support our method.



### Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB
- **Arxiv ID**: http://arxiv.org/abs/1712.03453v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03453v3)
- **Published**: 2017-12-09 23:57:23+00:00
- **Updated**: 2018-08-28 16:48:16+00:00
- **Authors**: Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu, Srinath Sridhar, Gerard Pons-Moll, Christian Theobalt
- **Comment**: International Conference on 3D Vision (3DV), 2018
- **Journal**: None
- **Summary**: We propose a new single-shot method for multi-person 3D pose estimation in general scenes from a monocular RGB camera. Our approach uses novel occlusion-robust pose-maps (ORPM) which enable full body pose inference even under strong partial occlusions by other people and objects in the scene. ORPM outputs a fixed number of maps which encode the 3D joint locations of all people in the scene. Body part associations allow us to infer 3D pose for an arbitrary number of people without explicit bounding box prediction. To train our approach we introduce MuCo-3DHP, the first large scale training data set showing real images of sophisticated multi-person interactions and occlusions. We synthesize a large corpus of multi-person images by compositing images of individual people (with ground truth from mutli-view performance capture). We evaluate our method on our new challenging 3D annotated multi-person test set MuPoTs-3D where we achieve state-of-the-art performance. To further stimulate research in multi-person 3D pose estimation, we will make our new datasets, and associated code publicly available for research purposes.



