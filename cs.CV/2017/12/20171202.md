# Arxiv Papers in cs.CV on 2017-12-02
### Towards understanding feedback from supermassive black holes using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1712.00523v1
- **DOI**: None
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.00523v1)
- **Published**: 2017-12-02 00:05:16+00:00
- **Updated**: 2017-12-02 00:05:16+00:00
- **Authors**: Stanislav Fort
- **Comment**: 5 pages, 5 figures, accepted at Workshop on Deep Learning for
  Physical Sciences (DLPS 2017), NIPS 2017, Long Beach, CA, USA
- **Journal**: None
- **Summary**: Supermassive black holes at centers of clusters of galaxies strongly interact with their host environment via AGN feedback. Key tracers of such activity are X-ray cavities -- regions of lower X-ray brightness within the cluster. We present an automatic method for detecting, and characterizing X-ray cavities in noisy, low-resolution X-ray images. We simulate clusters of galaxies, insert cavities into them, and produce realistic low-quality images comparable to observations at high redshifts. We then train a custom-built convolutional neural network to generate pixel-wise analysis of presence of cavities in a cluster. A ResNet architecture is then used to decode radii of cavities from the pixel-wise predictions. We surpass the accuracy, stability, and speed of current visual inspection based methods on simulated data.



### A Perceptual Measure for Deep Single Image Camera Calibration
- **Arxiv ID**: http://arxiv.org/abs/1712.01259v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01259v3)
- **Published**: 2017-12-02 00:21:58+00:00
- **Updated**: 2018-04-22 16:12:04+00:00
- **Authors**: Yannick Hold-Geoffroy, Kalyan Sunkavalli, Jonathan Eisenmann, Matt Fisher, Emiliano Gambaretto, Sunil Hadap, Jean-Fran√ßois Lalonde
- **Comment**: Published at CVPR'18
- **Journal**: None
- **Summary**: Most current single image camera calibration methods rely on specific image features or user input, and cannot be applied to natural images captured in uncontrolled settings. We propose directly inferring camera calibration parameters from a single image using a deep convolutional neural network. This network is trained using automatically generated samples from a large-scale panorama dataset, and considerably outperforms other methods, including recent deep learning-based approaches, in terms of standard L2 error. However, we argue that in many cases it is more important to consider how humans perceive errors in camera estimation. To this end, we conduct a large-scale human perception study where we ask users to judge the realism of 3D objects composited with and without ground truth camera calibration. Based on this study, we develop a new perceptual measure for camera calibration, and demonstrate that our deep calibration network outperforms other methods on this measure. Finally, we demonstrate the use of our calibration network for a number of applications including virtual object insertion, image retrieval and compositing.



### An Ensemble of Deep Convolutional Neural Networks for Alzheimer's Disease Detection and Classification
- **Arxiv ID**: http://arxiv.org/abs/1712.01675v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.01675v2)
- **Published**: 2017-12-02 03:13:52+00:00
- **Updated**: 2017-12-19 19:17:09+00:00
- **Authors**: Jyoti Islam, Yanqing Zhang
- **Comment**: Accepted poster at NIPS 2017 Workshop on Machine Learning for Health
  (https://ml4health.github.io/2017/)
- **Journal**: None
- **Summary**: Alzheimer's Disease destroys brain cells causing people to lose their memory, mental functions and ability to continue daily activities. It is a severe neurological brain disorder which is not curable, but earlier detection of Alzheimer's Disease can help for proper treatment and to prevent brain tissue damage. Detection and classification of Alzheimer's Disease (AD) is challenging because sometimes the signs that distinguish Alzheimer's Disease MRI data can be found in normal healthy brain MRI data of older people. Moreover, there are relatively small amount of dataset available to train the automated Alzheimer's Disease detection and classification model. In this paper, we present a novel Alzheimer's Disease detection and classification model using brain MRI data analysis. We develop an ensemble of deep convolutional neural networks and demonstrate superior performance on the Open Access Series of Imaging Studies (OASIS) dataset.



### Splenomegaly Segmentation using Global Convolutional Kernels and Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.00542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00542v1)
- **Published**: 2017-12-02 03:47:37+00:00
- **Updated**: 2017-12-02 03:47:37+00:00
- **Authors**: Yuankai Huo, Zhoubing Xu, Shunxing Bao, Camilo Bermudez, Andrew J. Plassard, Jiaqi Liu, Yuang Yao, Albert Assad, Richard G. Abramson, Bennett A. Landman
- **Comment**: SPIE Medical Imaging 2018
- **Journal**: None
- **Summary**: Spleen volume estimation using automated image segmentation technique may be used to detect splenomegaly (abnormally enlarged spleen) on Magnetic Resonance Imaging (MRI) scans. In recent years, Deep Convolutional Neural Networks (DCNN) segmentation methods have demonstrated advantages for abdominal organ segmentation. However, variations in both size and shape of the spleen on MRI images may result in large false positive and false negative labeling when deploying DCNN based methods. In this paper, we propose the Splenomegaly Segmentation Network (SSNet) to address spatial variations when segmenting extraordinarily large spleens. SSNet was designed based on the framework of image-to-image conditional generative adversarial networks (cGAN). Specifically, the Global Convolutional Network (GCN) was used as the generator to reduce false negatives, while the Markovian discriminator (PatchGAN) was used to alleviate false positives. A cohort of clinically acquired 3D MRI scans (both T1 weighted and T2 weighted) from patients with splenomegaly were used to train and test the networks. The experimental results demonstrated that a mean Dice coefficient of 0.9260 and a median Dice coefficient of 0.9262 using SSNet on independently tested MRI volumes of patients with splenomegaly.



### Improved Stability of Whole Brain Surface Parcellation with Multi-Atlas Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1712.00543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00543v1)
- **Published**: 2017-12-02 03:48:42+00:00
- **Updated**: 2017-12-02 03:48:42+00:00
- **Authors**: Yuankai Huo, Shunxing Bao, Prasanna Parvathaneni, Bennett A. Landman
- **Comment**: SPIE Medical Imaging 2018
- **Journal**: None
- **Summary**: Whole brain segmentation and cortical surface parcellation are essential in understanding the anatomical-functional relationships of the brain. Multi-atlas segmentation has been regarded as one of the leading segmentation methods for the whole brain segmentation. In our recent work, the multi-atlas technique has been adapted to surface reconstruction using a method called Multi-atlas CRUISE (MaCRUISE). The MaCRUISE method not only performed consistent volume-surface analyses but also showed advantages on robustness compared with the FreeSurfer method. However, a detailed surface parcellation was not provided by MaCRUISE, which hindered the region of interest (ROI) based analyses on surfaces. Herein, the MaCRUISE surface parcellation (MaCRUISEsp) method is proposed to perform the surface parcellation upon the inner, central and outer surfaces that are reconstructed from MaCRUISE. MaCRUISEsp parcellates inner, central and outer surfaces with 98 cortical labels respectively using a volume segmentation based surface parcellation (VSBSP), following a topological correction step. To validate the performance of MaCRUISEsp, 21 scan-rescan magnetic resonance imaging (MRI) T1 volume pairs from the Kirby21 dataset were used to perform a reproducibility analyses. MaCRUISEsp achieved 0.948 on median Dice Similarity Coefficient (DSC) for central surfaces. Meanwhile, FreeSurfer achieved 0.905 DSC for inner surfaces and 0.881 DSC for outer surfaces, while the proposed method achieved 0.929 DSC for inner surfaces and 0.835 DSC for outer surfaces. Qualitatively, the results are encouraging, but are not directly comparable as the two approaches use different definitions of cortical labels.



### SfSNet: Learning Shape, Reflectance and Illuminance of Faces in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1712.01261v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01261v2)
- **Published**: 2017-12-02 04:08:22+00:00
- **Updated**: 2018-04-19 04:56:39+00:00
- **Authors**: Soumyadip Sengupta, Angjoo Kanazawa, Carlos D. Castillo, David Jacobs
- **Comment**: Accepted to CVPR 2018 (Spotlight)
- **Journal**: None
- **Summary**: We present SfSNet, an end-to-end learning framework for producing an accurate decomposition of an unconstrained human face image into shape, reflectance and illuminance. SfSNet is designed to reflect a physical lambertian rendering model. SfSNet learns from a mixture of labeled synthetic and unlabeled real world images. This allows the network to capture low frequency variations from synthetic and high frequency details from real images through the photometric reconstruction loss. SfSNet consists of a new decomposition architecture with residual blocks that learns a complete separation of albedo and normal. This is used along with the original image to predict lighting. SfSNet produces significantly better quantitative and qualitative results than state-of-the-art methods for inverse rendering and independent normal and illumination estimation.



### Compatibility Family Learning for Item Recommendation and Generation
- **Arxiv ID**: http://arxiv.org/abs/1712.01262v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.01262v1)
- **Published**: 2017-12-02 04:22:56+00:00
- **Updated**: 2017-12-02 04:22:56+00:00
- **Authors**: Yong-Siang Shih, Kai-Yueh Chang, Hsuan-Tien Lin, Min Sun
- **Comment**: 9 pages, accepted to AAAI 2018
- **Journal**: None
- **Summary**: Compatibility between items, such as clothes and shoes, is a major factor among customer's purchasing decisions. However, learning "compatibility" is challenging due to (1) broader notions of compatibility than those of similarity, (2) the asymmetric nature of compatibility, and (3) only a small set of compatible and incompatible items are observed. We propose an end-to-end trainable system to embed each item into a latent vector and project a query item into K compatible prototypes in the same space. These prototypes reflect the broad notions of compatibility. We refer to both the embedding and prototypes as "Compatibility Family". In our learned space, we introduce a novel Projected Compatibility Distance (PCD) function which is differentiable and ensures diversity by aiming for at least one prototype to be close to a compatible item, whereas none of the prototypes are close to an incompatible item. We evaluate our system on a toy dataset, two Amazon product datasets, and Polyvore outfit dataset. Our method consistently achieves state-of-the-art performance. Finally, we show that we can visualize the candidate compatible prototypes using a Metric-regularized Conditional Generative Adversarial Network (MrCGAN), where the input is a projected prototype and the output is a generated image of a compatible item. We ask human evaluators to judge the relative compatibility between our generated images and images generated by CGANs conditioned directly on query items. Our generated images are significantly preferred, with roughly twice the number of votes as others.



### Progressive Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1712.00559v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.00559v3)
- **Published**: 2017-12-02 06:23:16+00:00
- **Updated**: 2018-07-26 19:51:26+00:00
- **Authors**: Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, Kevin Murphy
- **Comment**: To appear in ECCV 2018 as oral. The code and checkpoint for PNASNet-5
  trained on ImageNet (both Mobile and Large) can now be downloaded from
  https://github.com/tensorflow/models/tree/master/research/slim#Pretrained.
  Also see https://github.com/chenxi116/PNASNet.TF for refactored and
  simplified TensorFlow code; see https://github.com/chenxi116/PNASNet.pytorch
  for exact conversion to PyTorch
- **Journal**: None
- **Summary**: We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.



### Lecture video indexing using boosted margin maximizing neural networks
- **Arxiv ID**: http://arxiv.org/abs/1712.00575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00575v1)
- **Published**: 2017-12-02 09:08:05+00:00
- **Updated**: 2017-12-02 09:08:05+00:00
- **Authors**: Di Ma, Xi Zhang, Xu Ouyang, Gady Agam
- **Comment**: Accepted by ICMLA 2017
- **Journal**: None
- **Summary**: This paper presents a novel approach for lecture video indexing using a boosted deep convolutional neural network system. The indexing is performed by matching high quality slide images, for which text is either known or extracted, to lower resolution video frames with possible noise, perspective distortion, and occlusions. We propose a deep neural network integrated with a boosting framework composed of two sub-networks targeting feature extraction and similarity determination to perform the matching. The trained network is given as input a pair of slide image and a candidate video frame image and produces the similarity between them. A boosting framework is integrated into our proposed network during the training process. Experimental results show that the proposed approach is much more capable of handling occlusion, spatial transformations, and other types of noises when compared with known approaches.



### Fruit recognition from images using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1712.00580v10
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1712.00580v10)
- **Published**: 2017-12-02 09:48:37+00:00
- **Updated**: 2021-01-24 11:19:36+00:00
- **Authors**: Horea Mure≈üan, Mihai Oltean
- **Comment**: 38 pages
- **Journal**: Acta Univ. Sapientiae, Informatica Vol. 10, Issue 1, pp. 26-42,
  2018
- **Summary**: In this paper we introduce a new, high-quality, dataset of images containing fruits. We also present the results of some numerical experiment for training a neural network to detect fruits. We discuss the reason why we chose to use fruits in this project by proposing a few applications that could use this kind of neural network.



### Taming Adversarial Domain Transfer with Structural Constraints for Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1712.00598v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00598v3)
- **Published**: 2017-12-02 12:37:34+00:00
- **Updated**: 2018-12-07 14:24:36+00:00
- **Authors**: Elias Vansteenkiste, Patrick Kern
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this work is to improve images of traffic scenes that are degraded by natural causes such as fog, rain and limited visibility during the night. For these applications, it is next to impossible to get pixel perfect pairs of the same scene, with and without the degrading conditions. This makes it unsuitable for conventional supervised learning approaches, however, it is easy to collect unpaired images of the scenes in a perfect and in a degraded condition. To enhance the images taken in a poor visibility condition, domain transfer models can be trained to transform an image from the degraded to the clear domain. A well-known concept for unsupervised domain transfer are cycle-consistent generative adversarial models. Unfortunately, the resulting generators often change the structure of the scene. This causes an undesirable change in the semantics. We propose three ways to cope with this problem depending on the type of degradation.



### Recurrent Neural Networks for Semantic Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1712.00617v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00617v4)
- **Published**: 2017-12-02 15:01:27+00:00
- **Updated**: 2019-04-12 14:02:32+00:00
- **Authors**: Amaia Salvador, Miriam Bellver, Victor Campos, Manel Baradad, Ferran Marques, Jordi Torres, Xavier Giro-i-Nieto
- **Comment**: None
- **Journal**: None
- **Summary**: We present a recurrent model for semantic instance segmentation that sequentially generates binary masks and their associated class probabilities for every object in an image. Our proposed system is trainable end-to-end from an input image to a sequence of labeled masks and, compared to methods relying on object proposals, does not require post-processing steps on its output. We study the suitability of our recurrent model on three different instance segmentation benchmarks, namely Pascal VOC 2012, CVPPP Plant Leaf Segmentation and Cityscapes. Further, we analyze the object sorting patterns generated by our model and observe that it learns to follow a consistent pattern, which correlates with the activations learned in the encoder part of our network. Source code and models are available at https://imatge-upc.github.io/rsis/



### DR-Net: Transmission Steered Single Image Dehazing Network with Weakly Supervised Refinement
- **Arxiv ID**: http://arxiv.org/abs/1712.00621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00621v1)
- **Published**: 2017-12-02 15:23:43+00:00
- **Updated**: 2017-12-02 15:23:43+00:00
- **Authors**: Chongyi Li, Jichang Guo, Fatih Porikli, Chunle Guo, Huzhu Fu, Xi Li
- **Comment**: 8 pages, 8 figures, submitted to CVPR 2018
- **Journal**: None
- **Summary**: Despite the recent progress in image dehazing, several problems remain largely unsolved such as robustness for varying scenes, the visual quality of reconstructed images, and effectiveness and flexibility for applications. To tackle these problems, we propose a new deep network architecture for single image dehazing called DR-Net. Our model consists of three main subnetworks: a transmission prediction network that predicts transmission map for the input image, a haze removal network that reconstructs latent image steered by the transmission map, and a refinement network that enhances the details and color properties of the dehazed result via weakly supervised learning. Compared to previous methods, our method advances in three aspects: (i) pure data-driven model; (ii) the end-to-end system; (iii) superior robustness, accuracy, and applicability. Extensive experiments demonstrate that our DR-Net outperforms the state-of-the-art methods on both synthetic and real images in qualitative and quantitative metrics. Additionally, the utility of DR-Net has been illustrated by its potential usage in several important computer vision tasks.



### Compressed Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.00636v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00636v2)
- **Published**: 2017-12-02 16:47:41+00:00
- **Updated**: 2018-03-29 18:14:00+00:00
- **Authors**: Chao-Yuan Wu, Manzil Zaheer, Hexiang Hu, R. Manmatha, Alexander J. Smola, Philipp Kr√§henb√ºhl
- **Comment**: CVPR 2018 (Selected for spotlight presentation)
- **Journal**: None
- **Summary**: Training robust deep video representations has proven to be much more challenging than learning deep image representations. This is in part due to the enormous size of raw video streams and the high temporal redundancy; the true and interesting signal is often drowned in too much irrelevant data. Motivated by that the superfluous information can be reduced by up to two orders of magnitude by video compression (using H.264, HEVC, etc.), we propose to train a deep network directly on the compressed video.   This representation has a higher information density, and we found the training to be easier. In addition, the signals in a compressed video provide free, albeit noisy, motion information. We propose novel techniques to use them effectively. Our approach is about 4.6 times faster than Res3D and 2.7 times faster than ResNet-152. On the task of action recognition, our approach outperforms all the other methods on the UCF-101, HMDB-51, and Charades dataset.



### Mix-and-Match Tuning for Self-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1712.00661v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.00661v3)
- **Published**: 2017-12-02 20:25:37+00:00
- **Updated**: 2018-01-30 00:16:05+00:00
- **Authors**: Xiaohang Zhan, Ziwei Liu, Ping Luo, Xiaoou Tang, Chen Change Loy
- **Comment**: To appear in AAAI 2018 as a spotlight paper. More details at the
  project page: http://mmlab.ie.cuhk.edu.hk/projects/M%26M/
- **Journal**: None
- **Summary**: Deep convolutional networks for semantic image segmentation typically require large-scale labeled data, e.g. ImageNet and MS COCO, for network pre-training. To reduce annotation efforts, self-supervised semantic segmentation is recently proposed to pre-train a network without any human-provided labels. The key of this new form of learning is to design a proxy task (e.g. image colorization), from which a discriminative loss can be formulated on unlabeled data. Many proxy tasks, however, lack the critical supervision signals that could induce discriminative representation for the target image segmentation task. Thus self-supervision's performance is still far from that of supervised pre-training. In this study, we overcome this limitation by incorporating a "mix-and-match" (M&M) tuning stage in the self-supervision pipeline. The proposed approach is readily pluggable to many self-supervision methods and does not use more annotated samples than the original process. Yet, it is capable of boosting the performance of target image segmentation task to surpass fully-supervised pre-trained counterpart. The improvement is made possible by better harnessing the limited pixel-wise annotations in the target dataset. Specifically, we first introduce the "mix" stage, which sparsely samples and mixes patches from the target set to reflect rich and diverse local patch statistics of target images. A "match" stage then forms a class-wise connected graph, which can be used to derive a strong triplet-based discriminative loss for fine-tuning the network. Our paradigm follows the standard practice in existing self-supervised studies and no extra data or label is required. With the proposed M&M approach, for the first time, a self-supervision method can achieve comparable or even better performance compared to its ImageNet pre-trained counterpart on both PASCAL VOC2012 dataset and CityScapes dataset.



