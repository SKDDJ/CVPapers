# Arxiv Papers in cs.CV on 2017-12-08
### Audio-Visual Sentiment Analysis for Learning Emotional Arcs in Movies
- **Arxiv ID**: http://arxiv.org/abs/1712.02896v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1712.02896v1)
- **Published**: 2017-12-08 00:27:08+00:00
- **Updated**: 2017-12-08 00:27:08+00:00
- **Authors**: Eric Chu, Deb Roy
- **Comment**: Data Mining (ICDM), 2017 IEEE 17th International Conference on
- **Journal**: None
- **Summary**: Stories can have tremendous power -- not only useful for entertainment, they can activate our interests and mobilize our actions. The degree to which a story resonates with its audience may be in part reflected in the emotional journey it takes the audience upon. In this paper, we use machine learning methods to construct emotional arcs in movies, calculate families of arcs, and demonstrate the ability for certain arcs to predict audience engagement. The system is applied to Hollywood films and high quality shorts found on the web. We begin by using deep convolutional neural networks for audio and visual sentiment analysis. These models are trained on both new and existing large-scale datasets, after which they can be used to compute separate audio and visual emotional arcs. We then crowdsource annotations for 30-second video clips extracted from highs and lows in the arcs in order to assess the micro-level precision of the system, with precision measured in terms of agreement in polarity between the system's predictions and annotators' ratings. These annotations are also used to combine the audio and visual predictions. Next, we look at macro-level characterizations of movies by investigating whether there exist `universal shapes' of emotional arcs. In particular, we develop a clustering approach to discover distinct classes of emotional arcs. Finally, we show on a sample corpus of short web videos that certain emotional arcs are statistically significant predictors of the number of comments a video receives. These results suggest that the emotional arcs learned by our approach successfully represent macroscopic aspects of a video story that drive audience engagement. Such machine understanding could be used to predict audience reactions to video stories, ultimately improving our ability as storytellers to communicate with each other.



### Representations of Sound in Deep Learning of Audio Features from Music
- **Arxiv ID**: http://arxiv.org/abs/1712.02898v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1712.02898v1)
- **Published**: 2017-12-08 00:37:23+00:00
- **Updated**: 2017-12-08 00:37:23+00:00
- **Authors**: Sergey Shuvaev, Hamza Giaffar, Alexei A. Koulakov
- **Comment**: None
- **Journal**: None
- **Summary**: The work of a single musician, group or composer can vary widely in terms of musical style. Indeed, different stylistic elements, from performance medium and rhythm to harmony and texture, are typically exploited and developed across an artist's lifetime. Yet, there is often a discernable character to the work of, for instance, individual composers at the perceptual level - an experienced listener can often pick up on subtle clues in the music to identify the composer or performer. Here we suggest that a convolutional network may learn these subtle clues or features given an appropriate representation of the music. In this paper, we apply a deep convolutional neural network to a large audio dataset and empirically evaluate its performance on audio classification tasks. Our trained network demonstrates accurate performance on such classification tasks when presented with 5 s examples of music obtained by simple transformations of the raw audio waveform. A particularly interesting example is the spectral representation of music obtained by application of a logarithmically spaced filter bank, mirroring the early stages of auditory signal transduction in mammals. The most successful representation of music to facilitate discrimination was obtained via a random matrix transform (RMT). Networks based on logarithmic filter banks and RMT were able to correctly guess the one composer out of 31 possibilities in 68 and 84 percent of cases respectively.



### Exploiting Modern Hardware for High-Dimensional Nearest Neighbor Search
- **Arxiv ID**: http://arxiv.org/abs/1712.02912v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB, cs.IR, cs.MM, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1712.02912v1)
- **Published**: 2017-12-08 02:14:17+00:00
- **Updated**: 2017-12-08 02:14:17+00:00
- **Authors**: Fabien Andr√©
- **Comment**: PhD Thesis, 123 Pages
- **Journal**: None
- **Summary**: Many multimedia information retrieval or machine learning problems require efficient high-dimensional nearest neighbor search techniques. For instance, multimedia objects (images, music or videos) can be represented by high-dimensional feature vectors. Finding two similar multimedia objects then comes down to finding two objects that have similar feature vectors. In the current context of mass use of social networks, large scale multimedia databases or large scale machine learning applications are more and more common, calling for efficient nearest neighbor search approaches.   This thesis builds on product quantization, an efficient nearest neighbor search technique that compresses high-dimensional vectors into short codes. This makes it possible to store very large databases entirely in RAM, enabling low response times. We propose several contributions that exploit the capabilities of modern CPUs, especially SIMD and the cache hierarchy, to further decrease response times offered by product quantization.



### Chaining Identity Mapping Modules for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1712.02933v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02933v2)
- **Published**: 2017-12-08 03:51:54+00:00
- **Updated**: 2019-07-19 01:35:32+00:00
- **Authors**: Saeed Anwar, Cong Phouc Huynh, Fatih Porikli
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to learn a fully-convolutional network model that consists of a Chain of Identity Mapping Modules (CIMM) for image denoising. The CIMM structure possesses two distinctive features that are important for the noise removal task. Firstly, each residual unit employs identity mappings as the skip connections and receives pre-activated input in order to preserve the gradient magnitude propagated in both the forward and backward directions. Secondly, by utilizing dilated kernels for the convolution layers in the residual branch, in other words within an identity mapping module, each neuron in the last convolution layer can observe the full receptive field of the first layer. After being trained on the BSD400 dataset, the proposed network produces remarkably higher numerical accuracy and better visual image quality than the state-of-the-art when being evaluated on conventional benchmark images and the BSD68 dataset.



### Dense Optical Flow based Change Detection Network Robust to Difference of Camera Viewpoints
- **Arxiv ID**: http://arxiv.org/abs/1712.02941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02941v1)
- **Published**: 2017-12-08 05:05:51+00:00
- **Updated**: 2017-12-08 05:05:51+00:00
- **Authors**: Ken Sakurada, Weimin Wang, Nobuo Kawaguchi, Ryosuke Nakamura
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel method for detecting scene changes from a pair of images with a difference of camera viewpoints using a dense optical flow based change detection network. In the case that camera poses of input images are fixed or known, such as with surveillance and satellite cameras, the pixel correspondence between the images captured at different times can be known. Hence, it is possible to comparatively accurately detect scene changes between the images by modeling the appearance of the scene. On the other hand, in case of cameras mounted on a moving object, such as ground and aerial vehicles, we must consider the spatial correspondence between the images captured at different times. However, it can be difficult to accurately estimate the camera pose or 3D model of a scene, owing to the scene changes or lack of imagery. To solve this problem, we propose a change detection convolutional neural network utilizing dense optical flow between input images to improve the robustness to the difference between camera viewpoints. Our evaluation based on the panoramic change detection dataset shows that the proposed method outperforms state-of-the-art change detection algorithms.



### CycleGAN, a Master of Steganography
- **Arxiv ID**: http://arxiv.org/abs/1712.02950v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.02950v2)
- **Published**: 2017-12-08 06:07:52+00:00
- **Updated**: 2017-12-16 07:42:46+00:00
- **Authors**: Casey Chu, Andrey Zhmoginov, Mark Sandler
- **Comment**: NIPS 2017, workshop on Machine Deception
- **Journal**: None
- **Summary**: CycleGAN (Zhu et al. 2017) is one recent successful approach to learn a transformation between two image distributions. In a series of experiments, we demonstrate an intriguing property of the model: CycleGAN learns to "hide" information about a source image into the images it generates in a nearly imperceptible, high-frequency signal. This trick ensures that the generator can recover the original sample and thus satisfy the cyclic consistency requirement, while the generated image remains realistic. We connect this phenomenon with adversarial attacks by viewing CycleGAN's training procedure as training a generator of adversarial examples and demonstrate that the cyclic consistency loss causes CycleGAN to be especially vulnerable to adversarial attacks.



### Compact Hash Code Learning with Binary Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1712.02956v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02956v3)
- **Published**: 2017-12-08 06:25:35+00:00
- **Updated**: 2019-08-30 00:15:04+00:00
- **Authors**: Thanh-Toan Do, Tuan Hoang, Dang-Khoa Le Tan, Anh-Dzung Doan, Ngai-Man Cheung
- **Comment**: Accepted to IEEE Transactions on Multimedia, 2019. arXiv admin note:
  text overlap with arXiv:1607.05140
- **Journal**: None
- **Summary**: Learning compact binary codes for image retrieval problem using deep neural networks has recently attracted increasing attention. However, training deep hashing networks is challenging due to the binary constraints on the hash codes. In this paper, we propose deep network models and learning algorithms for learning binary hash codes given image representations under both unsupervised and supervised manners. The novelty of our network design is that we constrain one hidden layer to directly output the binary codes. This design has overcome a challenging problem in some previous works: optimizing non-smooth objective functions because of binarization. In addition, we propose to incorporate independence and balance properties in the direct and strict forms into the learning schemes. We also include a similarity preserving property in our objective functions. The resulting optimizations involving these binary, independence, and balance constraints are difficult to solve. To tackle this difficulty, we propose to learn the networks with alternating optimization and careful relaxation. Furthermore, by leveraging the powerful capacity of convolutional neural networks, we propose an end-to-end architecture that jointly learns to extract visual features and produce binary hash codes. Experimental results for the benchmark datasets show that the proposed methods compare favorably or outperform the state of the art.



### Shape from Shading through Shape Evolution
- **Arxiv ID**: http://arxiv.org/abs/1712.02961v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02961v1)
- **Published**: 2017-12-08 06:41:42+00:00
- **Updated**: 2017-12-08 06:41:42+00:00
- **Authors**: Dawei Yang, Jia Deng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the shape-from-shading problem by training deep networks with synthetic images. Unlike conventional approaches that combine deep learning and synthetic imagery, we propose an approach that does not need any external shape dataset to render synthetic images. Our approach consists of two synergistic processes: the evolution of complex shapes from simple primitives, and the training of a deep network for shape-from-shading. The evolution generates better shapes guided by the network training, while the training improves by using the evolved shapes. We show that our approach achieves state-of-the-art performance on a shape-from-shading benchmark.



### Learning 2D Gabor Filters by Infinite Kernel Learning Regression
- **Arxiv ID**: http://arxiv.org/abs/1712.02974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02974v1)
- **Published**: 2017-12-08 07:50:58+00:00
- **Updated**: 2017-12-08 07:50:58+00:00
- **Authors**: Kamaledin Ghiasi-Shirazi
- **Comment**: None
- **Journal**: None
- **Summary**: Gabor functions have wide-spread applications in image processing and computer vision. In this paper, we prove that 2D Gabor functions are translation-invariant positive-definite kernels and propose a novel formulation for the problem of image representation with Gabor functions based on infinite kernel learning regression. Using this formulation, we obtain a support vector expansion of an image based on a mixture of Gabor functions. The problem with this representation is that all Gabor functions are present at all support vector pixels. Applying LASSO to this support vector expansion, we obtain a sparse representation in which each Gabor function is positioned at a very small set of pixels. As an application, we introduce a method for learning a dataset-specific set of Gabor filters that can be used subsequently for feature extraction. Our experiments show that use of the learned Gabor filters improves the recognition accuracy of a recently introduced face recognition algorithm.



### 3D Surface-to-Structure Translation using Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.01449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01449v1)
- **Published**: 2017-12-08 08:16:06+00:00
- **Updated**: 2017-12-08 08:16:06+00:00
- **Authors**: Takumi Moriya, Kazuyuki Saito, Hiroya Tanaka
- **Comment**: 2 pages, 3 figures
- **Journal**: None
- **Summary**: Our demonstration shows a system that estimates internal body structures from 3D surface models using deep convolutional neural networks trained on CT (computed tomography) images of the human body. To take pictures of structures inside the body, we need to use a CT scanner or an MRI (Magnetic Resonance Imaging) scanner. However, assuming that the mutual information between outer shape of the body and its inner structure is not zero, we can obtain an approximate internal structure from a 3D surface model based on MRI and CT image database. This suggests that we could know where and what kind of disease a person is likely to have in his/her body simply by 3D scanning surface of the body. As a first prototype, we developed a system for estimating internal body structures from surface models based on Visible Human Project DICOM CT Datasets from the University of Iowa Magnetic Resonance Research Facility.



### Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser
- **Arxiv ID**: http://arxiv.org/abs/1712.02976v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02976v2)
- **Published**: 2017-12-08 08:20:45+00:00
- **Updated**: 2018-05-08 06:06:06+00:00
- **Authors**: Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, Jun Zhu
- **Comment**: None
- **Journal**: CVPR 2018
- **Summary**: Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose high-level representation guided denoiser (HGD) as a defense for image classification. Standard denoiser suffers from the error amplification effect, in which small residual adversarial noise is progressively amplified and leads to wrong classifications. HGD overcomes this problem by using a loss function defined as the difference between the target model's outputs activated by the clean image and denoised image. Compared with ensemble adversarial training which is the state-of-the-art defending method on large images, HGD has three advantages. First, with HGD as a defense, the target model is more robust to either white-box or black-box adversarial attacks. Second, HGD can be trained on a small subset of the images and generalizes well to other images and unseen classes. Third, HGD can be transferred to defend models other than the one guiding it. In NIPS competition on defense against adversarial attacks, our HGD solution won the first place and outperformed other models by a large margin.



### Direct and Real-Time Cardiovascular Risk Prediction
- **Arxiv ID**: http://arxiv.org/abs/1712.02982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02982v1)
- **Published**: 2017-12-08 08:53:09+00:00
- **Updated**: 2017-12-08 08:53:09+00:00
- **Authors**: Bob D. de Vos, Nikolas Lessmann, Pim A. de Jong, Max A. Viergever, Ivana Isgum
- **Comment**: Scientific paper at RSNA 2017
- **Journal**: None
- **Summary**: Coronary artery calcium (CAC) burden quantified in low-dose chest CT is a predictor of cardiovascular events. We propose an automatic method for CAC quantification, circumventing intermediate segmentation of CAC. The method determines a bounding box around the heart using a ConvNet for localization. Subsequently, a dedicated ConvNet analyzes axial slices within the bounding boxes to determine CAC quantity by regression. A dataset of 1,546 baseline CT scans was used from the National Lung Screening Trial with manually identified CAC. The method achieved an ICC of 0.98 between manual reference and automatically obtained Agatston scores. Stratification of subjects into five cardiovascular risk categories resulted in an accuracy of 85\% and Cohen's linearly weighted $\kappa$ of 0.90. The results demonstrate that real-time quantification of CAC burden in chest CT without the need for segmentation of CAC is possible.



### Variational models for joint subsampling and reconstruction of turbulence-degraded images
- **Arxiv ID**: http://arxiv.org/abs/1712.03825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03825v1)
- **Published**: 2017-12-08 09:41:24+00:00
- **Updated**: 2017-12-08 09:41:24+00:00
- **Authors**: Chun Pong Lau, Yu Hin Lai, Lok Ming Lui
- **Comment**: arXiv admin note: text overlap with arXiv:1704.03140
- **Journal**: None
- **Summary**: Turbulence-degraded image frames are distorted by both turbulent deformations and space-time-varying blurs. To suppress these effects, we propose a multi-frame reconstruction scheme to recover a latent image from the observed image sequence. Recent approaches are commonly based on registering each frame to a reference image, by which geometric turbulent deformations can be estimated and a sharp image can be restored. A major challenge is that a fine reference image is usually unavailable, as every turbulence-degraded frame is distorted. A high-quality reference image is crucial for the accurate estimation of geometric deformations and fusion of frames. Besides, it is unlikely that all frames from the image sequence are useful, and thus frame selection is necessary and highly beneficial. In this work, we propose a variational model for joint subsampling of frames and extraction of a clear image. A fine image and a suitable choice of subsample are simultaneously obtained by iteratively reducing an energy functional. The energy consists of a fidelity term measuring the discrepancy between the extracted image and the subsampled frames, as well as regularization terms on the extracted image and the subsample. Different choices of fidelity and regularization terms are explored. By carefully selecting suitable frames and extracting the image, the quality of the reconstructed image can be significantly improved. Extensive experiments have been carried out, which demonstrate the efficacy of our proposed model. In addition, the extracted subsamples and images can be put in existing algorithms to produce improved results.



### A Frequency Domain Neural Network for Fast Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1712.03037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03037v1)
- **Published**: 2017-12-08 11:53:52+00:00
- **Updated**: 2017-12-08 11:53:52+00:00
- **Authors**: Junxuan Li, Shaodi You, Antonio Robles-Kelly
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a frequency domain neural network for image super-resolution. The network employs the convolution theorem so as to cast convolutions in the spatial domain as products in the frequency domain. Moreover, the non-linearity in deep nets, often achieved by a rectifier unit, is here cast as a convolution in the frequency domain. This not only yields a network which is very computationally efficient at testing but also one whose parameters can all be learnt accordingly. The network can be trained using back propagation and is devoid of complex numbers due to the use of the Hartley transform as an alternative to the Fourier transform. Moreover, the network is potentially applicable to other problems elsewhere in computer vision and image processing which are often cast in the frequency domain. We show results on super-resolution and compare against alternatives elsewhere in the literature. In our experiments, our network is one to two orders of magnitude faster than the alternatives with an imperceptible loss of performance.



### An Integrated Platform for Live 3D Human Reconstruction and Motion Capturing
- **Arxiv ID**: http://arxiv.org/abs/1712.03084v1
- **DOI**: 10.1109/TCSVT.2016.2576922
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03084v1)
- **Published**: 2017-12-08 14:31:54+00:00
- **Updated**: 2017-12-08 14:31:54+00:00
- **Authors**: Dimitrios S. Alexiadis, Anargyros Chatzitofis, Nikolaos Zioulis, Olga Zoidi, Georgios Louizis, Dimitrios Zarpalas, Petros Daras
- **Comment**: 16 pages, 12 figures, 3 tables
- **Journal**: Alexiadis, D. S., Chatzitofis, A., Zioulis, N., Zoidi, O.,
  Louizis, G., Zarpalas, D., & Daras, P. (2017). An Integrated Platform for
  Live 3D Human Reconstruction and Motion Capturing. IEEE TCSVT Page(s): 798 -
  813
- **Summary**: The latest developments in 3D capturing, processing, and rendering provide means to unlock novel 3D application pathways. The main elements of an integrated platform, which target tele-immersion and future 3D applications, are described in this paper, addressing the tasks of real-time capturing, robust 3D human shape/appearance reconstruction, and skeleton-based motion tracking. More specifically, initially, the details of a multiple RGB-depth (RGB-D) capturing system are given, along with a novel sensors' calibration method. A robust, fast reconstruction method from multiple RGB-D streams is then proposed, based on an enhanced variation of the volumetric Fourier transform-based method, parallelized on the Graphics Processing Unit, and accompanied with an appropriate texture-mapping algorithm. On top of that, given the lack of relevant objective evaluation methods, a novel framework is proposed for the quantitative evaluation of real-time 3D reconstruction systems. Finally, a generic, multiple depth stream-based method for accurate real-time human skeleton tracking is proposed. Detailed experimental results with multi-Kinect2 data sets verify the validity of our arguments and the effectiveness of the proposed system and methodologies.



### Image Inpainting for High-Resolution Textures using CNN Texture Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1712.03111v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03111v2)
- **Published**: 2017-12-08 15:02:27+00:00
- **Updated**: 2018-02-12 11:52:11+00:00
- **Authors**: Pascal Laube, Michael Grunwald, Matthias O. Franz, Georg Umlauf
- **Comment**: Submitted
- **Journal**: None
- **Summary**: Deep neural networks have been successfully applied to problems such as image segmentation, image super-resolution, coloration and image inpainting. In this work we propose the use of convolutional neural networks (CNN) for image inpainting of large regions in high-resolution textures. Due to limited computational resources processing high-resolution images with neural networks is still an open problem. Existing methods separate inpainting of global structure and the transfer of details, which leads to blurry results and loss of global coherence in the detail transfer step. Based on advances in texture synthesis using CNNs we propose patch-based image inpainting by a CNN that is able to optimize for global as well as detail texture statistics. Our method is capable of filling large inpainting regions, oftentimes exceeding the quality of comparable methods for high-resolution images. For reference patch look-up we propose to use the same summary statistics that are used in the inpainting process.



### Simultaneous Hand Pose and Skeleton Bone-Lengths Estimation from a Single Depth Image
- **Arxiv ID**: http://arxiv.org/abs/1712.03121v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.03121v1)
- **Published**: 2017-12-08 15:25:00+00:00
- **Updated**: 2017-12-08 15:25:00+00:00
- **Authors**: Jameel Malik, Ahmed Elhayek, Didier Stricker
- **Comment**: This paper has been accepted and presented in 3DV-2017 conference
  held at Qingdao, China. http://irc.cs.sdu.edu.cn/3dv/
- **Journal**: None
- **Summary**: Articulated hand pose estimation is a challenging task for human-computer interaction. The state-of-the-art hand pose estimation algorithms work only with one or a few subjects for which they have been calibrated or trained. Particularly, the hybrid methods based on learning followed by model fitting or model based deep learning do not explicitly consider varying hand shapes and sizes. In this work, we introduce a novel hybrid algorithm for estimating the 3D hand pose as well as bone-lengths of the hand skeleton at the same time, from a single depth image. The proposed CNN architecture learns hand pose parameters and scale parameters associated with the bone-lengths simultaneously. Subsequently, a new hybrid forward kinematics layer employs both parameters to estimate 3D joint positions of the hand. For end-to-end training, we combine three public datasets NYU, ICVL and MSRA-2015 in one unified format to achieve large variation in hand shapes and sizes. Among hybrid methods, our method shows improved accuracy over the state-of-the-art on the combined dataset and the ICVL dataset that contain multiple subjects. Also, our algorithm is demonstrated to work well with unseen images.



### Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.03141v2
- **DOI**: 10.1016/j.patcog.2018.07.023
- **Categories**: **cs.CV**, cs.CR, cs.GT, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.03141v2)
- **Published**: 2017-12-08 15:59:41+00:00
- **Updated**: 2018-07-19 08:27:23+00:00
- **Authors**: Battista Biggio, Fabio Roli
- **Comment**: Accepted for publication on Pattern Recognition, 2018
- **Journal**: None
- **Summary**: Learning-based pattern classifiers, including deep networks, have shown impressive performance in several application domains, ranging from computer vision to cybersecurity. However, it has also been shown that adversarial input perturbations carefully crafted either at training or at test time can easily subvert their predictions. The vulnerability of machine learning to such wild patterns (also referred to as adversarial examples), along with the design of suitable countermeasures, have been investigated in the research field of adversarial machine learning. In this work, we provide a thorough overview of the evolution of this research area over the last ten years and beyond, starting from pioneering, earlier work on the security of non-deep learning algorithms up to more recent work aimed to understand the security properties of deep learning algorithms, in the context of computer vision and cybersecurity tasks. We report interesting connections between these apparently-different lines of work, highlighting common misconceptions related to the security evaluation of machine-learning algorithms. We review the main threat models and attacks defined to this end, and discuss the main limitations of current work, along with the corresponding future challenges towards the design of more secure learning algorithms.



### Weaving Multi-scale Context for Single Shot Detector
- **Arxiv ID**: http://arxiv.org/abs/1712.03149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03149v1)
- **Published**: 2017-12-08 16:16:08+00:00
- **Updated**: 2017-12-08 16:16:08+00:00
- **Authors**: Yunpeng Chen, Jianshu Li, Bin Zhou, Jiashi Feng, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Aggregating context information from multiple scales has been proved to be effective for improving accuracy of Single Shot Detectors (SSDs) on object detection. However, existing multi-scale context fusion techniques are computationally expensive, which unfavorably diminishes the advantageous speed of SSD. In this work, we propose a novel network topology, called WeaveNet, that can efficiently fuse multi-scale information and boost the detection accuracy with negligible extra cost. The proposed WeaveNet iteratively weaves context information from adjacent scales together to enable more sophisticated context reasoning while maintaining fast speed. Built by stacking light-weight blocks, WeaveNet is easy to train without requiring batch normalization and can be further accelerated by our proposed architecture simplification. Experimental results on PASCAL VOC 2007, PASCAL VOC 2012 benchmarks show signification performance boost brought by WeaveNet. For 320x320 input of batch size = 8, WeaveNet reaches 79.5% mAP on PASCAL VOC 2007 test in 101 fps with only 4 fps extra cost, and further improves to 79.7% mAP with more iterations.



### Combining Deep Universal Features, Semantic Attributes, and Hierarchical Classification for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.03151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03151v1)
- **Published**: 2017-12-08 16:17:31+00:00
- **Updated**: 2017-12-08 16:17:31+00:00
- **Authors**: Jared Markowitz, Aurora C. Schmidt, Philippe M. Burlina, I-Jeng Wang
- **Comment**: 17 pages, 4 figures, extension to work published in conference
  proceedings of 2017 IAPR MVA Conference
- **Journal**: None
- **Summary**: We address zero-shot (ZS) learning, building upon prior work in hierarchical classification by combining it with approaches based on semantic attribute estimation. For both non-novel and novel image classes we compare multiple formulations of the problem, starting with deep universal features in each case. We investigate the effect of using different posterior probabilities as inputs to the hierarchical classifier, comparing the performances of posteriors derived from distances to SVM classifier boundaries with those of posteriors based on semantic attribute estimation. Using a dataset consisting of 150 object classes from the ImageNet ILSVRC2012 data set, we find that the hierarchical classification method that maximizes expected reward for non-novel classes differs from the method that maximizes expected reward for novel classes. We also show that using input posteriors based on semantic attributes improves the expected reward for novel classes.



### Minimal Solvers for Monocular Rolling Shutter Compensation under Ackermann Motion
- **Arxiv ID**: http://arxiv.org/abs/1712.03159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03159v1)
- **Published**: 2017-12-08 16:26:43+00:00
- **Updated**: 2017-12-08 16:26:43+00:00
- **Authors**: Pulak Purkait, Christopher Zach
- **Comment**: Submitted to WACV 2018
- **Journal**: None
- **Summary**: Modern automotive vehicles are often equipped with a budget commercial rolling shutter camera. These devices often produce distorted images due to the inter-row delay of the camera while capturing the image. Recent methods for monocular rolling shutter motion compensation utilize blur kernel and the straightness property of line segments. However, these methods are limited to handling rotational motion and also are not fast enough to operate in real time. In this paper, we propose a minimal solver for the rolling shutter motion compensation which assumes known vertical direction of the camera. Thanks to the Ackermann motion model of vehicles which consists of only two motion parameters, and two parameters for the simplified depth assumption that lead to a 4-line algorithm. The proposed minimal solver estimates the rolling shutter camera motion efficiently and accurately. The extensive experiments on real and simulated datasets demonstrate the benefits of our approach in terms of qualitative and quantitative results.



### Class Rectification Hard Mining for Imbalanced Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.03162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03162v1)
- **Published**: 2017-12-08 16:32:56+00:00
- **Updated**: 2017-12-08 16:32:56+00:00
- **Authors**: Qi Dong, Shaogang Gong, Xiatian Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Recognising detailed facial or clothing attributes in images of people is a challenging task for computer vision, especially when the training data are both in very large scale and extremely imbalanced among different attribute classes. To address this problem, we formulate a novel scheme for batch incremental hard sample mining of minority attribute classes from imbalanced large scale training data. We develop an end-to-end deep learning framework capable of avoiding the dominant effect of majority classes by discovering sparsely sampled boundaries of minority classes. This is made possible by introducing a Class Rectification Loss (CRL) regularising algorithm. We demonstrate the advantages and scalability of CRL over existing state-of-the-art attribute recognition and imbalanced data learning models on two large scale imbalanced benchmark datasets, the CelebA facial attribute dataset and the X-Domain clothing attribute dataset.



### Basic Thresholding Classification
- **Arxiv ID**: http://arxiv.org/abs/1712.03217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03217v1)
- **Published**: 2017-12-08 18:46:11+00:00
- **Updated**: 2017-12-08 18:46:11+00:00
- **Authors**: Mehmet Altan Toks√∂z
- **Comment**: 128 pages, 45 figures, PHd Thesis
- **Journal**: None
- **Summary**: In this thesis, we propose a light-weight sparsity-based algorithm, basic thresholding classifier (BTC), for classification applications (such as face identification, hyper-spectral image classification, etc.) which is capable of identifying test samples extremely rapidly and performing high classification accuracy. Originally BTC is a linear classifier which works based on the assumption that the samples of the classes of a given dataset are linearly separable. However, in practice those samples may not be linearly separable. In this context, we also propose another algorithm namely kernel basic thresholding classifier (KBTC) which is a non-linear kernel version of the BTC algorithm. KBTC can achieve promising results especially when the given samples are linearly non-separable. For both proposals, we introduce sufficient identification conditions (SICs) under which BTC and KBTC can identify any test sample in the range space of a given dictionary. By using SICs, we develop parameter estimation procedures which do not require any cross validation. Both BTC and KBTC algorithms provide efficient classifier fusion schemes in which individual classifier outputs are combined to produce better classification results. For instance, for the application of face identification, this is done by combining the residuals having different random projectors. For spatial applications such as hyper-spectral image classification, the fusion is carried out by incorporating the spatial information, in which the output residual maps are filtered using a smoothing filter. Numerical results on publicly available face and hyper-spectral datasets show that our proposal outperforms well-known support vector machines (SVM)-based techniques, multinomial logistic regression (MLR)-based methods, and sparsity-based approaches like $l_1$-minimization and simultaneous orthogonal matching pursuit (SOMP).



### Transformational Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1712.03257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.03257v1)
- **Published**: 2017-12-08 19:21:15+00:00
- **Updated**: 2017-12-08 19:21:15+00:00
- **Authors**: Dimitrios C. Gklezakos, Rajesh P. N. Rao
- **Comment**: None
- **Journal**: None
- **Summary**: A fundamental problem faced by object recognition systems is that objects and their features can appear in different locations, scales and orientations. Current deep learning methods attempt to achieve invariance to local translations via pooling, discarding the locations of features in the process. Other approaches explicitly learn transformed versions of the same feature, leading to representations that quickly explode in size. Instead of discarding the rich and useful information about feature transformations to achieve invariance, we argue that models should learn object features conjointly with their transformations to achieve equivariance. We propose a new model of unsupervised learning based on sparse coding that can learn object features jointly with their affine transformations directly from images. Results based on learning from natural images indicate that our approach matches the reconstruction quality of traditional sparse coding but with significantly fewer degrees of freedom while simultaneously learning transformations from data. These results open the door to scaling up unsupervised learning to allow deep feature+transformation learning in a manner consistent with the ventral+dorsal stream architecture of the primate visual cortex.



