# Arxiv Papers in cs.CV on 2017-12-07
### CURE-TSR: Challenging Unreal and Real Environments for Traffic Sign Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.02463v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1712.02463v2)
- **Published**: 2017-12-07 01:09:23+00:00
- **Updated**: 2018-11-13 16:33:52+00:00
- **Authors**: Dogancan Temel, Gukyeong Kwon, Mohit Prabhushankar, Ghassan AlRegib
- **Comment**: 31st Conference on Neural Information Processing Systems (NIPS),
  Machine Learning for Intelligent Transportation Systems Workshop, Long Beach,
  CA, USA, 2017
- **Journal**: D. Temel, G. Kwon*, M. Prabhushankar*, and G. AlRegib, "CURE-TSR:
  Challenging unreal and real environments for traffic sign recognition,"
  Neural Information Processing Systems (NIPS) MLITSW, December 2017
- **Summary**: In this paper, we investigate the robustness of traffic sign recognition algorithms under challenging conditions. Existing datasets are limited in terms of their size and challenging condition coverage, which motivated us to generate the Challenging Unreal and Real Environments for Traffic Sign Recognition (CURE-TSR) dataset. It includes more than two million traffic sign images that are based on real-world and simulator data. We benchmark the performance of existing solutions in real-world scenarios and analyze the performance variation with respect to challenging conditions. We show that challenging conditions can decrease the performance of baseline methods significantly, especially if these challenging conditions result in loss or misplacement of spatial information. We also investigate the effect of data augmentation and show that utilization of simulator data along with real-world data enhance the average recognition performance in real-world scenarios. The dataset is publicly available at https://ghassanalregib.com/cure-tsr/.



### Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal
- **Arxiv ID**: http://arxiv.org/abs/1712.02478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02478v1)
- **Published**: 2017-12-07 02:57:38+00:00
- **Updated**: 2017-12-07 02:57:38+00:00
- **Authors**: Jifeng Wang, Xiang Li, Le Hui, Jian Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding shadows from a single image spontaneously derives into two types of task in previous studies, containing shadow detection and shadow removal. In this paper, we present a multi-task perspective, which is not embraced by any existing work, to jointly learn both detection and removal in an end-to-end fashion that aims at enjoying the mutually improved benefits from each other. Our framework is based on a novel STacked Conditional Generative Adversarial Network (ST-CGAN), which is composed of two stacked CGANs, each with a generator and a discriminator. Specifically, a shadow image is fed into the first generator which produces a shadow detection mask. That shadow image, concatenated with its predicted mask, goes through the second generator in order to recover its shadow-free image consequently. In addition, the two corresponding discriminators are very likely to model higher level relationships and global scene characteristics for the detected shadow region and reconstruction via removing shadows, respectively. More importantly, for multi-task learning, our design of stacked paradigm provides a novel view which is notably different from the commonly used one as the multi-branch version. To fully evaluate the performance of our proposed framework, we construct the first large-scale benchmark with 1870 image triplets (shadow image, shadow mask image, and shadow-free image) under 135 scenes. Extensive experimental results consistently show the advantages of ST-CGAN over several representative state-of-the-art methods on two large-scale publicly available datasets and our newly released one.



### Adversarial Examples that Fool Detectors
- **Arxiv ID**: http://arxiv.org/abs/1712.02494v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.02494v1)
- **Published**: 2017-12-07 05:13:54+00:00
- **Updated**: 2017-12-07 05:13:54+00:00
- **Authors**: Jiajun Lu, Hussein Sibai, Evan Fabry
- **Comment**: Follow up paper for adversarial stop signs. Submitted to CVPR 2018
- **Journal**: None
- **Summary**: An adversarial example is an example that has been adjusted to produce a wrong label when presented to a system at test time. To date, adversarial example constructions have been demonstrated for classifiers, but not for detectors. If adversarial examples that could fool a detector exist, they could be used to (for example) maliciously create security hazards on roads populated with smart vehicles. In this paper, we demonstrate a construction that successfully fools two standard detectors, Faster RCNN and YOLO. The existence of such examples is surprising, as attacking a classifier is very different from attacking a detector, and that the structure of detectors - which must search for their own bounding box, and which cannot estimate that box very accurately - makes it quite likely that adversarial patterns are strongly disrupted. We show that our construction produces adversarial examples that generalize well across sequences digitally, even though large perturbations are needed. We also show that our construction yields physical objects that are adversarial.



### CNNs are Globally Optimal Given Multi-Layer Support
- **Arxiv ID**: http://arxiv.org/abs/1712.02501v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1712.02501v2)
- **Published**: 2017-12-07 06:06:52+00:00
- **Updated**: 2017-12-14 14:21:43+00:00
- **Authors**: Chen Huang, Chen Kong, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: Stochastic Gradient Descent (SGD) is the central workhorse for training modern CNNs. Although giving impressive empirical performance it can be slow to converge. In this paper we explore a novel strategy for training a CNN using an alternation strategy that offers substantial speedups during training. We make the following contributions: (i) replace the ReLU non-linearity within a CNN with positive hard-thresholding, (ii) reinterpret this non-linearity as a binary state vector making the entire CNN linear if the multi-layer support is known, and (iii) demonstrate that under certain conditions a global optima to the CNN can be found through local descent. We then employ a novel alternation strategy (between weights and support) for CNN training that leads to substantially faster convergence rates, nice theoretical properties, and achieving state of the art results across large scale datasets (e.g. ImageNet) as well as other standard benchmarks.



### Take it in your stride: Do we need striding in CNNs?
- **Arxiv ID**: http://arxiv.org/abs/1712.02502v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.02502v1)
- **Published**: 2017-12-07 06:07:22+00:00
- **Updated**: 2017-12-07 06:07:22+00:00
- **Authors**: Chen Kong, Simon Lucey
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Since their inception, CNNs have utilized some type of striding operator to reduce the overlap of receptive fields and spatial dimensions. Although having clear heuristic motivations (i.e. lowering the number of parameters to learn) the mathematical role of striding within CNN learning remains unclear. This paper offers a novel and mathematical rigorous perspective on the role of the striding operator within modern CNNs. Specifically, we demonstrate theoretically that one can always represent a CNN that incorporates striding with an equivalent non-striding CNN which has more filters and smaller size. Through this equivalence we are then able to characterize striding as an additional mechanism for parameter sharing among channels, thus reducing training complexity. Finally, the framework presented in this paper offers a new mathematical perspective on the role of striding which we hope shall facilitate and simplify the future theoretical analysis of CNNs.



### TV-GAN: Generative Adversarial Network Based Thermal to Visible Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.02514v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02514v1)
- **Published**: 2017-12-07 07:06:39+00:00
- **Updated**: 2017-12-07 07:06:39+00:00
- **Authors**: Teng Zhang, Arnold Wiliem, Siqi Yang, Brian C. Lovell
- **Comment**: None
- **Journal**: None
- **Summary**: This work tackles the face recognition task on images captured using thermal camera sensors which can operate in the non-light environment. While it can greatly increase the scope and benefits of the current security surveillance systems, performing such a task using thermal images is a challenging problem compared to face recognition task in the Visible Light Domain (VLD). This is partly due to the much smaller amount number of thermal imagery data collected compared to the VLD data. Unfortunately, direct application of the existing very strong face recognition models trained using VLD data into the thermal imagery data will not produce a satisfactory performance. This is due to the existence of the domain gap between the thermal and VLD images. To this end, we propose a Thermal-to-Visible Generative Adversarial Network (TV-GAN) that is able to transform thermal face images into their corresponding VLD images whilst maintaining identity information which is sufficient enough for the existing VLD face recognition models to perform recognition. Some examples are presented in Figure 1. Unlike the previous methods, our proposed TV-GAN uses an explicit closed-set face recognition loss to regularize the discriminator network training. This information will then be conveyed into the generator network in the forms of gradient loss. In the experiment, we show that by using this additional explicit regularization for the discriminator network, the TV-GAN is able to preserve more identity information when translating a thermal image of a person which is not seen before by the TV-GAN.



### Broadcasting Convolutional Network for Visual Relational Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1712.02517v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02517v3)
- **Published**: 2017-12-07 07:21:04+00:00
- **Updated**: 2018-08-24 08:31:11+00:00
- **Authors**: Simyung Chang, John Yang, Seonguk Park, Nojun Kwak
- **Comment**: Accepted paper at ECCV 2018. 24 pages
- **Journal**: None
- **Summary**: In this paper, we propose the Broadcasting Convolutional Network (BCN) that extracts key object features from the global field of an entire input image and recognizes their relationship with local features. BCN is a simple network module that collects effective spatial features, embeds location information and broadcasts them to the entire feature maps. We further introduce the Multi-Relational Network (multiRN) that improves the existing Relation Network (RN) by utilizing the BCN module. In pixel-based relation reasoning problems, with the help of BCN, multiRN extends the concept of `pairwise relations' in conventional RNs to `multiwise relations' by relating each object with multiple objects at once. This yields in O(n) complexity for n objects, which is a vast computational gain from RNs that take O(n^2). Through experiments, multiRN has achieved a state-of-the-art performance on CLEVR dataset, which proves the usability of BCN on relation reasoning problems.



### Learning Random Fourier Features by Hybrid Constrained Optimization
- **Arxiv ID**: http://arxiv.org/abs/1712.02527v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.02527v1)
- **Published**: 2017-12-07 08:07:26+00:00
- **Updated**: 2017-12-07 08:07:26+00:00
- **Authors**: Jianqiao Wangni, Jingwei Zhuo, Jun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: The kernel embedding algorithm is an important component for adapting kernel methods to large datasets. Since the algorithm consumes a major computation cost in the testing phase, we propose a novel teacher-learner framework of learning computation-efficient kernel embeddings from specific data. In the framework, the high-precision embeddings (teacher) transfer the data information to the computation-efficient kernel embeddings (learner). We jointly select informative embedding functions and pursue an orthogonal transformation between two embeddings. We propose a novel approach of constrained variational expectation maximization (CVEM), where the alternate direction method of multiplier (ADMM) is applied over a nonconvex domain in the maximization step. We also propose two specific formulations based on the prevalent Random Fourier Feature (RFF), the masked and blocked version of Computation-Efficient RFF (CERF), by imposing a random binary mask or a block structure on the transformation matrix. By empirical studies of several applications on different real-world datasets, we demonstrate that the CERF significantly improves the performance of kernel methods upon the RFF, under certain arithmetic operation requirements, and suitable for structured matrix multiplication in Fastfood type algorithms.



### Maximum Classifier Discrepancy for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1712.02560v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02560v4)
- **Published**: 2017-12-07 10:49:33+00:00
- **Updated**: 2018-04-03 08:44:29+00:00
- **Authors**: Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: Accepted to CVPR2018 Oral, Code is available at
  https://github.com/mil-tokyo/MCD_DA
- **Journal**: None
- **Summary**: In this work, we present a method for unsupervised domain adaptation. Many adversarial learning methods train domain classifier networks to distinguish the features as either a source or target and train a feature generator network to mimic the discriminator. Two problems exist with these methods. First, the domain classifier only tries to distinguish the features as a source or target and thus does not consider task-specific decision boundaries between classes. Therefore, a trained generator can generate ambiguous features near class boundaries. Second, these methods aim to completely match the feature distributions between different domains, which is difficult because of each domain's characteristics.   To solve these problems, we introduce a new approach that attempts to align distributions of source and target by utilizing the task-specific decision boundaries. We propose to maximize the discrepancy between two classifiers' outputs to detect target samples that are far from the support of the source. A feature generator learns to generate target features near the support to minimize the discrepancy. Our method outperforms other methods on several datasets of image classification and semantic segmentation. The codes are available at \url{https://github.com/mil-tokyo/MCD_DA}



### Consistent Multiple Graph Matching with Multi-layer Random Walks Synchronization
- **Arxiv ID**: http://arxiv.org/abs/1712.02575v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02575v2)
- **Published**: 2017-12-07 11:59:54+00:00
- **Updated**: 2018-03-15 02:24:48+00:00
- **Authors**: Han-Mu Park, Kuk-Jin Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: We address the correspondence search problem among multiple graphs with complex properties while considering the matching consistency. We describe each pair of graphs by combining multiple attributes, then jointly match them in a unified framework. The main contribution of this paper is twofold. First, we formulate the global correspondence search problem of multi-attributed graphs by utilizing a set of multi-layer structures. The proposed formulation describes each pair of graphs as a multi-layer structure, and jointly considers whole matching pairs. Second, we propose a robust multiple graph matching method based on the multi-layer random walks framework. The proposed framework synchronizes movements of random walkers, and leads them to consistent matching candidates. In our extensive experiments, the proposed method exhibits robust and accurate performance over the state-of-the-art multiple graph matching algorithms.



### Solving internal covariate shift in deep learning with linked neurons
- **Arxiv ID**: http://arxiv.org/abs/1712.02609v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.02609v1)
- **Published**: 2017-12-07 13:26:26+00:00
- **Updated**: 2017-12-07 13:26:26+00:00
- **Authors**: Carles Roger Riera Molina, Oriol Pujol Vila
- **Comment**: Submitted to CVPR 2018. Code available at
  https://github.com/blauigris/linked_neurons
- **Journal**: None
- **Summary**: This work proposes a novel solution to the problem of internal covariate shift and dying neurons using the concept of linked neurons. We define the neuron linkage in terms of two constraints: first, all neuron activations in the linkage must have the same operating point. That is to say, all of them share input weights. Secondly, a set of neurons is linked if and only if there is at least one member of the linkage that has a non-zero gradient in regard to the input of the activation function. This means that for any input in the activation function, there is at least one member of the linkage that operates in a non-flat and non-zero area. This simple change has profound implications in the network learning dynamics. In this article we explore the consequences of this proposal and show that by using this kind of units, internal covariate shift is implicitly solved. As a result of this, the use of linked neurons allows to train arbitrarily large networks without any architectural or algorithmic trick, effectively removing the need of using re-normalization schemes such as Batch Normalization, which leads to halving the required training time. It also solves the problem of the need for standarized input data. Results show that the units using the linkage not only do effectively solve the aforementioned problems, but are also a competitive alternative with respect to state-of-the-art with very promising results.



### In-Place Activated BatchNorm for Memory-Optimized Training of DNNs
- **Arxiv ID**: http://arxiv.org/abs/1712.02616v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02616v3)
- **Published**: 2017-12-07 13:43:45+00:00
- **Updated**: 2018-10-26 07:46:48+00:00
- **Authors**: Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we present In-Place Activated Batch Normalization (InPlace-ABN) - a novel approach to drastically reduce the training memory footprint of modern deep neural networks in a computationally efficient way. Our solution substitutes the conventionally used succession of BatchNorm + Activation layers with a single plugin layer, hence avoiding invasive framework surgery while providing straightforward applicability for existing deep learning frameworks. We obtain memory savings of up to 50% by dropping intermediate results and by recovering required information during the backward pass through the inversion of stored forward results, with only minor increase (0.8-2%) in computation time. Also, we demonstrate how frequently used checkpointing approaches can be made computationally as efficient as InPlace-ABN. In our experiments on image classification, we demonstrate on-par results on ImageNet-1k with state-of-the-art approaches. On the memory-demanding task of semantic segmentation, we report results for COCO-Stuff, Cityscapes and Mapillary Vistas, obtaining new state-of-the-art results on the latter without additional training data but in a single-scale and -model scenario. Code can be found at https://github.com/mapillary/inplace_abn .



### Disentangled Person Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1712.02621v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02621v4)
- **Published**: 2017-12-07 14:03:12+00:00
- **Updated**: 2018-06-15 04:48:49+00:00
- **Authors**: Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc Van Gool, Bernt Schiele, Mario Fritz
- **Comment**: Published at CVPR'18 (Spotlight). Corresponding author is Qianru Sun
- **Journal**: None
- **Summary**: Generating novel, yet realistic, images of persons is a challenging task due to the complex interplay between the different image factors, such as the foreground, background and pose information. In this work, we aim at generating such images based on a novel, two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates novel person images at the same time. First, a multi-branched reconstruction network is proposed to disentangle and encode the three factors into embedding features, which are then combined to re-compose the input image itself. Second, three corresponding mapping functions are learned in an adversarial manner in order to map Gaussian noise to the learned embedding feature space, for each factor respectively. Using the proposed framework, we can manipulate the foreground, background and pose of the input image, and also sample new embedding features to generate such targeted manipulations, that provide more control over the generation process. Experiments on Market-1501 and Deepfashion datasets show that our model does not only generate realistic person images with new foregrounds, backgrounds and poses, but also manipulates the generated factors and interpolates the in-between states. Another set of experiments on Market-1501 shows that our model can also be beneficial for the person re-identification task.



### Using SVDD in SimpleMKL for 3D-Shapes Filtering
- **Arxiv ID**: http://arxiv.org/abs/1712.02658v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.02658v1)
- **Published**: 2017-12-07 14:56:53+00:00
- **Updated**: 2017-12-07 14:56:53+00:00
- **Authors**: Gaëlle Loosli, Hattoibe Aboubacar
- **Comment**: 9 pages, 6 figures, conference : https://cap2014.sciencesconf.org/
- **Journal**: CAp, conference d'apprentissage, July 2014, Saint-Etienne, France,
  pp.84-92
- **Summary**: This paper proposes the adaptation of Support Vector Data Description (SVDD) to the multiple kernel case (MK-SVDD), based on SimpleMKL. It also introduces a variant called Slim-MK-SVDD that is able to produce a tighter frontier around the data. For the sake of comparison, the equivalent methods are also developed for One-Class SVM, known to be very similar to SVDD for certain shapes of kernels.   Those algorithms are illustrated in the context of 3D-shapes filtering and outliers detection. For the 3D-shapes problem, the objective is to be able to select a sub-category of 3D-shapes, each sub-category being learned with our algorithm in order to create a filter. For outliers detection, we apply the proposed algorithms for unsupervised outliers detection as well as for the supervised case.



### Creating Capsule Wardrobes from Fashion Images
- **Arxiv ID**: http://arxiv.org/abs/1712.02662v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02662v2)
- **Published**: 2017-12-07 15:06:26+00:00
- **Updated**: 2018-04-14 17:02:40+00:00
- **Authors**: Wei-Lin Hsiao, Kristen Grauman
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: We propose to automatically create capsule wardrobes. Given an inventory of candidate garments and accessories, the algorithm must assemble a minimal set of items that provides maximal mix-and-match outfits. We pose the task as a subset selection problem. To permit efficient subset selection over the space of all outfit combinations, we develop submodular objective functions capturing the key ingredients of visual compatibility, versatility, and user-specific preference. Since adding garments to a capsule only expands its possible outfits, we devise an iterative approach to allow near-optimal submodular function maximization. Finally, we present an unsupervised approach to learn visual compatibility from "in the wild" full body outfit photos; the compatibility metric translates well to cleaner catalog photos and improves over existing methods. Our results on thousands of pieces from popular fashion websites show that automatic capsule creation has potential to mimic skilled fashionistas in assembling flexible wardrobes, while being significantly more scalable.



### Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing
- **Arxiv ID**: http://arxiv.org/abs/1712.02719v4
- **DOI**: 10.1109/ACCESS.2019.2963056
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02719v4)
- **Published**: 2017-12-07 17:05:54+00:00
- **Updated**: 2019-05-02 06:20:19+00:00
- **Authors**: Syed Shakib Sarwar, Aayush Ankit, Kaushik Roy
- **Comment**: 18 pages, 13 figures. IEEE Access 2019
- **Journal**: None
- **Summary**: Deep convolutional neural network (DCNN) based supervised learning is a widely practiced approach for large-scale image classification. However, retraining these large networks to accommodate new, previously unseen data demands high computational time and energy requirements. Also, previously seen training samples may not be available at the time of retraining. We propose an efficient training methodology and incrementally growing DCNN to learn new tasks while sharing part of the base network. Our proposed methodology is inspired by transfer learning techniques, although it does not forget previously learned tasks. An updated network for learning new set of classes is formed using previously learned convolutional layers (shared from initial part of base network) with addition of few newly added convolutional kernels included in the later layers of the network. We employed a `clone-and-branch' technique which allows the network to learn new tasks one after another without any performance loss in old tasks. We evaluated the proposed scheme on several recognition applications. The classification accuracy achieved by our approach is comparable to the regular incremental learning approach (where networks are updated with new training samples only, without any network sharing), while achieving energy efficiency, reduction in storage requirements, memory access and training time.



### Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for Transferable Chemical Property Prediction
- **Arxiv ID**: http://arxiv.org/abs/1712.02734v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.02734v2)
- **Published**: 2017-12-07 17:25:48+00:00
- **Updated**: 2018-03-18 13:50:02+00:00
- **Authors**: Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas
- **Comment**: Submitted to SIGKDD 2018
- **Journal**: None
- **Summary**: With access to large datasets, deep neural networks (DNN) have achieved human-level accuracy in image and speech recognition tasks. However, in chemistry, data is inherently small and fragmented. In this work, we develop an approach of using rule-based knowledge for training ChemNet, a transferable and generalizable deep neural network for chemical property prediction that learns in a weak-supervised manner from large unlabeled chemical databases. When coupled with transfer learning approaches to predict other smaller datasets for chemical properties that it was not originally trained on, we show that ChemNet's accuracy outperforms contemporary DNN models that were trained using conventional supervised learning. Furthermore, we demonstrate that the ChemNet pre-training approach is equally effective on both CNN (Chemception) and RNN (SMILES2vec) models, indicating that this approach is network architecture agnostic and is effective across multiple data modalities. Our results indicate a pre-trained ChemNet that incorporates chemistry domain knowledge, enables the development of generalizable neural networks for more accurate prediction of novel chemical properties.



### End-to-end Learning of Deterministic Decision Trees
- **Arxiv ID**: http://arxiv.org/abs/1712.02743v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.02743v1)
- **Published**: 2017-12-07 17:40:25+00:00
- **Updated**: 2017-12-07 17:40:25+00:00
- **Authors**: Thomas Hehn, Fred A. Hamprecht
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional decision trees have a number of favorable properties, including interpretability, a small computational footprint and the ability to learn from little training data. However, they lack a key quality that has helped fuel the deep learning revolution: that of being end-to-end trainable, and to learn from scratch those features that best allow to solve a given supervised learning problem. Recent work (Kontschieder 2015) has addressed this deficit, but at the cost of losing a main attractive trait of decision trees: the fact that each sample is routed along a small subset of tree nodes only. We here propose a model and Expectation-Maximization training scheme for decision trees that are fully probabilistic at train time, but after a deterministic annealing process become deterministic at test time. We also analyze the learned oblique split parameters on image datasets and show that Neural Networks can be trained at each split node. In summary, we present the first end-to-end learning scheme for deterministic decision trees and present results on par with or superior to published standard oblique decision tree algorithms.



### On the Duality Between Retinex and Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1712.02754v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02754v2)
- **Published**: 2017-12-07 18:03:19+00:00
- **Updated**: 2018-04-06 16:48:15+00:00
- **Authors**: Adrian Galdran, Aitor Alvarez-Gila, Alessandro Bria, Javier Vazquez-Corral, Marcelo Bertalmio
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: Image dehazing deals with the removal of undesired loss of visibility in outdoor images due to the presence of fog. Retinex is a color vision model mimicking the ability of the Human Visual System to robustly discount varying illuminations when observing a scene under different spectral lighting conditions. Retinex has been widely explored in the computer vision literature for image enhancement and other related tasks. While these two problems are apparently unrelated, the goal of this work is to show that they can be connected by a simple linear relationship. Specifically, most Retinex-based algorithms have the characteristic feature of always increasing image brightness, which turns them into ideal candidates for effective image dehazing by directly applying Retinex to a hazy image whose intensities have been inverted. In this paper, we give theoretical proof that Retinex on inverted intensities is a solution to the image dehazing problem. Comprehensive qualitative and quantitative results indicate that several classical and modern implementations of Retinex can be transformed into competing image dehazing algorithms performing on pair with more complex fog removal methods, and can overcome some of the main challenges associated with this problem.



### Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs
- **Arxiv ID**: http://arxiv.org/abs/1712.02765v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02765v2)
- **Published**: 2017-12-07 18:24:48+00:00
- **Updated**: 2018-03-27 13:38:27+00:00
- **Authors**: Adrian Bulat, Georgios Tzimiropoulos
- **Comment**: CVPR 2018 SPOTLIGHT
- **Journal**: None
- **Summary**: This paper addresses 2 challenging tasks: improving the quality of low resolution facial images and accurately locating the facial landmarks on such poor resolution images. To this end, we make the following 5 contributions: (a) we propose Super-FAN: the very first end-to-end system that addresses both tasks simultaneously, i.e. both improves face resolution and detects the facial landmarks. The novelty or Super-FAN lies in incorporating structural information in a GAN-based super-resolution algorithm via integrating a sub-network for face alignment through heatmap regression and optimizing a novel heatmap loss. (b) We illustrate the benefit of training the two networks jointly by reporting good results not only on frontal images (as in prior work) but on the whole spectrum of facial poses, and not only on synthetic low resolution images (as in prior work) but also on real-world images. (c) We improve upon the state-of-the-art in face super-resolution by proposing a new residual-based architecture. (d) Quantitatively, we show large improvement over the state-of-the-art for both face super-resolution and alignment. (e) Qualitatively, we show for the first time good results on real-world low resolution images.



### Exploring the Landscape of Spatial Robustness
- **Arxiv ID**: http://arxiv.org/abs/1712.02779v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.02779v4)
- **Published**: 2017-12-07 18:53:52+00:00
- **Updated**: 2019-09-16 04:38:13+00:00
- **Authors**: Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, Aleksander Madry
- **Comment**: ICML 2019. Presented in NIPS 2017 Workshop on Machine Learning and
  Computer Security as "A Rotation and a Translation Suffice: Fooling CNNs with
  Simple Transformations."
- **Journal**: None
- **Summary**: The study of adversarial robustness has so far largely focused on perturbations bound in p-norms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of neural network--based classifiers to rotations and translations. While data augmentation offers relatively small robustness, we use ideas from robust optimization and test-time input aggregation to significantly improve robustness. Finally we find that, in contrast to the p-norm case, first-order methods cannot reliably find worst-case perturbations. This highlights spatial robustness as a fundamentally different setting requiring additional study. Code available at https://github.com/MadryLab/adversarial_spatial and https://github.com/MadryLab/spatial-pytorch.



### On Usage of Autoencoders and Siamese Networks for Online Handwritten Signature Verification
- **Arxiv ID**: http://arxiv.org/abs/1712.02781v2
- **DOI**: 10.1007/s00521-018-3844-z
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.02781v2)
- **Published**: 2017-12-07 18:55:42+00:00
- **Updated**: 2017-12-29 07:12:41+00:00
- **Authors**: Kian Ahrabian, Bagher Babaali
- **Comment**: 13 pages, 10 figures, Submitted to Neural Computing and Applications
  journal
- **Journal**: None
- **Summary**: In this paper, we propose a novel writer-independent global feature extraction framework for the task of automatic signature verification which aims to make robust systems for automatically distinguishing negative and positive samples. Our method consists of an autoencoder for modeling the sample space into a fixed length latent space and a Siamese Network for classifying the fixed-length samples obtained from the autoencoder based on the reference samples of a subject as being "Genuine" or "Forged." During our experiments, usage of Attention Mechanism and applying Downsampling significantly improved the accuracy of the proposed framework. We evaluated our proposed framework using SigWiComp2013 Japanese and GPDSsyntheticOnLineOffLineSignature datasets. On the SigWiComp2013 Japanese dataset, we achieved 8.65% EER that means 1.2% relative improvement compared to the best-reported result. Furthermore, on the GPDSsyntheticOnLineOffLineSignature dataset, we achieved average EERs of 0.13%, 0.12%, 0.21% and 0.25% respectively for 150, 300, 1000 and 2000 test subjects which indicates improvement of relative EER on the best-reported result by 95.67%, 95.26%, 92.9% and 91.52% respectively. Apart from the accuracy gain, because of the nature of our proposed framework which is based on neural networks and consequently is as simple as some consecutive matrix multiplications, it has less computational cost than conventional methods such as DTW and could be used concurrently on devices such as GPU, TPU, etc.



### Hybrid eye center localization using cascaded regression and hand-crafted model fitting
- **Arxiv ID**: http://arxiv.org/abs/1712.02822v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02822v1)
- **Published**: 2017-12-07 19:21:19+00:00
- **Updated**: 2017-12-07 19:21:19+00:00
- **Authors**: Alex Levinshtein, Edmund Phung, Parham Aarabi
- **Comment**: 12 pages, 5 figures, submitted to Journal of Image and Vision
  Computing
- **Journal**: None
- **Summary**: We propose a new cascaded regressor for eye center detection. Previous methods start from a face or an eye detector and use either advanced features or powerful regressors for eye center localization, but not both. Instead, we detect the eyes more accurately using an existing facial feature alignment method. We improve the robustness of localization by using both advanced features and powerful regression machinery. Unlike most other methods that do not refine the regression results, we make the localization more accurate by adding a robust circle fitting post-processing step. Finally, using a simple hand-crafted method for eye center localization, we show how to train the cascaded regressor without the need for manually annotated training data. We evaluate our new approach and show that it achieves state-of-the-art performance on the BioID, GI4E, and the TalkingFace datasets. At an average normalized error of e < 0.05, the regressor trained on manually annotated data yields an accuracy of 95.07% (BioID), 99.27% (GI4E), and 95.68% (TalkingFace). The automatically trained regressor is nearly as good, yielding an accuracy of 93.9% (BioID), 99.27% (GI4E), and 95.46% (TalkingFace).



### Stacked Denoising Autoencoders and Transfer Learning for Immunogold Particles Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.02824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02824v1)
- **Published**: 2017-12-07 19:28:05+00:00
- **Updated**: 2017-12-07 19:28:05+00:00
- **Authors**: Ricardo Gamelas Sousa, Jorge M. Santos, Luís M. Silva, Luís A. Alexandre, Tiago Esteves, Sara Rocha, Paulo Monjardino, Joaquim Marques de Sá, Francisco Figueiredo, Pedro Quelhas
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a system for the detection of immunogold particles and a Transfer Learning (TL) framework for the recognition of these immunogold particles. Immunogold particles are part of a high-magnification method for the selective localization of biological molecules at the subcellular level only visible through Electron Microscopy. The number of immunogold particles in the cell walls allows the assessment of the differences in their compositions providing a tool to analise the quality of different plants. For its quantization one requires a laborious manual labeling (or annotation) of images containing hundreds of particles. The system that is proposed in this paper can leverage significantly the burden of this manual task.   For particle detection we use a LoG filter coupled with a SDA. In order to improve the recognition, we also study the applicability of TL settings for immunogold recognition. TL reuses the learning model of a source problem on other datasets (target problems) containing particles of different sizes. The proposed system was developed to solve a particular problem on maize cells, namely to determine the composition of cell wall ingrowths in endosperm transfer cells. This novel dataset as well as the code for reproducing our experiments is made publicly available.   We determined that the LoG detector alone attained more than 84\% of accuracy with the F-measure. Developing immunogold recognition with TL also provided superior performance when compared with the baseline models augmenting the accuracy rates by 10\%.



### Stochastic reconstruction of an oolitic limestone by generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1712.02854v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.geo-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.02854v1)
- **Published**: 2017-12-07 20:21:01+00:00
- **Updated**: 2017-12-07 20:21:01+00:00
- **Authors**: Lukas Mosser, Olivier Dubrule, Martin J. Blunt
- **Comment**: 22 pages, 14 figures
- **Journal**: None
- **Summary**: Stochastic image reconstruction is a key part of modern digital rock physics and materials analysis that aims to create numerous representative samples of material micro-structures for upscaling, numerical computation of effective properties and uncertainty quantification. We present a method of three-dimensional stochastic image reconstruction based on generative adversarial neural networks (GANs). GANs represent a framework of unsupervised learning methods that require no a priori inference of the probability distribution associated with the training data. Using a fully convolutional neural network allows fast sampling of large volumetric images.We apply a GAN based workflow of network training and image generation to an oolitic Ketton limestone micro-CT dataset. Minkowski functionals, effective permeability as well as velocity distributions of simulated flow within the acquired images are compared with the synthetic reconstructions generated by the deep neural network. While our results show that GANs allow a fast and accurate reconstruction of the evaluated image dataset, we address a number of open questions and challenges involved in the evaluation of generative network-based methods.



### Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250 Hz
- **Arxiv ID**: http://arxiv.org/abs/1712.02859v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02859v2)
- **Published**: 2017-12-07 21:04:51+00:00
- **Updated**: 2018-03-29 18:41:32+00:00
- **Authors**: Ayush Tewari, Michael Zollhöfer, Pablo Garrido, Florian Bernard, Hyeongwoo Kim, Patrick Pérez, Christian Theobalt
- **Comment**: CVPR 2018 (Oral). Project webpage:
  https://gvv.mpi-inf.mpg.de/projects/FML/
- **Journal**: None
- **Summary**: The reconstruction of dense 3D models of face geometry and appearance from a single image is highly challenging and ill-posed. To constrain the problem, many approaches rely on strong priors, such as parametric face models learned from limited 3D scan data. However, prior models restrict generalization of the true diversity in facial geometry, skin reflectance and illumination. To alleviate this problem, we present the first approach that jointly learns 1) a regressor for face shape, expression, reflectance and illumination on the basis of 2) a concurrently learned parametric face model. Our multi-level face model combines the advantage of 3D Morphable Models for regularization with the out-of-space generalization of a learned corrective space. We train end-to-end on in-the-wild images without dense annotations by fusing a convolutional encoder with a differentiable expert-designed renderer and a self-supervised training loss, both defined at multiple detail levels. Our approach compares favorably to the state-of-the-art in terms of reconstruction quality, better generalizes to real world faces, and runs at over 250 Hz.



### Per-Pixel Feedback for improving Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1712.02861v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.02861v1)
- **Published**: 2017-12-07 21:12:53+00:00
- **Updated**: 2017-12-07 21:12:53+00:00
- **Authors**: Aditya Ganeshan
- **Comment**: 33 pages,18 figures,3 tables
- **Journal**: None
- **Summary**: Semantic segmentation is the task of assigning a label to each pixel in the image.In recent years, deep convolutional neural networks have been driving advances in multiple tasks related to cognition. Although, DCNNs have resulted in unprecedented visual recognition performances, they offer little transparency. To understand how DCNN based models work at the task of semantic segmentation, we try to analyze the DCNN models in semantic segmentation. We try to find the importance of global image information for labeling pixels.   Based on the experiments on discriminative regions, and modeling of fixations, we propose a set of new training loss functions for fine-tuning DCNN based models. The proposed training regime has shown improvement in performance of DeepLab Large FOV(VGG-16) Segmentation model for PASCAL VOC 2012 dataset. However, further test remains to conclusively evaluate the benefits due to the proposed loss functions across models, and data-sets.   Submitted in part fulfillment of the requirements for the degree of Integrated Masters of Science in Applied Mathematics.   Update: Further Experiment showed minimal benefits.   Code Available [here](https://github.com/BardOfCodes/Seg-Unravel).



### MoDL: Model Based Deep Learning Architecture for Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/1712.02862v4
- **DOI**: 10.1109/TMI.2018.2865356
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02862v4)
- **Published**: 2017-12-07 21:13:13+00:00
- **Updated**: 2019-06-05 16:31:11+00:00
- **Authors**: Hemant Kumar Aggarwal, Merry P. Mani, Mathews Jacob
- **Comment**: published in IEEE Transaction on Medical Imaging
- **Journal**: None
- **Summary**: We introduce a model-based image reconstruction framework with a convolution neural network (CNN) based regularization prior. The proposed formulation provides a systematic approach for deriving deep architectures for inverse problems with the arbitrary structure. Since the forward model is explicitly accounted for, a smaller network with fewer parameters is sufficient to capture the image information compared to black-box deep learning approaches, thus reducing the demand for training data and training time. Since we rely on end-to-end training, the CNN weights are customized to the forward model, thus offering improved performance over approaches that rely on pre-trained denoisers. The main difference of the framework from existing end-to-end training strategies is the sharing of the network weights across iterations and channels. Our experiments show that the decoupling of the number of iterations from the network complexity offered by this approach provides benefits including lower demand for training data, reduced risk of overfitting, and implementations with significantly reduced memory footprint. We propose to enforce data-consistency by using numerical optimization blocks such as conjugate gradients algorithm within the network; this approach offers faster convergence per iteration, compared to methods that rely on proximal gradients steps to enforce data consistency. Our experiments show that the faster convergence translates to improved performance, especially when the available GPU memory restricts the number of iterations.



### Learned Perceptual Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1712.02864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02864v1)
- **Published**: 2017-12-07 21:23:12+00:00
- **Updated**: 2017-12-07 21:23:12+00:00
- **Authors**: Hossein Talebi, Peyman Milanfar
- **Comment**: None
- **Journal**: None
- **Summary**: Learning a typical image enhancement pipeline involves minimization of a loss function between enhanced and reference images. While L1 and L2 losses are perhaps the most widely used functions for this purpose, they do not necessarily lead to perceptually compelling results. In this paper, we show that adding a learned no-reference image quality metric to the loss can significantly improve enhancement operators. This metric is implemented using a CNN (convolutional neural network) trained on a large-scale dataset labelled with aesthetic preferences of human raters. This loss allows us to conveniently perform back-propagation in our learning framework to simultaneously optimize for similarity to a given ground truth reference and perceptual quality. This perceptual loss is only used to train parameters of image processing operators, and does not impose any extra complexity at inference time. Our experiments demonstrate that this loss can be effective for tuning a variety of operators such as local tone mapping and dehazing.



### Multi-Scale Video Frame-Synthesis Network with Transitive Consistency Loss
- **Arxiv ID**: http://arxiv.org/abs/1712.02874v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02874v2)
- **Published**: 2017-12-07 22:01:00+00:00
- **Updated**: 2018-03-19 21:38:12+00:00
- **Authors**: Zhe Hu, Yinglan Ma, Lizhuang Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional approaches to interpolate/extrapolate frames in a video sequence require accurate pixel correspondences between images, e.g., using optical flow. Their results stem on the accuracy of optical flow estimation, and could generate heavy artifacts when flow estimation failed. Recently methods using auto-encoder has shown impressive progress, however they are usually trained for specific interpolation/extrapolation settings and lack of flexibility and In order to reduce these limitations, we propose a unified network to parameterize the interest frame position and therefore infer interpolate/extrapolate frames within the same framework. To achieve this, we introduce a transitive consistency loss to better regularize the network. We adopt a multi-scale structure for the network so that the parameters can be shared across multi-layers. Our approach avoids expensive global optimization of optical flow methods, and is efficient and flexible for video interpolation/extrapolation applications. Experimental results have shown that our method performs favorably against state-of-the-art methods.



### An End to End Deep Neural Network for Iris Segmentation in Unconstraint Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1712.02877v1
- **DOI**: 10.1016/j.neunet.2018.06.011
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.02877v1)
- **Published**: 2017-12-07 22:18:38+00:00
- **Updated**: 2017-12-07 22:18:38+00:00
- **Authors**: Shabab Bazrafkan, Shejin Thavalengal, Peter Corcoran
- **Comment**: None
- **Journal**: None
- **Summary**: With the increasing imaging and processing capabilities of today's mobile devices, user authentication using iris biometrics has become feasible. However, as the acquisition conditions become more unconstrained and as image quality is typically lower than dedicated iris acquisition systems, the accurate segmentation of iris regions is crucial for these devices. In this work, an end to end Fully Convolutional Deep Neural Network (FCDNN) design is proposed to perform the iris segmentation task for lower-quality iris images. The network design process is explained in detail, and the resulting network is trained and tuned using several large public iris datasets. A set of methods to generate and augment suitable lower quality iris images from the high-quality public databases are provided. The network is trained on Near InfraRed (NIR) images initially and later tuned on additional datasets derived from visible images. Comprehensive inter-database comparisons are provided together with results from a selection of experiments detailing the effects of different tunings of the network. Finally, the proposed model is compared with SegNet-basic, and a near-optimal tuning of the network is compared to a selection of other state-of-art iris segmentation algorithms. The results show very promising performance from the optimized Deep Neural Networks design when compared with state-of-art techniques applied to the same lower quality datasets.



### Network Analysis for Explanation
- **Arxiv ID**: http://arxiv.org/abs/1712.02890v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02890v1)
- **Published**: 2017-12-07 23:35:11+00:00
- **Updated**: 2017-12-07 23:35:11+00:00
- **Authors**: Hiroshi Kuwajima, Masayuki Tanaka
- **Comment**: None
- **Journal**: None
- **Summary**: Safety critical systems strongly require the quality aspects of artificial intelligence including explainability. In this paper, we analyzed a trained network to extract features which mainly contribute the inference. Based on the analysis, we developed a simple solution to generate explanations of the inference processes.



### Deep Texture and Structure Aware Filtering Network for Image Smoothing
- **Arxiv ID**: http://arxiv.org/abs/1712.02893v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.02893v2)
- **Published**: 2017-12-07 23:54:26+00:00
- **Updated**: 2018-05-08 00:51:15+00:00
- **Authors**: Kaiyue Lu, Shaodi You, Nick Barnes
- **Comment**: None
- **Journal**: None
- **Summary**: Image smoothing is a fundamental task in computer vision, that aims to retain salient structures and remove insignificant textures. In this paper, we aim to address the fundamental shortcomings of existing image smoothing methods, which cannot properly distinguish textures and structures with similar low-level appearance. While deep learning approaches have started to explore the preservation of structure through image smoothing, existing work does not yet properly address textures. To this end, we generate a large dataset by blending natural textures with clean structure-only images, and then build a texture prediction network (TPN) that predicts the location and magnitude of textures. We then combine the TPN with a semantic structure prediction network (SPN) so that the final texture and structure aware filtering network (TSAFN) is able to identify the textures to remove ("texture-awareness") and the structures to preserve ("structure-awareness"). The proposed model is easy to understand and implement, and shows excellent performance on real images in the wild as well as our generated dataset.



