# Arxiv Papers in cs.CV on 2017-12-01
### Video retrieval based on deep convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1712.00133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00133v1)
- **Published**: 2017-12-01 00:07:47+00:00
- **Updated**: 2017-12-01 00:07:47+00:00
- **Authors**: Yj Dong, JG Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, with the enormous growth of online videos, fast video retrieval research has received increasing attention. As an extension of image hashing techniques, traditional video hashing methods mainly depend on hand-crafted features and transform the real-valued features into binary hash codes. As videos provide far more diverse and complex visual information than images, extracting features from videos is much more challenging than that from images. Therefore, high-level semantic features to represent videos are needed rather than low-level hand-crafted methods. In this paper, a deep convolutional neural network is proposed to extract high-level semantic features and a binary hash function is then integrated into this framework to achieve an end-to-end optimization. Particularly, our approach also combines triplet loss function which preserves the relative similarity and difference of videos and classification loss function as the optimization objective. Experiments have been performed on two public datasets and the results demonstrate the superiority of our proposed method compared with other state-of-the-art video retrieval methods.



### Distance-based Camera Network Topology Inference for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1712.00158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00158v1)
- **Published**: 2017-12-01 02:21:58+00:00
- **Updated**: 2017-12-01 02:21:58+00:00
- **Authors**: Yeong-Jun Cho, Kuk-Jin Yoon
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: In this paper, we propose a novel distance-based camera network topology inference method for efficient person re-identification. To this end, we first calibrate each camera and estimate relative scales between cameras. Using the calibration results of multiple cameras, we calculate the speed of each person and infer the distance between cameras to generate distance-based camera network topology. The proposed distance-based topology can be applied adaptively to each person according to its speed and handle diverse transition time of people between non-overlapping cameras. To validate the proposed method, we tested the proposed method using an open person re-identification dataset and compared to state-of-the-art methods. The experimental results show that the proposed method is effective for person re-identification in the large-scale camera network with various people transition time.



### Learning Depth from Monocular Videos using Direct Methods
- **Arxiv ID**: http://arxiv.org/abs/1712.00175v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00175v1)
- **Published**: 2017-12-01 03:37:18+00:00
- **Updated**: 2017-12-01 03:37:18+00:00
- **Authors**: Chaoyang Wang, Jose Miguel Buenaposada, Rui Zhu, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to predict depth from a single image - using recent advances in CNNs - is of increasing interest to the vision community. Unsupervised strategies to learning are particularly appealing as they can utilize much larger and varied monocular video datasets during learning without the need for ground truth depth or stereo. In previous works, separate pose and depth CNN predictors had to be determined such that their joint outputs minimized the photometric error. Inspired by recent advances in direct visual odometry (DVO), we argue that the depth CNN predictor can be learned without a pose CNN predictor. Further, we demonstrate empirically that incorporation of a differentiable implementation of DVO, along with a novel depth normalization strategy - substantially improves performance over state of the art that use monocular videos for training.



### Inertial-aided Rolling Shutter Relative Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1712.00184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00184v1)
- **Published**: 2017-12-01 04:16:36+00:00
- **Updated**: 2017-12-01 04:16:36+00:00
- **Authors**: Chang-Ryeol Lee, Kuk-Jin Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: Relative pose estimation is a fundamental problem in computer vision and it has been studied for conventional global shutter cameras for decades. However, recently, a rolling shutter camera has been widely used due to its low cost imaging capability and, since the rolling shutter camera captures the image line-by-line, the relative pose estimation of a rolling shutter camera is more difficult than that of a global shutter camera. In this paper, we propose to exploit inertial measurements (gravity and angular velocity) for the rolling shutter relative pose estimation problem. The inertial measurements provide information about the partial relative rotation between two views (cameras) and the instantaneous motion that causes the rolling shutter distortion. Based on this information, we simplify the rolling shutter relative pose estimation problem and propose effective methods to solve it. Unlike the previous methods, which require 44 (linear) or 17 (nonlinear) points with the uniform rolling shutter camera model, the proposed methods require at most 9 or 11 points to estimate the relative pose between the rolling shutter cameras. Experimental results on synthetic data and the public PennCOSYVIO dataset show that the proposed methods outperform the existing methods.



### Rank of Experts: Detection Network Ensemble
- **Arxiv ID**: http://arxiv.org/abs/1712.00185v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00185v1)
- **Published**: 2017-12-01 04:27:20+00:00
- **Updated**: 2017-12-01 04:27:20+00:00
- **Authors**: Seung-Hwan Bae, Youngwan Lee, Youngjoo Jo, Yuseok Bae, Joong-won Hwang
- **Comment**: None
- **Journal**: None
- **Summary**: The recent advances of convolutional detectors show impressive performance improvement for large scale object detection. However, in general, the detection performance usually decreases as the object classes to be detected increases, and it is a practically challenging problem to train a dominant model for all classes due to the limitations of detection models and datasets. In most cases, therefore, there are distinct performance differences of the modern convolutional detectors for each object class detection. In this paper, in order to build an ensemble detector for large scale object detection, we present a conceptually simple but very effective class-wise ensemble detection which is named as Rank of Experts. We first decompose an intractable problem of finding the best detections for all object classes into small subproblems of finding the best ones for each object class. We then solve the detection problem by ranking detectors in order of the average precision rate for each class, and then aggregate the responses of the top ranked detectors (i.e. experts) for class-wise ensemble detection. The main benefit of our method is easy to implement and does not require any joint training of experts for ensemble. Based on the proposed Rank of Experts, we won the 2nd place in the ILSVRC 2017 object detection competition.



### Delineation of Skin Strata in Reflectance Confocal Microscopy Images using Recurrent Convolutional Networks with Toeplitz Attention
- **Arxiv ID**: http://arxiv.org/abs/1712.00192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00192v1)
- **Published**: 2017-12-01 04:59:25+00:00
- **Updated**: 2017-12-01 04:59:25+00:00
- **Authors**: Alican Bozkurt, Kivanc Kose, Jaume Coll-Font, Christi Alessi-Fox, Dana H. Brooks, Jennifer G. Dy, Milind Rajadhyaksha
- **Comment**: Accepted for ML4H Workshop at NIPS 2017
- **Journal**: None
- **Summary**: Reflectance confocal microscopy (RCM) is an effective, non-invasive pre-screening tool for skin cancer diagnosis, but it requires extensive training and experience to assess accurately. There are few quantitative tools available to standardize image acquisition and analysis, and the ones that are available are not interpretable. In this study, we use a recurrent neural network with attention on convolutional network features. We apply it to delineate skin strata in vertically-oriented stacks of transverse RCM image slices in an interpretable manner. We introduce a new attention mechanism called Toeplitz attention, which constrains the attention map to have a Toeplitz structure. Testing our model on an expert labeled dataset of 504 RCM stacks, we achieve 88.17% image-wise classification accuracy, which is the current state-of-art.



### InclusiveFaceNet: Improving Face Attribute Detection with Race and Gender Diversity
- **Arxiv ID**: http://arxiv.org/abs/1712.00193v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1712.00193v3)
- **Published**: 2017-12-01 05:00:16+00:00
- **Updated**: 2018-07-17 12:29:08+00:00
- **Authors**: Hee Jung Ryu, Hartwig Adam, Margaret Mitchell
- **Comment**: Presented as a talk at the 2018 Workshop on Fairness, Accountability,
  and Transparency in Machine Learning (FAT/ML 2018)
- **Journal**: None
- **Summary**: We demonstrate an approach to face attribute detection that retains or improves attribute detection accuracy across gender and race subgroups by learning demographic information prior to learning the attribute detection task. The system, which we call InclusiveFaceNet, detects face attributes by transferring race and gender representations learned from a held-out dataset of public race and gender identities. Leveraging learned demographic representations while withholding demographic inference from the downstream face attribute detection task preserves potential users' demographic privacy while resulting in some of the best reported numbers to date on attribute detection in the Faces of the World and CelebA datasets.



### 3D Facial Action Units Recognition for Emotional Expression
- **Arxiv ID**: http://arxiv.org/abs/1712.00195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00195v1)
- **Published**: 2017-12-01 05:03:17+00:00
- **Updated**: 2017-12-01 05:03:17+00:00
- **Authors**: N. Hussain, H. Ujir, I. Hipiny, J-L Minoi
- **Comment**: To be published in Advanced Science Letters Volume 24 (ICCSE2017)
- **Journal**: None
- **Summary**: The muscular activities caused the activation of certain AUs for every facial expression at the certain duration of time throughout the facial expression. This paper presents the methods to recognise facial Action Unit (AU) using facial distance of the facial features which activates the muscles. The seven facial action units involved are AU1, AU4, AU6, AU12, AU15, AU17 and AU25 that characterises happy and sad expression. The recognition is performed on each AU according to rules defined based on the distance of each facial points. The facial distances chosen are extracted from twelve facial features. Then the facial distances are trained using Support Vector Machine (SVM) and Neural Network (NN). Classification result using SVM is presented with several different SVM kernels while result using NN is presented for each training, validation and testing phase.



### A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1712.00201v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00201v2)
- **Published**: 2017-12-01 05:57:19+00:00
- **Updated**: 2018-08-02 02:30:38+00:00
- **Authors**: Zhuotun Zhu, Yingda Xia, Wei Shen, Elliot K. Fishman, Alan L. Yuille
- **Comment**: 9 pages, 4 figures, Accepted to 3DV
- **Journal**: None
- **Summary**: In this paper, we adopt 3D Convolutional Neural Networks to segment volumetric medical images. Although deep neural networks have been proven to be very effective on many 2D vision tasks, it is still challenging to apply them to 3D tasks due to the limited amount of annotated 3D data and limited computational resources. We propose a novel 3D-based coarse-to-fine framework to effectively and efficiently tackle these challenges. The proposed 3D-based framework outperforms the 2D counterpart to a large margin since it can leverage the rich spatial infor- mation along all three axes. We conduct experiments on two datasets which include healthy and pathological pancreases respectively, and achieve the current state-of-the-art in terms of Dice-S{\o}rensen Coefficient (DSC). On the NIH pancreas segmentation dataset, we outperform the previous best by an average of over 2%, and the worst case is improved by 7% to reach almost 70%, which indicates the reliability of our framework in clinical applications.



### InverseNet: Solving Inverse Problems with Splitting Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.00202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00202v1)
- **Published**: 2017-12-01 06:04:05+00:00
- **Updated**: 2017-12-01 06:04:05+00:00
- **Authors**: Kai Fan, Qi Wei, Wenlin Wang, Amit Chakraborty, Katherine Heller
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new method that uses deep learning techniques to solve the inverse problems. The inverse problem is cast in the form of learning an end-to-end mapping from observed data to the ground-truth. Inspired by the splitting strategy widely used in regularized iterative algorithm to tackle inverse problems, the mapping is decomposed into two networks, with one handling the inversion of the physical forward model associated with the data term and one handling the denoising of the output from the former network, i.e., the inverted version, associated with the prior/regularization term. The two networks are trained jointly to learn the end-to-end mapping, getting rid of a two-step training. The training is annealing as the intermediate variable between these two networks bridges the gap between the input (the degraded version of output) and output and progressively approaches to the ground-truth. The proposed network, referred to as InverseNet, is flexible in the sense that most of the existing end-to-end network structure can be leveraged in the first network and most of the existing denoising network structure can be used in the second one. Extensive experiments on both synthetic data and real datasets on the tasks, motion deblurring, super-resolution, and colorization, demonstrate the efficiency and accuracy of the proposed method compared with other image processing algorithms.



### Real-time Semantic Image Segmentation via Spatial Sparsity
- **Arxiv ID**: http://arxiv.org/abs/1712.00213v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00213v1)
- **Published**: 2017-12-01 07:15:28+00:00
- **Updated**: 2017-12-01 07:15:28+00:00
- **Authors**: Zifeng Wu, Chunhua Shen, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an approach to semantic (image) segmentation that reduces the computational costs by a factor of 25 with limited impact on the quality of results. Semantic segmentation has a number of practical applications, and for most such applications the computational costs are critical. The method follows a typical two-column network structure, where one column accepts an input image, while the other accepts a half-resolution version of that image. By identifying specific regions in the full-resolution image that can be safely ignored, as well as carefully tailoring the network structure, we can process approximately 15 highresolution Cityscapes images (1024x2048) per second using a single GTX 980 video card, while achieving a mean intersection-over-union score of 72.9% on the Cityscapes test set.



### Shape Complementarity Analysis for Objects of Arbitrary Shape
- **Arxiv ID**: http://arxiv.org/abs/1712.00238v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1712.00238v1)
- **Published**: 2017-12-01 09:07:14+00:00
- **Updated**: 2017-12-01 09:07:14+00:00
- **Authors**: Morad Behandish, Horea T. Ilies
- **Comment**: Technical Report, University of Connecticut, 2014
- **Journal**: None
- **Summary**: The basic problem of shape complementarity analysis appears fundamental to applications as diverse as mechanical design, assembly automation, robot motion planning, micro- and nano-fabrication, protein-ligand binding, and rational drug design. However, the current challenge lies in the lack of a general mathematical formulation that applies to objects of arbitrary shape. We propose that a measure of shape complementarity can be obtained from the extent of approximate overlap between shape skeletons. A space-continuous implicit generalization of the skeleton, called the skeletal density function (SDF) is defined over the Euclidean space that contains the individual assembly partners. The SDF shape descriptors capture the essential features that are relevant to proper contact alignment, and are considerably more robust than the conventional explicit skeletal representations. We express the shape complementarity score as a convolution of the individual SDFs. The problem then breaks down to a global optimization of the score over the configuration space of spatial relations, which can be efficiently implemented using fast Fourier transforms (FFTs) on nonequispaced samples. We demonstrate the effectiveness of the scoring approach for several examples from 2D peg-in-hole alignment to more complex 3D examples in mechanical assembly and protein docking. We show that the proposed method is reliable, inherently robust against small perturbations, and effective in steering gradient-based optimization.



### Deep Learning for Metagenomic Data: using 2D Embeddings and Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.00244v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.00244v1)
- **Published**: 2017-12-01 09:18:04+00:00
- **Updated**: 2017-12-01 09:18:04+00:00
- **Authors**: Thanh Hai Nguyen, Yann Chevaleyre, Edi Prifti, Nataliya Sokolovska, Jean-Daniel Zucker
- **Comment**: Accepted at NIPS 2017 Workshop on Machine Learning for Health
  (https://ml4health.github.io/2017/); In Proceedings of the NIPS ML4H 2017
  Workshop in Long Beach, CA, USA;
- **Journal**: None
- **Summary**: Deep learning (DL) techniques have had unprecedented success when applied to images, waveforms, and texts to cite a few. In general, when the sample size (N) is much greater than the number of features (d), DL outperforms previous machine learning (ML) techniques, often through the use of convolution neural networks (CNNs). However, in many bioinformatics ML tasks, we encounter the opposite situation where d is greater than N. In these situations, applying DL techniques (such as feed-forward networks) would lead to severe overfitting. Thus, sparse ML techniques (such as LASSO e.g.) usually yield the best results on these tasks. In this paper, we show how to apply CNNs on data which do not have originally an image structure (in particular on metagenomic data). Our first contribution is to show how to map metagenomic data in a meaningful way to 1D or 2D images. Based on this representation, we then apply a CNN, with the aim of predicting various diseases. The proposed approach is applied on six different datasets including in total over 1000 samples from various diseases. This approach could be a promising one for prediction tasks in the bioinformatics field.



### Learning Deep Representations for Word Spotting Under Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/1712.00250v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00250v3)
- **Published**: 2017-12-01 09:41:13+00:00
- **Updated**: 2018-01-26 10:42:16+00:00
- **Authors**: Neha Gurjar, Sebastian Sudholt, Gernot A. Fink
- **Comment**: submitted to DAS 2018
- **Journal**: None
- **Summary**: Convolutional Neural Networks have made their mark in various fields of computer vision in recent years. They have achieved state-of-the-art performance in the field of document analysis as well. However, CNNs require a large amount of annotated training data and, hence, great manual effort. In our approach, we introduce a method to drastically reduce the manual annotation effort while retaining the high performance of a CNN for word spotting in handwritten documents. The model is learned with weak supervision using a combination of synthetically generated training data and a small subset of the training partition of the handwritten data set. We show that the network achieves results highly competitive to the state-of-the-art in word spotting with shorter training times and a fraction of the annotation effort.



### Deformable Shape Completion with Graph Convolutional Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1712.00268v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00268v4)
- **Published**: 2017-12-01 10:34:19+00:00
- **Updated**: 2018-04-03 18:14:47+00:00
- **Authors**: Or Litany, Alex Bronstein, Michael Bronstein, Ameesh Makadia
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: The availability of affordable and portable depth sensors has made scanning objects and people simpler than ever. However, dealing with occlusions and missing parts is still a significant challenge. The problem of reconstructing a (possibly non-rigidly moving) 3D object from a single or multiple partial scans has received increasing attention in recent years. In this work, we propose a novel learning-based method for the completion of partial shapes. Unlike the majority of existing approaches, our method focuses on objects that can undergo non-rigid deformations. The core of our method is a variational autoencoder with graph convolutional operations that learns a latent space for complete realistic shapes. At inference, we optimize to find the representation in this latent space that best fits the generated shape to the known partial input. The completed shape exhibits a realistic appearance on the unknown part. We show promising results towards the completion of synthetic and real scans of human body and face meshes exhibiting different styles of articulation and partiality.



### GANosaic: Mosaic Creation with Generative Texture Manifolds
- **Arxiv ID**: http://arxiv.org/abs/1712.00269v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.00269v1)
- **Published**: 2017-12-01 10:35:13+00:00
- **Updated**: 2017-12-01 10:35:13+00:00
- **Authors**: Nikolay Jetchev, Urs Bergmann, Calvin Seward
- **Comment**: 31st Conference on Neural Information Processing Systems (NIPS 2017),
  Long Beach, CA, USA. Workshop on Machine Learning for Creativity and Design
- **Journal**: None
- **Summary**: This paper presents a novel framework for generating texture mosaics with convolutional neural networks. Our method is called GANosaic and performs optimization in the latent noise space of a generative texture model, which allows the transformation of a content image into a mosaic exhibiting the visual properties of the underlying texture manifold. To represent that manifold, we use a state-of-the-art generative adversarial method for texture synthesis, which can learn expressive texture representations from data and produce mosaic images with very high resolution. This fully convolutional model generates smooth (without any visible borders) mosaic images which morph and blend different textures locally. In addition, we develop a new type of differentiable statistical regularization appropriate for optimization over the prior noise space of the PSGAN model.



### A Novel Brain Decoding Method: a Correlation Network Framework for Revealing Brain Connections
- **Arxiv ID**: http://arxiv.org/abs/1712.01668v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T99
- **Links**: [PDF](http://arxiv.org/pdf/1712.01668v1)
- **Published**: 2017-12-01 11:24:54+00:00
- **Updated**: 2017-12-01 11:24:54+00:00
- **Authors**: Siyu Yu, Nanning Zheng, Yongqiang Ma, Hao Wu, Badong Chen
- **Comment**: 10 pages, 8 figures, IEEE Transactions on Cognitive and Developmental
  Systems(TCDS)
- **Journal**: None
- **Summary**: Brain decoding is a hot spot in cognitive science, which focuses on reconstructing perceptual images from brain activities. Analyzing the correlations of collected data from human brain activities and representing activity patterns are two problems in brain decoding based on functional magnetic resonance imaging (fMRI) signals. However, existing correlation analysis methods mainly focus on the strength information of voxel, which reveals functional connectivity in the cerebral cortex. They tend to neglect the structural information that implies the intracortical or intrinsic connections; that is, structural connectivity. Hence, the effective connectivity inferred by these methods is relatively unilateral. Therefore, we proposed a correlation network (CorrNet) framework that could be flexibly combined with diverse pattern representation models. In the CorrNet framework, the topological correlation was introduced to reveal structural information. Rich correlations were obtained, which contributed to specifying the underlying effective connectivity. We also combined the CorrNet framework with a linear support vector machine (SVM) and a dynamic evolving spike neuron network (SNN) for pattern representation separately, thus providing a novel method for decoding cognitive activity patterns. Experimental results verified the reliability and robustness of our CorrNet framework and demonstrated that the new method achieved significant improvement in brain decoding over comparable methods.



### Neural Signatures for Licence Plate Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1712.00282v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00282v1)
- **Published**: 2017-12-01 11:36:15+00:00
- **Updated**: 2017-12-01 11:36:15+00:00
- **Authors**: Abhinav Kumar, Shantanu Gupta, Vladimir Kozitsky, Sriganesh Madhvanath
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of vehicle licence plate re-identification is generally considered as a one-shot image retrieval problem. The objective of this task is to learn a feature representation (called a "signature") for licence plates. Incoming licence plate images are converted to signatures and matched to a previously collected template database through a distance measure. Then, the input image is recognized as the template whose signature is "nearest" to the input signature. The template database is restricted to contain only a single signature per unique licence plate for our problem.   We measure the performance of deep convolutional net-based features adapted from face recognition on this task. In addition, we also test a hybrid approach combining the Fisher vector with a neural network-based embedding called "f2nn" trained with the Triplet loss function. We find that the hybrid approach performs comparably while providing computational benefits. The signature generated by the hybrid approach also shows higher generalizability to datasets more dissimilar to the training corpus.



### Folded Recurrent Neural Networks for Future Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1712.00311v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.00311v2)
- **Published**: 2017-12-01 13:31:56+00:00
- **Updated**: 2018-03-16 15:15:24+00:00
- **Authors**: Marc Oliu, Javier Selva, Sergio Escalera
- **Comment**: Submitted to European Conference on Computer Vision
- **Journal**: None
- **Summary**: Future video prediction is an ill-posed Computer Vision problem that recently received much attention. Its main challenges are the high variability in video content, the propagation of errors through time, and the non-specificity of the future frames: given a sequence of past frames there is a continuous distribution of possible futures. This work introduces bijective Gated Recurrent Units, a double mapping between the input and output of a GRU layer. This allows for recurrent auto-encoders with state sharing between encoder and decoder, stratifying the sequence representation and helping to prevent capacity problems. We show how with this topology only the encoder or decoder needs to be applied for input encoding and prediction, respectively. This reduces the computational cost and avoids re-encoding the predictions when generating a sequence of frames, mitigating the propagation of errors. Furthermore, it is possible to remove layers from an already trained model, giving an insight to the role performed by each layer and making the model more explainable. We evaluate our approach on three video datasets, outperforming state of the art prediction results on MMNIST and UCF101, and obtaining competitive results on KTH with 2 and 3 times less memory usage and computational cost than the best scored approach.



### Semi-Adversarial Networks: Convolutional Autoencoders for Imparting Privacy to Face Images
- **Arxiv ID**: http://arxiv.org/abs/1712.00321v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.00321v3)
- **Published**: 2017-12-01 14:05:50+00:00
- **Updated**: 2018-05-03 03:09:02+00:00
- **Authors**: Vahid Mirjalili, Sebastian Raschka, Anoop Namboodiri, Arun Ross
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we design and evaluate a convolutional autoencoder that perturbs an input face image to impart privacy to a subject. Specifically, the proposed autoencoder transforms an input face image such that the transformed image can be successfully used for face recognition but not for gender classification. In order to train this autoencoder, we propose a novel training scheme, referred to as semi-adversarial training in this work. The training is facilitated by attaching a semi-adversarial module consisting of a pseudo gender classifier and a pseudo face matcher to the autoencoder. The objective function utilized for training this network has three terms: one to ensure that the perturbed image is a realistic face image; another to ensure that the gender attributes of the face are confounded; and a third to ensure that biometric recognition performance due to the perturbed image is not impacted. Extensive experiments confirm the efficacy of the proposed architecture in extending gender privacy to face images.



### Unsupervised Generative Adversarial Cross-modal Hashing
- **Arxiv ID**: http://arxiv.org/abs/1712.00358v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00358v1)
- **Published**: 2017-12-01 15:20:51+00:00
- **Updated**: 2017-12-01 15:20:51+00:00
- **Authors**: Jian Zhang, Yuxin Peng, Mingkuan Yuan
- **Comment**: 8 pages, accepted by 32th AAAI Conference on Artificial Intelligence
  (AAAI), 2018
- **Journal**: None
- **Summary**: Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Unsupervised cross-modal hashing is more flexible and applicable than supervised methods, since no intensive labeling work is involved. However, existing unsupervised methods learn hashing functions by preserving inter and intra correlations, while ignoring the underlying manifold structure across different modalities, which is extremely helpful to capture meaningful nearest neighbors of different modalities for cross-modal retrieval. To address the above problem, in this paper we propose an Unsupervised Generative Adversarial Cross-modal Hashing approach (UGACH), which makes full use of GAN's ability for unsupervised representation learning to exploit the underlying manifold structure of cross-modal data. The main contributions can be summarized as follows: (1) We propose a generative adversarial network to model cross-modal hashing in an unsupervised fashion. In the proposed UGACH, given a data of one modality, the generative model tries to fit the distribution over the manifold structure, and select informative data of another modality to challenge the discriminative model. The discriminative model learns to distinguish the generated data and the true positive data sampled from correlation graph to achieve better retrieval accuracy. These two models are trained in an adversarial way to improve each other and promote hashing function learning. (2) We propose a correlation graph based approach to capture the underlying manifold structure across different modalities, so that data of different modalities but within the same manifold can have smaller Hamming distance and promote retrieval accuracy. Extensive experiments compared with 6 state-of-the-art methods verify the effectiveness of our proposed approach.



### Hierarchical Bayesian image analysis: from low-level modeling to robust supervised learning
- **Arxiv ID**: http://arxiv.org/abs/1712.00368v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.00368v1)
- **Published**: 2017-12-01 15:32:58+00:00
- **Updated**: 2017-12-01 15:32:58+00:00
- **Authors**: Adrien Lagrange, Mathieu Fauvel, Stéphane May, Nicolas Dobigeon
- **Comment**: None
- **Journal**: None
- **Summary**: Within a supervised classification framework, labeled data are used to learn classifier parameters. Prior to that, it is generally required to perform dimensionality reduction via feature extraction. These preprocessing steps have motivated numerous research works aiming at recovering latent variables in an unsupervised context. This paper proposes a unified framework to perform classification and low-level modeling jointly. The main objective is to use the estimated latent variables as features for classification and to incorporate simultaneously supervised information to help latent variable extraction. The proposed hierarchical Bayesian model is divided into three stages: a first low-level modeling stage to estimate latent variables, a second stage clustering these features into statistically homogeneous groups and a last classification stage exploiting the (possibly badly) labeled data. Performance of the model is assessed in the specific context of hyperspectral image interpretation, unifying two standard analysis techniques, namely unmixing and classification.



### Precision Learning: Towards Use of Known Operators in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.00374v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00374v4)
- **Published**: 2017-12-01 15:44:15+00:00
- **Updated**: 2018-10-12 08:09:28+00:00
- **Authors**: Andreas Maier, Frank Schebesch, Christopher Syben, Tobias Würfl, Stefan Steidl, Jang-Hwan Choi, Rebecca Fahrig
- **Comment**: accepted on ICPR 2018
- **Journal**: A. Maier, F. Schebesch, C. Syben, T. W\"urfl, S. Steidl, J.-H.
  Choi, R. Fahrig, Precision Learning: Towards Use of Known Operators in Neural
  Networks, in: 24rd International Conference on Pattern Recognition (ICPR),
  2018, pp. 183-188
- **Summary**: In this paper, we consider the use of prior knowledge within neural networks. In particular, we investigate the effect of a known transform within the mapping from input data space to the output domain. We demonstrate that use of known transforms is able to change maximal error bounds.   In order to explore the effect further, we consider the problem of X-ray material decomposition as an example to incorporate additional prior knowledge. We demonstrate that inclusion of a non-linear function known from the physical properties of the system is able to reduce prediction errors therewith improving prediction quality from SSIM values of 0.54 to 0.88.   This approach is applicable to a wide set of applications in physics and signal processing that provide prior knowledge on such transforms. Also maximal error estimation and network understanding could be facilitated within the context of precision learning.



### Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1712.00377v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.00377v2)
- **Published**: 2017-12-01 15:48:50+00:00
- **Updated**: 2018-06-03 15:32:06+00:00
- **Authors**: Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Aniruddha Kembhavi
- **Comment**: 15 pages, 10 figures. To appear in IEEE Conference on Computer Vision
  and Pattern Recognition (CVPR), 2018
- **Journal**: None
- **Summary**: A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model -- Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.



### Probabilistic Adaptive Computation Time
- **Arxiv ID**: http://arxiv.org/abs/1712.00386v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.00386v1)
- **Published**: 2017-12-01 16:09:26+00:00
- **Updated**: 2017-12-01 16:09:26+00:00
- **Authors**: Michael Figurnov, Artem Sobolev, Dmitry Vetrov
- **Comment**: None
- **Journal**: None
- **Summary**: We present a probabilistic model with discrete latent variables that control the computation time in deep learning models such as ResNets and LSTMs. A prior on the latent variables expresses the preference for faster computation. The amount of computation for an input is determined via amortized maximum a posteriori (MAP) inference. MAP inference is performed using a novel stochastic variational optimization method. The recently proposed Adaptive Computation Time mechanism can be seen as an ad-hoc relaxation of this model. We demonstrate training using the general-purpose Concrete relaxation of discrete variables. Evaluation on ResNet shows that our method matches the speed-accuracy trade-off of Adaptive Computation Time, while allowing for evaluation with a simple deterministic procedure that has a lower memory footprint.



### Learning to Segment Moving Objects
- **Arxiv ID**: http://arxiv.org/abs/1712.01127v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01127v1)
- **Published**: 2017-12-01 16:50:26+00:00
- **Updated**: 2017-12-01 16:50:26+00:00
- **Authors**: Pavel Tokmakov, Cordelia Schmid, Karteek Alahari
- **Comment**: arXiv admin note: text overlap with arXiv:1704.05737,
  arXiv:1612.07217
- **Journal**: None
- **Summary**: We study the problem of segmenting moving objects in unconstrained videos. Given a video, the task is to segment all the objects that exhibit independent motion in at least one frame. We formulate this as a learning problem and design our framework with three cues: (i) independent object motion between a pair of frames, which complements object recognition, (ii) object appearance, which helps to correct errors in motion estimation, and (iii) temporal consistency, which imposes additional constraints on the segmentation. The framework is a two-stream neural network with an explicit memory module. The two streams encode appearance and motion cues in a video sequence respectively, while the memory module captures the evolution of objects over time, exploiting the temporal consistency. The motion stream is a convolutional neural network trained on synthetic videos to segment independently moving objects in the optical flow field. The module to build a 'visual memory' in video, i.e., a joint representation of all the video frames, is realized with a convolutional recurrent unit learned from a small number of training video sequences.   For every pixel in a frame of a test video, our approach assigns an object or background label based on the learned spatio-temporal features as well as the 'visual memory' specific to the video. We evaluate our method extensively on three benchmarks, DAVIS, Freiburg-Berkeley motion segmentation dataset and SegTrack. In addition, we provide an extensive ablation study to investigate both the choice of the training data and the influence of each component in the proposed framework.



### DeepCache: Principled Cache for Mobile Deep Vision
- **Arxiv ID**: http://arxiv.org/abs/1712.01670v5
- **DOI**: 10.1145/3241539.3241563
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01670v5)
- **Published**: 2017-12-01 16:52:04+00:00
- **Updated**: 2020-03-30 04:16:09+00:00
- **Authors**: Mengwei Xu, Mengze Zhu, Yunxin Liu, Felix Xiaozhu Lin, Xuanzhe Liu
- **Comment**: Accepted for publication in the MobiCom 2018, copyright the ACM,
  posted with permission
- **Journal**: None
- **Summary**: We present DeepCache, a principled cache design for deep learning inference in continuous mobile vision. DeepCache benefits model execution efficiency by exploiting temporal locality in input video streams. It addresses a key challenge raised by mobile vision: the cache must operate under video scene variation, while trading off among cacheability, overhead, and loss in model accuracy. At the input of a model, DeepCache discovers video temporal locality by exploiting the video's internal structure, for which it borrows proven heuristics from video compression; into the model, DeepCache propagates regions of reusable results by exploiting the model's internal structure. Notably, DeepCache eschews applying video heuristics to model internals which are not pixels but high-dimensional, difficult-to-interpret data. Our implementation of DeepCache works with unmodified deep learning models, requires zero developer's manual effort, and is therefore immediately deployable on off-the-shelf mobile devices. Our experiments show that DeepCache saves inference execution time by 18% on average and up to 47%. DeepCache reduces system energy consumption by 20% on average.



### Unsupervised Classification of PolSAR Data Using a Scattering Similarity Measure Derived from a Geodesic Distance
- **Arxiv ID**: http://arxiv.org/abs/1712.00427v1
- **DOI**: 10.1109/LGRS.2017.2778749
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00427v1)
- **Published**: 2017-12-01 17:58:42+00:00
- **Updated**: 2017-12-01 17:58:42+00:00
- **Authors**: Debanshu Ratha, Avik Bhattacharya, Alejandro C. Frery
- **Comment**: Accepted for publication at IEEE Geoscience and Remote Sensing
  Letters
- **Journal**: None
- **Summary**: In this letter, we propose a novel technique for obtaining scattering components from Polarimetric Synthetic Aperture Radar (PolSAR) data using the geodesic distance on the unit sphere. This geodesic distance is obtained between an elementary target and the observed Kennaugh matrix, and it is further utilized to compute a similarity measure between scattering mechanisms. The normalized similarity measure for each elementary target is then modulated with the total scattering power (Span). This measure is used to categorize pixels into three categories i.e. odd-bounce, double-bounce and volume, depending on which of the above scattering mechanisms dominate. Then the maximum likelihood classifier of [J.-S. Lee, M. R. Grunes, E. Pottier, and L. Ferro-Famil, Unsupervised terrain classification preserving polarimetric scattering characteristics, IEEE Trans. Geos. Rem. Sens., vol. 42, no. 4, pp. 722731, April 2004.] based on the complex Wishart distribution is iteratively used for each category. Dominant scattering mechanisms are thus preserved in this classification scheme. We show results for L-band AIRSAR and ALOS-2 datasets acquired over San Francisco and Mumbai, respectively. The scattering mechanisms are better preserved using the proposed methodology than the unsupervised classification results using the Freeman-Durden scattering powers on an orientation angle (OA) corrected PolSAR image. Furthermore, (1) the scattering similarity is a completely non-negative quantity unlike the negative powers that might occur in double- bounce and odd-bounce scattering component under Freeman Durden decomposition (FDD), and (2) the methodology can be extended to more canonical targets as well as for bistatic scattering.



### Single-Shot Object Detection with Enriched Semantics
- **Arxiv ID**: http://arxiv.org/abs/1712.00433v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00433v2)
- **Published**: 2017-12-01 18:18:42+00:00
- **Updated**: 2018-04-08 01:01:25+00:00
- **Authors**: Zhishuai Zhang, Siyuan Qiao, Cihang Xie, Wei Shen, Bo Wang, Alan L. Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel single shot object detection network named Detection with Enriched Semantics (DES). Our motivation is to enrich the semantics of object detection features within a typical deep detector, by a semantic segmentation branch and a global activation module. The segmentation branch is supervised by weak segmentation ground-truth, i.e., no extra annotation is required. In conjunction with that, we employ a global activation module which learns relationship between channels and object classes in a self-supervised manner. Comprehensive experimental results on both PASCAL VOC and MS COCO detection datasets demonstrate the effectiveness of the proposed method. In particular, with a VGG16 based DES, we achieve an mAP of 81.7 on VOC2007 test and an mAP of 32.8 on COCO test-dev with an inference speed of 31.5 milliseconds per image on a Titan Xp GPU. With a lower resolution version, we achieve an mAP of 79.7 on VOC2007 with an inference speed of 13.0 milliseconds per image.



### Unsupervised Learning for Color Constancy
- **Arxiv ID**: http://arxiv.org/abs/1712.00436v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00436v4)
- **Published**: 2017-12-01 18:38:36+00:00
- **Updated**: 2019-03-19 09:58:28+00:00
- **Authors**: Nikola Banić, Karlo Koščević, Sven Lončarić
- **Comment**: 15 pages, 16 figures
- **Journal**: None
- **Summary**: Most digital camera pipelines use color constancy methods to reduce the influence of illumination and camera sensor on the colors of scene objects. The highest accuracy of color correction is obtained with learning-based color constancy methods, but they require a significant amount of calibrated training images with known ground-truth illumination. Such calibration is time consuming, preferably done for each sensor individually, and therefore a major bottleneck in acquiring high color constancy accuracy. Statistics-based methods do not require calibrated training images, but they are less accurate. In this paper an unsupervised learning-based method is proposed that learns its parameter values after approximating the unknown ground-truth illumination of the training images, thus avoiding calibration. In terms of accuracy the proposed method outperforms all statistics-based and many learning-based methods. An extension of the method is also proposed, which learns the needed parameters from non-calibrated images taken with one sensor and which can then be successfully applied to images taken with another sensor. This effectively enables inter-camera unsupervised learning for color constancy. Additionally, a new high quality color constancy benchmark dataset with 1707 calibrated images is created, used for testing, and made publicly available. The results are presented and discussed. The source code and the dataset are available at http://www.fer.unizg.hr/ipg/resources/color_constancy/.



### Image to Image Translation for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1712.00479v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00479v1)
- **Published**: 2017-12-01 20:15:20+00:00
- **Updated**: 2017-12-01 20:15:20+00:00
- **Authors**: Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ramamoorthi, Kyungnam Kim
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a general framework for unsupervised domain adaptation, which allows deep neural networks trained on a source domain to be tested on a different target domain without requiring any training annotations in the target domain. This is achieved by adding extra networks and losses that help regularize the features extracted by the backbone encoder network. To this end we propose the novel use of the recently proposed unpaired image-toimage translation framework to constrain the features extracted by the encoder network. Specifically, we require that the features extracted are able to reconstruct the images in both domains. In addition we require that the distribution of features extracted from images in the two domains are indistinguishable. Many recent works can be seen as specific cases of our general framework. We apply our method for domain adaptation between MNIST, USPS, and SVHN datasets, and Amazon, Webcam and DSLR Office datasets in classification tasks, and also between GTA5 and Cityscapes datasets for a segmentation task. We demonstrate state of the art performance on each of these datasets.



### Visual Features for Context-Aware Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.00489v1
- **DOI**: 10.1109/ICASSP.2017.7953112
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1712.00489v1)
- **Published**: 2017-12-01 20:56:31+00:00
- **Updated**: 2017-12-01 20:56:31+00:00
- **Authors**: Abhinav Gupta, Yajie Miao, Leonardo Neves, Florian Metze
- **Comment**: 5 pages and 3 figures
- **Journal**: IEEE Xplore (ICASSP) (2017) 5020-5024
- **Summary**: Automatic transcriptions of consumer-generated multi-media content such as "Youtube" videos still exhibit high word error rates. Such data typically occupies a very broad domain, has been recorded in challenging conditions, with cheap hardware and a focus on the visual modality, and may have been post-processed or edited. In this paper, we extend our earlier work on adapting the acoustic model of a DNN-based speech recognition system to an RNN language model and show how both can be adapted to the objects and scenes that can be automatically detected in the video. We are working on a corpus of "how-to" videos from the web, and the idea is that an object that can be seen ("car"), or a scene that is being detected ("kitchen") can be used to condition both models on the "context" of the recording, thereby reducing perplexity and improving transcription. We achieve good improvements in both cases and compare and analyze the respective reductions in word error rate. We expect that our results can be used for any type of speech processing in which "context" information is available, for example in robotics, man-machine interaction, or when indexing large audio-visual archives, and should ultimately help to bring together the "video-to-text" and "speech-to-text" communities.



### Propagating Uncertainty in Multi-Stage Bayesian Convolutional Neural Networks with Application to Pulmonary Nodule Detection
- **Arxiv ID**: http://arxiv.org/abs/1712.00497v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.00497v1)
- **Published**: 2017-12-01 21:21:35+00:00
- **Updated**: 2017-12-01 21:21:35+00:00
- **Authors**: Onur Ozdemir, Benjamin Woodward, Andrew A. Berlin
- **Comment**: NIPS Workshop on Bayesian Deep Learning, 2017
- **Journal**: None
- **Summary**: Motivated by the problem of computer-aided detection (CAD) of pulmonary nodules, we introduce methods to propagate and fuse uncertainty information in a multi-stage Bayesian convolutional neural network (CNN) architecture. The question we seek to answer is "can we take advantage of the model uncertainty provided by one deep learning model to improve the performance of the subsequent deep learning models and ultimately of the overall performance in a multi-stage Bayesian deep learning architecture?". Our experiments show that propagating uncertainty through the pipeline enables us to improve the overall performance in terms of both final prediction accuracy and model confidence.



### Learning Neural Markers of Schizophrenia Disorder Using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.00512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00512v1)
- **Published**: 2017-12-01 22:57:15+00:00
- **Updated**: 2017-12-01 22:57:15+00:00
- **Authors**: Jumana Dakka, Pouya Bashivan, Mina Gheiratmand, Irina Rish, Shantenu Jha, Russell Greiner
- **Comment**: To be published as a workshop paper at NIPS 2017 Machine Learning for
  Health (ML4H)
- **Journal**: None
- **Summary**: Smart systems that can accurately diagnose patients with mental disorders and identify effective treatments based on brain functional imaging data are of great applicability and are gaining much attention. Most previous machine learning studies use hand-designed features, such as functional connectivity, which does not maintain the potential useful information in the spatial relationship between brain regions and the temporal profile of the signal in each region. Here we propose a new method based on recurrent-convolutional neural networks to automatically learn useful representations from segments of 4-D fMRI recordings. Our goal is to exploit both spatial and temporal information in the functional MRI movie (at the whole-brain voxel level) for identifying patients with schizophrenia.



### Multi-Content GAN for Few-Shot Font Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1712.00516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.00516v1)
- **Published**: 2017-12-01 23:12:58+00:00
- **Updated**: 2017-12-01 23:12:58+00:00
- **Authors**: Samaneh Azadi, Matthew Fisher, Vladimir Kim, Zhaowen Wang, Eli Shechtman, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we focus on the challenge of taking partial observations of highly-stylized text and generalizing the observations to generate unobserved glyphs in the ornamented typeface. To generate a set of multi-content images following a consistent style from very few examples, we propose an end-to-end stacked conditional GAN model considering content along channels and style along network layers. Our proposed network transfers the style of given glyphs to the contents of unseen ones, capturing highly stylized fonts found in the real-world such as those on movie posters or infographics. We seek to transfer both the typographic stylization (ex. serifs and ears) as well as the textual stylization (ex. color gradients and effects.) We base our experiments on our collected data set including 10,000 fonts with different styles and demonstrate effective generalization from a very small number of observed glyphs.



