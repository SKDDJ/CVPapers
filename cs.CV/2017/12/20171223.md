# Arxiv Papers in cs.CV on 2017-12-23
### Aerial Spectral Super-Resolution using Conditional Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.08690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08690v1)
- **Published**: 2017-12-23 00:21:20+00:00
- **Updated**: 2017-12-23 00:21:20+00:00
- **Authors**: Aneesh Rangnekar, Nilay Mokashi, Emmett Ientilucci, Christopher Kanan, Matthew Hoffman
- **Comment**: None
- **Journal**: None
- **Summary**: Inferring spectral signatures from ground based natural images has acquired a lot of interest in applied deep learning. In contrast to the spectra of ground based images, aerial spectral images have low spatial resolution and suffer from higher noise interference. In this paper, we train a conditional adversarial network to learn an inverse mapping from a trichromatic space to 31 spectral bands within 400 to 700 nm. The network is trained on AeroCampus, a first of its kind aerial hyperspectral dataset. AeroCampus consists of high spatial resolution color images and low spatial resolution hyperspectral images (HSI). Color images synthesized from 31 spectral bands are used to train our network. With a baseline root mean square error of 2.48 on the synthesized RGB test data, we show that it is possible to generate spectral signatures in aerial imagery.



### Interpretable Counting for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1712.08697v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.08697v2)
- **Published**: 2017-12-23 01:44:45+00:00
- **Updated**: 2018-03-02 03:00:43+00:00
- **Authors**: Alexander Trott, Caiming Xiong, Richard Socher
- **Comment**: ICLR 2018
- **Journal**: None
- **Summary**: Questions that require counting a variety of objects in images remain a major challenge in visual question answering (VQA). The most common approaches to VQA involve either classifying answers based on fixed length representations of both the image and question or summing fractional counts estimated from each section of the image. In contrast, we treat counting as a sequential decision process and force our model to make discrete choices of what to count. Specifically, the model sequentially selects from detected objects and learns interactions between objects that influence subsequent selections. A distinction of our approach is its intuitive and interpretable output, as discrete counts are automatically grounded in the image. Furthermore, our method outperforms the state of the art architecture for VQA on multiple metrics that evaluate counting.



### Towards Structured Analysis of Broadcast Badminton Videos
- **Arxiv ID**: http://arxiv.org/abs/1712.08714v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1712.08714v1)
- **Published**: 2017-12-23 04:51:31+00:00
- **Updated**: 2017-12-23 04:51:31+00:00
- **Authors**: Anurag Ghosh, Suriya Singh, C. V. Jawahar
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Sports video data is recorded for nearly every major tournament but remains archived and inaccessible to large scale data mining and analytics. It can only be viewed sequentially or manually tagged with higher-level labels which is time consuming and prone to errors. In this work, we propose an end-to-end framework for automatic attributes tagging and analysis of sport videos. We use commonly available broadcast videos of matches and, unlike previous approaches, does not rely on special camera setups or additional sensors.   Our focus is on Badminton as the sport of interest. We propose a method to analyze a large corpus of badminton broadcast videos by segmenting the points played, tracking and recognizing the players in each point and annotating their respective badminton strokes. We evaluate the performance on 10 Olympic matches with 20 players and achieved 95.44% point segmentation accuracy, 97.38% player detection score (mAP@0.5), 97.98% player identification accuracy, and stroke segmentation edit scores of 80.48%. We further show that the automatically annotated videos alone could enable the gameplay analysis and inference by computing understandable metrics such as player's reaction time, speed, and footwork around the court, etc.



### Denoising of 3D magnetic resonance images with multi-channel residual learning of convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1712.08726v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08726v2)
- **Published**: 2017-12-23 07:35:51+00:00
- **Updated**: 2018-01-31 14:04:52+00:00
- **Authors**: Dongsheng Jiang, Weiqiang Dou, Luc Vosters, Xiayu Xu, Yue Sun, Tao Tan
- **Comment**: None
- **Journal**: None
- **Summary**: The denoising of magnetic resonance (MR) images is a task of great importance for improving the acquired image quality. Many methods have been proposed in the literature to retrieve noise free images with good performances. Howerever, the state-of-the-art denoising methods, all needs a time-consuming optimization processes and their performance strongly depend on the estimated noise level parameter. Within this manuscript we propose the idea of denoising MRI Rician noise using a convolutional neural network. The advantage of the proposed methodology is that the learning based model can be directly used in the denosing process without optimization and even without the noise level parameter. Specifically, a ten convolutional layers neural network combined with residual learning and multi-channel strategy was proposed. Two training ways: training on a specific noise level and training on a general level were conducted to demonstrate the capability of our methods. Experimental results over synthetic and real 3D MR data demonstrate our proposed network can achieve superior performance compared with other methods in term of both of the peak signal to noise ratio and the global of structure similarity index. Without noise level parameter, our general noise-applicable model is also better than the other compared methods in two datasets. Furthermore, our training model shows good general applicability.



### Combining Weakly and Webly Supervised Learning for Classifying Food Images
- **Arxiv ID**: http://arxiv.org/abs/1712.08730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08730v1)
- **Published**: 2017-12-23 08:00:06+00:00
- **Updated**: 2017-12-23 08:00:06+00:00
- **Authors**: Parneet Kaur, Karan Sikka, Ajay Divakaran
- **Comment**: None
- **Journal**: None
- **Summary**: Food classification from images is a fine-grained classification problem. Manual curation of food images is cost, time and scalability prohibitive. On the other hand, web data is available freely but contains noise. In this paper, we address the problem of classifying food images with minimal data curation. We also tackle a key problems with food images from the web where they often have multiple cooccuring food types but are weakly labeled with a single label. We first demonstrate that by sequentially adding a few manually curated samples to a larger uncurated dataset from two web sources, the top-1 classification accuracy increases from 50.3% to 72.8%. To tackle the issue of weak labels, we augment the deep model with Weakly Supervised learning (WSL) that results in an increase in performance to 76.2%. Finally, we show some qualitative results to provide insights into the performance improvements using the proposed ideas.



### Scene-Specific Pedestrian Detection Based on Parallel Vision
- **Arxiv ID**: http://arxiv.org/abs/1712.08745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08745v1)
- **Published**: 2017-12-23 09:33:29+00:00
- **Updated**: 2017-12-23 09:33:29+00:00
- **Authors**: Wenwen Zhang, Kunfeng Wang, Hua Qu, Jihong Zhao, Fei-Yue Wang
- **Comment**: To be published in IEEE ITSC 2017
- **Journal**: None
- **Summary**: As a special type of object detection, pedestrian detection in generic scenes has made a significant progress trained with large amounts of labeled training data manually. While the models trained with generic dataset work bad when they are directly used in specific scenes. With special viewpoints, flow light and backgrounds, datasets from specific scenes are much different from the datasets from generic scenes. In order to make the generic scene pedestrian detectors work well in specific scenes, the labeled data from specific scenes are needed to adapt the models to the specific scenes. While labeling the data manually spends much time and money, especially for specific scenes, each time with a new specific scene, large amounts of images must be labeled. What's more, the labeling information is not so accurate in the pixels manually and different people make different labeling information. In this paper, we propose an ACP-based method, with augmented reality's help, we build the virtual world of specific scenes, and make people walking in the virtual scenes where it is possible for them to appear to solve this problem of lacking labeled data and the results show that data from virtual world is helpful to adapt generic pedestrian detectors to specific scenes.



### Texture Object Segmentation Based on Affine Invariant Texture Detection
- **Arxiv ID**: http://arxiv.org/abs/1712.08776v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.08776v1)
- **Published**: 2017-12-23 13:56:13+00:00
- **Updated**: 2017-12-23 13:56:13+00:00
- **Authors**: Jianwei Zhang, Xu Chen, Xuezhong Xiao
- **Comment**: 6pages, 15 figures
- **Journal**: None
- **Summary**: To solve the issue of segmenting rich texture images, a novel detection methods based on the affine invariable principle is proposed. Considering the similarity between the texture areas, we first take the affine transform to get numerous shapes, and utilize the KLT algorithm to verify the similarity. The transforms include rotation, proportional transformation and perspective deformation to cope with a variety of situations. Then we propose an improved LBP method combining canny edge detection to handle the boundary in the segmentation process. Moreover, human-computer interaction of this method which helps splitting the matched texture area from the original images is user-friendly.



### Large-Scale Object Discovery and Detector Adaptation from Unlabeled Video
- **Arxiv ID**: http://arxiv.org/abs/1712.08832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08832v1)
- **Published**: 2017-12-23 20:22:11+00:00
- **Updated**: 2017-12-23 20:22:11+00:00
- **Authors**: Aljoša Ošep, Paul Voigtlaender, Jonathon Luiten, Stefan Breuers, Bastian Leibe
- **Comment**: CVPR'18 submission
- **Journal**: None
- **Summary**: We explore object discovery and detector adaptation based on unlabeled video sequences captured from a mobile platform. We propose a fully automatic approach for object mining from video which builds upon a generic object tracking approach. By applying this method to three large video datasets from autonomous driving and mobile robotics scenarios, we demonstrate its robustness and generality. Based on the object mining results, we propose a novel approach for unsupervised object discovery by appearance-based clustering. We show that this approach successfully discovers interesting objects relevant to driving scenarios. In addition, we perform self-supervised detector adaptation in order to improve detection performance on the KITTI dataset for existing categories. Our approach has direct relevance for enabling large-scale object learning for autonomous driving.



### Texture Synthesis with Recurrent Variational Auto-Encoder
- **Arxiv ID**: http://arxiv.org/abs/1712.08838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.08838v1)
- **Published**: 2017-12-23 20:38:57+00:00
- **Updated**: 2017-12-23 20:38:57+00:00
- **Authors**: Rohan Chandra, Sachin Grover, Kyungjun Lee, Moustafa Meshry, Ahmed Taha
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a recurrent variational auto-encoder for texture synthesis. A novel loss function, FLTBNK, is used for training the texture synthesizer. It is rotational and partially color invariant loss function. Unlike L2 loss, FLTBNK explicitly models the correlation of color intensity between pixels. Our texture synthesizer generates neighboring tiles to expand a sample texture and is evaluated using various texture patterns from Describable Textures Dataset (DTD). We perform both quantitative and qualitative experiments with various loss functions to evaluate the performance of our proposed loss function (FLTBNK) --- a mini-human subject study is used for the qualitative evaluation.



