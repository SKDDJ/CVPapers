# Arxiv Papers in cs.CV on 2017-12-26
### Deep Meta Learning for Real-Time Target-Aware Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1712.09153v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09153v3)
- **Published**: 2017-12-26 01:14:16+00:00
- **Updated**: 2019-08-16 06:49:29+00:00
- **Authors**: Janghoon Choi, Junseok Kwon, Kyoung Mu Lee
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: In this paper, we propose a novel on-line visual tracking framework based on the Siamese matching network and meta-learner network, which run at real-time speeds. Conventional deep convolutional feature-based discriminative visual tracking algorithms require continuous re-training of classifiers or correlation filters, which involve solving complex optimization tasks to adapt to the new appearance of a target object. To alleviate this complex process, our proposed algorithm incorporates and utilizes a meta-learner network to provide the matching network with new appearance information of the target objects by adding target-aware feature space. The parameters for the target-specific feature space are provided instantly from a single forward-pass of the meta-learner network. By eliminating the necessity of continuously solving complex optimization tasks in the course of tracking, experimental results demonstrate that our algorithm performs at a real-time speed while maintaining competitive performance among other state-of-the-art tracking algorithms.



### Segmenting Sky Pixels in Images
- **Arxiv ID**: http://arxiv.org/abs/1712.09161v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09161v2)
- **Published**: 2017-12-26 02:07:46+00:00
- **Updated**: 2018-01-08 19:19:55+00:00
- **Authors**: Cecilia La Place, Aisha Urooj Khan, Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: Outdoor scene parsing models are often trained on ideal datasets and produce quality results. However, this leads to a discrepancy when applied to the real world. The quality of scene parsing, particularly sky classification, decreases in night time images, images involving varying weather conditions, and scene changes due to seasonal weather. This project focuses on approaching these challenges by using a state-of-the-art model in conjunction with a non-ideal dataset: SkyFinder and a subset from SUN database with Sky object. We focus specifically on sky segmentation, the task of determining sky and not-sky pixels, and improving upon an existing state-of-the-art model: RefineNet. As a result of our efforts, we have seen an improvement of 10-15% in the average MCR compared to the prior methods on SkyFinder dataset. We have also improved from an off-the shelf-model in terms of average mIOU by nearly 35%. Further, we analyze our trained models on images w.r.t two aspects: times of day and weather, and find that, in spite of facing same challenges as prior methods, our trained models significantly outperform them.



### Detect-and-Track: Efficient Pose Estimation in Videos
- **Arxiv ID**: http://arxiv.org/abs/1712.09184v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09184v2)
- **Published**: 2017-12-26 05:56:39+00:00
- **Updated**: 2018-05-02 18:49:57+00:00
- **Authors**: Rohit Girdhar, Georgia Gkioxari, Lorenzo Torresani, Manohar Paluri, Du Tran
- **Comment**: In CVPR 2018. Ranked first in ICCV 2017 PoseTrack challenge (keypoint
  tracking in videos). Code: https://github.com/facebookresearch/DetectAndTrack
  and webpage: https://rohitgirdhar.github.io/DetectAndTrack/
- **Journal**: None
- **Summary**: This paper addresses the problem of estimating and tracking human body keypoints in complex, multi-person video. We propose an extremely lightweight yet highly effective approach that builds upon the latest advancements in human detection and video understanding. Our method operates in two-stages: keypoint estimation in frames or short clips, followed by lightweight tracking to generate keypoint predictions linked over the entire video. For frame-level pose estimation we experiment with Mask R-CNN, as well as our own proposed 3D extension of this model, which leverages temporal information over small clips to generate more robust frame predictions. We conduct extensive ablative experiments on the newly released multi-person video pose estimation benchmark, PoseTrack, to validate various design choices of our model. Our approach achieves an accuracy of 55.2% on the validation and 51.8% on the test set using the Multi-Object Tracking Accuracy (MOTA) metric, and achieves state of the art performance on the ICCV 2017 PoseTrack keypoint tracking challenge.



### The Robust Manifold Defense: Adversarial Training using Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1712.09196v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.09196v5)
- **Published**: 2017-12-26 07:28:14+00:00
- **Updated**: 2019-07-10 03:51:45+00:00
- **Authors**: Ajil Jalal, Andrew Ilyas, Constantinos Daskalakis, Alexandros G. Dimakis
- **Comment**: Added pseudo code for defense-gan break
- **Journal**: None
- **Summary**: We propose a new type of attack for finding adversarial examples for image classifiers. Our method exploits spanners, i.e. deep neural networks whose input space is low-dimensional and whose output range approximates the set of images of interest. Spanners may be generators of GANs or decoders of VAEs. The key idea in our attack is to search over latent code pairs to find ones that generate nearby images with different classifier outputs. We argue that our attack is stronger than searching over perturbations of real images. Moreover, we show that our stronger attack can be used to reduce the accuracy of Defense-GAN to 3\%, resolving an open problem from the well-known paper by Athalye et al. We combine our attack with normal adversarial training to obtain the most robust known MNIST classifier, significantly improving the state of the art against PGD attacks. Our formulation involves solving a min-max problem, where the min player sets the parameters of the classifier and the max player is running our attack, and is thus searching for adversarial examples in the {\em low-dimensional} input space of the spanner.   All code and models are available at \url{https://github.com/ajiljalal/manifold-defense.git}



### Aircraft Fuselage Defect Detection using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.09213v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09213v2)
- **Published**: 2017-12-26 09:07:34+00:00
- **Updated**: 2020-09-26 08:30:30+00:00
- **Authors**: Touba Malekzadeh, Milad Abdollahzadeh, Hossein Nejati, Ngai-Man Cheung
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: To ensure flight safety of aircraft structures, it is necessary to have regular maintenance using visual and nondestructive inspection (NDI) methods. In this paper, we propose an automatic image-based aircraft defect detection using Deep Neural Networks (DNNs). To the best of our knowledge, this is the first work for aircraft defect detection using DNNs. We perform a comprehensive evaluation of state-of-the-art feature descriptors and show that the best performance is achieved by vgg-f DNN as feature extractor with a linear SVM classifier. To reduce the processing time, we propose to apply SURF key point detector to identify defect patch candidates. Our experiment results suggest that we can achieve over 96% accuracy at around 15s processing time for a high-resolution (20-megapixel) image on a laptop.



### Large-Scale 3D Scene Classification With Multi-View Volumetric CNN
- **Arxiv ID**: http://arxiv.org/abs/1712.09216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09216v1)
- **Published**: 2017-12-26 09:13:12+00:00
- **Updated**: 2017-12-26 09:13:12+00:00
- **Authors**: Dror Aiger, Brett Allen, Aleksey Golovinskiy
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a method to classify imagery using a convo- lutional neural network (CNN) on multi-view image pro- jections. The power of our method comes from using pro- jections of multiple images at multiple depth planes near the reconstructed surface. This enables classification of categories whose salient aspect is appearance change un- der different viewpoints, such as water, trees, and other materials with complex reflection/light response proper- ties. Our method does not require boundary labelling in images and works on pixel-level classification with a small (few pixels) context, which simplifies the cre- ation of a training set. We demonstrate this application on large-scale aerial imagery collections, and extend the per-pixel classification to robustly create a consistent 2D classification which can be used to fill the gaps in non- reconstructible water regions. We also apply our method to classify tree regions. In both cases, the training data can quickly be generated using a small number of manually- created polygons on a map. We show that even with a very simple and standard network our CNN outperforms the state-of-the-art image classification, the Inception-V3 model retrained from a large collection of aerial images.



### A model for interpreting social interactions in local image regions
- **Arxiv ID**: http://arxiv.org/abs/1712.09299v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09299v1)
- **Published**: 2017-12-26 16:24:08+00:00
- **Updated**: 2017-12-26 16:24:08+00:00
- **Authors**: Guy Ben-Yosef, Alon Yachin, Shimon Ullman
- **Comment**: In AAAI spring symposium on Science of Intelligence: Computational
  Principles of Natural and Artificial Intelligence, Palo Alto, 2017
- **Journal**: None
- **Summary**: Understanding social interactions (such as 'hug' or 'fight') is a basic and important capacity of the human visual system, but a challenging and still open problem for modeling. In this work we study visual recognition of social interactions, based on small but recognizable local regions. The approach is based on two novel key components: (i) A given social interaction can be recognized reliably from reduced images (called 'minimal images'). (ii) The recognition of a social interaction depends on identifying components and relations within the minimal image (termed 'interpretation'). We show psychophysics data for minimal images and modeling results for their interpretation. We discuss the integration of minimal configurations in recognizing social interactions in a detailed, high-resolution image.



### Zero-Shot Learning via Latent Space Encoding
- **Arxiv ID**: http://arxiv.org/abs/1712.09300v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09300v2)
- **Published**: 2017-12-26 16:26:36+00:00
- **Updated**: 2018-04-20 15:31:25+00:00
- **Authors**: Yunlong Yu, Zhong Ji, Jichang Guo, Zhongfei, Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-Shot Learning (ZSL) is typically achieved by resorting to a class semantic embedding space to transfer the knowledge from the seen classes to unseen ones. Capturing the common semantic characteristics between the visual modality and the class semantic modality (e.g., attributes or word vector) is a key to the success of ZSL. In this paper, we propose a novel encoder-decoder approach, namely Latent Space Encoding (LSE), to connect the semantic relations of different modalities. Instead of requiring a projection function to transfer information across different modalities like most previous work, LSE per- forms the interactions of different modalities via a feature aware latent space, which is learned in an implicit way. Specifically, different modalities are modeled separately but optimized jointly. For each modality, an encoder-decoder framework is performed to learn a feature aware latent space via jointly maximizing the recoverability of the original space from the latent space and the predictability of the latent space from the original space. To relate different modalities together, their features referring to the same concept are enforced to share the same latent codings. In this way, the common semantic characteristics of different modalities are generalized with the latent representations. Another property of the proposed approach is that it is easily extended to more modalities. Extensive experimental results on four benchmark datasets (AwA, CUB, aPY, and ImageNet) clearly demonstrate the superiority of the proposed approach on several ZSL tasks, including traditional ZSL, generalized ZSL, and zero-shot retrieval (ZSR).



### HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization
- **Arxiv ID**: http://arxiv.org/abs/1712.09374v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1712.09374v3)
- **Published**: 2017-12-26 19:09:11+00:00
- **Updated**: 2019-09-04 07:35:48+00:00
- **Authors**: Hang Zhao, Antonio Torralba, Lorenzo Torresani, Zhicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new large-scale dataset for recognition and temporal localization of human actions collected from Web videos. We refer to it as HACS (Human Action Clips and Segments). We leverage both consensus and disagreement among visual classifiers to automatically mine candidate short clips from unlabeled videos, which are subsequently validated by human annotators. The resulting dataset is dubbed HACS Clips. Through a separate process we also collect annotations defining action segment boundaries. This resulting dataset is called HACS Segments. Overall, HACS Clips consists of 1.5M annotated clips sampled from 504K untrimmed videos, and HACS Seg-ments contains 139K action segments densely annotatedin 50K untrimmed videos spanning 200 action categories. HACS Clips contains more labeled examples than any existing video benchmark. This renders our dataset both a large scale action recognition benchmark and an excellent source for spatiotemporal feature learning. In our transferlearning experiments on three target datasets, HACS Clips outperforms Kinetics-600, Moments-In-Time and Sports1Mas a pretraining source. On HACS Segments, we evaluate state-of-the-art methods of action proposal generation and action localization, and highlight the new challenges posed by our dense temporal annotations.



### RaspiReader: Open Source Fingerprint Reader
- **Arxiv ID**: http://arxiv.org/abs/1712.09392v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09392v1)
- **Published**: 2017-12-26 20:21:17+00:00
- **Updated**: 2017-12-26 20:21:17+00:00
- **Authors**: Joshua J. Engelsma, Kai Cao, Anil K. Jain
- **Comment**: substantial text overlap with arXiv:1708.07887
- **Journal**: None
- **Summary**: We open source an easy to assemble, spoof resistant, high resolution, optical fingerprint reader, called RaspiReader, using ubiquitous components. By using our open source STL files and software, RaspiReader can be built in under one hour for only US $175. As such, RaspiReader provides the fingerprint research community a seamless and simple method for quickly prototyping new ideas involving fingerprint reader hardware. In particular, we posit that this open source fingerprint reader will facilitate the exploration of novel fingerprint spoof detection techniques involving both hardware and software. We demonstrate one such spoof detection technique by specially customizing RaspiReader with two cameras for fingerprint image acquisition. One camera provides high contrast, frustrated total internal reflection (FTIR) fingerprint images, and the other outputs direct images of the finger in contact with the platen. Using both of these image streams, we extract complementary information which, when fused together and used for spoof detection, results in marked performance improvement over previous methods relying only on grayscale FTIR images provided by COTS optical readers. Finally, fingerprint matching experiments between images acquired from the FTIR output of RaspiReader and images acquired from a COTS reader verify the interoperability of the RaspiReader with existing COTS optical readers.



### Robust Minutiae Extractor: Integrating Deep Networks and Fingerprint Domain Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1712.09401v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09401v1)
- **Published**: 2017-12-26 20:55:43+00:00
- **Updated**: 2017-12-26 20:55:43+00:00
- **Authors**: Dinh-Luan Nguyen, Kai Cao, Anil K. Jain
- **Comment**: Accepted to International Conference on Biometrics (ICB 2018)
- **Journal**: None
- **Summary**: We propose a fully automatic minutiae extractor, called MinutiaeNet, based on deep neural networks with compact feature representation for fast comparison of minutiae sets. Specifically, first a network, called CoarseNet, estimates the minutiae score map and minutiae orientation based on convolutional neural network and fingerprint domain knowledge (enhanced image, orientation field, and segmentation map). Subsequently, another network, called FineNet, refines the candidate minutiae locations based on score map. We demonstrate the effectiveness of using the fingerprint domain knowledge together with the deep networks. Experimental results on both latent (NIST SD27) and plain (FVC 2004) public domain fingerprint datasets provide comprehensive empirical support for the merits of our method. Further, our method finds minutiae sets that are better in terms of precision and recall in comparison with state-of-the-art on these two datasets. Given the lack of annotated fingerprint datasets with minutiae ground truth, the proposed approach to robust minutiae detection will be useful to train network-based fingerprint matching algorithms as well as for evaluating fingerprint individuality at scale. MinutiaeNet is implemented in Tensorflow: https://github.com/luannd/MinutiaeNet



### Multi-modal Geolocation Estimation Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.09458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.09458v1)
- **Published**: 2017-12-26 23:52:39+00:00
- **Updated**: 2017-12-26 23:52:39+00:00
- **Authors**: Jesse M. Johns, Jeremiah Rounds, Michael J. Henry
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating the location where an image was taken based solely on the contents of the image is a challenging task, even for humans, as properly labeling an image in such a fashion relies heavily on contextual information, and is not as simple as identifying a single object in the image. Thus any methods which attempt to do so must somehow account for these complexities, and no single model to date is completely capable of addressing all challenges. This work contributes to the state of research in image geolocation inferencing by introducing a novel global meshing strategy, outlining a variety of training procedures to overcome the considerable data limitations when training these models, and demonstrating how incorporating additional information can be used to improve the overall performance of a geolocation inference model. In this work, it is shown that Delaunay triangles are an effective type of mesh for geolocation in relatively low volume scenarios when compared to results from state of the art models which use quad trees and an order of magnitude more training data. In addition, the time of posting, learned user albuming, and other meta data are easily incorporated to improve geolocation by up to 11% for country-level (750 km) locality accuracy to 3% for city-level (25 km) localities.



