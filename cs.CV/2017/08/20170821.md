# Arxiv Papers in cs.CV on 2017-08-21
### More cat than cute? Interpretable Prediction of Adjective-Noun Pairs
- **Arxiv ID**: http://arxiv.org/abs/1708.06039v1
- **DOI**: 10.1145/3132515.3132520
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1708.06039v1)
- **Published**: 2017-08-21 00:33:05+00:00
- **Updated**: 2017-08-21 00:33:05+00:00
- **Authors**: Delia Fernandez, Alejandro Woodward, Victor Campos, Xavier Giro-i-Nieto, Brendan Jou, Shih-Fu Chang
- **Comment**: Oral paper at ACM Multimedia 2017 Workshop on Multimodal
  Understanding of Social, Affective and Subjective Attributes (MUSA2)
- **Journal**: None
- **Summary**: The increasing availability of affect-rich multimedia resources has bolstered interest in understanding sentiment and emotions in and from visual content. Adjective-noun pairs (ANP) are a popular mid-level semantic construct for capturing affect via visually detectable concepts such as "cute dog" or "beautiful landscape". Current state-of-the-art methods approach ANP prediction by considering each of these compound concepts as individual tokens, ignoring the underlying relationships in ANPs. This work aims at disentangling the contributions of the `adjectives' and `nouns' in the visual prediction of ANPs. Two specialised classifiers, one trained for detecting adjectives and another for nouns, are fused to predict 553 different ANPs. The resulting ANP prediction model is more interpretable as it allows us to study contributions of the adjective and noun components. Source code and models are available at https://imatge-upc.github.io/affective-2017-musa2/ .



### Distantly Supervised Road Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1708.06118v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06118v1)
- **Published**: 2017-08-21 08:34:17+00:00
- **Updated**: 2017-08-21 08:34:17+00:00
- **Authors**: Satoshi Tsutsui, Tommi Kerola, Shunta Saito
- **Comment**: Accepted for ICCV workshop CVRSUAD2017
- **Journal**: None
- **Summary**: We present an approach for road segmentation that only requires image-level annotations at training time. We leverage distant supervision, which allows us to train our model using images that are different from the target domain. Using large publicly available image databases as distant supervisors, we develop a simple method to automatically generate weak pixel-wise road masks. These are used to iteratively train a fully convolutional neural network, which produces our final segmentation model. We evaluate our method on the Cityscapes dataset, where we compare it with a fully supervised approach. Further, we discuss the trade-off between annotation cost and performance. Overall, our distantly supervised approach achieves 93.8% of the performance of the fully supervised approach, while using orders of magnitude less annotation work.



### e-Counterfeit: a mobile-server platform for document counterfeit detection
- **Arxiv ID**: http://arxiv.org/abs/1708.06126v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06126v1)
- **Published**: 2017-08-21 09:24:46+00:00
- **Updated**: 2017-08-21 09:24:46+00:00
- **Authors**: Albert Berenguel, Oriol Ramos Terrades, Josep Lladós, Cristina Cañero
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: This paper presents a novel application to detect counterfeit identity documents forged by a scan-printing operation. Texture analysis approaches are proposed to extract validation features from security background that is usually printed in documents as IDs or banknotes. The main contribution of this work is the end-to-end mobile-server architecture, which provides a service for non-expert users and therefore can be used in several scenarios. The system also provides a crowdsourcing mode so labeled images can be gathered, generating databases for incremental training of the algorithms.



### Revisiting knowledge transfer for training object class detectors
- **Arxiv ID**: http://arxiv.org/abs/1708.06128v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06128v3)
- **Published**: 2017-08-21 09:39:44+00:00
- **Updated**: 2018-03-28 14:51:59+00:00
- **Authors**: Jasper Uijlings, Stefan Popov, Vittorio Ferrari
- **Comment**: CVPR 18
- **Journal**: None
- **Summary**: We propose to revisit knowledge transfer for training object detectors on target classes from weakly supervised training images, helped by a set of source classes with bounding-box annotations. We present a unified knowledge transfer framework based on training a single neural network multi-class object detector over all source classes, organized in a semantic hierarchy. This generates proposals with scores at multiple levels in the hierarchy, which we use to explore knowledge transfer over a broad range of generality, ranging from class-specific (bicycle to motorbike) to class-generic (objectness to any class). Experiments on the 200 object classes in the ILSVRC 2013 detection dataset show that our technique: (1) leads to much better performance on the target classes (70.3% CorLoc, 36.9% mAP) than a weakly supervised baseline which uses manually engineered objectness [11] (50.5% CorLoc, 25.4% mAP). (2) delivers target object detectors reaching 80% of the mAP of their fully supervised counterparts. (3) outperforms the best reported transfer learning results on this dataset (+41% CorLoc and +3% mAP over [18, 46], +16.2% mAP over [32]). Moreover, we also carry out several across-dataset knowledge transfer experiments [27, 24, 35] and find that (4) our technique outperforms the weakly supervised baseline in all dataset pairs by 1.5x-1.9x, establishing its general applicability.



### Segmentation of retinal cysts from Optical Coherence Tomography volumes via selective enhancement
- **Arxiv ID**: http://arxiv.org/abs/1708.06197v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06197v2)
- **Published**: 2017-08-21 13:02:25+00:00
- **Updated**: 2017-08-26 10:25:43+00:00
- **Authors**: Karthik Gopinath, Jayanthi Sivaswamy
- **Comment**: Under review in Journal of Biomedical and Health Informatics
- **Journal**: None
- **Summary**: Automated and accurate segmentation of cystoid structures in Optical Coherence Tomography (OCT) is of interest in the early detection of retinal diseases. It is, however, a challenging task. We propose a novel method for localizing cysts in 3D OCT volumes. The proposed work is biologically inspired and based on selective enhancement of the cysts, by inducing motion to a given OCT slice. A Convolutional Neural Network (CNN) is designed to learn a mapping function that combines the result of multiple such motions to produce a probability map for cyst locations in a given slice. The final segmentation of cysts is obtained via simple clustering of the detected cyst locations. The proposed method is evaluated on two public datasets and one private dataset. The public datasets include the one released for the OPTIMA Cyst segmentation challenge (OCSC) in MICCAI 2015 and the DME dataset. After training on the OCSC train set, the method achieves a mean Dice Coefficient (DC) of 0.71 on the OCSC test set. The robustness of the algorithm was examined by cross-validation on the DME and AEI (private) datasets and a mean DC values obtained were 0.69 and 0.79, respectively. Overall, the proposed system outperforms all benchmarks. These results underscore the strengths of the proposed method in handling variations in both data acquisition protocols and scanners.



### Recognizing Involuntary Actions from 3D Skeleton Data Using Body States
- **Arxiv ID**: http://arxiv.org/abs/1708.06227v1
- **DOI**: 10.24200/SCI.2018.20446
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06227v1)
- **Published**: 2017-08-21 13:59:53+00:00
- **Updated**: 2017-08-21 13:59:53+00:00
- **Authors**: Mozhgan Mokari, Hoda Mohammadzade, Benyamin Ghojogh
- **Comment**: None
- **Journal**: None
- **Summary**: Human action recognition has been one of the most active fields of research in computer vision for last years. Two dimensional action recognition methods are facing serious challenges such as occlusion and missing the third dimension of data. Development of depth sensors has made it feasible to track positions of human body joints over time. This paper proposes a novel method of action recognition which uses temporal 3D skeletal Kinect data. This method introduces the definition of body states and then every action is modeled as a sequence of these states. The learning stage uses Fisher Linear Discriminant Analysis (LDA) to construct discriminant feature space for discriminating the body states. Moreover, this paper suggests the use of the Mahalonobis distance as an appropriate distance metric for the classification of the states of involuntary actions. Hidden Markov Model (HMM) is then used to model the temporal transition between the body states in each action. According to the results, this method significantly outperforms other popular methods, with recognition rate of 88.64% for eight different actions and up to 96.18% for classifying fall actions.



### Employing Weak Annotations for Medical Image Analysis Problems
- **Arxiv ID**: http://arxiv.org/abs/1708.06297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06297v1)
- **Published**: 2017-08-21 15:44:45+00:00
- **Updated**: 2017-08-21 15:44:45+00:00
- **Authors**: Martin Rajchl, Lisa M. Koch, Christian Ledig, Jonathan Passerat-Palmbach, Kazunari Misawa, Kensaku Mori, Daniel Rueckert
- **Comment**: None
- **Journal**: None
- **Summary**: To efficiently establish training databases for machine learning methods, collaborative and crowdsourcing platforms have been investigated to collectively tackle the annotation effort. However, when this concept is ported to the medical imaging domain, reading expertise will have a direct impact on the annotation accuracy. In this study, we examine the impact of expertise and the amount of available annotations on the accuracy outcome of a liver segmentation problem in an abdominal computed tomography (CT) image database. In controlled experiments, we study this impact for different types of weak annotations. To address the decrease in accuracy associated with lower expertise, we propose a method for outlier correction making use of a weakly labelled atlas. Using this approach, we demonstrate that weak annotations subject to high error rates can achieve a similarly high accuracy as state-of-the-art multi-atlas segmentation approaches relying on a large amount of expert manual segmentations. Annotations of this nature can realistically be obtained from a non-expert crowd and can potentially enable crowdsourcing of weak annotation tasks for medical image analysis.



### Learning Spread-out Local Feature Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1708.06320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06320v1)
- **Published**: 2017-08-21 16:47:20+00:00
- **Updated**: 2017-08-21 16:47:20+00:00
- **Authors**: Xu Zhang, Felix X. Yu, Sanjiv Kumar, Shih-Fu Chang
- **Comment**: ICCV 2017. 9 pages, 7 figures
- **Journal**: None
- **Summary**: We propose a simple, yet powerful regularization technique that can be used to significantly improve both the pairwise and triplet losses in learning local feature descriptors. The idea is that in order to fully utilize the expressive power of the descriptor space, good local feature descriptors should be sufficiently "spread-out" over the space. In this work, we propose a regularization term to maximize the spread in feature descriptor inspired by the property of uniform distribution. We show that the proposed regularization with triplet loss outperforms existing Euclidean distance based descriptor learning techniques by a large margin. As an extension, the proposed regularization technique can also be used to improve image-level deep feature embedding.



### STNet: Selective Tuning of Convolutional Networks for Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1708.06418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06418v1)
- **Published**: 2017-08-21 20:58:53+00:00
- **Updated**: 2017-08-21 20:58:53+00:00
- **Authors**: Mahdi Biparva, John Tsotsos
- **Comment**: None
- **Journal**: None
- **Summary**: Visual attention modeling has recently gained momentum in developing visual hierarchies provided by Convolutional Neural Networks. Despite recent successes of feedforward processing on the abstraction of concepts form raw images, the inherent nature of feedback processing has remained computationally controversial. Inspired by the computational models of covert visual attention, we propose the Selective Tuning of Convolutional Networks (STNet). It is composed of both streams of Bottom-Up and Top-Down information processing to selectively tune the visual representation of Convolutional networks. We experimentally evaluate the performance of STNet for the weakly-supervised localization task on the ImageNet benchmark dataset. We demonstrate that STNet not only successfully surpasses the state-of-the-art results but also generates attention-driven class hypothesis maps.



### PiCANet: Learning Pixel-wise Contextual Attention for Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.06433v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06433v2)
- **Published**: 2017-08-21 22:12:45+00:00
- **Updated**: 2018-04-03 09:50:03+00:00
- **Authors**: Nian Liu, Junwei Han, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Contexts play an important role in the saliency detection task. However, given a context region, not all contextual information is helpful for the final task. In this paper, we propose a novel pixel-wise contextual attention network, i.e., the PiCANet, to learn to selectively attend to informative context locations for each pixel. Specifically, for each pixel, it can generate an attention map in which each attention weight corresponds to the contextual relevance at each context location. An attended contextual feature can then be constructed by selectively aggregating the contextual information. We formulate the proposed PiCANet in both global and local forms to attend to global and local contexts, respectively. Both models are fully differentiable and can be embedded into CNNs for joint training. We also incorporate the proposed models with the U-Net architecture to detect salient objects. Extensive experiments show that the proposed PiCANets can consistently improve saliency detection performance. The global and local PiCANets facilitate learning global contrast and homogeneousness, respectively. As a result, our saliency model can detect salient objects more accurately and uniformly, thus performing favorably against the state-of-the-art methods.



