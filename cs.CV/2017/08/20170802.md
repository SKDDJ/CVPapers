# Arxiv Papers in cs.CV on 2017-08-02
### Automatic 3D Cardiovascular MR Segmentation with Densely-Connected Volumetric ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1708.00573v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00573v1)
- **Published**: 2017-08-02 01:47:41+00:00
- **Updated**: 2017-08-02 01:47:41+00:00
- **Authors**: Lequan Yu, Jie-Zhi Cheng, Qi Dou, Xin Yang, Hao Chen, Jing Qin, Pheng-Ann Heng
- **Comment**: Accepted at MICCAI 2017
- **Journal**: None
- **Summary**: Automatic and accurate whole-heart and great vessel segmentation from 3D cardiac magnetic resonance (MR) images plays an important role in the computer-assisted diagnosis and treatment of cardiovascular disease. However, this task is very challenging due to ambiguous cardiac borders and large anatomical variations among different subjects. In this paper, we propose a novel densely-connected volumetric convolutional neural network, referred as DenseVoxNet, to automatically segment the cardiac and vascular structures from 3D cardiac MR images. The DenseVoxNet adopts the 3D fully convolutional architecture for effective volume-to-volume prediction. From the learning perspective, our DenseVoxNet has three compelling advantages. First, it preserves the maximum information flow between layers by a densely-connected mechanism and hence eases the network training. Second, it avoids learning redundant feature maps by encouraging feature reuse and hence requires fewer parameters to achieve high performance, which is essential for medical applications with limited training data. Third, we add auxiliary side paths to strengthen the gradient propagation and stabilize the learning process. We demonstrate the effectiveness of DenseVoxNet by comparing it with the state-of-the-art approaches from HVSMR 2016 challenge in conjunction with MICCAI, and our network achieves the best dice coefficient. We also show that our network can achieve better performance than other 3D ConvNets but with fewer parameters.



### Kernalised Multi-resolution Convnet for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1708.00577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00577v1)
- **Published**: 2017-08-02 02:20:12+00:00
- **Updated**: 2017-08-02 02:20:12+00:00
- **Authors**: Di Wu, Wenbin Zou, Xia Li, Yong Zhao
- **Comment**: CVPRW 2017
- **Journal**: None
- **Summary**: Visual tracking is intrinsically a temporal problem. Discriminative Correlation Filters (DCF) have demonstrated excellent performance for high-speed generic visual object tracking. Built upon their seminal work, there has been a plethora of recent improvements relying on convolutional neural network (CNN) pretrained on ImageNet as a feature extractor for visual tracking. However, most of their works relying on ad hoc analysis to design the weights for different layers either using boosting or hedging techniques as an ensemble tracker. In this paper, we go beyond the conventional DCF framework and propose a Kernalised Multi-resolution Convnet (KMC) formulation that utilises hierarchical response maps to directly output the target movement. When directly deployed the learnt network to predict the unseen challenging UAV tracking dataset without any weight adjustment, the proposed model consistently achieves excellent tracking performance. Moreover, the transfered multi-reslution CNN renders it possible to be integrated into the RNN temporal learning framework, therefore opening the door on the end-to-end temporal deep learning (TDL) for visual tracking.



### Joint Transmission Map Estimation and Dehazing using Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.00581v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00581v2)
- **Published**: 2017-08-02 02:38:41+00:00
- **Updated**: 2019-04-20 17:52:49+00:00
- **Authors**: He Zhang, Vishwanath Sindagi, Vishal M. Patel
- **Comment**: This paper has been accepted in IEEE-TCSVT
- **Journal**: None
- **Summary**: Single image haze removal is an extremely challenging problem due to its inherent ill-posed nature. Several prior-based and learning-based methods have been proposed in the literature to solve this problem and they have achieved superior results. However, most of the existing methods assume constant atmospheric light model and tend to follow a two-step procedure involving prior-based methods for estimating transmission map followed by calculation of dehazed image using the closed form solution. In this paper, we relax the constant atmospheric light assumption and propose a novel unified single image dehazing network that jointly estimates the transmission map and performs dehazing. In other words, our new approach provides an end-to-end learning framework, where the inherent transmission map and dehazed result are learned directly from the loss function. Extensive experiments on synthetic and real datasets with challenging hazy images demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods.



### A Learning-based Framework for Hybrid Depth-from-Defocus and Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1708.00583v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00583v3)
- **Published**: 2017-08-02 02:43:04+00:00
- **Updated**: 2018-08-06 17:04:57+00:00
- **Authors**: Zhang Chen, Xinqing Guo, Siyuan Li, Xuan Cao, Jingyi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Depth from defocus (DfD) and stereo matching are two most studied passive depth sensing schemes. The techniques are essentially complementary: DfD can robustly handle repetitive textures that are problematic for stereo matching whereas stereo matching is insensitive to defocus blurs and can handle large depth range. In this paper, we present a unified learning-based technique to conduct hybrid DfD and stereo matching. Our input is image triplets: a stereo pair and a defocused image of one of the stereo views. We first apply depth-guided light field rendering to construct a comprehensive training dataset for such hybrid sensing setups. Next, we adopt the hourglass network architecture to separately conduct depth inference from DfD and stereo. Finally, we exploit different connection methods between the two separate networks for integrating them into a unified solution to produce high fidelity 3D disparity maps. Comprehensive experiments on real and synthetic data show that our new learning-based hybrid 3D sensing technique can significantly improve accuracy and robustness in 3D reconstruction.



### A Simple Loss Function for Improving the Convergence and Accuracy of Visual Question Answering Models
- **Arxiv ID**: http://arxiv.org/abs/1708.00584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00584v1)
- **Published**: 2017-08-02 02:51:32+00:00
- **Updated**: 2017-08-02 02:51:32+00:00
- **Authors**: Ilija Ilievski, Jiashi Feng
- **Comment**: accepted at CVPR 2017 VQA workshop
- **Journal**: None
- **Summary**: Visual question answering as recently proposed multimodal learning task has enjoyed wide attention from the deep learning community. Lately, the focus was on developing new representation fusion methods and attention mechanisms to achieve superior performance. On the other hand, very little focus has been put on the models' loss function, arguably one of the most important aspects of training deep learning models. The prevailing practice is to use cross entropy loss function that penalizes the probability given to all the answers in the vocabulary except the single most common answer for the particular question. However, the VQA evaluation function compares the predicted answer with all the ground-truth answers for the given question and if there is a matching, a partial point is given. This causes a discrepancy between the model's cross entropy loss and the model's accuracy as calculated by the VQA evaluation function. In this work, we propose a novel loss, termed as soft cross entropy, that considers all ground-truth answers and thus reduces the loss-accuracy discrepancy. The proposed loss leads to an improved training convergence of VQA models and an increase in accuracy as much as 1.6%.



### Controllable Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1708.00598v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.00598v5)
- **Published**: 2017-08-02 04:17:59+00:00
- **Updated**: 2019-03-30 08:00:54+00:00
- **Authors**: Minhyeok Lee, Junhee Seok
- **Comment**: A fully revised version of this paper is published in IEEE Access.
  Please refer to https://doi.org/10.1109/ACCESS.2019.2899108
- **Journal**: None
- **Summary**: Recently introduced generative adversarial network (GAN) has been shown numerous promising results to generate realistic samples. The essential task of GAN is to control the features of samples generated from a random distribution. While the current GAN structures, such as conditional GAN, successfully generate samples with desired major features, they often fail to produce detailed features that bring specific differences among samples. To overcome this limitation, here we propose a controllable GAN (ControlGAN) structure. By separating a feature classifier from a discriminator, the generator of ControlGAN is designed to learn generating synthetic samples with the specific detailed features. Evaluated with multiple image datasets, ControlGAN shows a power to generate improved samples with well-controlled features. Furthermore, we demonstrate that ControlGAN can generate intermediate features and opposite features for interpolated and extrapolated input labels that are not used in the training process. It implies that ControlGAN can significantly contribute to the variety of generated samples.



### Exact Tensor Completion from Sparsely Corrupted Observations via Convex Optimization
- **Arxiv ID**: http://arxiv.org/abs/1708.00601v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.00601v1)
- **Published**: 2017-08-02 04:45:42+00:00
- **Updated**: 2017-08-02 04:45:42+00:00
- **Authors**: Jonathan Q. Jiang, Michael K. Ng
- **Comment**: 36 pages, 9 figures
- **Journal**: None
- **Summary**: This paper conducts a rigorous analysis for provable estimation of multidimensional arrays, in particular third-order tensors, from a random subset of its corrupted entries. Our study rests heavily on a recently proposed tensor algebraic framework in which we can obtain tensor singular value decomposition (t-SVD) that is similar to the SVD for matrices, and define a new notion of tensor rank referred to as the tubal rank. We prove that by simply solving a convex program, which minimizes a weighted combination of tubal nuclear norm, a convex surrogate for the tubal rank, and the $\ell_1$-norm, one can recover an incoherent tensor exactly with overwhelming probability, provided that its tubal rank is not too large and that the corruptions are reasonably sparse. Interestingly, our result includes the recovery guarantees for the problems of tensor completion (TC) and tensor principal component analysis (TRPCA) under the same algebraic setup as special cases. An alternating direction method of multipliers (ADMM) algorithm is presented to solve this optimization problem. Numerical experiments verify our theory and real-world applications demonstrate the effectiveness of our algorithm.



### CT sinogram-consistency learning for metal-induced beam hardening correction
- **Arxiv ID**: http://arxiv.org/abs/1708.00607v2
- **DOI**: 10.1002/mp.13199
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.00607v2)
- **Published**: 2017-08-02 05:36:25+00:00
- **Updated**: 2018-01-12 07:05:01+00:00
- **Authors**: Hyung Suk Park, Sung Min Lee, Hwa Pyung Kim, Jin Keun Seo
- **Comment**: 17 pages, 8 figures
- **Journal**: None
- **Summary**: This paper proposes a sinogram consistency learning method to deal with beam-hardening related artifacts in polychromatic computerized tomography (CT). The presence of highly attenuating materials in the scan field causes an inconsistent sinogram, that does not match the range space of the Radon transform. When the mismatched data are entered into the range space during CT reconstruction, streaking and shading artifacts are generated owing to the inherent nature of the inverse Radon transform. The proposed learning method aims to repair inconsistent sinograms by removing the primary metal-induced beam-hardening factors along the metal trace in the sinogram. Taking account of the fundamental difficulty in obtaining sufficient training data in a medical environment, the learning method is designed to use simulated training data and a patient-type specific learning model is used to simplify the learning process. The feasibility of the proposed method is investigated using a dataset, consisting of real CT scan of pelvises containing hip prostheses. The anatomical areas in training and test data are different, in order to demonstrate that the proposed method extracts the beam hardening features, selectively. The results show that our method successfully corrects sinogram inconsistency by extracting beam-hardening sources by means of deep learning. This paper proposed a deep learning method of sinogram correction for beam hardening reduction in CT for the first time. Conventional methods for beam hardening reduction are based on regularizations, and have the fundamental drawback of being not easily able to use manifold CT images, while a deep learning approach has the potential to do so.



### Monocular Depth Estimation with Hierarchical Fusion of Dilated CNNs and Soft-Weighted-Sum Inference
- **Arxiv ID**: http://arxiv.org/abs/1708.02287v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02287v1)
- **Published**: 2017-08-02 06:41:50+00:00
- **Updated**: 2017-08-02 06:41:50+00:00
- **Authors**: Bo Li, Yuchao Dai, Mingyi He
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular depth estimation is a challenging task in complex compositions depicting multiple objects of diverse scales. Albeit the recent great progress thanks to the deep convolutional neural networks (CNNs), the state-of-the-art monocular depth estimation methods still fall short to handle such real-world challenging scenarios. In this paper, we propose a deep end-to-end learning framework to tackle these challenges, which learns the direct mapping from a color image to the corresponding depth map. First, we represent monocular depth estimation as a multi-category dense labeling task by contrast to the regression based formulation. In this way, we could build upon the recent progress in dense labeling such as semantic segmentation. Second, we fuse different side-outputs from our front-end dilated convolutional neural network in a hierarchical way to exploit the multi-scale depth cues for depth estimation, which is critical to achieve scale-aware depth estimation. Third, we propose to utilize soft-weighted-sum inference instead of the hard-max inference, transforming the discretized depth score to continuous depth value. Thus, we reduce the influence of quantization error and improve the robustness of our method. Extensive experiments on the NYU Depth V2 and KITTI datasets show the superiority of our method compared with current state-of-the-art methods. Furthermore, experiments on the NYU V2 dataset reveal that our model is able to learn the probability distribution of depth.



### On the Importance of Consistency in Training Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.00631v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1708.00631v1)
- **Published**: 2017-08-02 08:05:09+00:00
- **Updated**: 2017-08-02 08:05:09+00:00
- **Authors**: Chengxi Ye, Yezhou Yang, Cornelia Fermuller, Yiannis Aloimonos
- **Comment**: None
- **Journal**: None
- **Summary**: We explain that the difficulties of training deep neural networks come from a syndrome of three consistency issues. This paper describes our efforts in their analysis and treatment. The first issue is the training speed inconsistency in different layers. We propose to address it with an intuitive, simple-to-implement, low footprint second-order method. The second issue is the scale inconsistency between the layer inputs and the layer residuals. We explain how second-order information provides favorable convenience in removing this roadblock. The third and most challenging issue is the inconsistency in residual propagation. Based on the fundamental theorem of linear algebra, we provide a mathematical characterization of the famous vanishing gradient problem. Thus, an important design principle for future optimization and neural network design is derived. We conclude this paper with the construction of a novel contractive neural network.



### Dual-Glance Model for Deciphering Social Relationships
- **Arxiv ID**: http://arxiv.org/abs/1708.00634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00634v1)
- **Published**: 2017-08-02 08:13:28+00:00
- **Updated**: 2017-08-02 08:13:28+00:00
- **Authors**: Junnan Li, Yongkang Wong, Qi Zhao, Mohan S. Kankanhalli
- **Comment**: IEEE International Conference on Computer Vision (ICCV), 2017
- **Journal**: None
- **Summary**: Since the beginning of early civilizations, social relationships derived from each individual fundamentally form the basis of social structure in our daily life. In the computer vision literature, much progress has been made in scene understanding, such as object detection and scene parsing. Recent research focuses on the relationship between objects based on its functionality and geometrical relations. In this work, we aim to study the problem of social relationship recognition, in still images. We have proposed a dual-glance model for social relationship recognition, where the first glance fixates at the individual pair of interest and the second glance deploys attention mechanism to explore contextual cues. We have also collected a new large scale People in Social Context (PISC) dataset, which comprises of 22,670 images and 76,568 annotated samples from 9 types of social relationship. We provide benchmark results on the PISC dataset, and qualitatively demonstrate the efficacy of the proposed model.



### Generation of High Dynamic Range Illumination from a Single Image for the Enhancement of Undesirably Illuminated Images
- **Arxiv ID**: http://arxiv.org/abs/1708.00636v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1708.00636v1)
- **Published**: 2017-08-02 08:14:18+00:00
- **Updated**: 2017-08-02 08:14:18+00:00
- **Authors**: Jae Sung Park, Nam Ik Cho
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an algorithm that enhances undesirably illuminated images by generating and fusing multi-level illuminations from a single image.The input image is first decomposed into illumination and reflectance components by using an edge-preserving smoothing filter. Then the reflectance component is scaled up to improve the image details in bright areas. The illumination component is scaled up and down to generate several illumination images that correspond to certain camera exposure values different from the original. The virtual multi-exposure illuminations are blended into an enhanced illumination, where we also propose a method to generate appropriate weight maps for the tone fusion. Finally, an enhanced image is obtained by multiplying the equalized illumination and enhanced reflectance. Experiments show that the proposed algorithm produces visually pleasing output and also yields comparable objective results to the conventional enhancement methods, while requiring modest computational loads.



### Temporal Dynamic Graph LSTM for Action-driven Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.00666v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00666v1)
- **Published**: 2017-08-02 09:38:26+00:00
- **Updated**: 2017-08-02 09:38:26+00:00
- **Authors**: Yuan Yuan, Xiaodan Liang, Xiaolong Wang, Dit-Yan Yeung, Abhinav Gupta
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: In this paper, we investigate a weakly-supervised object detection framework. Most existing frameworks focus on using static images to learn object detectors. However, these detectors often fail to generalize to videos because of the existing domain shift. Therefore, we investigate learning these detectors directly from boring videos of daily activities. Instead of using bounding boxes, we explore the use of action descriptions as supervision since they are relatively easy to gather. A common issue, however, is that objects of interest that are not involved in human actions are often absent in global action descriptions known as "missing label". To tackle this problem, we propose a novel temporal dynamic graph Long Short-Term Memory network (TD-Graph LSTM). TD-Graph LSTM enables global temporal reasoning by constructing a dynamic graph that is based on temporal correlations of object proposals and spans the entire video. The missing label issue for each individual frame can thus be significantly alleviated by transferring knowledge across correlated objects proposals in the whole video. Extensive evaluations on a large-scale daily-life action dataset (i.e., Charades) demonstrates the superiority of our proposed method. We also release object bounding-box annotations for more than 5,000 frames in Charades. We believe this annotated data can also benefit other research on video-based object recognition in the future.



### Action recognition by learning pose representations
- **Arxiv ID**: http://arxiv.org/abs/1708.00672v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00672v1)
- **Published**: 2017-08-02 09:46:02+00:00
- **Updated**: 2017-08-02 09:46:02+00:00
- **Authors**: Alessia Saggese, Nicola Strisciuglio, Mario Vento, Nicolai Petkov
- **Comment**: Accepted at REACTS workshop (CAIP conference 2017)
- **Journal**: None
- **Summary**: Pose detection is one of the fundamental steps for the recognition of human actions. In this paper we propose a novel trainable detector for recognizing human poses based on the analysis of the skeleton. The main idea is that a skeleton pose can be described by the spatial arrangements of its joints. Starting from this consideration, we propose a trainable pose detector, that can be configured on a prototype skeleton in an automatic configuration process. The result of the configuration is a model of the position of the joints in the concerned skeleton. In the application phase, the joint positions contained in the model are compared with the ones of their homologous joints in the skeleton under test. The similarity of two skeletons is computed as a combination of the position scores achieved by homologous joints. In this paper we describe an action classification method based on the use of the proposed trainable detectors to extract features from the skeletons. We performed experiments on the publicly available MSDRA data set and the achieved results confirm the effectiveness of the proposed approach.



### Deep Detection of People and their Mobility Aids for a Hospital Robot
- **Arxiv ID**: http://arxiv.org/abs/1708.00674v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.00674v1)
- **Published**: 2017-08-02 09:54:22+00:00
- **Updated**: 2017-08-02 09:54:22+00:00
- **Authors**: Andres Vasquez, Marina Kollmitz, Andreas Eitel, Wolfram Burgard
- **Comment**: 7 pages, ECMR 2017, dataset and videos:
  http://www2.informatik.uni-freiburg.de/~kollmitz/MobilityAids/
- **Journal**: None
- **Summary**: Robots operating in populated environments encounter many different types of people, some of whom might have an advanced need for cautious interaction, because of physical impairments or their advanced age. Robots therefore need to recognize such advanced demands to provide appropriate assistance, guidance or other forms of support. In this paper, we propose a depth-based perception pipeline that estimates the position and velocity of people in the environment and categorizes them according to the mobility aids they use: pedestrian, person in wheelchair, person in a wheelchair with a person pushing them, person with crutches and person using a walker. We present a fast region proposal method that feeds a Region-based Convolutional Network (Fast R-CNN). With this, we speed up the object detection process by a factor of seven compared to a dense sliding window approach. We furthermore propose a probabilistic position, velocity and class estimator to smooth the CNN's detections and account for occlusions and misclassifications. In addition, we introduce a new hospital dataset with over 17,000 annotated RGB-D images. Extensive experiments confirm that our pipeline successfully keeps track of people and their mobility aids, even in challenging situations with multiple people from different categories and frequent occlusions. Videos of our experiments and the dataset are available at http://www2.informatik.uni-freiburg.de/~kollmitz/MobilityAids



### OmniArt: Multi-task Deep Learning for Artistic Data Analysis
- **Arxiv ID**: http://arxiv.org/abs/1708.00684v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.00684v1)
- **Published**: 2017-08-02 10:20:22+00:00
- **Updated**: 2017-08-02 10:20:22+00:00
- **Authors**: Gjorgji Strezoski, Marcel Worring
- **Comment**: 9 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: Vast amounts of artistic data is scattered on-line from both museums and art applications. Collecting, processing and studying it with respect to all accompanying attributes is an expensive process. With a motivation to speed up and improve the quality of categorical analysis in the artistic domain, in this paper we propose an efficient and accurate method for multi-task learning with a shared representation applied in the artistic domain. We continue to show how different multi-task configurations of our method behave on artistic data and outperform handcrafted feature approaches as well as convolutional neural networks. In addition to the method and analysis, we propose a challenge like nature to the new aggregated data set with almost half a million samples and structured meta-data to encourage further research and societal engagement.



### Accurate Lung Segmentation via Network-Wise Training of Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.00710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00710v1)
- **Published**: 2017-08-02 11:47:01+00:00
- **Updated**: 2017-08-02 11:47:01+00:00
- **Authors**: Sangheum Hwang, Sunggyun Park
- **Comment**: Accepted to the 3rd Workshop on Deep Learning in Medical Image
  Analysis (DLMIA 2017), MICCAI 2017
- **Journal**: None
- **Summary**: We introduce an accurate lung segmentation model for chest radiographs based on deep convolutional neural networks. Our model is based on atrous convolutional layers to increase the field-of-view of filters efficiently. To improve segmentation performances further, we also propose a multi-stage training strategy, network-wise training, which the current stage network is fed with both input images and the outputs from pre-stage network. It is shown that this strategy has an ability to reduce falsely predicted labels and produce smooth boundaries of lung fields. We evaluate the proposed model on a common benchmark dataset, JSRT, and achieve the state-of-the-art segmentation performances with much fewer model parameters.



### InfiniTAM v3: A Framework for Large-Scale 3D Reconstruction with Loop Closure
- **Arxiv ID**: http://arxiv.org/abs/1708.00783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00783v1)
- **Published**: 2017-08-02 14:50:02+00:00
- **Updated**: 2017-08-02 14:50:02+00:00
- **Authors**: Victor Adrian Prisacariu, Olaf Kähler, Stuart Golodetz, Michael Sapienza, Tommaso Cavallari, Philip H S Torr, David W Murray
- **Comment**: This article largely supersedes arxiv:1410.0925 (it describes version
  3 of the InfiniTAM framework)
- **Journal**: None
- **Summary**: Volumetric models have become a popular representation for 3D scenes in recent years. One breakthrough leading to their popularity was KinectFusion, which focuses on 3D reconstruction using RGB-D sensors. However, monocular SLAM has since also been tackled with very similar approaches. Representing the reconstruction volumetrically as a TSDF leads to most of the simplicity and efficiency that can be achieved with GPU implementations of these systems. However, this representation is memory-intensive and limits applicability to small-scale reconstructions. Several avenues have been explored to overcome this. With the aim of summarizing them and providing for a fast, flexible 3D reconstruction pipeline, we propose a new, unifying framework called InfiniTAM. The idea is that steps like camera tracking, scene representation and integration of new data can easily be replaced and adapted to the user's needs.   This report describes the technical implementation details of InfiniTAM v3, the third version of our InfiniTAM system. We have added various new features, as well as making numerous enhancements to the low-level code that significantly improve our camera tracking performance. The new features that we expect to be of most interest are (i) a robust camera tracking module; (ii) an implementation of Glocker et al.'s keyframe-based random ferns camera relocaliser; (iii) a novel approach to globally-consistent TSDF-based reconstruction, based on dividing the scene into rigid submaps and optimising the relative poses between them; and (iv) an implementation of Keller et al.'s surfel-based reconstruction approach.



### Structure-measure: A New Way to Evaluate Foreground Maps
- **Arxiv ID**: http://arxiv.org/abs/1708.00786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00786v1)
- **Published**: 2017-08-02 14:54:16+00:00
- **Updated**: 2017-08-02 14:54:16+00:00
- **Authors**: Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, Ali Borji
- **Comment**: Accepted by ICCV 2017
- **Journal**: None
- **Summary**: Foreground map evaluation is crucial for gauging the progress of object segmentation algorithms, in particular in the filed of salient object detection where the purpose is to accurately detect and segment the most salient object in a scene. Several widely-used measures such as Area Under the Curve (AUC), Average Precision (AP) and the recently proposed Fbw have been utilized to evaluate the similarity between a non-binary saliency map (SM) and a ground-truth (GT) map. These measures are based on pixel-wise errors and often ignore the structural similarities. Behavioral vision studies, however, have shown that the human visual system is highly sensitive to structures in scenes. Here, we propose a novel, efficient, and easy to calculate measure known an structural similarity measure (Structure-measure) to evaluate non-binary foreground maps. Our new measure simultaneously evaluates region-aware and object-aware structural similarity between a SM and a GT map. We demonstrate superiority of our measure over existing ones using 5 meta-measures on 5 benchmark datasets.



### Predictive Coding for Dynamic Visual Processing: Development of Functional Hierarchy in a Multiple Spatio-Temporal Scales RNN Model
- **Arxiv ID**: http://arxiv.org/abs/1708.00812v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00812v2)
- **Published**: 2017-08-02 16:17:32+00:00
- **Updated**: 2017-09-06 04:04:04+00:00
- **Authors**: Minkyu Choi, Jun Tani
- **Comment**: Accepted in Neural Computation (MIT press)
- **Journal**: None
- **Summary**: The current paper proposes a novel predictive coding type neural network model, the predictive multiple spatio-temporal scales recurrent neural network (P-MSTRNN). The P-MSTRNN learns to predict visually perceived human whole-body cyclic movement patterns by exploiting multiscale spatio-temporal constraints imposed on network dynamics by using differently sized receptive fields as well as different time constant values for each layer. After learning, the network becomes able to proactively imitate target movement patterns by inferring or recognizing corresponding intentions by means of the regression of prediction error. Results show that the network can develop a functional hierarchy by developing a different type of dynamic structure at each layer. The paper examines how model performance during pattern generation as well as predictive imitation varies depending on the stage of learning. The number of limit cycle attractors corresponding to target movement patterns increases as learning proceeds. And, transient dynamics developing early in the learning process successfully perform pattern generation and predictive imitation tasks. The paper concludes that exploitation of transient dynamics facilitates successful task performance during early learning periods.



### Land Cover Classification from Multi-temporal, Multi-spectral Remotely Sensed Imagery using Patch-Based Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.00813v1
- **DOI**: 10.1016/j.neunet.2018.05.019
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00813v1)
- **Published**: 2017-08-02 16:18:46+00:00
- **Updated**: 2017-08-02 16:18:46+00:00
- **Authors**: Atharva Sharma, Xiuwen Liu, Xiaojun Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Sustainability of the global environment is dependent on the accurate land cover information over large areas. Even with the increased number of satellite systems and sensors acquiring data with improved spectral, spatial, radiometric and temporal characteristics and the new data distribution policy, most existing land cover datasets were derived from a pixel-based single-date multi-spectral remotely sensed image with low accuracy. To improve the accuracy, the bottleneck is how to develop an accurate and effective image classification technique. By incorporating and utilizing the complete multi-spectral, multi-temporal and spatial information in remote sensing images and considering their inherit spatial and sequential interdependence, we propose a new patch-based RNN (PB-RNN) system tailored for multi-temporal remote sensing data. The system is designed by incorporating distinctive characteristics in multi-temporal remote sensing data. In particular, it uses multi-temporal-spectral-spatial samples and deals with pixels contaminated by clouds/shadow present in the multi-temporal data series. Using a Florida Everglades ecosystem study site covering an area of 771 square kilo-meters, the proposed PB-RNN system has achieved a significant improvement in the classification accuracy over pixel-based RNN system, pixel-based single-imagery NN system, pixel-based multi-images NN system, patch-based single-imagery NN system and patch-based multi-images NN system. For example, the proposed system achieves 97.21% classification accuracy while a pixel-based single-imagery NN system achieves 64.74%. By utilizing methods like the proposed PB-RNN one, we believe that much more accurate land cover datasets can be produced over large areas efficiently.



### An End-to-End Compression Framework Based on Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.00838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00838v1)
- **Published**: 2017-08-02 17:26:28+00:00
- **Updated**: 2017-08-02 17:26:28+00:00
- **Authors**: Feng Jiang, Wen Tao, Shaohui Liu, Jie Ren, Xun Guo, Debin Zhao
- **Comment**: Submitted to IEEE Transactions on Circuits and Systems for Video
  Technology
- **Journal**: None
- **Summary**: Deep learning, e.g., convolutional neural networks (CNNs), has achieved great success in image processing and computer vision especially in high level vision applications such as recognition and understanding. However, it is rarely used to solve low-level vision problems such as image compression studied in this paper. Here, we move forward a step and propose a novel compression framework based on CNNs. To achieve high-quality image compression at low bit rates, two CNNs are seamlessly integrated into an end-to-end compression framework. The first CNN, named compact convolutional neural network (ComCNN), learns an optimal compact representation from an input image, which preserves the structural information and is then encoded using an image codec (e.g., JPEG, JPEG2000 or BPG). The second CNN, named reconstruction convolutional neural network (RecCNN), is used to reconstruct the decoded image with high-quality in the decoding end. To make two CNNs effectively collaborate, we develop a unified end-to-end learning algorithm to simultaneously learn ComCNN and RecCNN, which facilitates the accurate reconstruction of the decoded image using RecCNN. Such a design also makes the proposed compression framework compatible with existing image coding standards. Experimental results validate that the proposed compression framework greatly outperforms several compression frameworks that use existing image coding standards with state-of-the-art deblocking or denoising post-processing methods.



### Fingerprint Extraction Using Smartphone Camera
- **Arxiv ID**: http://arxiv.org/abs/1708.00884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00884v1)
- **Published**: 2017-08-02 18:33:53+00:00
- **Updated**: 2017-08-02 18:33:53+00:00
- **Authors**: Saksham Gupta, Sukhad Anand, Atul Rai
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: In the previous decade, there has been a considerable rise in the usage of smartphones.Due to exorbitant advancement in technology, computational speed and quality of image capturing has increased considerably. With an increase in the need for remote fingerprint verification, smartphones can be used as a powerful alternative for fingerprint authentication instead of conventional optical sensors. In this research, wepropose a technique to capture finger-images from the smartphones and pre-process them in such a way that it can be easily matched with the optical sensor images.Effective finger-image capturing, image enhancement, fingerprint pattern extraction, core point detection and image alignment techniques have been discussed. The proposed approach has been validated on FVC 2004 DB1 & DB2 dataset and the results show the efficacy of the methodology proposed. The method can be deployed for real-time commercial usage.



### PIVO: Probabilistic Inertial-Visual Odometry for Occlusion-Robust Navigation
- **Arxiv ID**: http://arxiv.org/abs/1708.00894v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00894v2)
- **Published**: 2017-08-02 18:58:38+00:00
- **Updated**: 2018-01-23 14:12:24+00:00
- **Authors**: Arno Solin, Santiago Cortes, Esa Rahtu, Juho Kannala
- **Comment**: 10 pages, 4 figures. Paper to be published in WACV 2018
- **Journal**: None
- **Summary**: This paper presents a novel method for visual-inertial odometry. The method is based on an information fusion framework employing low-cost IMU sensors and the monocular camera in a standard smartphone. We formulate a sequential inference scheme, where the IMU drives the dynamical model and the camera frames are used in coupling trailing sequences of augmented poses. The novelty in the model is in taking into account all the cross-terms in the updates, thus propagating the inter-connected uncertainties throughout the model. Stronger coupling between the inertial and visual data sources leads to robustness against occlusion and feature-poor environments. We demonstrate results on data collected with an iPhone and provide comparisons against the Tango device and using the EuRoC data set.



### Learning Spherical Convolution for Fast Features from 360° Imagery
- **Arxiv ID**: http://arxiv.org/abs/1708.00919v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00919v3)
- **Published**: 2017-08-02 20:18:10+00:00
- **Updated**: 2018-12-07 17:34:28+00:00
- **Authors**: Yu-Chuan Su, Kristen Grauman
- **Comment**: NIPS 2017
- **Journal**: None
- **Summary**: While 360{\deg} cameras offer tremendous new possibilities in vision, graphics, and augmented reality, the spherical images they produce make core feature extraction non-trivial. Convolutional neural networks (CNNs) trained on images from perspective cameras yield "flat" filters, yet 360{\deg} images cannot be projected to a single plane without significant distortion. A naive solution that repeatedly projects the viewing sphere to all tangent planes is accurate, but much too computationally intensive for real problems. We propose to learn a spherical convolutional network that translates a planar CNN to process 360{\deg} imagery directly in its equirectangular projection. Our approach learns to reproduce the flat filter outputs on 360{\deg} data, sensitive to the varying distortion effects across the viewing sphere. The key benefits are 1) efficient feature extraction for 360{\deg} images and video, and 2) the ability to leverage powerful pre-trained networks researchers have carefully honed (together with massive labeled image training sets) for perspective images. We validate our approach compared to several alternative methods in terms of both raw CNN output accuracy as well as applying a state-of-the-art "flat" object detector to 360{\deg} data. Our method yields the most accurate results while saving orders of magnitude in computation versus the existing exact reprojection solution.



### Combining Keystroke Dynamics and Face Recognition for User Verification
- **Arxiv ID**: http://arxiv.org/abs/1708.00931v1
- **DOI**: 10.1109/CSE.2015.37
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.00931v1)
- **Published**: 2017-08-02 21:13:59+00:00
- **Updated**: 2017-08-02 21:13:59+00:00
- **Authors**: Abhinav Gupta, Agrim Khanna, Anmol Jagetia, Devansh Sharma, Sanchit Alekh, Vaibhav Choudhary
- **Comment**: None
- **Journal**: None
- **Summary**: The massive explosion and ubiquity of computing devices and the outreach of the web have been the most defining events of the century so far. As more and more people gain access to the internet, traditional know-something and have-something authentication methods such as PINs and passwords are proving to be insufficient for prohibiting unauthorized access to increasingly personal data on the web. Therefore, the need of the hour is a user-verification system that is not only more reliable and secure, but also unobtrusive and minimalistic. Keystroke Dynamics is a novel Biometric Technique; it is not only unobtrusive, but also transparent and inexpensive. The fusion of keystroke dynamics and Face Recognition engenders the most desirable characteristics of a verification system. Our implementation uses Hidden Markov Models (HMM) for modelling the Keystroke Dynamics, with the help of two widely used Feature Vectors: Keypress Latency and Keypress Duration. On the other hand, Face Recognition makes use of the traditional Eigenfaces approach.The results show that the system has a high precision, with a False Acceptance Rate of 5.4% and a False Rejection Rate of 9.2%. Moreover, it is also future-proof, as the hardware requirements, i.e. camera and keyboard (physical or on-screen), have become an indispensable part of modern computing.



### Associative Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1708.00938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00938v1)
- **Published**: 2017-08-02 21:38:45+00:00
- **Updated**: 2017-08-02 21:38:45+00:00
- **Authors**: Philip Haeusser, Thomas Frerix, Alexander Mordvintsev, Daniel Cremers
- **Comment**: In IEEE International Conference on Computer Vision (ICCV), 2017
- **Journal**: None
- **Summary**: We propose associative domain adaptation, a novel technique for end-to-end domain adaptation with neural networks, the task of inferring class labels for an unlabeled target domain based on the statistical properties of a labeled source domain. Our training scheme follows the paradigm that in order to effectively derive class labels for the target domain, a network should produce statistically domain invariant embeddings, while minimizing the classification error on the labeled source domain. We accomplish this by reinforcing associations between source and target data directly in embedding space. Our method can easily be added to any existing classification network with no structural and almost no computational overhead. We demonstrate the effectiveness of our approach on various benchmarks and achieve state-of-the-art results across the board with a generic convolutional neural network architecture not specifically tuned to the respective tasks. Finally, we show that the proposed association loss produces embeddings that are more effective for domain adaptation compared to methods employing maximum mean discrepancy as a similarity measure in embedding space.



### An Energy Minimization Approach to 3D Non-Rigid Deformable Surface Estimation Using RGBD Data
- **Arxiv ID**: http://arxiv.org/abs/1708.00940v1
- **DOI**: 10.1109/IROS.2012.6386213
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1708.00940v1)
- **Published**: 2017-08-02 21:43:29+00:00
- **Updated**: 2017-08-02 21:43:29+00:00
- **Authors**: Bryan Willimon, Steven Hickson, Ian Walker, Stan Birchfield
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an algorithm that uses energy mini- mization to estimate the current configuration of a non-rigid object. Our approach utilizes an RGBD image to calculate corresponding SURF features, depth, and boundary informa- tion. We do not use predetermined features, thus enabling our system to operate on unmodified objects. Our approach relies on a 3D nonlinear energy minimization framework to solve for the configuration using a semi-implicit scheme. Results show various scenarios of dynamic posters and shirts in different configurations to illustrate the performance of the method. In particular, we show that our method is able to estimate the configuration of a textureless nonrigid object with no correspondences available.



### Predicting Human Activities Using Stochastic Grammar
- **Arxiv ID**: http://arxiv.org/abs/1708.00945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00945v1)
- **Published**: 2017-08-02 22:01:48+00:00
- **Updated**: 2017-08-02 22:01:48+00:00
- **Authors**: Siyuan Qi, Siyuan Huang, Ping Wei, Song-Chun Zhu
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: This paper presents a novel method to predict future human activities from partially observed RGB-D videos. Human activity prediction is generally difficult due to its non-Markovian property and the rich context between human and environments.   We use a stochastic grammar model to capture the compositional structure of events, integrating human actions, objects, and their affordances. We represent the event by a spatial-temporal And-Or graph (ST-AOG). The ST-AOG is composed of a temporal stochastic grammar defined on sub-activities, and spatial graphs representing sub-activities that consist of human actions, objects, and their affordances. Future sub-activities are predicted using the temporal grammar and Earley parsing algorithm. The corresponding action, object, and affordance labels are then inferred accordingly. Extensive experiments are conducted to show the effectiveness of our model on both semantic event parsing and future activity prediction.



### Semantic Instance Labeling Leveraging Hierarchical Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1708.00946v1
- **DOI**: 10.1109/WACV.2015.147
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00946v1)
- **Published**: 2017-08-02 22:04:15+00:00
- **Updated**: 2017-08-02 22:04:15+00:00
- **Authors**: Steven Hickson, Irfan Essa, Henrik Christensen
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the approaches for indoor RGBD semantic la- beling focus on using pixels or superpixels to train a classi- fier. In this paper, we implement a higher level segmentation using a hierarchy of superpixels to obtain a better segmen- tation for training our classifier. By focusing on meaningful segments that conform more directly to objects, regardless of size, we train a random forest of decision trees as a clas- sifier using simple features such as the 3D size, LAB color histogram, width, height, and shape as specified by a his- togram of surface normals. We test our method on the NYU V2 depth dataset, a challenging dataset of cluttered indoor environments. Our experiments using the NYU V2 depth dataset show that our method achieves state of the art re- sults on both a general semantic labeling introduced by the dataset (floor, structure, furniture, and objects) and a more object specific semantic labeling. We show that training a classifier on a segmentation from a hierarchy of super pixels yields better results than training directly on super pixels, patches, or pixels as in previous work.



### Generating High-Quality Crowd Density Maps using Contextual Pyramid CNNs
- **Arxiv ID**: http://arxiv.org/abs/1708.00953v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00953v1)
- **Published**: 2017-08-02 22:54:21+00:00
- **Updated**: 2017-08-02 22:54:21+00:00
- **Authors**: Vishwanath A. Sindagi, Vishal M. Patel
- **Comment**: Accepted at ICCV 2017
- **Journal**: None
- **Summary**: We present a novel method called Contextual Pyramid CNN (CP-CNN) for generating high-quality crowd density and count estimation by explicitly incorporating global and local contextual information of crowd images. The proposed CP-CNN consists of four modules: Global Context Estimator (GCE), Local Context Estimator (LCE), Density Map Estimator (DME) and a Fusion-CNN (F-CNN). GCE is a VGG-16 based CNN that encodes global context and it is trained to classify input images into different density classes, whereas LCE is another CNN that encodes local context information and it is trained to perform patch-wise classification of input images into different density classes. DME is a multi-column architecture-based CNN that aims to generate high-dimensional feature maps from the input image which are fused with the contextual information estimated by GCE and LCE using F-CNN. To generate high resolution and high-quality density maps, F-CNN uses a set of convolutional and fractionally-strided convolutional layers and it is trained along with the DME in an end-to-end fashion using a combination of adversarial loss and pixel-level Euclidean loss. Extensive experiments on highly challenging datasets show that the proposed method achieves significant improvements over the state-of-the-art methods.



