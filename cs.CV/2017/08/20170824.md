# Arxiv Papers in cs.CV on 2017-08-24
### An Image Analysis Approach to the Calligraphy of Books
- **Arxiv ID**: http://arxiv.org/abs/1708.07265v1
- **DOI**: 10.1016/j.physa.2018.06.110
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.07265v1)
- **Published**: 2017-08-24 03:12:22+00:00
- **Updated**: 2017-08-24 03:12:22+00:00
- **Authors**: Henrique F. de Arruda, Vanessa Q. Marinho, Thales S. Lima, Diego R. Amancio, Luciano da F. Costa
- **Comment**: None
- **Journal**: Physica A 510, 110--120 (2018)
- **Summary**: Text network analysis has received increasing attention as a consequence of its wide range of applications. In this work, we extend a previous work founded on the study of topological features of mesoscopic networks. Here, the geometrical properties of visualized networks are quantified in terms of several image analysis techniques and used as subsidies for authorship attribution. It was found that the visual features account for performance similar to that achieved by using topological measurements. In addition, the combination of these two types of features improved the performance.



### Recent Advances in the Applications of Convolutional Neural Networks to Medical Image Contour Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.07281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07281v1)
- **Published**: 2017-08-24 05:28:42+00:00
- **Updated**: 2017-08-24 05:28:42+00:00
- **Authors**: Zizhao Zhang, Fuyong Xing, Hai Su, Xiaoshuang Shi, Lin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The fast growing deep learning technologies have become the main solution of many machine learning problems for medical image analysis. Deep convolution neural networks (CNNs), as one of the most important branch of the deep learning family, have been widely investigated for various computer-aided diagnosis tasks including long-term problems and continuously emerging new problems. Image contour detection is a fundamental but challenging task that has been studied for more than four decades. Recently, we have witnessed the significantly improved performance of contour detection thanks to the development of CNNs. Beyond purusing performance in existing natural image benchmarks, contour detection plays a particularly important role in medical image analysis. Segmenting various objects from radiology images or pathology images requires accurate detection of contours. However, some problems, such as discontinuity and shape constraints, are insufficiently studied in CNNs. It is necessary to clarify the challenges to encourage further exploration. The performance of CNN based contour detection relies on the state-of-the-art CNN architectures. Careful investigation of their design principles and motivations is critical and beneficial to contour detection. In this paper, we first review recent development of medical image contour detection and point out the current confronting challenges and problems. We discuss the development of general CNNs and their applications in image contours (or edges) detection. We compare those methods in detail, clarify their strengthens and weaknesses. Then we review their recent applications in medical image analysis and point out limitations, with the goal to light some potential directions in medical image analysis. We expect the paper to cover comprehensive technical ingredients of advanced CNNs to enrich the study in the medical image domain.



### Learning 6-DOF Grasping Interaction via Deep Geometry-aware 3D Representations
- **Arxiv ID**: http://arxiv.org/abs/1708.07303v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.07303v4)
- **Published**: 2017-08-24 08:09:04+00:00
- **Updated**: 2018-06-15 03:40:53+00:00
- **Authors**: Xinchen Yan, Jasmine Hsu, Mohi Khansari, Yunfei Bai, Arkanath Pathak, Abhinav Gupta, James Davidson, Honglak Lee
- **Comment**: Published at ICRA 2018
- **Journal**: None
- **Summary**: This paper focuses on the problem of learning 6-DOF grasping with a parallel jaw gripper in simulation. We propose the notion of a geometry-aware representation in grasping based on the assumption that knowledge of 3D geometry is at the heart of interaction. Our key idea is constraining and regularizing grasping interaction learning through 3D geometry prediction. Specifically, we formulate the learning of deep geometry-aware grasping model in two steps: First, we learn to build mental geometry-aware representation by reconstructing the scene (i.e., 3D occupancy grid) from RGBD input via generative 3D shape modeling. Second, we learn to predict grasping outcome with its internal geometry-aware representation. The learned outcome prediction model is used to sequentially propose grasping solutions via analysis-by-synthesis optimization. Our contributions are fourfold: (1) To best of our knowledge, we are presenting for the first time a method to learn a 6-DOF grasping net from RGBD input; (2) We build a grasping dataset from demonstrations in virtual reality with rich sensory and interaction annotations. This dataset includes 101 everyday objects spread across 7 categories, additionally, we propose a data augmentation strategy for effective learning; (3) We demonstrate that the learned geometry-aware representation leads to about 10 percent relative performance improvement over the baseline CNN on grasping objects from our dataset. (4) We further demonstrate that the model generalizes to novel viewpoints and object instances.



### Relaxed Spatio-Temporal Deep Feature Aggregation for Real-Fake Expression Prediction
- **Arxiv ID**: http://arxiv.org/abs/1708.07335v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07335v1)
- **Published**: 2017-08-24 09:40:29+00:00
- **Updated**: 2017-08-24 09:40:29+00:00
- **Authors**: Savas Ozkan, Gozde Bozdagi Akar
- **Comment**: Submitted to International Conference on Computer Vision Workshops
- **Journal**: None
- **Summary**: Frame-level visual features are generally aggregated in time with the techniques such as LSTM, Fisher Vectors, NetVLAD etc. to produce a robust video-level representation. We here introduce a learnable aggregation technique whose primary objective is to retain short-time temporal structure between frame-level features and their spatial interdependencies in the representation. Also, it can be easily adapted to the cases where there have very scarce training samples. We evaluate the method on a real-fake expression prediction dataset to demonstrate its superiority. Our method obtains 65% score on the test dataset in the official MAP evaluation and there is only one misclassified decision with the best reported result in the Chalearn Challenge (i.e. 66:7%) . Lastly, we believe that this method can be extended to different problems such as action/event recognition in future.



### Gradient-based Camera Exposure Control for Outdoor Mobile Platforms
- **Arxiv ID**: http://arxiv.org/abs/1708.07338v3
- **DOI**: 10.1109/TCSVT.2018.2846292
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1708.07338v3)
- **Published**: 2017-08-24 09:53:07+00:00
- **Updated**: 2018-06-13 13:07:14+00:00
- **Authors**: Inwook Shim, Tae-Hyun Oh, Joon-Young Lee, Jinwook Choi, Dong-Geol Choi, In So Kweon
- **Comment**: Extended version of IROS 2014
- **Journal**: None
- **Summary**: We introduce a novel method to automatically adjust camera exposure for image processing and computer vision applications on mobile robot platforms. Because most image processing algorithms rely heavily on low-level image features that are based mainly on local gradient information, we consider that gradient quantity can determine the proper exposure level, allowing a camera to capture the important image features in a manner robust to illumination conditions. We then extend this concept to a multi-camera system and present a new control algorithm to achieve both brightness consistency between adjacent cameras and a proper exposure level for each camera. We implement our prototype system with off-the-shelf machine-vision cameras and demonstrate the effectiveness of the proposed algorithms on practical applications, including pedestrian detection, visual odometry, surround-view imaging, panoramic imaging and stereo matching.



### Gait Recognition from Motion Capture Data
- **Arxiv ID**: http://arxiv.org/abs/1708.07755v2
- **DOI**: 10.1145/3152124
- **Categories**: **cs.CV**, 68T05, 68T10, I.5
- **Links**: [PDF](http://arxiv.org/pdf/1708.07755v2)
- **Published**: 2017-08-24 12:00:14+00:00
- **Updated**: 2022-12-07 22:17:01+00:00
- **Authors**: Michal Balazia, Petr Sojka
- **Comment**: Preprint. Full paper accepted at the ACM Transactions on Multimedia
  Computing, Communications, and Applications (TOMM), special issue on
  Representation, Analysis and Recognition of 3D Humans, February 2018. 18
  pages. arXiv admin note: substantial text overlap with arXiv:1701.00995,
  arXiv:1609.04392, arXiv:1609.06936
- **Journal**: None
- **Summary**: Gait recognition from motion capture data, as a pattern classification discipline, can be improved by the use of machine learning. This paper contributes to the state-of-the-art with a statistical approach for extracting robust gait features directly from raw data by a modification of Linear Discriminant Analysis with Maximum Margin Criterion. Experiments on the CMU MoCap database show that the suggested method outperforms thirteen relevant methods based on geometric features and a method to learn the features by a combination of Principal Component Analysis and Linear Discriminant Analysis. The methods are evaluated in terms of the distribution of biometric templates in respective feature spaces expressed in a number of class separability coefficients and classification metrics. Results also indicate a high portability of learned features, that means, we can learn what aspects of walk people generally differ in and extract those as general gait features. Recognizing people without needing group-specific features is convenient as particular people might not always provide annotated learning data. As a contribution to reproducible research, our evaluation framework and database have been made publicly available. This research makes motion capture technology directly applicable for human recognition.



### Interpretation of Mammogram and Chest X-Ray Reports Using Deep Neural Networks - Preliminary Results
- **Arxiv ID**: http://arxiv.org/abs/1708.09254v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09254v3)
- **Published**: 2017-08-24 12:21:22+00:00
- **Updated**: 2017-09-12 22:41:59+00:00
- **Authors**: Hojjat Salehinejad, Shahrokh Valaee, Aren Mnatzakanian, Tim Dowdell, Joseph Barfett, Errol Colak
- **Comment**: This paper is submitted for peer-review
- **Journal**: None
- **Summary**: Radiology reports are an important means of communication between radiologists and other physicians. These reports express a radiologist's interpretation of a medical imaging examination and are critical in establishing a diagnosis and formulating a treatment plan. In this paper, we propose a Bi-directional convolutional neural network (Bi-CNN) model for the interpretation and classification of mammograms based on breast density and chest radiographic radiology reports based on the basis of chest pathology. The proposed approach helps to organize databases of radiology reports, retrieve them expeditiously, and evaluate the radiology report that could be used in an auditing system to decrease incorrect diagnoses. Our study revealed that the proposed Bi-CNN outperforms the random forest and the support vector machine methods.



### Automatic Myocardial Segmentation by Using A Deep Learning Network in Cardiac MRI
- **Arxiv ID**: http://arxiv.org/abs/1708.07452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07452v1)
- **Published**: 2017-08-24 15:19:04+00:00
- **Updated**: 2017-08-24 15:19:04+00:00
- **Authors**: Ariel H. Curiale, Flavio D. Colavecchia, Pablo Kaluza, Roberto A. Isoardi, German Mato
- **Comment**: Accepted on CLEI-JAIIO-SLCGRVPI 2017 (submitted on April 2017)
- **Journal**: None
- **Summary**: Cardiac function is of paramount importance for both prognosis and treatment of different pathologies such as mitral regurgitation, ischemia, dyssynchrony and myocarditis. Cardiac behavior is determined by structural and functional features. In both cases, the analysis of medical imaging studies requires to detect and segment the myocardium. Nowadays, magnetic resonance imaging (MRI) is one of the most relevant and accurate non-invasive diagnostic tools for cardiac structure and function.   In this work we propose to use a deep learning technique to assist the automatization of myocardial segmentation in cardiac MRI. We present several improvements to previous works in this paper: we propose to use the Jaccard distance as optimization objective function, we integrate a residual learning strategy into the code, and we introduce a batch normalization layer to train the fully convolutional neural network. Our results demonstrate that this architecture outperforms previous approaches based on a similar network architecture, and that provides a suitable approach for myocardial segmentation. Our benchmark shows that the automatic myocardial segmentation takes less than 22 seg. for a volume of 128~x~128~x~13 pixels in a 3.1 GHz intel core i7.



### Review on Computer Vision Techniques in Emergency Situation
- **Arxiv ID**: http://arxiv.org/abs/1708.07455v2
- **DOI**: 10.1007/s11042-017-5276-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07455v2)
- **Published**: 2017-08-24 15:24:47+00:00
- **Updated**: 2018-03-09 12:59:22+00:00
- **Authors**: Laura Lopez-Fuentes, Joost van de Weijer, Manuel Gonzalez-Hidalgo, Harald Skinnemoen, Andrew D. Bagdanov
- **Comment**: 25 pages
- **Journal**: Multimedia Tools and Applications, 2017, p. 1-39
- **Summary**: In emergency situations, actions that save lives and limit the impact of hazards are crucial. In order to act, situational awareness is needed to decide what to do. Geolocalized photos and video of the situations as they evolve can be crucial in better understanding them and making decisions faster. Cameras are almost everywhere these days, either in terms of smartphones, installed CCTV cameras, UAVs or others. However, this poses challenges in big data and information overflow. Moreover, most of the time there are no disasters at any given location, so humans aiming to detect sudden situations may not be as alert as needed at any point in time. Consequently, computer vision tools can be an excellent decision support. The number of emergencies where computer vision tools has been considered or used is very wide, and there is a great overlap across related emergency research. Researchers tend to focus on state-of-the-art systems that cover the same emergency as they are studying, obviating important research in other fields. In order to unveil this overlap, the survey is divided along four main axes: the types of emergencies that have been studied in computer vision, the objective that the algorithms can address, the type of hardware needed and the algorithms used. Therefore, this review provides a broad overview of the progress of computer vision covering all sorts of emergencies.



### Analyzing Cloud Optical Properties Using Sky Cameras
- **Arxiv ID**: http://arxiv.org/abs/1708.08995v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/1708.08995v1)
- **Published**: 2017-08-24 16:50:54+00:00
- **Updated**: 2017-08-24 16:50:54+00:00
- **Authors**: Shilpa Manandhar, Soumyabrata Dev, Yee Hui Lee, Yu Song Meng
- **Comment**: Published in Proc. Progress In Electromagnetics Research Symposium
  (PIERS), 2017
- **Journal**: None
- **Summary**: Clouds play a significant role in the fluctuation of solar radiation received by the earth's surface. It is important to study the various cloud properties, as it impacts the total solar irradiance falling on the earth's surface. One of such important optical properties of the cloud is the Cloud Optical Thickness (COT). It is defined with the amount of light that can pass through the clouds. The COT values are generally obtained from satellite images. However, satellite images have a low temporal- and spatial- resolutions; and are not suitable for study in applications as solar energy generation and forecasting. Therefore, ground-based sky cameras are now getting popular in such fields. In this paper, we analyze the cloud optical thickness value, from the ground-based sky cameras, and provide future research directions.



### Study of Clear Sky Models for Singapore
- **Arxiv ID**: http://arxiv.org/abs/1708.08760v1
- **DOI**: None
- **Categories**: **physics.ao-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.08760v1)
- **Published**: 2017-08-24 17:40:38+00:00
- **Updated**: 2017-08-24 17:40:38+00:00
- **Authors**: Soumyabrata Dev, Shilpa Manandhar, Yee Hui Lee, Stefan Winkler
- **Comment**: Published in Proc. Progress In Electromagnetics Research Symposium
  (PIERS), 2017
- **Journal**: None
- **Summary**: The estimation of total solar irradiance falling on the earth's surface is important in the field of solar energy generation and forecasting. Several clear-sky solar radiation models have been developed over the last few decades. Most of these models are based on empirical distribution of various geographical parameters; while a few models consider various atmospheric effects in the solar energy estimation. In this paper, we perform a comparative analysis of several popular clear-sky models, in the tropical region of Singapore. This is important in countries like Singapore, where we are primarily focused on reliable and efficient solar energy generation. We analyze and compare three popular clear-sky models that are widely used in the literature. We validate our solar estimation results using actual solar irradiance measurements obtained from collocated weather stations. We finally conclude the most reliable clear sky model for Singapore, based on all clear sky days in a year.



### Correlating Satellite Cloud Cover with Sky Cameras
- **Arxiv ID**: http://arxiv.org/abs/1709.05283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05283v1)
- **Published**: 2017-08-24 17:54:57+00:00
- **Updated**: 2017-08-24 17:54:57+00:00
- **Authors**: Shilpa Manandhar, Soumyabrata Dev, Yee Hui Lee, Yu Song Meng
- **Comment**: Published in Proc. Progress In Electromagnetics Research Symposium
  (PIERS), 2017
- **Journal**: None
- **Summary**: The role of clouds is manifold in understanding the various events in the atmosphere, and also in studying the radiative balance of the earth. The conventional manner of such cloud analysis is performed mainly via satellite images. However, because of its low temporal- and spatial- resolutions, ground-based sky cameras are now getting popular. In this paper, we study the relation between the cloud cover obtained from MODIS images, with the coverage obtained from ground-based sky cameras. This will help us to better understand cloud formation in the atmosphere - both from satellite images and ground-based observations.



### FacePoseNet: Making a Case for Landmark-Free Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1708.07517v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07517v2)
- **Published**: 2017-08-24 18:06:54+00:00
- **Updated**: 2017-08-31 22:17:44+00:00
- **Authors**: Fengju Chang, Anh Tuan Tran, Tal Hassner, Iacopo Masi, Ram Nevatia, Gerard Medioni
- **Comment**: None
- **Journal**: None
- **Summary**: We show how a simple convolutional neural network (CNN) can be trained to accurately and robustly regress 6 degrees of freedom (6DoF) 3D head pose, directly from image intensities. We further explain how this FacePoseNet (FPN) can be used to align faces in 2D and 3D as an alternative to explicit facial landmark detection for these tasks. We claim that in many cases the standard means of measuring landmark detector accuracy can be misleading when comparing different face alignments. Instead, we compare our FPN with existing methods by evaluating how they affect face recognition accuracy on the IJB-A and IJB-B benchmarks: using the same recognition pipeline, but varying the face alignment method. Our results show that (a) better landmark detection accuracy measured on the 300W benchmark does not necessarily imply better face recognition accuracy. (b) Our FPN provides superior 2D and 3D face alignment on both benchmarks. Finally, (c), FPN aligns faces at a small fraction of the computational cost of comparably accurate landmark detectors. For many purposes, FPN is thus a far faster and far more accurate face alignment method than using facial landmark detectors.



### SPARCNN: SPAtially Related Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.07522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07522v1)
- **Published**: 2017-08-24 18:31:03+00:00
- **Updated**: 2017-08-24 18:31:03+00:00
- **Authors**: JT Turner, Kalyan Moy Gupta, David Aha
- **Comment**: 6 pages, AIPR 2016 submission
- **Journal**: None
- **Summary**: The ability to accurately detect and classify objects at varying pixel sizes in cluttered scenes is crucial to many Navy applications. However, detection performance of existing state-of the-art approaches such as convolutional neural networks (CNNs) degrade and suffer when applied to such cluttered and multi-object detection tasks. We conjecture that spatial relationships between objects in an image could be exploited to significantly improve detection accuracy, an approach that had not yet been considered by any existing techniques (to the best of our knowledge) at the time the research was conducted. We introduce a detection and classification technique called Spatially Related Detection with Convolutional Neural Networks (SPARCNN) that learns and exploits a probabilistic representation of inter-object spatial configurations within images from training sets for more effective region proposals to use with state-of-the-art CNNs. Our empirical evaluation of SPARCNN on the VOC 2007 dataset shows that it increases classification accuracy by 8% when compared to a region proposal technique that does not exploit spatial relations. More importantly, we obtained a higher performance boost of 18.8% when task difficulty in the test set is increased by including highly obscured objects and increased image clutter.



### Objective Classes for Micro-Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.07549v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07549v2)
- **Published**: 2017-08-24 20:37:10+00:00
- **Updated**: 2017-12-03 06:12:57+00:00
- **Authors**: Adrian K. Davison, Walied Merghani, Moi Hoon Yap
- **Comment**: 11 pages, 4 figures and 5 tables. This paper will be submitted for
  journal review
- **Journal**: None
- **Summary**: Micro-expressions are brief spontaneous facial expressions that appear on a face when a person conceals an emotion, making them different to normal facial expressions in subtlety and duration. Currently, emotion classes within the CASME II dataset are based on Action Units and self-reports, creating conflicts during machine learning training. We will show that classifying expressions using Action Units, instead of predicted emotion, removes the potential bias of human reporting. The proposed classes are tested using LBP-TOP, HOOF and HOG 3D feature descriptors. The experiments are evaluated on two benchmark FACS coded datasets: CASME II and SAMM. The best result achieves 86.35\% accuracy when classifying the proposed 5 classes on CASME II using HOG 3D, outperforming the result of the state-of-the-art 5-class emotional-based classification in CASME II. Results indicate that classification based on Action Units provides an objective method to improve micro-expression recognition.



### A Robust Indoor Scene Recognition Method based on Sparse Representation
- **Arxiv ID**: http://arxiv.org/abs/1708.07555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07555v1)
- **Published**: 2017-08-24 21:01:08+00:00
- **Updated**: 2017-08-24 21:01:08+00:00
- **Authors**: Guilherme Nascimento, Camila Laranjeira, Vinicius Braz, Anisio Lacerda, Erickson R. Nascimento
- **Comment**: CIARP 2017. To appear
- **Journal**: None
- **Summary**: In this paper, we present a robust method for scene recognition, which leverages Convolutional Neural Networks (CNNs) features and Sparse Coding setting by creating a new representation of indoor scenes. Although CNNs highly benefited the fields of computer vision and pattern recognition, convolutional layers adjust weights on a global-approach, which might lead to losing important local details such as objects and small structures. Our proposed scene representation relies on both: global features that mostly refers to environment's structure, and local features that are sparsely combined to capture characteristics of common objects of a given scene. This new representation is based on fragments of the scene and leverages features extracted by CNNs. The experimental evaluation shows that the resulting representation outperforms previous scene recognition methods on Scene15 and MIT67 datasets, and performs competitively on SUN397, while being highly robust to perturbations in the input image such as noise and occlusion.



### Leaf Counting with Deep Convolutional and Deconvolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.07570v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07570v2)
- **Published**: 2017-08-24 22:32:23+00:00
- **Updated**: 2017-08-29 00:59:53+00:00
- **Authors**: Shubhra Aich, Ian Stavness
- **Comment**: Workshop: ICCV 2017 Workshop on Computer Vision Problems in Plant
  Phenotyping (Code repository: https://github.com/p2irc/leaf_count_ICCVW-2017)
- **Journal**: None
- **Summary**: In this paper, we investigate the problem of counting rosette leaves from an RGB image, an important task in plant phenotyping. We propose a data-driven approach for this task generalized over different plant species and imaging setups. To accomplish this task, we use state-of-the-art deep learning architectures: a deconvolutional network for initial segmentation and a convolutional network for leaf counting. Evaluation is performed on the leaf counting challenge dataset at CVPPP-2017. Despite the small number of training samples in this dataset, as compared to typical deep learning image sets, we obtain satisfactory performance on segmenting leaves from the background as a whole and counting the number of leaves using simple data augmentation strategies. Comparative analysis is provided against methods evaluated on the previous competition datasets. Our framework achieves mean and standard deviation of absolute count difference of 1.62 and 2.30 averaged over all five test datasets.



### Learning a 3D descriptor for cross-source point cloud registration from synthetic data
- **Arxiv ID**: http://arxiv.org/abs/1708.08997v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08997v1)
- **Published**: 2017-08-24 22:38:02+00:00
- **Updated**: 2017-08-24 22:38:02+00:00
- **Authors**: Xiaoshui Huang
- **Comment**: None
- **Journal**: None
- **Summary**: As the development of 3D sensors, registration of 3D data (e.g. point cloud) coming from different kind of sensor is dispensable and shows great demanding. However, point cloud registration between different sensors is challenging because of the variant of density, missing data, different viewpoint, noise and outliers, and geometric transformation. In this paper, we propose a method to learn a 3D descriptor for finding the correspondent relations between these challenging point clouds. To train the deep learning framework, we use synthetic 3D point cloud as input. Starting from synthetic dataset, we use region-based sampling method to select reasonable, large and diverse training samples from synthetic samples. Then, we use data augmentation to extend our network be robust to rotation transformation. We focus our work on more general cases that point clouds coming from different sensors, named cross-source point cloud. The experiments show that our descriptor is not only able to generalize to new scenes, but also generalize to different sensors. The results demonstrate that the proposed method successfully aligns two 3D cross-source point clouds which outperforms state-of-the-art method.



