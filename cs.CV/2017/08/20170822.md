# Arxiv Papers in cs.CV on 2017-08-22
### Sharpness-aware Low dose CT denoising using conditional generative adversarial network
- **Arxiv ID**: http://arxiv.org/abs/1708.06453v2
- **DOI**: 10.1007/s10278-018-0056-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06453v2)
- **Published**: 2017-08-22 00:16:51+00:00
- **Updated**: 2017-10-19 18:55:14+00:00
- **Authors**: Xin Yi, Paul Babyn
- **Comment**: 1. updated results, related works and discussion 2. fixed an error
  for the noise level calculation 3. redrawn two diagrams 4. added an
  experiment on unknown dose level CT scans
- **Journal**: None
- **Summary**: Low Dose Computed Tomography (LDCT) has offered tremendous benefits in radiation restricted applications, but the quantum noise as resulted by the insufficient number of photons could potentially harm the diagnostic performance. Current image-based denoising methods tend to produce a blur effect on the final reconstructed results especially in high noise levels. In this paper, a deep learning based approach was proposed to mitigate this problem. An adversarially trained network and a sharpness detection network were trained to guide the training process. Experiments on both simulated and real dataset shows that the results of the proposed method have very small resolution loss and achieves better performance relative to the-state-of-art methods both quantitatively and visually.



### Towards Automatic Construction of Diverse, High-quality Image Dataset
- **Arxiv ID**: http://arxiv.org/abs/1708.06495v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1708.06495v2)
- **Published**: 2017-08-22 04:36:12+00:00
- **Updated**: 2019-03-01 07:08:08+00:00
- **Authors**: Yazhou Yao, Jian Zhang, Fumin Shen, Li Liu, Fan Zhu, Dongxiang Zhang, Heng-Tao Shen
- **Comment**: Accepted by IEEE Transactions on Knowledge and Data Engineering
- **Journal**: None
- **Summary**: The availability of labeled image datasets has been shown critical for high-level image understanding, which continuously drives the progress of feature designing and models developing. However, constructing labeled image datasets is laborious and monotonous. To eliminate manual annotation, in this work, we propose a novel image dataset construction framework by employing multiple textual queries. We aim at collecting diverse and accurate images for given queries from the Web. Specifically, we formulate noisy textual queries removing and noisy images filtering as a multi-view and multi-instance learning problem separately. Our proposed approach not only improves the accuracy but also enhances the diversity of the selected images. To verify the effectiveness of our proposed approach, we construct an image dataset with 100 categories. The experiments show significant performance gains by using the generated data of our approach on several tasks, such as image classification, cross-dataset generalization, and object detection. The proposed method also consistently outperforms existing weakly supervised and web-supervised approaches.



### Sparsity Invariant CNNs
- **Arxiv ID**: http://arxiv.org/abs/1708.06500v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06500v2)
- **Published**: 2017-08-22 05:29:20+00:00
- **Updated**: 2017-08-30 18:12:47+00:00
- **Authors**: Jonas Uhrig, Nick Schneider, Lukas Schneider, Uwe Franke, Thomas Brox, Andreas Geiger
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider convolutional neural networks operating on sparse inputs with an application to depth upsampling from sparse laser scan data. First, we show that traditional convolutional networks perform poorly when applied to sparse data even when the location of missing data is provided to the network. To overcome this problem, we propose a simple yet effective sparse convolution layer which explicitly considers the location of missing data during the convolution operation. We demonstrate the benefits of the proposed network architecture in synthetic and real experiments with respect to various baseline approaches. Compared to dense baselines, the proposed sparse convolution network generalizes well to novel datasets and is invariant to the level of sparsity in the data. For our evaluation, we derive a novel dataset from the KITTI benchmark, comprising 93k depth annotated RGB images. Our dataset allows for training and evaluating depth upsampling and depth prediction techniques in challenging real-world settings and will be made available upon publication.



### ProbFlow: Joint Optical Flow and Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/1708.06509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06509v1)
- **Published**: 2017-08-22 06:40:32+00:00
- **Updated**: 2017-08-22 06:40:32+00:00
- **Authors**: Anne S. Wannenwetsch, Margret Keuper, Stefan Roth
- **Comment**: To appear at ICCV 2017
- **Journal**: None
- **Summary**: Optical flow estimation remains challenging due to untextured areas, motion boundaries, occlusions, and more. Thus, the estimated flow is not equally reliable across the image. To that end, post-hoc confidence measures have been introduced to assess the per-pixel reliability of the flow. We overcome the artificial separation of optical flow and confidence estimation by introducing a method that jointly predicts optical flow and its underlying uncertainty. Starting from common energy-based formulations, we rely on the corresponding posterior distribution of the flow given the images. We derive a variational inference scheme based on mean field, which incorporates best practices from energy minimization. An uncertainty measure is obtained along the flow at every pixel as the (marginal) entropy of the variational distribution. We demonstrate the flexibility of our probabilistic approach by applying it to two different energies and on two benchmarks. We not only obtain flow results that are competitive with the underlying energy minimization approach, but also a reliable uncertainty measure that significantly outperforms existing post-hoc approaches.



### Learning Efficient Convolutional Networks through Network Slimming
- **Arxiv ID**: http://arxiv.org/abs/1708.06519v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.06519v1)
- **Published**: 2017-08-22 07:35:26+00:00
- **Updated**: 2017-08-22 07:35:26+00:00
- **Authors**: Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang
- **Comment**: Accepted by ICCV 2017
- **Journal**: None
- **Summary**: The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations.



### Deep Residual Bidir-LSTM for Human Activity Recognition Using Wearable Sensors
- **Arxiv ID**: http://arxiv.org/abs/1708.08989v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.08989v2)
- **Published**: 2017-08-22 11:02:13+00:00
- **Updated**: 2017-09-07 07:36:31+00:00
- **Authors**: Yu Zhao, Rennong Yang, Guillaume Chevalier, Maoguo Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Human activity recognition (HAR) has become a popular topic in research because of its wide application. With the development of deep learning, new ideas have appeared to address HAR problems. Here, a deep network architecture using residual bidirectional long short-term memory (LSTM) cells is proposed. The advantages of the new network include that a bidirectional connection can concatenate the positive time direction (forward state) and the negative time direction (backward state). Second, residual connections between stacked cells act as highways for gradients, which can pass underlying information directly to the upper layer, effectively avoiding the gradient vanishing problem. Generally, the proposed network shows improvements on both the temporal (using bidirectional cells) and the spatial (residual connections stacked deeply) dimensions, aiming to enhance the recognition rate. When tested with the Opportunity data set and the public domain UCI data set, the accuracy was increased by 4.78% and 3.68%, respectively, compared with previously reported results. Finally, the confusion matrix of the public domain UCI data set was analyzed.



### Color and Gradient Features for Text Segmentation from Video Frames
- **Arxiv ID**: http://arxiv.org/abs/1708.06561v1
- **DOI**: 10.1007/978-81-322-1143-3_22
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06561v1)
- **Published**: 2017-08-22 11:10:57+00:00
- **Updated**: 2017-08-22 11:10:57+00:00
- **Authors**: P. Shivakumara, D. S. Guru, H. T. Basavaraju
- **Comment**: None
- **Journal**: Multimedia Processing, Communication and Computing Applications
  (pp. 267-278). Springer, New Delhi (2013)
- **Summary**: Text segmentation in a video is drawing attention of researchers in the field of image processing, pattern recognition and document image analysis because it helps in annotating and labeling video events accurately. We propose a novel idea of generating an enhanced frame from the R, G, and B channels of an input frame by grouping high and low values using Min-Max clustering criteria. We also perform sliding window on enhanced frame to group high and low values from the neighboring pixel values to further enhance the frame. Subsequently, we use k-means with k=2 clustering algorithm to separate text and non-text regions. The fully connected components will be identified in the skeleton of the frame obtained by k-means clustering. Concept of connected component analysis based on gradient feature has been adapted for the purpose of symmetry verification. The components which satisfy symmetric verification are selected to be the representatives of text regions and they are permitted to grow to cover their respective region fully containing text. The method is tested on variety of video frames to evaluate the performance of the method in terms of recall, precision and f-measure. The results show that method is promising and encouraging.



### Automatic detection and decoding of honey bee waggle dances
- **Arxiv ID**: http://arxiv.org/abs/1708.06590v3
- **DOI**: 10.1371/journal.pone.0188626
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1708.06590v3)
- **Published**: 2017-08-22 13:02:08+00:00
- **Updated**: 2017-12-05 02:06:23+00:00
- **Authors**: Fernando Wario, Benjamin Wild, Raúl Rojas, Tim Landgraf
- **Comment**: 16 pages, LaTeX; a new value for the ratio distance-waggle run
  duration was computed. Figure 2 has been updated and discussion section was
  improved
- **Journal**: None
- **Summary**: The waggle dance is one of the most popular examples of animal communication. Forager bees direct their nestmates to profitable resources via a complex motor display. Essentially, the dance encodes the polar coordinates to the resource in the field. Unemployed foragers follow the dancer's movements and then search for the advertised spots in the field. Throughout the last decades, biologists have employed different techniques to measure key characteristics of the waggle dance and decode the information it conveys. Early techniques involved the use of protractors and stopwatches to measure the dance orientation and duration directly from the observation hive. Recent approaches employ digital video recordings and manual measurements on screen. However, manual approaches are very time-consuming. Most studies, therefore, regard only small numbers of animals in short periods of time. We have developed a system capable of automatically detecting, decoding and mapping communication dances in real-time. In this paper, we describe our recording setup, the image processing steps performed for dance detection and decoding and an algorithm to map dances to the field. The proposed system performs with a detection accuracy of 90.07\%. The decoded waggle orientation has an average error of -2.92{\deg} ($\pm$ 7.37{\deg} ), well within the range of human error. To evaluate and exemplify the system's performance, a group of bees was trained to an artificial feeder, and all dances in the colony were automatically detected, decoded and mapped. The system presented here is the first of this kind made publicly available, including source code and hardware specifications. We hope this will foster quantitative analyses of the honey bee waggle dance.



### Contrast and visual saliency similarity-induced index for assessing image quality
- **Arxiv ID**: http://arxiv.org/abs/1708.06616v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06616v3)
- **Published**: 2017-08-22 14:03:19+00:00
- **Updated**: 2018-10-16 09:54:59+00:00
- **Authors**: Huizhen Jia, Lu Zhang, Tonghan Wang
- **Comment**: The paper was accepted for publication in IEEE ACCESS. The code has
  been published online at
  https://ww2.mathworks.cn/matlabcentral/fileexchange/69123-vspluscontrast
- **Journal**: None
- **Summary**: Image quality that is consistent with human opinion is assessed by a perceptual image quality assessment (IQA) that defines/utilizes a computational model. A good model should take effectiveness and efficiency into consideration, but most of the previously proposed IQA models do not simultaneously consider these factors. Therefore, this study attempts to develop an effective and efficient IQA metric. Contrast is an inherent visual attribute that indicates image quality, and visual saliency (VS) is a quality that attracts the attention of human beings. The proposed model utilized these two features to characterize the image local quality. After obtaining the local contrast quality map and the global VS quality map, we added the weighted standard deviation of the previous two quality maps together to yield the final quality score. The experimental results for three benchmark databases (LIVE, TID2008, and CSIQ) demonstrated that our model performs the best in terms of a correlation with the human judgment of visual quality. Furthermore, compared with competing IQA models, this proposed model is more efficient.



### Activity Recognition based on a Magnitude-Orientation Stream Network
- **Arxiv ID**: http://arxiv.org/abs/1708.06637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06637v1)
- **Published**: 2017-08-22 14:27:26+00:00
- **Updated**: 2017-08-22 14:27:26+00:00
- **Authors**: Carlos Caetano, Victor H. C. de Melo, Jefersson A. dos Santos, William Robson Schwartz
- **Comment**: 8 pages, SIBGRAPI 2017
- **Journal**: None
- **Summary**: The temporal component of videos provides an important clue for activity recognition, as a number of activities can be reliably recognized based on the motion information. In view of that, this work proposes a novel temporal stream for two-stream convolutional networks based on images computed from the optical flow magnitude and orientation, named Magnitude-Orientation Stream (MOS), to learn the motion in a better and richer manner. Our method applies simple nonlinear transformations on the vertical and horizontal components of the optical flow to generate input images for the temporal stream. Experimental results, carried on two well-known datasets (HMDB51 and UCF101), demonstrate that using our proposed temporal stream as input to existing neural network architectures can improve their performance for activity recognition. Results demonstrate that our temporal stream provides complementary information able to improve the classical two-stream methods, indicating the suitability of our approach to be used as a temporal video representation.



### Causally Regularized Learning with Agnostic Data Selection Bias
- **Arxiv ID**: http://arxiv.org/abs/1708.06656v2
- **DOI**: 10.1145/3240508.3240577
- **Categories**: **cs.CV**, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.06656v2)
- **Published**: 2017-08-22 14:49:07+00:00
- **Updated**: 2018-08-19 16:33:36+00:00
- **Authors**: Zheyan Shen, Peng Cui, Kun Kuang, Bo Li, Peixuan Chen
- **Comment**: Oral paper of 2018 ACM Multimedia Conference (MM'18)
- **Journal**: None
- **Summary**: Most of previous machine learning algorithms are proposed based on the i.i.d. hypothesis. However, this ideal assumption is often violated in real applications, where selection bias may arise between training and testing process. Moreover, in many scenarios, the testing data is not even available during the training process, which makes the traditional methods like transfer learning infeasible due to their need on prior of test distribution. Therefore, how to address the agnostic selection bias for robust model learning is of paramount importance for both academic research and real applications. In this paper, under the assumption that causal relationships among variables are robust across domains, we incorporate causal technique into predictive modeling and propose a novel Causally Regularized Logistic Regression (CRLR) algorithm by jointly optimize global confounder balancing and weighted logistic regression. Global confounder balancing helps to identify causal features, whose causal effect on outcome are stable across domains, then performing logistic regression on those causal features constructs a robust predictive model against the agnostic bias. To validate the effectiveness of our CRLR algorithm, we conduct comprehensive experiments on both synthetic and real world datasets. Experimental results clearly demonstrate that our CRLR algorithm outperforms the state-of-the-art methods, and the interpretability of our method can be fully depicted by the feature visualization.



### CNN Fixations: An unraveling approach to visualize the discriminative image regions
- **Arxiv ID**: http://arxiv.org/abs/1708.06670v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06670v3)
- **Published**: 2017-08-22 15:23:31+00:00
- **Updated**: 2018-12-12 18:40:26+00:00
- **Authors**: Konda Reddy Mopuri, Utsav Garg, R. Venkatesh Babu
- **Comment**: Accepted in Trans. on Image Processing (TIP) 2018 and Codes are
  available at https://github.com/utsavgarg/cnn-fixations
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNN) have revolutionized various fields of vision research and have seen unprecedented adoption for multiple tasks such as classification, detection, captioning, etc. However, they offer little transparency into their inner workings and are often treated as black boxes that deliver excellent performance. In this work, we aim at alleviating this opaqueness of CNNs by providing visual explanations for the network's predictions. Our approach can analyze variety of CNN based models trained for vision applications such as object recognition and caption generation. Unlike existing methods, we achieve this via unraveling the forward pass operation. Proposed method exploits feature dependencies across the layer hierarchy and uncovers the discriminative image locations that guide the network's predictions. We name these locations CNN-Fixations, loosely analogous to human eye fixations.   Our approach is a generic method that requires no architectural changes, additional training or gradient computation and computes the important image locations (CNN Fixations). We demonstrate through a variety of applications that our approach is able to localize the discriminative image locations across different network architectures, diverse vision tasks and data modalities.



### Tags2Parts: Discovering Semantic Regions from Shape Tags
- **Arxiv ID**: http://arxiv.org/abs/1708.06673v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1708.06673v3)
- **Published**: 2017-08-22 15:26:00+00:00
- **Updated**: 2018-04-30 20:29:41+00:00
- **Authors**: Sanjeev Muralikrishnan, Vladimir G. Kim, Siddhartha Chaudhuri
- **Comment**: To appear at CVPR 2018
- **Journal**: None
- **Summary**: We propose a novel method for discovering shape regions that strongly correlate with user-prescribed tags. For example, given a collection of chairs tagged as either "has armrest" or "lacks armrest", our system correctly highlights the armrest regions as the main distinctive parts between the two chair types. To obtain point-wise predictions from shape-wise tags we develop a novel neural network architecture that is trained with tag classification loss, but is designed to rely on segmentation to predict the tag. Our network is inspired by U-Net, but we replicate shallow U structures several times with new skip connections and pooling layers, and call the resulting architecture "WU-Net". We test our method on segmentation benchmarks and show that even with weak supervision of whole shape tags, our method can infer meaningful semantic regions, without ever observing shape segmentations. Further, once trained, the model can process shapes for which the tag is entirely unknown. As a bonus, our architecture is directly operational under full supervision and performs strongly on standard benchmarks. We validate our method through experiments with many variant architectures and prior baselines, and demonstrate several applications.



### A Spatiotemporal Oriented Energy Network for Dynamic Texture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.06690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06690v1)
- **Published**: 2017-08-22 16:06:19+00:00
- **Updated**: 2017-08-22 16:06:19+00:00
- **Authors**: Isma Hadji, Richard P. Wildes
- **Comment**: accepted at ICCV 2017
- **Journal**: None
- **Summary**: This paper presents a novel hierarchical spatiotemporal orientation representation for spacetime image analysis. It is designed to combine the benefits of the multilayer architecture of ConvNets and a more controlled approach to spacetime analysis. A distinguishing aspect of the approach is that unlike most contemporary convolutional networks no learning is involved; rather, all design decisions are specified analytically with theoretical motivations. This approach makes it possible to understand what information is being extracted at each stage and layer of processing as well as to minimize heuristic choices in design. Another key aspect of the network is its recurrent nature, whereby the output of each layer of processing feeds back to the input. To keep the network size manageable across layers, a novel cross-channel feature pooling is proposed. The multilayer architecture that results systematically reveals hierarchical image structure in terms of multiscale, multiorientation properties of visual spacetime. To illustrate its utility, the network has been applied to the task of dynamic texture recognition. Empirical evaluation on multiple standard datasets shows that it sets a new state-of-the-art.



### What does 2D geometric information really tell us about 3D face shape?
- **Arxiv ID**: http://arxiv.org/abs/1708.06703v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06703v3)
- **Published**: 2017-08-22 16:21:13+00:00
- **Updated**: 2019-07-30 19:08:29+00:00
- **Authors**: Anil Bas, William A. P. Smith
- **Comment**: None
- **Journal**: None
- **Summary**: A face image contains geometric cues in the form of configurational information and contours that can be used to estimate 3D face shape. While it is clear that 3D reconstruction from 2D points is highly ambiguous if no further constraints are enforced, one might expect that the face-space constraint solves this problem. We show that this is not the case and that geometric information is an ambiguous cue. There are two sources for this ambiguity. The first is that, within the space of 3D face shapes, there are flexibility modes that remain when some parts of the face are fixed. The second occurs only under perspective projection and is a result of perspective transformation as camera distance varies. Two different faces, when viewed at different distances, can give rise to the same 2D geometry. To demonstrate these ambiguities, we develop new algorithms for fitting a 3D morphable model to 2D landmarks or contours under either orthographic or perspective projection and show how to compute flexibility modes for both cases. We show that both fitting problems can be posed as a separable nonlinear least squares problem and solved efficiently. We demonstrate both quantitatively and qualitatively that the ambiguity is present in reconstructions from geometric information alone but also in reconstructions from a state-of-the-art CNN-based method.



### WordSup: Exploiting Word Annotations for Character based Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.06720v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06720v1)
- **Published**: 2017-08-22 16:55:24+00:00
- **Updated**: 2017-08-22 16:55:24+00:00
- **Authors**: Han Hu, Chengquan Zhang, Yuxuan Luo, Yuzhuo Wang, Junyu Han, Errui Ding
- **Comment**: 2017 International Conference on Computer Vision
- **Journal**: None
- **Summary**: Imagery texts are usually organized as a hierarchy of several visual elements, i.e. characters, words, text lines and text blocks. Among these elements, character is the most basic one for various languages such as Western, Chinese, Japanese, mathematical expression and etc. It is natural and convenient to construct a common text detection engine based on character detectors. However, training character detectors requires a vast of location annotated characters, which are expensive to obtain. Actually, the existing real text datasets are mostly annotated in word or line level. To remedy this dilemma, we propose a weakly supervised framework that can utilize word annotations, either in tight quadrangles or the more loose bounding boxes, for character detector training. When applied in scene text detection, we are thus able to train a robust character detector by exploiting word annotations in the rich large-scale real scene text datasets, e.g. ICDAR15 and COCO-text. The character detector acts as a key role in the pipeline of our text detection engine. It achieves the state-of-the-art performance on several challenging scene text detection benchmarks. We also demonstrate the flexibility of our pipeline by various scenarios, including deformed text detection and math expression recognition.



### VIGAN: Missing View Imputation with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.06724v5
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.06724v5)
- **Published**: 2017-08-22 17:05:38+00:00
- **Updated**: 2017-11-01 15:43:36+00:00
- **Authors**: Chao Shang, Aaron Palmer, Jiangwen Sun, Ko-Shin Chen, Jin Lu, Jinbo Bi
- **Comment**: 10 pages, 8 figures, conference
- **Journal**: None
- **Summary**: In an era when big data are becoming the norm, there is less concern with the quantity but more with the quality and completeness of the data. In many disciplines, data are collected from heterogeneous sources, resulting in multi-view or multi-modal datasets. The missing data problem has been challenging to address in multi-view data analysis. Especially, when certain samples miss an entire view of data, it creates the missing view problem. Classic multiple imputations or matrix completion methods are hardly effective here when no information can be based on in the specific view to impute data for such samples. The commonly-used simple method of removing samples with a missing view can dramatically reduce sample size, thus diminishing the statistical power of a subsequent analysis. In this paper, we propose a novel approach for view imputation via generative adversarial networks (GANs), which we name by VIGAN. This approach first treats each view as a separate domain and identifies domain-to-domain mappings via a GAN using randomly-sampled data from each view, and then employs a multi-modal denoising autoencoder (DAE) to reconstruct the missing view from the GAN outputs based on paired data across the views. Then, by optimizing the GAN and DAE jointly, our model enables the knowledge integration for domain mappings and view correspondences to effectively recover the missing view. Empirical results on benchmark datasets validate the VIGAN approach by comparing against the state of the art. The evaluation of VIGAN in a genetic study of substance use disorders further proves the effectiveness and usability of this approach in life science.



### Representation Learning by Learning to Count
- **Arxiv ID**: http://arxiv.org/abs/1708.06734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06734v1)
- **Published**: 2017-08-22 17:33:47+00:00
- **Updated**: 2017-08-22 17:33:47+00:00
- **Authors**: Mehdi Noroozi, Hamed Pirsiavash, Paolo Favaro
- **Comment**: ICCV 2017(oral)
- **Journal**: None
- **Summary**: We introduce a novel method for representation learning that uses an artificial supervision signal based on counting visual primitives. This supervision signal is obtained from an equivariance relation, which does not require any manual annotation. We relate transformations of images to transformations of the representations. More specifically, we look for the representation that satisfies such relation rather than the transformations that match a given representation. In this paper, we use two image transformations in the context of counting: scaling and tiling. The first transformation exploits the fact that the number of visual primitives should be invariant to scale. The second transformation allows us to equate the total number of visual primitives in each tile to that in the whole image. These two transformations are combined in one constraint and used to train a neural network with a contrastive loss. The proposed task produces representations that perform on par or exceed the state of the art in transfer learning benchmarks.



### Seeing Through Noise: Visually Driven Speaker Separation and Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1708.06767v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1708.06767v3)
- **Published**: 2017-08-22 18:02:27+00:00
- **Updated**: 2018-02-09 21:34:19+00:00
- **Authors**: Aviv Gabbay, Ariel Ephrat, Tavi Halperin, Shmuel Peleg
- **Comment**: Supplementary video: https://www.youtube.com/watch?v=qmsyj7vAzoI
- **Journal**: None
- **Summary**: Isolating the voice of a specific person while filtering out other voices or background noises is challenging when video is shot in noisy environments. We propose audio-visual methods to isolate the voice of a single speaker and eliminate unrelated sounds. First, face motions captured in the video are used to estimate the speaker's voice, by passing the silent video frames through a video-to-speech neural network-based model. Then the speech predictions are applied as a filter on the noisy input audio. This approach avoids using mixtures of sounds in the learning process, as the number of such possible mixtures is huge, and would inevitably bias the trained model. We evaluate our method on two audio-visual datasets, GRID and TCD-TIMIT, and show that our method attains significant SDR and PESQ improvements over the raw video-to-speech predictions, and a well-known audio-only method.



### Reflection Separation and Deblurring of Plenoptic Images
- **Arxiv ID**: http://arxiv.org/abs/1708.06779v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06779v1)
- **Published**: 2017-08-22 18:34:32+00:00
- **Updated**: 2017-08-22 18:34:32+00:00
- **Authors**: Paramanand Chandramouli, Mehdi Noroozi, Paolo Favaro
- **Comment**: ACCV 2016
- **Journal**: None
- **Summary**: In this paper, we address the problem of reflection removal and deblurring from a single image captured by a plenoptic camera. We develop a two-stage approach to recover the scene depth and high resolution textures of the reflected and transmitted layers. For depth estimation in the presence of reflections, we train a classifier through convolutional neural networks. For recovering high resolution textures, we assume that the scene is composed of planar regions and perform the reconstruction of each layer by using an explicit form of the plenoptic camera point spread function. The proposed framework also recovers the sharp scene texture with different motion blurs applied to each layer. We demonstrate our method on challenging real and synthetic images.



### Real-Time 6DOF Pose Relocalization for Event Cameras with Stacked Spatial LSTM Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.09011v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1708.09011v3)
- **Published**: 2017-08-22 18:59:22+00:00
- **Updated**: 2018-04-14 03:15:12+00:00
- **Authors**: Anh Nguyen, Thanh-Toan Do, Darwin G. Caldwell, Nikos G. Tsagarakis
- **Comment**: 7 pages, 5 figures
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW), 2019
- **Summary**: We present a new method to relocalize the 6DOF pose of an event camera solely based on the event stream. Our method first creates the event image from a list of events that occurs in a very short time interval, then a Stacked Spatial LSTM Network (SP-LSTM) is used to learn the camera pose. Our SP-LSTM is composed of a CNN to learn deep features from the event images and a stack of LSTM to learn spatial dependencies in the image feature space. We show that the spatial dependency plays an important role in the relocalization task and the SP-LSTM can effectively learn this information. The experimental results on a publicly available dataset show that our approach generalizes well and outperforms recent methods by a substantial margin. Overall, our proposed method reduces by approx. 6 times the position error and 3 times the orientation error compared to the current state of the art. The source code and trained models will be released.



### Human Action Recognition System using Good Features and Multilayer Perceptron Network
- **Arxiv ID**: http://arxiv.org/abs/1708.06794v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1708.06794v1)
- **Published**: 2017-08-22 19:39:45+00:00
- **Updated**: 2017-08-22 19:39:45+00:00
- **Authors**: Jonti Talukdar, Bhavana Mehta
- **Comment**: 6 pages, 7 Figures, IEEE International Conference on Communication
  and Signal Processing 2017 (ICCSP 2017)
- **Journal**: None
- **Summary**: Human action recognition involves the characterization of human actions through the automated analysis of video data and is integral in the development of smart computer vision systems. However, several challenges like dynamic backgrounds, camera stabilization, complex actions, occlusions etc. make action recognition in a real time and robust fashion difficult. Several complex approaches exist but are computationally intensive. This paper presents a novel approach of using a combination of good features along with iterative optical flow algorithm to compute feature vectors which are classified using a multilayer perceptron (MLP) network. The use of multiple features for motion descriptors enhances the quality of tracking. Resilient backpropagation algorithm is used for training the feedforward neural network reducing the learning time. The overall system accuracy is improved by optimizing the various parameters of the multilayer perceptron network.



### Deep EndoVO: A Recurrent Convolutional Neural Network (RCNN) based Visual Odometry Approach for Endoscopic Capsule Robots
- **Arxiv ID**: http://arxiv.org/abs/1708.06822v2
- **DOI**: 10.1016/j.neucom.2017.10.014
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06822v2)
- **Published**: 2017-08-22 21:13:18+00:00
- **Updated**: 2017-09-08 13:47:53+00:00
- **Authors**: Mehmet Turan, Yasin Almalioglu, Helder Araujo, Ender Konukoglu, Metin Sitti
- **Comment**: None
- **Journal**: None
- **Summary**: Ingestible wireless capsule endoscopy is an emerging minimally invasive diagnostic technology for inspection of the GI tract and diagnosis of a wide range of diseases and pathologies. Medical device companies and many research groups have recently made substantial progresses in converting passive capsule endoscopes to active capsule robots, enabling more accurate, precise, and intuitive detection of the location and size of the diseased areas. Since a reliable real time pose estimation functionality is crucial for actively controlled endoscopic capsule robots, in this study, we propose a monocular visual odometry (VO) method for endoscopic capsule robot operations. Our method lies on the application of the deep Recurrent Convolutional Neural Networks (RCNNs) for the visual odometry task, where Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) are used for the feature extraction and inference of dynamics across the frames, respectively. Detailed analyses and evaluations made on a real pig stomach dataset proves that our system achieves high translational and rotational accuracies for different types of endoscopic capsule robot trajectories.



### Multiple-Kernel Based Vehicle Tracking Using 3D Deformable Model and Camera Self-Calibration
- **Arxiv ID**: http://arxiv.org/abs/1708.06831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06831v1)
- **Published**: 2017-08-22 21:41:11+00:00
- **Updated**: 2017-08-22 21:41:11+00:00
- **Authors**: Zheng Tang, Gaoang Wang, Tao Liu, Young-Gun Lee, Adwin Jahn, Xu Liu, Xiaodong He, Jenq-Neng Hwang
- **Comment**: 6 pages, 6 figures, 3 tables, 2017 IEEE Smart World NVIDIA AI City
  Challenge (Winner of Track 2: Applications)
- **Journal**: None
- **Summary**: Tracking of multiple objects is an important application in AI City geared towards solving salient problems related to safety and congestion in an urban environment. Frequent occlusion in traffic surveillance has been a major problem in this research field. In this challenge, we propose a model-based vehicle localization method, which builds a kernel at each patch of the 3D deformable vehicle model and associates them with constraints in 3D space. The proposed method utilizes shape fitness evaluation besides color information to track vehicle objects robustly and efficiently. To build 3D car models in a fully unsupervised manner, we also implement evolutionary camera self-calibration from tracking of walking humans to automatically compute camera parameters. Additionally, the segmented foreground masks which are crucial to 3D modeling and camera self-calibration are adaptively refined by multiple-kernel feedback from tracking. For object detection/classification, the state-of-the-art single shot multibox detector (SSD) is adopted to train and test on the NVIDIA AI City Dataset. To improve the accuracy on categories with only few objects, like bus, bicycle and motorcycle, we also employ the pretrained model from YOLO9000 with multi-scale testing. We combine the results from SSD and YOLO9000 based on ensemble learning. Experiments show that our proposed tracking system outperforms both state-of-the-art of tracking by segmentation and tracking by detection.



### Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.06834v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.06834v3)
- **Published**: 2017-08-22 21:53:34+00:00
- **Updated**: 2018-02-05 17:14:12+00:00
- **Authors**: Victor Campos, Brendan Jou, Xavier Giro-i-Nieto, Jordi Torres, Shih-Fu Chang
- **Comment**: Accepted as conference paper at ICLR 2018
- **Journal**: None
- **Summary**: Recurrent Neural Networks (RNNs) continue to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/ .



