# Arxiv Papers in cs.CV on 2017-08-18
### Dilated Deep Residual Network for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1708.05473v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05473v3)
- **Published**: 2017-08-18 00:30:41+00:00
- **Updated**: 2017-09-27 23:37:46+00:00
- **Authors**: Tianyang Wang, Mingxuan Sun, Kaoning Hu
- **Comment**: camera ready, 8 pages, accepted to IEEE ICTAI 2017
- **Journal**: None
- **Summary**: Variations of deep neural networks such as convolutional neural network (CNN) have been successfully applied to image denoising. The goal is to automatically learn a mapping from a noisy image to a clean image given training data consisting of pairs of noisy and clean images. Most existing CNN models for image denoising have many layers. In such cases, the models involve a large amount of parameters and are computationally expensive to train. In this paper, we develop a dilated residual CNN for Gaussian image denoising. Compared with the recently proposed residual denoiser, our method can achieve comparable performance with less computational cost. Specifically, we enlarge receptive field by adopting dilated convolution in residual network, and the dilation factor is set to a certain value. We utilize appropriate zero padding to make the dimension of the output the same as the input. It has been proven that the expansion of receptive field can boost the CNN performance in image classification, and we further demonstrate that it can also lead to competitive performance for denoising problem. Moreover, we present a formula to calculate receptive field size when dilated convolution is incorporated. Thus, the change of receptive field can be interpreted mathematically. To validate the efficacy of our approach, we conduct extensive experiments for both gray and color image denoising with specific or randomized noise levels. Both of the quantitative measurements and the visual results of denoising are promising comparing with state-of-the-art baselines.



### Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1708.05493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05493v1)
- **Published**: 2017-08-18 03:01:35+00:00
- **Updated**: 2017-08-18 03:01:35+00:00
- **Authors**: Yinpeng Dong, Hang Su, Jun Zhu, Fan Bao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have demonstrated impressive performance on a wide array of tasks, but they are usually considered opaque since internal structure and learned parameters are not interpretable. In this paper, we re-examine the internal representations of DNNs using adversarial images, which are generated by an ensemble-optimization algorithm. We find that: (1) the neurons in DNNs do not truly detect semantic objects/parts, but respond to objects/parts only as recurrent discriminative patches; (2) deep visual representations are not robust distributed codes of visual concepts because the representations of adversarial images are largely not consistent with those of real images, although they have similar visual appearance, both of which are different from previous findings. To further improve the interpretability of DNNs, we propose an adversarial training scheme with a consistent loss such that the neurons are endowed with human-interpretable concepts. The induced interpretable representations enable us to trace eventual outcomes back to influential neurons. Therefore, human users can know how the models make predictions, as well as when and why they make errors.



### Towards the Automatic Anime Characters Creation with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.05509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05509v1)
- **Published**: 2017-08-18 04:57:28+00:00
- **Updated**: 2017-08-18 04:57:28+00:00
- **Authors**: Yanghua Jin, Jiakai Zhang, Minjun Li, Yingtao Tian, Huachun Zhu, Zhihao Fang
- **Comment**: 16 pages, 15 figures. This paper is presented as a Doujinshi in
  Comiket 92, summer 2017, with the booth number 05a, East-U, Third Day
- **Journal**: None
- **Summary**: Automatic generation of facial images has been well studied after the Generative Adversarial Network (GAN) came out. There exists some attempts applying the GAN model to the problem of generating facial images of anime characters, but none of the existing work gives a promising result. In this work, we explore the training of GAN models specialized on an anime facial image dataset. We address the issue from both the data and the model aspect, by collecting a more clean, well-suited dataset and leverage proper, empirical application of DRAGAN. With quantitative analysis and case studies we demonstrate that our efforts lead to a stable and high-quality model. Moreover, to assist people with anime character design, we build a website (http://make.girls.moe) with our pre-trained model available online, which makes the model easily accessible to general public.



### Large Margin Learning in Set to Set Similarity Comparison for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1708.05512v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.05512v1)
- **Published**: 2017-08-18 05:19:01+00:00
- **Updated**: 2017-08-18 05:19:01+00:00
- **Authors**: Sanping Zhou, Jinjun Wang, Rui Shi, Qiqi Hou, Yihong Gong, Nanning Zheng
- **Comment**: Accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) aims at matching images of the same person across disjoint camera views, which is a challenging problem in multimedia analysis, multimedia editing and content-based media retrieval communities. The major challenge lies in how to preserve similarity of the same person across video footages with large appearance variations, while discriminating different individuals. To address this problem, conventional methods usually consider the pairwise similarity between persons by only measuring the point to point (P2P) distance. In this paper, we propose to use deep learning technique to model a novel set to set (S2S) distance, in which the underline objective focuses on preserving the compactness of intra-class samples for each camera view, while maximizing the margin between the intra-class set and inter-class set. The S2S distance metric is consisted of three terms, namely the class-identity term, the relative distance term and the regularization term. The class-identity term keeps the intra-class samples within each camera view gathering together, the relative distance term maximizes the distance between the intra-class class set and inter-class set across different camera views, and the regularization term smoothness the parameters of deep convolutional neural network (CNN). As a result, the final learned deep model can effectively find out the matched target to the probe object among various candidates in the video gallery by learning discriminative and stable feature representations. Using the CUHK01, CUHK03, PRID2011 and Market1501 benchmark datasets, we extensively conducted comparative evaluations to demonstrate the advantages of our method over the state-of-the-art approaches.



### Reflectance Intensity Assisted Automatic and Accurate Extrinsic Calibration of 3D LiDAR and Panoramic Camera Using a Printed Chessboard
- **Arxiv ID**: http://arxiv.org/abs/1708.05514v1
- **DOI**: 10.3390/rs9080851
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1708.05514v1)
- **Published**: 2017-08-18 05:58:02+00:00
- **Updated**: 2017-08-18 05:58:02+00:00
- **Authors**: Weimin Wang, Ken Sakurada, Nobuo Kawaguchi
- **Comment**: 20 pages, submitted to the journal of Remote Sensing
- **Journal**: Remote Sensing, 9(8):851 (2017)
- **Summary**: This paper presents a novel method for fully automatic and convenient extrinsic calibration of a 3D LiDAR and a panoramic camera with a normally printed chessboard. The proposed method is based on the 3D corner estimation of the chessboard from the sparse point cloud generated by one frame scan of the LiDAR. To estimate the corners, we formulate a full-scale model of the chessboard and fit it to the segmented 3D points of the chessboard. The model is fitted by optimizing the cost function under constraints of correlation between the reflectance intensity of laser and the color of the chessboard's patterns. Powell's method is introduced for resolving the discontinuity problem in optimization. The corners of the fitted model are considered as the 3D corners of the chessboard. Once the corners of the chessboard in the 3D point cloud are estimated, the extrinsic calibration of the two sensors is converted to a 3D-2D matching problem. The corresponding 3D-2D points are used to calculate the absolute pose of the two sensors with Unified Perspective-n-Point (UPnP). Further, the calculated parameters are regarded as initial values and are refined using the Levenberg-Marquardt method. The performance of the proposed corner detection method from the 3D point cloud is evaluated using simulations. The results of experiments, conducted on a Velodyne HDL-32e LiDAR and a Ladybug3 camera under the proposed re-projection error metric, qualitatively and quantitatively demonstrate the accuracy and stability of the final extrinsic calibration parameters.



### Word Searching in Scene Image and Video Frame in Multi-Script Scenario using Dynamic Shape Coding
- **Arxiv ID**: http://arxiv.org/abs/1708.05529v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1708.05529v6)
- **Published**: 2017-08-18 07:47:05+00:00
- **Updated**: 2018-07-30 10:41:30+00:00
- **Authors**: Partha Pratim Roy, Ayan Kumar Bhunia, Avirup Bhattacharyya, Umapada Pal
- **Comment**: Multimedia Tools and Applications, Springer
- **Journal**: None
- **Summary**: Retrieval of text information from natural scene images and video frames is a challenging task due to its inherent problems like complex character shapes, low resolution, background noise, etc. Available OCR systems often fail to retrieve such information in scene/video frames. Keyword spotting, an alternative way to retrieve information, performs efficient text searching in such scenarios. However, current word spotting techniques in scene/video images are script-specific and they are mainly developed for Latin script. This paper presents a novel word spotting framework using dynamic shape coding for text retrieval in natural scene image and video frames. The framework is designed to search query keyword from multiple scripts with the help of on-the-fly script-wise keyword generation for the corresponding script. We have used a two-stage word spotting approach using Hidden Markov Model (HMM) to detect the translated keyword in a given text line by identifying the script of the line. A novel unsupervised dynamic shape coding based scheme has been used to group similar shape characters to avoid confusion and to improve text alignment. Next, the hypotheses locations are verified to improve retrieval performance. To evaluate the proposed system for searching keyword from natural scene image and video frames, we have considered two popular Indic scripts such as Bangla (Bengali) and Devanagari along with English. Inspired by the zone-wise recognition approach in Indic scripts[1], zone-wise text information has been used to improve the traditional word spotting performance in Indic scripts. For our experiment, a dataset consisting of images of different scenes and video frames of English, Bangla and Devanagari scripts were considered. The results obtained showed the effectiveness of our proposed word spotting approach.



### Pillar Networks++: Distributed non-parametric deep and wide networks
- **Arxiv ID**: http://arxiv.org/abs/1708.06250v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, stat.CO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.06250v1)
- **Published**: 2017-08-18 07:51:43+00:00
- **Updated**: 2017-08-18 07:51:43+00:00
- **Authors**: Biswa Sengupta, Yu Qian
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1707.06923
- **Journal**: None
- **Summary**: In recent work, it was shown that combining multi-kernel based support vector machines (SVMs) can lead to near state-of-the-art performance on an action recognition dataset (HMDB-51 dataset). This was 0.4\% lower than frameworks that used hand-crafted features in addition to the deep convolutional feature extractors. In the present work, we show that combining distributed Gaussian Processes with multi-stream deep convolutional neural networks (CNN) alleviate the need to augment a neural network with hand-crafted features. In contrast to prior work, we treat each deep neural convolutional network as an expert wherein the individual predictions (and their respective uncertainties) are combined into a Product of Experts (PoE) framework.



### Mesh-based 3D Textured Urban Mapping
- **Arxiv ID**: http://arxiv.org/abs/1708.05543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05543v1)
- **Published**: 2017-08-18 09:43:10+00:00
- **Updated**: 2017-08-18 09:43:10+00:00
- **Authors**: Andrea Romanoni, Daniele Fiorenti, Matteo Matteucci
- **Comment**: accepted at iros 2017
- **Journal**: None
- **Summary**: In the era of autonomous driving, urban mapping represents a core step to let vehicles interact with the urban context. Successful mapping algorithms have been proposed in the last decade building the map leveraging on data from a single sensor. The focus of the system presented in this paper is twofold: the joint estimation of a 3D map from lidar data and images, based on a 3D mesh, and its texturing. Indeed, even if most surveying vehicles for mapping are endowed by cameras and lidar, existing mapping algorithms usually rely on either images or lidar data; moreover both image-based and lidar-based systems often represent the map as a point cloud, while a continuous textured mesh representation would be useful for visualization and navigation purposes. In the proposed framework, we join the accuracy of the 3D lidar data, and the dense information and appearance carried by the images, in estimating a visibility consistent map upon the lidar measurements, and refining it photometrically through the acquired images. We evaluate the proposed framework against the KITTI dataset and we show the performance improvement with respect to two state of the art urban mapping algorithms, and two widely used surface reconstruction algorithms in Computer Graphics.



### Spotting Separator Points at Line Terminals in Compressed Document Images for Text-line Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1708.05545v1
- **DOI**: 10.5120/ijca2017915133
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05545v1)
- **Published**: 2017-08-18 09:51:17+00:00
- **Updated**: 2017-08-18 09:51:17+00:00
- **Authors**: Amarnath R, P. Nagabhushan
- **Comment**: Line separators, Document image analysis, Handwritten text,
  Compression and decompression, RLE, CCITT. Line separator points at every
  line terminal in a compressed handwritten document images enabling text line
  segmentation
- **Journal**: International Journal of Computer Applications 172(4): 40-47
  (2017)
- **Summary**: Line separators are used to segregate text-lines from one another in document image analysis. Finding the separator points at every line terminal in a document image would enable text-line segmentation. In particular, identifying the separators in handwritten text could be a thrilling exercise. Obviously it would be challenging to perform this in the compressed version of a document image and that is the proposed objective in this research. Such an effort would prevent the computational burden of decompressing a document for text-line segmentation. Since document images are generally compressed using run length encoding (RLE) technique as per the CCITT standards, the first column in the RLE will be a white column. The value (depth) in the white column is very low when a particular line is a text line and the depth could be larger at the point of text line separation. A longer consecutive sequence of such larger depth should indicate the gap between the text lines, which provides the separator region. In case of over separation and under separation issues, corrective actions such as deletion and insertion are suggested respectively. An extensive experimentation is conducted on the compressed images of the benchmark datasets of ICDAR13 and Alireza et al [17] to demonstrate the efficacy.



### Practical Block-wise Neural Network Architecture Generation
- **Arxiv ID**: http://arxiv.org/abs/1708.05552v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.05552v3)
- **Published**: 2017-08-18 10:12:43+00:00
- **Updated**: 2018-05-14 15:18:35+00:00
- **Authors**: Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, Cheng-Lin Liu
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained sequentially to choose component layers. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it performs competitive results in comparison to the hand-crafted state-of-the-art networks on image classification, additionally, the best network generated by BlockQNN achieves 3.54% top-1 error rate on CIFAR-10 which beats all existing auto-generate networks. (2) in the meanwhile, it offers tremendous reduction of the search space in designing networks which only spends 3 days with 32 GPUs, and (3) moreover, it has strong generalizability that the network built on CIFAR also performs well on a larger-scale ImageNet dataset.



### Self-explanatory Deep Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.05595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05595v1)
- **Published**: 2017-08-18 13:19:01+00:00
- **Updated**: 2017-08-18 13:19:01+00:00
- **Authors**: Huaxin Xiao, Jiashi Feng, Yunchao Wei, Maojun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Salient object detection has seen remarkable progress driven by deep learning techniques. However, most of deep learning based salient object detection methods are black-box in nature and lacking in interpretability. This paper proposes the first self-explanatory saliency detection network that explicitly exploits low- and high-level features for salient object detection. We demonstrate that such supportive clues not only significantly enhances performance of salient object detection but also gives better justified detection results. More specifically, we develop a multi-stage saliency encoder to extract multi-scale features which contain both low- and high-level saliency context. Dense short- and long-range connections are introduced to reuse these features iteratively. Benefiting from the direct access to low- and high-level features, the proposed saliency encoder can not only model the object context but also preserve the boundary. Furthermore, a self-explanatory generator is proposed to interpret how the proposed saliency encoder or other deep saliency models making decisions. The generator simulates the absence of interesting features by preventing these features from contributing to the saliency classifier and estimates the corresponding saliency prediction without these features. A comparison function, saliency explanation, is defined to measure the prediction changes between deep saliency models and corresponding generator. Through visualizing the differences, we can interpret the capability of different deep neural networks based saliency detection models and demonstrate that our proposed model indeed uses more reasonable structure for salient object detection. Extensive experiments on five popular benchmark datasets and the visualized saliency explanation demonstrate that the proposed method provides new state-of-the-art.



### CoBe -- Coded Beacons for Localization, Object Tracking, and SLAM Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1708.05625v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05625v3)
- **Published**: 2017-08-18 14:22:45+00:00
- **Updated**: 2020-04-21 20:48:15+00:00
- **Authors**: Roman Rabinovich, Ibrahim Jubran, Aaron Wetzler, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel beacon light coding protocol, which enables fast and accurate identification of the beacons in an image. The protocol is provably robust to a predefined set of detection and decoding errors, and does not require any synchronization between the beacons themselves and the optical sensor. A detailed guide is then given for developing an optical tracking and localization system, which is based on the suggested protocol and readily available hardware. Such a system operates either as a standalone system for recovering the six degrees of freedom of fast moving objects, or integrated with existing SLAM pipelines providing them with error-free and easily identifiable landmarks. Based on this guide, we implemented a low-cost positional tracking system which can run in real-time on an IoT board. We evaluate our system's accuracy and compare it to other popular methods which utilize the same optical hardware, in experiments where the ground truth is known. A companion video containing multiple real-world experiments demonstrates the accuracy, speed, and applicability of the proposed system in a wide range of environments and real-world tasks. Open source code is provided to encourage further development of low-cost localization systems integrating the suggested technology at its navigation core.



### 3D Pose Regression using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.05628v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05628v1)
- **Published**: 2017-08-18 14:36:11+00:00
- **Updated**: 2017-08-18 14:36:11+00:00
- **Authors**: Siddharth Mahendran, Haider Ali, Rene Vidal
- **Comment**: None
- **Journal**: None
- **Summary**: 3D pose estimation is a key component of many important computer vision tasks such as autonomous navigation and 3D scene understanding. Most state-of-the-art approaches to 3D pose estimation solve this problem as a pose-classification problem in which the pose space is discretized into bins and a CNN classifier is used to predict a pose bin. We argue that the 3D pose space is continuous and propose to solve the pose estimation problem in a CNN regression framework with a suitable representation, data augmentation and loss function that captures the geometry of the pose space. Experiments on PASCAL3D+ show that the proposed 3D pose regression approach achieves competitive performance compared to the state-of-the-art.



### What does a convolutional neural network recognize in the moon?
- **Arxiv ID**: http://arxiv.org/abs/1708.05636v2
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.EP, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/1708.05636v2)
- **Published**: 2017-08-18 14:59:51+00:00
- **Updated**: 2017-08-21 05:30:33+00:00
- **Authors**: Daigo Shoji
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Many people see a human face or animals in the pattern of the maria on the moon. Although the pattern corresponds to the actual variation in composition of the lunar surface, the culture and environment of each society influence the recognition of these objects (i.e., symbols) as specific entities. In contrast, a convolutional neural network (CNN) recognizes objects from characteristic shapes in a training data set. Using CNN, this study evaluates the probabilities of the pattern of lunar maria categorized into the shape of a crab, a lion and a hare. If Mare Frigoris (a dark band on the moon) is included in the lunar image, the lion is recognized. However, in an image without Mare Frigoris, the hare has the highest probability of recognition. Thus, the recognition of objects similar to the lunar pattern depends on which part of the lunar maria is taken into account. In human recognition, before we find similarities between the lunar maria and objects such as animals, we may be persuaded in advance to see a particular image from our culture and environment and then adjust the lunar pattern to the shape of the imagined object.



