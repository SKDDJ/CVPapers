# Arxiv Papers in cs.CV on 2017-08-05
### Video Frame Interpolation via Adaptive Separable Convolution
- **Arxiv ID**: http://arxiv.org/abs/1708.01692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01692v1)
- **Published**: 2017-08-05 00:18:03+00:00
- **Updated**: 2017-08-05 00:18:03+00:00
- **Authors**: Simon Niklaus, Long Mai, Feng Liu
- **Comment**: ICCV 2017, http://graphics.cs.pdx.edu/project/sepconv/
- **Journal**: None
- **Summary**: Standard video frame interpolation methods first estimate optical flow between input frames and then synthesize an intermediate frame guided by motion. Recent approaches merge these two steps into a single convolution process by convolving input frames with spatially adaptive kernels that account for motion and re-sampling simultaneously. These methods require large kernels to handle large motion, which limits the number of pixels whose kernels can be estimated at once due to the large memory demand. To address this problem, this paper formulates frame interpolation as local separable convolution over input frames using pairs of 1D kernels. Compared to regular 2D kernels, the 1D kernels require significantly fewer parameters to be estimated. Our method develops a deep fully convolutional neural network that takes two input frames and estimates pairs of 1D kernels for all pixels simultaneously. Since our method is able to estimate kernels and synthesizes the whole video frame at once, it allows for the incorporation of perceptual loss to train the neural network to produce visually pleasing frames. This deep neural network is trained end-to-end using widely available video data without any human annotation. Both qualitative and quantitative experiments show that our method provides a practical solution to high-quality video frame interpolation.



### Adversarial Robustness: Softmax versus Openmax
- **Arxiv ID**: http://arxiv.org/abs/1708.01697v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01697v1)
- **Published**: 2017-08-05 01:44:52+00:00
- **Updated**: 2017-08-05 01:44:52+00:00
- **Authors**: Andras Rozsa, Manuel GÃ¼nther, Terrance E. Boult
- **Comment**: Accepted to British Machine Vision Conference (BMVC) 2017
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) provide state-of-the-art results on various tasks and are widely used in real world applications. However, it was discovered that machine learning models, including the best performing DNNs, suffer from a fundamental problem: they can unexpectedly and confidently misclassify examples formed by slightly perturbing otherwise correctly recognized inputs. Various approaches have been developed for efficiently generating these so-called adversarial examples, but those mostly rely on ascending the gradient of loss. In this paper, we introduce the novel logits optimized targeting system (LOTS) to directly manipulate deep features captured at the penultimate layer. Using LOTS, we analyze and compare the adversarial robustness of DNNs using the traditional Softmax layer with Openmax, which was designed to provide open set recognition by defining classes derived from deep representations, and is claimed to be more robust to adversarial perturbations. We demonstrate that Openmax provides less vulnerable systems than Softmax to traditional attacks, however, we show that it can be equally susceptible to more sophisticated adversarial generation techniques that directly work on deep representations.



### Optimizing Region Selection for Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.01723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01723v1)
- **Published**: 2017-08-05 07:24:43+00:00
- **Updated**: 2017-08-05 07:24:43+00:00
- **Authors**: Wenhui Jiang, Thuyen Ngo, B. S. Manjunath, Zhicheng Zhao, Fei Su
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Training object detectors with only image-level annotations is very challenging because the target objects are often surrounded by a large number of background clutters. Many existing approaches tackle this problem through object proposal mining. However, the collected positive regions are either low in precision or lack of diversity, and the strategy of collecting negative regions is not carefully designed, neither. Moreover, training is often slow because region selection and object detector training are processed separately. In this context, the primary contribution of this work is to improve weakly supervised detection with an optimized region selection strategy. The proposed method collects purified positive training regions by progressively removing easy background clutters, and selects discriminative negative regions by mining class-specific hard samples. This region selection procedure is further integrated into a CNN-based weakly supervised detection (WSD) framework, and can be performed in each stochastic gradient descent mini-batch during training. Therefore, the entire model can be trained end-to-end efficiently. Extensive evaluation results on PASCAL VOC 2007, VOC 2010 and VOC 2012 datasets are presented which demonstrate that the proposed method effectively improves WSD.



### Inception Score, Label Smoothing, Gradient Vanishing and -log(D(x)) Alternative
- **Arxiv ID**: http://arxiv.org/abs/1708.01729v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.01729v3)
- **Published**: 2017-08-05 08:15:07+00:00
- **Updated**: 2018-06-30 07:02:11+00:00
- **Authors**: Zhiming Zhou, Weinan Zhang, Jun Wang
- **Comment**: An advanced version is included in arXiv:1703.02000 "Activation
  Maximization Generative Adversarial Nets"
- **Journal**: None
- **Summary**: In this article, we mathematically study several GAN related topics, including Inception score, label smoothing, gradient vanishing and the -log(D(x)) alternative.   ---   An advanced version is included in arXiv:1703.02000 "Activation Maximization Generative Adversarial Nets".   Please refer Section 6 in 1703.02000 for detailed analysis on Inception Score, and refer its appendix for the discussions on Label Smoothing, Gradient Vanishing and -log(D(x)) Alternative.



### Learning Discriminative Alpha-Beta-divergence for Positive Definite Matrices (Extended Version)
- **Arxiv ID**: http://arxiv.org/abs/1708.01741v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01741v1)
- **Published**: 2017-08-05 09:38:20+00:00
- **Updated**: 2017-08-05 09:38:20+00:00
- **Authors**: Anoop Cherian, Panagiotis Stanitsas, Mehrtash Harandi, Vassilios Morellas, Nikolaos Papanikolopoulos
- **Comment**: Accepted at the International Conference on Computer Vision (ICCV)
- **Journal**: None
- **Summary**: Symmetric positive definite (SPD) matrices are useful for capturing second-order statistics of visual data. To compare two SPD matrices, several measures are available, such as the affine-invariant Riemannian metric, Jeffreys divergence, Jensen-Bregman logdet divergence, etc.; however, their behaviors may be application dependent, raising the need of manual selection to achieve the best possible performance. Further and as a result of their overwhelming complexity for large-scale problems, computing pairwise similarities by clever embedding of SPD matrices is often preferred to direct use of the aforementioned measures. In this paper, we propose a discriminative metric learning framework, Information Divergence and Dictionary Learning (IDDL), that not only learns application specific measures on SPD matrices automatically, but also embeds them as vectors using a learned dictionary. To learn the similarity measures (which could potentially be distinct for every dictionary atom), we use the recently introduced alpha-beta-logdet divergence, which is known to unify the measures listed above. We propose a novel IDDL objective, that learns the parameters of the divergence and the dictionary atoms jointly in a discriminative setup and is solved efficiently using Riemannian optimization. We showcase extensive experiments on eight computer vision datasets, demonstrating state-of-the-art performances.



### SurfaceNet: An End-to-end 3D Neural Network for Multiview Stereopsis
- **Arxiv ID**: http://arxiv.org/abs/1708.01749v1
- **DOI**: 10.1109/ICCV.2017.253
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01749v1)
- **Published**: 2017-08-05 10:58:19+00:00
- **Updated**: 2017-08-05 10:58:19+00:00
- **Authors**: Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, Lu Fang
- **Comment**: 2017 iccv poster
- **Journal**: 2017 ICCV
- **Summary**: This paper proposes an end-to-end learning framework for multiview stereopsis. We term the network SurfaceNet. It takes a set of images and their corresponding camera parameters as input and directly infers the 3D model. The key advantage of the framework is that both photo-consistency as well geometric relations of the surface structure can be directly learned for the purpose of multiview stereopsis in an end-to-end fashion. SurfaceNet is a fully 3D convolutional network which is achieved by encoding the camera parameters together with the images in a 3D voxel representation. We evaluate SurfaceNet on the large-scale DTU benchmark.



### A Novel data Pre-processing method for multi-dimensional and non-uniform data
- **Arxiv ID**: http://arxiv.org/abs/1708.04664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04664v1)
- **Published**: 2017-08-05 13:15:36+00:00
- **Updated**: 2017-08-05 13:15:36+00:00
- **Authors**: Farhana Javed Zareen, Suraiya Jabin
- **Comment**: 11 pages, 4 Figures, 7 Tables
- **Journal**: None
- **Summary**: We are in the era of data analytics and data science which is on full bloom. There is abundance of all kinds of data for example biometrics based data, satellite images data, chip-seq data, social network data, sensor based data etc. from a variety of sources. This data abundance is the result of the fact that storage cost is getting cheaper day by day, so people as well as almost all business or scientific organizations are storing more and more data. Most of the real data is multi-dimensional, non-uniform, and big in size, such that it requires a unique pre-processing before analyzing it. In order to make data useful for any kind of analysis, pre-processing is a very important step. This paper presents a unique and novel pre-processing method for multi-dimensional and non-uniform data with the aim of making it uniform and reduced in size without losing much of its value. We have chosen biometric signature data to demonstrate the proposed method as it qualifies for the attributes of being multi-dimensional, non-uniform and big in size. Biometric signature data does not only captures the structural characteristics of a signature but also its behavioral characteristics that are captured using a dynamic signature capture device. These features like pen pressure, pen tilt angle, time taken to sign a document when collected in real-time turn out to be of varying dimensions. This feature data set along with the structural data needs to be pre-processed in order to use it to train a machine learning based model for signature verification purposes. We demonstrate the success of the proposed method over other methods using experimental results for biometric signature data but the same can be implemented for any other data with similar properties from a different domain.



### Interactively Transferring CNN Patterns for Part Localization
- **Arxiv ID**: http://arxiv.org/abs/1708.01783v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01783v2)
- **Published**: 2017-08-05 15:55:20+00:00
- **Updated**: 2017-11-22 03:29:22+00:00
- **Authors**: Quanshi Zhang, Ruiming Cao, Shengming Zhang, Mark Redmonds, Ying Nian Wu, Song-Chun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In the scenario of one/multi-shot learning, conventional end-to-end learning strategies without sufficient supervision are usually not powerful enough to learn correct patterns from noisy signals. Thus, given a CNN pre-trained for object classification, this paper proposes a method that first summarizes the knowledge hidden inside the CNN into a dictionary of latent activation patterns, and then builds a new model for part localization by manually assembling latent patterns related to the target part via human interactions. We use very few (e.g., three) annotations of a semantic object part to retrieve certain latent patterns from conv-layers to represent the target part. We then visualize these latent patterns and ask users to further remove incorrect patterns, in order to refine part representation. With the guidance of human interactions, our method exhibited superior performance of part localization in experiments.



### Interpreting CNN Knowledge via an Explanatory Graph
- **Arxiv ID**: http://arxiv.org/abs/1708.01785v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01785v3)
- **Published**: 2017-08-05 15:59:13+00:00
- **Updated**: 2017-11-22 02:29:51+00:00
- **Authors**: Quanshi Zhang, Ruiming Cao, Feng Shi, Ying Nian Wu, Song-Chun Zhu
- **Comment**: in AAAI 2018
- **Journal**: None
- **Summary**: This paper learns a graphical model, namely an explanatory graph, which reveals the knowledge hierarchy hidden inside a pre-trained CNN. Considering that each filter in a conv-layer of a pre-trained CNN usually represents a mixture of object parts, we propose a simple yet efficient method to automatically disentangles different part patterns from each filter, and construct an explanatory graph. In the explanatory graph, each node represents a part pattern, and each edge encodes co-activation relationships and spatial relationships between patterns. More importantly, we learn the explanatory graph for a pre-trained CNN in an unsupervised manner, i.e., without a need of annotating object parts. Experiments show that each graph node consistently represents the same object part through different images. We transfer part patterns in the explanatory graph to the task of part localization, and our method significantly outperforms other approaches.



### Detecting Noteheads in Handwritten Scores with ConvNets and Bounding Box Regression
- **Arxiv ID**: http://arxiv.org/abs/1708.01806v1
- **DOI**: None
- **Categories**: **cs.CV**, I.7.5, I.5.4, I.4.6, I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1708.01806v1)
- **Published**: 2017-08-05 18:54:06+00:00
- **Updated**: 2017-08-05 18:54:06+00:00
- **Authors**: Jan HajiÄ Jr., Pavel Pecina
- **Comment**: None
- **Journal**: None
- **Summary**: Noteheads are the interface between the written score and music. Each notehead on the page signifies one note to be played, and detecting noteheads is thus an unavoidable step for Optical Music Recognition. Noteheads are clearly distinct objects, however, the variety of music notation handwriting makes noteheads harder to identify, and while handwritten music notation symbol {\em classification} is a well-studied task, symbol {\em detection} has usually been limited to heuristics and rule-based systems instead of machine learning methods better suited to deal with the uncertainties in handwriting. We present ongoing work on a simple notehead detector using convolutional neural networks for pixel classification and bounding box regression that achieves a detection f-score of 0.97 on binary score images in the MUSCIMA++ dataset, does not require staff removal, and is applicable to a variety of handwriting styles and levels of musical complexity.



### Depth Adaptive Deep Neural Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1708.01818v2
- **DOI**: 10.1109/TMM.2018.2798282
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01818v2)
- **Published**: 2017-08-05 19:54:59+00:00
- **Updated**: 2018-01-29 18:31:30+00:00
- **Authors**: Byeongkeun Kang, Yeejin Lee, Truong Q. Nguyen
- **Comment**: IEEE Transactions on Multimedia, 2018
- **Journal**: None
- **Summary**: In this work, we present the depth-adaptive deep neural network using a depth map for semantic segmentation. Typical deep neural networks receive inputs at the predetermined locations regardless of the distance from the camera. This fixed receptive field presents a challenge to generalize the features of objects at various distances in neural networks. Specifically, the predetermined receptive fields are too small at a short distance, and vice versa. To overcome this challenge, we develop a neural network which is able to adapt the receptive field not only for each layer but also for each neuron at the spatial location. To adjust the receptive field, we propose the depth-adaptive multiscale (DaM) convolution layer consisting of the adaptive perception neuron and the in-layer multiscale neuron. The adaptive perception neuron is to adjust the receptive field at each spatial location using the corresponding depth information. The in-layer multiscale neuron is to apply the different size of the receptive field at each feature space to learn features at multiple scales. The proposed DaM convolution is applied to two fully convolutional neural networks. We demonstrate the effectiveness of the proposed neural networks on the publicly available RGB-D dataset for semantic segmentation and the novel hand segmentation dataset for hand-object interaction. The experimental results show that the proposed method outperforms the state-of-the-art methods without any additional layers or pre/post-processing.



