# Arxiv Papers in cs.CV on 2017-08-15
### Deep Edge-Aware Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.04366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04366v1)
- **Published**: 2017-08-15 00:35:55+00:00
- **Updated**: 2017-08-15 00:35:55+00:00
- **Authors**: Jing Zhang, Yuchao Dai, Fatih Porikli, Mingyi He
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: There has been profound progress in visual saliency thanks to the deep learning architectures, however, there still exist three major challenges that hinder the detection performance for scenes with complex compositions, multiple salient objects, and salient objects of diverse scales. In particular, output maps of the existing methods remain low in spatial resolution causing blurred edges due to the stride and pooling operations, networks often neglect descriptive statistical and handcrafted priors that have potential to complement saliency detection results, and deep features at different layers stay mainly desolate waiting to be effectively fused to handle multi-scale salient objects. In this paper, we tackle these issues by a new fully convolutional neural network that jointly learns salient edges and saliency labels in an end-to-end fashion. Our framework first employs convolutional layers that reformulate the detection task as a dense labeling problem, then integrates handcrafted saliency features in a hierarchical manner into lower and higher levels of the deep network to leverage available information for multi-scale response, and finally refines the saliency map through dilated convolutions by imposing context. In this way, the salient edge priors are efficiently incorporated and the output resolution is significantly improved while keeping the memory requirements low, leading to cleaner and sharper object boundaries. Extensive experimental analyses on ten benchmarks demonstrate that our framework achieves consistently superior performance and attains robustness for complex scenes in comparison to the very recent state-of-the-art approaches.



### Dockerface: an Easy to Install and Use Faster R-CNN Face Detector in a Docker Container
- **Arxiv ID**: http://arxiv.org/abs/1708.04370v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04370v2)
- **Published**: 2017-08-15 00:55:57+00:00
- **Updated**: 2018-04-05 05:32:40+00:00
- **Authors**: Nataniel Ruiz, James M. Rehg
- **Comment**: None
- **Journal**: None
- **Summary**: Face detection is a very important task and a necessary pre-processing step for many applications such as facial landmark detection, pose estimation, sentiment analysis and face recognition. Not only is face detection an important pre-processing step in computer vision applications but also in computational psychology, behavioral imaging and other fields where researchers might not be initiated in computer vision frameworks and state-of-the-art detection applications. A large part of existing research that includes face detection as a pre-processing step uses existing out-of-the-box detectors such as the HoG-based dlib and the OpenCV Haar face detector which are no longer state-of-the-art - they are primarily used because of their ease of use and accessibility. We introduce Dockerface, a very accurate Faster R-CNN face detector in a Docker container which requires no training and is easy to install and use.



### Monocular Dense 3D Reconstruction of a Complex Dynamic Scene from Two Perspective Frames
- **Arxiv ID**: http://arxiv.org/abs/1708.04398v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04398v2)
- **Published**: 2017-08-15 05:33:33+00:00
- **Updated**: 2017-12-20 14:42:02+00:00
- **Authors**: Suryansh Kumar, Yuchao Dai, Hongdong Li
- **Comment**: International Conference on Computer Vision (ICCV) 2017 pp: 4649-4657
- **Journal**: None
- **Summary**: This paper proposes a new approach for monocular dense 3D reconstruction of a complex dynamic scene from two perspective frames. By applying superpixel over-segmentation to the image, we model a generically dynamic (hence non-rigid) scene with a piecewise planar and rigid approximation. In this way, we reduce the dynamic reconstruction problem to a "3D jigsaw puzzle" problem which takes pieces from an unorganized "soup of superpixels". We show that our method provides an effective solution to the inherent relative scale ambiguity in structure-from-motion. Since our method does not assume a template prior, or per-object segmentation, or knowledge about the rigidity of the dynamic scene, it is applicable to a wide range of scenarios. Extensive experiments on both synthetic and real monocular sequences demonstrate the superiority of our method compared with the state-of-the-art methods.



### Bringing Background into the Foreground: Making All Classes Equal in Weakly-supervised Video Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1708.04400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04400v1)
- **Published**: 2017-08-15 05:36:52+00:00
- **Updated**: 2017-08-15 05:36:52+00:00
- **Authors**: Fatemeh Sadat Saleh, Mohammad Sadegh Aliakbarian, Mathieu Salzmann, Lars Petersson, Jose M. Alvarez
- **Comment**: 11 pages, 4 figures, 7 tables, Accepted in ICCV 2017
- **Journal**: None
- **Summary**: Pixel-level annotations are expensive and time-consuming to obtain. Hence, weak supervision using only image tags could have a significant impact in semantic segmentation. Recent years have seen great progress in weakly-supervised semantic segmentation, whether from a single image or from videos. However, most existing methods are designed to handle a single background class. In practical applications, such as autonomous navigation, it is often crucial to reason about multiple background classes. In this paper, we introduce an approach to doing so by making use of classifier heatmaps. We then develop a two-stream deep architecture that jointly leverages appearance and motion, and design a loss based on our heatmaps to train it. Our experiments demonstrate the benefits of our classifier heatmaps and of our two-stream architecture on challenging urban scene datasets and on the YouTube-Objects benchmark, where we obtain state-of-the-art results.



### Knock-Knock: Acoustic Object Recognition by using Stacked Denoising Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1708.04432v1
- **DOI**: 10.1016/j.neucom.2017.03.014
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04432v1)
- **Published**: 2017-08-15 08:40:02+00:00
- **Updated**: 2017-08-15 08:40:02+00:00
- **Authors**: Shan Luo, Leqi Zhu, Kaspar Althoefer, Hongbin Liu
- **Comment**: 6 pages, 10 figures, Neurocomputing
- **Journal**: None
- **Summary**: This paper presents a successful application of deep learning for object recognition based on acoustic data. The shortcomings of previously employed approaches where handcrafted features describing the acoustic data are being used, include limiting the capability of the found representation to be widely applicable and facing the risk of capturing only insignificant characteristics for a task. In contrast, there is no need to define the feature representation format when using multilayer/deep learning architecture methods: features can be learned from raw sensor data without defining discriminative characteristics a-priori. In this paper, stacked denoising autoencoders are applied to train a deep learning model. Knocking each object in our test set 120 times with a marker pen to obtain the auditory data, thirty different objects were successfully classified in our experiment and each object was knocked 120 times by a marker pen to obtain the auditory data. By employing the proposed deep learning framework, a high accuracy of 91.50% was achieved. A traditional method using handcrafted features with a shallow classifier was taken as a benchmark and the attained recognition rate was only 58.22%. Interestingly, a recognition rate of 82.00% was achieved when using a shallow classifier with raw acoustic data as input. In addition, we could show that the time taken to classify one object using deep learning was far less (by a factor of more than 6) than utilizing the traditional method. It was also explored how different model parameters in our deep architecture affect the recognition performance.



### Learning with Rethinking: Recurrently Improving Convolutional Neural Networks through Feedback
- **Arxiv ID**: http://arxiv.org/abs/1708.04483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04483v1)
- **Published**: 2017-08-15 13:09:52+00:00
- **Updated**: 2017-08-15 13:09:52+00:00
- **Authors**: Xin Li, Zequn Jie, Jiashi Feng, Changsong Liu, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed the great success of convolutional neural network (CNN) based models in the field of computer vision. CNN is able to learn hierarchically abstracted features from images in an end-to-end training manner. However, most of the existing CNN models only learn features through a feedforward structure and no feedback information from top to bottom layers is exploited to enable the networks to refine themselves. In this paper, we propose a "Learning with Rethinking" algorithm. By adding a feedback layer and producing the emphasis vector, the model is able to recurrently boost the performance based on previous prediction. Particularly, it can be employed to boost any pre-trained models. This algorithm is tested on four object classification benchmark datasets: CIFAR-100, CIFAR-10, MNIST-background-image and ILSVRC-2012 dataset. These results have demonstrated the advantage of training CNN models with the proposed feedback mechanism.



### Pathological Pulmonary Lobe Segmentation from CT Images using Progressive Holistically Nested Neural Networks and Random Walker
- **Arxiv ID**: http://arxiv.org/abs/1708.04503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04503v1)
- **Published**: 2017-08-15 14:11:26+00:00
- **Updated**: 2017-08-15 14:11:26+00:00
- **Authors**: Kevin George, Adam P. Harrison, Dakai Jin, Ziyue Xu, Daniel J. Mollura
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic pathological pulmonary lobe segmentation(PPLS) enables regional analyses of lung disease, a clinically important capability. Due to often incomplete lobe boundaries, PPLS is difficult even for experts, and most prior art requires inference from contextual information. To address this, we propose a novel PPLS method that couples deep learning with the random walker (RW) algorithm. We first employ the recent progressive holistically-nested network (P-HNN) model to identify potential lobar boundaries, then generate final segmentations using a RW that is seeded and weighted by the P-HNN output. We are the first to apply deep learning to PPLS. The advantages are independence from prior airway/vessel segmentations, increased robustness in diseased lungs, and methodological simplicity that does not sacrifice accuracy. Our method posts a high mean Jaccard score of 0.888$\pm$0.164 on a held-out set of 154 CT scans from lung-disease patients, while also significantly (p < 0.001) outperforming a state-of-the-art method.



### DesnowNet: Context-Aware Deep Network for Snow Removal
- **Arxiv ID**: http://arxiv.org/abs/1708.04512v1
- **DOI**: 10.1109/TIP.2018.2806202
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04512v1)
- **Published**: 2017-08-15 14:33:52+00:00
- **Updated**: 2017-08-15 14:33:52+00:00
- **Authors**: Yun-Fu Liu, Da-Wei Jaw, Shih-Chia Huang, Jenq-Neng Hwang
- **Comment**: None
- **Journal**: None
- **Summary**: Existing learning-based atmospheric particle-removal approaches such as those used for rainy and hazy images are designed with strong assumptions regarding spatial frequency, trajectory, and translucency. However, the removal of snow particles is more complicated because it possess the additional attributes of particle size and shape, and these attributes may vary within a single image. Currently, hand-crafted features are still the mainstream for snow removal, making significant generalization difficult to achieve. In response, we have designed a multistage network codenamed DesnowNet to in turn deal with the removal of translucent and opaque snow particles. We also differentiate snow into attributes of translucency and chromatic aberration for accurate estimation. Moreover, our approach individually estimates residual complements of the snow-free images to recover details obscured by opaque snow. Additionally, a multi-scale design is utilized throughout the entire network to model the diversity of snow. As demonstrated in experimental results, our approach outperforms state-of-the-art learning-based atmospheric phenomena removal methods and one semantic segmentation baseline on the proposed Snow100K dataset in both qualitative and quantitative comparisons. The results indicate our network would benefit applications involving computer vision and graphics.



### Improved Regularization of Convolutional Neural Networks with Cutout
- **Arxiv ID**: http://arxiv.org/abs/1708.04552v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04552v2)
- **Published**: 2017-08-15 15:21:53+00:00
- **Updated**: 2017-11-29 14:51:40+00:00
- **Authors**: Terrance DeVries, Graham W. Taylor
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56%, 15.20%, and 1.30% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout



### Segmentation-Aware Convolutional Networks Using Local Attention Masks
- **Arxiv ID**: http://arxiv.org/abs/1708.04607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04607v1)
- **Published**: 2017-08-15 17:55:36+00:00
- **Updated**: 2017-08-15 17:55:36+00:00
- **Authors**: Adam W. Harley, Konstantinos G. Derpanis, Iasonas Kokkinos
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an approach to integrate segmentation information within a convolutional neural network (CNN). This counter-acts the tendency of CNNs to smooth information across regions and increases their spatial precision. To obtain segmentation information, we set up a CNN to provide an embedding space where region co-membership can be estimated based on Euclidean distance. We use these embeddings to compute a local attention mask relative to every neuron position. We incorporate such masks in CNNs and replace the convolution operation with a "segmentation-aware" variant that allows a neuron to selectively attend to inputs coming from its own region. We call the resulting network a segmentation-aware CNN because it adapts its filters at each image point according to local segmentation cues. We demonstrate the merit of our method on two widely different dense prediction tasks, that involve classification (semantic segmentation) and regression (optical flow). Our results show that in semantic segmentation we can match the performance of DenseCRFs while being faster and simpler, and in optical flow we obtain clearly sharper responses than networks that do not use local attention masks. In both cases, segmentation-aware convolution yields systematic improvements over strong baselines. Source code for this work is available online at http://cs.cmu.edu/~aharley/segaware.



### Convolutional Neural Networks for Non-iterative Reconstruction of Compressively Sensed Images
- **Arxiv ID**: http://arxiv.org/abs/1708.04669v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04669v2)
- **Published**: 2017-08-15 20:14:17+00:00
- **Updated**: 2017-08-17 00:18:44+00:00
- **Authors**: Suhas Lohit, Kuldeep Kulkarni, Ronan Kerviche, Pavan Turaga, Amit Ashok
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional algorithms for compressive sensing recovery are computationally expensive and are ineffective at low measurement rates. In this work, we propose a data driven non-iterative algorithm to overcome the shortcomings of earlier iterative algorithms. Our solution, ReconNet, is a deep neural network, whose parameters are learned end-to-end to map block-wise compressive measurements of the scene to the desired image blocks. Reconstruction of an image becomes a simple forward pass through the network and can be done in real-time. We show empirically that our algorithm yields reconstructions with higher PSNRs compared to iterative algorithms at low measurement rates and in presence of measurement noise. We also propose a variant of ReconNet which uses adversarial loss in order to further improve reconstruction quality. We discuss how adding a fully connected layer to the existing ReconNet architecture allows for jointly learning the measurement matrix and the reconstruction algorithm in a single network. Experiments on real data obtained from a block compressive imager show that our networks are robust to unseen sensor noise. Finally, through an experiment in object tracking, we show that even at very low measurement rates, reconstructions using our algorithm possess rich semantic content that can be used for high level inference.



### Sequence-to-Label Script Identification for Multilingual OCR
- **Arxiv ID**: http://arxiv.org/abs/1708.04671v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.7.5
- **Links**: [PDF](http://arxiv.org/pdf/1708.04671v2)
- **Published**: 2017-08-15 20:14:51+00:00
- **Updated**: 2017-08-17 20:20:25+00:00
- **Authors**: Yasuhisa Fujii, Karel Driesen, Jonathan Baccash, Ash Hurst, Ashok C. Popat
- **Comment**: ICDAR2017, The 14th IAPR International Conference on Document
  Analysis and Recognition, Kyoto, Japan
- **Journal**: None
- **Summary**: We describe a novel line-level script identification method. Previous work repurposed an OCR model generating per-character script codes, counted to obtain line-level script identification. This has two shortcomings. First, as a sequence-to-sequence model it is more complex than necessary for the sequence-to-label problem of line script identification. This makes it harder to train and inefficient to run. Second, the counting heuristic may be suboptimal compared to a learned model. Therefore we reframe line script identification as a sequence-to-label problem and solve it using two components, trained end-toend: Encoder and Summarizer. The encoder converts a line image into a feature sequence. The summarizer aggregates the sequence to classify the line. We test various summarizers with identical inception-style convolutional networks as encoders. Experiments on scanned books and photos containing 232 languages in 30 scripts show 16% reduction of script identification error rate compared to the baseline. This improved script identification reduces the character error rate attributable to script misidentification by 33%.



### VQS: Linking Segmentations to Questions and Answers for Supervised Attention in VQA and Question-Focused Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1708.04686v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.04686v1)
- **Published**: 2017-08-15 20:47:02+00:00
- **Updated**: 2017-08-15 20:47:02+00:00
- **Authors**: Chuang Gan, Yandong Li, Haoxiang Li, Chen Sun, Boqing Gong
- **Comment**: To appear on ICCV 2017
- **Journal**: None
- **Summary**: Rich and dense human labeled datasets are among the main enabling factors for the recent advance on vision-language understanding. Many seemingly distant annotations (e.g., semantic segmentation and visual question answering (VQA)) are inherently connected in that they reveal different levels and perspectives of human understandings about the same visual scenes --- and even the same set of images (e.g., of COCO). The popularity of COCO correlates those annotations and tasks. Explicitly linking them up may significantly benefit both individual tasks and the unified vision and language modeling. We present the preliminary work of linking the instance segmentations provided by COCO to the questions and answers (QAs) in the VQA dataset, and name the collected links visual questions and segmentation answers (VQS). They transfer human supervision between the previously separate tasks, offer more effective leverage to existing problems, and also open the door for new research problems and models. We study two applications of the VQS data in this paper: supervised attention for VQA and a novel question-focused semantic segmentation task. For the former, we obtain state-of-the-art results on the VQA real multiple-choice task by simply augmenting the multilayer perceptrons with some attention features that are learned using the segmentation-QA links as explicit supervision. To put the latter in perspective, we study two plausible methods and compare them to an oracle method assuming that the instance segmentations are given at the test stage.



### GANs for Biological Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1708.04692v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.04692v2)
- **Published**: 2017-08-15 21:04:11+00:00
- **Updated**: 2017-09-12 09:18:24+00:00
- **Authors**: Anton Osokin, Anatole Chessel, Rafael E. Carazo Salas, Federico Vaggi
- **Comment**: The paper appearing at the International Conference on Computer
  Vision (ICCV) 2017 + its supplementary materials
- **Journal**: None
- **Summary**: In this paper, we propose a novel application of Generative Adversarial Networks (GAN) to the synthesis of cells imaged by fluorescence microscopy. Compared to natural images, cells tend to have a simpler and more geometric global structure that facilitates image generation. However, the correlation between the spatial pattern of different fluorescent proteins reflects important biological functions, and synthesized images have to capture these relationships to be relevant for biological applications. We adapt GANs to the task at hand and propose new models with casual dependencies between image channels that can generate multi-channel images, which would be impossible to obtain experimentally. We evaluate our approach using two independent techniques and compare it against sensible baselines. Finally, we demonstrate that by interpolating across the latent space we can mimic the known changes in protein localization that occur through time during the cell cycle, allowing us to predict temporal evolution from static images.



