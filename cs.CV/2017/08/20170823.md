# Arxiv Papers in cs.CV on 2017-08-23
### ElasticPlay: Interactive Video Summarization with Dynamic Time Budgets
- **Arxiv ID**: http://arxiv.org/abs/1708.06858v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1708.06858v1)
- **Published**: 2017-08-23 00:25:13+00:00
- **Updated**: 2017-08-23 00:25:13+00:00
- **Authors**: Haojian Jin, Yale Song, Koji Yatani
- **Comment**: ACM Multimedia 2017 preprint
- **Journal**: None
- **Summary**: Video consumption is being shifted from sit-and-watch to selective skimming. Existing video player interfaces, however, only provide indirect manipulation to support this emerging behavior. Video summarization alleviates this issue to some extent, shortening a video based on the desired length of a summary as an input variable. But an optimal length of a summarized video is often not available in advance. Moreover, the user cannot edit the summary once it is produced, limiting its practical applications. We argue that video summarization should be an interactive, mixed-initiative process in which users have control over the summarization procedure while algorithms help users achieve their goal via video understanding. In this paper, we introduce ElasticPlay, a mixed-initiative approach that combines an advanced video summarization technique with direct interface manipulation to help users control the video summarization process. Users can specify a time budget for the remaining content while watching a video; our system then immediately updates the playback plan using our proposed cut-and-forward algorithm, determining which parts to skip or to fast-forward. This interactive process allows users to fine-tune the summarization result with immediate feedback. We show that our system outperforms existing video summarization techniques on the TVSum50 dataset. We also report two lab studies (22 participants) and a Mechanical Turk deployment study (60 participants), and show that the participants responded favorably to ElasticPlay.



### A Type II Fuzzy Entropy Based Multi-Level Image Thresholding Using Adaptive Plant Propagation Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1708.09461v1
- **DOI**: 10.17605/OSF.IO/5KQZD
- **Categories**: **cs.CV**, math.OC, physics.data-an, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/1708.09461v1)
- **Published**: 2017-08-23 09:43:00+00:00
- **Updated**: 2017-08-23 09:43:00+00:00
- **Authors**: Sayan Nag
- **Comment**: 12 Pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1708.07040
- **Journal**: None
- **Summary**: One of the most straightforward, direct and efficient approaches to Image Segmentation is Image Thresholding. Multi-level Image Thresholding is an essential viewpoint in many image processing and Pattern Recognition based real-time applications which can effectively and efficiently classify the pixels into various groups denoting multiple regions in an Image. Thresholding based Image Segmentation using fuzzy entropy combined with intelligent optimization approaches are commonly used direct methods to properly identify the thresholds so that they can be used to segment an Image accurately. In this paper a novel approach for multi-level image thresholding is proposed using Type II Fuzzy sets combined with Adaptive Plant Propagation Algorithm (APPA). Obtaining the optimal thresholds for an image by maximizing the entropy is extremely tedious and time consuming with increase in the number of thresholds. Hence, Adaptive Plant Propagation Algorithm (APPA), a memetic algorithm based on plant intelligence, is used for fast and efficient selection of optimal thresholds. This fact is reasonably justified by comparing the accuracy of the outcomes and computational time consumed by other modern state-of-the-art algorithms such as Particle Swarm Optimization (PSO), Gravitational Search Algorithm (GSA) and Genetic Algorithm (GA).



### Pose Estimation using Local Structure-Specific Shape and Appearance Context
- **Arxiv ID**: http://arxiv.org/abs/1708.06963v1
- **DOI**: 10.1109/ICRA.2013.6630856
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06963v1)
- **Published**: 2017-08-23 11:46:30+00:00
- **Updated**: 2017-08-23 11:46:30+00:00
- **Authors**: Anders Glent Buch, Dirk Kraft, Joni-Kristian Kamarainen, Henrik Gordon Petersen, Norbert Krüger
- **Comment**: None
- **Journal**: 2013 IEEE International Conference on Robotics and Automation
  (ICRA)
- **Summary**: We address the problem of estimating the alignment pose between two models using structure-specific local descriptors. Our descriptors are generated using a combination of 2D image data and 3D contextual shape data, resulting in a set of semi-local descriptors containing rich appearance and shape information for both edge and texture structures. This is achieved by defining feature space relations which describe the neighborhood of a descriptor. By quantitative evaluations, we show that our descriptors provide high discriminative power compared to state of the art approaches. In addition, we show how to utilize this for the estimation of the alignment pose between two point sets. We present experiments both in controlled and real-life scenarios to validate our approach.



### In search of inliers: 3d correspondence by local and global voting
- **Arxiv ID**: http://arxiv.org/abs/1708.06966v1
- **DOI**: 10.1109/CVPR.2014.266
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06966v1)
- **Published**: 2017-08-23 11:54:47+00:00
- **Updated**: 2017-08-23 11:54:47+00:00
- **Authors**: Anders Glent Buch, Yang Yang, Norbert Krüger, Henrik Gordon Petersen
- **Comment**: None
- **Journal**: 2014 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)
- **Summary**: We present a method for finding correspondence between 3D models. From an initial set of feature correspondences, our method uses a fast voting scheme to separate the inliers from the outliers. The novelty of our method lies in the use of a combination of local and global constraints to determine if a vote should be cast. On a local scale, we use simple, low-level geometric invariants. On a global scale, we apply covariant constraints for finding compatible correspondences. We guide the sampling for collecting voters by downward dependencies on previous voting stages. All of this together results in an accurate matching procedure. We evaluate our algorithm by controlled and comparative testing on different datasets, giving superior performance compared to state of the art methods. In a final experiment, we apply our method for 3D object detection, showing potential use of our method within higher-level vision.



### Exploiting Convolution Filter Patterns for Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.06973v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06973v1)
- **Published**: 2017-08-23 12:13:30+00:00
- **Updated**: 2017-08-23 12:13:30+00:00
- **Authors**: Mehmet Aygün, Yusuf Aytar, Hazım Kemal Ekenel
- **Comment**: Accepted to TASK-CV Workshop at ICCV 2017
- **Journal**: None
- **Summary**: In this paper, we introduce a new regularization technique for transfer learning. The aim of the proposed approach is to capture statistical relationships among convolution filters learned from a well-trained network and transfer this knowledge to another network. Since convolution filters of the prevalent deep Convolutional Neural Network (CNN) models share a number of similar patterns, in order to speed up the learning procedure, we capture such correlations by Gaussian Mixture Models (GMMs) and transfer them using a regularization term. We have conducted extensive experiments on the CIFAR10, Places2, and CMPlaces datasets to assess generalizability, task transferability, and cross-model transferability of the proposed approach, respectively. The experimental results show that the feature representations have efficiently been learned and transferred through the proposed statistical regularization scheme. Moreover, our method is an architecture independent approach, which is applicable for a variety of CNN architectures.



### Generating Visual Representations for Zero-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/1708.06975v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.06975v3)
- **Published**: 2017-08-23 12:23:51+00:00
- **Updated**: 2017-12-11 16:16:01+00:00
- **Authors**: Maxime Bucher, Stéphane Herbin, Frédéric Jurie
- **Comment**: None
- **Journal**: International Conference on Computer Vision (ICCV) Workshops :
  TASK-CV: Transferring and Adapting Source Knowledge in Computer Vision, Oct
  2017, venise, Italy. International Conference on Computer Vision (ICCV)
  Workshops, 2017
- **Summary**: This paper addresses the task of learning an image clas-sifier when some categories are defined by semantic descriptions only (e.g. visual attributes) while the others are defined by exemplar images as well. This task is often referred to as the Zero-Shot classification task (ZSC). Most of the previous methods rely on learning a common embedding space allowing to compare visual features of unknown categories with semantic descriptions. This paper argues that these approaches are limited as i) efficient discrimi-native classifiers can't be used ii) classification tasks with seen and unseen categories (Generalized Zero-Shot Classification or GZSC) can't be addressed efficiently. In contrast , this paper suggests to address ZSC and GZSC by i) learning a conditional generator using seen classes ii) generate artificial training examples for the categories without exemplars. ZSC is then turned into a standard supervised learning problem. Experiments with 4 generative models and 5 datasets experimentally validate the approach, giving state-of-the-art results on both ZSC and GZSC.



### A simple expression for the map of Asplund's distances with the multiplicative Logarithmic Image Processing (LIP) law
- **Arxiv ID**: http://arxiv.org/abs/1708.08992v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08992v2)
- **Published**: 2017-08-23 12:24:41+00:00
- **Updated**: 2018-01-25 07:54:05+00:00
- **Authors**: Guillaume Noyel, Michel Jourlin
- **Comment**: Accepted to the 12th European Congress for Stereology and Image
  Analysis 2017, Kaiserslautern, Germany, September 11-14, 2017
- **Journal**: 12th European Congress for Stereology and Image Analysis 2017, Sep
  2017, Kaiserslautern, Germany. Proceedings of the 12th European Congress for
  Stereology and Image Analysis 2017.
  http://www.mathematik.uni-kl.de/events/ecsia-2017/12th-ecsia-2017
- **Summary**: We introduce a simple expression for the map of Asplund's distances with the multiplicative Logarithmic Image Processing (LIP) law. It is a difference between a morphological dilation and a morphological erosion with an additive structuring function which corresponds to a morphological gradient.



### Incremental Learning of Object Detectors without Catastrophic Forgetting
- **Arxiv ID**: http://arxiv.org/abs/1708.06977v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06977v1)
- **Published**: 2017-08-23 12:33:44+00:00
- **Updated**: 2017-08-23 12:33:44+00:00
- **Authors**: Konstantin Shmelkov, Cordelia Schmid, Karteek Alahari
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: Despite their success for object detection, convolutional neural networks are ill-equipped for incremental learning, i.e., adapting the original model trained on a set of classes to additionally detect objects of new classes, in the absence of the initial training data. They suffer from "catastrophic forgetting" - an abrupt degradation of performance on the original set of classes, when the training objective is adapted to the new classes. We present a method to address this issue, and learn object detectors incrementally, when neither the original training data nor annotations for the original classes in the new training set are available. The core of our proposed solution is a loss function to balance the interplay between predictions on the new classes and a new distillation loss which minimizes the discrepancy between responses for old classes from the original and the updated networks. This incremental learning can be performed multiple times, for a new set of classes in each step, with a moderate drop in performance compared to the baseline network trained on the ensemble of data. We present object detection results on the PASCAL VOC 2007 and COCO datasets, along with a detailed empirical analysis of the approach.



### The Unconstrained Ear Recognition Challenge
- **Arxiv ID**: http://arxiv.org/abs/1708.06997v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.06997v2)
- **Published**: 2017-08-23 13:45:55+00:00
- **Updated**: 2019-02-01 07:52:53+00:00
- **Authors**: Žiga Emeršič, Dejan Štepec, Vitomir Štruc, Peter Peer, Anjith George, Adil Ahmad, Elshibani Omar, Terrance E. Boult, Reza Safdari, Yuxiang Zhou, Stefanos Zafeiriou, Dogucan Yaman, Fevziye I. Eyiokur, Hazim K. Ekenel
- **Comment**: International Joint Conference on Biometrics 2017
- **Journal**: None
- **Summary**: In this paper we present the results of the Unconstrained Ear Recognition Challenge (UERC), a group benchmarking effort centered around the problem of person recognition from ear images captured in uncontrolled conditions. The goal of the challenge was to assess the performance of existing ear recognition techniques on a challenging large-scale dataset and identify open problems that need to be addressed in the future. Five groups from three continents participated in the challenge and contributed six ear recognition techniques for the evaluation, while multiple baselines were made available for the challenge by the UERC organizers. A comprehensive analysis was conducted with all participating approaches addressing essential research questions pertaining to the sensitivity of the technology to head rotation, flipping, gallery size, large-scale recognition and others. The top performer of the UERC was found to ensure robust performance on a smaller part of the dataset (with 180 subjects) regardless of image characteristics, but still exhibited a significant performance drop when the entire dataset comprising 3,704 subjects was used for testing.



### Statistical Selection of CNN-Based Audiovisual Features for Instantaneous Estimation of Human Emotional States
- **Arxiv ID**: http://arxiv.org/abs/1708.07021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07021v1)
- **Published**: 2017-08-23 14:27:19+00:00
- **Updated**: 2017-08-23 14:27:19+00:00
- **Authors**: Ramesh Basnet, Mohammad Tariqul Islam, Tamanna Howlader, S. M. Mahbubur Rahman, Dimitrios Hatzinakos
- **Comment**: Accepted in International Conference on new Trends in Computer
  Sciences (ICTCS), Amman, Jordan, 2017
- **Journal**: None
- **Summary**: Automatic prediction of continuous-level emotional state requires selection of suitable affective features to develop a regression system based on supervised machine learning. This paper investigates the performance of features statistically learned using convolutional neural networks for instantaneously predicting the continuous dimensions of emotional states. Features with minimum redundancy and maximum relevancy are chosen by using the mutual information-based selection process. The performance of frame-by-frame prediction of emotional state using the moderate length features as proposed in this paper is evaluated on spontaneous and naturalistic human-human conversation of RECOLA database. Experimental results show that the proposed model can be used for instantaneous prediction of emotional state with an accuracy higher than traditional audio or video features that are used for affective computation.



### CNN-Based Prediction of Frame-Level Shot Importance for Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1708.07023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07023v1)
- **Published**: 2017-08-23 14:35:10+00:00
- **Updated**: 2017-08-23 14:35:10+00:00
- **Authors**: Mohaiminul Al Nahian, A. S. M. Iftekhar, Mohammad Tariqul Islam, S. M. Mahbubur Rahman, Dimitrios Hatzinakos
- **Comment**: Accepted in International Conference on new Trends in Computer
  Sciences (ICTCS), Amman-Jordan, 2017
- **Journal**: None
- **Summary**: In the Internet, ubiquitous presence of redundant, unedited, raw videos has made video summarization an important problem. Traditional methods of video summarization employ a heuristic set of hand-crafted features, which in many cases fail to capture subtle abstraction of a scene. This paper presents a deep learning method that maps the context of a video to the importance of a scene similar to that is perceived by humans. In particular, a convolutional neural network (CNN)-based architecture is proposed to mimic the frame-level shot importance for user-oriented video summarization. The weights and biases of the CNN are trained extensively through off-line processing, so that it can provide the importance of a frame of an unseen video almost instantaneously. Experiments on estimating the shot importance is carried out using the publicly available database TVSum50. It is shown that the performance of the proposed network is substantially better than that of commonly referred feature-based methods for estimating the shot importance in terms of mean absolute error, absolute error variance, and relative F-measure.



### Fast single image super-resolution based on sigmoid transformation
- **Arxiv ID**: http://arxiv.org/abs/1708.07029v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07029v3)
- **Published**: 2017-08-23 14:46:26+00:00
- **Updated**: 2017-11-05 04:42:11+00:00
- **Authors**: Longguang Wang, Zaiping Lin, Jinyan Gao, Xinpu Deng, Wei An
- **Comment**: None
- **Journal**: None
- **Summary**: Single image super-resolution aims to generate a high-resolution image from a single low-resolution image, which is of great significance in extensive applications. As an ill-posed problem, numerous methods have been proposed to reconstruct the missing image details based on exemplars or priors. In this paper, we propose a fast and simple single image super-resolution strategy utilizing patch-wise sigmoid transformation as an imposed sharpening regularization term in the reconstruction, which realizes amazing reconstruction performance. Extensive experiments compared with other state-of-the-art approaches demonstrate the superior effectiveness and efficiency of the proposed algorithm.



### Application of a Convolutional Neural Network for image classification to the analysis of collisions in High Energy Physics
- **Arxiv ID**: http://arxiv.org/abs/1708.07034v1
- **DOI**: None
- **Categories**: **cs.CV**, hep-ex
- **Links**: [PDF](http://arxiv.org/pdf/1708.07034v1)
- **Published**: 2017-08-23 15:03:08+00:00
- **Updated**: 2017-08-23 15:03:08+00:00
- **Authors**: Celia Fernández Madrazo, Ignacio Heredia Cacha, Lara Lloret Iglesias, Jesús Marco de Lucas
- **Comment**: 14 pages, 8 figures, educational
- **Journal**: None
- **Summary**: The application of deep learning techniques using convolutional neural networks to the classification of particle collisions in High Energy Physics is explored. An intuitive approach to transform physical variables, like momenta of particles and jets, into a single image that captures the relevant information, is proposed. The idea is tested using a well known deep learning framework on a simulation dataset, including leptonic ttbar events and the corresponding background at 7 TeV from the CMS experiment at LHC, available as Open Data. This initial test shows competitive results when compared to more classical approaches, like those using feedforward neural networks.



### Non-linear Convolution Filters for CNN-based Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.07038v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1708.07038v1)
- **Published**: 2017-08-23 15:07:35+00:00
- **Updated**: 2017-08-23 15:07:35+00:00
- **Authors**: Georgios Zoumpourlis, Alexandros Doumanoglou, Nicholas Vretos, Petros Daras
- **Comment**: 9 pages, 5 figures, code link, ICCV 2017
- **Journal**: None
- **Summary**: During the last years, Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in image classification. Their architectures have largely drawn inspiration by models of the primate visual system. However, while recent research results of neuroscience prove the existence of non-linear operations in the response of complex visual cells, little effort has been devoted to extend the convolution technique to non-linear forms. Typical convolutional layers are linear systems, hence their expressiveness is limited. To overcome this, various non-linearities have been used as activation functions inside CNNs, while also many pooling strategies have been applied. We address the issue of developing a convolution method in the context of a computational model of the visual cortex, exploring quadratic forms through the Volterra kernels. Such forms, constituting a more rich function space, are used as approximations of the response profile of visual cells. Our proposed second-order convolution is tested on CIFAR-10 and CIFAR-100. We show that a network which combines linear and non-linear filters in its convolutional layers, can outperform networks that use standard linear filters with the same architecture, yielding results competitive with the state-of-the-art on these datasets.



### Single Reference Image based Scene Relighting via Material Guided Filtering
- **Arxiv ID**: http://arxiv.org/abs/1708.07066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07066v1)
- **Published**: 2017-08-23 15:51:47+00:00
- **Updated**: 2017-08-23 15:51:47+00:00
- **Authors**: Xin Jin, Yannan Li, Ningning Liu, Xiaodong Li, Xianggang Jiang, Chaoen Xiao, Shiming Ge
- **Comment**: Best student paper award of ISAIR 2017 (International Symposium on
  Artificial Intelligence and Robotics), recommended to Optics and Laser
  Technology
- **Journal**: None
- **Summary**: Image relighting is to change the illumination of an image to a target illumination effect without known the original scene geometry, material information and illumination condition. We propose a novel outdoor scene relighting method, which needs only a single reference image and is based on material constrained layer decomposition. Firstly, the material map is extracted from the input image. Then, the reference image is warped to the input image through patch match based image warping. Lastly, the input image is relit using material constrained layer decomposition. The experimental results reveal that our method can produce similar illumination effect as that of the reference image on the input image using only a single reference image.



### Predicting Aesthetic Score Distribution through Cumulative Jensen-Shannon Divergence
- **Arxiv ID**: http://arxiv.org/abs/1708.07089v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07089v2)
- **Published**: 2017-08-23 16:37:50+00:00
- **Updated**: 2017-11-20 20:12:21+00:00
- **Authors**: Xin Jin, Le Wu, Xiaodong Li, Siyu Chen, Siwei Peng, Jingying Chi, Shiming Ge, Chenggen Song, Geng Zhao
- **Comment**: AAAI Conference on Artificial Intelligence (AAAI), New Orleans,
  Louisiana, USA. 2-7 Feb. 2018
- **Journal**: None
- **Summary**: Aesthetic quality prediction is a challenging task in the computer vision community because of the complex interplay with semantic contents and photographic technologies. Recent studies on the powerful deep learning based aesthetic quality assessment usually use a binary high-low label or a numerical score to represent the aesthetic quality. However the scalar representation cannot describe well the underlying varieties of the human perception of aesthetics. In this work, we propose to predict the aesthetic score distribution (i.e., a score distribution vector of the ordinal basic human ratings) using Deep Convolutional Neural Network (DCNN). Conventional DCNNs which aim to minimize the difference between the predicted scalar numbers or vectors and the ground truth cannot be directly used for the ordinal basic rating distribution. Thus, a novel CNN based on the Cumulative distribution with Jensen-Shannon divergence (CJS-CNN) is presented to predict the aesthetic score distribution of human ratings, with a new reliability-sensitive learning method based on the kurtosis of the score distribution, which eliminates the requirement of the original full data of human ratings (without normalization). Experimental results on large scale aesthetic dataset demonstrate the effectiveness of our introduced CJS-CNN in this task.



### Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates
- **Arxiv ID**: http://arxiv.org/abs/1708.07120v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.07120v3)
- **Published**: 2017-08-23 17:51:57+00:00
- **Updated**: 2018-05-17 17:40:34+00:00
- **Authors**: Leslie N. Smith, Nicholay Topin
- **Comment**: This paper was significantly revised to show super-convergence as a
  general fast training methodology
- **Journal**: None
- **Summary**: In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).



### 3D Morphable Models as Spatial Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.07199v1
- **DOI**: 10.1109/ICCVW.2017.110
- **Categories**: **cs.CV**, cs.LG, 68T45, I.4.8; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1708.07199v1)
- **Published**: 2017-08-23 22:01:03+00:00
- **Updated**: 2017-08-23 22:01:03+00:00
- **Authors**: Anil Bas, Patrik Huber, William A. P. Smith, Muhammad Awais, Josef Kittler
- **Comment**: Accepted to ICCV 2017 2nd Workshop on Geometry Meets Deep Learning
- **Journal**: None
- **Summary**: In this paper, we show how a 3D Morphable Model (i.e. a statistical model of the 3D shape of a class of objects such as faces) can be used to spatially transform input data as a module (a 3DMM-STN) within a convolutional neural network. This is an extension of the original spatial transformer network in that we are able to interpret and normalise 3D pose changes and self-occlusions. The trained localisation part of the network is independently useful since it learns to fit a 3D morphable model to a single image. We show that the localiser can be trained using only simple geometric loss functions on a relatively small dataset yet is able to perform robust normalisation on highly uncontrolled images including occlusion, self-occlusion and large pose changes.



