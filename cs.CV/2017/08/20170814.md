# Arxiv Papers in cs.CV on 2017-08-14
### SSH: Single Stage Headless Face Detector
- **Arxiv ID**: http://arxiv.org/abs/1708.03979v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03979v3)
- **Published**: 2017-08-14 01:12:24+00:00
- **Updated**: 2017-10-18 00:07:03+00:00
- **Authors**: Mahyar Najibi, Pouya Samangouei, Rama Chellappa, Larry Davis
- **Comment**: International Conference on Computer Vision (ICCV) 2017
- **Journal**: None
- **Summary**: We introduce the Single Stage Headless (SSH) face detector. Unlike two stage proposal-classification detectors, SSH detects faces in a single stage directly from the early convolutional layers in a classification network. SSH is headless. That is, it is able to achieve state-of-the-art results while removing the "head" of its underlying classification network -- i.e. all fully connected layers in the VGG-16 which contains a large number of parameters. Additionally, instead of relying on an image pyramid to detect faces with various scales, SSH is scale-invariant by design. We simultaneously detect faces with different scales in a single forward pass of the network, but from different layers. These properties make SSH fast and light-weight. Surprisingly, with a headless VGG-16, SSH beats the ResNet-101-based state-of-the-art on the WIDER dataset. Even though, unlike the current state-of-the-art, SSH does not use an image pyramid and is 5X faster. Moreover, if an image pyramid is deployed, our light-weight network achieves state-of-the-art on all subsets of the WIDER dataset, improving the AP by 2.5%. SSH also reaches state-of-the-art results on the FDDB and Pascal-Faces datasets while using a small input size, leading to a runtime of 50 ms/image on a GPU. The code is available at https://github.com/mahyarnajibi/SSH.



### AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1708.03985v4
- **DOI**: 10.1109/TAFFC.2017.2740923
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03985v4)
- **Published**: 2017-08-14 01:40:47+00:00
- **Updated**: 2017-10-09 21:55:09+00:00
- **Authors**: Ali Mollahosseini, Behzad Hasani, Mohammad H. Mahoor
- **Comment**: IEEE Transactions on Affective Computing, 2017
- **Journal**: None
- **Summary**: Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.



### Fast, Accurate Thin-Structure Obstacle Detection for Autonomous Mobile Robots
- **Arxiv ID**: http://arxiv.org/abs/1708.04006v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04006v1)
- **Published**: 2017-08-14 04:35:04+00:00
- **Updated**: 2017-08-14 04:35:04+00:00
- **Authors**: Chen Zhou, Jiaolong Yang, Chunshui Zhao, Gang Hua
- **Comment**: Appeared at IEEE CVPR 2017 Workshop on Embedded Vision
- **Journal**: None
- **Summary**: Safety is paramount for mobile robotic platforms such as self-driving cars and unmanned aerial vehicles. This work is devoted to a task that is indispensable for safety yet was largely overlooked in the past -- detecting obstacles that are of very thin structures, such as wires, cables and tree branches. This is a challenging problem, as thin objects can be problematic for active sensors such as lidar and sonar and even for stereo cameras. In this work, we propose to use video sequences for thin obstacle detection. We represent obstacles with edges in the video frames, and reconstruct them in 3D using efficient edge-based visual odometry techniques. We provide both a monocular camera solution and a stereo camera solution. The former incorporates Inertial Measurement Unit (IMU) data to solve scale ambiguity, while the latter enjoys a novel, purely vision-based solution. Experiments demonstrated that the proposed methods are fast and able to detect thin obstacles robustly and accurately under various conditions.



### Style2Vec: Representation Learning for Fashion Items from Style Sets
- **Arxiv ID**: http://arxiv.org/abs/1708.04014v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04014v1)
- **Published**: 2017-08-14 05:22:54+00:00
- **Updated**: 2017-08-14 05:22:54+00:00
- **Authors**: Hanbit Lee, Jinseok Seol, Sang-goo Lee
- **Comment**: 6 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: With the rapid growth of online fashion market, demand for effective fashion recommendation systems has never been greater. In fashion recommendation, the ability to find items that goes well with a few other items based on style is more important than picking a single item based on the user's entire purchase history. Since the same user may have purchased dress suits in one month and casual denims in another, it is impossible to learn the latent style features of those items using only the user ratings. If we were able to represent the style features of fashion items in a reasonable way, we will be able to recommend new items that conform to some small subset of pre-purchased items that make up a coherent style set. We propose Style2Vec, a vector representation model for fashion items. Based on the intuition of distributional semantics used in word embeddings, Style2Vec learns the representation of a fashion item using other items in matching outfits as context. Two different convolutional neural networks are trained to maximize the probability of item co-occurrences. For evaluation, a fashion analogy test is conducted to show that the resulting representation connotes diverse fashion related semantics like shapes, colors, patterns and even latent styles. We also perform style classification using Style2Vec features and show that our method outperforms other baselines.



### Colorimetric Calibration of a Digital Camera
- **Arxiv ID**: http://arxiv.org/abs/1708.04685v1
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.IM
- **Links**: [PDF](http://arxiv.org/pdf/1708.04685v1)
- **Published**: 2017-08-14 09:04:05+00:00
- **Updated**: 2017-08-14 09:04:05+00:00
- **Authors**: Renata Rychtarikova, Pavel Soucek, Dalibor Stys
- **Comment**: 14 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper, we introduce a novel - physico-chemical - approach for calibration of a digital camera chip. This approach utilizes results of measurement of incident light spectra of calibration films of different levels of gray for construction of calibration curve (number of incident photons vs. image pixel intensity) for each camera pixel. We show spectral characteristics of such corrected digital raw image files (a primary camera signal) and demonstrate their suitability for next image processing and analysis.



### Kinship Verification from Videos using Spatio-Temporal Texture Features and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.04069v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04069v1)
- **Published**: 2017-08-14 10:41:33+00:00
- **Updated**: 2017-08-14 10:41:33+00:00
- **Authors**: Elhocine Boutellaa, Miguel Bordallo López, Samy Ait-Aoudia, Xiaoyi Feng, Abdenour Hadid
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Automatic kinship verification using facial images is a relatively new and challenging research problem in computer vision. It consists in automatically predicting whether two persons have a biological kin relation by examining their facial attributes. While most of the existing works extract shallow handcrafted features from still face images, we approach this problem from spatio-temporal point of view and explore the use of both shallow texture features and deep features for characterizing faces. Promising results, especially those of deep features, are obtained on the benchmark UvA-NEMO Smile database. Our extensive experiments also show the superiority of using videos over still images, hence pointing out the important role of facial dynamics in kinship verification. Furthermore, the fusion of the two types of features (i.e. shallow spatio-temporal texture features and deep features) shows significant performance improvements compared to state-of-the-art methods.



### Context-based Normalization of Histological Stains using Deep Convolutional Features
- **Arxiv ID**: http://arxiv.org/abs/1708.04099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04099v1)
- **Published**: 2017-08-14 12:40:23+00:00
- **Updated**: 2017-08-14 12:40:23+00:00
- **Authors**: Daniel Bug, Steffen Schneider, Anne Grote, Eva Oswald, Friedrich Feuerhake, Julia Schüler, Dorit Merhof
- **Comment**: In: 3rd Workshop on Deep Learning in Medical Image Analysis (DLMIA
  2017)
- **Journal**: None
- **Summary**: While human observers are able to cope with variations in color and appearance of histological stains, digital pathology algorithms commonly require a well-normalized setting to achieve peak performance, especially when a limited amount of labeled data is available. This work provides a fully automated, end-to-end learning-based setup for normalizing histological stains, which considers the texture context of the tissue. We introduce Feature Aware Normalization, which extends the framework of batch normalization in combination with gating elements from Long Short-Term Memory units for normalization among different spatial regions of interest. By incorporating a pretrained deep neural network as a feature extractor steering a pixelwise processing pipeline, we achieve excellent normalization results and ensure a consistent representation of color and texture. The evaluation comprises a comparison of color histogram deviations, structural similarity and measures the color volume obtained by the different methods.



### Towards Semantic Fast-Forward and Stabilized Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/1708.04146v2
- **DOI**: 10.1007/978-3-319-46604-0_40
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04146v2)
- **Published**: 2017-08-14 14:32:11+00:00
- **Updated**: 2017-08-16 13:36:34+00:00
- **Authors**: Michel Melo Silva, Washington Luis Souza Ramos, Joao Pedro Klock Ferreira, Mario Fernando Montenegro Campos, Erickson Rangel Nascimento
- **Comment**: Accepted for publication and presented in the First International
  Workshop on Egocentric Perception, Interaction and Computing at European
  Conference on Computer Vision (EPIC@ECCV) 2016
- **Journal**: None
- **Summary**: The emergence of low-cost personal mobiles devices and wearable cameras and the increasing storage capacity of video-sharing websites have pushed forward a growing interest towards first-person videos. Since most of the recorded videos compose long-running streams with unedited content, they are tedious and unpleasant to watch. The fast-forward state-of-the-art methods are facing challenges of balancing the smoothness of the video and the emphasis in the relevant frames given a speed-up rate. In this work, we present a methodology capable of summarizing and stabilizing egocentric videos by extracting the semantic information from the frames. This paper also describes a dataset collection with several semantically labeled videos and introduces a new smoothness evaluation metric for egocentric videos that is used to test our method.



### Fast-Forward Video Based on Semantic Extraction
- **Arxiv ID**: http://arxiv.org/abs/1708.04160v3
- **DOI**: 10.1109/ICIP.2016.7532977
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04160v3)
- **Published**: 2017-08-14 14:55:22+00:00
- **Updated**: 2017-08-16 13:34:56+00:00
- **Authors**: Washington Luis Souza Ramos, Michel Melo Silva, Mario Fernando Montenegro Campos, Erickson Rangel Nascimento
- **Comment**: Accepted for publication and presented in 2016 IEEE International
  Conference on Image Processing (ICIP)
- **Journal**: None
- **Summary**: Thanks to the low operational cost and large storage capacity of smartphones and wearable devices, people are recording many hours of daily activities, sport actions and home videos. These videos, also known as egocentric videos, are generally long-running streams with unedited content, which make them boring and visually unpalatable, bringing up the challenge to make egocentric videos more appealing. In this work we propose a novel methodology to compose the new fast-forward video by selecting frames based on semantic information extracted from images. The experiments show that our approach outperforms the state-of-the-art as far as semantic information is concerned and that it is also able to produce videos that are more pleasant to be watched.



### Tensor Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Tensors via Convex Optimization
- **Arxiv ID**: http://arxiv.org/abs/1708.04181v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04181v3)
- **Published**: 2017-08-14 15:42:05+00:00
- **Updated**: 2018-05-26 21:22:35+00:00
- **Authors**: Canyi Lu, Jiashi Feng, Yudong Chen, Wei Liu, Zhouchen Lin, Shuicheng Yan
- **Comment**: IEEE International Conference on Computer Vision and Pattern
  Recognition (CVPR, 2016)
- **Journal**: None
- **Summary**: This paper studies the Tensor Robust Principal Component (TRPCA) problem which extends the known Robust PCA (Candes et al. 2011) to the tensor case. Our model is based on a new tensor Singular Value Decomposition (t-SVD) (Kilmer and Martin 2011) and its induced tensor tubal rank and tensor nuclear norm. Consider that we have a 3-way tensor ${\mathcal{X}}\in\mathbb{R}^{n_1\times n_2\times n_3}$ such that ${\mathcal{X}}={\mathcal{L}}_0+{\mathcal{E}}_0$, where ${\mathcal{L}}_0$ has low tubal rank and ${\mathcal{E}}_0$ is sparse. Is that possible to recover both components? In this work, we prove that under certain suitable assumptions, we can recover both the low-rank and the sparse components exactly by simply solving a convex program whose objective is a weighted combination of the tensor nuclear norm and the $\ell_1$-norm, i.e., $\min_{{\mathcal{L}},\ {\mathcal{E}}} \ \|{{\mathcal{L}}}\|_*+\lambda\|{{\mathcal{E}}}\|_1, \ \text{s.t.} \ {\mathcal{X}}={\mathcal{L}}+{\mathcal{E}}$, where $\lambda= {1}/{\sqrt{\max(n_1,n_2)n_3}}$. Interestingly, TRPCA involves RPCA as a special case when $n_3=1$ and thus it is a simple and elegant tensor extension of RPCA. Also numerical experiments verify our theory and the application for the image denoising demonstrates the effectiveness of our method.



### Learning Blind Motion Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1708.04208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04208v1)
- **Published**: 2017-08-14 17:03:53+00:00
- **Updated**: 2017-08-14 17:03:53+00:00
- **Authors**: Patrick Wieschollek, Michael Hirsch, Bernhard Schölkopf, Hendrik P. A. Lensch
- **Comment**: International Conference on Computer Vision (ICCV) (2017)
- **Journal**: None
- **Summary**: As handheld video cameras are now commonplace and available in every smartphone, images and videos can be recorded almost everywhere at anytime. However, taking a quick shot frequently yields a blurry result due to unwanted camera shake during recording or moving objects in the scene. Removing these artifacts from the blurry recordings is a highly ill-posed problem as neither the sharp image nor the motion blur kernel is known. Propagating information between multiple consecutive blurry observations can help restore the desired sharp image or video. Solutions for blind deconvolution based on neural networks rely on a massive amount of ground-truth data which is hard to acquire. In this work, we propose an efficient approach to produce a significant amount of realistic training data and introduce a novel recurrent network architecture to deblur frames taking temporal information into account, which can efficiently handle arbitrary spatial and temporal input sizes. We demonstrate the versatility of our approach in a comprehensive comparison on a number of challening real-world examples.



### Learning spectro-temporal features with 3D CNNs for speech emotion recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.05071v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.05071v1)
- **Published**: 2017-08-14 17:32:06+00:00
- **Updated**: 2017-08-14 17:32:06+00:00
- **Authors**: Jaebok Kim, Khiet P. Truong, Gwenn Englebienne, Vanessa Evers
- **Comment**: ACII, 2017, San Antonio
- **Journal**: None
- **Summary**: In this paper, we propose to use deep 3-dimensional convolutional networks (3D CNNs) in order to address the challenge of modelling spectro-temporal dynamics for speech emotion recognition (SER). Compared to a hybrid of Convolutional Neural Network and Long-Short-Term-Memory (CNN-LSTM), our proposed 3D CNNs simultaneously extract short-term and long-term spectral features with a moderate number of parameters. We evaluated our proposed and other state-of-the-art methods in a speaker-independent manner using aggregated corpora that give a large and diverse set of speakers. We found that 1) shallow temporal and moderately deep spectral kernels of a homogeneous architecture are optimal for the task; and 2) our 3D CNNs are more effective for spectro-temporal feature learning compared to other methods. Finally, we visualised the feature space obtained with our proposed method using t-distributed stochastic neighbour embedding (T-SNE) and could observe distinct clusters of emotions.



### Deep Object-Centric Representations for Generalizable Robot Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.04225v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.04225v3)
- **Published**: 2017-08-14 17:42:59+00:00
- **Updated**: 2017-09-26 17:06:36+00:00
- **Authors**: Coline Devin, Pieter Abbeel, Trevor Darrell, Sergey Levine
- **Comment**: None
- **Journal**: None
- **Summary**: Robotic manipulation in complex open-world scenarios requires both reliable physical manipulation skills and effective and generalizable perception. In this paper, we propose a method where general purpose pretrained visual models serve as an object-centric prior for the perception system of a learned policy. We devise an object-level attentional mechanism that can be used to determine relevant objects from a few trajectories or demonstrations, and then immediately incorporate those objects into a learned policy. A task-independent meta-attention locates possible objects in the scene, and a task-specific attention identifies which objects are predictive of the trajectories. The scope of the task-specific attention is easily adjusted by showing demonstrations with distractor objects or with diverse relevant objects. Our results indicate that this approach exhibits good generalization across object instances using very few samples, and can be used to learn a variety of manipulation tasks using reinforcement learning.



### Attacking Automatic Video Analysis Algorithms: A Case Study of Google Cloud Video Intelligence API
- **Arxiv ID**: http://arxiv.org/abs/1708.04301v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.04301v1)
- **Published**: 2017-08-14 20:10:04+00:00
- **Updated**: 2017-08-14 20:10:04+00:00
- **Authors**: Hossein Hosseini, Baicen Xiao, Andrew Clark, Radha Poovendran
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the growth of video data on Internet, automatic video analysis has gained a lot of attention from academia as well as companies such as Facebook, Twitter and Google. In this paper, we examine the robustness of video analysis algorithms in adversarial settings. Specifically, we propose targeted attacks on two fundamental classes of video analysis algorithms, namely video classification and shot detection. We show that an adversary can subtly manipulate a video in such a way that a human observer would perceive the content of the original video, but the video analysis algorithm will return the adversary's desired outputs.   We then apply the attacks on the recently released Google Cloud Video Intelligence API. The API takes a video file and returns the video labels (objects within the video), shot changes (scene changes within the video) and shot labels (description of video events over time). Through experiments, we show that the API generates video and shot labels by processing only the first frame of every second of the video. Hence, an adversary can deceive the API to output only her desired video and shot labels by periodically inserting an image into the video at the rate of one frame per second. We also show that the pattern of shot changes returned by the API can be mostly recovered by an algorithm that compares the histograms of consecutive frames. Based on our equivalent model, we develop a method for slightly modifying the video frames, in order to deceive the API into generating our desired pattern of shot changes. We perform extensive experiments with different videos and show that our attacks are consistently successful across videos with different characteristics. At the end, we propose introducing randomness to video analysis algorithms as a countermeasure to our attacks.



### An ELU Network with Total Variation for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1708.04317v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04317v1)
- **Published**: 2017-08-14 20:47:35+00:00
- **Updated**: 2017-08-14 20:47:35+00:00
- **Authors**: Tianyang Wang, Zhengrui Qin, Michelle Zhu
- **Comment**: 10 pages, Accepted by the 24th International Conference on Neural
  Information Processing (2017)
- **Journal**: None
- **Summary**: In this paper, we propose a novel convolutional neural network (CNN) for image denoising, which uses exponential linear unit (ELU) as the activation function. We investigate the suitability by analyzing ELU's connection with trainable nonlinear reaction diffusion model (TNRD) and residual denoising. On the other hand, batch normalization (BN) is indispensable for residual denoising and convergence purpose. However, direct stacking of BN and ELU degrades the performance of CNN. To mitigate this issue, we design an innovative combination of activation layer and normalization layer to exploit and leverage the ELU network, and discuss the corresponding rationale. Moreover, inspired by the fact that minimizing total variation (TV) can be applied to image denoising, we propose a TV regularized L2 loss to evaluate the training effect during the iterations. Finally, we conduct extensive experiments, showing that our model outperforms some recent and popular approaches on Gaussian denoising with specific or randomized noise levels for both gray and color images.



### Situation Recognition with Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.04320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04320v1)
- **Published**: 2017-08-14 20:51:05+00:00
- **Updated**: 2017-08-14 20:51:05+00:00
- **Authors**: Ruiyu Li, Makarand Tapaswi, Renjie Liao, Jiaya Jia, Raquel Urtasun, Sanja Fidler
- **Comment**: ICCV2017
- **Journal**: None
- **Summary**: We address the problem of recognizing situations in images. Given an image, the task is to predict the most salient verb (action), and fill its semantic roles such as who is performing the action, what is the source and target of the action, etc. Different verbs have different roles (e.g. attacking has weapon), and each role can take on many possible values (nouns). We propose a model based on Graph Neural Networks that allows us to efficiently capture joint dependencies between roles using neural networks defined on a graph. Experiments with different graph connectivities show that our approach that propagates information between roles significantly outperforms existing work, as well as multiple baselines. We obtain roughly 3-5% improvement over previous work in predicting the full situation. We also provide a thorough qualitative analysis of our model and influence of different roles in the verbs.



### Image Augmentation using Radial Transform for Training Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.04347v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04347v4)
- **Published**: 2017-08-14 22:35:35+00:00
- **Updated**: 2018-02-14 15:58:46+00:00
- **Authors**: Hojjat Salehinejad, Shahrokh Valaee, Timothy Dowdell, Joseph Barfett
- **Comment**: This paper is accepted for presentation at IEEE International
  Conference on Acoustics, Speech and Signal Processing (IEEE ICASSP), 2018
- **Journal**: None
- **Summary**: Deep learning models have a large number of free parameters that must be estimated by efficient training of the models on a large number of training data samples to increase their generalization performance. In real-world applications, the data available to train these networks is often limited or imbalanced. We propose a sampling method based on the radial transform in a polar coordinate system for image augmentation to facilitate the training of deep learning models from limited source data. This pixel-wise transform provides representations of the original image in the polar coordinate system by generating a new image from each pixel. This technique can generate radial transformed images up to the number of pixels in the original image to increase the diversity of poorly represented image classes. Our experiments show improved generalization performance in training deep convolutional neural networks with radial transformed images.



