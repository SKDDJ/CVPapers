# Arxiv Papers in cs.CV on 2017-08-26
### Deep Learning for Target Classification from SAR Imagery: Data Augmentation and Translation Invariance
- **Arxiv ID**: http://arxiv.org/abs/1708.07920v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07920v1)
- **Published**: 2017-08-26 02:44:09+00:00
- **Updated**: 2017-08-26 02:44:09+00:00
- **Authors**: Hidetoshi Furukawa
- **Comment**: Technical Report, 5 pages, 8 figures, Copyright(C)2017 IEICE
- **Journal**: IEICE Technical Report, vol.117, no.182, SANE2017-30, pp.13-17,
  Aug. 2017
- **Summary**: This report deals with translation invariance of convolutional neural networks (CNNs) for automatic target recognition (ATR) from synthetic aperture radar (SAR) imagery. In particular, the translation invariance of CNNs for SAR ATR represents the robustness against misalignment of target chips extracted from SAR images. To understand the translation invariance of the CNNs, we trained CNNs which classify the target chips from the MSTAR into the ten classes under the condition of with and without data augmentation, and then visualized the translation invariance of the CNNs. According to our results, even if we use a deep residual network, the translation invariance of the CNN without data augmentation using the aligned images such as the MSTAR target chips is not so large. A more important factor of translation invariance is the use of augmented training data. Furthermore, our CNN using augmented training data achieved a state-of-the-art classification accuracy of 99.6%. These results show an importance of domain-specific data augmentation.



### Robust Stereo Feature Descriptor for Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1708.07933v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07933v2)
- **Published**: 2017-08-26 05:35:02+00:00
- **Updated**: 2018-09-23 14:17:48+00:00
- **Authors**: Ehsan Shojaedini, Reza Safabakhsh
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a simple way to utilize stereo camera data to improve feature descriptors. Computer vision algorithms that use a stereo camera require some calculations of 3D information. We leverage this pre-calculated information to improve feature descriptor algorithms. We use the 3D feature information to estimate the scale of each feature. This way, each feature descriptor will be more robust to scale change without significant computations. In addition, we use stereo images to construct the descriptor vector. The Scale-Invariant Feature Transform (SIFT) and Fast Retina Keypoint (FREAK) descriptors are used to evaluate the proposed method. The scale normalization technique in feature tracking test improves the standard SIFT by 8.75% and improves the standard FREAK by 28.65%. Using the proposed stereo feature descriptor, a visual odometry algorithm is designed and tested on the KITTI dataset. The stereo FREAK descriptor raises the number of inlier matches by 19% and consequently improves the accuracy of visual odometry by 23%.



### 3D Binary Signatures
- **Arxiv ID**: http://arxiv.org/abs/1708.07937v1
- **DOI**: 10.1145/3009977.3010009
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07937v1)
- **Published**: 2017-08-26 06:01:09+00:00
- **Updated**: 2017-08-26 06:01:09+00:00
- **Authors**: Siddharth Srivastava, Brejesh Lall
- **Comment**: Tenth Indian Conference on Computer Vision, Graphics and Image
  Processing 2016
- **Journal**: None
- **Summary**: In this paper, we propose a novel binary descriptor for 3D point clouds. The proposed descriptor termed as 3D Binary Signature (3DBS) is motivated from the matching efficiency of the binary descriptors for 2D images. 3DBS describes keypoints from point clouds with a binary vector resulting in extremely fast matching. The method uses keypoints from standard keypoint detectors. The descriptor is built by constructing a Local Reference Frame and aligning a local surface patch accordingly. The local surface patch constitutes of identifying nearest neighbours based upon an angular constraint among them. The points are ordered with respect to the distance from the keypoints. The normals of the ordered pairs of these keypoints are projected on the axes and the relative magnitude is used to assign a binary digit. The vector thus constituted is used as a signature for representing the keypoints. The matching is done by using hamming distance. We show that 3DBS outperforms state of the art descriptors on various evaluation metrics.



### Distributed Bundle Adjustment
- **Arxiv ID**: http://arxiv.org/abs/1708.07954v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07954v1)
- **Published**: 2017-08-26 09:13:11+00:00
- **Updated**: 2017-08-26 09:13:11+00:00
- **Authors**: Karthikeyan Natesan Ramamurthy, Chung-Ching Lin, Aleksandr Aravkin, Sharath Pankanti, Raphael Viguier
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Most methods for Bundle Adjustment (BA) in computer vision are either centralized or operate incrementally. This leads to poor scaling and affects the quality of solution as the number of images grows in large scale structure from motion (SfM). Furthermore, they cannot be used in scenarios where image acquisition and processing must be distributed. We address this problem with a new distributed BA algorithm. Our distributed formulation uses alternating direction method of multipliers (ADMM), and, since each processor sees only a small portion of the data, we show that robust formulations improve performance. We analyze convergence of the proposed algorithm, and illustrate numerical performance, accuracy of the parameter estimates, and scalability of the distributed implementation in the context of synthetic 3D datasets with known camera position and orientation ground truth. The results are comparable to an alternate state-of-the-art centralized bundle adjustment algorithm on synthetic and real 3D reconstruction problems. The runtime of our implementation scales linearly with the number of observed points.



### 3D Object Reconstruction from a Single Depth View with Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.07969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1708.07969v1)
- **Published**: 2017-08-26 13:46:21+00:00
- **Updated**: 2017-08-26 13:46:21+00:00
- **Authors**: Bo Yang, Hongkai Wen, Sen Wang, Ronald Clark, Andrew Markham, Niki Trigoni
- **Comment**: ICCV Workshops 2017
- **Journal**: None
- **Summary**: In this paper, we propose a novel 3D-RecGAN approach, which reconstructs the complete 3D structure of a given object from a single arbitrary depth view using generative adversarial networks. Unlike the existing work which typically requires multiple views of the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN only takes the voxel grid representation of a depth view of the object as input, and is able to generate the complete 3D occupancy grid by filling in the occluded/missing regions. The key idea is to combine the generative capabilities of autoencoders and the conditional Generative Adversarial Networks (GAN) framework, to infer accurate and fine-grained 3D structures of objects in high-dimensional voxel space. Extensive experiments on large synthetic datasets show that the proposed 3D-RecGAN significantly outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects. Our code and data are available at: https://github.com/Yang7879/3D-RecGAN.



### Maximum A Posteriori Estimation of Distances Between Deep Features in Still-to-Video Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.07972v1
- **DOI**: 10.1016/j.eswa.2018.04.039
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1708.07972v1)
- **Published**: 2017-08-26 13:56:48+00:00
- **Updated**: 2017-08-26 13:56:48+00:00
- **Authors**: Andrey V. Savchenko, Natalya S. Belova
- **Comment**: 20 pages, 5 figures, 40 references
- **Journal**: None
- **Summary**: The paper deals with the still-to-video face recognition for the small sample size problem based on computation of distances between high-dimensional deep bottleneck features. We present the novel statistical recognition method, in which the still-to-video recognition task is casted into Maximum A Posteriori estimation. In this method we maximize the joint probabilistic density of the distances to all reference still images. It is shown that this likelihood can be estimated with the known asymptotically normal distribution of the Kullback-Leibler discriminations between nonnegative features. The experimental study with the LFW (Labeled Faces in the Wild), YTF (YouTube Faces) and IJB-A (IARPA Janus Benchmark A) datasets has been provided. We demonstrated, that the proposed approach can be applied with the state-of-the-art deep features and dissimilarity measures. Our algorithm achieves 3-5% higher accuracy when compared with conventional aggregation of decisions obtained for all frames.



### Synthesising Wider Field Images from Narrow-Field Retinal Video Acquired Using a Low-Cost Direct Ophthalmoscope (Arclight) Attached to a Smartphone
- **Arxiv ID**: http://arxiv.org/abs/1708.07977v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07977v1)
- **Published**: 2017-08-26 14:28:21+00:00
- **Updated**: 2017-08-26 14:28:21+00:00
- **Authors**: Keylor Daniel Chaves Viquez, Ognjen Arandjelovic, Andrew Blaikie, In Ae Hwang
- **Comment**: International Conference on Computer Vision Workshop on BioImage
  Computing, 2017
- **Journal**: None
- **Summary**: Access to low cost retinal imaging devices in low and middle income countries is limited, compromising progress in preventing needless blindness. The Arclight is a recently developed low-cost solar powered direct ophthalmoscope which can be attached to the camera of a smartphone to acquire retinal images and video. However, the acquired data is inherently limited by the optics of direct ophthalmoscopy, resulting in a narrow field of view with associated corneal reflections, limiting its usefulness. In this work we describe the first fully automatic method utilizing videos acquired using the Arclight attached to a mobile phone camera to create wider view, higher quality still images comparable with images obtained using much more expensive and bulky dedicated traditional retinal cameras.



### Stereo Matching With Color-Weighted Correlation, Hierarchical Belief Propagation And Occlusion Handling
- **Arxiv ID**: http://arxiv.org/abs/1708.07987v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07987v2)
- **Published**: 2017-08-26 15:43:39+00:00
- **Updated**: 2017-08-30 13:32:04+00:00
- **Authors**: Vamshhi Pavan Kumar Varma Vegeshna
- **Comment**: arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources
- **Journal**: None
- **Summary**: In this paper, we contrive a stereo matching algorithm with careful handling of disparity, discontinuity and occlusion. This algorithm works a worldwide matching stereo model which is based on minimization of energy. The global energy comprises two terms, firstly the data term and secondly the smoothness term. The data term is approximated by a color-weighted correlation, then refined in obstruct and low-texture areas in many applications of hierarchical loopy belief propagation algorithm. The results during the experiment are evaluated on the Middlebury data sets, showing that out algorithm is the top performer among all the algorithms listed there



### Facial Expression Recognition using Visual Saliency and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.08016v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08016v1)
- **Published**: 2017-08-26 20:03:38+00:00
- **Updated**: 2017-08-26 20:03:38+00:00
- **Authors**: Viraj Mavani, Shanmuganathan Raman, Krishna P Miyapuram
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: We have developed a convolutional neural network for the purpose of recognizing facial expressions in human beings. We have fine-tuned the existing convolutional neural network model trained on the visual recognition dataset used in the ILSVRC2012 to two widely used facial expression datasets - CFEE and RaFD, which when trained and tested independently yielded test accuracies of 74.79% and 95.71%, respectively. Generalization of results was evident by training on one dataset and testing on the other. Further, the image product of the cropped faces and their visual saliency maps were computed using Deep Multi-Layer Network for saliency prediction and were fed to the facial expression recognition CNN. In the most generalized experiment, we observed the top-1 accuracy in the test set to be 65.39%. General confusion trends between different facial expressions as exhibited by humans were also observed.



