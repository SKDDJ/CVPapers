# Arxiv Papers in cs.CV on 2017-08-30
### Block-Simultaneous Direction Method of Multipliers: A proximal primal-dual splitting algorithm for nonconvex problems with multiple constraints
- **Arxiv ID**: http://arxiv.org/abs/1708.09066v1
- **DOI**: 10.1007/s11081-018-9380-y
- **Categories**: **math.OC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.09066v1)
- **Published**: 2017-08-30 00:15:50+00:00
- **Updated**: 2017-08-30 00:15:50+00:00
- **Authors**: Fred Moolekamp, Peter Melchior
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: We introduce a generalization of the linearized Alternating Direction Method of Multipliers to optimize a real-valued function $f$ of multiple arguments with potentially multiple constraints $g_\circ$ on each of them. The function $f$ may be nonconvex as long as it is convex in every argument, while the constraints $g_\circ$ need to be convex but not smooth. If $f$ is smooth, the proposed Block-Simultaneous Direction Method of Multipliers (bSDMM) can be interpreted as a proximal analog to inexact coordinate descent methods under constraints. Unlike alternative approaches for joint solvers of multiple-constraint problems, we do not require linear operators $L$ of a constraint function $g(L\ \cdot)$ to be invertible or linked between each other. bSDMM is well-suited for a range of optimization problems, in particular for data analysis, where $f$ is the likelihood function of a model and $L$ could be a transformation matrix describing e.g. finite differences or basis transforms. We apply bSDMM to the Non-negative Matrix Factorization task of a hyperspectral unmixing problem and demonstrate convergence and effectiveness of multiple constraints on both matrix factors. The algorithms are implemented in python and released as an open-source package.



### Continual One-Shot Learning of Hidden Spike-Patterns with Neural Network Simulation Expansion and STDP Convergence Predictions
- **Arxiv ID**: http://arxiv.org/abs/1708.09072v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.09072v1)
- **Published**: 2017-08-30 01:07:18+00:00
- **Updated**: 2017-08-30 01:07:18+00:00
- **Authors**: Toby Lightheart, Steven Grainger, Tien-Fu Lu
- **Comment**: 41 pages, 16 figures
- **Journal**: None
- **Summary**: This paper presents a constructive algorithm that achieves successful one-shot learning of hidden spike-patterns in a competitive detection task. It has previously been shown (Masquelier et al., 2008) that spike-timing-dependent plasticity (STDP) and lateral inhibition can result in neurons competitively tuned to repeating spike-patterns concealed in high rates of overall presynaptic activity. One-shot construction of neurons with synapse weights calculated as estimates of converged STDP outcomes results in immediate selective detection of hidden spike-patterns. The capability of continual learning is demonstrated through the successful one-shot detection of new sets of spike-patterns introduced after long intervals in the simulation time. Simulation expansion (Lightheart et al., 2013) has been proposed as an approach to the development of constructive algorithms that are compatible with simulations of biological neural networks. A simulation of a biological neural network may have orders of magnitude fewer neurons and connections than the related biological neural systems; therefore, simulated neural networks can be assumed to be a subset of a larger neural system. The constructive algorithm is developed using simulation expansion concepts to perform an operation equivalent to the exchange of neurons between the simulation and the larger hypothetical neural system. The dynamic selection of neurons to simulate within a larger neural system (hypothetical or stored in memory) may be a starting point for a wide range of developments and applications in machine learning and the simulation of biology.



### Adaptive SVM+: Learning with Privileged Information for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1708.09083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09083v1)
- **Published**: 2017-08-30 01:57:16+00:00
- **Updated**: 2017-08-30 01:57:16+00:00
- **Authors**: Nikolaos Sarafianos, Michalis Vrigkas, Ioannis A. Kakadiaris
- **Comment**: To appear in ICCV Workshops 2017 (TASK-CV)
- **Journal**: None
- **Summary**: Incorporating additional knowledge in the learning process can be beneficial for several computer vision and machine learning tasks. Whether privileged information originates from a source domain that is adapted to a target domain, or as additional features available at training time only, using such privileged (i.e., auxiliary) information is of high importance as it improves the recognition performance and generalization. However, both primary and privileged information are rarely derived from the same distribution, which poses an additional challenge to the recognition task. To address these challenges, we present a novel learning paradigm that leverages privileged information in a domain adaptation setup to perform visual recognition tasks. The proposed framework, named Adaptive SVM+, combines the advantages of both the learning using privileged information (LUPI) paradigm and the domain adaptation framework, which are naturally embedded in the objective function of a regular SVM. We demonstrate the effectiveness of our approach on the publicly available Animals with Attributes and INTERACT datasets and report state-of-the-art results in both of them.



### A Deep Learning Approach for Population Estimation from Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1708.09086v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.09086v1)
- **Published**: 2017-08-30 02:05:16+00:00
- **Updated**: 2017-08-30 02:05:16+00:00
- **Authors**: Caleb Robinson, Fred Hohman, Bistra Dilkina
- **Comment**: None
- **Journal**: None
- **Summary**: Knowing where people live is a fundamental component of many decision making processes such as urban development, infectious disease containment, evacuation planning, risk management, conservation planning, and more. While bottom-up, survey driven censuses can provide a comprehensive view into the population landscape of a country, they are expensive to realize, are infrequently performed, and only provide population counts over broad areas. Population disaggregation techniques and population projection methods individually address these shortcomings, but also have shortcomings of their own. To jointly answer the questions of "where do people live" and "how many people live there," we propose a deep learning model for creating high-resolution population estimations from satellite imagery. Specifically, we train convolutional neural networks to predict population in the USA at a $0.01^{\circ} \times 0.01^{\circ}$ resolution grid from 1-year composite Landsat imagery. We validate these models in two ways: quantitatively, by comparing our model's grid cell estimates aggregated at a county-level to several US Census county-level population projections, and qualitatively, by directly interpreting the model's predictions in terms of the satellite image inputs. We find that aggregating our model's estimates gives comparable results to the Census county-level population projections and that the predictions made by our model can be directly interpreted, which give it advantages over traditional population disaggregation methods. In general, our model is an example of how machine learning techniques can be an effective tool for extracting information from inherently unstructured, remotely sensed data to provide effective solutions to social problems.



### Simultaneously Color-Depth Super-Resolution with Conditional Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1708.09105v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09105v3)
- **Published**: 2017-08-30 04:17:16+00:00
- **Updated**: 2018-11-28 09:46:59+00:00
- **Authors**: Lijun Zhao, Huihui Bai, Jie Liang, Bing Zeng, Anhong Wang, Yao Zhao
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: Recently, Generative Adversarial Network (GAN) has been found wide applications in style transfer, image-to-image translation and image super-resolution. In this paper, a color-depth conditional GAN is proposed to concurrently resolve the problems of depth super-resolution and color super-resolution in 3D videos. Firstly, given the low-resolution depth image and low-resolution color image, a generative network is proposed to leverage mutual information of color image and depth image to enhance each other in consideration of the geometry structural dependency of color-depth image in the same scene. Secondly, three loss functions, including data loss, total variation loss, and 8-connected gradient difference loss are introduced to train this generative network in order to keep generated images close to the real ones, in addition to the adversarial loss. Experimental results demonstrate that the proposed approach produces high-quality color image and depth image from low-quality image pair, and it is superior to several other leading methods. Besides, we use the same neural network framework to resolve the problem of image smoothing and edge detection at the same time.



### Photorealistic Facial Expression Synthesis by the Conditional Difference Adversarial Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1708.09126v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09126v1)
- **Published**: 2017-08-30 05:37:58+00:00
- **Updated**: 2017-08-30 05:37:58+00:00
- **Authors**: Yuqian Zhou, Bertram Emil Shi
- **Comment**: Accepted by ACII2017
- **Journal**: None
- **Summary**: Photorealistic facial expression synthesis from single face image can be widely applied to face recognition, data augmentation for emotion recognition or entertainment. This problem is challenging, in part due to a paucity of labeled facial expression data, making it difficult for algorithms to disambiguate changes due to identity and changes due to expression. In this paper, we propose the conditional difference adversarial autoencoder, CDAAE, for facial expression synthesis. The CDAAE takes a facial image of a previously unseen person and generates an image of that human face with a target emotion or facial action unit label. The CDAAE adds a feedforward path to an autoencoder structure connecting low level features at the encoder to features at the corresponding level at the decoder. It handles the problem of disambiguating changes due to identity and changes due to facial expression by learning to generate the difference between low-level features of images of the same person but with different facial expressions. The CDAAE structure can be used to generate novel expressions by combining and interpolating between facial expressions/action units within the training set. Our experimental results demonstrate that the CDAAE can preserve identity information when generating facial expression for unseen subjects more faithfully than previous approaches. This is especially advantageous when training with small databases.



### A Greedy Part Assignment Algorithm for Real-time Multi-person 2D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1708.09182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09182v1)
- **Published**: 2017-08-30 09:13:38+00:00
- **Updated**: 2017-08-30 09:13:38+00:00
- **Authors**: Srenivas Varadarajan, Parual Datta, Omesh Tickoo
- **Comment**: None
- **Journal**: None
- **Summary**: Human pose-estimation in a multi-person image involves detection of various body parts and grouping them into individual person clusters. While the former task is challenging due to mutual occlusions, the combinatorial complexity of the latter task is very high. We propose a greedy part assignment algorithm that exploits the inherent structure of the human body to achieve a lower complexity, compared to any of the prior published works. This is accomplished by (i) reducing the number of part-candidates using the estimated number of people in the image, (ii) doing a greedy sequential assignment of part-classes, following the kinematic chain from head to ankle (iii) doing a greedy assignment of parts in each part-class set, to person-clusters (iv) limiting the candidate person clusters to the most proximal clusters using human anthropometric data and (v) using only a specific subset of pre-assigned parts for establishing pairwise structural constraints. We show that, these steps result in a sparse body parts relationship graph and reduces the complexity. We also propose methods for improving the accuracy of pose-estimation by (i) spawning person-clusters from any unassigned significant body part and (ii) suppressing hallucinated parts. On the MPII multi-person pose database, pose-estimation using the proposed method takes only 0.14 seconds per image. We show that, our proposed algorithm, by using a large spatial and structural context, achieves the state-of-the-art accuracy on both MPII and WAF multi-person pose datasets, demonstrating the robustness of our approach.



### Joint Maximum Purity Forest with Application to Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1708.09200v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09200v1)
- **Published**: 2017-08-30 10:00:11+00:00
- **Updated**: 2017-08-30 10:00:11+00:00
- **Authors**: Hailiang Li, Kin-Man Lam, Dong Li
- **Comment**: 18 pages, 7 figures
- **Journal**: None
- **Summary**: In this paper, we propose a novel random-forest scheme, namely Joint Maximum Purity Forest (JMPF), for classification, clustering, and regression tasks. In the JMPF scheme, the original feature space is transformed into a compactly pre-clustered feature space, via a trained rotation matrix. The rotation matrix is obtained through an iterative quantization process, where the input data belonging to different classes are clustered to the respective vertices of the new feature space with maximum purity. In the new feature space, orthogonal hyperplanes, which are employed at the split-nodes of decision trees in random forests, can tackle the clustering problems effectively. We evaluated our proposed method on public benchmark datasets for regression and classification tasks, and experiments showed that JMPF remarkably outperforms other state-of-the-art random-forest-based approaches. Furthermore, we applied JMPF to image super-resolution, because the transformed, compact features are more discriminative to the clustering-regression scheme. Experiment results on several public benchmark datasets also showed that the JMPF-based image super-resolution scheme is consistently superior to recent state-of-the-art image super-resolution algorithms.



### Cascade Residual Learning: A Two-stage Convolutional Neural Network for Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1708.09204v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09204v2)
- **Published**: 2017-08-30 10:20:37+00:00
- **Updated**: 2018-07-30 17:01:28+00:00
- **Authors**: Jiahao Pang, Wenxiu Sun, Jimmy SJ. Ren, Chengxi Yang, Qiong Yan
- **Comment**: Accepted at ICCVW 2017. The first two authors contributed equally to
  this paper
- **Journal**: None
- **Summary**: Leveraging on the recent developments in convolutional neural networks (CNNs), matching dense correspondence from a stereo pair has been cast as a learning problem, with performance exceeding traditional approaches. However, it remains challenging to generate high-quality disparities for the inherently ill-posed regions. To tackle this problem, we propose a novel cascade CNN architecture composing of two stages. The first stage advances the recently proposed DispNet by equipping it with extra up-convolution modules, leading to disparity images with more details. The second stage explicitly rectifies the disparity initialized by the first stage; it couples with the first-stage and generates residual signals across multiple scales. The summation of the outputs from the two stages gives the final disparity. As opposed to directly learning the disparity at the second stage, we show that residual learning provides more effective refinement. Moreover, it also benefits the training of the overall cascade network. Experimentation shows that our cascade residual learning scheme provides state-of-the-art performance for matching stereo correspondence. By the time of the submission of this paper, our method ranks first in the KITTI 2015 stereo benchmark, surpassing the prior works by a noteworthy margin.



### ScatterNet Hybrid Deep Learning (SHDL) Network For Object Classification
- **Arxiv ID**: http://arxiv.org/abs/1708.09212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09212v1)
- **Published**: 2017-08-30 11:02:20+00:00
- **Updated**: 2017-08-30 11:02:20+00:00
- **Authors**: Amarjot Singh, Nick Kingsbury
- **Comment**: To Appear in the 27th IEEE International Workshop on Machine Learning
  For Signal Processing (MLSP) 2017
- **Journal**: None
- **Summary**: The paper proposes the ScatterNet Hybrid Deep Learning (SHDL) network that extracts invariant and discriminative image representations for object recognition. SHDL framework is constructed with a multi-layer ScatterNet front-end, an unsupervised learning middle, and a supervised learning back-end module. Each layer of the SHDL network is automatically designed as an explicit optimization problem leading to an optimal deep learning architecture with improved computational performance as compared to the more usual deep network architectures. SHDL network produces the state-of-the-art classification performance against unsupervised and semi-supervised learning (GANs) on two image datasets. Advantages of the SHDL network over supervised methods (NIN, VGG) are also demonstrated with experiments performed on training datasets of reduced size.



### Two-stream Flow-guided Convolutional Attention Networks for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.09268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09268v1)
- **Published**: 2017-08-30 13:43:32+00:00
- **Updated**: 2017-08-30 13:43:32+00:00
- **Authors**: An Tran, Loong-Fah Cheong
- **Comment**: To appear in International Conference of Computer Vision Workshop
  (ICCVW), 2017
- **Journal**: None
- **Summary**: This paper proposes a two-stream flow-guided convolutional attention networks for action recognition in videos. The central idea is that optical flows, when properly compensated for the camera motion, can be used to guide attention to the human foreground. We thus develop cross-link layers from the temporal network (trained on flows) to the spatial network (trained on RGB frames). These cross-link layers guide the spatial-stream to pay more attention to the human foreground areas and be less affected by background clutter. We obtain promising performances with our approach on the UCF101, HMDB51 and Hollywood2 datasets.



### Texture and Structure Incorporated ScatterNet Hybrid Deep Learning Network (TS-SHDL) For Brain Matter Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1708.09300v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09300v1)
- **Published**: 2017-08-30 14:38:06+00:00
- **Updated**: 2017-08-30 14:38:06+00:00
- **Authors**: Amarjot Singh, Devamanyu Hazarika, Aniruddha Bhattacharya
- **Comment**: To Appear in the IEEE International Conference on Computer Vision
  Workshops (ICCVW) 2017
- **Journal**: None
- **Summary**: Automation of brain matter segmentation from MR images is a challenging task due to the irregular boundaries between the grey and white matter regions. In addition, the presence of intensity inhomogeneity in the MR images further complicates the problem. In this paper, we propose a texture and vesselness incorporated version of the ScatterNet Hybrid Deep Learning Network (TS-SHDL) that extracts hierarchical invariant mid-level features, used by fisher vector encoding and a conditional random field (CRF) to perform the desired segmentation. The performance of the proposed network is evaluated by extensive experimentation and comparison with the state-of-the-art methods on several 2D MRI scans taken from the synthetic McGill Brain Web as well as on the MRBrainS dataset of real 3D MRI scans. The advantages of the TS-SHDL network over supervised deep learning networks is also presented in addition to its superior performance over the state-of-the-art.



### Disguised Face Identification (DFI) with Facial KeyPoints using Spatial Fusion Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1708.09317v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09317v1)
- **Published**: 2017-08-30 15:11:06+00:00
- **Updated**: 2017-08-30 15:11:06+00:00
- **Authors**: Amarjot Singh, Devendra Patil, G Meghana Reddy, SN Omkar
- **Comment**: To Appear in the IEEE International Conference on Computer Vision
  Workshops (ICCVW) 2017
- **Journal**: None
- **Summary**: Disguised face identification (DFI) is an extremely challenging problem due to the numerous variations that can be introduced using different disguises. This paper introduces a deep learning framework to first detect 14 facial key-points which are then utilized to perform disguised face identification. Since the training of deep learning architectures relies on large annotated datasets, two annotated facial key-points datasets are introduced. The effectiveness of the facial keypoint detection framework is presented for each keypoint. The superiority of the key-point detection framework is also demonstrated by a comparison with other deep networks. The effectiveness of classification performance is also demonstrated by comparison with the state-of-the-art face disguise classification methods.



### Adversarial nets with perceptual losses for text-to-image synthesis
- **Arxiv ID**: http://arxiv.org/abs/1708.09321v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09321v1)
- **Published**: 2017-08-30 15:21:11+00:00
- **Updated**: 2017-08-30 15:21:11+00:00
- **Authors**: Miriam Cha, Youngjune Gwon, H. T. Kung
- **Comment**: None
- **Journal**: None
- **Summary**: Recent approaches in generative adversarial networks (GANs) can automatically synthesize realistic images from descriptive text. Despite the overall fair quality, the generated images often expose visible flaws that lack structural definition for an object of interest. In this paper, we aim to extend state of the art for GAN-based text-to-image synthesis by improving perceptual quality of generated images. Differentiated from previous work, our synthetic image generator optimizes on perceptual loss functions that measure pixel, feature activation, and texture differences against a natural image. We present visually more compelling synthetic images of birds and flowers generated from text descriptions in comparison to some of the most prominent existing work.



### Deep Learning to Improve Breast Cancer Early Detection on Screening Mammography
- **Arxiv ID**: http://arxiv.org/abs/1708.09427v5
- **DOI**: 10.1038/s41598-019-48995-4
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.09427v5)
- **Published**: 2017-08-30 18:46:16+00:00
- **Updated**: 2018-12-31 23:23:07+00:00
- **Authors**: Li Shen, Laurie R. Margolies, Joseph H. Rothstein, Eugene Fluder, Russell B. McBride, Weiva Sieh
- **Comment**: Major modification with an additional figure and new results
- **Journal**: Scientific Reports, volume 9, Article number: 12495 (2019)
- **Summary**: The rapid development of deep learning, a family of machine learning techniques, has spurred much interest in its application to medical imaging problems. Here, we develop a deep learning algorithm that can accurately detect breast cancer on screening mammograms using an "end-to-end" training approach that efficiently leverages training datasets with either complete clinical annotation or only the cancer status (label) of the whole image. In this approach, lesion annotations are required only in the initial training stage, and subsequent stages require only image-level labels, eliminating the reliance on rarely available lesion annotations. Our all convolutional network method for classifying screening mammograms attained excellent performance in comparison with previous methods. On an independent test set of digitized film mammograms from Digital Database for Screening Mammography (DDSM), the best single model achieved a per-image AUC of 0.88, and four-model averaging improved the AUC to 0.91 (sensitivity: 86.1%, specificity: 80.1%). On a validation set of full-field digital mammography (FFDM) images from the INbreast database, the best single model achieved a per-image AUC of 0.95, and four-model averaging improved the AUC to 0.98 (sensitivity: 86.7%, specificity: 96.1%). We also demonstrate that a whole image classifier trained using our end-to-end approach on the DDSM digitized film mammograms can be transferred to INbreast FFDM images using only a subset of the INbreast data for fine-tuning and without further reliance on the availability of lesion annotations. These findings show that automatic deep learning methods can be readily trained to attain high accuracy on heterogeneous mammography platforms, and hold tremendous promise for improving clinical tools to reduce false positive and false negative screening mammography results.



### Learning Invariant Riemannian Geometric Representations Using Deep Nets
- **Arxiv ID**: http://arxiv.org/abs/1708.09485v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09485v2)
- **Published**: 2017-08-30 21:57:17+00:00
- **Updated**: 2017-09-22 23:16:22+00:00
- **Authors**: Suhas Lohit, Pavan Turaga
- **Comment**: Accepted at International Conference on Computer Vision Workshop
  (ICCVW), 2017 on Manifold Learning: from Euclid to Riemann
- **Journal**: None
- **Summary**: Non-Euclidean constraints are inherent in many kinds of data in computer vision and machine learning, typically as a result of specific invariance requirements that need to be respected during high-level inference. Often, these geometric constraints can be expressed in the language of Riemannian geometry, where conventional vector space machine learning does not apply directly. The central question this paper deals with is: How does one train deep neural nets whose final outputs are elements on a Riemannian manifold? To answer this, we propose a general framework for manifold-aware training of deep neural networks -- we utilize tangent spaces and exponential maps in order to convert the proposed problem into a form that allows us to bring current advances in deep learning to bear upon this problem. We describe two specific applications to demonstrate this approach: prediction of probability distributions for multi-class image classification, and prediction of illumination-invariant subspaces from a single face-image via regression on the Grassmannian. These applications show the generality of the proposed framework, and result in improved performance over baselines that ignore the geometry of the output space. In addition to solving this specific problem, we believe this paper opens new lines of enquiry centered on the implications of Riemannian geometry on deep architectures.



