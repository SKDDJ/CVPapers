# Arxiv Papers in cs.CV on 2017-08-28
### ChainerCV: a Library for Deep Learning in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1708.08169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08169v1)
- **Published**: 2017-08-28 02:54:11+00:00
- **Updated**: 2017-08-28 02:54:11+00:00
- **Authors**: Yusuke Niitani, Toru Ogawa, Shunta Saito, Masaki Saito
- **Comment**: Accepted to ACM MM 2017 Open Source Software Competition
- **Journal**: None
- **Summary**: Despite significant progress of deep learning in the field of computer vision, there has not been a software library that covers these methods in a unifying manner. We introduce ChainerCV, a software library that is intended to fill this gap. ChainerCV supports numerous neural network models as well as software components needed to conduct research in computer vision. These implementations emphasize simplicity, flexibility and good software engineering practices. The library is designed to perform on par with the results reported in published papers and its tools can be used as a baseline for future research in computer vision. Our implementation includes sophisticated models like Faster R-CNN and SSD, and covers tasks such as object detection and semantic segmentation.



### An Optimized Union-Find Algorithm for Connected Components Labeling Using GPUs
- **Arxiv ID**: http://arxiv.org/abs/1708.08180v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08180v2)
- **Published**: 2017-08-28 03:54:19+00:00
- **Updated**: 2017-08-29 00:54:28+00:00
- **Authors**: Jun Chen, Qiang Yao, Houari Sabirin, Keisuke Nonaka, Hiroshi Sankoh, Sei Naito
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we report an optimized union-find (UF) algorithm that can label the connected components on a 2D image efficiently by employing the GPU architecture. The proposed method contains three phases: UF-based local merge, boundary analysis, and link. The coarse labeling in local merge reduces the number atomic operations, while the boundary analysis only manages the pixels on the boundary of each block. Evaluation results showed that the proposed algorithm speed up the average running time by more than 1.3X.



### An IoT Real-Time Biometric Authentication System Based on ECG Fiducial Extracted Features Using Discrete Cosine Transform
- **Arxiv ID**: http://arxiv.org/abs/1708.08189v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, J.3; K.6.5; H.1.2; C.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1708.08189v1)
- **Published**: 2017-08-28 04:59:08+00:00
- **Updated**: 2017-08-28 04:59:08+00:00
- **Authors**: Ahmed F. Hussein, Abbas K. AlZubaidi, Ali Al-Bayaty, Qais A. Habash
- **Comment**: 6 pages, 8 figures, IoT, Authentication, ECG, DCT
- **Journal**: None
- **Summary**: The conventional authentication technologies, like RFID tags and authentication cards/badges, suffer from different weaknesses, therefore a prompt replacement to use biometric method of authentication should be applied instead. Biometrics, such as fingerprints, voices, and ECG signals, are unique human characters that can be used for authentication processing. In this work, we present an IoT real-time authentication system based on using extracted ECG features to identify the unknown persons. The Discrete Cosine Transform (DCT) is used as an ECG feature extraction, where it has better characteristics for real-time system implementations. There are a substantial number of researches with a high accuracy of authentication, but most of them ignore the real-time capability of authenticating individuals. With the accuracy rate of 97.78% at around 1.21 seconds of processing time, the proposed system is more suitable for use in many applications that require fast and reliable authentication processing demands.



### A Probabilistic Quality Representation Approach to Deep Blind Image Quality Prediction
- **Arxiv ID**: http://arxiv.org/abs/1708.08190v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08190v2)
- **Published**: 2017-08-28 05:09:44+00:00
- **Updated**: 2017-12-05 04:55:26+00:00
- **Authors**: Hui Zeng, Lei Zhang, Alan C. Bovik
- **Comment**: Add the link of source code
- **Journal**: None
- **Summary**: Blind image quality assessment (BIQA) remains a very challenging problem due to the unavailability of a reference image. Deep learning based BIQA methods have been attracting increasing attention in recent years, yet it remains a difficult task to train a robust deep BIQA model because of the very limited number of training samples with human subjective scores. Most existing methods learn a regression network to minimize the prediction error of a scalar image quality score. However, such a scheme ignores the fact that an image will receive divergent subjective scores from different subjects, which cannot be adequately represented by a single scalar number. This is particularly true on complex, real-world distorted images. Moreover, images may broadly differ in their distributions of assigned subjective scores. Recognizing this, we propose a new representation of perceptual image quality, called probabilistic quality representation (PQR), to describe the image subjective score distribution, whereby a more robust loss function can be employed to train a deep BIQA model. The proposed PQR method is shown to not only speed up the convergence of deep model training, but to also greatly improve the achievable level of quality prediction accuracy relative to scalar quality score regression methods. The source code is available at https://github.com/HuiZeng/BIQA_Toolbox.



### Cross-Age LFW: A Database for Studying Cross-Age Face Recognition in Unconstrained Environments
- **Arxiv ID**: http://arxiv.org/abs/1708.08197v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/1708.08197v1)
- **Published**: 2017-08-28 06:07:27+00:00
- **Updated**: 2017-08-28 06:07:27+00:00
- **Authors**: Tianyue Zheng, Weihong Deng, Jiani Hu
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Labeled Faces in the Wild (LFW) database has been widely utilized as the benchmark of unconstrained face verification and due to big data driven machine learning methods, the performance on the database approaches nearly 100%. However, we argue that this accuracy may be too optimistic because of some limiting factors. Besides different poses, illuminations, occlusions and expressions, cross-age face is another challenge in face recognition. Different ages of the same person result in large intra-class variations and aging process is unavoidable in real world face verification. However, LFW does not pay much attention on it. Thereby we construct a Cross-Age LFW (CALFW) which deliberately searches and selects 3,000 positive face pairs with age gaps to add aging process intra-class variance. Negative pairs with same gender and race are also selected to reduce the influence of attribute difference between positive/negative pairs and achieve face verification instead of attributes classification. We evaluate several metric learning and deep learning methods on the new database. Compared to the accuracy on LFW, the accuracy drops about 10%-17% on CALFW.



### Automatic Dataset Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1708.08201v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08201v2)
- **Published**: 2017-08-28 06:22:00+00:00
- **Updated**: 2018-04-16 04:38:45+00:00
- **Authors**: Yalong Bai, Kuiyuan Yang, Tao Mei, Wei-Ying Ma, Tiejun Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Large scale image dataset and deep convolutional neural network (DCNN) are two primary driving forces for the rapid progress made in generic object recognition tasks in recent years. While lots of network architectures have been continuously designed to pursue lower error rates, few efforts are devoted to enlarge existing datasets due to high labeling cost and unfair comparison issues. In this paper, we aim to achieve lower error rate by augmenting existing datasets in an automatic manner. Our method leverages both Web and DCNN, where Web provides massive images with rich contextual information, and DCNN replaces human to automatically label images under guidance of Web contextual information. Experiments show our method can automatically scale up existing datasets significantly from billions web pages with high accuracy, and significantly improve the performance on object recognition tasks by using the automatically augmented datasets, which demonstrates that more supervisory information has been automatically gathered from the Web. Both the dataset and models trained on the dataset are made publicly available.



### Digital image splicing detection based on Markov features in QDCT and QWT domain
- **Arxiv ID**: http://arxiv.org/abs/1708.08245v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1708.08245v3)
- **Published**: 2017-08-28 09:24:51+00:00
- **Updated**: 2017-12-21 03:42:52+00:00
- **Authors**: Ruxin Wang, Wei Lu, Shijun Xiang, Xianfeng Zhao, Jinwei Wang
- **Comment**: We found that in the experiments all the features are not normalized
  with the same parameter and setup. So the whole experiments must be re-done
  since the data is not proper for its current state. Maybe the method should
  be re-designed
- **Journal**: None
- **Summary**: Image splicing detection is of fundamental importance in digital forensics and therefore has attracted increasing attention recently. In this paper, a color image splicing detection approach is proposed based on Markov transition probability of quaternion component separation in quaternion discrete cosine transform (QDCT) domain and quaternion wavelet transform (QWT) domain. Firstly, Markov features of the intra-block and inter-block between block QDCT coefficients are obtained from the real part and three imaginary parts of QDCT coefficients respectively. Then, additional Markov features are extracted from luminance (Y) channel in quaternion wavelet transform domain to characterize the dependency of position among quaternion wavelet subband coefficients. Finally, ensemble classifier (EC) is exploited to classify the spliced and authentic color images. The experiment results demonstrate that the proposed approach can outperforms some state-of-the-art methods.



### A Compromise Principle in Deep Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1708.08267v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08267v2)
- **Published**: 2017-08-28 10:51:35+00:00
- **Updated**: 2018-06-12 05:33:06+00:00
- **Authors**: Huan Fu, Mingming Gong, Chaohui Wang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular depth estimation, which plays a key role in understanding 3D scene geometry, is fundamentally an ill-posed problem. Existing methods based on deep convolutional neural networks (DCNNs) have examined this problem by learning convolutional networks to estimate continuous depth maps from monocular images. However, we find that training a network to predict a high spatial resolution continuous depth map often suffers from poor local solutions. In this paper, we hypothesize that achieving a compromise between spatial and depth resolutions can improve network training. Based on this "compromise principle", we propose a regression-classification cascaded network (RCCN), which consists of a regression branch predicting a low spatial resolution continuous depth map and a classification branch predicting a high spatial resolution discrete depth map. The two branches form a cascaded structure allowing the classification and regression branches to benefit from each other. By leveraging large-scale raw training datasets and some data augmentation strategies, our network achieves top or state-of-the-art results on the NYU Depth V2, KITTI, and Make3D benchmarks.



### Stylizing Face Images via Multiple Exemplars
- **Arxiv ID**: http://arxiv.org/abs/1708.08288v1
- **DOI**: 10.1016/j.cviu.2017.08.009
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1708.08288v1)
- **Published**: 2017-08-28 12:36:33+00:00
- **Updated**: 2017-08-28 12:36:33+00:00
- **Authors**: Yibing Song, Linchao Bao, Shengfeng He, Qingxiong Yang, Ming-Hsuan Yang
- **Comment**: In CVIU 2017. Project Page:
  http://www.cs.cityu.edu.hk/~yibisong/cviu17/index.html
- **Journal**: None
- **Summary**: We address the problem of transferring the style of a headshot photo to face images. Existing methods using a single exemplar lead to inaccurate results when the exemplar does not contain sufficient stylized facial components for a given photo. In this work, we propose an algorithm to stylize face images using multiple exemplars containing different subjects in the same style. Patch correspondences between an input photo and multiple exemplars are established using a Markov Random Field (MRF), which enables accurate local energy transfer via Laplacian stacks. As image patches from multiple exemplars are used, the boundaries of facial components on the target image are inevitably inconsistent. The artifacts are removed by a post-processing step using an edge-preserving filter. Experimental results show that the proposed algorithm consistently produces visually pleasing results.



### Open-World Visual Recognition Using Knowledge Graphs
- **Arxiv ID**: http://arxiv.org/abs/1708.08310v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.08310v1)
- **Published**: 2017-08-28 13:45:07+00:00
- **Updated**: 2017-08-28 13:45:07+00:00
- **Authors**: Vincent P. A. Lonij, Ambrish Rawat, Maria-Irina Nicolae
- **Comment**: None
- **Journal**: None
- **Summary**: In a real-world setting, visual recognition systems can be brought to make predictions for images belonging to previously unknown class labels. In order to make semantically meaningful predictions for such inputs, we propose a two-step approach that utilizes information from knowledge graphs. First, a knowledge-graph representation is learned to embed a large set of entities into a semantic space. Second, an image representation is learned to embed images into the same space. Under this setup, we are able to predict structured properties in the form of relationship triples for any open-world image. This is true even when a set of labels has been omitted from the training protocols of both the knowledge graph and image embeddings. Furthermore, we append this learning framework with appropriate smoothness constraints and show how prior knowledge can be incorporated into the model. Both these improvements combined increase performance for visual recognition by a factor of six compared to our baseline. Finally, we propose a new, extended dataset which we use for experiments.



### Deep Learning Sparse Ternary Projections for Compressed Sensing of Images
- **Arxiv ID**: http://arxiv.org/abs/1708.08311v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.08311v1)
- **Published**: 2017-08-28 13:51:09+00:00
- **Updated**: 2017-08-28 13:51:09+00:00
- **Authors**: Duc Minh Nguyen, Evaggelia Tsiligianni, Nikos Deligiannis
- **Comment**: To appear in GlobalSIP 2017
- **Journal**: None
- **Summary**: Compressed sensing (CS) is a sampling theory that allows reconstruction of sparse (or compressible) signals from an incomplete number of measurements, using of a sensing mechanism implemented by an appropriate projection matrix. The CS theory is based on random Gaussian projection matrices, which satisfy recovery guarantees with high probability; however, sparse ternary {0, -1, +1} projections are more suitable for hardware implementation. In this paper, we present a deep learning approach to obtain very sparse ternary projections for compressed sensing. Our deep learning architecture jointly learns a pair of a projection matrix and a reconstruction operator in an end-to-end fashion. The experimental results on real images demonstrate the effectiveness of the proposed approach compared to state-of-the-art methods, with significant advantage in terms of complexity.



### NODDI-SH: a computational efficient NODDI extension for fODF estimation in diffusion MRI
- **Arxiv ID**: http://arxiv.org/abs/1708.08999v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1708.08999v2)
- **Published**: 2017-08-28 14:15:14+00:00
- **Updated**: 2017-09-01 09:43:22+00:00
- **Authors**: Mauro Zucchelli, Maxime Descoteaux, Gloria Menegaz
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion Magnetic Resonance Imaging (DMRI) is the only non-invasive imaging technique which is able to detect the principal directions of water diffusion as well as neurites density in the human brain. Exploiting the ability of Spherical Harmonics (SH) to model spherical functions, we propose a new reconstruction model for DMRI data which is able to estimate both the fiber Orientation Distribution Function (fODF) and the relative volume fractions of the neurites in each voxel, which is robust to multiple fiber crossings. We consider a Neurite Orientation Dispersion and Density Imaging (NODDI) inspired single fiber diffusion signal to be derived from three compartments: intracellular, extracellular, and cerebrospinal fluid. The model, called NODDI-SH, is derived by convolving the single fiber response with the fODF in each voxel. NODDI-SH embeds the calculation of the fODF and the neurite density in a unified mathematical model providing efficient, robust and accurate results. Results were validated on simulated data and tested on \textit{in-vivo} data of human brain, and compared to and Constrained Spherical Deconvolution (CSD) for benchmarking. Results revealed competitive performance in all respects and inherent adaptivity to local microstructure, while sensibly reducing the computational cost. We also investigated NODDI-SH performance when only a limited number of samples are available for the fitting, demonstrating that 60 samples are enough to obtain reliable results. The fast computational time and the low number of signal samples required, make NODDI-SH feasible for clinical application.



### DeepPrior++: Improving Fast and Accurate 3D Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1708.08325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08325v1)
- **Published**: 2017-08-28 14:15:49+00:00
- **Updated**: 2017-08-28 14:15:49+00:00
- **Authors**: Markus Oberweger, Vincent Lepetit
- **Comment**: To appear in ICCV Workshops 2017
- **Journal**: None
- **Summary**: DeepPrior is a simple approach based on Deep Learning that predicts the joint 3D locations of a hand given a depth map. Since its publication early 2015, it has been outperformed by several impressive works. Here we show that with simple improvements: adding ResNet layers, data augmentation, and better initial hand localization, we achieve better or similar performance than more sophisticated recent methods on the three main benchmarks (NYU, ICVL, MSRA) while keeping the simplicity of the original method. Our new implementation is available at https://github.com/moberweger/deep-prior-pp .



### Framing U-Net via Deep Convolutional Framelets: Application to Sparse-view CT
- **Arxiv ID**: http://arxiv.org/abs/1708.08333v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.08333v3)
- **Published**: 2017-08-28 14:31:50+00:00
- **Updated**: 2018-03-28 07:20:28+00:00
- **Authors**: Yoseob Han, Jong Chul Ye
- **Comment**: This will appear in IEEE Transaction on Medical Imaging, a special
  issue of Machine Learning for Image Reconstruction
- **Journal**: None
- **Summary**: X-ray computed tomography (CT) using sparse projection views is a recent approach to reduce the radiation dose. However, due to the insufficient projection views, an analytic reconstruction approach using the filtered back projection (FBP) produces severe streaking artifacts. Recently, deep learning approaches using large receptive field neural networks such as U-Net have demonstrated impressive performance for sparse- view CT reconstruction. However, theoretical justification is still lacking. Inspired by the recent theory of deep convolutional framelets, the main goal of this paper is, therefore, to reveal the limitation of U-Net and propose new multi-resolution deep learning schemes. In particular, we show that the alternative U- Net variants such as dual frame and the tight frame U-Nets satisfy the so-called frame condition which make them better for effective recovery of high frequency edges in sparse view- CT. Using extensive experiments with real patient data set, we demonstrate that the new network architectures provide better reconstruction performance.



### Automatic Discovery and Geotagging of Objects from Street View Imagery
- **Arxiv ID**: http://arxiv.org/abs/1708.08417v2
- **DOI**: 10.3390/rs10050661
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08417v2)
- **Published**: 2017-08-28 16:54:16+00:00
- **Updated**: 2017-12-01 17:05:02+00:00
- **Authors**: Vladimir A. Krylov, Eamonn Kenny, Rozenn Dahyot
- **Comment**: Video demo at https://youtu.be/X0tM_iSRJMw
- **Journal**: None
- **Summary**: Many applications such as autonomous navigation, urban planning and asset monitoring, rely on the availability of accurate information about objects and their geolocations. In this paper we propose to automatically detect and compute the GPS coordinates of recurring stationary objects of interest using street view imagery. Our processing pipeline relies on two fully convolutional neural networks: the first segments objects in the images while the second estimates their distance from the camera. To geolocate all the detected objects coherently we propose a novel custom Markov Random Field model to perform objects triangulation. The novelty of the resulting pipeline is the combined use of monocular depth estimation and triangulation to enable automatic mapping of complex scenes with multiple visually similar objects of interest. We validate experimentally the effectiveness of our approach on two object classes: traffic lights and telegraph poles. The experiments report high object recall rates and GPS accuracy within 2 meters, which is comparable with the precision of single-frequency GPS receivers.



### Deep Belief Networks used on High Resolution Multichannel Electroencephalography Data for Seizure Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.08430v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1708.08430v1)
- **Published**: 2017-08-28 17:28:48+00:00
- **Updated**: 2017-08-28 17:28:48+00:00
- **Authors**: JT Turner, Adam Page, Tinoosh Mohsenin, Tim Oates
- **Comment**: Old draft of AAAI paper, AAAI Spring Symposium Series. 2014
- **Journal**: None
- **Summary**: Ubiquitous bio-sensing for personalized health monitoring is slowly becoming a reality with the increasing availability of small, diverse, robust, high fidelity sensors. This oncoming flood of data begs the question of how we will extract useful information from it. In this paper we explore the use of a variety of representations and machine learning algorithms applied to the task of seizure detection in high resolution, multichannel EEG data. We explore classification accuracy, computational complexity and memory requirements with a view toward understanding which approaches are most suitable for such tasks as the number of people involved and the amount of data they produce grows to be quite large. In particular, we show that layered learning approaches such as Deep Belief Networks excel along these dimensions.



### On denoising autoencoders trained to minimise binary cross-entropy
- **Arxiv ID**: http://arxiv.org/abs/1708.08487v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.08487v2)
- **Published**: 2017-08-28 19:07:33+00:00
- **Updated**: 2017-10-09 08:40:39+00:00
- **Authors**: Antonia Creswell, Kai Arulkumaran, Anil A. Bharath
- **Comment**: Submitted to Pattern Recognition Letters
- **Journal**: None
- **Summary**: Denoising autoencoders (DAEs) are powerful deep learning models used for feature extraction, data generation and network pre-training. DAEs consist of an encoder and decoder which may be trained simultaneously to minimise a loss (function) between an input and the reconstruction of a corrupted version of the input. There are two common loss functions used for training autoencoders, these include the mean-squared error (MSE) and the binary cross-entropy (BCE). When training autoencoders on image data a natural choice of loss function is BCE, since pixel values may be normalised to take values in [0,1] and the decoder model may be designed to generate samples that take values in (0,1). We show theoretically that DAEs trained to minimise BCE may be used to take gradient steps in the data space towards regions of high probability under the data-generating distribution. Previously this had only been shown for DAEs trained using MSE. As a consequence of the theory, iterative application of a trained DAE moves a data sample from regions of low probability to regions of higher probability under the data-generating distribution. Firstly, we validate the theory by showing that novel data samples, consistent with the training data, may be synthesised when the initial data samples are random noise. Secondly, we motivate the theory by showing that initial data samples synthesised via other methods may be improved via iterative application of a trained DAE to those initial samples.



### Subspace Selection to Suppress Confounding Source Domain Information in AAM Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.08508v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.08508v2)
- **Published**: 2017-08-28 20:21:21+00:00
- **Updated**: 2017-10-03 18:26:16+00:00
- **Authors**: Azin Asgarian, Ahmed Bilal Ashraf, David Fleet, Babak Taati
- **Comment**: Copyright IEEE. To be published in the proceedings of International
  Joint Conference on Biometrics (IJCB) 2017
- **Journal**: None
- **Summary**: Active appearance models (AAMs) are a class of generative models that have seen tremendous success in face analysis. However, model learning depends on the availability of detailed annotation of canonical landmark points. As a result, when accurate AAM fitting is required on a different set of variations (expression, pose, identity), a new dataset is collected and annotated. To overcome the need for time consuming data collection and annotation, transfer learning approaches have received recent attention. The goal is to transfer knowledge from previously available datasets (source) to a new dataset (target). We propose a subspace transfer learning method, in which we select a subspace from the source that best describes the target space. We propose a metric to compute the directional similarity between the source eigenvectors and the target subspace. We show an equivalence between this metric and the variance of target data when projected onto source eigenvectors. Using this equivalence, we select a subset of source principal directions that capture the variance in target data. To define our model, we augment the selected source subspace with the target subspace learned from a handful of target examples. In experiments done on six publicly available datasets, we show that our approach outperforms the state of the art in terms of the RMS fitting error as well as the percentage of test examples for which AAM fitting converges to the ground truth.



