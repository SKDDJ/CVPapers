# Arxiv Papers in cs.CV on 2017-08-03
### Low Dose CT Image Denoising Using a Generative Adversarial Network with Wasserstein Distance and Perceptual Loss
- **Arxiv ID**: http://arxiv.org/abs/1708.00961v2
- **DOI**: 10.1109/TMI.2018.2827462
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00961v2)
- **Published**: 2017-08-03 00:37:11+00:00
- **Updated**: 2018-04-24 14:28:27+00:00
- **Authors**: Qingsong Yang, Pingkun Yan, Yanbo Zhang, Hengyong Yu, Yongyi Shi, Xuanqin Mou, Mannudeep K. Kalra, Ge Wang
- **Comment**: None
- **Journal**: IEEE Trans. Med. Imaging. 37(2018) 1348-1357
- **Summary**: In this paper, we introduce a new CT image denoising method based on the generative adversarial network (GAN) with Wasserstein distance and perceptual similarity. The Wasserstein distance is a key concept of the optimal transform theory, and promises to improve the performance of the GAN. The perceptual loss compares the perceptual features of a denoised output against those of the ground truth in an established feature space, while the GAN helps migrate the data noise distribution from strong to weak. Therefore, our proposed method transfers our knowledge of visual perception to the image denoising task, is capable of not only reducing the image noise level but also keeping the critical information at the same time. Promising results have been obtained in our experiments with clinical CT images.



### Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1708.02286v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.02286v2)
- **Published**: 2017-08-03 02:35:17+00:00
- **Updated**: 2017-09-29 14:41:58+00:00
- **Authors**: Shuangjie Xu, Yu Cheng, Kang Gu, Yang Yang, Shiyu Chang, Pan Zhou
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: Person Re-Identification (person re-id) is a crucial task as its applications in visual surveillance and human-computer interaction. In this work, we present a novel joint Spatial and Temporal Attention Pooling Network (ASTPN) for video-based person re-identification, which enables the feature extractor to be aware of the current input video sequences, in a way that interdependency from the matching items can directly influence the computation of each other's representation. Specifically, the spatial pooling layer is able to select regions from each frame, while the attention temporal pooling performed can select informative frames over the sequence, both pooling guided by the information from distance matching. Experiments are conduced on the iLIDS-VID, PRID-2011 and MARS datasets and the results demonstrate that this approach outperforms existing state-of-art methods. We also analyze how the joint pooling in both dimensions can boost the person re-id performance more effectively than using either of them separately.



### Attention Transfer from Web Images for Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.00973v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1708.00973v1)
- **Published**: 2017-08-03 02:41:15+00:00
- **Updated**: 2017-08-03 02:41:15+00:00
- **Authors**: Junnan Li, Yongkang Wong, Qi Zhao, Mohan Kankanhalli
- **Comment**: ACM Multimedia, 2017
- **Journal**: None
- **Summary**: Training deep learning based video classifiers for action recognition requires a large amount of labeled videos. The labeling process is labor-intensive and time-consuming. On the other hand, large amount of weakly-labeled images are uploaded to the Internet by users everyday. To harness the rich and highly diverse set of Web images, a scalable approach is to crawl these images to train deep learning based classifier, such as Convolutional Neural Networks (CNN). However, due to the domain shift problem, the performance of Web images trained deep classifiers tend to degrade when directly deployed to videos. One way to address this problem is to fine-tune the trained models on videos, but sufficient amount of annotated videos are still required. In this work, we propose a novel approach to transfer knowledge from image domain to video domain. The proposed method can adapt to the target domain (i.e. video data) with limited amount of training data. Our method maps the video frames into a low-dimensional feature space using the class-discriminative spatial attention map for CNNs. We design a novel Siamese EnergyNet structure to learn energy functions on the attention maps by jointly optimizing two loss functions, such that the attention map corresponding to a ground truth concept would have higher energy. We conduct extensive experiments on two challenging video recognition datasets (i.e. TVHI and UCF101), and demonstrate the efficacy of our proposed method.



### ORGB: Offset Correction in RGB Color Space for Illumination-Robust Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1708.00975v1
- **DOI**: 10.1109/ICASSP.2017.7952418
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00975v1)
- **Published**: 2017-08-03 02:54:05+00:00
- **Updated**: 2017-08-03 02:54:05+00:00
- **Authors**: Zhenqiang Ying, Ge Li, Sixin Wen, Guozhen Tan
- **Comment**: Project website: https://baidut.github.io/ORGB/
- **Journal**: None
- **Summary**: Single materials have colors which form straight lines in RGB space. However, in severe shadow cases, those lines do not intersect the origin, which is inconsistent with the description of most literature. This paper is concerned with the detection and correction of the offset between the intersection and origin. First, we analyze the reason for forming that offset via an optical imaging model. Second, we present a simple and effective way to detect and remove the offset. The resulting images, named ORGB, have almost the same appearance as the original RGB images while are more illumination-robust for color space conversion. Besides, image processing using ORGB instead of RGB is free from the interference of shadows. Finally, the proposed offset correction method is applied to road detection task, improving the performance both in quantitative and qualitative evaluations.



### CNN-based Real-time Dense Face Reconstruction with Inverse-rendered Photo-realistic Face Images
- **Arxiv ID**: http://arxiv.org/abs/1708.00980v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00980v3)
- **Published**: 2017-08-03 03:18:34+00:00
- **Updated**: 2018-05-15 07:02:35+00:00
- **Authors**: Yudong Guo, Juyong Zhang, Jianfei Cai, Boyi Jiang, Jianmin Zheng
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence, 2018
- **Journal**: None
- **Summary**: With the powerfulness of convolution neural networks (CNN), CNN based face reconstruction has recently shown promising performance in reconstructing detailed face shape from 2D face images. The success of CNN-based methods relies on a large number of labeled data. The state-of-the-art synthesizes such data using a coarse morphable face model, which however has difficulty to generate detailed photo-realistic images of faces (with wrinkles). This paper presents a novel face data generation method. Specifically, we render a large number of photo-realistic face images with different attributes based on inverse rendering. Furthermore, we construct a fine-detailed face image dataset by transferring different scales of details from one image to another. We also construct a large number of video-type adjacent frame pairs by simulating the distribution of real video data. With these nicely constructed datasets, we propose a coarse-to-fine learning framework consisting of three convolutional networks. The networks are trained for real-time detailed 3D face reconstruction from monocular video as well as from a single image. Extensive experimental results demonstrate that our framework can produce high-quality reconstruction but with much less computation time compared to the state-of-the-art. Moreover, our method is robust to pose, expression and lighting due to the diversity of data.



### Multi-Planar Deep Segmentation Networks for Cardiac Substructures from MRI and CT
- **Arxiv ID**: http://arxiv.org/abs/1708.00983v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.00983v1)
- **Published**: 2017-08-03 03:30:03+00:00
- **Updated**: 2017-08-03 03:30:03+00:00
- **Authors**: Aliasghar Mortazi, Jeremy Burt, Ulas Bagci
- **Comment**: The paper is accepted to STACOM 2017
- **Journal**: None
- **Summary**: Non-invasive detection of cardiovascular disorders from radiology scans requires quantitative image analysis of the heart and its substructures. There are well-established measurements that radiologists use for diseases assessment such as ejection fraction, volume of four chambers, and myocardium mass. These measurements are derived as outcomes of precise segmentation of the heart and its substructures. The aim of this paper is to provide such measurements through an accurate image segmentation algorithm that automatically delineates seven substructures of the heart from MRI and/or CT scans. Our proposed method is based on multi-planar deep convolutional neural networks (CNN) with an adaptive fusion strategy where we automatically utilize complementary information from different planes of the 3D scans for improved delineations. For CT and MRI, we have separately designed three CNNs (the same architectural configuration) for three planes, and have trained the networks from scratch for voxel-wise labeling for the following cardiac structures: myocardium of left ventricle (Myo), left atrium (LA), left ventricle (LV), right atrium (RA), right ventricle (RV), ascending aorta (Ao), and main pulmonary artery (PA). We have evaluated the proposed method with 4-fold-cross validation on the multi-modality whole heart segmentation challenge (MM-WHS 2017) dataset. The precision and dice index of 0.93 and 0.90, and 0.87 and 0.85 were achieved for CT and MR images, respectively. While a CT volume was segmented about 50 seconds, an MRI scan was segmented around 17 seconds with the GPUs/CUDA implementation.



### Extreme Low Resolution Activity Recognition with Multi-Siamese Embedding Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.00999v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00999v2)
- **Published**: 2017-08-03 04:52:28+00:00
- **Updated**: 2018-02-04 03:46:19+00:00
- **Authors**: Michael S. Ryoo, Kiyoon Kim, Hyun Jong Yang
- **Comment**: AAAI 2018
- **Journal**: None
- **Summary**: This paper presents an approach for recognizing human activities from extreme low resolution (e.g., 16x12) videos. Extreme low resolution recognition is not only necessary for analyzing actions at a distance but also is crucial for enabling privacy-preserving recognition of human activities. We design a new two-stream multi-Siamese convolutional neural network. The idea is to explicitly capture the inherent property of low resolution (LR) videos that two images originated from the exact same scene often have totally different pixel values depending on their LR transformations. Our approach learns the shared embedding space that maps LR videos with the same content to the same location regardless of their transformations. We experimentally confirm that our approach of jointly learning such transform robust LR video representation and the classifier outperforms the previous state-of-the-art low resolution recognition approaches on two public standard datasets by a meaningful margin.



### Learning Accurate Low-Bit Deep Neural Networks with Stochastic Quantization
- **Arxiv ID**: http://arxiv.org/abs/1708.01001v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01001v1)
- **Published**: 2017-08-03 05:16:25+00:00
- **Updated**: 2017-08-03 05:16:25+00:00
- **Authors**: Yinpeng Dong, Renkun Ni, Jianguo Li, Yurong Chen, Jun Zhu, Hang Su
- **Comment**: BMVC 2017 Oral
- **Journal**: None
- **Summary**: Low-bit deep neural networks (DNNs) become critical for embedded applications due to their low storage requirement and computing efficiency. However, they suffer much from the non-negligible accuracy drop. This paper proposes the stochastic quantization (SQ) algorithm for learning accurate low-bit DNNs. The motivation is due to the following observation. Existing training algorithms approximate the real-valued elements/filters with low-bit representation all together in each iteration. The quantization errors may be small for some elements/filters, while are remarkable for others, which lead to inappropriate gradient direction during training, and thus bring notable accuracy drop. Instead, SQ quantizes a portion of elements/filters to low-bit with a stochastic probability inversely proportional to the quantization error, while keeping the other portion unchanged with full-precision. The quantized and full-precision portions are updated with corresponding gradients separately in each iteration. The SQ ratio is gradually increased until the whole network is quantized. This procedure can greatly compensate the quantization error and thus yield better accuracy for low-bit DNNs. Experiments show that SQ can consistently and significantly improve the accuracy for different low-bit DNNs on various datasets and various network structures.



### Beyond Low Rank: A Data-Adaptive Tensor Completion Method
- **Arxiv ID**: http://arxiv.org/abs/1708.01008v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01008v1)
- **Published**: 2017-08-03 05:51:27+00:00
- **Updated**: 2017-08-03 05:51:27+00:00
- **Authors**: Lei Zhang, Wei Wei, Qinfeng Shi, Chunhua Shen, Anton van den Hengel, Yanning Zhang
- **Comment**: 14 pages, 5 figures
- **Journal**: None
- **Summary**: Low rank tensor representation underpins much of recent progress in tensor completion. In real applications, however, this approach is confronted with two challenging problems, namely (1) tensor rank determination; (2) handling real tensor data which only approximately fulfils the low-rank requirement. To address these two issues, we develop a data-adaptive tensor completion model which explicitly represents both the low-rank and non-low-rank structures in a latent tensor. Representing the non-low-rank structure separately from the low-rank one allows priors which capture the important distinctions between the two, thus enabling more accurate modelling, and ultimately, completion. Through defining a new tensor rank, we develop a sparsity induced prior for the low-rank structure, with which the tensor rank can be automatically determined. The prior for the non-low-rank structure is established based on a mixture of Gaussians which is shown to be flexible enough, and powerful enough, to inform the completion process for a variety of real tensor data. With these two priors, we develop a Bayesian minimum mean squared error estimate (MMSE) framework for inference which provides the posterior mean of missing entries as well as their uncertainty. Compared with the state-of-the-art methods in various applications, the proposed model produces more accurate completion results.



### Sensor Transformation Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.01015v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.01015v1)
- **Published**: 2017-08-03 06:35:36+00:00
- **Updated**: 2017-08-03 06:35:36+00:00
- **Authors**: Stefan Braun, Daniel Neil, Enea Ceolini, Jithendar Anumula, Shih-Chii Liu
- **Comment**: 8 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: Recent work on encoder-decoder models for sequence-to-sequence mapping has shown that integrating both temporal and spatial attention mechanisms into neural networks increases the performance of the system substantially. In this work, we report on the application of an attentional signal not on temporal and spatial regions of the input, but instead as a method of switching among inputs themselves. We evaluate the particular role of attentional switching in the presence of dynamic noise in the sensors, and demonstrate how the attentional signal responds dynamically to changing noise levels in the environment to achieve increased performance on both audio and visual tasks in three commonly-used datasets: TIDIGITS, Wall Street Journal, and GRID. Moreover, the proposed sensor transformation network architecture naturally introduces a number of advantages that merit exploration, including ease of adding new sensors to existing architectures, attentional interpretability, and increased robustness in a variety of noisy environments not seen during training. Finally, we demonstrate that the sensor selection attention mechanism of a model trained only on the small TIDIGITS dataset can be transferred directly to a pre-existing larger network trained on the Wall Street Journal dataset, maintaining functionality of switching between sensors to yield a dramatic reduction of error in the presence of noise.



### When Kernel Methods meet Feature Learning: Log-Covariance Network for Action Recognition from Skeletal Data
- **Arxiv ID**: http://arxiv.org/abs/1708.01022v1
- **DOI**: 10.1109/CVPRW.2017.165
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01022v1)
- **Published**: 2017-08-03 06:52:51+00:00
- **Updated**: 2017-08-03 06:52:51+00:00
- **Authors**: Jacopo Cavazza, Pietro Morerio, Vittorio Murino
- **Comment**: 2017 IEEE Computer Vision and Pattern Recognition (CVPR) Workshops
- **Journal**: None
- **Summary**: Human action recognition from skeletal data is a hot research topic and important in many open domain applications of computer vision, thanks to recently introduced 3D sensors. In the literature, naive methods simply transfer off-the-shelf techniques from video to the skeletal representation. However, the current state-of-the-art is contended between to different paradigms: kernel-based methods and feature learning with (recurrent) neural networks. Both approaches show strong performances, yet they exhibit heavy, but complementary, drawbacks. Motivated by this fact, our work aims at combining together the best of the two paradigms, by proposing an approach where a shallow network is fed with a covariance representation. Our intuition is that, as long as the dynamics is effectively modeled, there is no need for the classification network to be deep nor recurrent in order to score favorably. We validate this hypothesis in a broad experimental analysis over 6 publicly available datasets.



### What Will I Do Next? The Intention from Motion Experiment
- **Arxiv ID**: http://arxiv.org/abs/1708.01034v1
- **DOI**: 10.1109/CVPRW.2017.7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01034v1)
- **Published**: 2017-08-03 07:37:58+00:00
- **Updated**: 2017-08-03 07:37:58+00:00
- **Authors**: Andrea Zunino, Jacopo Cavazza, Atesh Koul, Andrea Cavallo, Cristina Becchio, Vittorio Murino
- **Comment**: 2017 IEEE Conference on Computer Vision and Pattern Recognition
  Workshops
- **Journal**: None
- **Summary**: In computer vision, video-based approaches have been widely explored for the early classification and the prediction of actions or activities. However, it remains unclear whether this modality (as compared to 3D kinematics) can still be reliable for the prediction of human intentions, defined as the overarching goal embedded in an action sequence. Since the same action can be performed with different intentions, this problem is more challenging but yet affordable as proved by quantitative cognitive studies which exploit the 3D kinematics acquired through motion capture systems. In this paper, we bridge cognitive and computer vision studies, by demonstrating the effectiveness of video-based approaches for the prediction of human intentions. Precisely, we propose Intention from Motion, a new paradigm where, without using any contextual information, we consider instantaneous grasping motor acts involving a bottle in order to forecast why the bottle itself has been reached (to pass it or to place in a box, or to pour or to drink the liquid inside). We process only the grasping onsets casting intention prediction as a classification framework. Leveraging on our multimodal acquisition (3D motion capture data and 2D optical videos), we compare the most commonly used 3D descriptors from cognitive studies with state-of-the-art video-based techniques. Since the two analyses achieve an equivalent performance, we demonstrate that computer vision tools are effective in capturing the kinematics and facing the cognitive problem of human intention prediction.



### Using the SLEUTH urban growth model to simulate the impacts of future policy scenarios on urban land use in the Tehran metropolitan area in Iran
- **Arxiv ID**: http://arxiv.org/abs/1708.01089v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1708.01089v1)
- **Published**: 2017-08-03 10:38:51+00:00
- **Updated**: 2017-08-03 10:38:51+00:00
- **Authors**: Shaghayegh Kargozar Nahavandya, Lalit Kumar, Pedram Ghamisi
- **Comment**: 27 pages, 6 figures, and 6 tables
- **Journal**: None
- **Summary**: The SLEUTH model, based on the Cellular Automata (CA), can be applied to city development simulation in metropolitan areas. In this study the SLEUTH model was used to model the urban expansion and predict the future possible behavior of the urban growth in Tehran. The fundamental data were five Landsat TM and ETM images of 1988, 1992, 1998, 2001 and 2010. Three scenarios were designed to simulate the spatial pattern. The first scenario assumed historical urbanization mode would persist and the only limitations for development were height and slope. The second one was a compact scenario which makes the growth mostly internal and limited the expansion of suburban areas. The last scenario proposed a polycentric urban structure which let the little patches grow without any limitation and would not consider the areas beyond the specific buffer zone from the larger patches for development. Results showed that the urban growth rate was greater in the first scenario in comparison with the other two scenarios. Also it was shown that the third scenario was more suitable for Tehran since it could avoid undesirable effects such as congestion and pollution and was more in accordance with the conditions of Tehran city.



### Learning Feature Pyramids for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1708.01101v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01101v1)
- **Published**: 2017-08-03 11:26:04+00:00
- **Updated**: 2017-08-03 11:26:04+00:00
- **Authors**: Wei Yang, Shuang Li, Wanli Ouyang, Hongsheng Li, Xiaogang Wang
- **Comment**: Submitted to ICCV 2017
- **Journal**: None
- **Summary**: Articulated human pose estimation is a fundamental yet challenging task in computer vision. The difficulty is particularly pronounced in scale variations of human body parts when camera view changes or severe foreshortening happens. Although pyramid methods are widely used to handle scale changes at inference time, learning feature pyramids in deep convolutional neural networks (DCNNs) is still not well explored. In this work, we design a Pyramid Residual Module (PRMs) to enhance the invariance in scales of DCNNs. Given input features, the PRMs learn convolutional filters on various scales of input features, which are obtained with different subsampling ratios in a multi-branch network. Moreover, we observe that it is inappropriate to adopt existing methods to initialize the weights of multi-branch networks, which achieve superior performance than plain networks in many tasks recently. Therefore, we provide theoretic derivation to extend the current weight initialization scheme to multi-branch network structures. We investigate our method on two standard benchmarks for human pose estimation. Our approach obtains state-of-the-art results on both benchmarks. Code is available at https://github.com/bearpaw/PyraNet.



### A Unified View-Graph Selection Framework for Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1708.01125v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01125v2)
- **Published**: 2017-08-03 13:17:01+00:00
- **Updated**: 2017-12-04 11:56:25+00:00
- **Authors**: Rajvi Shah, Visesh Chari, P J Narayanan
- **Comment**: Submitted to CVPR 2018
- **Journal**: None
- **Summary**: View-graph is an essential input to large-scale structure from motion (SfM) pipelines. Accuracy and efficiency of large-scale SfM is crucially dependent on the input view-graph. Inconsistent or inaccurate edges can lead to inferior or wrong reconstruction. Most SfM methods remove `undesirable' images and pairs using several, fixed heuristic criteria, and propose tailor-made solutions to achieve specific reconstruction objectives such as efficiency, accuracy, or disambiguation. In contrast to these disparate solutions, we propose a single optimization framework that can be used to achieve these different reconstruction objectives with task-specific cost modeling. We also construct a very efficient network-flow based formulation for its approximate solution. The abstraction brought on by this selection mechanism separates the challenges specific to datasets and reconstruction objectives from the standard SfM pipeline and improves its generalization. This paper demonstrates the application of the proposed view-graph framework with standard SfM pipeline for two particular use-cases, (i) accurate and ghost-free reconstructions of highly ambiguous datasets using costs based on disambiguation priors, and (ii) accurate and efficient reconstruction of large-scale Internet datasets using costs based on commonly used priors.



### Automatic Segmentation and Disease Classification Using Cardiac Cine MR Images
- **Arxiv ID**: http://arxiv.org/abs/1708.01141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01141v1)
- **Published**: 2017-08-03 13:55:24+00:00
- **Updated**: 2017-08-03 13:55:24+00:00
- **Authors**: Jelmer M. Wolterink, Tim Leiner, Max A. Viergever, Ivana Isgum
- **Comment**: Accepted in STACOM Automated Cardiac Diagnosis Challenge 2017
- **Journal**: None
- **Summary**: Segmentation of the heart in cardiac cine MR is clinically used to quantify cardiac function. We propose a fully automatic method for segmentation and disease classification using cardiac cine MR images. A convolutional neural network (CNN) was designed to simultaneously segment the left ventricle (LV), right ventricle (RV) and myocardium in end-diastole (ED) and end-systole (ES) images. Features derived from the obtained segmentations were used in a Random Forest classifier to label patients as suffering from dilated cardiomyopathy, hypertrophic cardiomyopathy, heart failure following myocardial infarction, right ventricular abnormality, or no cardiac disease. The method was developed and evaluated using a balanced dataset containing images of 100 patients, which was provided in the MICCAI 2017 automated cardiac diagnosis challenge (ACDC). The segmentation and classification pipeline were evaluated in a four-fold stratified cross-validation. Average Dice scores between reference and automatically obtained segmentations were 0.94, 0.88 and 0.87 for the LV, RV and myocardium. The classifier assigned 91% of patients to the correct disease category. Segmentation and disease classification took 5 s per patient. The results of our study suggest that image-based diagnosis using cine MR cardiac scans can be performed automatically with high accuracy.



### Three-dimensional planar model estimation using multi-constraint knowledge based on k-means and RANSAC
- **Arxiv ID**: http://arxiv.org/abs/1708.01143v1
- **DOI**: 10.1016/j.asoc.2015.05.007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01143v1)
- **Published**: 2017-08-03 13:58:18+00:00
- **Updated**: 2017-08-03 13:58:18+00:00
- **Authors**: Marcelo Saval-Calvo, Jorge Azorin-Lopez, Andres Fuster-Guillo, Jose Garcia-Rodriguez
- **Comment**: None
- **Journal**: Applied Soft Computing, Vol. 34, p. 572-586 (2015)
- **Summary**: Plane model extraction from three-dimensional point clouds is a necessary step in many different applications such as planar object reconstruction, indoor mapping and indoor localization. Different RANdom SAmple Consensus (RANSAC)-based methods have been proposed for this purpose in recent years. In this study, we propose a novel method-based on RANSAC called Multiplane Model Estimation, which can estimate multiple plane models simultaneously from a noisy point cloud using the knowledge extracted from a scene (or an object) in order to reconstruct it accurately. This method comprises two steps: first, it clusters the data into planar faces that preserve some constraints defined by knowledge related to the object (e.g., the angles between faces); and second, the models of the planes are estimated based on these data using a novel multi-constraint RANSAC. We performed experiments in the clustering and RANSAC stages, which showed that the proposed method performed better than state-of-the-art methods.



### Deep MR to CT Synthesis using Unpaired Data
- **Arxiv ID**: http://arxiv.org/abs/1708.01155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01155v1)
- **Published**: 2017-08-03 14:18:43+00:00
- **Updated**: 2017-08-03 14:18:43+00:00
- **Authors**: Jelmer M. Wolterink, Anna M. Dinkla, Mark H. F. Savenije, Peter R. Seevinck, Cornelis A. T. van den Berg, Ivana Isgum
- **Comment**: MICCAI 2017 Workshop on Simulation and Synthesis in Medical Imaging
- **Journal**: None
- **Summary**: MR-only radiotherapy treatment planning requires accurate MR-to-CT synthesis. Current deep learning methods for MR-to-CT synthesis depend on pairwise aligned MR and CT training images of the same patient. However, misalignment between paired images could lead to errors in synthesized CT images. To overcome this, we propose to train a generative adversarial network (GAN) with unpaired MR and CT images. A GAN consisting of two synthesis convolutional neural networks (CNNs) and two discriminator CNNs was trained with cycle consistency to transform 2D brain MR image slices into 2D brain CT image slices and vice versa. Brain MR and CT images of 24 patients were analyzed. A quantitative evaluation showed that the model was able to synthesize CT images that closely approximate reference CT images, and was able to outperform a GAN model trained with paired MR and CT images.



### Patch-based adaptive weighting with segmentation and scale (PAWSS) for visual tracking
- **Arxiv ID**: http://arxiv.org/abs/1708.01179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01179v1)
- **Published**: 2017-08-03 15:19:27+00:00
- **Updated**: 2017-08-03 15:19:27+00:00
- **Authors**: Xiaofei Du, Alessio Dore, Danail Stoyanov
- **Comment**: 10 pages, 8 figures. The paper is under consideration at Pattern
  Recognition Letters
- **Journal**: None
- **Summary**: Tracking-by-detection algorithms are widely used for visual tracking, where the problem is treated as a classification task where an object model is updated over time using online learning techniques. In challenging conditions where an object undergoes deformation or scale variations, the update step is prone to include background information in the model appearance or to lack the ability to estimate the scale change, which degrades the performance of the classifier. In this paper, we incorporate a Patch-based Adaptive Weighting with Segmentation and Scale (PAWSS) tracking framework that tackles both the scale and background problems. A simple but effective colour-based segmentation model is used to suppress background information and multi-scale samples are extracted to enrich the training pool, which allows the tracker to handle both incremental and abrupt scale variations between frames. Experimentally, we evaluate our approach on the online tracking benchmark (OTB) dataset and Visual Object Tracking (VOT) challenge datasets. The results show that our approach outperforms recent state-of-the-art trackers, and it especially improves the successful rate score on the OTB dataset, while on the VOT datasets, PAWSS ranks among the top trackers while operating at real-time frame rates.



### Unsupervised Video Understanding by Reconciliation of Posture Similarities
- **Arxiv ID**: http://arxiv.org/abs/1708.01191v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01191v1)
- **Published**: 2017-08-03 16:09:03+00:00
- **Updated**: 2017-08-03 16:09:03+00:00
- **Authors**: Timo Milbich, Miguel Bautista, Ekaterina Sutter, Bjorn Ommer
- **Comment**: Accepted by ICCV 2017
- **Journal**: None
- **Summary**: Understanding human activity and being able to explain it in detail surpasses mere action classification by far in both complexity and value. The challenge is thus to describe an activity on the basis of its most fundamental constituents, the individual postures and their distinctive transitions. Supervised learning of such a fine-grained representation based on elementary poses is very tedious and does not scale. Therefore, we propose a completely unsupervised deep learning procedure based solely on video sequences, which starts from scratch without requiring pre-trained networks, predefined body models, or keypoints. A combinatorial sequence matching algorithm proposes relations between frames from subsets of the training data, while a CNN is reconciling the transitivity conflicts of the different subsets to learn a single concerted pose embedding despite changes in appearance across sequences. Without any manual annotation, the model learns a structured representation of postures and their temporal development. The model not only enables retrieval of similar postures but also temporal super-resolution. Additionally, based on a recurrent formulation, next frames can be synthesized.



### Estimating speech from lip dynamics
- **Arxiv ID**: http://arxiv.org/abs/1708.01198v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.CO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.01198v1)
- **Published**: 2017-08-03 16:23:13+00:00
- **Updated**: 2017-08-03 16:23:13+00:00
- **Authors**: Jithin Donny George, Ronan Keane, Conor Zellmer
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this project is to develop a limited lip reading algorithm for a subset of the English language. We consider a scenario in which no audio information is available. The raw video is processed and the position of the lips in each frame is extracted. We then prepare the lip data for processing and classify the lips into visemes and phonemes. Hidden Markov Models are used to predict the words the speaker is saying based on the sequences of classified phonemes and visemes. The GRID audiovisual sentence corpus [10][11] database is used for our study.



### Semantic Augmented Reality Environment with Material-Aware Physical Interactions
- **Arxiv ID**: http://arxiv.org/abs/1708.01208v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GT
- **Links**: [PDF](http://arxiv.org/pdf/1708.01208v3)
- **Published**: 2017-08-03 16:52:14+00:00
- **Updated**: 2018-03-16 14:29:07+00:00
- **Authors**: Long Chen, Karl Francis, Wen Tang
- **Comment**: ISMAR 2017 Poster
- **Journal**: None
- **Summary**: In Augmented Reality (AR) environment, realistic interactions between the virtual and real objects play a crucial role in user experience. Much of recent advances in AR has been largely focused on developing geometry-aware environment, but little has been done in dealing with interactions at the semantic level. High-level scene understanding and semantic descriptions in AR would allow effective design of complex applications and enhanced user experience. In this paper, we present a novel approach and a prototype system that enables the deeper understanding of semantic properties of the real world environment, so that realistic physical interactions between the real and the virtual objects can be generated. A material-aware AR environment has been created based on the deep material learning using a fully convolutional network (FCN). The state-of-the-art dense Simultaneous Localisation and Mapping (SLAM) has been used for the semantic mapping. Together with efficient accelerated 3D ray casting, natural and realistic physical interactions are generated for interactive AR games. Our approach has significant impact on the future development of advanced AR systems and applications.



### Recent Developments and Future Challenges in Medical Mixed Reality
- **Arxiv ID**: http://arxiv.org/abs/1708.01225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01225v1)
- **Published**: 2017-08-03 17:15:18+00:00
- **Updated**: 2017-08-03 17:15:18+00:00
- **Authors**: Long Chen, Thomas Day, Wen Tang, Nigel W. John
- **Comment**: None
- **Journal**: None
- **Summary**: Mixed Reality (MR) is of increasing interest within technology-driven modern medicine but is not yet used in everyday practice. This situation is changing rapidly, however, and this paper explores the emergence of MR technology and the importance of its utility within medical applications. A classification of medical MR has been obtained by applying an unbiased text mining method to a database of 1,403 relevant research papers published over the last two decades. The classification results reveal a taxonomy for the development of medical MR research during this period as well as suggesting future trends. We then use the classification to analyse the technology and applications developed in the last five years. Our objective is to aid researchers to focus on the areas where technology advancements in medical MR are most needed, as well as providing medical practitioners with a useful source of reference.



### Real-time Geometry-Aware Augmented Reality in Minimally Invasive Surgery
- **Arxiv ID**: http://arxiv.org/abs/1708.01234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01234v1)
- **Published**: 2017-08-03 17:25:38+00:00
- **Updated**: 2017-08-03 17:25:38+00:00
- **Authors**: Long Chen, Wen Tang, Nigel W. John
- **Comment**: None
- **Journal**: None
- **Summary**: The potential of Augmented Reality (AR) technology to assist minimally invasive surgeries (MIS) lies in its computational performance and accuracy in dealing with challenging MIS scenes. Even with the latest hardware and software technologies, achieving both real-time and accurate augmented information overlay in MIS is still a formidable task. In this paper, we present a novel real-time AR framework for MIS that achieves interactive geometric aware augmented reality in endoscopic surgery with stereo views. Our framework tracks the movement of the endoscopic camera and simultaneously reconstructs a dense geometric mesh of the MIS scene. The movement of the camera is predicted by minimising the re-projection error to achieve a fast tracking performance, while the 3D mesh is incrementally built by a dense zero mean normalised cross correlation stereo matching method to improve the accuracy of the surface reconstruction. Our proposed system does not require any prior template or pre-operative scan and can infer the geometric information intra-operatively in real-time. With the geometric information available, our proposed AR framework is able to interactively add annotations, localisation of tumours and vessels, and measurement labelling with greater precision and accuracy compared with the state of the art approaches.



### DSOD: Learning Deeply Supervised Object Detectors from Scratch
- **Arxiv ID**: http://arxiv.org/abs/1708.01241v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.01241v2)
- **Published**: 2017-08-03 17:33:05+00:00
- **Updated**: 2018-04-30 02:17:30+00:00
- **Authors**: Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang, Yurong Chen, Xiangyang Xue
- **Comment**: ICCV 2017. Code and models are available at:
  https://github.com/szq0214/DSOD
- **Journal**: None
- **Summary**: We present Deeply Supervised Object Detector (DSOD), a framework that can learn object detectors from scratch. State-of-the-art object objectors rely heavily on the off-the-shelf networks pre-trained on large-scale classification datasets like ImageNet, which incurs learning bias due to the difference on both the loss functions and the category distributions between classification and detection tasks. Model fine-tuning for the detection task could alleviate this bias to some extent but not fundamentally. Besides, transferring pre-trained models from classification to detection between discrepant domains is even more difficult (e.g. RGB to depth images). A better solution to tackle these two critical problems is to train object detectors from scratch, which motivates our proposed DSOD. Previous efforts in this direction mostly failed due to much more complicated loss functions and limited training data in object detection. In DSOD, we contribute a set of design principles for training object detectors from scratch. One of the key findings is that deep supervision, enabled by dense layer-wise connections, plays a critical role in learning a good detector. Combining with several other principles, we develop DSOD following the single-shot detection (SSD) framework. Experiments on PASCAL VOC 2007, 2012 and MS COCO datasets demonstrate that DSOD can achieve better results than the state-of-the-art solutions with much more compact models. For instance, DSOD outperforms SSD on all three benchmarks with real-time detection speed, while requires only 1/2 parameters to SSD and 1/10 parameters to Faster RCNN. Our code and models are available at: https://github.com/szq0214/DSOD .



### Image reconstruction with imperfect forward models and applications in deblurring
- **Arxiv ID**: http://arxiv.org/abs/1708.01244v3
- **DOI**: 10.1137/17M1141965
- **Categories**: **cs.NA**, cs.CV, 65J20, 94A08, 49N45, 49N30
- **Links**: [PDF](http://arxiv.org/pdf/1708.01244v3)
- **Published**: 2017-08-03 17:49:31+00:00
- **Updated**: 2017-10-23 08:11:10+00:00
- **Authors**: Yury Korolev, Jan Lellmann
- **Comment**: None
- **Journal**: None
- **Summary**: We present and analyse an approach to image reconstruction problems with imperfect forward models based on partially ordered spaces - Banach lattices. In this approach, errors in the data and in the forward models are described using order intervals. The method can be characterised as the lattice analogue of the residual method, where the feasible set is defined by linear inequality constraints. The study of this feasible set is the main contribution of this paper. Convexity of this feasible set is examined in several settings and modifications for introducing additional information about the forward operator are considered. Numerical examples demonstrate the performance of the method in deblurring with errors in the blurring kernel.



### Unsupervised Representation Learning by Sorting Sequences
- **Arxiv ID**: http://arxiv.org/abs/1708.01246v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01246v1)
- **Published**: 2017-08-03 17:51:38+00:00
- **Updated**: 2017-08-03 17:51:38+00:00
- **Authors**: Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, Ming-Hsuan Yang
- **Comment**: ICCV 2017. Project page: http://vllab1.ucmerced.edu/~hylee/OPN/
- **Journal**: None
- **Summary**: We present an unsupervised representation learning approach using videos without semantic labels. We leverage the temporal coherence as a supervisory signal by formulating representation learning as a sequence sorting task. We take temporally shuffled frames (i.e., in non-chronological order) as inputs and train a convolutional neural network to sort the shuffled sequences. Similar to comparison-based sorting algorithms, we propose to extract features from all frame pairs and aggregate them to predict the correct order. As sorting shuffled image sequence requires an understanding of the statistical temporal structure of images, training with such a proxy task allows us to learn rich and generalizable visual representation. We validate the effectiveness of the learned representation using our method as pre-training on high-level recognition problems. The experimental results show that our method compares favorably against state-of-the-art methods on action recognition, image classification and object detection tasks.



### What your Facebook Profile Picture Reveals about your Personality
- **Arxiv ID**: http://arxiv.org/abs/1708.01292v2
- **DOI**: 10.1145/3123266.3123331
- **Categories**: **cs.CV**, cs.CY, cs.MM, cs.SI, H.4.m; J.4
- **Links**: [PDF](http://arxiv.org/pdf/1708.01292v2)
- **Published**: 2017-08-03 19:58:36+00:00
- **Updated**: 2017-08-13 07:51:17+00:00
- **Authors**: Cristina Segalin, Fabio Celli, Luca Polonio, Michal Kosinski, David Stillwell, Nicu Sebe, Marco Cristani, Bruno Lepri
- **Comment**: None
- **Journal**: None
- **Summary**: People spend considerable effort managing the impressions they give others. Social psychologists have shown that people manage these impressions differently depending upon their personality. Facebook and other social media provide a new forum for this fundamental process; hence, understanding people's behaviour on social media could provide interesting insights on their personality. In this paper we investigate automatic personality recognition from Facebook profile pictures. We analyze the effectiveness of four families of visual features and we discuss some human interpretable patterns that explain the personality traits of the individuals. For example, extroverts and agreeable individuals tend to have warm colored pictures and to exhibit many faces in their portraits, mirroring their inclination to socialize; while neurotic ones have a prevalence of pictures of indoor places. Then, we propose a classification approach to automatically recognize personality traits from these visual features. Finally, we compare the performance of our classification approach to the one obtained by human raters and we show that computer-based classifications are significantly more accurate than averaged human-based classifications for Extraversion and Neuroticism.



### Automatic Spatially-aware Fashion Concept Discovery
- **Arxiv ID**: http://arxiv.org/abs/1708.01311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01311v1)
- **Published**: 2017-08-03 21:13:04+00:00
- **Updated**: 2017-08-03 21:13:04+00:00
- **Authors**: Xintong Han, Zuxuan Wu, Phoenix X. Huang, Xiao Zhang, Menglong Zhu, Yuan Li, Yang Zhao, Larry S. Davis
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: This paper proposes an automatic spatially-aware concept discovery approach using weakly labeled image-text data from shopping websites. We first fine-tune GoogleNet by jointly modeling clothing images and their corresponding descriptions in a visual-semantic embedding space. Then, for each attribute (word), we generate its spatially-aware representation by combining its semantic word vector representation with its spatial representation derived from the convolutional maps of the fine-tuned network. The resulting spatially-aware representations are further used to cluster attributes into multiple groups to form spatially-aware concepts (e.g., the neckline concept might consist of attributes like v-neck, round-neck, etc). Finally, we decompose the visual-semantic embedding space into multiple concept-specific subspaces, which facilitates structured browsing and attribute-feedback product retrieval by exploiting multimodal linguistic regularities. We conducted extensive experiments on our newly collected Fashion200K dataset, and results on clustering quality evaluation and attribute-feedback product retrieval task demonstrate the effectiveness of our automatically discovered spatially-aware concepts.



### Flare Prediction Using Photospheric and Coronal Image Data
- **Arxiv ID**: http://arxiv.org/abs/1708.01323v1
- **DOI**: 10.1007/s11207-018-1258-9
- **Categories**: **astro-ph.SR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.01323v1)
- **Published**: 2017-08-03 22:31:38+00:00
- **Updated**: 2017-08-03 22:31:38+00:00
- **Authors**: Eric Jonas, Monica G. Bobra, Vaishaal Shankar, J. Todd Hoeksema, Benjamin Recht
- **Comment**: submitted for publication in the Astrophysical Journal
- **Journal**: None
- **Summary**: The precise physical process that triggers solar flares is not currently understood. Here we attempt to capture the signature of this mechanism in solar image data of various wavelengths and use these signatures to predict flaring activity. We do this by developing an algorithm that [1] automatically generates features in 5.5 TB of image data taken by the Solar Dynamics Observatory of the solar photosphere, chromosphere, transition region, and corona during the time period between May 2010 and May 2014, [2] combines these features with other features based on flaring history and a physical understanding of putative flaring processes, and [3] classifies these features to predict whether a solar active region will flare within a time period of $T$ hours, where $T$ = 2 and 24. We find that when optimizing for the True Skill Score (TSS), photospheric vector magnetic field data combined with flaring history yields the best performance, and when optimizing for the area under the precision-recall curve, all the data are helpful. Our model performance yields a TSS of $0.84 \pm 0.03$ and $0.81 \pm 0.03$ in the $T$ = 2 and 24 hour cases, respectively, and a value of $0.13 \pm 0.07$ and $0.43 \pm 0.08$ for the area under the precision-recall curve in the $T$ = 2 and 24 hour cases, respectively. These relatively high scores are similar to, but not greater than, other attempts to predict solar flares. Given the similar values of algorithm performance across various types of models reported in the literature, we conclude that we can expect a certain baseline predictive capacity using these data. This is the first attempt to predict solar flares using photospheric vector magnetic field data as well as multiple wavelengths of image data from the chromosphere, transition region, and corona.



