# Arxiv Papers in cs.CV on 2017-08-11
### Pose Guided Structured Region Ensemble Network for Cascaded Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1708.03416v2
- **DOI**: 10.1016/j.neucom.2018.06.097
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03416v2)
- **Published**: 2017-08-11 00:36:24+00:00
- **Updated**: 2018-06-24 02:00:25+00:00
- **Authors**: Xinghao Chen, Guijin Wang, Hengkai Guo, Cairong Zhang
- **Comment**: Accepted by Neurocomputing
- **Journal**: Neurocomputing 2019
- **Summary**: Hand pose estimation from a single depth image is an essential topic in computer vision and human computer interaction. Despite recent advancements in this area promoted by convolutional neural network, accurate hand pose estimation is still a challenging problem. In this paper we propose a Pose guided structured Region Ensemble Network (Pose-REN) to boost the performance of hand pose estimation. The proposed method extracts regions from the feature maps of convolutional neural network under the guide of an initially estimated pose, generating more optimal and representative features for hand pose estimation. The extracted feature regions are then integrated hierarchically according to the topology of hand joints by employing tree-structured fully connections. A refined estimation of hand pose is directly regressed by the proposed network and the final hand pose is obtained by utilizing an iterative cascaded method. Comprehensive experiments on public hand pose datasets demonstrate that our proposed method outperforms state-of-the-art algorithms.



### GlobeNet: Convolutional Neural Networks for Typhoon Eye Tracking from Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/1708.03417v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1708.03417v1)
- **Published**: 2017-08-11 00:41:56+00:00
- **Updated**: 2017-08-11 00:41:56+00:00
- **Authors**: Seungkyun Hong, Seongchan Kim, Minsu Joh, Sa-kwang Song
- **Comment**: Under review as a workshop paper at CI 2017
- **Journal**: None
- **Summary**: Advances in remote sensing technologies have made it possible to use high-resolution visual data for weather observation and forecasting tasks. We propose the use of multi-layer neural networks for understanding complex atmospheric dynamics based on multichannel satellite images. The capability of our model was evaluated by using a linear regression task for single typhoon coordinates prediction. A specific combination of models and different activation policies enabled us to obtain an interesting prediction result in the northeastern hemisphere (ENH).



### DeformNet: Free-Form Deformation Network for 3D Shape Reconstruction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1708.04672v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1708.04672v1)
- **Published**: 2017-08-11 00:43:19+00:00
- **Updated**: 2017-08-11 00:43:19+00:00
- **Authors**: Andrey Kurenkov, Jingwei Ji, Animesh Garg, Viraj Mehta, JunYoung Gwak, Christopher Choy, Silvio Savarese
- **Comment**: 11 pages, 9 figures, NIPS
- **Journal**: None
- **Summary**: 3D reconstruction from a single image is a key problem in multiple applications ranging from robotic manipulation to augmented reality. Prior methods have tackled this problem through generative models which predict 3D reconstructions as voxels or point clouds. However, these methods can be computationally expensive and miss fine details. We introduce a new differentiable layer for 3D data deformation and use it in DeformNet to learn a model for 3D reconstruction-through-deformation. DeformNet takes an image input, searches the nearest shape template from a database, and deforms the template to match the query image. We evaluate our approach on the ShapeNet dataset and show that - (a) the Free-Form Deformation layer is a powerful new building block for Deep Learning models that manipulate 3D data (b) DeformNet uses this FFD layer combined with shape retrieval for smooth and detail-preserving 3D reconstruction of qualitatively plausible point clouds with respect to a single query image (c) compared to other state-of-the-art 3D reconstruction methods, DeformNet quantitatively matches or outperforms their benchmarks by significant margins. For more information, visit: https://deformnet-site.github.io/DeformNet-website/ .



### Acoustic Feature Learning via Deep Variational Canonical Correlation Analysis
- **Arxiv ID**: http://arxiv.org/abs/1708.04673v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04673v2)
- **Published**: 2017-08-11 03:14:44+00:00
- **Updated**: 2017-08-31 06:30:12+00:00
- **Authors**: Qingming Tang, Weiran Wang, Karen Livescu
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of acoustic feature learning in the setting where we have access to another (non-acoustic) modality for feature learning but not at test time. We use deep variational canonical correlation analysis (VCCA), a recently proposed deep generative method for multi-view representation learning. We also extend VCCA with improved latent variable priors and with adversarial learning. Compared to other techniques for multi-view feature learning, VCCA's advantages include an intuitive latent variable interpretation and a variational lower bound objective that can be trained end-to-end efficiently. We compare VCCA and its extensions with previous feature learning methods on the University of Wisconsin X-ray Microbeam Database, and show that VCCA-based feature learning improves over previous methods for speaker-independent phonetic recognition.



### Video Deblurring via Semantic Segmentation and Pixel-Wise Non-Linear Kernel
- **Arxiv ID**: http://arxiv.org/abs/1708.03423v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03423v1)
- **Published**: 2017-08-11 03:15:48+00:00
- **Updated**: 2017-08-11 03:15:48+00:00
- **Authors**: Wenqi Ren, Jinshan Pan, Xiaochun Cao, Ming-Hsuan Yang
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: Video deblurring is a challenging problem as the blur is complex and usually caused by the combination of camera shakes, object motions, and depth variations. Optical flow can be used for kernel estimation since it predicts motion trajectories. However, the estimates are often inaccurate in complex scenes at object boundaries, which are crucial in kernel estimation. In this paper, we exploit semantic segmentation in each blurry frame to understand the scene contents and use different motion models for image regions to guide optical flow estimation. While existing pixel-wise blur models assume that the blur kernel is the same as optical flow during the exposure time, this assumption does not hold when the motion blur trajectory at a pixel is different from the estimated linear optical flow. We analyze the relationship between motion blur trajectory and optical flow, and present a novel pixel-wise non-linear kernel model to account for motion blur. The proposed blur model is based on the non-linear optical flow, which describes complex motion blur more effectively. Extensive experiments on challenging blurry videos demonstrate the proposed algorithm performs favorably against the state-of-the-art methods.



### Divide and Fuse: A Re-ranking Approach for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1708.04169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04169v1)
- **Published**: 2017-08-11 03:41:26+00:00
- **Updated**: 2017-08-11 03:41:26+00:00
- **Authors**: Rui Yu, Zhichao Zhou, Song Bai, Xiang Bai
- **Comment**: Accepted by BMVC2017
- **Journal**: None
- **Summary**: As re-ranking is a necessary procedure to boost person re-identification (re-ID) performance on large-scale datasets, the diversity of feature becomes crucial to person reID for its importance both on designing pedestrian descriptions and re-ranking based on feature fusion. However, in many circumstances, only one type of pedestrian feature is available. In this paper, we propose a "Divide and use" re-ranking framework for person re-ID. It exploits the diversity from different parts of a high-dimensional feature vector for fusion-based re-ranking, while no other features are accessible. Specifically, given an image, the extracted feature is divided into sub-features. Then the contextual information of each sub-feature is iteratively encoded into a new feature. Finally, the new features from the same image are fused into one vector for re-ranking. Experimental results on two person re-ID benchmarks demonstrate the effectiveness of the proposed framework. Especially, our method outperforms the state-of-the-art on the Market-1501 dataset.



### Iterative Deep Convolutional Encoder-Decoder Network for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1708.03431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03431v1)
- **Published**: 2017-08-11 04:58:46+00:00
- **Updated**: 2017-08-11 04:58:46+00:00
- **Authors**: Jung Uk Kim, Hak Gu Kim, Yong Man Ro
- **Comment**: accepted at EMBC 2017
- **Journal**: None
- **Summary**: In this paper, we propose a novel medical image segmentation using iterative deep learning framework. We have combined an iterative learning approach and an encoder-decoder network to improve segmentation results, which enables to precisely localize the regions of interest (ROIs) including complex shapes or detailed textures of medical images in an iterative manner. The proposed iterative deep convolutional encoder-decoder network consists of two main paths: convolutional encoder path and convolutional decoder path with iterative learning. Experimental results show that the proposed iterative deep learning framework is able to yield excellent medical image segmentation performances for various medical images. The effectiveness of the proposed method has been proved by comparing with other state-of-the-art medical image segmentation methods.



### A Generic Deep Architecture for Single Image Reflection Removal and Image Smoothing
- **Arxiv ID**: http://arxiv.org/abs/1708.03474v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03474v2)
- **Published**: 2017-08-11 08:47:33+00:00
- **Updated**: 2018-06-10 13:12:31+00:00
- **Authors**: Qingnan Fan, Jiaolong Yang, Gang Hua, Baoquan Chen, David Wipf
- **Comment**: Appeared at ICCV'17 (International Conference on Computer Vision)
- **Journal**: None
- **Summary**: This paper proposes a deep neural network structure that exploits edge information in addressing representative low-level vision tasks such as layer separation and image filtering. Unlike most other deep learning strategies applied in this context, our approach tackles these challenging problems by estimating edges and reconstructing images using only cascaded convolutional layers arranged such that no handcrafted or application-specific image-processing components are required. We apply the resulting transferrable pipeline to two different problem domains that are both sensitive to edges, namely, single image reflection removal and image smoothing. For the former, using a mild reflection smoothness assumption and a novel synthetic data generation method that acts as a type of weak supervision, our network is able to solve much more difficult reflection cases that cannot be handled by previous methods. For the latter, we also exceed the state-of-the-art quantitative and qualitative results by wide margins. In all cases, the proposed framework is simple, fast, and easy to transfer across disparate domains.



### Augmentor: An Image Augmentation Library for Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.04680v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.04680v1)
- **Published**: 2017-08-11 11:19:44+00:00
- **Updated**: 2017-08-11 11:19:44+00:00
- **Authors**: Marcus D. Bloice, Christof Stocker, Andreas Holzinger
- **Comment**: None
- **Journal**: None
- **Summary**: The generation of artificial data based on existing observations, known as data augmentation, is a technique used in machine learning to improve model accuracy, generalisation, and to control overfitting. Augmentor is a software package, available in both Python and Julia versions, that provides a high level API for the expansion of image data using a stochastic, pipeline-based approach which effectively allows for images to be sampled from a distribution of augmented images at runtime. Augmentor provides methods for most standard augmentation practices as well as several advanced features such as label-preserving, randomised elastic distortions, and provides many helper functions for typical augmentation tasks used in machine learning.



### Unsupervised Incremental Learning of Deep Descriptors From Video Streams
- **Arxiv ID**: http://arxiv.org/abs/1708.03615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03615v1)
- **Published**: 2017-08-11 17:04:03+00:00
- **Updated**: 2017-08-11 17:04:03+00:00
- **Authors**: Federico Pernici, Alberto Del Bimbo
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel unsupervised method for face identity learning from video sequences. The method exploits the ResNet deep network for face detection and VGGface fc7 face descriptors together with a smart learning mechanism that exploits the temporal coherence of visual data in video streams. We present a novel feature matching solution based on Reverse Nearest Neighbour and a feature forgetting strategy that supports incremental learning with memory size control, while time progresses. It is shown that the proposed learning procedure is asymptotically stable and can be effectively applied to relevant applications like multiple face tracking.



### Convolutional Neural Networks for Font Classification
- **Arxiv ID**: http://arxiv.org/abs/1708.03669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03669v1)
- **Published**: 2017-08-11 19:25:44+00:00
- **Updated**: 2017-08-11 19:25:44+00:00
- **Authors**: Chris Tensmeyer, Daniel Saunders, Tony Martinez
- **Comment**: ICDAR 2017
- **Journal**: None
- **Summary**: Classifying pages or text lines into font categories aids transcription because single font Optical Character Recognition (OCR) is generally more accurate than omni-font OCR. We present a simple framework based on Convolutional Neural Networks (CNNs), where a CNN is trained to classify small patches of text into predefined font classes. To classify page or line images, we average the CNN predictions over densely extracted patches. We show that this method achieves state-of-the-art performance on a challenging dataset of 40 Arabic computer fonts with 98.8\% line level accuracy. This same method also achieves the highest reported accuracy of 86.6% in predicting paleographic scribal script classes at the page level on medieval Latin manuscripts. Finally, we analyze what features are learned by the CNN on Latin manuscripts and find evidence that the CNN is learning both the defining morphological differences between scribal script classes as well as overfitting to class-correlated nuisance factors. We propose a novel form of data augmentation that improves robustness to text darkness, further increasing classification performance.



### Deep Recurrent Neural Networks for mapping winter vegetation quality coverage via multi-temporal SAR Sentinel-1
- **Arxiv ID**: http://arxiv.org/abs/1708.03694v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03694v1)
- **Published**: 2017-08-11 20:28:07+00:00
- **Updated**: 2017-08-11 20:28:07+00:00
- **Authors**: Dinh Ho Tong Minh, Dino Ienco, Raffaele Gaetano, Nathalie Lalande, Emile Ndikumana, Faycal Osman, Pierre Maurel
- **Comment**: In submission to IEEE Geoscience and Remote Sensing Letters
- **Journal**: None
- **Summary**: Mapping winter vegetation quality coverage is a challenge problem of remote sensing. This is due to the cloud coverage in winter period, leading to use radar rather than optical images. The objective of this paper is to provide a better understanding of the capabilities of radar Sentinel-1 and deep learning concerning about mapping winter vegetation quality coverage. The analysis presented in this paper is carried out on multi-temporal Sentinel-1 data over the site of La Rochelle, France, during the campaign in December 2016. This dataset were processed in order to produce an intensity radar data stack from October 2016 to February 2017. Two deep Recurrent Neural Network (RNN) based classifier methods were employed. We found that the results of RNNs clearly outperformed the classical machine learning approaches (Support Vector Machine and Random Forest). This study confirms that the time series radar Sentinel-1 and RNNs could be exploited for winter vegetation quality cover mapping.



### Learning Rotation for Kernel Correlation Filter
- **Arxiv ID**: http://arxiv.org/abs/1708.03698v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1708.03698v1)
- **Published**: 2017-08-11 20:35:25+00:00
- **Updated**: 2017-08-11 20:35:25+00:00
- **Authors**: Abdullah Hamdi, Bernard Ghanem
- **Comment**: 6 pages, 11 figures, tracking, CVPR, Correlation Filters,KCF, visual
  object tracking
- **Journal**: None
- **Summary**: Kernel Correlation Filters have shown a very promising scheme for visual tracking in terms of speed and accuracy on several benchmarks. However it suffers from problems that affect its performance like occlusion, rotation and scale change. This paper tries to tackle the problem of rotation by reformulating the optimization problem for learning the correlation filter. This modification (RKCF) includes learning rotation filter that utilizes circulant structure of HOG feature to guesstimate rotation from one frame to another and enhance the detection of KCF. Hence it gains boost in overall accuracy in many of OBT50 detest videos with minimal additional computation.



### Deep Incremental Boosting
- **Arxiv ID**: http://arxiv.org/abs/1708.03704v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.03704v1)
- **Published**: 2017-08-11 21:05:58+00:00
- **Updated**: 2017-08-11 21:05:58+00:00
- **Authors**: Alan Mosca, George D Magoulas
- **Comment**: None
- **Journal**: Christoph Benzm\"uller, Geoff Sutcliffe and Raul Rojas (editors).
  GCAI 2016. 2nd Global Conference on Artificial Intelligence, vol 41, pages
  293--302
- **Summary**: This paper introduces Deep Incremental Boosting, a new technique derived from AdaBoost, specifically adapted to work with Deep Learning methods, that reduces the required training time and improves generalisation. We draw inspiration from Transfer of Learning approaches to reduce the start-up time to training each incremental Ensemble member. We show a set of experiments that outlines some preliminary results on some common Deep Learning datasets and discuss the potential improvements Deep Incremental Boosting brings to traditional Ensemble methods in Deep Learning.



### Going Deeper with Semantics: Video Activity Interpretation using Semantic Contextualization
- **Arxiv ID**: http://arxiv.org/abs/1708.03725v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03725v3)
- **Published**: 2017-08-11 22:49:47+00:00
- **Updated**: 2018-11-15 15:58:14+00:00
- **Authors**: Sathyanarayanan N. Aakur, Fillipe DM de Souza, Sudeep Sarkar
- **Comment**: Accepted to WACV 2019
- **Journal**: None
- **Summary**: A deeper understanding of video activities extends beyond recognition of underlying concepts such as actions and objects: constructing deep semantic representations requires reasoning about the semantic relationships among these concepts, often beyond what is directly observed in the data. To this end, we propose an energy minimization framework that leverages large-scale commonsense knowledge bases, such as ConceptNet, to provide contextual cues to establish semantic relationships among entities directly hypothesized from video signal. We mathematically express this using the language of Grenander's canonical pattern generator theory. We show that the use of prior encoded commonsense knowledge alleviate the need for large annotated training datasets and help tackle imbalance in training through prior knowledge. Using three different publicly available datasets - Charades, Microsoft Visual Description Corpus and Breakfast Actions datasets, we show that the proposed model can generate video interpretations whose quality is better than those reported by state-of-the-art approaches, which have substantial training needs. Through extensive experiments, we show that the use of commonsense knowledge from ConceptNet allows the proposed approach to handle various challenges such as training data imbalance, weak features, and complex semantic relationships and visual scenes.



