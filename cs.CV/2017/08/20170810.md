# Arxiv Papers in cs.CV on 2017-08-10
### Learning Graph While Training: An Evolving Graph Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1708.04675v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.04675v1)
- **Published**: 2017-08-10 01:55:01+00:00
- **Updated**: 2017-08-10 01:55:01+00:00
- **Authors**: Ruoyu Li, Junzhou Huang
- **Comment**: 10 pages, submitted to NIPS 2017
- **Journal**: None
- **Summary**: Convolution Neural Networks on Graphs are important generalization and extension of classical CNNs. While previous works generally assumed that the graph structures of samples are regular with unified dimensions, in many applications, they are highly diverse or even not well defined. Under some circumstances, e.g. chemical molecular data, clustering or coarsening for simplifying the graphs is hard to be justified chemically. In this paper, we propose a more general and flexible graph convolution network (EGCN) fed by batch of arbitrarily shaped data together with their evolving graph Laplacians trained in supervised fashion. Extensive experiments have been conducted to demonstrate the superior performance in terms of both the acceleration of parameter fitting and the significantly improved prediction accuracy on multiple graph-structured datasets.



### TandemNet: Distilling Knowledge from Medical Images Using Diagnostic Reports as Optional Semantic References
- **Arxiv ID**: http://arxiv.org/abs/1708.03070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03070v1)
- **Published**: 2017-08-10 04:12:00+00:00
- **Updated**: 2017-08-10 04:12:00+00:00
- **Authors**: Zizhao Zhang, Pingjun Chen, Manish Sapkota, Lin Yang
- **Comment**: MICCAI2017 Oral
- **Journal**: None
- **Summary**: In this paper, we introduce the semantic knowledge of medical images from their diagnostic reports to provide an inspirational network training and an interpretable prediction mechanism with our proposed novel multimodal neural network, namely TandemNet. Inside TandemNet, a language model is used to represent report text, which cooperates with the image model in a tandem scheme. We propose a novel dual-attention model that facilitates high-level interactions between visual and semantic information and effectively distills useful features for prediction. In the testing stage, TandemNet can make accurate image prediction with an optional report text input. It also interprets its prediction by producing attention on the image and text informative feature pieces, and further generating diagnostic report paragraphs. Based on a pathological bladder cancer images and their diagnostic reports (BCIDR) dataset, sufficient experiments demonstrate that our method effectively learns and integrates knowledge from multimodalities and obtains significantly improved performance than comparing baselines.



### Semantic Video CNNs through Representation Warping
- **Arxiv ID**: http://arxiv.org/abs/1708.03088v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03088v1)
- **Published**: 2017-08-10 06:18:02+00:00
- **Updated**: 2017-08-10 06:18:02+00:00
- **Authors**: Raghudeep Gadde, Varun Jampani, Peter V. Gehler
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: In this work, we propose a technique to convert CNN models for semantic segmentation of static images into CNNs for video data. We describe a warping method that can be used to augment existing architectures with very little extra computational cost. This module is called NetWarp and we demonstrate its use for a range of network architectures. The main design principle is to use optical flow of adjacent frames for warping internal network representations across time. A key insight of this work is that fast optical flow methods can be combined with many different CNN architectures for improved performance and end-to-end training. Experiments validate that the proposed approach incurs only little extra computational cost, while improving performance, when video streams are available. We achieve new state-of-the-art results on the CamVid and Cityscapes benchmark datasets and show consistent improvements over different baseline networks. Our code and models will be available at http://segmentation.is.tue.mpg.de



### Modality-bridge Transfer Learning for Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1708.03111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03111v1)
- **Published**: 2017-08-10 07:57:05+00:00
- **Updated**: 2017-08-10 07:57:05+00:00
- **Authors**: Hak Gu Kim, Yeoreum Choi, Yong Man Ro
- **Comment**: accepted at CISP-BMEI 2017
- **Journal**: None
- **Summary**: This paper presents a new approach of transfer learning-based medical image classification to mitigate insufficient labeled data problem in medical domain. Instead of direct transfer learning from source to small number of labeled target data, we propose a modality-bridge transfer learning which employs the bridge database in the same medical imaging acquisition modality as target database. By learning the projection function from source to bridge and from bridge to target, the domain difference between source (e.g., natural images) and target (e.g., X-ray images) can be mitigated. Experimental results show that the proposed method can achieve a high classification performance even for a small number of labeled target medical images, compared to various transfer learning approaches.



### Beyond Bilinear: Generalized Multimodal Factorized High-order Pooling for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1708.03619v2
- **DOI**: 10.1109/TNNLS.2018.2817340
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03619v2)
- **Published**: 2017-08-10 09:09:23+00:00
- **Updated**: 2019-05-16 04:16:57+00:00
- **Authors**: Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, Dacheng Tao
- **Comment**: 13 pages, 9 figures. arXiv admin note: substantial text overlap with
  arXiv:1708.01471
- **Journal**: IEEE Transactions On Neural Networks And Learning Systems, Vol.
  26, No. 10, October 2015, Pp. 2275-2290
- **Summary**: Visual question answering (VQA) is challenging because it requires a simultaneous understanding of both visual content of images and textual content of questions. To support the VQA task, we need to find good solutions for the following three issues: 1) fine-grained feature representations for both the image and the question; 2) multi-modal feature fusion that is able to capture the complex interactions between multi-modal features; 3) automatic answer prediction that is able to consider the complex correlations between multiple diverse answers for the same question. For fine-grained image and question representations, a `co-attention' mechanism is developed by using a deep neural network architecture to jointly learn the attentions for both the image and the question, which can allow us to reduce the irrelevant features effectively and obtain more discriminative features for image and question representations. For multi-modal feature fusion, a generalized Multi-modal Factorized High-order pooling approach (MFH) is developed to achieve more effective fusion of multi-modal features by exploiting their correlations sufficiently, which can further result in superior VQA performance as compared with the state-of-the-art approaches. For answer prediction, the KL (Kullback-Leibler) divergence is used as the loss function to achieve precise characterization of the complex correlations between multiple diverse answers with the same or similar meaning, which can allow us to achieve faster convergence rate and obtain slightly better accuracy on answer prediction. A deep neural network architecture is designed to integrate all these aforementioned modules into a unified model for achieving superior VQA performance. With an ensemble of our MFH models, we achieve the state-of-the-art performance on the large-scale VQA datasets and win the runner-up in VQA Challenge 2017.



### Attention-Aware Face Hallucination via Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.03132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03132v1)
- **Published**: 2017-08-10 09:12:03+00:00
- **Updated**: 2017-08-10 09:12:03+00:00
- **Authors**: Qingxing Cao, Liang Lin, Yukai Shi, Xiaodan Liang, Guanbin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Face hallucination is a domain-specific super-resolution problem with the goal to generate high-resolution (HR) faces from low-resolution (LR) input images. In contrast to existing methods that often learn a single patch-to-patch mapping from LR to HR images and are regardless of the contextual interdependency between patches, we propose a novel Attention-aware Face Hallucination (Attention-FH) framework which resorts to deep reinforcement learning for sequentially discovering attended patches and then performing the facial part enhancement by fully exploiting the global interdependency of the image. Specifically, in each time step, the recurrent policy network is proposed to dynamically specify a new attended region by incorporating what happened in the past. The state (i.e., face hallucination result for the whole image) can thus be exploited and updated by the local enhancement network on the selected region. The Attention-FH approach jointly learns the recurrent policy network and local enhancement network through maximizing the long-term reward that reflects the hallucination performance over the whole image. Therefore, our proposed Attention-FH is capable of adaptively personalizing an optimal searching path for each face image according to its own characteristic. Extensive experiments show our approach significantly surpasses the state-of-the-arts on in-the-wild faces with large pose and illumination variations.



### Analysis of Convolutional Neural Networks for Document Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1708.03273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03273v1)
- **Published**: 2017-08-10 15:50:30+00:00
- **Updated**: 2017-08-10 15:50:30+00:00
- **Authors**: Chris Tensmeyer, Tony Martinez
- **Comment**: Accepted ICDAR 2017
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are state-of-the-art models for document image classification tasks. However, many of these approaches rely on parameters and architectures designed for classifying natural images, which differ from document images. We question whether this is appropriate and conduct a large empirical study to find what aspects of CNNs most affect performance on document images. Among other results, we exceed the state-of-the-art on the RVL-CDIP dataset by using shear transform data augmentation and an architecture designed for a larger input image. Additionally, we analyze the learned features and find evidence that CNNs trained on RVL-CDIP learn region-specific layout features.



### Incremental 3D Line Segment Extraction from Semi-dense SLAM
- **Arxiv ID**: http://arxiv.org/abs/1708.03275v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03275v3)
- **Published**: 2017-08-10 15:52:34+00:00
- **Updated**: 2018-04-26 19:58:42+00:00
- **Authors**: Shida He, Xuebin Qin, Zichen Zhang, Martin Jagersand
- **Comment**: Accepted at ICPR 2018
- **Journal**: None
- **Summary**: Although semi-dense Simultaneous Localization and Mapping (SLAM) has been becoming more popular over the last few years, there is a lack of efficient methods for representing and processing their large scale point clouds. In this paper, we propose using 3D line segments to simplify the point clouds generated by semi-dense SLAM. Specifically, we present a novel incremental approach for 3D line segment extraction. This approach reduces a 3D line segment fitting problem into two 2D line segment fitting problems and takes advantage of both images and depth maps. In our method, 3D line segments are fitted incrementally along detected edge segments via minimizing fitting errors on two planes. By clustering the detected line segments, the resulting 3D representation of the scene achieves a good balance between compactness and completeness. Our experimental results show that the 3D line segments generated by our method are highly accurate. As an application, we demonstrate that these line segments greatly improve the quality of 3D surface reconstruction compared to a feature point based baseline.



### Document Image Binarization with Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.03276v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03276v1)
- **Published**: 2017-08-10 16:00:35+00:00
- **Updated**: 2017-08-10 16:00:35+00:00
- **Authors**: Chris Tensmeyer, Tony Martinez
- **Comment**: ICDAR 2017 (oral)
- **Journal**: None
- **Summary**: Binarization of degraded historical manuscript images is an important pre-processing step for many document processing tasks. We formulate binarization as a pixel classification learning task and apply a novel Fully Convolutional Network (FCN) architecture that operates at multiple image scales, including full resolution. The FCN is trained to optimize a continuous version of the Pseudo F-measure metric and an ensemble of FCNs outperform the competition winners on 4 of 7 DIBCO competitions. This same binarization technique can also be applied to different domains such as Palm Leaf Manuscripts with good performance. We analyze the performance of the proposed model w.r.t. the architectural hyperparameters, size and diversity of training data, and the input features chosen.



### Motion Feature Augmented Recurrent Neural Network for Skeleton-based Dynamic Hand Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.03278v1
- **DOI**: 10.1109/ICIP.2017.8296809
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03278v1)
- **Published**: 2017-08-10 16:02:58+00:00
- **Updated**: 2017-08-10 16:02:58+00:00
- **Authors**: Xinghao Chen, Hengkai Guo, Guijin Wang, Li Zhang
- **Comment**: Accepted by ICIP 2017
- **Journal**: None
- **Summary**: Dynamic hand gesture recognition has attracted increasing interests because of its importance for human computer interaction. In this paper, we propose a new motion feature augmented recurrent neural network for skeleton-based dynamic hand gesture recognition. Finger motion features are extracted to describe finger movements and global motion features are utilized to represent the global movement of hand skeleton. These motion features are then fed into a bidirectional recurrent neural network (RNN) along with the skeleton sequence, which can augment the motion features for RNN and improve the classification performance. Experiments demonstrate that our proposed method is effective and outperforms start-of-the-art methods.



### Exploring Temporal Preservation Networks for Precise Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1708.03280v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03280v2)
- **Published**: 2017-08-10 16:07:16+00:00
- **Updated**: 2017-09-11 08:32:50+00:00
- **Authors**: Ke Yang, Peng Qiao, Dongsheng Li, Shaohe Lv, Yong Dou
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal action localization is an important task of computer vision. Though a variety of methods have been proposed, it still remains an open question how to predict the temporal boundaries of action segments precisely. Most works use segment-level classifiers to select video segments pre-determined by action proposal or dense sliding windows. However, in order to achieve more precise action boundaries, a temporal localization system should make dense predictions at a fine granularity. A newly proposed work exploits Convolutional-Deconvolutional-Convolutional (CDC) filters to upsample the predictions of 3D ConvNets, making it possible to perform per-frame action predictions and achieving promising performance in terms of temporal action localization. However, CDC network loses temporal information partially due to the temporal downsampling operation. In this paper, we propose an elegant and powerful Temporal Preservation Convolutional (TPC) Network that equips 3D ConvNets with TPC filters. TPC network can fully preserve temporal resolution and downsample the spatial resolution simultaneously, enabling frame-level granularity action localization. TPC network can be trained in an end-to-end manner. Experiment results on public datasets show that TPC network achieves significant improvement on per-frame action prediction and competing results on segment-level temporal action localization.



### Learning to Synthesize a 4D RGBD Light Field from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1708.03292v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1708.03292v1)
- **Published**: 2017-08-10 16:50:29+00:00
- **Updated**: 2017-08-10 16:50:29+00:00
- **Authors**: Pratul P. Srinivasan, Tongzhou Wang, Ashwin Sreelal, Ravi Ramamoorthi, Ren Ng
- **Comment**: International Conference on Computer Vision (ICCV) 2017
- **Journal**: None
- **Summary**: We present a machine learning algorithm that takes as input a 2D RGB image and synthesizes a 4D RGBD light field (color and depth of the scene in each ray direction). For training, we introduce the largest public light field dataset, consisting of over 3300 plenoptic camera light fields of scenes containing flowers and plants. Our synthesis pipeline consists of a convolutional neural network (CNN) that estimates scene geometry, a stage that renders a Lambertian light field using that geometry, and a second CNN that predicts occluded rays and non-Lambertian effects. Our algorithm builds on recent view synthesis methods, but is unique in predicting RGBD for each light field ray and improving unsupervised single image depth estimation by enforcing consistency of ray depths that should intersect the same scene point. Please see our supplementary video at https://youtu.be/yLCvWoQLnms



### Cell Detection in Microscopy Images with Deep Convolutional Neural Network and Compressed Sensing
- **Arxiv ID**: http://arxiv.org/abs/1708.03307v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03307v3)
- **Published**: 2017-08-10 17:27:09+00:00
- **Updated**: 2018-02-21 00:11:48+00:00
- **Authors**: Yao Xue, Nilanjan Ray
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to automatically detect certain types of cells or cellular subunits in microscopy images is of significant interest to a wide range of biomedical research and clinical practices. Cell detection methods have evolved from employing hand-crafted features to deep learning-based techniques. The essential idea of these methods is that their cell classifiers or detectors are trained in the pixel space, where the locations of target cells are labeled. In this paper, we seek a different route and propose a convolutional neural network (CNN)-based cell detection method that uses encoding of the output pixel space. For the cell detection problem, the output space is the sparsely labeled pixel locations indicating cell centers. We employ random projections to encode the output space to a compressed vector of fixed dimension. Then, CNN regresses this compressed vector from the input pixels. Furthermore, it is possible to stably recover sparse cell locations on the output pixel space from the predicted compressed vector using $L_1$-norm optimization. In the past, output space encoding using compressed sensing (CS) has been used in conjunction with linear and non-linear predictors. To the best of our knowledge, this is the first successful use of CNN with CS-based output space encoding. We made substantial experiments on several benchmark datasets, where the proposed CNN + CS framework (referred to as CNNCS) achieved the highest or at least top-3 performance in terms of F1-score, compared with other state-of-the-art methods.



### Systematic Testing of Convolutional Neural Networks for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1708.03309v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1708.03309v2)
- **Published**: 2017-08-10 17:33:52+00:00
- **Updated**: 2017-08-11 17:34:23+00:00
- **Authors**: Tommaso Dreossi, Shromona Ghosh, Alberto Sangiovanni-Vincentelli, Sanjit A. Seshia
- **Comment**: None
- **Journal**: None
- **Summary**: We present a framework to systematically analyze convolutional neural networks (CNNs) used in classification of cars in autonomous vehicles. Our analysis procedure comprises an image generator that produces synthetic pictures by sampling in a lower dimension image modification subspace and a suite of visualization tools. The image generator produces images which can be used to test the CNN and hence expose its vulnerabilities. The presented framework can be used to extract insights of the CNN classifier, compare across classification models, or generate training and validation datasets.



### An Empirical Study on Writer Identification & Verification from Intra-variable Individual Handwriting
- **Arxiv ID**: http://arxiv.org/abs/1708.03361v3
- **DOI**: 10.1109/ACCESS.2019.2899908
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03361v3)
- **Published**: 2017-08-10 19:23:11+00:00
- **Updated**: 2019-01-20 15:23:29+00:00
- **Authors**: Chandranath Adak, Bidyut B. Chaudhuri, Michael Blumenstein
- **Comment**: None
- **Journal**: None
- **Summary**: The handwriting of an individual may vary substantially with factors such as mood, time, space, writing speed, writing medium and tool, writing topic, etc. It becomes challenging to perform automated writer verification/identification on a particular set of handwritten patterns (e.g., speedy handwriting) of a person, especially when the system is trained using a different set of writing patterns (e.g., normal speed) of that same person. However, it would be interesting to experimentally analyze if there exists any implicit characteristic of individuality which is insensitive to high intra-variable handwriting. In this paper, we study some handcrafted features and auto-derived features extracted from intra-variable writing. Here, we work on writer identification/verification from offline Bengali handwriting of high intra-variability. To this end, we use various models mainly based on handcrafted features with SVM (Support Vector Machine) and features auto-derived by the convolutional network. For experimentation, we have generated two handwritten databases from two different sets of 100 writers and enlarged the dataset by a data-augmentation technique. We have obtained some interesting results.



### Joint Multi-Person Pose Estimation and Semantic Part Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1708.03383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03383v1)
- **Published**: 2017-08-10 20:59:31+00:00
- **Updated**: 2017-08-10 20:59:31+00:00
- **Authors**: Fangting Xia, Peng Wang, Xianjie Chen, Alan Yuille
- **Comment**: This paper has been accepted by CVPR 2017
- **Journal**: None
- **Summary**: Human pose estimation and semantic part segmentation are two complementary tasks in computer vision. In this paper, we propose to solve the two tasks jointly for natural multi-person images, in which the estimated pose provides object-level shape prior to regularize part segments while the part-level segments constrain the variation of pose locations. Specifically, we first train two fully convolutional neural networks (FCNs), namely Pose FCN and Part FCN, to provide initial estimation of pose joint potential and semantic part potential. Then, to refine pose joint location, the two types of potentials are fused with a fully-connected conditional random field (FCRF), where a novel segment-joint smoothness term is used to encourage semantic and spatial consistency between parts and joints. To refine part segments, the refined pose and the original part potential are integrated through a Part FCN, where the skeleton feature from pose serves as additional regularization cues for part segments. Finally, to reduce the complexity of the FCRF, we induce human detection boxes and infer the graph inside each box, making the inference forty times faster.   Since there's no dataset that contains both part segments and pose labels, we extend the PASCAL VOC part dataset with human pose joints and perform extensive experiments to compare our method against several most recent strategies. We show that on this dataset our algorithm surpasses competing methods by a large margin in both tasks.



