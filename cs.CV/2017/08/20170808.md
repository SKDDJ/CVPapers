# Arxiv Papers in cs.CV on 2017-08-08
### Unconstrained Face Detection and Open-Set Face Recognition Challenge
- **Arxiv ID**: http://arxiv.org/abs/1708.02337v3
- **DOI**: 10.1109/BTAS.2017.8272759
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02337v3)
- **Published**: 2017-08-08 00:28:02+00:00
- **Updated**: 2018-09-25 22:44:35+00:00
- **Authors**: Manuel Günther, Peiyun Hu, Christian Herrmann, Chi Ho Chan, Min Jiang, Shufan Yang, Akshay Raj Dhamija, Deva Ramanan, Jürgen Beyerer, Josef Kittler, Mohamad Al Jazaery, Mohammad Iqbal Nouyed, Guodong Guo, Cezary Stankiewicz, Terrance E. Boult
- **Comment**: This is an ERRATA version of the paper originally presented at the
  International Joint Conference on Biometrics. Due to a bug in our evaluation
  code, the results of the participants changed. The final conclusion, however,
  is still the same
- **Journal**: None
- **Summary**: Face detection and recognition benchmarks have shifted toward more difficult environments. The challenge presented in this paper addresses the next step in the direction of automatic detection and identification of people from outdoor surveillance cameras. While face detection has shown remarkable success in images collected from the web, surveillance cameras include more diverse occlusions, poses, weather conditions and image blur. Although face verification or closed-set face identification have surpassed human capabilities on some datasets, open-set identification is much more complex as it needs to reject both unknown identities and false accepts from the face detector. We show that unconstrained face detection can approach high detection rates albeit with moderate false accept rates. By contrast, open-set face recognition is currently weak and requires much more attention.



### Temporal Context Network for Activity Localization in Videos
- **Arxiv ID**: http://arxiv.org/abs/1708.02349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02349v1)
- **Published**: 2017-08-08 01:46:03+00:00
- **Updated**: 2017-08-08 01:46:03+00:00
- **Authors**: Xiyang Dai, Bharat Singh, Guyue Zhang, Larry S. Davis, Yan Qiu Chen
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: We present a Temporal Context Network (TCN) for precise temporal localization of human activities. Similar to the Faster-RCNN architecture, proposals are placed at equal intervals in a video which span multiple temporal scales. We propose a novel representation for ranking these proposals. Since pooling features only inside a segment is not sufficient to predict activity boundaries, we construct a representation which explicitly captures context around a proposal for ranking it. For each temporal segment inside a proposal, features are uniformly sampled at a pair of scales and are input to a temporal convolutional neural network for classification. After ranking proposals, non-maximum suppression is applied and classification is performed to obtain final detections. TCN outperforms state-of-the-art methods on the ActivityNet dataset and the THUMOS14 dataset.



### Learning a Repression Network for Precise Vehicle Search
- **Arxiv ID**: http://arxiv.org/abs/1708.02386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02386v1)
- **Published**: 2017-08-08 07:14:14+00:00
- **Updated**: 2017-08-08 07:14:14+00:00
- **Authors**: Qiantong Xu, Ke Yan, Yonghong Tian
- **Comment**: None
- **Journal**: None
- **Summary**: The growing explosion in the use of surveillance cameras in public security highlights the importance of vehicle search from large-scale image databases. Precise vehicle search, aiming at finding out all instances for a given query vehicle image, is a challenging task as different vehicles will look very similar to each other if they share same visual attributes. To address this problem, we propose the Repression Network (RepNet), a novel multi-task learning framework, to learn discriminative features for each vehicle image from both coarse-grained and detailed level simultaneously. Besides, benefited from the satisfactory accuracy of attribute classification, a bucket search method is proposed to reduce the retrieval time while still maintaining competitive performance. We conduct extensive experiments on the revised VehcileID dataset. Experimental results show that our RepNet achieves the state-of-the-art performance and the bucket search method can reduce the retrieval time by about 24 times.



### MHTN: Modal-adversarial Hybrid Transfer Network for Cross-modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1708.04308v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.04308v1)
- **Published**: 2017-08-08 07:50:52+00:00
- **Updated**: 2017-08-08 07:50:52+00:00
- **Authors**: Xin Huang, Yuxin Peng, Mingkuan Yuan
- **Comment**: 12 pages, submitted to IEEE Transactions on Cybernetics
- **Journal**: None
- **Summary**: Cross-modal retrieval has drawn wide interest for retrieval across different modalities of data. However, existing methods based on DNN face the challenge of insufficient cross-modal training data, which limits the training effectiveness and easily leads to overfitting. Transfer learning is for relieving the problem of insufficient training data, but it mainly focuses on knowledge transfer only from large-scale datasets as single-modal source domain to single-modal target domain. Such large-scale single-modal datasets also contain rich modal-independent semantic knowledge that can be shared across different modalities. Besides, large-scale cross-modal datasets are very labor-consuming to collect and label, so it is significant to fully exploit the knowledge in single-modal datasets for boosting cross-modal retrieval. This paper proposes modal-adversarial hybrid transfer network (MHTN), which to the best of our knowledge is the first work to realize knowledge transfer from single-modal source domain to cross-modal target domain, and learn cross-modal common representation. It is an end-to-end architecture with two subnetworks: (1) Modal-sharing knowledge transfer subnetwork is proposed to jointly transfer knowledge from a large-scale single-modal dataset in source domain to all modalities in target domain with a star network structure, which distills modal-independent supplementary knowledge for promoting cross-modal common representation learning. (2) Modal-adversarial semantic learning subnetwork is proposed to construct an adversarial training mechanism between common representation generator and modality discriminator, making the common representation discriminative for semantics but indiscriminative for modalities to enhance cross-modal semantic consistency during transfer process. Comprehensive experiments on 4 widely-used datasets show its effectiveness and generality.



### Wasserstein CNN: Learning Invariant Features for NIR-VIS Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.02412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02412v1)
- **Published**: 2017-08-08 09:07:34+00:00
- **Updated**: 2017-08-08 09:07:34+00:00
- **Authors**: Ran He, Xiang Wu, Zhenan Sun, Tieniu Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Heterogeneous face recognition (HFR) aims to match facial images acquired from different sensing modalities with mission-critical applications in forensics, security and commercial sectors. However, HFR is a much more challenging problem than traditional face recognition because of large intra-class variations of heterogeneous face images and limited training samples of cross-modality face image pairs. This paper proposes a novel approach namely Wasserstein CNN (convolutional neural networks, or WCNN for short) to learn invariant features between near-infrared and visual face images (i.e. NIR-VIS face recognition). The low-level layers of WCNN are trained with widely available face images in visual spectrum. The high-level layer is divided into three parts, i.e., NIR layer, VIS layer and NIR-VIS shared layer. The first two layers aims to learn modality-specific features and NIR-VIS shared layer is designed to learn modality-invariant feature subspace. Wasserstein distance is introduced into NIR-VIS shared layer to measure the dissimilarity between heterogeneous feature distributions. So W-CNN learning aims to achieve the minimization of Wasserstein distance between NIR distribution and VIS distribution for invariant deep feature representation of heterogeneous face images. To avoid the over-fitting problem on small-scale heterogeneous face data, a correlation prior is introduced on the fully-connected layers of WCNN network to reduce parameter space. This prior is implemented by a low-rank constraint in an end-to-end network. The joint formulation leads to an alternating minimization for deep feature representation at training stage and an efficient computation for heterogeneous data at testing stage. Extensive experiments on three challenging NIR-VIS face recognition databases demonstrate the significant superiority of Wasserstein CNN over state-of-the-art methods.



### FoveaNet: Perspective-aware Urban Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/1708.02421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02421v1)
- **Published**: 2017-08-08 09:29:50+00:00
- **Updated**: 2017-08-08 09:29:50+00:00
- **Authors**: Xin Li, Zequn Jie, Wei Wang, Changsong Liu, Jimei Yang, Xiaohui Shen, Zhe Lin, Qiang Chen, Shuicheng Yan, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Parsing urban scene images benefits many applications, especially self-driving. Most of the current solutions employ generic image parsing models that treat all scales and locations in the images equally and do not consider the geometry property of car-captured urban scene images. Thus, they suffer from heterogeneous object scales caused by perspective projection of cameras on actual scenes and inevitably encounter parsing failures on distant objects as well as other boundary and recognition errors. In this work, we propose a new FoveaNet model to fully exploit the perspective geometry of scene images and address the common failures of generic parsing models. FoveaNet estimates the perspective geometry of a scene image through a convolutional network which integrates supportive evidence from contextual objects within the image. Based on the perspective geometry information, FoveaNet "undoes" the camera perspective projection analyzing regions in the space of the actual scene, and thus provides much more reliable parsing results. Furthermore, to effectively address the recognition errors, FoveaNet introduces a new dense CRFs model that takes the perspective geometry as a prior potential. We evaluate FoveaNet on two urban scene parsing datasets, Cityspaces and CamVid, which demonstrates that FoveaNet can outperform all the well-established baselines and provide new state-of-the-art performance.



### Prune the Convolutional Neural Networks with Sparse Shrink
- **Arxiv ID**: http://arxiv.org/abs/1708.02439v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02439v1)
- **Published**: 2017-08-08 10:28:20+00:00
- **Updated**: 2017-08-08 10:28:20+00:00
- **Authors**: Xin Li, Changsong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, it is still difficult to adapt Convolutional Neural Network (CNN) based models for deployment on embedded devices. The heavy computation and large memory footprint of CNN models become the main burden in real application. In this paper, we propose a "Sparse Shrink" algorithm to prune an existing CNN model. By analyzing the importance of each channel via sparse reconstruction, the algorithm is able to prune redundant feature maps accordingly. The resulting pruned model thus directly saves computational resource. We have evaluated our algorithm on CIFAR-100. As shown in our experiments, we can reduce 56.77% parameters and 73.84% multiplication in total with only minor decrease in accuracy. These results have demonstrated the effectiveness of our "Sparse Shrink" algorithm.



### An Effective Feature Selection Method Based on Pair-Wise Feature Proximity for High Dimensional Low Sample Size Data
- **Arxiv ID**: http://arxiv.org/abs/1708.02443v1
- **DOI**: 10.23919/EUSIPCO.2017.8081474
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02443v1)
- **Published**: 2017-08-08 11:05:18+00:00
- **Updated**: 2017-08-08 11:05:18+00:00
- **Authors**: S L Happy, Ramanarayan Mohanty, Aurobinda Routray
- **Comment**: European Signal Processing Conference 2017
- **Journal**: None
- **Summary**: Feature selection has been studied widely in the literature. However, the efficacy of the selection criteria for low sample size applications is neglected in most cases. Most of the existing feature selection criteria are based on the sample similarity. However, the distance measures become insignificant for high dimensional low sample size (HDLSS) data. Moreover, the variance of a feature with a few samples is pointless unless it represents the data distribution efficiently. Instead of looking at the samples in groups, we evaluate their efficiency based on pairwise fashion. In our investigation, we noticed that considering a pair of samples at a time and selecting the features that bring them closer or put them far away is a better choice for feature selection. Experimental results on benchmark data sets demonstrate the effectiveness of the proposed method with low sample size, which outperforms many other state-of-the-art feature selection methods.



### Weakly Supervised Image Annotation and Segmentation with Objects and Attributes
- **Arxiv ID**: http://arxiv.org/abs/1708.02459v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02459v1)
- **Published**: 2017-08-08 12:19:48+00:00
- **Updated**: 2017-08-08 12:19:48+00:00
- **Authors**: Zhiyuan Shi, Yongxin Yang, Timothy M. Hospedales, Tao Xiang
- **Comment**: Accepted in IEEE Transaction on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: We propose to model complex visual scenes using a non-parametric Bayesian model learned from weakly labelled images abundant on media sharing sites such as Flickr. Given weak image-level annotations of objects and attributes without locations or associations between them, our model aims to learn the appearance of object and attribute classes as well as their association on each object instance. Once learned, given an image, our model can be deployed to tackle a number of vision problems in a joint and coherent manner, including recognising objects in the scene (automatic object annotation), describing objects using their attributes (attribute prediction and association), and localising and delineating the objects (object detection and semantic segmentation). This is achieved by developing a novel Weakly Supervised Markov Random Field Stacked Indian Buffet Process (WS-MRF-SIBP) that models objects and attributes as latent factors and explicitly captures their correlations within and across superpixels. Extensive experiments on benchmark datasets demonstrate that our weakly supervised model significantly outperforms weakly supervised alternatives and is often comparable with existing strongly supervised models on a variety of tasks including semantic segmentation, automatic image annotation and retrieval based on object-attribute associations.



### An Unsupervised Game-Theoretic Approach to Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.02476v1
- **DOI**: 10.1109/TIP.2018.2838761
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02476v1)
- **Published**: 2017-08-08 13:22:21+00:00
- **Updated**: 2017-08-08 13:22:21+00:00
- **Authors**: Yu Zeng, Huchuan Lu, Ali Borji, Mengyang Feng
- **Comment**: This paper has been submitted to IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: We propose a novel unsupervised game-theoretic salient object detection algorithm that does not require labeled training data. First, saliency detection problem is formulated as a non-cooperative game, hereinafter referred to as Saliency Game, in which image regions are players who choose to be "background" or "foreground" as their pure strategies. A payoff function is constructed by exploiting multiple cues and combining complementary features. Saliency maps are generated according to each region's strategy in the Nash equilibrium of the proposed Saliency Game. Second, we explore the complementary relationship between color and deep features and propose an Iterative Random Walk algorithm to combine saliency maps produced by the Saliency Game using different features. Iterative random walk allows sharing information across feature spaces, and detecting objects that are otherwise very hard to detect. Extensive experiments over 6 challenging datasets demonstrate the superiority of our proposed unsupervised algorithm compared to several state of the art supervised algorithms.



### From Deterministic to Generative: Multi-Modal Stochastic RNNs for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1708.02478v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02478v2)
- **Published**: 2017-08-08 13:27:13+00:00
- **Updated**: 2017-10-20 09:27:14+00:00
- **Authors**: Jingkuan Song, Yuyu Guo, Lianli Gao, Xuelong Li, Alan Hanjalic, Heng Tao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Video captioning in essential is a complex natural process, which is affected by various uncertainties stemming from video content, subjective judgment, etc. In this paper we build on the recent progress in using encoder-decoder framework for video captioning and address what we find to be a critical deficiency of the existing methods, that most of the decoders propagate deterministic hidden states. Such complex uncertainty cannot be modeled efficiently by the deterministic models. In this paper, we propose a generative approach, referred to as multi-modal stochastic RNNs networks (MS-RNN), which models the uncertainty observed in the data using latent stochastic variables. Therefore, MS-RNN can improve the performance of video captioning, and generate multiple sentences to describe a video considering different random factors. Specifically, a multi-modal LSTM (M-LSTM) is first proposed to interact with both visual and textual features to capture a high-level representation. Then, a backward stochastic LSTM (S-LSTM) is proposed to support uncertainty propagation by introducing latent variables. Experimental results on the challenging datasets MSVD and MSR-VTT show that our proposed MS-RNN approach outperforms the state-of-the-art video captioning benchmarks.



### Binary Generative Adversarial Networks for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1708.04150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.04150v1)
- **Published**: 2017-08-08 14:02:40+00:00
- **Updated**: 2017-08-08 14:02:40+00:00
- **Authors**: Jingkuan Song
- **Comment**: arXiv admin note: text overlap with arXiv:1702.00758 by other authors
- **Journal**: None
- **Summary**: The most striking successes in image retrieval using deep hashing have mostly involved discriminative models, which require labels. In this paper, we use binary generative adversarial networks (BGAN) to embed images to binary codes in an unsupervised way. By restricting the input noise variable of generative adversarial networks (GAN) to be binary and conditioned on the features of each input image, BGAN can simultaneously learn a binary representation per image, and generate an image plausibly similar to the original one. In the proposed framework, we address two main problems: 1) how to directly generate binary codes without relaxation? 2) how to equip the binary representation with the ability of accurate image retrieval? We resolve these problems by proposing new sign-activation strategy and a loss function steering the learning process, which consists of new models for adversarial loss, a content loss, and a neighborhood structure loss. Experimental results on standard datasets (CIFAR-10, NUSWIDE, and Flickr) demonstrate that our BGAN significantly outperforms existing hashing methods by up to 107\% in terms of~mAP (See Table tab.res.map.comp) Our anonymous code is available at: https://github.com/htconquer/BGAN.



### Deep Binaries: Encoding Semantic-Rich Cues for Efficient Textual-Visual Cross Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1708.02531v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1708.02531v1)
- **Published**: 2017-08-08 15:46:16+00:00
- **Updated**: 2017-08-08 15:46:16+00:00
- **Authors**: Yuming Shen, Li Liu, Ling Shao, Jingkuan Song
- **Comment**: Accepted by ICCV 2017 as a conference paper
- **Journal**: None
- **Summary**: Cross-modal hashing is usually regarded as an effective technique for large-scale textual-visual cross retrieval, where data from different modalities are mapped into a shared Hamming space for matching. Most of the traditional textual-visual binary encoding methods only consider holistic image representations and fail to model descriptive sentences. This renders existing methods inappropriate to handle the rich semantics of informative cross-modal data for quality textual-visual search tasks. To address the problem of hashing cross-modal data with semantic-rich cues, in this paper, a novel integrated deep architecture is developed to effectively encode the detailed semantics of informative images and long descriptive sentences, named as Textual-Visual Deep Binaries (TVDB). In particular, region-based convolutional networks with long short-term memory units are introduced to fully explore image regional details while semantic cues of sentences are modeled by a text convolutional network. Additionally, we propose a stochastic batch-wise training routine, where high-quality binary codes and deep encoding functions are efficiently optimized in an alternating manner. Experiments are conducted on three multimedia datasets, i.e. Microsoft COCO, IAPR TC-12, and INRIA Web Queries, where the proposed TVDB model significantly outperforms state-of-the-art binary coding methods in the task of cross-modal retrieval.



### Fast Scene Understanding for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1708.02550v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1708.02550v1)
- **Published**: 2017-08-08 16:31:52+00:00
- **Updated**: 2017-08-08 16:31:52+00:00
- **Authors**: Davy Neven, Bert De Brabandere, Stamatios Georgoulis, Marc Proesmans, Luc Van Gool
- **Comment**: Published at "Deep Learning for Vehicle Perception", workshop at the
  IEEE Symposium on Intelligent Vehicles 2017
- **Journal**: None
- **Summary**: Most approaches for instance-aware semantic labeling traditionally focus on accuracy. Other aspects like runtime and memory footprint are arguably as important for real-time applications such as autonomous driving. Motivated by this observation and inspired by recent works that tackle multiple tasks with a single integrated architecture, in this paper we present a real-time efficient implementation based on ENet that solves three autonomous driving related tasks at once: semantic scene segmentation, instance segmentation and monocular depth estimation. Our approach builds upon a branched ENet architecture with a shared encoder but different decoder branches for each of the three tasks. The presented method can run at 21 fps at a resolution of 1024x512 on the Cityscapes dataset without sacrificing accuracy compared to running each task separately.



### Semantic Instance Segmentation with a Discriminative Loss Function
- **Arxiv ID**: http://arxiv.org/abs/1708.02551v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1708.02551v1)
- **Published**: 2017-08-08 16:32:48+00:00
- **Updated**: 2017-08-08 16:32:48+00:00
- **Authors**: Bert De Brabandere, Davy Neven, Luc Van Gool
- **Comment**: Published at "Deep Learning for Robotic Vision", workshop at CVPR
  2017
- **Journal**: None
- **Summary**: Semantic instance segmentation remains a challenging task. In this work we propose to tackle the problem with a discriminative loss function, operating at the pixel level, that encourages a convolutional network to produce a representation of the image that can easily be clustered into instances with a simple post-processing step. The loss function encourages the network to map each pixel to a point in feature space so that pixels belonging to the same instance lie close together while different instances are separated by a wide margin. Our approach of combining an off-the-shelf network with a principled loss function inspired by a metric learning objective is conceptually simple and distinct from recent efforts in instance segmentation. In contrast to previous works, our method does not rely on object proposals or recurrent mechanisms. A key contribution of our work is to demonstrate that such a simple setup without bells and whistles is effective and can perform on par with more complex methods. Moreover, we show that it does not suffer from some of the limitations of the popular detect-and-segment approaches. We achieve competitive performance on the Cityscapes and CVPPP leaf segmentation benchmarks.



### An Error Detection and Correction Framework for Connectomics
- **Arxiv ID**: http://arxiv.org/abs/1708.02599v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02599v2)
- **Published**: 2017-08-08 18:26:12+00:00
- **Updated**: 2017-12-03 21:46:45+00:00
- **Authors**: Jonathan Zung, Ignacio Tartavull, Kisuk Lee, H. Sebastian Seung
- **Comment**: None
- **Journal**: None
- **Summary**: We define and study error detection and correction tasks that are useful for 3D reconstruction of neurons from electron microscopic imagery, and for image segmentation more generally. Both tasks take as input the raw image and a binary mask representing a candidate object. For the error detection task, the desired output is a map of split and merge errors in the object. For the error correction task, the desired output is the true object. We call this object mask pruning, because the candidate object mask is assumed to be a superset of the true object. We train multiscale 3D convolutional networks to perform both tasks. We find that the error-detecting net can achieve high accuracy. The accuracy of the error-correcting net is enhanced if its input object mask is "advice" (union of erroneous objects) from the error-detecting net.



### Learning Visual Importance for Graphic Designs and Data Visualizations
- **Arxiv ID**: http://arxiv.org/abs/1708.02660v1
- **DOI**: 10.1145/3126594.3126653
- **Categories**: **cs.HC**, cs.CV, H.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1708.02660v1)
- **Published**: 2017-08-08 21:50:51+00:00
- **Updated**: 2017-08-08 21:50:51+00:00
- **Authors**: Zoya Bylinskii, Nam Wook Kim, Peter O'Donovan, Sami Alsheikh, Spandan Madan, Hanspeter Pfister, Fredo Durand, Bryan Russell, Aaron Hertzmann
- **Comment**: None
- **Journal**: UIST 2017
- **Summary**: Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process.



### A discriminative view of MRF pre-processing algorithms
- **Arxiv ID**: http://arxiv.org/abs/1708.02668v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02668v1)
- **Published**: 2017-08-08 22:41:43+00:00
- **Updated**: 2017-08-08 22:41:43+00:00
- **Authors**: Chen Wang, Charles Herrmann, Ramin Zabih
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: While Markov Random Fields (MRFs) are widely used in computer vision, they present a quite challenging inference problem. MRF inference can be accelerated by pre-processing techniques like Dead End Elimination (DEE) or QPBO-based approaches which compute the optimal labeling of a subset of variables. These techniques are guaranteed to never wrongly label a variable but they often leave a large number of variables unlabeled. We address this shortcoming by interpreting pre-processing as a classification problem, which allows us to trade off false positives (i.e., giving a variable an incorrect label) versus false negatives (i.e., failing to label a variable). We describe an efficient discriminative rule that finds optimal solutions for a subset of variables. Our technique provides both per-instance and worst-case guarantees concerning the quality of the solution. Empirical studies were conducted over several benchmark datasets. We obtain a speedup factor of 2 to 12 over expansion moves without preprocessing, and on difficult non-submodular energy functions produce slightly lower energy.



### Improved Fixed-Rank Nyström Approximation via QR Decomposition: Practical and Theoretical Aspects
- **Arxiv ID**: http://arxiv.org/abs/1708.03218v2
- **DOI**: 10.1016/j.neucom.2019.06.070
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.03218v2)
- **Published**: 2017-08-08 23:52:53+00:00
- **Updated**: 2019-12-02 15:11:19+00:00
- **Authors**: Farhad Pourkamali-Anaraki, Stephen Becker
- **Comment**: Accepted in Neurocomputing. arXiv admin note: text overlap with
  arXiv:1612.06470
- **Journal**: Neurocomputing, 2019
- **Summary**: The Nystrom method is a popular technique that uses a small number of landmark points to compute a fixed-rank approximation of large kernel matrices that arise in machine learning problems. In practice, to ensure high quality approximations, the number of landmark points is chosen to be greater than the target rank. However, for simplicity the standard Nystrom method uses a sub-optimal procedure for rank reduction. In this paper, we examine the drawbacks of the standard Nystrom method in terms of poor performance and lack of theoretical guarantees. To address these issues, we present an efficient modification for generating improved fixed-rank Nystrom approximations. Theoretical analysis and numerical experiments are provided to demonstrate the advantages of the modified method over the standard Nystrom method. Overall, the aim of this paper is to convince researchers to use the modified method, as it has nearly identical computational complexity, is easy to code, has greatly improved accuracy in many cases, and is optimal in a sense that we make precise.



### Generative Adversarial Network-based Synthesis of Visible Faces from Polarimetric Thermal Faces
- **Arxiv ID**: http://arxiv.org/abs/1708.02681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02681v1)
- **Published**: 2017-08-08 23:57:12+00:00
- **Updated**: 2017-08-08 23:57:12+00:00
- **Authors**: He Zhang, Vishal M. Patel, Benjamin S. Riggan, Shuowen Hu
- **Comment**: None
- **Journal**: None
- **Summary**: The large domain discrepancy between faces captured in polarimetric (or conventional) thermal and visible domain makes cross-domain face recognition quite a challenging problem for both human-examiners and computer vision algorithms. Previous approaches utilize a two-step procedure (visible feature estimation and visible image reconstruction) to synthesize the visible image given the corresponding polarimetric thermal image. However, these are regarded as two disjoint steps and hence may hinder the performance of visible face reconstruction. We argue that joint optimization would be a better way to reconstruct more photo-realistic images for both computer vision algorithms and human-examiners to examine. To this end, this paper proposes a Generative Adversarial Network-based Visible Face Synthesis (GAN-VFS) method to synthesize more photo-realistic visible face images from their corresponding polarimetric images. To ensure that the encoded visible-features contain more semantically meaningful information in reconstructing the visible face image, a guidance sub-network is involved into the training procedure. To achieve photo realistic property while preserving discriminative characteristics for the reconstructed outputs, an identity loss combined with the perceptual loss are optimized in the framework. Multiple experiments evaluated on different experimental protocols demonstrate that the proposed method achieves state-of-the-art performance.



