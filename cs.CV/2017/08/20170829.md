# Arxiv Papers in cs.CV on 2017-08-29
### CirCNN: Accelerating and Compressing Deep Neural Networks Using Block-CirculantWeight Matrices
- **Arxiv ID**: http://arxiv.org/abs/1708.08917v1
- **DOI**: 10.1145/3123939.3124552
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.08917v1)
- **Published**: 2017-08-29 04:18:57+00:00
- **Updated**: 2017-08-29 04:18:57+00:00
- **Authors**: Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei Zhuo, Chao Wang, Xuehai Qian, Yu Bai, Geng Yuan, Xiaolong Ma, Yipeng Zhang, Jian Tang, Qinru Qiu, Xue Lin, Bo Yuan
- **Comment**: 14 pages, 15 Figures, conference
- **Journal**: None
- **Summary**: Large-scale deep neural networks (DNNs) are both compute and memory intensive. As the size of DNNs continues to grow, it is critical to improve the energy efficiency and performance while maintaining accuracy. For DNNs, the model size is an important factor affecting performance, scalability and energy efficiency. Weight pruning achieves good compression ratios but suffers from three drawbacks: 1) the irregular network structure after pruning; 2) the increased training complexity; and 3) the lack of rigorous guarantee of compression ratio and inference accuracy. To overcome these limitations, this paper proposes CirCNN, a principled approach to represent weights and process neural networks using block-circulant matrices. CirCNN utilizes the Fast Fourier Transform (FFT)-based fast multiplication, simultaneously reducing the computational complexity (both in inference and training) from O(n2) to O(nlogn) and the storage complexity from O(n2) to O(n), with negligible accuracy loss. Compared to other approaches, CirCNN is distinct due to its mathematical rigor: it can converge to the same effectiveness as DNNs without compression. The CirCNN architecture, a universal DNN inference engine that can be implemented on various hardware/software platforms with configurable network architecture. To demonstrate the performance and energy efficiency, we test CirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN architecture achieves very high energy efficiency and performance with a small hardware footprint. Based on the FPGA implementation and ASIC synthesis results, CirCNN achieves 6-102X energy efficiency improvements compared with the best state-of-the-art results.



### Performance Analysis of Open Source Machine Learning Frameworks for Various Parameters in Single-Threaded and Multi-Threaded Modes
- **Arxiv ID**: http://arxiv.org/abs/1708.08670v1
- **DOI**: 10.1007/978-3-319-70581-1_17
- **Categories**: **cs.LG**, cs.CV, cs.DC, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1708.08670v1)
- **Published**: 2017-08-29 09:54:28+00:00
- **Updated**: 2017-08-29 09:54:28+00:00
- **Authors**: Yuriy Kochura, Sergii Stirenko, Oleg Alienin, Michail Novotarskiy, Yuri Gordienko
- **Comment**: 15 pages, 11 figures, 4 tables; this paper summarizes the activities
  which were started recently and described shortly in the previous conference
  presentations arXiv:1706.02248 and arXiv:1707.04940; it is accepted for
  Springer book series "Advances in Intelligent Systems and Computing"
- **Journal**: Advances in Intelligent Systems and Computing II. CSIT 2017.
  Advances in Intelligent Systems and Computing, vol 689, pp 243-256. Springer,
  Cham
- **Summary**: The basic features of some of the most versatile and popular open source frameworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are considered and compared. Their comparative analysis was performed and conclusions were made as to the advantages and disadvantages of these platforms. The performance tests for the de facto standard MNIST data set were carried out on H2O framework for deep learning algorithms designed for CPU and GPU platforms for single-threaded and multithreaded modes of operation Also, we present the results of testing neural networks architectures on H2O platform for various activation functions, stopping metrics, and other parameters of machine learning algorithm. It was demonstrated for the use case of MNIST database of handwritten digits in single-threaded mode that blind selection of these parameters can hugely increase (by 2-3 orders) the runtime without the significant increase of precision. This result can have crucial influence for optimization of available and new machine learning methods, especially for image recognition problems.



### Performance Guaranteed Network Acceleration via High-Order Residual Quantization
- **Arxiv ID**: http://arxiv.org/abs/1708.08687v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08687v1)
- **Published**: 2017-08-29 10:42:29+00:00
- **Updated**: 2017-08-29 10:42:29+00:00
- **Authors**: Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, Wen Gao
- **Comment**: 9 pages, 8 figures, Proceeding of IEEE International Conference on
  Computer Vision 2017
- **Journal**: None
- **Summary**: Input binarization has shown to be an effective way for network acceleration. However, previous binarization scheme could be regarded as simple pixel-wise thresholding operations (i.e., order-one approximation) and suffers a big accuracy loss. In this paper, we propose a highorder binarization scheme, which achieves more accurate approximation while still possesses the advantage of binary operation. In particular, the proposed scheme recursively performs residual quantization and yields a series of binary input images with decreasing magnitude scales. Accordingly, we propose high-order binary filtering and gradient propagation operations for both forward and backward computations. Theoretical analysis shows approximation error guarantee property of proposed method. Extensive experimental results demonstrate that the proposed scheme yields great recognition accuracy while being accelerated.



### Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.08705v2
- **DOI**: 10.1109/TSP.2018.2846226
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.08705v2)
- **Published**: 2017-08-29 11:43:40+00:00
- **Updated**: 2018-06-30 19:46:15+00:00
- **Authors**: Jeremias Sulam, Vardan Papyan, Yaniv Romano, Michael Elad
- **Comment**: None
- **Journal**: IEEE Transactions on Signal Processing, vol. 66, no. 15, pp.
  4090-4104, Aug.1, 1 2018
- **Summary**: The recently proposed Multi-Layer Convolutional Sparse Coding (ML-CSC) model, consisting of a cascade of convolutional sparse layers, provides a new interpretation of Convolutional Neural Networks (CNNs). Under this framework, the computation of the forward pass in a CNN is equivalent to a pursuit algorithm aiming to estimate the nested sparse representation vectors -- or feature maps -- from a given input signal. Despite having served as a pivotal connection between CNNs and sparse modeling, a deeper understanding of the ML-CSC is still lacking: there are no pursuit algorithms that can serve this model exactly, nor are there conditions to guarantee a non-empty model. While one can easily obtain signals that approximately satisfy the ML-CSC constraints, it remains unclear how to simply sample from the model and, more importantly, how one can train the convolutional filters from real data.   In this work, we propose a sound pursuit algorithm for the ML-CSC model by adopting a projection approach. We provide new and improved bounds on the stability of the solution of such pursuit and we analyze different practical alternatives to implement this in practice. We show that the training of the filters is essential to allow for non-trivial signals in the model, and we derive an online algorithm to learn the dictionaries from real data, effectively resulting in cascaded sparse convolutional layers. Last, but not least, we demonstrate the applicability of the ML-CSC model for several applications in an unsupervised setting, providing competitive results. Our work represents a bridge between matrix factorization, sparse dictionary learning and sparse auto-encoders, and we analyze these connections in detail.



### Setting an attention region for convolutional neural networks using region selective features, for recognition of materials within glass vessels
- **Arxiv ID**: http://arxiv.org/abs/1708.08711v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08711v3)
- **Published**: 2017-08-29 11:53:37+00:00
- **Updated**: 2017-09-09 19:25:27+00:00
- **Authors**: Sagi Eppel
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks have emerged as the leading method for the classification and segmentation of images. In some cases, it is desirable to focus the attention of the net on a specific region in the image; one such case is the recognition of the contents of transparent vessels, where the vessel region in the image is already known. This work presents a valve filter approach for focusing the attention of the net on a region of interest (ROI). In this approach, the ROI is inserted into the net as a binary map. The net uses a different set of convolution filters for the ROI and background image regions, resulting in a different set of features being extracted from each region. More accurately, for each filter used on the image, a corresponding valve filter exists that acts on the ROI map and determines the regions in which the corresponding image filter will be used. This valve filter effectively acts as a valve that inhibits specific features in different image regions according to the ROI map. In addition, a new data set for images of materials in glassware vessels in a chemistry laboratory setting is presented. This data set contains a thousand images with pixel-wise annotation according to categories ranging from filled and empty to the exact phase of the material inside the vessel. The results of the valve filter approach and fully convolutional neural nets (FCN) with no ROI input are compared based on this data set.



### Curriculum Learning for Multi-Task Classification of Visual Attributes
- **Arxiv ID**: http://arxiv.org/abs/1708.08728v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08728v1)
- **Published**: 2017-08-29 12:40:52+00:00
- **Updated**: 2017-08-29 12:40:52+00:00
- **Authors**: Nikolaos Sarafianos, Theodore Giannakopoulos, Christophoros Nikou, Ioannis A. Kakadiaris
- **Comment**: To appear in ICCV Workshops 2017 (TASK-CV)
- **Journal**: None
- **Summary**: Visual attributes, from simple objects (e.g., backpacks, hats) to soft-biometrics (e.g., gender, height, clothing) have proven to be a powerful representational approach for many applications such as image description and human identification. In this paper, we introduce a novel method to combine the advantages of both multi-task and curriculum learning in a visual attribute classification framework. Individual tasks are grouped based on their correlation so that two groups of strongly and weakly correlated tasks are formed. The two groups of tasks are learned in a curriculum learning setup by transferring the acquired knowledge from the strongly to the weakly correlated. The learning process within each group though, is performed in a multi-task classification setup. The proposed method learns better and converges faster than learning all the tasks in a typical multi-task learning paradigm. We demonstrate the effectiveness of our approach on the publicly available, SoBiR, VIPeR and PETA datasets and report state-of-the-art results across the board.



### Multi-view Low-rank Sparse Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1708.08732v1
- **DOI**: 10.1016/j.patcog.2017.08.024
- **Categories**: **cs.CV**, cs.LG, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.08732v1)
- **Published**: 2017-08-29 13:07:56+00:00
- **Updated**: 2017-08-29 13:07:56+00:00
- **Authors**: Maria Brbic, Ivica Kopriva
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing approaches address multi-view subspace clustering problem by constructing the affinity matrix on each view separately and afterwards propose how to extend spectral clustering algorithm to handle multi-view data. This paper presents an approach to multi-view subspace clustering that learns a joint subspace representation by constructing affinity matrix shared among all views. Relying on the importance of both low-rank and sparsity constraints in the construction of the affinity matrix, we introduce the objective that balances between the agreement across different views, while at the same time encourages sparsity and low-rankness of the solution. Related low-rank and sparsity constrained optimization problem is for each view solved using the alternating direction method of multipliers. Furthermore, we extend our approach to cluster data drawn from nonlinear subspaces by solving the corresponding problem in a reproducing kernel Hilbert space. The proposed algorithm outperforms state-of-the-art multi-view subspace clustering algorithms on one synthetic and four real-world datasets.



### Autoencoder with recurrent neural networks for video forgery detection
- **Arxiv ID**: http://arxiv.org/abs/1708.08754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08754v1)
- **Published**: 2017-08-29 14:06:16+00:00
- **Updated**: 2017-08-29 14:06:16+00:00
- **Authors**: Dario D'Avino, Davide Cozzolino, Giovanni Poggi, Luisa Verdoliva
- **Comment**: Presented at IS&T Electronic Imaging: Media Watermarking, Security,
  and Forensics, January 2017
- **Journal**: None
- **Summary**: Video forgery detection is becoming an important issue in recent years, because modern editing software provide powerful and easy-to-use tools to manipulate videos. In this paper we propose to perform detection by means of deep learning, with an architecture based on autoencoders and recurrent neural networks. A training phase on a few pristine frames allows the autoencoder to learn an intrinsic model of the source. Then, forged material is singled out as anomalous, as it does not fit the learned model, and is encoded with a large reconstruction error. Recursive networks, implemented with the long short-term memory model, are used to exploit temporal dependencies. Preliminary results on forged videos show the potential of this approach.



### 4D Multi-atlas Label Fusion using Longitudinal Images
- **Arxiv ID**: http://arxiv.org/abs/1708.08825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08825v1)
- **Published**: 2017-08-29 15:33:53+00:00
- **Updated**: 2017-08-29 15:33:53+00:00
- **Authors**: Yuankai Huo, Susan M. Resnick, Bennett A. Landman
- **Comment**: None
- **Journal**: None
- **Summary**: Longitudinal reproducibility is an essential concern in automated medical image segmentation, yet has proven to be an elusive objective as manual brain structure tracings have shown more than 10% variability. To improve reproducibility, lon-gitudinal segmentation (4D) approaches have been investigated to reconcile tem-poral variations with traditional 3D approaches. In the past decade, multi-atlas la-bel fusion has become a state-of-the-art segmentation technique for 3D image and many efforts have been made to adapt it to a 4D longitudinal fashion. However, the previous methods were either limited by using application specified energy function (e.g., surface fusion and multi model fusion) or only considered tem-poral smoothness on two consecutive time points (t and t+1) under sparsity as-sumption. Therefore, a 4D multi-atlas label fusion theory for general label fusion purpose and simultaneously considering temporal consistency on all time points is appealing. Herein, we propose a novel longitudinal label fusion algorithm, called 4D joint label fusion (4DJLF), to incorporate the temporal consistency modeling via non-local patch-intensity covariance models. The advantages of 4DJLF include: (1) 4DJLF is under the general label fusion framework by simul-taneously incorporating the spatial and temporal covariance on all longitudinal time points. (2) The proposed algorithm is a longitudinal generalization of a lead-ing joint label fusion method (JLF) that has proven adaptable to a wide variety of applications. (3) The spatial temporal consistency of atlases is modeled in a prob-abilistic model inspired from both voting based and statistical fusion. The pro-posed approach improves the consistency of the longitudinal segmentation while retaining sensitivity compared with original JLF approach using the same set of atlases. The method is available online in open-source.



### Semantic Texture for Robust Dense Tracking
- **Arxiv ID**: http://arxiv.org/abs/1708.08844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08844v1)
- **Published**: 2017-08-29 15:58:18+00:00
- **Updated**: 2017-08-29 15:58:18+00:00
- **Authors**: Jan Czarnowski, Stefan Leutenegger, Andrew Davison
- **Comment**: None
- **Journal**: None
- **Summary**: We argue that robust dense SLAM systems can make valuable use of the layers of features coming from a standard CNN as a pyramid of `semantic texture' which is suitable for dense alignment while being much more robust to nuisance factors such as lighting than raw RGB values. We use a straightforward Lucas-Kanade formulation of image alignment, with a schedule of iterations over the coarse-to-fine levels of a pyramid, and simply replace the usual image pyramid by the hierarchy of convolutional feature maps from a pre-trained CNN. The resulting dense alignment performance is much more robust to lighting and other variations, as we show by camera rotation tracking experiments on time-lapse sequences captured over many hours. Looking towards the future of scene representation for real-time visual SLAM, we further demonstrate that a selection using simple criteria of a small number of the total set of features output by a CNN gives just as accurate but much more efficient tracking performance.



### Sparsity-Based Super Resolution for SEM Images
- **Arxiv ID**: http://arxiv.org/abs/1709.02235v1
- **DOI**: 10.1021/acs.nanolett.7b02091
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02235v1)
- **Published**: 2017-08-29 16:23:43+00:00
- **Updated**: 2017-08-29 16:23:43+00:00
- **Authors**: Shahar Tsiper, Or Dicker, Idan Kaizerman, Zeev Zohar, Mordechai Segev, Yonina C. Eldar
- **Comment**: Final publication available at ACS Nano Letters
- **Journal**: None
- **Summary**: The scanning electron microscope (SEM) produces an image of a sample by scanning it with a focused beam of electrons. The electrons interact with the atoms in the sample, which emit secondary electrons that contain information about the surface topography and composition. The sample is scanned by the electron beam point by point, until an image of the surface is formed. Since its invention in 1942, SEMs have become paramount in the discovery and understanding of the nanometer world, and today it is extensively used for both research and in industry. In principle, SEMs can achieve resolution better than one nanometer. However, for many applications, working at sub-nanometer resolution implies an exceedingly large number of scanning points. For exactly this reason, the SEM diagnostics of microelectronic chips is performed either at high resolution (HR) over a small area or at low resolution (LR) while capturing a larger portion of the chip. Here, we employ sparse coding and dictionary learning to algorithmically enhance LR SEM images of microelectronic chips up to the level of the HR images acquired by slow SEM scans, while considerably reducing the noise. Our methodology consists of two steps: an offline stage of learning a joint dictionary from a sequence of LR and HR images of the same region in the chip, followed by a fast-online super-resolution step where the resolution of a new LR image is enhanced. We provide several examples with typical chips used in the microelectronics industry, as well as a statistical study on arbitrary images with characteristic structural features. Conceptually, our method works well when the images have similar characteristics. This work demonstrates that employing sparsity concepts can greatly improve the performance of SEM, thereby considerably increasing the scanning throughput without compromising on analysis quality and resolution.



### Reasoning about Fine-grained Attribute Phrases using Reference Games
- **Arxiv ID**: http://arxiv.org/abs/1708.08874v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08874v1)
- **Published**: 2017-08-29 16:57:39+00:00
- **Updated**: 2017-08-29 16:57:39+00:00
- **Authors**: Jong-Chyi Su, Chenyun Wu, Huaizu Jiang, Subhransu Maji
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: We present a framework for learning to describe fine-grained visual differences between instances using attribute phrases. Attribute phrases capture distinguishing aspects of an object (e.g., "propeller on the nose" or "door near the wing" for airplanes) in a compositional manner. Instances within a category can be described by a set of these phrases and collectively they span the space of semantic attributes for a category. We collect a large dataset of such phrases by asking annotators to describe several visual differences between a pair of instances within a category. We then learn to describe and ground these phrases to images in the context of a *reference game* between a speaker and a listener. The goal of a speaker is to describe attributes of an image that allows the listener to correctly identify it within a pair. Data collected in a pairwise manner improves the ability of the speaker to generate, and the ability of the listener to interpret visual descriptions. Moreover, due to the compositionality of attribute phrases, the trained listeners can interpret descriptions not seen during training for image retrieval, and the speakers can generate attribute-based explanations for differences between previously unseen categories. We also show that embedding an image into the semantic space of attribute phrases derived from listeners offers 20% improvement in accuracy over existing attribute-based representations on the FGVC-aircraft dataset.



### Visual Cues to Improve Myoelectric Control of Upper Limb Prostheses
- **Arxiv ID**: http://arxiv.org/abs/1709.02236v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.02236v1)
- **Published**: 2017-08-29 16:59:19+00:00
- **Updated**: 2017-08-29 16:59:19+00:00
- **Authors**: Andrea Gigli, Arjan Gijsberts, Valentina Gregori, Matteo Cognolato, Manfredo Atzori, Barbara Caputo
- **Comment**: None
- **Journal**: None
- **Summary**: The instability of myoelectric signals over time complicates their use to control highly articulated prostheses. To address this problem, studies have tried to combine surface electromyography with modalities that are less affected by the amputation and environment, such as accelerometry or gaze information. In the latter case, the hypothesis is that a subject looks at the object he or she intends to manipulate and that knowing this object's affordances allows to constrain the set of possible grasps. In this paper, we develop an automated way to detect stable fixations and show that gaze information is indeed helpful in predicting hand movements. In our multimodal approach, we automatically detect stable gazes and segment an object of interest around the subject's fixation in the visual frame. The patch extracted around this object is subsequently fed through an off-the-shelf deep convolutional neural network to obtain a high level feature representation, which is then combined with traditional surface electromyography in the classification stage. Tests have been performed on a dataset acquired from five intact subjects who performed ten types of grasps on various objects as well as in a functional setting. They show that the addition of gaze information increases the classification accuracy considerably. Further analysis demonstrates that this improvement is consistent for all grasps and concentrated during the movement onset and offset.



### Pix2face: Direct 3D Face Model Estimation
- **Arxiv ID**: http://arxiv.org/abs/1708.09006v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09006v1)
- **Published**: 2017-08-29 20:13:00+00:00
- **Updated**: 2017-08-29 20:13:00+00:00
- **Authors**: Daniel Crispell, Maxim Bazik
- **Comment**: To appear in 2017 ICCV "300 3D Facial-Videos in-the-Wild Challenge"
  Workshop
- **Journal**: None
- **Summary**: An efficient, fully automatic method for 3D face shape and pose estimation in unconstrained 2D imagery is presented. The proposed method jointly estimates a dense set of 3D landmarks and facial geometry using a single pass of a modified version of the popular "U-Net" neural network architecture. Additionally, we propose a method for directly estimating a set of 3D Morphable Model (3DMM) parameters, using the estimated 3D landmarks and geometry as constraints in a simple linear system. Qualitative modeling results are presented, as well as quantitative evaluation of predicted 3D face landmarks in unconstrained video sequences.



### Sparse-then-Dense Alignment based 3D Map Reconstruction Method for Endoscopic Capsule Robots
- **Arxiv ID**: http://arxiv.org/abs/1708.09740v1
- **DOI**: 10.1007/s00138-017-0905-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09740v1)
- **Published**: 2017-08-29 20:58:29+00:00
- **Updated**: 2017-08-29 20:58:29+00:00
- **Authors**: Mehmet Turan, Yusuf Yigit Pilavci, Ipek Ganiyusufoglu, Helder Araujo, Ender Konukoglu, Metin Sitti
- **Comment**: arXiv admin note: text overlap with arXiv:1705.06524
- **Journal**: None
- **Summary**: Since the development of capsule endoscopcy technology, substantial progress were made in converting passive capsule endoscopes to robotic active capsule endoscopes which can be controlled by the doctor. However, robotic capsule endoscopy still has some challenges. In particular, the use of such devices to generate a precise and globally consistent three-dimensional (3D) map of the entire inner organ remains an unsolved problem. Such global 3D maps of inner organs would help doctors to detect the location and size of diseased areas more accurately, precisely, and intuitively, thus permitting more accurate and intuitive diagnoses. The proposed 3D reconstruction system is built in a modular fashion including preprocessing, frame stitching, and shading-based 3D reconstruction modules. We propose an efficient scheme to automatically select the key frames out of the huge quantity of raw endoscopic images. Together with a bundle fusion approach that aligns all the selected key frames jointly in a globally consistent way, a significant improvement of the mosaic and 3D map accuracy was reached. To the best of our knowledge, this framework is the first complete pipeline for an endoscopic capsule robot based 3D map reconstruction containing all of the necessary steps for a reliable and accurate endoscopic 3D map. For the qualitative evaluations, a real pig stomach is employed. Moreover, for the first time in literature, a detailed and comprehensive quantitative analysis of each proposed pipeline modules is performed using a non-rigid esophagus gastro duodenoscopy simulator, four different endoscopic cameras, a magnetically activated soft capsule robot (MASCE), a sub-millimeter precise optical motion tracker and a fine-scale 3D optical scanner.



### Convolutional Sparse Coding with Overlapping Group Norms
- **Arxiv ID**: http://arxiv.org/abs/1708.09038v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1708.09038v1)
- **Published**: 2017-08-29 21:37:15+00:00
- **Updated**: 2017-08-29 21:37:15+00:00
- **Authors**: Brendt Wohlberg
- **Comment**: None
- **Journal**: None
- **Summary**: The most widely used form of convolutional sparse coding uses an $\ell_1$ regularization term. While this approach has been successful in a variety of applications, a limitation of the $\ell_1$ penalty is that it is homogeneous across the spatial and filter index dimensions of the sparse representation array, so that sparsity cannot be separately controlled across these dimensions. The present paper considers the consequences of replacing the $\ell_1$ penalty with a mixed group norm, motivated by recent theoretical results for convolutional sparse representations. Algorithms are developed for solving the resulting problems, which are quite challenging, and the impact on the performance of the denoising problem is evaluated. The mixed group norms are found to perform very poorly in this application. While their performance is greatly improved by introducing a weighting strategy, such a strategy also improves the performance obtained from the much simpler and computationally cheaper $\ell_1$ norm.



