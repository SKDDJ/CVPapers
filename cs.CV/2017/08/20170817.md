# Arxiv Papers in cs.CV on 2017-08-17
### Evaluating Visual Conversational Agents via Cooperative Human-AI Games
- **Arxiv ID**: http://arxiv.org/abs/1708.05122v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.05122v1)
- **Published**: 2017-08-17 03:27:53+00:00
- **Updated**: 2017-08-17 03:27:53+00:00
- **Authors**: Prithvijit Chattopadhyay, Deshraj Yadav, Viraj Prabhu, Arjun Chandrasekaran, Abhishek Das, Stefan Lee, Dhruv Batra, Devi Parikh
- **Comment**: HCOMP 2017
- **Journal**: None
- **Summary**: As AI continues to advance, human-AI teams are inevitable. However, progress in AI is routinely measured in isolation, without a human in the loop. It is crucial to benchmark progress in AI, not just in isolation, but also in terms of how it translates to helping humans perform certain tasks, i.e., the performance of human-AI teams.   In this work, we design a cooperative game - GuessWhich - to measure human-AI team performance in the specific context of the AI being a visual conversational agent. GuessWhich involves live interaction between the human and the AI. The AI, which we call ALICE, is provided an image which is unseen by the human. Following a brief description of the image, the human questions ALICE about this secret image to identify it from a fixed pool of images.   We measure performance of the human-ALICE team by the number of guesses it takes the human to correctly identify the secret image after a fixed number of dialog rounds with ALICE. We compare performance of the human-ALICE teams for two versions of ALICE. Our human studies suggest a counterintuitive trend - that while AI literature shows that one version outperforms the other when paired with an AI questioner bot, we find that this improvement in AI-AI performance does not translate to improved human-AI performance. This suggests a mismatch between benchmarking of AI in isolation and in the context of human-AI teams.



### Hyperspectral Unmixing: Ground Truth Labeling, Datasets, Benchmark Performances and Survey
- **Arxiv ID**: http://arxiv.org/abs/1708.05125v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05125v2)
- **Published**: 2017-08-17 03:35:02+00:00
- **Updated**: 2017-10-11 16:22:06+00:00
- **Authors**: Feiyun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral unmixing (HU) is a very useful and increasingly popular preprocessing step for a wide range of hyperspectral applications. However, the HU research has been constrained a lot by three factors: (a) the number of hyperspectral images (especially the ones with ground truths) are very limited; (b) the ground truths of most hyperspectral images are not shared on the web, which may cause lots of unnecessary troubles for researchers to evaluate their algorithms; (c) the codes of most state-of-the-art methods are not shared, which may also delay the testing of new methods.   Accordingly, this paper deals with the above issues from the following three perspectives: (1) as a profound contribution, we provide a general labeling method for the HU. With it, we labeled up to 15 hyperspectral images, providing 18 versions of ground truths. To the best of our knowledge, this is the first paper to summarize and share up to 15 hyperspectral images and their 18 versions of ground truths for the HU. Observing that the hyperspectral classification (HyC) has much more standard datasets (whose ground truths are generally publicly shared) than the HU, we propose an interesting method to transform the HyC datasets for the HU research. (2) To further facilitate the evaluation of HU methods under different conditions, we reviewed and implemented the algorithm to generate a complex synthetic hyperspectral image. By tuning the hyper-parameters in the code, we may verify the HU methods from four perspectives. The code would also be shared on the web. (3) To provide a standard comparison, we reviewed up to 10 state-of-the-art HU algorithms, then selected the 5 most benchmark HU algorithms, and compared them on the 15 real hyperspectral datasets. The experiment results are surely reproducible; the implemented codes would be shared on the web.



### Deep Binary Reconstruction for Cross-modal Hashing
- **Arxiv ID**: http://arxiv.org/abs/1708.05127v2
- **DOI**: 10.1145/3123266.3123355
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1708.05127v2)
- **Published**: 2017-08-17 04:05:58+00:00
- **Updated**: 2017-08-24 01:35:54+00:00
- **Authors**: Xuelong Li, Di Hu, Feiping Nie
- **Comment**: 8 pages, 5 figures, accepted by ACM Multimedia 2017
- **Journal**: None
- **Summary**: With the increasing demand of massive multimodal data storage and organization, cross-modal retrieval based on hashing technique has drawn much attention nowadays. It takes the binary codes of one modality as the query to retrieve the relevant hashing codes of another modality. However, the existing binary constraint makes it difficult to find the optimal cross-modal hashing function. Most approaches choose to relax the constraint and perform thresholding strategy on the real-value representation instead of directly solving the original objective. In this paper, we first provide a concrete analysis about the effectiveness of multimodal networks in preserving the inter- and intra-modal consistency. Based on the analysis, we provide a so-called Deep Binary Reconstruction (DBRC) network that can directly learn the binary hashing codes in an unsupervised fashion. The superiority comes from a proposed simple but efficient activation function, named as Adaptive Tanh (ATanh). The ATanh function can adaptively learn the binary codes and be trained via back-propagation. Extensive experiments on three benchmark datasets demonstrate that DBRC outperforms several state-of-the-art methods in both image2text and text2image retrieval task.



### Deep Scene Text Detection with Connected Component Proposals
- **Arxiv ID**: http://arxiv.org/abs/1708.05133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05133v1)
- **Published**: 2017-08-17 04:44:03+00:00
- **Updated**: 2017-08-17 04:44:03+00:00
- **Authors**: Fan Jiang, Zhihui Hao, Xinran Liu
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: A growing demand for natural-scene text detection has been witnessed by the computer vision community since text information plays a significant role in scene understanding and image indexing. Deep neural networks are being used due to their strong capabilities of pixel-wise classification or word localization, similar to being used in common vision problems. In this paper, we present a novel two-task network with integrating bottom and top cues. The first task aims to predict a pixel-by-pixel labeling and based on which, word proposals are generated with a canonical connected component analysis. The second task aims to output a bundle of character candidates used later to verify the word proposals. The two sub-networks share base convolutional features and moreover, we present a new loss to strengthen the interaction between them. We evaluate the proposed network on public benchmark datasets and show it can detect arbitrary-orientation scene text with a finer output boundary. In ICDAR 2013 text localization task, we achieve the state-of-the-art performance with an F-score of 0.919 and a much better recall of 0.915.



### Pixel-Level Matching for Video Object Segmentation using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.05137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05137v1)
- **Published**: 2017-08-17 05:04:23+00:00
- **Updated**: 2017-08-17 05:04:23+00:00
- **Authors**: Jae Shin Yoon, Francois Rameau, Junsik Kim, Seokju Lee, Seunghak Shin, In So Kweon
- **Comment**: To appear on ICCV 2017
- **Journal**: None
- **Summary**: We propose a novel video object segmentation algorithm based on pixel-level matching using Convolutional Neural Networks (CNN). Our network aims to distinguish the target area from the background on the basis of the pixel-level similarity between two object units. The proposed network represents a target object using features from different depth layers in order to take advantage of both the spatial details and the category-level semantic information. Furthermore, we propose a feature compression technique that drastically reduces the memory requirements while maintaining the capability of feature representation. Two-stage training (pre-training and fine-tuning) allows our network to handle any target object regardless of its category (even if the object's type does not belong to the pre-training data) or of variations in its appearance through a video sequence. Experiments on large datasets demonstrate the effectiveness of our model - against related methods - in terms of accuracy, speed, and stability. Finally, we introduce the transferability of our network to different domains, such as the infrared data domain.



### High Efficient Reconstruction of Single-shot T2 Mapping from OverLapping-Echo Detachment Planar Imaging Based on Deep Residual Network
- **Arxiv ID**: http://arxiv.org/abs/1708.05170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05170v1)
- **Published**: 2017-08-17 08:48:21+00:00
- **Updated**: 2017-08-17 08:48:21+00:00
- **Authors**: Congbo Cai, Yiqing Zeng, Chao Wang, Shuhui Cai, Jun Zhang, Zhong Chen, Xinghao Ding, Jianhui Zhong
- **Comment**: 18 pages, 7 figures
- **Journal**: None
- **Summary**: Purpose: An end-to-end deep convolutional neural network (CNN) based on deep residual network (ResNet) was proposed to efficiently reconstruct reliable T2 mapping from single-shot OverLapping-Echo Detachment (OLED) planar imaging. Methods: The training dataset was obtained from simulations carried out on SPROM software developed by our group. The relationship between the original OLED image containing two echo signals and the corresponded T2 mapping was learned by ResNet training. After the ResNet was trained, it was applied to reconstruct the T2 mapping from simulation and in vivo human brain data. Results: Though the ResNet was trained entirely on simulated data, the trained network was generalized well to real human brain data. The results from simulation and in vivo human brain experiments show that the proposed method significantly outperformed the echo-detachment-based method. Reliable T2 mapping was achieved within tens of milliseconds after the network had been trained while the echo-detachment-based OLED reconstruction method took minutes. Conclusion: The proposed method will greatly facilitate real-time dynamic and quantitative MR imaging via OLED sequence, and ResNet has the potential to reconstruct images from complex MRI sequence efficiently.



### Brain Abnormality Detection by Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1708.05206v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1708.05206v1)
- **Published**: 2017-08-17 11:24:58+00:00
- **Updated**: 2017-08-17 11:24:58+00:00
- **Authors**: Mina Rezaei, Haojin Yang, Christoph Meinel
- **Comment**: Accepted for presenting in ACM-womENcourage_2016
- **Journal**: None
- **Summary**: In this paper, we describe our method for classification of brain magnetic resonance (MR) images into different abnormalities and healthy classes based on the deep neural network. We propose our method to detect high and low-grade glioma, multiple sclerosis, and Alzheimer diseases as well as healthy cases. Our network architecture has ten learning layers that include seven convolutional layers and three fully connected layers. We have achieved a promising result in five categories of brain images (classification task) with 95.7% accuracy.



### Energy-based Models for Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.05211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05211v1)
- **Published**: 2017-08-17 11:44:34+00:00
- **Updated**: 2017-08-17 11:44:34+00:00
- **Authors**: Hung Vu, Dinh Phung, Tu Dinh Nguyen, Anthony Trevors, Svetha Venkatesh
- **Comment**: None
- **Journal**: None
- **Summary**: Automated detection of abnormalities in data has been studied in research area in recent years because of its diverse applications in practice including video surveillance, industrial damage detection and network intrusion detection. However, building an effective anomaly detection system is a non-trivial task since it requires to tackle challenging issues of the shortage of annotated data, inability of defining anomaly objects explicitly and the expensive cost of feature engineering procedure. Unlike existing appoaches which only partially solve these problems, we develop a unique framework to cope the problems above simultaneously. Instead of hanlding with ambiguous definition of anomaly objects, we propose to work with regular patterns whose unlabeled data is abundant and usually easy to collect in practice. This allows our system to be trained completely in an unsupervised procedure and liberate us from the need for costly data annotation. By learning generative model that capture the normality distribution in data, we can isolate abnormal data points that result in low normality scores (high abnormality scores). Moreover, by leverage on the power of generative networks, i.e. energy-based models, we are also able to learn the feature representation automatically rather than replying on hand-crafted features that have been dominating anomaly detection research over many decades. We demonstrate our proposal on the specific application of video anomaly detection and the experimental results indicate that our method performs better than baselines and are comparable with state-of-the-art methods in many benchmark video anomaly detection datasets.



### Deep Learning for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1708.08987v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08987v1)
- **Published**: 2017-08-17 12:09:12+00:00
- **Updated**: 2017-08-17 12:09:12+00:00
- **Authors**: Mina Rezaei, Haojin Yang, Christoph Meinel
- **Comment**: Presented in doctoral consortium in the AIME-2017 conference
- **Journal**: None
- **Summary**: This report describes my research activities in the Hasso Plattner Institute and summarizes my Ph.D. plan and several novels, end-to-end trainable approaches for analyzing medical images using deep learning algorithm. In this report, as an example, we explore different novel methods based on deep learning for brain abnormality detection, recognition, and segmentation. This report prepared for the doctoral consortium in the AIME-2017 conference.



### Deep Neural Network with l2-norm Unit for Brain Lesions Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.05221v1
- **DOI**: 10.1007/978-3-319-70093-9_85
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05221v1)
- **Published**: 2017-08-17 12:11:45+00:00
- **Updated**: 2017-08-17 12:11:45+00:00
- **Authors**: Mina Rezaei, Haojin Yang, Christoph Meinel
- **Comment**: Accepted for presentation in ICONIP-2017
- **Journal**: Springer2017
- **Summary**: Automated brain lesions detection is an important and very challenging clinical diagnostic task because the lesions have different sizes, shapes, contrasts, and locations. Deep Learning recently has shown promising progress in many application fields, which motivates us to apply this technology for such important problem. In this paper, we propose a novel and end-to-end trainable approach for brain lesions classification and detection by using deep Convolutional Neural Network (CNN). In order to investigate the applicability, we applied our approach on several brain diseases including high and low-grade glioma tumor, ischemic stroke, Alzheimer diseases, by which the brain Magnetic Resonance Images (MRI) have been applied as an input for the analysis. We proposed a new operating unit which receives features from several projections of a subset units of the bottom layer and computes a normalized l2-norm for next layer. We evaluated the proposed approach on two different CNN architectures and number of popular benchmark datasets. The experimental results demonstrate the superior ability of the proposed approach.



### Conditional Adversarial Network for Semantic Segmentation of Brain Tumor
- **Arxiv ID**: http://arxiv.org/abs/1708.05227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05227v1)
- **Published**: 2017-08-17 12:18:54+00:00
- **Updated**: 2017-08-17 12:18:54+00:00
- **Authors**: Mina Rezaei, Konstantin Harmuth, Willi Gierke, Thomas Kellermeier, Martin Fischer, Haojin Yang, Christoph Meinel
- **Comment**: Submitted to BraTS challenges which is part of MICCAI-2017
- **Journal**: None
- **Summary**: Automated medical image analysis has a significant value in diagnosis and treatment of lesions. Brain tumors segmentation has a special importance and difficulty due to the difference in appearances and shapes of the different tumor regions in magnetic resonance images. Additionally, the data sets are heterogeneous and usually limited in size in comparison with the computer vision problems. The recently proposed adversarial training has shown promising results in generative image modeling. In this paper, we propose a novel end-to-end trainable architecture for brain tumor semantic segmentation through conditional adversarial training. We exploit conditional Generative Adversarial Network (cGAN) and train a semantic segmentation Convolution Neural Network (CNN) along with an adversarial network that discriminates segmentation maps coming from the ground truth or from the segmentation network for BraTS 2017 segmentation task[15, 4, 2, 3]. We also propose an end-to-end trainable CNN for survival day prediction based on deep learning techniques for BraTS 2017 prediction task [15, 4, 2, 3]. The experimental results demonstrate the superior ability of the proposed approach for both tasks. The proposed model achieves on validation data a DICE score, Sensitivity and Specificity respectively 0.68, 0.99 and 0.98 for the whole tumor, regarding online judgment system.



### FaceBoxes: A CPU Real-time Face Detector with High Accuracy
- **Arxiv ID**: http://arxiv.org/abs/1708.05234v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05234v4)
- **Published**: 2017-08-17 12:38:32+00:00
- **Updated**: 2018-12-25 05:49:00+00:00
- **Authors**: Shifeng Zhang, Xiangyu Zhu, Zhen Lei, Hailin Shi, Xiaobo Wang, Stan Z. Li
- **Comment**: Accepted by IJCB 2017; Added references; Released codes
- **Journal**: None
- **Summary**: Although tremendous strides have been made in face detection, one of the remaining open challenges is to achieve real-time speed on the CPU as well as maintain high performance, since effective models for face detection tend to be computationally prohibitive. To address this challenge, we propose a novel face detector, named FaceBoxes, with superior performance on both speed and accuracy. Specifically, our method has a lightweight yet powerful network structure that consists of the Rapidly Digested Convolutional Layers (RDCL) and the Multiple Scale Convolutional Layers (MSCL). The RDCL is designed to enable FaceBoxes to achieve real-time speed on the CPU. The MSCL aims at enriching the receptive fields and discretizing anchors over different layers to handle faces of various scales. Besides, we propose a new anchor densification strategy to make different types of anchors have the same density on the image, which significantly improves the recall rate of small faces. As a consequence, the proposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA-resolution images. Moreover, the speed of FaceBoxes is invariant to the number of faces. We comprehensively evaluate this method and present state-of-the-art detection performance on several face detection benchmark datasets, including the AFW, PASCAL face, and FDDB. Code is available at https://github.com/sfzhang15/FaceBoxes



### S$^3$FD: Single Shot Scale-invariant Face Detector
- **Arxiv ID**: http://arxiv.org/abs/1708.05237v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05237v3)
- **Published**: 2017-08-17 12:40:35+00:00
- **Updated**: 2017-11-15 16:22:41+00:00
- **Authors**: Shifeng Zhang, Xiangyu Zhu, Zhen Lei, Hailin Shi, Xiaobo Wang, Stan Z. Li
- **Comment**: Accepted by ICCV 2017 + its supplementary materials; Updated the
  latest results on WIDER FACE
- **Journal**: None
- **Summary**: This paper presents a real-time face detector, named Single Shot Scale-invariant Face Detector (S$^3$FD), which performs superiorly on various scales of faces with a single deep neural network, especially for small faces. Specifically, we try to solve the common problem that anchor-based detectors deteriorate dramatically as the objects become smaller. We make contributions in the following three aspects: 1) proposing a scale-equitable face detection framework to handle different scales of faces well. We tile anchors on a wide range of layers to ensure that all scales of faces have enough features for detection. Besides, we design anchor scales based on the effective receptive field and a proposed equal proportion interval principle; 2) improving the recall rate of small faces by a scale compensation anchor matching strategy; 3) reducing the false positive rate of small faces via a max-out background label. As a consequence, our method achieves state-of-the-art detection performance on all the common face detection benchmarks, including the AFW, PASCAL face, FDDB and WIDER FACE datasets, and can run at 36 FPS on a Nvidia Titan X (Pascal) for VGA-resolution images.



### Deep Learning at 15PF: Supervised and Semi-Supervised Classification for Scientific Data
- **Arxiv ID**: http://arxiv.org/abs/1708.05256v1
- **DOI**: None
- **Categories**: **cs.PF**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.05256v1)
- **Published**: 2017-08-17 13:21:36+00:00
- **Updated**: 2017-08-17 13:21:36+00:00
- **Authors**: Thorsten Kurth, Jian Zhang, Nadathur Satish, Ioannis Mitliagkas, Evan Racah, Mostofa Ali Patwary, Tareq Malas, Narayanan Sundaram, Wahid Bhimji, Mikhail Smorkalov, Jack Deslippe, Mikhail Shiryaev, Srinivas Sridharan, Prabhat, Pradeep Dubey
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: This paper presents the first, 15-PetaFLOP Deep Learning system for solving scientific pattern classification problems on contemporary HPC architectures. We develop supervised convolutional architectures for discriminating signals in high-energy physics data as well as semi-supervised architectures for localizing and classifying extreme weather in climate data. Our Intelcaffe-based implementation obtains $\sim$2TFLOP/s on a single Cori Phase-II Xeon-Phi node. We use a hybrid strategy employing synchronous node-groups, while using asynchronous communication across groups. We use this strategy to scale training of a single model to $\sim$9600 Xeon-Phi nodes; obtaining peak performance of 11.73-15.07 PFLOP/s and sustained performance of 11.41-13.27 PFLOP/s. At scale, our HEP architecture produces state-of-the-art classification accuracy on a dataset with 10M images, exceeding that achieved by selections on high-level physics-motivated features. Our semi-supervised architecture successfully extracts weather patterns in a 15TB climate dataset. Our results demonstrate that Deep Learning can be optimized and scaled effectively on many-core, HPC systems.



### Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects
- **Arxiv ID**: http://arxiv.org/abs/1708.05271v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1708.05271v1)
- **Published**: 2017-08-17 13:51:39+00:00
- **Updated**: 2017-08-17 13:51:39+00:00
- **Authors**: Ting Yao, Yingwei Pan, Yehao Li, Tao Mei
- **Comment**: CVPR17
- **Journal**: None
- **Summary**: Image captioning often requires a large set of training image-sentence pairs. In practice, however, acquiring sufficient training pairs is always expensive, making the recent captioning models limited in their ability to describe objects outside of training corpora (i.e., novel objects). In this paper, we present Long Short-Term Memory with Copying Mechanism (LSTM-C) --- a new architecture that incorporates copying into the Convolutional Neural Networks (CNN) plus Recurrent Neural Networks (RNN) image captioning framework, for describing novel objects in captions. Specifically, freely available object recognition datasets are leveraged to develop classifiers for novel objects. Our LSTM-C then nicely integrates the standard word-by-word sentence generation by a decoder RNN with copying mechanism which may instead select words from novel objects at proper places in the output sentence. Extensive experiments are conducted on both MSCOCO image captioning and ImageNet datasets, demonstrating the ability of our proposed LSTM-C architecture to describe novel objects. Furthermore, superior results are reported when compared to state-of-the-art deep models.



### Robust Registration and Geometry Estimation from Unstructured Facial Scans
- **Arxiv ID**: http://arxiv.org/abs/1708.05340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05340v1)
- **Published**: 2017-08-17 15:50:27+00:00
- **Updated**: 2017-08-17 15:50:27+00:00
- **Authors**: Maxim Bazik, Daniel Crispell
- **Comment**: None
- **Journal**: None
- **Summary**: Commercial off the shelf (COTS) 3D scanners are capable of generating point clouds covering visible portions of a face with sub-millimeter accuracy at close range, but lack the coverage and specialized anatomic registration provided by more expensive 3D facial scanners. We demonstrate an effective pipeline for joint alignment of multiple unstructured 3D point clouds and registration to a parameterized 3D model which represents shape variation of the human head. Most algorithms separate the problems of pose estimation and mesh warping, however we propose a new iterative method where these steps are interwoven. Error decreases with each iteration, showing the proposed approach is effective in improving geometry and alignment. The approach described is used to align the NDOff-2007 dataset, which contains 7,358 individual scans at various poses of 396 subjects. The dataset has a number of full profile scans which are correctly aligned and contribute directly to the associated mesh geometry. The dataset in its raw form contains a significant number of mislabeled scans, which are identified and corrected based on alignment error using the proposed algorithm. The average point to surface distance between the aligned scans and the produced geometries is one half millimeter.



### PixelNN: Example-based Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1708.05349v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.05349v1)
- **Published**: 2017-08-17 16:13:42+00:00
- **Updated**: 2017-08-17 16:13:42+00:00
- **Authors**: Aayush Bansal, Yaser Sheikh, Deva Ramanan
- **Comment**: Project Page: http://www.cs.cmu.edu/~aayushb/pixelNN/
- **Journal**: None
- **Summary**: We present a simple nearest-neighbor (NN) approach that synthesizes high-frequency photorealistic images from an "incomplete" signal such as a low-resolution image, a surface normal map, or edges. Current state-of-the-art deep generative models designed for such conditional image synthesis lack two important things: (1) they are unable to generate a large set of diverse outputs, due to the mode collapse problem. (2) they are not interpretable, making it difficult to control the synthesized output. We demonstrate that NN approaches potentially address such limitations, but suffer in accuracy on small datasets. We design a simple pipeline that combines the best of both worlds: the first stage uses a convolutional neural network (CNN) to maps the input to a (overly-smoothed) image, and the second stage uses a pixel-wise nearest neighbor method to map the smoothed output to multiple high-quality, high-frequency outputs in a controllable manner. We demonstrate our approach for various input modalities, and for various domains ranging from human faces to cats-and-dogs to shoes and handbags.



### MirrorFlow: Exploiting Symmetries in Joint Optical Flow and Occlusion Estimation
- **Arxiv ID**: http://arxiv.org/abs/1708.05355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05355v1)
- **Published**: 2017-08-17 16:32:34+00:00
- **Updated**: 2017-08-17 16:32:34+00:00
- **Authors**: Junhwa Hur, Stefan Roth
- **Comment**: 14 pages, To appear in ICCV 2017
- **Journal**: None
- **Summary**: Optical flow estimation is one of the most studied problems in computer vision, yet recent benchmark datasets continue to reveal problem areas of today's approaches. Occlusions have remained one of the key challenges. In this paper, we propose a symmetric optical flow method to address the well-known chicken-and-egg relation between optical flow and occlusions. In contrast to many state-of-the-art methods that consider occlusions as outliers, possibly filtered out during post-processing, we highlight the importance of joint occlusion reasoning in the optimization and show how to utilize occlusion as an important cue for estimating optical flow. The key feature of our model is to fully exploit the symmetry properties that characterize optical flow and occlusions in the two consecutive images. Specifically through utilizing forward-backward consistency and occlusion-disocclusion symmetry in the energy, our model jointly estimates optical flow in both forward and backward direction, as well as consistent occlusion maps in both views. We demonstrate significant performance benefits on standard benchmarks, especially from the occlusion-disocclusion symmetry. On the challenging KITTI dataset we report the most accurate two-frame results to date.



### Learning a Multi-View Stereo Machine
- **Arxiv ID**: http://arxiv.org/abs/1708.05375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05375v1)
- **Published**: 2017-08-17 17:36:40+00:00
- **Updated**: 2017-08-17 17:36:40+00:00
- **Authors**: Abhishek Kar, Christian Häne, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches as well as recent learning based methods.



### Deformable Modeling for Human Body Acquired from Depth Sensors
- **Arxiv ID**: http://arxiv.org/abs/1708.05401v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05401v2)
- **Published**: 2017-08-17 18:15:52+00:00
- **Updated**: 2017-08-30 13:32:14+00:00
- **Authors**: Vamshhi Pavan Kumar Varma Vegeshna
- **Comment**: arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources
- **Journal**: None
- **Summary**: This paper presents a novel approach to reconstruct complete 3D deformable models over time by a single depth camera. These are the steps employed for deforming objects from single depth camera. The partial surfaces reconstructed from various times of capture are assembled together to form a complete 3D surface. A mesh warping algorithm is used to align different partial surfaces based on linear mesh deformation. A volumetric method is then applied to combine partial surfaces, fix missing holes and smooth alignment errors.



### Computer-aided position planning of miniplates to treat facial bone defects
- **Arxiv ID**: http://arxiv.org/abs/1708.05711v1
- **DOI**: 10.1371/journal.pone.0182839
- **Categories**: **cs.CV**, cs.CE, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1708.05711v1)
- **Published**: 2017-08-17 18:37:02+00:00
- **Updated**: 2017-08-17 18:37:02+00:00
- **Authors**: Jan Egger, Jürgen Wallner, Markus Gall, Xiaojun Chen, Katja Schwenzer-Zimmerer, Knut Reinbacher, Dieter Schmalstieg
- **Comment**: 19 pages, 13 Figures, 2 Tables
- **Journal**: PLoS ONE 12(8): e0182839 (2017)
- **Summary**: In this contribution, a software system for computer-aided position planning of miniplates to treat facial bone defects is proposed. The intra-operatively used bone plates have to be passively adapted on the underlying bone contours for adequate bone fragment stabilization. However, this procedure can lead to frequent intra-operatively performed material readjustments especially in complex surgical cases. Our approach is able to fit a selection of common implant models on the surgeon's desired position in a 3D computer model. This happens with respect to the surrounding anatomical structures, always including the possibility of adjusting both the direction and the position of the used osteosynthesis material. By using the proposed software, surgeons are able to pre-plan the out coming implant in its form and morphology with the aid of a computer-visualized model within a few minutes. Further, the resulting model can be stored in STL file format, the commonly used format for 3D printing. Using this technology, surgeons are able to print the virtual generated implant, or create an individually designed bending tool. This method leads to adapted osteosynthesis materials according to the surrounding anatomy and requires further a minimum amount of money and time.



### Simultaneous Detection and Quantification of Retinal Fluid with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.05464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05464v1)
- **Published**: 2017-08-17 23:31:05+00:00
- **Updated**: 2017-08-17 23:31:05+00:00
- **Authors**: Dustin Morley, Hassan Foroosh, Saad Shaikh, Ulas Bagci
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new deep learning approach for automatic detection and segmentation of fluid within retinal OCT images. The proposed framework utilizes both ResNet and Encoder-Decoder neural network architectures. When training the network, we apply a novel data augmentation method called myopic warping together with standard rotation-based augmentation to increase the training set size to 45 times the original amount. Finally, the network output is post-processed with an energy minimization algorithm (graph cut) along with a few other knowledge guided morphological operations to finalize the segmentation process. Based on OCT imaging data and its ground truth from the RETOUCH challenge, the proposed system achieves dice indices of 0.522, 0.682, and 0.612, and average absolute volume differences of 0.285, 0.115, and 0.156 mm$^3$ for intaretinal fluid, subretinal fluid, and pigment epithelial detachment respectively.



### Eigen Evolution Pooling for Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.05465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05465v1)
- **Published**: 2017-08-17 23:34:45+00:00
- **Updated**: 2017-08-17 23:34:45+00:00
- **Authors**: Yang Wang, Vinh Tran, Minh Hoai
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Eigen Evolution Pooling, an efficient method to aggregate a sequence of feature vectors. Eigen evolution pooling is designed to produce compact feature representations for a sequence of feature vectors, while maximally preserving as much information about the sequence as possible, especially the temporal evolution of the features over time. Eigen evolution pooling is a general pooling method that can be applied to any sequence of feature vectors, from low-level RGB values to high-level Convolutional Neural Network (CNN) feature vectors. We show that eigen evolution pooling is more effective than average, max, and rank pooling for encoding the dynamics of human actions in video. We demonstrate the power of eigen evolution pooling on UCF101 and Hollywood2 datasets, two human action recognition benchmarks, and achieve state-of-the-art performance.



