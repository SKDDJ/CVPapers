# Arxiv Papers in cs.CV on 2017-08-09
### Statistics of Deep Generated Images
- **Arxiv ID**: http://arxiv.org/abs/1708.02688v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02688v5)
- **Published**: 2017-08-09 01:12:20+00:00
- **Updated**: 2019-11-24 02:59:10+00:00
- **Authors**: Yu Zeng, Huchuan Lu, Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: Here, we explore the low-level statistics of images generated by state-of-the-art deep generative models. First, Variational auto-encoder (VAE~\cite{kingma2013auto}), Wasserstein generative adversarial network (WGAN~\cite{arjovsky2017wasserstein}) and deep convolutional generative adversarial network (DCGAN~\cite{radford2015unsupervised}) are trained on the ImageNet dataset and a large set of cartoon frames from animations. Then, for images generated by these models as well as natural scenes and cartoons, statistics including mean power spectrum, the number of connected components in a given image area, distribution of random filter responses, and contrast distribution are computed. Our analyses on training images support current findings on scale invariance, non-Gaussianity, and Weibull contrast distribution of natural scenes. We find that although similar results hold over cartoon images, there is still a significant difference between statistics of natural scenes and images generated by VAE, DCGAN and WGAN models. In particular, generated images do not have scale invariant mean power spectrum magnitude, which indicates existence of extra structures in these images. Inspecting how well the statistics of deep generated images match the known statistical properties of natural images, such as scale invariance, non-Gaussianity, and Weibull contrast distribution, can a) reveal the degree to which deep learning models capture the essence of the natural scenes, b) provide a new dimension to evaluate models, and c) allow possible improvement of image generative models (e.g., via defining new loss functions).



### Human Skin Detection Using RGB, HSV and YCbCr Color Models
- **Arxiv ID**: http://arxiv.org/abs/1708.02694v1
- **DOI**: 10.2991/iccasp-16.2017.51
- **Categories**: **cs.CV**, q-bio.OT
- **Links**: [PDF](http://arxiv.org/pdf/1708.02694v1)
- **Published**: 2017-08-09 02:08:29+00:00
- **Updated**: 2017-08-09 02:08:29+00:00
- **Authors**: S. Kolkur, D. Kalbande, P. Shimpi, C. Bapat, J. Jatakia
- **Comment**: ICCASP/ICMMD-2016. Published by Atlantic Press. Part of series: AISR
  ISBN: 978-94-6252-305-0 ISSN: 1951-6851
- **Journal**: None
- **Summary**: Human Skin detection deals with the recognition of skin-colored pixels and regions in a given image. Skin color is often used in human skin detection because it is invariant to orientation and size and is fast to process. A new human skin detection algorithm is proposed in this paper. The three main parameters for recognizing a skin pixel are RGB (Red, Green, Blue), HSV (Hue, Saturation, Value) and YCbCr (Luminance, Chrominance) color models. The objective of proposed algorithm is to improve the recognition of skin pixels in given images. The algorithm not only considers individual ranges of the three color parameters but also takes into ac- count combinational ranges which provide greater accuracy in recognizing the skin area in a given image.



### What Actions are Needed for Understanding Human Actions in Videos?
- **Arxiv ID**: http://arxiv.org/abs/1708.02696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02696v1)
- **Published**: 2017-08-09 02:25:56+00:00
- **Updated**: 2017-08-09 02:25:56+00:00
- **Authors**: Gunnar A. Sigurdsson, Olga Russakovsky, Abhinav Gupta
- **Comment**: ICCV2017
- **Journal**: None
- **Summary**: What is the right way to reason about human activities? What directions forward are most promising? In this work, we analyze the current state of human activity understanding in videos. The goal of this paper is to examine datasets, evaluation metrics, algorithms, and potential future directions. We look at the qualitative attributes that define activities such as pose variability, brevity, and density. The experiments consider multiple state-of-the-art algorithms and multiple datasets. The results demonstrate that while there is inherent ambiguity in the temporal extent of activities, current datasets still permit effective benchmarking. We discover that fine-grained understanding of objects and pose when combined with temporal reasoning is likely to yield substantial improvements in algorithmic accuracy. We present the many kinds of information that will be needed to achieve substantial gains in activity understanding: objects, verbs, intent, and sequential reasoning. The software and additional information will be made available to provide other researchers detailed diagnostics to understand their own algorithms.



### Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge
- **Arxiv ID**: http://arxiv.org/abs/1708.02711v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1708.02711v1)
- **Published**: 2017-08-09 04:19:42+00:00
- **Updated**: 2017-08-09 04:19:42+00:00
- **Authors**: Damien Teney, Peter Anderson, Xiaodong He, Anton van den Hengel
- **Comment**: Winner of the 2017 Visual Question Answering (VQA) Challenge at CVPR
- **Journal**: None
- **Summary**: This paper presents a state-of-the-art model for visual question answering (VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of significant importance for research in artificial intelligence, given its multimodal nature, clear evaluation protocol, and potential real-world applications. The performance of deep neural networks for VQA is very dependent on choices of architectures and hyperparameters. To help further research in the area, we describe in detail our high-performing, though relatively simple model. Through a massive exploration of architectures and hyperparameters representing more than 3,000 GPU-hours, we identified tips and tricks that lead to its success, namely: sigmoid outputs, soft training targets, image features from bottom-up attention, gated tanh activations, output embeddings initialized using GloVe and Google Images, large mini-batches, and smart shuffling of training data. We provide a detailed analysis of their impact on performance to assist others in making an appropriate selection.



### Sequential Dual Deep Learning with Shape and Texture Features for Sketch Recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.02716v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T10, I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1708.02716v1)
- **Published**: 2017-08-09 04:42:25+00:00
- **Updated**: 2017-08-09 04:42:25+00:00
- **Authors**: Qi Jia, Meiyu Yu, Xin Fan, Haojie Li
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Recognizing freehand sketches with high arbitrariness is greatly challenging. Most existing methods either ignore the geometric characteristics or treat sketches as handwritten characters with fixed structural ordering. Consequently, they can hardly yield high recognition performance even though sophisticated learning techniques are employed. In this paper, we propose a sequential deep learning strategy that combines both shape and texture features. A coded shape descriptor is exploited to characterize the geometry of sketch strokes with high flexibility, while the outputs of constitutional neural networks (CNN) are taken as the abstract texture feature. We develop dual deep networks with memorable gated recurrent units (GRUs), and sequentially feed these two types of features into the dual networks, respectively. These dual networks enable the feature fusion by another gated recurrent unit (GRU), and thus accurately recognize sketches invariant to stroke ordering. The experiments on the TU-Berlin data set show that our method outperforms the average of human and state-of-the-art algorithms even when significant shape and appearance variations occur.



### Deep Face Feature for Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1708.02721v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02721v2)
- **Published**: 2017-08-09 05:39:36+00:00
- **Updated**: 2018-03-12 12:30:36+00:00
- **Authors**: Boyi Jiang, Juyong Zhang, Bailin Deng, Yudong Guo, Ligang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a deep learning based image feature extraction method designed specifically for face images. To train the feature extraction model, we construct a large scale photo-realistic face image dataset with ground-truth correspondence between multi-view face images, which are synthesized from real photographs via an inverse rendering procedure. The deep face feature (DFF) is trained using correspondence between face images rendered from different views. Using the trained DFF model, we can extract a feature vector for each pixel of a face image, which distinguishes different facial regions and is shown to be more effective than general-purpose feature descriptors for face-related tasks such as matching and alignment. Based on the DFF, we develop a robust face alignment method, which iteratively updates landmarks, pose and 3D shape. Extensive experiments demonstrate that our method can achieve state-of-the-art results for face alignment under highly unconstrained face images.



### Weakly- and Self-Supervised Learning for Content-Aware Deep Image Retargeting
- **Arxiv ID**: http://arxiv.org/abs/1708.02731v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1708.02731v1)
- **Published**: 2017-08-09 06:43:51+00:00
- **Updated**: 2017-08-09 06:43:51+00:00
- **Authors**: Donghyeon Cho, Jinsun Park, Tae-Hyun Oh, Yu-Wing Tai, In So Kweon
- **Comment**: 10 pages, 11 figures. To appear in ICCV 2017, Spotlight Presentation
- **Journal**: None
- **Summary**: This paper proposes a weakly- and self-supervised deep convolutional neural network (WSSDCNN) for content-aware image retargeting. Our network takes a source image and a target aspect ratio, and then directly outputs a retargeted image. Retargeting is performed through a shift map, which is a pixel-wise mapping from the source to the target grid. Our method implicitly learns an attention map, which leads to a content-aware shift map for image retargeting. As a result, discriminative parts in an image are preserved, while background regions are adjusted seamlessly. In the training phase, pairs of an image and its image-level annotation are used to compute content and structure losses. We demonstrate the effectiveness of our proposed method for a retargeting application with insightful analyses.



### Probabilistic Neural Network with Complex Exponential Activation Functions in Image Recognition using Deep Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/1708.02733v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1708.02733v1)
- **Published**: 2017-08-09 06:50:32+00:00
- **Updated**: 2017-08-09 06:50:32+00:00
- **Authors**: Andrey Savchenko
- **Comment**: 14 pages, 13 figures, 5 tables, 69 references
- **Journal**: None
- **Summary**: If the training dataset is not very large, image recognition is usually implemented with the transfer learning methods. In these methods the features are extracted using a deep convolutional neural network, which was preliminarily trained with an external very-large dataset. In this paper we consider the nonparametric classification of extracted feature vectors with the probabilistic neural network (PNN). The number of neurons at the pattern layer of the PNN is equal to the database size, which causes the low recognition performance and high memory space complexity of this network. We propose to overcome these drawbacks by replacing the exponential activation function in the Gaussian Parzen kernel to the complex exponential functions in the Fej\'er kernel. We demonstrate that in this case it is possible to implement the network with the number of neurons in the pattern layer proportional to the cubic root of the database size. Thus, the proposed modification of the PNN makes it possible to significantly decrease runtime and memory complexities without loosing its main advantages, namely, extremely fast training procedure and the convergence to the optimal Bayesian decision. An experimental study in visual object category classification and unconstrained face recognition with contemporary deep neural networks have shown, that our approach obtains very efficient and rather accurate decisions for the small training sample in comparison with the well-known classifiers.



### Joint Face Alignment and 3D Face Reconstruction with Application to Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.02734v2
- **DOI**: 10.1109/TPAMI.2018.2885995
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02734v2)
- **Published**: 2017-08-09 06:52:26+00:00
- **Updated**: 2018-12-22 02:32:28+00:00
- **Authors**: Feng Liu, Qijun Zhao, Xiaoming Liu, Dan Zeng
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence, Nov.
  2018
- **Journal**: None
- **Summary**: Face alignment and 3D face reconstruction are traditionally accomplished as separated tasks. By exploring the strong correlation between 2D landmarks and 3D shapes, in contrast, we propose a joint face alignment and 3D face reconstruction method to simultaneously solve these two problems for 2D face images of arbitrary poses and expressions. This method, based on a summation model of 3D faces and cascaded regression in 2D and 3D shape spaces, iteratively and alternately applies two cascaded regressors, one for updating 2D landmarks and the other for 3D shape. The 3D shape and the landmarks are correlated via a 3D-to-2D mapping matrix, which is updated in each iteration to refine the location and visibility of 2D landmarks. Unlike existing methods, the proposed method can fully automatically generate both pose-and-expression-normalized (PEN) and expressive 3D faces and localize both visible and invisible 2D landmarks. Based on the PEN 3D faces, we devise a method to enhance face recognition accuracy across poses and expressions. Both linear and nonlinear implementations of the proposed method are presented and evaluated in this paper. Extensive experiments show that the proposed method can achieve the state-of-the-art accuracy in both face alignment and 3D face reconstruction, and benefit face recognition owing to its reconstructed PEN 3D face.



### Gaussian Prototypical Networks for Few-Shot Learning on Omniglot
- **Arxiv ID**: http://arxiv.org/abs/1708.02735v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.02735v1)
- **Published**: 2017-08-09 06:53:31+00:00
- **Updated**: 2017-08-09 06:53:31+00:00
- **Authors**: Stanislav Fort
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel architecture for $k$-shot classification on the Omniglot dataset. Building on prototypical networks, we extend their architecture to what we call Gaussian prototypical networks. Prototypical networks learn a map between images and embedding vectors, and use their clustering for classification. In our model, a part of the encoder output is interpreted as a confidence region estimate about the embedding point, and expressed as a Gaussian covariance matrix. Our network then constructs a direction and class dependent distance metric on the embedding space, using uncertainties of individual data points as weights. We show that Gaussian prototypical networks are a preferred architecture over vanilla prototypical networks with an equivalent number of parameters. We report state-of-the-art performance in 1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot 5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset. We explore artificially down-sampling a fraction of images in the training set, which improves our performance even further. We therefore hypothesize that Gaussian prototypical networks might perform better in less homogeneous, noisier datasets, which are commonplace in real world applications.



### An automatic water detection approach based on Dempster-Shafer theory for multi spectral images
- **Arxiv ID**: http://arxiv.org/abs/1708.02747v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.02747v2)
- **Published**: 2017-08-09 07:59:39+00:00
- **Updated**: 2017-09-04 08:04:27+00:00
- **Authors**: Na Li, Arnaud Martin, Rémi Estival
- **Comment**: 20th International Conference on Information Fusion, Jul 2017, XI'AN,
  China
- **Journal**: None
- **Summary**: Detection of surface water in natural environment via multi-spectral imagery has been widely utilized in many fields, such land cover identification. However, due to the similarity of the spectra of water bodies, built-up areas, approaches based on high-resolution satellites sometimes confuse these features. A popular direction to detect water is spectral index, often requiring the ground truth to find appropriate thresholds manually. As for traditional machine learning methods, they identify water merely via differences of spectra of various land covers, without taking specific properties of spectral reflection into account. In this paper, we propose an automatic approach to detect water bodies based on Dempster-Shafer theory, combining supervised learning with specific property of water in spectral band in a fully unsupervised context. The benefits of our approach are twofold. On the one hand, it performs well in mapping principle water bodies, including little streams and branches. On the other hand, it labels all objects usually confused with water as `ignorance', including half-dry watery areas, built-up areas and semi-transparent clouds and shadows. `Ignorance' indicates not only limitations of the spectral properties of water and supervised learning itself but insufficiency of information from multi-spectral bands as well, providing valuable information for further land cover classification.



### Extreme clicking for efficient object annotation
- **Arxiv ID**: http://arxiv.org/abs/1708.02750v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02750v1)
- **Published**: 2017-08-09 08:05:53+00:00
- **Updated**: 2017-08-09 08:05:53+00:00
- **Authors**: Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, Vittorio Ferrari
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: Manually annotating object bounding boxes is central to building computer vision datasets, and it is very time consuming (annotating ILSVRC [53] took 35s for one high-quality box [62]). It involves clicking on imaginary corners of a tight box around the object. This is difficult as these corners are often outside the actual object and several adjustments are required to obtain a tight box. We propose extreme clicking instead: we ask the annotator to click on four physical points on the object: the top, bottom, left- and right-most points. This task is more natural and these points are easy to find. We crowd-source extreme point annotations for PASCAL VOC 2007 and 2012 and show that (1) annotation time is only 7s per box, 5x faster than the traditional way of drawing boxes [62]; (2) the quality of the boxes is as good as the original ground-truth drawn the traditional way; (3) detectors trained on our annotations are as accurate as those trained on the original ground-truth. Moreover, our extreme clicking strategy not only yields box coordinates, but also four accurate boundary points. We show (4) how to incorporate them into GrabCut to obtain more accurate segmentations than those delivered when initializing it from bounding boxes; (5) semantic segmentations models trained on these segmentations outperform those trained on segmentations derived from bounding boxes.



### Isointense infant brain MRI segmentation with a dilated convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1708.02757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02757v1)
- **Published**: 2017-08-09 08:49:12+00:00
- **Updated**: 2017-08-09 08:49:12+00:00
- **Authors**: Pim Moeskops, Josien P. W. Pluim
- **Comment**: MICCAI grand challenge on 6-month infant brain MRI segmentation
- **Journal**: None
- **Summary**: Quantitative analysis of brain MRI at the age of 6 months is difficult because of the limited contrast between white matter and gray matter. In this study, we use a dilated triplanar convolutional neural network in combination with a non-dilated 3D convolutional neural network for the segmentation of white matter, gray matter and cerebrospinal fluid in infant brain MR images, as provided by the MICCAI grand challenge on 6-month infant brain MRI segmentation.



### Learning to Disambiguate by Asking Discriminative Questions
- **Arxiv ID**: http://arxiv.org/abs/1708.02760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02760v1)
- **Published**: 2017-08-09 08:52:25+00:00
- **Updated**: 2017-08-09 08:52:25+00:00
- **Authors**: Yining Li, Chen Huang, Xiaoou Tang, Chen-Change Loy
- **Comment**: 14 pages, 12 figures, ICCV2017
- **Journal**: None
- **Summary**: The ability to ask questions is a powerful tool to gather information in order to learn about the world and resolve ambiguities. In this paper, we explore a novel problem of generating discriminative questions to help disambiguate visual instances. Our work can be seen as a complement and new extension to the rich research studies on image captioning and question answering. We introduce the first large-scale dataset with over 10,000 carefully annotated images-question tuples to facilitate benchmarking. In particular, each tuple consists of a pair of images and 4.6 discriminative questions (as positive samples) and 5.9 non-discriminative questions (as negative samples) on average. In addition, we present an effective method for visual discriminative question generation. The method can be trained in a weakly supervised manner without discriminative images-question tuples but just existing visual question answering datasets. Promising results are shown against representative baselines through quantitative evaluations and user studies.



### Multi-dimensional Gated Recurrent Units for Automated Anatomical Landmark Localization
- **Arxiv ID**: http://arxiv.org/abs/1708.02766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02766v1)
- **Published**: 2017-08-09 09:10:02+00:00
- **Updated**: 2017-08-09 09:10:02+00:00
- **Authors**: Simon Andermatt, Simon Pezold, Michael Amann, Philippe C. Cattin
- **Comment**: 8 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: We present an automated method for localizing an anatomical landmark in three-dimensional medical images. The method combines two recurrent neural networks in a coarse-to-fine approach: The first network determines a candidate neighborhood by analyzing the complete given image volume. The second network localizes the actual landmark precisely and accurately in the candidate neighborhood. Both networks take advantage of multi-dimensional gated recurrent units in their main layers, which allow for high model complexity with a comparatively small set of parameters. We localize the medullopontine sulcus in 3D magnetic resonance images of the head and neck. We show that the proposed approach outperforms similar localization techniques both in terms of mean distance in millimeters and voxels w.r.t. manual labelings of the data. With a mean localization error of 1.7 mm, the proposed approach performs on par with neurological experts, as we demonstrate in an interrater comparison.



### DeepFaceLIFT: Interpretable Personalized Models for Automatic Estimation of Self-Reported Pain
- **Arxiv ID**: http://arxiv.org/abs/1708.04670v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.04670v1)
- **Published**: 2017-08-09 12:07:45+00:00
- **Updated**: 2017-08-09 12:07:45+00:00
- **Authors**: Dianbo Liu, Fengjiao Peng, Andrew Shea, Ognjen, Rudovic, Rosalind Picard
- **Comment**: None
- **Journal**: None
- **Summary**: Previous research on automatic pain estimation from facial expressions has focused primarily on "one-size-fits-all" metrics (such as PSPI). In this work, we focus on directly estimating each individual's self-reported visual-analog scale (VAS) pain metric, as this is considered the gold standard for pain measurement. The VAS pain score is highly subjective and context-dependent, and its range can vary significantly among different persons. To tackle these issues, we propose a novel two-stage personalized model, named DeepFaceLIFT, for automatic estimation of VAS. This model is based on (1) Neural Network and (2) Gaussian process regression models, and is used to personalize the estimation of self-reported pain via a set of hand-crafted personal features and multi-task learning. We show on the benchmark dataset for pain analysis (The UNBC-McMaster Shoulder Pain Expression Archive) that the proposed personalized model largely outperforms the traditional, unpersonalized models: the intra-class correlation improves from a baseline performance of 19\% to a personalized performance of 35\% while also providing confidence in the model\textquotesingle s estimates -- in contrast to existing models for the target task. Additionally, DeepFaceLIFT automatically discovers the pain-relevant facial regions for each person, allowing for an easy interpretation of the pain-related facial cues.



### BlitzNet: A Real-Time Deep Network for Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1708.02813v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02813v1)
- **Published**: 2017-08-09 12:36:17+00:00
- **Updated**: 2017-08-09 12:36:17+00:00
- **Authors**: Nikita Dvornik, Konstantin Shmelkov, Julien Mairal, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time scene understanding has become crucial in many applications such as autonomous driving. In this paper, we propose a deep architecture, called BlitzNet, that jointly performs object detection and semantic segmentation in one forward pass, allowing real-time computations. Besides the computational gain of having a single network to perform several tasks, we show that object detection and semantic segmentation benefit from each other in terms of accuracy. Experimental results for VOC and COCO datasets show state-of-the-art performance for object detection and segmentation among real time systems.



### Importance of Image Enhancement Techniques in Color Image Segmentation: A Comprehensive and Comparative Study
- **Arxiv ID**: http://arxiv.org/abs/1708.05081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.05081v1)
- **Published**: 2017-08-09 13:35:20+00:00
- **Updated**: 2017-08-09 13:35:20+00:00
- **Authors**: Dibya Jyoti Bora
- **Comment**: 27 pages, 17 figures, 2 Tables, 1 flowchart
- **Journal**: Indian J.Sci.Res. 15 (1): 115-131, 2017
- **Summary**: Color image segmentation is a very emerging research topic in the area of color image analysis and pattern recognition. Many state-of-the-art algorithms have been developed for this purpose. But, often the segmentation results of these algorithms seem to be suffering from miss-classifications and over-segmentation. The reasons behind these are the degradation of image quality during the acquisition, transmission and color space conversion. So, here arises the need of an efficient image enhancement technique which can remove the redundant pixels or noises from the color image before proceeding for final segmentation. In this paper, an effort has been made to study and analyze different image enhancement techniques and thereby finding out the better one for color image segmentation. Also, this comparative study is done on two well-known color spaces HSV and LAB separately to find out which color space supports segmentation task more efficiently with respect to those enhancement techniques.



### Anveshak - A Groundtruth Generation Tool for Foreground Regions of Document Images
- **Arxiv ID**: http://arxiv.org/abs/1708.02831v1
- **DOI**: 10.1007/978-3-319-68124-5_22
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02831v1)
- **Published**: 2017-08-09 13:41:18+00:00
- **Updated**: 2017-08-09 13:41:18+00:00
- **Authors**: Soumyadeep Dey, Jayanta Mukherjee, Shamik Sural, Amit Vijay Nandedkar
- **Comment**: Accepted in DAR 2016
- **Journal**: None
- **Summary**: We propose a graphical user interface based groundtruth generation tool in this paper. Here, annotation of an input document image is done based on the foreground pixels. Foreground pixels are grouped together with user interaction to form labeling units. These units are then labeled by the user with the user defined labels. The output produced by the tool is an image with an XML file containing its metadata information. This annotated data can be further used in different applications of document image analysis.



### SPLODE: Semi-Probabilistic Point and Line Odometry with Depth Estimation from RGB-D Camera Motion
- **Arxiv ID**: http://arxiv.org/abs/1708.02837v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.02837v1)
- **Published**: 2017-08-09 13:50:30+00:00
- **Updated**: 2017-08-09 13:50:30+00:00
- **Authors**: Pedro F. Proença, Yang Gao
- **Comment**: IROS 2017
- **Journal**: None
- **Summary**: Active depth cameras suffer from several limitations, which cause incomplete and noisy depth maps, and may consequently affect the performance of RGB-D Odometry. To address this issue, this paper presents a visual odometry method based on point and line features that leverages both measurements from a depth sensor and depth estimates from camera motion. Depth estimates are generated continuously by a probabilistic depth estimation framework for both types of features to compensate for the lack of depth measurements and inaccurate feature depth associations. The framework models explicitly the uncertainty of triangulating depth from both point and line observations to validate and obtain precise estimates. Furthermore, depth measurements are exploited by propagating them through a depth map registration module and using a frame-to-frame motion estimation method that considers 3D-to-2D and 2D-to-3D reprojection errors, independently. Results on RGB-D sequences captured on large indoor and outdoor scenes, where depth sensor limitations are critical, show that the combination of depth measurements and estimates through our approach is able to overcome the absence and inaccuracy of depth measurements.



### Online Multi-Object Tracking Using CNN-based Single Object Tracker with Spatial-Temporal Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/1708.02843v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02843v2)
- **Published**: 2017-08-09 13:56:13+00:00
- **Updated**: 2017-08-14 03:30:17+00:00
- **Authors**: Qi Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, Bin Liu, Nenghai Yu
- **Comment**: Accepted at International Conference on Computer Vision (ICCV) 2017
- **Journal**: None
- **Summary**: In this paper, we propose a CNN-based framework for online MOT. This framework utilizes the merits of single object trackers in adapting appearance models and searching for target in the next frame. Simply applying single object tracker for MOT will encounter the problem in computational efficiency and drifted results caused by occlusion. Our framework achieves computational efficiency by sharing features and using ROI-Pooling to obtain individual features for each target. Some online learned target-specific CNN layers are used for adapting the appearance model for each target. In the framework, we introduce spatial-temporal attention mechanism (STAM) to handle the drift caused by occlusion and interaction among targets. The visibility map of the target is learned and used for inferring the spatial attention map. The spatial attention map is then applied to weight the features. Besides, the occlusion status can be estimated from the visibility map, which controls the online updating process via weighted loss on training samples with different occlusion statuses in different frames. It can be considered as temporal attention mechanism. The proposed algorithm achieves 34.3% and 46.0% in MOTA on challenging MOT15 and MOT16 benchmark dataset respectively.



### WebVision Database: Visual Learning and Understanding from Web Data
- **Arxiv ID**: http://arxiv.org/abs/1708.02862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02862v1)
- **Published**: 2017-08-09 14:59:30+00:00
- **Updated**: 2017-08-09 14:59:30+00:00
- **Authors**: Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a study on learning visual recognition models from large scale noisy web data. We build a new database called WebVision, which contains more than $2.4$ million web images crawled from the Internet by using queries generated from the 1,000 semantic concepts of the benchmark ILSVRC 2012 dataset. Meta information along with those web images (e.g., title, description, tags, etc.) are also crawled. A validation set and test set containing human annotated images are also provided to facilitate algorithmic development. Based on our new database, we obtain a few interesting observations: 1) the noisy web images are sufficient for training a good deep CNN model for visual recognition; 2) the model learnt from our WebVision database exhibits comparable or even better generalization ability than the one trained from the ILSVRC 2012 dataset when being transferred to new datasets and tasks; 3) a domain adaptation issue (a.k.a., dataset bias) is observed, which means the dataset can be used as the largest benchmark dataset for visual domain adaptation. Our new WebVision database and relevant studies in this work would benefit the advance of learning state-of-the-art visual models with minimum supervision based on web data.



### CoupleNet: Coupling Global Structure with Local Parts for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.02863v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02863v1)
- **Published**: 2017-08-09 15:07:23+00:00
- **Updated**: 2017-08-09 15:07:23+00:00
- **Authors**: Yousong Zhu, Chaoyang Zhao, Jinqiao Wang, Xu Zhao, Yi Wu, Hanqing Lu
- **Comment**: Accepted by ICCV 2017
- **Journal**: None
- **Summary**: The region-based Convolutional Neural Network (CNN) detectors such as Faster R-CNN or R-FCN have already shown promising results for object detection by combining the region proposal subnetwork and the classification subnetwork together. Although R-FCN has achieved higher detection speed while keeping the detection performance, the global structure information is ignored by the position-sensitive score maps. To fully explore the local and global properties, in this paper, we propose a novel fully convolutional network, named as CoupleNet, to couple the global structure with local parts for object detection. Specifically, the object proposals obtained by the Region Proposal Network (RPN) are fed into the the coupling module which consists of two branches. One branch adopts the position-sensitive RoI (PSRoI) pooling to capture the local part information of the object, while the other employs the RoI pooling to encode the global and context information. Next, we design different coupling strategies and normalization ways to make full use of the complementary advantages between the global and local branches. Extensive experiments demonstrate the effectiveness of our approach. We achieve state-of-the-art results on all three challenging datasets, i.e. a mAP of 82.7% on VOC07, 80.4% on VOC12, and 34.4% on COCO. Codes will be made publicly available.



### Privacy Preserving Face Retrieval in the Cloud for Mobile Users
- **Arxiv ID**: http://arxiv.org/abs/1708.02872v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1708.02872v1)
- **Published**: 2017-08-09 15:21:42+00:00
- **Updated**: 2017-08-09 15:21:42+00:00
- **Authors**: Xin Jin, Shiming Ge, Chenggen Song
- **Comment**: Abuse Preventive Data Mining (APDM2017, IJCAI Workshop), 19-25
  August, 2017 Melbourne, Australia
- **Journal**: None
- **Summary**: Recently, cloud storage and processing have been widely adopted. Mobile users in one family or one team may automatically backup their photos to the same shared cloud storage space. The powerful face detector trained and provided by a 3rd party may be used to retrieve the photo collection which contains a specific group of persons from the cloud storage server. However, the privacy of the mobile users may be leaked to the cloud server providers. In the meanwhile, the copyright of the face detector should be protected. Thus, in this paper, we propose a protocol of privacy preserving face retrieval in the cloud for mobile users, which protects the user photos and the face detector simultaneously. The cloud server only provides the resources of storage and computing and can not learn anything of the user photos and the face detector. We test our protocol inside several families and classes. The experimental results reveal that our protocol can successfully retrieve the proper photos from the cloud server and protect the user photos and the face detector.



### Interacting with Acoustic Simulation and Fabrication
- **Arxiv ID**: http://arxiv.org/abs/1708.02895v2
- **DOI**: 10.1145/3131785.3131842
- **Categories**: **cs.HC**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1708.02895v2)
- **Published**: 2017-08-09 16:20:12+00:00
- **Updated**: 2017-12-04 17:14:39+00:00
- **Authors**: Dingzeyu Li
- **Comment**: ACM UIST 2017 Doctoral Symposium
- **Journal**: None
- **Summary**: Incorporating accurate physics-based simulation into interactive design tools is challenging. However, adding the physics accurately becomes crucial to several emerging technologies. For example, in virtual/augmented reality (VR/AR) videos, the faithful reproduction of surrounding audios is required to bring the immersion to the next level. Similarly, as personal fabrication is made possible with accessible 3D printers, more intuitive tools that respect the physical constraints can help artists to prototype designs. One main hurdle is the sheer amount of computation complexity to accurately reproduce the real-world phenomena through physics-based simulation. In my thesis research, I develop interactive tools that implement efficient physics-based simulation algorithms for automatic optimization and intuitive user interaction.



### An evaluation of large-scale methods for image instance and class discovery
- **Arxiv ID**: http://arxiv.org/abs/1708.02898v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1708.02898v1)
- **Published**: 2017-08-09 16:29:21+00:00
- **Updated**: 2017-08-09 16:29:21+00:00
- **Authors**: Matthijs Douze, Hervé Jégou, Jeff Johnson
- **Comment**: Published at ACM Multimedia workshop
- **Journal**: None
- **Summary**: This paper aims at discovering meaningful subsets of related images from large image collections without annotations. We search groups of images related at different levels of semantic, i.e., either instances or visual classes. While k-means is usually considered as the gold standard for this task, we evaluate and show the interest of diffusion methods that have been neglected by the state of the art, such as the Markov Clustering algorithm.   We report results on the ImageNet and the Paris500k instance dataset, both enlarged with images from YFCC100M. We evaluate our methods with a labelling cost that reflects how much effort a human would require to correct the generated clusters.   Our analysis highlights several properties. First, when powered with an efficient GPU implementation, the cost of the discovery process is small compared to computing the image descriptors, even for collections as large as 100 million images. Second, we show that descriptions selected for instance search improve the discovery of object classes. Third, the Markov Clustering technique consistently outperforms other methods; to our knowledge it has never been considered in this large scale scenario.



### Transitive Invariance for Self-supervised Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.02901v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02901v3)
- **Published**: 2017-08-09 16:32:44+00:00
- **Updated**: 2017-08-15 02:34:50+00:00
- **Authors**: Xiaolong Wang, Kaiming He, Abhinav Gupta
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: Learning visual representations with self-supervised learning has become popular in computer vision. The idea is to design auxiliary tasks where labels are free to obtain. Most of these tasks end up providing data to learn specific kinds of invariance useful for recognition. In this paper, we propose to exploit different self-supervised approaches to learn representations invariant to (i) inter-instance variations (two objects in the same class should have similar features) and (ii) intra-instance variations (viewpoint, pose, deformations, illumination, etc). Instead of combining two approaches with multi-task learning, we argue to organize and reason the data with multiple variations. Specifically, we propose to generate a graph with millions of objects mined from hundreds of thousands of videos. The objects are connected by two types of edges which correspond to two types of invariance: "different instances but a similar viewpoint and category" and "different viewpoints of the same instance". By applying simple transitivity on the graph with these edges, we can obtain pairs of images exhibiting richer visual invariance. We use this data to train a Triplet-Siamese network with VGG16 as the base architecture and apply the learned representations to different recognition tasks. For object detection, we achieve 63.2% mAP on PASCAL VOC 2007 using Fast R-CNN (compare to 67.3% with ImageNet pre-training). For the challenging COCO dataset, our method is surprisingly close (23.5%) to the ImageNet-supervised counterpart (24.4%) using the Faster R-CNN framework. We also show that our network can perform significantly better than the ImageNet network in the surface normal estimation task.



### SUBIC: A supervised, structured binary code for image search
- **Arxiv ID**: http://arxiv.org/abs/1708.02932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02932v1)
- **Published**: 2017-08-09 17:56:27+00:00
- **Updated**: 2017-08-09 17:56:27+00:00
- **Authors**: Himalaya Jain, Joaquin Zepeda, Patrick Pérez, Rémi Gribonval
- **Comment**: Accepted at ICCV 2017 (Spotlight)
- **Journal**: None
- **Summary**: For large-scale visual search, highly compressed yet meaningful representations of images are essential. Structured vector quantizers based on product quantization and its variants are usually employed to achieve such compression while minimizing the loss of accuracy. Yet, unlike binary hashing schemes, these unsupervised methods have not yet benefited from the supervision, end-to-end learning and novel architectures ushered in by the deep learning revolution. We hence propose herein a novel method to make deep convolutional neural networks produce supervised, compact, structured binary codes for visual search. Our method makes use of a novel block-softmax non-linearity and of batch-based entropy losses that together induce structure in the learned encodings. We show that our method outperforms state-of-the-art compact representations based on deep hashing or structured quantization in single and cross-domain category retrieval, instance retrieval and classification. We make our code and models publicly available online.



### Personalized Cinemagraphs using Semantic Understanding and Collaborative Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.02970v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1708.02970v1)
- **Published**: 2017-08-09 19:03:12+00:00
- **Updated**: 2017-08-09 19:03:12+00:00
- **Authors**: Tae-Hyun Oh, Kyungdon Joo, Neel Joshi, Baoyuan Wang, In So Kweon, Sing Bing Kang
- **Comment**: To appear in ICCV 2017. Total 17 pages including the supplementary
  material
- **Journal**: None
- **Summary**: Cinemagraphs are a compelling way to convey dynamic aspects of a scene. In these media, dynamic and still elements are juxtaposed to create an artistic and narrative experience. Creating a high-quality, aesthetically pleasing cinemagraph requires isolating objects in a semantically meaningful way and then selecting good start times and looping periods for those objects to minimize visual artifacts (such a tearing). To achieve this, we present a new technique that uses object recognition and semantic segmentation as part of an optimization method to automatically create cinemagraphs from videos that are both visually appealing and semantically meaningful. Given a scene with multiple objects, there are many cinemagraphs one could create. Our method evaluates these multiple candidates and presents the best one, as determined by a model trained to predict human preferences in a collaborative way. We demonstrate the effectiveness of our approach with multiple results and a user study.



### Learning Policies for Adaptive Tracking with Deep Feature Cascades
- **Arxiv ID**: http://arxiv.org/abs/1708.02973v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02973v2)
- **Published**: 2017-08-09 19:09:11+00:00
- **Updated**: 2017-09-13 19:39:45+00:00
- **Authors**: Chen Huang, Simon Lucey, Deva Ramanan
- **Comment**: ICCV 2017 Spotlight, with Supplementary Material
- **Journal**: None
- **Summary**: Visual object tracking is a fundamental and time-critical vision task. Recent years have seen many shallow tracking methods based on real-time pixel-based correlation filters, as well as deep methods that have top performance but need a high-end GPU. In this paper, we learn to improve the speed of deep trackers without losing accuracy. Our fundamental insight is to take an adaptive approach, where easy frames are processed with cheap features (such as pixel values), while challenging frames are processed with invariant but expensive deep features. We formulate the adaptive tracking problem as a decision-making process, and learn an agent to decide whether to locate objects with high confidence on an early layer, or continue processing subsequent layers of a network. This significantly reduces the feed-forward cost for easy frames with distinct or slow-moving objects. We train the agent offline in a reinforcement learning fashion, and further demonstrate that learning all deep layers (so as to provide good features for adaptive tracking) can lead to near real-time average tracking speed of 23 fps on a single CPU while achieving state-of-the-art performance. Perhaps most tellingly, our approach provides a 100X speedup for almost 50% of the time, indicating the power of an adaptive approach.



### Random Binary Trees for Approximate Nearest Neighbour Search in Binary Space
- **Arxiv ID**: http://arxiv.org/abs/1708.02976v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02976v2)
- **Published**: 2017-08-09 19:16:01+00:00
- **Updated**: 2019-03-24 19:38:18+00:00
- **Authors**: Michal Komorowski, Tomasz Trzcinski
- **Comment**: The final publication is available at Springer via
  https://doi.org/10.1007/978-3-319-69900-4_60
- **Journal**: None
- **Summary**: Approximate nearest neighbour (ANN) search is one of the most important problems in computer science fields such as data mining or computer vision. In this paper, we focus on ANN for high-dimensional binary vectors and we propose a simple yet powerful search method that uses Random Binary Search Trees (RBST). We apply our method to a dataset of 1.25M binary local feature descriptors obtained from a real-life image-based localisation system provided by Google as a part of Project Tango. An extensive evaluation of our method against the state-of-the-art variations of Locality Sensitive Hashing (LSH), namely Uniform LSH and Multi-probe LSH, shows the superiority of our method in terms of retrieval precision with performance boost of over 20%



### Hierarchically-Attentive RNN for Album Summarization and Storytelling
- **Arxiv ID**: http://arxiv.org/abs/1708.02977v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.02977v1)
- **Published**: 2017-08-09 19:26:47+00:00
- **Updated**: 2017-08-09 19:26:47+00:00
- **Authors**: Licheng Yu, Mohit Bansal, Tamara L. Berg
- **Comment**: To appear at EMNLP-2017 (7 pages)
- **Journal**: None
- **Summary**: We address the problem of end-to-end visual storytelling. Given a photo album, our model first selects the most representative (summary) photos, and then composes a natural language story for the album. For this task, we make use of the Visual Storytelling dataset and a model composed of three hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album photos, select representative (summary) photos, and compose the story. Automatic and human evaluations show our model achieves better performance on selection, generation, and retrieval than baselines.



### ChromaTag: A Colored Marker and Fast Detection Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1708.02982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02982v1)
- **Published**: 2017-08-09 19:41:51+00:00
- **Updated**: 2017-08-09 19:41:51+00:00
- **Authors**: Joseph DeGol, Timothy Bretl, Derek Hoiem
- **Comment**: International Conference on Computer Vision (ICCV '17)
- **Journal**: None
- **Summary**: Current fiducial marker detection algorithms rely on marker IDs for false positive rejection. Time is wasted on potential detections that will eventually be rejected as false positives. We introduce ChromaTag, a fiducial marker and detection algorithm designed to use opponent colors to limit and quickly reject initial false detections and grayscale for precise localization. Through experiments, we show that ChromaTag is significantly faster than current fiducial markers while achieving similar or better detection accuracy. We also show how tag size and viewing direction effect detection accuracy. Our contribution is significant because fiducial markers are often used in real-time applications (e.g. marker assisted robot navigation) where heavy computation is required by other parts of the system.



### A Unified Model for Near and Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/1708.03035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.03035v1)
- **Published**: 2017-08-09 23:55:07+00:00
- **Updated**: 2017-08-09 23:55:07+00:00
- **Authors**: Scott Workman, Menghua Zhai, David J. Crandall, Nathan Jacobs
- **Comment**: International Conference on Computer Vision (ICCV) 2017
- **Journal**: None
- **Summary**: We propose a novel convolutional neural network architecture for estimating geospatial functions such as population density, land cover, or land use. In our approach, we combine overhead and ground-level images in an end-to-end trainable neural network, which uses kernel regression and density estimation to convert features extracted from the ground-level images into a dense feature map. The output of this network is a dense estimate of the geospatial function in the form of a pixel-level labeling of the overhead image. To evaluate our approach, we created a large dataset of overhead and ground-level images from a major urban area with three sets of labels: land use, building function, and building age. We find that our approach is more accurate for all tasks, in some cases dramatically so.



