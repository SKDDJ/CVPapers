# Arxiv Papers in cs.CV on 2017-08-06
### Automated Assessment of Facial Wrinkling: a case study on the effect of smoking
- **Arxiv ID**: http://arxiv.org/abs/1708.01844v2
- **DOI**: 10.1109/SMC.2017.8122755
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01844v2)
- **Published**: 2017-08-06 04:56:57+00:00
- **Updated**: 2017-12-31 22:06:35+00:00
- **Authors**: Omaima FathElrahman Osman, Remah Mutasim Ibrahim Elbashir, Imad Eldain Abbass, Connah Kendrick, Manu Goyal, Moi Hoon Yap
- **Comment**: 6 pages, 8 figures, Accepted in 2017 IEEE SMC International
  Conference
- **Journal**: 2017 IEEE International Conference on Systems, Man, and
  Cybernetics (SMC), Banff, AB, 2017, pp. 1081-1086
- **Summary**: Facial wrinkle is one of the most prominent biological changes that accompanying the natural aging process. However, there are some external factors contributing to premature wrinkles development, such as sun exposure and smoking. Clinical studies have shown that heavy smoking causes premature wrinkles development. However, there is no computerised system that can automatically assess the facial wrinkles on the whole face. This study investigates the effect of smoking on facial wrinkling using a social habit face dataset and an automated computerised computer vision algorithm. The wrinkles pattern represented in the intensity of 0-255 was first extracted using a modified Hybrid Hessian Filter. The face was divided into ten predefined regions, where the wrinkles in each region was extracted. Then the statistical analysis was performed to analyse which region is effected mainly by smoking. The result showed that the density of wrinkles for smokers in two regions around the mouth was significantly higher than the non-smokers, at p-value of 0.05. Other regions are inconclusive due to lack of large scale dataset. Finally, the wrinkle was visually compared between smoker and non-smoker faces by generating a generic 3D face model.



### Manifold Constrained Low-Rank Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1708.01846v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01846v1)
- **Published**: 2017-08-06 05:12:48+00:00
- **Updated**: 2017-08-06 05:12:48+00:00
- **Authors**: Chen Chen, Baochang Zhang, Alessio Del Bue, Vittorio Murino
- **Comment**: None
- **Journal**: None
- **Summary**: Low-rank decomposition (LRD) is a state-of-the-art method for visual data reconstruction and modelling. However, it is a very challenging problem when the image data contains significant occlusion, noise, illumination variation, and misalignment from rotation or viewpoint changes. We leverage the specific structure of data in order to improve the performance of LRD when the data are not ideal. To this end, we propose a new framework that embeds manifold priors into LRD. To implement the framework, we design an alternating direction method of multipliers (ADMM) method which efficiently integrates the manifold constraints during the optimization process. The proposed approach is successfully used to calculate low-rank models from face images, hand-written digits and planar surface images. The results show a consistent increase of performance when compared to the state-of-the-art over a wide range of realistic image misalignments and corruptions.



### Long Short-Term Memory Kalman Filters:Recurrent Neural Estimators for Pose Regularization
- **Arxiv ID**: http://arxiv.org/abs/1708.01885v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01885v1)
- **Published**: 2017-08-06 13:08:56+00:00
- **Updated**: 2017-08-06 13:08:56+00:00
- **Authors**: Huseyin Coskun, Felix Achilles, Robert DiPietro, Nassir Navab, Federico Tombari
- **Comment**: Accepted ICCV 2017
- **Journal**: None
- **Summary**: One-shot pose estimation for tasks such as body joint localization, camera pose estimation, and object tracking are generally noisy, and temporal filters have been extensively used for regularization. One of the most widely-used methods is the Kalman filter, which is both extremely simple and general. However, Kalman filters require a motion model and measurement model to be specified a priori, which burdens the modeler and simultaneously demands that we use explicit models that are often only crude approximations of reality. For example, in the pose-estimation tasks mentioned above, it is common to use motion models that assume constant velocity or constant acceleration, and we believe that these simplified representations are severely inhibitive. In this work, we propose to instead learn rich, dynamic representations of the motion and noise models. In particular, we propose learning these models from data using long short term memory, which allows representations that depend on all previous observations and all previous states. We evaluate our method using three of the most popular pose estimation tasks in computer vision, and in all cases we obtain state-of-the-art performance.



### End-to-end learning potentials for structured attribute prediction
- **Arxiv ID**: http://arxiv.org/abs/1708.01892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01892v1)
- **Published**: 2017-08-06 13:56:31+00:00
- **Updated**: 2017-08-06 13:56:31+00:00
- **Authors**: Kota Yamaguchi, Takayuki Okatani, Takayuki Umeda, Kazuhiko Murasaki, Kyoko Sudo
- **Comment**: None
- **Journal**: None
- **Summary**: We present a structured inference approach in deep neural networks for multiple attribute prediction. In attribute prediction, a common approach is to learn independent classifiers on top of a good feature representation. However, such classifiers assume conditional independence on features and do not explicitly consider the dependency between attributes in the inference process. We propose to formulate attribute prediction in terms of marginal inference in the conditional random field. We model potential functions by deep neural networks and apply the sum-product algorithm to solve for the approximate marginal distribution in feed-forward networks. Our message passing layer implements sparse pairwise potentials by a softplus-linear function that is equivalent to a higher-order classifier, and learns all the model parameters by end-to-end back propagation. The experimental results using SUN attributes and CelebA datasets suggest that the structured inference improves the attribute prediction performance, and possibly uncovers the hidden relationship between attributes.



### EndNet: Sparse AutoEncoder Network for Endmember Extraction and Hyperspectral Unmixing
- **Arxiv ID**: http://arxiv.org/abs/1708.01894v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01894v4)
- **Published**: 2017-08-06 14:21:32+00:00
- **Updated**: 2018-07-16 08:39:38+00:00
- **Authors**: Savas Ozkan, Berk Kaya, Gozde Bozdagi Akar
- **Comment**: To appear in IEEE Transaction on Geoscience and Remote Sensing
- **Journal**: None
- **Summary**: Data acquired from multi-channel sensors is a highly valuable asset to interpret the environment for a variety of remote sensing applications. However, low spatial resolution is a critical limitation for previous sensors and the constituent materials of a scene can be mixed in different fractions due to their spatial interactions. Spectral unmixing is a technique that allows us to obtain the material spectral signatures and their fractions from hyperspectral data. In this paper, we propose a novel endmember extraction and hyperspectral unmixing scheme, so called \textit{EndNet}, that is based on a two-staged autoencoder network. This well-known structure is completely enhanced and restructured by introducing additional layers and a projection metric (i.e., spectral angle distance (SAD) instead of inner product) to achieve an optimum solution. Moreover, we present a novel loss function that is composed of a Kullback-Leibler divergence term with SAD similarity and additional penalty terms to improve the sparsity of the estimates. These modifications enable us to set the common properties of endmembers such as non-linearity and sparsity for autoencoder networks. Lastly, due to the stochastic-gradient based approach, the method is scalable for large-scale data and it can be accelerated on Graphical Processing Units (GPUs). To demonstrate the superiority of our proposed method, we conduct extensive experiments on several well-known datasets. The results confirm that the proposed method considerably improves the performance compared to the state-of-the-art techniques in literature.



### Image Quality Assessment Techniques Show Improved Training and Evaluation of Autoencoder Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.02237v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.02237v1)
- **Published**: 2017-08-06 16:31:07+00:00
- **Updated**: 2017-08-06 16:31:07+00:00
- **Authors**: Michael O. Vertolli, Jim Davies
- **Comment**: 10 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: We propose a training and evaluation approach for autoencoder Generative Adversarial Networks (GANs), specifically the Boundary Equilibrium Generative Adversarial Network (BEGAN), based on methods from the image quality assessment literature. Our approach explores a multidimensional evaluation criterion that utilizes three distance functions: an $l_1$ score, the Gradient Magnitude Similarity Mean (GMSM) score, and a chrominance score. We show that each of the different distance functions captures a slightly different set of properties in image space and, consequently, requires its own evaluation criterion to properly assess whether the relevant property has been adequately learned. We show that models using the new distance functions are able to produce better images than the original BEGAN model in predicted ways.



### Fully Convolutional Networks for Diabetic Foot Ulcer Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1708.01928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01928v1)
- **Published**: 2017-08-06 19:45:37+00:00
- **Updated**: 2017-08-06 19:45:37+00:00
- **Authors**: Manu Goyal, Neil D. Reeves, Satyan Rajbhandari, Jennifer Spragg, Moi Hoon Yap
- **Comment**: 7 pages, 5 figures, 2017 IEEE SMC International Conference (To
  appear)
- **Journal**: None
- **Summary**: Diabetic Foot Ulcer (DFU) is a major complication of Diabetes, which if not managed properly can lead to amputation. DFU can appear anywhere on the foot and can vary in size, colour, and contrast depending on various pathologies. Current clinical approaches to DFU treatment rely on patients and clinician vigilance, which has significant limitations such as the high cost involved in the diagnosis, treatment and lengthy care of the DFU. We introduce a dataset of 705 foot images. We provide the ground truth of ulcer region and the surrounding skin that is an important indicator for clinicians to assess the progress of ulcer. Then, we propose a two-tier transfer learning from bigger datasets to train the Fully Convolutional Networks (FCNs) to automatically segment the ulcer and surrounding skin. Using 5-fold cross-validation, the proposed two-tier transfer learning FCN Models achieve a Dice Similarity Coefficient of 0.794 ($\pm$0.104) for ulcer region, 0.851 ($\pm$0.148) for surrounding skin region, and 0.899 ($\pm$0.072) for the combination of both regions. This demonstrates the potential of FCNs in DFU segmentation, which can be further improved with a larger dataset.



### Face Parsing via Recurrent Propagation
- **Arxiv ID**: http://arxiv.org/abs/1708.01936v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01936v1)
- **Published**: 2017-08-06 21:34:01+00:00
- **Updated**: 2017-08-06 21:34:01+00:00
- **Authors**: Sifei Liu, Jianping Shi, Ji Liang, Ming-Hsuan Yang
- **Comment**: 10 pages, 5 figures, BMVC 2017
- **Journal**: None
- **Summary**: Face parsing is an important problem in computer vision that finds numerous applications including recognition and editing. Recently, deep convolutional neural networks (CNNs) have been applied to image parsing and segmentation with the state-of-the-art performance. In this paper, we propose a face parsing algorithm that combines hierarchical representations learned by a CNN, and accurate label propagations achieved by a spatially variant recurrent neural network (RNN). The RNN-based propagation approach enables efficient inference over a global space with the guidance of semantic edges generated by a local convolutional model. Since the convolutional architecture can be shallow and the spatial RNN can have few parameters, the framework is much faster and more light-weighted than the state-of-the-art CNNs for the same task. We apply the proposed model to coarse-grained and fine-grained face parsing. For fine-grained face parsing, we develop a two-stage approach by first identifying the main regions and then segmenting the detail components, which achieves better performance in terms of accuracy and efficiency. With a single GPU, the proposed algorithm parses face images accurately at 300 frames per second, which facilitates real-time applications.



### A Framework for Visually Realistic Multi-robot Simulation in Natural Environment
- **Arxiv ID**: http://arxiv.org/abs/1708.01938v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.01938v1)
- **Published**: 2017-08-06 21:41:42+00:00
- **Updated**: 2017-08-06 21:41:42+00:00
- **Authors**: Ori Ganoni, Ramakrishnan Mukundan
- **Comment**: WSCG 2017 conference
- **Journal**: None
- **Summary**: This paper presents a generalized framework for the simulation of multiple robots and drones in highly realistic models of natural environments. The proposed simulation architecture uses the Unreal Engine4 for generating both optical and depth sensor outputs from any position and orientation within the environment and provides several key domain specific simulation capabilities. Various components and functionalities of the system have been discussed in detail. The simulation engine also allows users to test and validate a wide range of computer vision algorithms involving different drone configurations under many types of environmental effects such as wind gusts. The paper demonstrates the effectiveness of the system by giving experimental results for a test scenario where one drone tracks the simulated motion of another in a complex natural environment.



### Intensity Video Guided 4D Fusion for Improved Highly Dynamic 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1708.01946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01946v1)
- **Published**: 2017-08-06 22:25:13+00:00
- **Updated**: 2017-08-06 22:25:13+00:00
- **Authors**: Jie Zhang, Christos Maniatis, Luis Horna, Robert B. Fisher
- **Comment**: None
- **Journal**: None
- **Summary**: The availability of high-speed 3D video sensors has greatly facilitated 3D shape acquisition of dynamic and deformable objects, but high frame rate 3D reconstruction is always degraded by spatial noise and temporal fluctuations. This paper presents a simple yet powerful intensity video guided multi-frame 4D fusion pipeline. Temporal tracking of intensity image points (of moving and deforming objects) allows registration of the corresponding 3D data points, whose 3D noise and fluctuations are then reduced by spatio-temporal multi-frame 4D fusion. We conducted simulated noise tests and real experiments on four 3D objects using a 1000 fps 3D video sensor. The results demonstrate that the proposed algorithm is effective at reducing 3D noise and is robust against intensity noise. It outperforms existing algorithms with good scalability on both stationary and dynamic objects.



