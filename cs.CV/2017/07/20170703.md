# Arxiv Papers in cs.CV on 2017-07-03
### Deep Convolutional Framelets: A General Deep Learning Framework for Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/1707.00372v5
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1707.00372v5)
- **Published**: 2017-07-03 00:16:04+00:00
- **Updated**: 2018-01-25 09:37:10+00:00
- **Authors**: Jong Chul Ye, Yoseob Han, Eunju Cha
- **Comment**: This will appear in SIAM Journal on Imaging Sciences
- **Journal**: None
- **Summary**: Recently, deep learning approaches with various network architectures have achieved significant performance improvement over existing iterative reconstruction methods in various imaging problems. However, it is still unclear why these deep learning architectures work for specific inverse problems. To address these issues, here we show that the long-searched-for missing link is the convolution framelets for representing a signal by convolving local and non-local bases. The convolution framelets was originally developed to generalize the theory of low-rank Hankel matrix approaches for inverse problems, and this paper further extends the idea so that we can obtain a deep neural network using multilayer convolution framelets with perfect reconstruction (PR) under rectilinear linear unit nonlinearity (ReLU). Our analysis also shows that the popular deep network components such as residual block, redundant filter channels, and concatenated ReLU (CReLU) do indeed help to achieve the PR, while the pooling and unpooling layers should be augmented with high-pass branches to meet the PR condition. Moreover, by changing the number of filter channels and bias, we can control the shrinkage behaviors of the neural network. This discovery leads us to propose a novel theory for deep convolutional framelets neural network. Using numerical experiments with various inverse problems, we demonstrated that our deep convolution framelets network shows consistent improvement over existing deep architectures.This discovery suggests that the success of deep learning is not from a magical power of a black-box, but rather comes from the power of a novel signal representation using non-local basis combined with data-driven local basis, which is indeed a natural extension of classical signal processing theory.



### Vectorial Dimension Reduction for Tensors Based on Bayesian Inference
- **Arxiv ID**: http://arxiv.org/abs/1707.00380v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00380v1)
- **Published**: 2017-07-03 02:35:21+00:00
- **Updated**: 2017-07-03 02:35:21+00:00
- **Authors**: Fujiao Ju, Yanfeng Sun, Junbin Gao, Yongli Hu, Baocai Yin
- **Comment**: Submiting to TNNLS
- **Journal**: None
- **Summary**: Dimensionality reduction for high-order tensors is a challenging problem. In conventional approaches, higher order tensors are `vectorized` via Tucker decomposition to obtain lower order tensors. This will destroy the inherent high-order structures or resulting in undesired tensors, respectively. This paper introduces a probabilistic vectorial dimensionality reduction model for tensorial data. The model represents a tensor by employing a linear combination of same order basis tensors, thus it offers a mechanism to directly reduce a tensor to a vector. Under this expression, the projection base of the model is based on the tensor CandeComp/PARAFAC (CP) decomposition and the number of free parameters in the model only grows linearly with the number of modes rather than exponentially. A Bayesian inference has been established via the variational EM approach. A criterion to set the parameters (factor number of CP decomposition and the number of extracted features) is empirically given. The model outperforms several existing PCA-based methods and CP decomposition on several publicly available databases in terms of classification and clustering accuracy.



### Joint Pose and Principal Curvature Refinement Using Quadrics
- **Arxiv ID**: http://arxiv.org/abs/1707.00381v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00381v2)
- **Published**: 2017-07-03 02:35:59+00:00
- **Updated**: 2017-07-14 02:56:57+00:00
- **Authors**: Andrew Spek, Tom Drummond
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a novel joint approach for optimising surface curvature and pose alignment. We present two implementations of this joint optimisation strategy, including a fast implementation that uses two frames and an offline multi-frame approach. We demonstrate an order of magnitude improvement in simulation over state of the art dense relative point-to-plane Iterative Closest Point (ICP) pose alignment using our dense joint frame-to-frame approach and show comparable pose drift to dense point-to-plane ICP bundle adjustment using low-cost depth sensors. Additionally our improved joint quadric based approach can be used to more accurately estimate surface curvature on noisy point clouds than previous approaches.



### Physics Inspired Optimization on Semantic Transfer Features: An Alternative Method for Room Layout Estimation
- **Arxiv ID**: http://arxiv.org/abs/1707.00383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00383v1)
- **Published**: 2017-07-03 02:46:02+00:00
- **Updated**: 2017-07-03 02:46:02+00:00
- **Authors**: Hao Zhao, Ming Lu, Anbang Yao, Yiwen Guo, Yurong Chen, Li Zhang
- **Comment**: To appear in CVPR 2017. Project Page:
  https://sites.google.com/view/st-pio/
- **Journal**: None
- **Summary**: In this paper, we propose an alternative method to estimate room layouts of cluttered indoor scenes. This method enjoys the benefits of two novel techniques. The first one is semantic transfer (ST), which is: (1) a formulation to integrate the relationship between scene clutter and room layout into convolutional neural networks; (2) an architecture that can be end-to-end trained; (3) a practical strategy to initialize weights for very deep networks under unbalanced training data distribution. ST allows us to extract highly robust features under various circumstances, and in order to address the computation redundance hidden in these features we develop a principled and efficient inference scheme named physics inspired optimization (PIO). PIO's basic idea is to formulate some phenomena observed in ST features into mechanics concepts. Evaluations on public datasets LSUN and Hedau show that the proposed method is more accurate than state-of-the-art methods.



### A Fast Method For Computing Principal Curvatures From Range Images
- **Arxiv ID**: http://arxiv.org/abs/1707.00385v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00385v2)
- **Published**: 2017-07-03 03:01:44+00:00
- **Updated**: 2017-07-14 02:35:35+00:00
- **Authors**: Andrew Spek, Wai Ho Li, Tom Drummond
- **Comment**: None
- **Journal**: None
- **Summary**: Estimation of surface curvature from range data is important for a range of tasks in computer vision and robotics, object segmentation, object recognition and robotic grasping estimation. This work presents a fast method of robustly computing accurate metric principal curvature values from noisy point clouds which was implemented on GPU. In contrast to existing readily available solutions which first differentiate the surface to estimate surface normals and then differentiate these to obtain curvature, amplifying noise, our method iteratively fits parabolic quadric surface patches to the data. Additionally previous methods with a similar formulation use less robust techniques less applicable to a high noise sensor. We demonstrate that our method is fast and provides better curvature estimates than existing techniques. In particular we compare our method to several alternatives to demonstrate the improvement.



### Pedestrian Alignment Network for Large-scale Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1707.00408v1
- **DOI**: 10.1109/TCSVT.2018.2873599
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00408v1)
- **Published**: 2017-07-03 05:48:54+00:00
- **Updated**: 2017-07-03 05:48:54+00:00
- **Authors**: Zhedong Zheng, Liang Zheng, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (person re-ID) is mostly viewed as an image retrieval problem. This task aims to search a query person in a large image pool. In practice, person re-ID usually adopts automatic detectors to obtain cropped pedestrian images. However, this process suffers from two types of detector errors: excessive background and part missing. Both errors deteriorate the quality of pedestrian alignment and may compromise pedestrian matching due to the position and scale variances. To address the misalignment problem, we propose that alignment can be learned from an identification procedure. We introduce the pedestrian alignment network (PAN) which allows discriminative embedding learning and pedestrian alignment without extra annotations. Our key observation is that when the convolutional neural network (CNN) learns to discriminate between different identities, the learned feature maps usually exhibit strong activations on the human body rather than the background. The proposed network thus takes advantage of this attention mechanism to adaptively locate and align pedestrians within a bounding box. Visual examples show that pedestrians are better aligned with PAN. Experiments on three large-scale re-ID datasets confirm that PAN improves the discriminative ability of the feature embeddings and yields competitive accuracy with the state-of-the-art methods.



### Deep Ranking Model by Large Adaptive Margin Learning for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1707.00409v2
- **DOI**: 10.1016/j.patcog.2017.09.024
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00409v2)
- **Published**: 2017-07-03 05:58:46+00:00
- **Updated**: 2017-09-17 05:38:21+00:00
- **Authors**: Jiayun Wang, Sanping Zhou, Jinjun Wang, Qiqi Hou
- **Comment**: Accepted to Pattern Recognition
- **Journal**: None
- **Summary**: Person re-identification aims to match images of the same person across disjoint camera views, which is a challenging problem in video surveillance. The major challenge of this task lies in how to preserve the similarity of the same person against large variations caused by complex backgrounds, mutual occlusions and different illuminations, while discriminating the different individuals. In this paper, we present a novel deep ranking model with feature learning and fusion by learning a large adaptive margin between the intra-class distance and inter-class distance to solve the person re-identification problem. Specifically, we organize the training images into a batch of pairwise samples. Treating these pairwise samples as inputs, we build a novel part-based deep convolutional neural network (CNN) to learn the layered feature representations by preserving a large adaptive margin. As a result, the final learned model can effectively find out the matched target to the anchor image among a number of candidates in the gallery image set by learning discriminative and stable feature representations. Overcoming the weaknesses of conventional fixed-margin loss functions, our adaptive margin loss function is more appropriate for the dynamic feature space. On four benchmark datasets, PRID2011, Market1501, CUHK01 and 3DPeS, we extensively conduct comparative evaluations to demonstrate the advantages of the proposed method over the state-of-the-art approaches in person re-identification.



### Detection and Localization of Image Forgeries using Resampling Features and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.00433v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00433v1)
- **Published**: 2017-07-03 07:50:15+00:00
- **Updated**: 2017-07-03 07:50:15+00:00
- **Authors**: Jason Bunk, Jawadul H. Bappy, Tajuddin Manhar Mohammed, Lakshmanan Nataraj, Arjuna Flenner, B. S. Manjunath, Shivkumar Chandrasekaran, Amit K. Roy-Chowdhury, Lawrence Peterson
- **Comment**: None
- **Journal**: None
- **Summary**: Resampling is an important signature of manipulated images. In this paper, we propose two methods to detect and localize image manipulations based on a combination of resampling features and deep learning. In the first method, the Radon transform of resampling features are computed on overlapping image patches. Deep learning classifiers and a Gaussian conditional random field model are then used to create a heatmap. Tampered regions are located using a Random Walker segmentation method. In the second method, resampling features computed on overlapping image patches are passed through a Long short-term memory (LSTM) based network for classification and localization. We compare the performance of detection/localization of both these methods. Our experimental results show that both techniques are effective in detecting and localizing digital image forgeries.



### End-to-End Learning of Video Super-Resolution with Motion Compensation
- **Arxiv ID**: http://arxiv.org/abs/1707.00471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00471v1)
- **Published**: 2017-07-03 10:16:10+00:00
- **Updated**: 2017-07-03 10:16:10+00:00
- **Authors**: Osama Makansi, Eddy Ilg, Thomas Brox
- **Comment**: Accepted to GCPR2017
- **Journal**: None
- **Summary**: Learning approaches have shown great success in the task of super-resolving an image given a low resolution input. Video super-resolution aims for exploiting additionally the information from multiple images. Typically, the images are related via optical flow and consecutive image warping. In this paper, we provide an end-to-end video super-resolution network that, in contrast to previous works, includes the estimation of optical flow in the overall network architecture. We analyze the usage of optical flow for video super-resolution and find that common off-the-shelf image warping does not allow video super-resolution to benefit much from optical flow. We rather propose an operation for motion compensation that performs warping from low to high resolution directly. We show that with this network configuration, video super-resolution can benefit from optical flow and we obtain state-of-the-art results on the popular test sets. We also show that the processing of whole images rather than independent patches is responsible for a large increase in accuracy.



### Generalised Wasserstein Dice Score for Imbalanced Multi-class Segmentation using Holistic Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.00478v4
- **DOI**: 10.1007/978-3-319-75238-9_6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00478v4)
- **Published**: 2017-07-03 10:53:03+00:00
- **Updated**: 2017-08-29 12:26:16+00:00
- **Authors**: Lucas Fidon, Wenqi Li, Luis C. Garcia-Peraza-Herrera, Jinendra Ekanayake, Neil Kitchen, Sebastien Ourselin, Tom Vercauteren
- **Comment**: Accepted as an oral presentation at the MICCAI 2017 Brain Lesion
  (BrainLes) Workshop
- **Journal**: None
- **Summary**: The Dice score is widely used for binary segmentation due to its robustness to class imbalance. Soft generalisations of the Dice score allow it to be used as a loss function for training convolutional neural networks (CNN). Although CNNs trained using mean-class Dice score achieve state-of-the-art results on multi-class segmentation, this loss function does neither take advantage of inter-class relationships nor multi-scale information. We argue that an improved loss function should balance misclassifications to favour predictions that are semantically meaningful. This paper investigates these issues in the context of multi-class brain tumour segmentation. Our contribution is threefold. 1) We propose a semantically-informed generalisation of the Dice score for multi-class segmentation based on the Wasserstein distance on the probabilistic label space. 2) We propose a holistic CNN that embeds spatial information at multiple scales with deep supervision. 3) We show that the joint use of holistic CNNs and generalised Wasserstein Dice scores achieves segmentations that are more semantically meaningful for brain tumour segmentation.



### Efficient Eye Typing with 9-direction Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/1707.00548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00548v1)
- **Published**: 2017-07-03 13:51:13+00:00
- **Updated**: 2017-07-03 13:51:13+00:00
- **Authors**: Chi Zhang, Rui Yao, Jinpeng Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Vision based text entry systems aim to help disabled people achieve text communication using eye movement. Most previous methods have employed an existing eye tracker to predict gaze direction and design an input method based upon that. However, these methods can result in eye tracking quality becoming easily affected by various factors and lengthy amounts of time for calibration. Our paper presents a novel efficient gaze based text input method, which has the advantage of low cost and robustness. Users can type in words by looking at an on-screen keyboard and blinking. Rather than estimate gaze angles directly to track eyes, we introduce a method that divides the human gaze into nine directions. This method can effectively improve the accuracy of making a selection by gaze and blinks. We build a Convolutional Neural Network (CNN) model for 9-direction gaze estimation. On the basis of the 9-direction gaze, we use a nine-key T9 input method which is widely used in candy bar phones. Bar phones were very popular in the world decades ago and have cultivated strong user habits and language models. To train a robust gaze estimator, we created a large-scale dataset with images of eyes sourced from 25 people. According to the results from our experiments, our CNN model is able to accurately estimate different people's gaze under various lighting conditions by different devices. In considering disable people's needs, we removed the complex calibration process. The input methods can run in screen mode and portable off-screen mode. Moreover, The datasets used in our experiments are made available to the community to allow further experimentation.



### An In-Depth Analysis of Visual Tracking with Siamese Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.00569v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00569v2)
- **Published**: 2017-07-03 14:27:10+00:00
- **Updated**: 2018-08-02 07:15:14+00:00
- **Authors**: Roman Pflugfelder
- **Comment**: submitted to IEEE TPAMI
- **Journal**: None
- **Summary**: This survey presents a deep analysis of the learning and inference capabilities in nine popular trackers. It is neither intended to study the whole literature nor is it an attempt to review all kinds of neural networks proposed for visual tracking. We focus instead on Siamese neural networks which are a promising starting point for studying the challenging problem of tracking. These networks integrate efficiently feature learning and the temporal matching and have so far shown state-of-the-art performance. In particular, the branches of Siamese networks, their layers connecting these branches, specific aspects of training and the embedding of these networks into the tracker are highlighted. Quantitative results from existing papers are compared with the conclusion that the current evaluation methodology shows problems with the reproducibility and the comparability of results. The paper proposes a novel Lisp-like formalism for a better comparison of trackers. This assumes a certain functional design and functional decomposition of trackers. The paper tries to give foundation for tracker design by a formulation of the problem based on the theory of machine learning and by the interpretation of a tracker as a decision function. The work concludes with promising lines of research and suggests future work.



### Automatic Cardiac Disease Assessment on cine-MRI via Time-Series Segmentation and Domain Specific Features
- **Arxiv ID**: http://arxiv.org/abs/1707.00587v2
- **DOI**: 10.1007/978-3-319-75541-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00587v2)
- **Published**: 2017-07-03 15:10:30+00:00
- **Updated**: 2018-01-25 08:15:24+00:00
- **Authors**: Fabian Isensee, Paul Jaeger, Peter M. Full, Ivo Wolf, Sandy Engelhardt, Klaus H. Maier-Hein
- **Comment**: To appear in the STACOM 2017 proceedings
- **Journal**: None
- **Summary**: Cardiac magnetic resonance imaging improves on diagnosis of cardiovascular diseases by providing images at high spatiotemporal resolution. Manual evaluation of these time-series, however, is expensive and prone to biased and non-reproducible outcomes. In this paper, we present a method that addresses named limitations by integrating segmentation and disease classification into a fully automatic processing pipeline. We use an ensemble of UNet inspired architectures for segmentation of cardiac structures such as the left and right ventricular cavity (LVC, RVC) and the left ventricular myocardium (LVM) on each time instance of the cardiac cycle. For the classification task, information is extracted from the segmented time-series in form of comprehensive features handcrafted to reflect diagnostic clinical procedures. Based on these features we train an ensemble of heavily regularized multilayer perceptrons (MLP) and a random forest classifier to predict the pathologic target class. We evaluated our method on the ACDC dataset (4 pathology groups, 1 healthy group) and achieve dice scores of 0.945 (LVC), 0.908 (RVC) and 0.905 (LVM) in a cross-validation over the training set (100 cases) and 0.950 (LVC), 0.923 (RVC) and 0.911 (LVM) on the test set (50 cases). We report a classification accuracy of 94% on a training set cross-validation and 92% on the test set. Our results underpin the potential of machine learning methods for accurate, fast and reproducible segmentation and computer-assisted diagnosis (CAD).



### Zero-Shot Learning -- A Comprehensive Evaluation of the Good, the Bad and the Ugly
- **Arxiv ID**: http://arxiv.org/abs/1707.00600v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00600v4)
- **Published**: 2017-07-03 15:41:41+00:00
- **Updated**: 2020-09-23 15:02:08+00:00
- **Authors**: Yongqin Xian, Christoph H. Lampert, Bernt Schiele, Zeynep Akata
- **Comment**: Accepted by TPAMI in July, 2018. We introduce Proposed Split Version
  2.0 (Please download it from our project webpage). arXiv admin note:
  substantial text overlap with arXiv:1703.04394
- **Journal**: None
- **Summary**: Due to the importance of zero-shot learning, i.e. classifying images where there is a lack of labeled training data, the number of proposed approaches has recently increased steadily. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits of publicly available datasets used for this task. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Moreover, we propose a new zero-shot learning dataset, the Animals with Attributes 2 (AWA2) dataset which we make publicly available both in terms of image features and the images themselves. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss in detail the limitations of the current status of the area which can be taken as a basis for advancing it.



### Geometric calibration of Colour and Stereo Surface Imaging System of ESA's Trace Gas Orbiter
- **Arxiv ID**: http://arxiv.org/abs/1707.00606v1
- **DOI**: 10.1016/j.asr.2017.10.025
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.00606v1)
- **Published**: 2017-07-03 15:45:21+00:00
- **Updated**: 2017-07-03 15:45:21+00:00
- **Authors**: Stepan Tulyakov, Anton Ivanov, Nicolas Thomas, Victoria Roloff, Antoine Pommerol, Gabriele Cremonese, Thomas Weigel, Francois Fleuret
- **Comment**: Submitted to Advances in Space Research
- **Journal**: None
- **Summary**: There are many geometric calibration methods for "standard" cameras. These methods, however, cannot be used for the calibration of telescopes with large focal lengths and complex off-axis optics. Moreover, specialized calibration methods for the telescopes are scarce in literature. We describe the calibration method that we developed for the Colour and Stereo Surface Imaging System (CaSSIS) telescope, on board of the ExoMars Trace Gas Orbiter (TGO). Although our method is described in the context of CaSSIS, with camera-specific experiments, it is general and can be applied to other telescopes. We further encourage re-use of the proposed method by making our calibration code and data available on-line.



### DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.00652v3
- **DOI**: 10.1109/TPAMI.2018.2840695
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00652v3)
- **Published**: 2017-07-03 17:04:54+00:00
- **Updated**: 2017-09-19 08:48:14+00:00
- **Authors**: Guotai Wang, Maria A. Zuluaga, Wenqi Li, Rosalind Pratt, Premal A. Patel, Michael Aertsen, Tom Doel, Anna L. David, Jan Deprest, Sebastien Ourselin, Tom Vercauteren
- **Comment**: 14 pages, 15 figures
- **Journal**: None
- **Summary**: Accurate medical image segmentation is essential for diagnosis, surgical planning and many other applications. Convolutional Neural Networks (CNNs) have become the state-of-the-art automatic segmentation methods. However, fully automatic results may still need to be refined to become accurate and robust enough for clinical use. We propose a deep learning-based interactive segmentation method to improve the results obtained by an automatic CNN and to reduce user interactions during refinement for higher accuracy. We use one CNN to obtain an initial automatic segmentation, on which user interactions are added to indicate mis-segmentations. Another CNN takes as input the user interactions with the initial segmentation and gives a refined result. We propose to combine user interactions with CNNs through geodesic distance transforms, and propose a resolution-preserving network that gives a better dense prediction. In addition, we integrate user interactions as hard constraints into a back-propagatable Conditional Random Field. We validated the proposed framework in the context of 2D placenta segmentation from fetal MRI and 3D brain tumor segmentation from FLAIR images. Experimental results show our method achieves a large improvement from automatic CNNs, and obtains comparable and even higher accuracy with fewer user interventions and less time compared with traditional interactive methods.



### Temporal HeartNet: Towards Human-Level Automatic Analysis of Fetal Cardiac Screening Video
- **Arxiv ID**: http://arxiv.org/abs/1707.00665v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00665v1)
- **Published**: 2017-07-03 17:31:16+00:00
- **Updated**: 2017-07-03 17:31:16+00:00
- **Authors**: Weilin Huang, Christopher P. Bridge, J. Alison Noble, Andrew Zisserman
- **Comment**: To appear in MICCAI, 2017
- **Journal**: None
- **Summary**: We present an automatic method to describe clinically useful information about scanning, and to guide image interpretation in ultrasound (US) videos of the fetal heart. Our method is able to jointly predict the visibility, viewing plane, location and orientation of the fetal heart at the frame level. The contributions of the paper are three-fold: (i) a convolutional neural network architecture is developed for a multi-task prediction, which is computed by sliding a 3x3 window spatially through convolutional maps. (ii) an anchor mechanism and Intersection over Union (IoU) loss are applied for improving localization accuracy. (iii) a recurrent architecture is designed to recursively compute regional convolutional features temporally over sequential frames, allowing each prediction to be conditioned on the whole video. This results in a spatial-temporal model that precisely describes detailed heart parameters in challenging US videos. We report results on a real-world clinical dataset, where our method achieves performance on par with expert annotations.



### Appearance invariance in convolutional networks with neighborhood similarity
- **Arxiv ID**: http://arxiv.org/abs/1707.00755v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1707.00755v1)
- **Published**: 2017-07-03 20:53:56+00:00
- **Updated**: 2017-07-03 20:53:56+00:00
- **Authors**: Tolga Tasdizen, Mehdi Sajjadi, Mehran Javanmardi, Nisha Ramesh
- **Comment**: None
- **Journal**: None
- **Summary**: We present a neighborhood similarity layer (NSL) which induces appearance invariance in a network when used in conjunction with convolutional layers. We are motivated by the observation that, even though convolutional networks have low generalization error, their generalization capability does not extend to samples which are not represented by the training data. For instance, while novel appearances of learned concepts pose no problem for the human visual system, feedforward convolutional networks are generally not successful in such situations. Motivated by the Gestalt principle of grouping with respect to similarity, the proposed NSL transforms its input feature map using the feature vectors at each pixel as a frame of reference, i.e. center of attention, for its surrounding neighborhood. This transformation is spatially varying, hence not a convolution. It is differentiable; therefore, networks including the proposed layer can be trained in an end-to-end manner. We analyze the invariance of NSL to significant changes in appearance that are not represented in the training data. We also demonstrate its advantages for digit recognition, semantic labeling and cell detection problems.



