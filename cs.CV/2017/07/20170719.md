# Arxiv Papers in cs.CV on 2017-07-19
### Secure SURF with Fully Homomorphic Encryption
- **Arxiv ID**: http://arxiv.org/abs/1707.05905v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1707.05905v1)
- **Published**: 2017-07-19 00:30:23+00:00
- **Updated**: 2017-07-19 00:30:23+00:00
- **Authors**: Thomas Shortell, Ali Shokoufandeh
- **Comment**: None
- **Journal**: None
- **Summary**: Cloud computing is an important part of today's world because offloading computations is a method to reduce costs. In this paper, we investigate computing the Speeded Up Robust Features (SURF) using Fully Homomorphic Encryption (FHE). Performing SURF in FHE enables a method to offload the computations while maintaining security and privacy of the original data. In support of this research, we developed a framework to compute SURF via a rational number based compatible with FHE. Although floating point (R) to rational numbers (Q) conversion introduces error, our research provides tight bounds on the magnitude of error in terms of parameters of FHE. We empirically verified the proposed method against a set of images at different sizes and showed that our framework accurately computes most of the SURF keypoints in FHE.



### Recognizing and Curating Photo Albums via Event-Specific Image Importance
- **Arxiv ID**: http://arxiv.org/abs/1707.05911v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05911v1)
- **Published**: 2017-07-19 01:03:04+00:00
- **Updated**: 2017-07-19 01:03:04+00:00
- **Authors**: Yufei Wang, Zhe Lin, Xiaohui Shen, Radomir Mech, Gavin Miller, Garrison W. Cottrell
- **Comment**: Accepted as oral in BMVC 2017
- **Journal**: None
- **Summary**: Automatic organization of personal photos is a problem with many real world ap- plications, and can be divided into two main tasks: recognizing the event type of the photo collection, and selecting interesting images from the collection. In this paper, we attempt to simultaneously solve both tasks: album-wise event recognition and image- wise importance prediction. We collected an album dataset with both event type labels and image importance labels, refined from an existing CUFED dataset. We propose a hybrid system consisting of three parts: A siamese network-based event-specific image importance prediction, a Convolutional Neural Network (CNN) that recognizes the event type, and a Long Short-Term Memory (LSTM)-based sequence level event recognizer. We propose an iterative updating procedure for event type and image importance score prediction. We experimentally verified that image importance score prediction and event type recognition can each help the performance of the other.



### Learning Unified Embedding for Apparel Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.05929v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05929v2)
- **Published**: 2017-07-19 03:33:51+00:00
- **Updated**: 2017-08-15 13:36:30+00:00
- **Authors**: Yang Song, Yuan Li, Bo Wu, Chao-Yeh Chen, Xiao Zhang, Hartwig Adam
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: In apparel recognition, specialized models (e.g. models trained for a particular vertical like dresses) can significantly outperform general models (i.e. models that cover a wide range of verticals). Therefore, deep neural network models are often trained separately for different verticals. However, using specialized models for different verticals is not scalable and expensive to deploy. This paper addresses the problem of learning one unified embedding model for multiple object verticals (e.g. all apparel classes) without sacrificing accuracy. The problem is tackled from two aspects: training data and training difficulty. On the training data aspect, we figure out that for a single model trained with triplet loss, there is an accuracy sweet spot in terms of how many verticals are trained together. To ease the training difficulty, a novel learning scheme is proposed by using the output from specialized models as learning targets so that L2 loss can be used instead of triplet loss. This new loss makes the training easier and make it possible for more efficient use of the feature space. The end result is a unified model which can achieve the same retrieval accuracy as a number of separate specialized models, while having the model complexity as one. The effectiveness of our approach is shown in experiments.



### Face Alignment Robust to Pose, Expressions and Occlusions
- **Arxiv ID**: http://arxiv.org/abs/1707.05938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05938v1)
- **Published**: 2017-07-19 05:05:31+00:00
- **Updated**: 2017-07-19 05:05:31+00:00
- **Authors**: Vishnu Naresh Boddeti, Myung-Cheol Roh, Jongju Shin, Takaharu Oguri, Takeo Kanade
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an Ensemble of Robust Constrained Local Models for alignment of faces in the presence of significant occlusions and of any unknown pose and expression. To account for partial occlusions we introduce, Robust Constrained Local Models, that comprises of a deformable shape and local landmark appearance model and reasons over binary occlusion labels. Our occlusion reasoning proceeds by a hypothesize-and-test search over occlusion labels. Hypotheses are generated by Constrained Local Model based shape fitting over randomly sampled subsets of landmark detector responses and are evaluated by the quality of face alignment. To span the entire range of facial pose and expression variations we adopt an ensemble of independent Robust Constrained Local Models to search over a discretized representation of pose and expression. We perform extensive evaluation on a large number of face images, both occluded and unoccluded. We find that our face alignment system trained entirely on facial images captured "in-the-lab" exhibits a high degree of generalization to facial images captured "in-the-wild". Our results are accurate and stable over a wide spectrum of occlusions, pose and expression variations resulting in excellent performance on many real-world face datasets.



### Image Projective Invariants
- **Arxiv ID**: http://arxiv.org/abs/1707.05950v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05950v1)
- **Published**: 2017-07-19 06:25:56+00:00
- **Updated**: 2017-07-19 06:25:56+00:00
- **Authors**: Erbo Li, Hanlin Mo, Dong Xu, Hua Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose relative projective differential invariants (RPDIs) which are invariant to general projective transformations. By using RPDIs and the structural frame of integral invariant, projective weighted moment invariants (PIs) can be constructed very easily. It is first proved that a kind of projective invariants exists in terms of weighted integration of images, with relative differential invariants as the weight functions. Then, some simple instances of PIs are given. In order to ensure the stability and discriminability of PIs, we discuss how to calculate partial derivatives of discrete images more accurately. Since the number of pixels in discrete images before and after the geometric transformation may be different, we design the method to normalize the number of pixels. These ways enhance the performance of PIs. Finally, we carry out some experiments based on synthetic and real image datasets. We choose commonly used moment invariants for comparison. The results indicate that PIs have better performance than other moment invariants in image retrieval and classification. With PIs, one can compare the similarity between images under the projective transformation without knowing the parameters of the transformation, which provides a good tool to shape analysis in image processing, computer vision and pattern recognition.



### When Unsupervised Domain Adaptation Meets Tensor Representations
- **Arxiv ID**: http://arxiv.org/abs/1707.05956v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05956v1)
- **Published**: 2017-07-19 07:11:11+00:00
- **Updated**: 2017-07-19 07:11:11+00:00
- **Authors**: Hao Lu, Lei Zhang, Zhiguo Cao, Wei Wei, Ke Xian, Chunhua Shen, Anton van den Hengel
- **Comment**: 16 pages. Accepted to Proc. Int. Conf. Computer Vision (ICCV 2017)
- **Journal**: None
- **Summary**: Domain adaption (DA) allows machine learning methods trained on data sampled from one distribution to be applied to data sampled from another. It is thus of great practical importance to the application of such methods. Despite the fact that tensor representations are widely used in Computer Vision to capture multi-linear relationships that affect the data, most existing DA methods are applicable to vectors only. This renders them incapable of reflecting and preserving important structure in many problems. We thus propose here a learning-based method to adapt the source and target tensor representations directly, without vectorization. In particular, a set of alignment matrices is introduced to align the tensor representations from both domains into the invariant tensor subspace. These alignment matrices and the tensor subspace are modeled as a joint optimization problem and can be learned adaptively from the data using the proposed alternative minimization scheme. Extensive experiments show that our approach is capable of preserving the discriminative power of the source domain, of resisting the effects of label noise, and works effectively for small sample sizes, and even one-shot DA. We show that our method outperforms the state-of-the-art on the task of cross-domain visual recognition in both efficacy and efficiency, and particularly that it outperforms all comparators when applied to DA of the convolutional activations of deep convolutional networks.



### Multidimensional classification of hippocampal shape features discriminates Alzheimer's disease and mild cognitive impairment from normal aging
- **Arxiv ID**: http://arxiv.org/abs/1707.05961v1
- **DOI**: 10.1016/j.neuroimage.2009.05.036
- **Categories**: **cs.CV**, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1707.05961v1)
- **Published**: 2017-07-19 07:33:02+00:00
- **Updated**: 2017-07-19 07:33:02+00:00
- **Authors**: Emilie Gerardin, Gaël Chételat, Marie Chupin, Rémi Cuingnet, Béatrice Desgranges, Ho-Sung Kim, Marc Niethammer, Bruno Dubois, Stéphane Lehéricy, Line Garnero, Francis Eustache, Olivier Colliot
- **Comment**: Data used in the preparation of this article were obtained from the
  Alzheimer's Disease Neuroimaging Initiative (ADNI) database
- **Journal**: NeuroImage, 47 (4), pp.1476-86, 2009
- **Summary**: We describe a new method to automatically discriminate between patients with Alzheimer's disease (AD) or mild cognitive impairment (MCI) and elderly controls, based on multidimensional classification of hippocampal shape features. This approach uses spherical harmonics (SPHARM) coefficients to model the shape of the hippocampi, which are segmented from magnetic resonance images (MRI) using a fully automatic method that we previously developed. SPHARM coefficients are used as features in a classification procedure based on support vector machines (SVM). The most relevant features for classification are selected using a bagging strategy. We evaluate the accuracy of our method in a group of 23 patients with AD (10 males, 13 females, age $\pm$ standard-deviation (SD) = 73 $\pm$ 6 years, mini-mental score (MMS) = 24.4 $\pm$ 2.8), 23 patients with amnestic MCI (10 males, 13 females, age $\pm$ SD = 74 $\pm$ 8 years, MMS = 27.3 $\pm$ 1.4) and 25 elderly healthy controls (13 males, 12 females, age $\pm$ SD = 64 $\pm$ 8 years), using leave-one-out cross-validation. For AD vs controls, we obtain a correct classification rate of 94%, a sensitivity of 96%, and a specificity of 92%. For MCI vs controls, we obtain a classification rate of 83%, a sensitivity of 83%, and a specificity of 84%. This accuracy is superior to that of hippocampal volumetry and is comparable to recently published SVM-based whole-brain classification methods, which relied on a different strategy. This new method may become a useful tool to assist in the diagnosis of Alzheimer's disease.



### Drone-based Object Counting by Spatially Regularized Regional Proposal Network
- **Arxiv ID**: http://arxiv.org/abs/1707.05972v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05972v3)
- **Published**: 2017-07-19 08:27:17+00:00
- **Updated**: 2017-08-03 00:06:06+00:00
- **Authors**: Meng-Ru Hsieh, Yen-Liang Lin, Winston H. Hsu
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: Existing counting methods often adopt regression-based approaches and cannot precisely localize the target objects, which hinders the further analysis (e.g., high-level understanding and fine-grained classification). In addition, most of prior work mainly focus on counting objects in static environments with fixed cameras. Motivated by the advent of unmanned flying vehicles (i.e., drones), we are interested in detecting and counting objects in such dynamic environments. We propose Layout Proposal Networks (LPNs) and spatial kernels to simultaneously count and localize target objects (e.g., cars) in videos recorded by the drone. Different from the conventional region proposal methods, we leverage the spatial layout information (e.g., cars often park regularly) and introduce these spatially regularized constraints into our network to improve the localization accuracy. To evaluate our counting method, we present a new large-scale car parking lot dataset (CARPK) that contains nearly 90,000 cars captured from different parking lots. To the best of our knowledge, it is the first and the largest drone view dataset that supports object counting, and provides the bounding box annotations.



### Orthogonal and Idempotent Transformations for Learning Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.05974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05974v1)
- **Published**: 2017-07-19 08:35:10+00:00
- **Updated**: 2017-07-19 08:35:10+00:00
- **Authors**: Jingdong Wang, Yajie Xing, Kexin Zhang, Cha Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Identity transformations, used as skip-connections in residual networks, directly connect convolutional layers close to the input and those close to the output in deep neural networks, improving information flow and thus easing the training. In this paper, we introduce two alternative linear transforms, orthogonal transformation and idempotent transformation. According to the definition and property of orthogonal and idempotent matrices, the product of multiple orthogonal (same idempotent) matrices, used to form linear transformations, is equal to a single orthogonal (idempotent) matrix, resulting in that information flow is improved and the training is eased. One interesting point is that the success essentially stems from feature reuse and gradient reuse in forward and backward propagation for maintaining the information during flow and eliminating the gradient vanishing problem because of the express way through skip-connections. We empirically demonstrate the effectiveness of the proposed two transformations: similar performance in single-branch networks and even superior in multi-branch networks in comparison to identity transformations.



### Detecting Parts for Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1707.06005v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06005v2)
- **Published**: 2017-07-19 10:09:59+00:00
- **Updated**: 2017-07-21 21:40:33+00:00
- **Authors**: Nicolas Chesneau, Grégory Rogez, Karteek Alahari, Cordelia Schmid
- **Comment**: BMVC 2017
- **Journal**: None
- **Summary**: In this paper, we propose a new framework for action localization that tracks people in videos and extracts full-body human tubes, i.e., spatio-temporal regions localizing actions, even in the case of occlusions or truncations. This is achieved by training a novel human part detector that scores visible parts while regressing full-body bounding boxes. The core of our method is a convolutional neural network which learns part proposals specific to certain body parts. These are then combined to detect people robustly in each frame. Our tracking algorithm connects the image detections temporally to extract full-body human tubes. We apply our new tube extraction method on the problem of human action localization, on the popular JHMDB dataset, and a very recent challenging dataset DALY (Daily Action Localization in YouTube), showing state-of-the-art results.



### EnzyNet: enzyme classification using 3D convolutional neural networks on spatial representation
- **Arxiv ID**: http://arxiv.org/abs/1707.06017v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1707.06017v1)
- **Published**: 2017-07-19 10:59:29+00:00
- **Updated**: 2017-07-19 10:59:29+00:00
- **Authors**: Afshine Amidi, Shervine Amidi, Dimitrios Vlachakis, Vasileios Megalooikonomou, Nikos Paragios, Evangelia I. Zacharaki
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: During the past decade, with the significant progress of computational power as well as ever-rising data availability, deep learning techniques became increasingly popular due to their excellent performance on computer vision problems. The size of the Protein Data Bank has increased more than 15 fold since 1999, which enabled the expansion of models that aim at predicting enzymatic function via their amino acid composition. Amino acid sequence however is less conserved in nature than protein structure and therefore considered a less reliable predictor of protein function. This paper presents EnzyNet, a novel 3D-convolutional neural networks classifier that predicts the Enzyme Commission number of enzymes based only on their voxel-based spatial structure. The spatial distribution of biochemical properties was also examined as complementary information. The 2-layer architecture was investigated on a large dataset of 63,558 enzymes from the Protein Data Bank and achieved an accuracy of 78.4% by exploiting only the binary representation of the protein shape. Code and datasets are available at https://github.com/shervinea/enzynet.



### Supervising Neural Attention Models for Video Captioning by Human Gaze Data
- **Arxiv ID**: http://arxiv.org/abs/1707.06029v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06029v1)
- **Published**: 2017-07-19 11:44:36+00:00
- **Updated**: 2017-07-19 11:44:36+00:00
- **Authors**: Youngjae Yu, Jongwook Choi, Yeonhwa Kim, Kyung Yoo, Sang-Hun Lee, Gunhee Kim
- **Comment**: In CVPR 2017. 9 pages + supplementary 17 pages
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2017, pp. 490-498
- **Summary**: The attention mechanisms in deep neural networks are inspired by human's attention that sequentially focuses on the most relevant parts of the information over time to generate prediction output. The attention parameters in those models are implicitly trained in an end-to-end manner, yet there have been few trials to explicitly incorporate human gaze tracking to supervise the attention models. In this paper, we investigate whether attention models can benefit from explicit human gaze labels, especially for the task of video captioning. We collect a new dataset called VAS, consisting of movie clips, and corresponding multiple descriptive sentences along with human gaze tracking data. We propose a video captioning model named Gaze Encoding Attention Network (GEAN) that can leverage gaze tracking information to provide the spatial and temporal attention for sentence generation. Through evaluation of language similarity metrics and human assessment via Amazon mechanical Turk, we demonstrate that spatial attentions guided by human gaze data indeed improve the performance of multiple captioning methods. Moreover, we show that the proposed approach achieves the state-of-the-art performance for both gaze prediction and video captioning not only in our VAS dataset but also in standard datasets (e.g. LSMDC and Hollywood2).



### Unmixing dynamic PET images with variable specific binding kinetics
- **Arxiv ID**: http://arxiv.org/abs/1707.09867v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.data-an, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1707.09867v2)
- **Published**: 2017-07-19 12:17:57+00:00
- **Updated**: 2017-12-09 13:33:48+00:00
- **Authors**: Yanna Cruz Cavalcanti, Thomas Oberlin, Nicolas Dobigeon, Simon Stute, Maria Ribeiro, Clovis Tauber
- **Comment**: None
- **Journal**: None
- **Summary**: To analyze dynamic positron emission tomography (PET) images, various generic multivariate data analysis techniques have been considered in the literature, such as principal component analysis (PCA), independent component analysis (ICA), factor analysis and nonnegative matrix factorization (NMF). Nevertheless, these conventional approaches neglect any possible nonlinear variations in the time activity curves describing the kinetic behavior of tissues with specific binding, which limits their ability to recover a reliable, understandable and interpretable description of the data. This paper proposes an alternative analysis paradigm that accounts for spatial fluctuations in the exchange rate of the tracer between a free compartment and a specifically bound ligand compartment. The method relies on the concept of linear unmixing, usually applied on the hyperspectral domain, which combines NMF with a sum-to-one constraint that ensures an exhaustive description of the mixtures. The spatial variability of the signature corresponding to the specific binding tissue is explicitly modeled through a perturbed component. The performance of the method is assessed on both synthetic and real data and is shown to compete favorably when compared to other conventional analysis methods. The proposed method improved both factor estimation and proportions extraction for specific binding. Modeling the variability of the specific binding factor has a strong potential impact for dynamic PET image analysis.



### Modeling the Intra-class Variability for Liver Lesion Detection using a Multi-class Patch-based CNN
- **Arxiv ID**: http://arxiv.org/abs/1707.06053v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06053v2)
- **Published**: 2017-07-19 12:52:38+00:00
- **Updated**: 2017-07-20 05:13:29+00:00
- **Authors**: Maayan Frid-Adar, Idit Diamant, Eyal Klang, Michal Amitai, Jacob Goldberger, Hayit Greenspan
- **Comment**: To be presented at PatchMI: 3rd International Workshop on Patch-based
  Techniques in Medical Imaging, MICCAI 2017
- **Journal**: None
- **Summary**: Automatic detection of liver lesions in CT images poses a great challenge for researchers. In this work we present a deep learning approach that models explicitly the variability within the non-lesion class, based on prior knowledge of the data, to support an automated lesion detection system. A multi-class convolutional neural network (CNN) is proposed to categorize input image patches into sub-categories of boundary and interior patches, the decisions of which are fused to reach a binary lesion vs non-lesion decision. For validation of our system, we use CT images of 132 livers and 498 lesions. Our approach shows highly improved detection results that outperform the state-of-the-art fully convolutional network. Automated computerized tools, as shown in this work, have the potential in the future to support the radiologists towards improved detection.



### On Finding Maximum Cardinality Subset of Vectors with a Constraint on Normalized Squared Length of Vectors Sum
- **Arxiv ID**: http://arxiv.org/abs/1707.06068v1
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.06068v1)
- **Published**: 2017-07-19 13:05:46+00:00
- **Updated**: 2017-07-19 13:05:46+00:00
- **Authors**: Anton V. Eremeev, Alexander V. Kelmanov, Artem V. Pyatkin, Igor A. Ziegler
- **Comment**: To appear in Proceedings of the 6th International Conference on
  Analysis of Images, Social Networks, and Texts (AIST'2017)
- **Journal**: None
- **Summary**: In this paper, we consider the problem of finding a maximum cardinality subset of vectors, given a constraint on the normalized squared length of vectors sum. This problem is closely related to Problem 1 from (Eremeev, Kel'manov, Pyatkin, 2016). The main difference consists in swapping the constraint with the optimization criterion.   We prove that the problem is NP-hard even in terms of finding a feasible solution. An exact algorithm for solving this problem is proposed. The algorithm has a pseudo-polynomial time complexity in the special case of the problem, where the dimension of the space is bounded from above by a constant and the input data are integer. A computational experiment is carried out, where the proposed algorithm is compared to COINBONMIN solver, applied to a mixed integer quadratic programming formulation of the problem. The results of the experiment indicate superiority of the proposed algorithm when the dimension of Euclidean space is low, while the COINBONMIN has an advantage for larger dimensions.



### Deep View-Sensitive Pedestrian Attribute Inference in an end-to-end Model
- **Arxiv ID**: http://arxiv.org/abs/1707.06089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06089v1)
- **Published**: 2017-07-19 13:49:47+00:00
- **Updated**: 2017-07-19 13:49:47+00:00
- **Authors**: M. Saquib Sarfraz, Arne Schumann, Yan Wang, Rainer Stiefelhagen
- **Comment**: accepted BMVC 2017
- **Journal**: None
- **Summary**: Pedestrian attribute inference is a demanding problem in visual surveillance that can facilitate person retrieval, search and indexing. To exploit semantic relations between attributes, recent research treats it as a multi-label image classification task. The visual cues hinting at attributes can be strongly localized and inference of person attributes such as hair, backpack, shorts, etc., are highly dependent on the acquired view of the pedestrian. In this paper we assert this dependence in an end-to-end learning framework and show that a view-sensitive attribute inference is able to learn better attribute predictions. Our proposed model jointly predicts the coarse pose (view) of the pedestrian and learns specialized view-specific multi-label attribute predictions. We show in an extensive evaluation on three challenging datasets (PETA, RAP and WIDER) that our proposed end-to-end view-aware attribute prediction model provides competitive performance and improves on the published state-of-the-art on these datasets.



### Discriminative convolutional Fisher vector network for action recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.06119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06119v1)
- **Published**: 2017-07-19 14:35:28+00:00
- **Updated**: 2017-07-19 14:35:28+00:00
- **Authors**: Petar Palasek, Ioannis Patras
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we propose a novel neural network architecture for the problem of human action recognition in videos. The proposed architecture expresses the processing steps of classical Fisher vector approaches, that is dimensionality reduction by principal component analysis (PCA) projection, Gaussian mixture model (GMM) and Fisher vector descriptor extraction, as network layers. By contrast to other methods where these steps are performed consecutively and the corresponding parameters are learned in an unsupervised manner, having them defined as a single neural network allows us to refine the whole model discriminatively in an end to end fashion. Furthermore, we show that the proposed architecture can be used as a replacement for the fully connected layers in popular convolutional networks achieving a comparable classification performance, or even significantly surpassing the performance of similar architectures while reducing the total number of trainable parameters by a factor of 5. We show that our method achieves significant improvements in comparison to the classical chain.



### Self-paced Convolutional Neural Network for Computer Aided Detection in Medical Imaging Analysis
- **Arxiv ID**: http://arxiv.org/abs/1707.06145v1
- **DOI**: 10.1007/978-3-319-67389-9_25
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1707.06145v1)
- **Published**: 2017-07-19 15:15:36+00:00
- **Updated**: 2017-07-19 15:15:36+00:00
- **Authors**: Xiang Li, Aoxiao Zhong, Ming Lin, Ning Guo, Mu Sun, Arkadiusz Sitek, Jieping Ye, James Thrall, Quanzheng Li
- **Comment**: accepted by 8th International Workshop on Machine Learning in Medical
  Imaging (MLMI 2017)
- **Journal**: None
- **Summary**: Tissue characterization has long been an important component of Computer Aided Diagnosis (CAD) systems for automatic lesion detection and further clinical planning. Motivated by the superior performance of deep learning methods on various computer vision problems, there has been increasing work applying deep learning to medical image analysis. However, the development of a robust and reliable deep learning model for computer-aided diagnosis is still highly challenging due to the combination of the high heterogeneity in the medical images and the relative lack of training samples. Specifically, annotation and labeling of the medical images is much more expensive and time-consuming than other applications and often involves manual labor from multiple domain experts. In this work, we propose a multi-stage, self-paced learning framework utilizing a convolutional neural network (CNN) to classify Computed Tomography (CT) image patches. The key contribution of this approach is that we augment the size of training samples by refining the unlabeled instances with a self-paced learning CNN. By implementing the framework on high performance computing servers including the NVIDIA DGX1 machine, we obtained the experimental result, showing that the self-pace boosted network consistently outperformed the original network even with very scarce manual labels. The performance gain indicates that applications with limited training samples such as medical image analysis can benefit from using the proposed framework.



### Channel Pruning for Accelerating Very Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.06168v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06168v2)
- **Published**: 2017-07-19 15:52:14+00:00
- **Updated**: 2017-08-21 09:44:26+00:00
- **Authors**: Yihui He, Xiangyu Zhang, Jian Sun
- **Comment**: To be appear at ICCV 2017
- **Journal**: None
- **Summary**: In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks.Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5x speed-up along with only 0.3% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2x speed-up respectively, which is significant. Code has been made publicly available.



### Deformable Part-based Fully Convolutional Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1707.06175v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.06175v1)
- **Published**: 2017-07-19 16:03:05+00:00
- **Updated**: 2017-07-19 16:03:05+00:00
- **Authors**: Taylor Mordan, Nicolas Thome, Matthieu Cord, Gilles Henaff
- **Comment**: Accepted to BMVC 2017 (oral)
- **Journal**: None
- **Summary**: Existing region-based object detectors are limited to regions with fixed box geometry to represent objects, even if those are highly non-rectangular. In this paper we introduce DP-FCN, a deep model for object detection which explicitly adapts to shapes of objects with deformable parts. Without additional annotations, it learns to focus on discriminative elements and to align them, and simultaneously brings more invariance for classification and geometric information to refine localization. DP-FCN is composed of three main modules: a Fully Convolutional Network to efficiently maintain spatial resolution, a deformable part-based RoI pooling layer to optimize positions of parts and build invariance, and a deformation-aware localization module explicitly exploiting displacements of parts to improve accuracy of bounding box regression. We experimentally validate our model and show significant gains. DP-FCN achieves state-of-the-art performances of 83.1% and 80.9% on PASCAL VOC 2007 and 2012 with VOC data only.



### Object-Extent Pooling for Weakly Supervised Single-Shot Localization
- **Arxiv ID**: http://arxiv.org/abs/1707.06180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06180v1)
- **Published**: 2017-07-19 16:10:53+00:00
- **Updated**: 2017-07-19 16:10:53+00:00
- **Authors**: Amogh Gudi, Nicolai van Rosmalen, Marco Loog, Jan van Gemert
- **Comment**: In British Machine Vision Conference 2017 (BMVC'17)
- **Journal**: None
- **Summary**: In the face of scarcity in detailed training annotations, the ability to perform object localization tasks in real-time with weak-supervision is very valuable. However, the computational cost of generating and evaluating region proposals is heavy. We adapt the concept of Class Activation Maps (CAM) into the very first weakly-supervised 'single-shot' detector that does not require the use of region proposals. To facilitate this, we propose a novel global pooling technique called Spatial Pyramid Averaged Max (SPAM) pooling for training this CAM-based network for object extent localisation with only weak image-level supervision. We show this global pooling layer possesses a near ideal flow of gradients for extent localization, that offers a good trade-off between the extremes of max and average pooling. Our approach only requires a single network pass and uses a fast-backprojection technique, completely omitting any region proposal steps. To the best of our knowledge, this is the first approach to do so. Due to this, we are able to perform inference in real-time at 35fps, which is an order of magnitude faster than all previous weakly supervised object localization frameworks.



### Domain-adversarial neural networks to address the appearance variability of histopathology images
- **Arxiv ID**: http://arxiv.org/abs/1707.06183v1
- **DOI**: 10.1007/978-3-319-67558-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06183v1)
- **Published**: 2017-07-19 16:21:59+00:00
- **Updated**: 2017-07-19 16:21:59+00:00
- **Authors**: Maxime W. Lafarge, Josien P. W. Pluim, Koen A. J. Eppenhof, Pim Moeskops, Mitko Veta
- **Comment**: MICCAI 2017 Workshop on Deep Learning in Medical Image Analysis
- **Journal**: None
- **Summary**: Preparing and scanning histopathology slides consists of several steps, each with a multitude of parameters. The parameters can vary between pathology labs and within the same lab over time, resulting in significant variability of the tissue appearance that hampers the generalization of automatic image analysis methods. Typically, this is addressed with ad-hoc approaches such as staining normalization that aim to reduce the appearance variability. In this paper, we propose a systematic solution based on domain-adversarial neural networks. We hypothesize that removing the domain information from the model representation leads to better generalization. We tested our hypothesis for the problem of mitosis detection in breast cancer histopathology images and made a comparative analysis with two other approaches. We show that combining color augmentation with domain-adversarial training is a better alternative than standard approaches to improve the generalization of deep learning methods.



### Deformable Registration through Learning of Context-Specific Metric Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1707.06263v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.06263v1)
- **Published**: 2017-07-19 19:06:38+00:00
- **Updated**: 2017-07-19 19:06:38+00:00
- **Authors**: Enzo Ferrante, Puneet K Dokania, Rafael Marini, Nikos Paragios
- **Comment**: Accepted for publication in the 8th International Workshop on Machine
  Learning in Medical Imaging (MLMI 2017), in conjunction with MICCAI 2017
- **Journal**: None
- **Summary**: We propose a novel weakly supervised discriminative algorithm for learning context specific registration metrics as a linear combination of conventional similarity measures. Conventional metrics have been extensively used over the past two decades and therefore both their strengths and limitations are known. The challenge is to find the optimal relative weighting (or parameters) of different metrics forming the similarity measure of the registration algorithm. Hand-tuning these parameters would result in sub optimal solutions and quickly become infeasible as the number of metrics increases. Furthermore, such hand-crafted combination can only happen at global scale (entire volume) and therefore will not be able to account for the different tissue properties. We propose a learning algorithm for estimating these parameters locally, conditioned to the data semantic classes. The objective function of our formulation is a special case of non-convex function, difference of convex function, which we optimize using the concave convex procedure. As a proof of concept, we show the impact of our approach on three challenging datasets for different anatomical structures and modalities.



### Shape Generation using Spatially Partitioned Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1707.06267v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1707.06267v1)
- **Published**: 2017-07-19 19:32:47+00:00
- **Updated**: 2017-07-19 19:32:47+00:00
- **Authors**: Matheus Gadelha, Subhransu Maji, Rui Wang
- **Comment**: To appear at BMVC 2017
- **Journal**: None
- **Summary**: We propose a method to generate 3D shapes using point clouds. Given a point-cloud representation of a 3D shape, our method builds a kd-tree to spatially partition the points. This orders them consistently across all shapes, resulting in reasonably good correspondences across all shapes. We then use PCA analysis to derive a linear shape basis across the spatially partitioned points, and optimize the point ordering by iteratively minimizing the PCA reconstruction error. Even with the spatial sorting, the point clouds are inherently noisy and the resulting distribution over the shape coefficients can be highly multi-modal. We propose to use the expressive power of neural networks to learn a distribution over the shape coefficients in a generative-adversarial framework. Compared to 3D shape generative models trained on voxel-representations, our point-based method is considerably more light-weight and scalable, with little loss of quality. It also outperforms simpler linear factor models such as Probabilistic PCA, both qualitatively and quantitatively, on a number of categories from the ShapeNet dataset. Furthermore, our method can easily incorporate other point attributes such as normal and color information, an additional advantage over voxel-based representations.



### Pose-Invariant Face Alignment with a Single CNN
- **Arxiv ID**: http://arxiv.org/abs/1707.06286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06286v1)
- **Published**: 2017-07-19 20:34:08+00:00
- **Updated**: 2017-07-19 20:34:08+00:00
- **Authors**: Amin Jourabloo, Mao Ye, Xiaoming Liu, Liu Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Face alignment has witnessed substantial progress in the last decade. One of the recent focuses has been aligning a dense 3D face shape to face images with large head poses. The dominant technology used is based on the cascade of regressors, e.g., CNN, which has shown promising results. Nonetheless, the cascade of CNNs suffers from several drawbacks, e.g., lack of end-to-end training, hand-crafted features and slow training speed. To address these issues, we propose a new layer, named visualization layer, that can be integrated into the CNN architecture and enables joint optimization with different loss functions. Extensive evaluation of the proposed method on multiple datasets demonstrates state-of-the-art accuracy, while reducing the training time by more than half compared to the typical cascade of CNNs. In addition, we compare multiple CNN architectures with the visualization layer to further demonstrate the advantage of its utilization.



### STag: A Stable Fiducial Marker System
- **Arxiv ID**: http://arxiv.org/abs/1707.06292v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06292v2)
- **Published**: 2017-07-19 20:53:27+00:00
- **Updated**: 2019-07-01 10:35:14+00:00
- **Authors**: Burak Benligiray, Cihan Topal, Cuneyt Akinlar
- **Comment**: Accepted to be published in Image and Vision Computing
- **Journal**: None
- **Summary**: Fiducial markers provide better-defined features than the ones naturally available in the scene. For this reason, they are widely utilized in computer vision applications where reliable pose estimation is required. Factors such as imaging noise and subtle changes in illumination induce jitter on the estimated pose. Jitter impairs robustness in vision and robotics applications, and deteriorates the sense of presence and immersion in AR/VR applications. In this paper, we propose STag, a fiducial marker system that provides stable pose estimation. STag is designed to be robust against jitter factors, thus sustains pose stability better than the existing solutions. This is achieved by utilizing geometric features that can be localized more repeatably. The outer square border of the marker is used for detection and homography estimation. This is followed by a novel homography refinement step using the inner circular border. After refinement, the pose can be estimated stably and robustly across viewing conditions. These features are demonstrated with a comprehensive set of experiments, including comparisons with the state of the art fiducial marker systems.



### Fast, Simple Calcium Imaging Segmentation with Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.06314v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1707.06314v1)
- **Published**: 2017-07-19 22:27:29+00:00
- **Updated**: 2017-07-19 22:27:29+00:00
- **Authors**: Aleksander Klibisz, Derek Rose, Matthew Eicholtz, Jay Blundon, Stanislav Zakharenko
- **Comment**: Accepted to 3rd Workshop on Deep Learning in Medical Image Analysis
  (http://cs.adelaide.edu.au/~dlmia3/)
- **Journal**: None
- **Summary**: Calcium imaging is a technique for observing neuron activity as a series of images showing indicator fluorescence over time. Manually segmenting neurons is time-consuming, leading to research on automated calcium imaging segmentation (ACIS). We evaluated several deep learning models for ACIS on the Neurofinder competition datasets and report our best model: U-Net2DS, a fully convolutional network that operates on 2D mean summary images. U-Net2DS requires minimal domain-specific pre/post-processing and parameter adjustment, and predictions are made on full $512\times512$ images at $\approx$9K images per minute. It ranks third in the Neurofinder competition ($F_1=0.569$) and is the best model to exclusively use deep learning. We also demonstrate useful segmentations on data from outside the competition. The model's simplicity, speed, and quality results make it a practical choice for ACIS and a strong baseline for more complex models in the future.



### DenseNet for Dense Flow
- **Arxiv ID**: http://arxiv.org/abs/1707.06316v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1707.06316v1)
- **Published**: 2017-07-19 22:37:46+00:00
- **Updated**: 2017-07-19 22:37:46+00:00
- **Authors**: Yi Zhu, Shawn Newsam
- **Comment**: Accepted at ICIP 2017
- **Journal**: None
- **Summary**: Classical approaches for estimating optical flow have achieved rapid progress in the last decade. However, most of them are too slow to be applied in real-time video analysis. Due to the great success of deep learning, recent work has focused on using CNNs to solve such dense prediction problems. In this paper, we investigate a new deep architecture, Densely Connected Convolutional Networks (DenseNet), to learn optical flow. This specific architecture is ideal for the problem at hand as it provides shortcut connections throughout the network, which leads to implicit deep supervision. We extend current DenseNet to a fully convolutional network to learn motion estimation in an unsupervised manner. Evaluation results on three standard benchmarks demonstrate that DenseNet is a better fit than other widely adopted CNN architectures for optical flow estimation.



### Learning Visually Grounded Sentence Representations
- **Arxiv ID**: http://arxiv.org/abs/1707.06320v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.06320v2)
- **Published**: 2017-07-19 23:12:57+00:00
- **Updated**: 2018-06-04 20:19:28+00:00
- **Authors**: Douwe Kiela, Alexis Conneau, Allan Jabri, Maximilian Nickel
- **Comment**: Published at NAACL-18
- **Journal**: None
- **Summary**: We introduce a variety of models, trained on a supervised image captioning corpus to predict the image features for a given caption, to perform sentence representation grounding. We train a grounded sentence encoder that achieves good performance on COCO caption and image retrieval and subsequently show that this encoder can successfully be transferred to various NLP tasks, with improved performance over text-only models. Lastly, we analyze the contribution of grounding, and show that word embeddings learned by this system outperform non-grounded ones.



### Automatic Segmentation of Retinal Vasculature
- **Arxiv ID**: http://arxiv.org/abs/1707.06323v1
- **DOI**: 10.1109/ICASSP.2017.7952283
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06323v1)
- **Published**: 2017-07-19 23:29:13+00:00
- **Updated**: 2017-07-19 23:29:13+00:00
- **Authors**: Renoh Johnson Chalakkal, Waleed Abdulla
- **Comment**: Published at IEEE International Conference on Acoustics Speech and
  Signal Processing (ICASSP), 2017
- **Journal**: IEEE International Conference on Acoustics Speech and Signal
  Processing (ICASSP), page: 886-890, 2017
- **Summary**: Segmentation of retinal vessels from retinal fundus images is the key step in the automatic retinal image analysis. In this paper, we propose a new unsupervised automatic method to segment the retinal vessels from retinal fundus images. Contrast enhancement and illumination correction are carried out through a series of image processing steps followed by adaptive histogram equalization and anisotropic diffusion filtering. This image is then converted to a gray scale using weighted scaling. The vessel edges are enhanced by boosting the detail curvelet coefficients. Optic disk pixels are removed before applying fuzzy C-mean classification to avoid the misclassification. Morphological operations and connected component analysis are applied to obtain the segmented retinal vessels. The performance of the proposed method is evaluated using DRIVE database to be able to compare with other state-of-art supervised and unsupervised methods. The overall segmentation accuracy of the proposed method is 95.18% which outperforms the other algorithms.



