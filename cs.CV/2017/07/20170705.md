# Arxiv Papers in cs.CV on 2017-07-05
### A Survey of Recent Advances in CNN-based Single Image Crowd Counting and Density Estimation
- **Arxiv ID**: http://arxiv.org/abs/1707.01202v1
- **DOI**: 10.1016/j.patrec.2017.07.007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01202v1)
- **Published**: 2017-07-05 03:05:17+00:00
- **Updated**: 2017-07-05 03:05:17+00:00
- **Authors**: Vishwanath A. Sindagi, Vishal M. Patel
- **Comment**: 16 pages, 17 figures
- **Journal**: None
- **Summary**: Estimating count and density maps from crowd images has a wide range of applications such as video surveillance, traffic monitoring, public safety and urban planning. In addition, techniques developed for crowd counting can be applied to related tasks in other fields of study such as cell microscopy, vehicle counting and environmental survey. The task of crowd counting and density map estimation is riddled with many challenges such as occlusions, non-uniform density, intra-scene and inter-scene variations in scale and perspective. Nevertheless, over the last few years, crowd count analysis has evolved from earlier methods that are often limited to small variations in crowd density and scales to the current state-of-the-art methods that have developed the ability to perform successfully on a wide range of scenarios. The success of crowd counting methods in the recent years can be largely attributed to deep learning and publications of challenging datasets. In this paper, we provide a comprehensive survey of recent Convolutional Neural Network (CNN) based approaches that have demonstrated significant improvements over earlier methods that rely largely on hand-crafted representations. First, we briefly review the pioneering methods that use hand-crafted representations and then we delve in detail into the deep learning-based approaches and recently published datasets. Furthermore, we discuss the merits and drawbacks of existing CNN-based approaches and identify promising avenues of research in this rapidly evolving field.



### Data-Driven Sparse Structure Selection for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.01213v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1707.01213v3)
- **Published**: 2017-07-05 05:21:50+00:00
- **Updated**: 2018-09-05 05:14:37+00:00
- **Authors**: Zehao Huang, Naiyan Wang
- **Comment**: ECCV Camera ready version
- **Journal**: None
- **Summary**: Deep convolutional neural networks have liberated its extraordinary power on various tasks. However, it is still very challenging to deploy state-of-the-art models into real-world applications due to their high computational complexity. How can we design a compact and effective network without massive experiments and expert knowledge? In this paper, we propose a simple and effective framework to learn and prune deep models in an end-to-end manner. In our framework, a new type of parameter -- scaling factor is first introduced to scale the outputs of specific structures, such as neurons, groups or residual blocks. Then we add sparsity regularizations on these factors, and solve this optimization problem by a modified stochastic Accelerated Proximal Gradient (APG) method. By forcing some of the factors to zero, we can safely remove the corresponding structures, thus prune the unimportant parts of a CNN. Comparing with other structure selection methods that may need thousands of trials or iterative fine-tuning, our method is trained fully end-to-end in one training pass without bells and whistles. We evaluate our method, Sparse Structure Selection with several state-of-the-art CNNs, and demonstrate very promising results with adaptive depth and width selection.



### Like What You Like: Knowledge Distill via Neuron Selectivity Transfer
- **Arxiv ID**: http://arxiv.org/abs/1707.01219v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1707.01219v2)
- **Published**: 2017-07-05 05:44:02+00:00
- **Updated**: 2017-12-18 22:35:22+00:00
- **Authors**: Zehao Huang, Naiyan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Despite deep neural networks have demonstrated extraordinary power in various applications, their superior performances are at expense of high storage and computational costs. Consequently, the acceleration and compression of neural networks have attracted much attention recently. Knowledge Transfer (KT), which aims at training a smaller student network by transferring knowledge from a larger teacher model, is one of the popular solutions. In this paper, we propose a novel knowledge transfer method by treating it as a distribution matching problem. Particularly, we match the distributions of neuron selectivity patterns between teacher and student networks. To achieve this goal, we devise a new KT loss function by minimizing the Maximum Mean Discrepancy (MMD) metric between these distributions. Combined with the original loss function, our method can significantly improve the performance of student networks. We validate the effectiveness of our method across several datasets, and further combine it with other KT methods to explore the best possible results. Last but not least, we fine-tune the model to other tasks such as object detection. The results are also encouraging, which confirm the transferability of the learned features.



### DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer
- **Arxiv ID**: http://arxiv.org/abs/1707.01220v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1707.01220v2)
- **Published**: 2017-07-05 05:47:11+00:00
- **Updated**: 2017-12-18 22:44:44+00:00
- **Authors**: Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We have witnessed rapid evolution of deep neural network architecture design in the past years. These latest progresses greatly facilitate the developments in various areas such as computer vision and natural language processing. However, along with the extraordinary performance, these state-of-the-art models also bring in expensive computational cost. Directly deploying these models into applications with real-time requirement is still infeasible. Recently, Hinton etal. have shown that the dark knowledge within a powerful teacher model can significantly help the training of a smaller and faster student network. These knowledge are vastly beneficial to improve the generalization ability of the student model. Inspired by their work, we introduce a new type of knowledge -- cross sample similarities for model compression and acceleration. This knowledge can be naturally derived from deep metric learning model. To transfer them, we bring the "learning to rank" technique into deep metric learning formulation. We test our proposed DarkRank method on various metric learning tasks including pedestrian re-identification, image retrieval and image clustering. The results are quite encouraging. Our method can improve over the baseline method by a large margin. Moreover, it is fully compatible with other existing methods. When combined, the performance can be further boosted.



### Copy-move Forgery Detection based on Convolutional Kernel Network
- **Arxiv ID**: http://arxiv.org/abs/1707.01221v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1707.01221v1)
- **Published**: 2017-07-05 05:54:18+00:00
- **Updated**: 2017-07-05 05:54:18+00:00
- **Authors**: Yaqi Liu, Qingxiao Guan, Xianfeng Zhao
- **Comment**: 26 pages, 8 figures, submitted to Multimedia Tools and Applications
- **Journal**: None
- **Summary**: In this paper, a copy-move forgery detection method based on Convolutional Kernel Network is proposed. Different from methods based on conventional hand-crafted features, Convolutional Kernel Network is a kind of data-driven local descriptor with the deep convolutional structure. Thanks to the development of deep learning theories and widely available datasets, the data-driven methods can achieve competitive performance on different conditions for its excellent discriminative capability. Besides, our Convolutional Kernel Network is reformulated as a series of matrix computations and convolutional operations which are easy to parallelize and accelerate by GPU, leading to high efficiency. Then, appropriate preprocessing and postprocessing for Convolutional Kernel Network are adopted to achieve copy-move forgery detection. Particularly, a segmentation-based keypoints distribution strategy is proposed and a GPU-based adaptive oversegmentation method is adopted. Numerous experiments are conducted to demonstrate the effectiveness and robustness of the GPU version of Convolutional Kernel Network, and the state-of-the-art performance of the proposed copy-move forgery detection method based on Convolutional Kernel Network.



### Exploration of object recognition from 3D point cloud
- **Arxiv ID**: http://arxiv.org/abs/1707.01243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01243v1)
- **Published**: 2017-07-05 07:43:00+00:00
- **Updated**: 2017-07-05 07:43:00+00:00
- **Authors**: Lin Duan
- **Comment**: None
- **Journal**: None
- **Summary**: We present our latest experiment results of object recognition from 3D point cloud data collected through moving car.



### Laplacian-Steered Neural Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1707.01253v2
- **DOI**: 10.1145/3123266.3123425
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01253v2)
- **Published**: 2017-07-05 08:10:41+00:00
- **Updated**: 2017-07-30 15:46:17+00:00
- **Authors**: Shaohua Li, Xinxing Xu, Liqiang Nie, Tat-Seng Chua
- **Comment**: Accepted by the ACM Multimedia Conference (MM) 2017. 9 pages, 65
  figures
- **Journal**: None
- **Summary**: Neural Style Transfer based on Convolutional Neural Networks (CNN) aims to synthesize a new image that retains the high-level structure of a content image, rendered in the low-level texture of a style image. This is achieved by constraining the new image to have high-level CNN features similar to the content image, and lower-level CNN features similar to the style image. However in the traditional optimization objective, low-level features of the content image are absent, and the low-level features of the style image dominate the low-level detail structures of the new image. Hence in the synthesized image, many details of the content image are lost, and a lot of inconsistent and unpleasing artifacts appear. As a remedy, we propose to steer image synthesis with a novel loss function: the Laplacian loss. The Laplacian matrix ("Laplacian" in short), produced by a Laplacian operator, is widely used in computer vision to detect edges and contours. The Laplacian loss measures the difference of the Laplacians, and correspondingly the difference of the detail structures, between the content image and a new image. It is flexible and compatible with the traditional style transfer constraints. By incorporating the Laplacian loss, we obtain a new optimization objective for neural style transfer named Lapstyle. Minimizing this objective will produce a stylized image that better preserves the detail structures of the content image and eliminates the artifacts. Experiments show that Lapstyle produces more appealing stylized images with less artifacts, without compromising their "stylishness".



### A deep learning architecture for temporal sleep stage classification using multivariate and multimodal time series
- **Arxiv ID**: http://arxiv.org/abs/1707.03321v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1707.03321v2)
- **Published**: 2017-07-05 08:29:36+00:00
- **Updated**: 2017-11-27 09:37:28+00:00
- **Authors**: Stanislas Chambon, Mathieu Galtier, Pierrick Arnal, Gilles Wainrib, Alexandre Gramfort
- **Comment**: None
- **Journal**: None
- **Summary**: Sleep stage classification constitutes an important preliminary exam in the diagnosis of sleep disorders. It is traditionally performed by a sleep expert who assigns to each 30s of signal a sleep stage, based on the visual inspection of signals such as electroencephalograms (EEG), electrooculograms (EOG), electrocardiograms (ECG) and electromyograms (EMG). We introduce here the first deep learning approach for sleep stage classification that learns end-to-end without computing spectrograms or extracting hand-crafted features, that exploits all multivariate and multimodal Polysomnography (PSG) signals (EEG, EMG and EOG), and that can exploit the temporal context of each 30s window of data. For each modality the first layer learns linear spatial filters that exploit the array of sensors to increase the signal-to-noise ratio, and the last layer feeds the learnt representation to a softmax classifier. Our model is compared to alternative automatic approaches based on convolutional networks or decisions trees. Results obtained on 61 publicly available PSG records with up to 20 EEG channels demonstrate that our network architecture yields state-of-the-art performance. Our study reveals a number of insights on the spatio-temporal distribution of the signal of interest: a good trade-off for optimal classification performance measured with balanced accuracy is to use 6 EEG with 2 EOG (left and right) and 3 EMG chin channels. Also exploiting one minute of data before and after each data segment offers the strongest improvement when a limited number of channels is available. As sleep experts, our system exploits the multivariate and multimodal nature of PSG signals in order to deliver state-of-the-art classification performance with a small computational cost.



### Learning-based Image Enhancement for Visual Odometry in Challenging HDR Environments
- **Arxiv ID**: http://arxiv.org/abs/1707.01274v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01274v3)
- **Published**: 2017-07-05 09:26:34+00:00
- **Updated**: 2018-04-09 10:59:20+00:00
- **Authors**: Ruben Gomez-Ojeda, Zichao Zhang, Javier Gonzalez-Jimenez, Davide Scaramuzza
- **Comment**: None
- **Journal**: IEEE Conference on Robotics and Automation (ICRA), 2018
- **Summary**: One of the main open challenges in visual odometry (VO) is the robustness to difficult illumination conditions or high dynamic range (HDR) environments. The main difficulties in these situations come from both the limitations of the sensors and the inability to perform a successful tracking of interest points because of the bold assumptions in VO, such as brightness constancy. We address this problem from a deep learning perspective, for which we first fine-tune a Deep Neural Network (DNN) with the purpose of obtaining enhanced representations of the sequences for VO. Then, we demonstrate how the insertion of Long Short Term Memory (LSTM) allows us to obtain temporally consistent sequences, as the estimation depends on previous states. However, the use of very deep networks does not allow the insertion into a real-time VO framework; therefore, we also propose a Convolutional Neural Network (CNN) of reduced size capable of performing faster. Finally, we validate the enhanced representations by evaluating the sequences produced by the two architectures in several state-of-art VO algorithms, such as ORB-SLAM and DSO.



### Computer methods for 3D motion tracking in real-time
- **Arxiv ID**: http://arxiv.org/abs/1707.01745v1
- **DOI**: None
- **Categories**: **cs.CV**, D.1.3; I.2.10; I.4.8; I.5.4; I.3.3
- **Links**: [PDF](http://arxiv.org/pdf/1707.01745v1)
- **Published**: 2017-07-05 10:07:19+00:00
- **Updated**: 2017-07-05 10:07:19+00:00
- **Authors**: Bogusław Rymut
- **Comment**: PhD Thesis "Komputerowe algorytmy ekstrakcji i \'sledzenia obiekt\'ow
  w czasie rzeczywistym" (in Polish) Supervisor: PhD Bogdan Kwolek, Prof. AGH
  Key words: model-based 3D motion tracking, parallel computing, computer
  vision
- **Journal**: None
- **Summary**: This thesis is devoted to marker-less 3D human motion tracking in calibrated and synchronized multicamera systems. Pose estimation is based on a 3D model, which is transformed into the image plane and then rendered. Owing to elaborated techniques the tracking of the full body has been achieved in real-time via dynamic optimization or dynamic Bayesian filtering. The objective function of a particle swarm optimization algorithm and the observation model of a particle filter are based on matching between the rendered 3D models in the required poses and image features representing the extracted person. In such an approach the main part of the computational overload is associated with the rendering of 3D models in hypothetical poses as well as determination of value of objective function. Effective methods for rendering of 3D models in real-time with support of OpenGL as well as parallel methods for determining the objective function on the GPU were developed. The elaborated solutions permit 3D tracking of full body motion in real-time.



### R-PHOC: Segmentation-Free Word Spotting using CNN
- **Arxiv ID**: http://arxiv.org/abs/1707.01294v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01294v1)
- **Published**: 2017-07-05 10:08:51+00:00
- **Updated**: 2017-07-05 10:08:51+00:00
- **Authors**: Suman Ghosh, Ernest Valveny
- **Comment**: Accepted in ICDAR'2017
- **Journal**: None
- **Summary**: This paper proposes a region based convolutional neural network for segmentation-free word spotting. Our net- work takes as input an image and a set of word candidate bound- ing boxes and embeds all bounding boxes into an embedding space, where word spotting can be casted as a simple nearest neighbour search between the query representation and each of the candidate bounding boxes. We make use of PHOC embedding as it has previously achieved significant success in segmentation- based word spotting. Word candidates are generated using a simple procedure based on grouping connected components using some spatial constraints. Experiments show that R-PHOC which operates on images directly can improve the current state-of- the-art in the standard GW dataset and performs as good as PHOCNET in some cases designed for segmentation based word spotting.



### Fast Multi-frame Stereo Scene Flow with Motion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.01307v1
- **DOI**: 10.1109/CVPR.2017.729
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01307v1)
- **Published**: 2017-07-05 10:42:34+00:00
- **Updated**: 2017-07-05 10:42:34+00:00
- **Authors**: Tatsunori Taniai, Sudipta N. Sinha, Yoichi Sato
- **Comment**: 15 pages. To appear at IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2017). Our results were submitted to KITTI 2015 Stereo
  Scene Flow Benchmark in November 2016
- **Journal**: 2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), Honolulu, HI, USA, 2017, pp. 6891-6900
- **Summary**: We propose a new multi-frame method for efficiently computing scene flow (dense depth and optical flow) and camera ego-motion for a dynamic scene observed from a moving stereo camera rig. Our technique also segments out moving objects from the rigid scene. In our method, we first estimate the disparity map and the 6-DOF camera motion using stereo matching and visual odometry. We then identify regions inconsistent with the estimated camera motion and compute per-pixel optical flow only at these regions. This flow proposal is fused with the camera motion-based flow proposal using fusion moves to obtain the final optical flow and motion segmentation. This unified framework benefits all four tasks - stereo, optical flow, visual odometry and motion segmentation leading to overall higher accuracy and efficiency. Our method is currently ranked third on the KITTI 2015 scene flow benchmark. Furthermore, our CPU implementation runs in 2-3 seconds per frame which is 1-3 orders of magnitude faster than the top six methods. We also report a thorough evaluation on challenging Sintel sequences with fast camera and object motion, where our method consistently outperforms OSF [Menze and Geiger, 2015], which is currently ranked second on the KITTI benchmark.



### Benchmarking Denoising Algorithms with Real Photographs
- **Arxiv ID**: http://arxiv.org/abs/1707.01313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01313v1)
- **Published**: 2017-07-05 10:51:59+00:00
- **Updated**: 2017-07-05 10:51:59+00:00
- **Authors**: Tobias Plötz, Stefan Roth
- **Comment**: To appear at CVPR17. See our website (www.visinf.tu-darmstadt.de) for
  a version with high-resolution images
- **Journal**: None
- **Summary**: Lacking realistic ground truth data, image denoising techniques are traditionally evaluated on images corrupted by synthesized i.i.d. Gaussian noise. We aim to obviate this unrealistic setting by developing a methodology for benchmarking denoising techniques on real photographs. We capture pairs of images with different ISO values and appropriately adjusted exposure times, where the nearly noise-free low-ISO image serves as reference. To derive the ground truth, careful post-processing is needed. We correct spatial misalignment, cope with inaccuracies in the exposure parameters through a linear intensity transform based on a novel heteroscedastic Tobit regression model, and remove residual low-frequency bias that stems, e.g., from minor illumination changes. We then capture a novel benchmark dataset, the Darmstadt Noise Dataset (DND), with consumer cameras of differing sensor sizes. One interesting finding is that various recent techniques that perform well on synthetic noise are clearly outperformed by BM3D on photographs with real noise. Our benchmark delineates realistic evaluation scenarios that deviate strongly from those commonly used in the scientific literature.



### Robust Multi-Image HDR Reconstruction for the Modulo Camera
- **Arxiv ID**: http://arxiv.org/abs/1707.01317v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01317v1)
- **Published**: 2017-07-05 11:00:36+00:00
- **Updated**: 2017-07-05 11:00:36+00:00
- **Authors**: Florian Lang, Tobias Plötz, Stefan Roth
- **Comment**: to appear at the 39th German Conference on Pattern Recognition (GCPR)
  2017
- **Journal**: None
- **Summary**: Photographing scenes with high dynamic range (HDR) poses great challenges to consumer cameras with their limited sensor bit depth. To address this, Zhao et al. recently proposed a novel sensor concept - the modulo camera - which captures the least significant bits of the recorded scene instead of going into saturation. Similar to conventional pipelines, HDR images can be reconstructed from multiple exposures, but significantly fewer images are needed than with a typical saturating sensor. While the concept is appealing, we show that the original reconstruction approach assumes noise-free measurements and quickly breaks down otherwise. To address this, we propose a novel reconstruction algorithm that is robust to image noise and produces significantly fewer artifacts. We theoretically analyze correctness as well as limitations, and show that our approach significantly outperforms the baseline on real data.



### A dataset for Computer-Aided Detection of Pulmonary Embolism in CTA images
- **Arxiv ID**: http://arxiv.org/abs/1707.01330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01330v1)
- **Published**: 2017-07-05 11:35:27+00:00
- **Updated**: 2017-07-05 11:35:27+00:00
- **Authors**: Mojtaba Masoudi, Hamidreza Pourreza, Mahdi Saadatmand Tarzjan, Fateme Shafiee Zargar, Masoud Pezeshki Rad, Noushin Eftekhari
- **Comment**: None
- **Journal**: None
- **Summary**: Todays, researchers in the field of Pulmonary Embolism (PE) analysis need to use a publicly available dataset to assess and compare their methods. Different systems have been designed for the detection of pulmonary embolism (PE), but none of them have used any public datasets. All papers have used their own private dataset. In order to fill this gap, we have collected 5160 slices of computed tomography angiography (CTA) images acquired from 20 patients, and after labeling the image by experts in this field, we provided a reliable dataset which is now publicly available. In some situation, PE detection can be difficult, for example when it occurs in the peripheral branches or when patients have pulmonary diseases (such as parenchymal disease). Therefore, the efficiency of CAD systems highly depends on the dataset. In the given dataset, 66% of PE are located in peripheral branches, and different pulmonary diseases are also included.



### Generative diffeomorphic atlas construction from brain and spinal cord MRI data
- **Arxiv ID**: http://arxiv.org/abs/1707.01342v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01342v1)
- **Published**: 2017-07-05 12:10:44+00:00
- **Updated**: 2017-07-05 12:10:44+00:00
- **Authors**: Claudia Blaiotta, Patrick Freund, M. Jorge Cardoso, John Ashburner
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we will focus on the potential and on the challenges associated with the development of an integrated brain and spinal cord modelling framework for processing MR neuroimaging data. The aim of the work is to explore how a hierarchical generative model of imaging data, which captures simultaneously the distribution of signal intensities and the variability of anatomical shapes across a large population of subjects, can serve to quantitatively investigate, in vivo, the morphology of the central nervous system (CNS). In fact, the generality of the proposed Bayesian approach, which extends the hierarchical structure of the segmentation method implemented in the SPM software, allows processing simultaneously information relative to different compartments of the CNS, namely the brain and the spinal cord, without having to resort to organ specific solutions (e.g. tools optimised only for the brain, or only for the spinal cord), which are inevitably harder to integrate and generalise.



### Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object Rotation
- **Arxiv ID**: http://arxiv.org/abs/1707.01357v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1707.01357v1)
- **Published**: 2017-07-05 12:28:43+00:00
- **Updated**: 2017-07-05 12:28:43+00:00
- **Authors**: Stefan Lattner, Maarten Grachten
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Content-invariance in mapping codes learned by GAEs is a useful feature for various relation learning tasks. In this paper we show that the content-invariance of mapping codes for images of 2D and 3D rotated objects can be substantially improved by extending the standard GAE loss (symmetric reconstruction error) with a regularization term that penalizes the symmetric cross-reconstruction error. This error term involves reconstruction of pairs with mapping codes obtained from other pairs exhibiting similar transformations. Although this would principally require knowledge of the transformations exhibited by training pairs, our experiments show that a bootstrapping approach can sidestep this issue, and that the regularization term can effectively be used in an unsupervised setting.



### Development & Implementation of the Trigger for a Short-baseline Reactor Antineutrino Experiment (SoLid)
- **Arxiv ID**: http://arxiv.org/abs/1707.01394v1
- **DOI**: None
- **Categories**: **physics.ins-det**, cs.CV, hep-ex
- **Links**: [PDF](http://arxiv.org/pdf/1707.01394v1)
- **Published**: 2017-07-05 13:52:49+00:00
- **Updated**: 2017-07-05 13:52:49+00:00
- **Authors**: Lukas On Arnold
- **Comment**: University of Bristol, MSc 2017
- **Journal**: None
- **Summary**: SoLid, located at SCK-CEN in Mol, Belgium, is a reactor antineutrino experiment at a very short baseline of 5.5 - 10m aiming at the search for sterile neutrinos and for high precision measurement of the neutrino energy spectrum of Uranium-235. It uses a novel approach using Lithium-6 sheets and PVT cubes as scintillators for tagging the Inverse Beta-Decay products (neutron and positron). Being located overground and close to the BR2 research reactor, the experiment faces a large amount of backgrounds. Efficient real-time background and noise rejection is essential in order to increase the signal-background ratio for precise oscillation measurement and decrease data production to a rate which can be handled by the online software. Therefore, a reliable distinction between the neutrons and background signals is crucial. This can be performed online with a dedicated firmware trigger. A peak counting algorithm and an algorithm measuring time over threshold have been identified as performing well both in terms of efficiency and fake rate, and have been implemented onto an FPGA. After having introduced the experimental and theoretical background of neutrino oscillation physics, as well as SoLid's detector technology, read-out system and trigger scheme, the thesis presents the design of the firmware neutron trigger implemented by applying machine learning methods.



### Towards lightweight convolutional neural networks for object detection
- **Arxiv ID**: http://arxiv.org/abs/1707.01395v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1707.01395v3)
- **Published**: 2017-07-05 13:53:00+00:00
- **Updated**: 2017-10-05 12:08:49+00:00
- **Authors**: Dmitriy Anisimov, Tatiana Khanova
- **Comment**: Submitted to the International Workshop on Traffic and Street
  Surveillance for Safety and Security (IWT4S) in conjunction with the 14th
  IEEE International Conference on Advanced Video and Signal based Surveillance
  (AVSS 2017)
- **Journal**: None
- **Summary**: We propose model with larger spatial size of feature maps and evaluate it on object detection task. With the goal to choose the best feature extraction network for our model we compare several popular lightweight networks. After that we conduct a set of experiments with channels reduction algorithms in order to accelerate execution. Our vehicle detection models are accurate, fast and therefore suit for embedded visual applications. With only 1.5 GFLOPs our best model gives 93.39 AP on validation subset of challenging DETRAC dataset. The smallest of our models is the first to achieve real-time inference speed on CPU with reasonable accuracy drop to 91.43 AP.



### AlignGAN: Learning to Align Cross-Domain Images with Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.01400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01400v1)
- **Published**: 2017-07-05 14:02:59+00:00
- **Updated**: 2017-07-05 14:02:59+00:00
- **Authors**: Xudong Mao, Qing Li, Haoran Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, several methods based on generative adversarial network (GAN) have been proposed for the task of aligning cross-domain images or learning a joint distribution of cross-domain images. One of the methods is to use conditional GAN for alignment. However, previous attempts of adopting conditional GAN do not perform as well as other methods. In this work we present an approach for improving the capability of the methods which are based on conditional GAN. We evaluate the proposed method on numerous tasks and the experimental results show that it is able to align the cross-domain images successfully in absence of paired samples. Furthermore, we also propose another model which conditions on multiple information such as domain information and label information. Conditioning on domain information and label information, we are able to conduct label propagation from the source domain to the target domain. A 2-step alternating training algorithm is proposed to learn this model.



### Video Representation Learning and Latent Concept Mining for Large-scale Multi-label Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1707.01408v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01408v3)
- **Published**: 2017-07-05 14:15:06+00:00
- **Updated**: 2017-07-25 07:50:26+00:00
- **Authors**: Po-Yao Huang, Ye Yuan, Zhenzhong Lan, Lu Jiang, Alexander G. Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: We report on CMU Informedia Lab's system used in Google's YouTube 8 Million Video Understanding Challenge. In this multi-label video classification task, our pipeline achieved 84.675% and 84.662% GAP on our evaluation split and the official test set. We attribute the good performance to three components: 1) Refined video representation learning with residual links and hypercolumns 2) Latent concept mining which captures interactions among concepts. 3) Learning with temporal segments and weighted multi-model ensemble. We conduct experiments to validate and analyze the contribution of our models. We also share some unsuccessful trials leveraging conventional approaches such as recurrent neural networks for video representation learning for this large-scale video dataset. All the codes to reproduce our results are publicly available at https://github.com/Martini09/informedia-yt8m-release.



### On the Fusion of Compton Scatter and Attenuation Data for Limited-view X-ray Tomographic Applications
- **Arxiv ID**: http://arxiv.org/abs/1707.01530v1
- **DOI**: None
- **Categories**: **cs.CV**, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1707.01530v1)
- **Published**: 2017-07-05 18:39:18+00:00
- **Updated**: 2017-07-05 18:39:18+00:00
- **Authors**: Hamideh Rezaee, Brian Tracey, Eric L. Miller
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we demonstrate the utility of fusing energy-resolved observations of Compton scattered photons with traditional attenuation data for the joint recovery of mass density and photoelectric absorption in the context of limited view tomographic imaging applications. We begin with the development of a physical and associated numerical model for the Compton scatter process. Using this model, we propose a variational approach recovering these two material properties. In addition to the typical data-fidelity terms, the optimization functional includes regularization for both the mass density and photoelectric coefficients. We consider a novel edge-preserving method in the case of mass density. To aid in the recovery of the photoelectric information, we draw on our recent method in \cite{r15} and employ a non-local regularization scheme that builds on the fact that mass density is more stably imaged. Simulation results demonstrate clear advantages associated with the use of both scattered photon data and energy resolved information in mapping the two material properties of interest. Specifically, comparing images obtained using only conventional attenuation data with those where we employ only Compton scatter photons and images formed from the combination of the two, shows that taking advantage of both types of data for reconstruction provides far more accurate results.



