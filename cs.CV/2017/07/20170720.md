# Arxiv Papers in cs.CV on 2017-07-20
### Multi-Branch Fully Convolutional Network for Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1707.06330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06330v1)
- **Published**: 2017-07-20 00:49:38+00:00
- **Updated**: 2017-07-20 00:49:38+00:00
- **Authors**: Yancheng Bai, Bernard Ghanem
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Face detection is a fundamental problem in computer vision. It is still a challenging task in unconstrained conditions due to significant variations in scale, pose, expressions, and occlusion. In this paper, we propose a multi-branch fully convolutional network (MB-FCN) for face detection, which considers both efficiency and effectiveness in the design process. Our MB-FCN detector can deal with faces at all scale ranges with only a single pass through the backbone network. As such, our MB-FCN model saves computation and thus is more efficient, compared to previous methods that make multiple passes. For each branch, the specific skip connections of the convolutional feature maps at different layers are exploited to represent faces in specific scale ranges. Specifically, small faces can be represented with both shallow fine-grained and deep powerful coarse features. With this representation, superior improvement in performance is registered for the task of detecting small faces. We test our MB-FCN detector on two public face detection benchmarks, including FDDB and WIDER FACE. Extensive experiments show that our detector outperforms state-of-the-art methods on all these datasets in general and by a substantial margin on the most challenging among them (e.g. WIDER FACE Hard subset). Also, MB-FCN runs at 15 FPS on a GPU for images of size 640 x 480 with no assumption on the minimum detectable face size.



### Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.06335v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06335v1)
- **Published**: 2017-07-20 01:44:08+00:00
- **Updated**: 2017-07-20 01:44:08+00:00
- **Authors**: Hong-Yu Zhou, Bin-Bin Gao, Jianxin Wu
- **Comment**: accepted as a poster in BMVC 2017
- **Journal**: None
- **Summary**: The difficulty of image recognition has gradually increased from general category recognition to fine-grained recognition and to the recognition of some subtle attributes such as temperature and geolocation. In this paper, we try to focus on the classification between sunrise and sunset and hope to give a hint about how to tell the difference in subtle attributes. Sunrise vs. sunset is a difficult recognition task, which is challenging even for humans. Towards understanding this new problem, we first collect a new dataset made up of over one hundred webcams from different places. Since existing algorithmic methods have poor accuracy, we propose a new pairwise learning strategy to learn features from selective pairs of images. Experiments show that our approach surpasses baseline methods by a large margin and achieves better results even compared with humans. We also apply our approach to existing subtle attribute recognition problems, such as temperature estimation, and achieve state-of-the-art results.



### ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression
- **Arxiv ID**: http://arxiv.org/abs/1707.06342v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06342v1)
- **Published**: 2017-07-20 02:16:16+00:00
- **Updated**: 2017-07-20 02:16:16+00:00
- **Authors**: Jian-Hao Luo, Jianxin Wu, Weiyao Lin
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: We propose an efficient and unified framework, namely ThiNet, to simultaneously accelerate and compress CNN models in both training and inference stages. We focus on the filter level pruning, i.e., the whole filter would be discarded if it is less important. Our method does not change the original network structure, thus it can be perfectly supported by any off-the-shelf deep learning libraries. We formally establish filter pruning as an optimization problem, and reveal that we need to prune filters based on statistics information computed from its next layer, not the current layer, which differentiates ThiNet from existing methods. Experimental results demonstrate the effectiveness of this strategy, which has advanced the state-of-the-art. We also show the performance of ThiNet on ILSVRC-12 benchmark. ThiNet achieves 3.31$\times$ FLOPs reduction and 16.63$\times$ compression on VGG-16, with only 0.52$\%$ top-5 accuracy drop. Similar experiments with ResNet-50 reveal that even for a compact network, ThiNet can also reduce more than half of the parameters and FLOPs, at the cost of roughly 1$\%$ top-5 accuracy drop. Moreover, the original VGG-16 model can be further pruned into a very small model with only 5.05MB model size, preserving AlexNet level accuracy but showing much stronger generalization ability.



### Video Question Answering via Attribute-Augmented Attention Network Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.06355v1
- **DOI**: 10.1145/3077136.3080655
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1707.06355v1)
- **Published**: 2017-07-20 03:12:29+00:00
- **Updated**: 2017-07-20 03:12:29+00:00
- **Authors**: Yunan Ye, Zhou Zhao, Yimeng Li, Long Chen, Jun Xiao, Yueting Zhuang
- **Comment**: Accepted for SIGIR 2017
- **Journal**: None
- **Summary**: Video Question Answering is a challenging problem in visual information retrieval, which provides the answer to the referenced video content according to the question. However, the existing visual question answering approaches mainly tackle the problem of static image question, which may be ineffectively for video question answering due to the insufficiency of modeling the temporal dynamics of video contents. In this paper, we study the problem of video question answering by modeling its temporal dynamics with frame-level attention mechanism. We propose the attribute-augmented attention network learning framework that enables the joint frame-level attribute detection and unified video representation learning for video question answering. We then incorporate the multi-step reasoning process for our proposed attention network to further improve the performance. We construct a large-scale video question answering dataset. We conduct the experiments on both multiple-choice and open-ended video question answering tasks to show the effectiveness of the proposed method.



### 3D Shape Reconstruction from Sketches via Multi-view Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.06375v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1707.06375v3)
- **Published**: 2017-07-20 05:05:26+00:00
- **Updated**: 2017-09-29 05:18:30+00:00
- **Authors**: Zhaoliang Lun, Matheus Gadelha, Evangelos Kalogerakis, Subhransu Maji, Rui Wang
- **Comment**: 3DV 2017 (oral)
- **Journal**: None
- **Summary**: We propose a method for reconstructing 3D shapes from 2D sketches in the form of line drawings. Our method takes as input a single sketch, or multiple sketches, and outputs a dense point cloud representing a 3D reconstruction of the input sketch(es). The point cloud is then converted into a polygon mesh. At the heart of our method lies a deep, encoder-decoder network. The encoder converts the sketch into a compact representation encoding shape information. The decoder converts this representation into depth and normal maps capturing the underlying surface from several output viewpoints. The multi-view maps are then consolidated into a 3D point cloud by solving an optimization problem that fuses depth and normals across all viewpoints. Based on our experiments, compared to other methods, such as volumetric networks, our architecture offers several advantages, including more faithful reconstruction, higher output surface resolution, better preservation of topology and shape structure.



### Unsupervised Object Discovery and Co-Localization by Deep Descriptor Transforming
- **Arxiv ID**: http://arxiv.org/abs/1707.06397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06397v1)
- **Published**: 2017-07-20 07:07:33+00:00
- **Updated**: 2017-07-20 07:07:33+00:00
- **Authors**: Xiu-Shen Wei, Chen-Lin Zhang, Jianxin Wu, Chunhua Shen, Zhi-Hua Zhou
- **Comment**: This paper is extended based on our preliminary work published in
  IJCAI 2017 [arXiv:1705.02758]
- **Journal**: None
- **Summary**: Reusable model design becomes desirable with the rapid expansion of computer vision and machine learning applications. In this paper, we focus on the reusability of pre-trained deep convolutional models. Specifically, different from treating pre-trained models as feature extractors, we reveal more treasures beneath convolutional layers, i.e., the convolutional activations could act as a detector for the common object in the image co-localization problem. We propose a simple yet effective method, termed Deep Descriptor Transforming (DDT), for evaluating the correlations of descriptors and then obtaining the category-consistent regions, which can accurately locate the common object in a set of unlabeled images, i.e., unsupervised object discovery. Empirical studies validate the effectiveness of the proposed DDT method. On benchmark image co-localization datasets, DDT consistently outperforms existing state-of-the-art methods by a large margin. Moreover, DDT also demonstrates good generalization ability for unseen categories and robustness for dealing with noisy data. Beyond those, DDT can be also employed for harvesting web images into valid external data sources for improving performance of both image recognition and object detection.



### Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/1707.06399v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06399v1)
- **Published**: 2017-07-20 07:22:01+00:00
- **Updated**: 2017-07-20 07:22:01+00:00
- **Authors**: Hong-Yu Zhou, Bin-Bin Gao, Jianxin Wu
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: Object detection aims at high speed and accuracy simultaneously. However, fast models are usually less accurate, while accurate models cannot satisfy our need for speed. A fast model can be 10 times faster but 50\% less accurate than an accurate model. In this paper, we propose Adaptive Feeding (AF) to combine a fast (but less accurate) detector and an accurate (but slow) detector, by adaptively determining whether an image is easy or hard and choosing an appropriate detector for it. In practice, we build a cascade of detectors, including the AF classifier which make the easy vs. hard decision and the two detectors. The AF classifier can be tuned to obtain different tradeoff between speed and accuracy, which has negligible training time and requires no additional training data. Experimental results on the PASCAL VOC, MS COCO and Caltech Pedestrian datasets confirm that AF has the ability to achieve comparable speed as the fast detector and comparable accuracy as the accurate one at the same time. As an example, by combining the fast SSD300 with the accurate SSD500 detector, AF leads to 50\% speedup over SSD500 with the same precision on the VOC2007 test set.



### Semantic Segmentation with Reverse Attention
- **Arxiv ID**: http://arxiv.org/abs/1707.06426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06426v1)
- **Published**: 2017-07-20 09:45:08+00:00
- **Updated**: 2017-07-20 09:45:08+00:00
- **Authors**: Qin Huang, Chunyang Xia, Chihao Wu, Siyang Li, Ye Wang, Yuhang Song, C. -C. Jay Kuo
- **Comment**: accepted for oral presentation in BMVC 2017
- **Journal**: None
- **Summary**: Recent development in fully convolutional neural network enables efficient end-to-end learning of semantic segmentation. Traditionally, the convolutional classifiers are taught to learn the representative semantic features of labeled semantic objects. In this work, we propose a reverse attention network (RAN) architecture that trains the network to capture the opposite concept (i.e., what are not associated with a target class) as well. The RAN is a three-branch network that performs the direct, reverse and reverse-attention learning processes simultaneously. Extensive experiments are conducted to show the effectiveness of the RAN in semantic segmentation. Being built upon the DeepLabv2-LargeFOV, the RAN achieves the state-of-the-art mIoU score (48.1%) for the challenging PASCAL-Context dataset. Significant performance improvements are also observed for the PASCAL-VOC, Person-Part, NYUDv2 and ADE20K datasets.



### Scalable Full Flow with Learned Binary Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1707.06427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06427v1)
- **Published**: 2017-07-20 09:49:29+00:00
- **Updated**: 2017-07-20 09:49:29+00:00
- **Authors**: Gottfried Munda, Alexander Shekhovtsov, Patrick Knöbelreiter, Thomas Pock
- **Comment**: GCPR 2017
- **Journal**: None
- **Summary**: We propose a method for large displacement optical flow in which local matching costs are learned by a convolutional neural network (CNN) and a smoothness prior is imposed by a conditional random field (CRF). We tackle the computation- and memory-intensive operations on the 4D cost volume by a min-projection which reduces memory complexity from quadratic to linear and binary descriptors for efficient matching. This enables evaluation of the cost on the fly and allows to perform learning and CRF inference on high resolution images without ever storing the 4D cost volume. To address the problem of learning binary descriptors we propose a new hybrid learning scheme. In contrast to current state of the art approaches for learning binary CNNs we can compute the exact non-zero gradient within our model. We compare several methods for training binary descriptors and show results on public available benchmarks.



### cvpaper.challenge in 2016: Futuristic Computer Vision through 1,600 Papers Survey
- **Arxiv ID**: http://arxiv.org/abs/1707.06436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06436v1)
- **Published**: 2017-07-20 10:06:07+00:00
- **Updated**: 2017-07-20 10:06:07+00:00
- **Authors**: Hirokatsu Kataoka, Soma Shirakabe, Yun He, Shunya Ueta, Teppei Suzuki, Kaori Abe, Asako Kanezaki, Shin'ichiro Morita, Toshiyuki Yabe, Yoshihiro Kanehara, Hiroya Yatsuyanagi, Shinya Maruyama, Ryosuke Takasawa, Masataka Fuchida, Yudai Miyashita, Kazushige Okayasu, Yuta Matsuzaki
- **Comment**: None
- **Journal**: None
- **Summary**: The paper gives futuristic challenges disscussed in the cvpaper.challenge. In 2015 and 2016, we thoroughly study 1,600+ papers in several conferences/journals such as CVPR/ICCV/ECCV/NIPS/PAMI/IJCV.



### A Novel Space-Time Representation on the Positive Semidefinite Con for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.06440v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06440v1)
- **Published**: 2017-07-20 10:36:12+00:00
- **Updated**: 2017-07-20 10:36:12+00:00
- **Authors**: Anis Kacem, Mohamed Daoudi, Boulbaba Ben Amor, Juan Carlos Alvarez-Paiva
- **Comment**: To be appeared at ICCV 2017
- **Journal**: None
- **Summary**: In this paper, we study the problem of facial expression recognition using a novel space-time geometric representation. We describe the temporal evolution of facial landmarks as parametrized trajectories on the Riemannian manifold of positive semidefinite matrices of fixed-rank. Our representation has the advantage to bring naturally a second desirable quantity when comparing shapes -- the spatial covariance -- in addition to the conventional affine-shape representation. We derive then geometric and computational tools for rate-invariant analysis and adaptive re-sampling of trajectories, grounding on the Riemannian geometry of the manifold. Specifically, our approach involves three steps: 1) facial landmarks are first mapped into the Riemannian manifold of positive semidefinite matrices of rank 2, to build time-parameterized trajectories; 2) a temporal alignment is performed on the trajectories, providing a geometry-aware (dis-)similarity measure between them; 3) finally, pairwise proximity function SVM (ppfSVM) is used to classify them, incorporating the latter (dis-)similarity measure into the kernel function. We show the effectiveness of the proposed approach on four publicly available benchmarks (CK+, MMI, Oulu-CASIA, and AFEW). The results of the proposed approach are comparable to or better than the state-of-the-art methods when involving only facial landmarks.



### Learned Primal-dual Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1707.06474v3
- **DOI**: 10.1109/TMI.2018.2799231
- **Categories**: **math.OC**, cs.CV, cs.NE, math.FA
- **Links**: [PDF](http://arxiv.org/pdf/1707.06474v3)
- **Published**: 2017-07-20 12:34:51+00:00
- **Updated**: 2018-07-05 15:34:04+00:00
- **Authors**: Jonas Adler, Ozan Öktem
- **Comment**: 11 pages, 5 figures
- **Journal**: IEEE Transactions on Medical Imaging (2018)
- **Summary**: We propose the Learned Primal-Dual algorithm for tomographic reconstruction. The algorithm accounts for a (possibly non-linear) forward operator in a deep neural network by unrolling a proximal primal-dual optimization method, but where the proximal operators have been replaced with convolutional neural networks. The algorithm is trained end-to-end, working directly from raw measured data and it does not depend on any initial reconstruction such as FBP.   We compare performance of the proposed method on low dose CT reconstruction against FBP, TV, and deep learning based post-processing of FBP. For the Shepp-Logan phantom we obtain >6dB PSNR improvement against all compared methods. For human phantoms the corresponding improvement is 6.6dB over TV and 2.2dB over learned post-processing along with a substantial improvement in the SSIM. Finally, our algorithm involves only ten forward-back-projection computations, making the method feasible for time critical clinical applications.



### Deep Layer Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1707.06484v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.06484v3)
- **Published**: 2017-07-20 12:59:08+00:00
- **Updated**: 2019-01-04 09:26:55+00:00
- **Authors**: Fisher Yu, Dequan Wang, Evan Shelhamer, Trevor Darrell
- **Comment**: Published at the Conference on Computer Vision and Pattern
  Recognition (CVPR) 2018
- **Journal**: None
- **Summary**: Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been "shallow" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. The code is at https://github.com/ucbdrive/dla.



### An All-in-One Network for Dehazing and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1707.06543v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1707.06543v1)
- **Published**: 2017-07-20 14:30:35+00:00
- **Updated**: 2017-07-20 14:30:35+00:00
- **Authors**: Boyi Li, Xiulian Peng, Zhangyang Wang, Jizheng Xu, Dan Feng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an image dehazing model built with a convolutional neural network (CNN), called All-in-One Dehazing Network (AOD-Net). It is designed based on a re-formulated atmospheric scattering model. Instead of estimating the transmission matrix and the atmospheric light separately as most previous models did, AOD-Net directly generates the clean image through a light-weight CNN. Such a novel end-to-end design makes it easy to embed AOD-Net into other deep models, e.g., Faster R-CNN, for improving high-level task performance on hazy images. Experimental results on both synthesized and natural hazy image datasets demonstrate our superior performance than the state-of-the-art in terms of PSNR, SSIM and the subjective visual quality. Furthermore, when concatenating AOD-Net with Faster R-CNN and training the joint pipeline from end to end, we witness a large improvement of the object detection performance on hazy images.



### Video Object Segmentation using Tracked Object Proposals
- **Arxiv ID**: http://arxiv.org/abs/1707.06545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06545v1)
- **Published**: 2017-07-20 14:31:12+00:00
- **Updated**: 2017-07-20 14:31:12+00:00
- **Authors**: Gilad Sharir, Eddie Smolyansky, Itamar Friedman
- **Comment**: All authors contributed equally, CVPR-2017 workshop, DAVIS-2017
  Challenge
- **Journal**: None
- **Summary**: We present an approach to semi-supervised video object segmentation, in the context of the DAVIS 2017 challenge. Our approach combines category-based object detection, category-independent object appearance segmentation and temporal object tracking. We are motivated by the fact that the objects semantic category tends not to change throughout the video while its appearance and location can vary considerably. In order to capture the specific object appearance independent of its category, for each video we train a fully convolutional network using augmentations of the given annotated frame. We refine the appearance segmentation mask with the bounding boxes provided either by a semantic object detection network, when applicable, or by a previous frame prediction. By introducing a temporal continuity constraint on the detected boxes, we are able to improve the object segmentation mask of the appearance network and achieve competitive results on the DAVIS datasets.



### leave a trace - A People Tracking System Meets Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1707.06557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1707.06557v1)
- **Published**: 2017-07-20 15:03:11+00:00
- **Updated**: 2017-07-20 15:03:11+00:00
- **Authors**: Dominik Rueß, Konstantinos Amplianitis, Niklas Deckers, Michele Adduci, Kristian Manthey, Ralf Reulke
- **Comment**: None
- **Journal**: None
- **Summary**: Video surveillance always had a negative connotation, among others because of the loss of privacy and because it may not automatically increase public safety. If it was able to detect atypical (i.e. dangerous) situations in real time, autonomously and anonymously, this could change. A prerequisite for this is a reliable automatic detection of possibly dangerous situations from video data. This is done classically by object extraction and tracking. From the derived trajectories, we then want to determine dangerous situations by detecting atypical trajectories. However, due to ethical considerations it is better to develop such a system on data without people being threatened or even harmed, plus with having them know that there is such a tracking system installed. Another important point is that these situations do not occur very often in real, public CCTV areas and may be captured properly even less. In the artistic project leave a trace the tracked objects, people in an atrium of a institutional building, become actor and thus part of the installation. Visualisation in real-time allows interaction by these actors, which in turn creates many atypical interaction situations on which we can develop our situation detection. The data set has evolved over three years and hence, is huge. In this article we describe the tracking system and several approaches for the detection of atypical trajectories.



### Pictures of Combinatorial Cubes
- **Arxiv ID**: http://arxiv.org/abs/1707.06563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06563v1)
- **Published**: 2017-07-20 15:17:08+00:00
- **Updated**: 2017-07-20 15:17:08+00:00
- **Authors**: André Wagner
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: We prove that the 8-point algorithm always fails to reconstruct a unique fundamental matrix $F$ independent on the camera positions, when its input are image point configurations that are perspective projections of the vertices of a combinatorial cube in $\mathbb{R}^3$. We give an algorithm that improves the 7- and 8-point algorithm in such a pathological situation. Additionally we analyze the regions of focal point positions where a reconstruction of $F$ is possible at all, when the world points are the vertices of a combinatorial cube in $\mathbb{R}^3$.



### Acting Thoughts: Towards a Mobile Robotic Service Assistant for Users with Limited Communication Skills
- **Arxiv ID**: http://arxiv.org/abs/1707.06633v4
- **DOI**: 10.1109/ECMR.2017.8098658
- **Categories**: **cs.AI**, cs.CV, cs.HC, cs.LG, cs.RO, I.2.4; I.2.6; I.2.8; I.2.9; I.2.10; I.4.8; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1707.06633v4)
- **Published**: 2017-07-20 17:51:12+00:00
- **Updated**: 2018-06-12 14:52:41+00:00
- **Authors**: Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin Völker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka Boedecker, Bernhard Nebel, Tonio Ball, Wolfram Burgard
- **Comment**: * FB, LDJF, DK, MV and JA contributed equally to the work. Accepted
  as a conference paper at the European Conference on Mobile Robotics 2017
  (ECMR 2017), 6 pages, 3 figures
- **Journal**: 2017 European Conference on Mobile Robots (ECMR)
- **Summary**: As autonomous service robots become more affordable and thus available also for the general public, there is a growing need for user friendly interfaces to control the robotic system. Currently available control modalities typically expect users to be able to express their desire through either touch, speech or gesture commands. While this requirement is fulfilled for the majority of users, paralyzed users may not be able to use such systems. In this paper, we present a novel framework, that allows these users to interact with a robotic service assistant in a closed-loop fashion, using only thoughts. The brain-computer interface (BCI) system is composed of several interacting components, i.e., non-invasive neuronal signal recording and decoding, high-level task planning, motion and manipulation planning as well as environment perception. In various experiments, we demonstrate its applicability and robustness in real world scenarios, considering fetch-and-carry tasks and tasks involving human-robot interaction. As our results demonstrate, our system is capable of adapting to frequent changes in the environment and reliably completing given tasks within a reasonable amount of time. Combined with high-level planning and autonomous robotic systems, interesting new perspectives open up for non-invasive BCI-based human-robot interactions.



### The iNaturalist Species Classification and Detection Dataset
- **Arxiv ID**: http://arxiv.org/abs/1707.06642v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06642v2)
- **Published**: 2017-07-20 17:59:55+00:00
- **Updated**: 2018-04-10 20:22:13+00:00
- **Authors**: Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, Serge Belongie
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Existing image classification datasets used in computer vision tend to have a uniform distribution of images across object categories. In contrast, the natural world is heavily imbalanced, as some species are more abundant and easier to photograph than others. To encourage further progress in challenging real world conditions we present the iNaturalist species classification and detection dataset, consisting of 859,000 images from over 5,000 different species of plants and animals. It features visually similar species, captured in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, feature a large class imbalance, and have been verified by multiple citizen scientists. We discuss the collection of the dataset and present extensive baseline experiments using state-of-the-art computer vision classification and detection models. Results show that current non-ensemble based methods achieve only 67% top one classification accuracy, illustrating the difficulty of the dataset. Specifically, we observe poor results for classes with small numbers of training examples suggesting more attention is needed in low-shot learning.



### Resting state fMRI functional connectivity-based classification using a convolutional neural network architecture
- **Arxiv ID**: http://arxiv.org/abs/1707.06682v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, 68T99, I.5.1; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1707.06682v1)
- **Published**: 2017-07-20 19:12:58+00:00
- **Updated**: 2017-07-20 19:12:58+00:00
- **Authors**: Regina Meszlényi, Krisztian Buza, Zoltán Vidnyánszky
- **Comment**: 25 pages, 4 figures, 1 table, plus supplementary material
- **Journal**: None
- **Summary**: Machine learning techniques have become increasingly popular in the field of resting state fMRI (functional magnetic resonance imaging) network based classification. However, the application of convolutional networks has been proposed only very recently and has remained largely unexplored. In this paper we describe a convolutional neural network architecture for functional connectome classification called connectome-convolutional neural network (CCNN). Our results on simulated datasets and a publicly available dataset for amnestic mild cognitive impairment classification demonstrate that our CCNN model can efficiently distinguish between subject groups. We also show that the connectome-convolutional network is capable to combine information from diverse functional connectivity metrics and that models using a combination of different connectivity descriptors are able to outperform classifiers using only one metric. From this flexibility follows that our proposed CCNN model can be easily adapted to a wide range of connectome based classification or regression tasks, by varying which connectivity descriptor combinations are used to train the network.



### Local Geometry Inclusive Global Shape Representation
- **Arxiv ID**: http://arxiv.org/abs/1707.06699v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, 68U05 (Primary), 68T10, 68T01 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/1707.06699v1)
- **Published**: 2017-07-20 20:32:11+00:00
- **Updated**: 2017-07-20 20:32:11+00:00
- **Authors**: Somenath Das, Suchendra M. Bhandarkar
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Knowledge of shape geometry plays a pivotal role in many shape analysis applications. In this paper we introduce a local geometry-inclusive global representation of 3D shapes based on computation of the shortest quasi-geodesic paths between all possible pairs of points on the 3D shape manifold. In the proposed representation, the normal curvature along the quasi-geodesic paths between any two points on the shape surface is preserved. We employ the eigenspectrum of the proposed global representation to address the problems of determination of region-based correspondence between isometric shapes and characterization of self-symmetry in the absence of prior knowledge in the form of user-defined correspondence maps. We further utilize the commutative property of the resulting shape descriptor to extract stable regions between isometric shapes that differ from one another by a high degree of isometry transformation. We also propose various shape characterization metrics in terms of the eigenvector decomposition of the shape descriptor spectrum to quantify the correspondence and self-symmetry of 3D shapes. The performance of the proposed 3D shape descriptor is experimentally compared with the performance of other relevant state-of-the-art 3D shape descriptors.



### Convolutional Sparse Coding: Boundary Handling Revisited
- **Arxiv ID**: http://arxiv.org/abs/1707.06718v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1707.06718v1)
- **Published**: 2017-07-20 23:10:02+00:00
- **Updated**: 2017-07-20 23:10:02+00:00
- **Authors**: Brendt Wohlberg, Paul Rodriguez
- **Comment**: None
- **Journal**: None
- **Summary**: Two different approaches have recently been proposed for boundary handling in convolutional sparse representations, avoiding potential boundary artifacts arising from the circular boundary conditions implied by the use of frequency domain solution methods by introducing a spatial mask into the convolutional sparse coding problem. In the present paper we show that, under certain circumstances, these methods fail in their design goal of avoiding boundary artifacts. The reasons for this failure are discussed, a solution is proposed, and the practical implications are illustrated in an image deblurring problem.



### Generalized Convolutional Neural Networks for Point Cloud Data
- **Arxiv ID**: http://arxiv.org/abs/1707.06719v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1707.06719v2)
- **Published**: 2017-07-20 23:12:11+00:00
- **Updated**: 2018-10-19 02:09:03+00:00
- **Authors**: Aleksandr Savchenkov, Andrew Davis, Xuan Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: The introduction of cheap RGB-D cameras, stereo cameras, and LIDAR devices has given the computer vision community 3D information that conventional RGB cameras cannot provide. This data is often stored as a point cloud. In this paper, we present a novel method to apply the concept of convolutional neural networks to this type of data. By creating a mapping of nearest neighbors in a dataset, and individually applying weights to spatial relationships between points, we achieve an architecture that works directly with point clouds, but closely resembles a convolutional neural net in both design and behavior. Such a method bypasses the need for extensive feature engineering, while proving to be computationally efficient and requiring few parameters.



