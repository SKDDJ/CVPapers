# Arxiv Papers in cs.CV on 2017-07-17
### Visual Question Answering with Memory-Augmented Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.04968v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1707.04968v2)
- **Published**: 2017-07-17 00:42:56+00:00
- **Updated**: 2018-03-25 09:46:14+00:00
- **Authors**: Chao Ma, Chunhua Shen, Anthony Dick, Qi Wu, Peng Wang, Anton van den Hengel, Ian Reid
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: In this paper, we exploit a memory-augmented neural network to predict accurate answers to visual questions, even when those answers occur rarely in the training set. The memory network incorporates both internal and external memory blocks and selectively pays attention to each training exemplar. We show that memory-augmented neural networks are able to maintain a relatively long-term memory of scarce training exemplars, which is important for visual question answering due to the heavy-tailed distribution of answers in a general VQA setting. Experimental results on two large-scale benchmark datasets show the favorable performance of the proposed algorithm with a comparison to state of the art.



### Adaptive Low-Rank Kernel Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1707.04974v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04974v4)
- **Published**: 2017-07-17 01:25:07+00:00
- **Updated**: 2019-01-25 18:02:31+00:00
- **Authors**: Pan Ji, Ian Reid, Ravi Garg, Hongdong Li, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a kernel subspace clustering method that can handle non-linear models. In contrast to recent kernel subspace clustering methods which use predefined kernels, we propose to learn a low-rank kernel matrix, with which mapped data in feature space are not only low-rank but also self-expressive. In this manner, the low-dimensional subspace structures of the (implicitly) mapped data are retained and manifested in the high-dimensional feature space. We evaluate the proposed method extensively on both motion segmentation and image clustering benchmarks, and obtain superior results, outperforming the kernel subspace clustering method that uses standard kernels[Patel 2014] and other state-of-the-art linear subspace clustering methods.



### Tracking as Online Decision-Making: Learning a Policy from Streaming Videos with Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.04991v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04991v1)
- **Published**: 2017-07-17 03:38:35+00:00
- **Updated**: 2017-07-17 03:38:35+00:00
- **Authors**: James Steven Supancic III, Deva Ramanan
- **Comment**: None
- **Journal**: None
- **Summary**: We formulate tracking as an online decision-making process, where a tracking agent must follow an object despite ambiguous image frames and a limited computational budget. Crucially, the agent must decide where to look in the upcoming frames, when to reinitialize because it believes the target has been lost, and when to update its appearance model for the tracked object. Such decisions are typically made heuristically. Instead, we propose to learn an optimal decision-making policy by formulating tracking as a partially observable decision-making process (POMDP). We learn policies with deep reinforcement learning algorithms that need supervision (a reward signal) only when the track has gone awry. We demonstrate that sparse rewards allow us to quickly train on massive datasets, several orders of magnitude more than past work. Interestingly, by treating the data source of Internet videos as unlimited streams, we both learn and evaluate our trackers in a single, unified computational stream.



### MoCoGAN: Decomposing Motion and Content for Video Generation
- **Arxiv ID**: http://arxiv.org/abs/1707.04993v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04993v2)
- **Published**: 2017-07-17 03:42:17+00:00
- **Updated**: 2017-12-14 00:04:02+00:00
- **Authors**: Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: Visual signals in a video can be divided into content and motion. While content specifies which objects are in the video, motion describes their dynamics. Based on this prior, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation. The proposed framework generates a video by mapping a sequence of random vectors to a sequence of video frames. Each random vector consists of a content part and a motion part. While the content part is kept fixed, the motion part is realized as a stochastic process. To learn motion and content decomposition in an unsupervised manner, we introduce a novel adversarial learning scheme utilizing both image and video discriminators. Extensive experimental results on several challenging datasets with qualitative and quantitative comparison to the state-of-the-art approaches, verify effectiveness of the proposed framework. In addition, we show that MoCoGAN allows one to generate videos with same content but different motion as well as videos with different content and same motion.



### "Maximizing rigidity" revisited: a convex programming approach for generic 3D shape reconstruction from multiple perspective views
- **Arxiv ID**: http://arxiv.org/abs/1707.05009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05009v1)
- **Published**: 2017-07-17 06:18:28+00:00
- **Updated**: 2017-07-17 06:18:28+00:00
- **Authors**: Pan Ji, Hongdong Li, Yuchao Dai, Ian Reid
- **Comment**: to appear in ICCV'17
- **Journal**: None
- **Summary**: Rigid structure-from-motion (RSfM) and non-rigid structure-from-motion (NRSfM) have long been treated in the literature as separate (different) problems. Inspired by a previous work which solved directly for 3D scene structure by factoring the relative camera poses out, we revisit the principle of "maximizing rigidity" in structure-from-motion literature, and develop a unified theory which is applicable to both rigid and non-rigid structure reconstruction in a rigidity-agnostic way. We formulate these problems as a convex semi-definite program, imposing constraints that seek to apply the principle of minimizing non-rigidity. Our results demonstrate the efficacy of the approach, with state-of-the-art accuracy on various 3D reconstruction problems.



### Residual Features and Unified Prediction Network for Single Stage Detection
- **Arxiv ID**: http://arxiv.org/abs/1707.05031v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05031v4)
- **Published**: 2017-07-17 07:54:44+00:00
- **Updated**: 2018-01-05 04:32:45+00:00
- **Authors**: Kyoungmin Lee, Jaeseok Choi, Jisoo Jeong, Nojun Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, a lot of single stage detectors using multi-scale features have been actively proposed. They are much faster than two stage detectors that use region proposal networks (RPN) without much degradation in the detection performances. However, the feature maps in the lower layers close to the input which are responsible for detecting small objects in a single stage detector have a problem of insufficient representation power because they are too shallow. There is also a structural contradiction that the feature maps have to deliver low-level information to next layers as well as contain high-level abstraction for prediction. In this paper, we propose a method to enrich the representation power of feature maps using Resblock and deconvolution layers. In addition, a unified prediction module is applied to generalize output results and boost earlier layers' representation power for prediction. The proposed method enables more precise prediction, which achieved higher score than SSD on PASCAL VOC and MS COCO. In addition, it maintains the advantage of fast computation of a single stage detector, which requires much less computation than other detectors with similar performance. Code is available at https://github.com/kmlee-snu/run



### Information-Flow Matting
- **Arxiv ID**: http://arxiv.org/abs/1707.05055v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1707.05055v2)
- **Published**: 2017-07-17 09:35:14+00:00
- **Updated**: 2019-04-12 13:08:18+00:00
- **Authors**: Yağız Aksoy, Tunç Ozan Aydın, Marc Pollefeys
- **Comment**: 16 pages, 13 figures, extended version of CVPR 2017 publication
  titled "Designing Effective Inter-pixel Information Flow for Natural Image
  Matting"
- **Journal**: None
- **Summary**: We present a novel, purely affinity-based natural image matting algorithm. Our method relies on carefully defined pixel-to-pixel connections that enable effective use of information available in the image. We control the information flow from the known-opacity regions into the unknown region, as well as within the unknown region itself, by utilizing multiple definitions of pixel affinities. Among other forms of information flow, we introduce color-mixture flow, which builds upon local linear embedding and effectively encapsulates the relation between different pixel opacities. Our resulting novel linear system formulation can be solved in closed-form and is robust against several fundamental challenges of natural matting such as holes and remote intricate structures. While our method is primarily designed as a standalone matting tool, we show that it can also be used for regularizing mattes obtained by sampling-based methods. The formulation is also extended to layer color estimation and we show that the use of multiple channels of flow increases the layer color quality. We also demonstrate our performance in green-screen keying and analyze the characteristics of the utilized affinities.



### Speeding up the Köhler's method of contrast thresholding
- **Arxiv ID**: http://arxiv.org/abs/1707.05062v2
- **DOI**: 10.1109/ICIP.2017.8296295
- **Categories**: **cs.CV**, cs.CC, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1707.05062v2)
- **Published**: 2017-07-17 09:41:04+00:00
- **Updated**: 2018-03-02 10:43:48+00:00
- **Authors**: Guillaume Noyel
- **Comment**: IEEE CopyrightProceedings of the IEEE International Conference on
  Image Processing ICIP 2017
- **Journal**: IEEE. IEEE International Conference on Image Processing ICIP 2017,
  Sep 2017, Beijing, China. IEEE, 2017, http://2017.ieeeicip.org
- **Summary**: K{\"o}hler's method is a useful multi-thresholding technique based on boundary contrast. However, the direct algorithm has a too high complexity-O(N 2) i.e. quadratic with the pixel numbers N-to process images at a sufficient speed for practical applications. In this paper, a new algorithm to speed up K{\"o}hler's method is introduced with a complexity in O(N M), M is the number of grey levels. The proposed algorithm is designed for parallelisation and vector processing , which are available in current processors, using OpenMP (Open Multi-Processing) and SIMD instructions (Single Instruction on Multiple Data). A fast implementation allows a gain factor of 405 in an image of 18 million pixels and a video processing in real time (gain factor of 96).



### Fully Automatic and Real-Time Catheter Segmentation in X-Ray Fluoroscopy
- **Arxiv ID**: http://arxiv.org/abs/1707.05137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05137v1)
- **Published**: 2017-07-17 13:18:37+00:00
- **Updated**: 2017-07-17 13:18:37+00:00
- **Authors**: Pierre Ambrosini, Daniel Ruijters, Wiro J. Niessen, Adriaan Moelker, Theo van Walsum
- **Comment**: Accepted to MICCAI 2017
- **Journal**: None
- **Summary**: Augmenting X-ray imaging with 3D roadmap to improve guidance is a common strategy. Such approaches benefit from automated analysis of the X-ray images, such as the automatic detection and tracking of instruments. In this paper, we propose a real-time method to segment the catheter and guidewire in 2D X-ray fluoroscopic sequences. The method is based on deep convolutional neural networks. The network takes as input the current image and the three previous ones, and segments the catheter and guidewire in the current image. Subsequently, a centerline model of the catheter is constructed from the segmented image. A small set of annotated data combined with data augmentation is used to train the network. We trained the method on images from 182 X-ray sequences from 23 different interventions. On a testing set with images of 55 X-ray sequences from 5 other interventions, a median centerline distance error of 0.2 mm and a median tip distance error of 0.9 mm was obtained. The segmentation of the instruments in 2D X-ray sequences is performed in a real-time fully-automatic manner.



### Aesthetic-Driven Image Enhancement by Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.05251v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05251v2)
- **Published**: 2017-07-17 15:58:49+00:00
- **Updated**: 2018-07-02 06:25:18+00:00
- **Authors**: Yubin Deng, Chen Change Loy, Xiaoou Tang
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce EnhanceGAN, an adversarial learning based model that performs automatic image enhancement. Traditional image enhancement frameworks typically involve training models in a fully-supervised manner, which require expensive annotations in the form of aligned image pairs. In contrast to these approaches, our proposed EnhanceGAN only requires weak supervision (binary labels on image aesthetic quality) and is able to learn enhancement operators for the task of aesthetic-based image enhancement. In particular, we show the effectiveness of a piecewise color enhancement module trained with weak supervision, and extend the proposed EnhanceGAN framework to learning a deep filtering-based aesthetic enhancer. The full differentiability of our image enhancement operators enables the training of EnhanceGAN in an end-to-end manner. We further demonstrate the capability of EnhanceGAN in learning aesthetic-based image cropping without any groundtruth cropping pairs. Our weakly-supervised EnhanceGAN reports competitive quantitative results on aesthetic-based color enhancement as well as automatic image cropping, and a user study confirms that our image enhancement results are on par with or even preferred over professional enhancement.



### Show and Recall: Learning What Makes Videos Memorable
- **Arxiv ID**: http://arxiv.org/abs/1707.05357v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05357v3)
- **Published**: 2017-07-17 18:34:37+00:00
- **Updated**: 2017-08-28 07:43:28+00:00
- **Authors**: Sumit Shekhar, Dhruv Singal, Harvineet Singh, Manav Kedia, Akhil Shetty
- **Comment**: 10 pages, updated abstract, added few references, project page link
  and acknowledgements. Accepted at ICCV 2017 Workshop on Mutual Benefits of
  Cognitive and Computer Vision (MBCC)
- **Journal**: None
- **Summary**: With the explosion of video content on the Internet, there is a need for research on methods for video analysis which take human cognition into account. One such cognitive measure is memorability, or the ability to recall visual content after watching it. Prior research has looked into image memorability and shown that it is intrinsic to visual content, but the problem of modeling video memorability has not been addressed sufficiently. In this work, we develop a prediction model for video memorability, including complexities of video content in it. Detailed feature analysis reveals that the proposed method correlates well with existing findings on memorability. We also describe a novel experiment of predicting video sub-shot memorability and show that our approach improves over current memorability methods in this task. Experiments on standard datasets demonstrate that the proposed metric can achieve results on par or better than the state-of-the art methods for video summarization.



### Remote sensing of forests using discrete return airborne LiDAR
- **Arxiv ID**: http://arxiv.org/abs/1707.09865v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CE, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1707.09865v1)
- **Published**: 2017-07-17 19:06:18+00:00
- **Updated**: 2017-07-17 19:06:18+00:00
- **Authors**: Hamid Hamraz, Marco A. Contreras
- **Comment**: This manuscript is a book chapter that has provisionally been
  accepted to be published in "Recent Advances and Applications in Remote
  Sensing", ISBN 978-953-51-5564-5. Ed.: Hung, Ming Cheh. InTechOpen. The
  chapter summarizes novel methods from four recently published journal ppapers
  by the authors in a concise and cohesive manner
- **Journal**: None
- **Summary**: Airborne discrete return light detection and ranging (LiDAR) point clouds covering forested areas can be processed to segment individual trees and retrieve their morphological attributes. Segmenting individual trees in natural deciduous forests however remained a challenge because of the complex and multi-layered canopy. In this chapter, we present (i) a robust segmentation method that avoids a priori assumptions about the canopy structure, (ii) a vertical canopy stratification procedure that improves segmentation of understory trees, (iii) an occlusion model for estimating the point density of each canopy stratum, and (iv) a distributed computing approach for efficient processing at the forest level. When applied to the University of Kentucky Robinson Forest, the segmentation method detected about 90% of overstory and 47% of understory trees with over-segmentation rates of 14% and 2%. Stratifying the canopy improved the detection rate of understory trees to 68% at the cost of increasing their over-segmentations to 16%. According to our occlusion model, a point density of ~170 pt/m-sqr is needed to segment understory trees as accurately as overstory trees. Lastly, using the distributed approach, we segmented about two million trees in the 7,440-ha forest in 2.5 hours using 192 processors, which is 167 times faster than using a single processor. Keywords: individual tree segmentation, multi-layered stand, vertical canopy stratification, segmentation evaluation, point density, canopy occlusion effect, big data, distributed computing.



### Houdini: Fooling Deep Structured Prediction Models
- **Arxiv ID**: http://arxiv.org/abs/1707.05373v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.05373v1)
- **Published**: 2017-07-17 19:11:08+00:00
- **Updated**: 2017-07-17 19:11:08+00:00
- **Authors**: Moustapha Cisse, Yossi Adi, Natalia Neverova, Joseph Keshet
- **Comment**: 12 pages, 8 figures, under review
- **Journal**: None
- **Summary**: Generating adversarial examples is a critical step for evaluating and improving the robustness of learning machines. So far, most existing methods only work for classification and are not designed to alter the true performance measure of the problem at hand. We introduce a novel flexible approach named Houdini for generating adversarial examples specifically tailored for the final performance measure of the task considered, be it combinatorial and non-decomposable. We successfully apply Houdini to a range of applications such as speech recognition, pose estimation and semantic segmentation. In all cases, the attacks based on Houdini achieve higher success rate than those based on the traditional surrogates used to train the models while using a less perceptible adversarial perturbation.



### Make Your Bone Great Again : A study on Osteoporosis Classification
- **Arxiv ID**: http://arxiv.org/abs/1707.05385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05385v1)
- **Published**: 2017-07-17 20:22:20+00:00
- **Updated**: 2017-07-17 20:22:20+00:00
- **Authors**: Rahul Paul, Saeed Alahamri, Sulav Malla, Ghulam Jilani Quadri
- **Comment**: None
- **Journal**: None
- **Summary**: Osteoporosis can be identified by looking at 2D x-ray images of the bone. The high degree of similarity between images of a healthy bone and a diseased one makes classification a challenge. A good bone texture characterization technique is essential for identifying osteoporosis cases. Standard texture feature extraction techniques like Local Binary Pattern (LBP), Gray Level Co-occurrence Matrix (GLCM) have been used for this purpose. In this paper, we draw a comparison between deep features extracted from convolution neural network against these traditional features. Our results show that deep features have more discriminative power as classifiers trained on them always outperform the ones trained on traditional features.



### Benchmarking and Error Diagnosis in Multi-Instance Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1707.05388v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05388v2)
- **Published**: 2017-07-17 20:32:37+00:00
- **Updated**: 2017-08-05 00:55:29+00:00
- **Authors**: Matteo Ruggero Ronchi, Pietro Perona
- **Comment**: Project page available at
  http://www.vision.caltech.edu/~mronchi/projects/PoseErrorDiagnosis/; Code
  available at https://github.com/matteorr/coco-analyze; published at ICCV 17
- **Journal**: None
- **Summary**: We propose a new method to analyze the impact of errors in algorithms for multi-instance pose estimation and a principled benchmark that can be used to compare them. We define and characterize three classes of errors - localization, scoring, and background - study how they are influenced by instance attributes and their impact on an algorithm's performance. Our technique is applied to compare the two leading methods for human pose estimation on the COCO Dataset, measure the sensitivity of pose estimation with respect to instance size, type and number of visible keypoints, clutter due to multiple instances, and the relative score of instances. The performance of algorithms, and the types of error they make, are highly dependent on all these variables, but mostly on the number of keypoints and the clutter. The analysis and software tools we propose offer a novel and insightful approach for understanding the behavior of pose estimation algorithms and an effective method for measuring their strengths and weaknesses.



### Freehand Ultrasound Image Simulation with Spatially-Conditioned Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.05392v1
- **DOI**: 10.1007/978-3-319-67564-0_11
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.05392v1)
- **Published**: 2017-07-17 20:48:28+00:00
- **Updated**: 2017-07-17 20:48:28+00:00
- **Authors**: Yipeng Hu, Eli Gibson, Li-Lin Lee, Weidi Xie, Dean C. Barratt, Tom Vercauteren, J. Alison Noble
- **Comment**: Accepted to MICCAI RAMBO 2017
- **Journal**: None
- **Summary**: Sonography synthesis has a wide range of applications, including medical procedure simulation, clinical training and multimodality image registration. In this paper, we propose a machine learning approach to simulate ultrasound images at given 3D spatial locations (relative to the patient anatomy), based on conditional generative adversarial networks (GANs). In particular, we introduce a novel neural network architecture that can sample anatomically accurate images conditionally on spatial position of the (real or mock) freehand ultrasound probe. To ensure an effective and efficient spatial information assimilation, the proposed spatially-conditioned GANs take calibrated pixel coordinates in global physical space as conditioning input, and utilise residual network units and shortcuts of conditioning data in the GANs' discriminator and generator, respectively. Using optically tracked B-mode ultrasound images, acquired by an experienced sonographer on a fetus phantom, we demonstrate the feasibility of the proposed method by two sets of quantitative results: distances were calculated between corresponding anatomical landmarks identified in the held-out ultrasound images and the simulated data at the same locations unseen to the networks; a usability study was carried out to distinguish the simulated data from the real images. In summary, we present what we believe are state-of-the-art visually realistic ultrasound images, simulated by the proposed GAN architecture that is stable to train and capable of generating plausibly diverse image samples.



### Incremental Boosting Convolutional Neural Network for Facial Action Unit Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.05395v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05395v1)
- **Published**: 2017-07-17 21:04:13+00:00
- **Updated**: 2017-07-17 21:04:13+00:00
- **Authors**: Shizhong Han, Zibo Meng, Ahmed Shehab Khan, Yan Tong
- **Comment**: NIPS2016
- **Journal**: None
- **Summary**: Recognizing facial action units (AUs) from spontaneous facial expressions is still a challenging problem. Most recently, CNNs have shown promise on facial AU recognition. However, the learned CNNs are often overfitted and do not generalize well to unseen subjects due to limited AU-coded training images. We proposed a novel Incremental Boosting CNN (IB-CNN) to integrate boosting into the CNN via an incremental boosting layer that selects discriminative neurons from the lower layer and is incrementally updated on successive mini-batches. In addition, a novel loss function that accounts for errors from both the incremental boosted classifier and individual weak classifiers was proposed to fine-tune the IB-CNN. Experimental results on four benchmark AU databases have demonstrated that the IB-CNN yields significant improvement over the traditional CNN and the boosting CNN without incremental learning, as well as outperforming the state-of-the-art CNN-based methods in AU recognition. The improvement is more impressive for the AUs that have the lowest frequencies in the databases.



### Slanted Stixels: Representing San Francisco's Steepest Streets
- **Arxiv ID**: http://arxiv.org/abs/1707.05397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05397v1)
- **Published**: 2017-07-17 21:14:26+00:00
- **Updated**: 2017-07-17 21:14:26+00:00
- **Authors**: Daniel Hernandez-Juarez, Lukas Schneider, Antonio Espinosa, David Vázquez, Antonio M. López, Uwe Franke, Marc Pollefeys, Juan C. Moure
- **Comment**: Accepted to BMVC 2017 as oral presentation
- **Journal**: None
- **Summary**: In this work we present a novel compact scene representation based on Stixels that infers geometric and semantic information. Our approach overcomes the previous rather restrictive geometric assumptions for Stixels by introducing a novel depth model to account for non-flat roads and slanted objects. Both semantic and depth cues are used jointly to infer the scene representation in a sound global energy minimization formulation. Furthermore, a novel approximation scheme is introduced that uses an extremely efficient over-segmentation. In doing so, the computational complexity of the Stixel inference algorithm is reduced significantly, achieving real-time computation capabilities with only a slight drop in accuracy. We evaluate the proposed approach in terms of semantic and geometric accuracy as well as run-time on four publicly available benchmark datasets. Our approach maintains accuracy on flat road scene datasets while improving substantially on a novel non-flat road dataset.



### Hybrid PS-V Technique: A Novel Sensor Fusion Approach for Fast Mobile Eye-Tracking with Sensor-Shift Aware Correction
- **Arxiv ID**: http://arxiv.org/abs/1707.05411v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1707.05411v2)
- **Published**: 2017-07-17 23:26:09+00:00
- **Updated**: 2017-07-19 04:28:11+00:00
- **Authors**: Ioannis Rigas, Hayes Raffle, Oleg V. Komogortsev
- **Comment**: 11 pages, 14 figures
- **Journal**: None
- **Summary**: This paper introduces and evaluates a hybrid technique that fuses efficiently the eye-tracking principles of photosensor oculography (PSOG) and video oculography (VOG). The main concept of this novel approach is to use a few fast and power-economic photosensors as the core mechanism for performing high speed eye-tracking, whereas in parallel, use a video sensor operating at low sampling-rate (snapshot mode) to perform dead-reckoning error correction when sensor movements occur. In order to evaluate the proposed method, we simulate the functional components of the technique and present our results in experimental scenarios involving various combinations of horizontal and vertical eye and sensor movements. Our evaluation shows that the developed technique can be used to provide robustness to sensor shifts that otherwise could induce error larger than 5 deg. Our analysis suggests that the technique can potentially enable high speed eye-tracking at low power profiles, making it suitable to be used in emerging head-mounted devices, e.g. AR/VR headsets.



### Photosensor Oculography: Survey and Parametric Analysis of Designs using Model-Based Simulation
- **Arxiv ID**: http://arxiv.org/abs/1707.05413v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1707.05413v2)
- **Published**: 2017-07-17 23:31:57+00:00
- **Updated**: 2017-07-19 04:26:55+00:00
- **Authors**: Ioannis Rigas, Hayes Raffle, Oleg V. Komogortsev
- **Comment**: 12 pages, 18 figures
- **Journal**: None
- **Summary**: This paper presents a renewed overview of photosensor oculography (PSOG), an eye-tracking technique based on the principle of using simple photosensors to measure the amount of reflected (usually infrared) light when the eye rotates. Photosensor oculography can provide measurements with high precision, low latency and reduced power consumption, and thus it appears as an attractive option for performing eye-tracking in the emerging head-mounted interaction devices, e.g. augmented and virtual reality (AR/VR) headsets. In our current work we employ an adjustable simulation framework as a common basis for performing an exploratory study of the eye-tracking behavior of different photosensor oculography designs. With the performed experiments we explore the effects from the variation of some basic parameters of the designs on the resulting accuracy and cross-talk, which are crucial characteristics for the seamless operation of human-computer interaction applications based on eye-tracking. Our experimental results reveal the design trade-offs that need to be adopted to tackle the competing conditions that lead to optimum performance of different eye-tracking characteristics. We also present the transformations that arise in the eye-tracking output when sensor shifts occur, and assess the resulting degradation in accuracy for different combinations of eye movements and sensor shifts.



### Wide Inference Network for Image Denoising via Learning Pixel-distribution Prior
- **Arxiv ID**: http://arxiv.org/abs/1707.05414v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05414v5)
- **Published**: 2017-07-17 23:39:38+00:00
- **Updated**: 2018-06-03 21:25:45+00:00
- **Authors**: Peng Liu, Ruogu Fang
- **Comment**: There is a code issue that makes our work may be regarded as entirely
  out the way of the correct research direction. Therefore, we add the
  correction into abstract to answer the questions being often asked. Besides.
  we hope the most talent you may try to think about how to map the particular
  matrix to generative ones. Then, you may have a significant innovation
  published
- **Journal**: None
- **Summary**: We explore an innovative strategy for image denoising by using convolutional neural networks (CNN) to learn similar pixel-distribution features from noisy images. Many types of image noise follow a certain pixel-distribution in common, such as additive white Gaussian noise (AWGN). By increasing CNN's width with larger reception fields and more channels in each layer, CNNs can reveal the ability to extract more accurate pixel-distribution features. The key to our approach is a discovery that wider CNNs with more convolutions tend to learn the similar pixel-distribution features, which reveals a new strategy to solve low-level vision problems effectively that the inference mapping primarily relies on the priors behind the noise property instead of deeper CNNs with more stacked nonlinear layers. We evaluate our work, Wide inference Networks (WIN), on AWGN and demonstrate that by learning pixel-distribution features from images, WIN-based network consistently achieves significantly better performance than current state-of-the-art deep CNN-based methods in both quantitative and visual evaluations. \textit{Code and models are available at \url{https://github.com/cswin/WIN}}.



