# Arxiv Papers in cs.CV on 2017-07-24
### Group-wise Deep Co-saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1707.07381v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07381v2)
- **Published**: 2017-07-24 02:32:43+00:00
- **Updated**: 2017-07-25 05:58:49+00:00
- **Authors**: Lina Wei, Shanshan Zhao, Omar El Farouk Bourahla, Xi Li, Fei Wu
- **Comment**: IJCAI 2017
- **Journal**: None
- **Summary**: In this paper, we propose an end-to-end group-wise deep co-saliency detection approach to address the co-salient object discovery problem based on the fully convolutional network (FCN) with group input and group output. The proposed approach captures the group-wise interaction information for group images by learning a semantics-aware image representation based on a convolutional neural network, which adaptively learns the group-wise features for co-saliency detection. Furthermore, the proposed approach discovers the collaborative and interactive relationships between group-wise feature representation and single-image individual feature representation, and model this in a collaborative learning framework. Finally, we set up a unified end-to-end deep learning scheme to jointly optimize the process of group-wise feature representation learning and the collaborative learning, leading to more reliable and robust co-saliency detection results. Experimental results demonstrate the effectiveness of our approach in comparison with the state-of-the-art approaches.



### Semantic 3D Occupancy Mapping through Efficient High Order CRFs
- **Arxiv ID**: http://arxiv.org/abs/1707.07388v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1707.07388v1)
- **Published**: 2017-07-24 03:24:52+00:00
- **Updated**: 2017-07-24 03:24:52+00:00
- **Authors**: Shichao Yang, Yulan Huang, Sebastian Scherer
- **Comment**: IROS 2017
- **Journal**: None
- **Summary**: Semantic 3D mapping can be used for many applications such as robot navigation and virtual interaction. In recent years, there has been great progress in semantic segmentation and geometric 3D mapping. However, it is still challenging to combine these two tasks for accurate and large-scale semantic mapping from images. In the paper, we propose an incremental and (near) real-time semantic mapping system. A 3D scrolling occupancy grid map is built to represent the world, which is memory and computationally efficient and bounded for large scale environments. We utilize the CNN segmentation as prior prediction and further optimize 3D grid labels through a novel CRF model. Superpixels are utilized to enforce smoothness and form robust P N high order potential. An efficient mean field inference is developed for the graph optimization. We evaluate our system on the KITTI dataset and improve the segmentation accuracy by 10% over existing systems.



### Contrastive-center loss for deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1707.07391v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07391v2)
- **Published**: 2017-07-24 03:45:47+00:00
- **Updated**: 2017-08-23 12:17:30+00:00
- **Authors**: Ce Qi, Fei Su
- **Comment**: None
- **Journal**: None
- **Summary**: The deep convolutional neural network(CNN) has significantly raised the performance of image classification and face recognition. Softmax is usually used as supervision, but it only penalizes the classification loss. In this paper, we propose a novel auxiliary supervision signal called contrastivecenter loss, which can further enhance the discriminative power of the features, for it learns a class center for each class. The proposed contrastive-center loss simultaneously considers intra-class compactness and inter-class separability, by penalizing the contrastive values between: (1)the distances of training samples to their corresponding class centers, and (2)the sum of the distances of training samples to their non-corresponding class centers. Experiments on different datasets demonstrate the effectiveness of contrastive-center loss.



### Wavelet Convolutional Neural Networks for Texture Classification
- **Arxiv ID**: http://arxiv.org/abs/1707.07394v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.07394v1)
- **Published**: 2017-07-24 03:59:04+00:00
- **Updated**: 2017-07-24 03:59:04+00:00
- **Authors**: Shin Fujieda, Kohei Takayama, Toshiya Hachisuka
- **Comment**: 9 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: Texture classification is an important and challenging problem in many image processing applications. While convolutional neural networks (CNNs) achieved significant successes for image classification, texture classification remains a difficult problem since textures usually do not contain enough information regarding the shape of object. In image processing, texture classification has been traditionally studied well with spectral analyses which exploit repeated structures in many textures. Since CNNs process images as-is in the spatial domain whereas spectral analyses process images in the frequency domain, these models have different characteristics in terms of performance. We propose a novel CNN architecture, wavelet CNNs, which integrates a spectral analysis into CNNs. Our insight is that the pooling layer and the convolution layer can be viewed as a limited form of a spectral analysis. Based on this insight, we generalize both layers to perform a spectral analysis with wavelet transform. Wavelet CNNs allow us to utilize spectral information which is lost in conventional CNNs but useful in texture classification. The experiments demonstrate that our model achieves better accuracy in texture classification than existing models. We also show that our model has significantly fewer parameters than CNNs, making our model easier to train with less memory.



### Synthesizing Robust Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1707.07397v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07397v3)
- **Published**: 2017-07-24 04:17:33+00:00
- **Updated**: 2018-06-07 16:25:12+00:00
- **Authors**: Anish Athalye, Logan Engstrom, Andrew Ilyas, Kevin Kwok
- **Comment**: ICML 2018
- **Journal**: None
- **Summary**: Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.



### Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM
- **Arxiv ID**: http://arxiv.org/abs/1707.09870v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09870v2)
- **Published**: 2017-07-24 04:50:50+00:00
- **Updated**: 2017-09-13 03:21:48+00:00
- **Authors**: Cong Leng, Hao Li, Shenghuo Zhu, Rong Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep learning models are highly effective for various learning tasks, their high computational costs prohibit the deployment to scenarios where either memory or computational resources are limited. In this paper, we focus on compressing and accelerating deep models with network weights represented by very small numbers of bits, referred to as extremely low bit neural network. We model this problem as a discretely constrained optimization problem. Borrowing the idea from Alternating Direction Method of Multipliers (ADMM), we decouple the continuous parameters from the discrete constraints of network, and cast the original hard problem into several subproblems. We propose to solve these subproblems using extragradient and iterative quantization algorithms that lead to considerably faster convergency compared to conventional optimization methods. Extensive experiments on image recognition and object detection verify that the proposed algorithm is more effective than state-of-the-art approaches when coming to extremely low bit neural network.



### Toward Geometric Deep SLAM
- **Arxiv ID**: http://arxiv.org/abs/1707.07410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07410v1)
- **Published**: 2017-07-24 05:41:35+00:00
- **Updated**: 2017-07-24 05:41:35+00:00
- **Authors**: Daniel DeTone, Tomasz Malisiewicz, Andrew Rabinovich
- **Comment**: None
- **Journal**: None
- **Summary**: We present a point tracking system powered by two deep convolutional neural networks. The first network, MagicPoint, operates on single images and extracts salient 2D points. The extracted points are "SLAM-ready" because they are by design isolated and well-distributed throughout the image. We compare this network against classical point detectors and discover a significant performance gap in the presence of image noise. As transformation estimation is more simple when the detected points are geometrically stable, we designed a second network, MagicWarp, which operates on pairs of point images (outputs of MagicPoint), and estimates the homography that relates the inputs. This transformation engine differs from traditional approaches because it does not use local point descriptors, only point locations. Both networks are trained with simple synthetic data, alleviating the requirement of expensive external camera ground truthing and advanced graphics rendering pipelines. The system is fast and lean, easily running 30+ FPS on a single CPU.



### Traffic scene recognition based on deep cnn and vlad spatial pyramids
- **Arxiv ID**: http://arxiv.org/abs/1707.07411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07411v1)
- **Published**: 2017-07-24 05:44:39+00:00
- **Updated**: 2017-07-24 05:44:39+00:00
- **Authors**: Fang-Yu Wu, Shi-Yang Yan, Jeremy S. Smith, Bai-Ling Zhang
- **Comment**: 6 pages,4 figures, 2017 9th International Conference on Machine
  Learning and Computing (ICMLC 2017)
- **Journal**: None
- **Summary**: Traffic scene recognition is an important and challenging issue in Intelligent Transportation Systems (ITS). Recently, Convolutional Neural Network (CNN) models have achieved great success in many applications, including scene classification. The remarkable representational learning capability of CNN remains to be further explored for solving real-world problems. Vector of Locally Aggregated Descriptors (VLAD) encoding has also proved to be a powerful method in catching global contextual information. In this paper, we attempted to solve the traffic scene recognition problem by combining the features representational capabilities of CNN with the VLAD encoding scheme. More specifically, the CNN features of image patches generated by a region proposal algorithm are encoded by applying VLAD, which subsequently represent an image in a compact representation. To catch the spatial information, spatial pyramids are exploited to encode CNN features. We experimented with a dataset of 10 categories of traffic scenes, with satisfactory categorization performances.



### Generative OpenMax for Multi-Class Open Set Classification
- **Arxiv ID**: http://arxiv.org/abs/1707.07418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07418v1)
- **Published**: 2017-07-24 06:45:05+00:00
- **Updated**: 2017-07-24 06:45:05+00:00
- **Authors**: ZongYuan Ge, Sergey Demyanov, Zetao Chen, Rahil Garnavi
- **Comment**: None
- **Journal**: None
- **Summary**: We present a conceptually new and flexible method for multi-class open set classification. Unlike previous methods where unknown classes are inferred with respect to the feature or decision distance to the known classes, our approach is able to provide explicit modelling and decision score for unknown classes. The proposed method, called Gener- ative OpenMax (G-OpenMax), extends OpenMax by employing generative adversarial networks (GANs) for novel category image synthesis. We validate the proposed method on two datasets of handwritten digits and characters, resulting in superior results over previous deep learning based method OpenMax Moreover, G-OpenMax provides a way to visualize samples representing the unknown classes from open space. Our simple and effective approach could serve as a new direction to tackle the challenging multi-class open set classification problem.



### LV-ROVER: Lexicon Verified Recognizer Output Voting Error Reduction
- **Arxiv ID**: http://arxiv.org/abs/1707.07432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07432v1)
- **Published**: 2017-07-24 08:04:09+00:00
- **Updated**: 2017-07-24 08:04:09+00:00
- **Authors**: Bruno Stuner, Clément Chatelain, Thierry Paquet
- **Comment**: Submitted to Pattern Recognition Letters
- **Journal**: None
- **Summary**: Offline handwritten text line recognition is a hard task that requires both an efficient optical character recognizer and language model. Handwriting recognition state of the art methods are based on Long Short Term Memory (LSTM) recurrent neural networks (RNN) coupled with the use of linguistic knowledge. Most of the proposed approaches in the literature focus on improving one of the two components and use constraint, dedicated to a database lexicon. However, state of the art performance is achieved by combining multiple optical models, and possibly multiple language models with the Recognizer Output Voting Error Reduction (ROVER) framework. Though handwritten line recognition with ROVER has been implemented by combining only few recognizers because training multiple complete recognizers is hard. In this paper we propose a Lexicon Verified ROVER: LV-ROVER, that has a reduce complexity compare to the original one and that can combine hundreds of recognizers without language models. We achieve state of the art for handwritten line text on the RIMES dataset.



### Feature Extraction via Recurrent Random Deep Ensembles and its Application in Gruop-level Happiness Estimation
- **Arxiv ID**: http://arxiv.org/abs/1707.09871v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.09871v1)
- **Published**: 2017-07-24 08:16:43+00:00
- **Updated**: 2017-07-24 08:16:43+00:00
- **Authors**: Shitao Tang, Yichen Pan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel ensemble framework to extract highly discriminative feature representation of image and its application for group-level happpiness intensity prediction in wild. In order to generate enough diversity of decisions, n convolutional neural networks are trained by bootstrapping the training set and extract n features for each image from them. A recurrent neural network (RNN) is then used to remember which network extracts better feature and generate the final feature representation for one individual image. Several group emotion models (GEM) are used to aggregate face fea- tures in a group and use parameter-optimized support vector regressor (SVR) to get the final results. Through extensive experiments, the great effectiveness of the proposed recurrent random deep ensembles (RRDE) is demonstrated in both structural and decisional ways. The best result yields a 0.55 root-mean-square error (RMSE) on validation set of HAPPEI dataset, significantly better than the baseline of 0.78.



### Delineation of line patterns in images using B-COSFIRE filters
- **Arxiv ID**: http://arxiv.org/abs/1707.07438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07438v1)
- **Published**: 2017-07-24 08:28:59+00:00
- **Updated**: 2017-07-24 08:28:59+00:00
- **Authors**: Nicola Strisciuglio, Nicolai Petkov
- **Comment**: International Work Conference on Bioinspired Intelligence, July
  10-13, 2017
- **Journal**: None
- **Summary**: Delineation of line patterns in images is a basic step required in various applications such as blood vessel detection in medical images, segmentation of rivers or roads in aerial images, detection of cracks in walls or pavements, etc. In this paper we present trainable B-COSFIRE filters, which are a model of some neurons in area V1 of the primary visual cortex, and apply it to the delineation of line patterns in different kinds of images. B-COSFIRE filters are trainable as their selectivity is determined in an automatic configuration process given a prototype pattern of interest. They are configurable to detect any preferred line structure (e.g. segments, corners, cross-overs, etc.), so usable for automatic data representation learning. We carried out experiments on two data sets, namely a line-network data set from INRIA and a data set of retinal fundus images named IOSTAR. The results that we achieved confirm the robustness of the proposed approach and its effectiveness in the delineation of line structures in different kinds of images.



### Full-Network Embedding in a Multimodal Embedding Pipeline
- **Arxiv ID**: http://arxiv.org/abs/1707.09872v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1707.09872v2)
- **Published**: 2017-07-24 10:27:33+00:00
- **Updated**: 2017-08-09 13:11:42+00:00
- **Authors**: Armand Vilalta, Dario Garcia-Gasulla, Ferran Parés, Eduard Ayguadé, Jesus Labarta, Ulises Cortés, Toyotaro Suzumura
- **Comment**: In 2nd Workshop on Semantic Deep Learning (SemDeep-2) at the 12th
  International Conference on Computational Semantics (IWCS) 2017
- **Journal**: None
- **Summary**: The current state-of-the-art for image annotation and image retrieval tasks is obtained through deep neural networks, which combine an image representation and a text representation into a shared embedding space. In this paper we evaluate the impact of using the Full-Network embedding in this setting, replacing the original image representation in a competitive multimodal embedding generation scheme. Unlike the one-layer image embeddings typically used by most approaches, the Full-Network embedding provides a multi-scale representation of images, which results in richer characterizations. To measure the influence of the Full-Network embedding, we evaluate its performance on three different datasets, and compare the results with the original multimodal embedding generation scheme when using a one-layer image embedding, and with the rest of the state-of-the-art. Results for image annotation and image retrieval tasks indicate that the Full-Network embedding is consistently superior to the one-layer embedding. These results motivate the integration of the Full-Network embedding on any multimodal embedding generation scheme, something feasible thanks to the flexibility of the approach.



### Infinite Latent Feature Selection: A Probabilistic Latent Graph-Based Ranking Approach
- **Arxiv ID**: http://arxiv.org/abs/1707.07538v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07538v1)
- **Published**: 2017-07-24 13:21:25+00:00
- **Updated**: 2017-07-24 13:21:25+00:00
- **Authors**: Giorgio Roffo, Simone Melzi, Umberto Castellani, Alessandro Vinciarelli
- **Comment**: Accepted at the IEEE International Conference on Computer Vision
  (ICCV), 2017, Venice. Preprint copy
- **Journal**: None
- **Summary**: Feature selection is playing an increasingly significant role with respect to many computer vision applications spanning from object recognition to visual object tracking. However, most of the recent solutions in feature selection are not robust across different and heterogeneous set of data. In this paper, we address this issue proposing a robust probabilistic latent graph-based feature selection algorithm that performs the ranking step while considering all the possible subsets of features, as paths on a graph, bypassing the combinatorial problem analytically. An appealing characteristic of the approach is that it aims to discover an abstraction behind low-level sensory data, that is, relevancy. Relevancy is modelled as a latent variable in a PLSA-inspired generative process that allows the investigation of the importance of a feature when injected into an arbitrary set of cues. The proposed method has been tested on ten diverse benchmarks, and compared against eleven state of the art feature selection methods. Results show that the proposed approach attains the highest performance levels across many different scenarios and difficulties, thereby confirming its strong robustness while setting a new state of the art in feature selection domain.



### Towards Accurate Markerless Human Shape and Pose Estimation over Time
- **Arxiv ID**: http://arxiv.org/abs/1707.07548v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07548v5)
- **Published**: 2017-07-24 13:31:37+00:00
- **Updated**: 2018-04-30 12:19:54+00:00
- **Authors**: Yinghao Huang, Federica Bogo, Christoph Lassner, Angjoo Kanazawa, Peter V. Gehler, Ijaz Akhter, Michael J. Black
- **Comment**: 10 pages, 6 figures, 5 tables, published in 3DV-2017
- **Journal**: None
- **Summary**: Existing marker-less motion capture methods often assume known backgrounds, static cameras, and sequence specific motion priors, which narrows its application scenarios. Here we propose a fully automatic method that given multi-view video, estimates 3D human motion and body shape. We take recent SMPLify \cite{bogo2016keep} as the base method, and extend it in several ways. First we fit the body to 2D features detected in multi-view images. Second, we use a CNN method to segment the person in each image and fit the 3D body model to the contours to further improves accuracy. Third we utilize a generic and robust DCT temporal prior to handle the left and right side swapping issue sometimes introduced by the 2D pose estimator. Validation on standard benchmarks shows our results are comparable to the state of the art and also provide a realistic 3D shape avatar. We also demonstrate accurate results on HumanEva and on challenging dance sequences from YouTube in monocular case.



### Automatic breast cancer grading in lymph nodes using a deep neural network
- **Arxiv ID**: http://arxiv.org/abs/1707.07565v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.07565v1)
- **Published**: 2017-07-24 14:09:47+00:00
- **Updated**: 2017-07-24 14:09:47+00:00
- **Authors**: Thomas Wollmann, Karl Rohr
- **Comment**: None
- **Journal**: None
- **Summary**: The progression of breast cancer can be quantified in lymph node whole-slide images (WSIs). We describe a novel method for effectively performing classification of whole-slide images and patient level breast cancer grading. Our method utilises a deep neural network. The method performs classification on small patches and uses model averaging for boosting. In the first step, region of interest patches are determined and cropped automatically by color thresholding and then classified by the deep neural network. The classification results are used to determine a slide level class and for further aggregation to predict a patient level grade. Fast processing speed of our method enables high throughput image analysis.



### Joint Background Reconstruction and Foreground Segmentation via A Two-stage Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1707.07584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07584v1)
- **Published**: 2017-07-24 14:45:58+00:00
- **Updated**: 2017-07-24 14:45:58+00:00
- **Authors**: Xu Zhao, Yingying Chen, Ming Tang, Jinqiao Wang
- **Comment**: ICME 2017
- **Journal**: None
- **Summary**: Foreground segmentation in video sequences is a classic topic in computer vision. Due to the lack of semantic and prior knowledge, it is difficult for existing methods to deal with sophisticated scenes well. Therefore, in this paper, we propose an end-to-end two-stage deep convolutional neural network (CNN) framework for foreground segmentation in video sequences. In the first stage, a convolutional encoder-decoder sub-network is employed to reconstruct the background images and encode rich prior knowledge of background scenes. In the second stage, the reconstructed background and current frame are input into a multi-channel fully-convolutional sub-network (MCFCN) for accurate foreground segmentation. In the two-stage CNN, the reconstruction loss and segmentation loss are jointly optimized. The background images and foreground objects are output simultaneously in an end-to-end way. Moreover, by incorporating the prior semantic knowledge of foreground and background in the pre-training process, our method could restrain the background noise and keep the integrity of foreground objects at the same time. Experiments on CDNet 2014 show that our method outperforms the state-of-the-art by 4.9%.



### Image Pivoting for Learning Multilingual Multimodal Representations
- **Arxiv ID**: http://arxiv.org/abs/1707.07601v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.07601v1)
- **Published**: 2017-07-24 15:08:13+00:00
- **Updated**: 2017-07-24 15:08:13+00:00
- **Authors**: Spandana Gella, Rico Sennrich, Frank Keller, Mirella Lapata
- **Comment**: 7 pages, EMNLP 2017
- **Journal**: None
- **Summary**: In this paper we propose a model to learn multimodal multilingual representations for matching images and sentences in different languages, with the aim of advancing multilingual versions of image search and image understanding. Our model learns a common representation for images and their descriptions in two different languages (which need not be parallel) by considering the image as a pivot between two languages. We introduce a new pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance.



### Vision-Based Fallen Person Detection for the Elderly
- **Arxiv ID**: http://arxiv.org/abs/1707.07608v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07608v2)
- **Published**: 2017-07-24 15:33:49+00:00
- **Updated**: 2017-08-15 14:28:06+00:00
- **Authors**: Markus D. Solbach, John K. Tsotsos
- **Comment**: Accepted at ACVR 2017 Project page:
  https://github.com/TsotsosLab/fallen-person-detector
- **Journal**: None
- **Summary**: Falls are serious and costly for elderly people. The Centers for Disease Control and Prevention of the US reports that millions of older people, 65 and older, fall each year at least once. Serious injuries such as; hip fractures, broken bones or head injury, are caused by 20% of the falls. The time it takes to respond and treat a fallen person is crucial. With this paper we present a new , non-invasive system for fallen people detection. Our approach uses only stereo camera data for passively sensing the environment. The key novelty is a human fall detector which uses a CNN based human pose estimator in combination with stereo data to reconstruct the human pose in 3D and estimate the ground plane in 3D. Furthermore, our system consists of a reasoning module which formulates a number of measures to reason whether a person is fallen. We have tested our approach in different scenarios covering most activities elderly people might encounter living at home. Based on our extensive evaluations, our systems shows high accuracy and almost no miss-classification. To reproduce our results, the implementation is publicly available to the scientific community.



### Liver lesion segmentation informed by joint liver segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.07734v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07734v3)
- **Published**: 2017-07-24 20:03:46+00:00
- **Updated**: 2018-08-11 21:31:45+00:00
- **Authors**: Eugene Vorontsov, An Tang, Chris Pal, Samuel Kadoury
- **Comment**: Late upload of conference version (ISBI)
- **Journal**: None
- **Summary**: We propose a model for the joint segmentation of the liver and liver lesions in computed tomography (CT) volumes. We build the model from two fully convolutional networks, connected in tandem and trained together end-to-end. We evaluate our approach on the 2017 MICCAI Liver Tumour Segmentation Challenge, attaining competitive liver and liver lesion detection and segmentation scores across a wide range of metrics. Unlike other top performing methods, our model output post-processing is trivial, we do not use data external to the challenge, and we propose a simple single-stage model that is trained end-to-end. However, our method nearly matches the top lesion segmentation performance and achieves the second highest precision for lesion detection while maintaining high recall.



### Detection of curved lines with B-COSFIRE filters: A case study on crack delineation
- **Arxiv ID**: http://arxiv.org/abs/1707.07747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07747v1)
- **Published**: 2017-07-24 20:36:55+00:00
- **Updated**: 2017-07-24 20:36:55+00:00
- **Authors**: Nicola Strisciuglio, George Azzopardi, Nicolai Petkov
- **Comment**: Accepted at Computer Analysis of Images and Patterns (CAIP) 2017
- **Journal**: None
- **Summary**: The detection of curvilinear structures is an important step for various computer vision applications, ranging from medical image analysis for segmentation of blood vessels, to remote sensing for the identification of roads and rivers, and to biometrics and robotics, among others. %The visual system of the brain has remarkable abilities to detect curvilinear structures in noisy images. This is a nontrivial task especially for the detection of thin or incomplete curvilinear structures surrounded with noise. We propose a general purpose curvilinear structure detector that uses the brain-inspired trainable B-COSFIRE filters. It consists of four main steps, namely nonlinear filtering with B-COSFIRE, thinning with non-maximum suppression, hysteresis thresholding and morphological closing. We demonstrate its effectiveness on a data set of noisy images with cracked pavements, where we achieve state-of-the-art results (F-measure=0.865). The proposed method can be employed in any computer vision methodology that requires the delineation of curvilinear and elongated structures.



### Human Pose Forecasting via Deep Markov Models
- **Arxiv ID**: http://arxiv.org/abs/1707.09240v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.09240v2)
- **Published**: 2017-07-24 23:50:23+00:00
- **Updated**: 2017-09-05 23:26:48+00:00
- **Authors**: Sam Toyer, Anoop Cherian, Tengda Han, Stephen Gould
- **Comment**: Accepted to DICTA'17
- **Journal**: None
- **Summary**: Human pose forecasting is an important problem in computer vision with applications to human-robot interaction, visual surveillance, and autonomous driving. Usually, forecasting algorithms use 3D skeleton sequences and are trained to forecast for a few milliseconds into the future. Long-range forecasting is challenging due to the difficulty of estimating how long a person continues an activity. To this end, our contributions are threefold: (i) we propose a generative framework for poses using variational autoencoders based on Deep Markov Models (DMMs); (ii) we evaluate our pose forecasts using a pose-based action classifier, which we argue better reflects the subjective quality of pose forecasts than distance in coordinate space; (iii) last, for evaluation of the new model, we introduce a 480,000-frame video dataset called Ikea Furniture Assembly (Ikea FA), which depicts humans repeatedly assembling and disassembling furniture. We demonstrate promising results for our approach on both Ikea FA and the existing NTU RGB+D dataset.



