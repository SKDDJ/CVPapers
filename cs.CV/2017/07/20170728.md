# Arxiv Papers in cs.CV on 2017-07-28
### Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions
- **Arxiv ID**: http://arxiv.org/abs/1707.09092v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09092v3)
- **Published**: 2017-07-28 02:51:11+00:00
- **Updated**: 2018-04-04 18:08:02+00:00
- **Authors**: Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, Tomas Pajdla
- **Comment**: Accepted to CVPR 2018 as a spotlight
- **Journal**: None
- **Summary**: Visual localization enables autonomous vehicles to navigate in their surroundings and augmented reality applications to link virtual to real worlds. Practical visual localization approaches need to be robust to a wide variety of viewing condition, including day-night changes, as well as weather and seasonal variations, while providing highly accurate 6 degree-of-freedom (6DOF) camera pose estimates. In this paper, we introduce the first benchmark datasets specifically designed for analyzing the impact of such factors on visual localization. Using carefully created ground truth poses for query images taken under a wide variety of conditions, we evaluate the impact of various factors on 6DOF camera pose estimation accuracy through extensive experiments with state-of-the-art localization approaches. Based on our results, we draw conclusions about the difficulty of different conditions, showing that long-term localization is far from solved, and propose promising avenues for future work, including sequence-based localization approaches and the need for better local features. Our benchmark is available at visuallocalization.net.



### Object Detection of Satellite Images Using Multi-Channel Higher-order Local Autocorrelation
- **Arxiv ID**: http://arxiv.org/abs/1707.09099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09099v1)
- **Published**: 2017-07-28 03:59:27+00:00
- **Updated**: 2017-07-28 03:59:27+00:00
- **Authors**: Kazuki Uehara, Hidenori Sakanashi, Hirokazu Nosato, Masahiro Murakawa, Hiroki Miyamoto, Ryosuke Nakamura
- **Comment**: 6 pages, 2 column, 7 figures, Accepted by IEEE International
  Conference on Systems, Man, and Cybernetics (SMC) 2017
- **Journal**: None
- **Summary**: The Earth observation satellites have been monitoring the earth's surface for a long time, and the images taken by the satellites contain large amounts of valuable data. However, it is extremely hard work to manually analyze such huge data. Thus, a method of automatic object detection is needed for satellite images to facilitate efficient data analyses. This paper describes a new image feature extended from higher-order local autocorrelation to the object detection of multispectral satellite images. The feature has been extended to extract spectral inter-relationships in addition to spatial relationships to fully exploit multispectral information. The results of experiments with object detection tasks conducted to evaluate the effectiveness of the proposed feature extension indicate that the feature realized a higher performance compared to existing methods.



### MixedPeds: Pedestrian Detection in Unannotated Videos using Synthetically Generated Human-agents for Training
- **Arxiv ID**: http://arxiv.org/abs/1707.09100v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09100v2)
- **Published**: 2017-07-28 04:05:33+00:00
- **Updated**: 2017-11-11 19:12:55+00:00
- **Authors**: Ernest C. Cheung, Tsan Kwong Wong, Aniket Bera, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new method for training pedestrian detectors on an unannotated set of images. We produce a mixed reality dataset that is composed of real-world background images and synthetically generated static human-agents. Our approach is general, robust, and makes no other assumptions about the unannotated dataset regarding the number or location of pedestrians. We automatically extract from the dataset: i) the vanishing point to calibrate the virtual camera, and ii) the pedestrians' scales to generate a Spawn Probability Map, which is a novel concept that guides our algorithm to place the pedestrians at appropriate locations. After putting synthetic human-agents in the unannotated images, we use these augmented images to train a Pedestrian Detector, with the annotations generated along with the synthetic agents. We conducted our experiments using Faster R-CNN by comparing the detection results on the unannotated dataset performed by the detector trained using our approach and detectors trained with other manually labeled datasets. We showed that our approach improves the average precision by 5-13% over these detectors.



### Fine-Pruning: Joint Fine-Tuning and Compression of a Convolutional Network with Bayesian Optimization
- **Arxiv ID**: http://arxiv.org/abs/1707.09102v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09102v1)
- **Published**: 2017-07-28 04:40:32+00:00
- **Updated**: 2017-07-28 04:40:32+00:00
- **Authors**: Frederick Tung, Srikanth Muralidharan, Greg Mori
- **Comment**: BMVC 2017 oral
- **Journal**: None
- **Summary**: When approaching a novel visual recognition problem in a specialized image domain, a common strategy is to start with a pre-trained deep neural network and fine-tune it to the specialized domain. If the target domain covers a smaller visual space than the source domain used for pre-training (e.g. ImageNet), the fine-tuned network is likely to be over-parameterized. However, applying network pruning as a post-processing step to reduce the memory requirements has drawbacks: fine-tuning and pruning are performed independently; pruning parameters are set once and cannot adapt over time; and the highly parameterized nature of state-of-the-art pruning methods make it prohibitive to manually search the pruning parameter space for deep networks, leading to coarse approximations. We propose a principled method for jointly fine-tuning and compressing a pre-trained convolutional network that overcomes these limitations. Experiments on two specialized image domains (remote sensing images and describable textures) demonstrate the validity of the proposed approach.



### Improved Face Detection and Alignment using Cascade Deep Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1707.09364v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.09364v1)
- **Published**: 2017-07-28 06:07:38+00:00
- **Updated**: 2017-07-28 06:07:38+00:00
- **Authors**: Weilin Cong, Sanyuan Zhao, Hui Tian, Jianbing Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world face detection and alignment demand an advanced discriminative model to address challenges by pose, lighting and expression. Illuminated by the deep learning algorithm, some convolutional neural networks based face detection and alignment methods have been proposed. Recent studies have utilized the relation between face detection and alignment to make models computationally efficiency, however they ignore the connection between each cascade CNNs. In this paper, we propose an structure to propose higher quality training data for End-to-End cascade network training, which give computers more space to automatic adjust weight parameter and accelerate convergence. Experiments demonstrate considerable improvement over existing detection and alignment models.



### Deep Co-Space: Sample Mining Across Feature Transformation for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.09119v1
- **DOI**: 10.1109/TCSVT.2017.2710478
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09119v1)
- **Published**: 2017-07-28 06:41:34+00:00
- **Updated**: 2017-07-28 06:41:34+00:00
- **Authors**: Ziliang Chen, Keze Wang, Xiao Wang, Pai Peng, Ebroul Izquierdo, Liang Lin
- **Comment**: To appear in IEEE Transactions on Circuits and Systems for Video
  Technology (T-CSVT), 2017
- **Journal**: None
- **Summary**: Aiming at improving performance of visual classification in a cost-effective manner, this paper proposes an incremental semi-supervised learning paradigm called Deep Co-Space (DCS). Unlike many conventional semi-supervised learning methods usually performing within a fixed feature space, our DCS gradually propagates information from labeled samples to unlabeled ones along with deep feature learning. We regard deep feature learning as a series of steps pursuing feature transformation, i.e., projecting the samples from a previous space into a new one, which tends to select the reliable unlabeled samples with respect to this setting. Specifically, for each unlabeled image instance, we measure its reliability by calculating the category variations of feature transformation from two different neighborhood variation perspectives, and merged them into an unified sample mining criterion deriving from Hellinger distance. Then, those samples keeping stable correlation to their neighboring samples (i.e., having small category variation in distribution) across the successive feature space transformation, are automatically received labels and incorporated into the model for incrementally training in terms of classification. Our extensive experiments on standard image classification benchmarks (e.g., Caltech-256 and SUN-397) demonstrate that the proposed framework is capable of effectively mining from large-scale unlabeled images, which boosts image classification performance and achieves promising results compared to other semi-supervised learning methods.



### Learning Pixel-Distribution Prior with Wider Convolution for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1707.09135v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09135v1)
- **Published**: 2017-07-28 08:00:21+00:00
- **Updated**: 2017-07-28 08:00:21+00:00
- **Authors**: Peng Liu, Ruogu Fang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we explore an innovative strategy for image denoising by using convolutional neural networks (CNN) to learn pixel-distribution from noisy data. By increasing CNN's width with large reception fields and more channels in each layer, CNNs can reveal the ability to learn pixel-distribution, which is a prior existing in many different types of noise. The key to our approach is a discovery that wider CNNs tends to learn the pixel-distribution features, which provides the probability of that inference-mapping primarily relies on the priors instead of deeper CNNs with more stacked nonlinear layers. We evaluate our work: Wide inference Networks (WIN) on additive white Gaussian noise (AWGN) and demonstrate that by learning the pixel-distribution in images, WIN-based network consistently achieves significantly better performance than current state-of-the-art deep CNN-based methods in both quantitative and visual evaluations. \textit{Code and models are available at \url{https://github.com/cswin/WIN}}.



### Localizing Actions from Video Labels and Pseudo-Annotations
- **Arxiv ID**: http://arxiv.org/abs/1707.09143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09143v1)
- **Published**: 2017-07-28 08:16:34+00:00
- **Updated**: 2017-07-28 08:16:34+00:00
- **Authors**: Pascal Mettes, Cees G. M. Snoek, Shih-Fu Chang
- **Comment**: BMVC
- **Journal**: None
- **Summary**: The goal of this paper is to determine the spatio-temporal location of actions in video. Where training from hard to obtain box annotations is the norm, we propose an intuitive and effective algorithm that localizes actions from their class label only. We are inspired by recent work showing that unsupervised action proposals selected with human point-supervision perform as well as using expensive box annotations. Rather than asking users to provide point supervision, we propose fully automatic visual cues that replace manual point annotations. We call the cues pseudo-annotations, introduce five of them, and propose a correlation metric for automatically selecting and combining them. Thorough evaluation on challenging action localization datasets shows that we reach results comparable to results with full box supervision. We also show that pseudo-annotations can be leveraged during testing to improve weakly- and strongly-supervised localizers.



### Spatial-Aware Object Embeddings for Zero-Shot Localization and Classification of Actions
- **Arxiv ID**: http://arxiv.org/abs/1707.09145v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09145v1)
- **Published**: 2017-07-28 08:21:11+00:00
- **Updated**: 2017-07-28 08:21:11+00:00
- **Authors**: Pascal Mettes, Cees G. M. Snoek
- **Comment**: ICCV
- **Journal**: None
- **Summary**: We aim for zero-shot localization and classification of human actions in video. Where traditional approaches rely on global attribute or object classification scores for their zero-shot knowledge transfer, our main contribution is a spatial-aware object embedding. To arrive at spatial awareness, we build our embedding on top of freely available actor and object detectors. Relevance of objects is determined in a word embedding space and further enforced with estimated spatial preferences. Besides local object awareness, we also embed global object awareness into our embedding to maximize actor and object interaction. Finally, we exploit the object positions and sizes in the spatial-aware embedding to demonstrate a new spatio-temporal action retrieval scenario with composite queries. Action localization and classification experiments on four contemporary action video datasets support our proposal. Apart from state-of-the-art results in the zero-shot localization and classification settings, our spatial-aware embedding is even competitive with recent supervised action localization alternatives.



### Group Re-Identification via Unsupervised Transfer of Sparse Features Encoding
- **Arxiv ID**: http://arxiv.org/abs/1707.09173v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09173v1)
- **Published**: 2017-07-28 10:02:14+00:00
- **Updated**: 2017-07-28 10:02:14+00:00
- **Authors**: Giuseppe Lisanti, Niki Martinel, Alberto Del Bimbo, Gian Luca Foresti
- **Comment**: This paper has been accepted for publication at ICCV 2017
- **Journal**: None
- **Summary**: Person re-identification is best known as the problem of associating a single person that is observed from one or more disjoint cameras. The existing literature has mainly addressed such an issue, neglecting the fact that people usually move in groups, like in crowded scenarios. We believe that the additional information carried by neighboring individuals provides a relevant visual context that can be exploited to obtain a more robust match of single persons within the group. Despite this, re-identifying groups of people compound the common single person re-identification problems by introducing changes in the relative position of persons within the group and severe self-occlusions. In this paper, we propose a solution for group re-identification that grounds on transferring knowledge from single person re-identification to group re-identification by exploiting sparse dictionary learning. First, a dictionary of sparse atoms is learned using patches extracted from single person images. Then, the learned dictionary is exploited to obtain a sparsity-driven residual group representation, which is finally matched to perform the re-identification. Extensive experiments on the i-LIDS groups and two newly collected datasets show that the proposed solution outperforms state-of-the-art approaches.



### A weighting strategy for Active Shape Models
- **Arxiv ID**: http://arxiv.org/abs/1707.09233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09233v1)
- **Published**: 2017-07-28 13:55:56+00:00
- **Updated**: 2017-07-28 13:55:56+00:00
- **Authors**: Alma Eguizabal, Peter J. Schreier
- **Comment**: None
- **Journal**: None
- **Summary**: Active Shape Models (ASM) are an iterative segmentation technique to find a landmark-based contour of an object. In each iteration, a least-squares fit of a plausible shape to some detected target landmarks is determined. Finding these targets is a critical step: some landmarks are more reliably detected than others, and some landmarks may not be within the field of view of their detectors. To add robustness while preserving simplicity at the same time, a generalized least-squares approach can be used, where a weighting matrix incorporates reliability information about the landmarks. We propose a strategy to choose this matrix, based on the covariance of empirically determined residuals of the fit. We perform a further step to determine whether the target landmarks are within the range of their detectors. We evaluate our strategy on fluoroscopic X-ray images to segment the femur. We show that our technique outperforms the standard ASM as well as other more heuristic weighted least-squares strategies.



### The WILDTRACK Multi-Camera Person Dataset
- **Arxiv ID**: http://arxiv.org/abs/1707.09299v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09299v1)
- **Published**: 2017-07-28 16:05:06+00:00
- **Updated**: 2017-07-28 16:05:06+00:00
- **Authors**: Tatjana Chavdarova, Pierre Baqué, Stéphane Bouquet, Andrii Maksai, Cijo Jose, Louis Lettry, Pascal Fua, Luc Van Gool, François Fleuret
- **Comment**: None
- **Journal**: None
- **Summary**: People detection methods are highly sensitive to the perpetual occlusions among the targets. As multi-camera set-ups become more frequently encountered, joint exploitation of the across views information would allow for improved detection performances. We provide a large-scale HD dataset named WILDTRACK which finally makes advanced deep learning methods applicable to this problem. The seven-static-camera set-up captures realistic and challenging scenarios of walking people.   Notably, its camera calibration with jointly high-precision projection widens the range of algorithms which may make use of this dataset. In aim to help accelerate the research on automatic camera calibration, such annotations also accompany this dataset.   Furthermore, the rich-in-appearance visual context of the pedestrian class makes this dataset attractive for monocular pedestrian detection as well, since: the HD cameras are placed relatively close to the people, and the size of the dataset further increases seven-fold.   In summary, we overview existing multi-camera datasets and detection methods, enumerate details of our dataset, and we benchmark multi-camera state of the art detectors on this new dataset.



### Sparse Deep Nonnegative Matrix Factorization
- **Arxiv ID**: http://arxiv.org/abs/1707.09316v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.1; G.1.6; H.2.8
- **Links**: [PDF](http://arxiv.org/pdf/1707.09316v1)
- **Published**: 2017-07-28 16:37:37+00:00
- **Updated**: 2017-07-28 16:37:37+00:00
- **Authors**: Zhenxing Guo, Shihua Zhang
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: Nonnegative matrix factorization is a powerful technique to realize dimension reduction and pattern recognition through single-layer data representation learning. Deep learning, however, with its carefully designed hierarchical structure, is able to combine hidden features to form more representative features for pattern recognition. In this paper, we proposed sparse deep nonnegative matrix factorization models to analyze complex data for more accurate classification and better feature interpretation. Such models are designed to learn localized features or generate more discriminative representations for samples in distinct classes by imposing $L_1$-norm penalty on the columns of certain factors. By extending one-layer model into multi-layer one with sparsity, we provided a hierarchical way to analyze big data and extract hidden features intuitively due to nonnegativity. We adopted the Nesterov's accelerated gradient algorithm to accelerate the computing process with the convergence rate of $O(1/k^2)$ after $k$ steps iteration. We also analyzed the computing complexity of our framework to demonstrate their efficiency. To improve the performance of dealing with linearly inseparable data, we also considered to incorporate popular nonlinear functions into this framework and explored their performance. We applied our models onto two benchmarking image datasets, demonstrating our models can achieve competitive or better classification performance and produce intuitive interpretations compared with the typical NMF and competing multi-layer models.



### Two Hilbert schemes in computer vision
- **Arxiv ID**: http://arxiv.org/abs/1707.09332v6
- **DOI**: None
- **Categories**: **math.AG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.09332v6)
- **Published**: 2017-07-28 17:13:22+00:00
- **Updated**: 2020-01-07 19:58:40+00:00
- **Authors**: Max Lieblich, Lucas Van Meter
- **Comment**: Shortened version, to appear in SIAGA. Note that theorem, etc.,
  numbering is different in published version than it is here. Corrected
  notational error in Notation 3.2 and Proposition 3.3, enhanced reference
  capitalization
- **Journal**: None
- **Summary**: We study multiview moduli problems that arise in computer vision. We show that these moduli spaces are always smooth and irreducible, in both the calibrated and uncalibrated cases, for any number of views. We also show that these moduli spaces always admit open immersions into Hilbert schemes for more than two views, extending and refining work of Aholt-Sturmfels-Thomas. We use these moduli spaces to study and extend the classical twisted pair covering of the essential variety.



### Face Deidentification with Generative Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.09376v1
- **DOI**: 10.1049/iet-spr.2017.0049
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.09376v1)
- **Published**: 2017-07-28 18:39:09+00:00
- **Updated**: 2017-07-28 18:39:09+00:00
- **Authors**: Blaž Meden, Refik Can Mallı, Sebastjan Fabijan, Hazım Kemal Ekenel, Vitomir Štruc, Peter Peer
- **Comment**: IET Signal Processing Special Issue on Deidentification 2017
- **Journal**: None
- **Summary**: Face deidentification is an active topic amongst privacy and security researchers. Early deidentification methods relying on image blurring or pixelization were replaced in recent years with techniques based on formal anonymity models that provide privacy guaranties and at the same time aim at retaining certain characteristics of the data even after deidentification. The latter aspect is particularly important, as it allows to exploit the deidentified data in applications for which identity information is irrelevant. In this work we present a novel face deidentification pipeline, which ensures anonymity by synthesizing artificial surrogate faces using generative neural networks (GNNs). The generated faces are used to deidentify subjects in images or video, while preserving non-identity-related aspects of the data and consequently enabling data utilization. Since generative networks are very adaptive and can utilize a diverse set of parameters (pertaining to the appearance of the generated output in terms of facial expressions, gender, race, etc.), they represent a natural choice for the problem of face deidentification. To demonstrate the feasibility of our approach, we perform experiments using automated recognition tools and human annotators. Our results show that the recognition performance on deidentified images is close to chance, suggesting that the deidentification process based on GNNs is highly effective.



### Photographic Image Synthesis with Cascaded Refinement Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.09405v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.09405v1)
- **Published**: 2017-07-28 20:24:44+00:00
- **Updated**: 2017-07-28 20:24:44+00:00
- **Authors**: Qifeng Chen, Vladlen Koltun
- **Comment**: Published at the International Conference on Computer Vision (ICCV
  2017)
- **Journal**: None
- **Summary**: We present an approach to synthesizing photographic images conditioned on semantic layouts. Given a semantic label map, our approach produces an image with photographic appearance that conforms to the input layout. The approach thus functions as a rendering engine that takes a two-dimensional semantic specification of the scene and produces a corresponding photographic image. Unlike recent and contemporaneous work, our approach does not rely on adversarial training. We show that photographic images can be synthesized from semantic layouts by a single feedforward network with appropriate structure, trained end-to-end with a direct regression objective. The presented approach scales seamlessly to high resolutions; we demonstrate this by synthesizing photographic images at 2-megapixel resolution, the full resolution of our training data. Extensive perceptual experiments on datasets of outdoor and indoor scenes demonstrate that images synthesized by the presented approach are considerably more realistic than alternative approaches. The results are shown in the supplementary video at https://youtu.be/0fhUJT21-bs



### FontCode: Embedding Information in Text Documents using Glyph Perturbation
- **Arxiv ID**: http://arxiv.org/abs/1707.09418v1
- **DOI**: 10.1145/3152823
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09418v1)
- **Published**: 2017-07-28 21:03:50+00:00
- **Updated**: 2017-07-28 21:03:50+00:00
- **Authors**: Chang Xiao, Cheng Zhang, Changxi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce FontCode, an information embedding technique for text documents. Provided a text document with specific fonts, our method embeds user-specified information in the text by perturbing the glyphs of text characters while preserving the text content. We devise an algorithm to chooses unobtrusive yet machine-recognizable glyph perturbations, leveraging a recently developed generative model that alters the glyphs of each character continuously on a font manifold. We then introduce an algorithm that embeds a user-provided message in the text document and produces an encoded document whose appearance is minimally perturbed from the original document. We also present a glyph recognition method that recovers the embedded information from an encoded document stored as a vector graphic or pixel image, or even on a printed paper. In addition, we introduce a new error-correction coding scheme that rectifies a certain number of recognition errors. Lastly, we demonstrate that our technique enables a wide array of applications, using it as a text document metadata holder, an unobtrusive optical barcode, a cryptographic message embedding scheme, and a text document signature.



### Visual Relationship Detection with Internal and External Linguistic Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1707.09423v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09423v2)
- **Published**: 2017-07-28 21:31:00+00:00
- **Updated**: 2017-08-03 00:11:33+00:00
- **Authors**: Ruichi Yu, Ang Li, Vlad I. Morariu, Larry S. Davis
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: Understanding visual relationships involves identifying the subject, the object, and a predicate relating them. We leverage the strong correlations between the predicate and the (subj,obj) pair (both semantically and spatially) to predict the predicates conditioned on the subjects and the objects. Modeling the three entities jointly more accurately reflects their relationships, but complicates learning since the semantic space of visual relationships is huge and the training data is limited, especially for the long-tail relationships that have few instances. To overcome this, we use knowledge of linguistic statistics to regularize visual model learning. We obtain linguistic knowledge by mining from both training annotations (internal knowledge) and publicly available text, e.g., Wikipedia (external knowledge), computing the conditional probability distribution of a predicate given a (subj,obj) pair. Then, we distill the knowledge into a deep model to achieve better generalization. Our experimental results on the Visual Relationship Detection (VRD) and Visual Genome datasets suggest that with this linguistic knowledge distillation, our model outperforms the state-of-the-art methods significantly, especially when predicting unseen relationships (e.g., recall improved from 8.45% to 19.17% on VRD zero-shot testing set).



