# Arxiv Papers in cs.CV on 2017-07-04
### Zero-Shot Fine-Grained Classification by Deep Feature Learning with Semantics
- **Arxiv ID**: http://arxiv.org/abs/1707.00785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00785v1)
- **Published**: 2017-07-04 00:18:32+00:00
- **Updated**: 2017-07-04 00:18:32+00:00
- **Authors**: Aoxue Li, Zhiwu Lu, Liwei Wang, Tao Xiang, Xinqi Li, Ji-Rong Wen
- **Comment**: This paper has been submitted to IEEE TIP for peer-review
- **Journal**: None
- **Summary**: Fine-grained image classification, which aims to distinguish images with subtle distinctions, is a challenging task due to two main issues: lack of sufficient training data for every class and difficulty in learning discriminative features for representation. In this paper, to address the two issues, we propose a two-phase framework for recognizing images from unseen fine-grained classes, i.e. zero-shot fine-grained classification. In the first feature learning phase, we finetune deep convolutional neural networks using hierarchical semantic structure among fine-grained classes to extract discriminative deep visual features. Meanwhile, a domain adaptation structure is induced into deep convolutional neural networks to avoid domain shift from training data to test data. In the second label inference phase, a semantic directed graph is constructed over attributes of fine-grained classes. Based on this graph, we develop a label propagation algorithm to infer the labels of images in the unseen classes. Experimental results on two benchmark datasets demonstrate that our model outperforms the state-of-the-art zero-shot learning models. In addition, the features obtained by our feature learning model also yield significant gains when they are used by other zero-shot learning models, which shows the flexility of our model in zero-shot fine-grained classification.



### Deep Representation Learning with Part Loss for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1707.00798v2
- **DOI**: 10.1109/TIP.2019.2891888
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00798v2)
- **Published**: 2017-07-04 02:07:00+00:00
- **Updated**: 2017-11-16 13:00:23+00:00
- **Authors**: Hantao Yao, Shiliang Zhang, Yongdong Zhang, Jintao Li, Qi Tian
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: Learning discriminative representations for unseen person images is critical for person Re-Identification (ReID). Most of current approaches learn deep representations in classification tasks, which essentially minimize the empirical classification risk on the training set. As shown in our experiments, such representations commonly focus on several body parts discriminative to the training set, rather than the entire human body. Inspired by the structural risk minimization principle in SVM, we revise the traditional deep representation learning procedure to minimize both the empirical classification risk and the representation learning risk. The representation learning risk is evaluated by the proposed part loss, which automatically generates several parts for an image, and computes the person classification loss on each part separately. Compared with traditional global classification loss, simultaneously considering multiple part loss enforces the deep network to focus on the entire human body and learn discriminative representations for different parts. Experimental results on three datasets, i.e., Market1501, CUHK03, VIPeR, show that our representation outperforms the existing deep representations.



### Arabic Character Segmentation Using Projection Based Approach with Profile's Amplitude Filter
- **Arxiv ID**: http://arxiv.org/abs/1707.00800v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1707.00800v1)
- **Published**: 2017-07-04 02:29:07+00:00
- **Updated**: 2017-07-04 02:29:07+00:00
- **Authors**: Mahmoud A. A. Mousa, Mohammed S. Sayed, Mahmoud I. Abdalla
- **Comment**: None
- **Journal**: None
- **Summary**: Arabic is one of the languages that present special challenges to Optical character recognition (OCR). The main challenge in Arabic is that it is mostly cursive. Therefore, a segmentation process must be carried out to determine where the character begins and where it ends. This step is essential for character recognition. This paper presents Arabic character segmentation algorithm. The proposed algorithm uses the projection-based approach concepts to separate lines, words, and characters. This is done using profile's amplitude filter and simple edge tool to find characters separations. Our algorithm shows promising performance when applied on different machine printed documents with different Arabic fonts.



### Aggregating Frame-level Features for Large-Scale Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1707.00803v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00803v1)
- **Published**: 2017-07-04 02:55:48+00:00
- **Updated**: 2017-07-04 02:55:48+00:00
- **Authors**: Shaoxiang Chen, Xi Wang, Yongyi Tang, Xinpeng Chen, Zuxuan Wu, Yu-Gang Jiang
- **Comment**: Youtube-8M Challenge, 4th place
- **Journal**: None
- **Summary**: This paper introduces the system we developed for the Google Cloud & YouTube-8M Video Understanding Challenge, which can be considered as a multi-label classification problem defined on top of the large scale YouTube-8M Dataset. We employ a large set of techniques to aggregate the provided frame-level feature representations and generate video-level predictions, including several variants of recurrent neural networks (RNN) and generalized VLAD. We also adopt several fusion strategies to explore the complementarity among the models. In terms of the official metric GAP@20 (global average precision at 20), our best fusion model attains 0.84198 on the public 50\% of test data and 0.84193 on the private 50\% of test data, ranking 4th out of 650 teams worldwide in the competition.



### Selective Deep Convolutional Features for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1707.00809v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00809v2)
- **Published**: 2017-07-04 03:38:15+00:00
- **Updated**: 2017-11-27 06:59:23+00:00
- **Authors**: Tuan Hoang, Thanh-Toan Do, Dang-Khoa Le Tan, Ngai-Man Cheung
- **Comment**: Accepted to ACM MM 2017
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) is a very powerful approach to extract discriminative local descriptors for effective image search. Recent work adopts fine-tuned strategies to further improve the discriminative power of the descriptors. Taking a different approach, in this paper, we propose a novel framework to achieve competitive retrieval performance. Firstly, we propose various masking schemes, namely SIFT-mask, SUM-mask, and MAX-mask, to select a representative subset of local convolutional features and remove a large number of redundant features. We demonstrate that this can effectively address the burstiness issue and improve retrieval accuracy. Secondly, we propose to employ recent embedding and aggregating methods to further enhance feature discriminability. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art retrieval accuracy.



### High-Quality Face Image SR Using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.00737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00737v1)
- **Published**: 2017-07-04 03:39:43+00:00
- **Updated**: 2017-07-04 03:39:43+00:00
- **Authors**: Huang Bin, Chen Weihai, Wu Xingming, Lin Chun-Liang
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: We propose a novel single face image super-resolution method, which named Face Conditional Generative Adversarial Network(FCGAN), based on boundary equilibrium generative adversarial networks. Without taking any facial prior information, our method can generate a high-resolution face image from a low-resolution one. Compared with existing studies, both our training and testing phases are end-to-end pipeline with little pre/post-processing. To enhance the convergence speed and strengthen feature propagation, skip-layer connection is further employed in the generative and discriminative networks. Extensive experiments demonstrate that our model achieves competitive performance compared with state-of-the-art models.



### One-Shot Fine-Grained Instance Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1707.00811v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00811v1)
- **Published**: 2017-07-04 03:51:32+00:00
- **Updated**: 2017-07-04 03:51:32+00:00
- **Authors**: Hantao Yao, Shiliang Zhang, Yongdong Zhang, Jintao Li, Qi Tian
- **Comment**: Accepted by MM2017, 9 pages, 7 figures
- **Journal**: None
- **Summary**: Fine-Grained Visual Categorization (FGVC) has achieved significant progress recently. However, the number of fine-grained species could be huge and dynamically increasing in real scenarios, making it difficult to recognize unseen objects under the current FGVC framework. This raises an open issue to perform large-scale fine-grained identification without a complete training set. Aiming to conquer this issue, we propose a retrieval task named One-Shot Fine-Grained Instance Retrieval (OSFGIR). "One-Shot" denotes the ability of identifying unseen objects through a fine-grained retrieval task assisted with an incomplete auxiliary training set. This paper first presents the detailed description to OSFGIR task and our collected OSFGIR-378K dataset. Next, we propose the Convolutional and Normalization Networks (CN-Nets) learned on the auxiliary dataset to generate a concise and discriminative representation. Finally, we present a coarse-to-fine retrieval framework consisting of three components, i.e., coarse retrieval, fine-grained retrieval, and query expansion, respectively. The framework progressively retrieves images with similar semantics, and performs fine-grained identification. Experiments show our OSFGIR framework achieves significantly better accuracy and efficiency than existing FGVC and image retrieval methods, thus could be a better solution for large-scale fine-grained object identification.



### Spatial and Angular Resolution Enhancement of Light Fields Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.00815v2
- **DOI**: 10.1109/TIP.2018.2794181
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00815v2)
- **Published**: 2017-07-04 04:33:32+00:00
- **Updated**: 2018-02-13 13:53:43+00:00
- **Authors**: M. Shahzeb Khan Gul, Bahadir K. Gunturk
- **Comment**: None
- **Journal**: None
- **Summary**: Light field imaging extends the traditional photography by capturing both spatial and angular distribution of light, which enables new capabilities, including post-capture refocusing, post-capture aperture control, and depth estimation from a single shot. Micro-lens array (MLA) based light field cameras offer a cost-effective approach to capture light field. A major drawback of MLA based light field cameras is low spatial resolution, which is due to the fact that a single image sensor is shared to capture both spatial and angular information. In this paper, we present a learning based light field enhancement approach. Both spatial and angular resolution of captured light field is enhanced using convolutional neural networks. The proposed method is tested with real light field data captured with a Lytro light field camera, clearly demonstrating spatial and angular resolution improvement.



### Learning Human Pose Models from Synthesized Data for Robust RGB-D Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.00823v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00823v2)
- **Published**: 2017-07-04 06:18:34+00:00
- **Updated**: 2018-05-01 06:24:35+00:00
- **Authors**: Jian Liu, Naveed Akhtar, Ajmal Mian
- **Comment**: Revision submitted to IJCV
- **Journal**: None
- **Summary**: We propose Human Pose Models that represent RGB and depth images of human poses independent of clothing textures, backgrounds, lighting conditions, body shapes and camera viewpoints. Learning such universal models requires training images where all factors are varied for every human pose. Capturing such data is prohibitively expensive. Therefore, we develop a framework for synthesizing the training data. First, we learn representative human poses from a large corpus of real motion captured human skeleton data. Next, we fit synthetic 3D humans with different body shapes to each pose and render each from 180 camera viewpoints while randomly varying the clothing textures, background and lighting. Generative Adversarial Networks are employed to minimize the gap between synthetic and real image distributions. CNN models are then learned that transfer human poses to a shared high-level invariant space. The learned CNN models are then used as invariant feature extractors from real RGB and depth frames of human action videos and the temporal variations are modelled by Fourier Temporal Pyramid. Finally, linear SVM is used for classification. Experiments on three benchmark cross-view human action datasets show that our algorithm outperforms existing methods by significant margins for RGB only and RGB-D action recognition.



### Face Recognition with Machine Learning in OpenCV_ Fusion of the results with the Localization Data of an Acoustic Camera for Speaker Identification
- **Arxiv ID**: http://arxiv.org/abs/1707.00835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00835v1)
- **Published**: 2017-07-04 07:42:00+00:00
- **Updated**: 2017-07-04 07:42:00+00:00
- **Authors**: Johannes Reschke, Armin Sehr
- **Comment**: Applied Research Conference 2017 (Munich)
- **Journal**: None
- **Summary**: This contribution gives an overview of face recogni-tion algorithms, their implementation and practical uses. First, a training set of different persons' faces has to be collected and used to train a face recognizer. The resulting face model can be utilized to classify people in specific individuals or unknowns. After tracking the recognized face and estimating the acoustic sound source's position, both can be combined to give detailed information about possible speakers and if they are talking or not. This leads to a precise real-time description of the situation, which can be used for further applications, e.g. for multi-channel speech enhancement by adaptive beamformers.



### DeepStory: Video Story QA by Deep Embedded Memory Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.00836v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1707.00836v1)
- **Published**: 2017-07-04 07:42:05+00:00
- **Updated**: 2017-07-04 07:42:05+00:00
- **Authors**: Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, Byoung-Tak Zhang
- **Comment**: 7 pages, accepted for IJCAI 2017
- **Journal**: None
- **Summary**: Question-answering (QA) on video contents is a significant challenge for achieving human-level intelligence as it involves both vision and language in real-world settings. Here we demonstrate the possibility of an AI agent performing video story QA by learning from a large amount of cartoon videos. We develop a video-story learning model, i.e. Deep Embedded Memory Networks (DEMN), to reconstruct stories from a joint scene-dialogue video stream using a latent embedding space of observed data. The video stories are stored in a long-term memory component. For a given question, an LSTM-based attention model uses the long-term memory to recall the best question-story-answer triplet by focusing on specific words containing key information. We trained the DEMN on a novel QA dataset of children's cartoon video series, Pororo. The dataset contains 16,066 scene-dialogue pairs of 20.5-hour videos, 27,328 fine-grained sentences for scene description, and 8,913 story-related QA pairs. Our experimental results show that the DEMN outperforms other QA models. This is mainly due to 1) the reconstruction of video stories in a scene-dialogue combined form that utilize the latent embedding and 2) attention. DEMN also achieved state-of-the-art results on the MovieQA benchmark.



### Weighted Low Rank Approximation for Background Estimation Problems
- **Arxiv ID**: http://arxiv.org/abs/1707.01753v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.01753v1)
- **Published**: 2017-07-04 08:30:23+00:00
- **Updated**: 2017-07-04 08:30:23+00:00
- **Authors**: Aritra Dutta, Xin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Classical principal component analysis (PCA) is not robust to the presence of sparse outliers in the data. The use of the $\ell_1$ norm in the Robust PCA (RPCA) method successfully eliminates the weakness of PCA in separating the sparse outliers. In this paper, by sticking a simple weight to the Frobenius norm, we propose a weighted low rank (WLR) method to avoid the often computationally expensive algorithms relying on the $\ell_1$ norm. As a proof of concept, a background estimation model has been presented and compared with two $\ell_1$ norm minimization algorithms. We illustrate that as long as a simple weight matrix is inferred from the data, one can use the weighted Frobenius norm and achieve the same or better performance.



### Conditional generation of multi-modal data using constrained embedding space mapping
- **Arxiv ID**: http://arxiv.org/abs/1707.00860v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.00860v2)
- **Published**: 2017-07-04 09:00:38+00:00
- **Updated**: 2017-07-26 00:51:04+00:00
- **Authors**: Subhajit Chaudhury, Sakyasingha Dasgupta, Asim Munawar, Md. A. Salam Khan, Ryuki Tachibana
- **Comment**: 7 pages, 4 figures, ICML 2017 Workshop on Implicit Models
- **Journal**: None
- **Summary**: We present a conditional generative model that maps low-dimensional embeddings of multiple modalities of data to a common latent space hence extracting semantic relationships between them. The embedding specific to a modality is first extracted and subsequently a constrained optimization procedure is performed to project the two embedding spaces to a common manifold. The individual embeddings are generated back from this common latent space. However, in order to enable independent conditional inference for separately extracting the corresponding embeddings from the common latent space representation, we deploy a proxy variable trick - wherein, the single shared latent space is replaced by the respective separate latent spaces of each modality. We design an objective function, such that, during training we can force these separate spaces to lie close to each other, by minimizing the distance between their probability distribution functions. Experimental results demonstrate that the learned joint model can generalize to learning concepts of double MNIST digits with additional attributes of colors,from both textual and speech input.



### Optimization Beyond the Convolution: Generalizing Spatial Relations with End-to-End Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.00893v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.00893v4)
- **Published**: 2017-07-04 10:19:34+00:00
- **Updated**: 2019-07-01 13:44:40+00:00
- **Authors**: Philipp Jund, Andreas Eitel, Nichola Abdo, Wolfram Burgard
- **Comment**: Accepted for publication at ICRA2018. Supplementary Video:
  http://spatialrelations.cs.uni-freiburg.de/
- **Journal**: None
- **Summary**: To operate intelligently in domestic environments, robots require the ability to understand arbitrary spatial relations between objects and to generalize them to objects of varying sizes and shapes. In this work, we present a novel end-to-end approach to generalize spatial relations based on distance metric learning. We train a neural network to transform 3D point clouds of objects to a metric space that captures the similarity of the depicted spatial relations, using only geometric models of the objects. Our approach employs gradient-based optimization to compute object poses in order to imitate an arbitrary target relation by reducing the distance to it under the learned metric. Our results based on simulated and real-world experiments show that the proposed method enables robots to generalize spatial relations to unknown objects over a continuous spectrum.



### The Candidate Multi-Cut for Cell Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.00907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00907v1)
- **Published**: 2017-07-04 11:05:40+00:00
- **Updated**: 2017-07-04 11:05:40+00:00
- **Authors**: Jan Funke, Chong Zhang, Tobias Pietzsch, Stephan Saalfeld
- **Comment**: None
- **Journal**: None
- **Summary**: Two successful approaches for the segmentation of biomedical images are (1) the selection of segment candidates from a merge-tree, and (2) the clustering of small superpixels by solving a Multi-Cut problem. In this paper, we introduce a model that unifies both approaches. Our model, the Candidate Multi-Cut (CMC), allows joint selection and clustering of segment candidates from a merge-tree. This way, we overcome the respective limitations of the individual methods: (1) the space of possible segmentations is not constrained to candidates of a merge-tree, and (2) the decision for clustering can be made on candidates larger than superpixels, using features over larger contexts. We solve the optimization problem of selecting and clustering of candidates using an integer linear program. On datasets of 2D light microscopy of cell populations and 3D electron microscopy of neurons, we show that our method generalizes well and generates more accurate segmentations than merge-tree or Multi-Cut methods alone.



### LED-based Photometric Stereo: Modeling, Calibration and Numerical Solution
- **Arxiv ID**: http://arxiv.org/abs/1707.01018v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01018v2)
- **Published**: 2017-07-04 14:39:27+00:00
- **Updated**: 2017-09-04 17:38:30+00:00
- **Authors**: Yvain Quéau, Bastien Durix, Tao Wu, Daniel Cremers, François Lauze, Jean-Denis Durou
- **Comment**: None
- **Journal**: None
- **Summary**: We conduct a thorough study of photometric stereo under nearby point light source illumination, from modeling to numerical solution, through calibration. In the classical formulation of photometric stereo, the luminous fluxes are assumed to be directional, which is very difficult to achieve in practice. Rather, we use light-emitting diodes (LEDs) to illuminate the scene to reconstruct. Such point light sources are very convenient to use, yet they yield a more complex photometric stereo model which is arduous to solve. We first derive in a physically sound manner this model, and show how to calibrate its parameters. Then, we discuss two state-of-the-art numerical solutions. The first one alternatingly estimates the albedo and the normals, and then integrates the normals into a depth map. It is shown empirically to be independent from the initialization, but convergence of this sequential approach is not established. The second one directly recovers the depth, by formulating photometric stereo as a system of PDEs which are partially linearized using image ratios. Although the sequential approach is avoided, initialization matters a lot and convergence is not established either. Therefore, we introduce a provably convergent alternating reweighted least-squares scheme for solving the original system of PDEs, without resorting to image ratios for linearization. Finally, we extend this study to the case of RGB images.



### Skeleton-aided Articulated Motion Generation
- **Arxiv ID**: http://arxiv.org/abs/1707.01058v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01058v2)
- **Published**: 2017-07-04 16:25:23+00:00
- **Updated**: 2017-09-14 09:06:02+00:00
- **Authors**: Yichao Yan, Jingwei Xu, Bingbing Ni, Xiaokang Yang
- **Comment**: ACM MM 2017
- **Journal**: None
- **Summary**: This work make the first attempt to generate articulated human motion sequence from a single image. On the one hand, we utilize paired inputs including human skeleton information as motion embedding and a single human image as appearance reference, to generate novel motion frames, based on the conditional GAN infrastructure. On the other hand, a triplet loss is employed to pursue appearance-smoothness between consecutive frames. As the proposed framework is capable of jointly exploiting the image appearance space and articulated/kinematic motion space, it generates realistic articulated motion sequence, in contrast to most previous video generation methods which yield blurred motion effects. We test our model on two human action datasets including KTH and Human3.6M, and the proposed framework generates very promising results on both datasets.



### ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1707.01083v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01083v2)
- **Published**: 2017-07-04 17:42:58+00:00
- **Updated**: 2017-12-07 18:06:34+00:00
- **Authors**: Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13x actual speedup over AlexNet while maintaining comparable accuracy.



### Discriminative Localization in CNNs for Weakly-Supervised Segmentation of Pulmonary Nodules
- **Arxiv ID**: http://arxiv.org/abs/1707.01086v2
- **DOI**: 10.1007/978-3-319-66179-7_65
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01086v2)
- **Published**: 2017-07-04 17:54:57+00:00
- **Updated**: 2018-02-22 19:24:06+00:00
- **Authors**: Xinyang Feng, Jie Yang, Andrew F. Laine, Elsa D. Angelini
- **Comment**: None
- **Journal**: International Conference on Medical Image Computing and
  Computer-Assisted Intervention (MICCAI) 2017
- **Summary**: Automated detection and segmentation of pulmonary nodules on lung computed tomography (CT) scans can facilitate early lung cancer diagnosis. Existing supervised approaches for automated nodule segmentation on CT scans require voxel-based annotations for training, which are labor- and time-consuming to obtain. In this work, we propose a weakly-supervised method that generates accurate voxel-level nodule segmentation trained with image-level labels only. By adapting a convolutional neural network (CNN) trained for image classification, our proposed method learns discriminative regions from the activation maps of convolution units at different scales, and identifies the true nodule location with a novel candidate-screening framework. Experimental results on the public LIDC-IDRI dataset demonstrate that, our weakly-supervised nodule segmentation framework achieves competitive performance compared to a fully-supervised CNN-based segmentation method.



### UPSET and ANGRI : Breaking High Performance Image Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1707.01159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01159v1)
- **Published**: 2017-07-04 21:34:08+00:00
- **Updated**: 2017-07-04 21:34:08+00:00
- **Authors**: Sayantan Sarkar, Ankan Bansal, Upal Mahbub, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, targeted fooling of high performance image classifiers is achieved by developing two novel attack methods. The first method generates universal perturbations for target classes and the second generates image specific perturbations. Extensive experiments are conducted on MNIST and CIFAR10 datasets to provide insights about the proposed algorithms and show their effectiveness.



