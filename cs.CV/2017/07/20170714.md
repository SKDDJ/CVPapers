# Arxiv Papers in cs.CV on 2017-07-14
### The Reversible Residual Network: Backpropagation Without Storing Activations
- **Arxiv ID**: http://arxiv.org/abs/1707.04585v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.04585v1)
- **Published**: 2017-07-14 03:05:43+00:00
- **Updated**: 2017-07-14 03:05:43+00:00
- **Authors**: Aidan N. Gomez, Mengye Ren, Raquel Urtasun, Roger B. Grosse
- **Comment**: None
- **Journal**: None
- **Summary**: Deep residual networks (ResNets) have significantly pushed forward the state-of-the-art on image classification, increasing in performance as networks grow both deeper and wider. However, memory consumption becomes a bottleneck, as one needs to store the activations in order to calculate gradients using backpropagation. We present the Reversible Residual Network (RevNet), a variant of ResNets where each layer's activations can be reconstructed exactly from the next layer's. Therefore, the activations for most layers need not be stored in memory during backpropagation. We demonstrate the effectiveness of RevNets on CIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification accuracy to equally-sized ResNets, even though the activation storage requirements are independent of depth.



### Inner-Scene Similarities as a Contextual Cue for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1707.04406v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04406v1)
- **Published**: 2017-07-14 07:37:21+00:00
- **Updated**: 2017-07-14 07:37:21+00:00
- **Authors**: Noa Arbel, Tamar Avraham, Michael Lindenbaum
- **Comment**: None
- **Journal**: None
- **Summary**: Using image context is an effective approach for improving object detection. Previously proposed methods used contextual cues that rely on semantic or spatial information. In this work, we explore a different kind of contextual information: inner-scene similarity. We present the CISS (Context by Inner Scene Similarity) algorithm, which is based on the observation that two visually similar sub-image patches are likely to share semantic identities, especially when both appear in the same image. CISS uses base-scores provided by a base detector and performs as a post-detection stage. For each candidate sub-image (denoted anchor), the CISS algorithm finds a few similar sub-images (denoted supporters), and, using them, calculates a new enhanced score for the anchor. This is done by utilizing the base-scores of the supporters and a pre-trained dependency model. The new scores are modeled as a linear function of the base scores of the anchor and the supporters and is estimated using a minimum mean square error optimization. This approach results in: (a) improved detection of partly occluded objects (when there are similar non-occluded objects in the scene), and (b) fewer false alarms (when the base detector mistakenly classifies a background patch as an object). This work relates to Duncan and Humphreys' "similarity theory," a psychophysical study. which suggested that the human visual system perceptually groups similar image regions and that the classification of one region is affected by the estimated identity of the other. Experimental results demonstrate the enhancement of a base detector's scores on the PASCAL VOC dataset.



### Iterative Manifold Embedding Layer Learned by Incomplete Data for Large-scale Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1707.09862v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09862v2)
- **Published**: 2017-07-14 08:53:11+00:00
- **Updated**: 2018-04-03 07:21:16+00:00
- **Authors**: Jian Xu, Chunheng Wang, Chengzuo Qi, Cunzhao Shi, Baihua Xiao
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Existing manifold learning methods are not appropriate for image retrieval task, because most of them are unable to process query image and they have much additional computational cost especially for large scale database. Therefore, we propose the iterative manifold embedding (IME) layer, of which the weights are learned off-line by unsupervised strategy, to explore the intrinsic manifolds by incomplete data. On the large scale database that contains 27000 images, IME layer is more than 120 times faster than other manifold learning methods to embed the original representations at query time. We embed the original descriptors of database images which lie on manifold in a high dimensional space into manifold-based representations iteratively to generate the IME representations in off-line learning stage. According to the original descriptors and the IME representations of database images, we estimate the weights of IME layer by ridge regression. In on-line retrieval stage, we employ the IME layer to map the original representation of query image with ignorable time cost (2 milliseconds). We experiment on five public standard datasets for image retrieval. The proposed IME layer significantly outperforms related dimension reduction methods and manifold learning methods. Without post-processing, Our IME layer achieves a boost in performance of state-of-the-art image retrieval methods with post-processing on most datasets, and needs less computational cost.



### Spatially variant PSF modeling in confocal macroscopy
- **Arxiv ID**: http://arxiv.org/abs/1707.09858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09858v1)
- **Published**: 2017-07-14 09:02:51+00:00
- **Updated**: 2017-07-14 09:02:51+00:00
- **Authors**: Anna Jezierska, Hugues Talbot, Jean-Christophe Pesquet, Gilbert Engler
- **Comment**: None
- **Journal**: None
- **Summary**: Point spread function (PSF) plays an essential role in image reconstruction. In the context of confocal microscopy, optical performance degrades towards the edge of the field of view as astigmatism, coma and vignetting. Thus, one should expect the related artifacts to be even stronger in macroscopy, where the field of view is much larger. The field aberrations in macroscopy fluorescence imaging system was observed to be symmetrical and to increase with the distance from the center of the field of view. In this paper we propose an experiment and an optimization method for assessing the center of the field of view. The obtained results constitute a step towards reducing the number of parameters in macroscopy PSF model.



### Monocular Visual Odometry for an Unmanned Sea-Surface Vehicle
- **Arxiv ID**: http://arxiv.org/abs/1707.04444v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1707.04444v2)
- **Published**: 2017-07-14 10:13:59+00:00
- **Updated**: 2017-07-19 10:39:29+00:00
- **Authors**: George Terzakis, Riccardo Polvara, Sanjay Sharma, Phil Culverhouse, Robert Sutton
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of localizing an autonomous sea-surface vehicle in river estuarine areas using monocular camera and angular velocity input from an inertial sensor. Our method is challenged by two prominent drawbacks associated with the environment, which are typically not present in standard visual simultaneous localization and mapping (SLAM) applications on land (or air): a) Scene depth varies significantly (from a few meters to several kilometers) and, b) In conjunction to the latter, there exists no ground plane to provide features with enough disparity based on which to reliably detect motion. To that end, we use the IMU orientation feedback in order to re-cast the problem of visual localization without the mapping component, although the map can be implicitly obtained from the camera pose estimates. We find that our method produces reliable odometry estimates for trajectories several hundred meters long in the water. To compare the visual odometry estimates with GPS based ground truth, we interpolate the trajectory with splines on a common parameter and obtain position error in meters recovering an optimal affine transformation between the two splines.



### Generalizing the Convolution Operator in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.09864v1
- **DOI**: 10.1007/s11063-019-10043-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09864v1)
- **Published**: 2017-07-14 10:16:25+00:00
- **Updated**: 2017-07-14 10:16:25+00:00
- **Authors**: Kamaledin Ghiasi-Shirazi
- **Comment**: Neural Process Lett (2019)
- **Journal**: None
- **Summary**: Convolutional neural networks have become a main tool for solving many machine vision and machine learning problems. A major element of these networks is the convolution operator which essentially computes the inner product between a weight vector and the vectorized image patches extracted by sliding a window in the image planes of the previous layer. In this paper, we propose two classes of surrogate functions for the inner product operation inherent in the convolution operator and so attain two generalizations of the convolution operator. The first one is the class of positive definite kernel functions where their application is justified by the kernel trick. The second one is the class of similarity measures defined based on a distance function. We justify this by tracing back to the basic idea behind the neocognitron which is the ancestor of CNNs. Both methods are then further generalized by allowing a monotonically increasing function to be applied subsequently. Like any trainable parameter in a neural network, the template pattern and the parameters of the kernel/distance function are trained with the back-propagation algorithm. As an aside, we use the proposed framework to justify the use of sine activation function in CNNs. Our experiments on the MNIST dataset show that the performance of ordinary CNNs can be achieved by generalized CNNs based on weighted L1/L2 distances, proving the applicability of the proposed generalization of the convolutional neural networks.



### Guiding InfoGAN with Semi-Supervision
- **Arxiv ID**: http://arxiv.org/abs/1707.04487v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.04487v1)
- **Published**: 2017-07-14 12:44:22+00:00
- **Updated**: 2017-07-14 12:44:22+00:00
- **Authors**: Adrian Spurr, Emre Aksan, Otmar Hilliges
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a new semi-supervised GAN architecture (ss-InfoGAN) for image synthesis that leverages information from few labels (as little as 0.22%, max. 10% of the dataset) to learn semantically meaningful and controllable data representations where latent variables correspond to label categories. The architecture builds on Information Maximizing Generative Adversarial Networks (InfoGAN) and is shown to learn both continuous and categorical codes and achieves higher quality of synthetic samples compared to fully unsupervised settings. Furthermore, we show that using small amounts of labeled data speeds-up training convergence. The architecture maintains the ability to disentangle latent variables for which no labels are available. Finally, we contribute an information-theoretic reasoning on how introducing semi-supervision increases mutual information between synthetic and real data.



### Temporal Modeling Approaches for Large-scale Youtube-8M Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/1707.04555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04555v1)
- **Published**: 2017-07-14 16:11:14+00:00
- **Updated**: 2017-07-14 16:11:14+00:00
- **Authors**: Fu Li, Chuang Gan, Xiao Liu, Yunlong Bian, Xiang Long, Yandong Li, Zhichao Li, Jie Zhou, Shilei Wen
- **Comment**: To appear on CVPR 2017 YouTube-8M Workshop(Rank 3rd out of 650 teams)
- **Journal**: None
- **Summary**: This paper describes our solution for the video recognition task of the Google Cloud and YouTube-8M Video Understanding Challenge that ranked the 3rd place. Because the challenge provides pre-extracted visual and audio features instead of the raw videos, we mainly investigate various temporal modeling approaches to aggregate the frame-level features for multi-label video recognition. Our system contains three major components: two-stream sequence model, fast-forward sequence model and temporal residual neural networks. Experiment results on the challenging Youtube-8M dataset demonstrate that our proposed temporal modeling approaches can significantly improve existing temporal modeling approaches in the large-scale video recognition tasks. To be noted, our fast-forward LSTM with a depth of 7 layers achieves 82.75% in term of GAP@20 on the Kaggle Public test set.



### Cloud-based or On-device: An Empirical Study of Mobile Deep Inference
- **Arxiv ID**: http://arxiv.org/abs/1707.04610v2
- **DOI**: None
- **Categories**: **cs.PF**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.04610v2)
- **Published**: 2017-07-14 19:05:50+00:00
- **Updated**: 2018-04-15 17:48:20+00:00
- **Authors**: Tian Guo
- **Comment**: Accepted at The IEEE International Conference on Cloud Engineering
  (IC2E) conference 2018
- **Journal**: None
- **Summary**: Modern mobile applications are benefiting significantly from the advancement in deep learning, e.g., implementing real-time image recognition and conversational system. Given a trained deep learning model, applications usually need to perform a series of matrix operations based on the input data, in order to infer possible output values. Because of computational complexity and size constraints, these trained models are often hosted in the cloud. To utilize these cloud-based models, mobile apps will have to send input data over the network. While cloud-based deep learning can provide reasonable response time for mobile apps, it restricts the use case scenarios, e.g. mobile apps need to have network access. With mobile specific deep learning optimizations, it is now possible to employ on-device inference. However, because mobile hardware, such as GPU and memory size, can be very limited when compared to its desktop counterpart, it is important to understand the feasibility of this new on-device deep learning inference architecture. In this paper, we empirically evaluate the inference performance of three Convolutional Neural Networks (CNNs) using a benchmark Android application we developed. Our measurement and analysis suggest that on-device inference can cost up to two orders of magnitude greater response time and energy when compared to cloud-based inference, and that loading model and computing probability are two performance bottlenecks for on-device deep inferences.



### Recognizing Abnormal Heart Sounds Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.04642v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.04642v2)
- **Published**: 2017-07-14 21:17:24+00:00
- **Updated**: 2017-10-19 08:27:01+00:00
- **Authors**: Jonathan Rubin, Rui Abreu, Anurag Ganguli, Saigopal Nelaturi, Ion Matei, Kumar Sricharan
- **Comment**: IJCAI 2017 Knowledge Discovery in Healthcare Workshop
- **Journal**: None
- **Summary**: The work presented here applies deep learning to the task of automated cardiac auscultation, i.e. recognizing abnormalities in heart sounds. We describe an automated heart sound classification algorithm that combines the use of time-frequency heat map representations with a deep convolutional neural network (CNN). Given the cost-sensitive nature of misclassification, our CNN architecture is trained using a modified loss function that directly optimizes the trade-off between sensitivity and specificity. We evaluated our algorithm at the 2016 PhysioNet Computing in Cardiology challenge where the objective was to accurately classify normal and abnormal heart sounds from single, short, potentially noisy recordings. Our entry to the challenge achieved a final specificity of 0.95, sensitivity of 0.73 and overall score of 0.84. We achieved the greatest specificity score out of all challenge entries and, using just a single CNN, our algorithm differed in overall score by only 0.02 compared to the top place finisher, which used an ensemble approach.



