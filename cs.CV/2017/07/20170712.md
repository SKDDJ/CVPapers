# Arxiv Papers in cs.CV on 2017-07-12
### NO Need to Worry about Adversarial Examples in Object Detection in Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1707.03501v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1707.03501v1)
- **Published**: 2017-07-12 00:09:50+00:00
- **Updated**: 2017-07-12 00:09:50+00:00
- **Authors**: Jiajun Lu, Hussein Sibai, Evan Fabry, David Forsyth
- **Comment**: Accepted to CVPR 2017, Spotlight Oral Workshop
- **Journal**: None
- **Summary**: It has been shown that most machine learning algorithms are susceptible to adversarial perturbations. Slightly perturbing an image in a carefully chosen direction in the image space may cause a trained neural network model to misclassify it. Recently, it was shown that physical adversarial examples exist: printing perturbed images then taking pictures of them would still result in misclassification. This raises security and safety concerns.   However, these experiments ignore a crucial property of physical objects: the camera can view objects from different distances and at different angles. In this paper, we show experiments that suggest that current constructions of physical adversarial examples do not disrupt object detection from a moving platform. Instead, a trained neural network classifies most of the pictures taken from different distances and angles of a perturbed image correctly. We believe this is because the adversarial property of the perturbation is sensitive to the scale at which the perturbed picture is viewed, so (for example) an autonomous car will misclassify a stop sign only from a small range of distances.   Our work raises an important question: can one construct examples that are adversarial for many or most viewing conditions? If so, the construction should offer very significant insights into the internal representation of patterns by deep networks. If not, there is a good prospect that adversarial examples can be reduced to a curiosity with little practical impact.



### Deep Learning for Sensor-based Activity Recognition: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1707.03502v2
- **DOI**: 10.1016/j.patrec.2018.02.010
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1707.03502v2)
- **Published**: 2017-07-12 00:21:04+00:00
- **Updated**: 2017-12-14 03:11:15+00:00
- **Authors**: Jindong Wang, Yiqiang Chen, Shuji Hao, Xiaohui Peng, Lisha Hu
- **Comment**: 10 pages, 2 figures, and 5 tables; submitted to Pattern Recognition
  Letters (second revision)
- **Journal**: None
- **Summary**: Sensor-based activity recognition seeks the profound high-level knowledge about human activities from multitudes of low-level sensor readings. Conventional pattern recognition approaches have made tremendous progress in the past years. However, those methods often heavily rely on heuristic hand-crafted feature extraction, which could hinder their generalization performance. Additionally, existing methods are undermined for unsupervised and incremental learning tasks. Recently, the recent advancement of deep learning makes it possible to perform automatic high-level feature extraction thus achieves promising performance in many areas. Since then, deep learning based methods have been widely adopted for the sensor-based activity recognition tasks. This paper surveys the recent advance of deep learning based sensor-based activity recognition. We summarize existing literature from three aspects: sensor modality, deep model, and application. We also present detailed insights on existing work and propose grand challenges for future research.



### Discriminative Block-Diagonal Representation Learning for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.03548v1
- **DOI**: 10.1109/TNNLS.2017.2712801
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1707.03548v1)
- **Published**: 2017-07-12 05:33:57+00:00
- **Updated**: 2017-07-12 05:33:57+00:00
- **Authors**: Zheng Zhang, Yong Xu, Ling Shao, Jian Yang
- **Comment**: Accepted by TNNLS, and the matlab codes are available at
  https://sites.google.com/site/darrenzz219/Home/publication
- **Journal**: None
- **Summary**: Existing block-diagonal representation researches mainly focuses on casting block-diagonal regularization on training data, while only little attention is dedicated to concurrently learning both block-diagonal representations of training and test data. In this paper, we propose a discriminative block-diagonal low-rank representation (BDLRR) method for recognition. In particular, the elaborate BDLRR is formulated as a joint optimization problem of shrinking the unfavorable representation from off-block-diagonal elements and strengthening the compact block-diagonal representation under the semi-supervised framework of low-rank representation. To this end, we first impose penalty constraints on the negative representation to eliminate the correlation between different classes such that the incoherence criterion of the extra-class representation is boosted. Moreover, a constructed subspace model is developed to enhance the self-expressive power of training samples and further build the representation bridge between the training and test samples, such that the coherence of the learned intra-class representation is consistently heightened. Finally, the resulting optimization problem is solved elegantly by employing an alternative optimization strategy, and a simple recognition algorithm on the learned representation is utilized for final prediction. Extensive experimental results demonstrate that the proposed method achieves superb recognition results on four face image datasets, three character datasets, and the fifteen scene multi-categories dataset. It not only shows superior potential on image recognition but also outperforms state-of-the-art methods.



### Aerial Vehicle Tracking by Adaptive Fusion of Hyperspectral Likelihood Maps
- **Arxiv ID**: http://arxiv.org/abs/1707.03553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03553v1)
- **Published**: 2017-07-12 05:55:54+00:00
- **Updated**: 2017-07-12 05:55:54+00:00
- **Authors**: Burak Uzkent, Aneesh Rangnekar, M. J. Hoffman
- **Comment**: Accepted at the International Conference on Computer Vision and
  Pattern Recognition Workshops, 2017
- **Journal**: None
- **Summary**: Hyperspectral cameras can provide unique spectral signatures for consistently distinguishing materials that can be used to solve surveillance tasks. In this paper, we propose a novel real-time hyperspectral likelihood maps-aided tracking method (HLT) inspired by an adaptive hyperspectral sensor. A moving object tracking system generally consists of registration, object detection, and tracking modules. We focus on the target detection part and remove the necessity to build any offline classifiers and tune a large amount of hyperparameters, instead learning a generative target model in an online manner for hyperspectral channels ranging from visible to infrared wavelengths. The key idea is that, our adaptive fusion method can combine likelihood maps from multiple bands of hyperspectral imagery into one single more distinctive representation increasing the margin between mean value of foreground and background pixels in the fused map. Experimental results show that the HLT not only outperforms all established fusion methods but is on par with the current state-of-the-art hyperspectral target tracking frameworks.



### Terahertz Security Image Quality Assessment by No-reference Model Observers
- **Arxiv ID**: http://arxiv.org/abs/1707.03574v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03574v2)
- **Published**: 2017-07-12 07:26:06+00:00
- **Updated**: 2017-10-03 14:01:53+00:00
- **Authors**: Menghan Hu, Xiongkuo Min, Guangtao Zhai, Wenhan Zhu, Yucheng Zhu, Zhaodi Wang, Xiaokang Yang, Guang Tian
- **Comment**: 13 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: To provide the possibility of developing objective image quality assessment (IQA) algorithms for THz security images, we constructed the THz security image database (THSID) including a total of 181 THz security images with the resolution of 127*380. The main distortion types in THz security images were first analyzed for the design of subjective evaluation criteria to acquire the mean opinion scores. Subsequently, the existing no-reference IQA algorithms, which were 5 opinion-aware approaches viz., NFERM, GMLF, DIIVINE, BRISQUE and BLIINDS2, and 8 opinion-unaware approaches viz., QAC, SISBLIM, NIQE, FISBLIM, CPBD, S3 and Fish_bb, were executed for the evaluation of the THz security image quality. The statistical results demonstrated the superiority of Fish_bb over the other testing IQA approaches for assessing the THz image quality with PLCC (SROCC) values of 0.8925 (-0.8706), and with RMSE value of 0.3993. The linear regression analysis and Bland-Altman plot further verified that the Fish__bb could substitute for the subjective IQA. Nonetheless, for the classification of THz security images, we tended to use S3 as a criterion for ranking THz security image grades because of the relatively low false positive rate in classifying bad THz image quality into acceptable category (24.69%). Interestingly, due to the specific property of THz image, the average pixel intensity gave the best performance than the above complicated IQA algorithms, with the PLCC, SROCC and RMSE of 0.9001, -0.8800 and 0.3857, respectively. This study will help the users such as researchers or security staffs to obtain the THz security images of good quality. Currently, our research group is attempting to make this research more comprehensive.



### Learning a CNN-based End-to-End Controller for a Formula SAE Racecar
- **Arxiv ID**: http://arxiv.org/abs/1708.02215v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02215v1)
- **Published**: 2017-07-12 08:04:13+00:00
- **Updated**: 2017-07-12 08:04:13+00:00
- **Authors**: Skanda Koppula
- **Comment**: None
- **Journal**: None
- **Summary**: We present a set of CNN-based end-to-end models for controls of a Formula SAE racecar, along with various benchmarking and visualization tools to understand model performance. We tackled three main problems in the context of cone-delineated racetrack driving: (1) discretized steering, which translates a first-person frame along to the track to a predicted steering direction. (2) real-value steering, which translates a frame view to a real-value steering angle, and (3) a network design for predicting brake and throttle. We demonstrate high accuracy on our discretization task, low theoretical testing errors with our model for real-value steering, and a starting point for future work regarding a controller for our vehicle's brake and throttle. Timing benchmarks suggests that the networks we propose have the latency and throughput required for real-time controllers, when run on GPU-enabled hardware.



### Machine Learning for RealisticBall Detection in RoboCup SPL
- **Arxiv ID**: http://arxiv.org/abs/1707.03628v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03628v1)
- **Published**: 2017-07-12 10:21:05+00:00
- **Updated**: 2017-07-12 10:21:05+00:00
- **Authors**: Domenico Bloisi, Francesco Del Duchetto, Tiziano Manoni, Vincenzo Suriani
- **Comment**: None
- **Journal**: None
- **Summary**: In this technical report, we describe the use of a machine learning approach for detecting the realistic black and white ball currently in use in the RoboCup Standard Platform League. Our aim is to provide a ready-to-use software module that can be useful for the RoboCup SPL community. To this end, the approach is integrated within the official B-Human code release 2016. The complete code for the approach presented in this work can be downloaded from the SPQR Team homepage at http://spqr.diag.uniroma1.it and from the SPQR Team GitHub repository at https://github.com/SPQRTeam/SPQRBallPerceptor. The approach has been tested in multiple environments, both indoor and outdoor. Furthermore, the ball detector described in this technical report has been used by the SPQR Robot Soccer Team during the competitions of the Robocup German Open 2017. To facilitate the use of our code by other teams, we have prepared a step-by-step installation guide.



### Adversarial Dropout for Supervised and Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.03631v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.03631v2)
- **Published**: 2017-07-12 10:25:57+00:00
- **Updated**: 2017-09-18 00:36:45+00:00
- **Authors**: Sungrae Park, Jun-Keon Park, Su-Jin Shin, Il-Chul Moon
- **Comment**: submitted to AAAI-18
- **Journal**: None
- **Summary**: Recently, the training with adversarial examples, which are generated by adding a small but worst-case perturbation on input examples, has been proved to improve generalization performance of neural networks. In contrast to the individually biased inputs to enhance the generality, this paper introduces adversarial dropout, which is a minimal set of dropouts that maximize the divergence between the outputs from the network with the dropouts and the training supervisions. The identified adversarial dropout are used to reconfigure the neural network to train, and we demonstrated that training on the reconfigured sub-network improves the generalization performance of supervised and semi-supervised learning tasks on MNIST and CIFAR-10. We analyzed the trained model to reason the performance improvement, and we found that adversarial dropout increases the sparsity of neural networks more than the standard dropout does.



### Deep Fisher Discriminant Learning for Mobile Hand Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.03692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03692v1)
- **Published**: 2017-07-12 13:05:19+00:00
- **Updated**: 2017-07-12 13:05:19+00:00
- **Authors**: Chunyu Xie, Ce Li, Baochang Zhang, Chen Chen, Jungong Han
- **Comment**: None
- **Journal**: None
- **Summary**: Gesture recognition is a challenging problem in the field of biometrics. In this paper, we integrate Fisher criterion into Bidirectional Long-Short Term Memory (BLSTM) network and Bidirectional Gated Recurrent Unit (BGRU),thus leading to two new deep models termed as F-BLSTM and F-BGRU. BothFisher discriminative deep models can effectively classify the gesture based on analyzing the acceleration and angular velocity data of the human gestures. Moreover, we collect a large Mobile Gesture Database (MGD) based on the accelerations and angular velocities containing 5547 sequences of 12 gestures. Extensive experiments are conducted to validate the superior performance of the proposed networks as compared to the state-of-the-art BLSTM and BGRU on MGD database and two benchmark databases (i.e. BUAA mobile gesture and SmartWatch gesture).



### Large-scale Multiview 3D Hand Pose Dataset
- **Arxiv ID**: http://arxiv.org/abs/1707.03742v3
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.03742v3)
- **Published**: 2017-07-12 14:39:49+00:00
- **Updated**: 2017-07-18 19:02:55+00:00
- **Authors**: Francisco Gomez-Donoso, Sergio Orts-Escolano, Miguel Cazorla
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate hand pose estimation at joint level has several uses on human-robot interaction, user interfacing and virtual reality applications. Yet, it currently is not a solved problem. The novel deep learning techniques could make a great improvement on this matter but they need a huge amount of annotated data. The hand pose datasets released so far present some issues that make them impossible to use on deep learning methods such as the few number of samples, high-level abstraction annotations or samples consisting in depth maps. In this work, we introduce a multiview hand pose dataset in which we provide color images of hands and different kind of annotations for each, i.e the bounding box and the 2D and 3D location on the joints in the hand. Besides, we introduce a simple yet accurate deep learning architecture for real-time robust 2D hand pose estimation.



### Pixel-variant Local Homography for Fisheye Stereo Rectification Minimizing Resampling Distortion
- **Arxiv ID**: http://arxiv.org/abs/1707.03775v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03775v1)
- **Published**: 2017-07-12 15:50:37+00:00
- **Updated**: 2017-07-12 15:50:37+00:00
- **Authors**: Dingfu Zhou, Yuchao Dai, Hongdong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Large field-of-view fisheye lens cameras have attracted more and more researchers' attention in the field of robotics. However, there does not exist a convenient off-the-shelf stereo rectification approach which can be applied directly to fisheye stereo rig. One obvious drawback of existing methods is that the resampling distortion (which is defined as the loss of pixels due to under-sampling and the creation of new pixels due to over-sampling during rectification process) is severe if we want to obtain a rectification with epipolar line (not epipolar circle) constraint. To overcome this weakness, we propose a novel pixel-wise local homography technique for stereo rectification. First, we prove that there indeed exist enough degrees of freedom to apply pixel-wise local homography for stereo rectification. Then we present a method to exploit these freedoms and the solution via an optimization framework. Finally, the robustness and effectiveness of the proposed method have been verified on real fisheye lens images. The rectification results show that the proposed approach can effectively reduce the resampling distortion in comparison with existing methods while satisfying the epipolar line constraint. By employing the proposed method, dense stereo matching and 3D reconstruction for fisheye lens camera become as easy as perspective lens cameras.



### Robust Visual Tracking via Hierarchical Convolutional Features
- **Arxiv ID**: http://arxiv.org/abs/1707.03816v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03816v2)
- **Published**: 2017-07-12 17:54:21+00:00
- **Updated**: 2018-08-11 06:02:26+00:00
- **Authors**: Chao Ma, Jia-Bin Huang, Xiaokang Yang, Ming-Hsuan Yang
- **Comment**: To appear in T-PAMI 2018, project page at
  https://sites.google.com/site/chaoma99/hcft-tracking
- **Journal**: None
- **Summary**: In this paper, we propose to exploit the rich hierarchical features of deep convolutional neural networks to improve the accuracy and robustness of visual tracking. Deep neural networks trained on object recognition datasets consist of multiple convolutional layers. These layers encode target appearance with different levels of abstraction. For example, the outputs of the last convolutional layers encode the semantic information of targets and such representations are invariant to significant appearance variations. However, their spatial resolutions are too coarse to precisely localize the target. In contrast, features from earlier convolutional layers provide more precise localization but are less invariant to appearance changes. We interpret the hierarchical features of convolutional layers as a nonlinear counterpart of an image pyramid representation and explicitly exploit these multiple levels of abstraction to represent target objects. Specifically, we learn adaptive correlation filters on the outputs from each convolutional layer to encode the target appearance. We infer the maximum response of each layer to locate targets in a coarse-to-fine manner. To further handle the issues with scale estimation and re-detecting target objects from tracking failures caused by heavy occlusion or out-of-the-view movement, we conservatively learn another correlation filter, that maintains a long-term memory of target appearance, as a discriminative classifier. We apply the classifier to two types of object proposals: (1) proposals with a small step size and tightly around the estimated location for scale estimation; and (2) proposals with large step size and across the whole image for target re-detection. Extensive experimental results on large-scale benchmark datasets show that the proposed algorithm performs favorably against state-of-the-art tracking methods.



### Unsupervised Body Part Regression via Spatially Self-ordering Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.03891v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03891v2)
- **Published**: 2017-07-12 19:56:55+00:00
- **Updated**: 2018-03-07 01:18:39+00:00
- **Authors**: Ke Yan, Le Lu, Ronald M. Summers
- **Comment**: Oral presentation in ISBI18
- **Journal**: None
- **Summary**: Automatic body part recognition for CT slices can benefit various medical image applications. Recent deep learning methods demonstrate promising performance, with the requirement of large amounts of labeled images for training. The intrinsic structural or superior-inferior slice ordering information in CT volumes is not fully exploited. In this paper, we propose a convolutional neural network (CNN) based Unsupervised Body part Regression (UBR) algorithm to address this problem. A novel unsupervised learning method and two inter-sample CNN loss functions are presented. Distinct from previous work, UBR builds a coordinate system for the human body and outputs a continuous score for each axial slice, representing the normalized position of the body part in the slice. The training process of UBR resembles a self-organization process: slice scores are learned from inter-slice relationships. The training samples are unlabeled CT volumes that are abundant, thus no extra annotation effort is needed. UBR is simple, fast, and accurate. Quantitative and qualitative experiments validate its effectiveness. In addition, we show two applications of UBR in network initialization and anomaly detection.



