# Arxiv Papers in cs.CV on 2017-07-29
### Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints
- **Arxiv ID**: http://arxiv.org/abs/1707.09457v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1707.09457v1)
- **Published**: 2017-07-29 03:38:32+00:00
- **Updated**: 2017-07-29 03:38:32+00:00
- **Authors**: Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang
- **Comment**: 11 pages, published in EMNLP 2017
- **Journal**: None
- **Summary**: Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively.



### Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes
- **Arxiv ID**: http://arxiv.org/abs/1707.09465v5
- **DOI**: 10.1109/ICCV.2017.223
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.09465v5)
- **Published**: 2017-07-29 05:47:43+00:00
- **Updated**: 2018-11-14 01:03:47+00:00
- **Authors**: Yang Zhang, Philip David, Boqing Gong
- **Comment**: This is the extended version of the ICCV 2017 paper "Curriculum
  Domain Adaptation for Semantic Segmentation of Urban Scenes" with additional
  GTA experiment
- **Journal**: None
- **Summary**: During the last half decade, convolutional neural networks (CNNs) have triumphed over semantic segmentation, which is one of the core tasks in many applications such as autonomous driving. However, to train CNNs requires a considerable amount of data, which is difficult to collect and laborious to annotate. Recent advances in computer graphics make it possible to train CNNs on photo-realistic synthetic imagery with computer-generated annotations. Despite this, the domain mismatch between the real images and the synthetic data cripples the models' performance. Hence, we propose a curriculum-style learning approach to minimize the domain gap in urban scenery semantic segmentation. The curriculum domain adaptation solves easy tasks first to infer necessary properties about the target domain; in particular, the first task is to learn global label distributions over images and local distributions over landmark superpixels. These are easy to estimate because images of urban scenes have strong idiosyncrasies (e.g., the size and spatial relations of buildings, streets, cars, etc.). We then train a segmentation network while regularizing its predictions in the target domain to follow those inferred properties. In experiments, our method outperforms the baselines on two datasets and two backbone networks. We also report extensive ablation studies about our approach.



### Zero-Shot Activity Recognition with Verb Attribute Induction
- **Arxiv ID**: http://arxiv.org/abs/1707.09468v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.09468v2)
- **Published**: 2017-07-29 06:05:52+00:00
- **Updated**: 2017-09-02 19:53:20+00:00
- **Authors**: Rowan Zellers, Yejin Choi
- **Comment**: accepted to EMNLP 2017
- **Journal**: None
- **Summary**: In this paper, we investigate large-scale zero-shot activity recognition by modeling the visual and linguistic attributes of action verbs. For example, the verb "salute" has several properties, such as being a light movement, a social act, and short in duration. We use these attributes as the internal mapping between visual and textual representations to reason about a previously unseen action. In contrast to much prior work that assumes access to gold standard attributes for zero-shot classes and focuses primarily on object attributes, our model uniquely learns to infer action attributes from dictionary definitions and distributed word representations. Experimental results confirm that action attributes inferred from language can provide a predictive signal for zero-shot prediction of previously unseen activities.



### Weakly-supervised learning of visual relations
- **Arxiv ID**: http://arxiv.org/abs/1707.09472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09472v1)
- **Published**: 2017-07-29 06:41:36+00:00
- **Updated**: 2017-07-29 06:41:36+00:00
- **Authors**: Julia Peyre, Ivan Laptev, Cordelia Schmid, Josef Sivic
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel approach for modeling visual relations between pairs of objects. We call relation a triplet of the form (subject, predicate, object) where the predicate is typically a preposition (eg. 'under', 'in front of') or a verb ('hold', 'ride') that links a pair of objects (subject, object). Learning such relations is challenging as the objects have different spatial configurations and appearances depending on the relation in which they occur. Another major challenge comes from the difficulty to get annotations, especially at box-level, for all possible triplets, which makes both learning and evaluation difficult. The contributions of this paper are threefold. First, we design strong yet flexible visual features that encode the appearance and spatial configuration for pairs of objects. Second, we propose a weakly-supervised discriminative clustering model to learn relations from image-level labels only. Third we introduce a new challenging dataset of unusual relations (UnRel) together with an exhaustive annotation, that enables accurate evaluation of visual relation retrieval. We show experimentally that our model results in state-of-the-art results on the visual relationship dataset significantly improving performance on previously unseen relations (zero-shot learning), and confirm this observation on our newly introduced UnRel dataset.



### FCN-rLSTM: Deep Spatio-Temporal Neural Networks for Vehicle Counting in City Cameras
- **Arxiv ID**: http://arxiv.org/abs/1707.09476v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09476v2)
- **Published**: 2017-07-29 07:22:48+00:00
- **Updated**: 2017-08-01 00:33:29+00:00
- **Authors**: Shanghang Zhang, Guanhang Wu, João P. Costeira, José M. F. Moura
- **Comment**: Accepted by International Conference on Computer Vision (ICCV), 2017
- **Journal**: None
- **Summary**: In this paper, we develop deep spatio-temporal neural networks to sequentially count vehicles from low quality videos captured by city cameras (citycams). Citycam videos have low resolution, low frame rate, high occlusion and large perspective, making most existing methods lose their efficacy. To overcome limitations of existing methods and incorporate the temporal information of traffic video, we design a novel FCN-rLSTM network to jointly estimate vehicle density and vehicle count by connecting fully convolutional neural networks (FCN) with long short term memory networks (LSTM) in a residual learning fashion. Such design leverages the strengths of FCN for pixel-level prediction and the strengths of LSTM for learning complex temporal dynamics. The residual learning connection reformulates the vehicle count regression as learning residual functions with reference to the sum of densities in each frame, which significantly accelerates the training of networks. To preserve feature map resolution, we propose a Hyper-Atrous combination to integrate atrous convolution in FCN and combine feature maps of different convolution layers. FCN-rLSTM enables refined feature representation and a novel end-to-end trainable mapping from pixels to vehicle count. We extensively evaluated the proposed method on different counting tasks with three datasets, with experimental results demonstrating their effectiveness and robustness. In particular, FCN-rLSTM reduces the mean absolute error (MAE) from 5.31 to 4.21 on TRANCOS, and reduces the MAE from 2.74 to 1.53 on WebCamT. Training process is accelerated by 5 times on average.



### Deep Feature Consistent Deep Image Transformations: Downscaling, Decolorization and HDR Tone Mapping
- **Arxiv ID**: http://arxiv.org/abs/1707.09482v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09482v2)
- **Published**: 2017-07-29 08:37:32+00:00
- **Updated**: 2017-09-11 02:26:53+00:00
- **Authors**: Xianxu Hou, Jiang Duan, Guoping Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: Building on crucial insights into the determining factors of the visual integrity of an image and the property of deep convolutional neural network (CNN), we have developed the Deep Feature Consistent Deep Image Transformation (DFC-DIT) framework which unifies challenging one-to-many mapping image processing problems such as image downscaling, decolorization (colour to grayscale conversion) and high dynamic range (HDR) image tone mapping. We train one CNN as a non-linear mapper to transform an input image to an output image following what we term the deep feature consistency principle which is enforced through another pretrained and fixed deep CNN. This is the first work that uses deep learning to solve and unify these three common image processing tasks. We present experimental results to demonstrate the effectiveness of the DFC-DIT technique and its state of the art performances.



### Graph Classification with 2D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.02218v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02218v4)
- **Published**: 2017-07-29 09:20:29+00:00
- **Updated**: 2019-09-03 12:28:16+00:00
- **Authors**: Antoine Jean-Pierre Tixier, Giannis Nikolentzos, Polykarpos Meladianos, Michalis Vazirgiannis
- **Comment**: Published at ICANN 2019
- **Journal**: None
- **Summary**: Graph learning is currently dominated by graph kernels, which, while powerful, suffer some significant limitations. Convolutional Neural Networks (CNNs) offer a very appealing alternative, but processing graphs with CNNs is not trivial. To address this challenge, many sophisticated extensions of CNNs have recently been introduced. In this paper, we reverse the problem: rather than proposing yet another graph CNN model, we introduce a novel way to represent graphs as multi-channel image-like structures that allows them to be handled by vanilla 2D CNNs. Experiments reveal that our method is more accurate than state-of-the-art graph kernels and graph CNNs on 4 out of 6 real-world datasets (with and without continuous node attributes), and close elsewhere. Our approach is also preferable to graph kernels in terms of time complexity. Code and data are publicly available.



### Recurrent Scale Approximation for Object Detection in CNN
- **Arxiv ID**: http://arxiv.org/abs/1707.09531v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09531v2)
- **Published**: 2017-07-29 15:38:27+00:00
- **Updated**: 2018-02-08 05:49:21+00:00
- **Authors**: Yu Liu, Hongyang Li, Junjie Yan, Fangyin Wei, Xiaogang Wang, Xiaoou Tang
- **Comment**: Accepted in ICCV 2017
- **Journal**: None
- **Summary**: Since convolutional neural network (CNN) lacks an inherent mechanism to handle large scale variations, we always need to compute feature maps multiple times for multi-scale object detection, which has the bottleneck of computational cost in practice. To address this, we devise a recurrent scale approximation (RSA) to compute feature map once only, and only through this map can we approximate the rest maps on other levels. At the core of RSA is the recursive rolling out mechanism: given an initial map at a particular scale, it generates the prediction at a smaller scale that is half the size of input. To further increase efficiency and accuracy, we (a): design a scale-forecast network to globally predict potential scales in the image since there is no need to compute maps on all levels of the pyramid. (b): propose a landmark retracing network (LRN) to trace back locations of the regressed landmarks and generate a confidence score for each landmark; LRN can effectively alleviate false positives caused by the accumulated error in RSA. The whole system can be trained end-to-end in a unified CNN framework. Experiments demonstrate that our proposed algorithm is superior against state-of-the-art methods on face detection benchmarks and achieves comparable results for generic proposal generation. The source code of RSA is available at github.com/sciencefans/RSA-for-object-detection.



### Synthetic Database for Evaluation of General, Fundamental Biometric Principles
- **Arxiv ID**: http://arxiv.org/abs/1707.09543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09543v1)
- **Published**: 2017-07-29 17:35:49+00:00
- **Updated**: 2017-07-29 17:35:49+00:00
- **Authors**: Lee Friedman, Oleg Komogortsev
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: We create synthetic biometric databases to study general, fundamental, biometric principles. First, we check the validity of the synthetic database design by comparing it to real data in terms of biometric performance. The real data used for this validity check was from an eye-movement related biometric database. Next, we employ our database to evaluate the impact of variations of temporal persistence of features on biometric performance. We index temporal persistence with the intraclass correlation coefficient (ICC). We find that variations in temporal persistence are extremely highly correlated with variations in biometric performance. Finally, we use our synthetic database strategy to determine how many features are required to achieve particular levels of performance as the number of subjects in the database increases from 100 to 10,000. An important finding is that the number of features required to achieve various EER values (2%, 0.3%, 0.15%) is essentially constant in the database sizes that we studied. We hypothesize that the insights obtained from our study would be applicable to many biometric modalities where extracted feature properties resemble the properties of the synthetic features we discuss in this work.



### Improved Adversarial Systems for 3D Object Generation and Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1707.09557v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09557v3)
- **Published**: 2017-07-29 21:03:22+00:00
- **Updated**: 2017-10-30 18:49:29+00:00
- **Authors**: Edward Smith, David Meger
- **Comment**: 10 pages, accepted at CORL. Figures are best view in color, and
  details only appear when zoomed in
- **Journal**: None
- **Summary**: This paper describes a new approach for training generative adversarial networks (GAN) to understand the detailed 3D shape of objects. While GANs have been used in this domain previously, they are notoriously hard to train, especially for the complex joint data distribution over 3D objects of many categories and orientations. Our method extends previous work by employing the Wasserstein distance normalized with gradient penalization as a training objective. This enables improved generation from the joint object shape distribution. Our system can also reconstruct 3D shape from 2D images and perform shape completion from occluded 2.5D range scans. We achieve notable quantitative improvements in comparison to existing baselines



