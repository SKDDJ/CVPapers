# Arxiv Papers in cs.CV on 2017-07-31
### Recurrent 3D Pose Sequence Machines
- **Arxiv ID**: http://arxiv.org/abs/1707.09695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09695v1)
- **Published**: 2017-07-31 02:06:45+00:00
- **Updated**: 2017-07-31 02:06:45+00:00
- **Authors**: Mude Lin, Liang Lin, Xiaodan Liang, Keze Wang, Hui Cheng
- **Comment**: Published in CVPR 2017
- **Journal**: None
- **Summary**: 3D human articulated pose recovery from monocular image sequences is very challenging due to the diverse appearances, viewpoints, occlusions, and also the human 3D pose is inherently ambiguous from the monocular imagery. It is thus critical to exploit rich spatial and temporal long-range dependencies among body joints for accurate 3D pose sequence prediction. Existing approaches usually manually design some elaborate prior terms and human body kinematic constraints for capturing structures, which are often insufficient to exploit all intrinsic structures and not scalable for all scenarios. In contrast, this paper presents a Recurrent 3D Pose Sequence Machine(RPSM) to automatically learn the image-dependent structural constraint and sequence-dependent temporal context by using a multi-stage sequential refinement. At each stage, our RPSM is composed of three modules to predict the 3D pose sequences based on the previously learned 2D pose representations and 3D poses: (i) a 2D pose module extracting the image-dependent pose representations, (ii) a 3D pose recurrent module regressing 3D poses and (iii) a feature adaption module serving as a bridge between module (i) and (ii) to enable the representation transformation from 2D to 3D domain. These three modules are then assembled into a sequential prediction framework to refine the predicted poses with multiple recurrent stages. Extensive evaluations on the Human3.6M dataset and HumanEva-I dataset show that our RPSM outperforms all state-of-the-art approaches for 3D pose estimation.



### Scene Graph Generation from Objects, Phrases and Region Captions
- **Arxiv ID**: http://arxiv.org/abs/1707.09700v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09700v2)
- **Published**: 2017-07-31 02:40:19+00:00
- **Updated**: 2017-09-15 05:05:29+00:00
- **Authors**: Yikang Li, Wanli Ouyang, Bolei Zhou, Kun Wang, Xiaogang Wang
- **Comment**: accepted by ICCV 2017
- **Journal**: None
- **Summary**: Object detection, scene graph generation and region captioning, which are three scene understanding tasks at different semantic levels, are tied together: scene graphs are generated on top of objects detected in an image with their pairwise relationship predicted, while region captioning gives a language description of the objects, their attributes, relations, and other context information. In this work, to leverage the mutual connections across semantic levels, we propose a novel neural network model, termed as Multi-level Scene Description Network (denoted as MSDN), to solve the three vision tasks jointly in an end-to-end manner. Objects, phrases, and caption regions are first aligned with a dynamic graph based on their spatial and semantic connections. Then a feature refining structure is used to pass messages across the three levels of semantic tasks through the graph. We benchmark the learned model on three tasks, and show the joint learning across three tasks with our proposed method can bring mutual improvements over previous models. Particularly, on the scene graph generation task, our proposed method outperforms the state-of-art method with more than 3% margin.



### Automatic Crack Detection in Built Infrastructure Using Unmanned Aerial Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1707.09715v1
- **DOI**: 10.22260/ISARC2017/0115
- **Categories**: **cs.SY**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1707.09715v1)
- **Published**: 2017-07-31 04:18:09+00:00
- **Updated**: 2017-07-31 04:18:09+00:00
- **Authors**: Manh Duong Phung, Van Truong Hoang, Tran Hiep Dinh, Quang Ha
- **Comment**: In proceeding of The 34th International Symposium on Automation and
  Robotics in Construction (ISARC), pp. 823-829, Taipei, Taiwan, 2017
- **Journal**: None
- **Summary**: This paper addresses the problem of crack detection which is essential for health monitoring of built infrastructure. Our approach includes two stages, data collection using unmanned aerial vehicles (UAVs) and crack detection using histogram analysis. For the data collection, a 3D model of the structure is first created by using laser scanners. Based on the model, geometric properties are extracted to generate way points necessary for navigating the UAV to take images of the structure. Then, our next step is to stick together those obtained images from the overlapped field of view. The resulting image is then clustered by histogram analysis and peak detection. Potential cracks are finally identified by using locally adaptive thresholds. The whole process is automatically carried out so that the inspection time is significantly improved while safety hazards can be minimised. A prototypical system has been developed for evaluation and experimental results are included.



### Analysis and Optimization of Convolutional Neural Network Architectures
- **Arxiv ID**: http://arxiv.org/abs/1707.09725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09725v1)
- **Published**: 2017-07-31 05:35:12+00:00
- **Updated**: 2017-07-31 05:35:12+00:00
- **Authors**: Martin Thoma
- **Comment**: Master's thesis. 73 pages + 24 pages appendix; 39 figures; 33 tables
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) dominate various computer vision tasks since Alex Krizhevsky showed that they can be trained effectively and reduced the top-5 error from 26.2 % to 15.3 % on the ImageNet large scale visual recognition challenge. Many aspects of CNNs are examined in various publications, but literature about the analysis and construction of neural network architectures is rare. This work is one step to close this gap. A comprehensive overview over existing techniques for CNN analysis and topology construction is provided. A novel way to visualize classification errors with confusion matrices was developed. Based on this method, hierarchical classifiers are described and evaluated. Additionally, some results are confirmed and quantified for CIFAR-100. For example, the positive impact of smaller batch sizes, averaging ensembles, data augmentation and test-time transformations on the accuracy. Other results, such as the positive impact of learned color transformation on the test accuracy could not be confirmed. A model which has only one million learned parameters for an input size of 32x32x3 and 100 classes and which beats the state of the art on the benchmark dataset Asirra, GTSRB, HASYv2 and STL-10 was developed.



### Camera Relocalization by Computing Pairwise Relative Poses Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1707.09733v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09733v2)
- **Published**: 2017-07-31 06:36:55+00:00
- **Updated**: 2017-08-01 10:50:39+00:00
- **Authors**: Zakaria Laskar, Iaroslav Melekhov, Surya Kalia, Juho Kannala
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new deep learning based approach for camera relocalization. Our approach localizes a given query image by using a convolutional neural network (CNN) for first retrieving similar database images and then predicting the relative pose between the query and the database images, whose poses are known. The camera location for the query image is obtained via triangulation from two relative translation estimates using a RANSAC based approach. Each relative pose estimate provides a hypothesis for the camera orientation and they are fused in a second RANSAC scheme. The neural network is trained for relative pose estimation in an end-to-end manner using training image pairs. In contrast to previous work, our approach does not require scene-specific training of the network, which improves scalability, and it can also be applied to scenes which are not available during the training of the network. As another main contribution, we release a challenging indoor localisation dataset covering 5 different scenes registered to a common coordinate frame. We evaluate our approach using both our own dataset and the standard 7 Scenes benchmark. The results show that the proposed approach generalizes well to previously unseen scenes and compares favourably to other recent CNN-based methods.



### Synthesis of Positron Emission Tomography (PET) Images via Multi-channel Generative Adversarial Networks (GANs)
- **Arxiv ID**: http://arxiv.org/abs/1707.09747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09747v1)
- **Published**: 2017-07-31 07:56:34+00:00
- **Updated**: 2017-07-31 07:56:34+00:00
- **Authors**: Lei Bi, Jinman Kim, Ashnil Kumar, Dagan Feng, Michael Fulham
- **Comment**: 9 pages, 2 figures
- **Journal**: None
- **Summary**: Positron emission tomography (PET) image synthesis plays an important role, which can be used to boost the training data for computer aided diagnosis systems. However, existing image synthesis methods have problems in synthesizing the low resolution PET images. To address these limitations, we propose multi-channel generative adversarial networks (M-GAN) based PET image synthesis method. Different to the existing methods which rely on using low-level features, the proposed M-GAN is capable to represent the features in a high-level of semantic based on the adversarial learning concept. In addition, M-GAN enables to take the input from the annotation (label) to synthesize the high uptake regions e.g., tumors and from the computed tomography (CT) images to constrain the appearance consistency and output the synthetic PET images directly. Our results on 50 lung cancer PET-CT studies indicate that our method was much closer to the real PET images when compared with the existing methods.



### Capacity limitations of visual search in deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1707.09775v2
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1707.09775v2)
- **Published**: 2017-07-31 09:14:14+00:00
- **Updated**: 2021-03-29 09:53:06+00:00
- **Authors**: Endel Poder
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Deep convolutional neural networks follow roughly the architecture of biological visual systems and have shown a performance comparable to human observers in object recognition tasks. In this study, I tested three pretrained deep neural networks in visual search for simple visual features, and for feature configurations. The results reveal a qualitative difference from human performance. It appears that there is no clear difference between searches for simple features that pop out in experiments with humans, and for feature configurations that exhibit strict capacity limitations in human vision. Both types of stimuli reveal comparable capacity limitations in the neural networks tested here.



### Unsupervised Visual Attribute Transfer with Reconfigurable Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.09798v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09798v1)
- **Published**: 2017-07-31 10:52:41+00:00
- **Updated**: 2017-07-31 10:52:41+00:00
- **Authors**: Taeksoo Kim, Byoungjip Kim, Moonsu Cha, Jiwon Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Learning to transfer visual attributes requires supervision dataset. Corresponding images with varying attribute values with the same identity are required for learning the transfer function. This largely limits their applications, because capturing them is often a difficult task. To address the issue, we propose an unsupervised method to learn to transfer visual attribute. The proposed method can learn the transfer function without any corresponding images. Inspecting visualization results from various unsupervised attribute transfer tasks, we verify the effectiveness of the proposed method.



### 2D-3D Fully Convolutional Neural Networks for Cardiac MR Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.09813v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09813v1)
- **Published**: 2017-07-31 12:17:23+00:00
- **Updated**: 2017-07-31 12:17:23+00:00
- **Authors**: Jay Patravali, Shubham Jain, Sasank Chilamkurthy
- **Comment**: Accepted in STACOM '17
- **Journal**: None
- **Summary**: In this paper, we develop a 2D and 3D segmentation pipelines for fully automated cardiac MR image segmentation using Deep Convolutional Neural Networks (CNN). Our models are trained end-to-end from scratch using the ACD Challenge 2017 dataset comprising of 100 studies, each containing Cardiac MR images in End Diastole and End Systole phase. We show that both our segmentation models achieve near state-of-the-art performance scores in terms of distance metrics and have convincing accuracy in terms of clinical parameters. A comparative analysis is provided by introducing a novel dice loss function and its combination with cross entropy loss. By exploring different network structures and comprehensive experiments, we discuss several key insights to obtain optimal model performance, which also is central to the theme of this challenge.



### Real-Time Visual Localisation in a Tagged Environment
- **Arxiv ID**: http://arxiv.org/abs/1708.02283v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1708.02283v1)
- **Published**: 2017-07-31 13:11:55+00:00
- **Updated**: 2017-07-31 13:11:55+00:00
- **Authors**: Jérémy Taquet, Gaël Écorchard, Libor Přeučil
- **Comment**: Student Conference on Planning in Artificial Intelligence and
  Robotics, Sept. 2016
- **Journal**: None
- **Summary**: In a robotised warehouse a major issue is the safety of human operators in case of intervention in the work area of the robots. The current solution is to shut down every robot but it causes a loss of productivity, especially for large robotised warehouses. In order to avoid this loss we need to ensure the operator's security during his/her intervention in the warehouse without powering off the robots. The human operator needs to be localised in the warehouse and the trajectories of the robots have to be modified so that they do not interfere with the human. The purpose of this paper is to demonstrate a visual localisation method with visual elements that are already available in the current warehouse setup.



### Convolution with Logarithmic Filter Groups for Efficient Shallow CNN
- **Arxiv ID**: http://arxiv.org/abs/1707.09855v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09855v2)
- **Published**: 2017-07-31 14:16:06+00:00
- **Updated**: 2017-09-14 12:23:14+00:00
- **Authors**: Tae Kwan Lee, Wissam J. Baddar, Seong Tae Kim, Yong Man Ro
- **Comment**: 8 pages, 4 figures, 3 tables. Changes in abstract, result
  representations and typo corrections
- **Journal**: None
- **Summary**: In convolutional neural networks (CNNs), the filter grouping in convolution layers is known to be useful to reduce the network parameter size. In this paper, we propose a new logarithmic filter grouping which can capture the nonlinearity of filter distribution in CNNs. The proposed logarithmic filter grouping is installed in shallow CNNs applicable in a mobile application. Experiments were performed with the shallow CNNs for classification tasks. Our classification results on Multi-PIE dataset for facial expression recognition and CIFAR-10 dataset for object classification reveal that the compact CNN with the proposed logarithmic filter grouping scheme outperforms the same network with the uniform filter grouping in terms of accuracy and parameter efficiency. Our results indicate that the efficiency of shallow CNNs can be improved by the proposed logarithmic filter grouping.



### Automatic segmentation of the intracranialvolume in fetal MR images
- **Arxiv ID**: http://arxiv.org/abs/1708.02282v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02282v1)
- **Published**: 2017-07-31 14:44:38+00:00
- **Updated**: 2017-07-31 14:44:38+00:00
- **Authors**: N. Khalili, P. Moeskops, N. H. P. Claessens, S. Scherpenzeel, E. Turk, R. de Heus, M. J. N. L. Benders, M. A. Viergever, J. P. W. Pluim, I. Išgum
- **Comment**: None
- **Journal**: None
- **Summary**: MR images of the fetus allow non-invasive analysis of the fetal brain. Quantitative analysis of fetal brain development requires automatic brain tissue segmentation that is typically preceded by segmentation of the intracranial volume (ICV). This is challenging because fetal MR images visualize the whole moving fetus and in addition partially visualize the maternal body. This paper presents an automatic method for segmentation of the ICV in fetal MR images. The method employs a multi-scale convolutional neural network in 2D slices to enable learning spatial information from larger context as well as detailed local information. The method is developed and evaluated with 30 fetal T2-weighted MRI scans (average age $33.2\pm1.2$ weeks postmenstrual age). The set contains $10$ scans acquired in axial, $10$ in coronal and $10$ in sagittal imaging planes. A reference standard was defined in all images by manual annotation of the intracranial volume in $10$ equidistantly distributed slices. The automatic analysis was performed by training and testing the network using scans acquired in the representative imaging plane as well as combining the training data from all imaging planes. On average, the automatic method achieved Dice coefficients of 0.90 for the axial images, 0.90 for the coronal images and 0.92 for the sagittal images. Combining the training sets resulted in average Dice coefficients of 0.91 for the axial images, 0.95 for the coronal images, and 0.92 for the sagittal images. The results demonstrate that the evaluated method achieved good performance in extracting ICV in fetal MR scans regardless of the imaging plane.



### Fashioning with Networks: Neural Style Transfer to Design Clothes
- **Arxiv ID**: http://arxiv.org/abs/1707.09899v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1707.09899v1)
- **Published**: 2017-07-31 14:54:11+00:00
- **Updated**: 2017-07-31 14:54:11+00:00
- **Authors**: Prutha Date, Ashwinkumar Ganesan, Tim Oates
- **Comment**: ML4Fashion 2017
- **Journal**: None
- **Summary**: Convolutional Neural Networks have been highly successful in performing a host of computer vision tasks such as object recognition, object detection, image segmentation and texture synthesis. In 2015, Gatys et. al [7] show how the style of a painter can be extracted from an image of the painting and applied to another normal photograph, thus recreating the photo in the style of the painter. The method has been successfully applied to a wide range of images and has since spawned multiple applications and mobile apps. In this paper, the neural style transfer algorithm is applied to fashion so as to synthesize new custom clothes. We construct an approach to personalize and generate new custom clothes based on a users preference and by learning the users fashion choices from a limited set of clothes from their closet. The approach is evaluated by analyzing the generated images of clothes and how well they align with the users fashion style.



### A Framework for Super-Resolution of Scalable Video via Sparse Reconstruction of Residual Frames
- **Arxiv ID**: http://arxiv.org/abs/1707.09926v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09926v1)
- **Published**: 2017-07-31 15:47:47+00:00
- **Updated**: 2017-07-31 15:47:47+00:00
- **Authors**: Mohammad Hossein Moghaddam, Mohammad Javad Azizipour, Saeed Vahidian, Besma Smida
- **Comment**: IEEE Military Communications Conference, MILCOM, 2017
- **Journal**: None
- **Summary**: This paper introduces a framework for super-resolution of scalable video based on compressive sensing and sparse representation of residual frames in reconnaissance and surveillance applications. We exploit efficient compressive sampling and sparse reconstruction algorithms to super-resolve the video sequence with respect to different compression rates. We use the sparsity of residual information in residual frames as the key point in devising our framework. Moreover, a controlling factor as the compressibility threshold to control the complexity-performance trade-off is defined. Numerical experiments confirm the efficiency of the proposed framework in terms of the compression rate as well as the quality of reconstructed video sequence in terms of PSNR measure. The framework leads to a more efficient compression rate and higher video quality compared to other state-of-the-art algorithms considering performance-complexity trade-offs.



### Deep Convolutional Framelet Denosing for Low-Dose CT via Wavelet Residual Network
- **Arxiv ID**: http://arxiv.org/abs/1707.09938v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.09938v3)
- **Published**: 2017-07-31 16:17:31+00:00
- **Updated**: 2018-03-28 07:46:15+00:00
- **Authors**: Eunhee Kang, Jaejun Yoo, Jong Chul Ye
- **Comment**: This will appear in IEEE Transaction on Medical Imaging, a special
  issue of Machine Learning for Image Reconstruction
- **Journal**: None
- **Summary**: Model based iterative reconstruction (MBIR) algorithms for low-dose X-ray CT are computationally expensive. To address this problem, we recently proposed a deep convolutional neural network (CNN) for low-dose X-ray CT and won the second place in 2016 AAPM Low-Dose CT Grand Challenge. However, some of the texture were not fully recovered. To address this problem, here we propose a novel framelet-based denoising algorithm using wavelet residual network which synergistically combines the expressive power of deep learning and the performance guarantee from the framelet-based denoising algorithms. The new algorithms were inspired by the recent interpretation of the deep convolutional neural network (CNN) as a cascaded convolution framelet signal representation. Extensive experimental results confirm that the proposed networks have significantly improved performance and preserves the detail texture of the original images.



### Spatio-Temporal Action Detection with Cascade Proposal and Location Anticipation
- **Arxiv ID**: http://arxiv.org/abs/1708.00042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00042v1)
- **Published**: 2017-07-31 19:03:19+00:00
- **Updated**: 2017-07-31 19:03:19+00:00
- **Authors**: Zhenheng Yang, Jiyang Gao, Ram Nevatia
- **Comment**: Accepted at BMVC 2017 (oral)
- **Journal**: None
- **Summary**: In this work, we address the problem of spatio-temporal action detection in temporally untrimmed videos. It is an important and challenging task as finding accurate human actions in both temporal and spatial space is important for analyzing large-scale video data. To tackle this problem, we propose a cascade proposal and location anticipation (CPLA) model for frame-level action detection. There are several salient points of our model: (1) a cascade region proposal network (casRPN) is adopted for action proposal generation and shows better localization accuracy compared with single region proposal network (RPN); (2) action spatio-temporal consistencies are exploited via a location anticipation network (LAN) and thus frame-level action detection is not conducted independently. Frame-level detections are then linked by solving an linking score maximization problem, and temporally trimmed into spatio-temporal action tubes. We demonstrate the effectiveness of our model on the challenging UCF101 and LIRIS-HARL datasets, both achieving state-of-the-art performance.



### Statistics on the (compact) Stiefel manifold: Theory and Applications
- **Arxiv ID**: http://arxiv.org/abs/1708.00045v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00045v1)
- **Published**: 2017-07-31 19:32:18+00:00
- **Updated**: 2017-07-31 19:32:18+00:00
- **Authors**: Rudrasis Chakraborty, Baba Vemuri
- **Comment**: None
- **Journal**: None
- **Summary**: A Stiefel manifold of the compact type is often encountered in many fields of Engineering including, signal and image processing, machine learning, numerical optimization and others. The Stiefel manifold is a Riemannian homogeneous space but not a symmetric space. In previous work, researchers have defined probability distributions on symmetric spaces and performed statistical analysis of data residing in these spaces. In this paper, we present original work involving definition of Gaussian distributions on a homogeneous space and show that the maximum-likelihood estimate of the location parameter of a Gaussian distribution on the homogeneous space yields the Fr\'echet mean (FM) of the samples drawn from this distribution. Further, we present an algorithm to sample from the Gaussian distribution on the Stiefel manifold and recursively compute the FM of these samples. We also prove the weak consistency of this recursive FM estimator. Several synthetic and real data experiments are then presented, demonstrating the superior computational performance of this estimator over the gradient descent based non-recursive counter part as well as the stochastic gradient descent based method prevalent in literature.



### Streaming Architecture for Large-Scale Quantized Neural Networks on an FPGA-Based Dataflow Platform
- **Arxiv ID**: http://arxiv.org/abs/1708.00052v3
- **DOI**: 10.1109/IPDPSW.2018.00032
- **Categories**: **cs.CV**, cs.AR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.00052v3)
- **Published**: 2017-07-31 19:53:48+00:00
- **Updated**: 2018-03-13 13:20:13+00:00
- **Authors**: Chaim Baskin, Natan Liss, Evgenii Zheltonozhskii, Alex M. Bronshtein, Avi Mendelson
- **Comment**: Will appear in RAW 2018
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are used by different applications that are executed on a range of computer architectures, from IoT devices to supercomputers. The footprint of these networks is huge as well as their computational and communication needs. In order to ease the pressure on resources, research indicates that in many cases a low precision representation (1-2 bit per parameter) of weights and other parameters can achieve similar accuracy while requiring less resources. Using quantized values enables the use of FPGAs to run NNs, since FPGAs are well fitted to these primitives; e.g., FPGAs provide efficient support for bitwise operations and can work with arbitrary-precision representation of numbers.   This paper presents a new streaming architecture for running QNNs on FPGAs. The proposed architecture scales out better than alternatives, allowing us to take advantage of systems with multiple FPGAs. We also included support for skip connections, that are used in state-of-the art NNs, and shown that our architecture allows to add those connections almost for free. All this allowed us to implement an 18-layer ResNet for 224x224 images classification, achieving 57.5% top-1 accuracy.   In addition, we implemented a full-sized quantized AlexNet. In contrast to previous works, we use 2-bit activations instead of 1-bit ones, which improves AlexNet's top-1 accuracy from 41.8% to 51.03% for the ImageNet classification. Both AlexNet and ResNet can handle 1000-class real-time classification on an FPGA.   Our implementation of ResNet-18 consumes 5x less power and is 4x slower for ImageNet, when compared to the same NN on the latest Nvidia GPUs. Smaller NNs, that fit a single FPGA, are running faster then on GPUs on small (32x32) inputs, while consuming up to 20x less energy and power.



### An Adaptive Cluster-based Filtering Framework for Speckle Reduction of OCT Skin Images
- **Arxiv ID**: http://arxiv.org/abs/1708.02285v4
- **DOI**: None
- **Categories**: **cs.CV**, I.5.3; I.5.4; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1708.02285v4)
- **Published**: 2017-07-31 20:40:40+00:00
- **Updated**: 2018-07-02 18:37:22+00:00
- **Authors**: Elaheh Rashedi, Saba Adabi, Darius Mehregan, Silvia Conforto, Xue-wen Chen
- **Comment**: 17 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) has become a favorable device in the Dermatology discipline due to its moderate resolution and penetration depth. OCT images however contain a grainy pattern, called speckle, due to the use of a broadband source in the configuration of OCT. So far, a variety of filtering (de-speckling) techniques is introduced to reduce speckle in OCT images. Most of these methods are generic and can be applied to OCT images of different tissues. The ambition of this work is to provide a de-speckling framework specialized for filtering skin tissues for the community to utilize, adapt or build upon. In this paper, we present an adaptive cluster-based filtering framework, optimized for speckle reduction of OCT skin images. In this framework, by considering the layered structure of skin, first the OCT skin images are segmented into differentiable layers utilizing clustering algorithms, and then each cluster is de-speckled individually using adaptive filtering techniques. In this study, hierarchical clustering algorithm and adaptive Wiener filtering technique are utilized to develop the framework. The proposed method is tested on optical solid phantoms with predetermined optical properties. The method is also tested on healthy human skin images. The results show that the proposed cluster-based filtering method can effectively reduce the speckle and increase the signal-to-noise ratio and contrast while preserving the edges in the image. The proposed cluster-based filtering framework enables researchers to develop unsupervised learning solutions for de-speckling OCT skin images using adaptive filtering methods, or extend the framework to new applications.



### Learning Robust Representations for Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1708.00069v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.00069v1)
- **Published**: 2017-07-31 20:50:01+00:00
- **Updated**: 2017-07-31 20:50:01+00:00
- **Authors**: Peng Zheng, Aleksandr Y. Aravkin, Karthikeyan Natesan Ramamurthy, Jayaraman Jayaraman Thiagarajan
- **Comment**: 8 pages, 7 pages
- **Journal**: None
- **Summary**: Unsupervised learning techniques in computer vision often require learning latent representations, such as low-dimensional linear and non-linear subspaces. Noise and outliers in the data can frustrate these approaches by obscuring the latent spaces.   Our main goal is deeper understanding and new development of robust approaches for representation learning. We provide a new interpretation for existing robust approaches and present two specific contributions: a new robust PCA approach, which can separate foreground features from dynamic background, and a novel robust spectral clustering method, that can cluster facial images with high accuracy. Both contributions show superior performance to standard methods on real-world test sets.



### Towards the Success Rate of One: Real-time Unconstrained Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.00079v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00079v2)
- **Published**: 2017-07-31 21:56:14+00:00
- **Updated**: 2017-08-02 05:22:48+00:00
- **Authors**: Mahyar Najibi, Fan Yang, Qiaosong Wang, Robinson Piramuthu
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose an efficient and effective approach for unconstrained salient object detection in images using deep convolutional neural networks. Instead of generating thousands of candidate bounding boxes and refining them, our network directly learns to generate the saliency map containing the exact number of salient objects. During training, we convert the ground-truth rectangular boxes to Gaussian distributions that better capture the ROI regarding individual salient objects. During inference, the network predicts Gaussian distributions centered at salient objects with an appropriate covariance, from which bounding boxes are easily inferred. Notably, our network performs saliency map prediction without pixel-level annotations, salient object detection without object proposals, and salient object subitizing simultaneously, all in a single pass within a unified framework. Extensive experiments show that our approach outperforms existing methods on various datasets by a large margin, and achieves more than 100 fps with VGG16 network on a single GPU during inference.



