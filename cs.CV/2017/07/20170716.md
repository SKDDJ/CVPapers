# Arxiv Papers in cs.CV on 2017-07-16
### RED: Reinforced Encoder-Decoder Networks for Action Anticipation
- **Arxiv ID**: http://arxiv.org/abs/1707.04818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04818v1)
- **Published**: 2017-07-16 04:23:57+00:00
- **Updated**: 2017-07-16 04:23:57+00:00
- **Authors**: Jiyang Gao, Zhenheng Yang, Ram Nevatia
- **Comment**: None
- **Journal**: None
- **Summary**: Action anticipation aims to detect an action before it happens. Many real world applications in robotics and surveillance are related to this predictive capability. Current methods address this problem by first anticipating visual representations of future frames and then categorizing the anticipated representations to actions. However, anticipation is based on a single past frame's representation, which ignores the history trend. Besides, it can only anticipate a fixed future time. We propose a Reinforced Encoder-Decoder (RED) network for action anticipation. RED takes multiple history representations as input and learns to anticipate a sequence of future representations. One salient aspect of RED is that a reinforcement module is adopted to provide sequence-level supervision; the reward function is designed to encourage the system to make correct predictions as early as possible. We test RED on TVSeries, THUMOS-14 and TV-Human-Interaction datasets for action anticipation and achieve state-of-the-art performance on all datasets.



### Optical Music Recognition with Convolutional Sequence-to-Sequence Models
- **Arxiv ID**: http://arxiv.org/abs/1707.04877v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1707.04877v1)
- **Published**: 2017-07-16 13:11:22+00:00
- **Updated**: 2017-07-16 13:11:22+00:00
- **Authors**: Eelco van der Wel, Karen Ullrich
- **Comment**: ISMIR 2017
- **Journal**: None
- **Summary**: Optical Music Recognition (OMR) is an important technology within Music Information Retrieval. Deep learning models show promising results on OMR tasks, but symbol-level annotated data sets of sufficient size to train such models are not available and difficult to develop. We present a deep learning architecture called a Convolutional Sequence-to-Sequence model to both move towards an end-to-end trainable OMR pipeline, and apply a learning process that trains on full sentences of sheet music instead of individually labeled symbols. The model is trained and evaluated on a human generated data set, with various image augmentations based on real-world scenarios. This data set is the first publicly available set in OMR research with sufficient size to train and evaluate deep learning models. With the introduced augmentations a pitch recognition accuracy of 81% and a duration accuracy of 94% is achieved, resulting in a note level accuracy of 80%. Finally, the model is compared to commercially available methods, showing a large improvements over these applications.



### Generative Adversarial Network based on Resnet for Conditional Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1707.04881v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04881v1)
- **Published**: 2017-07-16 13:38:36+00:00
- **Updated**: 2017-07-16 13:38:36+00:00
- **Authors**: Meng Wang, Huafeng Li, Fang Li
- **Comment**: 6 pages, 15 figures, conference
- **Journal**: None
- **Summary**: The GANs promote an adversarive game to approximate complex and jointed example probability. The networks driven by noise generate fake examples to approximate realistic data distributions. Later the conditional GAN merges prior-conditions as input in order to transfer attribute vectors to the corresponding data. However, the CGAN is not designed to deal with the high dimension conditions since indirect guide of the learning is inefficiency. In this paper, we proposed a network ResGAN to generate fine images in terms of extremely degenerated images. The coarse images aligned to attributes are embedded as the generator inputs and classifier labels. In generative network, a straight path similar to the Resnet is cohered to directly transfer the coarse images to the higher layers. And adversarial training is circularly implemented to prevent degeneration of the generated images. Experimental results of applying the ResGAN to datasets MNIST, CIFAR10/100 and CELEBA show its higher accuracy to the state-of-art GANs.



### Chinese Typography Transfer
- **Arxiv ID**: http://arxiv.org/abs/1707.04904v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04904v2)
- **Published**: 2017-07-16 16:21:37+00:00
- **Updated**: 2017-08-02 04:46:30+00:00
- **Authors**: Jie Chang, Yujun Gu
- **Comment**: There is an error in Figure 5.(b) where the figure caption is
  "evaluation mse" instead of "Loss curve". It can lead to the misunderstanding
  of my performance under different configuration. So I request to withdraw
- **Journal**: None
- **Summary**: In this paper, we propose a new network architecture for Chinese typography transformation based on deep learning. The architecture consists of two sub-networks: (1)a fully convolutional network(FCN) aiming at transferring specified typography style to another in condition of preserving structure information; (2)an adversarial network aiming at generating more realistic strokes in some details. Unlike models proposed before 2012 relying on the complex segmentation of Chinese components or strokes, our model treats every Chinese character as an inseparable image, so pre-processing or post-preprocessing are abandoned. Besides, our model adopts end-to-end training without pre-trained used in other deep models. The experiments demonstrates that our model can synthesize realistic-looking target typography from any source typography both on printed style and handwriting style.



### Expected exponential loss for gaze-based video and volume ground truth annotation
- **Arxiv ID**: http://arxiv.org/abs/1707.04905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04905v1)
- **Published**: 2017-07-16 16:22:12+00:00
- **Updated**: 2017-07-16 16:22:12+00:00
- **Authors**: Laurent Lejeune, Mario Christoudias, Raphael Sznitman
- **Comment**: 9 pages, 5 figues, MICCAI 2017 - LABELS Workshop
- **Journal**: None
- **Summary**: Many recent machine learning approaches used in medical imaging are highly reliant on large amounts of image and ground truth data. In the context of object segmentation, pixel-wise annotations are extremely expensive to collect, especially in video and 3D volumes. To reduce this annotation burden, we propose a novel framework to allow annotators to simply observe the object to segment and record where they have looked at with a \$200 eye gaze tracker. Our method then estimates pixel-wise probabilities for the presence of the object throughout the sequence from which we train a classifier in semi-supervised setting using a novel Expected Exponential loss function. We show that our framework provides superior performances on a wide range of medical image settings compared to existing strategies and that our method can be combined with current crowd-sourcing paradigms as well.



### Improving Deep Pancreas Segmentation in CT and MRI Images via Recurrent Neural Contextual Learning and Direct Loss Function
- **Arxiv ID**: http://arxiv.org/abs/1707.04912v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04912v2)
- **Published**: 2017-07-16 16:57:25+00:00
- **Updated**: 2017-07-18 01:33:08+00:00
- **Authors**: Jinzheng Cai, Le Lu, Yuanpu Xie, Fuyong Xing, Lin Yang
- **Comment**: 8 pages, 7 figures, accepted to Medical Image Computing and Computer
  Assisted Interventions Conference (MICCAI) 2017
- **Journal**: None
- **Summary**: Deep neural networks have demonstrated very promising performance on accurate segmentation of challenging organs (e.g., pancreas) in abdominal CT and MRI scans. The current deep learning approaches conduct pancreas segmentation by processing sequences of 2D image slices independently through deep, dense per-pixel masking for each image, without explicitly enforcing spatial consistency constraint on segmentation of successive slices. We propose a new convolutional/recurrent neural network architecture to address the contextual learning and segmentation consistency problem. A deep convolutional sub-network is first designed and pre-trained from scratch. The output layer of this network module is then connected to recurrent layers and can be fine-tuned for contextual learning, in an end-to-end manner. Our recurrent sub-network is a type of Long short-term memory (LSTM) network that performs segmentation on an image by integrating its neighboring slice segmentation predictions, in the form of a dependent sequence processing. Additionally, a novel segmentation-direct loss function (named Jaccard Loss) is proposed and deep networks are trained to optimize Jaccard Index (JI) directly. Extensive experiments are conducted to validate our proposed deep models, on quantitative pancreas segmentation using both CT and MRI scans. Our method outperforms the state-of-the-art work on CT [11] and MRI pancreas segmentation [1], respectively.



### Pathological OCT Retinal Layer Segmentation using Branch Residual U-shape Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.04931v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04931v1)
- **Published**: 2017-07-16 19:17:16+00:00
- **Updated**: 2017-07-16 19:17:16+00:00
- **Authors**: Stefanos Apostolopoulos, Sandro De Zanet, Carlos Ciller, Sebastian Wolf, Raphael Sznitman
- **Comment**: 9 pages, 5 figures, MICCAI 2017
- **Journal**: None
- **Summary**: The automatic segmentation of retinal layer structures enables clinically-relevant quantification and monitoring of eye disorders over time in OCT imaging. Eyes with late-stage diseases are particularly challenging to segment, as their shape is highly warped due to pathological biomarkers. In this context, we propose a novel fully Convolutional Neural Network (CNN) architecture which combines dilated residual blocks in an asymmetric U-shape configuration, and can segment multiple layers of highly pathological eyes in one shot. We validate our approach on a dataset of late-stage AMD patients and demonstrate lower computational costs and higher performance compared to other state-of-the-art methods.



### Comparative Performance Analysis of Neural Networks Architectures on H2O Platform for Various Activation Functions
- **Arxiv ID**: http://arxiv.org/abs/1707.04940v1
- **DOI**: 10.1109/YSF.2017.8126654
- **Categories**: **cs.LG**, cs.CV, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1707.04940v1)
- **Published**: 2017-07-16 19:57:28+00:00
- **Updated**: 2017-07-16 19:57:28+00:00
- **Authors**: Yuriy Kochura, Sergii Stirenko, Yuri Gordienko
- **Comment**: 4 pages, 6 figures, 6 tables; 2017 IEEE International Young
  Scientists Forum on Applied Physics and Engineering (YSF-2017) (Lviv,
  Ukraine)
- **Journal**: None
- **Summary**: Deep learning (deep structured learning, hierarchi- cal learning or deep machine learning) is a branch of machine learning based on a set of algorithms that attempt to model high- level abstractions in data by using multiple processing layers with complex structures or otherwise composed of multiple non-linear transformations. In this paper, we present the results of testing neural networks architectures on H2O platform for various activation functions, stopping metrics, and other parameters of machine learning algorithm. It was demonstrated for the use case of MNIST database of handwritten digits in single-threaded mode that blind selection of these parameters can hugely increase (by 2-3 orders) the runtime without the significant increase of precision. This result can have crucial influence for opitmization of available and new machine learning methods, especially for image recognition problems.



### Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach
- **Arxiv ID**: http://arxiv.org/abs/1707.04960v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04960v1)
- **Published**: 2017-07-16 23:11:28+00:00
- **Updated**: 2017-07-16 23:11:28+00:00
- **Authors**: Aidean Sharghi, Jacob S. Laurel, Boqing Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed a resurgence of interest in video summarization. However, one of the main obstacles to the research on video summarization is the user subjectivity - users have various preferences over the summaries. The subjectiveness causes at least two problems. First, no single video summarizer fits all users unless it interacts with and adapts to the individual users. Second, it is very challenging to evaluate the performance of a video summarizer.   To tackle the first problem, we explore the recently proposed query-focused video summarization which introduces user preferences in the form of text queries about the video into the summarization process. We propose a memory network parameterized sequential determinantal point process in order to attend the user query onto different video frames and shots. To address the second challenge, we contend that a good evaluation metric for video summarization should focus on the semantic information that humans can perceive rather than the visual features or temporal overlaps. To this end, we collect dense per-video-shot concept annotations, compile a new dataset, and suggest an efficient evaluation method defined upon the concept annotations. We conduct extensive experiments contrasting our video summarizer to existing ones and present detailed analyses about the dataset and the new evaluation method.



