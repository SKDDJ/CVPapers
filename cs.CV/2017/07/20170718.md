# Arxiv Papers in cs.CV on 2017-07-18
### Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network
- **Arxiv ID**: http://arxiv.org/abs/1707.05425v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05425v7)
- **Published**: 2017-07-18 00:50:55+00:00
- **Updated**: 2020-09-08 08:23:11+00:00
- **Authors**: Jin Yamanaka, Shigesumi Kuwashima, Takio Kurita
- **Comment**: 9 pages, 4 figures. This paper is accepted at 24th International
  Conference On Neural Information Processing (ICONIP 2017)
- **Journal**: 24th International Conference of Neural Information Processing,
  ICONIP 2017, Proceedings, Part II (pp.217-225)
- **Summary**: We propose a highly efficient and faster Single Image Super-Resolution (SISR) model with Deep Convolutional neural networks (Deep CNN). Deep CNN have recently shown that they have a significant reconstruction performance on single-image super-resolution. Current trend is using deeper CNN layers to improve performance. However, deep models demand larger computation resources and is not suitable for network edge devices like mobile, tablet and IoT devices. Our model achieves state of the art reconstruction performance with at least 10 times lower calculation cost by Deep CNN with Residual Net, Skip Connection and Network in Network (DCSCN). A combination of Deep CNNs and Skip connection layers is used as a feature extractor for image features on both local and global area. Parallelized 1x1 CNNs, like the one called Network in Network, is also used for image reconstruction. That structure reduces the dimensions of the previous layer's output for faster computation with less information loss, and make it possible to process original images directly. Also we optimize the number of layers and filters of each CNN to significantly reduce the calculation cost. Thus, the proposed algorithm not only achieves the state of the art performance but also achieves faster and efficient computation. Code is available at https://github.com/jiny2001/dcscn-super-resolution



### Visually Aligned Word Embeddings for Improving Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.05427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05427v1)
- **Published**: 2017-07-18 01:07:32+00:00
- **Updated**: 2017-07-18 01:07:32+00:00
- **Authors**: Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, Anton van den Hengel
- **Comment**: Appearing in Proc. British Mach. Vis. Conf. (BMVC) 2017
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) highly depends on a good semantic embedding to connect the seen and unseen classes. Recently, distributed word embeddings (DWE) pre-trained from large text corpus have become a popular choice to draw such a connection. Compared with human defined attributes, DWEs are more scalable and easier to obtain. However, they are designed to reflect semantic similarity rather than visual similarity and thus using them in ZSL often leads to inferior performance. To overcome this visual-semantic discrepancy, this work proposes an objective function to re-align the distributed word embeddings with visual information by learning a neural network to map it into a new representation called visually aligned word embedding (VAWE). Thus the neighbourhood structure of VAWEs becomes similar to that in the visual domain. Note that in this work we do not design a ZSL method that projects the visual features and semantic embeddings onto a shared space but just impose a requirement on the structure of the mapped word embeddings. This strategy allows the learned VAWE to generalize to various ZSL methods and visual features. As evaluated via four state-of-the-art ZSL methods on four benchmark datasets, the VAWE exhibit consistent performance improvement.



### Discriminative Transformation Learning for Fuzzy Sparse Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1707.05446v1
- **DOI**: 10.1109/TCYB.2017.2729542
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05446v1)
- **Published**: 2017-07-18 03:06:14+00:00
- **Updated**: 2017-07-18 03:06:14+00:00
- **Authors**: Zaidao Wen, Biao Hou, Qian Wu, Licheng Jiao
- **Comment**: IEEE Trans. Cybern. Accept
- **Journal**: None
- **Summary**: This paper develops a novel iterative framework for subspace clustering in a learned discriminative feature domain. This framework consists of two modules of fuzzy sparse subspace clustering and discriminative transformation learning. In the first module, fuzzy latent labels containing discriminative information and latent representations capturing the subspace structure will be simultaneously evaluated in a feature domain. Then the linear transforming operator with respect to the feature domain will be successively updated in the second module with the advantages of more discrimination, subspace structure preservation and robustness to outliers. These two modules will be alternatively carried out and both theoretical analysis and empirical evaluations will demonstrate its effectiveness and superiorities. In particular, experimental results on three benchmark databases for subspace clustering clearly illustrate that the proposed framework can achieve significant improvements than other state-of-the-art approaches in terms of clustering accuracy.



### Pruning Convolutional Neural Networks for Image Instance Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1707.05455v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05455v1)
- **Published**: 2017-07-18 03:49:59+00:00
- **Updated**: 2017-07-18 03:49:59+00:00
- **Authors**: Gaurav Manek, Jie Lin, Vijay Chandrasekhar, Lingyu Duan, Sateesh Giduthuri, Xiaoli Li, Tomaso Poggio
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: In this work, we focus on the problem of image instance retrieval with deep descriptors extracted from pruned Convolutional Neural Networks (CNN). The objective is to heavily prune convolutional edges while maintaining retrieval performance. To this end, we introduce both data-independent and data-dependent heuristics to prune convolutional edges, and evaluate their performance across various compression rates with different deep descriptors over several benchmark datasets. Further, we present an end-to-end framework to fine-tune the pruned network, with a triplet loss function specially designed for the retrieval task. We show that the combination of heuristic pruning and fine-tuning offers 5x compression rate without considerable loss in retrieval performance.



### Coresets for Triangulation
- **Arxiv ID**: http://arxiv.org/abs/1707.05466v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.05466v1)
- **Published**: 2017-07-18 04:47:31+00:00
- **Updated**: 2017-07-18 04:47:31+00:00
- **Authors**: Qianggong Zhang, Tat-Jun Chin
- **Comment**: 14 pages for TPAMI summission
- **Journal**: None
- **Summary**: Multiple-view triangulation by $\ell_{\infty}$ minimisation has become established in computer vision. State-of-the-art $\ell_{\infty}$ triangulation algorithms exploit the quasiconvexity of the cost function to derive iterative update rules that deliver the global minimum. Such algorithms, however, can be computationally costly for large problem instances that contain many image measurements, e.g., from web-based photo sharing sites or long-term video recordings. In this paper, we prove that $\ell_{\infty}$ triangulation admits a coreset approximation scheme, which seeks small representative subsets of the input data called coresets. A coreset possesses the special property that the error of the $\ell_{\infty}$ solution on the coreset is within known bounds from the global minimum. We establish the necessary mathematical underpinnings of the coreset algorithm, specifically, by enacting the stopping criterion of the algorithm and proving that the resulting coreset gives the desired approximation accuracy. On large-scale triangulation problems, our method provides theoretically sound approximate solutions. Iterated until convergence, our coreset algorithm is also guaranteed to reach the true optimum. On practical datasets, we show that our technique can in fact attain the global minimiser much faster than current methods



### DCTM: Discrete-Continuous Transformation Matching for Semantic Flow
- **Arxiv ID**: http://arxiv.org/abs/1707.05471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05471v1)
- **Published**: 2017-07-18 05:16:12+00:00
- **Updated**: 2017-07-18 05:16:12+00:00
- **Authors**: Seungryong Kim, Dongbo Min, Stephen Lin, Kwanghoon Sohn
- **Comment**: None
- **Journal**: None
- **Summary**: Techniques for dense semantic correspondence have provided limited ability to deal with the geometric variations that commonly exist between semantically similar images. While variations due to scale and rotation have been examined, there lack practical solutions for more complex deformations such as affine transformations because of the tremendous size of the associated solution space. To address this problem, we present a discrete-continuous transformation matching (DCTM) framework where dense affine transformation fields are inferred through a discrete label optimization in which the labels are iteratively updated via continuous regularization. In this way, our approach draws solutions from the continuous space of affine transformations in a manner that can be computed efficiently through constant-time edge-aware filtering and a proposed affine-varying CNN-based descriptor. Experimental results show that this model outperforms the state-of-the-art methods for dense semantic correspondence on various benchmarks.



### APE-GAN: Adversarial Perturbation Elimination with GAN
- **Arxiv ID**: http://arxiv.org/abs/1707.05474v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05474v3)
- **Published**: 2017-07-18 05:29:27+00:00
- **Updated**: 2017-09-26 03:38:36+00:00
- **Authors**: Shiwei Shen, Guoqing Jin, Ke Gao, Yongdong Zhang
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Although neural networks could achieve state-of-the-art performance while recongnizing images, they often suffer a tremendous defeat from adversarial examples--inputs generated by utilizing imperceptible but intentional perturbation to clean samples from the datasets. How to defense against adversarial examples is an important problem which is well worth researching. So far, very few methods have provided a significant defense to adversarial examples. In this paper, a novel idea is proposed and an effective framework based Generative Adversarial Nets named APE-GAN is implemented to defense against the adversarial examples. The experimental results on three benchmark datasets including MNIST, CIFAR10 and ImageNet indicate that APE-GAN is effective to resist adversarial examples generated from five attacks.



### Guided Co-training for Large-Scale Multi-View Spectral Clustering
- **Arxiv ID**: http://arxiv.org/abs/1707.09866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09866v1)
- **Published**: 2017-07-18 05:44:52+00:00
- **Updated**: 2017-07-18 05:44:52+00:00
- **Authors**: Tyng-Luh Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In many real-world applications, we have access to multiple views of the data, each of which characterizes the data from a distinct aspect. Several previous algorithms have demonstrated that one can achieve better clustering accuracy by integrating information from all views appropriately than using only an individual view. Owing to the effectiveness of spectral clustering, many multi-view clustering methods are based on it. Unfortunately, they have limited applicability to large-scale data due to the high computational complexity of spectral clustering. In this work, we propose a novel multi-view spectral clustering method for large-scale data. Our approach is structured under the guided co-training scheme to fuse distinct views, and uses the sampling technique to accelerate spectral clustering. More specifically, we first select $p$ ($\ll n$) landmark points and then approximate the eigen-decomposition accordingly. The augmented view, which is essential to guided co-training process, can then be quickly determined by our method. The proposed algorithm scales linearly with the number of given data. Extensive experiments have been performed and the results support the advantage of our method for handling the large-scale multi-view situation.



### Vision-based Real Estate Price Estimation
- **Arxiv ID**: http://arxiv.org/abs/1707.05489v3
- **DOI**: 10.1007/s00138-018-0922-2
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.05489v3)
- **Published**: 2017-07-18 06:29:02+00:00
- **Updated**: 2018-10-03 15:39:27+00:00
- **Authors**: Omid Poursaeed, Tomas Matera, Serge Belongie
- **Comment**: None
- **Journal**: Machine Vision and Applications, 29(4), 667-676, 2018
- **Summary**: Since the advent of online real estate database companies like Zillow, Trulia and Redfin, the problem of automatic estimation of market values for houses has received considerable attention. Several real estate websites provide such estimates using a proprietary formula. Although these estimates are often close to the actual sale prices, in some cases they are highly inaccurate. One of the key factors that affects the value of a house is its interior and exterior appearance, which is not considered in calculating automatic value estimates. In this paper, we evaluate the impact of visual characteristics of a house on its market value. Using deep convolutional neural networks on a large dataset of photos of home interiors and exteriors, we develop a method for estimating the luxury level of real estate photos. We also develop a novel framework for automated value assessment using the above photos in addition to home characteristics including size, offered price and number of bedrooms. Finally, by applying our proposed method for price estimation to a new dataset of real estate photos and metadata, we show that it outperforms Zillow's estimates.



### Order-Free RNN with Visual Attention for Multi-Label Classification
- **Arxiv ID**: http://arxiv.org/abs/1707.05495v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05495v3)
- **Published**: 2017-07-18 06:44:16+00:00
- **Updated**: 2017-12-20 06:07:28+00:00
- **Authors**: Shang-Fu Chen, Yi-Chen Chen, Chih-Kuan Yeh, Yu-Chiang Frank Wang
- **Comment**: Accepted at 32nd AAAI Conference on Artificial Intelligence (AAAI-18)
- **Journal**: None
- **Summary**: In this paper, we propose the joint learning attention and recurrent neural network (RNN) models for multi-label classification. While approaches based on the use of either model exist (e.g., for the task of image captioning), training such existing network architectures typically require pre-defined label sequences. For multi-label classification, it would be desirable to have a robust inference process, so that the prediction error would not propagate and thus affect the performance. Our proposed model uniquely integrates attention and Long Short Term Memory (LSTM) models, which not only addresses the above problem but also allows one to identify visual objects of interests with varying sizes without the prior knowledge of particular label ordering. More importantly, label co-occurrence information can be jointly exploited by our LSTM model. Finally, by advancing the technique of beam search, prediction of multiple labels can be efficiently achieved by our proposed network model.



### Beyond Forward Shortcuts: Fully Convolutional Master-Slave Networks (MSNets) with Backward Skip Connections for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.05537v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05537v1)
- **Published**: 2017-07-18 09:31:05+00:00
- **Updated**: 2017-07-18 09:31:05+00:00
- **Authors**: Abrar H. Abdulnabi, Stefan Winkler, Gang Wang
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Recent deep CNNs contain forward shortcut connections; i.e. skip connections from low to high layers. Reusing features from lower layers that have higher resolution (location information) benefit higher layers to recover lost details and mitigate information degradation. However, during inference the lower layers do not know about high layer features, although they contain contextual high semantics that benefit low layers to adaptively extract informative features for later layers. In this paper, we study the influence of backward skip connections which are in the opposite direction to forward shortcuts, i.e. paths from high layers to low layers. To achieve this -- which indeed runs counter to the nature of feed-forward networks -- we propose a new fully convolutional model that consists of a pair of networks. A `Slave' network is dedicated to provide the backward connections from its top layers to the `Master' network's bottom layers. The Master network is used to produce the final label predictions. In our experiments we validate the proposed FCN model on ADE20K (ImageNet scene parsing), PASCAL-Context, and PASCAL VOC 2011 datasets.



### Spectral Filter Tracking
- **Arxiv ID**: http://arxiv.org/abs/1707.05553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05553v1)
- **Published**: 2017-07-18 10:40:08+00:00
- **Updated**: 2017-07-18 10:40:08+00:00
- **Authors**: Zhen Cui, You yi Cai, Wen ming Zheng, Jian Yang
- **Comment**: 11pages
- **Journal**: None
- **Summary**: Visual object tracking is a challenging computer vision task with numerous real-world applications. Here we propose a simple but efficient Spectral Filter Tracking (SFT)method. To characterize rotational and translation invariance of tracking targets, the candidate image region is models as a pixelwise grid graph. Instead of the conventional graph matching, we convert the tracking into a plain least square regression problem to estimate the best center coordinate of the target. But different from the holistic regression of correlation filter based methods, SFT can operate on localized surrounding regions of each pixel (i.e.,vertex) by using spectral graph filters, which thus is more robust to resist local variations and cluttered background.To bypass the eigenvalue decomposition problem of the graph Laplacian matrix L, we parameterize spectral graph filters as the polynomial of L by spectral graph theory, in which L k exactly encodes a k-hop local neighborhood of each vertex. Finally, the filter parameters (i.e., polynomial coefficients) as well as feature projecting functions are jointly integrated into the regression model.



### Robust Monocular SLAM for Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/1707.05564v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05564v2)
- **Published**: 2017-07-18 11:31:13+00:00
- **Updated**: 2018-11-17 20:00:08+00:00
- **Authors**: Suvam Patra, Kartikeya Gupta, Faran Ahmad, Chetan Arora, Subhashis Banerjee
- **Comment**: Accepted for publication at IEEE WACV 2019
- **Journal**: None
- **Summary**: Regardless of the tremendous progress, a truly general purpose pipeline for Simultaneous Localization and Mapping (SLAM) remains a challenge. We investigate the reported failure of state of the art (SOTA) SLAM techniques on egocentric videos. We find that the dominant 3D rotations, low parallax between successive frames, and primarily forward motion in egocentric videos are the most common causes of failures. The incremental nature of SOTA SLAM, in the presence of unreliable pose and 3D estimates in egocentric videos, with no opportunities for global loop closures, generates drifts and leads to the eventual failures of such techniques. Taking inspiration from batch mode Structure from Motion (SFM) techniques, we propose to solve SLAM as an SFM problem over the sliding temporal windows. This makes the problem well constrained. Further, we propose to initialize the camera poses using 2D rotation averaging, followed by translation averaging before structure estimation using bundle adjustment. This helps in stabilizing the camera poses when 3D estimates are not reliable. We show that the proposed SLAM technique, incorporating the two key ideas works successfully for long, shaky egocentric videos where other SOTA techniques have been reported to fail. Qualitative and quantitative comparisons on publicly available egocentric video datasets validate our results.



### Fast Feature Fool: A data independent approach to universal adversarial perturbations
- **Arxiv ID**: http://arxiv.org/abs/1707.05572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05572v1)
- **Published**: 2017-07-18 11:48:14+00:00
- **Updated**: 2017-07-18 11:48:14+00:00
- **Authors**: Konda Reddy Mopuri, Utsav Garg, R. Venkatesh Babu
- **Comment**: BMVC 2017 and codes are available at
  https://github.com/utsavgarg/fast-feature-fool
- **Journal**: None
- **Summary**: State-of-the-art object recognition Convolutional Neural Networks (CNNs) are shown to be fooled by image agnostic perturbations, called universal adversarial perturbations. It is also observed that these perturbations generalize across multiple networks trained on the same target data. However, these algorithms require training data on which the CNNs were trained and compute adversarial perturbations via complex optimization. The fooling performance of these approaches is directly proportional to the amount of available training data. This makes them unsuitable for practical attacks since its unreasonable for an attacker to have access to the training data. In this paper, for the first time, we propose a novel data independent approach to generate image agnostic perturbations for a range of CNNs trained for object recognition. We further show that these perturbations are transferable across multiple network architectures trained either on same or different data. In the absence of data, our method generates universal adversarial perturbations efficiently via fooling the features learned at multiple layers thereby causing CNNs to misclassify. Experiments demonstrate impressive fooling rates and surprising transferability for the proposed universal perturbations generated without any training data.



### One-shot Face Recognition by Promoting Underrepresented Classes
- **Arxiv ID**: http://arxiv.org/abs/1707.05574v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05574v2)
- **Published**: 2017-07-18 11:51:13+00:00
- **Updated**: 2018-03-15 21:44:39+00:00
- **Authors**: Yandong Guo, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of training large-scale face identification model with imbalanced training data. This problem naturally exists in many real scenarios including large-scale celebrity recognition, movie actor annotation, etc. Our solution contains two components. First, we build a face feature extraction model, and improve its performance, especially for the persons with very limited training samples, by introducing a regularizer to the cross entropy loss for the multi-nomial logistic regression (MLR) learning. This regularizer encourages the directions of the face features from the same class to be close to the direction of their corresponding classification weight vector in the logistic regression. Second, we build a multi-class classifier using MLR on top of the learned face feature extraction model. Since the standard MLR has poor generalization capability for the one-shot classes even if these classes have been oversampled, we propose a novel supervision signal called underrepresented-classes promotion loss, which aligns the norms of the weight vectors of the one-shot classes (a.k.a. underrepresented-classes) to those of the normal classes. In addition to the original cross entropy loss, this new loss term effectively promotes the underrepresented classes in the learned model and leads to a remarkable improvement in face recognition performance.   We test our solution on the MS-Celeb-1M low-shot learning benchmark task. Our solution recognizes 94.89% of the test images at the precision of 99\% for the one-shot classes. To the best of our knowledge, this is the best performance among all the published methods using this benchmark task with the same setup, including all the participants in the recent MS-Celeb-1M challenge at ICCV 2017.



### Domain Adaptation for Resume Classification Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.05576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05576v1)
- **Published**: 2017-07-18 12:06:09+00:00
- **Updated**: 2017-07-18 12:06:09+00:00
- **Authors**: Luiza Sayfullina, Eric Malmi, Yiping Liao, Alex Jung
- **Comment**: To be published in AIST proceedings: Springer's Lecture Notes in
  Computer Science (LNCS) series
- **Journal**: None
- **Summary**: We propose a novel method for classifying resume data of job applicants into 27 different job categories using convolutional neural networks. Since resume data is costly and hard to obtain due to its sensitive nature, we use domain adaptation. In particular, we train a classifier on a large number of freely available job description snippets and then use it to classify resume data. We empirically verify a reasonable classification performance of our approach despite having only a small amount of labeled resume data available.



### VSE++: Improving Visual-Semantic Embeddings with Hard Negatives
- **Arxiv ID**: http://arxiv.org/abs/1707.05612v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.05612v4)
- **Published**: 2017-07-18 13:51:32+00:00
- **Updated**: 2018-07-29 19:11:57+00:00
- **Authors**: Fartash Faghri, David J. Fleet, Jamie Ryan Kiros, Sanja Fidler
- **Comment**: Accepted as spotlight presentation at British Machine Vision
  Conference (BMVC) 2018. Code: https://github.com/fartashf/vsepp
- **Journal**: None
- **Summary**: We present a new technique for learning visual-semantic embeddings for cross-modal retrieval. Inspired by hard negative mining, the use of hard negatives in structured prediction, and ranking loss functions, we introduce a simple change to common loss functions used for multi-modal embeddings. That, combined with fine-tuning and use of augmented data, yields significant gains in retrieval performance. We showcase our approach, VSE++, on MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods. On MS-COCO our approach outperforms state-of-the-art methods by 8.8% in caption retrieval and 11.3% in image retrieval (at R@1).



### Fast Screening Algorithm for Rotation and Scale Invariant Template Matching
- **Arxiv ID**: http://arxiv.org/abs/1707.05647v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05647v2)
- **Published**: 2017-07-18 14:38:07+00:00
- **Updated**: 2017-07-19 07:51:08+00:00
- **Authors**: Bolin Liu, Xiao Shu, Xiaolin Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a generic pre-processor for expediting conventional template matching techniques. Instead of locating the best matched patch in the reference image to a query template via exhaustive search, the proposed algorithm rules out regions with no possible matches with minimum computational efforts. While working on simple patch features, such as mean, variance and gradient, the fast pre-screening is highly discriminative. Its computational efficiency is gained by using a novel octagonal-star-shaped template and the inclusion-exclusion principle to extract and compare patch features. Moreover, it can handle arbitrary rotation and scaling of reference images effectively. Extensive experiments demonstrate that the proposed algorithm greatly reduces the search space while never missing the best match.



### Faster Than Real-time Facial Alignment: A 3D Spatial Transformer Network Approach in Unconstrained Poses
- **Arxiv ID**: http://arxiv.org/abs/1707.05653v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05653v2)
- **Published**: 2017-07-18 14:51:35+00:00
- **Updated**: 2017-09-08 16:58:53+00:00
- **Authors**: Chandrasekhar Bhagavatula, Chenchen Zhu, Khoa Luu, Marios Savvides
- **Comment**: International Conference on Computer Vision (ICCV) 2017
- **Journal**: None
- **Summary**: Facial alignment involves finding a set of landmark points on an image with a known semantic meaning. However, this semantic meaning of landmark points is often lost in 2D approaches where landmarks are either moved to visible boundaries or ignored as the pose of the face changes. In order to extract consistent alignment points across large poses, the 3D structure of the face must be considered in the alignment step. However, extracting a 3D structure from a single 2D image usually requires alignment in the first place. We present our novel approach to simultaneously extract the 3D shape of the face and the semantically consistent 2D alignment through a 3D Spatial Transformer Network (3DSTN) to model both the camera projection matrix and the warping parameters of a 3D model. By utilizing a generic 3D model and a Thin Plate Spline (TPS) warping function, we are able to generate subject specific 3D shapes without the need for a large 3D shape basis. In addition, our proposed network can be trained in an end-to-end framework on entirely synthetic data from the 300W-LP dataset. Unlike other 3D methods, our approach only requires one pass through the network resulting in a faster than real-time alignment. Evaluations of our model on the Annotated Facial Landmarks in the Wild (AFLW) and AFLW2000-3D datasets show our method achieves state-of-the-art performance over other 3D approaches to alignment.



### Exploiting Convolutional Representations for Multiscale Human Settlement Detection
- **Arxiv ID**: http://arxiv.org/abs/1707.05683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05683v1)
- **Published**: 2017-07-18 15:27:18+00:00
- **Updated**: 2017-07-18 15:27:18+00:00
- **Authors**: Dalton Lunga, Dilip Patlolla, Lexie Yang, Jeanette Weaver, Budhendra Bhadhuri
- **Comment**: 4 pages, 5 figures, IGARSS 2017
- **Journal**: None
- **Summary**: We test this premise and explore representation spaces from a single deep convolutional network and their visualization to argue for a novel unified feature extraction framework. The objective is to utilize and re-purpose trained feature extractors without the need for network retraining on three remote sensing tasks i.e. superpixel mapping, pixel-level segmentation and semantic based image visualization. By leveraging the same convolutional feature extractors and viewing them as visual information extractors that encode different image representation spaces, we demonstrate a preliminary inductive transfer learning potential on multiscale experiments that incorporate edge-level details up to semantic-level information.



### Hashed Binary Search Sampling for Convolutional Network Training with Large Overhead Image Patches
- **Arxiv ID**: http://arxiv.org/abs/1707.05685v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05685v1)
- **Published**: 2017-07-18 15:29:27+00:00
- **Updated**: 2017-07-18 15:29:27+00:00
- **Authors**: Dalton Lunga, Lexie Yang, Budhendra Bhaduri
- **Comment**: 4 pages, 5 figures, IGARSS 2017
- **Journal**: None
- **Summary**: Very large overhead imagery associated with ground truth maps has the potential to generate billions of training image patches for machine learning algorithms. However, random sampling selection criteria often leads to redundant and noisy-image patches for model training. With minimal research efforts behind this challenge, the current status spells missed opportunities to develop supervised learning algorithms that generalize over wide geographical scenes. In addition, much of the computational cycles for large scale machine learning are poorly spent crunching through noisy and redundant image patches. We demonstrate a potential framework to address these challenges specifically, while evaluating a human settlement detection task. A novel binary search tree sampling scheme is fused with a kernel based hashing procedure that maps image patches into hash-buckets using binary codes generated from image content. The framework exploits inherent redundancy within billions of image patches to promote mostly high variance preserving samples for accelerating algorithmic training and increasing model generalization.



### Learning Fashion Compatibility with Bidirectional LSTMs
- **Arxiv ID**: http://arxiv.org/abs/1707.05691v1
- **DOI**: 10.1145/3123266.3123394
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05691v1)
- **Published**: 2017-07-18 15:36:53+00:00
- **Updated**: 2017-07-18 15:36:53+00:00
- **Authors**: Xintong Han, Zuxuan Wu, Yu-Gang Jiang, Larry S. Davis
- **Comment**: ACM MM 17
- **Journal**: None
- **Summary**: The ubiquity of online fashion shopping demands effective recommendation services for customers. In this paper, we study two types of fashion recommendation: (i) suggesting an item that matches existing components in a set to form a stylish outfit (a collection of fashion items), and (ii) generating an outfit with multimodal (images/text) specifications from a user. To this end, we propose to jointly learn a visual-semantic embedding and the compatibility relationships among fashion items in an end-to-end fashion. More specifically, we consider a fashion outfit to be a sequence (usually from top to bottom and then accessories) and each item in the outfit as a time step. Given the fashion items in an outfit, we train a bidirectional LSTM (Bi-LSTM) model to sequentially predict the next item conditioned on previous ones to learn their compatibility relationships. Further, we learn a visual-semantic space by regressing image features to their semantic representations aiming to inject attribute and category information as a regularization for training the LSTM. The trained network can not only perform the aforementioned recommendations effectively but also predict the compatibility of a given outfit. We conduct extensive experiments on our newly collected Polyvore dataset, and the results provide strong qualitative and quantitative evidence that our framework outperforms alternative methods.



### Choosing Smartly: Adaptive Multimodal Fusion for Object Detection in Changing Environments
- **Arxiv ID**: http://arxiv.org/abs/1707.05733v2
- **DOI**: 10.1109/IROS.2016.7759048
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.05733v2)
- **Published**: 2017-07-18 16:36:56+00:00
- **Updated**: 2019-11-19 12:43:54+00:00
- **Authors**: Oier Mees, Andreas Eitel, Wolfram Burgard
- **Comment**: Published at the 2016 IEEE/RSJ International Conference on
  Intelligent Robots and Systems. Added a new baseline with respect to the IROS
  version. Project page with code, pretrained models and our InOutDoorPeople
  RGB-D dataset at http://adaptivefusion.cs.uni-freiburg.de/
- **Journal**: None
- **Summary**: Object detection is an essential task for autonomous robots operating in dynamic and changing environments. A robot should be able to detect objects in the presence of sensor noise that can be induced by changing lighting conditions for cameras and false depth readings for range sensors, especially RGB-D cameras. To tackle these challenges, we propose a novel adaptive fusion approach for object detection that learns weighting the predictions of different sensor modalities in an online manner. Our approach is based on a mixture of convolutional neural network (CNN) experts and incorporates multiple modalities including appearance, depth and motion. We test our method in extensive robot experiments, in which we detect people in a combined indoor and outdoor scenario from RGB-D data, and we demonstrate that our method can adapt to harsh lighting changes and severe camera motion blur. Furthermore, we present a new RGB-D dataset for people detection in mixed in- and outdoor environments, recorded with a mobile robot. Code, pretrained models and dataset are available at http://adaptivefusion.cs.uni-freiburg.de



### Skeleton-Based Human Action Recognition with Global Context-Aware Attention LSTM Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.05740v5
- **DOI**: 10.1109/TIP.2017.2785279
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05740v5)
- **Published**: 2017-07-18 17:03:53+00:00
- **Updated**: 2018-01-11 15:36:27+00:00
- **Authors**: Jun Liu, Gang Wang, Ling-Yu Duan, Kamila Abdiyeva, Alex C. Kot
- **Comment**: None
- **Journal**: None
- **Summary**: Human action recognition in 3D skeleton sequences has attracted a lot of research attention. Recently, Long Short-Term Memory (LSTM) networks have shown promising performance in this task due to their strengths in modeling the dependencies and dynamics in sequential data. As not all skeletal joints are informative for action recognition, and the irrelevant joints often bring noise which can degrade the performance, we need to pay more attention to the informative ones. However, the original LSTM network does not have explicit attention ability. In this paper, we propose a new class of LSTM network, Global Context-Aware Attention LSTM (GCA-LSTM), for skeleton based action recognition. This network is capable of selectively focusing on the informative joints in each frame of each skeleton sequence by using a global context memory cell. To further improve the attention capability of our network, we also introduce a recurrent attention mechanism, with which the attention performance of the network can be enhanced progressively. Moreover, we propose a stepwise training scheme in order to train our network effectively. Our approach achieves state-of-the-art performance on five challenging benchmark datasets for skeleton based action recognition.



### Transitioning between Convolutional and Fully Connected Layers in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.05743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05743v1)
- **Published**: 2017-07-18 17:07:20+00:00
- **Updated**: 2017-07-18 17:07:20+00:00
- **Authors**: Shazia Akbar, Mohammad Peikari, Sherine Salama, Sharon Nofech-Mozes, Anne Martel
- **Comment**: This work is to appear at the 3rd workshop on Deep Learning in
  Medical Image Analysis (DLMIA), MICCAI 2017
- **Journal**: None
- **Summary**: Digital pathology has advanced substantially over the last decade however tumor localization continues to be a challenging problem due to highly complex patterns and textures in the underlying tissue bed. The use of convolutional neural networks (CNNs) to analyze such complex images has been well adopted in digital pathology. However in recent years, the architecture of CNNs have altered with the introduction of inception modules which have shown great promise for classification tasks. In this paper, we propose a modified "transition" module which learns global average pooling layers from filters of varying sizes to encourage class-specific filters at multiple spatial resolutions. We demonstrate the performance of the transition module in AlexNet and ZFNet, for classifying breast tumors in two independent datasets of scanned histology sections, of which the transition module was superior.



### AirCode: Unobtrusive Physical Tags for Digital Fabrication
- **Arxiv ID**: http://arxiv.org/abs/1707.05754v2
- **DOI**: 10.1145/3126594.3126635
- **Categories**: **cs.HC**, cs.CV, cs.GR, I.2.10; J.6; H.5
- **Links**: [PDF](http://arxiv.org/pdf/1707.05754v2)
- **Published**: 2017-07-18 17:27:16+00:00
- **Updated**: 2017-08-07 22:34:53+00:00
- **Authors**: Dingzeyu Li, Avinash S. Nair, Shree K. Nayar, Changxi Zheng
- **Comment**: ACM UIST 2017 Technical Papers
- **Journal**: None
- **Summary**: We present AirCode, a technique that allows the user to tag physically fabricated objects with given information. An AirCode tag consists of a group of carefully designed air pockets placed beneath the object surface. These air pockets are easily produced during the fabrication process of the object, without any additional material or postprocessing. Meanwhile, the air pockets affect only the scattering light transport under the surface, and thus are hard to notice to our naked eyes. But, by using a computational imaging method, the tags become detectable. We present a tool that automates the design of air pockets for the user to encode information. AirCode system also allows the user to retrieve the information from captured images via a robust decoding algorithm. We demonstrate our tagging technique with applications for metadata embedding, robotic grasping, as well as conveying object affordances.



### Optimizing the Latent Space of Generative Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.05776v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.05776v2)
- **Published**: 2017-07-18 17:58:34+00:00
- **Updated**: 2019-05-20 13:19:44+00:00
- **Authors**: Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most successful applications, GAN models share two common aspects: solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions; and parameterizing the generator and the discriminator as deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators using simple reconstruction losses. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors; all of this without the adversarial optimization scheme.



### A Novel Deep Learning Architecture for Testis Histology Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1707.05809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05809v1)
- **Published**: 2017-07-18 18:19:41+00:00
- **Updated**: 2017-07-18 18:19:41+00:00
- **Authors**: Chia-Yu Kao, Leonard McMillan
- **Comment**: None
- **Journal**: None
- **Summary**: Unlike other histology analysis, classification of tubule status in testis histology is very challenging due to their high similarity of texture and shape. Traditional deep learning networks have difficulties to capture nuance details among different tubule categories. In this paper, we propose a novel deep learning architecture for feature learning, image classification, and image reconstruction. It is based on stacked auto-encoders with an additional layer, called a hyperlayer, which is created to capture features of an image at different layers in the network. This addition effectively combines features at different scales and thus provides a more complete profile for further classification. Evaluation is performed on a set of 10,542 tubule image patches. We demonstrate our approach with two experiments on two different subsets of the dataset. The results show that the features learned from our architecture achieve more than 98% accuracy and represent an improvement over traditional deep network architectures.



### Discovering Class-Specific Pixels for Weakly-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.05821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05821v1)
- **Published**: 2017-07-18 19:03:22+00:00
- **Updated**: 2017-07-18 19:03:22+00:00
- **Authors**: Arslan Chaudhry, Puneet K. Dokania, Philip H. S. Torr
- **Comment**: None
- **Journal**: 28th British Machine Vision Conference (BMVC), 2017
- **Summary**: We propose an approach to discover class-specific pixels for the weakly-supervised semantic segmentation task. We show that properly combining saliency and attention maps allows us to obtain reliable cues capable of significantly boosting the performance. First, we propose a simple yet powerful hierarchical approach to discover the class-agnostic salient regions, obtained using a salient object detector, which otherwise would be ignored. Second, we use fully convolutional attention maps to reliably localize the class-specific regions in a given image. We combine these two cues to discover class-specific pixels which are then used as an approximate ground truth for training a CNN. While solving the weakly supervised semantic segmentation task, we ensure that the image-level classification task is also solved in order to enforce the CNN to assign at least one pixel to each object present in the image. Experimentally, on the PASCAL VOC12 val and test sets, we obtain the mIoU of 60.8% and 61.9%, achieving the performance gains of 5.1% and 5.2% compared to the published state-of-the-art results. The code is made publicly available.



### The Devil is in the Decoder: Classification, Regression and GANs
- **Arxiv ID**: http://arxiv.org/abs/1707.05847v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05847v3)
- **Published**: 2017-07-18 20:33:54+00:00
- **Updated**: 2019-02-19 21:27:50+00:00
- **Authors**: Zbigniew Wojna, Vittorio Ferrari, Sergio Guadarrama, Nathan Silberman, Liang-Chieh Chen, Alireza Fathi, Jasper Uijlings
- **Comment**: None
- **Journal**: None
- **Summary**: Many machine vision applications, such as semantic segmentation and depth prediction, require predictions for every pixel of the input image. Models for such problems usually consist of encoders which decrease spatial resolution while learning a high-dimensional representation, followed by decoders who recover the original input resolution and result in low-dimensional predictions. While encoders have been studied rigorously, relatively few studies address the decoder side. This paper presents an extensive comparison of a variety of decoders for a variety of pixel-wise tasks ranging from classification, regression to synthesis. Our contributions are: (1) Decoders matter: we observe significant variance in results between different types of decoders on various problems. (2) We introduce new residual-like connections for decoders. (3) We introduce a novel decoder: bilinear additive upsampling. (4) We explore prediction artifacts.



