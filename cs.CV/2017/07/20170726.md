# Arxiv Papers in cs.CV on 2017-07-26
### Efficient Yet Deep Convolutional Neural Networks for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.08254v3
- **DOI**: 10.1109/SAIN.2018.8673354
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08254v3)
- **Published**: 2017-07-26 00:15:35+00:00
- **Updated**: 2018-07-28 09:30:54+00:00
- **Authors**: Sharif Amit Kamran, Ali Shihab Sabbir
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Semantic Segmentation using deep convolutional neural network pose more complex challenge for any GPU intensive task. As it has to compute million of parameters, it results to huge memory consumption. Moreover, extracting finer features and conducting supervised training tends to increase the complexity. With the introduction of Fully Convolutional Neural Network, which uses finer strides and utilizes deconvolutional layers for upsampling, it has been a go to for any image segmentation task. In this paper, we propose two segmentation architecture which not only needs one-third the parameters to compute but also gives better accuracy than the similar architectures. The model weights were transferred from the popular neural net like VGG19 and VGG16 which were trained on Imagenet classification data-set. Then we transform all the fully connected layers to convolutional layers and use dilated convolution for decreasing the parameters. Lastly, we add finer strides and attach four skip architectures which are element-wise summed with the deconvolutional layers in steps. We train and test on different sparse and fine data-sets like Pascal VOC2012, Pascal-Context and NYUDv2 and show how better our model performs in this tasks. On the other hand our model has a faster inference time and consumes less memory for training and testing on NVIDIA Pascal GPUs, making it more efficient and less memory consuming architecture for pixel-wise segmentation.



### Fast Deep Matting for Portrait Animation on Mobile Phone
- **Arxiv ID**: http://arxiv.org/abs/1707.08289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08289v1)
- **Published**: 2017-07-26 05:05:48+00:00
- **Updated**: 2017-07-26 05:05:48+00:00
- **Authors**: Bingke Zhu, Yingying Chen, Jinqiao Wang, Si Liu, Bo Zhang, Ming Tang
- **Comment**: ACM Multimedia Conference (MM) 2017 camera-ready
- **Journal**: None
- **Summary**: Image matting plays an important role in image and video editing. However, the formulation of image matting is inherently ill-posed. Traditional methods usually employ interaction to deal with the image matting problem with trimaps and strokes, and cannot run on the mobile phone in real-time. In this paper, we propose a real-time automatic deep matting approach for mobile devices. By leveraging the densely connected blocks and the dilated convolution, a light full convolutional network is designed to predict a coarse binary mask for portrait images. And a feathering block, which is edge-preserving and matting adaptive, is further developed to learn the guided filter and transform the binary mask into alpha matte. Finally, an automatic portrait animation system based on fast deep matting is built on mobile devices, which does not need any interaction and can realize real-time matting with 15 fps. The experiments show that the proposed approach achieves comparable results with the state-of-the-art matting solvers.



### Graph-Based Classification of Omnidirectional Images
- **Arxiv ID**: http://arxiv.org/abs/1707.08301v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.08301v1)
- **Published**: 2017-07-26 06:39:45+00:00
- **Updated**: 2017-07-26 06:39:45+00:00
- **Authors**: Renata Khasanova, Pascal Frossard
- **Comment**: None
- **Journal**: None
- **Summary**: Omnidirectional cameras are widely used in such areas as robotics and virtual reality as they provide a wide field of view. Their images are often processed with classical methods, which might unfortunately lead to non-optimal solutions as these methods are designed for planar images that have different geometrical properties than omnidirectional ones. In this paper we study image classification task by taking into account the specific geometry of omnidirectional cameras with graph-based representations. In particular, we extend deep learning architectures to data on graphs; we propose a principled way of graph construction such that convolutional filters respond similarly for the same pattern on different positions of the image regardless of lens distortions. Our experiments show that the proposed method outperforms current techniques for the omnidirectional image classification problem.



### Cascaded Scene Flow Prediction using Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.08313v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08313v2)
- **Published**: 2017-07-26 07:53:54+00:00
- **Updated**: 2017-10-05 22:52:31+00:00
- **Authors**: Zhile Ren, Deqing Sun, Jan Kautz, Erik B. Sudderth
- **Comment**: International Conference on 3D Vision (3DV), 2017 (oral presentation)
- **Journal**: None
- **Summary**: Given two consecutive frames from a pair of stereo cameras, 3D scene flow methods simultaneously estimate the 3D geometry and motion of the observed scene. Many existing approaches use superpixels for regularization, but may predict inconsistent shapes and motions inside rigidly moving objects. We instead assume that scenes consist of foreground objects rigidly moving in front of a static background, and use semantic cues to produce pixel-accurate scene flow estimates. Our cascaded classification framework accurately models 3D scenes by iteratively refining semantic segmentation masks, stereo correspondences, 3D rigid motion estimates, and optical flow fields. We evaluate our method on the challenging KITTI autonomous driving benchmark, and show that accounting for the motion of segmented vehicles leads to state-of-the-art performance.



### Structure-Preserving Image Super-resolution via Contextualized Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.08340v1
- **DOI**: 10.1109/TMM.2017.2711263
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08340v1)
- **Published**: 2017-07-26 09:48:03+00:00
- **Updated**: 2017-07-26 09:48:03+00:00
- **Authors**: Yukai Shi, Keze Wang, Chongyu Chen, Li Xu, Liang Lin
- **Comment**: To appear in Transactions on Multimedia 2017
- **Journal**: None
- **Summary**: Single image super resolution (SR), which refers to reconstruct a higher-resolution (HR) image from the observed low-resolution (LR) image, has received substantial attention due to its tremendous application potentials. Despite the breakthroughs of recently proposed SR methods using convolutional neural networks (CNNs), their generated results usually lack of preserving structural (high-frequency) details. In this paper, regarding global boundary context and residual context as complimentary information for enhancing structural details in image restoration, we develop a contextualized multi-task learning framework to address the SR problem. Specifically, our method first extracts convolutional features from the input LR image and applies one deconvolutional module to interpolate the LR feature maps in a content-adaptive way. Then, the resulting feature maps are fed into two branched sub-networks. During the neural network training, one sub-network outputs salient image boundaries and the HR image, and the other sub-network outputs the local residual map, i.e., the residual difference between the generated HR image and ground-truth image. On several standard benchmarks (i.e., Set5, Set14 and BSD200), our extensive evaluations demonstrate the effectiveness of our SR method on achieving both higher restoration quality and computational efficiency compared with several state-of-the-art SR approaches. The source code and some SR results can be found at: http://hcp.sysu.edu.cn/structure-preserving-image-super-resolution/



### RankIQA: Learning from Rankings for No-reference Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1707.08347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08347v1)
- **Published**: 2017-07-26 10:02:40+00:00
- **Updated**: 2017-07-26 10:02:40+00:00
- **Authors**: Xialei Liu, Joost van de Weijer, Andrew D. Bagdanov
- **Comment**: Accepted by ICCV 2017
- **Journal**: None
- **Summary**: We propose a no-reference image quality assessment (NR-IQA) approach that learns from rankings (RankIQA). To address the problem of limited IQA dataset size, we train a Siamese Network to rank images in terms of image quality by using synthetically generated distortions for which relative image quality is known. These ranked image sets can be automatically generated without laborious human labeling. We then use fine-tuning to transfer the knowledge represented in the trained Siamese Network to a traditional CNN that estimates absolute image quality from single images. We demonstrate how our approach can be made significantly more efficient than traditional Siamese Networks by forward propagating a batch of images through a single network and backpropagating gradients derived from all pairs of images in the batch. Experiments on the TID2013 benchmark show that we improve the state-of-the-art by over 5%. Furthermore, on the LIVE benchmark we show that our approach is superior to existing NR-IQA techniques and that we even outperform the state-of-the-art in full-reference IQA (FR-IQA) methods without having to resort to high-quality reference images to infer IQA.



### Modelling the Scene Dependent Imaging in Cameras with a Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1707.08350v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08350v1)
- **Published**: 2017-07-26 10:04:23+00:00
- **Updated**: 2017-07-26 10:04:23+00:00
- **Authors**: Seonghyeon Nam, Seon Joo Kim
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: We present a novel deep learning framework that models the scene dependent image processing inside cameras. Often called as the radiometric calibration, the process of recovering RAW images from processed images (JPEG format in the sRGB color space) is essential for many computer vision tasks that rely on physically accurate radiance values. All previous works rely on the deterministic imaging model where the color transformation stays the same regardless of the scene and thus they can only be applied for images taken under the manual mode. In this paper, we propose a data-driven approach to learn the scene dependent and locally varying image processing inside cameras under the automode. Our method incorporates both the global and the local scene context into pixel-wise features via multi-scale pyramid of learnable histogram layers. The results show that we can model the imaging pipeline of different cameras that operate under the automode accurately in both directions (from RAW to sRGB, from sRGB to RAW) and we show how we can apply our method to improve the performance of image deblurring.



### Deep Interactive Region Segmentation and Captioning
- **Arxiv ID**: http://arxiv.org/abs/1707.08364v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1707.08364v1)
- **Published**: 2017-07-26 10:40:33+00:00
- **Updated**: 2017-07-26 10:40:33+00:00
- **Authors**: Ali Sharifi Boroujerdi, Maryam Khanian, Michael Breuss
- **Comment**: 17, pages, 9 figures
- **Journal**: None
- **Summary**: With recent innovations in dense image captioning, it is now possible to describe every object of the scene with a caption while objects are determined by bounding boxes. However, interpretation of such an output is not trivial due to the existence of many overlapping bounding boxes. Furthermore, in current captioning frameworks, the user is not able to involve personal preferences to exclude out of interest areas. In this paper, we propose a novel hybrid deep learning architecture for interactive region segmentation and captioning where the user is able to specify an arbitrary region of the image that should be processed. To this end, a dedicated Fully Convolutional Network (FCN) named Lyncean FCN (LFCN) is trained using our special training data to isolate the User Intention Region (UIR) as the output of an efficient segmentation. In parallel, a dense image captioning model is utilized to provide a wide variety of captions for that region. Then, the UIR will be explained with the caption of the best match bounding box. To the best of our knowledge, this is the first work that provides such a comprehensive output. Our experiments show the superiority of the proposed approach over state-of-the-art interactive segmentation methods on several well-known datasets. In addition, replacement of the bounding boxes with the result of the interactive segmentation leads to a better understanding of the dense image captioning output as well as accuracy enhancement for the object detection in terms of Intersection over Union (IoU).



### Product recognition in store shelves as a sub-graph isomorphism problem
- **Arxiv ID**: http://arxiv.org/abs/1707.08378v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08378v2)
- **Published**: 2017-07-26 11:20:58+00:00
- **Updated**: 2017-09-19 13:52:04+00:00
- **Authors**: Alessio Tonioni, Luigi Di Stefano
- **Comment**: Slightly extended version of the paper accepted at ICIAP 2017. More
  information @project_page -->
  http://vision.disi.unibo.it/index.php?option=com_content&view=article&id=111&catid=78
- **Journal**: None
- **Summary**: The arrangement of products in store shelves is carefully planned to maximize sales and keep customers happy. However, verifying compliance of real shelves to the ideal layout is a costly task routinely performed by the store personnel. In this paper, we propose a computer vision pipeline to recognize products on shelves and verify compliance to the planned layout. We deploy local invariant features together with a novel formulation of the product recognition problem as a sub-graph isomorphism between the items appearing in the given image and the ideal layout. This allows for auto-localizing the given image within the aisle or store and improving recognition dramatically.



### A Novel Transfer Learning Approach upon Hindi, Arabic, and Bangla Numerals using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.08385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08385v1)
- **Published**: 2017-07-26 11:40:13+00:00
- **Updated**: 2017-07-26 11:40:13+00:00
- **Authors**: Abdul Kawsar Tushar, Akm Ashiquzzaman, Afia Afrin, Md. Rashedul Islam
- **Comment**: 10 pages; 2 figures, 4 tables; conference - International Conference
  On Computational Vision and Bio Inspired Computing 2017 (http://iccvbic.com/)
  (accepted)
- **Journal**: None
- **Summary**: Increased accuracy in predictive models for handwritten character recognition will open up new frontiers for optical character recognition. Major drawbacks of predictive machine learning models are headed by the elongated training time taken by some models, and the requirement that training and test data be in the same feature space and consist of the same distribution. In this study, these obstacles are minimized by presenting a model for transferring knowledge from one task to another. This model is presented for the recognition of handwritten numerals in Indic languages. The model utilizes convolutional neural networks with backpropagation for error reduction and dropout for data overfitting. The output performance of the proposed neural network is shown to have closely matched other state-of-the-art methods using only a fraction of time used by the state-of-the-arts.



### Reduction of Overfitting in Diabetes Prediction Using Deep Learning Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1707.08386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08386v1)
- **Published**: 2017-07-26 11:44:55+00:00
- **Updated**: 2017-07-26 11:44:55+00:00
- **Authors**: Akm Ashiquzzaman, Abdul Kawsar Tushar, Md. Rashedul Islam, Jong-Myon Kim
- **Comment**: 8 pages, 3 Figures, 3 Tables; Conference - 7th iCatse International
  Conference on IT Convergence and Security, 2017
  (http://icatse.org/icitcs2017/) (accepted)
- **Journal**: None
- **Summary**: Augmented accuracy in prediction of diabetes will open up new frontiers in health prognostics. Data overfitting is a performance-degrading issue in diabetes prognosis. In this study, a prediction system for the disease of diabetes is pre-sented where the issue of overfitting is minimized by using the dropout method. Deep learning neural network is used where both fully connected layers are fol-lowed by dropout layers. The output performance of the proposed neural network is shown to have outperformed other state-of-art methods and it is recorded as by far the best performance for the Pima Indians Diabetes Data Set.



### 3D Sketching using Multi-View Deep Volumetric Prediction
- **Arxiv ID**: http://arxiv.org/abs/1707.08390v4
- **DOI**: 10.1145/3203197
- **Categories**: **cs.GR**, cs.CV, I.3.5
- **Links**: [PDF](http://arxiv.org/pdf/1707.08390v4)
- **Published**: 2017-07-26 11:49:48+00:00
- **Updated**: 2018-06-19 12:24:54+00:00
- **Authors**: Johanna Delanoy, Mathieu Aubry, Phillip Isola, Alexei A. Efros, Adrien Bousseau
- **Comment**: See our accompanying video on https://youtu.be/DGIYzmlm2pQ, networks
  and databases available at https://ns.inria.fr/d3/3DSketching/. To appear in
  PACMCGIT
- **Journal**: None
- **Summary**: Sketch-based modeling strives to bring the ease and immediacy of drawing to the 3D world. However, while drawings are easy for humans to create, they are very challenging for computers to interpret due to their sparsity and ambiguity. We propose a data-driven approach that tackles this challenge by learning to reconstruct 3D shapes from one or more drawings. At the core of our approach is a deep convolutional neural network (CNN) that predicts occupancy of a voxel grid from a line drawing. This CNN provides us with an initial 3D reconstruction as soon as the user completes a single drawing of the desired shape. We complement this single-view network with an updater CNN that refines an existing prediction given a new drawing of the shape created from a novel viewpoint. A key advantage of our approach is that we can apply the updater iteratively to fuse information from an arbitrary number of viewpoints, without requiring explicit stroke correspondences between the drawings. We train both CNNs by rendering synthetic contour drawings from hand-modeled shape collections as well as from procedurally-generated abstract shapes. Finally, we integrate our CNNs in a minimal modeling interface that allows users to seamlessly draw an object, rotate it to see its 3D reconstruction, and refine it by re-drawing from another vantage point using the 3D reconstruction as guidance. The main strengths of our approach are its robustness to freehand bitmap drawings, its ability to adapt to different object categories, and the continuum it offers between single-view and multi-view sketch-based modeling.



### Maximum entropy based non-negative optoacoustic tomographic image reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1707.08391v3
- **DOI**: 10.1109/TBME.2019.2892842
- **Categories**: **physics.med-ph**, cs.CV, eess.IV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1707.08391v3)
- **Published**: 2017-07-26 11:50:06+00:00
- **Updated**: 2019-01-12 01:01:42+00:00
- **Authors**: Jaya Prakash, Subhamoy Mandal, Daniel Razansky, Vasilis Ntziachristos
- **Comment**: This article has been accepted for publication in IEEE Transactions
  on Biomedical Engineering (30 Dec 2018)
- **Journal**: None
- **Summary**: Objective:Optoacoustic (photoacoustic) tomography is aimed at reconstructing maps of the initial pressure rise induced by the absorption of light pulses in tissue. In practice, due to inaccurate assumptions in the forward model, noise and other experimental factors, the images are often afflicted by artifacts, occasionally manifested as negative values. The aim of the work is to develop an inversion method which reduces the occurrence of negative values and improves the quantitative performance of optoacoustic imaging. Methods: We present a novel method for optoacoustic tomography based on an entropy maximization algorithm, which uses logarithmic regularization for attaining non-negative reconstructions. The reconstruction image quality is further improved using structural prior based fluence correction. Results: We report the performance achieved by the entropy maximization scheme on numerical simulation, experimental phantoms and in-vivo samples. Conclusion: The proposed algorithm demonstrates superior reconstruction performance by delivering non-negative pixel values with no visible distortion of anatomical structures. Significance: Our method can enable quantitative optoacoustic imaging, and has the potential to improve pre-clinical and translational imaging applications.



### A Harmony Search Based Wrapper Feature Selection Method for Holistic Bangla word Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.08398v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1707.08398v1)
- **Published**: 2017-07-26 12:03:39+00:00
- **Updated**: 2017-07-26 12:03:39+00:00
- **Authors**: Supratim Das, Pawan Kumar Singh, Showmik Bhowmik, Ram Sarkar, Mita Nasipuri
- **Comment**: None
- **Journal**: None
- **Summary**: A lot of search approaches have been explored for the selection of features in pattern classification domain in order to discover significant subset of the features which produces better accuracy. In this paper, we introduced a Harmony Search (HS) algorithm based feature selection method for feature dimensionality reduction in handwritten Bangla word recognition problem. This algorithm has been implemented to reduce the feature dimensionality of a technique described in one of our previous papers by S. Bhowmik et al.[1]. In the said paper, a set of 65 elliptical features were computed for handwritten Bangla word recognition purpose and a recognition accuracy of 81.37% was achieved using Multi Layer Perceptron (MLP) classifier. In the present work, a subset containing 48 features (approximately 75% of said feature vector) has been selected by HS based wrapper feature selection method which produces an accuracy rate of 90.29%. Reasonable outcomes also validates that the introduced algorithm utilizes optimal number of features while showing higher classification accuracies when compared to two standard evolutionary algorithms like Genetic Algorithm (GA), Particle Swarm Optimization (PSO) and statistical feature dimensionality reduction technique like Principal Component Analysis (PCA). This confirms the suitability of HS algorithm to the holistic handwritten word recognition problem.



### Detecting and classifying lesions in mammograms with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.08401v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08401v3)
- **Published**: 2017-07-26 12:07:45+00:00
- **Updated**: 2017-11-09 14:31:13+00:00
- **Authors**: Dezső Ribli, Anna Horváth, Zsuzsa Unger, Péter Pollner, István Csabai
- **Comment**: None
- **Journal**: None
- **Summary**: In the last two decades Computer Aided Diagnostics (CAD) systems were developed to help radiologists analyze screening mammograms. The benefits of current CAD technologies appear to be contradictory and they should be improved to be ultimately considered useful. Since 2012 deep convolutional neural networks (CNN) have been a tremendous success in image recognition, reaching human performance. These methods have greatly surpassed the traditional approaches, which are similar to currently used CAD solutions. Deep CNN-s have the potential to revolutionize medical image analysis. We propose a CAD system based on one of the most successful object detection frameworks, Faster R-CNN. The system detects and classifies malignant or benign lesions on a mammogram without any human intervention. The proposed method sets the state of the art classification performance on the public INbreast database, AUC = 0.95 . The approach described here has achieved the 2nd place in the Digital Mammography DREAM Challenge with AUC = 0.85 . When used as a detector, the system reaches high sensitivity with very few false positive marks per image on the INbreast dataset. Source code, the trained model and an OsiriX plugin are availaible online at https://github.com/riblidezso/frcnn_cad .



### A Guided Spatial Transformer Network for Histology Cell Differentiation
- **Arxiv ID**: http://arxiv.org/abs/1707.08525v1
- **DOI**: 10.2312/vcbm.20171233
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08525v1)
- **Published**: 2017-07-26 16:21:39+00:00
- **Updated**: 2017-07-26 16:21:39+00:00
- **Authors**: Marc Aubreville, Maximilian Krappmann, Christof Bertram, Robert Klopfleisch, Andreas Maier
- **Comment**: 5 pages, 4 figures, EG VCBM 2017 contribution
- **Journal**: EG VCBM 2017 Proceedings
- **Summary**: Identification and counting of cells and mitotic figures is a standard task in diagnostic histopathology. Due to the large overall cell count on histological slides and the potential sparse prevalence of some relevant cell types or mitotic figures, retrieving annotation data for sufficient statistics is a tedious task and prone to a significant error in assessment. Automatic classification and segmentation is a classic task in digital pathology, yet it is not solved to a sufficient degree.   We present a novel approach for cell and mitotic figure classification, based on a deep convolutional network with an incorporated Spatial Transformer Network. The network was trained on a novel data set with ten thousand mitotic figures, about ten times more than previous data sets. The algorithm is able to derive the cell class (mitotic tumor cells, non-mitotic tumor cells and granulocytes) and their position within an image. The mean accuracy of the algorithm in a five-fold cross-validation is 91.45%.   In our view, the approach is a promising step into the direction of a more objective and accurate, semi-automatized mitosis counting supporting the pathologist.



### Interpatient Respiratory Motion Model Transfer for Virtual Reality Simulations of Liver Punctures
- **Arxiv ID**: http://arxiv.org/abs/1707.08554v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08554v2)
- **Published**: 2017-07-26 17:34:09+00:00
- **Updated**: 2017-08-02 10:46:40+00:00
- **Authors**: Andre Mastmeyer, Matthias Wilms, Heinz Handels
- **Comment**: World Society for Computer Graphics - WSCG 2017 publication, 9 pages,
  5 figures, 1 movie online
- **Journal**: Journal of WSCG, Vol.25, No.1, ISSN 1213-6972, 2017
- **Summary**: Current virtual reality (VR) training simulators of liver punctures often rely on static 3D patient data and use an unrealistic (sinusoidal) periodic animation of the respiratory movement. Existing methods for the animation of breathing motion support simple mathematical or patient-specific, estimated breathing models. However with personalized breathing models for each new patient, a heavily dose relevant or expensive 4D data acquisition is mandatory for keyframe-based motion modeling. Given the reference 4D data, first a model building stage using linear regression motion field modeling takes place. Then the methodology shown here allows the transfer of existing reference respiratory motion models of a 4D reference patient to a new static 3D patient. This goal is achieved by using non-linear inter-patient registration to warp one personalized 4D motion field model to new 3D patient data. This cost- and dose-saving new method is shown here visually in a qualitative proof-of-concept study.



### Video Highlight Prediction Using Audience Chat Reactions
- **Arxiv ID**: http://arxiv.org/abs/1707.08559v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1707.08559v1)
- **Published**: 2017-07-26 17:44:38+00:00
- **Updated**: 2017-07-26 17:44:38+00:00
- **Authors**: Cheng-Yang Fu, Joon Lee, Mohit Bansal, Alexander C. Berg
- **Comment**: EMNLP 2017
- **Journal**: None
- **Summary**: Sports channel video portals offer an exciting domain for research on multimodal, multilingual analysis. We present methods addressing the problem of automatic video highlight prediction based on joint visual features and textual analysis of the real-world audience discourse with complex slang, in both English and traditional Chinese. We present a novel dataset based on League of Legends championships recorded from North American and Taiwanese Twitch.tv channels (will be released for further research), and demonstrate strong results on these using multimodal, character-level CNN-RNN model architectures.



### Robust Rigid Point Registration based on Convolution of Adaptive Gaussian Mixture Models
- **Arxiv ID**: http://arxiv.org/abs/1707.08626v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1707.08626v1)
- **Published**: 2017-07-26 20:06:38+00:00
- **Updated**: 2017-07-26 20:06:38+00:00
- **Authors**: Can Pu, Nanbo Li, Robert B Fisher
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Matching 3D rigid point clouds in complex environments robustly and accurately is still a core technique used in many applications. This paper proposes a new architecture combining error estimation from sample covariances and dual global probability alignment based on the convolution of adaptive Gaussian Mixture Models (GMM) from point clouds. Firstly, a novel adaptive GMM is defined using probability distributions from the corresponding points. Then rigid point cloud alignment is performed by maximizing the global probability from the convolution of dual adaptive GMMs in the whole 2D or 3D space, which can be efficiently optimized and has a large zone of accurate convergence. Thousands of trials have been conducted on 200 models from public 2D and 3D datasets to demonstrate superior robustness and accuracy in complex environments with unpredictable noise, outliers, occlusion, initial rotation, shape and missing points.



### Optimizing Filter Size in Convolutional Neural Networks for Facial Action Unit Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.08630v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08630v2)
- **Published**: 2017-07-26 20:15:13+00:00
- **Updated**: 2017-11-22 19:33:09+00:00
- **Authors**: Shizhong Han, Zibo Meng, Zhiyuan Li, James O'Reilly, Jie Cai, Xiaofeng Wang, Yan Tong
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing facial action units (AUs) during spontaneous facial displays is a challenging problem. Most recently, Convolutional Neural Networks (CNNs) have shown promise for facial AU recognition, where predefined and fixed convolution filter sizes are employed. In order to achieve the best performance, the optimal filter size is often empirically found by conducting extensive experimental validation. Such a training process suffers from expensive training cost, especially as the network becomes deeper.   This paper proposes a novel Optimized Filter Size CNN (OFS-CNN), where the filter sizes and weights of all convolutional layers are learned simultaneously from the training data along with learning convolution filters. Specifically, the filter size is defined as a continuous variable, which is optimized by minimizing the training loss. Experimental results on two AU-coded spontaneous databases have shown that the proposed OFS-CNN is capable of estimating optimal filter size for varying image resolution and outperforms traditional CNNs with the best filter size obtained by exhaustive search. The OFS-CNN also beats the CNN using multiple filter sizes and more importantly, is much more efficient during testing with the proposed forward-backward propagation algorithm.



### Learning a Target Sample Re-Generator for Cross-Database Micro-Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.08645v1
- **DOI**: 10.1109/TIP.2018.2797479
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08645v1)
- **Published**: 2017-07-26 21:13:45+00:00
- **Updated**: 2017-07-26 21:13:45+00:00
- **Authors**: Yuan Zong, Xiaohua Huang, Wenming Zheng, Zhen Cui, Guoying Zhao
- **Comment**: To appear at ACM Multimedia 2017
- **Journal**: None
- **Summary**: In this paper, we investigate the cross-database micro-expression recognition problem, where the training and testing samples are from two different micro-expression databases. Under this setting, the training and testing samples would have different feature distributions and hence the performance of most existing micro-expression recognition methods may decrease greatly. To solve this problem, we propose a simple yet effective method called Target Sample Re-Generator (TSRG) in this paper. By using TSRG, we are able to re-generate the samples from target micro-expression database and the re-generated target samples would share same or similar feature distributions with the original source samples. For this reason, we can then use the classifier learned based on the labeled source samples to accurately predict the micro-expression categories of the unlabeled target samples. To evaluate the performance of the proposed TSRG method, extensive cross-database micro-expression recognition experiments designed based on SMIC and CASME II databases are conducted. Compared with recent state-of-the-art cross-database emotion recognition methods, the proposed TSRG achieves more promising results.



