# Arxiv Papers in cs.CV on 2017-07-13
### The Surfacing of Multiview 3D Drawings via Lofting and Occlusion Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1707.03946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03946v1)
- **Published**: 2017-07-13 01:11:15+00:00
- **Updated**: 2017-07-13 01:11:15+00:00
- **Authors**: Anil Usumezbas, Ricardo Fabbri, Benjamin Kimia
- **Comment**: CVPR 2017 expanded version with improvements over camera ready,
  Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
  CVPR, 2017
- **Journal**: None
- **Summary**: The three-dimensional reconstruction of scenes from multiple views has made impressive strides in recent years, chiefly by methods correlating isolated feature points, intensities, or curvilinear structure. In the general setting, i.e., without requiring controlled acquisition, limited number of objects, abundant patterns on objects, or object curves to follow particular models, the majority of these methods produce unorganized point clouds, meshes, or voxel representations of the reconstructed scene, with some exceptions producing 3D drawings as networks of curves. Many applications, e.g., robotics, urban planning, industrial design, and hard surface modeling, however, require structured representations which make explicit 3D curves, surfaces, and their spatial relationships. Reconstructing surface representations can now be constrained by the 3D drawing acting like a scaffold to hang on the computed representations, leading to increased robustness and quality of reconstruction. This paper presents one way of completing such 3D drawings with surface reconstructions, by exploring occlusion reasoning through lofting algorithms.



### Learning Photography Aesthetics with Deep CNNs
- **Arxiv ID**: http://arxiv.org/abs/1707.03981v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1707.03981v1)
- **Published**: 2017-07-13 05:16:03+00:00
- **Updated**: 2017-07-13 05:16:03+00:00
- **Authors**: Gautam Malu, Raju S. Bapi, Bipin Indurkhya
- **Comment**: Accepted in The 28th Modern Artificial Intelligence and Cognitive
  Science Conference
- **Journal**: None
- **Summary**: Automatic photo aesthetic assessment is a challenging artificial intelligence task. Existing computational approaches have focused on modeling a single aesthetic score or a class (good or bad), however these do not provide any details on why the photograph is good or bad, or which attributes contribute to the quality of the photograph. To obtain both accuracy and human interpretation of the score, we advocate learning the aesthetic attributes along with the prediction of the overall score. For this purpose, we propose a novel multitask deep convolution neural network, which jointly learns eight aesthetic attributes along with the overall aesthetic score. We report near human performance in the prediction of the overall aesthetic score. To understand the internal representation of these attributes in the learned model, we also develop the visualization technique using back propagation of gradients. These visualizations highlight the important image regions for the corresponding attributes, thus providing insights about model's representation of these attributes. We showcase the diversity and complexity associated with different attributes through a qualitative analysis of the activation maps.



### Towards End-to-end Text Spotting with Convolutional Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.03985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03985v1)
- **Published**: 2017-07-13 06:02:00+00:00
- **Updated**: 2017-07-13 06:02:00+00:00
- **Authors**: Hui Li, Peng Wang, Chunhua Shen
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: In this work, we jointly address the problem of text detection and recognition in natural scene images based on convolutional recurrent neural networks. We propose a unified network that simultaneously localizes and recognizes text with a single forward pass, avoiding intermediate processes like image cropping and feature re-calculation, word separation, or character grouping. In contrast to existing approaches that consider text detection and recognition as two distinct tasks and tackle them one by one, the proposed framework settles these two tasks concurrently. The whole framework can be trained end-to-end, requiring only images, the ground-truth bounding boxes and text labels. Through end-to-end training, the learned features can be more informative, which improves the overall performance. The convolutional features are calculated only once and shared by both detection and recognition, which saves processing time. Our proposed method has achieved competitive performance on several benchmark datasets.



### Merge or Not? Learning to Group Faces via Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.03986v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.03986v1)
- **Published**: 2017-07-13 06:16:43+00:00
- **Updated**: 2017-07-13 06:16:43+00:00
- **Authors**: Yue He, Kaidi Cao, Cheng Li, Chen Change Loy
- **Comment**: None
- **Journal**: None
- **Summary**: Given a large number of unlabeled face images, face grouping aims at clustering the images into individual identities present in the data. This task remains a challenging problem despite the remarkable capability of deep learning approaches in learning face representation. In particular, grouping results can still be egregious given profile faces and a large number of uninteresting faces and noisy detections. Often, a user needs to correct the erroneous grouping manually. In this study, we formulate a novel face grouping framework that learns clustering strategy from ground-truth simulated behavior. This is achieved through imitation learning (a.k.a apprenticeship learning or learning by watching) via inverse reinforcement learning (IRL). In contrast to existing clustering approaches that group instances by similarity, our framework makes sequential decision to dynamically decide when to merge two face instances/groups driven by short- and long-term rewards. Extensive experiments on three benchmark datasets show that our framework outperforms unsupervised and supervised baselines.



### Developing the Path Signature Methodology and its Application to Landmark-based Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.03993v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03993v2)
- **Published**: 2017-07-13 07:02:37+00:00
- **Updated**: 2019-12-12 14:52:36+00:00
- **Authors**: Weixin Yang, Terry Lyons, Hao Ni, Cordelia Schmid, Lianwen Jin
- **Comment**: 14 pages, 11 figures
- **Journal**: None
- **Summary**: Landmark-based human action recognition in videos is a challenging task in computer vision. One key step is to design a generic approach that generates discriminative features for the spatial structure and temporal dynamics. To this end, we regard the evolving landmark data as a high-dimensional path and apply non-linear path signature techniques to provide an expressive, robust, non-linear, and interpretable representation for the sequential events. We do not extract signature features from the raw path, rather we propose path disintegrations and path transformations as preprocessing steps. Path disintegrations turn a high-dimensional path linearly into a collection of lower-dimensional paths; some of these paths are in pose space while others are defined over a multiscale collection of temporal intervals. Path transformations decorate the paths with additional coordinates in standard ways to allow the truncated signatures of transformed paths to expose additional features. For spatial representation, we apply the signature transform to vectorize the paths that arise out of pose disintegration, and for temporal representation, we apply it again to describe this evolving vectorization. Finally, all the features are collected together to constitute the input vector of a linear single-hidden-layer fully-connected network for classification. Experimental results on four datasets demonstrated that the proposed feature set with only a linear shallow network and Dropconnect is effective and achieves comparable state-of-the-art results to the advanced deep networks, and meanwhile, is capable of interpretation.



### Query-Aware Sparse Coding for Multi-Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1707.04021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04021v1)
- **Published**: 2017-07-13 08:18:21+00:00
- **Updated**: 2017-07-13 08:18:21+00:00
- **Authors**: Zhong Ji, Yaru Ma, Yanwei Pang, Xuelong Li
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Given the explosive growth of online videos, it is becoming increasingly important to relieve the tedious work of browsing and managing the video content of interest. Video summarization aims at providing such a technique by transforming one or multiple videos into a compact one. However, conventional multi-video summarization methods often fail to produce satisfying results as they ignore the user's search intent. To this end, this paper proposes a novel query-aware approach by formulating the multi-video summarization in a sparse coding framework, where the web images searched by the query are taken as the important preference information to reveal the query intent. To provide a user-friendly summarization, this paper also develops an event-keyframe presentation structure to present keyframes in groups of specific events related to the query by using an unsupervised multi-graph fusion method. We release a new public dataset named MVS1K, which contains about 1, 000 videos from 10 queries and their video tags, manual annotations, and associated web images. Extensive experiments on MVS1K dataset validate our approaches produce superior objective and subjective results against several recently proposed approaches.



### On Measuring and Quantifying Performance: Error Rates, Surrogate Loss, and an Example in SSL
- **Arxiv ID**: http://arxiv.org/abs/1707.04025v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1707.04025v1)
- **Published**: 2017-07-13 08:29:00+00:00
- **Updated**: 2017-07-13 08:29:00+00:00
- **Authors**: Marco Loog, Jesse H. Krijthe, Are C. Jensen
- **Comment**: None
- **Journal**: In Handbook of Pattern Recognition and Computer Vision (pp. 53-68)
  (2016)
- **Summary**: In various approaches to learning, notably in domain adaptation, active learning, learning under covariate shift, semi-supervised learning, learning with concept drift, and the like, one often wants to compare a baseline classifier to one or more advanced (or at least different) strategies. In this chapter, we basically argue that if such classifiers, in their respective training phases, optimize a so-called surrogate loss that it may also be valuable to compare the behavior of this loss on the test set, next to the regular classification error rate. It can provide us with an additional view on the classifiers' relative performances that error rates cannot capture. As an example, limited but convincing empirical results demonstrates that we may be able to find semi-supervised learning strategies that can guarantee performance improvements with increasing numbers of unlabeled data in terms of log-likelihood. In contrast, the latter may be impossible to guarantee for the classification error rate.



### Deep Learning with Topological Signatures
- **Arxiv ID**: http://arxiv.org/abs/1707.04041v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.AT
- **Links**: [PDF](http://arxiv.org/pdf/1707.04041v3)
- **Published**: 2017-07-13 09:36:05+00:00
- **Updated**: 2018-02-16 07:12:19+00:00
- **Authors**: Christoph Hofer, Roland Kwitt, Marc Niethammer, Andreas Uhl
- **Comment**: None
- **Journal**: None
- **Summary**: Inferring topological and geometrical information from data can offer an alternative perspective on machine learning problems. Methods from topological data analysis, e.g., persistent homology, enable us to obtain such information, typically in the form of summary representations of topological features. However, such topological signatures often come with an unusual structure (e.g., multisets of intervals) that is highly impractical for most machine learning techniques. While many strategies have been proposed to map these topological signatures into machine learning compatible representations, they suffer from being agnostic to the target learning task. In contrast, we propose a technique that enables us to input topological signatures to deep neural networks and learn a task-optimal representation during training. Our approach is realized as a novel input layer with favorable theoretical properties. Classification experiments on 2D object shapes and social network graphs demonstrate the versatility of the approach and, in case of the latter, we even outperform the state-of-the-art by a large margin.



### Large-scale Video Classification guided by Batch Normalized LSTM Translator
- **Arxiv ID**: http://arxiv.org/abs/1707.04045v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04045v1)
- **Published**: 2017-07-13 09:48:03+00:00
- **Updated**: 2017-07-13 09:48:03+00:00
- **Authors**: Jae Hyeon Yoo
- **Comment**: This is accepted by CVPR2017 Workshop on Youtube-8M Large-scale Video
  Understanding
- **Journal**: None
- **Summary**: Youtube-8M dataset enhances the development of large-scale video recognition technology as ImageNet dataset has encouraged image classification, recognition and detection of artificial intelligence fields. For this large video dataset, it is a challenging task to classify a huge amount of multi-labels. By change of perspective, we propose a novel method by regarding labels as words. In details, we describe online learning approaches to multi-label video classification that are guided by deep recurrent neural networks for video to sentence translator. We designed the translator based on LSTMs and found out that a stochastic gating before the input of each LSTM cell can help us to design the structural details. In addition, we adopted batch normalizations into our models to improve our LSTM models. Since our models are feature extractors, they can be used with other classifiers. Finally we report improved validation results of our models on large-scale Youtube-8M datasets and discussions for the further improvement.



### Stable Distribution Alignment Using the Dual of the Adversarial Distance
- **Arxiv ID**: http://arxiv.org/abs/1707.04046v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.04046v4)
- **Published**: 2017-07-13 09:50:14+00:00
- **Updated**: 2018-01-30 20:49:21+00:00
- **Authors**: Ben Usman, Kate Saenko, Brian Kulis
- **Comment**: ICLR 2018 Conference Invite to Workshop
- **Journal**: None
- **Summary**: Methods that align distributions by minimizing an adversarial distance between them have recently achieved impressive results. However, these approaches are difficult to optimize with gradient descent and they often do not converge well without careful hyperparameter tuning and proper initialization. We investigate whether turning the adversarial min-max problem into an optimization problem by replacing the maximization part with its dual improves the quality of the resulting alignment and explore its connections to Maximum Mean Discrepancy. Our empirical results suggest that using the dual formulation for the restricted family of linear discriminators results in a more stable convergence to a desirable solution when compared with the performance of a primal min-max GAN-like objective and an MMD objective under the same restrictions. We test our hypothesis on the problem of aligning two synthetic point clouds on a plane and on a real-image domain adaptation problem on digits. In both cases, the dual formulation yields an iterative procedure that gives more stable and monotonic improvement over time.



### Discrete Multi-modal Hashing with Canonical Views for Robust Mobile Landmark Search
- **Arxiv ID**: http://arxiv.org/abs/1707.04047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04047v1)
- **Published**: 2017-07-13 09:54:26+00:00
- **Updated**: 2017-07-13 09:54:26+00:00
- **Authors**: Lei Zhu, Zi Huang, Xiaobai Liu, Xiangnan He, Jingkuan Song, Xiaofang Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Mobile landmark search (MLS) recently receives increasing attention for its great practical values. However, it still remains unsolved due to two important challenges. One is high bandwidth consumption of query transmission, and the other is the huge visual variations of query images sent from mobile devices. In this paper, we propose a novel hashing scheme, named as canonical view based discrete multi-modal hashing (CV-DMH), to handle these problems via a novel three-stage learning procedure. First, a submodular function is designed to measure visual representativeness and redundancy of a view set. With it, canonical views, which capture key visual appearances of landmark with limited redundancy, are efficiently discovered with an iterative mining strategy. Second, multi-modal sparse coding is applied to transform visual features from multiple modalities into an intermediate representation. It can robustly and adaptively characterize visual contents of varied landmark images with certain canonical views. Finally, compact binary codes are learned on intermediate representation within a tailored discrete binary embedding model which preserves visual relations of images measured with canonical views and removes the involved noises. In this part, we develop a new augmented Lagrangian multiplier (ALM) based optimization method to directly solve the discrete binary codes. We can not only explicitly deal with the discrete constraint, but also consider the bit-uncorrelated constraint and balance constraint together. Experiments on real world landmark datasets demonstrate the superior performance of CV-DMH over several state-of-the-art methods.



### Automatic Recognition of Facial Displays of Unfelt Emotions
- **Arxiv ID**: http://arxiv.org/abs/1707.04061v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04061v2)
- **Published**: 2017-07-13 10:57:31+00:00
- **Updated**: 2018-01-09 11:44:01+00:00
- **Authors**: Kaustubh Kulkarni, Ciprian Adrian Corneanu, Ikechukwu Ofodile, Sergio Escalera, Xavier Baro, Sylwia Hyniewska, Juri Allik, Gholamreza Anbarjafari
- **Comment**: None
- **Journal**: None
- **Summary**: Humans modify their facial expressions in order to communicate their internal states and sometimes to mislead observers regarding their true emotional states. Evidence in experimental psychology shows that discriminative facial responses are short and subtle. This suggests that such behavior would be easier to distinguish when captured in high resolution at an increased frame rate. We are proposing SASE-FE, the first dataset of facial expressions that are either congruent or incongruent with underlying emotion states. We show that overall the problem of recognizing whether facial movements are expressions of authentic emotions or not can be successfully addressed by learning spatio-temporal representations of the data. For this purpose, we propose a method that aggregates features along fiducial trajectories in a deeply learnt space. Performance of the proposed model shows that on average it is easier to distinguish among genuine facial expressions of emotion than among unfelt facial expressions of emotion and that certain emotion pairs such as contempt and disgust are more difficult to distinguish than the rest. Furthermore, the proposed methodology improves state of the art results on CK+ and OULU-CASIA datasets for video emotion recognition, and achieves competitive results when classifying facial action units on BP4D datase.



### Deep Domain Adaptation by Geodesic Distance Minimization
- **Arxiv ID**: http://arxiv.org/abs/1707.09842v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09842v2)
- **Published**: 2017-07-13 11:34:11+00:00
- **Updated**: 2017-10-10 20:26:55+00:00
- **Authors**: Yifei Wang, Wen Li, Dengxin Dai, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new approach called Deep LogCORAL for unsupervised visual domain adaptation. Our work builds on the recently proposed Deep CORAL method, which proposed to train a convolutional neural network and simultaneously minimize the Euclidean distance of convariance matrices between the source and target domains. We propose to use the Riemannian distance, approximated by Log-Euclidean distance, to replace the naive Euclidean distance in Deep CORAL. We also consider first-order information, and minimize the distance of mean vectors between two domains. We build an end-to-end model, in which we minimize both the classification loss, and the domain difference based on the first and second order information between two domains. Our experiments on the benchmark Office dataset demonstrate the improvements of our newly proposed Deep LogCORAL approach over the Deep CORAL method, as well as further improvement when optimizing both orders of information.



### Disentangling Motion, Foreground and Background Features in Videos
- **Arxiv ID**: http://arxiv.org/abs/1707.04092v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1707.04092v2)
- **Published**: 2017-07-13 12:40:28+00:00
- **Updated**: 2017-07-17 13:50:01+00:00
- **Authors**: Xunyu Lin, Victor Campos, Xavier Giro-i-Nieto, Jordi Torres, Cristian Canton Ferrer
- **Comment**: Poster presented at the CVPR 2017 Workshop Brave New Ideas for Motion
  Representations in Videos
- **Journal**: None
- **Summary**: This paper introduces an unsupervised framework to extract semantically rich features for video representation. Inspired by how the human visual system groups objects based on motion cues, we propose a deep convolutional neural network that disentangles motion, foreground and background information. The proposed architecture consists of a 3D convolutional feature encoder for blocks of 16 frames, which is trained for reconstruction tasks over the first and last frames of the sequence. A preliminary supervised experiment was conducted to verify the feasibility of proposed method by training the model with a fraction of videos from the UCF-101 dataset taking as ground truth the bounding boxes around the activity regions. Qualitative results indicate that the network can successfully segment foreground and background in videos as well as update the foreground appearance based on disentangled motion features. The benefits of these learned features are shown in a discriminative classification task, where initializing the network with the proposed pretraining method outperforms both random initialization and autoencoder pretraining. Our model and source code are publicly available at https://imatge-upc.github.io/unsupervised-2017-cvprw/ .



### Foolbox: A Python toolbox to benchmark the robustness of machine learning models
- **Arxiv ID**: http://arxiv.org/abs/1707.04131v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1707.04131v3)
- **Published**: 2017-07-13 13:59:15+00:00
- **Updated**: 2018-03-20 10:10:10+00:00
- **Authors**: Jonas Rauber, Wieland Brendel, Matthias Bethge
- **Comment**: Code and examples available at https://github.com/bethgelab/foolbox
  and documentation available at http://foolbox.readthedocs.io
- **Journal**: None
- **Summary**: Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at https://github.com/bethgelab/foolbox . The most up-to-date documentation can be found at http://foolbox.readthedocs.io .



### UTS submission to Google YouTube-8M Challenge 2017
- **Arxiv ID**: http://arxiv.org/abs/1707.04143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04143v1)
- **Published**: 2017-07-13 14:24:39+00:00
- **Updated**: 2017-07-13 14:24:39+00:00
- **Authors**: Linchao Zhu, Yanbin Liu, Yi Yang
- **Comment**: CVPR'17 Workshop on YouTube-8M
- **Journal**: None
- **Summary**: In this paper, we present our solution to Google YouTube-8M Video Classification Challenge 2017. We leveraged both video-level and frame-level features in the submission. For video-level classification, we simply used a 200-mixture Mixture of Experts (MoE) layer, which achieves GAP 0.802 on the validation set with a single model. For frame-level classification, we utilized several variants of recurrent neural networks, sequence aggregation with attention mechanism and 1D convolutional models. We achieved GAP 0.8408 on the private testing set with the ensemble model.   The source code of our models can be found in \url{https://github.com/ffmpbgrnn/yt8m}.



### Be Careful What You Backpropagate: A Case For Linear Output Activations & Gradient Boosting
- **Arxiv ID**: http://arxiv.org/abs/1707.04199v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.04199v1)
- **Published**: 2017-07-13 16:19:09+00:00
- **Updated**: 2017-07-13 16:19:09+00:00
- **Authors**: Anders Oland, Aayush Bansal, Roger B. Dannenberg, Bhiksha Raj
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we show that saturating output activation functions, such as the softmax, impede learning on a number of standard classification tasks. Moreover, we present results showing that the utility of softmax does not stem from the normalization, as some have speculated. In fact, the normalization makes things worse. Rather, the advantage is in the exponentiation of error gradients. This exponential gradient boosting is shown to speed up convergence and improve generalization. To this end, we demonstrate faster convergence and better performance on diverse classification tasks: image classification using CIFAR-10 and ImageNet, and semantic segmentation using PASCAL VOC 2012. In the latter case, using the state-of-the-art neural network architecture, the model converged 33% faster with our method (roughly two days of training less) than with the standard softmax activation, and with a slightly better performance to boot.



### Cultivating DNN Diversity for Large Scale Video Labelling
- **Arxiv ID**: http://arxiv.org/abs/1707.04272v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04272v1)
- **Published**: 2017-07-13 18:11:02+00:00
- **Updated**: 2017-07-13 18:11:02+00:00
- **Authors**: Mikel Bober-Irizar, Sameed Husain, Eng-Jon Ong, Miroslaw Bober
- **Comment**: CVPR 2017 Youtube-8M Workshop
- **Journal**: None
- **Summary**: We investigate factors controlling DNN diversity in the context of the Google Cloud and YouTube-8M Video Understanding Challenge. While it is well-known that ensemble methods improve prediction performance, and that combining accurate but diverse predictors helps, there is little knowledge on how to best promote & measure DNN diversity. We show that diversity can be cultivated by some unexpected means, such as model over-fitting or dropout variations. We also present details of our solution to the video understanding problem, which ranked #7 in the Kaggle competition (competing as the Yeti team).



### Superposition de calques monochromes d'opacités variables
- **Arxiv ID**: http://arxiv.org/abs/1707.09839v4
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.09839v4)
- **Published**: 2017-07-13 18:16:06+00:00
- **Updated**: 2023-05-30 19:33:36+00:00
- **Authors**: Alexandre Bali
- **Comment**: poor research and unrigorous
- **Journal**: None
- **Summary**: For a monochrome layer $x$ of opacity $0\le o_x\le1 $ placed on another monochrome layer of opacity 1, the result given by the standard formula is $$\small\Pi\left({\bf C}_\varphi\right)=1+\sum_{n=1}^2\left(2-n-(-1)^no_{\chi(\varphi+1)}\right)\left(\chi(n+\varphi-1)-o_{\chi(n+\varphi-1)}\right),$$ the formula being of course explained in detail in this paper. We will eventually deduce a very simple theorem, generalize it and then see its validity with alternative formulas to this standard containing the same main properties here exposed.



### Discriminative Optimization: Theory and Applications to Computer Vision Problems
- **Arxiv ID**: http://arxiv.org/abs/1707.04318v1
- **DOI**: 10.1109/TPAMI.2018.2826536
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04318v1)
- **Published**: 2017-07-13 20:58:32+00:00
- **Updated**: 2017-07-13 20:58:32+00:00
- **Authors**: Jayakorn Vongkulbhisal, Fernando De la Torre, João P. Costeira
- **Comment**: 26 pages, 28 figures
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence (
  Volume: 41, Issue: 4, Apr 2019 )
- **Summary**: Many computer vision problems are formulated as the optimization of a cost function. This approach faces two main challenges: (i) designing a cost function with a local optimum at an acceptable solution, and (ii) developing an efficient numerical method to search for one (or multiple) of these local optima. While designing such functions is feasible in the noiseless case, the stability and location of local optima are mostly unknown under noise, occlusion, or missing data. In practice, this can result in undesirable local optima or not having a local optimum in the expected place. On the other hand, numerical optimization algorithms in high-dimensional spaces are typically local and often rely on expensive first or second order information to guide the search. To overcome these limitations, this paper proposes Discriminative Optimization (DO), a method that learns search directions from data without the need of a cost function. Specifically, DO explicitly learns a sequence of updates in the search space that leads to stationary points that correspond to desired solutions. We provide a formal analysis of DO and illustrate its benefits in the problem of 3D point cloud registration, camera pose estimation, and image denoising. We show that DO performed comparably or outperformed state-of-the-art algorithms in terms of accuracy, robustness to perturbations, and computational efficiency.



