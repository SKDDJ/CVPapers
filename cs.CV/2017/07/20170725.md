# Arxiv Papers in cs.CV on 2017-07-25
### Efficient Deformable Shape Correspondence via Kernel Matching
- **Arxiv ID**: http://arxiv.org/abs/1707.08991v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08991v3)
- **Published**: 2017-07-25 01:41:44+00:00
- **Updated**: 2017-09-15 12:33:33+00:00
- **Authors**: Zorah Lähner, Matthias Vestner, Amit Boyarski, Or Litany, Ron Slossberg, Tal Remez, Emanuele Rodolà, Alex Bronstein, Michael Bronstein, Ron Kimmel, Daniel Cremers
- **Comment**: Accepted for oral presentation at 3DV 2017, including supplementary
  material
- **Journal**: None
- **Summary**: We present a method to match three dimensional shapes under non-isometric deformations, topology changes and partiality. We formulate the problem as matching between a set of pair-wise and point-wise descriptors, imposing a continuity prior on the mapping, and propose a projected descent optimization procedure inspired by difference of convex functions (DC) programming. Surprisingly, in spite of the highly non-convex nature of the resulting quadratic assignment problem, our method converges to a semantically meaningful and continuous mapping in most of our experiments, and scales well. We provide preliminary theoretical analysis and several interpretations of the method.



### Deep Feature Learning via Structured Graph Laplacian Embedding for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1707.07791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07791v1)
- **Published**: 2017-07-25 02:28:22+00:00
- **Updated**: 2017-07-25 02:28:22+00:00
- **Authors**: De Cheng, Yihong Gong, Zhihui Li, Weiwei Shi, Alexander G. Hauptmann, Nanning Zheng
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: Learning the distance metric between pairs of examples is of great importance for visual recognition, especially for person re-identification (Re-Id). Recently, the contrastive and triplet loss are proposed to enhance the discriminative power of the deeply learned features, and have achieved remarkable success. As can be seen, either the contrastive or triplet loss is just one special case of the Euclidean distance relationships among these training samples. Therefore, we propose a structured graph Laplacian embedding algorithm, which can formulate all these structured distance relationships into the graph Laplacian form. The proposed method can take full advantages of the structured distance relationships among these training samples, with the constructed complete graph. Besides, this formulation makes our method easy-to-implement and super-effective. When embedding the proposed algorithm with the softmax loss for the CNN training, our method can obtain much more robust and discriminative deep features with inter-personal dispersion and intra-personal compactness, which is essential to person Re-Id. We illustrate the effectiveness of our proposed method on top of three popular networks, namely AlexNet, DGDNet and ResNet50, on recent four widely used Re-Id benchmark datasets. Our proposed method achieves state-of-the-art performances.



### SAR Target Recognition Using the Multi-aspect-aware Bidirectional LSTM Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.09875v1
- **DOI**: 10.1109/ACCESS.2017.2773363
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.09875v1)
- **Published**: 2017-07-25 04:01:25+00:00
- **Updated**: 2017-07-25 04:01:25+00:00
- **Authors**: Fan Zhang, Chen Hu, Qiang Yin, Wei Li, Hengchao Li, Wen Hong
- **Comment**: 11 pages, 10 figures
- **Journal**: IEEE Access, vol.5, 2017
- **Summary**: The outstanding pattern recognition performance of deep learning brings new vitality to the synthetic aperture radar (SAR) automatic target recognition (ATR). However, there is a limitation in current deep learning based ATR solution that each learning process only handle one SAR image, namely learning the static scattering information, while missing the space-varying information. It is obvious that multi-aspect joint recognition introduced space-varying scattering information should improve the classification accuracy and robustness. In this paper, a novel multi-aspect-aware method is proposed to achieve this idea through the bidirectional Long Short-Term Memory (LSTM) recurrent neural networks based space-varying scattering information learning. Specifically, we first select different aspect images to generate the multi-aspect space-varying image sequences. Then, the Gabor filter and three-patch local binary pattern (TPLBP) are progressively implemented to extract a comprehensive spatial features, followed by dimensionality reduction with the Multi-layer Perceptron (MLP) network. Finally, we design a bidirectional LSTM recurrent neural network to learn the multi-aspect features with further integrating the softmax classifier to achieve target recognition. Experimental results demonstrate that the proposed method can achieve 99.9% accuracy for 10-class recognition. Besides, its anti-noise and anti-confusion performance are also better than the conventional deep learning based methods.



### Representation Learning on Large and Small Data
- **Arxiv ID**: http://arxiv.org/abs/1707.09873v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09873v1)
- **Published**: 2017-07-25 04:14:18+00:00
- **Updated**: 2017-07-25 04:14:18+00:00
- **Authors**: Chun-Nan Chou, Chuen-Kai Shie, Fu-Chieh Chang, Jocelyn Chang, Edward Y. Chang
- **Comment**: Book chapter
- **Journal**: None
- **Summary**: Deep learning owes its success to three key factors: scale of data, enhanced models to learn representations from data, and scale of computation. This book chapter presented the importance of the data-driven approach to learn good representations from both big data and small data. In terms of big data, it has been widely accepted in the research community that the more data the better for both representation and classification improvement. The question is then how to learn representations from big data, and how to perform representation learning when data is scarce. We addressed the first question by presenting CNN model enhancements in the aspects of representation, optimization, and generalization. To address the small data challenge, we showed transfer representation learning to be effective. Transfer representation learning transfers the learned representation from a source domain where abundant training data is available to a target domain where training data is scarce. Transfer representation learning gave the OM and melanoma diagnosis modules of our XPRIZE Tricorder device (which finished $2^{nd}$ out of $310$ competing teams) a significant boost in diagnosis accuracy.



### Correction of "Cloud Removal By Fusing Multi-Source and Multi-Temporal Images"
- **Arxiv ID**: http://arxiv.org/abs/1707.09959v1
- **DOI**: 10.1109/IGARSS.2017.8127522
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09959v1)
- **Published**: 2017-07-25 04:20:18+00:00
- **Updated**: 2017-07-25 04:20:18+00:00
- **Authors**: Chengyue Zhang, Zhiwei Li, Qing Cheng, Xinghua Li, Huanfeng Shen
- **Comment**: This is a correction version of the accepted IGARSS 2017 conference
  paper
- **Journal**: 2017 IEEE International Geoscience and Remote Sensing Symposium
  (IGARSS), pp.2577-2580, 2017
- **Summary**: Remote sensing images often suffer from cloud cover. Cloud removal is required in many applications of remote sensing images. Multitemporal-based methods are popular and effective to cope with thick clouds. This paper contributes to a summarization and experimental comparation of the existing multitemporal-based methods. Furthermore, we propose a spatiotemporal-fusion with poisson-adjustment method to fuse multi-sensor and multi-temporal images for cloud removal. The experimental results show that the proposed method has potential to address the problem of accuracy reduction of cloud removal in multi-temporal images with significant changes.



### Graph-Theoretic Spatiotemporal Context Modeling for Video Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1707.07815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07815v1)
- **Published**: 2017-07-25 05:36:05+00:00
- **Updated**: 2017-07-25 05:36:05+00:00
- **Authors**: Lina Wei, Fangfang Wang, Xi Li, Fei Wu, Jun Xiao
- **Comment**: ICIP 2017
- **Journal**: None
- **Summary**: As an important and challenging problem in computer vision, video saliency detection is typically cast as a spatiotemporal context modeling problem over consecutive frames. As a result, a key issue in video saliency detection is how to effectively capture the intrinsical properties of atomic video structures as well as their associated contextual interactions along the spatial and temporal dimensions. Motivated by this observation, we propose a graph-theoretic video saliency detection approach based on adaptive video structure discovery, which is carried out within a spatiotemporal atomic graph. Through graph-based manifold propagation, the proposed approach is capable of effectively modeling the semantically contextual interactions among atomic video structures for saliency detection while preserving spatial smoothness and temporal consistency. Experiments demonstrate the effectiveness of the proposed approach over several benchmark datasets.



### Detecting Semantic Parts on Partially Occluded Objects
- **Arxiv ID**: http://arxiv.org/abs/1707.07819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07819v1)
- **Published**: 2017-07-25 05:54:01+00:00
- **Updated**: 2017-07-25 05:54:01+00:00
- **Authors**: Jianyu Wang, Cihang Xie, Zhishuai Zhang, Jun Zhu, Lingxi Xie, Alan Yuille
- **Comment**: Accepted to BMVC 2017 (13 pages, 3 figures)
- **Journal**: None
- **Summary**: In this paper, we address the task of detecting semantic parts on partially occluded objects. We consider a scenario where the model is trained using non-occluded images but tested on occluded images. The motivation is that there are infinite number of occlusion patterns in real world, which cannot be fully covered in the training data. So the models should be inherently robust and adaptive to occlusions instead of fitting / learning the occlusion patterns in the training data. Our approach detects semantic parts by accumulating the confidence of local visual cues. Specifically, the method uses a simple voting method, based on log-likelihood ratio tests and spatial constraints, to combine the evidence of local cues. These cues are called visual concepts, which are derived by clustering the internal states of deep networks. We evaluate our voting scheme on the VehicleSemanticPart dataset with dense part annotations. We randomly place two, three or four irrelevant objects onto the target object to generate testing images with various occlusions. Experiments show that our algorithm outperforms several competitors in semantic part detection when occlusions are present.



### Multiple-Kernel Local-Patch Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1707.07825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07825v1)
- **Published**: 2017-07-25 06:15:41+00:00
- **Updated**: 2017-07-25 06:15:41+00:00
- **Authors**: Arun Mukundan, Giorgos Tolias, Ondrej Chum
- **Comment**: To appear in the British Machine Vision Conference (BMVC), September
  2017
- **Journal**: None
- **Summary**: We propose a multiple-kernel local-patch descriptor based on efficient match kernels of patch gradients. It combines two parametrizations of gradient position and direction, each parametrization provides robustness to a different type of patch miss-registration: polar parametrization for noise in the patch dominant orientation detection, Cartesian for imprecise location of the feature point. Even though handcrafted, the proposed method consistently outperforms the state-of-the-art methods on two local patch benchmarks.



### Improving Robustness of Feature Representations to Image Deformations using Powered Convolution in CNNs
- **Arxiv ID**: http://arxiv.org/abs/1707.07830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07830v1)
- **Published**: 2017-07-25 06:33:43+00:00
- **Updated**: 2017-07-25 06:33:43+00:00
- **Authors**: Zhun Sun, Mete Ozay, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we address the problem of improvement of robustness of feature representations learned using convolutional neural networks (CNNs) to image deformation. We argue that higher moment statistics of feature distributions could be shifted due to image deformations, and the shift leads to degrade of performance and cannot be reduced by ordinary normalization methods as observed in experimental analyses. In order to attenuate this effect, we apply additional non-linearity in CNNs by combining power functions with learnable parameters into convolution operation. In the experiments, we observe that CNNs which employ the proposed method obtain remarkable boost in both the generalization performance and the robustness under various types of deformations using large scale benchmark datasets. For instance, a model equipped with the proposed method obtains 3.3\% performance boost in mAP on Pascal Voc object detection task using deformed images, compared to the reference model, while both models provide the same performance using original images. To the best of our knowledge, this is the first work that studies robustness of deep features learned using CNNs to a wide range of deformations for object recognition and detection.



### ssEMnet: Serial-section Electron Microscopy Image Registration using a Spatial Transformer Network with Learned Features
- **Arxiv ID**: http://arxiv.org/abs/1707.07833v2
- **DOI**: 10.1007/978-3-319-67558-9_29
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07833v2)
- **Published**: 2017-07-25 06:50:34+00:00
- **Updated**: 2017-12-05 06:56:20+00:00
- **Authors**: Inwan Yoo, David G. C. Hildebrand, Willie F. Tobin, Wei-Chung Allen Lee, Won-Ki Jeong
- **Comment**: DLMIA 2017 accepted
- **Journal**: None
- **Summary**: The alignment of serial-section electron microscopy (ssEM) images is critical for efforts in neuroscience that seek to reconstruct neuronal circuits. However, each ssEM plane contains densely packed structures that vary from one section to the next, which makes matching features across images a challenge. Advances in deep learning has resulted in unprecedented performance in similar computer vision problems, but to our knowledge, they have not been successfully applied to ssEM image co-registration. In this paper, we introduce a novel deep network model that combines a spatial transformer for image deformation and a convolutional autoencoder for unsupervised feature learning for robust ssEM image alignment. This results in improved accuracy and robustness while requiring substantially less user intervention than conventional methods. We evaluate our method by comparing registration quality across several datasets.



### Motion-Appearance Interactive Encoding for Object Segmentation in Unconstrained Videos
- **Arxiv ID**: http://arxiv.org/abs/1707.07857v1
- **DOI**: 10.1109/TCSVT.2019.2908779
- **Categories**: **cs.CV**, I.4.6; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1707.07857v1)
- **Published**: 2017-07-25 09:01:59+00:00
- **Updated**: 2017-07-25 09:01:59+00:00
- **Authors**: Chunchao Guo, Jianhuang Lai, Xiaohua Xie
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: We present a novel method of integrating motion and appearance cues for foreground object segmentation in unconstrained videos. Unlike conventional methods encoding motion and appearance patterns individually, our method puts particular emphasis on their mutual assistance. Specifically, we propose using an interactively constrained encoding (ICE) scheme to incorporate motion and appearance patterns into a graph that leads to a spatiotemporal energy optimization. The reason of utilizing ICE is that both motion and appearance cues for the same target share underlying correlative structure, thus can be exploited in a deeply collaborative manner. We perform ICE not only in the initialization but also in the refinement stage of a two-layer framework for object segmentation. This scheme allows our method to consistently capture structural patterns about object perceptions throughout the whole framework. Our method can be operated on superpixels instead of raw pixels to reduce the number of graph nodes by two orders of magnitude. Moreover, we propose to partially explore the multi-object localization problem with inter-occlusion by weighted bipartite graph matching. Comprehensive experiments on three benchmark datasets (i.e., SegTrack, MOViCS, and GaTech) demonstrate the effectiveness of our approach compared with extensive state-of-the-art methods.



### Analyzing First-Person Stories Based on Socializing, Eating and Sedentary Patterns
- **Arxiv ID**: http://arxiv.org/abs/1707.07863v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07863v1)
- **Published**: 2017-07-25 09:15:44+00:00
- **Updated**: 2017-07-25 09:15:44+00:00
- **Authors**: Pedro Herruzo, Laura Portell, Alberto Soto, Beatriz Remeseiro
- **Comment**: Accepted at First International Workshop on Social Signal Processing
  and Beyond, 19th International Conference on Image Analysis and Processing
  (ICIAP), September 2017
- **Journal**: None
- **Summary**: First-person stories can be analyzed by means of egocentric pictures acquired throughout the whole active day with wearable cameras. This manuscript presents an egocentric dataset with more than 45,000 pictures from four people in different environments such as working or studying. All the images were manually labeled to identify three patterns of interest regarding people's lifestyle: socializing, eating and sedentary. Additionally, two different approaches are proposed to classify egocentric images into one of the 12 target categories defined to characterize these three patterns. The approaches are based on machine learning and deep learning techniques, including traditional classifiers and state-of-art convolutional neural networks. The experimental results obtained when applying these methods to the egocentric dataset demonstrated their adequacy for the problem at hand.



### Spatiotemporal Modeling for Crowd Counting in Videos
- **Arxiv ID**: http://arxiv.org/abs/1707.07890v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07890v1)
- **Published**: 2017-07-25 10:02:33+00:00
- **Updated**: 2017-07-25 10:02:33+00:00
- **Authors**: Feng Xiong, Xingjian Shi, Dit-Yan Yeung
- **Comment**: Accepted by ICCV 2017
- **Journal**: None
- **Summary**: Region of Interest (ROI) crowd counting can be formulated as a regression problem of learning a mapping from an image or a video frame to a crowd density map. Recently, convolutional neural network (CNN) models have achieved promising results for crowd counting. However, even when dealing with video data, CNN-based methods still consider each video frame independently, ignoring the strong temporal correlation between neighboring frames. To exploit the otherwise very useful temporal information in video sequences, we propose a variant of a recent deep learning model called convolutional LSTM (ConvLSTM) for crowd counting. Unlike the previous CNN-based methods, our method fully captures both spatial and temporal dependencies. Furthermore, we extend the ConvLSTM model to a bidirectional ConvLSTM model which can access long-range information in both directions. Extensive experiments using four publicly available datasets demonstrate the reliability of our approach and the effectiveness of incorporating temporal information to boost the accuracy of crowd counting. In addition, we also conduct some transfer learning experiments to show that once our model is trained on one dataset, its learning experience can be transferred easily to a new dataset which consists of only very few video frames for model adaptation.



### Enhancing Convolutional Neural Networks for Face Recognition with Occlusion Maps and Batch Triplet Loss
- **Arxiv ID**: http://arxiv.org/abs/1707.07923v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07923v4)
- **Published**: 2017-07-25 11:35:18+00:00
- **Updated**: 2018-06-10 09:42:33+00:00
- **Authors**: Daniel Sáez Trigueros, Li Meng, Margaret Hartnett
- **Comment**: 12 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: Despite the recent success of convolutional neural networks for computer vision applications, unconstrained face recognition remains a challenge. In this work, we make two contributions to the field. Firstly, we consider the problem of face recognition with partial occlusions and show how current approaches might suffer significant performance degradation when dealing with this kind of face images. We propose a simple method to find out which parts of the human face are more important to achieve a high recognition rate, and use that information during training to force a convolutional neural network to learn discriminative features from all the face regions more equally, including those that typical approaches tend to pay less attention to. We test the accuracy of the proposed method when dealing with real-life occlusions using the AR face database. Secondly, we propose a novel loss function called batch triplet loss that improves the performance of the triplet loss by adding an extra term to the loss function to cause minimisation of the standard deviation of both positive and negative scores. We show consistent improvement in the Labeled Faces in the Wild (LFW) benchmark by applying both proposed adjustments to the convolutional neural network training.



### Functional connectivity patterns of autism spectrum disorder identified by deep feature learning
- **Arxiv ID**: http://arxiv.org/abs/1707.07932v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1707.07932v1)
- **Published**: 2017-07-25 11:59:33+00:00
- **Updated**: 2017-07-25 11:59:33+00:00
- **Authors**: Hongyoon Choi
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Autism spectrum disorder (ASD) is regarded as a brain disease with globally disrupted neuronal networks. Even though fMRI studies have revealed abnormal functional connectivity in ASD, they have not reached a consensus of the disrupted patterns. Here, a deep learning-based feature extraction method identifies multivariate and nonlinear functional connectivity patterns of ASD. Resting-state fMRI data of 972 subjects (465 ASD 507 normal controls) acquired from the Autism Brain Imaging Data Exchange were used. A functional connectivity matrix of each subject was generated using 90 predefined brain regions. As a data-driven feature extraction method without prior knowledge such as subjects diagnosis, variational autoencoder (VAE) summarized the functional connectivity matrix into 2 features. Those feature values of ASD patients were statistically compared with those of controls. A feature was significantly different between ASD and normal controls. The extracted features were visualized by VAE-based generator which can produce virtual functional connectivity matrices. The ASD-related feature was associated with frontoparietal connections, interconnections of the dorsal medial frontal cortex and corticostriatal connections. It also showed a trend of negative correlation with full-scale IQ. A data-driven feature extraction based on deep learning could identify complex patterns of functional connectivity of ASD. This approach will help discover complex patterns of abnormalities in brain connectivity in various brain disorders.



### Residual Conv-Deconv Grid Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.07958v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07958v2)
- **Published**: 2017-07-25 12:49:11+00:00
- **Updated**: 2017-07-26 08:18:54+00:00
- **Authors**: Damien Fourure, Rémi Emonet, Elisa Fromont, Damien Muselet, Alain Tremeau, Christian Wolf
- **Comment**: Accepted for publication at BMVC 2017
- **Journal**: None
- **Summary**: This paper presents GridNet, a new Convolutional Neural Network (CNN) architecture for semantic image segmentation (full scene labelling). Classical neural networks are implemented as one stream from the input to the output with subsampling operators applied in the stream in order to reduce the feature maps size and to increase the receptive field for the final prediction. However, for semantic image segmentation, where the task consists in providing a semantic class to each pixel of an image, feature maps reduction is harmful because it leads to a resolution loss in the output prediction. To tackle this problem, our GridNet follows a grid pattern allowing multiple interconnected streams to work at different resolutions. We show that our network generalizes many well known networks such as conv-deconv, residual or U-Net networks. GridNet is trained from scratch and achieves competitive results on the Cityscapes dataset.



### Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1707.07998v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07998v3)
- **Published**: 2017-07-25 13:50:17+00:00
- **Updated**: 2018-03-14 05:24:23+00:00
- **Authors**: Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang
- **Comment**: CVPR 2018 full oral, winner of the 2017 Visual Question Answering
  challenge
- **Journal**: None
- **Summary**: Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.



### Automatic Liver Segmentation Using an Adversarial Image-to-Image Network
- **Arxiv ID**: http://arxiv.org/abs/1707.08037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08037v1)
- **Published**: 2017-07-25 15:16:07+00:00
- **Updated**: 2017-07-25 15:16:07+00:00
- **Authors**: Dong Yang, Daguang Xu, S. Kevin Zhou, Bogdan Georgescu, Mingqing Chen, Sasa Grbic, Dimitris Metaxas, Dorin Comaniciu
- **Comment**: Accepted by MICCAI 2017
- **Journal**: None
- **Summary**: Automatic liver segmentation in 3D medical images is essential in many clinical applications, such as pathological diagnosis of hepatic diseases, surgical planning, and postoperative assessment. However, it is still a very challenging task due to the complex background, fuzzy boundary, and various appearance of liver. In this paper, we propose an automatic and efficient algorithm to segment liver from 3D CT volumes. A deep image-to-image network (DI2IN) is first deployed to generate the liver segmentation, employing a convolutional encoder-decoder architecture combined with multi-level feature concatenation and deep supervision. Then an adversarial network is utilized during training process to discriminate the output of DI2IN from ground truth, which further boosts the performance of DI2IN. The proposed method is trained on an annotated dataset of 1000 CT volumes with various different scanning protocols (e.g., contrast and non-contrast, various resolution and position) and large variations in populations (e.g., ages and pathology). Our approach outperforms the state-of-the-art solutions in terms of segmentation accuracy and computing efficiency.



### A Simple Exponential Family Framework for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.08040v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1707.08040v3)
- **Published**: 2017-07-25 15:28:22+00:00
- **Updated**: 2018-01-25 05:37:04+00:00
- **Authors**: Vinay Kumar Verma, Piyush Rai
- **Comment**: Accepted in ECML-PKDD 2017, 16 Pages: Code and Data are available:
  https://github.com/vkverma01/Zero-Shot/
- **Journal**: None
- **Summary**: We present a simple generative framework for learning to predict previously unseen classes, based on estimating class-attribute-gated class-conditional distributions. We model each class-conditional distribution as an exponential family distribution and the parameters of the distribution of each seen/unseen class are defined as functions of the respective observed class attributes. These functions can be learned using only the seen class data and can be used to predict the parameters of the class-conditional distribution of each unseen class. Unlike most existing methods for zero-shot learning that represent classes as fixed embeddings in some vector space, our generative model naturally represents each class as a probability distribution. It is simple to implement and also allows leveraging additional unlabeled data from unseen classes to improve the estimates of their class-conditional distributions using transductive/semi-supervised learning. Moreover, it extends seamlessly to few-shot learning by easily updating these distributions when provided with a small number of additional labelled examples from unseen classes. Through a comprehensive set of experiments on several benchmark data sets, we demonstrate the efficacy of our framework.



### Relative Depth Order Estimation Using Multi-scale Densely Connected Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.08063v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08063v2)
- **Published**: 2017-07-25 16:11:01+00:00
- **Updated**: 2017-07-27 10:19:13+00:00
- **Authors**: Ruoxi Deng, Tianqi Zhao, Chunhua Shen, Shengjun Liu
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: We study the problem of estimating the relative depth order of point pairs in a monocular image. Recent advances mainly focus on using deep convolutional neural networks (DCNNs) to learn and infer the ordinal information from multiple contextual information of the points pair such as global scene context, local contextual information, and the locations. However, it remains unclear how much each context contributes to the task. To address this, we first examine the contribution of each context cue [1], [2] to the performance in the context of depth order estimation. We find out the local context surrounding the points pair contributes the most and the global scene context helps little. Based on the findings, we propose a simple method, using a multi-scale densely-connected network to tackle the task. Instead of learning the global structure, we dedicate to explore the local structure by learning to regress from regions of multiple sizes around the point pairs. Moreover, we use the recent densely connected network [3] to encourage substantial feature reuse as well as deepen our network to boost the performance. We show in experiments that the results of our approach is on par with or better than the state-of-the-art methods with the benefit of using only a small number of training data.



### Line-Circle: A Geometric Filter for Single Camera Edge-Based Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1707.08095v1
- **DOI**: 10.1109/ICRoM.2017.8466193
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.08095v1)
- **Published**: 2017-07-25 17:19:42+00:00
- **Updated**: 2017-07-25 17:19:42+00:00
- **Authors**: Seyed Amir Tafrishi, Vahid E. Kandjani
- **Comment**: Submitted to 5th International Conference on Robotics and
  Mechatronics 2017
- **Journal**: 2017 5th RSI Int. Conf. on Robot. and Mech. (2017) 588-594
- **Summary**: This paper presents a state-of-the-art approach in object detection for being applied in future SLAM problems. Although, many SLAM methods are proposed to create suitable autonomy for mobile robots namely ground vehicles, they still face overconfidence and large computations during entrance to immense spaces with many landmarks. In particular, they suffer from impractical applications via sole reliance on the limited sensors like camera. Proposed method claims that unmanned ground vehicles without having huge amount of database for object definition and highly advance prediction parameters can deal with incoming objects during straight motion of camera in real-time. Line-Circle (LC) filter tries to apply detection, tracking and learning to each defined experts to obtain more information for judging scene without over-calculation. In this filter, circle expert let us summarize edges in groups. The Interactive feedback learning between each expert creates minimal error that fights against overwhelming landmark signs in crowded scenes without mapping. Our experts basically are dependent on trust factors' covariance with geometric definitions to ignore, emerge and compare detected landmarks. The experiment for validating the model is taken place utilizing a camera beside an IMU sensor for location estimation.



### Learning Bag-of-Features Pooling for Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.08105v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08105v2)
- **Published**: 2017-07-25 17:47:30+00:00
- **Updated**: 2017-07-26 04:25:06+00:00
- **Authors**: Nikolaos Passalis, Anastasios Tefas
- **Comment**: Accepted at ICCV 2017
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are well established models capable of achieving state-of-the-art classification accuracy for various computer vision tasks. However, they are becoming increasingly larger, using millions of parameters, while they are restricted to handling images of fixed size. In this paper, a quantization-based approach, inspired from the well-known Bag-of-Features model, is proposed to overcome these limitations. The proposed approach, called Convolutional BoF (CBoF), uses RBF neurons to quantize the information extracted from the convolutional layers and it is able to natively classify images of various sizes as well as to significantly reduce the number of parameters in the network. In contrast to other global pooling operators and CNN compression techniques the proposed method utilizes a trainable pooling layer that it is end-to-end differentiable, allowing the network to be trained using regular back-propagation and to achieve greater distribution shift invariance than competitive methods. The ability of the proposed method to reduce the parameters of the network and increase the classification accuracy over other state-of-the-art techniques is demonstrated using three image datasets.



### Automatic Image Transformation for Inducing Affect
- **Arxiv ID**: http://arxiv.org/abs/1707.08148v3
- **DOI**: 10.5244/C.31.171
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08148v3)
- **Published**: 2017-07-25 18:38:35+00:00
- **Updated**: 2021-08-07 14:52:37+00:00
- **Authors**: Afsheen Rafaqat Ali, Mohsen Ali
- **Comment**: Published at British Machine Vision Conference (BMVC) 2017
- **Journal**: None
- **Summary**: Current image transformation and recoloring algorithms try to introduce artistic effects in the photographed images, based on user input of target image(s) or selection of pre-designed filters. These manipulations, although intended to enhance the impact of an image on the viewer, do not include the option of image transformation by specifying the affect information. In this paper we present an automatic image-transformation method that transforms the source image such that it can induce an emotional affect on the viewer, as desired by the user. Our proposed novel image emotion transfer algorithm does not require a user-specified target image. The proposed algorithm uses features extracted from top layers of deep convolutional neural network and the user-specified emotion distribution to select multiple target images from an image database for color transformation, such that the resultant image has desired emotional impact. Our method can handle more diverse set of photographs than the previous methods. We conducted a detailed user study showing the effectiveness of our proposed method. A discussion and reasoning of failure cases has also been provided, indicating inherent limitation of color-transfer based methods in the use of emotion assignment.   Project Page: http://im.itu.edu.pk/affective-image-transfer/



### Patch-based Carcinoma Detection on Confocal Laser Endomicroscopy Images -- A Cross-Site Robustness Assessment
- **Arxiv ID**: http://arxiv.org/abs/1707.08149v2
- **DOI**: 10.5220/0006534700270034
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08149v2)
- **Published**: 2017-07-25 18:43:03+00:00
- **Updated**: 2020-01-03 13:49:53+00:00
- **Authors**: Marc Aubreville, Miguel Goncalves, Christian Knipfer, Nicolai Oetter, Tobias Wuerfl, Helmut Neumann, Florian Stelzle, Christopher Bohr, Andreas Maier
- **Comment**: Erratum: In the previous version, the number of CLE sequences in the
  vocal folds data set was inadequately reported
- **Journal**: Proceedings of BIOIMAGING 2018, ISBN: 978-989-758-278-3
- **Summary**: Deep learning technologies such as convolutional neural networks (CNN) provide powerful methods for image recognition and have recently been employed in the field of automated carcinoma detection in confocal laser endomicroscopy (CLE) images. CLE is a (sub-)surface microscopic imaging technique that reaches magnifications of up to 1000x and is thus suitable for in vivo structural tissue analysis. In this work, we aim to evaluate the prospects of a priorly developed deep learning-based algorithm targeted at the identification of oral squamous cell carcinoma with regard to its generalization to further anatomic locations of squamous cell carcinomas in the area of head and neck. We applied the algorithm on images acquired from the vocal fold area of five patients with histologically verified squamous cell carcinoma and presumably healthy control images of the clinically normal contra-lateral vocal cord. We find that the network trained on the oral cavity data reaches an accuracy of 89.45% and an area-under-the-curve (AUC) value of 0.955, when applied on the vocal cords data. Compared to the state of the art, we achieve very similar results, yet with an algorithm that was trained on a completely disjunct data set. Concatenating both data sets yielded further improvements in cross-validation with an accuracy of 90.81% and AUC of 0.970. In this study, for the first time to our knowledge, a deep learning mechanism for the identification of oral carcinomas using CLE Images could be applied to other disciplines in the area of head and neck. This study shows the prospect of the algorithmic approach to generalize well on other malignant entities of the head and neck, regardless of the anatomical location and furthermore in an examiner-independent manner.



### A Unified Joint Matrix Factorization Framework for Data Integration
- **Arxiv ID**: http://arxiv.org/abs/1707.08183v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.1; G.1.6; H.2.8
- **Links**: [PDF](http://arxiv.org/pdf/1707.08183v1)
- **Published**: 2017-07-25 19:38:25+00:00
- **Updated**: 2017-07-25 19:38:25+00:00
- **Authors**: Lihua Zhang, Shihua Zhang
- **Comment**: 14 pages, 7 figures
- **Journal**: None
- **Summary**: Nonnegative matrix factorization (NMF) is a powerful tool in data exploratory analysis by discovering the hidden features and part-based patterns from high-dimensional data. NMF and its variants have been successfully applied into diverse fields such as pattern recognition, signal processing, data mining, bioinformatics and so on. Recently, NMF has been extended to analyze multiple matrices simultaneously. However, a unified framework is still lacking. In this paper, we introduce a sparse multiple relationship data regularized joint matrix factorization (JMF) framework and two adapted prediction models for pattern recognition and data integration. Next, we present four update algorithms to solve this framework. The merits and demerits of these algorithms are systematically explored. Furthermore, extensive computational experiments using both synthetic data and real data demonstrate the effectiveness of JMF framework and related algorithms on pattern recognition and data mining.



### Vision-Based Assessment of Parkinsonism and Levodopa-Induced Dyskinesia with Deep Learning Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1707.09416v2
- **DOI**: 10.1186/s12984-018-0446-z
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.09416v2)
- **Published**: 2017-07-25 20:56:22+00:00
- **Updated**: 2017-08-01 16:03:22+00:00
- **Authors**: Michael H. Li, Tiago A. Mestre, Susan H. Fox, Babak Taati
- **Comment**: 8 pages, 1 figure. Under review
- **Journal**: Journal of NeuroEngineering and Rehabilitation (2018) 15:97
- **Summary**: Objective: To apply deep learning pose estimation algorithms for vision-based assessment of parkinsonism and levodopa-induced dyskinesia (LID). Methods: Nine participants with Parkinson's disease (PD) and LID completed a levodopa infusion protocol, where symptoms were assessed at regular intervals using the Unified Dyskinesia Rating Scale (UDysRS) and Unified Parkinson's Disease Rating Scale (UPDRS). A state-of-the-art deep learning pose estimation method was used to extract movement trajectories from videos of PD assessments. Features of the movement trajectories were used to detect and estimate the severity of parkinsonism and LID using random forest. Communication and drinking tasks were used to assess LID, while leg agility and toe tapping tasks were used to assess parkinsonism. Feature sets from tasks were also combined to predict total UDysRS and UPDRS Part III scores. Results: For LID, the communication task yielded the best results for dyskinesia (severity estimation: r = 0.661, detection: AUC = 0.930). For parkinsonism, leg agility had better results for severity estimation (r = 0.618), while toe tapping was better for detection (AUC = 0.773). UDysRS and UPDRS Part III scores were predicted with r = 0.741 and 0.530, respectively. Conclusion: This paper presents the first application of deep learning for vision-based assessment of parkinsonism and LID and demonstrates promising performance for the future translation of deep learning to PD clinical practices. Significance: The proposed system provides insight into the potential of computer vision and deep learning for clinical application in PD.



