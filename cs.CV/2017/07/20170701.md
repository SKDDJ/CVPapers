# Arxiv Papers in cs.CV on 2017-07-01
### Better than Real: Complex-valued Neural Nets for MRI Fingerprinting
- **Arxiv ID**: http://arxiv.org/abs/1707.00070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00070v1)
- **Published**: 2017-07-01 00:15:33+00:00
- **Updated**: 2017-07-01 00:15:33+00:00
- **Authors**: Patrick Virtue, Stella X. Yu, Michael Lustig
- **Comment**: Accepted in Proc. IEEE International Conference on Image Processing
  (ICIP), 2017
- **Journal**: None
- **Summary**: The task of MRI fingerprinting is to identify tissue parameters from complex-valued MRI signals. The prevalent approach is dictionary based, where a test MRI signal is compared to stored MRI signals with known tissue parameters and the most similar signals and tissue parameters retrieved. Such an approach does not scale with the number of parameters and is rather slow when the tissue parameter space is large.   Our first novel contribution is to use deep learning as an efficient nonlinear inverse mapping approach. We generate synthetic (tissue, MRI) data from an MRI simulator, and use them to train a deep net to map the MRI signal to the tissue parameters directly.   Our second novel contribution is to develop a complex-valued neural network with new cardioid activation functions. Our results demonstrate that complex-valued neural nets could be much more accurate than real-valued neural nets at complex-valued MRI fingerprinting.



### Synthesizing Deep Neural Network Architectures using Biological Synaptic Strength Distributions
- **Arxiv ID**: http://arxiv.org/abs/1707.00081v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1707.00081v1)
- **Published**: 2017-07-01 01:30:21+00:00
- **Updated**: 2017-07-01 01:30:21+00:00
- **Authors**: A. H. Karimi, M. J. Shafiee, A. Ghodsi, A. Wong
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we perform an exploratory study on synthesizing deep neural networks using biological synaptic strength distributions, and the potential influence of different distributions on modelling performance particularly for the scenario associated with small data sets. Surprisingly, a CNN with convolutional layer synaptic strengths drawn from biologically-inspired distributions such as log-normal or correlated center-surround distributions performed relatively well suggesting a possibility for designing deep neural network architectures that do not require many data samples to learn, and can sidestep current training procedures while maintaining or boosting modelling performance.



### Exploring the Imposition of Synaptic Precision Restrictions For Evolutionary Synthesis of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.00095v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.00095v1)
- **Published**: 2017-07-01 04:56:08+00:00
- **Updated**: 2017-07-01 04:56:08+00:00
- **Authors**: Mohammad Javad Shafiee, Francis Li, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: A key contributing factor to incredible success of deep neural networks has been the significant rise on massively parallel computing devices allowing researchers to greatly increase the size and depth of deep neural networks, leading to significant improvements in modeling accuracy. Although deeper, larger, or complex deep neural networks have shown considerable promise, the computational complexity of such networks is a major barrier to utilization in resource-starved scenarios. We explore the synaptogenesis of deep neural networks in the formation of efficient deep neural network architectures within an evolutionary deep intelligence framework, where a probabilistic generative modeling strategy is introduced to stochastically synthesize increasingly efficient yet effective offspring deep neural networks over generations, mimicking evolutionary processes such as heredity, random mutation, and natural selection in a probabilistic manner. In this study, we primarily explore the imposition of synaptic precision restrictions and its impact on the evolutionary synthesis of deep neural networks to synthesize more efficient network architectures tailored for resource-starved scenarios. Experimental results show significant improvements in synaptic efficiency (~10X decrease for GoogLeNet-based DetectNet) and inference speed (>5X increase for GoogLeNet-based DetectNet) while preserving modeling accuracy.



### Structured Sparse Ternary Weight Coding of Deep Neural Networks for Efficient Hardware Implementations
- **Arxiv ID**: http://arxiv.org/abs/1707.03684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03684v1)
- **Published**: 2017-07-01 05:38:55+00:00
- **Updated**: 2017-07-01 05:38:55+00:00
- **Authors**: Yoonho Boo, Wonyong Sung
- **Comment**: This paper is accepted in SIPS 2017
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) usually demand a large amount of operations for real-time inference. Especially, fully-connected layers contain a large number of weights, thus they usually need many off-chip memory accesses for inference. We propose a weight compression method for deep neural networks, which allows values of +1 or -1 only at predetermined positions of the weights so that decoding using a table can be conducted easily. For example, the structured sparse (8,2) coding allows at most two non-zero values among eight weights. This method not only enables multiplication-free DNN implementations but also compresses the weight storage by up to x32 compared to floating-point networks. Weight distribution normalization and gradual pruning techniques are applied to mitigate the performance degradation. The experiments are conducted with fully-connected deep neural networks and convolutional neural networks.



### Image Companding and Inverse Halftoning using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.00116v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.00116v2)
- **Published**: 2017-07-01 08:42:48+00:00
- **Updated**: 2017-07-21 14:01:29+00:00
- **Authors**: Xianxu Hou, Guoping Qiu
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: In this paper, we introduce deep learning technology to tackle two traditional low-level image processing problems, companding and inverse halftoning. We make two main contributions. First, to the best knowledge of the authors, this is the first work that has successfully developed deep learning based solutions to these two traditional low-level image processing problems. This not only introduces new methods to tackle well-known image processing problems but also demonstrates the power of deep learning in solving traditional signal processing problems. Second, we have developed an effective deep learning algorithm based on insights into the properties of visual quality of images and the internal representation properties of a deep convolutional neural network (CNN). We train a deep CNN as a nonlinear transformation function to map a low bit depth image to higher bit depth or from a halftone image to a continuous tone image. We also employ another pretrained deep CNN as a feature extractor to derive visually important features to construct the objective function for the training of the mapping CNN. We present experimental results to demonstrate the effectiveness of the new deep learning based solutions.



