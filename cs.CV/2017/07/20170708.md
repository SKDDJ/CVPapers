# Arxiv Papers in cs.CV on 2017-07-08
### Learning Representations and Generative Models for 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1707.02392v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.02392v3)
- **Published**: 2017-07-08 03:44:49+00:00
- **Updated**: 2018-06-12 04:27:00+00:00
- **Authors**: Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, Leonidas Guibas
- **Comment**: None
- **Journal**: 35th International Conference on Machine Learning (ICML), 2018
- **Summary**: Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.



### Effective Approaches to Batch Parallelization for Dynamic Neural Network Architectures
- **Arxiv ID**: http://arxiv.org/abs/1707.02402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02402v1)
- **Published**: 2017-07-08 06:04:31+00:00
- **Updated**: 2017-07-08 06:04:31+00:00
- **Authors**: Joseph Suarez, Clare Zhu
- **Comment**: Code at https://github.com/jsuarez5341/Efficient-Dynamic-Batching
- **Journal**: None
- **Summary**: We present a simple dynamic batching approach applicable to a large class of dynamic architectures that consistently yields speedups of over 10x. We provide performance bounds when the architecture is not known a priori and a stronger bound in the special case where the architecture is a predetermined balanced tree. We evaluate our approach on Johnson et al.'s recent visual question answering (VQA) result of his CLEVR dataset by Inferring and Executing Programs (IEP). We also evaluate on sparsely gated mixture of experts layers and achieve speedups of up to 1000x over the naive implementation.



### Embedding Visual Hierarchy with Deep Networks for Large-Scale Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.02406v1
- **DOI**: 10.1109/TIP.2018.2845118
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02406v1)
- **Published**: 2017-07-08 07:08:14+00:00
- **Updated**: 2017-07-08 07:08:14+00:00
- **Authors**: Tianyi Zhao, Baopeng Zhang, Wei Zhang, Ning Zhou, Jun Yu, Jianping Fan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a level-wise mixture model (LMM) is developed by embedding visual hierarchy with deep networks to support large-scale visual recognition (i.e., recognizing thousands or even tens of thousands of object classes), and a Bayesian approach is used to adapt a pre-trained visual hierarchy automatically to the improvements of deep features (that are used for image and object class representation) when more representative deep networks are learned along the time. Our LMM model can provide an end-to-end approach for jointly learning: (a) the deep networks to extract more discriminative deep features for image and object class representation; (b) the tree classifier for recognizing large numbers of object classes hierarchically; and (c) the visual hierarchy adaptation for achieving more accurate indexing of large numbers of object classes hierarchically. By supporting joint learning of the tree classifier, the deep networks and the visual hierarchy adaptation, our LMM algorithm can provide an effective approach for controlling inter-level error propagation effectively, thus it can achieve better accuracy rates on large-scale visual recognition. Our experiments are carried on ImageNet1K and ImageNet10K image sets, and our LMM algorithm can achieve very competitive results on both the accuracy rates and the computation efficiency as compared with the baseline methods.



### Deep Learning for Vanishing Point Detection Using an Inverse Gnomonic Projection
- **Arxiv ID**: http://arxiv.org/abs/1707.02427v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02427v2)
- **Published**: 2017-07-08 11:41:51+00:00
- **Updated**: 2017-11-16 13:13:03+00:00
- **Authors**: Florian Kluger, Hanno Ackermann, Michael Ying Yang, Bodo Rosenhahn
- **Comment**: Accepted for publication at German Conference on Pattern Recognition
  (GCPR) 2017. This research was supported by German Research Foundation DFG
  within Priority Research Programme 1894 "Volunteered Geographic Information:
  Interpretation, Visualisation and Social Computing"
- **Journal**: None
- **Summary**: We present a novel approach for vanishing point detection from uncalibrated monocular images. In contrast to state-of-the-art, we make no a priori assumptions about the observed scene. Our method is based on a convolutional neural network (CNN) which does not use natural images, but a Gaussian sphere representation arising from an inverse gnomonic projection of lines detected in an image. This allows us to rely on synthetic data for training, eliminating the need for labelled images. Our method achieves competitive performance on three horizon estimation benchmark datasets. We further highlight some additional use cases for which our vanishing point detection algorithm can be used.



### Deep Semantic Segmentation for Automated Driving: Taxonomy, Roadmap and Challenges
- **Arxiv ID**: http://arxiv.org/abs/1707.02432v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.02432v2)
- **Published**: 2017-07-08 12:32:51+00:00
- **Updated**: 2017-08-03 14:54:40+00:00
- **Authors**: Mennatullah Siam, Sara Elkerdawy, Martin Jagersand, Senthil Yogamani
- **Comment**: To appear in IEEE ITSC 2017
- **Journal**: None
- **Summary**: Semantic segmentation was seen as a challenging computer vision problem few years ago. Due to recent advancements in deep learning, relatively accurate solutions are now possible for its use in automated driving. In this paper, the semantic segmentation problem is explored from the perspective of automated driving. Most of the current semantic segmentation algorithms are designed for generic images and do not incorporate prior structure and end goal for automated driving. First, the paper begins with a generic taxonomic survey of semantic segmentation algorithms and then discusses how it fits in the context of automated driving. Second, the particular challenges of deploying it into a safety system which needs high level of accuracy and robustness are listed. Third, different alternatives instead of using an independent semantic segmentation module are explored. Finally, an empirical evaluation of various semantic segmentation architectures was performed on CamVid dataset in terms of accuracy and speed. This paper is a preliminary shorter version of a more detailed survey which is work in progress.



### Self Adversarial Training for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1707.02439v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02439v2)
- **Published**: 2017-07-08 13:24:44+00:00
- **Updated**: 2017-08-15 10:10:26+00:00
- **Authors**: Chia-Jung Chou, Jui-Ting Chien, Hwann-Tzong Chen
- **Comment**: CVPR 2017 Workshop on Visual Understanding of Humans in Crowd Scene
  and the 1st Look Into Person (LIP) Challenge
- **Journal**: None
- **Summary**: This paper presents a deep learning based approach to the problem of human pose estimation. We employ generative adversarial networks as our learning paradigm in which we set up two stacked hourglass networks with the same architecture, one as the generator and the other as the discriminator. The generator is used as a human pose estimator after the training is done. The discriminator distinguishes ground-truth heatmaps from generated ones, and back-propagates the adversarial loss to the generator. This process enables the generator to learn plausible human body configurations and is shown to be useful for improving the prediction accuracy.



### Hyperspectral Image Restoration via Total Variation Regularized Low-rank Tensor Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1707.02477v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02477v1)
- **Published**: 2017-07-08 18:41:06+00:00
- **Updated**: 2017-07-08 18:41:06+00:00
- **Authors**: Yao Wang, Jiangjun Peng, Qian Zhao, Deyu Meng, Yee Leung, Xi-Le Zhao
- **Comment**: 15 pages, 20 figures
- **Journal**: None
- **Summary**: Hyperspectral images (HSIs) are often corrupted by a mixture of several types of noise during the acquisition process, e.g., Gaussian noise, impulse noise, dead lines, stripes, and many others. Such complex noise could degrade the quality of the acquired HSIs, limiting the precision of the subsequent processing. In this paper, we present a novel tensor-based HSI restoration approach by fully identifying the intrinsic structures of the clean HSI part and the mixed noise part respectively. Specifically, for the clean HSI part, we use tensor Tucker decomposition to describe the global correlation among all bands, and an anisotropic spatial-spectral total variation (SSTV) regularization to characterize the piecewise smooth structure in both spatial and spectral domains. For the mixed noise part, we adopt the $\ell_1$ norm regularization to detect the sparse noise, including stripes, impulse noise, and dead pixels. Despite that TV regulariztion has the ability of removing Gaussian noise, the Frobenius norm term is further used to model heavy Gaussian noise for some real-world scenarios. Then, we develop an efficient algorithm for solving the resulting optimization problem by using the augmented Lagrange multiplier (ALM) method. Finally, extensive experiments on simulated and real-world noise HSIs are carried out to demonstrate the superiority of the proposed method over the existing state-of-the-art ones.



### A Reconfigurable Streaming Deep Convolutional Neural Network Accelerator for Internet of Things
- **Arxiv ID**: http://arxiv.org/abs/1707.02973v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/1707.02973v1)
- **Published**: 2017-07-08 19:31:38+00:00
- **Updated**: 2017-07-08 19:31:38+00:00
- **Authors**: Li Du, Yuan Du, Yilei Li, Mau-Chung Frank Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural network (CNN) offers significant accuracy in image detection. To implement image detection using CNN in the internet of things (IoT) devices, a streaming hardware accelerator is proposed. The proposed accelerator optimizes the energy efficiency by avoiding unnecessary data movement. With unique filter decomposition technique, the accelerator can support arbitrary convolution window size. In addition, max pooling function can be computed in parallel with convolution by using separate pooling unit, thus achieving throughput improvement. A prototype accelerator was implemented in TSMC 65nm technology with a core size of 5mm2. The accelerator can support major CNNs and achieve 152GOPS peak throughput and 434GOPS/W energy efficiency at 350mW, making it a promising hardware accelerator for intelligent IoT devices.



### MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network
- **Arxiv ID**: http://arxiv.org/abs/1707.02485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02485v1)
- **Published**: 2017-07-08 19:48:30+00:00
- **Updated**: 2017-07-08 19:48:30+00:00
- **Authors**: Zizhao Zhang, Yuanpu Xie, Fuyong Xing, Mason McGough, Lin Yang
- **Comment**: CVPR2017 Oral
- **Journal**: None
- **Summary**: The inability to interpret the model prediction in semantically and visually meaningful ways is a well-known shortcoming of most existing computer-aided diagnosis methods. In this paper, we propose MDNet to establish a direct multimodal mapping between medical images and diagnostic reports that can read images, generate diagnostic reports, retrieve images by symptom descriptions, and visualize attention, to provide justifications of the network diagnosis process. MDNet includes an image model and a language model. The image model is proposed to enhance multi-scale feature ensembles and utilization efficiency. The language model, integrated with our improved attention mechanism, aims to read and explore discriminative image feature descriptions from reports to learn a direct mapping from sentence words to image pixels. The overall network is trained end-to-end by using our developed optimization strategy. Based on a pathology bladder cancer images and its diagnostic reports (BCIDR) dataset, we conduct sufficient experiments to demonstrate that MDNet outperforms comparative baselines. The proposed image model obtains state-of-the-art performance on two CIFAR datasets as well.



