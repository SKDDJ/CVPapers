# Arxiv Papers in cs.CV on 2017-07-11
### Online Handwritten Mathematical Expressions Recognition System Using Fuzzy Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1707.03088v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/1707.03088v2)
- **Published**: 2017-07-11 00:28:23+00:00
- **Updated**: 2017-07-12 04:38:13+00:00
- **Authors**: E. Naderan
- **Comment**: in Russian
- **Journal**: ITHEA, Information Content and Processing, 2014, 1 (3) , 262-268
- **Summary**: The article describes developed information technology for online recognition of handwritten mathematical expressions that based on proposed approaches to handwritten symbols recognition and structural analysis.



### Distance-to-Mean Continuous Conditional Random Fields to Enhance Prediction Problem in Traffic Flow Data
- **Arxiv ID**: http://arxiv.org/abs/1707.03110v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.03110v1)
- **Published**: 2017-07-11 02:36:28+00:00
- **Updated**: 2017-07-11 02:36:28+00:00
- **Authors**: Sumarsih Condroayu Purbarani, Hadaiq Rolis Sanabila, Wisnu Jatmiko
- **Comment**: None
- **Journal**: None
- **Summary**: The increase of vehicle in highways may cause traffic congestion as well as in the normal roadways. Predicting the traffic flow in highways especially, is demanded to solve this congestion problem. Predictions on time-series multivariate data, such as in the traffic flow dataset, have been largely accomplished through various approaches. The approach with conventional prediction algorithms, such as with Support Vector Machine (SVM), is only capable of accommodating predictions that are independent in each time unit. Hence, the sequential relationships in this time series data is hardly explored. Continuous Conditional Random Field (CCRF) is one of Probabilistic Graphical Model (PGM) algorithms which can accommodate this problem. The neighboring aspects of sequential data such as in the time series data can be expressed by CCRF so that its predictions are more reliable. In this article, a novel approach called DM-CCRF is adopted by modifying the CCRF prediction algorithm to strengthen the probability of the predictions made by the baseline regressor. The result shows that DM-CCRF is superior in performance compared to CCRF. This is validated by the error decrease of the baseline up to 9% significance. This is twice the standard CCRF performance which can only decrease baseline error by 4.582% at most.



### SaltiNet: Scan-path Prediction on 360 Degree Images using Saliency Volumes
- **Arxiv ID**: http://arxiv.org/abs/1707.03123v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1707.03123v5)
- **Published**: 2017-07-11 04:04:09+00:00
- **Updated**: 2017-08-17 10:48:16+00:00
- **Authors**: Marc Assens, Kevin McGuinness, Xavier Giro-i-Nieto, Noel E. O'Connor
- **Comment**: Winner of the Best Scan-path Award at the Salient360!: Visual
  attention modeling for 360 degrees Images Grand Challenge of ICME 2017.
  Presented at the ICCV 2017 Workshop on Egocentric Perception, Interaction and
  Computing (EPIC)
- **Journal**: None
- **Summary**: We introduce SaltiNet, a deep neural network for scanpath prediction trained on 360-degree images. The model is based on a temporal-aware novel representation of saliency information named the saliency volume. The first part of the network consists of a model trained to generate saliency volumes, whose parameters are fit by back-propagation computed from a binary cross entropy (BCE) loss over downsampled versions of the saliency volumes. Sampling strategies over these volumes are used to generate scanpaths over the 360-degree images. Our experiments show the advantages of using saliency volumes, and how they can be used for related tasks. Our source code and trained models available at https://github.com/massens/saliency-360salient-2017.



### Adversarial Generation of Training Examples: Applications to Moving Vehicle License Plate Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.03124v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03124v3)
- **Published**: 2017-07-11 04:17:17+00:00
- **Updated**: 2017-11-10 23:33:46+00:00
- **Authors**: Xinlong Wang, Zhipeng Man, Mingyu You, Chunhua Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GAN) have attracted much research attention recently, leading to impressive results for natural image generation. However, to date little success was observed in using GAN generated images for improving classification tasks. Here we attempt to explore, in the context of car license plate recognition, whether it is possible to generate synthetic training data using GAN to improve recognition accuracy. With a carefully-designed pipeline, we show that the answer is affirmative. First, a large-scale image set is generated using the generator of GAN, without manual annotation. Then, these images are fed to a deep convolutional neural network (DCNN) followed by a bidirectional recurrent neural network (BRNN) with long short-term memory (LSTM), which performs the feature learning and sequence labelling. Finally, the pre-trained model is fine-tuned on real images. Our experimental results on a few data sets demonstrate the effectiveness of using GAN images: an improvement of 7.5% over a strong baseline with moderate-sized real data being available. We show that the proposed framework achieves competitive recognition accuracy on challenging test datasets. We also leverage the depthwise separate convolution to construct a lightweight convolutional RNN, which is about half size and 2x faster on CPU. Combining this framework and the proposed pipeline, we make progress in performing accurate recognition on mobile and embedded devices.



### Impulsive noise removal from color images with morphological filtering
- **Arxiv ID**: http://arxiv.org/abs/1707.03126v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03126v1)
- **Published**: 2017-07-11 04:48:24+00:00
- **Updated**: 2017-07-11 04:48:24+00:00
- **Authors**: Alexey Ruchay, Vitaly Kober
- **Comment**: The 6th international conference on analysis of images, social
  networks, and texts (AIST 2017), 27-29 July, 2017, Moscow, Russia
- **Journal**: None
- **Summary**: This paper deals with impulse noise removal from color images. The proposed noise removal algorithm employs a novel approach with morphological filtering for color image denoising; that is, detection of corrupted pixels and removal of the detected noise by means of morphological filtering. With the help of computer simulation we show that the proposed algorithm can effectively remove impulse noise. The performance of the proposed algorithm is compared in terms of image restoration metrics and processing speed with that of common successful algorithms.



### Underwater object classification using scattering transform of sonar signals
- **Arxiv ID**: http://arxiv.org/abs/1707.03133v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03133v3)
- **Published**: 2017-07-11 05:21:20+00:00
- **Updated**: 2017-09-03 22:03:33+00:00
- **Authors**: Naoki Saito, David S. Weber
- **Comment**: 13 pages, 12 figures, SPIE conference on Wavelets and Sparsity 17.
  I've also included a compiled version
- **Journal**: None
- **Summary**: In this paper, we apply the scattering transform (ST), a nonlinear map based off of a convolutional neural network (CNN), to classification of underwater objects using sonar signals. The ST formalizes the observation that the filters learned by a CNN have wavelet like structure. We achieve effective binary classification both on a real dataset of Unexploded Ordinance (UXOs), as well as synthetically generated examples. We also explore the effects on the waveforms with respect to changes in the object domain (e.g., translation, rotation, and acoustic impedance, etc.), and examine the consequences coming from theoretical results for the scattering transform. We show that the scattering transform is capable of excellent classification on both the synthetic and real problems, thanks to having more quasi-invariance properties that are well-suited to translation and rotation of the object.



### Experimental comparison of single-pixel imaging algorithms
- **Arxiv ID**: http://arxiv.org/abs/1707.03164v2
- **DOI**: 10.1364/JOSAA.35.000078
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1707.03164v2)
- **Published**: 2017-07-11 08:21:03+00:00
- **Updated**: 2017-10-24 07:51:10+00:00
- **Authors**: Liheng Bian, Jinli Suo, Qionghai Dai, Feng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Single-pixel imaging (SPI) is a novel technique capturing 2D images using a photodiode, instead of conventional 2D array sensors. SPI owns high signal-to-noise ratio, wide spectrum range, low cost, and robustness to light scattering. Various algorithms have been proposed for SPI reconstruction, including the linear correlation methods, the alternating projection method (AP), and the compressive sensing based methods. However, there has been no comprehensive review discussing respective advantages, which is important for SPI's further applications and development. In this paper, we reviewed and compared these algorithms in a unified reconstruction framework. Besides, we proposed two other SPI algorithms including a conjugate gradient descent based method (CGD) and a Poisson maximum likelihood based method. Both simulations and experiments validate the following conclusions: to obtain comparable reconstruction accuracy, the compressive sensing based total variation regularization method (TV) requires the least measurements and consumes the least running time for small-scale reconstruction; the CGD and AP methods run fastest in large-scale cases; the TV and AP methods are the most robust to measurement noise. In a word, there are trade-offs between capture efficiency, computational complexity and robustness to noise among different SPI algorithms. We have released our source code for non-commercial use.



### Foreground Detection in Camouflaged Scenes
- **Arxiv ID**: http://arxiv.org/abs/1707.03166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03166v1)
- **Published**: 2017-07-11 08:21:45+00:00
- **Updated**: 2017-07-11 08:21:45+00:00
- **Authors**: Shuai Li, Dinei Florencio, Yaqin Zhao, Chris Cook, Wanqing Li
- **Comment**: IEEE International Conference on Image Processing, 2017
- **Journal**: None
- **Summary**: Foreground detection has been widely studied for decades due to its importance in many practical applications. Most of the existing methods assume foreground and background show visually distinct characteristics and thus the foreground can be detected once a good background model is obtained. However, there are many situations where this is not the case. Of particular interest in video surveillance is the camouflage case. For example, an active attacker camouflages by intentionally wearing clothes that are visually similar to the background. In such cases, even given a decent background model, it is not trivial to detect foreground objects. This paper proposes a texture guided weighted voting (TGWV) method which can efficiently detect foreground objects in camouflaged scenes. The proposed method employs the stationary wavelet transform to decompose the image into frequency bands. We show that the small and hardly noticeable differences between foreground and background in the image domain can be effectively captured in certain wavelet frequency bands. To make the final foreground decision, a weighted voting scheme is developed based on intensity and texture of all the wavelet bands with weights carefully designed. Experimental results demonstrate that the proposed method achieves superior performance compared to the current state-of-the-art results.



### RegNet: Multimodal Sensor Registration Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.03167v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1707.03167v1)
- **Published**: 2017-07-11 08:21:58+00:00
- **Updated**: 2017-07-11 08:21:58+00:00
- **Authors**: Nick Schneider, Florian Piewak, Christoph Stiller, Uwe Franke
- **Comment**: published in IEEE Intelligent Vehicles Symposium, 2017
- **Journal**: None
- **Summary**: In this paper, we present RegNet, the first deep convolutional neural network (CNN) to infer a 6 degrees of freedom (DOF) extrinsic calibration between multimodal sensors, exemplified using a scanning LiDAR and a monocular camera. Compared to existing approaches, RegNet casts all three conventional calibration steps (feature extraction, feature matching and global regression) into a single real-time capable CNN. Our method does not require any human interaction and bridges the gap between classical offline and target-less online calibration approaches as it provides both a stable initial estimation as well as a continuous online correction of the extrinsic parameters. During training we randomly decalibrate our system in order to train RegNet to infer the correspondence between projected depth measurements and RGB image and finally regress the extrinsic calibration. Additionally, with an iterative execution of multiple CNNs, that are trained on different magnitudes of decalibration, our approach compares favorably to state-of-the-art methods in terms of a mean calibration error of 0.28 degrees for the rotational and 6 cm for the translation components even for large decalibrations up to 1.5 m and 20 degrees.



### Sensitivity Analysis for Mirror-Stratifiable Convex Functions
- **Arxiv ID**: http://arxiv.org/abs/1707.03194v3
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, stat.ML, 65K05, 65K10, 90C25, 90C31
- **Links**: [PDF](http://arxiv.org/pdf/1707.03194v3)
- **Published**: 2017-07-11 09:35:14+00:00
- **Updated**: 2018-06-05 08:25:18+00:00
- **Authors**: Jalal Fadili, Jérôme Malick, Gabriel Peyré
- **Comment**: None
- **Journal**: None
- **Summary**: This paper provides a set of sensitivity analysis and activity identification results for a class of convex functions with a strong geometric structure, that we coined "mirror-stratifiable". These functions are such that there is a bijection between a primal and a dual stratification of the space into partitioning sets, called strata. This pairing is crucial to track the strata that are identifiable by solutions of parametrized optimization problems or by iterates of optimization algorithms. This class of functions encompasses all regularizers routinely used in signal and image processing, machine learning, and statistics. We show that this "mirror-stratifiable" structure enjoys a nice sensitivity theory, allowing us to study stability of solutions of optimization problems to small perturbations, as well as activity identification of first-order proximal splitting-type algorithms. Existing results in the literature typically assume that, under a non-degeneracy condition, the active set associated to a minimizer is stable to small perturbations and is identified in finite time by optimization schemes. In contrast, our results do not require any non-degeneracy assumption: in consequence, the optimal active set is not necessarily stable anymore, but we are able to track precisely the set of identifiable strata.We show that these results have crucial implications when solving challenging ill-posed inverse problems via regularization, a typical scenario where the non-degeneracy condition is not fulfilled. Our theoretical results, illustrated by numerical simulations, allow to characterize the instability behaviour of the regularized solutions, by locating the set of all low-dimensional strata that can be potentially identified by these solutions.



### Adversarial training and dilated convolutions for brain MRI segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.03195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03195v1)
- **Published**: 2017-07-11 09:37:11+00:00
- **Updated**: 2017-07-11 09:37:11+00:00
- **Authors**: Pim Moeskops, Mitko Veta, Maxime W. Lafarge, Koen A. J. Eppenhof, Josien P. W. Pluim
- **Comment**: MICCAI 2017 Workshop on Deep Learning in Medical Image Analysis
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been applied to various automatic image segmentation tasks in medical image analysis, including brain MRI segmentation. Generative adversarial networks have recently gained popularity because of their power in generating images that are difficult to distinguish from real images.   In this study we use an adversarial training approach to improve CNN-based brain MRI segmentation. To this end, we include an additional loss function that motivates the network to generate segmentations that are difficult to distinguish from manual segmentations. During training, this loss function is optimised together with the conventional average per-voxel cross entropy loss.   The results show improved segmentation performance using this adversarial training procedure for segmentation of two different sets of images and using two different network architectures, both visually and in terms of Dice coefficients.



### Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations
- **Arxiv ID**: http://arxiv.org/abs/1707.03237v3
- **DOI**: 10.1007/978-3-319-67558-9_28
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03237v3)
- **Published**: 2017-07-11 12:07:52+00:00
- **Updated**: 2017-07-14 16:57:58+00:00
- **Authors**: Carole H Sudre, Wenqi Li, Tom Vercauteren, Sébastien Ourselin, M. Jorge Cardoso
- **Comment**: None
- **Journal**: None
- **Summary**: Deep-learning has proved in recent years to be a powerful tool for image analysis and is now widely used to segment both 2D and 3D medical images. Deep-learning segmentation frameworks rely not only on the choice of network architecture but also on the choice of loss function. When the segmentation process targets rare observations, a severe class imbalance is likely to occur between candidate labels, thus resulting in sub-optimal performance. In order to mitigate this issue, strategies such as the weighted cross-entropy function, the sensitivity function or the Dice loss function, have been proposed. In this work, we investigate the behavior of these loss functions and their sensitivity to learning rate tuning in the presence of different rates of label imbalance across 2D and 3D segmentation tasks. We also propose to use the class re-balancing properties of the Generalized Dice overlap, a known metric for segmentation assessment, as a robust and accurate deep-learning loss function for unbalanced tasks.



### Hierarchical Deep Recurrent Architecture for Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/1707.03296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03296v1)
- **Published**: 2017-07-11 14:25:16+00:00
- **Updated**: 2017-07-11 14:25:16+00:00
- **Authors**: Luming Tang, Boyang Deng, Haiyu Zhao, Shuai Yi
- **Comment**: Accepted as Classification Challenge Track paper in CVPR 2017
  Workshop on YouTube-8M Large-Scale Video Understanding
- **Journal**: None
- **Summary**: This paper introduces the system we developed for the Youtube-8M Video Understanding Challenge, in which a large-scale benchmark dataset was used for multi-label video classification. The proposed framework contains hierarchical deep architecture, including the frame-level sequence modeling part and the video-level classification part. In the frame-level sequence modelling part, we explore a set of methods including Pooling-LSTM (PLSTM), Hierarchical-LSTM (HLSTM), Random-LSTM (RLSTM) in order to address the problem of large amount of frames in a video. We also introduce two attention pooling methods, single attention pooling (ATT) and multiply attention pooling (Multi-ATT) so that we can pay more attention to the informative frames in a video and ignore the useless frames. In the video-level classification part, two methods are proposed to increase the classification performance, i.e. Hierarchical-Mixture-of-Experts (HMoE) and Classifier Chains (CC). Our final submission is an ensemble consisting of 18 sub-models. In terms of the official evaluation metric Global Average Precision (GAP) at 20, our best submission achieves 0.84346 on the public 50% of test dataset and 0.84333 on the private 50% of test data.



### Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation
- **Arxiv ID**: http://arxiv.org/abs/1707.03374v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1707.03374v2)
- **Published**: 2017-07-11 17:23:53+00:00
- **Updated**: 2018-06-18 21:00:13+00:00
- **Authors**: YuXuan Liu, Abhishek Gupta, Pieter Abbeel, Sergey Levine
- **Comment**: Accepted at ICRA 2018, Brisbane. YuXuan Liu and Abhishek Gupta had
  equal contribution
- **Journal**: None
- **Summary**: Imitation learning is an effective approach for autonomous systems to acquire control policies when an explicit reward function is unavailable, using supervision provided as demonstrations from an expert, typically a human operator. However, standard imitation learning methods assume that the agent receives examples of observation-action tuples that could be provided, for instance, to a supervised learning algorithm. This stands in contrast to how humans and animals imitate: we observe another person performing some behavior and then figure out which actions will realize that behavior, compensating for changes in viewpoint, surroundings, object positions and types, and other factors. We term this kind of imitation learning "imitation-from-observation," and propose an imitation learning method based on video prediction with context translation and deep reinforcement learning. This lifts the assumption in imitation learning that the demonstration should consist of observations in the same environment configuration, and enables a variety of interesting applications, including learning robotic skills that involve tool use simply by observing videos of human tool use. Our experimental results show the effectiveness of our approach in learning a wide range of real-world robotic tasks modeled after common household chores from videos of a human demonstrator, including sweeping, ladling almonds, pushing objects as well as a number of tasks in simulation.



### Learning the Latent "Look": Unsupervised Discovery of a Style-Coherent Embedding from Fashion Images
- **Arxiv ID**: http://arxiv.org/abs/1707.03376v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03376v2)
- **Published**: 2017-07-11 17:28:59+00:00
- **Updated**: 2017-08-03 05:10:52+00:00
- **Authors**: Wei-Lin Hsiao, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: What defines a visual style? Fashion styles emerge organically from how people assemble outfits of clothing, making them difficult to pin down with a computational model. Low-level visual similarity can be too specific to detect stylistically similar images, while manually crafted style categories can be too abstract to capture subtle style differences. We propose an unsupervised approach to learn a style-coherent representation. Our method leverages probabilistic polylingual topic models based on visual attributes to discover a set of latent style factors. Given a collection of unlabeled fashion images, our approach mines for the latent styles, then summarizes outfits by how they mix those styles. Our approach can organize galleries of outfits by style without requiring any style labels. Experiments on over 100K images demonstrate its promise for retrieving, mixing, and summarizing fashion images by their style.



### A step towards procedural terrain generation with GANs
- **Arxiv ID**: http://arxiv.org/abs/1707.03383v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.03383v1)
- **Published**: 2017-07-11 17:44:20+00:00
- **Updated**: 2017-07-11 17:44:20+00:00
- **Authors**: Christopher Beckham, Christopher Pal
- **Comment**: None
- **Journal**: None
- **Summary**: Procedural terrain generation for video games has been traditionally been done with smartly designed but handcrafted algorithms that generate heightmaps. We propose a first step toward the learning and synthesis of these using recent advances in deep generative modelling with openly available satellite imagery from NASA.



### Creatism: A deep-learning photographer capable of creating professional work
- **Arxiv ID**: http://arxiv.org/abs/1707.03491v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03491v1)
- **Published**: 2017-07-11 23:18:50+00:00
- **Updated**: 2017-07-11 23:18:50+00:00
- **Authors**: Hui Fang, Meng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Machine-learning excels in many areas with well-defined goals. However, a clear goal is usually not available in art forms, such as photography. The success of a photograph is measured by its aesthetic value, a very subjective concept. This adds to the challenge for a machine learning approach.   We introduce Creatism, a deep-learning system for artistic content creation. In our system, we break down aesthetics into multiple aspects, each can be learned individually from a shared dataset of professional examples. Each aspect corresponds to an image operation that can be optimized efficiently. A novel editing tool, dramatic mask, is introduced as one operation that improves dramatic lighting for a photo. Our training does not require a dataset with before/after image pairs, or any additional labels to indicate different aspects in aesthetics.   Using our system, we mimic the workflow of a landscape photographer, from framing for the best composition to carrying out various post-processing operations. The environment for our virtual photographer is simulated by a collection of panorama images from Google Street View. We design a "Turing-test"-like experiment to objectively measure quality of its creations, where professional photographers rate a mixture of photographs from different sources blindly. Experiments show that a portion of our robot's creation can be confused with professional work.



