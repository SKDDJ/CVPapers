# Arxiv Papers in cs.CV on 2017-07-22
### Automatic Curation of Golf Highlights using Multimodal Excitement Features
- **Arxiv ID**: http://arxiv.org/abs/1707.07075v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1707.07075v1)
- **Published**: 2017-07-22 00:06:50+00:00
- **Updated**: 2017-07-22 00:06:50+00:00
- **Authors**: Michele Merler, Dhiraj Joshi, Quoc-Bao Nguyen, Stephen Hammer, John Kent, John R. Smith, Rogerio S. Feris
- **Comment**: None
- **Journal**: None
- **Summary**: The production of sports highlight packages summarizing a game's most exciting moments is an essential task for broadcast media. Yet, it requires labor-intensive video editing. We propose a novel approach for auto-curating sports highlights, and use it to create a real-world system for the editorial aid of golf highlight reels. Our method fuses information from the players' reactions (action recognition such as high-fives and fist pumps), spectators (crowd cheering), and commentator (tone of the voice and word analysis) to determine the most interesting moments of a game. We accurately identify the start and end frames of key shot highlights with additional metadata, such as the player's name and the hole number, allowing personalized content summarization and retrieval. In addition, we introduce new techniques for learning our classifiers with reduced manual training data annotation by exploiting the correlation of different modalities. Our work has been demonstrated at a major golf tournament, successfully extracting highlights from live video streams over four consecutive days.



### Motion Compensated Dynamic MRI Reconstruction with Local Affine Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1707.07089v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07089v3)
- **Published**: 2017-07-22 02:23:21+00:00
- **Updated**: 2019-02-14 00:00:52+00:00
- **Authors**: Ningning Zhao, Daniel O'Connor, Adrian Basarab, Dan Ruan, Peng Hu, Ke Sheng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel framework to reconstruct the dynamic magnetic resonance images (DMRI) with motion compensation (MC). Due to the inherent motion effects during DMRI acquisition, reconstruction of DMRI using motion estimation/compensation (ME/MC) has been studied under a compressed sensing (CS) scheme. In this paper, by embedding the intensity-based optical flow (OF) constraint into the traditional CS scheme, we are able to couple the DMRI reconstruction with motion field estimation. The formulated optimization problem is solved by a primal-dual algorithm with linesearch due to its efficiency when dealing with non-differentiable problems. With the estimated motion field, the DMRI reconstruction is refined through MC. By employing the multi-scale coarse-to-fine strategy, we are able to update the variables(temporal image sequences and motion vectors) and to refine the image reconstruction alternately. Moreover, the proposed framework is capable of handling a wide class of prior information (regularizations) for DMRI reconstruction, such as sparsity, low rank and total variation. Experiments on various DMRI data, ranging from in vivo lung to cardiac dataset, validate the reconstruction quality improvement using the proposed scheme in comparison to several state-of-the-art algorithms.



### OBJ2TEXT: Generating Visually Descriptive Language from Object Layouts
- **Arxiv ID**: http://arxiv.org/abs/1707.07102v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1707.07102v1)
- **Published**: 2017-07-22 04:17:42+00:00
- **Updated**: 2017-07-22 04:17:42+00:00
- **Authors**: Xuwang Yin, Vicente Ordonez
- **Comment**: Accepted at EMNLP 2017
- **Journal**: None
- **Summary**: Generating captions for images is a task that has recently received considerable attention. In this work we focus on caption generation for abstract scenes, or object layouts where the only information provided is a set of objects and their locations. We propose OBJ2TEXT, a sequence-to-sequence model that encodes a set of objects and their locations as an input sequence using an LSTM network, and decodes this representation using an LSTM language model. We show that our model, despite encoding object layouts as a sequence, can represent spatial relationships between objects, and generate descriptions that are globally coherent and semantically relevant. We test our approach in a task of object-layout captioning by using only object annotations as inputs. We additionally show that our model, combined with a state-of-the-art object detector, improves an image captioning model from 0.863 to 0.950 (CIDEr score) in the test benchmark of the standard MS-COCO Captioning task.



### PatchShuffle Regularization
- **Arxiv ID**: http://arxiv.org/abs/1707.07103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07103v1)
- **Published**: 2017-07-22 04:33:02+00:00
- **Updated**: 2017-07-22 04:33:02+00:00
- **Authors**: Guoliang Kang, Xuanyi Dong, Liang Zheng, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on regularizing the training of the convolutional neural network (CNN). We propose a new regularization approach named ``PatchShuffle`` that can be adopted in any classification-oriented CNN models. It is easy to implement: in each mini-batch, images or feature maps are randomly chosen to undergo a transformation such that pixels within each local patch are shuffled. Through generating images and feature maps with interior orderless patches, PatchShuffle creates rich local variations, reduces the risk of network overfitting, and can be viewed as a beneficial supplement to various kinds of training regularization techniques, such as weight decay, model ensemble and dropout. Experiments on four representative classification datasets show that PatchShuffle improves the generalization ability of CNN especially when the data is scarce. Moreover, we empirically illustrate that CNN models trained with PatchShuffle are more robust to noise and local changes in an image.



### Deep Networks for Compressed Image Sensing
- **Arxiv ID**: http://arxiv.org/abs/1707.07119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07119v1)
- **Published**: 2017-07-22 08:22:18+00:00
- **Updated**: 2017-07-22 08:22:18+00:00
- **Authors**: Wuzhen Shi, Feng Jiang, Shengping Zhang, Debin Zhao
- **Comment**: This paper has been accepted by the IEEE International Conference on
  Multimedia and Expo (ICME) 2017
- **Journal**: None
- **Summary**: The compressed sensing (CS) theory has been successfully applied to image compression in the past few years as most image signals are sparse in a certain domain. Several CS reconstruction models have been recently proposed and obtained superior performance. However, there still exist two important challenges within the CS theory. The first one is how to design a sampling mechanism to achieve an optimal sampling efficiency, and the second one is how to perform the reconstruction to get the highest quality to achieve an optimal signal recovery. In this paper, we try to deal with these two problems with a deep network. First of all, we train a sampling matrix via the network training instead of using a traditional manually designed one, which is much appropriate for our deep network based reconstruct process. Then, we propose a deep network to recover the image, which imitates traditional compressed sensing reconstruction processes. Experimental results demonstrate that our deep networks based CS reconstruction method offers a very significant quality improvement compared against state of the art ones.



### Single Image Super-Resolution with Dilated Convolution based Multi-Scale Information Learning Inception Module
- **Arxiv ID**: http://arxiv.org/abs/1707.07128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07128v1)
- **Published**: 2017-07-22 09:27:04+00:00
- **Updated**: 2017-07-22 09:27:04+00:00
- **Authors**: Wuzhen Shi, Feng Jiang, Debin Zhao
- **Comment**: This paper has been accepted by the IEEE International Conference on
  Image Processing (ICIP) 2017
- **Journal**: None
- **Summary**: Traditional works have shown that patches in a natural image tend to redundantly recur many times inside the image, both within the same scale, as well as across different scales. Make full use of these multi-scale information can improve the image restoration performance. However, the current proposed deep learning based restoration methods do not take the multi-scale information into account. In this paper, we propose a dilated convolution based inception module to learn multi-scale information and design a deep network for single image super-resolution. Different dilated convolution learns different scale feature, then the inception module concatenates all these features to fuse multi-scale information. In order to increase the reception field of our network to catch more contextual information, we cascade multiple inception modules to constitute a deep network to conduct single image super-resolution. With the novel dilated convolution based inception module, the proposed end-to-end single image super-resolution network can take advantage of multi-scale information to improve image super-resolution performance. Experimental results show that our proposed method outperforms many state-of-the-art single image super-resolution methods.



### Clinical Patient Tracking in the Presence of Transient and Permanent Occlusions via Geodesic Feature
- **Arxiv ID**: http://arxiv.org/abs/1707.07139v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.07139v2)
- **Published**: 2017-07-22 11:12:18+00:00
- **Updated**: 2017-07-25 00:29:31+00:00
- **Authors**: Kun Li, Joel W. Burdick
- **Comment**: None
- **Journal**: None
- **Summary**: This paper develops a method to use RGB-D cameras to track the motions of a human spinal cord injury patient undergoing spinal stimulation and physical rehabilitation. Because clinicians must remain close to the patient during training sessions, the patient is usually under permanent and transient occlusions due to the training equipment and the movements of the attending clinicians. These occlusions can significantly degrade the accuracy of existing human tracking methods. To improve the data association problem in these circumstances, we present a new global feature based on the geodesic distances of surface mesh points to a set of anchor points. Transient occlusions are handled via a multi-hypothesis tracking framework. To evaluate the method, we simulated different occlusion sizes on a data set captured from a human in varying movement patterns, and compared the proposed feature with other tracking methods. The results show that the proposed method achieves robustness to both surface deformations and transient occlusions.



### Multi-Oriented Text Detection and Verification in Video Frames and Scene Images
- **Arxiv ID**: http://arxiv.org/abs/1707.07150v2
- **DOI**: 10.1016/j.neucom.2017.09.089
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07150v2)
- **Published**: 2017-07-22 12:09:28+00:00
- **Updated**: 2017-10-04 16:48:08+00:00
- **Authors**: Aneeshan Sain, Ayan Kumar Bhunia, Partha Pratim Roy, Umapada Pal
- **Comment**: Accepted in Neurocomputing, Elsevier
- **Journal**: None
- **Summary**: In this paper, we bring forth a novel approach of video text detection using Fourier-Laplacian filtering in the frequency domain that includes a verification technique using Hidden Markov Model (HMM). The proposed approach deals with the text region appearing not only in horizontal or vertical directions, but also in any other oblique or curved orientation in the image. Until now only a few methods have been proposed that look into curved text detection in video frames, wherein lies our novelty. In our approach, we first apply Fourier-Laplacian transform on the image followed by an ideal Laplacian-Gaussian filtering. Thereafter K-means clustering is employed to obtain the asserted text areas depending on a maximum difference map. Next, the obtained connected components (CC) are skeletonized to distinguish various text strings. Complex components are disintegrated into simpler ones according to a junction removal algorithm followed by a concatenation performed on possible combination of the disjoint skeletons to obtain the corresponding text area. Finally these text hypotheses are verified using HMM-based text/non-text classification system. False positives are thus eliminated giving us a robust text detection performance. We have tested our framework in multi-oriented text lines in four scripts, namely, English, Chinese, Devanagari and Bengali, in video frames and scene texts. The results obtained show that proposed approach surpasses existing methods on text detection.



### Single-Shot Clothing Category Recognition in Free-Configurations with Application to Autonomous Clothes Sorting
- **Arxiv ID**: http://arxiv.org/abs/1707.07157v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.07157v1)
- **Published**: 2017-07-22 13:06:24+00:00
- **Updated**: 2017-07-22 13:06:24+00:00
- **Authors**: Li Sun, Gerardo Aragon-Camarasa, Simon Rogers, Rustam Stolkin, J. Paul Siebert
- **Comment**: 9 pages, accepted by IROS2017
- **Journal**: None
- **Summary**: This paper proposes a single-shot approach for recognising clothing categories from 2.5D features. We propose two visual features, BSP (B-Spline Patch) and TSD (Topology Spatial Distances) for this task. The local BSP features are encoded by LLC (Locality-constrained Linear Coding) and fused with three different global features. Our visual feature is robust to deformable shapes and our approach is able to recognise the category of unknown clothing in unconstrained and random configurations. We integrated the category recognition pipeline with a stereo vision system, clothing instance detection, and dual-arm manipulators to achieve an autonomous sorting system. To verify the performance of our proposed method, we build a high-resolution RGBD clothing dataset of 50 clothing items of 5 categories sampled in random configurations (a total of 2,100 clothing samples). Experimental results show that our approach is able to reach 83.2\% accuracy while classifying clothing items which were previously unseen during training. This advances beyond the previous state-of-the-art by 36.2\%. Finally, we evaluate the proposed approach in an autonomous robot sorting system, in which the robot recognises a clothing item from an unconstrained pile, grasps it, and sorts it into a box according to its category. Our proposed sorting system achieves reasonable sorting success rates with single-shot perception.



### Coarse-to-Fine Lifted MAP Inference in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1707.07165v1
- **DOI**: 10.24963/ijcai.2017/641
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07165v1)
- **Published**: 2017-07-22 13:45:28+00:00
- **Updated**: 2017-07-22 13:45:28+00:00
- **Authors**: Haroun Habeeb, Ankit Anand, Mausam, Parag Singla
- **Comment**: Published in IJCAI 2017
- **Journal**: None
- **Summary**: There is a vast body of theoretical research on lifted inference in probabilistic graphical models (PGMs). However, few demonstrations exist where lifting is applied in conjunction with top of the line applied algorithms. We pursue the applicability of lifted inference for computer vision (CV), with the insight that a globally optimal (MAP) labeling will likely have the same label for two symmetric pixels. The success of our approach lies in efficiently handling a distinct unary potential on every node (pixel), typical of CV applications. This allows us to lift the large class of algorithms that model a CV problem via PGM inference. We propose a generic template for coarse-to-fine (C2F) inference in CV, which progressively refines an initial coarsely lifted PGM for varying quality-time trade-offs. We demonstrate the performance of C2F inference by developing lifted versions of two near state-of-the-art CV algorithms for stereo vision and interactive image segmentation. We find that, against flat algorithms, the lifted versions have a much superior anytime performance, without any loss in final solution quality.



### Comparing Apples and Oranges: Off-Road Pedestrian Detection on the NREC Agricultural Person-Detection Dataset
- **Arxiv ID**: http://arxiv.org/abs/1707.07169v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07169v2)
- **Published**: 2017-07-22 14:16:36+00:00
- **Updated**: 2017-10-26 17:22:37+00:00
- **Authors**: Zachary Pezzementi, Trenton Tabor, Peiyun Hu, Jonathan K. Chang, Deva Ramanan, Carl Wellington, Benzun P. Wisely Babu, Herman Herman
- **Comment**: Accepted to Journal of Field Robotics
- **Journal**: None
- **Summary**: Person detection from vehicles has made rapid progress recently with the advent of multiple highquality datasets of urban and highway driving, yet no large-scale benchmark is available for the same problem in off-road or agricultural environments. Here we present the NREC Agricultural Person-Detection Dataset to spur research in these environments. It consists of labeled stereo video of people in orange and apple orchards taken from two perception platforms (a tractor and a pickup truck), along with vehicle position data from RTK GPS. We define a benchmark on part of the dataset that combines a total of 76k labeled person images and 19k sampled person-free images. The dataset highlights several key challenges of the domain, including varying environment, substantial occlusion by vegetation, people in motion and in non-standard poses, and people seen from a variety of distances; meta-data are included to allow targeted evaluation of each of these effects. Finally, we present baseline detection performance results for three leading approaches from urban pedestrian detection and our own convolutional neural network approach that benefits from the incorporation of additional image context. We show that the success of existing approaches on urban data does not transfer directly to this domain.



### Emotion Recognition by Body Movement Representation on the Manifold of Symmetric Positive Definite Matrices
- **Arxiv ID**: http://arxiv.org/abs/1707.07180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07180v1)
- **Published**: 2017-07-22 15:55:56+00:00
- **Updated**: 2017-07-22 15:55:56+00:00
- **Authors**: Mohamed Daoudi, Stefano Berretti, Pietro Pala, Yvonne Delevoye, Alberto Del Bimbo
- **Comment**: accepted in I19th International Conference on Image Analysis and
  processing (ICIAP), 11-15 september Catania, Italy, 2017
- **Journal**: None
- **Summary**: Emotion recognition is attracting great interest for its potential application in a multitude of real-life situations. Much of the Computer Vision research in this field has focused on relating emotions to facial expressions, with investigations rarely including more than upper body. In this work, we propose a new scenario, for which emotional states are related to 3D dynamics of the whole body motion. To address the complexity of human body movement, we used covariance descriptors of the sequence of the 3D skeleton joints, and represented them in the non-linear Riemannian manifold of Symmetric Positive Definite matrices. In doing so, we exploited geodesic distances and geometric means on the manifold to perform emotion classification. Using sequences of spontaneous walking under the five primary emotional states, we report a method that succeeded in classifying the different emotions, with comparable performance to those observed in a human-based force-choice classification task.



### A survey of exemplar-based texture synthesis
- **Arxiv ID**: http://arxiv.org/abs/1707.07184v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07184v2)
- **Published**: 2017-07-22 16:08:49+00:00
- **Updated**: 2017-11-24 15:57:10+00:00
- **Authors**: Lara Raad, Axel Davy, Agnès Desolneux, Jean-Michel Morel
- **Comment**: v2: Added comments and typos fixes. New section added to describe
  FRAME. New method presented: CNNMRF
- **Journal**: None
- **Summary**: Exemplar-based texture synthesis is the process of generating, from an input sample, new texture images of arbitrary size and which are perceptually equivalent to the sample. The two main approaches are statistics-based methods and patch re-arrangement methods. In the first class, a texture is characterized by a statistical signature; then, a random sampling conditioned to this signature produces genuinely different texture images. The second class boils down to a clever "copy-paste" procedure, which stitches together large regions of the sample. Hybrid methods try to combine ideas from both approaches to avoid their hurdles. The recent approaches using convolutional neural networks fit to this classification, some being statistical and others performing patch re-arrangement in the feature space. They produce impressive synthesis on various kinds of textures. Nevertheless, we found that most real textures are organized at multiple scales, with global structures revealed at coarse scales and highly varying details at finer ones. Thus, when confronted with large natural images of textures the results of state-of-the-art methods degrade rapidly, and the problem of modeling them remains wide open.



### An Event-based Fast Movement Detection Algorithm for a Positioning Robot Using POWERLINK Communication
- **Arxiv ID**: http://arxiv.org/abs/1707.07188v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.07188v1)
- **Published**: 2017-07-22 16:20:37+00:00
- **Updated**: 2017-07-22 16:20:37+00:00
- **Authors**: Juan Barrios-Avilés, Taras Iakymchuk, Jorge Samaniego, Alfredo Rosado-Muñoz
- **Comment**: Videos of assembly and live system action:
  https://youtu.be/KjkawFHd9_0 https://youtu.be/5X8d1Gw2Eco
  https://youtu.be/Ou9ngd9pZng https://youtu.be/UUgifzsseHQ
  https://youtu.be/L7G84E7jcoY https://youtu.be/RVOU5G1V7Io
  https://youtu.be/bmXIbk8I5sk
- **Journal**: None
- **Summary**: This work develops a tracking system based on an event-based camera. A bioinspired filtering algorithm to reduce noise and transmitted data while keeping the main features at the scene is implemented in FPGA which also serves as a network node. POWERLINK IEEE 61158 industrial network is used to communicate the FPGA with a controller connected to a self-developed two axis servo-controlled robot. The FPGA includes the network protocol to integrate the event-based camera as any other existing network node. The inverse kinematics for the robot is included in the controller. In addition, another network node is used to control pneumatic valves blowing the ball at different speed and trajectories. To complete the system and provide a comparison, a traditional frame-based camera is also connected to the controller. The imaging data for the tracking system are obtained either from the event-based or frame-based camera. Results show that the robot can accurately follow the ball using fast image recognition, with the intrinsic advantages of the event-based system (size, price, power). This works shows how the development of new equipment and algorithms can be efficiently integrated in an industrial system, merging commercial industrial equipment with the new devices so that new technologies can rapidly enter into the industrial field.



### Eyemotion: Classifying facial expressions in VR using eye-tracking cameras
- **Arxiv ID**: http://arxiv.org/abs/1707.07204v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07204v2)
- **Published**: 2017-07-22 19:39:19+00:00
- **Updated**: 2017-07-28 19:05:50+00:00
- **Authors**: Steven Hickson, Nick Dufour, Avneesh Sud, Vivek Kwatra, Irfan Essa
- **Comment**: Uploaded Supplementary PDF. Fixed author affiliation. Corrected typo
  in personalization accuracy
- **Journal**: None
- **Summary**: One of the main challenges of social interaction in virtual reality settings is that head-mounted displays occlude a large portion of the face, blocking facial expressions and thereby restricting social engagement cues among users. Hence, auxiliary means of sensing and conveying these expressions are needed. We present an algorithm to automatically infer expressions by analyzing only a partially occluded face while the user is engaged in a virtual reality experience. Specifically, we show that images of the user's eyes captured from an IR gaze-tracking camera within a VR headset are sufficient to infer a select subset of facial expressions without the use of any fixed external camera. Using these inferences, we can generate dynamic avatars in real-time which function as an expressive surrogate for the user. We propose a novel data collection pipeline as well as a novel approach for increasing CNN accuracy via personalization. Our results show a mean accuracy of 74% ($F1$ of 0.73) among 5 `emotive' expressions and a mean accuracy of 70% ($F1$ of 0.68) among 10 distinct facial action units, outperforming human raters.



### Inspiring Computer Vision System Solutions
- **Arxiv ID**: http://arxiv.org/abs/1707.07210v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1707.07210v1)
- **Published**: 2017-07-22 20:20:57+00:00
- **Updated**: 2017-07-22 20:20:57+00:00
- **Authors**: Julian Zilly, Amit Boyarski, Micael Carvalho, Amir Atapour Abarghouei, Konstantinos Amplianitis, Aleksandr Krasnov, Massimiliano Mancini, Hernán Gonzalez, Riccardo Spezialetti, Carlos Sampedro Pérez, Hao Li
- **Comment**: 5 pages. 3 figures
- **Journal**: None
- **Summary**: The "digital Michelangelo project" was a seminal computer vision project in the early 2000's that pushed the capabilities of acquisition systems and involved multiple people from diverse fields, many of whom are now leaders in industry and academia. Reviewing this project with modern eyes provides us with the opportunity to reflect on several issues, relevant now as then to the field of computer vision and research in general, that go beyond the technical aspects of the work.   This article was written in the context of a reading group competition at the week-long International Computer Vision Summer School 2017 (ICVSS) on Sicily, Italy. To deepen the participants understanding of computer vision and to foster a sense of community, various reading groups were tasked to highlight important lessons which may be learned from provided literature, going beyond the contents of the paper. This report is the winning entry of this guided discourse (Fig. 1). The authors closely examined the origins, fruits and most importantly lessons about research in general which may be distilled from the "digital Michelangelo project". Discussions leading to this report were held within the group as well as with Hao Li, the group mentor.



### Spatio-temporal Human Action Localisation and Instance Segmentation in Temporally Untrimmed Videos
- **Arxiv ID**: http://arxiv.org/abs/1707.07213v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07213v2)
- **Published**: 2017-07-22 20:46:11+00:00
- **Updated**: 2017-08-06 15:22:59+00:00
- **Authors**: Suman Saha, Gurkirt Singh, Michael Sapienza, Philip H. S. Torr, Fabio Cuzzolin
- **Comment**: Typos corrected
- **Journal**: None
- **Summary**: Current state-of-the-art human action recognition is focused on the classification of temporally trimmed videos in which only one action occurs per frame. In this work we address the problem of action localisation and instance segmentation in which multiple concurrent actions of the same class may be segmented out of an image sequence. We cast the action tube extraction as an energy maximisation problem in which configurations of region proposals in each frame are assigned a cost and the best action tubes are selected via two passes of dynamic programming. One pass associates region proposals in space and time for each action category, and another pass is used to solve for the tube's temporal extent and to enforce a smooth label sequence through the video. In addition, by taking advantage of recent work on action foreground-background segmentation, we are able to associate each tube with class-specific segmentations. We demonstrate the performance of our algorithm on the challenging LIRIS-HARL dataset and achieve a new state-of-the-art result which is 14.3 times better than previous methods.



### SAR Image Colorization: Converting Single-Polarization to Fully Polarimetric Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.07225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07225v1)
- **Published**: 2017-07-22 22:22:25+00:00
- **Updated**: 2017-07-22 22:22:25+00:00
- **Authors**: Qian Song, Feng Xu, Ya-Qiu Jin
- **Comment**: None
- **Journal**: None
- **Summary**: A deep neural networks based method is proposed to convert single polarization grayscale SAR image to fully polarimetric. It consists of two components: a feature extractor network to extract hierarchical multi-scale spatial features of grayscale SAR image, followed by a feature translator network to map spatial feature to polarimetric feature with which the polarimetric covariance matrix of each pixel can be reconstructed. Both qualitative and quantitative experiments with real fully polarimetric data are conducted to show the efficacy of the proposed method. The reconstructed full-pol SAR image agrees well with the true full-pol image. Existing PolSAR applications such as model-based decomposition and unsupervised classification can be applied directly to the reconstructed full-pol SAR images. This framework can be easily extended to reconstruction of full-pol data from compact-pol data. The experiment results also show that the proposed method could be potentially used for interference removal on the cross-polarization channel.



