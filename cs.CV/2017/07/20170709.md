# Arxiv Papers in cs.CV on 2017-07-09
### Visual Analytics of Movement Pattern Based on Time-Spatial Data: A Neural Net Approach
- **Arxiv ID**: http://arxiv.org/abs/1707.02554v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02554v1)
- **Published**: 2017-07-09 10:19:04+00:00
- **Updated**: 2017-07-09 10:19:04+00:00
- **Authors**: Zhenghao Chen, Jianlong Zhou, Xiuying Wang
- **Comment**: submitted to ICONIP 2017
- **Journal**: None
- **Summary**: Time-Spatial data plays a crucial role for different fields such as traffic management. These data can be collected via devices such as surveillance sensors or tracking systems. However, how to efficiently an- alyze and visualize these data to capture essential embedded pattern information is becoming a big challenge today. Classic visualization ap- proaches focus on revealing 2D and 3D spatial information and modeling statistical test. Those methods would easily fail when data become mas- sive. Recent attempts concern on how to simply cluster data and perform prediction with time-oriented information. However, those approaches could still be further enhanced as they also have limitations for han- dling massive clusters and labels. In this paper, we propose a visualiza- tion methodology for mobility data using artificial neural net techniques. This method aggregates three main parts that are Back-end Data Model, Neural Net Algorithm including clustering method Self-Organizing Map (SOM) and prediction approach Recurrent Neural Net (RNN) for ex- tracting the features and lastly a solid front-end that displays the results to users with an interactive system. SOM is able to cluster the visiting patterns and detect the abnormal pattern. RNN can perform the predic- tion for time series analysis using its dynamic architecture. Furthermore, an interactive system will enable user to interpret the result with graph- ics, animation and 3D model for a close-loop feedback. This method can be particularly applied in two tasks that Commercial-based Promotion and abnormal traffic patterns detection.



### Class-Weighted Convolutional Features for Visual Instance Search
- **Arxiv ID**: http://arxiv.org/abs/1707.02581v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1707.02581v1)
- **Published**: 2017-07-09 13:51:45+00:00
- **Updated**: 2017-07-09 13:51:45+00:00
- **Authors**: Albert Jimenez, Jose M. Alvarez, Xavier Giro-i-Nieto
- **Comment**: To appear in the British Machine Vision Conference (BMVC), September
  2017
- **Journal**: None
- **Summary**: Image retrieval in realistic scenarios targets large dynamic datasets of unlabeled images. In these cases, training or fine-tuning a model every time new images are added to the database is neither efficient nor scalable. Convolutional neural networks trained for image classification over large datasets have been proven effective feature extractors for image retrieval. The most successful approaches are based on encoding the activations of convolutional layers, as they convey the image spatial information. In this paper, we go beyond this spatial information and propose a local-aware encoding of convolutional features based on semantic information predicted in the target image. To this end, we obtain the most discriminative regions of an image using Class Activation Maps (CAMs). CAMs are based on the knowledge contained in the network and therefore, our approach, has the additional advantage of not requiring external information. In addition, we use CAMs to generate object proposals during an unsupervised re-ranking stage after a first fast search. Our experiments on two public available datasets for instance retrieval, Oxford5k and Paris6k, demonstrate the competitiveness of our approach outperforming the current state-of-the-art when using off-the-shelf models trained on ImageNet. The source code and model used in this paper are publicly available at http://imatge-upc.github.io/retrieval-2017-cam/.



### Detection of bimanual gestures everywhere: why it matters, what we need and what is missing
- **Arxiv ID**: http://arxiv.org/abs/1707.02605v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T05
- **Links**: [PDF](http://arxiv.org/pdf/1707.02605v1)
- **Published**: 2017-07-09 17:30:35+00:00
- **Updated**: 2017-07-09 17:30:35+00:00
- **Authors**: Divya Shah, Ernesto Denicia, Tiago Pimentel, Barbara Bruno, Fulvio Mastrogiovanni
- **Comment**: Submitted to Robotics and Autonomous Systems (Elsevier)
- **Journal**: None
- **Summary**: Bimanual gestures are of the utmost importance for the study of motor coordination in humans and in everyday activities. A reliable detection of bimanual gestures in unconstrained environments is fundamental for their clinical study and to assess common activities of daily living. This paper investigates techniques for a reliable, unconstrained detection and classification of bimanual gestures. It assumes the availability of inertial data originating from the two hands/arms, builds upon a previously developed technique for gesture modelling based on Gaussian Mixture Modelling (GMM) and Gaussian Mixture Regression (GMR), and compares different modelling and classification techniques, which are based on a number of assumptions inspired by literature about how bimanual gestures are represented and modelled in the brain. Experiments show results related to 5 everyday bimanual activities, which have been selected on the basis of three main parameters: (not) constraining the two hands by a physical tool, (not) requiring a specific sequence of single-hand gestures, being recursive (or not). In the best performing combination of modeling approach and classification technique, five out of five activities are recognized up to an accuracy of 97%, a precision of 82% and a level of recall of 100%.



### Local Activity-tuned Image Filtering for Noise Removal and Image Smoothing
- **Arxiv ID**: http://arxiv.org/abs/1707.02637v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02637v4)
- **Published**: 2017-07-09 21:07:05+00:00
- **Updated**: 2017-11-18 06:45:46+00:00
- **Authors**: Lijun Zhao, Jie Liang, Huihui Bai, Lili Meng, Anhong Wang, Yao Zhao
- **Comment**: 13 papers, 9 figures
- **Journal**: None
- **Summary**: In this paper, two local activity-tuned filtering frameworks are proposed for noise removal and image smoothing, where the local activity measurement is given by the clipped and normalized local variance or standard deviation. The first framework is a modified anisotropic diffusion for noise removal of piece-wise smooth image. The second framework is a local activity-tuned Relative Total Variation (LAT-RTV) method for image smoothing. Both frameworks employ the division of gradient and the local activity measurement to achieve noise removal. In addition, to better capture local information, the proposed LAT-RTV uses the product of gradient and local activity measurement to boost the performance of image smoothing. Experimental results are presented to demonstrate the efficiency of the proposed methods on various applications, including depth image filtering, clip-art compression artifact removal, image smoothing, and image denoising.



### Integration of LiDAR and Hyperspectral Data for Land-cover Classification: A Case Study
- **Arxiv ID**: http://arxiv.org/abs/1707.02642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02642v1)
- **Published**: 2017-07-09 21:35:54+00:00
- **Updated**: 2017-07-09 21:35:54+00:00
- **Authors**: Pedram Ghamisi, Gabriele Cavallaro, Dan, Wu, Jon Atli Benediktsson, Antonio Plaza
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, an approach is proposed to fuse LiDAR and hyperspectral data, which considers both spectral and spatial information in a single framework. Here, an extended self-dual attribute profile (ESDAP) is investigated to extract spatial information from a hyperspectral data set. To extract spectral information, a few well-known classifiers have been used such as support vector machines (SVMs), random forests (RFs), and artificial neural networks (ANNs). The proposed method accurately classify the relatively volumetric data set in a few CPU processing time in a real ill-posed situation where there is no balance between the number of training samples and the number of features. The classification part of the proposed approach is fully-automatic.



### A Human and Group Behaviour Simulation Evaluation Framework utilising Composition and Video Analysis
- **Arxiv ID**: http://arxiv.org/abs/1707.02655v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02655v3)
- **Published**: 2017-07-09 22:58:39+00:00
- **Updated**: 2018-11-24 02:35:22+00:00
- **Authors**: Rob Dupre, Vasileios Argyriou
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we present the modular Crowd Simulation Evaluation through Composition framework (CSEC) which provides a quantitative comparison between different pedestrian and crowd simulation approaches. Evaluation is made based on the comparison of source footage against synthetic video created through novel composition techniques. The proposed framework seeks to reduce the complexity of simulation evaluation and provide a platform from which the comparison of differing simulation algorithms as well as parametric tuning can be conducted to improve simulation accuracy or providing measures of similarity between crowd simulation algorithms and source data. Through the use of features designed to mimic the Human Visual System (HVS), specific simulation properties can be evaluated relative to sample footage. Validation was performed on a number of popular crowd datasets and through comparisons of multiple pedestrian and crowd simulation algorithms.



