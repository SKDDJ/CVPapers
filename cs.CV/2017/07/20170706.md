# Arxiv Papers in cs.CV on 2017-07-06
### Multimedia Semantic Integrity Assessment Using Joint Embedding Of Images And Text
- **Arxiv ID**: http://arxiv.org/abs/1707.01606v4
- **DOI**: 10.1145/3123266.3123385
- **Categories**: **cs.MM**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.01606v4)
- **Published**: 2017-07-06 01:25:17+00:00
- **Updated**: 2018-06-29 00:34:27+00:00
- **Authors**: Ayush Jaiswal, Ekraam Sabir, Wael AbdAlmageed, Premkumar Natarajan
- **Comment**: *Ayush Jaiswal and Ekraam Sabir contributed equally to the work in
  this paper
- **Journal**: In Proceedings of the 2017 ACM on Multimedia Conference, pp.
  1465-1471. ACM, 2017
- **Summary**: Real world multimedia data is often composed of multiple modalities such as an image or a video with associated text (e.g. captions, user comments, etc.) and metadata. Such multimodal data packages are prone to manipulations, where a subset of these modalities can be altered to misrepresent or repurpose data packages, with possible malicious intent. It is, therefore, important to develop methods to assess or verify the integrity of these multimedia packages. Using computer vision and natural language processing methods to directly compare the image (or video) and the associated caption to verify the integrity of a media package is only possible for a limited set of objects and scenes. In this paper, we present a novel deep learning-based approach for assessing the semantic integrity of multimedia packages containing images and captions, using a reference set of multimedia packages. We construct a joint embedding of images and captions with deep multimodal representation learning on the reference dataset in a framework that also provides image-caption consistency scores (ICCSs). The integrity of query media packages is assessed as the inlierness of the query ICCSs with respect to the reference dataset. We present the MultimodAl Information Manipulation dataset (MAIM), a new dataset of media packages from Flickr, which we make available to the research community. We use both the newly created dataset as well as Flickr30K and MS COCO datasets to quantitatively evaluate our proposed approach. The reference dataset does not contain unmanipulated versions of tampered query packages. Our method is able to achieve F1 scores of 0.75, 0.89 and 0.94 on MAIM, Flickr30K and MS COCO, respectively, for detecting semantically incoherent media packages.



### SSGAN: Secure Steganography Based on Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.01613v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1707.01613v4)
- **Published**: 2017-07-06 02:05:51+00:00
- **Updated**: 2018-11-24 02:32:03+00:00
- **Authors**: Haichao Shi, Jing Dong, Wei Wang, Yinlong Qian, Xiaoyu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a novel strategy of Secure Steganograpy based on Generative Adversarial Networks is proposed to generate suitable and secure covers for steganography. The proposed architecture has one generative network, and two discriminative networks. The generative network mainly evaluates the visual quality of the generated images for steganography, and the discriminative networks are utilized to assess their suitableness for information hiding. Different from the existing work which adopts Deep Convolutional Generative Adversarial Networks, we utilize another form of generative adversarial networks. By using this new form of generative adversarial networks, significant improvements are made on the convergence speed, the training stability and the image quality. Furthermore, a sophisticated steganalysis network is reconstructed for the discriminative network, and the network can better evaluate the performance of the generated images. Numerous experiments are conducted on the publicly available datasets to demonstrate the effectiveness and robustness of the proposed method.



### Rapid focus map surveying for whole slide imaging with continues sample motion
- **Arxiv ID**: http://arxiv.org/abs/1707.03039v1
- **DOI**: 10.1364/OL.42.003379
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03039v1)
- **Published**: 2017-07-06 02:07:06+00:00
- **Updated**: 2017-07-06 02:07:06+00:00
- **Authors**: Jun Liao, Yutong Jiang, Zichao Bian, Bahareh Mahrou, Aparna Nambiar, Alexander W. Magsam, Kaikai Guo, Yong Ku Cho, Guoan Zheng
- **Comment**: None
- **Journal**: Optics Letters, 42 (17), 3379-3382 (2017)
- **Summary**: Whole slide imaging (WSI) has recently been cleared for primary diagnosis in the US. A critical challenge of WSI is to perform accurate focusing in high speed. Traditional systems create a focus map prior to scanning. For each focus point on the map, sample needs to be static in the x-y plane and axial scanning is needed to maximize the contrast. Here we report a novel focus map surveying method for WSI. The reported method requires no axial scanning, no additional camera and lens, works for stained and transparent samples, and allows continuous sample motion in the surveying process. It can be used for both brightfield and fluorescence WSI. By using a 20X, 0.75 NA objective lens, we demonstrate a mean focusing error of ~0.08 microns in the static mode and ~0.17 microns in the continuous motion mode. The reported method may provide a turnkey solution for most existing WSI systems for its simplicity, robustness, accuracy, and high-speed. It may also standardize the imaging performance of WSI systems for digital pathology and find other applications in high-content microscopy such as DNA sequencing and time-lapse live-cell imaging.



### Dual Path Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.01629v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01629v2)
- **Published**: 2017-07-06 04:05:14+00:00
- **Updated**: 2017-08-01 01:15:57+00:00
- **Authors**: Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, Jiashi Feng
- **Comment**: for code and models, see https://github.com/cypw/DPNs
- **Journal**: None
- **Summary**: In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.



### RON: Reverse Connection with Objectness Prior Networks for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1707.01691v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01691v1)
- **Published**: 2017-07-06 08:53:33+00:00
- **Updated**: 2017-07-06 08:53:33+00:00
- **Authors**: Tao Kong, Fuchun Sun, Anbang Yao, Huaping Liu, Ming Lu, Yurong Chen
- **Comment**: Project page will be available at https://github.com/taokong/RON, and
  formal paper will appear in CVPR 2017
- **Journal**: None
- **Summary**: We present RON, an efficient and effective framework for generic object detection. Our motivation is to smartly associate the best of the region-based (e.g., Faster R-CNN) and region-free (e.g., SSD) methodologies. Under fully convolutional architecture, RON mainly focuses on two fundamental problems: (a) multi-scale object localization and (b) negative sample mining. To address (a), we design the reverse connection, which enables the network to detect objects on multi-levels of CNNs. To deal with (b), we propose the objectness prior to significantly reduce the searching space of objects. We optimize the reverse connection, objectness prior and object detector jointly by a multi-task loss function, thus RON can directly predict final detection results from all locations of various feature maps. Extensive experiments on the challenging PASCAL VOC 2007, PASCAL VOC 2012 and MS COCO benchmarks demonstrate the competitive performance of RON. Specifically, with VGG-16 and low resolution 384X384 input size, the network gets 81.3% mAP on PASCAL VOC 2007, 80.7% mAP on PASCAL VOC 2012 datasets. Its superiority increases when datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. With 1.5G GPU memory at test phase, the speed of the network is 15 FPS, 3X faster than the Faster R-CNN counterpart.



### Automated Lane Detection in Crowds using Proximity Graphs
- **Arxiv ID**: http://arxiv.org/abs/1707.01698v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.01698v1)
- **Published**: 2017-07-06 09:23:07+00:00
- **Updated**: 2017-07-06 09:23:07+00:00
- **Authors**: Stijn Heldens, Claudio Martella, Nelly Litvak, Maarten van Steen
- **Comment**: Presented at the 6th International Workshop on Urban Computing
  (UrbComp 2017) held in conjunction with the 23th ACM SIGKDD
- **Journal**: None
- **Summary**: Studying the behavior of crowds is vital for understanding and predicting human interactions in public areas. Research has shown that, under certain conditions, large groups of people can form collective behavior patterns: local interactions between individuals results in global movements patterns. To detect these patterns in a crowd, we assume each person is carrying an on-body device that acts a local proximity sensor, e.g., smartphone or bluetooth badge, and represent the texture of the crowd as a proximity graph. Our goal is extract information about crowds from these proximity graphs. In this work, we focus on one particular type of pattern: lane formation. We present a formal definition of a lane, proposed a simple probabilistic model that simulates lanes moving through a stationary crowd, and present an automated lane-detection method. Our preliminary results show that our method is able to detect lanes of different shapes and sizes. We see our work as an initial step towards rich pattern recognition using proximity graphs.



### CNN features are also great at unsupervised classification
- **Arxiv ID**: http://arxiv.org/abs/1707.01700v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.01700v2)
- **Published**: 2017-07-06 09:24:35+00:00
- **Updated**: 2018-09-11 11:11:15+00:00
- **Authors**: Joris Guérin, Olivier Gibaru, Stéphane Thiery, Eric Nyiri
- **Comment**: 10 pages, 2 figures, 4 tables. Proceedings of AIFU 2018, Melbourne,
  Australia
- **Journal**: None
- **Summary**: This paper aims at providing insight on the transferability of deep CNN features to unsupervised problems. We study the impact of different pretrained CNN feature extractors on the problem of image set clustering for object classification as well as fine-grained classification. We propose a rather straightforward pipeline combining deep-feature extraction using a CNN pretrained on ImageNet and a classic clustering algorithm to classify sets of images. This approach is compared to state-of-the-art algorithms in image-clustering and provides better results. These results strengthen the belief that supervised training of deep CNN on large datasets, with a large variability of classes, extracts better features than most carefully designed engineering approaches, even for unsupervised tasks. We also validate our approach on a robotic application, consisting in sorting and storing objects smartly based on clustering.



### Skeleton-based Action Recognition Using LSTM and CNN
- **Arxiv ID**: http://arxiv.org/abs/1707.02356v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02356v1)
- **Published**: 2017-07-06 11:06:26+00:00
- **Updated**: 2017-07-06 11:06:26+00:00
- **Authors**: Chuankun Li, Pichao Wang, Shuang Wang, Yonghong Hou, Wanqing Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recent methods based on 3D skeleton data have achieved outstanding performance due to its conciseness, robustness, and view-independent representation. With the development of deep learning, Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM)-based learning methods have achieved promising performance for action recognition. However, for CNN-based methods, it is inevitable to loss temporal information when a sequence is encoded into images. In order to capture as much spatial-temporal information as possible, LSTM and CNN are adopted to conduct effective recognition with later score fusion. In addition, experimental results show that the score fusion between CNN and LSTM performs better than that between LSTM and LSTM for the same feature. Our method achieved state-of-the-art results on NTU RGB+D datasets for 3D human action analysis. The proposed method achieved 87.40% in terms of accuracy and ranked $1^{st}$ place in Large Scale 3D Human Activity Analysis Challenge in Depth Videos.



### Cross-linguistic differences and similarities in image descriptions
- **Arxiv ID**: http://arxiv.org/abs/1707.01736v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.01736v2)
- **Published**: 2017-07-06 11:53:41+00:00
- **Updated**: 2017-08-13 10:18:44+00:00
- **Authors**: Emiel van Miltenburg, Desmond Elliott, Piek Vossen
- **Comment**: Accepted for INLG 2017, Santiago de Compostela, Spain, 4-7 September,
  2017. Camera-ready version. See the ACL anthology for full bibliographic
  information
- **Journal**: None
- **Summary**: Automatic image description systems are commonly trained and evaluated on large image description datasets. Recently, researchers have started to collect such datasets for languages other than English. An unexplored question is how different these datasets are from English and, if there are any differences, what causes them to differ. This paper provides a cross-linguistic comparison of Dutch, English, and German image descriptions. We find that these descriptions are similar in many respects, but the familiarity of crowd workers with the subjects of the images has a noticeable influence on description specificity.



### Tensor-Train Recurrent Neural Networks for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1707.01786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01786v1)
- **Published**: 2017-07-06 13:43:14+00:00
- **Updated**: 2017-07-06 13:43:14+00:00
- **Authors**: Yinchong Yang, Denis Krompass, Volker Tresp
- **Comment**: None
- **Journal**: None
- **Summary**: The Recurrent Neural Networks and their variants have shown promising performances in sequence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs' large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extractors. To address this challenge, we propose a new, more general and efficient approach by factorizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained simultaneously with the weights themselves. We test our model on classification tasks using multiple real-world video datasets and achieve competitive performances with state-of-the-art models, even though our model architecture is orders of magnitude less complex. We believe that the proposed approach provides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architectures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data.



### Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.01836v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01836v1)
- **Published**: 2017-07-06 15:42:46+00:00
- **Updated**: 2017-07-06 15:42:46+00:00
- **Authors**: Pranav Rajpurkar, Awni Y. Hannun, Masoumeh Haghpanahi, Codie Bourn, Andrew Y. Ng
- **Comment**: None
- **Journal**: None
- **Summary**: We develop an algorithm which exceeds the performance of board certified cardiologists in detecting a wide range of heart arrhythmias from electrocardiograms recorded with a single-lead wearable monitor. We build a dataset with more than 500 times the number of unique patients than previously studied corpora. On this dataset, we train a 34-layer convolutional neural network which maps a sequence of ECG samples to a sequence of rhythm classes. Committees of board-certified cardiologists annotate a gold standard test set on which we compare the performance of our model to that of 6 other individual cardiologists. We exceed the average cardiologist performance in both recall (sensitivity) and precision (positive predictive value).



### Changing Views on Curves and Surfaces
- **Arxiv ID**: http://arxiv.org/abs/1707.01877v2
- **DOI**: None
- **Categories**: **math.AG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.01877v2)
- **Published**: 2017-07-06 17:34:27+00:00
- **Updated**: 2017-11-11 10:57:40+00:00
- **Authors**: Kathlén Kohn, Bernd Sturmfels, Matthew Trager
- **Comment**: 31 pages
- **Journal**: None
- **Summary**: Visual events in computer vision are studied from the perspective of algebraic geometry. Given a sufficiently general curve or surface in 3-space, we consider the image or contour curve that arises by projecting from a viewpoint. Qualitative changes in that curve occur when the viewpoint crosses the visual event surface. We examine the components of this ruled surface, and observe that these coincide with the iterated singular loci of the coisotropic hypersurfaces associated with the original curve or surface. We derive formulas, due to Salmon and Petitjean, for the degrees of these surfaces, and show how to compute exact representations for all visual event surfaces using algebraic methods.



### Zero-Shot Deep Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1707.01922v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01922v5)
- **Published**: 2017-07-06 18:09:36+00:00
- **Updated**: 2018-07-24 13:45:23+00:00
- **Authors**: Kuan-Chuan Peng, Ziyan Wu, Jan Ernst
- **Comment**: This paper is accepted to the European Conference on Computer Vision
  (ECCV), 2018
- **Journal**: None
- **Summary**: Domain adaptation is an important tool to transfer knowledge about a task (e.g. classification) learned in a source domain to a second, or target domain. Current approaches assume that task-relevant target-domain data is available during training. We demonstrate how to perform domain adaptation when no such task-relevant target-domain data is available. To tackle this issue, we propose zero-shot deep domain adaptation (ZDDA), which uses privileged information from task-irrelevant dual-domain pairs. ZDDA learns a source-domain representation which is not only tailored for the task of interest but also close to the target-domain representation. Therefore, the source-domain task of interest solution (e.g. a classifier for classification tasks) which is jointly trained with the source-domain representation can be applicable to both the source and target representations. Using the MNIST, Fashion-MNIST, NIST, EMNIST, and SUN RGB-D datasets, we show that ZDDA can perform domain adaptation in classification tasks without access to task-relevant target-domain training data. We also extend ZDDA to perform sensor fusion in the SUN RGB-D scene classification task by simulating task-relevant target-domain representations with task-relevant source-domain data. To the best of our knowledge, ZDDA is the first domain adaptation and sensor fusion method which requires no task-relevant target-domain data. The underlying principle is not particular to computer vision data, but should be extensible to other domains.



### A Generalised Seizure Prediction with Convolutional Neural Networks for Intracranial and Scalp Electroencephalogram Data Analysis
- **Arxiv ID**: http://arxiv.org/abs/1707.01976v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01976v2)
- **Published**: 2017-07-06 21:54:55+00:00
- **Updated**: 2017-12-06 11:48:22+00:00
- **Authors**: Nhan Duy Truong, Anh Duy Nguyen, Levin Kuhlmann, Mohammad Reza Bonyadi, Jiawei Yang, Omid Kavehei
- **Comment**: None
- **Journal**: None
- **Summary**: Seizure prediction has attracted a growing attention as one of the most challenging predictive data analysis efforts in order to improve the life of patients living with drug-resistant epilepsy and tonic seizures. Many outstanding works have been reporting great results in providing a sensible indirect (warning systems) or direct (interactive neural-stimulation) control over refractory seizures, some of which achieved high performance. However, many works put heavily handcraft feature extraction and/or carefully tailored feature engineering to each patient to achieve very high sensitivity and low false prediction rate for a particular dataset. This limits the benefit of their approaches if a different dataset is used. In this paper we apply Convolutional Neural Networks (CNNs) on different intracranial and scalp electroencephalogram (EEG) datasets and proposed a generalized retrospective and patient-specific seizure prediction method. We use Short-Time Fourier Transform (STFT) on 30-second EEG windows with 50% overlapping to extract information in both frequency and time domains. A standardization step is then applied on STFT components across the whole frequency range to prevent high frequencies features being influenced by those at lower frequencies. A convolutional neural network model is used for both feature extraction and classification to separate preictal segments from interictal ones. The proposed approach achieves sensitivity of 81.4%, 81.2%, 82.3% and false prediction rate (FPR) of 0.06/h, 0.16/h, 0.22/h on Freiburg Hospital intracranial EEG (iEEG) dataset, Children's Hospital of Boston-MIT scalp EEG (sEEG) dataset, and Kaggle American Epilepsy Society Seizure Prediction Challenge's dataset, respectively. Our prediction method is also statistically better than an unspecific random predictor for most of patients in all three datasets.



### On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task
- **Arxiv ID**: http://arxiv.org/abs/1707.01992v1
- **DOI**: 10.1007/978-3-319-59050-9_28
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.01992v1)
- **Published**: 2017-07-06 23:13:03+00:00
- **Updated**: 2017-07-06 23:13:03+00:00
- **Authors**: Wenqi Li, Guotai Wang, Lucas Fidon, Sebastien Ourselin, M. Jorge Cardoso, Tom Vercauteren
- **Comment**: Paper accepted at IPMI 2017
- **Journal**: None
- **Summary**: Deep convolutional neural networks are powerful tools for learning visual representations from images. However, designing efficient deep architectures to analyse volumetric medical images remains challenging. This work investigates efficient and flexible elements of modern convolutional networks such as dilated convolution and residual connection. With these essential building blocks, we propose a high-resolution, compact convolutional network for volumetric image segmentation. To illustrate its efficiency of learning 3D representation from large-scale image data, the proposed network is validated with the challenging task of parcellating 155 neuroanatomical structures from brain MR images. Our experiments show that the proposed network architecture compares favourably with state-of-the-art volumetric segmentation networks while being an order of magnitude more compact. We consider the brain parcellation task as a pretext task for volumetric image segmentation; our trained network potentially provides a good starting point for transfer learning. Additionally, we show the feasibility of voxel-level uncertainty estimation using a sampling approximation through dropout.



