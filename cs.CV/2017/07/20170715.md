# Arxiv Papers in cs.CV on 2017-07-15
### Knowledge-Guided Recurrent Neural Network Learning for Task-Oriented Action Prediction
- **Arxiv ID**: http://arxiv.org/abs/1707.04677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04677v1)
- **Published**: 2017-07-15 01:40:07+00:00
- **Updated**: 2017-07-15 01:40:07+00:00
- **Authors**: Liang Lin, Lili Huang, Tianshui Chen, Yukang Gan, Hui Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims at task-oriented action prediction, i.e., predicting a sequence of actions towards accomplishing a specific task under a certain scene, which is a new problem in computer vision research. The main challenges lie in how to model task-specific knowledge and integrate it in the learning procedure. In this work, we propose to train a recurrent long-short term memory (LSTM) network for handling this problem, i.e., taking a scene image (including pre-located objects) and the specified task as input and recurrently predicting action sequences. However, training such a network usually requires large amounts of annotated samples for covering the semantic space (e.g., diverse action decomposition and ordering). To alleviate this issue, we introduce a temporal And-Or graph (AOG) for task description, which hierarchically represents a task into atomic actions. With this AOG representation, we can produce many valid samples (i.e., action sequences according with common sense) by training another auxiliary LSTM network with a small set of annotated samples. And these generated samples (i.e., task-oriented action sequences) effectively facilitate training the model for task-oriented action prediction. In the experiments, we create a new dataset containing diverse daily tasks and extensively evaluate the effectiveness of our approach.



### Rethinking Reprojection: Closing the Loop for Pose-aware ShapeReconstruction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1707.04682v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04682v2)
- **Published**: 2017-07-15 02:54:11+00:00
- **Updated**: 2017-07-26 17:08:57+00:00
- **Authors**: Rui Zhu, Hamed Kiani Galoogahi, Chaoyang Wang, Simon Lucey
- **Comment**: First sub
- **Journal**: None
- **Summary**: An emerging problem in computer vision is the reconstruction of 3D shape and pose of an object from a single image. Hitherto, the problem has been addressed through the application of canonical deep learning methods to regress from the image directly to the 3D shape and pose labels. These approaches, however, are problematic from two perspectives. First, they are minimizing the error between 3D shapes and pose labels - with little thought about the nature of this label error when reprojecting the shape back onto the image. Second, they rely on the onerous and ill-posed task of hand labeling natural images with respect to 3D shape and pose. In this paper we define the new task of pose-aware shape reconstruction from a single image, and we advocate that cheaper 2D annotations of objects silhouettes in natural images can be utilized. We design architectures of pose-aware shape reconstruction which re-project the predicted shape back on to the image using the predicted pose. Our evaluation on several object categories demonstrates the superiority of our method for predicting pose-aware 3D shapes from natural images.



### Dominant Sets for "Constrained" Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.05309v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.05309v1)
- **Published**: 2017-07-15 03:32:42+00:00
- **Updated**: 2017-07-15 03:32:42+00:00
- **Authors**: Eyasu Zemene, Leulseged Tesfaye Alemu, Marcello Pelillo
- **Comment**: arXiv admin note: text overlap with arXiv:1608.00641
- **Journal**: None
- **Summary**: Image segmentation has come a long way since the early days of computer vision, and still remains a challenging task. Modern variations of the classical (purely bottom-up) approach, involve, e.g., some form of user assistance (interactive segmentation) or ask for the simultaneous segmentation of two or more images (co-segmentation). At an abstract level, all these variants can be thought of as "constrained" versions of the original formulation, whereby the segmentation process is guided by some external source of information. In this paper, we propose a new approach to tackle this kind of problems in a unified way. Our work is based on some properties of a family of quadratic optimization problems related to dominant sets, a well-known graph-theoretic notion of a cluster which generalizes the concept of a maximal clique to edge-weighted graphs. In particular, we show that by properly controlling a regularization parameter which determines the structure and the scale of the underlying problem, we are in a position to extract groups of dominant-set clusters that are constrained to contain predefined elements. In particular, we shall focus on interactive segmentation and co-segmentation (in both the unsupervised and the interactive versions). The proposed algorithm can deal naturally with several type of constraints and input modality, including scribbles, sloppy contours, and bounding boxes, and is able to robustly handle noisy annotations on the part of the user. Experiments on standard benchmark datasets show the effectiveness of our approach as compared to state-of-the-art algorithms on a variety of natural images under several input conditions and constraints.



### Binarized Convolutional Neural Networks with Separable Filters for Efficient Hardware Acceleration
- **Arxiv ID**: http://arxiv.org/abs/1707.04693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04693v1)
- **Published**: 2017-07-15 06:17:07+00:00
- **Updated**: 2017-07-15 06:17:07+00:00
- **Authors**: Jeng-Hau Lin, Tianwei Xing, Ritchie Zhao, Zhiru Zhang, Mani Srivastava, Zhuowen Tu, Rajesh K. Gupta
- **Comment**: 9 pages, 6 figures, accepted for Embedded Vision Workshop (CVPRW)
- **Journal**: None
- **Summary**: State-of-the-art convolutional neural networks are enormously costly in both compute and memory, demanding massively parallel GPUs for execution. Such networks strain the computational capabilities and energy available to embedded and mobile processing platforms, restricting their use in many important applications. In this paper, we push the boundaries of hardware-effective CNN design by proposing BCNN with Separable Filters (BCNNw/SF), which applies Singular Value Decomposition (SVD) on BCNN kernels to further reduce computational and storage complexity. To enable its implementation, we provide a closed form of the gradient over SVD to calculate the exact gradient with respect to every binarized weight in backward propagation. We verify BCNNw/SF on the MNIST, CIFAR-10, and SVHN datasets, and implement an accelerator for CIFAR-10 on FPGA hardware. Our BCNNw/SF accelerator realizes memory savings of 17% and execution time reduction of 31.3% compared to BCNN with only minor accuracy sacrifices.



### Original Loop-closure Detection Algorithm for Monocular vSLAM
- **Arxiv ID**: http://arxiv.org/abs/1707.04771v1
- **DOI**: 10.1007/978-3-319-73013-4_19
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04771v1)
- **Published**: 2017-07-15 18:12:11+00:00
- **Updated**: 2017-07-15 18:12:11+00:00
- **Authors**: Andrey Bokovoy, Konstantin Yakovlev
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-based simultaneous localization and mapping (vSLAM) is a well-established problem in mobile robotics and monocular vSLAM is one of the most challenging variations of that problem nowadays. In this work we study one of the core post-processing optimization mechanisms in vSLAM, e.g. loop-closure detection. We analyze the existing methods and propose original algorithm for loop-closure detection, which is suitable for dense, semi-dense and feature-based vSLAM methods. We evaluate the algorithm experimentally and show that it contribute to more accurate mapping while speeding up the monocular vSLAM pipeline to the extent the latter can be used in real-time for controlling small multi-rotor vehicle (drone).



### Modified Alpha-Rooting Color Image Enhancement Method On The Two-Side 2-D Quaternion Discrete Fourier Transform And The 2-D Discrete Fourier Transform
- **Arxiv ID**: http://arxiv.org/abs/1707.04781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.04781v1)
- **Published**: 2017-07-15 19:55:26+00:00
- **Updated**: 2017-07-15 19:55:26+00:00
- **Authors**: Artyom M. Grigoryan, Aparna John, Sos S. Agaian
- **Comment**: 16 pages, 53 figures (including sub-figures)
- **Journal**: Applied Mathematics and Sciences: An International Journal
  (MathSJ), Vol. 4, No. 1/2, June 2017
- **Summary**: Color in an image is resolved into 3 or 4 color components and 2-Dimages of these components are stored in separate channels. Most of the color image enhancement algorithms are applied channel-by-channel on each image. But such a system of color image processing is not processing the original color. When a color image is represented as a quaternion image, processing is done in original colors. This paper proposes an implementation of the quaternion approach of enhancement algorithm for enhancing color images and is referred as the modified alpha-rooting by the two-dimensional quaternion discrete Fourier transform (2-D QDFT). Enhancement results of this proposed method are compared with the channel-by-channel image enhancement by the 2-D DFT. Enhancements in color images are quantitatively measured by the color enhancement measure estimation (CEME), which allows for selecting optimum parameters for processing by the genetic algorithm. Enhancement of color images by the quaternion based method allows for obtaining images which are closer to the genuine representation of the real original color.



### LabelFusion: A Pipeline for Generating Ground Truth Labels for Real RGBD Data of Cluttered Scenes
- **Arxiv ID**: http://arxiv.org/abs/1707.04796v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1707.04796v3)
- **Published**: 2017-07-15 23:17:11+00:00
- **Updated**: 2017-09-26 15:16:52+00:00
- **Authors**: Pat Marion, Peter R. Florence, Lucas Manuelli, Russ Tedrake
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural network (DNN) architectures have been shown to outperform traditional pipelines for object segmentation and pose estimation using RGBD data, but the performance of these DNN pipelines is directly tied to how representative the training data is of the true data. Hence a key requirement for employing these methods in practice is to have a large set of labeled data for your specific robotic manipulation task, a requirement that is not generally satisfied by existing datasets. In this paper we develop a pipeline to rapidly generate high quality RGBD data with pixelwise labels and object poses. We use an RGBD camera to collect video of a scene from multiple viewpoints and leverage existing reconstruction techniques to produce a 3D dense reconstruction. We label the 3D reconstruction using a human assisted ICP-fitting of object meshes. By reprojecting the results of labeling the 3D scene we can produce labels for each RGBD image of the scene. This pipeline enabled us to collect over 1,000,000 labeled object instances in just a few days. We use this dataset to answer questions related to how much training data is required, and of what quality the data must be, to achieve high performance from a DNN architecture.



