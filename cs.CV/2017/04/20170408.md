# Arxiv Papers in cs.CV on 2017-04-08
### A Deep Cascade of Convolutional Neural Networks for Dynamic MR Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1704.02422v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02422v2)
- **Published**: 2017-04-08 02:13:48+00:00
- **Updated**: 2017-11-23 11:28:10+00:00
- **Authors**: Jo Schlemper, Jose Caballero, Joseph V. Hajnal, Anthony Price, Daniel Rueckert
- **Comment**: To be published in IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Inspired by recent advances in deep learning, we propose a framework for reconstructing dynamic sequences of 2D cardiac magnetic resonance (MR) images from undersampled data using a deep cascade of convolutional neural networks (CNNs) to accelerate the data acquisition process. In particular, we address the case where data is acquired using aggressive Cartesian undersampling. Firstly, we show that when each 2D image frame is reconstructed independently, the proposed method outperforms state-of-the-art 2D compressed sensing approaches such as dictionary learning-based MR image reconstruction, in terms of reconstruction error and reconstruction speed. Secondly, when reconstructing the frames of the sequences jointly, we demonstrate that CNNs can learn spatio-temporal correlations efficiently by combining convolution and data sharing approaches. We show that the proposed method consistently outperforms state-of-the-art methods and is capable of preserving anatomical structure more faithfully up to 11-fold undersampling. Moreover, reconstruction is very fast: each complete dynamic sequence can be reconstructed in less than 10s and, for the 2D case, each image frame can be reconstructed in 23ms, enabling real-time applications.



### Learning Cross-Modal Deep Representations for Robust Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.02431v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02431v2)
- **Published**: 2017-04-08 03:12:23+00:00
- **Updated**: 2018-01-01 21:50:22+00:00
- **Authors**: Dan Xu, Wanli Ouyang, Elisa Ricci, Xiaogang Wang, Nicu Sebe
- **Comment**: Accepted at CVPR 2017
- **Journal**: None
- **Summary**: This paper presents a novel method for detecting pedestrians under adverse illumination conditions. Our approach relies on a novel cross-modality learning framework and it is based on two main phases. First, given a multimodal dataset, a deep convolutional network is employed to learn a non-linear mapping, modeling the relations between RGB and thermal data. Then, the learned feature representations are transferred to a second deep network, which receives as input an RGB image and outputs the detection results. In this way, features which are both discriminative and robust to bad illumination conditions are learned. Importantly, at test time, only the second pipeline is considered and no thermal data are required. Our extensive evaluation demonstrates that the proposed approach outperforms the state-of- the-art on the challenging KAIST multispectral pedestrian dataset and it is competitive with previous methods on the popular Caltech dataset.



### 3D seismic data denoising using two-dimensional sparse coding scheme
- **Arxiv ID**: http://arxiv.org/abs/1704.04429v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CE
- **Links**: [PDF](http://arxiv.org/pdf/1704.04429v1)
- **Published**: 2017-04-08 06:13:00+00:00
- **Updated**: 2017-04-08 06:13:00+00:00
- **Authors**: Ming-Jun Su, Jingbo Chang, Feng Qian, Guangmin Hu, Xiao-Yang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Seismic data denoising is vital to geophysical applications and the transform-based function method is one of the most widely used techniques. However, it is challenging to design a suit- able sparse representation to express a transform-based func- tion group due to the complexity of seismic data. In this paper, we apply a seismic data denoising method based on learning- type overcomplete dictionaries which uses two-dimensional sparse coding (2DSC). First, we model the input seismic data and dictionaries as third-order tensors and introduce tensor- linear combinations for data approximation. Second, we ap- ply learning-type overcomplete dictionary, i.e., optimal sparse data representation is achieved through learning and training. Third, we exploit the alternating minimization algorithm to solve the optimization problem of seismic denoising. Finally we evaluate its denoising performance on synthetic seismic data and land data survey. Experiment results show that the two-dimensional sparse coding scheme reduces computational costs and enhances the signal-to-noise ratio.



### Seismic facies recognition based on prestack data using deep convolutional autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1704.02446v1
- **DOI**: 10.1190/geo2017-0524.1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02446v1)
- **Published**: 2017-04-08 06:17:34+00:00
- **Updated**: 2017-04-08 06:17:34+00:00
- **Authors**: Feng Qian, Miao Yin, Ming-Jun Su, Yaojun Wang, Guangmin Hu
- **Comment**: None
- **Journal**: GEOPHYSICS, 2018, 83(3): A39-A43
- **Summary**: Prestack seismic data carries much useful information that can help us find more complex atypical reservoirs. Therefore, we are increasingly inclined to use prestack seismic data for seis- mic facies recognition. However, due to the inclusion of ex- cessive redundancy, effective feature extraction from prestack seismic data becomes critical. In this paper, we consider seis- mic facies recognition based on prestack data as an image clus- tering problem in computer vision (CV) by thinking of each prestack seismic gather as a picture. We propose a convo- lutional autoencoder (CAE) network for deep feature learn- ing from prestack seismic data, which is more effective than principal component analysis (PCA) in redundancy removing and valid information extraction. Then, using conventional classification or clustering techniques (e.g. K-means or self- organizing maps) on the extracted features, we can achieve seismic facies recognition. We applied our method to the prestack data from physical model and LZB region. The re- sult shows that our approach is superior to the conventionals.



### Towards 3D Human Pose Estimation in the Wild: a Weakly-supervised Approach
- **Arxiv ID**: http://arxiv.org/abs/1704.02447v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02447v2)
- **Published**: 2017-04-08 06:21:48+00:00
- **Updated**: 2017-07-30 15:01:30+00:00
- **Authors**: Xingyi Zhou, Qixing Huang, Xiao Sun, Xiangyang Xue, Yichen Wei
- **Comment**: Accepted to ICCV 2017
- **Journal**: None
- **Summary**: In this paper, we study the task of 3D human pose estimation in the wild. This task is challenging due to lack of training data, as existing datasets are either in the wild images with 2D pose or in the lab images with 3D pose.   We propose a weakly-supervised transfer learning method that uses mixed 2D and 3D labels in a unified deep neutral network that presents two-stage cascaded structure. Our network augments a state-of-the-art 2D pose estimation sub-network with a 3D depth regression sub-network. Unlike previous two stage approaches that train the two sub-networks sequentially and separately, our training is end-to-end and fully exploits the correlation between the 2D pose and depth estimation sub-tasks. The deep features are better learnt through shared representations. In doing so, the 3D pose labels in controlled lab environments are transferred to in the wild images. In addition, we introduce a 3D geometric constraint to regularize the 3D pose prediction, which is effective in the absence of ground truth depth labels. Our method achieves competitive results on both 2D and 3D benchmarks.



### Coupled Deep Learning for Heterogeneous Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.02450v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02450v2)
- **Published**: 2017-04-08 07:10:45+00:00
- **Updated**: 2017-11-16 08:48:23+00:00
- **Authors**: Xiang Wu, Lingxiao Song, Ran He, Tieniu Tan
- **Comment**: AAAI 2018
- **Journal**: None
- **Summary**: Heterogeneous face matching is a challenge issue in face recognition due to large domain difference as well as insufficient pairwise images in different modalities during training. This paper proposes a coupled deep learning (CDL) approach for the heterogeneous face matching. CDL seeks a shared feature space in which the heterogeneous face matching problem can be approximately treated as a homogeneous face matching problem. The objective function of CDL mainly includes two parts. The first part contains a trace norm and a block-diagonal prior as relevance constraints, which not only make unpaired images from multiple modalities be clustered and correlated, but also regularize the parameters to alleviate overfitting. An approximate variational formulation is introduced to deal with the difficulties of optimizing low-rank constraint directly. The second part contains a cross modal ranking among triplet domain specific images to maximize the margin for different identities and increase data for a small amount of training samples. Besides, an alternating minimization method is employed to iteratively update the parameters of CDL. Experimental results show that CDL achieves better performance on the challenging CASIA NIR-VIS 2.0 face recognition database, the IIIT-D Sketch database, the CUHK Face Sketch (CUFS), and the CUHK Face Sketch FERET (CUFSF), which significantly outperforms state-of-the-art heterogeneous face recognition methods.



### A New Pseudo-color Technique Based on Intensity Information Protection for Passive Sensor Imagery
- **Arxiv ID**: http://arxiv.org/abs/1704.02455v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02455v1)
- **Published**: 2017-04-08 08:16:56+00:00
- **Updated**: 2017-04-08 08:16:56+00:00
- **Authors**: Mohammad Reza Khosravi, Habib Rostami, Gholam Reza Ahmadi, Suleiman Mansouri, Ahmad Keshavarz
- **Comment**: None
- **Journal**: International Journal of Electronics Communication and Computer
  Engineering, vol. 6, no. 3, pp. 324-329 (2015)
- **Summary**: Remote sensing image processing is so important in geo-sciences. Images which are obtained by different types of sensors might initially be unrecognizable. To make an acceptable visual perception in the images, some pre-processing steps (for removing noises and etc) are preformed which they affect the analysis of images. There are different types of processing according to the types of remote sensing images. The method that we are going to introduce in this paper is to use virtual colors to colorize the gray-scale images of satellite sensors. This approach helps us to have a better analysis on a sample single-band image which has been taken by Landsat-8 (OLI) sensor (as a multi-band sensor with natural color bands, its images' natural color can be compared to synthetic color by our approach). A good feature of this method is the original image reversibility in order to keep the suitable resolution of output images.



### First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations
- **Arxiv ID**: http://arxiv.org/abs/1704.02463v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02463v2)
- **Published**: 2017-04-08 09:45:12+00:00
- **Updated**: 2018-04-10 13:02:01+00:00
- **Authors**: Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul Baek, Tae-Kyun Kim
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: In this work we study the use of 3D hand poses to recognize first-person dynamic hand actions interacting with 3D objects. Towards this goal, we collected RGB-D video sequences comprised of more than 100K frames of 45 daily hand action categories, involving 26 different objects in several hand configurations. To obtain hand pose annotations, we used our own mo-cap system that automatically infers the 3D location of each of the 21 joints of a hand model via 6 magnetic sensors and inverse kinematics. Additionally, we recorded the 6D object poses and provide 3D object models for a subset of hand-object interaction sequences. To the best of our knowledge, this is the first benchmark that enables the study of first-person hand actions with the use of 3D hand poses. We present an extensive experimental evaluation of RGB-D and pose-based action recognition by 18 baselines/state-of-the-art approaches. The impact of using appearance features, poses, and their combinations are measured, and the different training/testing protocols are evaluated. Finally, we assess how ready the 3D hand pose estimation field is when hands are severely occluded by objects in egocentric views and its influence on action recognition. From the results, we see clear benefits of using hand pose as a cue for action recognition compared to other data modalities. Our dataset and experiments can be of interest to communities of 3D hand pose estimation, 6D object pose, and robotics as well as action recognition.



### DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.02470v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02470v2)
- **Published**: 2017-04-08 10:27:36+00:00
- **Updated**: 2017-09-05 19:46:11+00:00
- **Authors**: Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Despite a rapid rise in the quality of built-in smartphone cameras, their physical limitations - small sensor size, compact lenses and the lack of specific hardware, - impede them to achieve the quality results of DSLR cameras. In this work we present an end-to-end deep learning approach that bridges this gap by translating ordinary photos into DSLR-quality images. We propose learning the translation function using a residual convolutional neural network that improves both color rendition and image sharpness. Since the standard mean squared loss is not well suited for measuring perceptual image quality, we introduce a composite perceptual error function that combines content, color and texture losses. The first two losses are defined analytically, while the texture loss is learned in an adversarial fashion. We also present DPED, a large-scale dataset that consists of real photos captured from three different phones and one high-end reflex camera. Our quantitative and qualitative assessments reveal that the enhanced image quality is comparable to that of DSLR-taken photos, while the methodology is generalized to any type of digital camera.



### Metric Learning in Codebook Generation of Bag-of-Words for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1704.02492v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02492v2)
- **Published**: 2017-04-08 13:11:12+00:00
- **Updated**: 2017-04-11 16:21:33+00:00
- **Authors**: Lu Tian, Shengjin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification is generally divided into two part: first how to represent a pedestrian by discriminative visual descriptors and second how to compare them by suitable distance metrics. Conventional methods isolate these two parts, the first part usually unsupervised and the second part supervised. The Bag-of-Words (BoW) model is a widely used image representing descriptor in part one. Its codebook is simply generated by clustering visual features in Euclidian space. In this paper, we propose to use part two metric learning techniques in the codebook generation phase of BoW. In particular, the proposed codebook is clustered under Mahalanobis distance which is learned supervised. Extensive experiments prove that our proposed method is effective. With several low level features extracted on superpixel and fused together, our method outperforms state-of-the-art on person re-identification benchmarks including VIPeR, PRID450S, and Market1501.



### DualGAN: Unsupervised Dual Learning for Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1704.02510v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02510v4)
- **Published**: 2017-04-08 16:13:52+00:00
- **Updated**: 2018-10-09 20:42:00+00:00
- **Authors**: Zili Yi, Hao Zhang, Ping Tan, Minglun Gong
- **Comment**: Accepted by ICCV 2017
- **Journal**: None
- **Summary**: Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation, we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators. Experiments on multiple image translation tasks with unlabeled data show considerable performance gain of DualGAN over a single GAN. For some tasks, DualGAN can even achieve comparable or slightly better results than conditional GAN trained on fully labeled data.



### An Empirical Evaluation of Visual Question Answering for Novel Objects
- **Arxiv ID**: http://arxiv.org/abs/1704.02516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02516v1)
- **Published**: 2017-04-08 17:51:46+00:00
- **Updated**: 2017-04-08 17:51:46+00:00
- **Authors**: Santhosh K. Ramakrishnan, Ambar Pal, Gaurav Sharma, Anurag Mittal
- **Comment**: 11 pages, 4 figures, accepted in CVPR 2017 (poster)
- **Journal**: None
- **Summary**: We study the problem of answering questions about images in the harder setting, where the test questions and corresponding images contain novel objects, which were not queried about in the training data. Such setting is inevitable in real world-owing to the heavy tailed distribution of the visual categories, there would be some objects which would not be annotated in the train set. We show that the performance of two popular existing methods drop significantly (up to 28%) when evaluated on novel objects cf. known objects. We propose methods which use large existing external corpora of (i) unlabeled text, i.e. books, and (ii) images tagged with classes, to achieve novel object based visual question answering. We do systematic empirical studies, for both an oracle case where the novel objects are known textually, as well as a fully automatic case without any explicit knowledge of the novel objects, but with the minimal assumption that the novel objects are semantically related to the existing objects in training. The proposed methods for novel object based visual question answering are modular and can potentially be used with many visual question answering architectures. We show consistent improvements with the two popular architectures and give qualitative analysis of the cases where the model does well and of those where it fails to bring improvements.



### Deep Generative Adversarial Compression Artifact Removal
- **Arxiv ID**: http://arxiv.org/abs/1704.02518v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02518v3)
- **Published**: 2017-04-08 18:15:58+00:00
- **Updated**: 2017-12-06 18:38:21+00:00
- **Authors**: Leonardo Galteri, Lorenzo Seidenari, Marco Bertini, Alberto Del Bimbo
- **Comment**: ICCV 2017 Camera Ready + Acknowledgements
- **Journal**: None
- **Summary**: Compression artifacts arise in images whenever a lossy compression algorithm is applied. These artifacts eliminate details present in the original image, or add noise and small structures; because of these effects they make images less pleasant for the human eye, and may also lead to decreased performance of computer vision algorithms such as object detectors. To eliminate such artifacts, when decompressing an image, it is required to recover the original image from a disturbed version. To this end, we present a feed-forward fully convolutional residual network model trained using a generative adversarial framework. To provide a baseline, we show that our model can be also trained optimizing the Structural Similarity (SSIM), which is a better loss with respect to the simpler Mean Squared Error (MSE). Our GAN is able to produce images with more photorealistic details than MSE or SSIM based networks. Moreover we show that our approach can be used as a pre-processing step for object detection in case images are degraded by compression to a point that state-of-the art detectors fail. In this task, our GAN method obtains better performance than MSE or SSIM trained networks.



