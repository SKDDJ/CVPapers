# Arxiv Papers in cs.CV on 2017-04-05
### Joint Regression and Ranking for Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1704.01235v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01235v1)
- **Published**: 2017-04-05 01:28:04+00:00
- **Updated**: 2017-04-05 01:28:04+00:00
- **Authors**: Parag S. Chandakkar, Baoxin Li
- **Comment**: WACV 2017
- **Journal**: None
- **Summary**: Research on automated image enhancement has gained momentum in recent years, partially due to the need for easy-to-use tools for enhancing pictures captured by ubiquitous cameras on mobile devices. Many of the existing leading methods employ machine-learning-based techniques, by which some enhancement parameters for a given image are found by relating the image to the training images with known enhancement parameters. While knowing the structure of the parameter space can facilitate search for the optimal solution, none of the existing methods has explicitly modeled and learned that structure. This paper presents an end-to-end, novel joint regression and ranking approach to model the interaction between desired enhancement parameters and images to be processed, employing a Gaussian process (GP). GP allows searching for ideal parameters using only the image features. The model naturally leads to a ranking technique for comparing images in the induced feature space. Comparative evaluation using the ground-truth based on the MIT-Adobe FiveK dataset plus subjective tests on an additional data-set were used to demonstrate the effectiveness of the proposed approach.



### Estimation of Tissue Microstructure Using a Deep Network Inspired by a Sparse Reconstruction Framework
- **Arxiv ID**: http://arxiv.org/abs/1704.01246v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01246v1)
- **Published**: 2017-04-05 02:37:44+00:00
- **Updated**: 2017-04-05 02:37:44+00:00
- **Authors**: Chuyang Ye
- **Comment**: 12 pages, 5 figures. Accepted by IPMI 2017
- **Journal**: None
- **Summary**: Diffusion magnetic resonance imaging (dMRI) provides a unique tool for noninvasively probing the microstructure of the neuronal tissue. The NODDI model has been a popular approach to the estimation of tissue microstructure in many neuroscience studies. It represents the diffusion signals with three types of diffusion in tissue: intra-cellular, extra-cellular, and cerebrospinal fluid compartments. However, the original NODDI method uses a computationally expensive procedure to fit the model and could require a large number of diffusion gradients for accurate microstructure estimation, which may be impractical for clinical use. Therefore, efforts have been devoted to efficient and accurate NODDI microstructure estimation with a reduced number of diffusion gradients. In this work, we propose a deep network based approach to the NODDI microstructure estimation, which is named Microstructure Estimation using a Deep Network (MEDN). Motivated by the AMICO algorithm which accelerates the computation of NODDI parameters, we formulate the microstructure estimation problem in a dictionary-based framework. The proposed network comprises two cascaded stages. The first stage resembles the solution to a dictionary-based sparse reconstruction problem and the second stage computes the final microstructure using the output of the first stage. The weights in the two stages are jointly learned from training data, which is obtained from training dMRI scans with diffusion gradients that densely sample the q-space. The proposed method was applied to brain dMRI scans, where two shells each with 30 gradient directions (60 diffusion gradients in total) were used. Estimation accuracy with respect to the gold standard was measured and the results demonstrate that MEDN outperforms the competing algorithms.



### A Computational Approach to Relative Aesthetics
- **Arxiv ID**: http://arxiv.org/abs/1704.01248v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01248v1)
- **Published**: 2017-04-05 02:49:30+00:00
- **Updated**: 2017-04-05 02:49:30+00:00
- **Authors**: Parag S. Chandakkar, Vijetha Gattupalli, Baoxin Li
- **Comment**: ICPR 2016
- **Journal**: None
- **Summary**: Computational visual aesthetics has recently become an active research area. Existing state-of-art methods formulate this as a binary classification task where a given image is predicted to be beautiful or not. In many applications such as image retrieval and enhancement, it is more important to rank images based on their aesthetic quality instead of binary-categorizing them. Furthermore, in such applications, it may be possible that all images belong to the same category. Hence determining the aesthetic ranking of the images is more appropriate. To this end, we formulate a novel problem of ranking images with respect to their aesthetic quality. We construct a new dataset of image pairs with relative labels by carefully selecting images from the popular AVA dataset. Unlike in aesthetics classification, there is no single threshold which would determine the ranking order of the images across our entire dataset. We propose a deep neural network based approach that is trained on image pairs by incorporating principles from relative learning. Results show that such relative training procedure allows our network to rank the images with a higher accuracy than a state-of-art network trained on the same set of images using binary labels.



### A Structured Approach to Predicting Image Enhancement Parameters
- **Arxiv ID**: http://arxiv.org/abs/1704.01249v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01249v1)
- **Published**: 2017-04-05 03:04:28+00:00
- **Updated**: 2017-04-05 03:04:28+00:00
- **Authors**: Parag S. Chandakkar, Baoxin Li
- **Comment**: WACV 2016
- **Journal**: None
- **Summary**: Social networking on mobile devices has become a commonplace of everyday life. In addition, photo capturing process has become trivial due to the advances in mobile imaging. Hence people capture a lot of photos everyday and they want them to be visually-attractive. This has given rise to automated, one-touch enhancement tools. However, the inability of those tools to provide personalized and content-adaptive enhancement has paved way for machine-learned methods to do the same. The existing typical machine-learned methods heuristically (e.g. kNN-search) predict the enhancement parameters for a new image by relating the image to a set of similar training images. These heuristic methods need constant interaction with the training images which makes the parameter prediction sub-optimal and computationally expensive at test time which is undesired. This paper presents a novel approach to predicting the enhancement parameters given a new image using only its features, without using any training images. We propose to model the interaction between the image features and its corresponding enhancement parameters using the matrix factorization (MF) principles. We also propose a way to integrate the image features in the MF formulation. We show that our approach outperforms heuristic approaches as well as recent approaches in MF and structured prediction on synthetic as well as real-world data of image enhancement.



### Relative Learning from Web Images for Content-adaptive Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1704.01250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01250v1)
- **Published**: 2017-04-05 03:13:01+00:00
- **Updated**: 2017-04-05 03:13:01+00:00
- **Authors**: Parag S. Chandakkar, Qiongjie Tian, Baoxin Li
- **Comment**: ICME 2015
- **Journal**: None
- **Summary**: Personalized and content-adaptive image enhancement can find many applications in the age of social media and mobile computing. This paper presents a relative-learning-based approach, which, unlike previous methods, does not require matching original and enhanced images for training. This allows the use of massive online photo collections to train a ranking model for improved enhancement. We first propose a multi-level ranking model, which is learned from only relatively-labeled inputs that are automatically crawled. Then we design a novel parameter sampling scheme under this model to generate the desired enhancement parameters for a new image. For evaluation, we first verify the effectiveness and the generalization abilities of our approach, using images that have been enhanced/labeled by experts. Then we carry out subjective tests, which show that users prefer images enhanced by our approach over other existing methods.



### Improving Vision-based Self-positioning in Intelligent Transportation Systems via Integrated Lane and Vehicle Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.01256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01256v1)
- **Published**: 2017-04-05 03:38:08+00:00
- **Updated**: 2017-04-05 03:38:08+00:00
- **Authors**: Parag S. Chandakkar, Yilin Wang, Baoxin Li
- **Comment**: WACV 2015
- **Journal**: None
- **Summary**: Traffic congestion is a widespread problem. Dynamic traffic routing systems and congestion pricing are getting importance in recent research. Lane prediction and vehicle density estimation is an important component of such systems. We introduce a novel problem of vehicle self-positioning which involves predicting the number of lanes on the road and vehicle's position in those lanes using videos captured by a dashboard camera. We propose an integrated closed-loop approach where we use the presence of vehicles to aid the task of self-positioning and vice-versa. To incorporate multiple factors and high-level semantic knowledge into the solution, we formulate this problem as a Bayesian framework. In the framework, the number of lanes, the vehicle's position in those lanes and the presence of other vehicles are considered as parameters. We also propose a bounding box selection scheme to reduce the number of false detections and increase the computational efficiency. We show that the number of box proposals decreases by a factor of 6 using the selection approach. It also results in large reduction in the number of false detections. The entire approach is tested on real-world videos and is found to give acceptable results.



### Investigating Human Factors in Image Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.01262v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01262v1)
- **Published**: 2017-04-05 04:07:07+00:00
- **Updated**: 2017-04-05 04:07:07+00:00
- **Authors**: Parag S. Chandakkar, Baoxin Li
- **Comment**: ACM MM 2014
- **Journal**: None
- **Summary**: In today's age of internet and social media, one can find an enormous volume of forged images on-line. These images have been used in the past to convey falsified information and achieve harmful intentions. The spread and the effect of the social media only makes this problem more severe. While creating forged images has become easier due to software advancements, there is no automated algorithm which can reliably detect forgery.   Image forgery detection can be seen as a subset of image understanding problem. Human performance is still the gold-standard for these type of problems when compared to existing state-of-art automated algorithms. We conduct a subjective evaluation test with the aid of eye-tracker to investigate into human factors associated with this problem. We compare the performance of an automated algorithm and humans for forgery detection problem. We also develop an algorithm which uses the data from the evaluation test to predict the difficulty-level of an image (the difficulty-level of an image here denotes how difficult it is for humans to detect forgery in an image. Terms such as "Easy/difficult image" will be used in the same context). The experimental results presented in this paper should facilitate development of better algorithms in the future.



### Classification of Diabetic Retinopathy Images Using Multi-Class Multiple-Instance Learning Based on Color Correlogram Features
- **Arxiv ID**: http://arxiv.org/abs/1704.01264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01264v1)
- **Published**: 2017-04-05 04:20:13+00:00
- **Updated**: 2017-04-05 04:20:13+00:00
- **Authors**: Ragav Venkatesan, Parag S. Chandakkar, Baoxin Li
- **Comment**: EMBS 2012
- **Journal**: None
- **Summary**: All people with diabetes have the risk of developing diabetic retinopathy (DR), a vision-threatening complication. Early detection and timely treatment can reduce the occurrence of blindness due to DR. Computer-aided diagnosis has the potential benefit of improving the accuracy and speed in DR detection. This study is concerned with automatic classification of images with microaneurysm (MA) and neovascularization (NV), two important DR clinical findings. Together with normal images, this presents a 3-class classification problem. We propose a modified color auto-correlogram feature (AutoCC) with low dimensionality that is spectrally tuned towards DR images. Recognizing the fact that the images with or without MA or NV are generally different only in small, localized regions, we propose to employ a multi-class, multiple-instance learning framework for performing the classification task using the proposed feature. Extensive experiments including comparison with a few state-of-art image classification approaches have been performed and the results suggest that the proposed approach is promising as it outperforms other methods by a large margin.



### Supporting Navigation of Outdoor Shopping Complexes for Visually-impaired Users through Multi-modal Data Fusion
- **Arxiv ID**: http://arxiv.org/abs/1704.01266v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1704.01266v1)
- **Published**: 2017-04-05 04:24:41+00:00
- **Updated**: 2017-04-05 04:24:41+00:00
- **Authors**: Archana Paladugu, Parag S. Chandakkar, Peng Zhang, Baoxin Li
- **Comment**: ICME 2013
- **Journal**: None
- **Summary**: Outdoor shopping complexes (OSC) are extremely difficult for people with visual impairment to navigate. Existing GPS devices are mostly designed for roadside navigation and seldom transition well into an OSC-like setting. We report our study on the challenges faced by a blind person in navigating OSC through developing a new mobile application named iExplore. We first report an exploratory study aiming at deriving specific design principles for building this system by learning the unique challenges of the problem. Then we present a methodology that can be used to derive the necessary information for the development of iExplore, followed by experimental validation of the technology by a group of visually impaired users in a local outdoor shopping center. User feedback and other experiments suggest that iExplore, while at its very initial phase, has the potential of filling a practical gap in existing assistive technologies for the visually impaired.



### Smart Mining for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1704.01285v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01285v3)
- **Published**: 2017-04-05 06:58:56+00:00
- **Updated**: 2017-07-27 05:27:22+00:00
- **Authors**: Ben Harwood, Vijay Kumar B G, Gustavo Carneiro, Ian Reid, Tom Drummond
- **Comment**: *Vijay Kumar B G and Ben Harwood contributed equally to this work.
  Accepted in IEEE International Conference on Computer Vision, ICCV 2017
- **Journal**: None
- **Summary**: To solve deep metric learning problems and producing feature embeddings, current methodologies will commonly use a triplet model to minimise the relative distance between samples from the same class and maximise the relative distance between samples from different classes. Though successful, the training convergence of this triplet model can be compromised by the fact that the vast majority of the training samples will produce gradients with magnitudes that are close to zero. This issue has motivated the development of methods that explore the global structure of the embedding and other methods that explore hard negative/positive mining. The effectiveness of such mining methods is often associated with intractable computational requirements. In this paper, we propose a novel deep metric learning method that combines the triplet model and the global structure of the embedding space. We rely on a smart mining procedure that produces effective training samples for a low computational cost. In addition, we propose an adaptive controller that automatically adjusts the smart mining hyper-parameters and speeds up the convergence of the training process. We show empirically that our proposed method allows for fast and more accurate training of triplet ConvNets than other competing mining methods. Additionally, we show that our method achieves new state-of-the-art embedding results for CUB-200-2011 and Cars196 datasets.



### Automated Diagnosis of Epilepsy Employing Multifractal Detrended Fluctuation Analysis Based Features
- **Arxiv ID**: http://arxiv.org/abs/1704.01297v1
- **DOI**: None
- **Categories**: **cs.CV**, nlin.AO, nlin.CD, q-bio.QM, stat.OT, 92B25 92F99
- **Links**: [PDF](http://arxiv.org/pdf/1704.01297v1)
- **Published**: 2017-04-05 08:00:14+00:00
- **Updated**: 2017-04-05 08:00:14+00:00
- **Authors**: S Pratiher, S Chatterjee, R Bose
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: This contribution reports an application of MultiFractal Detrended Fluctuation Analysis, MFDFA based novel feature extraction technique for automated detection of epilepsy. In fractal geometry, Multifractal Detrended Fluctuation Analysis MFDFA is a popular technique to examine the self-similarity of a nonlinear, chaotic and noisy time series. In the present research work, EEG signals representing healthy, interictal (seizure free) and ictal activities (seizure) are acquired from an existing available database. The acquired EEG signals of different states are at first analyzed using MFDFA. To requisite the time series singularity quantification at local and global scales, a novel set of fourteen different features. Suitable feature ranking employing students t-test has been done to select the most statistically significant features which are henceforth being used as inputs to a support vector machines (SVM) classifier for the classification of different EEG signals. Eight different classification problems have been presented in this paper and it has been observed that the overall classification accuracy using MFDFA based features are reasonably satisfactory for all classification problems. The performance of the proposed method are also found to be quite commensurable and in some cases even better when compared with the results published in existing literature studied on the similar data set.



### Not All Pixels Are Equal: Difficulty-aware Semantic Segmentation via Deep Layer Cascade
- **Arxiv ID**: http://arxiv.org/abs/1704.01344v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.01344v1)
- **Published**: 2017-04-05 09:58:51+00:00
- **Updated**: 2017-04-05 09:58:51+00:00
- **Authors**: Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, Xiaoou Tang
- **Comment**: To appear in CVPR 2017 as a spotlight paper
- **Journal**: None
- **Summary**: We propose a novel deep layer cascade (LC) method to improve the accuracy and speed of semantic segmentation. Unlike the conventional model cascade (MC) that is composed of multiple independent models, LC treats a single deep model as a cascade of several sub-models. Earlier sub-models are trained to handle easy and confident regions, and they progressively feed-forward harder regions to the next sub-model for processing. Convolutions are only calculated on these regions to reduce computations. The proposed method possesses several advantages. First, LC classifies most of the easy regions in the shallow stage and makes deeper stage focuses on a few hard regions. Such an adaptive and 'difficulty-aware' learning improves segmentation performance. Second, LC accelerates both training and testing of deep network thanks to early decisions in the shallow stage. Third, in comparison to MC, LC is an end-to-end trainable framework, allowing joint learning of all sub-models. We evaluate our method on PASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and fast speed.



### Incremental Tube Construction for Human Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.01358v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01358v2)
- **Published**: 2017-04-05 10:37:42+00:00
- **Updated**: 2018-07-23 18:20:21+00:00
- **Authors**: Harkirat Singh Behl, Michael Sapienza, Gurkirt Singh, Suman Saha, Fabio Cuzzolin, Philip H. S. Torr
- **Comment**: British Machine Vision Conference (BMVC) 2018
- **Journal**: None
- **Summary**: Current state-of-the-art action detection systems are tailored for offline batch-processing applications. However, for online applications like human-robot interaction, current systems fall short, either because they only detect one action per video, or because they assume that the entire video is available ahead of time. In this work, we introduce a real-time and online joint-labelling and association algorithm for action detection that can incrementally construct space-time action tubes on the most challenging action videos in which different action categories occur concurrently. In contrast to previous methods, we solve the detection-window association and action labelling problems jointly in a single pass. We demonstrate superior online association accuracy and speed (2.2ms per frame) as compared to the current state-of-the-art offline systems. We further demonstrate that the entire action detection pipeline can easily be made to work effectively in real-time using our action tube construction algorithm.



### On the Relation between Color Image Denoising and Classification
- **Arxiv ID**: http://arxiv.org/abs/1704.01372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01372v1)
- **Published**: 2017-04-05 11:28:25+00:00
- **Updated**: 2017-04-05 11:28:25+00:00
- **Authors**: Jiqing Wu, Radu Timofte, Zhiwu Huang, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Large amount of image denoising literature focuses on single channel images and often experimentally validates the proposed methods on tens of images at most. In this paper, we investigate the interaction between denoising and classification on large scale dataset. Inspired by classification models, we propose a novel deep learning architecture for color (multichannel) image denoising and report on thousands of images from ImageNet dataset as well as commonly used imagery. We study the importance of (sufficient) training data, how semantic class information can be traded for improved denoising results. As a result, our method greatly improves PSNR performance by 0.34 - 0.51 dB on average over state-of-the art methods on large scale dataset. We conclude that it is beneficial to incorporate in classification models. On the other hand, we also study how noise affect classification performance. In the end, we come to a number of interesting conclusions, some being counter-intuitive.



### The UMCD Dataset
- **Arxiv ID**: http://arxiv.org/abs/1704.01426v1
- **DOI**: 10.1109/TSMC.2018.2804766
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01426v1)
- **Published**: 2017-04-05 13:49:27+00:00
- **Updated**: 2017-04-05 13:49:27+00:00
- **Authors**: Danilo Avola, Gian Luca Foresti, Niki Martinel, Daniele Pannone, Claudio Piciarelli
- **Comment**: 3 pages, 5 figures
- **Journal**: IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2018
- **Summary**: In recent years, the technological improvements of low-cost small-scale Unmanned Aerial Vehicles (UAVs) are promoting an ever-increasing use of them in different tasks. In particular, the use of small-scale UAVs is useful in all these low-altitude tasks in which common UAVs cannot be adopted, such as recurrent comprehensive view of wide environments, frequent monitoring of military areas, real-time classification of static and moving entities (e.g., people, cars, etc.). These tasks can be supported by mosaicking and change detection algorithms achieved at low-altitude. Currently, public datasets for testing these algorithms are not available. This paper presents the UMCD dataset, the first collection of geo-referenced video sequences acquired at low-altitude for mosaicking and change detection purposes. Five reference scenarios are also reported.



### Non-Convex Weighted Lp Minimization based Group Sparse Representation Framework for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1704.01429v3
- **DOI**: 10.1109/LSP.2017.2731791
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01429v3)
- **Published**: 2017-04-05 13:53:25+00:00
- **Updated**: 2017-05-23 01:43:29+00:00
- **Authors**: Qiong Wang, Xinggan Zhang, Yu Wu, Lan Tang, Zhiyuan Zha
- **Comment**: None
- **Journal**: None
- **Summary**: Nonlocal image representation or group sparsity has attracted considerable interest in various low-level vision tasks and has led to several state-of-the-art image denoising techniques, such as BM3D, LSSC. In the past, convex optimization with sparsity-promoting convex regularization was usually regarded as a standard scheme for estimating sparse signals in noise. However, using convex regularization can not still obtain the correct sparsity solution under some practical problems including image inverse problems. In this paper we propose a non-convex weighted $\ell_p$ minimization based group sparse representation (GSR) framework for image denoising. To make the proposed scheme tractable and robust, the generalized soft-thresholding (GST) algorithm is adopted to solve the non-convex $\ell_p$ minimization problem. In addition, to improve the accuracy of the nonlocal similar patches selection, an adaptive patch search (APS) scheme is proposed. Experimental results have demonstrated that the proposed approach not only outperforms many state-of-the-art denoising methods such as BM3D and WNNM, but also results in a competitive speed.



### Convolutional Neural Networks for Page Segmentation of Historical Document Images
- **Arxiv ID**: http://arxiv.org/abs/1704.01474v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.01474v2)
- **Published**: 2017-04-05 15:12:25+00:00
- **Updated**: 2017-04-07 10:16:49+00:00
- **Authors**: Kai Chen, Mathias Seuret
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a Convolutional Neural Network (CNN) based page segmentation method for handwritten historical document images. We consider page segmentation as a pixel labeling problem, i.e., each pixel is classified as one of the predefined classes. Traditional methods in this area rely on carefully hand-crafted features or large amounts of prior knowledge. In contrast, we propose to learn features from raw image pixels using a CNN. While many researchers focus on developing deep CNN architectures to solve different problems, we train a simple CNN with only one convolution layer. We show that the simple architecture achieves competitive results against other deep architectures on different public datasets. Experiments also demonstrate the effectiveness and superiority of the proposed method compared to previous methods.



### Weakly Supervised Dense Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1704.01502v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01502v1)
- **Published**: 2017-04-05 16:06:09+00:00
- **Updated**: 2017-04-05 16:06:09+00:00
- **Authors**: Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong Chen, Yu-Gang Jiang, Xiangyang Xue
- **Comment**: To appear in CVPR 2017
- **Journal**: None
- **Summary**: This paper focuses on a novel and challenging vision task, dense video captioning, which aims to automatically describe a video clip with multiple informative and diverse caption sentences. The proposed method is trained without explicit annotation of fine-grained sentence to video region-sequence correspondence, but is only based on weak video-level sentence annotations. It differs from existing video captioning systems in three technical aspects. First, we propose lexical fully convolutional neural networks (Lexical-FCN) with weakly supervised multi-instance multi-label learning to weakly link video regions with lexical labels. Second, we introduce a novel submodular maximization scheme to generate multiple informative and diverse region-sequences based on the Lexical-FCN outputs. A winner-takes-all scheme is adopted to weakly associate sentences to region-sequences in the training phase. Third, a sequence-to-sequence learning based language model is trained with the weakly supervised information obtained through the association process. We show that the proposed method can not only produce informative and diverse dense captions, but also outperform state-of-the-art single video captioning methods by a large margin.



### Isotropic reconstruction of 3D fluorescence microscopy images using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1704.01510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01510v1)
- **Published**: 2017-04-05 16:20:36+00:00
- **Updated**: 2017-04-05 16:20:36+00:00
- **Authors**: Martin Weigert, Loic Royer, Florian Jug, Gene Myers
- **Comment**: None
- **Journal**: None
- **Summary**: Fluorescence microscopy images usually show severe anisotropy in axial versus lateral resolution. This hampers downstream processing, i.e. the automatic extraction of quantitative biological data. While deconvolution methods and other techniques to address this problem exist, they are either time consuming to apply or limited in their ability to remove anisotropy. We propose a method to recover isotropic resolution from readily acquired anisotropic data. We achieve this using a convolutional neural network that is trained end-to-end from the same anisotropic body of data we later apply the network to. The network effectively learns to restore the full isotropic resolution by restoring the image under a trained, sample specific image prior. We apply our method to $3$ synthetic and $3$ real datasets and show that our results improve on results from deconvolution and state-of-the-art super-resolution techniques. Finally, we demonstrate that a standard 3D segmentation pipeline performs on the output of our network with comparable accuracy as on the full isotropic data.



### Generating Descriptions with Grounded and Co-Referenced People
- **Arxiv ID**: http://arxiv.org/abs/1704.01518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01518v1)
- **Published**: 2017-04-05 16:36:13+00:00
- **Updated**: 2017-04-05 16:36:13+00:00
- **Authors**: Anna Rohrbach, Marcus Rohrbach, Siyu Tang, Seong Joon Oh, Bernt Schiele
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: Learning how to generate descriptions of images or videos received major interest both in the Computer Vision and Natural Language Processing communities. While a few works have proposed to learn a grounding during the generation process in an unsupervised way (via an attention mechanism), it remains unclear how good the quality of the grounding is and whether it benefits the description quality. In this work we propose a movie description model which learns to generate description and jointly ground (localize) the mentioned characters as well as do visual co-reference resolution between pairs of consecutive sentences/clips. We also propose to use weak localization supervision through character mentions provided in movie descriptions to learn the character grounding. At training time, we first learn how to localize characters by relating their visual appearance to mentions in the descriptions via a semi-supervised approach. We then provide this (noisy) supervision into our description model which greatly improves its performance. Our proposed description model improves over prior work w.r.t. generated description quality and additionally provides grounding and local co-reference resolution. We evaluate it on the MPII Movie Description dataset using automatic and human evaluation measures and using our newly collected grounding and co-reference data for characters.



### The Relative Performance of Ensemble Methods with Deep Convolutional Neural Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1704.01664v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1704.01664v1)
- **Published**: 2017-04-05 23:04:43+00:00
- **Updated**: 2017-04-05 23:04:43+00:00
- **Authors**: Cheng Ju, Aurélien Bibaut, Mark J. van der Laan
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial neural networks have been successfully applied to a variety of machine learning tasks, including image recognition, semantic segmentation, and machine translation. However, few studies fully investigated ensembles of artificial neural networks. In this work, we investigated multiple widely used ensemble methods, including unweighted averaging, majority voting, the Bayes Optimal Classifier, and the (discrete) Super Learner, for image recognition tasks, with deep neural networks as candidate algorithms. We designed several experiments, with the candidate algorithms being the same network structure with different model checkpoints within a single training process, networks with same structure but trained multiple times stochastically, and networks with different structure. In addition, we further studied the over-confidence phenomenon of the neural networks, as well as its impact on the ensemble methods. Across all of our experiments, the Super Learner achieved best performance among all the ensemble methods in this study.



