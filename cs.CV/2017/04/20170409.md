# Arxiv Papers in cs.CV on 2017-04-09
### Motion Saliency Based Automatic Delineation of Glottis Contour in High-speed Digital Images
- **Arxiv ID**: http://arxiv.org/abs/1704.02567v1
- **DOI**: 10.1109/ICIEA.2017.8282998
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02567v1)
- **Published**: 2017-04-09 06:27:27+00:00
- **Updated**: 2017-04-09 06:27:27+00:00
- **Authors**: Xin Chen, Emma Marriott, Yuling Yan
- **Comment**: 4 pages 2 figures
- **Journal**: None
- **Summary**: In recent years, high-speed videoendoscopy (HSV) has significantly aided the diagnosis of voice pathologies and furthered the understanding the voice production in recent years. As the first step of these studies, automatic segmentation of glottal images till presents a major challenge for this technique. In this paper, we propose an improved Saliency Network that automatically delineates the contour of the glottis from HSV image sequences. Our proposed additional saliency measure, Motion Saliency (MS), improves upon the original Saliency Network by using the velocities of defined edges. In our results and analysis, we demonstrate the effectiveness of our approach and discuss its potential applications for computer-aided assessment of voice pathologies and understanding voice production.



### Modeling Temporal Dynamics and Spatial Configurations of Actions Using Two-Stream Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.02581v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02581v2)
- **Published**: 2017-04-09 10:09:55+00:00
- **Updated**: 2017-04-12 09:08:19+00:00
- **Authors**: Hongsong Wang, Liang Wang
- **Comment**: Accepted to IEEE International Conference on Computer Vision and
  Pattern Recognition (CVPR) 2017
- **Journal**: None
- **Summary**: Recently, skeleton based action recognition gains more popularity due to cost-effective depth sensors coupled with real-time skeleton estimation algorithms. Traditional approaches based on handcrafted features are limited to represent the complexity of motion patterns. Recent methods that use Recurrent Neural Networks (RNN) to handle raw skeletons only focus on the contextual dependency in the temporal domain and neglect the spatial configurations of articulated skeletons. In this paper, we propose a novel two-stream RNN architecture to model both temporal dynamics and spatial configurations for skeleton based action recognition. We explore two different structures for the temporal stream: stacked RNN and hierarchical RNN. Hierarchical RNN is designed according to human body kinematics. We also propose two effective methods to model the spatial structure by converting the spatial graph into a sequence of joints. To improve generalization of our model, we further exploit 3D transformation based data augmentation techniques including rotation and scaling transformation to transform the 3D coordinates of skeletons during training. Experiments on 3D action recognition benchmark datasets show that our method brings a considerable improvement for a variety of actions, i.e., generic actions, interaction activities and gestures.



### Automatic Image Filtering on Social Networks Using Deep Learning and Perceptual Hashing During Crises
- **Arxiv ID**: http://arxiv.org/abs/1704.02602v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1704.02602v1)
- **Published**: 2017-04-09 13:34:27+00:00
- **Updated**: 2017-04-09 13:34:27+00:00
- **Authors**: Dat Tien Nguyen, Firoj Alam, Ferda Ofli, Muhammad Imran
- **Comment**: Accepted for publication in the 14th International Conference on
  Information Systems For Crisis Response and Management (ISCRAM), 2017
- **Journal**: None
- **Summary**: The extensive use of social media platforms, especially during disasters, creates unique opportunities for humanitarian organizations to gain situational awareness and launch relief operations accordingly. In addition to the textual content, people post overwhelming amounts of imagery data on social networks within minutes of a disaster hit. Studies point to the importance of this online imagery content for emergency response. Despite recent advances in the computer vision field, automatic processing of the crisis-related social media imagery data remains a challenging task. It is because a majority of which consists of redundant and irrelevant content. In this paper, we present an image processing pipeline that comprises de-duplication and relevancy filtering mechanisms to collect and filter social media image content in real-time during a crisis event. Results obtained from extensive experiments on real-world crisis datasets demonstrate the significance of the proposed pipeline for optimal utilization of both human and machine computing resources.



### BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis
- **Arxiv ID**: http://arxiv.org/abs/1704.02612v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02612v2)
- **Published**: 2017-04-09 15:00:31+00:00
- **Updated**: 2017-12-09 17:05:42+00:00
- **Authors**: Shanxin Yuan, Qi Ye, Bjorn Stenger, Siddhant Jain, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce a large-scale hand pose dataset, collected using a novel capture method. Existing datasets are either generated synthetically or captured using depth sensors: synthetic datasets exhibit a certain level of appearance difference from real depth images, and real datasets are limited in quantity and coverage, mainly due to the difficulty to annotate them. We propose a tracking system with six 6D magnetic sensors and inverse kinematics to automatically obtain 21-joints hand pose annotations of depth maps captured with minimal restriction on the range of motion. The capture protocol aims to fully cover the natural hand pose space. As shown in embedding plots, the new dataset exhibits a significantly wider and denser range of hand poses compared to existing benchmarks. Current state-of-the-art methods are evaluated on the dataset, and we demonstrate significant improvements in cross-benchmark performance. We also show significant improvements in egocentric hand pose estimation with a CNN trained on the new dataset.



### Quaternion Based Camera Pose Estimation From Matched Feature Points
- **Arxiv ID**: http://arxiv.org/abs/1704.02672v2
- **DOI**: 10.1109/LRA.2018.2792142
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1704.02672v2)
- **Published**: 2017-04-09 23:29:55+00:00
- **Updated**: 2017-04-11 18:41:05+00:00
- **Authors**: Kaveh Fathian, J. Pablo Ramirez-Paredes, Emily A. Doucette, J. Willard Curtis, Nicholas R. Gans
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel solution to the camera pose estimation problem, where rotation and translation of a camera between two views are estimated from matched feature points in the images. The camera pose estimation problem is traditionally solved via algorithms that are based on the essential matrix or the Euclidean homography. With six or more feature points in general positions in the space, essential matrix based algorithms can recover a unique solution. However, such algorithms fail when points are on critical surfaces (e.g., coplanar points) and homography should be used instead. By formulating the problem in quaternions and decoupling the rotation and translation estimation, our proposed algorithm works for all point configurations. Using both simulated and real world images, we compare the estimation accuracy of our algorithm with some of the most commonly used algorithms. Our method is shown to be more robust to noise and outliers. For the benefit of community, we have made the implementation of our algorithm available online and free.



