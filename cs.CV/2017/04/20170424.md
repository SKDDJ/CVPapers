# Arxiv Papers in cs.CV on 2017-04-24
### Model-based Iterative Restoration for Binary Document Image Compression with Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1704.07019v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07019v1)
- **Published**: 2017-04-24 02:31:37+00:00
- **Updated**: 2017-04-24 02:31:37+00:00
- **Authors**: Yandong Guo, Cheng Lu, Jan P. Allebach, Charles A. Bouman
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: The inherent noise in the observed (e.g., scanned) binary document image degrades the image quality and harms the compression ratio through breaking the pattern repentance and adding entropy to the document images. In this paper, we design a cost function in Bayesian framework with dictionary learning. Minimizing our cost function produces a restored image which has better quality than that of the observed noisy image, and a dictionary for representing and encoding the image. After the restoration, we use this dictionary (from the same cost function) to encode the restored image following the symbol-dictionary framework by JBIG2 standard with the lossless mode. Experimental results with a variety of document images demonstrate that our method improves the image quality compared with the observed image, and simultaneously improves the compression ratio. For the test images with synthetic noise, our method reduces the number of flipped pixels by 48.2% and improves the compression ratio by 36.36% as compared with the best encoding methods. For the test images with real noise, our method visually improves the image quality, and outperforms the cutting-edge method by 28.27% in terms of the compression ratio.



### Group-based Sparse Representation for Image Compressive Sensing Reconstruction with Non-Convex Regularization
- **Arxiv ID**: http://arxiv.org/abs/1704.07023v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07023v2)
- **Published**: 2017-04-24 03:17:20+00:00
- **Updated**: 2017-08-17 03:58:42+00:00
- **Authors**: Zhiyuan Zha, Xinggan Zhang, Qiong Wang, Lan Tang, Xin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Patch-based sparse representation modeling has shown great potential in image compressive sensing (CS) reconstruction. However, this model usually suffers from some limits, such as dictionary learning with great computational complexity, neglecting the relationship among similar patches. In this paper, a group-based sparse representation method with non-convex regularization (GSR-NCR) for image CS reconstruction is proposed. In GSR-NCR, the local sparsity and nonlocal self-similarity of images is simultaneously considered in a unified framework. Different from the previous methods based on sparsity-promoting convex regularization, we extend the non-convex weighted Lp (0 < p < 1) penalty function on group sparse coefficients of the data matrix, rather than conventional L1-based regularization. To reduce the computational complexity, instead of learning the dictionary with a high computational complexity from natural images, we learn the principle component analysis (PCA) based dictionary for each group. Moreover, to make the proposed scheme tractable and robust, we have developed an efficient iterative shrinkage/thresholding algorithm to solve the non-convex optimization problem. Experimental results demonstrate that the proposed method outperforms many state-of-the-art techniques for image CS reconstruction.



### Non-Convex Weighted Lp Nuclear Norm based ADMM Framework for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1704.07056v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07056v2)
- **Published**: 2017-04-24 07:02:53+00:00
- **Updated**: 2017-06-27 05:33:48+00:00
- **Authors**: Zhiyuan Zha, Xinggan Zhang, Yu Wu, Qiong Wang, Lan Tang
- **Comment**: arXiv admin note: text overlap with arXiv:1611.08983
- **Journal**: None
- **Summary**: Since the matrix formed by nonlocal similar patches in a natural image is of low rank, the nuclear norm minimization (NNM) has been widely used in various image processing studies. Nonetheless, nuclear norm based convex surrogate of the rank function usually over-shrinks the rank components and makes different components equally, and thus may produce a result far from the optimum. To alleviate the above-mentioned limitations of the nuclear norm, in this paper we propose a new method for image restoration via the non-convex weighted Lp nuclear norm minimization (NCW-NNM), which is able to more accurately enforce the image structural sparsity and self-similarity simultaneously. To make the proposed model tractable and robust, the alternative direction multiplier method (ADMM) is adopted to solve the associated non-convex minimization problem. Experimental results on various types of image restoration problems, including image deblurring, image inpainting and image compressive sensing (CS) recovery, demonstrate that the proposed method outperforms many current state-of-the-art methods in both the objective and the perceptual qualities.



### A Dual Sparse Decomposition Method for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1704.07063v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07063v1)
- **Published**: 2017-04-24 07:28:32+00:00
- **Updated**: 2017-04-24 07:28:32+00:00
- **Authors**: Hong Sun, Chen-guang Liu, Cheng-wei Sang
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: This article addresses the image denoising problem in the situations of strong noise. We propose a dual sparse decomposition method. This method makes a sub-dictionary decomposition on the over-complete dictionary in the sparse decomposition. The sub-dictionary decomposition makes use of a novel criterion based on the occurrence frequency of atoms of the over-complete dictionary over the data set. The experimental results demonstrate that the dual-sparse-decomposition method surpasses state-of-art denoising performance in terms of both peak-signal-to-noise ratio and structural-similarity-index-metric, and also at subjective visual quality.



### Camera Pose Filtering with Local Regression Geodesics on the Riemannian Manifold of Dual Quaternions
- **Arxiv ID**: http://arxiv.org/abs/1704.07072v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07072v4)
- **Published**: 2017-04-24 07:52:41+00:00
- **Updated**: 2017-08-29 16:28:16+00:00
- **Authors**: Benjamin Busam, Tolga Birdal, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Time-varying, smooth trajectory estimation is of great interest to the vision community for accurate and well behaving 3D systems. In this paper, we propose a novel principal component local regression filter acting directly on the Riemannian manifold of unit dual quaternions $\mathbb{D} \mathbb{H}_1$. We use a numerically stable Lie algebra of the dual quaternions together with $\exp$ and $\log$ operators to locally linearize the 6D pose space. Unlike state of the art path smoothing methods which either operate on $SO\left(3\right)$ of rotation matrices or the hypersphere $\mathbb{H}_1$ of quaternions, we treat the orientation and translation jointly on the dual quaternion quadric in the 7-dimensional real projective space $\mathbb{R}\mathbb{P}^7$. We provide an outlier-robust IRLS algorithm for generic pose filtering exploiting this manifold structure. Besides our theoretical analysis, our experiments on synthetic and real data show the practical advantages of the manifold aware filtering on pose tracking and smoothing.



### Exploiting Multi-layer Graph Factorization for Multi-attributed Graph Matching
- **Arxiv ID**: http://arxiv.org/abs/1704.07077v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07077v1)
- **Published**: 2017-04-24 08:08:32+00:00
- **Updated**: 2017-04-24 08:08:32+00:00
- **Authors**: Han-Mu Park, Kuk-Jin Yoon
- **Comment**: 10 pages, 4 figures, conference submitted
- **Journal**: None
- **Summary**: Multi-attributed graph matching is a problem of finding correspondences between two sets of data while considering their complex properties described in multiple attributes. However, the information of multiple attributes is likely to be oversimplified during a process that makes an integrated attribute, and this degrades the matching accuracy. For that reason, a multi-layer graph structure-based algorithm has been proposed recently. It can effectively avoid the problem by separating attributes into multiple layers. Nonetheless, there are several remaining issues such as a scalability problem caused by the huge matrix to describe the multi-layer structure and a back-projection problem caused by the continuous relaxation of the quadratic assignment problem. In this work, we propose a novel multi-attributed graph matching algorithm based on the multi-layer graph factorization. We reformulate the problem to be solved with several small matrices that are obtained by factorizing the multi-layer structure. Then, we solve the problem using a convex-concave relaxation procedure for the multi-layer structure. The proposed algorithm exhibits better performance than state-of-the-art algorithms based on the single-layer structure.



### Target Oriented High Resolution SAR Image Formation via Semantic Information Guided Regularizations
- **Arxiv ID**: http://arxiv.org/abs/1704.07082v1
- **DOI**: 10.1109/TGRS.2017.2769808
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07082v1)
- **Published**: 2017-04-24 08:27:06+00:00
- **Updated**: 2017-04-24 08:27:06+00:00
- **Authors**: Biao Hou, Zaidao Wen, Licheng Jiao, Qian Wu
- **Comment**: Submitted to IEEE TGRS
- **Journal**: None
- **Summary**: Sparsity-regularized synthetic aperture radar (SAR) imaging framework has shown its remarkable performance to generate a feature enhanced high resolution image, in which a sparsity-inducing regularizer is involved by exploiting the sparsity priors of some visual features in the underlying image. However, since the simple prior of low level features are insufficient to describe different semantic contents in the image, this type of regularizer will be incapable of distinguishing between the target of interest and unconcerned background clutters. As a consequence, the features belonging to the target and clutters are simultaneously affected in the generated image without concerning their underlying semantic labels. To address this problem, we propose a novel semantic information guided framework for target oriented SAR image formation, which aims at enhancing the interested target scatters while suppressing the background clutters. Firstly, we develop a new semantics-specific regularizer for image formation by exploiting the statistical properties of different semantic categories in a target scene SAR image. In order to infer the semantic label for each pixel in an unsupervised way, we moreover induce a novel high-level prior-driven regularizer and some semantic causal rules from the prior knowledge. Finally, our regularized framework for image formation is further derived as a simple iteratively reweighted $\ell_1$ minimization problem which can be conveniently solved by many off-the-shelf solvers. Experimental results demonstrate the effectiveness and superiority of our framework for SAR image formation in terms of target enhancement and clutters suppression, compared with the state of the arts. Additionally, the proposed framework opens a new direction of devoting some machine learning strategies to image formation, which can benefit the subsequent decision making tasks.



### Unified Framework for Automated Person Re-identification and Camera Network Topology Inference in Camera Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.07085v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07085v5)
- **Published**: 2017-04-24 08:39:16+00:00
- **Updated**: 2017-10-02 03:45:03+00:00
- **Authors**: Yeong-Jun Cho, Jae-Han Park, Su-A Kim, Kyuewang Lee, Kuk-Jin Yoon
- **Comment**: Accepted to International Workshop on Cross-domain Human
  Identification (in conjunction with ICCV), 2017
- **Journal**: None
- **Summary**: Person re-identification in large-scale multi-camera networks is a challenging task because of the spatio-temporal uncertainty and high complexity due to large numbers of cameras and people. To handle these difficulties, additional information such as camera network topology should be provided, which is also difficult to automatically estimate. In this paper, we propose a unified framework which jointly solves both person re-id and camera network topology inference problems. The proposed framework takes general multi-camera network environments into account. To effectively show the superiority of the proposed framework, we also provide a new person re-id dataset with full annotations, named SLP, captured in the synchronized multi-camera network. Experimental results show that the proposed methods are promising for both person re-id and camera topology inference tasks.



### Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets
- **Arxiv ID**: http://arxiv.org/abs/1704.07121v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.07121v2)
- **Published**: 2017-04-24 10:05:19+00:00
- **Updated**: 2018-06-10 20:34:21+00:00
- **Authors**: Wei-Lun Chao, Hexiang Hu, Fei Sha
- **Comment**: Accepted for Oral Presentation at NAACL-HLT 2018
- **Journal**: None
- **Summary**: Visual question answering (Visual QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve. In this paper, we study a crucial component of this task: how can we design good datasets for the task? We focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of candidate ones including the target (\ie the correct one) and the decoys (\ie the incorrect ones). Through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets, we show that the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets. In particular, the resulting learner can ignore the visual information, the question, or both while still doing well on the task. Inspired by this, we propose automatic procedures to remedy such design deficiencies. We apply the procedures to re-construct decoy answers for two popular Visual QA datasets as well as to create a new Visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task. Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models. The datasets are released and publicly available via http://www.teds.usc.edu/website_vqa/.



### An Analysis of Action Recognition Datasets for Language and Vision Tasks
- **Arxiv ID**: http://arxiv.org/abs/1704.07129v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.07129v1)
- **Published**: 2017-04-24 10:38:23+00:00
- **Updated**: 2017-04-24 10:38:23+00:00
- **Authors**: Spandana Gella, Frank Keller
- **Comment**: To appear in Proceedings of ACL 2017, 8 pages
- **Journal**: None
- **Summary**: A large amount of recent research has focused on tasks that combine language and vision, resulting in a proliferation of datasets and methods. One such task is action recognition, whose applications include image annotation, scene under- standing and image retrieval. In this survey, we categorize the existing ap- proaches based on how they conceptualize this problem and provide a detailed review of existing datasets, highlighting their di- versity as well as advantages and disad- vantages. We focus on recently devel- oped datasets which link visual informa- tion with linguistic resources and provide a fine-grained syntactic and semantic anal- ysis of actions in images.



### Dense 3D Facial Reconstruction from a Single Depth Image in Unconstrained Environment
- **Arxiv ID**: http://arxiv.org/abs/1704.07142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07142v1)
- **Published**: 2017-04-24 10:58:47+00:00
- **Updated**: 2017-04-24 10:58:47+00:00
- **Authors**: Shu Zhang, Hui Yu, Ting Wang, Junyu Dong, Honghai Liu
- **Comment**: None
- **Journal**: None
- **Summary**: With the increasing demands of applications in virtual reality such as 3D films, virtual Human-Machine Interactions and virtual agents, the analysis of 3D human face analysis is considered to be more and more important as a fundamental step for those virtual reality tasks. Due to information provided by an additional dimension, 3D facial reconstruction enables aforementioned tasks to be achieved with higher accuracy than those based on 2D facial analysis. The denser the 3D facial model is, the more information it could provide. However, most existing dense 3D facial reconstruction methods require complicated processing and high system cost. To this end, this paper presents a novel method that simplifies the process of dense 3D facial reconstruction by employing only one frame of depth data obtained with an off-the-shelf RGB-D sensor. The experiments showed competitive results with real world data.



### Body Joint guided 3D Deep Convolutional Descriptors for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.07160v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07160v2)
- **Published**: 2017-04-24 11:58:24+00:00
- **Updated**: 2017-04-25 15:08:05+00:00
- **Authors**: Congqi Cao, Yifan Zhang, Chunjie Zhang, Hanqing Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Three dimensional convolutional neural networks (3D CNNs) have been established as a powerful tool to simultaneously learn features from both spatial and temporal dimensions, which is suitable to be applied to video-based action recognition. In this work, we propose not to directly use the activations of fully-connected layers of a 3D CNN as the video feature, but to use selective convolutional layer activations to form a discriminative descriptor for video. It pools the feature on the convolutional layers under the guidance of body joint positions. Two schemes of mapping body joints into convolutional feature maps for pooling are discussed. The body joint positions can be obtained from any off-the-shelf skeleton estimation algorithm. The helpfulness of the body joint guided feature pooling with inaccurate skeleton estimation is systematically evaluated. To make it end-to-end and do not rely on any sophisticated body joint detection algorithm, we further propose a two-stream bilinear model which can learn the guidance from the body joints and capture the spatio-temporal features simultaneously. In this model, the body joint guided feature pooling is conveniently formulated as a bilinear product operation. Experimental results on three real-world datasets demonstrate the effectiveness of body joint guided pooling which achieves promising performance.



### Monocular Visual Odometry with a Rolling Shutter Camera
- **Arxiv ID**: http://arxiv.org/abs/1704.07163v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07163v1)
- **Published**: 2017-04-24 12:02:53+00:00
- **Updated**: 2017-04-24 12:02:53+00:00
- **Authors**: Chang-Ryeol Lee, Kuk-Jin Yoon
- **Comment**: 14 pages, 8 figures
- **Journal**: None
- **Summary**: Rolling Shutter (RS) cameras have become popularized because of low-cost imaging capability. However, the RS cameras suffer from undesirable artifacts when the camera or the subject is moving, or illumination condition changes. For that reason, Monocular Visual Odometry (MVO) with RS cameras produces inaccurate ego-motion estimates. Previous works solve this RS distortion problem with motion prediction from images and/or inertial sensors. However, the MVO still has trouble in handling the RS distortion when the camera motion changes abruptly (e.g. vibration of mobile cameras causes extremely fast motion instantaneously). To address the problem, we propose the novel MVO algorithm in consideration of the geometric characteristics of RS cameras. The key idea of the proposed algorithm is the new RS essential matrix which incorporates the instantaneous angular and linear velocities at each frame. Our algorithm produces accurate and robust ego-motion estimates in an online manner, and is applicable to various mobile applications with RS cameras. The superiority of the proposed algorithm is validated through quantitative and qualitative comparison on both synthetic and real dataset.



### Automatic Liver Lesion Segmentation Using A Deep Convolutional Neural Network Method
- **Arxiv ID**: http://arxiv.org/abs/1704.07239v1
- **DOI**: 10.1002/mp.12155
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07239v1)
- **Published**: 2017-04-24 13:58:29+00:00
- **Updated**: 2017-04-24 13:58:29+00:00
- **Authors**: Xiao Han
- **Comment**: Submission for ISBI'2017 LiTS Challenge ISIC2017
- **Journal**: None
- **Summary**: Liver lesion segmentation is an important step for liver cancer diagnosis, treatment planning and treatment evaluation. LiTS (Liver Tumor Segmentation Challenge) provides a common testbed for comparing different automatic liver lesion segmentation methods. We participate in this challenge by developing a deep convolutional neural network (DCNN) method. The particular DCNN model works in 2.5D in that it takes a stack of adjacent slices as input and produces the segmentation map corresponding to the center slice. The model has 32 layers in total and makes use of both long range concatenation connections of U-Net [1] and short-range residual connections from ResNet [2]. The model was trained using the 130 LiTS training datasets and achieved an average Dice score of 0.67 when evaluated on the 70 test CT scans, which ranked first for the LiTS challenge at the time of the ISBI 2017 conference.



### Supervised Adversarial Networks for Image Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.07242v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07242v2)
- **Published**: 2017-04-24 14:06:18+00:00
- **Updated**: 2017-04-26 01:37:03+00:00
- **Authors**: Hengyue Pan, Hui Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: In the past few years, Generative Adversarial Network (GAN) became a prevalent research topic. By defining two convolutional neural networks (G-Network and D-Network) and introducing an adversarial procedure between them during the training process, GAN has ability to generate good quality images that look like natural images from a random vector. Besides image generation, GAN may have potential to deal with wide range of real world problems. In this paper, we follow the basic idea of GAN and propose a novel model for image saliency detection, which is called Supervised Adversarial Networks (SAN). Specifically, SAN also trains two models simultaneously: the G-Network takes natural images as inputs and generates corresponding saliency maps (synthetic saliency maps), and the D-Network is trained to determine whether one sample is a synthetic saliency map or ground-truth saliency map. However, different from GAN, the proposed method uses fully supervised learning to learn both G-Network and D-Network by applying class labels of the training set. Moreover, a novel kind of layer call conv-comparison layer is introduced into the D-Network to further improve the saliency performance by forcing the high-level feature of synthetic saliency maps and ground-truthes as similar as possible. Experimental results on Pascal VOC 2012 database show that the SAN model can generate high quality saliency maps for many complicate natural images.



### Fast PET reconstruction using Multi-scale Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.07244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07244v1)
- **Published**: 2017-04-24 14:09:22+00:00
- **Updated**: 2017-04-24 14:09:22+00:00
- **Authors**: Jieqing Jiao, Sebastien Ourselin
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstruction of PET images is an ill-posed inverse problem and often requires iterative algorithms to achieve good image quality for reliable clinical use in practice, at huge computational costs. In this paper, we consider the PET reconstruction a dense prediction problem where the large scale contextual information is essential, and propose a novel architecture of multi-scale fully convolutional neural networks (msfCNN) for fast PET image reconstruction. The proposed msfCNN gains large receptive fields with both memory and computational efficiency, by using a downscaling-upscaling structure and dilated convolutions. Instead of pooling and deconvolution, we propose to use the periodic shuffling operation from sub-pixel convolution and its inverse to scale the size of feature maps without losing resolution. Residual connections were added to improve training. We trained the proposed msfCNN model with simulated data, and applied it to clinical PET data acquired on a Siemens mMR scanner. The results from real oncological and neurodegenerative cases show that the proposed msfCNN-based reconstruction outperforms the iterative approaches in terms of computational time while achieving comparable image quality for quantification. The proposed msfCNN model can be applied to other dense prediction tasks, and fast msfCNN-based PET reconstruction could facilitate the potential use of molecular imaging in interventional/surgical procedures, where cancer surgery can particularly benefit.



### Measuring the Accuracy of Object Detectors and Trackers
- **Arxiv ID**: http://arxiv.org/abs/1704.07293v1
- **DOI**: 10.1007/978-3-319-66709-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07293v1)
- **Published**: 2017-04-24 15:41:35+00:00
- **Updated**: 2017-04-24 15:41:35+00:00
- **Authors**: Tobias Bottger, Patrick Follmann, Michael Fauser
- **Comment**: 10 pages, 7 Figures
- **Journal**: None
- **Summary**: The accuracy of object detectors and trackers is most commonly evaluated by the Intersection over Union (IoU) criterion. To date, most approaches are restricted to axis-aligned or oriented boxes and, as a consequence, many datasets are only labeled with boxes. Nevertheless, axis-aligned or oriented boxes cannot accurately capture an object's shape. To address this, a number of densely segmented datasets has started to emerge in both the object detection and the object tracking communities. However, evaluating the accuracy of object detectors and trackers that are restricted to boxes on densely segmented data is not straightforward. To close this gap, we introduce the relative Intersection over Union (rIoU) accuracy measure. The measure normalizes the IoU with the optimal box for the segmentation to generate an accuracy measure that ranges between 0 and 1 and allows a more precise measurement of accuracies. Furthermore, it enables an efficient and easy way to understand scenes and the strengths and weaknesses of an object detection or tracking approach. We display how the new measure can be efficiently calculated and present an easy-to-use evaluation framework. The framework is tested on the DAVIS and the VOT2016 segmentations and has been made available to the community.



### A Real-time Hand Gesture Recognition and Human-Computer Interaction System
- **Arxiv ID**: http://arxiv.org/abs/1704.07296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07296v1)
- **Published**: 2017-04-24 15:44:56+00:00
- **Updated**: 2017-04-24 15:44:56+00:00
- **Authors**: Pei Xu
- **Comment**: None
- **Journal**: None
- **Summary**: In this project, we design a real-time human-computer interaction system based on hand gesture. The whole system consists of three components: hand detection, gesture recognition and human-computer interaction (HCI) based on recognition; and realizes the robust control of mouse and keyboard events with a higher accuracy of gesture recognition. Specifically, we use the convolutional neural network (CNN) to recognize gestures and makes it attainable to identify relatively complex gestures using only one cheap monocular camera. We introduce the Kalman filter to estimate the hand position based on which the mouse cursor control is realized in a stable and smooth way. During the HCI stage, we develop a simple strategy to avoid the false recognition caused by noises - mostly transient, false gestures, and thus to improve the reliability of interaction. The developed system is highly extendable and can be used in human-robotic or other human-machine interaction scenarios with more complex command formats rather than just mouse and keyboard events.



### Accurate Optical Flow via Direct Cost Volume Processing
- **Arxiv ID**: http://arxiv.org/abs/1704.07325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07325v1)
- **Published**: 2017-04-24 17:03:53+00:00
- **Updated**: 2017-04-24 17:03:53+00:00
- **Authors**: Jia Xu, René Ranftl, Vladlen Koltun
- **Comment**: Published at the Conference on Computer Vision and Pattern
  Recognition (CVPR 2017)
- **Journal**: None
- **Summary**: We present an optical flow estimation approach that operates on the full four-dimensional cost volume. This direct approach shares the structural benefits of leading stereo matching pipelines, which are known to yield high accuracy. To this day, such approaches have been considered impractical due to the size of the cost volume. We show that the full four-dimensional cost volume can be constructed in a fraction of a second due to its regularity. We then exploit this regularity further by adapting semi-global matching to the four-dimensional setting. This yields a pipeline that achieves significantly higher accuracy than state-of-the-art optical flow methods while being faster than most. Our approach outperforms all published general-purpose optical flow methods on both Sintel and KITTI 2015 benchmarks.



### Detecting and Recognizing Human-Object Interactions
- **Arxiv ID**: http://arxiv.org/abs/1704.07333v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07333v3)
- **Published**: 2017-04-24 17:14:24+00:00
- **Updated**: 2018-03-27 02:57:19+00:00
- **Authors**: Georgia Gkioxari, Ross Girshick, Piotr Dollár, Kaiming He
- **Comment**: None
- **Journal**: None
- **Summary**: To understand the visual world, a machine must not only recognize individual object instances but also how they interact. Humans are often at the center of such interactions and detecting human-object interactions is an important practical and scientific problem. In this paper, we address the task of detecting <human, verb, object> triplets in challenging everyday photos. We propose a novel model that is driven by a human-centric approach. Our hypothesis is that the appearance of a person -- their pose, clothing, action -- is a powerful cue for localizing the objects they are interacting with. To exploit this cue, our model learns to predict an action-specific density over target object locations based on the appearance of a detected person. Our model also jointly learns to detect people and objects, and by fusing these predictions it efficiently infers interaction triplets in a clean, jointly trained end-to-end system we call InteractNet. We validate our approach on the recently introduced Verbs in COCO (V-COCO) and HICO-DET datasets, where we show quantitatively compelling results.



### Accelerated Nearest Neighbor Search with Quick ADC
- **Arxiv ID**: http://arxiv.org/abs/1704.07355v1
- **DOI**: 10.1145/3078971.3078992
- **Categories**: **cs.CV**, cs.DB, cs.IR, cs.MM, cs.PF, H.5.1; H.2.4; H.2.8
- **Links**: [PDF](http://arxiv.org/pdf/1704.07355v1)
- **Published**: 2017-04-24 17:49:37+00:00
- **Updated**: 2017-04-24 17:49:37+00:00
- **Authors**: Fabien André, Anne-Marie Kermarrec, Nicolas Le Scouarnec
- **Comment**: 8 pages, 5 figures, published in Proceedings of ICMR'17, Bucharest,
  Romania, June 06-09, 2017
- **Journal**: None
- **Summary**: Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a foundation of many multimedia retrieval systems. Because it offers low responses times, Product Quantization (PQ) is a popular solution. PQ compresses high-dimensional vectors into short codes using several sub-quantizers, which enables in-RAM storage of large databases. This allows fast answers to NN queries, without accessing the SSD or HDD. The key feature of PQ is that it can compute distances between short codes and high-dimensional vectors using cache-resident lookup tables. The efficiency of this technique, named Asymmetric Distance Computation (ADC), remains limited because it performs many cache accesses.   In this paper, we introduce Quick ADC, a novel technique that achieves a 3 to 6 times speedup over ADC by exploiting Single Instruction Multiple Data (SIMD) units available in current CPUs. Efficiently exploiting SIMD requires algorithmic changes to the ADC procedure. Namely, Quick ADC relies on two key modifications of ADC: (i) the use 4-bit sub-quantizers instead of the standard 8-bit sub-quantizers and (ii) the quantization of floating-point distances. This allows Quick ADC to exceed the performance of state-of-the-art systems, e.g., it achieves a Recall@100 of 0.94 in 3.4 ms on 1 billion SIFT descriptors (128-bit codes).



### Towards Instance Segmentation with Object Priority: Prominent Object Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.07402v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1704.07402v2)
- **Published**: 2017-04-24 18:12:12+00:00
- **Updated**: 2017-08-04 12:15:34+00:00
- **Authors**: Hamed R. Tavakoli, Jorma Laaksonen
- **Comment**: None
- **Journal**: None
- **Summary**: This manuscript introduces the problem of prominent object detection and recognition inspired by the fact that human seems to priorities perception of scene elements. The problem deals with finding the most important region of interest, segmenting the relevant item/object in that area, and assigning it an object class label. In other words, we are solving the three problems of saliency modeling, saliency detection, and object recognition under one umbrella. The motivation behind such a problem formulation is (1) the benefits to the knowledge representation-based vision pipelines, and (2) the potential improvements in emulating bio-inspired vision systems by solving these three problems together. We are foreseeing extending this problem formulation to fully semantically segmented scenes with instance object priority for high-level inferences in various applications including assistive vision. Along with a new problem definition, we also propose a method to achieve such a task. The proposed model predicts the most important area in the image, segments the associated objects, and labels them. The proposed problem and method are evaluated against human fixations, annotated segmentation masks, and object class categories. We define a chance level for each of the evaluation criterion to compare the proposed algorithm with. Despite the good performance of the proposed baseline, the overall evaluations indicate that the problem of prominent object detection and recognition is a challenging task that is still worth investigating further.



### Paying Attention to Descriptions Generated by Image Captioning Models
- **Arxiv ID**: http://arxiv.org/abs/1704.07434v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1704.07434v3)
- **Published**: 2017-04-24 19:51:16+00:00
- **Updated**: 2017-08-04 11:24:45+00:00
- **Authors**: Hamed R. Tavakoli, Rakshith Shetty, Ali Borji, Jorma Laaksonen
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: To bridge the gap between humans and machines in image understanding and describing, we need further insight into how people describe a perceived scene. In this paper, we study the agreement between bottom-up saliency-based visual attention and object referrals in scene description constructs. We investigate the properties of human-written descriptions and machine-generated ones. We then propose a saliency-boosted image captioning model in order to investigate benefits from low-level cues in language models. We learn that (1) humans mention more salient objects earlier than less salient ones in their descriptions, (2) the better a captioning model performs, the better attention agreement it has with human descriptions, (3) the proposed saliency-boosted model, compared to its baseline form, does not improve significantly on the MS COCO database, indicating explicit bottom-up boosting does not help when the task is well learnt and tuned on a data, (4) a better generalization is, however, observed for the saliency-boosted model on unseen data.



### Multi-Task Video Captioning with Video and Entailment Generation
- **Arxiv ID**: http://arxiv.org/abs/1704.07489v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.07489v2)
- **Published**: 2017-04-24 23:07:32+00:00
- **Updated**: 2017-08-08 17:08:58+00:00
- **Authors**: Ramakanth Pasunuru, Mohit Bansal
- **Comment**: ACL 2017 (14 pages w/ supplementary)
- **Journal**: None
- **Summary**: Video captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data. We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations, and a logically-directed language entailment generation task to learn better video-entailed caption decoder representations. For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks. We achieve significant improvements and the new state-of-the-art on several standard video captioning datasets using diverse automatic and human evaluations. We also show mutual multi-task improvements on the entailment generation task.



### A Context Aware and Video-Based Risk Descriptor for Cyclists
- **Arxiv ID**: http://arxiv.org/abs/1704.07490v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07490v1)
- **Published**: 2017-04-24 23:10:05+00:00
- **Updated**: 2017-04-24 23:10:05+00:00
- **Authors**: Miguel Costa, Beatriz Quintino Ferreira, Manuel Marques
- **Comment**: Submitted to ITSC2017
- **Journal**: None
- **Summary**: Aiming to reduce pollutant emissions, bicycles are regaining popularity specially in urban areas. However, the number of cyclists' fatalities is not showing the same decreasing trend as the other traffic groups. Hence, monitoring cyclists' data appears as a keystone to foster urban cyclists' safety by helping urban planners to design safer cyclist routes. In this work, we propose a fully image-based framework to assess the rout risk from the cyclist perspective. From smartphone sequences of images, this generic framework is able to automatically identify events considering different risk criteria based on the cyclist's motion and object detection. Moreover, since it is entirely based on images, our method provides context on the situation and is independent from the expertise level of the cyclist. Additionally, we build on an existing platform and introduce several improvements on its mobile app to acquire smartphone sensor data, including video. From the inertial sensor data, we automatically detect the route segments performed by bicycle, applying behavior analysis techniques. We test our methods on real data, attaining very promising results in terms of risk classification, according to two different criteria, and behavior analysis accuracy.



