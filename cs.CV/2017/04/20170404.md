# Arxiv Papers in cs.CV on 2017-04-04
### Guided Proofreading of Automatic Segmentations for Connectomics
- **Arxiv ID**: http://arxiv.org/abs/1704.00848v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00848v1)
- **Published**: 2017-04-04 01:46:46+00:00
- **Updated**: 2017-04-04 01:46:46+00:00
- **Authors**: Daniel Haehn, Verena Kaynig, James Tompkin, Jeff W. Lichtman, Hanspeter Pfister
- **Comment**: Supplemental material available at
  http://rhoana.org/guidedproofreading/supplemental.pdf
- **Journal**: None
- **Summary**: Automatic cell image segmentation methods in connectomics produce merge and split errors, which require correction through proofreading. Previous research has identified the visual search for these errors as the bottleneck in interactive proofreading. To aid error correction, we develop two classifiers that automatically recommend candidate merges and splits to the user. These classifiers use a convolutional neural network (CNN) that has been trained with errors in automatic segmentations against expert-labeled ground truth. Our classifiers detect potentially-erroneous regions by considering a large context region around a segmentation boundary. Corrections can then be performed by a user with yes/no decisions, which reduces variation of information 7.5x faster than previous proofreading methods. We also present a fully-automatic mode that uses a probability threshold to make merge/split decisions. Extensive experiments using the automatic approach and comparing performance of novice and expert users demonstrate that our method performs favorably against state-of-the-art proofreading methods on different connectomics datasets.



### Learning a collaborative multiscale dictionary based on robust empirical mode decomposition
- **Arxiv ID**: http://arxiv.org/abs/1704.04422v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.04422v1)
- **Published**: 2017-04-04 02:31:20+00:00
- **Updated**: 2017-04-04 02:31:20+00:00
- **Authors**: Rui Chen, Huizhu Jia, Xiaodong Xie, Wen Gao
- **Comment**: to be published in Neurocomputing
- **Journal**: None
- **Summary**: Dictionary learning is a challenge topic in many image processing areas. The basic goal is to learn a sparse representation from an overcomplete basis set. Due to combining the advantages of generic multiscale representations with learning based adaptivity, multiscale dictionary representation approaches have the power in capturing structural characteristics of natural images. However, existing multiscale learning approaches still suffer from three main weaknesses: inadaptability to diverse scales of image data, sensitivity to noise and outliers, difficulty to determine optimal dictionary structure. In this paper, we present a novel multiscale dictionary learning paradigm for sparse image representations based on an improved empirical mode decomposition. This powerful data-driven analysis tool for multi-dimensional signal can fully adaptively decompose the image into multiscale oscillating components according to intrinsic modes of data self. This treatment can obtain a robust and effective sparse representation, and meanwhile generates a raw base dictionary at multiple geometric scales and spatial frequency bands. This dictionary is refined by selecting optimal oscillating atoms based on frequency clustering. In order to further enhance sparsity and generalization, a tolerance dictionary is learned using a coherence regularized model. A fast proximal scheme is developed to optimize this model. The multiscale dictionary is considered as the product of oscillating dictionary and tolerance dictionary. Experimental results demonstrate that the proposed learning approach has the superior performance in sparse image representations as compared with several competing methods. We also show the promising results in image denoising application.



### Simultaneous Feature Aggregating and Hashing for Large-scale Image Search
- **Arxiv ID**: http://arxiv.org/abs/1704.00860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00860v1)
- **Published**: 2017-04-04 03:04:30+00:00
- **Updated**: 2017-04-04 03:04:30+00:00
- **Authors**: Thanh-Toan Do, Dang-Khoa Le Tan, Trung T. Pham, Ngai-Man Cheung
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: In most state-of-the-art hashing-based visual search systems, local image descriptors of an image are first aggregated as a single feature vector. This feature vector is then subjected to a hashing function that produces a binary hash code. In previous work, the aggregating and the hashing processes are designed independently. In this paper, we propose a novel framework where feature aggregating and hashing are designed simultaneously and optimized jointly. Specifically, our joint optimization produces aggregated representations that can be better reconstructed by some binary codes. This leads to more discriminative binary hash codes and improved retrieval accuracy. In addition, we also propose a fast version of the recently-proposed Binary Autoencoder to be used in our proposed framework. We perform extensive retrieval experiments on several benchmark datasets with both SIFT and convolutional features. Our results suggest that the proposed framework achieves significant improvements over the state of the art.



### A Branch-and-Bound Algorithm for Checkerboard Extraction in Camera-Laser Calibration
- **Arxiv ID**: http://arxiv.org/abs/1704.00887v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.00887v1)
- **Published**: 2017-04-04 06:28:30+00:00
- **Updated**: 2017-04-04 06:28:30+00:00
- **Authors**: Alireza Khosravian, Tat-Jun Chin, Ian Reid
- **Comment**: To appear in IEEE Conference on Robotics and Automation 2017
- **Journal**: Proceedings of IEEE Conference on Robotics and Automation, 2017
- **Summary**: We address the problem of camera-to-laser-scanner calibration using a checkerboard and multiple image-laser scan pairs. Distinguishing which laser points measure the checkerboard and which lie on the background is essential to any such system. We formulate the checkerboard extraction as a combinatorial optimization problem with a clear cut objective function. We propose a branch-and-bound technique that deterministically and globally optimizes the objective. Unlike what is available in the literature, the proposed method is not heuristic and does not require assumptions such as constraints on the background or relying on discontinuity of the range measurements to partition the data into line segments. The proposed approach is generic and can be applied to both 3D or 2D laser scanners as well as the cases where multiple checkerboards are present. We demonstrate the effectiveness of the proposed approach by providing numerical simulations as well as experimental results.



### A Unified Multi-Faceted Video Summarization System
- **Arxiv ID**: http://arxiv.org/abs/1704.01466v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DM
- **Links**: [PDF](http://arxiv.org/pdf/1704.01466v1)
- **Published**: 2017-04-04 07:29:34+00:00
- **Updated**: 2017-04-04 07:29:34+00:00
- **Authors**: Anurag Sahoo, Vishal Kaushal, Khoshrav Doctor, Suyash Shetty, Rishabh Iyer, Ganesh Ramakrishnan
- **Comment**: 18 pages, 11 Figures
- **Journal**: None
- **Summary**: This paper addresses automatic summarization and search in visual data comprising of videos, live streams and image collections in a unified manner. In particular, we propose a framework for multi-faceted summarization which extracts key-frames (image summaries), skims (video summaries) and entity summaries (summarization at the level of entities like objects, scenes, humans and faces in the video). The user can either view these as extractive summarization, or query focused summarization. Our approach first pre-processes the video or image collection once, to extract all important visual features, following which we provide an interactive mechanism to the user to summarize the video based on their choice. We investigate several diversity, coverage and representation models for all these problems, and argue the utility of these different mod- els depending on the application. While most of the prior work on submodular summarization approaches has focused on combining several models and learning weighted mixtures, we focus on the explain-ability of different the diversity, coverage and representation models and their scalability. Most importantly, we also show that we can summarize hours of video data in a few seconds, and our system allows the user to generate summaries of various lengths and types interactively on the fly.



### Optic Disc and Cup Segmentation Methods for Glaucoma Detection with Modification of U-Net Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1704.00979v1
- **DOI**: 10.1134/S1054661817030269
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.00979v1)
- **Published**: 2017-04-04 12:28:12+00:00
- **Updated**: 2017-04-04 12:28:12+00:00
- **Authors**: Artem Sevastopolsky
- **Comment**: accepted for publication in "Pattern Recognition and Image Analysis:
  Advances in Mathematical Theory and Applications" journal, ISSN 1054-6618
- **Journal**: None
- **Summary**: Glaucoma is the second leading cause of blindness all over the world, with approximately 60 million cases reported worldwide in 2010. If undiagnosed in time, glaucoma causes irreversible damage to the optic nerve leading to blindness. The optic nerve head examination, which involves measurement of cup-to-disc ratio, is considered one of the most valuable methods of structural diagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation of optic disc and optic cup on eye fundus images and can be performed by modern computer vision algorithms. This work presents universal approach for automatic optic disc and cup segmentation, which is based on deep learning, namely, modification of U-Net convolutional neural network. Our experiments include comparison with the best known methods on publicly available databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation, our method achieves quality comparable to current state-of-the-art methods, outperforming them in terms of the prediction time.



### Automatic Breast Ultrasound Image Segmentation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1704.01472v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.01472v2)
- **Published**: 2017-04-04 14:23:26+00:00
- **Updated**: 2018-01-09 23:19:21+00:00
- **Authors**: Min Xian, Yingtao Zhang, H. D. Cheng, Fei Xu, Boyu Zhang, Jianrui Ding
- **Comment**: 40 pages, 6 tables, 180 references
- **Journal**: None
- **Summary**: Breast cancer is one of the leading causes of cancer death among women worldwide. In clinical routine, automatic breast ultrasound (BUS) image segmentation is very challenging and essential for cancer diagnosis and treatment planning. Many BUS segmentation approaches have been studied in the last two decades, and have been proved to be effective on private datasets. Currently, the advancement of BUS image segmentation seems to meet its bottleneck. The improvement of the performance is increasingly challenging, and only few new approaches were published in the last several years. It is the time to look at the field by reviewing previous approaches comprehensively and to investigate the future directions. In this paper, we study the basic ideas, theories, pros and cons of the approaches, group them into categories, and extensively review each category in depth by discussing the principles, application issues, and advantages/disadvantages.



### OctNetFusion: Learning Depth Fusion from Data
- **Arxiv ID**: http://arxiv.org/abs/1704.01047v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01047v3)
- **Published**: 2017-04-04 15:02:05+00:00
- **Updated**: 2017-10-31 20:58:32+00:00
- **Authors**: Gernot Riegler, Ali Osman Ulusoy, Horst Bischof, Andreas Geiger
- **Comment**: 3DV 2017, https://github.com/griegler/octnetfusion
- **Journal**: None
- **Summary**: In this paper, we present a learning based approach to depth fusion, i.e., dense 3D reconstruction from multiple depth images. The most common approach to depth fusion is based on averaging truncated signed distance functions, which was originally proposed by Curless and Levoy in 1996. While this method is simple and provides great results, it is not able to reconstruct (partially) occluded surfaces and requires a large number frames to filter out sensor noise and outliers. Motivated by the availability of large 3D model repositories and recent advances in deep learning, we present a novel 3D CNN architecture that learns to predict an implicit surface representation from the input depth maps. Our learning based method significantly outperforms the traditional volumetric fusion approach in terms of noise reduction and outlier suppression. By learning the structure of real world 3D objects and scenes, our approach is further able to reconstruct occluded regions and to fill in gaps in the reconstruction. We demonstrate that our learning based approach outperforms both vanilla TSDF fusion as well as TV-L1 fusion on the task of volumetric fusion. Further, we demonstrate state-of-the-art 3D shape completion results.



### ME R-CNN: Multi-Expert R-CNN for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.01069v3
- **DOI**: 10.1109/TIP.2019.2938879
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01069v3)
- **Published**: 2017-04-04 15:35:31+00:00
- **Updated**: 2022-04-06 13:22:27+00:00
- **Authors**: Hyungtae Lee, Sungmin Eum, Heesung Kwon
- **Comment**: IEEE Transactions on Image Processing
- **Journal**: in IEEE Transactions on Image Processing, vol. 29, pp. 1030-1044,
  2020
- **Summary**: We introduce Multi-Expert Region-based Convolutional Neural Network (ME R-CNN) which is equipped with multiple experts (ME) where each expert is learned to process a certain type of regions of interest (RoIs). This architecture better captures the appearance variations of the RoIs caused by different shapes, poses, and viewing angles. In order to direct each RoI to the appropriate expert, we devise a novel "learnable" network, which we call, expert assignment network (EAN). EAN automatically learns the optimal RoI-expert relationship even without any supervision of expert assignment. As the major components of ME R-CNN, ME and EAN, are mutually affecting each other while tied to a shared network, neither an alternating nor a naive end-to-end optimization is likely to fail. To address this problem, we introduce a practical training strategy which is tailored to optimize ME, EAN, and the shared network in an end-to-end fashion. We show that both of the architectures provide considerable performance increase over the baselines on PASCAL VOC 07, 12, and MS COCO datasets.



### Deep Depth From Focus
- **Arxiv ID**: http://arxiv.org/abs/1704.01085v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01085v3)
- **Published**: 2017-04-04 16:15:54+00:00
- **Updated**: 2018-10-28 15:00:55+00:00
- **Authors**: Caner Hazirbas, Sebastian Georg Soyer, Maximilian Christian Staab, Laura Leal-Taixé, Daniel Cremers
- **Comment**: accepted to Asian Conference on Computer Vision (ACCV) 2018
- **Journal**: None
- **Summary**: Depth from focus (DFF) is one of the classical ill-posed inverse problems in computer vision. Most approaches recover the depth at each pixel based on the focal setting which exhibits maximal sharpness. Yet, it is not obvious how to reliably estimate the sharpness level, particularly in low-textured areas. In this paper, we propose `Deep Depth From Focus (DDFF)' as the first end-to-end learning approach to this problem. One of the main challenges we face is the hunger for data of deep neural networks. In order to obtain a significant amount of focal stacks with corresponding groundtruth depth, we propose to leverage a light-field camera with a co-calibrated RGB-D sensor. This allows us to digitally create focal stacks of varying sizes. Compared to existing benchmarks our dataset is 25 times larger, enabling the use of machine learning for this inverse problem. We compare our results with state-of-the-art DFF methods and we also analyze the effect of several key deep architectural components. These experiments show that our proposed method `DDFFNet' achieves state-of-the-art performance in all scenes, reducing depth error by more than 75% compared to the classical DFF methods.



### Satellite Image-based Localization via Learned Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1704.01133v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.01133v2)
- **Published**: 2017-04-04 18:03:55+00:00
- **Updated**: 2022-03-07 17:19:43+00:00
- **Authors**: Dong-Ki Kim, Matthew R. Walter
- **Comment**: Published in IEEE International Conference on Robotics and Automation
  (ICRA), 2017; arXiv version has updated author information and added video
  highlight available at https://youtu.be/58K1-0WpGNs
- **Journal**: None
- **Summary**: We propose a vision-based method that localizes a ground vehicle using publicly available satellite imagery as the only prior knowledge of the environment. Our approach takes as input a sequence of ground-level images acquired by the vehicle as it navigates, and outputs an estimate of the vehicle's pose relative to a georeferenced satellite image. We overcome the significant viewpoint and appearance variations between the images through a neural multi-view model that learns location-discriminative embeddings in which ground-level images are matched with their corresponding satellite view of the scene. We use this learned function as an observation model in a filtering framework to maintain a distribution over the vehicle's pose. We evaluate our method on different benchmark datasets and demonstrate its ability localize ground-level images in environments novel relative to training, despite the challenges of significant viewpoint and appearance variations.



### DyVEDeep: Dynamic Variable Effort Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.01137v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.01137v1)
- **Published**: 2017-04-04 18:14:02+00:00
- **Updated**: 2017-04-04 18:14:02+00:00
- **Authors**: Sanjay Ganapathy, Swagath Venkataramani, Balaraman Ravindran, Anand Raghunathan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety of machine learning tasks and are deployed in increasing numbers of products and services. However, the computational requirements of training and evaluating large-scale DNNs are growing at a much faster pace than the capabilities of the underlying hardware platforms that they are executed upon. In this work, we propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep) to reduce the computational requirements of DNNs during inference. Previous efforts propose specialized hardware implementations for DNNs, statically prune the network, or compress the weights. Complementary to these approaches, DyVEDeep is a dynamic approach that exploits the heterogeneity in the inputs to DNNs to improve their compute efficiency with comparable classification accuracy. DyVEDeep equips DNNs with dynamic effort mechanisms that, in the course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computa- tions, while skipping or approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks - one for CIFAR-10 and four for ImageNet (AlexNet, OverFeat and VGG-16, weight-compressed AlexNet). Across all benchmarks, DyVEDeep achieves 2.1x-2.6x reduction in the number of scalar operations, which translates to 1.8x-2.3x performance improvement over a Caffe-based implementation, with < 0.5% loss in accuracy.



### Pose2Instance: Harnessing Keypoints for Person Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.01152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01152v1)
- **Published**: 2017-04-04 18:41:47+00:00
- **Updated**: 2017-04-04 18:41:47+00:00
- **Authors**: Subarna Tripathi, Maxwell Collins, Matthew Brown, Serge Belongie
- **Comment**: None
- **Journal**: None
- **Summary**: Human keypoints are a well-studied representation of people.We explore how to use keypoint models to improve instance-level person segmentation. The main idea is to harness the notion of a distance transform of oracle provided keypoints or estimated keypoint heatmaps as a prior for person instance segmentation task within a deep neural network. For training and evaluation, we consider all those images from COCO where both instance segmentation and human keypoints annotations are available. We first show how oracle keypoints can boost the performance of existing human segmentation model during inference without any training. Next, we propose a framework to directly learn a deep instance segmentation model conditioned on human pose. Experimental results show that at various Intersection Over Union (IOU) thresholds, in a constrained environment with oracle keypoints, the instance segmentation accuracy achieves 10% to 12% relative improvements over a strong baseline of oracle bounding boxes. In a more realistic environment, without the oracle keypoints, the proposed deep person instance segmentation model conditioned on human pose achieves 3.8% to 10.5% relative improvements comparing with its strongest baseline of a deep network trained only for segmentation.



### Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.01155v2
- **DOI**: 10.14722/ndss.2018.23198
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.01155v2)
- **Published**: 2017-04-04 18:56:53+00:00
- **Updated**: 2017-12-05 23:45:08+00:00
- **Authors**: Weilin Xu, David Evans, Yanjun Qi
- **Comment**: To appear in Network and Distributed Systems Security Symposium
  (NDSS) 2018
- **Journal**: None
- **Summary**: Although deep neural networks (DNNs) have achieved great success in many tasks, they can often be fooled by \emph{adversarial examples} that are generated by adding small but purposeful distortions to natural examples. Previous studies to defend against adversarial examples mostly focused on refining the DNN models, but have either shown limited success or required expensive computation. We propose a new strategy, \emph{feature squeezing}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on squeezed inputs, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two feature squeezing methods: reducing the color bit depth of each pixel and spatial smoothing. These simple strategies are inexpensive and complementary to other defenses, and can be combined in a joint detection framework to achieve high detection rates against state-of-the-art attacks.



### Two Stream LSTM: A Deep Fusion Framework for Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.01194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01194v1)
- **Published**: 2017-04-04 21:32:04+00:00
- **Updated**: 2017-04-04 21:32:04+00:00
- **Authors**: Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: Published as a conference paper at WACV 2017
- **Journal**: None
- **Summary**: In this paper we address the problem of human action recognition from video sequences. Inspired by the exemplary results obtained via automatic feature learning and deep learning approaches in computer vision, we focus our attention towards learning salient spatial features via a convolutional neural network (CNN) and then map their temporal relationship with the aid of Long-Short-Term-Memory (LSTM) networks. Our contribution in this paper is a deep fusion framework that more effectively exploits spatial features from CNNs with temporal features from LSTM models. We also extensively evaluate their strengths and weaknesses. We find that by combining both the sets of features, the fully connected features effectively act as an attention mechanism to direct the LSTM to interesting parts of the convolutional feature sequence. The significance of our fusion method is its simplicity and effectiveness compared to other state-of-the-art methods. The evaluation results demonstrate that this hierarchical multi stream fusion method has higher performance compared to single stream mapping methods allowing it to achieve high accuracy outperforming current state-of-the-art methods in three widely used databases: UCF11, UCFSports, jHMDB.



### Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models
- **Arxiv ID**: http://arxiv.org/abs/1704.01222v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01222v2)
- **Published**: 2017-04-04 23:52:40+00:00
- **Updated**: 2017-10-26 08:51:12+00:00
- **Authors**: Roman Klokov, Victor Lempitsky
- **Comment**: Spotlight at ICCV'17
- **Journal**: None
- **Summary**: We present a new deep learning architecture (called Kd-network) that is designed for 3D model recognition tasks and works with unstructured point clouds. The new architecture performs multiplicative transformations and share parameters of these transformations according to the subdivisions of the point clouds imposed onto them by Kd-trees. Unlike the currently dominant convolutional architectures that usually require rasterization on uniform two-dimensional or three-dimensional grids, Kd-networks do not rely on such grids in any way and therefore avoid poor scaling behaviour. In a series of experiments with popular shape recognition benchmarks, Kd-networks demonstrate competitive performance in a number of shape recognition tasks such as shape classification, shape retrieval and shape part segmentation.



