# Arxiv Papers in cs.CV on 2017-04-11
### Detecting Visual Relationships with Deep Relational Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.03114v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03114v2)
- **Published**: 2017-04-11 02:11:20+00:00
- **Updated**: 2017-04-12 08:18:58+00:00
- **Authors**: Bo Dai, Yuqi Zhang, Dahua Lin
- **Comment**: To be appeared in CVPR 2017 as an oral paper
- **Journal**: None
- **Summary**: Relationships among objects play a crucial role in image understanding. Despite the great success of deep learning techniques in recognizing individual objects, reasoning about the relationships among objects remains a challenging task. Previous methods often treat this as a classification problem, considering each type of relationship (e.g. "ride") or each distinct visual phrase (e.g. "person-ride-horse") as a category. Such approaches are faced with significant difficulties caused by the high diversity of visual appearance for each kind of relationships or the large number of distinct visual phrases. We propose an integrated framework to tackle this problem. At the heart of this framework is the Deep Relational Network, a novel formulation designed specifically for exploiting the statistical dependencies between objects and their relationships. On two large datasets, the proposed method achieves substantial improvement over state-of-the-art.



### DOPE: Distributed Optimization for Pairwise Energies
- **Arxiv ID**: http://arxiv.org/abs/1704.03116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03116v1)
- **Published**: 2017-04-11 02:21:13+00:00
- **Updated**: 2017-04-11 02:21:13+00:00
- **Authors**: Jose Dolz, Ismail Ben Ayed, Christian Desrosiers
- **Comment**: Accepted at CVPR 2017
- **Journal**: None
- **Summary**: We formulate an Alternating Direction Method of Mul-tipliers (ADMM) that systematically distributes the computations of any technique for optimizing pairwise functions, including non-submodular potentials. Such discrete functions are very useful in segmentation and a breadth of other vision problems. Our method decomposes the problem into a large set of small sub-problems, each involving a sub-region of the image domain, which can be solved in parallel. We achieve consistency between the sub-problems through a novel constraint that can be used for a large class of pair-wise functions. We give an iterative numerical solution that alternates between solving the sub-problems and updating consistency variables, until convergence. We report comprehensive experiments, which demonstrate the benefit of our general distributed solution in the case of the popular serial algorithm of Boykov and Kolmogorov (BK algorithm) and, also, in the context of non-submodular functions.



### Improving Pairwise Ranking for Multi-label Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1704.03135v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03135v3)
- **Published**: 2017-04-11 04:04:54+00:00
- **Updated**: 2017-06-01 02:35:58+00:00
- **Authors**: Yuncheng Li, Yale Song, Jiebo Luo
- **Comment**: cvpr 2017
- **Journal**: None
- **Summary**: Learning to rank has recently emerged as an attractive technique to train deep convolutional neural networks for various computer vision tasks. Pairwise ranking, in particular, has been successful in multi-label image classification, achieving state-of-the-art results on various benchmarks. However, most existing approaches use the hinge loss to train their models, which is non-smooth and thus is difficult to optimize especially with deep networks. Furthermore, they employ simple heuristics, such as top-k or thresholding, to determine which labels to include in the output from a ranked list of labels, which limits their use in the real-world setting. In this work, we propose two techniques to improve pairwise ranking based multi-label image classification: (1) we propose a novel loss function for pairwise ranking, which is smooth everywhere and thus is easier to optimize; and (2) we incorporate a label decision module into the model, estimating the optimal confidence thresholds for each visual concept. We provide theoretical analyses of our loss function in the Bayes consistency and risk minimization framework, and show its benefit over existing pairwise ranking formulations. We demonstrate the effectiveness of our approach on three large-scale datasets, VOC2007, NUS-WIDE and MS-COCO, achieving the best reported results in the literature.



### Restoration of Atmospheric Turbulence-distorted Images via RPCA and Quasiconformal Maps
- **Arxiv ID**: http://arxiv.org/abs/1704.03140v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1704.03140v2)
- **Published**: 2017-04-11 04:24:44+00:00
- **Updated**: 2017-09-19 03:40:28+00:00
- **Authors**: Chun Pong Lau, Yu Hin Lai, Lok Ming Lui
- **Comment**: 21 pages, 24 figures
- **Journal**: None
- **Summary**: We address the problem of restoring a high-quality image from an observed image sequence strongly distorted by atmospheric turbulence. A novel algorithm is proposed in this paper to reduce geometric distortion as well as space-and-time-varying blur due to strong turbulence. By considering a suitable energy functional, our algorithm first obtains a sharp reference image and a subsampled image sequence containing sharp and mildly distorted image frames with respect to the reference image. The subsampled image sequence is then stabilized by applying the Robust Principal Component Analysis (RPCA) on the deformation fields between image frames and warping the image frames by a quasiconformal map associated with the low-rank part of the deformation matrix. After image frames are registered to the reference image, the low-rank part of them are deblurred via a blind deconvolution, and the deblurred frames are then fused with the enhanced sparse part. Experiments have been carried out on both synthetic and real turbulence-distorted video. Results demonstrate that our method is effective in alleviating distortions and blur, restoring image details and enhancing visual quality.



### Deep Multimodal Representation Learning from Temporal Data
- **Arxiv ID**: http://arxiv.org/abs/1704.03152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03152v1)
- **Published**: 2017-04-11 05:47:42+00:00
- **Updated**: 2017-04-11 05:47:42+00:00
- **Authors**: Xitong Yang, Palghat Ramesh, Radha Chitta, Sriganesh Madhvanath, Edgar A. Bernal, Jiebo Luo
- **Comment**: To appear in CVPR 2017
- **Journal**: None
- **Summary**: In recent years, Deep Learning has been successfully applied to multimodal learning problems, with the aim of learning useful joint representations in data fusion applications. When the available modalities consist of time series data such as video, audio and sensor signals, it becomes imperative to consider their temporal structure during the fusion process. In this paper, we propose the Correlational Recurrent Neural Network (CorrRNN), a novel temporal fusion model for fusing multiple input modalities that are inherently temporal in nature. Key features of our proposed model include: (i) simultaneous learning of the joint representation and temporal dependencies between modalities, (ii) use of multiple loss terms in the objective function, including a maximum correlation loss term to enhance learning of cross-modal information, and (iii) the use of an attention model to dynamically adjust the contribution of different input modalities to the joint representation. We validate our model via experimentation on two different tasks: video- and sensor-based activity classification, and audio-visual speech recognition. We empirically analyze the contributions of different components of the proposed CorrRNN model, and demonstrate its robustness, effectiveness and state-of-the-art performance on multiple datasets.



### EAST: An Efficient and Accurate Scene Text Detector
- **Arxiv ID**: http://arxiv.org/abs/1704.03155v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03155v2)
- **Published**: 2017-04-11 06:04:12+00:00
- **Updated**: 2017-07-10 08:10:52+00:00
- **Authors**: Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, Jiajun Liang
- **Comment**: Accepted to CVPR 2017, fix equation (3)
- **Journal**: None
- **Summary**: Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.



### Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1704.03162v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03162v2)
- **Published**: 2017-04-11 06:22:57+00:00
- **Updated**: 2017-04-12 05:53:56+00:00
- **Authors**: Vahid Kazemi, Ali Elqursh
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new baseline for visual question answering task. Given an image and a question in natural language, our model produces accurate answers according to the content of the image. Our model, while being architecturally simple and relatively small in terms of trainable parameters, sets a new state of the art on both unbalanced and balanced VQA benchmark. On VQA 1.0 open ended challenge, our model achieves 64.6% accuracy on the test-standard set without using additional data, an improvement of 0.4% over state of the art, and on newly released VQA 2.0, our model scores 59.7% on validation set outperforming best previously reported results by 0.5%. The results presented in this paper are especially interesting because very similar models have been tried before but significantly lower performance were reported. In light of the new results we hope to see more meaningful research on visual question answering in the future.



### Mining Object Parts from CNNs via Active Question-Answering
- **Arxiv ID**: http://arxiv.org/abs/1704.03173v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03173v1)
- **Published**: 2017-04-11 07:27:33+00:00
- **Updated**: 2017-04-11 07:27:33+00:00
- **Authors**: Quanshi Zhang, Ruiming Cao, Ying Nian Wu, Song-Chun Zhu
- **Comment**: Published in CVPR 2017
- **Journal**: Quanshi Zhang, Ruiming Cao, Ying Nian Wu, and Song-Chun Zhu,
  "Mining Object Parts from CNNs via Active Question-Answering" in CVPR 2017
- **Summary**: Given a convolutional neural network (CNN) that is pre-trained for object classification, this paper proposes to use active question-answering to semanticize neural patterns in conv-layers of the CNN and mine part concepts. For each part concept, we mine neural patterns in the pre-trained CNN, which are related to the target part, and use these patterns to construct an And-Or graph (AOG) to represent a four-layer semantic hierarchy of the part. As an interpretable model, the AOG associates different CNN units with different explicit object parts. We use an active human-computer communication to incrementally grow such an AOG on the pre-trained CNN as follows. We allow the computer to actively identify objects, whose neural patterns cannot be explained by the current AOG. Then, the computer asks human about the unexplained objects, and uses the answers to automatically discover certain CNN patterns corresponding to the missing knowledge. We incrementally grow the AOG to encode new knowledge discovered during the active-learning process. In experiments, our method exhibits high learning efficiency. Our method uses about 1/6-1/3 of the part annotations for training, but achieves similar or better part-localization performance than fast-RCNN methods.



### Pyramidal Gradient Matching for Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1704.03217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03217v1)
- **Published**: 2017-04-11 09:38:52+00:00
- **Updated**: 2017-04-11 09:38:52+00:00
- **Authors**: Yuanwei Li
- **Comment**: This work was finished in August 2016 and then submitted to IEEE PAMI
  in August 17,2016 and submitted to IEEE TIP in April 9,2017 after revising
- **Journal**: None
- **Summary**: Initializing optical flow field by either sparse descriptor matching or dense patch matches has been proved to be particularly useful for capturing large displacements. In this paper, we present a pyramidal gradient matching approach that can provide dense matches for highly accurate and efficient optical flow estimation. A novel contribution of our method is that image gradient is used to describe image patches and proved to be able to produce robust matching. Therefore, our method is more efficient than methods that adopt special features (like SIFT) or patch distance metric. Moreover, we find that image gradient is scalable for optical flow estimation, which means we can use different levels of gradient feature (for example, full gradients or only direction information of gradients) to obtain different complexity without dramatic changes in accuracy. Another contribution is that we uncover the secrets of limited PatchMatch through a thorough analysis and design a pyramidal matching framework based these secrets. Our pyramidal matching framework is aimed at robust gradient matching and effective to grow inliers and reject outliers. In this framework, we present some special enhancements for outlier filtering in gradient matching. By initializing EpicFlow with our matches, experimental results show that our method is efficient and robust (ranking 1st on both clean pass and final pass of MPI Sintel dataset among published methods).



### Reconstruction of three-dimensional porous media using generative adversarial neural networks
- **Arxiv ID**: http://arxiv.org/abs/1704.03225v1
- **DOI**: 10.1103/PhysRevE.96.043309
- **Categories**: **cs.CV**, cond-mat.mtrl-sci, physics.flu-dyn, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/1704.03225v1)
- **Published**: 2017-04-11 09:55:55+00:00
- **Updated**: 2017-04-11 09:55:55+00:00
- **Authors**: Lukas Mosser, Olivier Dubrule, Martin J. Blunt
- **Comment**: 21 pages, 20 figures
- **Journal**: Phys. Rev. E 96, 043309 (2017)
- **Summary**: To evaluate the variability of multi-phase flow properties of porous media at the pore scale, it is necessary to acquire a number of representative samples of the void-solid structure. While modern x-ray computer tomography has made it possible to extract three-dimensional images of the pore space, assessment of the variability in the inherent material properties is often experimentally not feasible. We present a novel method to reconstruct the solid-void structure of porous media by applying a generative neural network that allows an implicit description of the probability distribution represented by three-dimensional image datasets. We show, by using an adversarial learning approach for neural networks, that this method of unsupervised learning is able to generate representative samples of porous media that honor their statistics. We successfully compare measures of pore morphology, such as the Euler characteristic, two-point statistics and directional single-phase permeability of synthetic realizations with the calculated properties of a bead pack, Berea sandstone, and Ketton limestone. Results show that GANs can be used to reconstruct high-resolution three-dimensional images of porous media at different scales that are representative of the morphology of the images used to train the neural network. The fully convolutional nature of the trained neural network allows the generation of large samples while maintaining computational efficiency. Compared to classical stochastic methods of image reconstruction, the implicit representation of the learned data distribution can be stored and reused to generate multiple realizations of the pore structure very rapidly.



### Learning Deep CNN Denoiser Prior for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1704.03264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03264v1)
- **Published**: 2017-04-11 12:30:46+00:00
- **Updated**: 2017-04-11 12:30:46+00:00
- **Authors**: Kai Zhang, Wangmeng Zuo, Shuhang Gu, Lei Zhang
- **Comment**: Accepted to CVPR 2017. Code: https://github.com/cszn/ircnn
- **Journal**: None
- **Summary**: Model-based optimization methods and discriminative learning methods have been the two dominant strategies for solving various inverse problems in low-level vision. Typically, those two kinds of methods have their respective merits and drawbacks, e.g., model-based optimization methods are flexible for handling different inverse problems but are usually time-consuming with sophisticated priors for the purpose of good performance; in the meanwhile, discriminative learning methods have fast testing speed but their application range is greatly restricted by the specialized task. Recent works have revealed that, with the aid of variable splitting techniques, denoiser prior can be plugged in as a modular part of model-based optimization methods to solve other inverse problems (e.g., deblurring). Such an integration induces considerable advantage when the denoiser is obtained via discriminative learning. However, the study of integration with fast discriminative denoiser prior is still lacking. To this end, this paper aims to train a set of fast and effective CNN (convolutional neural network) denoisers and integrate them into model-based optimization method to solve other inverse problems. Experimental results demonstrate that the learned set of denoisers not only achieve promising Gaussian denoising results but also can be used as prior to deliver good performance for various low-level vision applications.



### Simultaneous Stereo Video Deblurring and Scene Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1704.03273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03273v1)
- **Published**: 2017-04-11 13:04:15+00:00
- **Updated**: 2017-04-11 13:04:15+00:00
- **Authors**: Liyuan Pan, Yuchao Dai, Miaomiao Liu, Fatih Porikli
- **Comment**: Accepted to IEEE International Conference on Computer Vision and
  Pattern Recognition (CVPR) 2017
- **Journal**: None
- **Summary**: Videos for outdoor scene often show unpleasant blur effects due to the large relative motion between the camera and the dynamic objects and large depth variations. Existing works typically focus monocular video deblurring. In this paper, we propose a novel approach to deblurring from stereo videos. In particular, we exploit the piece-wise planar assumption about the scene and leverage the scene flow information to deblur the image. Unlike the existing approach [31] which used a pre-computed scene flow, we propose a single framework to jointly estimate the scene flow and deblur the image, where the motion cues from scene flow estimation and blur information could reinforce each other, and produce superior results than the conventional scene flow estimation or stereo deblurring methods. We evaluate our method extensively on two available datasets and achieve significant improvement in flow estimation and removing the blur effect over the state-of-the-art methods.



### Ensemble classifier approach in breast cancer detection and malignancy grading- A review
- **Arxiv ID**: http://arxiv.org/abs/1704.03801v1
- **DOI**: 10.5121/ijmpict.2017.8102
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03801v1)
- **Published**: 2017-04-11 13:31:31+00:00
- **Updated**: 2017-04-11 13:31:31+00:00
- **Authors**: Deepti Ameta
- **Comment**: 10 pages,1 figure,5 tables
- **Journal**: International Journal of Managing Public Sector Information and
  Communication Technologies (IJMPICT) Vol. 8, No. 1, March 2017
- **Summary**: The diagnosed cases of Breast cancer is increasing annually and unfortunately getting converted into a high mortality rate. Cancer, at the early stages, is hard to detect because the malicious cells show similar properties (density) as shown by the non-malicious cells. The mortality ratio could have been minimized if the breast cancer could have been detected in its early stages. But the current systems have not been able to achieve a fully automatic system which is not just capable of detecting the breast cancer but also can detect the stage of it. Estimation of malignancy grading is important in diagnosing the degree of growth of malicious cells as well as in selecting a proper therapy for the patient. Therefore, a complete and efficient clinical decision support system is proposed which is capable of achieving breast cancer malignancy grading scheme very efficiently. The system is based on Image processing and machine learning domains. Classification Imbalance problem, a machine learning problem, occurs when instances of one class is much higher than the instances of the other class resulting in an inefficient classification of samples and hence a bad decision support system. Therefore EUSBoost, ensemble based classifier is proposed which is efficient and is able to outperform other classifiers as it takes the benefits of both-boosting algorithm with Random Undersampling techniques. Also comparison of EUSBoost with other techniques is shown in the paper.



### Online Video Deblurring via Dynamic Temporal Blending Network
- **Arxiv ID**: http://arxiv.org/abs/1704.03285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03285v1)
- **Published**: 2017-04-11 13:41:50+00:00
- **Updated**: 2017-04-11 13:41:50+00:00
- **Authors**: Tae Hyun Kim, Kyoung Mu Lee, Bernhard Schölkopf, Michael Hirsch
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: State-of-the-art video deblurring methods are capable of removing non-uniform blur caused by unwanted camera shake and/or object motion in dynamic scenes. However, most existing methods are based on batch processing and thus need access to all recorded frames, rendering them computationally demanding and time consuming and thus limiting their practical use. In contrast, we propose an online (sequential) video deblurring method based on a spatio-temporal recurrent network that allows for real-time performance. In particular, we introduce a novel architecture which extends the receptive field while keeping the overall size of the network small to enable fast execution. In doing so, our network is able to remove even large blur caused by strong camera shake and/or fast moving objects. Furthermore, we propose a novel network layer that enforces temporal consistency between consecutive frames by dynamic temporal blending which compares and adaptively (at test time) shares features obtained at different time steps. We show the superiority of the proposed method in an extensive experimental evaluation.



### Automatic segmentation of MR brain images with a convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1704.03295v1
- **DOI**: 10.1109/TMI.2016.2548501
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03295v1)
- **Published**: 2017-04-11 14:15:07+00:00
- **Updated**: 2017-04-11 14:15:07+00:00
- **Authors**: Pim Moeskops, Max A. Viergever, Adriënne M. Mendrik, Linda S. de Vries, Manon J. N. L. Benders, Ivana Išgum
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging, 35(5), 1252-1261 (2016)
- **Summary**: Automatic segmentation in MR brain images is important for quantitative analysis in large-scale studies with images acquired at all ages.   This paper presents a method for the automatic segmentation of MR brain images into a number of tissue classes using a convolutional neural network. To ensure that the method obtains accurate segmentation details as well as spatial consistency, the network uses multiple patch sizes and multiple convolution kernel sizes to acquire multi-scale information about each voxel. The method is not dependent on explicit features, but learns to recognise the information that is important for the classification based on training data. The method requires a single anatomical MR image only.   The segmentation method is applied to five different data sets: coronal T2-weighted images of preterm infants acquired at 30 weeks postmenstrual age (PMA) and 40 weeks PMA, axial T2- weighted images of preterm infants acquired at 40 weeks PMA, axial T1-weighted images of ageing adults acquired at an average age of 70 years, and T1-weighted images of young adults acquired at an average age of 23 years. The method obtained the following average Dice coefficients over all segmented tissue classes for each data set, respectively: 0.87, 0.82, 0.84, 0.86 and 0.91.   The results demonstrate that the method obtains accurate segmentations in all five sets, and hence demonstrates its robustness to differences in age and acquisition protocol.



### Interpretable Explanations of Black Boxes by Meaningful Perturbation
- **Arxiv ID**: http://arxiv.org/abs/1704.03296v4
- **DOI**: 10.1109/ICCV.2017.371
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.03296v4)
- **Published**: 2017-04-11 14:15:20+00:00
- **Updated**: 2021-12-03 15:05:54+00:00
- **Authors**: Ruth Fong, Andrea Vedaldi
- **Comment**: Final camera-ready paper published at ICCV 2017 (Supplementary
  materials:
  http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Fong_Interpretable_Explanations_of_ICCV_2017_supplemental.pdf)
- **Journal**: Proceedings of the 2017 IEEE International Conference on Computer
  Vision (ICCV)
- **Summary**: As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks "look" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.



### Quality Aware Network for Set to Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.03373v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1704.03373v1)
- **Published**: 2017-04-11 15:47:41+00:00
- **Updated**: 2017-04-11 15:47:41+00:00
- **Authors**: Yu Liu, Junjie Yan, Wanli Ouyang
- **Comment**: Accepted at CVPR 2017
- **Journal**: None
- **Summary**: This paper targets on the problem of set to set recognition, which learns the metric between two image sets. Images in each set belong to the same identity. Since images in a set can be complementary, they hopefully lead to higher accuracy in practical applications. However, the quality of each sample cannot be guaranteed, and samples with poor quality will hurt the metric. In this paper, the quality aware network (QAN) is proposed to confront this problem, where the quality of each sample can be automatically learned although such information is not explicitly provided in the training stage. The network has two branches, where the first branch extracts appearance feature embedding for each sample and the other branch predicts quality score for each sample. Features and quality scores of all samples in a set are then aggregated to generate the final feature embedding. We show that the two branches can be trained in an end-to-end manner given only the set-level identity annotation. Analysis on gradient spread of this mechanism indicates that the quality learned by the network is beneficial to set-to-set recognition and simplifies the distribution that the network needs to fit. Experiments on both face verification and person re-identification show advantages of the proposed QAN. The source code and network structure can be downloaded at https://github.com/sciencefans/Quality-Aware-Network.



### Reconstruction of~3-D Rigid Smooth Curves Moving Free when Two Traceable Points Only are Available
- **Arxiv ID**: http://arxiv.org/abs/1704.03375v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1704.03375v1)
- **Published**: 2017-04-11 15:48:56+00:00
- **Updated**: 2017-04-11 15:48:56+00:00
- **Authors**: Mieczysław A. Kłopotek
- **Comment**: None
- **Journal**: Preliminaru version of the paper M.A. K{\l}opotek: Reconstruction
  of 3-D rigid smooth curves moving free when two traceable points only are
  available. Machine Graphics \& Vision 1(1992)1-2, pp. 392-405
- **Summary**: This paper extends previous research in that sense that for orthogonal projections of rigid smooth (true-3D) curves moving totally free it reduces the number of required traceable points to two only (the best results known so far to the author are 3 points from free motion and 2 for motion restricted to rotation around a fixed direction and and 2 for motion restricted to influence of a homogeneous force field). The method used is exploitation of information on tangential projections. It discusses also possibility of simplification of reconstruction of flat curves moving free for prospective projections.



### Deep Learning for Multi-Task Medical Image Segmentation in Multiple Modalities
- **Arxiv ID**: http://arxiv.org/abs/1704.03379v1
- **DOI**: 10.1007/978-3-319-46723-8_55
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03379v1)
- **Published**: 2017-04-11 15:52:34+00:00
- **Updated**: 2017-04-11 15:52:34+00:00
- **Authors**: Pim Moeskops, Jelmer M. Wolterink, Bas H. M. van der Velden, Kenneth G. A. Gilhuijs, Tim Leiner, Max A. Viergever, Ivana Išgum
- **Comment**: None
- **Journal**: Moeskops, P., Wolterink, J.M., van der Velden, B.H.M., Gilhuijs,
  K.G.A., Leiner, T., Viergever, M.A., I\v{s}gum, I. Deep learning for
  multi-task medical image segmentation in multiple modalities. In: MICCAI
  2016, pp. 478-486
- **Summary**: Automatic segmentation of medical images is an important task for many clinical applications. In practice, a wide range of anatomical structures are visualised using different imaging modalities. In this paper, we investigate whether a single convolutional neural network (CNN) can be trained to perform different segmentation tasks.   A single CNN is trained to segment six tissues in MR brain images, the pectoral muscle in MR breast images, and the coronary arteries in cardiac CTA. The CNN therefore learns to identify the imaging modality, the visualised anatomical structures, and the tissue classes.   For each of the three tasks (brain MRI, breast MRI and cardiac CTA), this combined training procedure resulted in a segmentation performance equivalent to that of a CNN trained specifically for that task, demonstrating the high capacity of CNN architectures. Hence, a single system could be used in clinical practice to automatically perform diverse segmentation tasks without task-specific training.



### A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.03414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03414v1)
- **Published**: 2017-04-11 16:57:52+00:00
- **Updated**: 2017-04-11 16:57:52+00:00
- **Authors**: Xiaolong Wang, Abhinav Shrivastava, Abhinav Gupta
- **Comment**: CVPR 2017 Camera Ready
- **Journal**: None
- **Summary**: How do we learn an object detector that is invariant to occlusions and deformations? Our current solution is to use a data-driven strategy -- collect large-scale datasets which have object instances under different conditions. The hope is that the final classifier can use these examples to learn invariances. But is it really possible to see all the occlusions in a dataset? We argue that like categories, occlusions and object deformations also follow a long-tail. Some occlusions and deformations are so rare that they hardly happen; yet we want to learn a model invariant to such occurrences. In this paper, we propose an alternative solution. We propose to learn an adversarial network that generates examples with occlusions and deformations. The goal of the adversary is to generate examples that are difficult for the object detector to classify. In our framework both the original detector and adversary are learned in a joint manner. Our experimental results indicate a 2.3% mAP boost on VOC07 and a 2.6% mAP boost on VOC2012 object detection challenge compared to the Fast-RCNN pipeline. We also release the code for this paper.



### Forecasting Human Dynamics from Static Images
- **Arxiv ID**: http://arxiv.org/abs/1704.03432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03432v1)
- **Published**: 2017-04-11 17:20:06+00:00
- **Updated**: 2017-04-11 17:20:06+00:00
- **Authors**: Yu-Wei Chao, Jimei Yang, Brian Price, Scott Cohen, Jia Deng
- **Comment**: Accepted in CVPR 2017
- **Journal**: None
- **Summary**: This paper presents the first study on forecasting human dynamics from static images. The problem is to input a single RGB image and generate a sequence of upcoming human body poses in 3D. To address the problem, we propose the 3D Pose Forecasting Network (3D-PFNet). Our 3D-PFNet integrates recent advances on single-image human pose estimation and sequence prediction, and converts the 2D predictions into 3D space. We train our 3D-PFNet using a three-step training strategy to leverage a diverse source of training data, including image and video based human pose datasets and 3D motion capture (MoCap) data. We demonstrate competitive performance of our 3D-PFNet on 2D pose forecasting and 3D pose recovery through quantitative and qualitative results.



### Solving the L1 regularized least square problem via a box-constrained smooth minimization
- **Arxiv ID**: http://arxiv.org/abs/1704.03443v3
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.03443v3)
- **Published**: 2017-04-11 17:41:24+00:00
- **Updated**: 2021-10-20 04:46:38+00:00
- **Authors**: Majid Mohammadi, Wout Hofman, Yaohua Tan, S. Hamid Mousavi
- **Comment**: I stoped working on the paper and cannot guarantee its scientific
  correctness
- **Journal**: None
- **Summary**: In this paper, an equivalent smooth minimization for the L1 regularized least square problem is proposed. The proposed problem is a convex box-constrained smooth minimization which allows applying fast optimization methods to find its solution. Further, it is investigated that the property "the dual of dual is primal" holds for the L1 regularized least square problem. A solver for the smooth problem is proposed, and its affinity to the proximal gradient is shown. Finally, the experiments on L1 and total variation regularized problems are performed, and the corresponding results are reported.



### Learning Two-Branch Neural Networks for Image-Text Matching Tasks
- **Arxiv ID**: http://arxiv.org/abs/1704.03470v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03470v4)
- **Published**: 2017-04-11 18:00:25+00:00
- **Updated**: 2018-05-01 18:21:59+00:00
- **Authors**: Liwei Wang, Yin Li, Jing Huang, Svetlana Lazebnik
- **Comment**: accepted version in TPAMI 2018
- **Journal**: None
- **Summary**: Image-language matching tasks have recently attracted a lot of attention in the computer vision field. These tasks include image-sentence matching, i.e., given an image query, retrieving relevant sentences and vice versa, and region-phrase matching or visual grounding, i.e., matching a phrase to relevant regions. This paper investigates two-branch neural networks for learning the similarity between these two data modalities. We propose two network structures that produce different output representations. The first one, referred to as an embedding network, learns an explicit shared latent embedding space with a maximum-margin ranking loss and novel neighborhood constraints. Compared to standard triplet sampling, we perform improved neighborhood sampling that takes neighborhood information into consideration while constructing mini-batches. The second network structure, referred to as a similarity network, fuses the two branches via element-wise product and is trained with regression loss to directly predict a similarity score. Extensive experiments show that our networks achieve high accuracies for phrase localization on the Flickr30K Entities dataset and for bi-directional image-sentence retrieval on Flickr30K and MSCOCO datasets.



### Learning Proximal Operators: Using Denoising Networks for Regularizing Inverse Imaging Problems
- **Arxiv ID**: http://arxiv.org/abs/1704.03488v2
- **DOI**: 10.1109/ICCV.2017.198
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03488v2)
- **Published**: 2017-04-11 18:33:51+00:00
- **Updated**: 2017-08-30 11:22:33+00:00
- **Authors**: Tim Meinhardt, Michael Moeller, Caner Hazirbas, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: While variational methods have been among the most powerful tools for solving linear inverse problems in imaging, deep (convolutional) neural networks have recently taken the lead in many challenging benchmarks. A remaining drawback of deep learning approaches is their requirement for an expensive retraining whenever the specific problem, the noise level, noise type, or desired measure of fidelity changes. On the contrary, variational methods have a plug-and-play nature as they usually consist of separate data fidelity and regularization terms.   In this paper we study the possibility of replacing the proximal operator of the regularization used in many convex energy minimization algorithms by a denoising neural network. The latter therefore serves as an implicit natural image prior, while the data term can still be chosen independently. Using a fixed denoising neural network in exemplary problems of image deconvolution with different blur kernels and image demosaicking, we obtain state-of-the-art reconstruction results. These indicate the high generalizability of our approach and a reduction of the need for problem-specific training. Additionally, we discuss novel results on the analysis of possible optimization algorithms to incorporate the network into, as well as the choices of algorithm parameters and their relation to the noise level the neural network is trained on.



### CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction
- **Arxiv ID**: http://arxiv.org/abs/1704.03489v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03489v1)
- **Published**: 2017-04-11 18:37:11+00:00
- **Updated**: 2017-04-11 18:37:11+00:00
- **Authors**: Keisuke Tateno, Federico Tombari, Iro Laina, Nassir Navab
- **Comment**: 10 pages, 6 figures, IEEE Computer Society Conference on Computer
  Vision and Pattern Recognition (CVPR), Hawaii, USA, June, 2017. The first two
  authors contribute equally to this paper
- **Journal**: None
- **Summary**: Given the recent advances in depth prediction from Convolutional Neural Networks (CNNs), this paper investigates how predicted depth maps from a deep neural network can be deployed for accurate and dense monocular reconstruction. We propose a method where CNN-predicted dense depth maps are naturally fused together with depth measurements obtained from direct monocular SLAM. Our fusion scheme privileges depth prediction in image locations where monocular SLAM approaches tend to fail, e.g. along low-textured regions, and vice-versa. We demonstrate the use of depth prediction for estimating the absolute scale of the reconstruction, hence overcoming one of the major limitations of monocular SLAM. Finally, we propose a framework to efficiently fuse semantic labels, obtained from a single frame, with dense SLAM, yielding semantically coherent scene reconstruction from a single view. Evaluation results on two benchmark datasets show the robustness and accuracy of our approach.



### Creativity: Generating Diverse Questions using Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1704.03493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03493v1)
- **Published**: 2017-04-11 18:46:58+00:00
- **Updated**: 2017-04-11 18:46:58+00:00
- **Authors**: Unnat Jain, Ziyu Zhang, Alexander Schwing
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: Generating diverse questions for given images is an important task for computational education, entertainment and AI assistants. Different from many conventional prediction techniques is the need for algorithms to generate a diverse set of plausible questions, which we refer to as "creativity". In this paper we propose a creative algorithm for visual question generation which combines the advantages of variational autoencoders with long short-term memory networks. We demonstrate that our framework is able to generate a large set of varying questions given a single input image.



### UC Merced Submission to the ActivityNet Challenge 2016
- **Arxiv ID**: http://arxiv.org/abs/1704.03503v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1704.03503v1)
- **Published**: 2017-04-11 19:11:36+00:00
- **Updated**: 2017-04-11 19:11:36+00:00
- **Authors**: Yi Zhu, Shawn Newsam, Zaikun Xu
- **Comment**: Notebook paper for ActivityNet 2016 challenge, untrimmed video
  classification track
- **Journal**: None
- **Summary**: This notebook paper describes our system for the untrimmed classification task in the ActivityNet challenge 2016. We investigate multiple state-of-the-art approaches for action recognition in long, untrimmed videos. We exploit hand-crafted motion boundary histogram features as well feature activations from deep networks such as VGG16, GoogLeNet, and C3D. These features are separately fed to linear, one-versus-rest support vector machine classifiers to produce confidence scores for each action class. These predictions are then fused along with the softmax scores of the recent ultra-deep ResNet-101 using weighted averaging.



### Toward a new approach for massive LiDAR data processing
- **Arxiv ID**: http://arxiv.org/abs/1704.03527v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.03527v1)
- **Published**: 2017-04-11 20:33:28+00:00
- **Updated**: 2017-04-11 20:33:28+00:00
- **Authors**: V-H Cao, K-X Chu, Nhien-An Le-Khac, M-T Kechadi, Debra F. Laefer, Linh Truong-Hong
- **Comment**: None
- **Journal**: None
- **Summary**: Laser scanning (also known as Light Detection And Ranging) has been widely applied in various application. As part of that, aerial laser scanning (ALS) has been used to collect topographic data points for a large area, which triggers to million points to be acquired. Furthermore, today, with integrating full wareform (FWF) technology during ALS data acquisition, all return information of laser pulse is stored. Thus, ALS data are to be massive and complexity since the FWF of each laser pulse can be stored up to 256 samples and density of ALS data is also increasing significantly. Processing LiDAR data demands heavy operations and the traditional approaches require significant hardware and running time. On the other hand, researchers have recently proposed parallel approaches for analysing LiDAR data. These approaches are normally based on parallel architecture of target systems such as multi-core processors, GPU, etc. However, there is still missing efficient approaches/tools supporting the analysis of LiDAR data due to the lack of a deep study on both library tools and algorithms used in processing this data. In this paper, we present a comparative study of software libraries and algorithms to optimise the processing of LiDAR data. We also propose new method to improve this process with experiments on large LiDAR data. Finally, we discuss on a parallel solution of our approach where we integrate parallel computing in processing LiDAR data.



### Feature Selection Parallel Technique for Remotely Sensed Imagery Classification
- **Arxiv ID**: http://arxiv.org/abs/1704.03530v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.03530v1)
- **Published**: 2017-04-11 20:44:10+00:00
- **Updated**: 2017-04-11 20:44:10+00:00
- **Authors**: Nhien-An Le-Khac, M-Tahar Kechadi, Bo Wu, C. Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Remote sensing research focusing on feature selection has long attracted the attention of the remote sensing community because feature selection is a prerequisite for image processing and various applications. Different feature selection methods have been proposed to improve the classification accuracy. They vary from basic search techniques to clonal selections, and various optimal criteria have been investigated. Recently, methods using dependence-based measures have attracted much attention due to their ability to deal with very high dimensional datasets. However, these methods are based on Cramers V test, which has performance issues with large datasets. In this paper, we propose a parallel approach to improve their performance. We evaluate our approach on hyper-spectral and high spatial resolution images and compare it to the proposed methods with a centralized version as preliminary results. The results are very promising.



### Learning Detection with Diverse Proposals
- **Arxiv ID**: http://arxiv.org/abs/1704.03533v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03533v1)
- **Published**: 2017-04-11 20:51:40+00:00
- **Updated**: 2017-04-11 20:51:40+00:00
- **Authors**: Samaneh Azadi, Jiashi Feng, Trevor Darrell
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: To predict a set of diverse and informative proposals with enriched representations, this paper introduces a differentiable Determinantal Point Process (DPP) layer that is able to augment the object detection architectures. Most modern object detection architectures, such as Faster R-CNN, learn to localize objects by minimizing deviations from the ground-truth but ignore correlation between multiple proposals and object categories. Non-Maximum Suppression (NMS) as a widely used proposal pruning scheme ignores label- and instance-level relations between object candidates resulting in multi-labeled detections. In the multi-class case, NMS selects boxes with the largest prediction scores ignoring the semantic relation between categories of potential election. In contrast, our trainable DPP layer, allowing for Learning Detection with Diverse Proposals (LDDP), considers both label-level contextual information and spatial layout relationships between proposals without increasing the number of parameters of the network, and thus improves location and category specifications of final detected bounding boxes substantially during both training and inference schemes. Furthermore, we show that LDDP keeps it superiority over Faster R-CNN even if the number of proposals generated by LDPP is only ~30% as many as those for Faster R-CNN.



### Attention-based Extraction of Structured Information from Street View Imagery
- **Arxiv ID**: http://arxiv.org/abs/1704.03549v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03549v4)
- **Published**: 2017-04-11 21:55:19+00:00
- **Updated**: 2017-08-20 11:34:31+00:00
- **Authors**: Zbigniew Wojna, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu, Yeqing Li, Julian Ibarz
- **Comment**: Updated references, added link to the source code
- **Journal**: None
- **Summary**: We present a neural network model - based on CNNs, RNNs and a novel attention mechanism - which achieves 84.2% accuracy on the challenging French Street Name Signs (FSNS) dataset, significantly outperforming the previous state of the art (Smith'16), which achieved 72.46%. Furthermore, our new method is much simpler and more general than the previous approach. To demonstrate the generality of our model, we show that it also performs well on an even more challenging dataset derived from Google Street View, in which the goal is to extract business names from store fronts. Finally, we study the speed/accuracy tradeoff that results from using CNN feature extractors of different depths. Surprisingly, we find that deeper is not always better (in terms of accuracy, as well as speed). Our resulting model is simple, accurate and fast, allowing it to be used at scale on a variety of challenging real-world text extraction problems.



### Cutting the Error by Half: Investigation of Very Deep CNN and Advanced Training Strategies for Document Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1704.03557v1
- **DOI**: 10.1109/ICDAR.2017.149
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03557v1)
- **Published**: 2017-04-11 22:35:58+00:00
- **Updated**: 2017-04-11 22:35:58+00:00
- **Authors**: Muhammad Zeshan Afzal, Andreas Kölsch, Sheraz Ahmed, Marcus Liwicki
- **Comment**: None
- **Journal**: None
- **Summary**: We present an exhaustive investigation of recent Deep Learning architectures, algorithms, and strategies for the task of document image classification to finally reduce the error by more than half. Existing approaches, such as the DeepDocClassifier, apply standard Convolutional Network architectures with transfer learning from the object recognition domain. The contribution of the paper is threefold: First, it investigates recently introduced very deep neural network architectures (GoogLeNet, VGG, ResNet) using transfer learning (from real images). Second, it proposes transfer learning from a huge set of document images, i.e. 400,000 documents. Third, it analyzes the impact of the amount of training data (document images) and other parameters to the classification abilities. We use two datasets, the Tobacco-3482 and the large-scale RVL-CDIP dataset. We achieve an accuracy of 91.13% for the Tobacco-3482 dataset while earlier approaches reach only 77.6%. Thus, a relative error reduction of more than 60% is achieved. For the large dataset RVL-CDIP, an accuracy of 90.97% is achieved, corresponding to a relative error reduction of 11.5%.



### Beyond Planar Symmetry: Modeling human perception of reflection and rotation symmetries in the wild
- **Arxiv ID**: http://arxiv.org/abs/1704.03568v2
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.03568v2)
- **Published**: 2017-04-11 23:25:25+00:00
- **Updated**: 2017-08-28 17:11:05+00:00
- **Authors**: Christopher Funk, Yanxi Liu
- **Comment**: To appear in the International Conference on Computer Vision (ICCV)
  2017
- **Journal**: None
- **Summary**: Humans take advantage of real world symmetries for various tasks, yet capturing their superb symmetry perception mechanism with a computational model remains elusive. Motivated by a new study demonstrating the extremely high inter-person accuracy of human perceived symmetries in the wild, we have constructed the first deep-learning neural network for reflection and rotation symmetry detection (Sym-NET), trained on photos from MS-COCO (Microsoft-Common Object in COntext) dataset with nearly 11K consistent symmetry-labels from more than 400 human observers. We employ novel methods to convert discrete human labels into symmetry heatmaps, capture symmetry densely in an image and quantitatively evaluate Sym-NET against multiple existing computer vision algorithms. On CVPR 2013 symmetry competition testsets and unseen MS-COCO photos, Sym-NET significantly outperforms all other competitors. Beyond mathematically well-defined symmetries on a plane, Sym-NET demonstrates abilities to identify viewpoint-varied 3D symmetries, partially occluded symmetrical objects, and symmetries at a semantic level.



