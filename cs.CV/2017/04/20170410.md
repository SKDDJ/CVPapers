# Arxiv Papers in cs.CV on 2017-04-10
### Learning Important Features Through Propagating Activation Differences
- **Arxiv ID**: http://arxiv.org/abs/1704.02685v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1704.02685v2)
- **Published**: 2017-04-10 02:23:57+00:00
- **Updated**: 2019-10-12 22:13:28+00:00
- **Authors**: Avanti Shrikumar, Peyton Greenside, Anshul Kundaje
- **Comment**: Updated to include changes present in the ICML camera-ready paper,
  and other small corrections
- **Journal**: PMLR 70:3145-3153, 2017
- **Summary**: The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.



### ClusterNet: Detecting Small Objects in Large Scenes by Exploiting Spatio-Temporal Information
- **Arxiv ID**: http://arxiv.org/abs/1704.02694v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02694v2)
- **Published**: 2017-04-10 03:44:13+00:00
- **Updated**: 2017-12-04 21:14:19+00:00
- **Authors**: Rodney LaLonde, Dong Zhang, Mubarak Shah
- **Comment**: Main paper is 8 pages. Supplemental section contains a walk-through
  of our method (using a qualitative example) and qualitative results for WPAFB
  2009 dataset
- **Journal**: None
- **Summary**: Object detection in wide area motion imagery (WAMI) has drawn the attention of the computer vision research community for a number of years. WAMI proposes a number of unique challenges including extremely small object sizes, both sparse and densely-packed objects, and extremely large search spaces (large video frames). Nearly all state-of-the-art methods in WAMI object detection report that appearance-based classifiers fail in this challenging data and instead rely almost entirely on motion information in the form of background subtraction or frame-differencing. In this work, we experimentally verify the failure of appearance-based classifiers in WAMI, such as Faster R-CNN and a heatmap-based fully convolutional neural network (CNN), and propose a novel two-stage spatio-temporal CNN which effectively and efficiently combines both appearance and motion information to significantly surpass the state-of-the-art in WAMI object detection. To reduce the large search space, the first stage (ClusterNet) takes in a set of extremely large video frames, combines the motion and appearance information within the convolutional architecture, and proposes regions of objects of interest (ROOBI). These ROOBI can contain from one to clusters of several hundred objects due to the large video frame size and varying object density in WAMI. The second stage (FoveaNet) then estimates the centroid location of all objects in that given ROOBI simultaneously via heatmap estimation. The proposed method exceeds state-of-the-art results on the WPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped objects, as well as being the first proposed method in wide area motion imagery to detect completely stationary objects.



### Automatic Liver Lesion Detection using Cascaded Deep Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.02703v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02703v2)
- **Published**: 2017-04-10 04:05:50+00:00
- **Updated**: 2017-05-21 02:58:40+00:00
- **Authors**: Lei Bi, Jinman Kim, Ashnil Kumar, Dagan Feng
- **Comment**: Submission for 2017 ISBI LiTS Challenge
- **Journal**: None
- **Summary**: Automatic segmentation of liver lesions is a fundamental requirement towards the creation of computer aided diagnosis (CAD) and decision support systems (CDS). Traditional segmentation approaches depend heavily upon hand-crafted features and a priori knowledge of the user. As such, these methods are difficult to adopt within a clinical environment. Recently, deep learning methods based on fully convolutional networks (FCNs) have been successful in many segmentation problems primarily because they leverage a large labelled dataset to hierarchically learn the features that best correspond to the shallow visual appearance as well as the deep semantics of the areas to be segmented. However, FCNs based on a 16 layer VGGNet architecture have limited capacity to add additional layers. Therefore, it is challenging to learn more discriminative features among different classes for FCNs. In this study, we overcome these limitations using deep residual networks (ResNet) to segment liver lesions. ResNet contain skip connections between convolutional layers, which solved the problem of the training degradation of training accuracy in very deep networks and thereby enables the use of additional layers for learning more discriminative features. In addition, we achieve more precise boundary definitions through a novel cascaded ResNet architecture with multi-scale fusion to gradually learn and infer the boundaries of both the liver and the liver lesions. Our proposed method achieved 4th place in the ISBI 2017 Liver Tumor Segmentation Challenge by the submission deadline.



### Adaptive Relaxed ADMM: Convergence Theory and Practical Implementation
- **Arxiv ID**: http://arxiv.org/abs/1704.02712v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.02712v1)
- **Published**: 2017-04-10 05:07:38+00:00
- **Updated**: 2017-04-10 05:07:38+00:00
- **Authors**: Zheng Xu, Mario A. T. Figueiredo, Xiaoming Yuan, Christoph Studer, Tom Goldstein
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Many modern computer vision and machine learning applications rely on solving difficult optimization problems that involve non-differentiable objective functions and constraints. The alternating direction method of multipliers (ADMM) is a widely used approach to solve such problems. Relaxed ADMM is a generalization of ADMM that often achieves better performance, but its efficiency depends strongly on algorithm parameters that must be chosen by an expert user. We propose an adaptive method that automatically tunes the key algorithm parameters to achieve optimal performance without user oversight. Inspired by recent work on adaptivity, the proposed adaptive relaxed ADMM (ARADMM) is derived by assuming a Barzilai-Borwein style linear gradient. A detailed convergence analysis of ARADMM is provided, and numerical results on several applications demonstrate fast practical convergence.



### DeepPermNet: Visual Permutation Learning
- **Arxiv ID**: http://arxiv.org/abs/1704.02729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02729v1)
- **Published**: 2017-04-10 06:59:00+00:00
- **Updated**: 2017-04-10 06:59:00+00:00
- **Authors**: Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, Stephen Gould
- **Comment**: Accepted in IEEE International Conference on Computer Vision and
  Pattern Recognition CVPR 2017
- **Journal**: None
- **Summary**: We present a principled approach to uncover the structure of visual data by solving a novel deep learning task coined visual permutation learning. The goal of this task is to find the permutation that recovers the structure of data from shuffled versions of it. In the case of natural images, this task boils down to recovering the original image from patches shuffled by an unknown permutation matrix. Unfortunately, permutation matrices are discrete, thereby posing difficulties for gradient-based methods. To this end, we resort to a continuous approximation of these matrices using doubly-stochastic matrices which we generate from standard CNN predictions using Sinkhorn iterations. Unrolling these iterations in a Sinkhorn network layer, we propose DeepPermNet, an end-to-end CNN model for this task. The utility of DeepPermNet is demonstrated on two challenging computer vision problems, namely, (i) relative attributes learning and (ii) self-supervised representation learning. Our results show state-of-the-art performance on the Public Figures and OSR benchmarks for (i) and on the classification and segmentation tasks on the PASCAL VOC dataset for (ii).



### Detail-revealing Deep Video Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1704.02738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02738v1)
- **Published**: 2017-04-10 07:28:27+00:00
- **Updated**: 2017-04-10 07:28:27+00:00
- **Authors**: Xin Tao, Hongyun Gao, Renjie Liao, Jue Wang, Jiaya Jia
- **Comment**: 9 pages, submitted to conference
- **Journal**: None
- **Summary**: Previous CNN-based video super-resolution approaches need to align multiple frames to the reference. In this paper, we show that proper frame alignment and motion compensation is crucial for achieving high quality results. We accordingly propose a `sub-pixel motion compensation' (SPMC) layer in a CNN framework. Analysis and experiments show the suitability of this layer in video SR. The final end-to-end, scalable CNN framework effectively incorporates the SPMC layer and fuses multiple frames to reveal image details. Our implementation can generate visually and quantitatively high-quality results, superior to current state-of-the-arts, without the need of parameter tuning.



### Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1704.02781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02781v1)
- **Published**: 2017-04-10 09:44:39+00:00
- **Updated**: 2017-04-10 09:44:39+00:00
- **Authors**: Laura Leal-Taixé, Anton Milan, Konrad Schindler, Daniel Cremers, Ian Reid, Stefan Roth
- **Comment**: None
- **Journal**: None
- **Summary**: Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for research. We present a benchmark for Multiple Object Tracking launched in the late 2014, with the goal of creating a framework for the standardized evaluation of multiple object tracking methods. This paper collects the two releases of the benchmark made so far, and provides an in-depth analysis of almost 50 state-of-the-art trackers that were tested on over 11000 frames. We show the current trends and weaknesses of multiple people tracking methods, and provide pointers of what researchers should be focusing on to push the field forward.



### Deep Affordance-grounded Sensorimotor Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.02787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02787v1)
- **Published**: 2017-04-10 10:06:53+00:00
- **Updated**: 2017-04-10 10:06:53+00:00
- **Authors**: Spyridon Thermos, Georgios Th. Papadopoulos, Petros Daras, Gerasimos Potamianos
- **Comment**: 9 pages, 7 figures, dataset link included, accepted to CVPR 2017
- **Journal**: None
- **Summary**: It is well-established by cognitive neuroscience that human perception of objects constitutes a complex process, where object appearance information is combined with evidence about the so-called object "affordances", namely the types of actions that humans typically perform when interacting with them. This fact has recently motivated the "sensorimotor" approach to the challenging task of automatic object recognition, where both information sources are fused to improve robustness. In this work, the aforementioned paradigm is adopted, surpassing current limitations of sensorimotor object recognition research. Specifically, the deep learning paradigm is introduced to the problem for the first time, developing a number of novel neuro-biologically and neuro-physiologically inspired architectures that utilize state-of-the-art neural networks for fusing the available information sources in multiple ways. The proposed methods are evaluated using a large RGB-D corpus, which is specifically collected for the task of sensorimotor object recognition and is made publicly available. Experimental results demonstrate the utility of affordance information to object recognition, achieving an up to 29% relative error reduction by its inclusion.



### Fine-graind Image Classification via Combining Vision and Language
- **Arxiv ID**: http://arxiv.org/abs/1704.02792v2
- **DOI**: 10.1109/CVPR.2017.775
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02792v2)
- **Published**: 2017-04-10 10:34:06+00:00
- **Updated**: 2017-05-03 03:01:38+00:00
- **Authors**: Xiangteng He, Yuxin Peng
- **Comment**: 9 pages, to appear in CVPR 2017
- **Journal**: None
- **Summary**: Fine-grained image classification is a challenging task due to the large intra-class variance and small inter-class variance, aiming at recognizing hundreds of sub-categories belonging to the same basic-level category. Most existing fine-grained image classification methods generally learn part detection models to obtain the semantic parts for better classification accuracy. Despite achieving promising results, these methods mainly have two limitations: (1) not all the parts which obtained through the part detection models are beneficial and indispensable for classification, and (2) fine-grained image classification requires more detailed visual descriptions which could not be provided by the part locations or attribute annotations. For addressing the above two limitations, this paper proposes the two-stream model combining vision and language (CVL) for learning latent semantic representations. The vision stream learns deep representations from the original visual information via deep convolutional neural network. The language stream utilizes the natural language descriptions which could point out the discriminative parts or characteristics for each image, and provides a flexible and compact way of encoding the salient visual aspects for distinguishing sub-categories. Since the two streams are complementary, combining the two streams can further achieves better classification accuracy. Comparing with 12 state-of-the-art methods on the widely used CUB-200-2011 dataset for fine-grained image classification, the experimental results demonstrate our CVL approach achieves the best performance.



### R-Clustering for Egocentric Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.02809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02809v1)
- **Published**: 2017-04-10 11:36:01+00:00
- **Updated**: 2017-04-10 11:36:01+00:00
- **Authors**: Estefania Talavera, Mariella Dimiccoli, Marc Bolaños, Maedeh Aghaei, Petia Radeva
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a new method for egocentric video temporal segmentation based on integrating a statistical mean change detector and agglomerative clustering(AC) within an energy-minimization framework. Given the tendency of most AC methods to oversegment video sequences when clustering their frames, we combine the clustering with a concept drift detection technique (ADWIN) that has rigorous guarantee of performances. ADWIN serves as a statistical upper bound for the clustering-based video segmentation. We integrate both techniques in an energy-minimization framework that serves to disambiguate the decision of both techniques and to complete the segmentation taking into account the temporal continuity of video frames descriptors. We present experiments over egocentric sets of more than 13.000 images acquired with different wearable cameras, showing that our method outperforms state-of-the-art clustering methods.



### Learning Human Motion Models for Long-term Predictions
- **Arxiv ID**: http://arxiv.org/abs/1704.02827v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02827v2)
- **Published**: 2017-04-10 12:22:22+00:00
- **Updated**: 2017-12-03 11:51:17+00:00
- **Authors**: Partha Ghosh, Jie Song, Emre Aksan, Otmar Hilliges
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new architecture for the learning of predictive spatio-temporal motion models from data alone. Our approach, dubbed the Dropout Autoencoder LSTM, is capable of synthesizing natural looking motion sequences over long time horizons without catastrophic drift or motion degradation. The model consists of two components, a 3-layer recurrent neural network to model temporal aspects and a novel auto-encoder that is trained to implicitly recover the spatial structure of the human skeleton via randomly removing information about joints during training time. This Dropout Autoencoder (D-AE) is then used to filter each predicted pose of the LSTM, reducing accumulation of error and hence drift over time. Furthermore, we propose new evaluation protocols to assess the quality of synthetic motion sequences even for which no ground truth data exists. The proposed protocols can be used to assess generated sequences of arbitrary length. Finally, we evaluate our proposed method on two of the largest motion-capture datasets available to date and show that our model outperforms the state-of-the-art on a variety of actions, including cyclic and acyclic motion, and that it can produce natural looking sequences over longer time horizons than previous methods.



### ActionVLAD: Learning spatio-temporal aggregation for action classification
- **Arxiv ID**: http://arxiv.org/abs/1704.02895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02895v1)
- **Published**: 2017-04-10 15:09:41+00:00
- **Updated**: 2017-04-10 15:09:41+00:00
- **Authors**: Rohit Girdhar, Deva Ramanan, Abhinav Gupta, Josef Sivic, Bryan Russell
- **Comment**: Accepted to CVPR 2017. Project page:
  https://rohitgirdhar.github.io/ActionVLAD/
- **Journal**: None
- **Summary**: In this work, we introduce a new video representation for action classification that aggregates local convolutional features across the entire spatio-temporal extent of the video. We do so by integrating state-of-the-art two-stream networks with learnable spatio-temporal feature aggregation. The resulting architecture is end-to-end trainable for whole-video classification. We investigate different strategies for pooling across space and time and combining signals from the different streams. We find that: (i) it is important to pool jointly across space and time, but (ii) appearance and motion streams are best aggregated into their own separate representations. Finally, we show that our representation outperforms the two-stream base architecture by a large margin (13% relative) as well as out-performs other baselines with comparable base architectures on HMDB51, UCF101, and Charades video classification benchmarks.



### Continuously heterogeneous hyper-objects in cryo-EM and 3-D movies of many temporal dimensions
- **Arxiv ID**: http://arxiv.org/abs/1704.02899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02899v1)
- **Published**: 2017-04-10 15:15:25+00:00
- **Updated**: 2017-04-10 15:15:25+00:00
- **Authors**: Roy R. Lederman, Amit Singer
- **Comment**: None
- **Journal**: None
- **Summary**: Single particle cryo-electron microscopy (EM) is an increasingly popular method for determining the 3-D structure of macromolecules from noisy 2-D images of single macromolecules whose orientations and positions are random and unknown. One of the great opportunities in cryo-EM is to recover the structure of macromolecules in heterogeneous samples, where multiple types or multiple conformations are mixed together. Indeed, in recent years, many tools have been introduced for the analysis of multiple discrete classes of molecules mixed together in a cryo-EM experiment. However, many interesting structures have a continuum of conformations which do not fit discrete models nicely; the analysis of such continuously heterogeneous models has remained a more elusive goal. In this manuscript, we propose to represent heterogeneous molecules and similar structures as higher dimensional objects. We generalize the basic operations used in many existing reconstruction algorithms, making our approach generic in the sense that, in principle, existing algorithms can be adapted to reconstruct those higher dimensional objects. As proof of concept, we present a prototype of a new algorithm which we use to solve simulated reconstruction problems.



### Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs
- **Arxiv ID**: http://arxiv.org/abs/1704.02901v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1704.02901v3)
- **Published**: 2017-04-10 15:18:54+00:00
- **Updated**: 2017-08-08 09:31:17+00:00
- **Authors**: Martin Simonovsky, Nikos Komodakis
- **Comment**: Accepted to CVPR 2017; extended version
- **Journal**: None
- **Summary**: A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches. The source code is available at https://github.com/mys007/ecc



### Multi-Agent Diverse Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.02906v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.02906v3)
- **Published**: 2017-04-10 15:26:23+00:00
- **Updated**: 2018-07-16 16:21:52+00:00
- **Authors**: Arnab Ghosh, Viveka Kulharia, Vinay Namboodiri, Philip H. S. Torr, Puneet K. Dokania
- **Comment**: This is an updated version of our CVPR'18 paper with the same title.
  In this version, we also introduce MAD-GAN-Sim in Appendix B
- **Journal**: None
- **Summary**: We propose MAD-GAN, an intuitive generalization to the Generative Adversarial Networks (GANs) and its conditional variants to address the well known problem of mode collapse. First, MAD-GAN is a multi-agent GAN architecture incorporating multiple generators and one discriminator. Second, to enforce that different generators capture diverse high probability modes, the discriminator of MAD-GAN is designed such that along with finding the real and fake samples, it is also required to identify the generator that generated the given fake sample. Intuitively, to succeed in this task, the discriminator must learn to push different generators towards different identifiable modes. We perform extensive experiments on synthetic and real datasets and compare MAD-GAN with different variants of GAN. We show high quality diverse sample generations for challenging tasks such as image-to-image translation and face generation. In addition, we also show that MAD-GAN is able to disentangle different modalities when trained using highly challenging diverse-class dataset (e.g. dataset with images of forests, icebergs, and bedrooms). In the end, we show its efficacy on the unsupervised feature representation task. In Appendix, we introduce a similarity based competing objective (MAD-GAN-Sim) which encourages different generators to generate diverse samples based on a user defined similarity metric. We show its performance on the image-to-image translation, and also show its effectiveness on the unsupervised feature representation task.



### Pay Attention to Those Sets! Learning Quantification from Images
- **Arxiv ID**: http://arxiv.org/abs/1704.02923v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.02923v1)
- **Published**: 2017-04-10 16:03:31+00:00
- **Updated**: 2017-04-10 16:03:31+00:00
- **Authors**: Ionut Sorodoc, Sandro Pezzelle, Aurélie Herbelot, Mariella Dimiccoli, Raffaella Bernardi
- **Comment**: Submitted to Journal Paper, 28 pages, 12 figures, 5 tables
- **Journal**: None
- **Summary**: Major advances have recently been made in merging language and vision representations. But most tasks considered so far have confined themselves to the processing of objects and lexicalised relations amongst objects (content words). We know, however, that humans (even pre-school children) can abstract over raw data to perform certain types of higher-level reasoning, expressed in natural language by function words. A case in point is given by their ability to learn quantifiers, i.e. expressions like 'few', 'some' and 'all'. From formal semantics and cognitive linguistics, we know that quantifiers are relations over sets which, as a simplification, we can see as proportions. For instance, in 'most fish are red', most encodes the proportion of fish which are red fish. In this paper, we study how well current language and vision strategies model such relations. We show that state-of-the-art attention mechanisms coupled with a traditional linguistic formalisation of quantifiers gives best performance on the task. Additionally, we provide insights on the role of 'gist' representations in quantification. A 'logical' strategy to tackle the task would be to first obtain a numerosity estimation for the two involved sets and then compare their cardinalities. We however argue that precisely identifying the composition of the sets is not only beyond current state-of-the-art models but perhaps even detrimental to a task that is most efficiently performed by refining the approximate numerosity estimator of the system.



### Fast Learning and Prediction for Object Detection using Whitened CNN Features
- **Arxiv ID**: http://arxiv.org/abs/1704.02930v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02930v2)
- **Published**: 2017-04-10 16:15:00+00:00
- **Updated**: 2017-04-12 15:35:43+00:00
- **Authors**: Björn Barz, Erik Rodner, Christoph Käding, Joachim Denzler
- **Comment**: Technical Report about the possibilities introduced with ARTOS v2,
  originally created March 2016
- **Journal**: None
- **Summary**: We combine features extracted from pre-trained convolutional neural networks (CNNs) with the fast, linear Exemplar-LDA classifier to get the advantages of both: the high detection performance of CNNs, automatic feature engineering, fast model learning from few training samples and efficient sliding-window detection. The Adaptive Real-Time Object Detection System (ARTOS) has been refactored broadly to be used in combination with Caffe for the experimental studies reported in this work.



### Surface Normals in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1704.02956v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02956v1)
- **Published**: 2017-04-10 17:13:00+00:00
- **Updated**: 2017-04-10 17:13:00+00:00
- **Authors**: Weifeng Chen, Donglai Xiang, Jia Deng
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of single-image depth estimation for images in the wild. We collect human annotated surface normals and use them to train a neural network that directly predicts pixel-wise depth. We propose two novel loss functions for training with surface normal annotations. Experiments on NYU Depth and our own dataset demonstrate that our approach can significantly improve the quality of depth estimation in the wild.



### Using convolutional networks and satellite imagery to identify patterns in urban environments at a large scale
- **Arxiv ID**: http://arxiv.org/abs/1704.02965v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02965v2)
- **Published**: 2017-04-10 17:42:37+00:00
- **Updated**: 2017-09-13 20:09:46+00:00
- **Authors**: Adrian Albert, Jasleen Kaur, Marta Gonzalez
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: Urban planning applications (energy audits, investment, etc.) require an understanding of built infrastructure and its environment, i.e., both low-level, physical features (amount of vegetation, building area and geometry etc.), as well as higher-level concepts such as land use classes (which encode expert understanding of socio-economic end uses). This kind of data is expensive and labor-intensive to obtain, which limits its availability (particularly in developing countries). We analyze patterns in land use in urban neighborhoods using large-scale satellite imagery data (which is available worldwide from third-party providers) and state-of-the-art computer vision techniques based on deep convolutional neural networks. For supervision, given the limited availability of standard benchmarks for remote-sensing data, we obtain ground truth land use class labels carefully sampled from open-source surveys, in particular the Urban Atlas land classification dataset of $20$ land use classes across $~300$ European cities. We use this data to train and compare deep architectures which have recently shown good performance on standard computer vision tasks (image classification and segmentation), including on geospatial data. Furthermore, we show that the deep representations extracted from satellite imagery of urban environments can be used to compare neighborhoods across several cities. We make our dataset available for other machine learning researchers to use for remote-sensing applications.



### Loss Max-Pooling for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.02966v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.02966v1)
- **Published**: 2017-04-10 17:44:33+00:00
- **Updated**: 2017-04-10 17:44:33+00:00
- **Authors**: Samuel Rota Bulò, Gerhard Neuhold, Peter Kontschieder
- **Comment**: accepted at CVPR 2017
- **Journal**: None
- **Summary**: We introduce a novel loss max-pooling concept for handling imbalanced training data distributions, applicable as alternative loss layer in the context of deep neural networks for semantic image segmentation. Most real-world semantic segmentation datasets exhibit long tail distributions with few object categories comprising the majority of data and consequently biasing the classifiers towards them. Our method adaptively re-weights the contributions of each pixel based on their observed losses, targeting under-performing classification results as often encountered for under-represented object classes. Our approach goes beyond conventional cost-sensitive learning attempts through adaptive considerations that allow us to indirectly address both, inter- and intra-class imbalances. We provide a theoretical justification of our approach, complementary to experimental analyses on benchmark datasets. In our experiments on the Cityscapes and Pascal VOC 2012 segmentation datasets we find consistently improved results, demonstrating the efficacy of our approach.



### Weakly-Supervised Spatial Context Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.02998v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02998v2)
- **Published**: 2017-04-10 18:15:34+00:00
- **Updated**: 2019-01-30 03:07:41+00:00
- **Authors**: Zuxuan Wu, Larry S. Davis, Leonid Sigal
- **Comment**: None
- **Journal**: None
- **Summary**: We explore the power of spatial context as a self-supervisory signal for learning visual representations. In particular, we propose spatial context networks that learn to predict a representation of one image patch from another image patch, within the same image, conditioned on their real-valued relative spatial offset. Unlike auto-encoders, that aim to encode and reconstruct original image patches, our network aims to encode and reconstruct intermediate representations of the spatially offset patches. As such, the network learns a spatially conditioned contextual representation. By testing performance with various patch selection mechanisms we show that focusing on object-centric patches is important, and that using object proposal as a patch selection mechanism leads to the highest improvement in performance. Further, unlike auto-encoders, context encoders [21], or other forms of unsupervised feature learning, we illustrate that contextual supervision (with pre-trained model initialization) can improve on existing pre-trained model performance. We build our spatial context networks on top of standard VGG_19 and CNN_M architectures and, among other things, show that we can achieve improvements (with no additional explicit supervision) over the original ImageNet pre-trained VGG_19 and CNN_M models in object categorization and detection on VOC2007.



### Semantically Consistent Regularization for Zero-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.03039v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.03039v1)
- **Published**: 2017-04-10 19:59:33+00:00
- **Updated**: 2017-04-10 19:59:33+00:00
- **Authors**: Pedro Morgado, Nuno Vasconcelos
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: The role of semantics in zero-shot learning is considered. The effectiveness of previous approaches is analyzed according to the form of supervision provided. While some learn semantics independently, others only supervise the semantic subspace explained by training classes. Thus, the former is able to constrain the whole space but lacks the ability to model semantic correlations. The latter addresses this issue but leaves part of the semantic space unsupervised. This complementarity is exploited in a new convolutional neural network (CNN) framework, which proposes the use of semantics as constraints for recognition.Although a CNN trained for classification has no transfer ability, this can be encouraged by learning an hidden semantic layer together with a semantic code for classification. Two forms of semantic constraints are then introduced. The first is a loss-based regularizer that introduces a generalization constraint on each semantic predictor. The second is a codeword regularizer that favors semantic-to-class mappings consistent with prior semantic knowledge while allowing these to be learned from data. Significant improvements over the state-of-the-art are achieved on several datasets.



### DRAW: Deep networks for Recognizing styles of Artists Who illustrate children's books
- **Arxiv ID**: http://arxiv.org/abs/1704.03057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03057v1)
- **Published**: 2017-04-10 21:03:51+00:00
- **Updated**: 2017-04-10 21:03:51+00:00
- **Authors**: Samet Hicsonmez, Nermin Samet, Fadime Sener, Pinar Duygulu
- **Comment**: ACM ICMR 2017
- **Journal**: None
- **Summary**: This paper is motivated from a young boy's capability to recognize an illustrator's style in a totally different context. In the book "We are All Born Free" [1], composed of selected rights from the Universal Declaration of Human Rights interpreted by different illustrators, the boy was surprised to see a picture similar to the ones in the "Winnie the Witch" series drawn by Korky Paul (Figure 1). The style was noticeable in other characters of the same illustrator in different books as well. The capability of a child to easily spot the style was shown to be valid for other illustrators such as Axel Scheffler and Debi Gliori. The boy's enthusiasm let us to start the journey to explore the capabilities of machines to recognize the style of illustrators.   We collected pages from children's books to construct a new illustrations dataset consisting of about 6500 pages from 24 artists. We exploited deep networks for categorizing illustrators and with around 94% classification performance our method over-performed the traditional methods by more than 10%. Going beyond categorization we explored transferring style. The classification performance on the transferred images has shown the ability of our system to capture the style. Furthermore, we discovered representative illustrations and discriminative stylistic elements.



### CERN: Confidence-Energy Recurrent Network for Group Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.03058v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.03058v1)
- **Published**: 2017-04-10 21:08:39+00:00
- **Updated**: 2017-04-10 21:08:39+00:00
- **Authors**: Tianmin Shu, Sinisa Todorovic, Song-Chun Zhu
- **Comment**: Accepted to IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2017
- **Journal**: None
- **Summary**: This work is about recognizing human activities occurring in videos at distinct semantic levels, including individual actions, interactions, and group activities. The recognition is realized using a two-level hierarchy of Long Short-Term Memory (LSTM) networks, forming a feed-forward deep architecture, which can be trained end-to-end. In comparison with existing architectures of LSTMs, we make two key contributions giving the name to our approach as Confidence-Energy Recurrent Network -- CERN. First, instead of using the common softmax layer for prediction, we specify a novel energy layer (EL) for estimating the energy of our predictions. Second, rather than finding the common minimum-energy class assignment, which may be numerically unstable under uncertainty, we specify that the EL additionally computes the p-values of the solutions, and in this way estimates the most confident energy minimum. The evaluation on the Collective Activity and Volleyball datasets demonstrates: (i) advantages of our two contributions relative to the common softmax and energy-minimization formulations and (ii) a superior performance relative to the state-of-the-art approaches.



### Action Unit Detection with Region Adaptation, Multi-labeling Learning and Optimal Temporal Fusing
- **Arxiv ID**: http://arxiv.org/abs/1704.03067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03067v1)
- **Published**: 2017-04-10 21:58:56+00:00
- **Updated**: 2017-04-10 21:58:56+00:00
- **Authors**: Wei Li, Farnaz Abitahi, Zhigang Zhu
- **Comment**: The paper is accepted to CVPR 2017
- **Journal**: None
- **Summary**: Action Unit (AU) detection becomes essential for facial analysis. Many proposed approaches face challenging problems in dealing with the alignments of different face regions, in the effective fusion of temporal information, and in training a model for multiple AU labels. To better address these problems, we propose a deep learning framework for AU detection with region of interest (ROI) adaptation, integrated multi-label learning, and optimal LSTM-based temporal fusing. First, ROI cropping nets (ROI Nets) are designed to make sure specifically interested regions of faces are learned independently; each sub-region has a local convolutional neural network (CNN) - an ROI Net, whose convolutional filters will only be trained for the corresponding region. Second, multi-label learning is employed to integrate the outputs of those individual ROI cropping nets, which learns the inter-relationships of various AUs and acquires global features across sub-regions for AU detection. Finally, the optimal selection of multiple LSTM layers to form the best LSTM Net is carried out to best fuse temporal features, in order to make the AU prediction the most accurate. The proposed approach is evaluated on two popular AU detection datasets, BP4D and DISFA, outperforming the state of the art significantly, with an average improvement of around 13% on BP4D and 25% on DISFA, respectively.



### A semidiscrete version of the Citti-Petitot-Sarti model as a plausible model for anthropomorphic image reconstruction and pattern recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.03069v3
- **DOI**: 10.1007/978-3-319-78482-3
- **Categories**: **cs.CV**, math.AP, math.RT
- **Links**: [PDF](http://arxiv.org/pdf/1704.03069v3)
- **Published**: 2017-04-10 22:15:42+00:00
- **Updated**: 2018-01-11 14:53:55+00:00
- **Authors**: Dario Prandi, Jean-Paul Gauthier
- **Comment**: 123 pages, revised version
- **Journal**: None
- **Summary**: In his beautiful book [66], Jean Petitot proposes a sub-Riemannian model for the primary visual cortex of mammals. This model is neurophysiologically justified. Further developments of this theory lead to efficient algorithms for image reconstruction, based upon the consideration of an associated hypoelliptic diffusion. The sub-Riemannian model of Petitot and Citti-Sarti (or certain of its improvements) is a left-invariant structure over the group $SE(2)$ of rototranslations of the plane. Here, we propose a semi-discrete version of this theory, leading to a left-invariant structure over the group $SE(2,N)$, restricting to a finite number of rotations. This apparently very simple group is in fact quite atypical: it is maximally almost periodic, which leads to much simpler harmonic analysis compared to $SE(2).$ Based upon this semi-discrete model, we improve on previous image-reconstruction algorithms and we develop a pattern-recognition theory that leads also to very efficient algorithms in practice.



### WRPN: Training and Inference using Wide Reduced-Precision Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.03079v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1704.03079v1)
- **Published**: 2017-04-10 22:54:38+00:00
- **Updated**: 2017-04-10 22:54:38+00:00
- **Authors**: Asit Mishra, Jeffrey J Cook, Eriko Nurvitadhi, Debbie Marr
- **Comment**: Under submission to CVPR Workshop
- **Journal**: None
- **Summary**: For computer vision applications, prior works have shown the efficacy of reducing the numeric precision of model parameters (network weights) in deep neural networks but also that reducing the precision of activations hurts model accuracy much more than reducing the precision of model parameters. We study schemes to train networks from scratch using reduced-precision activations without hurting the model accuracy. We reduce the precision of activation maps (along with model parameters) using a novel quantization scheme and increase the number of filter maps in a layer, and find that this scheme compensates or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly reduce the dynamic memory footprint, memory bandwidth, computational energy and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN - wide reduced-precision networks. We report results using our proposed schemes and show that our results are better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.



