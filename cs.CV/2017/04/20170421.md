# Arxiv Papers in cs.CV on 2017-04-21
### NormFace: L2 Hypersphere Embedding for Face Verification
- **Arxiv ID**: http://arxiv.org/abs/1704.06369v4
- **DOI**: 10.1145/3123266.3123359
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06369v4)
- **Published**: 2017-04-21 00:07:03+00:00
- **Updated**: 2017-07-26 17:58:43+00:00
- **Authors**: Feng Wang, Xiang Xiang, Jian Cheng, Alan L. Yuille
- **Comment**: camera-ready version
- **Journal**: None
- **Summary**: Thanks to the recent developments of Convolutional Neural Networks, the performance of face verification methods has increased rapidly. In a typical face verification method, feature normalization is a critical step for boosting performance. This motivates us to introduce and study the effect of normalization during training. But we find this is non-trivial, despite normalization being differentiable. We identify and study four issues related to normalization through mathematical analysis, which yields understanding and helps with parameter settings. Based on this analysis we propose two strategies for training using normalized features. The first is a modification of softmax loss, which optimizes cosine similarity instead of inner-product. The second is a reformulation of metric learning by introducing an agent vector for each class. We show that both strategies, and small variants, consistently improve performance by between 0.2% to 0.4% on the LFW dataset based on two models. This is significant because the performance of the two models on LFW dataset is close to saturation at over 98%. Codes and models are released on https://github.com/happynear/NormFace



### Robust Multi-view Pedestrian Tracking Using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.06370v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06370v1)
- **Published**: 2017-04-21 00:12:23+00:00
- **Updated**: 2017-04-21 00:12:23+00:00
- **Authors**: Md Zahangir Alom, Tarek M. Taha
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: In this paper, we present a real-time robust multi-view pedestrian detection and tracking system for video surveillance using neural networks which can be used in dynamic environments. The proposed system consists of two phases: multi-view pedestrian detection and tracking. First, pedestrian detection utilizes background subtraction to segment the foreground blob. An adaptive background subtraction method where each of the pixel of input image models as a mixture of Gaussians and uses an on-line approximation to update the model applies to extract the foreground region. The Gaussian distributions are then evaluated to determine which are most likely to result from a background process. This method produces a steady, real-time tracker in outdoor environment that consistently deals with changes of lighting condition, and long-term scene change. Second, the Tracking is performed at two phases: pedestrian classification and tracking the individual subject. A sliding window is applied on foreground binary image to select an input window which is used for selecting the input image patches from actually input frame. The neural networks is used for classification with PHOG features. Finally, a Kalman filter is applied to calculate the subsequent step for tracking that aims at finding the exact position of pedestrians in an input image. The experimental result shows that the proposed approach yields promising performance on multi-view pedestrian detection and tracking on different benchmark datasets.



### A data set for evaluating the performance of multi-class multi-object video tracking
- **Arxiv ID**: http://arxiv.org/abs/1704.06378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06378v1)
- **Published**: 2017-04-21 02:14:13+00:00
- **Updated**: 2017-04-21 02:14:13+00:00
- **Authors**: Avishek Chakraborty, Victor Stamatescu, Sebastien C. Wong, Grant Wigley, David Kearney
- **Comment**: Originally presented at SPIE Defense + Security conference on
  Automatic Target Recognition XXVII (2017)
- **Journal**: None
- **Summary**: One of the challenges in evaluating multi-object video detection, tracking and classification systems is having publically available data sets with which to compare different systems. However, the measures of performance for tracking and classification are different. Data sets that are suitable for evaluating tracking systems may not be appropriate for classification. Tracking video data sets typically only have ground truth track IDs, while classification video data sets only have ground truth class-label IDs. The former identifies the same object over multiple frames, while the latter identifies the type of object in individual frames. This paper describes an advancement of the ground truth meta-data for the DARPA Neovision2 Tower data set to allow both the evaluation of tracking and classification. The ground truth data sets presented in this paper contain unique object IDs across 5 different classes of object (Car, Bus, Truck, Person, Cyclist) for 24 videos of 871 image frames each. In addition to the object IDs and class labels, the ground truth data also contains the original bounding box coordinates together with new bounding boxes in instances where un-annotated objects were present. The unique IDs are maintained during occlusions between multiple objects or when objects re-enter the field of view. This will provide: a solid foundation for evaluating the performance of multi-object tracking of different types of objects, a straightforward comparison of tracking system performance using the standard Multi Object Tracking (MOT) framework, and classification performance using the Neovision2 metrics. These data have been hosted publically.



### Hierarchical 3D fully convolutional networks for multi-organ segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.06382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06382v1)
- **Published**: 2017-04-21 03:05:15+00:00
- **Updated**: 2017-04-21 03:05:15+00:00
- **Authors**: Holger R. Roth, Hirohisa Oda, Yuichiro Hayashi, Masahiro Oda, Natsuki Shimizu, Michitaka Fujiwara, Kazunari Misawa, Kensaku Mori
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in 3D fully convolutional networks (FCN) have made it feasible to produce dense voxel-wise predictions of full volumetric images. In this work, we show that a multi-class 3D FCN trained on manually labeled CT scans of seven abdominal structures (artery, vein, liver, spleen, stomach, gallbladder, and pancreas) can achieve competitive segmentation results, while avoiding the need for handcrafting features or training organ-specific models. To this end, we propose a two-stage, coarse-to-fine approach that trains an FCN model to roughly delineate the organs of interest in the first stage (seeing $\sim$40% of the voxels within a simple, automatically generated binary mask of the patient's body). We then use these predictions of the first-stage FCN to define a candidate region that will be used to train a second FCN. This step reduces the number of voxels the FCN has to classify to $\sim$10% while maintaining a recall high of $>$99%. This second-stage FCN can now focus on more detailed segmentation of the organs. We respectively utilize training and validation sets consisting of 281 and 50 clinical CT images. Our hierarchical approach provides an improved Dice score of 7.5 percentage points per organ on average in our validation set. We furthermore test our models on a completely unseen data collection acquired at a different hospital that includes 150 CT scans with three anatomical labels (liver, spleen, and pancreas). In such challenging organs as the pancreas, our hierarchical approach improves the mean Dice score from 68.5 to 82.2%, achieving the highest reported average score on this dataset.



### Multiple Reflection Symmetry Detection via Linear-Directional Kernel Density Estimation
- **Arxiv ID**: http://arxiv.org/abs/1704.06392v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06392v1)
- **Published**: 2017-04-21 04:15:15+00:00
- **Updated**: 2017-04-21 04:15:15+00:00
- **Authors**: Mohamed Elawady, Olivier Alata, Christophe Ducottet, Cecile Barat, Philippe Colantoni
- **Comment**: Submitted to CAIP 2017
- **Journal**: None
- **Summary**: Symmetry is an important composition feature by investigating similar sides inside an image plane. It has a crucial effect to recognize man-made or nature objects within the universe. Recent symmetry detection approaches used a smoothing kernel over different voting maps in the polar coordinate system to detect symmetry peaks, which split the regions of symmetry axis candidates in inefficient way. We propose a reliable voting representation based on weighted linear-directional kernel density estimation, to detect multiple symmetries over challenging real-world and synthetic images. Experimental evaluation on two public datasets demonstrates the superior performance of the proposed algorithm to detect global symmetry axes respect to the major image shapes.



### Solar Power Plant Detection on Multi-Spectral Satellite Imagery using Weakly-Supervised CNN with Feedback Features and m-PCNN Fusion
- **Arxiv ID**: http://arxiv.org/abs/1704.06410v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06410v2)
- **Published**: 2017-04-21 06:23:44+00:00
- **Updated**: 2017-06-21 06:49:45+00:00
- **Authors**: Nevrez Imamoglu, Motoki Kimura, Hiroki Miyamoto, Aito Fujita, Ryosuke Nakamura
- **Comment**: 9 pages, 9 figures, 4 tables
- **Journal**: British Machine Vision Conference (BMVC) 2017
- **Summary**: Most of the traditional convolutional neural networks (CNNs) implements bottom-up approach (feed-forward) for image classifications. However, many scientific studies demonstrate that visual perception in primates rely on both bottom-up and top-down connections. Therefore, in this work, we propose a CNN network with feedback structure for Solar power plant detection on middle-resolution satellite images. To express the strength of the top-down connections, we introduce feedback CNN network (FB-Net) to a baseline CNN model used for solar power plant classification on multi-spectral satellite data. Moreover, we introduce a method to improve class activation mapping (CAM) to our FB-Net, which takes advantage of multi-channel pulse coupled neural network (m-PCNN) for weakly-supervised localization of the solar power plants from the features of proposed FB-Net. For the proposed FB-Net CAM with m-PCNN, experimental results demonstrated promising results on both solar-power plant image classification and detection task.



### Track Everything: Limiting Prior Knowledge in Online Multi-Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.06415v1
- **DOI**: 10.1109/TIP.2017.2696744
- **Categories**: **cs.CV**, cs.NE, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1704.06415v1)
- **Published**: 2017-04-21 06:49:51+00:00
- **Updated**: 2017-04-21 06:49:51+00:00
- **Authors**: Sebastien C. Wong, Victor Stamatescu, Adam Gatt, David Kearney, Ivan Lee, Mark D. McDonnell
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: This paper addresses the problem of online tracking and classification of multiple objects in an image sequence. Our proposed solution is to first track all objects in the scene without relying on object-specific prior knowledge, which in other systems can take the form of hand-crafted features or user-based track initialization. We then classify the tracked objects with a fast-learning image classifier that is based on a shallow convolutional neural network architecture and demonstrate that object recognition improves when this is combined with object state information from the tracking algorithm. We argue that by transferring the use of prior knowledge from the detection and tracking stages to the classification stage we can design a robust, general purpose object recognition system with the ability to detect and track a variety of object types. We describe our biologically inspired implementation, which adaptively learns the shape and motion of tracked objects, and apply it to the Neovision2 Tower benchmark data set, which contains multiple object types. An experimental evaluation demonstrates that our approach is competitive with state-of-the-art video object recognition systems that do make use of object-specific prior knowledge in detection and tracking, while providing additional practical advantages by virtue of its generality.



### Robust and Fast Decoding of High-Capacity Color QR Codes for Mobile Applications
- **Arxiv ID**: http://arxiv.org/abs/1704.06447v3
- **DOI**: 10.1109/TIP.2018.2855419
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06447v3)
- **Published**: 2017-04-21 09:01:43+00:00
- **Updated**: 2018-05-19 21:00:18+00:00
- **Authors**: Zhibo Yang, Huanle Xu, Jianyuan Deng, Chen Change Loy, Wing Cheong Lau
- **Comment**: 15 pages, 10 figures, submitted to IEEE Transaction on Image
  Processing
- **Journal**: None
- **Summary**: The use of color in QR codes brings extra data capacity, but also inflicts tremendous challenges on the decoding process due to chromatic distortion, cross-channel color interference and illumination variation. Particularly, we further discover a new type of chromatic distortion in high-density color QR codes, cross-module color interference, caused by the high density which also makes the geometric distortion correction more challenging. To address these problems, we propose two approaches, namely, LSVM-CMI and QDA-CMI, which jointly model these different types of chromatic distortion. Extended from SVM and QDA, respectively, both LSVM-CMI and QDA-CMI optimize over a particular objective function to learn a color classifier. Furthermore, a robust geometric transformation method and several pipeline refinements are proposed to boost the decoding performance for mobile applications. We put forth and implement a framework for high-capacity color QR codes equipped with our methods, called HiQ. To evaluate the performance of HiQ, we collect a challenging large-scale color QR code dataset, CUHK-CQRC, which consists of 5390 high-density color QR code samples. The comparison with the baseline method [2] on CUHK-CQRC shows that HiQ at least outperforms [2] by 188% in decoding success rate and 60% in bit error rate. Our implementation of HiQ in iOS and Android also demonstrates the effectiveness of our framework in real-world applications.



### A Domain Based Approach to Social Relation Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.06456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06456v1)
- **Published**: 2017-04-21 09:27:32+00:00
- **Updated**: 2017-04-21 09:27:32+00:00
- **Authors**: Qianru Sun, Bernt Schiele, Mario Fritz
- **Comment**: To appear in CVPR 2017
- **Journal**: None
- **Summary**: Social relations are the foundation of human daily life. Developing techniques to analyze such relations from visual data bears great potential to build machines that better understand us and are capable of interacting with us at a social level. Previous investigations have remained partial due to the overwhelming diversity and complexity of the topic and consequently have only focused on a handful of social relations. In this paper, we argue that the domain-based theory from social psychology is a great starting point to systematically approach this problem. The theory provides coverage of all aspects of social relations and equally is concrete and predictive about the visual attributes and behaviors defining the relations included in each domain. We provide the first dataset built on this holistic conceptualization of social life that is composed of a hierarchical label space of social domains and social relations. We also contribute the first models to recognize such domains and relations and find superior performance for attribute based features. Beyond the encouraging performance of the attribute based approach, we also find interpretable features that are in accordance with the predictions from social psychology literature. Beyond our findings, we believe that our contributions more tightly interleave visual recognition and social psychology theory that has the potential to complement the theoretical work in the area with empirical and data-driven models of social life.



### Attend to You: Personalized Image Captioning with Context Sequence Memory Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.06485v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1704.06485v2)
- **Published**: 2017-04-21 11:29:07+00:00
- **Updated**: 2017-04-25 23:30:43+00:00
- **Authors**: Cesc Chunseong Park, Byeongchang Kim, Gunhee Kim
- **Comment**: Accepted paper at CVPR 2017
- **Journal**: None
- **Summary**: We address personalization issues of image captioning, which have not been discussed yet in previous research. For a query image, we aim to generate a descriptive sentence, accounting for prior knowledge such as the user's active vocabularies in previous documents. As applications of personalized image captioning, we tackle two post automation tasks: hashtag prediction and post generation, on our newly collected Instagram dataset, consisting of 1.1M posts from 6.3K users. We propose a novel captioning model named Context Sequence Memory Network (CSMN). Its unique updates over previous memory network models include (i) exploiting memory as a repository for multiple types of context information, (ii) appending previously generated words into memory to capture long-term information without suffering from the vanishing gradient problem, and (iii) adopting CNN memory structure to jointly represent nearby ordered memory slots for better context understanding. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the effectiveness of the three novel features of CSMN and its performance enhancement for personalized image captioning over state-of-the-art captioning models.



### A 3D fully convolutional neural network and a random walker to segment the esophagus in CT
- **Arxiv ID**: http://arxiv.org/abs/1704.06544v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06544v1)
- **Published**: 2017-04-21 13:54:00+00:00
- **Updated**: 2017-04-21 13:54:00+00:00
- **Authors**: Tobias Fechter, Sonja Adebahr, Dimos Baltas, Ismail Ben Ayed, Christian Desrosiers, Jose Dolz
- **Comment**: None
- **Journal**: None
- **Summary**: Precise delineation of organs at risk (OAR) is a crucial task in radiotherapy treatment planning, which aims at delivering high dose to the tumour while sparing healthy tissues. In recent years algorithms showed high performance and the possibility to automate this task for many OAR. However, for some OAR precise delineation remains challenging. The esophagus with a versatile shape and poor contrast is among these structures. To tackle these issues we propose a 3D fully (convolutional neural network (CNN) driven random walk (RW) approach to automatically segment the esophagus on CT. First, a soft probability map is generated by the CNN. Then an active contour model (ACM) is fitted on the probability map to get a first estimation of the center line. The outputs of the CNN and ACM are then used in addition to CT Hounsfield values to drive the RW. Evaluation and training was done on 50 CTs with peer reviewed esophagus contours. Results were assessed regarding spatial overlap and shape similarities.   The generated contours showed a mean Dice coefficient of 0.76, an average symmetric square distance of 1.36 mm and an average Hausdorff distance of 11.68 compared to the reference. These figures translate into a very good agreement with the reference contours and an increase in accuracy compared to other methods.   We show that by employing a CNN accurate estimations of esophagus location can be obtained and refined by a post processing RW step. One of the main advantages compared to previous methods is that our network performs convolutions in a 3D manner, fully exploiting the 3D spatial context and performing an efficient and precise volume-wise prediction. The whole segmentation process is fully automatic and yields esophagus delineations in very good agreement with the used gold standard, showing that it can compete with previously published methods.



### PQTable: Non-exhaustive Fast Search for Product-quantized Codes using Hash Tables
- **Arxiv ID**: http://arxiv.org/abs/1704.06556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06556v1)
- **Published**: 2017-04-21 14:22:16+00:00
- **Updated**: 2017-04-21 14:22:16+00:00
- **Authors**: Yusuke Matsui, Toshihiko Yamasaki, Kiyoharu Aizawa
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a product quantization table (PQTable); a fast search method for product-quantized codes via hash-tables. An identifier of each database vector is associated with the slot of a hash table by using its PQ-code as a key. For querying, an input vector is PQ-encoded and hashed, and the items associated with that code are then retrieved. The proposed PQTable produces the same results as a linear PQ scan, and is 10^2 to 10^5 times faster. Although state-of-the-art performance can be achieved by previous inverted-indexing-based approaches, such methods require manually-designed parameter setting and significant training; our PQTable is free of these limitations, and therefore offers a practical and effective solution for real-world problems. Specifically, when the vectors are highly compressed, our PQTable achieves one of the fastest search performances on a single CPU to date with significantly efficient memory usage (0.059 ms per query over 10^9 data points with just 5.5 GB memory consumption). Finally, we show that our proposed PQTable can naturally handle the codes of an optimized product quantization (OPQTable).



### Panorama to panorama matching for location recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.06591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06591v1)
- **Published**: 2017-04-21 15:23:29+00:00
- **Updated**: 2017-04-21 15:23:29+00:00
- **Authors**: Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon, Ondrej Chum
- **Comment**: None
- **Journal**: None
- **Summary**: Location recognition is commonly treated as visual instance retrieval on "street view" imagery. The dataset items and queries are panoramic views, i.e. groups of images taken at a single location. This work introduces a novel panorama-to-panorama matching process, either by aggregating features of individual images in a group or by explicitly constructing a larger panorama. In either case, multiple views are used as queries. We reach near perfect location recognition on a standard benchmark with only four query views.



### Context-based Object Viewpoint Estimation: A 2D Relational Approach
- **Arxiv ID**: http://arxiv.org/abs/1704.06610v1
- **DOI**: 10.1016/j.cviu.2017.04.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06610v1)
- **Published**: 2017-04-21 15:55:54+00:00
- **Updated**: 2017-04-21 15:55:54+00:00
- **Authors**: Jose Oramas, Luc De Raedt, Tinne Tuytelaars
- **Comment**: Computer Vision and Image Understanding (CVIU)
- **Journal**: None
- **Summary**: The task of object viewpoint estimation has been a challenge since the early days of computer vision. To estimate the viewpoint (or pose) of an object, people have mostly looked at object intrinsic features, such as shape or appearance. Surprisingly, informative features provided by other, extrinsic elements in the scene, have so far mostly been ignored. At the same time, contextual cues have been proven to be of great benefit for related tasks such as object detection or action recognition. In this paper, we explore how information from other objects in the scene can be exploited for viewpoint estimation. In particular, we look at object configurations by following a relational neighbor-based approach for reasoning about object relations. We show that, starting from noisy object detections and viewpoint estimates, exploiting the estimated viewpoint and location of other objects in the scene can lead to improved object viewpoint predictions. Experiments on the KITTI dataset demonstrate that object configurations can indeed be used as a complementary cue to appearance-based viewpoint estimation. Our analysis reveals that the proposed context-based method can improve object viewpoint estimation by reducing specific types of viewpoint estimation errors commonly made by methods that only consider local information. Moreover, considering contextual information produces superior performance in scenes where a high number of object instances occur. Finally, our results suggest that, following a cautious relational neighbor formulation brings improvements over its aggressive counterpart for the task of object viewpoint estimation.



### Scatteract: Automated extraction of data from scatter plots
- **Arxiv ID**: http://arxiv.org/abs/1704.06687v1
- **DOI**: 10.1007/978-3-319-71249-9_9
- **Categories**: **cs.CV**, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.06687v1)
- **Published**: 2017-04-21 19:25:32+00:00
- **Updated**: 2017-04-21 19:25:32+00:00
- **Authors**: Mathieu Cliche, David Rosenberg, Dhruv Madeka, Connie Yee
- **Comment**: Submitted to ECML PKDD 2017 proceedings, 16 pages
- **Journal**: Machine Learning and Knowledge Discovery in Databases. ECML PKDD
  2017. Lecture Notes in Computer Science, vol 10534. Springer, Cham
- **Summary**: Charts are an excellent way to convey patterns and trends in data, but they do not facilitate further modeling of the data or close inspection of individual data points. We present a fully automated system for extracting the numerical values of data points from images of scatter plots. We use deep learning techniques to identify the key components of the chart, and optical character recognition together with robust regression to map from pixels to the coordinate system of the chart. We focus on scatter plots with linear scales, which already have several interesting challenges. Previous work has done fully automatic extraction for other types of charts, but to our knowledge this is the first approach that is fully automatic for scatter plots. Our method performs well, achieving successful data extraction on 89% of the plots in our test set.



### SREFI: Synthesis of Realistic Example Face Images
- **Arxiv ID**: http://arxiv.org/abs/1704.06693v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06693v2)
- **Published**: 2017-04-21 19:59:47+00:00
- **Updated**: 2017-04-25 03:54:34+00:00
- **Authors**: Sandipan Banerjee, John S. Bernhard, Walter J. Scheirer, Kevin W. Bowyer, Patrick J. Flynn
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel face synthesis approach that can generate an arbitrarily large number of synthetic images of both real and synthetic identities. Thus a face image dataset can be expanded in terms of the number of identities represented and the number of images per identity using this approach, without the identity-labeling and privacy complications that come from downloading images from the web. To measure the visual fidelity and uniqueness of the synthetic face images and identities, we conducted face matching experiments with both human participants and a CNN pre-trained on a dataset of 2.6M real face images. To evaluate the stability of these synthetic faces, we trained a CNN model with an augmented dataset containing close to 200,000 synthetic faces. We used a snapshot of this trained CNN to recognize extremely challenging frontal (real) face images. Experiments showed training with the augmented faces boosted the face recognition performance of the CNN.



