# Arxiv Papers in cs.CV on 2017-04-14
### CBinfer: Change-Based Inference for Convolutional Neural Networks on Video Data
- **Arxiv ID**: http://arxiv.org/abs/1704.04313v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.PF, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1704.04313v2)
- **Published**: 2017-04-14 00:36:55+00:00
- **Updated**: 2017-06-21 09:27:14+00:00
- **Authors**: Lukas Cavigelli, Philippe Degen, Luca Benini
- **Comment**: None
- **Journal**: None
- **Summary**: Extracting per-frame features using convolutional neural networks for real-time processing of video data is currently mainly performed on powerful GPU-accelerated workstations and compute clusters. However, there are many applications such as smart surveillance cameras that require or would benefit from on-site processing. To this end, we propose and evaluate a novel algorithm for change-based evaluation of CNNs for video data recorded with a static camera setting, exploiting the spatio-temporal sparsity of pixel changes. We achieve an average speed-up of 8.6x over a cuDNN baseline on a realistic benchmark with a negligible accuracy loss of less than 0.1% and no retraining of the network. The resulting energy efficiency is 10x higher than that of per-frame evaluation and reaches an equivalent of 328 GOp/s/W on the Tegra X1 platform.



### Dataset Augmentation for Pose and Lighting Invariant Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.04326v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04326v1)
- **Published**: 2017-04-14 01:56:35+00:00
- **Updated**: 2017-04-14 01:56:35+00:00
- **Authors**: Daniel Crispell, Octavian Biris, Nate Crosswhite, Jeffrey Byrne, Joseph L. Mundy
- **Comment**: Appeared in 2016 IEEE Applied Imagery Pattern Recognition Workshop
  (AIPR)
- **Journal**: None
- **Summary**: The performance of modern face recognition systems is a function of the dataset on which they are trained. Most datasets are largely biased toward "near-frontal" views with benign lighting conditions, negatively effecting recognition performance on images that do not meet these criteria. The proposed approach demonstrates how a baseline training set can be augmented to increase pose and lighting variability using semi-synthetic images with simulated pose and lighting conditions. The semi-synthetic images are generated using a fast and robust 3-d shape estimation and rendering pipeline which includes the full head and background. Various methods of incorporating the semi-synthetic renderings into the training procedure of a state of the art deep neural network-based recognition system without modifying the structure of the network itself are investigated. Quantitative results are presented on the challenging IJB-A identification dataset using a state of the art recognition pipeline as a baseline.



### Camera Calibration by Global Constraints on the Motion of Silhouettes
- **Arxiv ID**: http://arxiv.org/abs/1704.04360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04360v1)
- **Published**: 2017-04-14 06:09:27+00:00
- **Updated**: 2017-04-14 06:09:27+00:00
- **Authors**: Gil Ben-Artzi
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of epipolar geometry using the motion of silhouettes. Such methods match epipolar lines or frontier points across views, which are then used as the set of putative correspondences. We introduce an approach that improves by two orders of magnitude the performance over state-of-the-art methods, by significantly reducing the number of outliers in the putative matching. We model the frontier points' correspondence problem as constrained flow optimization, requiring small differences between their coordinates over consecutive frames. Our approach is formulated as a Linear Integer Program and we show that due to the nature of our problem, it can be solved efficiently in an iterative manner. Our method was validated on four standard datasets providing accurate calibrations across very different viewpoints.



### DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents
- **Arxiv ID**: http://arxiv.org/abs/1704.04394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04394v1)
- **Published**: 2017-04-14 11:15:44+00:00
- **Updated**: 2017-04-14 11:15:44+00:00
- **Authors**: Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B. Choy, Philip H. S. Torr, Manmohan Chandraker
- **Comment**: Accepted at CVPR 2017
- **Journal**: None
- **Summary**: We introduce a Deep Stochastic IOC RNN Encoderdecoder framework, DESIRE, for the task of future predictions of multiple interacting agents in dynamic scenes. DESIRE effectively predicts future locations of objects in multiple scenes by 1) accounting for the multi-modal nature of the future prediction (i.e., given the same context, future may vary), 2) foreseeing the potential future outcomes and make a strategic prediction based on that, and 3) reasoning not only from the past motion history, but also from the scene context as well as the interactions among the agents. DESIRE achieves these in a single end-to-end trainable neural network model, while being computationally efficient. The model first obtains a diverse set of hypothetical future prediction samples employing a conditional variational autoencoder, which are ranked and refined by the following RNN scoring-regression module. Samples are scored by accounting for accumulated future rewards, which enables better long-term strategic decisions similar to IOC frameworks. An RNN scene context fusion module jointly captures past motion histories, the semantic scene context and interactions among multiple agents. A feedback mechanism iterates over the ranking and refinement to further boost the prediction accuracy. We evaluate our model on two publicly available datasets: KITTI and Stanford Drone Dataset. Our experiments show that the proposed model significantly improves the prediction accuracy compared to other baseline methods.



### Improved Human Emotion Recognition Using Symmetry of Facial Key Points with Dihedral Group
- **Arxiv ID**: http://arxiv.org/abs/1706.07757v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1706.07757v1)
- **Published**: 2017-04-14 14:54:14+00:00
- **Updated**: 2017-04-14 14:54:14+00:00
- **Authors**: Mehdi Ghayoumi, Arvind Bansal
- **Comment**: 7
- **Journal**: IJASCSE Volume 6 Issue 01 2017
- **Summary**: This article describes how to deploy dihedral group theory to detect Facial Key Points (FKP) symmetry to recognize emotions. The method can be applied in many other areas which those have the same data texture.



### Deep Structured Learning for Facial Action Unit Intensity Estimation
- **Arxiv ID**: http://arxiv.org/abs/1704.04481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04481v1)
- **Published**: 2017-04-14 16:51:40+00:00
- **Updated**: 2017-04-14 16:51:40+00:00
- **Authors**: Robert Walecki, Ognjen, Rudovic, Vladimir Pavlovic, Bj√∂rn Schuller, Maja Pantic
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the task of automated estimation of facial expression intensity. This involves estimation of multiple output variables (facial action units --- AUs) that are structurally dependent. Their structure arises from statistically induced co-occurrence patterns of AU intensity levels. Modeling this structure is critical for improving the estimation performance; however, this performance is bounded by the quality of the input features extracted from face images. The goal of this paper is to model these structures and estimate complex feature representations simultaneously by combining conditional random field (CRF) encoded AU dependencies with deep learning. To this end, we propose a novel Copula CNN deep learning approach for modeling multivariate ordinal variables. Our model accounts for $ordinal$ structure in output variables and their $non$-$linear$ dependencies via copula functions modeled as cliques of a CRF. These are jointly optimized with deep CNN feature encoding layers using a newly introduced balanced batch iterative training algorithm. We demonstrate the effectiveness of our approach on the task of AU intensity estimation on two benchmark datasets. We show that joint learning of the deep features and the target output structure results in significant performance gains compared to existing deep structured models for analysis of facial expressions.



### TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1704.04497v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04497v3)
- **Published**: 2017-04-14 17:57:01+00:00
- **Updated**: 2017-12-03 04:46:42+00:00
- **Authors**: Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim
- **Comment**: Accepted paper at CVPR 2017 (Spotlight)
- **Journal**: None
- **Summary**: Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention, and show its effectiveness over conventional VQA techniques through empirical evaluations.



### Soft-NMS -- Improving Object Detection With One Line of Code
- **Arxiv ID**: http://arxiv.org/abs/1704.04503v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04503v2)
- **Published**: 2017-04-14 18:00:03+00:00
- **Updated**: 2017-08-08 17:49:18+00:00
- **Authors**: Navaneeth Bodla, Bharat Singh, Rama Chellappa, Larry S. Davis
- **Comment**: ICCV 2017 camera ready version
- **Journal**: None
- **Summary**: Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predefined overlap threshold, it leads to a miss. To this end, we propose Soft-NMS, an algorithm which decays the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process. Soft-NMS obtains consistent improvements for the coco-style mAP metric on standard datasets like PASCAL VOC 2007 (1.7% for both R-FCN and Faster-RCNN) and MS-COCO (1.3% for R-FCN and 1.1% for Faster-RCNN) by just changing the NMS algorithm without any additional hyper-parameters. Using Deformable-RFCN, Soft-NMS improves state-of-the-art in object detection from 39.8% to 40.9% with a single model. Further, the computational complexity of Soft-NMS is the same as traditional NMS and hence it can be efficiently implemented. Since Soft-NMS does not require any extra training and is simple to implement, it can be easily integrated into any object detection pipeline. Code for Soft-NMS is publicly available on GitHub (http://bit.ly/2nJLNMu).



### Recovery of damped exponentials using structured low rank matrix completion
- **Arxiv ID**: http://arxiv.org/abs/1704.04511v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04511v2)
- **Published**: 2017-04-14 18:35:25+00:00
- **Updated**: 2017-07-12 14:46:30+00:00
- **Authors**: Arvind Balachandrasekaran, Vincent Magnotta, Mathews Jacob
- **Comment**: Accepted for publication in IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: We introduce a structured low rank matrix completion algorithm to recover a series of images from their under-sampled measurements, where the signal along the parameter dimension at every pixel is described by a linear combination of exponentials. We exploit the exponential behavior of the signal at every pixel, along with the spatial smoothness of the exponential parameters to derive an annihilation relation in the Fourier domain. This relation translates to a low-rank property on a structured matrix constructed from the Fourier samples. We enforce the low rank property of the structured matrix as a regularization prior to recover the images. Since the direct use of current low rank matrix recovery schemes to this problem is associated with high computational complexity and memory demand, we adopt an iterative re-weighted least squares (IRLS) algorithm, which facilitates the exploitation of the convolutional structure of the matrix. Novel approximations involving two dimensional Fast Fourier Transforms (FFT) are introduced to drastically reduce the memory demand and computational complexity, which facilitates the extension of structured low rank methods to large scale three dimensional problems. We demonstrate our algorithm in the MR parameter mapping setting and show improvement over the state-of-the-art methods.



### Interpretable 3D Human Action Analysis with Temporal Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.04516v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, 68T10 (Primary), I.2.10; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1704.04516v1)
- **Published**: 2017-04-14 19:00:36+00:00
- **Updated**: 2017-04-14 19:00:36+00:00
- **Authors**: Tae Soo Kim, Austin Reiter
- **Comment**: 8 pages, 5 figures, BNMW CVPR 2017 Submission
- **Journal**: None
- **Summary**: The discriminative power of modern deep learning models for 3D human action recognition is growing ever so potent. In conjunction with the recent resurgence of 3D human action representation with 3D skeletons, the quality and the pace of recent progress have been significant. However, the inner workings of state-of-the-art learning based methods in 3D human action recognition still remain mostly black-box. In this work, we propose to use a new class of models known as Temporal Convolutional Neural Networks (TCN) for 3D human action recognition. Compared to popular LSTM-based Recurrent Neural Network models, given interpretable input such as 3D skeletons, TCN provides us a way to explicitly learn readily interpretable spatio-temporal representations for 3D human action recognition. We provide our strategy in re-designing the TCN with interpretability in mind and how such characteristics of the model is leveraged to construct a powerful 3D activity recognition method. Through this work, we wish to take a step towards a spatio-temporal model that is easier to understand, explain and interpret. The resulting model, Res-TCN, achieves state-of-the-art results on the largest 3D human action recognition dataset, NTU-RGBD.



### ShapeWorld - A new test methodology for multimodal language understanding
- **Arxiv ID**: http://arxiv.org/abs/1704.04517v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.04517v1)
- **Published**: 2017-04-14 19:01:51+00:00
- **Updated**: 2017-04-14 19:01:51+00:00
- **Authors**: Alexander Kuhnle, Ann Copestake
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel framework for evaluating multimodal deep learning models with respect to their language understanding and generalization abilities. In this approach, artificial data is automatically generated according to the experimenter's specifications. The content of the data, both during training and evaluation, can be controlled in detail, which enables tasks to be created that require true generalization abilities, in particular the combination of previously introduced concepts in novel ways. We demonstrate the potential of our methodology by evaluating various visual question answering models on four different tasks, and show how our framework gives us detailed insights into their capabilities and limitations. By open-sourcing our framework, we hope to stimulate progress in the field of multimodal language understanding.



