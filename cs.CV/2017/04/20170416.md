# Arxiv Papers in cs.CV on 2017-04-16
### Trigger for the SoLid Reactor Antineutrino Experiment
- **Arxiv ID**: http://arxiv.org/abs/1704.04706v2
- **DOI**: None
- **Categories**: **physics.ins-det**, cs.CV, hep-ex
- **Links**: [PDF](http://arxiv.org/pdf/1704.04706v2)
- **Published**: 2017-04-16 01:27:22+00:00
- **Updated**: 2019-05-01 19:12:56+00:00
- **Authors**: Lukas On Arnold
- **Comment**: Poster presented at NuPhys2016 (London, 12-14 December 2016). 8
  pages, LaTeX, 6 png figures, 1 pdf figure
- **Journal**: None
- **Summary**: SoLid, located at SCK-CEN in Mol, Belgium, is a reactor antineutrino experiment at a very short baseline of 5.5 -- 10m aiming at the search for sterile neutrinos and for high precision measurement of the neutrino energy spectrum of Uranium-235. It uses a novel approach using Lithium-6 sheets and PVT cubes as scintillators for tagging the Inverse Beta-Decay products (neutron and positron). Being located overground and close to the BR2 research reactor, the experiment faces a large amount of backgrounds. Efficient real-time background and noise rejection is essential in order to increase the signal-background ratio for precise oscillation measurement and decrease data production to a rate which can be handled by the online software. Therefore, a reliable distinction between the neutrons and background signals is crucial. This can be performed online with a dedicated firmware trigger. A peak counting algorithm and an algorithm measuring time over threshold have been identified as performing well both in terms of efficiency and fake rate, and have been implemented onto FPGA.



### Google's Cloud Vision API Is Not Robust To Noise
- **Arxiv ID**: http://arxiv.org/abs/1704.05051v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.05051v2)
- **Published**: 2017-04-16 09:47:46+00:00
- **Updated**: 2017-07-20 05:31:16+00:00
- **Authors**: Hossein Hosseini, Baicen Xiao, Radha Poovendran
- **Comment**: None
- **Journal**: None
- **Summary**: Google has recently introduced the Cloud Vision API for image analysis. According to the demonstration website, the API "quickly classifies images into thousands of categories, detects individual objects and faces within images, and finds and reads printed words contained within images." It can be also used to "detect different types of inappropriate content from adult to violent content."   In this paper, we evaluate the robustness of Google Cloud Vision API to input perturbation. In particular, we show that by adding sufficient noise to the image, the API generates completely different outputs for the noisy image, while a human observer would perceive its original content. We show that the attack is consistently successful, by performing extensive experiments on different image types, including natural images, images containing faces and images with texts. For instance, using images from ImageNet dataset, we found that adding an average of 14.25% impulse noise is enough to deceive the API. Our findings indicate the vulnerability of the API in adversarial environments. For example, an adversary can bypass an image filtering system by adding noise to inappropriate images. We then show that when a noise filter is applied on input images, the API generates mostly the same outputs for restored images as for original images. This observation suggests that cloud vision API can readily benefit from noise filtering, without the need for updating image analysis algorithms.



### AnchorNet: A Weakly Supervised Network to Learn Geometry-sensitive Features For Semantic Matching
- **Arxiv ID**: http://arxiv.org/abs/1704.04749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04749v1)
- **Published**: 2017-04-16 11:07:02+00:00
- **Updated**: 2017-04-16 11:07:02+00:00
- **Authors**: David Novotny, Diane Larlus, Andrea Vedaldi
- **Comment**: Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition. 2017
- **Journal**: None
- **Summary**: Despite significant progress of deep learning in recent years, state-of-the-art semantic matching methods still rely on legacy features such as SIFT or HoG. We argue that the strong invariance properties that are key to the success of recent deep architectures on the classification task make them unfit for dense correspondence tasks, unless a large amount of supervision is used. In this work, we propose a deep network, termed AnchorNet, that produces image representations that are well-suited for semantic matching. It relies on a set of filters whose response is geometrically consistent across different object instances, even in the presence of strong intra-class, scale, or viewpoint variations. Trained only with weak image-level labels, the final representation successfully captures information about the object structure and improves results of state-of-the-art semantic matching methods such as the deformable spatial pyramid or the proposal flow methods. We show positive results on the cross-instance matching task where different instances of the same object category are matched as well as on a new cross-category semantic matching task aligning pairs of instances each from a different object class.



### Harvesting Multiple Views for Marker-less 3D Human Pose Annotations
- **Arxiv ID**: http://arxiv.org/abs/1704.04793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04793v1)
- **Published**: 2017-04-16 16:19:19+00:00
- **Updated**: 2017-04-16 16:19:19+00:00
- **Authors**: Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis, Kostas Daniilidis
- **Comment**: CVPR 2017 Camera Ready
- **Journal**: None
- **Summary**: Recent advances with Convolutional Networks (ConvNets) have shifted the bottleneck for many computer vision tasks to annotated data collection. In this paper, we present a geometry-driven approach to automatically collect annotations for human pose prediction tasks. Starting from a generic ConvNet for 2D human pose, and assuming a multi-view setup, we describe an automatic way to collect accurate 3D human pose annotations. We capitalize on constraints offered by the 3D geometry of the camera setup and the 3D structure of the human body to probabilistically combine per view 2D ConvNet predictions into a globally optimal 3D pose. This 3D pose is used as the basis for harvesting annotations. The benefit of the annotations produced automatically with our approach is demonstrated in two challenging settings: (i) fine-tuning a generic ConvNet-based 2D pose predictor to capture the discriminative aspects of a subject's appearance (i.e.,"personalization"), and (ii) training a ConvNet from scratch for single view 3D human pose prediction without leveraging 3D pose groundtruth. The proposed multi-view pose estimator achieves state-of-the-art results on standard benchmarks, demonstrating the effectiveness of our method in exploiting the available multi-view information.



### Replicator Equation: Applications Revisited
- **Arxiv ID**: http://arxiv.org/abs/1704.04805v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04805v2)
- **Published**: 2017-04-16 18:20:44+00:00
- **Updated**: 2017-05-20 17:22:47+00:00
- **Authors**: Tinsae G. Dulecha
- **Comment**: None
- **Journal**: None
- **Summary**: The replicator equation is a simple model of evolution that leads to stable form of Nash Equilibrium, Evolutionary Stable Strategy (ESS). It has been studied in connection with Evolutionary Game Theory and was originally developed for symmetric games. Beyond its first emphasis in biological use, evolutionary game theory has been expanded well beyond in social studies for behavioral analysis, in machine learning, computer vision and others. Its several applications in the fields of machine learning and computer vision has drawn my attention which is the reason to write this extended abstract



### CT Image Reconstruction in a Low Dimensional Manifold
- **Arxiv ID**: http://arxiv.org/abs/1704.04825v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.04825v1)
- **Published**: 2017-04-16 22:02:27+00:00
- **Updated**: 2017-04-16 22:02:27+00:00
- **Authors**: Wenxiang Cong, Ge Wang, Qingsong Yang, Jiang Hsieh, Jia Li, Rongjie Lai
- **Comment**: None
- **Journal**: None
- **Summary**: Regularization methods are commonly used in X-ray CT image reconstruction. Different regularization methods reflect the characterization of different prior knowledge of images. In a recent work, a new regularization method called a low-dimensional manifold model (LDMM) is investigated to characterize the low-dimensional patch manifold structure of natural images, where the manifold dimensionality characterizes structural information of an image. In this paper, we propose a CT image reconstruction method based on the prior knowledge of the low-dimensional manifold of CT image. Using the clinical raw projection data from GE clinic, we conduct comparisons for the CT image reconstruction among the proposed method, the simultaneous algebraic reconstruction technique (SART) with the total variation (TV) regularization, and the filtered back projection (FBP) method. Results show that the proposed method can successfully recover structural details of an imaging object, and achieve higher spatial and contrast resolution of the reconstructed image than counterparts of FBP and SART with TV.



