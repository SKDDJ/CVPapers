# Arxiv Papers in cs.CV on 2017-04-12
### Reformulating Level Sets as Deep Recurrent Neural Network Approach to Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.03593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03593v1)
- **Published**: 2017-04-12 01:51:52+00:00
- **Updated**: 2017-04-12 01:51:52+00:00
- **Authors**: Ngan Le, Kha Gia Quach, Khoa Luu, Marios Savvides, Chenchen Zhu
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Variational Level Set (LS) has been a widely used method in medical segmentation. However, it is limited when dealing with multi-instance objects in the real world. In addition, its segmentation results are quite sensitive to initial settings and highly depend on the number of iterations. To address these issues and boost the classic variational LS methods to a new level of the learnable deep learning approaches, we propose a novel definition of contour evolution named Recurrent Level Set (RLS)} to employ Gated Recurrent Unit under the energy minimization of a variational LS functional. The curve deformation process in RLS is formed as a hidden state evolution procedure and updated by minimizing an energy functional composed of fitting forces and contour length. By sharing the convolutional features in a fully end-to-end trainable framework, we extend RLS to Contextual RLS (CRLS) to address semantic segmentation in the wild. The experimental results have shown that our proposed RLS improves both computational time and segmentation accuracy against the classic variations LS-based method, whereas the fully end-to-end system CRLS achieves competitive performance compared to the state-of-the-art semantic segmentation approaches.



### Deep Contextual Recurrent Residual Networks for Scene Labeling
- **Arxiv ID**: http://arxiv.org/abs/1704.03594v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03594v1)
- **Published**: 2017-04-12 01:52:06+00:00
- **Updated**: 2017-04-12 01:52:06+00:00
- **Authors**: T. Hoang Ngan Le, Chi Nhan Duong, Ligong Han, Khoa Luu, Marios Savvides, Dipan Pal
- **Comment**: None
- **Journal**: None
- **Summary**: Designed as extremely deep architectures, deep residual networks which provide a rich visual representation and offer robust convergence behaviors have recently achieved exceptional performance in numerous computer vision problems. Being directly applied to a scene labeling problem, however, they were limited to capture long-range contextual dependence, which is a critical aspect. To address this issue, we propose a novel approach, Contextual Recurrent Residual Networks (CRRN) which is able to simultaneously handle rich visual representation learning and long-range context modeling within a fully end-to-end deep network. Furthermore, our proposed end-to-end CRRN is completely trained from scratch, without using any pre-trained models in contrast to most existing methods usually fine-tuned from the state-of-the-art pre-trained models, e.g. VGG-16, ResNet, etc. The experiments are conducted on four challenging scene labeling datasets, i.e. SiftFlow, CamVid, Stanford background and SUN datasets, and compared against various state-of-the-art scene labeling methods.



### Instance-Level Salient Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.03604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03604v1)
- **Published**: 2017-04-12 03:05:27+00:00
- **Updated**: 2017-04-12 03:05:27+00:00
- **Authors**: Guanbin Li, Yuan Xie, Liang Lin, Yizhou Yu
- **Comment**: To appear in CVPR2017
- **Journal**: None
- **Summary**: Image saliency detection has recently witnessed rapid progress due to deep convolutional neural networks. However, none of the existing methods is able to identify object instances in the detected salient regions. In this paper, we present a salient instance segmentation method that produces a saliency mask with distinct object instance labels for an input image. Our method consists of three steps, estimating saliency map, detecting salient object contours and identifying salient object instances. For the first two steps, we propose a multiscale saliency refinement network, which generates high-quality salient region masks and salient object contours. Once integrated with multiscale combinatorial grouping and a MAP-based subset optimization framework, our method can generate very promising salient object instance segmentation results. To promote further research and evaluation of salient instance segmentation, we also construct a new database of 1000 images and their pixelwise salient instance annotations. Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public benchmarks for salient region detection as well as on our new dataset for salient instance segmentation.



### Automatic Discovery, Association Estimation and Learning of Semantic Attributes for a Thousand Categories
- **Arxiv ID**: http://arxiv.org/abs/1704.03607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03607v1)
- **Published**: 2017-04-12 03:34:19+00:00
- **Updated**: 2017-04-12 03:34:19+00:00
- **Authors**: Ziad Al-Halah, Rainer Stiefelhagen
- **Comment**: Accepted as a conference paper at CVPR 2017
- **Journal**: None
- **Summary**: Attribute-based recognition models, due to their impressive performance and their ability to generalize well on novel categories, have been widely adopted for many computer vision applications. However, usually both the attribute vocabulary and the class-attribute associations have to be provided manually by domain experts or large number of annotators. This is very costly and not necessarily optimal regarding recognition performance, and most importantly, it limits the applicability of attribute-based models to large scale data sets. To tackle this problem, we propose an end-to-end unsupervised attribute learning approach. We utilize online text corpora to automatically discover a salient and discriminative vocabulary that correlates well with the human concept of semantic attributes. Moreover, we propose a deep convolutional model to optimize class-attribute associations with a linguistic prior that accounts for noise and missing data in text. In a thorough evaluation on ImageNet, we demonstrate that our model is able to efficiently discover and learn semantic attributes at a large scale. Furthermore, we demonstrate that our model outperforms the state-of-the-art in zero-shot learning on three data sets: ImageNet, Animals with Attributes and aPascal/aYahoo. Finally, we enable attribute-based learning on ImageNet and will share the attributes and associations for future research.



### Predictive-Corrective Networks for Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.03615v2
- **DOI**: 10.1109/CVPR.2017.223
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03615v2)
- **Published**: 2017-04-12 04:20:35+00:00
- **Updated**: 2017-12-12 05:13:43+00:00
- **Authors**: Achal Dave, Olga Russakovsky, Deva Ramanan
- **Comment**: Accepted to CVPR 2017. [v2]: Updated Multi-LSTM mAP on MultiTHUMOS
  (should be 29.7, was initially reported as 29.6). [Project URL]:
  http://www.achaldave.com/projects/predictive-corrective/
- **Journal**: None
- **Summary**: While deep feature learning has revolutionized techniques for static-image understanding, the same does not quite hold for video processing. Architectures and optimization techniques used for video are largely based off those for static images, potentially underutilizing rich video information. In this work, we rethink both the underlying network architecture and the stochastic learning paradigm for temporal data. To do so, we draw inspiration from classic theory on linear dynamic systems for modeling time series. By extending such models to include nonlinear mappings, we derive a series of novel recurrent neural networks that sequentially make top-down predictions about the future and then correct those predictions with bottom-up observations. Predictive-corrective networks have a number of desirable properties: (1) they can adaptively focus computation on "surprising" frames where predictions require large corrections, (2) they simplify learning in that only "residual-like" corrective terms need to be learned over time and (3) they naturally decorrelate an input data stream in a hierarchical fashion, producing a more reliable signal for learning at each layer of a network. We provide an extensive analysis of our lightweight and interpretable framework, and demonstrate that our model is competitive with the two-stream network on three challenging datasets without the need for computationally expensive optical flow.



### Feature Tracking Cardiac Magnetic Resonance via Deep Learning and Spline Optimization
- **Arxiv ID**: http://arxiv.org/abs/1704.03660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03660v1)
- **Published**: 2017-04-12 08:43:35+00:00
- **Updated**: 2017-04-12 08:43:35+00:00
- **Authors**: Davis M. Vigneault, Weidi Xie, David A. Bluemke, J. Alison Noble
- **Comment**: Accepted to Functional Imaging and Modeling of the Heart (FIMH) 2017
- **Journal**: None
- **Summary**: Feature tracking Cardiac Magnetic Resonance (CMR) has recently emerged as an area of interest for quantification of regional cardiac function from balanced, steady state free precession (SSFP) cine sequences. However, currently available techniques lack full automation, limiting reproducibility. We propose a fully automated technique whereby a CMR image sequence is first segmented with a deep, fully convolutional neural network (CNN) architecture, and quadratic basis splines are fitted simultaneously across all cardiac frames using least squares optimization. Experiments are performed using data from 42 patients with hypertrophic cardiomyopathy (HCM) and 21 healthy control subjects. In terms of segmentation, we compared state-of-the-art CNN frameworks, U-Net and dilated convolution architectures, with and without temporal context, using cross validation with three folds. Performance relative to expert manual segmentation was similar across all networks: pixel accuracy was ~97%, intersection-over-union (IoU) across all classes was ~87%, and IoU across foreground classes only was ~85%. Endocardial left ventricular circumferential strain calculated from the proposed pipeline was significantly different in control and disease subjects (-25.3% vs -29.1%, p = 0.006), in agreement with the current clinical literature.



### Dilated Convolutional Neural Networks for Cardiovascular MR Segmentation in Congenital Heart Disease
- **Arxiv ID**: http://arxiv.org/abs/1704.03669v1
- **DOI**: 10.1007/978-3-319-52280-7_9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03669v1)
- **Published**: 2017-04-12 09:28:40+00:00
- **Updated**: 2017-04-12 09:28:40+00:00
- **Authors**: Jelmer M. Wolterink, Tim Leiner, Max A. Viergever, Ivana Išgum
- **Comment**: None
- **Journal**: RAMBO 2016, HVSMR 2016. LNCS 10129. pp. 95-102
- **Summary**: We propose an automatic method using dilated convolutional neural networks (CNNs) for segmentation of the myocardium and blood pool in cardiovascular MR (CMR) of patients with congenital heart disease (CHD).   Ten training and ten test CMR scans cropped to an ROI around the heart were provided in the MICCAI 2016 HVSMR challenge. A dilated CNN with a receptive field of 131x131 voxels was trained for myocardium and blood pool segmentation in axial, sagittal and coronal image slices. Performance was evaluated within the HVSMR challenge.   Automatic segmentation of the test scans resulted in Dice indices of 0.80$\pm$0.06 and 0.93$\pm$0.02, average distances to boundaries of 0.96$\pm$0.31 and 0.89$\pm$0.24 mm, and Hausdorff distances of 6.13$\pm$3.76 and 7.07$\pm$3.01 mm for the myocardium and blood pool, respectively. Segmentation took 41.5$\pm$14.7 s per scan.   In conclusion, dilated CNNs trained on a small set of CMR images of CHD patients showing large anatomical variability provide accurate myocardium and blood pool segmentations.



### Detection, Recognition and Tracking of Moving Objects from Real-time Video via SP Theory of Intelligence and Species Inspired PSO
- **Arxiv ID**: http://arxiv.org/abs/1704.07312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07312v1)
- **Published**: 2017-04-12 10:42:37+00:00
- **Updated**: 2017-04-12 10:42:37+00:00
- **Authors**: Kumar S Ray, Sayandip Dutta, Anit Chakraborty
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the basic problem of recognizing moving objects in video images using SP Theory of Intelligence. The concept of SP Theory of Intelligence which is a framework of artificial intelligence, was first introduced by Gerard J Wolff, where S stands for Simplicity and P stands for Power. Using the concept of multiple alignment, we detect and recognize object of our interest in video frames with multilevel hierarchical parts and subparts, based on polythetic categories. We track the recognized objects using the species based Particle Swarm Optimization (PSO). First, we extract the multiple alignment of our object of interest from training images. In order to recognize accurately and handle occlusion, we use the polythetic concepts on raw data line to omit the redundant noise via searching for best alignment representing the features from the extracted alignments. We recognize the domain of interest from the video scenes in form of wide variety of multiple alignments to handle scene variability. Unsupervised learning is done in the SP model following the DONSVIC principle and natural structures are discovered via information compression and pattern analysis. After successful recognition of objects, we use species based PSO algorithm as the alignments of our object of interest is analogues to observation likelihood and fitness ability of species. Subsequently, we analyze the competition and repulsion among species with annealed Gaussian based PSO. We have tested our algorithms on David, Walking2, FaceOcc1, Jogging and Dudek, obtaining very satisfactory and competitive results.



### Object proposal generation applying the distance dependent Chinese restaurant process
- **Arxiv ID**: http://arxiv.org/abs/1704.03706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03706v1)
- **Published**: 2017-04-12 11:15:32+00:00
- **Updated**: 2017-04-12 11:15:32+00:00
- **Authors**: Mikko Lauri, Simone Frintrop
- **Comment**: To appear at Scandinavian Conference on Image Analysis (SCIA) 2017
- **Journal**: None
- **Summary**: In application domains such as robotics, it is useful to represent the uncertainty related to the robot's belief about the state of its environment. Algorithms that only yield a single "best guess" as a result are not sufficient. In this paper, we propose object proposal generation based on non-parametric Bayesian inference that allows quantification of the likelihood of the proposals. We apply Markov chain Monte Carlo to draw samples of image segmentations via the distance dependent Chinese restaurant process. Our method achieves state-of-the-art performance on an indoor object discovery data set, while additionally providing a likelihood term for each proposal. We show that the likelihood term can effectively be used to rank proposals according to their quality.



### Unsupervised Construction of Human Body Models Using Principles of Organic Computing
- **Arxiv ID**: http://arxiv.org/abs/1704.03724v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1704.03724v1)
- **Published**: 2017-04-12 12:32:20+00:00
- **Updated**: 2017-04-12 12:32:20+00:00
- **Authors**: Thomas Walther, Rolf P. Würtz
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised learning of a generalizable model of the visual appearance of humans from video data is of major importance for computing systems interacting naturally with their users and others. We propose a step towards automatic behavior understanding by integrating principles of Organic Computing into the posture estimation cycle, thereby relegating the need for human intervention while simultaneously raising the level of system autonomy. The system extracts coherent motion from moving upper bodies and autonomously decides about limbs and their possible spatial relationships. The models from many videos are integrated into meta-models, which show good generalization to different individuals, backgrounds, and attire. These models allow robust interpretation of single video frames without temporal continuity and posture mimicking by an android robot.



### Deep-FExt: Deep Feature Extraction for Vessel Segmentation and Centerline Prediction
- **Arxiv ID**: http://arxiv.org/abs/1704.03743v1
- **DOI**: 10.1007/978-3-319-67389-9_40
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.03743v1)
- **Published**: 2017-04-12 13:10:20+00:00
- **Updated**: 2017-04-12 13:10:20+00:00
- **Authors**: Giles Tetteh, Markus Rempfler, Bjoern H. Menze, Claus Zimmer
- **Comment**: 9 pages
- **Journal**: Wang Q., Shi Y., Suk HI., Suzuki K. (eds) Machine Learning in
  Medical Imaging. MLMI 2017. Lecture Notes in Computer Science, vol 10541.
  Springer, Cham
- **Summary**: Feature extraction is a very crucial task in image and pixel (voxel) classification and regression in biomedical image modeling. In this work we present a machine learning based feature extraction scheme based on inception models for pixel classification tasks. We extract features under multi-scale and multi-layer schemes through convolutional operators. Layers of Fully Convolutional Network are later stacked on this feature extraction layers and trained end-to-end for the purpose of classification. We test our model on the DRIVE and STARE public data sets for the purpose of segmentation and centerline detection and it out performs most existing hand crafted or deterministic feature schemes found in literature. We achieve an average maximum Dice of 0.85 on the DRIVE data set which out performs the scores from the second human annotator of this data set. We also achieve an average maximum Dice of 0.85 and kappa of 0.84 on the STARE data set. Though these datasets are mainly 2-D we also propose ways of extending this feature extraction scheme to handle 3-D datasets.



### Unsupervised part learning for visual recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.03755v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1704.03755v1)
- **Published**: 2017-04-12 13:35:03+00:00
- **Updated**: 2017-04-12 13:35:03+00:00
- **Authors**: Ronan Sicre, Yannis Avrithis, Ewa Kijak, Frederic Jurie
- **Comment**: None
- **Journal**: None
- **Summary**: Part-based image classification aims at representing categories by small sets of learned discriminative parts, upon which an image representation is built. Considered as a promising avenue a decade ago, this direction has been neglected since the advent of deep neural networks. In this context, this paper brings two contributions: first, it shows that despite the recent success of end-to-end holistic models, explicit part learning can boosts classification performance. Second, this work proceeds one step further than recent part-based models (PBM), focusing on how to learn parts without using any labeled data. Instead of learning a set of parts per class, as generally done in the PBM literature, the proposed approach both constructs a partition of a given set of images into visually similar groups, and subsequently learn a set of discriminative parts per group in a fully unsupervised fashion. This strategy opens the door to the use of PBM in new applications for which the notion of image categories is irrelevant, such as instance-based image retrieval, for example. We experimentally show that our learned parts can help building efficient image representations, for classification as well as for indexing tasks, resulting in performance superior to holistic state-of-the art Deep Convolutional Neural Networks (DCNN) encoding.



### Attention-Set based Metric Learning for Video Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.03805v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03805v3)
- **Published**: 2017-04-12 15:54:24+00:00
- **Updated**: 2017-08-28 09:25:35+00:00
- **Authors**: Yibo Hu, Xiang Wu, Ran He
- **Comment**: modify for ACPR
- **Journal**: None
- **Summary**: Face recognition has made great progress with the development of deep learning. However, video face recognition (VFR) is still an ongoing task due to various illumination, low-resolution, pose variations and motion blur. Most existing CNN-based VFR methods only obtain a feature vector from a single image and simply aggregate the features in a video, which less consider the correlations of face images in one video. In this paper, we propose a novel Attention-Set based Metric Learning (ASML) method to measure the statistical characteristics of image sets. It is a promising and generalized extension of Maximum Mean Discrepancy with memory attention weighting. First, we define an effective distance metric on image sets, which explicitly minimizes the intra-set distance and maximizes the inter-set distance simultaneously. Second, inspired by Neural Turing Machine, a Memory Attention Weighting is proposed to adapt set-aware global contents. Then ASML is naturally integrated into CNNs, resulting in an end-to-end learning scheme. Our method achieves state-of-the-art performance for the task of video face recognition on the three widely used benchmarks including YouTubeFace, YouTube Celebrities and Celebrity-1000.



### Connecting Look and Feel: Associating the visual and tactile properties of physical materials
- **Arxiv ID**: http://arxiv.org/abs/1704.03822v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03822v1)
- **Published**: 2017-04-12 16:28:14+00:00
- **Updated**: 2017-04-12 16:28:14+00:00
- **Authors**: Wenzhen Yuan, Shaoxiong Wang, Siyuan Dong, Edward Adelson
- **Comment**: None
- **Journal**: None
- **Summary**: For machines to interact with the physical world, they must understand the physical properties of objects and materials they encounter. We use fabrics as an example of a deformable material with a rich set of mechanical properties. A thin flexible fabric, when draped, tends to look different from a heavy stiff fabric. It also feels different when touched. Using a collection of 118 fabric sample, we captured color and depth images of draped fabrics along with tactile data from a high resolution touch sensor. We then sought to associate the information from vision and touch by jointly training CNNs across the three modalities. Through the CNN, each input, regardless of the modality, generates an embedding vector that records the fabric's physical property. By comparing the embeddings, our system is able to look at a fabric image and predict how it will feel, and vice versa. We also show that a system jointly trained on vision and touch data can outperform a similar system trained only on visual data when tested purely with visual inputs.



### Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1704.03847v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1704.03847v1)
- **Published**: 2017-04-12 17:12:57+00:00
- **Updated**: 2017-04-12 17:12:57+00:00
- **Authors**: Timo Hackel, Nikolay Savinov, Lubor Ladicky, Jan D. Wegner, Konrad Schindler, Marc Pollefeys
- **Comment**: Accepted to ISPRS Annals. The benchmark website is available at
  http://www.semantic3d.net/ . The baseline code is available at
  https://github.com/nsavinov/semantic3dnet
- **Journal**: None
- **Summary**: This paper presents a new 3D point cloud classification benchmark data set with over four billion manually labelled points, meant as input for data-hungry (deep) learning methods. We also discuss first submissions to the benchmark that use deep convolutional neural networks (CNNs) as a work horse, which already show remarkable performance improvements over state-of-the-art. CNNs have become the de-facto standard for many tasks in computer vision and machine learning like semantic segmentation or object detection in images, but have no yet led to a true breakthrough for 3D point cloud labelling tasks due to lack of training data. With the massive data set presented in this paper, we aim at closing this data gap to help unleash the full potential of deep learning methods for 3D labelling tasks. Our semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains 8 semantic classes and covers a wide range of urban outdoor scenes: churches, streets, railroad tracks, squares, villages, soccer fields and castles. We describe our labelling interface and show that our data set provides more dense and complete point clouds with much higher overall number of labelled points compared to those already available to the research community. We further provide baseline method descriptions and comparison between methods submitted to our online system. We hope semantic3D.net will pave the way for deep learning methods in 3D point cloud labelling to learn richer, more general 3D representations, and first submissions after only a few months indicate that this might indeed be the case.



### Optimal Threshold Design for Quanta Image Sensor
- **Arxiv ID**: http://arxiv.org/abs/1704.03886v2
- **DOI**: 10.1109/TCI.2017.2781185
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03886v2)
- **Published**: 2017-04-12 18:19:28+00:00
- **Updated**: 2017-10-30 20:03:08+00:00
- **Authors**: Omar A. Elgendy, Stanley H. Chan
- **Comment**: 12 pages main paper, and 8 pages supplementary
- **Journal**: None
- **Summary**: Quanta Image Sensor (QIS) is a binary imaging device envisioned to be the next generation image sensor after CCD and CMOS. Equipped with a massive number of single photon detectors, the sensor has a threshold $q$ above which the number of arriving photons will trigger a binary response "1", or "0" otherwise. Existing methods in the device literature typically assume that $q=1$ uniformly. We argue that a spatially varying threshold can significantly improve the signal-to-noise ratio of the reconstructed image. In this paper, we present an optimal threshold design framework. We make two contributions. First, we derive a set of oracle results to theoretically inform the maximally achievable performance. We show that the oracle threshold should match exactly with the underlying pixel intensity. Second, we show that around the oracle threshold there exists a set of thresholds that give asymptotically unbiased reconstructions. The asymptotic unbiasedness has a phase transition behavior which allows us to develop a practical threshold update scheme using a bisection method. Experimentally, the new threshold design method achieves better rate of convergence than existing methods.



### What's in a Question: Using Visual Questions as a Form of Supervision
- **Arxiv ID**: http://arxiv.org/abs/1704.03895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03895v1)
- **Published**: 2017-04-12 18:48:15+00:00
- **Updated**: 2017-04-12 18:48:15+00:00
- **Authors**: Siddha Ganju, Olga Russakovsky, Abhinav Gupta
- **Comment**: CVPR 2017 Spotlight paper and supplementary
- **Journal**: None
- **Summary**: Collecting fully annotated image datasets is challenging and expensive. Many types of weak supervision have been explored: weak manual annotations, web search results, temporal continuity, ambient sound and others. We focus on one particular unexplored mode: visual questions that are asked about images. The key observation that inspires our work is that the question itself provides useful information about the image (even without the answer being available). For instance, the question "what is the breed of the dog?" informs the AI that the animal in the scene is a dog and that there is only one dog present. We make three contributions: (1) providing an extensive qualitative and quantitative analysis of the information contained in human visual questions, (2) proposing two simple but surprisingly effective modifications to the standard visual question answering models that allow them to make use of weak supervision in the form of unanswered questions associated with images and (3) demonstrating that a simple data augmentation strategy inspired by our insights results in a 7.1% improvement on the standard VQA benchmark.



### Deep Reinforcement Learning-based Image Captioning with Embedding Reward
- **Arxiv ID**: http://arxiv.org/abs/1704.03899v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1704.03899v1)
- **Published**: 2017-04-12 18:55:03+00:00
- **Updated**: 2017-04-12 18:55:03+00:00
- **Authors**: Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, Li-Jia Li
- **Comment**: None
- **Journal**: None
- **Summary**: Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a "policy network" and a "value network" to collaboratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-of-the-art approaches across different evaluation metrics.



### Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1704.03915v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03915v2)
- **Published**: 2017-04-12 20:04:06+00:00
- **Updated**: 2017-10-09 22:47:12+00:00
- **Authors**: Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang
- **Comment**: This work is accepted in CVPR 2017. The code and datasets are
  available on http://vllab.ucmerced.edu/wlai24/LapSRN/
- **Journal**: None
- **Summary**: Convolutional neural networks have recently demonstrated high-quality reconstruction for single-image super-resolution. In this paper, we propose the Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively reconstruct the sub-band residuals of high-resolution images. At each pyramid level, our model takes coarse-resolution feature maps as input, predicts the high-frequency residuals, and uses transposed convolutions for upsampling to the finer level. Our method does not require the bicubic interpolation as the pre-processing step and thus dramatically reduces the computational complexity. We train the proposed LapSRN with deep supervision using a robust Charbonnier loss function and achieve high-quality reconstruction. Furthermore, our network generates multi-scale predictions in one feed-forward pass through the progressive reconstruction, thereby facilitates resource-aware applications. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of speed and accuracy.



### Provable Self-Representation Based Outlier Detection in a Union of Subspaces
- **Arxiv ID**: http://arxiv.org/abs/1704.03925v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.03925v1)
- **Published**: 2017-04-12 20:45:48+00:00
- **Updated**: 2017-04-12 20:45:48+00:00
- **Authors**: Chong You, Daniel P. Robinson, René Vidal
- **Comment**: 16 pages. CVPR 2017 spotlight oral presentation
- **Journal**: None
- **Summary**: Many computer vision tasks involve processing large amounts of data contaminated by outliers, which need to be detected and rejected. While outlier detection methods based on robust statistics have existed for decades, only recently have methods based on sparse and low-rank representation been developed along with guarantees of correct outlier detection when the inliers lie in one or more low-dimensional subspaces. This paper proposes a new outlier detection method that combines tools from sparse representation with random walks on a graph. By exploiting the property that data points can be expressed as sparse linear combinations of each other, we obtain an asymmetric affinity matrix among data points, which we use to construct a weighted directed graph. By defining a suitable Markov Chain from this graph, we establish a connection between inliers/outliers and essential/inessential states of the Markov chain, which allows us to detect outliers by using random walks. We provide a theoretical analysis that justifies the correctness of our method under geometric and connectivity assumptions. Experimental results on image databases demonstrate its superiority with respect to state-of-the-art sparse and low-rank outlier detection methods.



### Discriminative Bimodal Networks for Visual Localization and Detection with Natural Language Queries
- **Arxiv ID**: http://arxiv.org/abs/1704.03944v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.03944v2)
- **Published**: 2017-04-12 22:09:36+00:00
- **Updated**: 2017-04-17 07:22:14+00:00
- **Authors**: Yuting Zhang, Luyao Yuan, Yijie Guo, Zhiyuan He, I-An Huang, Honglak Lee
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2017
- **Journal**: None
- **Summary**: Associating image regions with text queries has been recently explored as a new way to bridge visual and linguistic representations. A few pioneering approaches have been proposed based on recurrent neural language models trained generatively (e.g., generating captions), but achieving somewhat limited localization accuracy. To better address natural-language-based visual entity localization, we propose a discriminative approach. We formulate a discriminative bimodal neural network (DBNet), which can be trained by a classifier with extensive use of negative samples. Our training objective encourages better localization on single images, incorporates text phrases in a broad range, and properly pairs image regions with text phrases into positive and negative examples. Experiments on the Visual Genome dataset demonstrate the proposed DBNet significantly outperforms previous state-of-the-art methods both for localization on single images and for detection on multiple images. We we also establish an evaluation protocol for natural-language visual detection.



### Asymmetric Feature Maps with Application to Sketch Based Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1704.03946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03946v1)
- **Published**: 2017-04-12 22:20:06+00:00
- **Updated**: 2017-04-12 22:20:06+00:00
- **Authors**: Giorgos Tolias, Ondřej Chum
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: We propose a novel concept of asymmetric feature maps (AFM), which allows to evaluate multiple kernels between a query and database entries without increasing the memory requirements. To demonstrate the advantages of the AFM method, we derive a short vector image representation that, due to asymmetric feature maps, supports efficient scale and translation invariant sketch-based image retrieval. Unlike most of the short-code based retrieval systems, the proposed method provides the query localization in the retrieved image. The efficiency of the search is boosted by approximating a 2D translation search via trigonometric polynomial of scores by 1D projections. The projections are a special case of AFM. An order of magnitude speed-up is achieved compared to traditional trigonometric polynomials. The results are boosted by an image-based average query expansion, exceeding significantly the state of the art on standard benchmarks.



