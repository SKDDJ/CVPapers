# Arxiv Papers in cs.CV on 2017-04-13
### Virtual to Real Reinforcement Learning for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1704.03952v4
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.03952v4)
- **Published**: 2017-04-13 00:03:40+00:00
- **Updated**: 2017-09-26 17:22:04+00:00
- **Authors**: Xinlei Pan, Yurong You, Ziyan Wang, Cewu Lu
- **Comment**: None
- **Journal**: Proceedings of the British Machine Vision Conference (BMVC) 2017
  (Spotlight)
- **Summary**: Reinforcement learning is considered as a promising direction for driving policy learning. However, training autonomous driving vehicle with reinforcement learning in real environment involves non-affordable trial-and-error. It is more desirable to first train in a virtual environment and then transfer to the real environment. In this paper, we propose a novel realistic translation network to make model trained in virtual environment be workable in real world. The proposed network can convert non-realistic virtual image input into a realistic one with similar scene structure. Given realistic frames as input, driving policy trained by reinforcement learning can nicely adapt to real world driving. Experiments show that our proposed virtual to real (VR) reinforcement learning (RL) works pretty well. To our knowledge, this is the first successful case of driving policy trained by reinforcement learning that can adapt to real world driving data.



### Efficient Sparse Subspace Clustering by Nearest Neighbour Filtering
- **Arxiv ID**: http://arxiv.org/abs/1704.03958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03958v1)
- **Published**: 2017-04-13 00:48:32+00:00
- **Updated**: 2017-04-13 00:48:32+00:00
- **Authors**: Stephen Tierney, Yi Guo, Junbin Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse Subspace Clustering (SSC) has been used extensively for subspace identification tasks due to its theoretical guarantees and relative ease of implementation. However SSC has quadratic computation and memory requirements with respect to the number of input data points. This burden has prohibited SSCs use for all but the smallest datasets. To overcome this we propose a new method, k-SSC, that screens out a large number of data points to both reduce SSC to linear memory and computational requirements. We provide theoretical analysis for the bounds of success for k-SSC. Our experiments show that k-SSC exceeds theoretical expectations and outperforms existing SSC approximations by maintaining the classification performance of SSC. Furthermore in the spirit of reproducible research we have publicly released the source code for k-SSC



### Tractable Clustering of Data on the Curve Manifold
- **Arxiv ID**: http://arxiv.org/abs/1704.03963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03963v1)
- **Published**: 2017-04-13 01:18:41+00:00
- **Updated**: 2017-04-13 01:18:41+00:00
- **Authors**: Stephen Tierney, Junbin Gao, Yi Guo, Zheng Zhang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1601.00732
- **Journal**: None
- **Summary**: In machine learning it is common to interpret each data point as a vector in Euclidean space. However the data may actually be functional i.e.\ each data point is a function of some variable such as time and the function is discretely sampled. The naive treatment of functional data as traditional multivariate data can lead to poor performance since the algorithms are ignoring the correlation in the curvature of each function. In this paper we propose a tractable method to cluster functional data or curves by adapting the Euclidean Low-Rank Representation (LRR) to the curve manifold. Experimental evaluation on synthetic and real data reveals that this method massively outperforms prior clustering methods in both speed and accuracy when clustering functional data.



### Collaborative Low-Rank Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1704.03966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03966v1)
- **Published**: 2017-04-13 01:41:30+00:00
- **Updated**: 2017-04-13 01:41:30+00:00
- **Authors**: Stephen Tierney, Yi Guo, Junbin Gao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present Collaborative Low-Rank Subspace Clustering. Given multiple observations of a phenomenon we learn a unified representation matrix. This unified matrix incorporates the features from all the observations, thus increasing the discriminative power compared with learning the representation matrix on each observation separately. Experimental evaluation shows that our method outperforms subspace clustering on separate observations and the state of the art collaborative learning algorithm.



### On the Effects of Batch and Weight Normalization in Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.03971v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.03971v4)
- **Published**: 2017-04-13 02:15:28+00:00
- **Updated**: 2017-12-04 01:56:42+00:00
- **Authors**: Sitao Xiang, Hao Li
- **Comment**: v3 rejected by NIPS 2017, updated and re-submitted to CVPR 2018. v4:
  add experiments with ResNet and like to new code
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) are highly effective unsupervised learning frameworks that can generate very sharp data, even for data such as images with complex, highly multimodal distributions. However GANs are known to be very hard to train, suffering from problems such as mode collapse and disturbing visual artifacts. Batch normalization (BN) techniques have been introduced to address the training. Though BN accelerates the training in the beginning, our experiments show that the use of BN can be unstable and negatively impact the quality of the trained model. The evaluation of BN and numerous other recent schemes for improving GAN training is hindered by the lack of an effective objective quality measure for GAN models. To address these issues, we first introduce a weight normalization (WN) approach for GAN training that significantly improves the stability, efficiency and the quality of the generated samples. To allow a methodical evaluation, we introduce squared Euclidean reconstruction error on a test set as a new objective measure, to assess training performance in terms of speed, stability, and quality of generated samples. Our experiments with a standard DCGAN architecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10) indicate that training using WN is generally superior to BN for GANs, achieving 10% lower mean squared loss for reconstruction and significantly better qualitative results than BN. We further demonstrate the stability of WN on a 21-layer ResNet trained with the CelebA data set. The code for this paper is available at https://github.com/stormraiser/gan-weightnorm-resnet



### 2D-3D Pose Consistency-based Conditional Random Fields for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1704.03986v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.03986v2)
- **Published**: 2017-04-13 03:46:57+00:00
- **Updated**: 2017-12-28 05:31:47+00:00
- **Authors**: Ju Yong Chang, Kyoung Mu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: This study considers the 3D human pose estimation problem in a single RGB image by proposing a conditional random field (CRF) model over 2D poses, in which the 3D pose is obtained as a byproduct of the inference process. The unary term of the proposed CRF model is defined based on a powerful heat-map regression network, which has been proposed for 2D human pose estimation. This study also presents a regression network for lifting the 2D pose to 3D pose and proposes the prior term based on the consistency between the estimated 3D pose and the 2D pose. To obtain the approximate solution of the proposed CRF model, the N-best strategy is adopted. The proposed inference algorithm can be viewed as sequential processes of bottom-up generation of 2D and 3D pose proposals from the input 2D image based on deep networks and top-down verification of such proposals by checking their consistencies. To evaluate the proposed method, we use two large-scale datasets: Human3.6M and HumanEva. Experimental results show that the proposed method achieves the state-of-the-art 3D human pose estimation performance.



### Interspecies Knowledge Transfer for Facial Keypoint Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.04023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04023v1)
- **Published**: 2017-04-13 07:52:21+00:00
- **Updated**: 2017-04-13 07:52:21+00:00
- **Authors**: Maheen Rashid, Xiuye Gu, Yong Jae Lee
- **Comment**: CVPR 2017 Camera Ready
- **Journal**: None
- **Summary**: We present a method for localizing facial keypoints on animals by transferring knowledge gained from human faces. Instead of directly finetuning a network trained to detect keypoints on human faces to animal faces (which is sub-optimal since human and animal faces can look quite different), we propose to first adapt the animal images to the pre-trained human detection network by correcting for the differences in animal and human face shape. We first find the nearest human neighbors for each animal image using an unsupervised shape matching method. We use these matches to train a thin plate spline warping network to warp each animal face to look more human-like. The warping network is then jointly finetuned with a pre-trained human facial keypoint detection network using an animal dataset. We demonstrate state-of-the-art results on both horse and sheep facial keypoint detection, and significant improvement over simple finetuning, especially when training data is scarce. Additionally, we present a new dataset with 3717 images with horse face and facial keypoint annotations.



### Close Yet Distinctive Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1704.04235v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.04235v1)
- **Published**: 2017-04-13 08:30:21+00:00
- **Updated**: 2017-04-13 08:30:21+00:00
- **Authors**: Lingkun Luo, Xiaofang Wang, Shiqiang Hu, Chao Wang, Yuxing Tang, Liming Chen
- **Comment**: 11pages, 3 figures, ICCV2017
- **Journal**: None
- **Summary**: Domain adaptation is transfer learning which aims to generalize a learning model across training and testing data with different distributions. Most previous research tackle this problem in seeking a shared feature representation between source and target domains while reducing the mismatch of their data distributions. In this paper, we propose a close yet discriminative domain adaptation method, namely CDDA, which generates a latent feature representation with two interesting properties. First, the discrepancy between the source and target domain, measured in terms of both marginal and conditional probability distribution via Maximum Mean Discrepancy is minimized so as to attract two domains close to each other. More importantly, we also design a repulsive force term, which maximizes the distances between each label dependent sub-domain to all others so as to drag different class dependent sub-domains far away from each other and thereby increase the discriminative power of the adapted domain. Moreover, given the fact that the underlying data manifold could have complex geometric structure, we further propose the constraints of label smoothness and geometric structure consistency for label propagation. Extensive experiments are conducted on 36 cross-domain image classification tasks over four public datasets. The comprehensive results show that the proposed method consistently outperforms the state-of-the-art methods with significant margins.



### Zero-order Reverse Filtering
- **Arxiv ID**: http://arxiv.org/abs/1704.04037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04037v1)
- **Published**: 2017-04-13 09:01:43+00:00
- **Updated**: 2017-04-13 09:01:43+00:00
- **Authors**: Xin Tao, Chao Zhou, Xiaoyong Shen, Jue Wang, Jiaya Jia
- **Comment**: 9 pages, submitted to conference
- **Journal**: None
- **Summary**: In this paper, we study an unconventional but practically meaningful reversibility problem of commonly used image filters. We broadly define filters as operations to smooth images or to produce layers via global or local algorithms. And we raise the intriguingly problem if they are reservable to the status before filtering. To answer it, we present a novel strategy to understand general filter via contraction mappings on a metric space. A very simple yet effective zero-order algorithm is proposed. It is able to practically reverse most filters with low computational cost. We present quite a few experiments in the paper and supplementary file to thoroughly verify its performance. This method can also be generalized to solve other inverse problems and enables new applications.



### Saliency-guided Adaptive Seeding for Supervoxel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.04054v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04054v2)
- **Published**: 2017-04-13 09:43:09+00:00
- **Updated**: 2017-10-19 20:02:15+00:00
- **Authors**: Ge Gao, Mikko Lauri, Jianwei Zhang, Simone Frintrop
- **Comment**: 6 pages, accepted to IROS2017
- **Journal**: None
- **Summary**: We propose a new saliency-guided method for generating supervoxels in 3D space. Rather than using an evenly distributed spatial seeding procedure, our method uses visual saliency to guide the process of supervoxel generation. This results in densely distributed, small, and precise supervoxels in salient regions which often contain objects, and larger supervoxels in less salient regions that often correspond to background. Our approach largely improves the quality of the resulting supervoxel segmentation in terms of boundary recall and under-segmentation error on publicly available benchmarks.



### Land Cover Classification via Multi-temporal Spatial Data by Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.04055v1
- **DOI**: 10.1109/LGRS.2017.2728698
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.04055v1)
- **Published**: 2017-04-13 09:47:12+00:00
- **Updated**: 2017-04-13 09:47:12+00:00
- **Authors**: Dino Ienco, Raffaele Gaetano, Claire Dupaquier, Pierre Maurel
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, modern earth observation programs produce huge volumes of satellite images time series (SITS) that can be useful to monitor geographical areas through time. How to efficiently analyze such kind of information is still an open question in the remote sensing field. Recently, deep learning methods proved suitable to deal with remote sensing data mainly for scene classification (i.e. Convolutional Neural Networks - CNNs - on single images) while only very few studies exist involving temporal deep learning approaches (i.e Recurrent Neural Networks - RNNs) to deal with remote sensing time series. In this letter we evaluate the ability of Recurrent Neural Networks, in particular the Long-Short Term Memory (LSTM) model, to perform land cover classification considering multi-temporal spatial data derived from a time series of satellite images. We carried out experiments on two different datasets considering both pixel-based and object-based classification. The obtained results show that Recurrent Neural Networks are competitive compared to state-of-the-art classifiers, and may outperform classical approaches in presence of low represented and/or highly mixed classes. We also show that using the alternative feature representation generated by LSTM can improve the performances of standard classifiers.



### DCFNet: Discriminant Correlation Filters Network for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1704.04057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04057v1)
- **Published**: 2017-04-13 10:08:14+00:00
- **Updated**: 2017-04-13 10:08:14+00:00
- **Authors**: Qiang Wang, Jin Gao, Junliang Xing, Mengdan Zhang, Weiming Hu
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Discriminant Correlation Filters (DCF) based methods now become a kind of dominant approach to online object tracking. The features used in these methods, however, are either based on hand-crafted features like HoGs, or convolutional features trained independently from other tasks like image classification. In this work, we present an end-to-end lightweight network architecture, namely DCFNet, to learn the convolutional features and perform the correlation tracking process simultaneously. Specifically, we treat DCF as a special correlation filter layer added in a Siamese network, and carefully derive the backpropagation through it by defining the network output as the probability heatmap of object location. Since the derivation is still carried out in Fourier frequency domain, the efficiency property of DCF is preserved. This enables our tracker to run at more than 60 FPS during test time, while achieving a significant accuracy gain compared with KCF using HoGs. Extensive evaluations on OTB-2013, OTB-2015, and VOT2015 benchmarks demonstrate that the proposed DCFNet tracker is competitive with several state-of-the-art trackers, while being more compact and much faster.



### Learning to Estimate Pose by Watching Videos
- **Arxiv ID**: http://arxiv.org/abs/1704.04081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04081v1)
- **Published**: 2017-04-13 11:54:53+00:00
- **Updated**: 2017-04-13 11:54:53+00:00
- **Authors**: Prabuddha Chakraborty, Vinay P. Namboodiri
- **Comment**: 11 pages, 8 figures, under review
- **Journal**: None
- **Summary**: In this paper we propose a technique for obtaining coarse pose estimation of humans in an image that does not require any manual supervision. While a general unsupervised technique would fail to estimate human pose, we suggest that sufficient information about coarse pose can be obtained by observing human motion in multiple frames. Specifically, we consider obtaining surrogate supervision through videos as a means for obtaining motion based grouping cues. We supplement the method using a basic object detector that detects persons. With just these components we obtain a rough estimate of the human pose.   With these samples for training, we train a fully convolutional neural network (FCNN)[20] to obtain accurate dense blob based pose estimation. We show that the results obtained are close to the ground-truth and to the results obtained using a fully supervised convolutional pose estimation method [31] as evaluated on a challenging dataset [15]. This is further validated by evaluating the obtained poses using a pose based action recognition method [5]. In this setting we outperform the results as obtained using the baseline method that uses a fully supervised pose estimation algorithm and is competitive with a new baseline created using convolutional pose estimation with full supervision.



### Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1704.04086v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04086v2)
- **Published**: 2017-04-13 12:18:13+00:00
- **Updated**: 2017-08-04 03:44:37+00:00
- **Authors**: Rui Huang, Shu Zhang, Tianyu Li, Ran He
- **Comment**: accepted at ICCV 2017, main paper & supplementary material, 11 pages
- **Journal**: None
- **Summary**: Photorealistic frontal view synthesis from a single face image has a wide range of applications in the field of face recognition. Although data-driven deep learning methods have been proposed to address this problem by seeking solutions from ample face data, this problem is still challenging because it is intrinsically ill-posed. This paper proposes a Two-Pathway Generative Adversarial Network (TP-GAN) for photorealistic frontal view synthesis by simultaneously perceiving global structures and local details. Four landmark located patch networks are proposed to attend to local textures in addition to the commonly used global encoder-decoder network. Except for the novel architecture, we make this ill-posed problem well constrained by introducing a combination of adversarial loss, symmetry loss and identity preserving loss. The combined loss function leverages both frontal face distribution and pre-trained discriminative deep face models to guide an identity preserving inference of frontal views from profiles. Different from previous deep learning methods that mainly rely on intermediate features for recognition, our method directly leverages the synthesized identity preserving image for downstream tasks like face recognition and attribution estimation. Experimental results demonstrate that our method not only presents compelling perceptual results but also outperforms state-of-the-art results on large pose face recognition.



### Recognizing Activities of Daily Living from Egocentric Images
- **Arxiv ID**: http://arxiv.org/abs/1704.04097v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04097v1)
- **Published**: 2017-04-13 12:49:28+00:00
- **Updated**: 2017-04-13 12:49:28+00:00
- **Authors**: Alejandro Cartas, Juan Marín, Petia Radeva, Mariella Dimiccoli
- **Comment**: To appear in the Proceedings of IbPRIA 2017
- **Journal**: None
- **Summary**: Recognizing Activities of Daily Living (ADLs) has a large number of health applications, such as characterize lifestyle for habit improvement, nursing and rehabilitation services. Wearable cameras can daily gather large amounts of image data that provide rich visual information about ADLs than using other wearable sensors. In this paper, we explore the classification of ADLs from images captured by low temporal resolution wearable camera (2fpm) by using a Convolutional Neural Networks (CNN) approach. We show that the classification accuracy of a CNN largely improves when its output is combined, through a random decision forest, with contextual information from a fully connected layer. The proposed method was tested on a subset of the NTCIR-12 egocentric dataset, consisting of 18,674 images and achieved an overall accuracy of 86% activity recognition on 21 classes.



### Single Image Super-Resolution based on Wiener Filter in Similarity Domain
- **Arxiv ID**: http://arxiv.org/abs/1704.04126v3
- **DOI**: 10.1109/TIP.2017.2779265
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04126v3)
- **Published**: 2017-04-13 13:38:36+00:00
- **Updated**: 2017-11-29 14:08:09+00:00
- **Authors**: Cristóvão Cruz, Rakesh Mehta, Vladimir Katkovnik, Karen Egiazarian
- **Comment**: Paper accepted for publication on IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: Single image super resolution (SISR) is an ill-posed problem aiming at estimating a plausible high resolution (HR) image from a single low resolution (LR) image. Current state-of-the-art SISR methods are patch-based. They use either external data or internal self-similarity to learn a prior for a HR image. External data based methods utilize large number of patches from the training data, while self-similarity based approaches leverage one or more similar patches from the input image. In this paper we propose a self-similarity based approach that is able to use large groups of similar patches extracted from the input image to solve the SISR problem. We introduce a novel prior leading to collaborative filtering of patch groups in 1D similarity domain and couple it with an iterative back-projection framework. The performance of the proposed algorithm is evaluated on a number of SISR benchmark datasets. Without using any external data, the proposed approach outperforms the current non-CNN based methods on the tested datasets for various scaling factors. On certain datasets, the gain is over 1 dB, when compared to the recent method A+. For high sampling rate (x4) the proposed method performs similarly to very recent state-of-the-art deep convolutional network based approaches.



### Neural Face Editing with Intrinsic Image Disentangling
- **Arxiv ID**: http://arxiv.org/abs/1704.04131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04131v1)
- **Published**: 2017-04-13 13:42:20+00:00
- **Updated**: 2017-04-13 13:42:20+00:00
- **Authors**: Zhixin Shu, Ersin Yumer, Sunil Hadap, Kalyan Sunkavalli, Eli Shechtman, Dimitris Samaras
- **Comment**: CVPR 2017 oral
- **Journal**: None
- **Summary**: Traditional face editing methods often require a number of sophisticated and task specific algorithms to be applied one after the other --- a process that is tedious, fragile, and computationally intensive. In this paper, we propose an end-to-end generative adversarial network that infers a face-specific disentangled representation of intrinsic face properties, including shape (i.e. normals), albedo, and lighting, and an alpha matte. We show that this network can be trained on "in-the-wild" images by incorporating an in-network physically-based image formation module and appropriate loss functions. Our disentangling latent representation allows for semantically relevant edits, where one aspect of facial appearance can be manipulated while keeping orthogonal properties fixed, and we demonstrate its use for a number of facial editing applications.



### Explaining the Unexplained: A CLass-Enhanced Attentive Response (CLEAR) Approach to Understanding Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.04133v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1704.04133v2)
- **Published**: 2017-04-13 13:44:33+00:00
- **Updated**: 2017-05-18 18:38:06+00:00
- **Authors**: Devinder Kumar, Alexander Wong, Graham W. Taylor
- **Comment**: Accepted at Computer Vision and Patter Recognition Workshop (CVPR-W)
  on Explainable Computer Vision, 2017
- **Journal**: None
- **Summary**: In this work, we propose CLass-Enhanced Attentive Response (CLEAR): an approach to visualize and understand the decisions made by deep neural networks (DNNs) given a specific input. CLEAR facilitates the visualization of attentive regions and levels of interest of DNNs during the decision-making process. It also enables the visualization of the most dominant classes associated with these attentive regions of interest. As such, CLEAR can mitigate some of the shortcomings of heatmap-based methods associated with decision ambiguity, and allows for better insights into the decision-making process of DNNs. Quantitative and qualitative experiments across three different datasets demonstrate the efficacy of CLEAR for gaining a better understanding of the inner workings of DNNs during the decision-making process.



### A Procedural Texture Generation Framework Based on Semantic Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1704.04141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04141v1)
- **Published**: 2017-04-13 14:02:38+00:00
- **Updated**: 2017-04-13 14:02:38+00:00
- **Authors**: Junyu Dong, Lina Wang, Jun Liu, Xin Sun
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: Procedural textures are normally generated from mathematical models with parameters carefully selected by experienced users. However, for naive users, the intuitive way to obtain a desired texture is to provide semantic descriptions such as "regular," "lacelike," and "repetitive" and then a procedural model with proper parameters will be automatically suggested to generate the corresponding textures. By contrast, it is less practical for users to learn mathematical models and tune parameters based on multiple examinations of large numbers of generated textures. In this study, we propose a novel framework that generates procedural textures according to user-defined semantic descriptions, and we establish a mapping between procedural models and semantic texture descriptions. First, based on a vocabulary of semantic attributes collected from psychophysical experiments, a multi-label learning method is employed to annotate a large number of textures with semantic attributes to form a semantic procedural texture dataset. Then, we derive a low dimensional semantic space in which the semantic descriptions can be separated from one other. Finally, given a set of semantic descriptions, the diverse properties of the samples in the semantic space can lead the framework to find an appropriate generation model that uses appropriate parameters to produce a desired texture. The experimental results show that the proposed framework is effective and that the generated textures closely correlate with the input semantic descriptions.



### Video Acceleration Magnification
- **Arxiv ID**: http://arxiv.org/abs/1704.04186v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04186v2)
- **Published**: 2017-04-13 15:55:19+00:00
- **Updated**: 2017-04-22 19:53:52+00:00
- **Authors**: Yichao Zhang, Silvia L. Pintea, Jan C. van Gemert
- **Comment**: Accepted paper at CVPR 2017. Project webpage:
  http://acceleration-magnification.github.io/
- **Journal**: None
- **Summary**: The ability to amplify or reduce subtle image changes over time is useful in contexts such as video editing, medical video analysis, product quality control and sports. In these contexts there is often large motion present which severely distorts current video amplification methods that magnify change linearly. In this work we propose a method to cope with large motions while still magnifying small changes. We make the following two observations: i) large motions are linear on the temporal scale of the small changes; ii) small changes deviate from this linearity. We ignore linear motion and propose to magnify acceleration. Our method is pure Eulerian and does not require any optical flow, temporal alignment or region annotations. We link temporal second-order derivative filtering to spatial acceleration magnification. We apply our method to moving objects where we show motion magnification and color magnification. We provide quantitative as well as qualitative evidence for our method while comparing to the state-of-the-art.



### Spatial Memory for Context Reasoning in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.04224v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04224v1)
- **Published**: 2017-04-13 17:47:03+00:00
- **Updated**: 2017-04-13 17:47:03+00:00
- **Authors**: Xinlei Chen, Abhinav Gupta
- **Comment**: Draft submitted to ICCV 2017
- **Journal**: None
- **Summary**: Modeling instance-level context and object-object relationships is extremely challenging. It requires reasoning about bounding boxes of different classes, locations \etc. Above all, instance-level spatial reasoning inherently requires modeling conditional distributions on previous detections. Unfortunately, our current object detection systems do not have any {\bf memory} to remember what to condition on! The state-of-the-art object detectors still detect all object in parallel followed by non-maximal suppression (NMS). While memory has been used for tasks such as captioning, they mostly use image-level memory cells without capturing the spatial layout. On the other hand, modeling object-object relationships requires {\bf spatial} reasoning -- not only do we need a memory to store the spatial layout, but also a effective reasoning module to extract spatial patterns. This paper presents a conceptually simple yet powerful solution -- Spatial Memory Network (SMN), to model the instance-level context efficiently and effectively. Our spatial memory essentially assembles object instances back into a pseudo "image" representation that is easy to be fed into another ConvNet for object-object context reasoning. This leads to a new sequential reasoning architecture where image and memory are processed in parallel to obtain detections which update the memory again. We show our SMN direction is promising as it provides 2.2\% improvement over baseline Faster RCNN on the COCO dataset so far.



### Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-supervised Object and Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1704.04232v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04232v2)
- **Published**: 2017-04-13 17:59:31+00:00
- **Updated**: 2017-12-23 10:43:54+00:00
- **Authors**: Krishna Kumar Singh, Yong Jae Lee
- **Comment**: Camera-Ready Version (ICCV 2017)
- **Journal**: None
- **Summary**: We propose `Hide-and-Seek', a weakly-supervised framework that aims to improve object localization in images and action localization in videos. Most existing weakly-supervised methods localize only the most discriminative parts of an object rather than all relevant parts, which leads to suboptimal performance. Our key idea is to hide patches in a training image randomly, forcing the network to seek other relevant parts when the most discriminative part is hidden. Our approach only needs to modify the input image and can work with any network designed for object localization. During testing, we do not need to hide any patches. Our Hide-and-Seek approach obtains superior performance compared to previous methods for weakly-supervised object localization on the ILSVRC dataset. We also demonstrate that our framework can be easily extended to weakly-supervised action localization.



### Visual Recognition of Paper Analytical Device Images for Detection of Falsified Pharmaceuticals
- **Arxiv ID**: http://arxiv.org/abs/1704.04251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04251v1)
- **Published**: 2017-04-13 18:35:33+00:00
- **Updated**: 2017-04-13 18:35:33+00:00
- **Authors**: Sandipan Banerjee, James Sweet, Christopher Sweet, Marya Lieberman
- **Comment**: in Proc. IEEE Winter Conference on Applications of Computer Vision
  (WACV), 2016
- **Journal**: None
- **Summary**: Falsification of medicines is a big problem in many developing countries, where technological infrastructure is inadequate to detect these harmful products. We have developed a set of inexpensive paper cards, called Paper Analytical Devices (PADs), which can efficiently classify drugs based on their chemical composition, as a potential solution to the problem. These cards have different reagents embedded in them which produce a set of distinctive color descriptors upon reacting with the chemical compounds that constitute pharmaceutical dosage forms. If a falsified version of the medicine lacks the active ingredient or includes substitute fillers, the difference in color is perceivable by humans. However, reading the cards with accuracy takes training and practice, which may hamper their scaling and implementation in low resource settings. To deal with this, we have developed an automatic visual recognition system to read the results from the PAD images. At first, the optimal set of reagents was found by running singular value decomposition on the intensity values of the color tones in the card images. A dataset of cards embedded with these reagents is produced to generate the most distinctive results for a set of 26 different active pharmaceutical ingredients (APIs) and excipients. Then, we train two popular convolutional neural network (CNN) models, with the card images. We also extract some "hand-crafted" features from the images and train a nearest neighbor classifier and a non-linear support vector machine with them. On testing, higher-level features performed much better in accurately classifying the PAD images, with the CNN models reaching the highest average accuracy of over 94\%.



### Applying High-Resolution Visible Imagery to Satellite Melt Pond Fraction Retrieval: A Neural Network Approach
- **Arxiv ID**: http://arxiv.org/abs/1704.04281v1
- **DOI**: None
- **Categories**: **physics.ao-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.04281v1)
- **Published**: 2017-04-13 21:21:57+00:00
- **Updated**: 2017-04-13 21:21:57+00:00
- **Authors**: Qi Liu, Yawen Zhang, Qin Lv, Li Shang
- **Comment**: None
- **Journal**: None
- **Summary**: During summer, melt ponds have a significant influence on Arctic sea-ice albedo. The melt pond fraction (MPF) also has the ability to forecast the Arctic sea-ice in a certain period. It is important to retrieve accurate melt pond fraction (MPF) from satellite data for Arctic research. This paper proposes a satellite MPF retrieval model based on the multi-layer neural network, named MPF-NN. Our model uses multi-spectral satellite data as model input and MPF information from multi-site and multi-period visible imagery as prior knowledge for modeling. It can effectively model melt ponds evolution of different regions and periods over the Arctic. Evaluation results show that the MPF retrieved from MODIS data using the proposed model has an RMSE of 3.91% and a correlation coefficient of 0.73. The seasonal distribution of MPF is also consistent with previous results.



### FastVentricle: Cardiac Segmentation with ENet
- **Arxiv ID**: http://arxiv.org/abs/1704.04296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04296v1)
- **Published**: 2017-04-13 22:57:54+00:00
- **Updated**: 2017-04-13 22:57:54+00:00
- **Authors**: Jesse Lieman-Sifry, Matthieu Le, Felix Lau, Sean Sall, Daniel Golden
- **Comment**: 11 pages, 6 figures, Accepted to Functional Imaging and Modeling of
  the Heart (FIMH) 2017
- **Journal**: None
- **Summary**: Cardiac Magnetic Resonance (CMR) imaging is commonly used to assess cardiac structure and function. One disadvantage of CMR is that post-processing of exams is tedious. Without automation, precise assessment of cardiac function via CMR typically requires an annotator to spend tens of minutes per case manually contouring ventricular structures. Automatic contouring can lower the required time per patient by generating contour suggestions that can be lightly modified by the annotator. Fully convolutional networks (FCNs), a variant of convolutional neural networks, have been used to rapidly advance the state-of-the-art in automated segmentation, which makes FCNs a natural choice for ventricular segmentation. However, FCNs are limited by their computational cost, which increases the monetary cost and degrades the user experience of production systems. To combat this shortcoming, we have developed the FastVentricle architecture, an FCN architecture for ventricular segmentation based on the recently developed ENet architecture. FastVentricle is 4x faster and runs with 6x less memory than the previous state-of-the-art ventricular segmentation architecture while still maintaining excellent clinical accuracy.



