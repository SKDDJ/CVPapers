# Arxiv Papers in cs.CV on 2017-04-22
### On Face Segmentation, Face Swapping, and Face Perception
- **Arxiv ID**: http://arxiv.org/abs/1704.06729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06729v1)
- **Published**: 2017-04-22 00:50:51+00:00
- **Updated**: 2017-04-22 00:50:51+00:00
- **Authors**: Yuval Nirkin, Iacopo Masi, Anh Tuan Tran, Tal Hassner, Gerard Medioni
- **Comment**: None
- **Journal**: None
- **Summary**: We show that even when face images are unconstrained and arbitrarily paired, face swapping between them is actually quite simple. To this end, we make the following contributions. (a) Instead of tailoring systems for face segmentation, as others previously proposed, we show that a standard fully convolutional network (FCN) can achieve remarkably fast and accurate segmentations, provided that it is trained on a rich enough example set. For this purpose, we describe novel data collection and generation routines which provide challenging segmented face examples. (b) We use our segmentations to enable robust face swapping under unprecedented conditions. (c) Unlike previous work, our swapping is robust enough to allow for extensive quantitative tests. To this end, we use the Labeled Faces in the Wild (LFW) benchmark and measure the effect of intra- and inter-subject face swapping on recognition. We show that our intra-subject swapped faces remain as recognizable as their sources, testifying to the effectiveness of our method. In line with well known perceptual studies, we show that better face swapping produces less recognizable inter-subject results. This is the first time this effect was quantitatively demonstrated for machine vision systems.



### Robust, Deep and Inductive Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.06743v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.06743v3)
- **Published**: 2017-04-22 04:12:24+00:00
- **Updated**: 2017-07-30 08:47:45+00:00
- **Authors**: Raghavendra Chalapathy, Aditya Krishna Menon, Sanjay Chawla
- **Comment**: Accepted ECML PKDD 2017 Skopje, Macedonia 18-22 September the
  European Conference On Machine Learning & Principles and Practice of
  Knowledge Discovery
- **Journal**: None
- **Summary**: PCA is a classical statistical technique whose simplicity and maturity has seen it find widespread use as an anomaly detection technique. However, it is limited in this regard by being sensitive to gross perturbations of the input, and by seeking a linear subspace that captures normal behaviour. The first issue has been dealt with by robust PCA, a variant of PCA that explicitly allows for some data points to be arbitrarily corrupted, however, this does not resolve the second issue, and indeed introduces the new issue that one can no longer inductively find anomalies on a test set. This paper addresses both issues in a single model, the robust autoencoder. This method learns a nonlinear subspace that captures the majority of data points, while allowing for some data to have arbitrary corruption. The model is simple to train and leverages recent advances in the optimisation of deep neural networks. Experiments on a range of real-world datasets highlight the model's effectiveness.



### ScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1704.06752v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06752v1)
- **Published**: 2017-04-22 06:05:31+00:00
- **Updated**: 2017-04-22 06:05:31+00:00
- **Authors**: Siyuan Qiao, Wei Shen, Weichao Qiu, Chenxi Liu, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Motivated by product detection in supermarkets, this paper studies the problem of object proposal generation in supermarket images and other natural images. We argue that estimation of object scales in images is helpful for generating object proposals, especially for supermarket images where object scales are usually within a small range. Therefore, we propose to estimate object scales of images before generating object proposals. The proposed method for predicting object scales is called ScaleNet. To validate the effectiveness of ScaleNet, we build three supermarket datasets, two of which are real-world datasets used for testing and the other one is a synthetic dataset used for training. In short, we extend the previous state-of-the-art object proposal methods by adding a scale prediction phase. The resulted method outperforms the previous state-of-the-art on the supermarket datasets by a large margin. We also show that the approach works for object proposal on other natural images and it outperforms the previous state-of-the-art object proposal methods on the MS COCO dataset. The supermarket datasets, the virtual supermarkets, and the tools for creating more synthetic datasets will be made public.



### Convolutional Neural Networks for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.06756v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06756v1)
- **Published**: 2017-04-22 06:27:56+00:00
- **Updated**: 2017-04-22 06:27:56+00:00
- **Authors**: Shima Alizadeh, Azar Fazel
- **Comment**: None
- **Journal**: None
- **Summary**: We have developed convolutional neural networks (CNN) for a facial expression recognition task. The goal is to classify each facial image into one of the seven facial emotion categories considered in this study. We trained CNN models with different depth using gray-scale images. We developed our models in Torch and exploited Graphics Processing Unit (GPU) computation in order to expedite the training process. In addition to the networks performing based on raw pixel data, we employed a hybrid feature strategy by which we trained a novel CNN model with the combination of raw pixel data and Histogram of Oriented Gradients (HOG) features. To reduce the overfitting of the models, we utilized different techniques including dropout and batch normalization in addition to L2 regularization. We applied cross validation to determine the optimal hyper-parameters and evaluated the performance of the developed models by looking at their training histories. We also present the visualization of different layers of a network to show what features of a face can be learned by CNN models.



### Content-Based Video-Music Retrieval Using Soft Intra-Modal Structure Constraint
- **Arxiv ID**: http://arxiv.org/abs/1704.06761v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06761v2)
- **Published**: 2017-04-22 07:40:16+00:00
- **Updated**: 2017-09-01 14:08:06+00:00
- **Authors**: Sungeun Hong, Woobin Im, Hyun S. Yang
- **Comment**: 13 pages, 9 figures, 4 tables, supplementary material link >>
  https://youtu.be/ZyINqDMo3Fg
- **Journal**: None
- **Summary**: Up to now, only limited research has been conducted on cross-modal retrieval of suitable music for a specified video or vice versa. Moreover, much of the existing research relies on metadata such as keywords, tags, or associated description that must be individually produced and attached posterior. This paper introduces a new content-based, cross-modal retrieval method for video and music that is implemented through deep neural networks. We train the network via inter-modal ranking loss such that videos and music with similar semantics end up close together in the embedding space. However, if only the inter-modal ranking constraint is used for embedding, modality-specific characteristics can be lost. To address this problem, we propose a novel soft intra-modal structure loss that leverages the relative distance relationship between intra-modal samples before embedding. We also introduce reasonable quantitative and qualitative experimental protocols to solve the lack of standard protocols for less-mature video-music related tasks. Finally, we construct a large-scale 200K video-music pair benchmark. All the datasets and source code can be found in our online repository (https://github.com/csehong/VM-NET).



### Deep Learning based Isolated Arabic Scene Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.06821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06821v1)
- **Published**: 2017-04-22 17:09:02+00:00
- **Updated**: 2017-04-22 17:09:02+00:00
- **Authors**: Saad Bin Ahmed, Saeeda Naz, Muhammad Imran Razzak, Rubiyah Yousaf
- **Comment**: 6 pages, 8 Figures, 3 Tables, Accepted in IEEE Workshop on Arabic
  Script Analysis and Recognition (ASAR) 2017
- **Journal**: None
- **Summary**: The technological advancement and sophistication in cameras and gadgets prompt researchers to have focus on image analysis and text understanding. The deep learning techniques demonstrated well to assess the potential for classifying text from natural scene images as reported in recent years. There are variety of deep learning approaches that prospects the detection and recognition of text, effectively from images. In this work, we presented Arabic scene text recognition using Convolutional Neural Networks (ConvNets) as a deep learning classifier. As the scene text data is slanted and skewed, thus to deal with maximum variations, we employ five orientations with respect to single occurrence of a character. The training is formulated by keeping filter size 3 x 3 and 5 x 5 with stride value as 1 and 2. During text classification phase, we trained network with distinct learning rates. Our approach reported encouraging results on recognition of Arabic characters from segmented Arabic scene images.



### Deep Learning for Medical Image Processing: Overview, Challenges and Future
- **Arxiv ID**: http://arxiv.org/abs/1704.06825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06825v1)
- **Published**: 2017-04-22 17:49:04+00:00
- **Updated**: 2017-04-22 17:49:04+00:00
- **Authors**: Muhammad Imran Razzak, Saeeda Naz, Ahmad Zaib
- **Comment**: None
- **Journal**: None
- **Summary**: Healthcare sector is totally different from other industry. It is on high priority sector and people expect highest level of care and services regardless of cost. It did not achieve social expectation even though it consume huge percentage of budget. Mostly the interpretations of medical data is being done by medical expert. In terms of image interpretation by human expert, it is quite limited due to its subjectivity, the complexity of the image, extensive variations exist across different interpreters, and fatigue. After the success of deep learning in other real world application, it is also providing exciting solutions with good accuracy for medical imaging and is seen as a key method for future applications in health secotr. In this chapter, we discussed state of the art deep learning architecture and its optimization used for medical image segmentation and classification. In the last section, we have discussed the challenges deep learning based methods for medical imaging and open research issue.



### On the Two-View Geometry of Unsynchronized Cameras
- **Arxiv ID**: http://arxiv.org/abs/1704.06843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06843v1)
- **Published**: 2017-04-22 19:45:46+00:00
- **Updated**: 2017-04-22 19:45:46+00:00
- **Authors**: Cenek Albl, Zuzana Kukelova, Andrew Fitzgibbon, Jan Heller, Matej Smid, Tomas Pajdla
- **Comment**: 12 pages, 9 figures, Computer Vision and Pattern Recognition (CVPR)
  2017
- **Journal**: None
- **Summary**: We present new methods for simultaneously estimating camera geometry and time shift from video sequences from multiple unsynchronized cameras. Algorithms for simultaneous computation of a fundamental matrix or a homography with unknown time shift between images are developed. Our methods use minimal correspondence sets (eight for fundamental matrix and four and a half for homography) and therefore are suitable for robust estimation using RANSAC. Furthermore, we present an iterative algorithm that extends the applicability on sequences which are significantly unsynchronized, finding the correct time shift up to several seconds. We evaluated the methods on synthetic and wide range of real world datasets and the results show a broad applicability to the problem of camera synchronization.



### A Review on Deep Learning Techniques Applied to Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.06857v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1704.06857v1)
- **Published**: 2017-04-22 23:37:43+00:00
- **Updated**: 2017-04-22 23:37:43+00:00
- **Authors**: Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu Oprea, Victor Villena-Martinez, Jose Garcia-Rodriguez
- **Comment**: Submitted to TPAMI on Apr. 22, 2017
- **Journal**: None
- **Summary**: Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.



