# Arxiv Papers in cs.CV on 2017-04-26
### Spatio-temporal Person Retrieval via Natural Language Queries
- **Arxiv ID**: http://arxiv.org/abs/1704.07945v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07945v2)
- **Published**: 2017-04-26 02:26:01+00:00
- **Updated**: 2017-08-22 21:16:27+00:00
- **Authors**: Masataka Yamaguchi, Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: Accepted to ICCV2017
- **Journal**: None
- **Summary**: In this paper, we address the problem of spatio-temporal person retrieval from multiple videos using a natural language query, in which we output a tube (i.e., a sequence of bounding boxes) which encloses the person described by the query. For this problem, we introduce a novel dataset consisting of videos containing people annotated with bounding boxes for each second and with five natural language descriptions. To retrieve the tube of the person described by a given natural language query, we design a model that combines methods for spatio-temporal human detection and multimodal retrieval. We conduct comprehensive experiments to compare a variety of tube and text representations and multimodal retrieval methods, and present a strong baseline in this task as well as demonstrate the efficacy of our tube representation and multimodal feature embedding technique. Finally, we demonstrate the versatility of our model by applying it to two other important tasks.



### Unsupervised Clustering and Active Learning of Hyperspectral Images with Nonlinear Diffusion
- **Arxiv ID**: http://arxiv.org/abs/1704.07961v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07961v5)
- **Published**: 2017-04-26 03:11:21+00:00
- **Updated**: 2018-10-15 21:36:25+00:00
- **Authors**: James M. Murphy, Mauro Maggioni
- **Comment**: 17 pages, 22 figures, 3 tables. IEEE accepted version
- **Journal**: None
- **Summary**: The problem of unsupervised learning and segmentation of hyperspectral images is a significant challenge in remote sensing. The high dimensionality of hyperspectral data, presence of substantial noise, and overlap of classes all contribute to the difficulty of automatically clustering and segmenting hyperspectral images. We propose an unsupervised learning technique called spectral-spatial diffusion learning (DLSS) that combines a geometric estimation of class modes with a diffusion-inspired labeling that incorporates both spectral and spatial information. The mode estimation incorporates the geometry of the hyperspectral data by using diffusion distance to promote learning a unique mode from each class. These class modes are then used to label all points by a joint spectral-spatial nonlinear diffusion process. A related variation of DLSS is also discussed, which enables active learning by requesting labels for a very small number of well-chosen pixels, dramatically boosting overall clustering results. Extensive experimental analysis demonstrates the efficacy of the proposed methods against benchmark and state-of-the-art hyperspectral analysis techniques on a variety of real datasets, their robustness to choices of parameters, and their low computational complexity.



### Anisotropic twicing for single particle reconstruction using autocorrelation analysis
- **Arxiv ID**: http://arxiv.org/abs/1704.07969v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.BM, stat.AP, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1704.07969v1)
- **Published**: 2017-04-26 04:47:01+00:00
- **Updated**: 2017-04-26 04:47:01+00:00
- **Authors**: Tejal Bhamre, Teng Zhang, Amit Singer
- **Comment**: None
- **Journal**: None
- **Summary**: The missing phase problem in X-ray crystallography is commonly solved using the technique of molecular replacement, which borrows phases from a previously solved homologous structure, and appends them to the measured Fourier magnitudes of the diffraction patterns of the unknown structure. More recently, molecular replacement has been proposed for solving the missing orthogonal matrices problem arising in Kam's autocorrelation analysis for single particle reconstruction using X-ray free electron lasers and cryo-EM. In classical molecular replacement, it is common to estimate the magnitudes of the unknown structure as twice the measured magnitudes minus the magnitudes of the homologous structure, a procedure known as `twicing'. Mathematically, this is equivalent to finding an unbiased estimator for a complex-valued scalar. We generalize this scheme for the case of estimating real or complex valued matrices arising in single particle autocorrelation analysis. We name this approach "Anisotropic Twicing" because unlike the scalar case, the unbiased estimator is not obtained by a simple magnitude isotropic correction. We compare the performance of the least squares, twicing and anisotropic twicing estimators on synthetic and experimental datasets. We demonstrate 3D homology modeling in cryo-EM directly from experimental data without iterative refinement or class averaging, for the first time.



### Towards Estimating the Upper Bound of Visual-Speech Recognition: The Visual Lip-Reading Feasibility Database
- **Arxiv ID**: http://arxiv.org/abs/1704.08028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08028v1)
- **Published**: 2017-04-26 09:19:26+00:00
- **Updated**: 2017-04-26 09:19:26+00:00
- **Authors**: Adriana Fernandez-Lopez, Oriol Martinez, Federico M. Sukno
- **Comment**: IEEE International Conference on Automatic Face and Gesture
  Recognition
- **Journal**: None
- **Summary**: Speech is the most used communication method between humans and it involves the perception of auditory and visual channels. Automatic speech recognition focuses on interpreting the audio signals, although the video can provide information that is complementary to the audio. Exploiting the visual information, however, has proven challenging. On one hand, researchers have reported that the mapping between phonemes and visemes (visual units) is one-to-many because there are phonemes which are visually similar and indistinguishable between them. On the other hand, it is known that some people are very good lip-readers (e.g: deaf people). We study the limit of visual only speech recognition in controlled conditions. With this goal, we designed a new database in which the speakers are aware of being read and aim to facilitate lip-reading. In the literature, there are discrepancies on whether hearing-impaired people are better lip-readers than normal-hearing people. Then, we analyze if there are differences between the lip-reading abilities of 9 hearing-impaired and 15 normal-hearing people. Finally, human abilities are compared with the performance of a visual automatic speech recognition system. In our tests, hearing-impaired participants outperformed the normal-hearing participants but without reaching statistical significance. Human observers were able to decode 44% of the spoken message. In contrast, the visual only automatic system achieved 20% of word recognition rate. However, if we repeat the comparison in terms of phonemes both obtained very similar recognition rates, just above 50%. This suggests that the gap between human lip-reading and automatic speech-reading might be more related to the use of context than to the ability to interpret mouth appearance.



### Airway segmentation from 3D chest CT volumes based on volume of interest using gradient vector flow
- **Arxiv ID**: http://arxiv.org/abs/1704.08030v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08030v1)
- **Published**: 2017-04-26 09:27:18+00:00
- **Updated**: 2017-04-26 09:27:18+00:00
- **Authors**: Qier Meng, Takayuki Kitasaka, Masahiro Oda, Junji Ueno, Kensaku Mori
- **Comment**: None
- **Journal**: None
- **Summary**: Some lung diseases are related to bronchial airway structures and morphology. Although airway segmentation from chest CT volumes is an important task in the computer-aided diagnosis and surgery assistance systems for the chest, complete 3-D airway structure segmentation is a quite challenging task due to its complex tree-like structure. In this paper, we propose a new airway segmentation method from 3D chest CT volumes based on volume of interests (VOI) using gradient vector flow (GVF). This method segments the bronchial regions by applying the cavity enhancement filter (CEF) to trace the bronchial tree structure from the trachea. It uses the CEF in the VOI to segment each branch. And a tube-likeness function based on GVF and the GVF magnitude map in each VOI are utilized to assist predicting the positions and directions of child branches. By calculating the tube-likeness function based on GVF and the GVF magnitude map, the airway-like candidate structures are identified and their centrelines are extracted. Based on the extracted centrelines, we can detect the branch points of the bifurcations and directions of the airway branches in the next level. At the same time, a leakage detection is performed to avoid the leakage by analysing the pixel information and the shape information of airway candidate regions extracted in the VOI. Finally, we unify all of the extracted bronchial regions to form an integrated airway tree. Preliminary experiments using four cases of chest CT volumes demonstrated that the proposed method can extract more bronchial branches in comparison with other methods.



### Automatic Viseme Vocabulary Construction to Enhance Continuous Lip-reading
- **Arxiv ID**: http://arxiv.org/abs/1704.08035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08035v1)
- **Published**: 2017-04-26 09:34:59+00:00
- **Updated**: 2017-04-26 09:34:59+00:00
- **Authors**: Adriana Fernandez-Lopez, Federico M. Sukno
- **Comment**: International Conference on Computer Vision Theory and Applications
  (VISAPP 2017)
- **Journal**: None
- **Summary**: Speech is the most common communication method between humans and involves the perception of both auditory and visual channels. Automatic speech recognition focuses on interpreting the audio signals, but it has been demonstrated that video can provide information that is complementary to the audio. Thus, the study of automatic lip-reading is important and is still an open problem. One of the key challenges is the definition of the visual elementary units (the visemes) and their vocabulary. Many researchers have analyzed the importance of the phoneme to viseme mapping and have proposed viseme vocabularies with lengths between 11 and 15 visemes. These viseme vocabularies have usually been manually defined by their linguistic properties and in some cases using decision trees or clustering techniques. In this work, we focus on the automatic construction of an optimal viseme vocabulary based on the association of phonemes with similar appearance. To this end, we construct an automatic system that uses local appearance descriptors to extract the main characteristics of the mouth region and HMMs to model the statistic relations of both viseme and phoneme sequences. To compare the performance of the system different descriptors (PCA, DCT and SIFT) are analyzed. We test our system in a Spanish corpus of continuous speech. Our results indicate that we are able to recognize approximately 58% of the visemes, 47% of the phonemes and 23% of the words in a continuous speech scenario and that the optimal viseme vocabulary for Spanish is composed by 20 visemes.



### The loss surface of deep and wide neural networks
- **Arxiv ID**: http://arxiv.org/abs/1704.08045v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.08045v2)
- **Published**: 2017-04-26 10:24:54+00:00
- **Updated**: 2017-06-12 19:43:39+00:00
- **Authors**: Quynh Nguyen, Matthias Hein
- **Comment**: ICML 2017. Main results now hold for larger classes of loss functions
- **Journal**: None
- **Summary**: While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.



### SphereFace: Deep Hypersphere Embedding for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.08063v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08063v4)
- **Published**: 2017-04-26 11:37:22+00:00
- **Updated**: 2018-01-29 23:24:56+00:00
- **Authors**: Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, Le Song
- **Comment**: CVPR 2017 (v4: updated the Appendix)
- **Journal**: None
- **Summary**: This paper addresses deep face recognition (FR) problem under open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. However, few existing algorithms can effectively achieve this criterion. To this end, we propose the angular softmax (A-Softmax) loss that enables convolutional neural networks (CNNs) to learn angularly discriminative features. Geometrically, A-Softmax loss can be viewed as imposing discriminative constraints on a hypersphere manifold, which intrinsically matches the prior that faces also lie on a manifold. Moreover, the size of angular margin can be quantitatively adjusted by a parameter $m$. We further derive specific $m$ to approximate the ideal feature criterion. Extensive analysis and experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace Challenge show the superiority of A-Softmax loss in FR tasks. The code has also been made publicly available.



### AutoDIAL: Automatic DomaIn Alignment Layers
- **Arxiv ID**: http://arxiv.org/abs/1704.08082v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08082v3)
- **Published**: 2017-04-26 12:50:33+00:00
- **Updated**: 2017-11-27 19:10:40+00:00
- **Authors**: Fabio Maria Carlucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, Samuel Rota Bulò
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1702.06332
  added supplementary material
- **Journal**: None
- **Summary**: Classifiers trained on given databases perform poorly when tested on data acquired in different settings. This is explained in domain adaptation through a shift among distributions of the source and target domains. Attempts to align them have traditionally resulted in works reducing the domain shift by introducing appropriate loss terms, measuring the discrepancies between source and target distributions, in the objective function. Here we take a different route, proposing to align the learned representations by embedding in any given network specific Domain Alignment Layers, designed to match the source and target feature distributions to a reference one. Opposite to previous works which define a priori in which layers adaptation should be performed, our method is able to automatically learn the degree of feature alignment required at different levels of the deep network. Thorough experiments on different public benchmarks, in the unsupervised setting, confirm the power of our approach.



### A Faster Patch Ordering Method for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1704.08090v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/1704.08090v1)
- **Published**: 2017-04-26 13:06:56+00:00
- **Updated**: 2017-04-26 13:06:56+00:00
- **Authors**: Badre Munir
- **Comment**: 4 pages, 1 figure, 2 tables
- **Journal**: None
- **Summary**: Among the patch-based image denoising processing methods, smooth ordering of local patches (patch ordering) has been shown to give state-of-art results. For image denoising the patch ordering method forms two large TSPs (Traveling Salesman Problem) comprised of nodes in N-dimensional space. Ten approximate solutions of the two large TSPs are then used in a filtering process to form the reconstructed image. Use of large TSPs makes patch ordering a computationally intensive method. A modified patch ordering method for image denoising is proposed. In the proposed method, several smaller-sized TSPs are formed and the filtering process varied to work with solutions of these smaller TSPs. In terms of PSNR, denoising results of the proposed method differed by 0.032 dB to 0.016 dB on average. In original method, solving TSPs was observed to consume 85% of execution time. In proposed method, the time for solving TSPs can be reduced to half of the time required in original method. The proposed method can denoise images in 40% less time.



### Misdirected Registration Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1704.08121v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08121v2)
- **Published**: 2017-04-26 13:52:57+00:00
- **Updated**: 2017-05-17 21:53:36+00:00
- **Authors**: Jie Luo, Karteek Popuri, Dana Cobzas, Hongyi Ding, William M. Wells III, Masashi Sugiyama
- **Comment**: raw version
- **Journal**: None
- **Summary**: Being a task of establishing spatial correspondences, medical image registration is often formalized as finding the optimal transformation that best aligns two images. Since the transformation is such an essential component of registration, most existing researches conventionally quantify the registration uncertainty, which is the confidence in the estimated spatial correspondences, by the transformation uncertainty. In this paper, we give concrete examples and reveal that using the transformation uncertainty to quantify the registration uncertainty is inappropriate and sometimes misleading. Based on this finding, we also raise attention to an important yet subtle aspect of probabilistic image registration, that is whether it is reasonable to determine the correspondence of a registered voxel solely by the mode of its transformation distribution.



### Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1704.08134v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T01, 62H35, I.4.0; I.5.0
- **Links**: [PDF](http://arxiv.org/pdf/1704.08134v1)
- **Published**: 2017-04-26 14:22:02+00:00
- **Updated**: 2017-04-26 14:22:02+00:00
- **Authors**: Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou, Nigel Allinson, Xujiong Ye
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel learning based method for automated segmenta-tion of brain tumor in multimodal MRI images. The machine learned features from fully convolutional neural network (FCN) and hand-designed texton fea-tures are used to classify the MRI image voxels. The score map with pixel-wise predictions is used as a feature map which is learned from multimodal MRI train-ing dataset using the FCN. The learned features are then applied to random for-ests to classify each MRI image voxel into normal brain tissues and different parts of tumor. The method was evaluated on BRATS 2013 challenge dataset. The results show that the application of the random forest classifier to multimodal MRI images using machine-learned features based on FCN and hand-designed features based on textons provides promising segmentations. The Dice overlap measure for automatic brain tumor segmentation against ground truth is 0.88, 080 and 0.73 for complete tumor, core and enhancing tumor, respectively.



### Compact Descriptors for Video Analysis: the Emerging MPEG Standard
- **Arxiv ID**: http://arxiv.org/abs/1704.08141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08141v1)
- **Published**: 2017-04-26 14:33:24+00:00
- **Updated**: 2017-04-26 14:33:24+00:00
- **Authors**: Ling-Yu Duan, Vijay Chandrasekhar, Shiqi Wang, Yihang Lou, Jie Lin, Yan Bai, Tiejun Huang, Alex Chichung Kot, Wen Gao
- **Comment**: 4 figures, 4 tables
- **Journal**: None
- **Summary**: This paper provides an overview of the on-going compact descriptors for video analysis standard (CDVA) from the ISO/IEC moving pictures experts group (MPEG). MPEG-CDVA targets at defining a standardized bitstream syntax to enable interoperability in the context of video analysis applications. During the developments of MPEGCDVA, a series of techniques aiming to reduce the descriptor size and improve the video representation ability have been proposed. This article describes the new standard that is being developed and reports the performance of these key technical contributions.



### A Generalization of Convolutional Neural Networks to Graph-Structured Data
- **Arxiv ID**: http://arxiv.org/abs/1704.08165v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.08165v1)
- **Published**: 2017-04-26 15:37:50+00:00
- **Updated**: 2017-04-26 15:37:50+00:00
- **Authors**: Yotam Hechtlinger, Purvasha Chakravarti, Jining Qin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a generalization of Convolutional Neural Networks (CNNs) from low-dimensional grid data, such as images, to graph-structured data. We propose a novel spatial convolution utilizing a random walk to uncover the relations within the input, analogous to the way the standard convolution uses the spatial neighborhood of a pixel on the grid. The convolution has an intuitive interpretation, is efficient and scalable and can also be used on data with varying graph structure. Furthermore, this generalization can be applied to many standard regression or classification problems, by learning the the underlying graph. We empirically demonstrate the performance of the proposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular activity data set.



### New region force for variational models in image segmentation and high dimensional data clustering
- **Arxiv ID**: http://arxiv.org/abs/1704.08218v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08218v1)
- **Published**: 2017-04-26 17:11:00+00:00
- **Updated**: 2017-04-26 17:11:00+00:00
- **Authors**: Ke Wei, Ke Yin, Xue-Cheng Tai, Tony F. Chan
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an effective framework for multi-phase image segmentation and semi-supervised data clustering by introducing a novel region force term into the Potts model. Assume the probability that a pixel or a data point belongs to each class is known a priori. We show that the corresponding indicator function obeys the Bernoulli distribution and the new region force function can be computed as the negative log-likelihood function under the Bernoulli distribution. We solve the Potts model by the primal-dual hybrid gradient method and the augmented Lagrangian method, which are based on two different dual problems of the same primal problem. Empirical evaluations of the Potts model with the new region force function on benchmark problems show that it is competitive with existing variational methods in both image segmentation and semi-supervised data clustering.



### Punny Captions: Witty Wordplay in Image Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1704.08224v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.08224v2)
- **Published**: 2017-04-26 17:22:53+00:00
- **Updated**: 2018-05-31 17:45:50+00:00
- **Authors**: Arjun Chandrasekaran, Devi Parikh, Mohit Bansal
- **Comment**: NAACL 2018 (11 pages)
- **Journal**: None
- **Summary**: Wit is a form of rich interaction that is often grounded in a specific situation (e.g., a comment in response to an event). In this work, we attempt to build computational models that can produce witty descriptions for a given image. Inspired by a cognitive account of humor appreciation, we employ linguistic wordplay, specifically puns, in image descriptions. We develop two approaches which involve retrieving witty descriptions for a given image from a large corpus of sentences, or generating them via an encoder-decoder neural network architecture. We compare our approach against meaningful baseline approaches via human studies and show substantial improvements. We find that when a human is subject to similar constraints as the model regarding word usage and style, people vote the image descriptions generated by our model to be slightly wittier than human-written witty descriptions. Unsurprisingly, humans are almost always wittier than the model when they are free to choose the vocabulary, style, etc.



### C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset
- **Arxiv ID**: http://arxiv.org/abs/1704.08243v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.08243v1)
- **Published**: 2017-04-26 17:57:59+00:00
- **Updated**: 2017-04-26 17:57:59+00:00
- **Authors**: Aishwarya Agrawal, Aniruddha Kembhavi, Dhruv Batra, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) has received a lot of attention over the past couple of years. A number of deep learning models have been proposed for this task. However, it has been shown that these models are heavily driven by superficial correlations in the training data and lack compositionality -- the ability to answer questions about unseen compositions of seen concepts. This compositionality is desirable and central to intelligence. In this paper, we propose a new setting for Visual Question Answering where the test question-answer pairs are compositionally novel compared to training question-answer pairs. To facilitate developing models under this setting, we present a new compositional split of the VQA v1.0 dataset, which we call Compositional VQA (C-VQA). We analyze the distribution of questions and answers in the C-VQA splits. Finally, we evaluate several existing VQA models under this new setting and show that the performances of these models degrade by a significant amount compared to the original VQA setting.



### Deep Cross-Modal Audio-Visual Generation
- **Arxiv ID**: http://arxiv.org/abs/1704.08292v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1704.08292v1)
- **Published**: 2017-04-26 18:46:10+00:00
- **Updated**: 2017-04-26 18:46:10+00:00
- **Authors**: Lele Chen, Sudhanshu Srivastava, Zhiyao Duan, Chenliang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-modal audio-visual perception has been a long-lasting topic in psychology and neurology, and various studies have discovered strong correlations in human perception of auditory and visual stimuli. Despite works in computational multimodal modeling, the problem of cross-modal audio-visual generation has not been systematically studied in the literature. In this paper, we make the first attempt to solve this cross-modal generation problem leveraging the power of deep generative adversarial training. Specifically, we use conditional generative adversarial networks to achieve cross-modal audio-visual generation of musical performances. We explore different encoding methods for audio and visual signals, and work on two scenarios: instrument-oriented generation and pose-oriented generation. Being the first to explore this new problem, we compose two new datasets with pairs of images and sounds of musical performances of different instruments. Our experiments using both classification and human evaluations demonstrate that our model has the ability to generate one modality, i.e., audio/visual, from the other modality, i.e., visual/audio, to a good extent. Our experiments on various design choices along with the datasets will facilitate future research in this new problem space.



### Face Identification and Clustering
- **Arxiv ID**: http://arxiv.org/abs/1704.08328v1
- **DOI**: 10.7282/T3R21487
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08328v1)
- **Published**: 2017-04-26 19:50:28+00:00
- **Updated**: 2017-04-26 19:50:28+00:00
- **Authors**: Atul Dhingra
- **Comment**: None
- **Journal**: None
- **Summary**: In this thesis, we study two problems based on clustering algorithms. In the first problem, we study the role of visual attributes using an agglomerative clustering algorithm to whittle down the search area where the number of classes is high to improve the performance of clustering. We observe that as we add more attributes, the clustering performance increases overall. In the second problem, we study the role of clustering in aggregating templates in a 1:N open set protocol using multi-shot video as a probe. We observe that by increasing the number of clusters, the performance increases with respect to the baseline and reaches a peak, after which increasing the number of clusters causes the performance to degrade. Experiments are conducted using recently introduced unconstrained IARPA Janus IJB-A, CS2, and CS3 face recognition datasets.



### Semantic Autoencoder for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1704.08345v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08345v1)
- **Published**: 2017-04-26 20:45:53+00:00
- **Updated**: 2017-04-26 20:45:53+00:00
- **Authors**: Elyor Kodirov, Tao Xiang, Shaogang Gong
- **Comment**: accepted to CVPR2017
- **Journal**: None
- **Summary**: Existing zero-shot learning (ZSL) models typically learn a projection function from a feature space to a semantic embedding space (e.g.~attribute space). However, such a projection function is only concerned with predicting the training seen class semantic representation (e.g.~attribute prediction) or classification. When applied to test data, which in the context of ZSL contains different (unseen) classes without training data, a ZSL model typically suffers from the project domain shift problem. In this work, we present a novel solution to ZSL based on learning a Semantic AutoEncoder (SAE). Taking the encoder-decoder paradigm, an encoder aims to project a visual feature vector into the semantic space as in the existing ZSL models. However, the decoder exerts an additional constraint, that is, the projection/code must be able to reconstruct the original visual feature. We show that with this additional reconstruction constraint, the learned projection function from the seen classes is able to generalise better to the new unseen classes. Importantly, the encoder and decoder are linear and symmetric which enable us to develop an extremely efficient learning algorithm. Extensive experiments on six benchmark datasets demonstrate that the proposed SAE outperforms significantly the existing ZSL models with the additional benefit of lower computational cost. Furthermore, when the SAE is applied to supervised clustering problem, it also beats the state-of-the-art.



### Konzept für Bildanalysen in Hochdurchsatz-Systemen am Beispiel des Zebrabärblings
- **Arxiv ID**: http://arxiv.org/abs/1705.02962v1
- **DOI**: 10.5445/IR/1000061759
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1705.02962v1)
- **Published**: 2017-04-26 21:48:49+00:00
- **Updated**: 2017-04-26 21:48:49+00:00
- **Authors**: Rüdiger Alshut
- **Comment**: None
- **Journal**: Ph.D. Thesis, Karlsruhe Institute of Technology, KIT Scientific
  Publishing, 2016
- **Summary**: With image-based high-throughput experiments, new challenges arise in both, the design of experiments and the automated analysis. To be able to handle the massive number of single experiments and the corresponding amount of data, a comprehensive concept for the design of experiments and a new evaluation method is needed. This work proposes a new method for an optimized experiment layout that enables the determination of parameters, adapted for the needs of automated image analysis. Furthermore, a catalogue of new image analysis modules, especially developed for zebrafish analysis, is presented. The combination of both parts offers the user, usually a biologist, an approach for high-throughput zebrafish image analysis, which enables the extraction of new signals and optimizes the design of experiments. The result is a reduction of data amount, redundant information and workload as well as classification errors.



### (Quasi)Periodicity Quantification in Video Data, Using Topology
- **Arxiv ID**: http://arxiv.org/abs/1704.08382v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1704.08382v2)
- **Published**: 2017-04-26 23:54:40+00:00
- **Updated**: 2018-01-21 23:45:44+00:00
- **Authors**: Christopher J. Tralie, Jose A. Perea
- **Comment**: 27 pages, 1 column, 23 figures, SIAM Journal on Imaging Sciences,
  2018
- **Journal**: None
- **Summary**: This work introduces a novel framework for quantifying the presence and strength of recurrent dynamics in video data. Specifically, we provide continuous measures of periodicity (perfect repetition) and quasiperiodicity (superposition of periodic modes with non-commensurate periods), in a way which does not require segmentation, training, object tracking or 1-dimensional surrogate signals. Our methodology operates directly on video data. The approach combines ideas from nonlinear time series analysis (delay embeddings) and computational topology (persistent homology), by translating the problem of finding recurrent dynamics in video data, into the problem of determining the circularity or toroidality of an associated geometric space. Through extensive testing, we show the robustness of our scores with respect to several noise models/levels, we show that our periodicity score is superior to other methods when compared to human-generated periodicity rankings, and furthermore, we show that our quasiperiodicity score clearly indicates the presence of biphonation in videos of vibrating vocal folds, which has never before been accomplished end to end quantitatively.



