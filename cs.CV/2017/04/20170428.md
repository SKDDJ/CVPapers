# Arxiv Papers in cs.CV on 2017-04-28
### Partially Occluded Leaf Recognition via Subgraph Matching and Energy Optimization
- **Arxiv ID**: http://arxiv.org/abs/1704.08778v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08778v2)
- **Published**: 2017-04-28 00:15:49+00:00
- **Updated**: 2017-07-03 20:04:12+00:00
- **Authors**: Ayan Chaudhury, John L. Barron
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach to match partially occluded plant leaves with databases of full plant leaves. Although contour based 2D shape matching has been studied extensively in the last couple of decades, matching occluded leaves with full leaf databases is an open and little worked on problem. Classifying occluded plant leaves is even more challenging than full leaf matching because of large variations and complexity of leaf structures. Matching an occluded contour with all the full contours in a database is an NP-hard problem [Su et al. ICCV2015], so our algorithm is necessarily suboptimal.



### Machine Vision System for 3D Plant Phenotyping
- **Arxiv ID**: http://arxiv.org/abs/1705.00540v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00540v1)
- **Published**: 2017-04-28 01:02:12+00:00
- **Updated**: 2017-04-28 01:02:12+00:00
- **Authors**: Ayan Chaudhury, Christopher Ward, Ali Talasaz, Alexander G. Ivanov, Mark Brophy, Bernard Grodzinski, Norman P. A. Huner, Rajni V. Patel, John L. Barron
- **Comment**: None
- **Journal**: None
- **Summary**: Machine vision for plant phenotyping is an emerging research area for producing high throughput in agriculture and crop science applications. Since 2D based approaches have their inherent limitations, 3D plant analysis is becoming state of the art for current phenotyping technologies. We present an automated system for analyzing plant growth in indoor conditions. A gantry robot system is used to perform scanning tasks in an automated manner throughout the lifetime of the plant. A 3D laser scanner mounted as the robot's payload captures the surface point cloud data of the plant from multiple views. The plant is monitored from the vegetative to reproductive stages in light/dark cycles inside a controllable growth chamber. An efficient 3D reconstruction algorithm is used, by which multiple scans are aligned together to obtain a 3D mesh of the plant, followed by surface area and volume computations. The whole system, including the programmable growth chamber, robot, scanner, data transfer and analysis is fully automated in such a way that a naive user can, in theory, start the system with a mouse click and get back the growth analysis results at the end of the lifetime of the plant with no intermediate intervention. As evidence of its functionality, we show and analyze quantitative results of the rhythmic growth patterns of the dicot Arabidopsis thaliana(L.), and the monocot barley (Hordeum vulgare L.) plants under their diurnal light/dark cycles.



### Risk Stratification of Lung Nodules Using 3D CNN-Based Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/1704.08797v1
- **DOI**: 10.1007/978-3-319-59050-9_20
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.08797v1)
- **Published**: 2017-04-28 03:32:54+00:00
- **Updated**: 2017-04-28 03:32:54+00:00
- **Authors**: Sarfaraz Hussein, Kunlin Cao, Qi Song, Ulas Bagci
- **Comment**: Accepted for publication at Information Processing in Medical Imaging
  (IPMI) 2017
- **Journal**: Information Processing in Medical Imaging. IPMI 2017. Lecture
  Notes in Computer Science, vol 10265. Springer, Cham
- **Summary**: Risk stratification of lung nodules is a task of primary importance in lung cancer diagnosis. Any improvement in robust and accurate nodule characterization can assist in identifying cancer stage, prognosis, and improving treatment planning. In this study, we propose a 3D Convolutional Neural Network (CNN) based nodule characterization strategy. With a completely 3D approach, we utilize the volumetric information from a CT scan which would be otherwise lost in the conventional 2D CNN based approaches. In order to address the need for a large amount for training data for CNN, we resort to transfer learning to obtain highly discriminative features. Moreover, we also acquire the task dependent feature representation for six high-level nodule attributes and fuse this complementary information via a Multi-task learning (MTL) framework. Finally, we propose to incorporate potential disagreement among radiologists while scoring different nodule attributes in a graph regularized sparse multi-task learning. We evaluated our proposed approach on one of the largest publicly available lung nodule datasets comprising 1018 scans and obtained state-of-the-art results in regressing the malignancy scores.



### Automatic Real-time Background Cut for Portrait Videos
- **Arxiv ID**: http://arxiv.org/abs/1704.08812v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08812v1)
- **Published**: 2017-04-28 05:29:34+00:00
- **Updated**: 2017-04-28 05:29:34+00:00
- **Authors**: Xiaoyong Shen, Ruixing Wang, Hengshuang Zhao, Jiaya Jia
- **Comment**: None
- **Journal**: None
- **Summary**: We in this paper solve the problem of high-quality automatic real-time background cut for 720p portrait videos. We first handle the background ambiguity issue in semantic segmentation by proposing a global background attenuation model. A spatial-temporal refinement network is developed to further refine the segmentation errors in each frame and ensure temporal coherence in the segmentation map. We form an end-to-end network for training and testing. Each module is designed considering efficiency and accuracy. We build a portrait dataset, which includes 8,000 images with high-quality labeled map for training and testing. To further improve the performance, we build a portrait video dataset with 50 sequences to fine-tune video segmentation. Our framework benefits many video processing applications.



### Active Collaborative Ensemble Tracking
- **Arxiv ID**: http://arxiv.org/abs/1704.08821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08821v1)
- **Published**: 2017-04-28 06:46:27+00:00
- **Updated**: 2017-04-28 06:46:27+00:00
- **Authors**: Kourosh Meshgi, Maryam Sadat Mirzaei, Shigeyuki Oba, Shin Ishii
- **Comment**: AVSS 2017 Submission
- **Journal**: None
- **Summary**: A discriminative ensemble tracker employs multiple classifiers, each of which casts a vote on all of the obtained samples. The votes are then aggregated in an attempt to localize the target object. Such method relies on collective competence and the diversity of the ensemble to approach the target/non-target classification task from different views. However, by updating all of the ensemble using a shared set of samples and their final labels, such diversity is lost or reduced to the diversity provided by the underlying features or internal classifiers' dynamics. Additionally, the classifiers do not exchange information with each other while striving to serve the collective goal, i.e., better classification. In this study, we propose an active collaborative information exchange scheme for ensemble tracking. This, not only orchestrates different classifier towards a common goal but also provides an intelligent update mechanism to keep the diversity of classifiers and to mitigate the shortcomings of one with the others. The data exchange is optimized with regard to an ensemble uncertainty utility function, and the ensemble is updated via co-training. The evaluations demonstrate promising results realized by the proposed algorithm for the real-world online tracking.



### A new image compression by gradient Haar wavelet
- **Arxiv ID**: http://arxiv.org/abs/1704.08822v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1704.08822v2)
- **Published**: 2017-04-28 06:49:37+00:00
- **Updated**: 2017-09-01 17:21:30+00:00
- **Authors**: Yaser Sadra
- **Comment**: 9 pages, 4 figures, 10 tables
- **Journal**: None
- **Summary**: With the development of human communications the usage of Visual Communications has also increased. The advancement of image compression methods is one of the main reasons for the enhancement. This paper first presents main modes of image compression methods such as JPEG and JPEG2000 without mathematical details. Also, the paper describes gradient Haar wavelet transforms in order to construct a preliminary image compression algorithm. Then, a new image compression method is proposed based on the preliminary image compression algorithm that can improve standards of image compression. The new method is compared with original modes of JPEG and JPEG2000 (based on Haar wavelet) by image quality measures such as MAE, PSNAR, and SSIM. The image quality and statistical results confirm that can boost image compression standards. It is suggested that the new method is used in a part or all of an image compression standard.



### Outline Colorization through Tandem Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.08834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08834v1)
- **Published**: 2017-04-28 07:57:18+00:00
- **Updated**: 2017-04-28 07:57:18+00:00
- **Authors**: Kevin Frans
- **Comment**: None
- **Journal**: None
- **Summary**: When creating digital art, coloring and shading are often time consuming tasks that follow the same general patterns. A solution to automatically colorize raw line art would have many practical applications. We propose a setup utilizing two networks in tandem: a color prediction network based only on outlines, and a shading network conditioned on both outlines and a color scheme. We present processing methods to limit information passed in the color scheme, improving generalization. Finally, we demonstrate natural-looking results when colorizing outlines from scratch, as well as from a messy, user-defined color scheme.



### Image reconstruction by domain transform manifold learning
- **Arxiv ID**: http://arxiv.org/abs/1704.08841v1
- **DOI**: 10.1038/nature25988
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08841v1)
- **Published**: 2017-04-28 08:24:03+00:00
- **Updated**: 2017-04-28 08:24:03+00:00
- **Authors**: Bo Zhu, Jeremiah Z. Liu, Bruce R. Rosen, Matthew S. Rosen
- **Comment**: 18 pages, 4 figures
- **Journal**: None
- **Summary**: Image reconstruction plays a critical role in the implementation of all contemporary imaging modalities across the physical and life sciences including optical, MRI, CT, PET, and radio astronomy. During an image acquisition, the sensor encodes an intermediate representation of an object in the sensor domain, which is subsequently reconstructed into an image by an inversion of the encoding function. Image reconstruction is challenging because analytic knowledge of the inverse transform may not exist a priori, especially in the presence of sensor non-idealities and noise. Thus, the standard reconstruction approach involves approximating the inverse function with multiple ad hoc stages in a signal processing chain whose composition depends on the details of each acquisition strategy, and often requires expert parameter tuning to optimize reconstruction performance. We present here a unified framework for image reconstruction, AUtomated TransfOrm by Manifold APproximation (AUTOMAP), which recasts image reconstruction as a data-driven, supervised learning task that allows a mapping between sensor and image domain to emerge from an appropriate corpus of training data. We implement AUTOMAP with a deep neural network and exhibit its flexibility in learning reconstruction transforms for a variety of MRI acquisition strategies, using the same network architecture and hyperparameters. We further demonstrate its efficiency in sparsely representing transforms along low-dimensional manifolds, resulting in superior immunity to noise and reconstruction artifacts compared with conventional handcrafted reconstruction methods. In addition to improving the reconstruction performance of existing acquisition methodologies, we anticipate accelerating the discovery of new acquisition strategies across modalities as the burden of reconstruction becomes lifted by AUTOMAP and learned-reconstruction approaches.



### Object Detection by Spatio-Temporal Analysis and Tracking of the Detected Objects in a Video with Variable Background
- **Arxiv ID**: http://arxiv.org/abs/1705.02949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02949v1)
- **Published**: 2017-04-28 09:32:06+00:00
- **Updated**: 2017-04-28 09:32:06+00:00
- **Authors**: Kumar S. Ray, Vijayan K. Asari, Soma Chakraborty
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a novel approach for detecting and tracking objects in videos with variable background i.e. videos captured by moving cameras without any additional sensor. In a video captured by a moving camera, both the background and foreground are changing in each frame of the image sequence. So for these videos, modeling a single background with traditional background modeling methods is infeasible and thus the detection of actual moving object in a variable background is a challenging task. To detect actual moving object in this work, spatio-temporal blobs have been generated in each frame by spatio-temporal analysis of the image sequence using a three-dimensional Gabor filter. Then individual blobs, which are parts of one object are merged using Minimum Spanning Tree to form the moving object in the variable background. The height, width and four-bin gray-value histogram of the object are calculated as its features and an object is tracked in each frame using these features to generate the trajectories of the object through the video sequence. In this work, problem of data association during tracking is solved by Linear Assignment Problem and occlusion is handled by the application of kalman filter. The major advantage of our method over most of the existing tracking algorithms is that, the proposed method does not require initialization in the first frame or training on sample data to perform. Performance of the algorithm has been tested on benchmark videos and very satisfactory result has been achieved. The performance of the algorithm is also comparable and superior with respect to some benchmark algorithms.



### Improving Small Object Proposals for Company Logo Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.08881v1
- **DOI**: 10.1145/3078971.3078990
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08881v1)
- **Published**: 2017-04-28 11:30:10+00:00
- **Updated**: 2017-04-28 11:30:10+00:00
- **Authors**: Christian Eggert, Dan Zecha, Stephan Brehm, Rainer Lienhart
- **Comment**: 8 Pages, ICMR 2017
- **Journal**: None
- **Summary**: Many modern approaches for object detection are two-staged pipelines. The first stage identifies regions of interest which are then classified in the second stage. Faster R-CNN is such an approach for object detection which combines both stages into a single pipeline. In this paper we apply Faster R-CNN to the task of company logo detection. Motivated by its weak performance on small object instances, we examine in detail both the proposal and the classification stage with respect to a wide range of object sizes. We investigate the influence of feature map resolution on the performance of those stages.   Based on theoretical considerations, we introduce an improved scheme for generating anchor proposals and propose a modification to Faster R-CNN which leverages higher-resolution feature maps for small objects. We evaluate our approach on the FlickrLogos dataset improving the RPN performance from 0.52 to 0.71 (MABO) and the detection performance from 0.52 to 0.67 (mAP).



### Unbiased Shape Compactness for Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.08908v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08908v2)
- **Published**: 2017-04-28 12:54:44+00:00
- **Updated**: 2017-05-17 00:49:38+00:00
- **Authors**: Jose Dolz, Ismail Ben Ayed, Christian Desrosiers
- **Comment**: Accepted at MICCAI 2017
- **Journal**: None
- **Summary**: We propose to constrain segmentation functionals with a dimensionless, unbiased and position-independent shape compactness prior, which we solve efficiently with an alternating direction method of multipliers (ADMM). Involving a squared sum of pairwise potentials, our prior results in a challenging high-order optimization problem, which involves dense (fully connected) graphs. We split the problem into a sequence of easier sub-problems, each performed efficiently at each iteration: (i) a sparse-matrix inversion based on Woodbury identity, (ii) a closed-form solution of a cubic equation and (iii) a graph-cut update of a sub-modular pairwise sub-problem with a sparse graph. We deploy our prior in an energy minimization, in conjunction with a supervised classifier term based on CNNs and standard regularization constraints. We demonstrate the usefulness of our energy in several medical applications. In particular, we report comprehensive evaluations of our fully automated algorithm over 40 subjects, showing a competitive performance for the challenging task of abdominal aorta segmentation in MRI.



### Object Discovery via Cohesion Measurement
- **Arxiv ID**: http://arxiv.org/abs/1704.08944v1
- **DOI**: 10.1109/TCYB.2017.2661995
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08944v1)
- **Published**: 2017-04-28 14:19:00+00:00
- **Updated**: 2017-04-28 14:19:00+00:00
- **Authors**: Guanjun Guo, Hanzi Wang, Wan-Lei Zhao, Yan Yan, Xuelong Li
- **Comment**: 14 pages, 14 figures
- **Journal**: IEEE Transactions on Cybernetics (2017) 1-14
- **Summary**: Color and intensity are two important components in an image. Usually, groups of image pixels, which are similar in color or intensity, are an informative representation for an object. They are therefore particularly suitable for computer vision tasks, such as saliency detection and object proposal generation. However, image pixels, which share a similar real-world color, may be quite different since colors are often distorted by intensity. In this paper, we reinvestigate the affinity matrices originally used in image segmentation methods based on spectral clustering. A new affinity matrix, which is robust to color distortions, is formulated for object discovery. Moreover, a Cohesion Measurement (CM) for object regions is also derived based on the formulated affinity matrix. Based on the new Cohesion Measurement, a novel object discovery method is proposed to discover objects latent in an image by utilizing the eigenvectors of the affinity matrix. Then we apply the proposed method to both saliency detection and object proposal generation. Experimental results on several evaluation benchmarks demonstrate that the proposed CM based method has achieved promising performance for these two tasks.



### Expressing Facial Structure and Appearance Information in Frequency Domain for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.08949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08949v1)
- **Published**: 2017-04-28 14:25:46+00:00
- **Updated**: 2017-04-28 14:25:46+00:00
- **Authors**: Chollette C. Olisah, Solomon Nunoo, Peter Ofedebe, Ghazali Sulong
- **Comment**: 17 pages, 9 figures, ISSA CONFERENCE 2016
- **Journal**: None
- **Summary**: Beneath the uncertain primitive visual features of face images are the primitive intrinsic structural patterns (PISP) essential for characterizing a sample face discriminative attributes. It is on this basis that this paper presents a simple yet effective facial descriptor formed from derivatives of Gaussian and Gabor Wavelets. The new descriptor is coined local edge gradient Gabor magnitude (LEGGM) pattern. LEGGM first uncovers the PISP locked in every pixel through determining the pixel gradient in relation to its neighbors using the Derivatives of Gaussians. Then, the resulting output is embedded into the global appearance of the face which are further processed using Gabor wavelets in order to express its frequency characteristics. Additionally, we adopted various subspace models for dimensionality reduction in order to ascertain the best fit model for reporting a more effective representation of the LEGGM patterns. The proposed descriptor-based face recognition method is evaluated on three databases: Plastic surgery, LFW, and GT face databases. Through experiments, using a base classifier, the efficacy of the proposed method is demonstrated, especially in the case of plastic surgery database. The heterogeneous database, which we created to typify real-world scenario, show that the proposed method is to an extent insensitive to image formation factors with impressive recognition performances.



### A Unified Approach of Multi-scale Deep and Hand-crafted Features for Defocus Estimation
- **Arxiv ID**: http://arxiv.org/abs/1704.08992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08992v1)
- **Published**: 2017-04-28 16:16:41+00:00
- **Updated**: 2017-04-28 16:16:41+00:00
- **Authors**: Jinsun Park, Yu-Wing Tai, Donghyeon Cho, In So Kweon
- **Comment**: 10 pages, 14 figures. To appear in CVPR 2017. Project page :
  https://github.com/zzangjinsun/DHDE_CVPR17
- **Journal**: None
- **Summary**: In this paper, we introduce robust and synergetic hand-crafted features and a simple but efficient deep feature from a convolutional neural network (CNN) architecture for defocus estimation. This paper systematically analyzes the effectiveness of different features, and shows how each feature can compensate for the weaknesses of other features when they are concatenated. For a full defocus map estimation, we extract image patches on strong edges sparsely, after which we use them for deep and hand-crafted feature extraction. In order to reduce the degree of patch-scale dependency, we also propose a multi-scale patch extraction strategy. A sparse defocus map is generated using a neural network classifier followed by a probability-joint bilateral filter. The final defocus map is obtained from the sparse defocus map with guidance from an edge-preserving filtered input image. Experimental results show that our algorithm is superior to state-of-the-art algorithms in terms of defocus estimation. Our work can be used for applications such as segmentation, blur magnification, all-in-focus image generation, and 3-D estimation.



### Understanding People Flow in Transportation Hubs
- **Arxiv ID**: http://arxiv.org/abs/1705.00027v2
- **DOI**: 10.1109/TITS.2017.2775285
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00027v2)
- **Published**: 2017-04-28 18:24:38+00:00
- **Updated**: 2019-02-11 16:48:58+00:00
- **Authors**: João Carvalho, Manuel Marques, João P. Costeira
- **Comment**: 10 pages, 19 figure, 1 table
- **Journal**: IEEE Transactions on Intelligent Transportation Systems ( Volume:
  19 , Issue: 10 , Oct. 2018 )
- **Summary**: In this paper, we aim to monitor the flow of people in large public infrastructures. We propose an unsupervised methodology to cluster people flow patterns into the most typical and meaningful configurations. By processing 3D images from a network of depth cameras, we build a descriptor for the flow pattern. We define a data-irregularity measure that assesses how well each descriptor fits a data model. This allows us to rank flow patterns from highly distinctive (outliers) to very common ones. By discarding outliers, we obtain more reliable key configurations (classes). Synthetic experiments show that the proposed method is superior to standard clustering methods. We applied it in an operational scenario during 14 days in the X-ray screening area of an international airport. Results show that our methodology is able to successfully summarize the representative patterns for such a long observation period, providing relevant information for airport management. Beyond regular flows, our method identifies a set of rare events corresponding to uncommon activities (cleaning, special security and circulating staff).



### Deep Multi-view Models for Glitch Classification
- **Arxiv ID**: http://arxiv.org/abs/1705.00034v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.00034v1)
- **Published**: 2017-04-28 18:45:57+00:00
- **Updated**: 2017-04-28 18:45:57+00:00
- **Authors**: Sara Bahaadini, Neda Rohani, Scott Coughlin, Michael Zevin, Vicky Kalogera, Aggelos K Katsaggelos
- **Comment**: Accepted to the 42nd IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP'17)
- **Journal**: None
- **Summary**: Non-cosmic, non-Gaussian disturbances known as "glitches", show up in gravitational-wave data of the Advanced Laser Interferometer Gravitational-wave Observatory, or aLIGO. In this paper, we propose a deep multi-view convolutional neural network to classify glitches automatically. The primary purpose of classifying glitches is to understand their characteristics and origin, which facilitates their removal from the data or from the detector entirely. We visualize glitches as spectrograms and leverage the state-of-the-art image classification techniques in our model. The suggested classifier is a multi-view deep neural network that exploits four different views for classification. The experimental results demonstrate that the proposed model improves the overall accuracy of the classification compared to traditional single view algorithms.



### The Pose Knows: Video Forecasting by Generating Pose Futures
- **Arxiv ID**: http://arxiv.org/abs/1705.00053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00053v1)
- **Published**: 2017-04-28 19:46:47+00:00
- **Updated**: 2017-04-28 19:46:47+00:00
- **Authors**: Jacob Walker, Kenneth Marino, Abhinav Gupta, Martial Hebert
- **Comment**: Project Website: http://www.cs.cmu.edu/~jcwalker/POS/POS.html
- **Journal**: None
- **Summary**: Current approaches in video forecasting attempt to generate videos directly in pixel space using Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs). However, since these approaches try to model all the structure and scene dynamics at once, in unconstrained settings they often generate uninterpretable results. Our insight is to model the forecasting problem at a higher level of abstraction. Specifically, we exploit human pose detectors as a free source of supervision and break the video forecasting problem into two discrete steps. First we explicitly model the high level structure of active objects in the scene---humans---and use a VAE to model the possible future movements of humans in the pose space. We then use the future poses generated as conditional information to a GAN to predict the future frames of the video in pixel space. By using the structured space of pose as an intermediate representation, we sidestep the problems that GANs have in generating video pixels directly. We show through quantitative and qualitative evaluation that our method outperforms state-of-the-art methods for video prediction.



### Effective scaling registration approach by imposing the emphasis on the scale factor
- **Arxiv ID**: http://arxiv.org/abs/1705.00086v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00086v2)
- **Published**: 2017-04-28 22:21:47+00:00
- **Updated**: 2019-03-08 12:02:24+00:00
- **Authors**: Minmin Xu, Siyu Xu, Jihua Zhu, Yaochen Li, Jun Wang, Huimin Lu
- **Comment**: 22 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: This paper proposes an effective approach for the scaling registration of $m$-D point sets. Different from the rigid transformation, the scaling registration can not be formulated into the common least square function due to the ill-posed problem caused by the scale factor. Therefore, this paper designs a novel objective function for the scaling registration problem. The appearance of this objective function is a rational fraction, where the numerator item is the least square error and the denominator item is the square of the scale factor. By imposing the emphasis on scale factor, the ill-posed problem can be avoided in the scaling registration. Subsequently, the new objective function can be solved by the proposed scaling iterative closest point (ICP) algorithm, which can obtain the optimal scaling transformation. For the practical applications, the scaling ICP algorithm is further extended to align partially overlapping point sets. Finally, the proposed approach is tested on public data sets and applied to merging grid maps of different resolutions. Experimental results demonstrate its superiority over previous approaches on efficiency and robustness.



