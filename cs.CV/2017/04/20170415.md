# Arxiv Papers in cs.CV on 2017-04-15
### Deep Learning for Photoacoustic Tomography from Sparse Data
- **Arxiv ID**: http://arxiv.org/abs/1704.04587v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.04587v3)
- **Published**: 2017-04-15 05:33:32+00:00
- **Updated**: 2018-08-30 13:45:40+00:00
- **Authors**: Stephan Antholzer, Markus Haltmeier, Johannes Schwab
- **Comment**: None
- **Journal**: None
- **Summary**: The development of fast and accurate image reconstruction algorithms is a central aspect of computed tomography. In this paper, we investigate this issue for the sparse data problem in photoacoustic tomography (PAT). We develop a direct and highly efficient reconstruction algorithm based on deep learning. In our approach image reconstruction is performed with a deep convolutional neural network (CNN), whose weights are adjusted prior to the actual image reconstruction based on a set of training data. The proposed reconstruction approach can be interpreted as a network that uses the PAT filtered backprojection algorithm for the first layer, followed by the U-net architecture for the remaining layers. Actual image reconstruction with deep learning consists in one evaluation of the trained CNN, which does not require time consuming solution of the forward and adjoint problems. At the same time, our numerical results demonstrate that the proposed deep learning approach reconstructs images with a quality comparable to state of the art iterative approaches for PAT from sparse data.



### A learning-based approach for automatic image and video colorization
- **Arxiv ID**: http://arxiv.org/abs/1704.04610v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.04610v1)
- **Published**: 2017-04-15 09:21:57+00:00
- **Updated**: 2017-04-15 09:21:57+00:00
- **Authors**: Raj Kumar Gupta, Alex Yong-Sang Chia, Deepu Rajan, Huang Zhiyong
- **Comment**: Computer Graphics International - 2012
- **Journal**: None
- **Summary**: In this paper, we present a color transfer algorithm to colorize a broad range of gray images without any user intervention. The algorithm uses a machine learning-based approach to automatically colorize grayscale images. The algorithm uses the superpixel representation of the reference color images to learn the relationship between different image features and their corresponding color values. We use this learned information to predict the color value of each grayscale image superpixel. As compared to processing individual image pixels, our use of superpixels helps us to achieve a much higher degree of spatial consistency as well as speeds up the colorization process. The predicted color values of the gray-scale image superpixels are used to provide a 'micro-scribble' at the centroid of the superpixels. These color scribbles are refined by using a voting based approach. To generate the final colorization result, we use an optimization-based approach to smoothly spread the color scribble across all pixels within a superpixel. Experimental results on a broad range of images and the comparison with existing state-of-the-art colorization methods demonstrate the greater effectiveness of the proposed algorithm.



### Integrating Scene Text and Visual Appearance for Fine-Grained Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1704.04613v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04613v2)
- **Published**: 2017-04-15 09:44:08+00:00
- **Updated**: 2017-05-30 01:27:20+00:00
- **Authors**: Xiang Bai, Mingkun Yang, Pengyuan Lyu, Yongchao Xu, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Text in natural images contains rich semantics that are often highly relevant to objects or scene. In this paper, we focus on the problem of fully exploiting scene text for visual understanding. The main idea is combining word representations and deep visual features into a globally trainable deep convolutional neural network. First, the recognized words are obtained by a scene text reading system. Then, we combine the word embedding of the recognized words and the deep visual features into a single representation, which is optimized by a convolutional neural network for fine-grained image classification. In our framework, the attention mechanism is adopted to reveal the relevance between each recognized word and the given image, which further enhances the recognition performance. We have performed experiments on two datasets: Con-Text dataset and Drink Bottle dataset, that are proposed for fine-grained classification of business places and drink bottles, respectively. The experimental results consistently demonstrate that the proposed method combining textual and visual cues significantly outperforms classification with only visual representations. Moreover, we have shown that the learned representation improves the retrieval performance on the drink bottle images by a large margin, making it potentially useful in product search.



### Big Universe, Big Data: Machine Learning and Image Analysis for Astronomy
- **Arxiv ID**: http://arxiv.org/abs/1704.04650v1
- **DOI**: 10.1109/MIS.2017.40
- **Categories**: **astro-ph.IM**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.04650v1)
- **Published**: 2017-04-15 15:32:13+00:00
- **Updated**: 2017-04-15 15:32:13+00:00
- **Authors**: Jan Kremer, Kristoffer Stensbo-Smidt, Fabian Gieseke, Kim Steenstrup Pedersen, Christian Igel
- **Comment**: None
- **Journal**: IEEE Intelligent Systems, vol. 32, no. , pp. 16-22, Mar.-Apr. 2017
- **Summary**: Astrophysics and cosmology are rich with data. The advent of wide-area digital cameras on large aperture telescopes has led to ever more ambitious surveys of the sky. Data volumes of entire surveys a decade ago can now be acquired in a single night and real-time analysis is often desired. Thus, modern astronomy requires big data know-how, in particular it demands highly efficient machine learning and image analysis algorithms. But scalability is not the only challenge: Astronomy applications touch several current machine learning research questions, such as learning from biased data and dealing with label and measurement noise. We argue that this makes astronomy a great domain for computer science research, as it pushes the boundaries of data analysis. In the following, we will present this exciting application area for data scientists. We will focus on exemplary results, discuss main challenges, and highlight some recent methodological advancements in machine learning and image analysis triggered by astronomical applications.



### Temporal Action Localization by Structured Maximal Sums
- **Arxiv ID**: http://arxiv.org/abs/1704.04671v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04671v1)
- **Published**: 2017-04-15 18:10:21+00:00
- **Updated**: 2017-04-15 18:10:21+00:00
- **Authors**: Zehuan Yuan, Jonathan C. Stroud, Tong Lu, Jia Deng
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: We address the problem of temporal action localization in videos. We pose action localization as a structured prediction over arbitrary-length temporal windows, where each window is scored as the sum of frame-wise classification scores. Additionally, our model classifies the start, middle, and end of each action as separate components, allowing our system to explicitly model each action's temporal evolution and take advantage of informative temporal dependencies present in this structure. In this framework, we localize actions by searching for the structured maximal sum, a problem for which we develop a novel, provably-efficient algorithmic solution. The frame-wise classification scores are computed using features from a deep Convolutional Neural Network (CNN), which are trained end-to-end to directly optimize for a novel structured objective. We evaluate our system on the THUMOS 14 action detection benchmark and achieve competitive performance.



### Video Fill In the Blank using LR/RL LSTMs with Spatial-Temporal Attentions
- **Arxiv ID**: http://arxiv.org/abs/1704.04689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04689v1)
- **Published**: 2017-04-15 21:13:41+00:00
- **Updated**: 2017-04-15 21:13:41+00:00
- **Authors**: Amir Mazaheri, Dong Zhang, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Given a video and a description sentence with one missing word (we call it the "source sentence"), Video-Fill-In-the-Blank (VFIB) problem is to find the missing word automatically. The contextual information of the sentence, as well as visual cues from the video, are important to infer the missing word accurately. Since the source sentence is broken into two fragments: the sentence's left fragment (before the blank) and the sentence's right fragment (after the blank), traditional Recurrent Neural Networks cannot encode this structure accurately because of many possible variations of the missing word in terms of the location and type of the word in the source sentence. For example, a missing word can be the first word or be in the middle of the sentence and it can be a verb or an adjective. In this paper, we propose a framework to tackle the textual encoding: Two separate LSTMs (the LR and RL LSTMs) are employed to encode the left and right sentence fragments and a novel structure is introduced to combine each fragment with an "external memory" corresponding the opposite fragments. For the visual encoding, end-to-end spatial and temporal attention models are employed to select discriminative visual representations to find the missing word. In the experiments, we demonstrate the superior performance of the proposed method on challenging VFIB problem. Furthermore, we introduce an extended and more generalized version of VFIB, which is not limited to a single blank. Our experiments indicate the generalization capability of our method in dealing with such more realistic scenarios.



