# Arxiv Papers in cs.CV on 2017-04-30
### Joint Denoising / Compression of Image Contours via Shape Prior and Context Tree
- **Arxiv ID**: http://arxiv.org/abs/1705.00268v1
- **DOI**: 10.1109/TIP.2018.2816818
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00268v1)
- **Published**: 2017-04-30 04:27:07+00:00
- **Updated**: 2017-04-30 04:27:07+00:00
- **Authors**: Amin Zheng, Gene Cheung, Dinei Florencio
- **Comment**: None
- **Journal**: None
- **Summary**: With the advent of depth sensing technologies, the extraction of object contours in images---a common and important pre-processing step for later higher-level computer vision tasks like object detection and human action recognition---has become easier. However, acquisition noise in captured depth images means that detected contours suffer from unavoidable errors. In this paper, we propose to jointly denoise and compress detected contours in an image for bandwidth-constrained transmission to a client, who can then carry out aforementioned application-specific tasks using the decoded contours as input. We first prove theoretically that in general a joint denoising / compression approach can outperform a separate two-stage approach that first denoises then encodes contours lossily. Adopting a joint approach, we first propose a burst error model that models typical errors encountered in an observed string y of directional edges. We then formulate a rate-constrained maximum a posteriori (MAP) problem that trades off the posterior probability p(x'|y) of an estimated string x' given y with its code rate R(x'). We design a dynamic programming (DP) algorithm that solves the posed problem optimally, and propose a compact context representation called total suffix tree (TST) that can reduce complexity of the algorithm dramatically. Experimental results show that our joint denoising / compression scheme outperformed a competing separate scheme in rate-distortion performance noticeably.



### Topologically Robust 3D Shape Matching via Gradual Deflation and Inflation
- **Arxiv ID**: http://arxiv.org/abs/1705.00274v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.00274v2)
- **Published**: 2017-04-30 06:40:18+00:00
- **Updated**: 2017-05-12 21:48:29+00:00
- **Authors**: Asli Genctav, Yusuf Sahillioglu, Sibel Tari
- **Comment**: Section 2 replaced
- **Journal**: None
- **Summary**: Despite being vastly ignored in the literature, coping with topological noise is an issue of increasing importance, especially as a consequence of the increasing number and diversity of 3D polygonal models that are captured by devices of different qualities or synthesized by algorithms of different stabilities. One approach for matching 3D shapes under topological noise is to replace the topology-sensitive geodesic distance with distances that are less sensitive to topological changes. We propose an alternative approach utilising gradual deflation (or inflation) of the shape volume, of which purpose is to bring the pair of shapes to be matched to a \emph{comparable} topology before the search for correspondences. Illustrative experiments using different datasets demonstrate that as the level of topological noise increases, our approach outperforms the other methods in the literature.



### Indoor Frame Recovery from Refined Line Segments
- **Arxiv ID**: http://arxiv.org/abs/1705.00279v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00279v1)
- **Published**: 2017-04-30 07:16:10+00:00
- **Updated**: 2017-04-30 07:16:10+00:00
- **Authors**: Luanzheng Guo, Jun Chu
- **Comment**: 39 pages, 11 figures
- **Journal**: None
- **Summary**: An important yet challenging problem in understanding indoor scene is recovering indoor frame structure from a monocular image. It is more difficult when occlusions and illumination vary, and object boundaries are weak. To overcome these difficulties, a new approach based on line segment refinement with two constraints is proposed. First, the line segments are refined by four consecutive operations, i.e., reclassifying, connecting, fitting, and voting. Specifically, misclassified line segments are revised by the reclassifying operation, some short line segments are joined by the connecting operation, the undetected key line segments are recovered by the fitting operation with the help of the vanishing points, the line segments converging on the frame are selected by the voting operation. Second, we construct four frame models according to four classes of possible shooting angles of the monocular image, the natures of all frame models are introduced via enforcing the cross ratio and depth constraints. The indoor frame is then constructed by fitting those refined line segments with related frame model under the two constraints, which jointly advance the accuracy of the frame. Experimental results on a collection of over 300 indoor images indicate that our algorithm has the capability of recovering the frame from complex indoor scenes.



### SurfCut: Surfaces of Minimal Paths From Topological Structures
- **Arxiv ID**: http://arxiv.org/abs/1705.00301v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00301v1)
- **Published**: 2017-04-30 11:56:51+00:00
- **Updated**: 2017-04-30 11:56:51+00:00
- **Authors**: Marei Algarni, Ganesh Sundaramoorthi
- **Comment**: None
- **Journal**: None
- **Summary**: We present SurfCut, an algorithm for extracting a smooth, simple surface with an unknown 3D curve boundary from a noisy 3D image and a seed point. Our method is built on the novel observation that certain ridge curves of a function defined on a front propagated using the Fast Marching algorithm lie on the surface. Our method extracts and cuts these ridges to form the surface boundary. Our surface extraction algorithm is built on the novel observation that the surface lies in a valley of the distance from Fast Marching. We show that the resulting surface is a collection of minimal paths. Using the framework of cubical complexes and Morse theory, we design algorithms to extract these critical structures robustly. Experiments on three 3D datasets show the robustness of our method, and that it achieves higher accuracy with lower computational cost than state-of-the-art.



### Discriminative Nonlinear Analysis Operator Learning: When Cosparse Model Meets Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1705.00322v1
- **DOI**: 10.1109/TIP.2017.2700761
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00322v1)
- **Published**: 2017-04-30 15:09:22+00:00
- **Updated**: 2017-04-30 15:09:22+00:00
- **Authors**: Zaidao Wen, Biao Hou, Licheng Jiao
- **Comment**: IEEE TIP Accepted
- **Journal**: None
- **Summary**: Linear synthesis model based dictionary learning framework has achieved remarkable performances in image classification in the last decade. Behaved as a generative feature model, it however suffers from some intrinsic deficiencies. In this paper, we propose a novel parametric nonlinear analysis cosparse model (NACM) with which a unique feature vector will be much more efficiently extracted. Additionally, we derive a deep insight to demonstrate that NACM is capable of simultaneously learning the task adapted feature transformation and regularization to encode our preferences, domain prior knowledge and task oriented supervised information into the features. The proposed NACM is devoted to the classification task as a discriminative feature model and yield a novel discriminative nonlinear analysis operator learning framework (DNAOL). The theoretical analysis and experimental performances clearly demonstrate that DNAOL will not only achieve the better or at least competitive classification accuracies than the state-of-the-art algorithms but it can also dramatically reduce the time complexities in both training and testing phases.



### Deep Learning in the Automotive Industry: Applications and Tools
- **Arxiv ID**: http://arxiv.org/abs/1705.00346v1
- **DOI**: 10.1109/BigData.2016.7841045
- **Categories**: **cs.LG**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1705.00346v1)
- **Published**: 2017-04-30 17:17:44+00:00
- **Updated**: 2017-04-30 17:17:44+00:00
- **Authors**: Andre Luckow, Matthew Cook, Nathan Ashcraft, Edwin Weill, Emil Djerekarov, Bennie Vorster
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Deep Learning refers to a set of machine learning techniques that utilize neural networks with many hidden layers for tasks, such as image classification, speech recognition, language understanding. Deep learning has been proven to be very effective in these domains and is pervasively used by many Internet services. In this paper, we describe different automotive uses cases for deep learning in particular in the domain of computer vision. We surveys the current state-of-the-art in libraries, tools and infrastructures (e.\,g.\ GPUs and clouds) for implementing, training and deploying deep neural networks. We particularly focus on convolutional neural networks and computer vision use cases, such as the visual inspection process in manufacturing plants and the analysis of social media data. To train neural networks, curated and labeled datasets are essential. In particular, both the availability and scope of such datasets is typically very limited. A main contribution of this paper is the creation of an automotive dataset, that allows us to learn and automatically recognize different vehicle properties. We describe an end-to-end deep learning application utilizing a mobile app for data collection and process support, and an Amazon-based cloud backend for storage and training. For training we evaluate the use of cloud and on-premises infrastructures (including multiple GPUs) in conjunction with different neural network architectures and frameworks. We assess both the training times as well as the accuracy of the classifier. Finally, we demonstrate the effectiveness of the trained classifier in a real world setting during manufacturing process.



### Real-Time Salient Closed Boundary Tracking via Line Segments Perceptual Grouping
- **Arxiv ID**: http://arxiv.org/abs/1705.00360v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00360v2)
- **Published**: 2017-04-30 19:01:07+00:00
- **Updated**: 2017-08-09 23:44:36+00:00
- **Authors**: Xuebin Qin, Shida He, Camilo Perez Quintero, Abhineet Singh, Masood Dehghan, Martin Jagersand
- **Comment**: 7 pages, 8 figures, The 2017 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2017) submission ID 1034
- **Journal**: None
- **Summary**: This paper presents a novel real-time method for tracking salient closed boundaries from video image sequences. This method operates on a set of straight line segments that are produced by line detection. The tracking scheme is coherently integrated into a perceptual grouping framework in which the visual tracking problem is tackled by identifying a subset of these line segments and connecting them sequentially to form a closed boundary with the largest saliency and a certain similarity to the previous one. Specifically, we define a new tracking criterion which combines a grouping cost and an area similarity constraint. The proposed criterion makes the resulting boundary tracking more robust to local minima. To achieve real-time tracking performance, we use Delaunay Triangulation to build a graph model with the detected line segments and then reduce the tracking problem to finding the optimal cycle in this graph. This is solved by our newly proposed closed boundary candidates searching algorithm called "Bidirectional Shortest Path (BDSP)". The efficiency and robustness of the proposed method are tested on real video sequences as well as during a robot arm pouring experiment.



### Predicting Foreground Object Ambiguity and Efficiently Crowdsourcing the Segmentation(s)
- **Arxiv ID**: http://arxiv.org/abs/1705.00366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00366v1)
- **Published**: 2017-04-30 19:27:30+00:00
- **Updated**: 2017-04-30 19:27:30+00:00
- **Authors**: Danna Gurari, Kun He, Bo Xiong, Jianming Zhang, Mehrnoosh Sameki, Suyog Dutt Jain, Stan Sclaroff, Margrit Betke, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: We propose the ambiguity problem for the foreground object segmentation task and motivate the importance of estimating and accounting for this ambiguity when designing vision systems. Specifically, we distinguish between images which lead multiple annotators to segment different foreground objects (ambiguous) versus minor inter-annotator differences of the same object. Taking images from eight widely used datasets, we crowdsource labeling the images as "ambiguous" or "not ambiguous" to segment in order to construct a new dataset we call STATIC. Using STATIC, we develop a system that automatically predicts which images are ambiguous. Experiments demonstrate the advantage of our prediction system over existing saliency-based methods on images from vision benchmarks and images taken by blind people who are trying to recognize objects in their environment. Finally, we introduce a crowdsourcing system to achieve cost savings for collecting the diversity of all valid "ground truth" foreground object segmentations by collecting extra segmentations only when ambiguity is expected. Experiments show our system eliminates up to 47% of human effort compared to existing crowdsourcing methods with no loss in capturing the diversity of ground truths.



### Adversarial PoseNet: A Structure-aware Convolutional Network for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1705.00389v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00389v2)
- **Published**: 2017-04-30 23:54:43+00:00
- **Updated**: 2017-05-02 03:54:39+00:00
- **Authors**: Yu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, Jian Yang
- **Comment**: Fixed typos. 14 pages. Demonstration videos are
  http://v.qq.com/x/page/c039862eira.html,
  http://v.qq.com/x/page/f0398zcvkl5.html,
  http://v.qq.com/x/page/w0398ei9m1r.html
- **Journal**: None
- **Summary**: For human pose estimation in monocular images, joint occlusions and overlapping upon human bodies often result in deviated pose predictions. Under these circumstances, biologically implausible pose predictions may be produced. In contrast, human vision is able to predict poses by exploiting geometric constraints of joint inter-connectivity. To address the problem by incorporating priors about the structure of human bodies, we propose a novel structure-aware convolutional network to implicitly take such priors into account during training of the deep network. Explicit learning of such constraints is typically challenging. Instead, we design discriminators to distinguish the real poses from the fake ones (such as biologically implausible ones). If the pose generator (G) generates results that the discriminator fails to distinguish from real ones, the network successfully learns the priors.



