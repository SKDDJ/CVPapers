# Arxiv Papers in cs.CV on 2017-04-01
### Learning to Predict Indoor Illumination from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1704.00090v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.00090v3)
- **Published**: 2017-04-01 00:50:12+00:00
- **Updated**: 2017-11-21 08:32:24+00:00
- **Authors**: Marc-André Gardner, Kalyan Sunkavalli, Ersin Yumer, Xiaohui Shen, Emiliano Gambaretto, Christian Gagné, Jean-François Lalonde
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an automatic method to infer high dynamic range illumination from a single, limited field-of-view, low dynamic range photograph of an indoor scene. In contrast to previous work that relies on specialized image capture, user input, and/or simple scene models, we train an end-to-end deep neural network that directly regresses a limited field-of-view photo to HDR illumination, without strong assumptions on scene geometry, material properties, or lighting. We show that this can be accomplished in a three step process: 1) we train a robust lighting classifier to automatically annotate the location of light sources in a large dataset of LDR environment maps, 2) we use these annotations to train a deep neural network that predicts the location of lights in a scene from a single limited field-of-view photo, and 3) we fine-tune this network using a small dataset of HDR environment maps to predict light intensities. This allows us to automatically recover high-quality HDR illumination estimates that significantly outperform previous state-of-the-art methods. Consequently, using our illumination estimates for applications like 3D object insertion, we can achieve results that are photo-realistic, which is validated via a perceptual user study.



### Customizing First Person Image Through Desired Actions
- **Arxiv ID**: http://arxiv.org/abs/1704.00098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00098v1)
- **Published**: 2017-04-01 01:55:28+00:00
- **Updated**: 2017-04-01 01:55:28+00:00
- **Authors**: Shan Su, Jianbo Shi, Hyun Soo Park
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies a problem of inverse visual path planning: creating a visual scene from a first person action. Our conjecture is that the spatial arrangement of a first person visual scene is deployed to afford an action, and therefore, the action can be inversely used to synthesize a new scene such that the action is feasible. As a proof-of-concept, we focus on linking visual experiences induced by walking.   A key innovation of this paper is a concept of ActionTunnel---a 3D virtual tunnel along the future trajectory encoding what the wearer will visually experience as moving into the scene. This connects two distinctive first person images through similar walking paths. Our method takes a first person image with a user defined future trajectory and outputs a new image that can afford the future motion. The image is created by combining present and future ActionTunnels in 3D where the missing pixels in adjoining area are computed by a generative adversarial network. Our work can provide a travel across different first person experiences in diverse real world scenes.



### SafetyNet: Detecting and Rejecting Adversarial Examples Robustly
- **Arxiv ID**: http://arxiv.org/abs/1704.00103v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.00103v2)
- **Published**: 2017-04-01 02:12:40+00:00
- **Updated**: 2017-08-15 05:59:38+00:00
- **Authors**: Jiajun Lu, Theerasit Issaranon, David Forsyth
- **Comment**: Accepted to ICCV 2017
- **Journal**: None
- **Summary**: We describe a method to produce a network where current methods such as DeepFool have great difficulty producing adversarial samples. Our construction suggests some insights into how deep networks work. We provide a reasonable analyses that our construction is difficult to defeat, and show experimentally that our method is hard to defeat with both Type I and Type II attacks using several standard networks and datasets. This SafetyNet architecture is used to an important and novel application SceneProof, which can reliably detect whether an image is a picture of a real scene or not. SceneProof applies to images captured with depth maps (RGBD images) and checks if a pair of image and depth map is consistent. It relies on the relative difficulty of producing naturalistic depth maps for images in post processing. We demonstrate that our SafetyNet is robust to adversarial examples built from currently known attacking approaches.



### Configurable 3D Scene Synthesis and 2D Image Rendering with Per-Pixel Ground Truth using Stochastic Grammars
- **Arxiv ID**: http://arxiv.org/abs/1704.00112v3
- **DOI**: 10.1007/s11263-018-1103-5
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.00112v3)
- **Published**: 2017-04-01 03:05:29+00:00
- **Updated**: 2018-06-20 15:24:55+00:00
- **Authors**: Chenfanfu Jiang, Siyuan Qi, Yixin Zhu, Siyuan Huang, Jenny Lin, Lap-Fai Yu, Demetri Terzopoulos, Song-Chun Zhu
- **Comment**: Accepted in IJCV 2018
- **Journal**: None
- **Summary**: We propose a systematic learning-based approach to the generation of massive quantities of synthetic 3D scenes and arbitrary numbers of photorealistic 2D images thereof, with associated ground truth information, for the purposes of training, benchmarking, and diagnosing learning-based computer vision and robotics algorithms. In particular, we devise a learning-based pipeline of algorithms capable of automatically generating and rendering a potentially infinite variety of indoor scenes by using a stochastic grammar, represented as an attributed Spatial And-Or Graph, in conjunction with state-of-the-art physics-based rendering. Our pipeline is capable of synthesizing scene layouts with high diversity, and it is configurable inasmuch as it enables the precise customization and control of important attributes of the generated scenes. It renders photorealistic RGB images of the generated scenes while automatically synthesizing detailed, per-pixel ground truth data, including visible surface depth and normal, object identity, and material information (detailed to object parts), as well as environments (e.g., illuminations and camera viewpoints). We demonstrate the value of our synthesized dataset, by improving performance in certain machine-learning-based scene understanding tasks--depth and surface normal prediction, semantic segmentation, reconstruction, etc.--and by providing benchmarks for and diagnostics of trained models by modifying object attributes and scene properties in a controllable manner.



### sWSI: A Low-cost and Commercial-quality Whole Slide Imaging System on Android and iOS Smartphones
- **Arxiv ID**: http://arxiv.org/abs/1704.01088v1
- **DOI**: None
- **Categories**: **physics.bio-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.01088v1)
- **Published**: 2017-04-01 06:15:25+00:00
- **Updated**: 2017-04-01 06:15:25+00:00
- **Authors**: Shuoxin Ma, Tan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, scalable Whole Slide Imaging (sWSI), a novel high-throughput, cost-effective and robust whole slide imaging system on both Android and iOS platforms is introduced and analyzed. With sWSI, most mainstream smartphone connected to a optical eyepiece of any manually controlled microscope can be automatically controlled to capture sequences of mega-pixel fields of views that are synthesized into giga-pixel virtual slides. Remote servers carry out the majority of computation asynchronously to support clients running at satisfying frame rates without sacrificing image quality nor robustness. A typical 15x15mm sample can be digitized in 30 seconds with 4X or in 3 minutes with 10X object magnification, costing under $1. The virtual slide quality is considered comparable to existing high-end scanners thus satisfying for clinical usage by surveyed pathologies. The scan procedure with features such as supporting magnification up to 100x, recoding z-stacks, specimen-type-neutral and giving real-time feedback, is deemed work-flow-friendly and reliable.



### Multiple Instance Detection Network with Online Instance Classifier Refinement
- **Arxiv ID**: http://arxiv.org/abs/1704.00138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00138v1)
- **Published**: 2017-04-01 08:32:40+00:00
- **Updated**: 2017-04-01 08:32:40+00:00
- **Authors**: Peng Tang, Xinggang Wang, Xiang Bai, Wenyu Liu
- **Comment**: Accepted by CVPR 2017, IEEE Conference on Computer Vision and Pattern
  Recognition 2017
- **Journal**: None
- **Summary**: Of late, weakly supervised object detection is with great importance in object recognition. Based on deep learning, weakly supervised detectors have achieved many promising results. However, compared with fully supervised detection, it is more challenging to train deep network based detectors in a weakly supervised manner. Here we formulate weakly supervised detection as a Multiple Instance Learning (MIL) problem, where instance classifiers (object detectors) are put into the network as hidden nodes. We propose a novel online instance classifier refinement algorithm to integrate MIL and the instance classifier refinement procedure into a single deep network, and train the network end-to-end with only image-level supervision, i.e., without object location information. More precisely, instance labels inferred from weak supervision are propagated to their spatially overlapped instances to refine instance classifier online. The iterative instance classifier refinement procedure is implemented using multiple streams in deep network, where each stream supervises its latter stream. Weakly supervised object detection experiments are carried out on the challenging PASCAL VOC 2007 and 2012 benchmarks. We obtain 47% mAP on VOC 2007 that significantly outperforms the previous state-of-the-art.



### Compositional Human Pose Regression
- **Arxiv ID**: http://arxiv.org/abs/1704.00159v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00159v3)
- **Published**: 2017-04-01 11:59:41+00:00
- **Updated**: 2017-08-02 03:31:39+00:00
- **Authors**: Xiao Sun, Jiaxiang Shang, Shuang Liang, Yichen Wei
- **Comment**: Accepted by International Conference on Computer Vision (ICCV) 2017
- **Journal**: None
- **Summary**: Regression based methods are not performing as well as detection based methods for human pose estimation. A central problem is that the structural information in the pose is not well exploited in the previous regression methods. In this work, we propose a structure-aware regression approach. It adopts a reparameterized pose representation using bones instead of joints. It exploits the joint connection structure to define a compositional loss function that encodes the long range interactions in the pose. It is simple, effective, and general for both 2D and 3D pose estimation in a unified setting. Comprehensive evaluation validates the effectiveness of our approach. It significantly advances the state-of-the-art on Human3.6M and is competitive with state-of-the-art results on MPII.



### Complexity-Aware Assignment of Latent Values in Discriminative Models for Accurate Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.00180v1
- **DOI**: 10.1109/SIBGRAPI.2016.059
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00180v1)
- **Published**: 2017-04-01 15:15:38+00:00
- **Updated**: 2017-04-01 15:15:38+00:00
- **Authors**: Manoel Horta Ribeiro, Bruno Teixeira, Antônio Otávio Fernandes, Wagner Meira Jr., Erickson R. Nascimento
- **Comment**: Conference paper published at 2016 29th SIBGRAPI, Conference on
  Graphics, Patterns and Images (SIBGRAPI). 8 pages, 7 figures
- **Journal**: None
- **Summary**: Many of the state-of-the-art algorithms for gesture recognition are based on Conditional Random Fields (CRFs). Successful approaches, such as the Latent-Dynamic CRFs, extend the CRF by incorporating latent variables, whose values are mapped to the values of the labels. In this paper we propose a novel methodology to set the latent values according to the gesture complexity. We use an heuristic that iterates through the samples associated with each label value, stimating their complexity. We then use it to assign the latent values to the label values. We evaluate our method on the task of recognizing human gestures from video streams. The experiments were performed in binary datasets, generated by grouping different labels. Our results demonstrate that our approach outperforms the arbitrary one in many cases, increasing the accuracy by up to 10%.



