# Arxiv Papers in cs.CV on 2017-04-07
### Convolutional Neural Pyramid for Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1704.02071v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02071v1)
- **Published**: 2017-04-07 02:15:42+00:00
- **Updated**: 2017-04-07 02:15:42+00:00
- **Authors**: Xiaoyong Shen, Ying-Cong Chen, Xin Tao, Jiaya Jia
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a principled convolutional neural pyramid (CNP) framework for general low-level vision and image processing tasks. It is based on the essential finding that many applications require large receptive fields for structure understanding. But corresponding neural networks for regression either stack many layers or apply large kernels to achieve it, which is computationally very costly. Our pyramid structure can greatly enlarge the field while not sacrificing computation efficiency. Extra benefit includes adaptive network depth and progressive upsampling for quasi-realtime testing on VGA-size input. Our method profits a broad set of applications, such as depth/RGB image restoration, completion, noise/artifact removal, edge refinement, image filtering, image enhancement and colorization.



### Evolution in Groups: A deeper look at synaptic cluster driven evolution of deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1704.02081v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.02081v1)
- **Published**: 2017-04-07 03:28:02+00:00
- **Updated**: 2017-04-07 03:28:02+00:00
- **Authors**: Mohammad Javad Shafiee, Elnaz Barshan, Alexander Wong
- **Comment**: 8 pages. arXiv admin note: substantial text overlap with
  arXiv:1609.01360
- **Journal**: None
- **Summary**: A promising paradigm for achieving highly efficient deep neural networks is the idea of evolutionary deep intelligence, which mimics biological evolution processes to progressively synthesize more efficient networks. A crucial design factor in evolutionary deep intelligence is the genetic encoding scheme used to simulate heredity and determine the architectures of offspring networks. In this study, we take a deeper look at the notion of synaptic cluster-driven evolution of deep neural networks which guides the evolution process towards the formation of a highly sparse set of synaptic clusters in offspring networks. Utilizing a synaptic cluster-driven genetic encoding, the probabilistic encoding of synaptic traits considers not only individual synaptic properties but also inter-synaptic relationships within a deep neural network. This process results in highly sparse offspring networks which are particularly tailored for parallel computational devices such as GPUs and deep neural network accelerator chips. Comprehensive experimental results using four well-known deep neural network architectures (LeNet-5, AlexNet, ResNet-56, and DetectNet) on two different tasks (object categorization and object detection) demonstrate the efficiency of the proposed method. Cluster-driven genetic encoding scheme synthesizes networks that can achieve state-of-the-art performance with significantly smaller number of synapses than that of the original ancestor network. ($\sim$125-fold decrease in synapses for MNIST). Furthermore, the improved cluster efficiency in the generated offspring networks ($\sim$9.71-fold decrease in clusters for MNIST and a $\sim$8.16-fold decrease in clusters for KITTI) is particularly useful for accelerated performance on parallel computing hardware architectures such as those in GPUs and deep neural network accelerator chips.



### "RAPID" Regions-of-Interest Detection In Big Histopathological Images
- **Arxiv ID**: http://arxiv.org/abs/1704.02083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02083v1)
- **Published**: 2017-04-07 03:34:40+00:00
- **Updated**: 2017-04-07 03:34:40+00:00
- **Authors**: Li Sulimowicz, Ishfaq Ahmad
- **Comment**: 6 pages, 5 figures, ICME conference
- **Journal**: None
- **Summary**: The sheer volume and size of histopathological images (e.g.,10^6 MPixel) underscores the need for faster and more accurate Regions-of-interest (ROI) detection algorithms. In this paper, we propose such an algorithm, which has four main components that help achieve greater accuracy and faster speed: First, while using coarse-to-fine topology preserving segmentation as the baseline, the proposed algorithm uses a superpixel regularity optimization scheme for avoiding irregular and extremely small superpixels. Second, the proposed technique employs a prediction strategy to focus only on important superpixels at finer image levels. Third, the algorithm reuses the information gained from the coarsest image level at other finer image levels. Both the second and the third components drastically lower the complexity. Fourth, the algorithm employs a highly effective parallelization scheme using adap- tive data partitioning, which gains high speedup. Experimental results, conducted on the BSD500 [1] and 500 whole-slide histological images from the National Lung Screening Trial (NLST)1 dataset, confirm that the proposed algorithm gained 13 times speedup compared with the baseline, and around 160 times compared with SLIC [11], without losing accuracy.



### Supervised Deep Hashing for Hierarchical Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/1704.02088v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02088v3)
- **Published**: 2017-04-07 05:03:37+00:00
- **Updated**: 2017-09-12 07:53:35+00:00
- **Authors**: Dan Wang, Heyan Huang, Chi Lu, Bo-Si Feng, Liqiang Nie, Guihua Wen, Xian-Ling Mao
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Recently, hashing methods have been widely used in large-scale image retrieval. However, most existing hashing methods did not consider the hierarchical relation of labels, which means that they ignored the rich information stored in the hierarchy. Moreover, most of previous works treat each bit in a hash code equally, which does not meet the scenario of hierarchical labeled data. In this paper, we propose a novel deep hashing method, called supervised hierarchical deep hashing (SHDH), to perform hash code learning for hierarchical labeled data. Specifically, we define a novel similarity formula for hierarchical labeled data by weighting each layer, and design a deep convolutional neural network to obtain a hash code for each data point. Extensive experiments on several real-world public datasets show that the proposed method outperforms the state-of-the-art baselines in the image retrieval task.



### Generalized Rank Pooling for Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.02112v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02112v3)
- **Published**: 2017-04-07 07:10:53+00:00
- **Updated**: 2017-07-22 04:22:16+00:00
- **Authors**: Anoop Cherian, Basura Fernando, Mehrtash Harandi, Stephen Gould
- **Comment**: Accepted at IEEE International Conference on Computer Vision and
  Pattern Recognition (CVPR), 2017
- **Journal**: None
- **Summary**: Most popular deep models for action recognition split video sequences into short sub-sequences consisting of a few frames; frame-based features are then pooled for recognizing the activity. Usually, this pooling step discards the temporal order of the frames, which could otherwise be used for better recognition. Towards this end, we propose a novel pooling method, generalized rank pooling (GRP), that takes as input, features from the intermediate layers of a CNN that is trained on tiny sub-sequences, and produces as output the parameters of a subspace which (i) provides a low-rank approximation to the features and (ii) preserves their temporal order. We propose to use these parameters as a compact representation for the video sequence, which is then used in a classification setup. We formulate an objective for computing this subspace as a Riemannian optimization problem on the Grassmann manifold, and propose an efficient conjugate gradient scheme for solving it. Experiments on several activity recognition datasets show that our scheme leads to state-of-the-art performance.



### Partial Face Detection in the Mobile Domain
- **Arxiv ID**: http://arxiv.org/abs/1704.02117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02117v1)
- **Published**: 2017-04-07 07:43:11+00:00
- **Updated**: 2017-04-07 07:43:11+00:00
- **Authors**: Upal Mahbub, Sayantan Sarkar, Rama Chellappa
- **Comment**: 18 pages, 22 figures, 3 tables, submitted to IEEE Transactions on
  Image Processing
- **Journal**: None
- **Summary**: Generic face detection algorithms do not perform well in the mobile domain due to significant presence of occluded and partially visible faces. One promising technique to handle the challenge of partial faces is to design face detectors based on facial segments. In this paper two different approaches of facial segment-based face detection are discussed, namely, proposal-based detection and detection by end-to-end regression. Methods that follow the first approach rely on generating face proposals that contain facial segment information. The three detectors following this approach, namely Facial Segment-based Face Detector (FSFD), SegFace and DeepSegFace, discussed in this paper, perform binary classification on each proposal based on features learned from facial segments. The process of proposal generation, however, needs to be handled separately, which can be very time consuming, and is not truly necessary given the nature of the active authentication problem. Hence a novel algorithm, Deep Regression-based User Image Detector (DRUID) is proposed, which shifts from the classification to the regression paradigm, thus obviating the need for proposal generation. DRUID has an unique network architecture with customized loss functions, is trained using a relatively small amount of data by utilizing a novel data augmentation scheme and is fast since it outputs the bounding boxes of a face and its segments in a single pass. Being robust to occlusion by design, the facial segment-based face detection methods, especially DRUID show superior performance over other state-of-the-art face detectors in terms of precision-recall and ROC curve on two mobile face datasets.



### Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1704.02157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02157v1)
- **Published**: 2017-04-07 09:39:01+00:00
- **Updated**: 2017-04-07 09:39:01+00:00
- **Authors**: Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, Nicu Sebe
- **Comment**: Accepted as a spotlight paper at CVPR 2017
- **Journal**: None
- **Summary**: This paper addresses the problem of depth estimation from a single still image. Inspired by recent works on multi- scale convolutional neural networks (CNN), we propose a deep model which fuses complementary information derived from multiple CNN side outputs. Different from previous methods, the integration is obtained by means of continuous Conditional Random Fields (CRFs). In particular, we propose two different variations, one based on a cascade of multiple CRFs, the other on a unified graphical model. By designing a novel CNN implementation of mean-field updates for continuous CRFs, we show that both proposed models can be regarded as sequential deep networks and that training can be performed end-to-end. Through extensive experimental evaluation we demonstrate the effective- ness of the proposed approach and establish new state of the art results on publicly available datasets.



### ReLayNet: Retinal Layer and Fluid Segmentation of Macular Optical Coherence Tomography using Fully Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1704.02161v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02161v2)
- **Published**: 2017-04-07 09:50:05+00:00
- **Updated**: 2017-07-07 10:14:41+00:00
- **Authors**: Abhijit Guha Roy, Sailesh Conjeti, Sri Phani Krishna Karri, Debdoot Sheet, Amin Katouzian, Christian Wachinger, Nassir Navab
- **Comment**: Accepted for Publication at Biomedical Optics Express
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) is used for non-invasive diagnosis of diabetic macular edema assessing the retinal layers. In this paper, we propose a new fully convolutional deep architecture, termed ReLayNet, for end-to-end segmentation of retinal layers and fluid masses in eye OCT scans. ReLayNet uses a contracting path of convolutional blocks (encoders) to learn a hierarchy of contextual features, followed by an expansive path of convolutional blocks (decoders) for semantic segmentation. ReLayNet is trained to optimize a joint loss function comprising of weighted logistic regression and Dice overlap loss. The framework is validated on a publicly available benchmark dataset with comparisons against five state-of-the-art segmentation methods including two deep learning based approaches to substantiate its effectiveness.



### Egocentric Video Description based on Temporally-Linked Sequences
- **Arxiv ID**: http://arxiv.org/abs/1704.02163v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02163v3)
- **Published**: 2017-04-07 09:56:32+00:00
- **Updated**: 2017-11-09 09:55:39+00:00
- **Authors**: Marc Bolaños, Álvaro Peris, Francisco Casacuberta, Sergi Soler, Petia Radeva
- **Comment**: 19 pages, 10 figures, 3 tables. Submitted to Journal of Visual
  Communication and Image Representation
- **Journal**: None
- **Summary**: Egocentric vision consists in acquiring images along the day from a first person point-of-view using wearable cameras. The automatic analysis of this information allows to discover daily patterns for improving the quality of life of the user. A natural topic that arises in egocentric vision is storytelling, that is, how to understand and tell the story relying behind the pictures. In this paper, we tackle storytelling as an egocentric sequences description problem. We propose a novel methodology that exploits information from temporally neighboring events, matching precisely the nature of egocentric sequences. Furthermore, we present a new method for multimodal data fusion consisting on a multi-input attention recurrent network. We also publish the first dataset for egocentric image sequences description, consisting of 1,339 events with 3,991 descriptions, from 55 days acquired by 11 people. Furthermore, we prove that our proposal outperforms classical attentional encoder-decoder methods for video description.



### Semi-Latent GAN: Learning to generate and modify facial images from attributes
- **Arxiv ID**: http://arxiv.org/abs/1704.02166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02166v1)
- **Published**: 2017-04-07 10:04:06+00:00
- **Updated**: 2017-04-07 10:04:06+00:00
- **Authors**: Weidong Yin, Yanwei Fu, Leonid Sigal, Xiangyang Xue
- **Comment**: 10 pages, submitted to ICCV 2017
- **Journal**: None
- **Summary**: Generating and manipulating human facial images using high-level attributal controls are important and interesting problems. The models proposed in previous work can solve one of these two problems (generation or manipulation), but not both coherently. This paper proposes a novel model that learns how to both generate and modify the facial image from high-level semantic attributes. Our key idea is to formulate a Semi-Latent Facial Attribute Space (SL-FAS) to systematically learn relationship between user-defined and latent attributes, as well as between those attributes and RGB imagery. As part of this newly formulated space, we propose a new model --- SL-GAN which is a specific form of Generative Adversarial Network. Finally, we present an iterative training algorithm for SL-GAN. The experiments on recent CelebA and CASIA-WebFace datasets validate the effectiveness of our proposed framework. We will also make data, pre-trained models and code available.



### Could you guess an interesting movie from the posters?: An evaluation of vision-based features on movie poster database
- **Arxiv ID**: http://arxiv.org/abs/1704.02199v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02199v1)
- **Published**: 2017-04-07 12:17:38+00:00
- **Updated**: 2017-04-07 12:17:38+00:00
- **Authors**: Yuta Matsuzaki, Kazushige Okayasu, Takaaki Imanari, Naomichi Kobayashi, Yoshihiro Kanehara, Ryousuke Takasawa, Akio Nakamura, Hirokatsu Kataoka
- **Comment**: 4 pages, 4 figures
- **Journal**: None
- **Summary**: In this paper, we aim to estimate the Winner of world-wide film festival from the exhibited movie poster. The task is an extremely challenging because the estimation must be done with only an exhibited movie poster, without any film ratings and box-office takings. In order to tackle this problem, we have created a new database which is consist of all movie posters included in the four biggest film festivals. The movie poster database (MPDB) contains historic movies over 80 years which are nominated a movie award at each year. We apply a couple of feature types, namely hand-craft, mid-level and deep feature to extract various information from a movie poster. Our experiments showed suggestive knowledge, for example, the Academy award estimation can be better rate with a color feature and a facial emotion feature generally performs good rate on the MPDB. The paper may suggest a possibility of modeling human taste for a movie recommendation.



### Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor
- **Arxiv ID**: http://arxiv.org/abs/1704.02201v2
- **DOI**: 10.1109/ICCV.2017.131
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02201v2)
- **Published**: 2017-04-07 12:23:03+00:00
- **Updated**: 2017-10-05 14:05:06+00:00
- **Authors**: Franziska Mueller, Dushyant Mehta, Oleksandr Sotnychenko, Srinath Sridhar, Dan Casas, Christian Theobalt
- **Comment**: Accepted at the International Conference on Computer Vision (ICCV)
  2017
- **Journal**: None
- **Summary**: We present an approach for real-time, robust and accurate hand pose estimation from moving egocentric RGB-D cameras in cluttered real environments. Existing methods typically fail for hand-object interactions in cluttered scenes imaged from egocentric viewpoints, common for virtual or augmented reality applications. Our approach uses two subsequently applied Convolutional Neural Networks (CNNs) to localize the hand and regress 3D joint locations. Hand localization is achieved by using a CNN to estimate the 2D position of the hand center in the input, even in the presence of clutter and occlusions. The localized hand position, together with the corresponding input depth value, is used to generate a normalized cropped image that is fed into a second CNN to regress relative 3D hand joint locations in real time. For added accuracy, robustness and temporal stability, we refine the pose estimates using a kinematic pose tracking energy. To train the CNNs, we introduce a new photorealistic dataset that uses a merged reality approach to capture and synthesize large amounts of annotated data of natural hand interaction in cluttered scenes. Through quantitative and qualitative evaluation, we show that our method is robust to self-occlusion and occlusions by objects, particularly in moving egocentric perspectives.



### Privacy-Preserving Visual Learning Using Doubly Permuted Homomorphic Encryption
- **Arxiv ID**: http://arxiv.org/abs/1704.02203v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1704.02203v2)
- **Published**: 2017-04-07 12:26:51+00:00
- **Updated**: 2017-07-28 15:39:23+00:00
- **Authors**: Ryo Yonetani, Vishnu Naresh Boddeti, Kris M. Kitani, Yoichi Sato
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: We propose a privacy-preserving framework for learning visual classifiers by leveraging distributed private image data. This framework is designed to aggregate multiple classifiers updated locally using private data and to ensure that no private information about the data is exposed during and after its learning procedure. We utilize a homomorphic cryptosystem that can aggregate the local classifiers while they are encrypted and thus kept secret. To overcome the high computational cost of homomorphic encryption of high-dimensional classifiers, we (1) impose sparsity constraints on local classifier updates and (2) propose a novel efficient encryption scheme named doubly-permuted homomorphic encryption (DPHE) which is tailored to sparse high-dimensional data. DPHE (i) decomposes sparse data into its constituent non-zero values and their corresponding support indices, (ii) applies homomorphic encryption only to the non-zero values, and (iii) employs double permutations on the support indices to make them secret. Our experimental evaluation on several public datasets shows that the proposed approach achieves comparable performance against state-of-the-art visual recognition methods while preserving privacy and significantly outperforms other privacy-preserving methods.



### High-Quality Correspondence and Segmentation Estimation for Dual-Lens Smart-Phone Portraits
- **Arxiv ID**: http://arxiv.org/abs/1704.02205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02205v1)
- **Published**: 2017-04-07 12:36:42+00:00
- **Updated**: 2017-04-07 12:36:42+00:00
- **Authors**: Xiaoyong Shen, Hongyun Gao, Xin Tao, Chao Zhou, Jiaya Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating correspondence between two images and extracting the foreground object are two challenges in computer vision. With dual-lens smart phones, such as iPhone 7Plus and Huawei P9, coming into the market, two images of slightly different views provide us new information to unify the two topics. We propose a joint method to tackle them simultaneously via a joint fully connected conditional random field (CRF) framework. The regional correspondence is used to handle textureless regions in matching and make our CRF system computationally efficient. Our method is evaluated over 2,000 new image pairs, and produces promising results on challenging portrait images.



### Investigating Natural Image Pleasantness Recognition using Deep Features and Eye Tracking for Loosely Controlled Human-computer Interaction
- **Arxiv ID**: http://arxiv.org/abs/1704.02218v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02218v1)
- **Published**: 2017-04-07 13:16:17+00:00
- **Updated**: 2017-04-07 13:16:17+00:00
- **Authors**: Hamed R. Tavakoli, Jorma Laaksonen, Esa Rahtu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper revisits recognition of natural image pleasantness by employing deep convolutional neural networks and affordable eye trackers. There exist several approaches to recognize image pleasantness: (1) computer vision, and (2) psychophysical signals. For natural images, computer vision approaches have not been as successful as for abstract paintings and is lagging behind the psychophysical signals like eye movements. Despite better results, the scalability of eye movements is adversely affected by the sensor cost. While the introduction of affordable sensors have helped the scalability issue by making the sensors more accessible, the application of such sensors in a loosely controlled human-computer interaction setup is not yet studied for affective image tagging. On the other hand, deep convolutional neural networks have boosted the performance of vision-based techniques significantly in recent years. To investigate the current status in regard to affective image tagging, we (1) introduce a new eye movement dataset using an affordable eye tracker, (2) study the use of deep neural networks for pleasantness recognition, (3) investigate the gap between deep features and eye movements. To meet these ends, we record eye movements in a less controlled setup, akin to daily human-computer interaction. We assess features from eye movements, visual features, and their combination. Our results show that (1) recognizing natural image pleasantness from eye movement under less restricted setup is difficult and previously used techniques are prone to fail, and (2) visual class categories are strong cues for predicting pleasantness, due to their correlation with emotions, necessitating careful study of this phenomenon. This latter finding is alerting as some deep learning approaches may fit to the class category bias.



### Hand3D: Hand Pose Estimation using 3D Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1704.02224v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02224v1)
- **Published**: 2017-04-07 13:27:48+00:00
- **Updated**: 2017-04-07 13:27:48+00:00
- **Authors**: Xiaoming Deng, Shuo Yang, Yinda Zhang, Ping Tan, Liang Chang, Hongan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel 3D neural network architecture for 3D hand pose estimation from a single depth image. Different from previous works that mostly run on 2D depth image domain and require intermediate or post process to bring in the supervision from 3D space, we convert the depth map to a 3D volumetric representation, and feed it into a 3D convolutional neural network(CNN) to directly produce the pose in 3D requiring no further process. Our system does not require the ground truth reference point for initialization, and our network architecture naturally integrates both local feature and global context in 3D space. To increase the coverage of the hand pose space of the training data, we render synthetic depth image by transferring hand pose from existing real image datasets. We evaluation our algorithm on two public benchmarks and achieve the state-of-the-art performance. The synthetic hand pose dataset will be available.



### Clothing and People - A Social Signal Processing Perspective
- **Arxiv ID**: http://arxiv.org/abs/1704.02231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02231v1)
- **Published**: 2017-04-07 13:45:52+00:00
- **Updated**: 2017-04-07 13:45:52+00:00
- **Authors**: Maedeh Aghaei, Federico Parezzan, Mariella Dimiccoli, Petia Radeva, Marco Cristani
- **Comment**: To appear in the 12th IEEE International Conference on Automatic Face
  and Gesture Recognition (FG 2017)
- **Journal**: None
- **Summary**: In our society and century, clothing is not anymore used only as a means for body protection. Our paper builds upon the evidence, studied within the social sciences, that clothing brings a clear communicative message in terms of social signals, influencing the impression and behaviour of others towards a person. In fact, clothing correlates with personality traits, both in terms of self-assessment and assessments that unacquainted people give to an individual. The consequences of these facts are important: the influence of clothing on the decision making of individuals has been investigated in the literature, showing that it represents a discriminative factor to differentiate among diverse groups of people. Unfortunately, this has been observed after cumbersome and expensive manual annotations, on very restricted populations, limiting the scope of the resulting claims. With this position paper, we want to sketch the main steps of the very first systematic analysis, driven by social signal processing techniques, of the relationship between clothing and social signals, both sent and perceived. Thanks to human parsing technologies, which exhibit high robustness owing to deep learning architectures, we are now capable to isolate visual patterns characterising a large types of garments. These algorithms will be used to capture statistical relations on a large corpus of evidence to confirm the sociological findings and to go beyond the state of the art.



### Learned Watershed: End-to-End Learning of Seeded Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.02249v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02249v2)
- **Published**: 2017-04-07 14:40:15+00:00
- **Updated**: 2017-09-04 08:10:24+00:00
- **Authors**: Steffen Wolf, Lukas Schott, Ullrich Köthe, Fred Hamprecht
- **Comment**: The first two authors contributed equally
- **Journal**: None
- **Summary**: Learned boundary maps are known to outperform hand- crafted ones as a basis for the watershed algorithm. We show, for the first time, how to train watershed computation jointly with boundary map prediction. The estimator for the merging priorities is cast as a neural network that is con- volutional (over space) and recurrent (over iterations). The latter allows learning of complex shape priors. The method gives the best known seeded segmentation results on the CREMI segmentation challenge.



### Deep Unsupervised Similarity Learning using Partially Ordered Sets
- **Arxiv ID**: http://arxiv.org/abs/1704.02268v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02268v3)
- **Published**: 2017-04-07 15:44:51+00:00
- **Updated**: 2017-04-11 12:39:41+00:00
- **Authors**: Miguel A Bautista, Artsiom Sanakoyeu, Björn Ommer
- **Comment**: Accepted for publication at IEEE Computer Vision and Pattern
  Recognition 2017
- **Journal**: None
- **Summary**: Unsupervised learning of visual similarities is of paramount importance to computer vision, particularly due to lacking training data for fine-grained similarities. Deep learning of similarities is often based on relationships between pairs or triplets of samples. Many of these relations are unreliable and mutually contradicting, implying inconsistencies when trained without supervision information that relates different tuples or triplets to each other. To overcome this problem, we use local estimates of reliable (dis-)similarities to initially group samples into compact surrogate classes and use local partial orders of samples to classes to link classes to each other. Similarity learning is then formulated as a partial ordering task with soft correspondences of all samples to classes. Adopting a strategy of self-supervision, a CNN is trained to optimally represent samples in a mutually consistent manner while updating the classes. The similarity learning and grouping procedure are integrated in a single model and optimized jointly. The proposed unsupervised approach shows competitive performance on detailed pose estimation and object classification.



### DeepCoder: Semi-parametric Variational Autoencoders for Automatic Facial Action Coding
- **Arxiv ID**: http://arxiv.org/abs/1704.02206v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02206v2)
- **Published**: 2017-04-07 16:23:56+00:00
- **Updated**: 2017-08-05 04:26:08+00:00
- **Authors**: Dieu Linh Tran, Robert Walecki, Ognjen Rudovic, Stefanos Eleftheriadis, Bjørn Schuller, Maja Pantic
- **Comment**: ICCV 2017 - accepted
- **Journal**: None
- **Summary**: Human face exhibits an inherent hierarchy in its representations (i.e., holistic facial expressions can be encoded via a set of facial action units (AUs) and their intensity). Variational (deep) auto-encoders (VAE) have shown great results in unsupervised extraction of hierarchical latent representations from large amounts of image data, while being robust to noise and other undesired artifacts. Potentially, this makes VAEs a suitable approach for learning facial features for AU intensity estimation. Yet, most existing VAE-based methods apply classifiers learned separately from the encoded features. By contrast, the non-parametric (probabilistic) approaches, such as Gaussian Processes (GPs), typically outperform their parametric counterparts, but cannot deal easily with large amounts of data. To this end, we propose a novel VAE semi-parametric modeling framework, named DeepCoder, which combines the modeling power of parametric (convolutional) and nonparametric (ordinal GPs) VAEs, for joint learning of (1) latent representations at multiple levels in a task hierarchy1, and (2) classification of multiple ordinal outputs. We show on benchmark datasets for AU intensity estimation that the proposed DeepCoder outperforms the state-of-the-art approaches, and related VAEs and deep learning models.



### It Takes (Only) Two: Adversarial Generator-Encoder Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.02304v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.02304v3)
- **Published**: 2017-04-07 17:38:29+00:00
- **Updated**: 2017-11-06 15:05:03+00:00
- **Authors**: Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new autoencoder-type architecture that is trainable in an unsupervised mode, sustains both generation and inference, and has the quality of conditional and unconditional samples boosted by adversarial learning. Unlike previous hybrids of autoencoders and adversarial networks, the adversarial game in our approach is set up directly between the encoder and the generator, and no external mappings are trained in the process of learning. The game objective compares the divergences of each of the real and the generated data distributions with the prior distribution in the latent space. We show that direct generator-vs-encoder game leads to a tight coupling of the two components, resulting in samples and reconstructions of a comparable quality to some recently-proposed more complex architectures.



### Automated Unsupervised Segmentation of Liver Lesions in CT scans via Cahn-Hilliard Phase Separation
- **Arxiv ID**: http://arxiv.org/abs/1704.02348v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02348v1)
- **Published**: 2017-04-07 18:59:13+00:00
- **Updated**: 2017-04-07 18:59:13+00:00
- **Authors**: Jana Lipková, Markus Rempfler, Patrick Christ, John Lowengrub, Bjoern H. Menze
- **Comment**: None
- **Journal**: None
- **Summary**: The segmentation of liver lesions is crucial for detection, diagnosis and monitoring progression of liver cancer. However, design of accurate automated methods remains challenging due to high noise in CT scans, low contrast between liver and lesions, as well as large lesion variability. We propose a 3D automatic, unsupervised method for liver lesions segmentation using a phase separation approach. It is assumed that liver is a mixture of two phases: healthy liver and lesions, represented by different image intensities polluted by noise. The Cahn-Hilliard equation is used to remove the noise and separate the mixture into two distinct phases with well-defined interfaces. This simplifies the lesion detection and segmentation task drastically and enables to segment liver lesions by thresholding the Cahn-Hilliard solution. The method was tested on 3Dircadb and LITS dataset.



### Three-Dimensional Segmentation of Vesicular Networks of Fungal Hyphae in Macroscopic Microscopy Image Stacks
- **Arxiv ID**: http://arxiv.org/abs/1704.02356v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02356v1)
- **Published**: 2017-04-07 19:47:02+00:00
- **Updated**: 2017-04-07 19:47:02+00:00
- **Authors**: P. Saponaro, W. Treible, A. Kolagunda, S. Rhein, J. Caplan, C. Kambhamettu, R. Wisser
- **Comment**: This is submitted to ICIP 2017
- **Journal**: None
- **Summary**: Automating the extraction and quantification of features from three-dimensional (3-D) image stacks is a critical task for advancing computer vision research. The union of 3-D image acquisition and analysis enables the quantification of biological resistance of a plant tissue to fungal infection through the analysis of attributes such as fungal penetration depth, fungal mass, and branching of the fungal network of connected cells. From an image processing perspective, these tasks reduce to segmentation of vessel-like structures and the extraction of features from their skeletonization. In order to sample multiple infection events for analysis, we have developed an approach we refer to as macroscopic microscopy. However, macroscopic microscopy produces high-resolution image stacks that pose challenges to routine approaches and are difficult for a human to annotate to obtain ground truth data. We present a synthetic hyphal network generator, a comparison of several vessel segmentation methods, and a minimum spanning tree method for connecting small gaps resulting from imperfections in imaging or incomplete skeletonization of hyphal networks. Qualitative results are shown for real microscopic data. We believe the comparison of vessel detectors on macroscopic microscopy data, the synthetic vessel generator, and the gap closing technique are beneficial to the image processing community.



### Pixelwise Instance Segmentation with a Dynamically Instantiated Network
- **Arxiv ID**: http://arxiv.org/abs/1704.02386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02386v1)
- **Published**: 2017-04-07 22:14:09+00:00
- **Updated**: 2017-04-07 22:14:09+00:00
- **Authors**: Anurag Arnab, Philip H. S Torr
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Semantic segmentation and object detection research have recently achieved rapid progress. However, the former task has no notion of different instances of the same object, and the latter operates at a coarse, bounding-box level. We propose an Instance Segmentation system that produces a segmentation map where each pixel is assigned an object class and instance identity label. Most approaches adapt object detectors to produce segments instead of boxes. In contrast, our method is based on an initial semantic segmentation module, which feeds into an instance subnetwork. This subnetwork uses the initial category-level segmentation, along with cues from the output of an object detector, within an end-to-end CRF to predict instances. This part of our model is dynamically instantiated to produce a variable number of instances per image. Our end-to-end approach requires no post-processing and considers the image holistically, instead of processing independent proposals. Therefore, unlike some related work, a pixel cannot belong to multiple instances. Furthermore, far more precise segmentations are achieved, as shown by our state-of-the-art results (particularly at high IoU thresholds) on the Pascal VOC and Cityscapes datasets.



### Learning Where to Look: Data-Driven Viewpoint Set Selection for 3D Scenes
- **Arxiv ID**: http://arxiv.org/abs/1704.02393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02393v1)
- **Published**: 2017-04-07 22:40:46+00:00
- **Updated**: 2017-04-07 22:40:46+00:00
- **Authors**: Kyle Genova, Manolis Savva, Angel X. Chang, Thomas Funkhouser
- **Comment**: ICCV submission, combined main paper and supplemental material
- **Journal**: None
- **Summary**: The use of rendered images, whether from completely synthetic datasets or from 3D reconstructions, is increasingly prevalent in vision tasks. However, little attention has been given to how the selection of viewpoints affects the performance of rendered training sets. In this paper, we propose a data-driven approach to view set selection. Given a set of example images, we extract statistics describing their contents and generate a set of views matching the distribution of those statistics. Motivated by semantic segmentation tasks, we model the spatial distribution of each semantic object category within an image view volume. We provide a search algorithm that generates a sampling of likely candidate views according to the example distribution, and a set selection algorithm that chooses a subset of the candidates that jointly cover the example distribution. Results of experiments with these algorithms on SUNCG indicate that they are indeed able to produce view distributions similar to an example set from NYUDv2 according to the earth mover's distance. Furthermore, the selected views improve performance on semantic segmentation compared to alternative view selection algorithms.



### GoDP: Globally optimized dual pathway system for facial landmark localization in-the-wild
- **Arxiv ID**: http://arxiv.org/abs/1704.02402v2
- **DOI**: 10.1016/j.imavis.2017.12.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.02402v2)
- **Published**: 2017-04-07 23:39:29+00:00
- **Updated**: 2017-12-19 15:14:18+00:00
- **Authors**: Yuhang Wu, Shishir K. Shah, Ioannis A. Kakadiaris
- **Comment**: Accepted by Image and Vision Computing in December, 2017
- **Journal**: Image and Vision Computing, 2018
- **Summary**: Facial landmark localization is a fundamental module for pose-invariant face recognition. The most common approach for facial landmark detection is cascaded regression, which is composed of two steps: feature extraction and facial shape regression. Recent methods employ deep convolutional networks to extract robust features for each step, while the whole system could be regarded as a deep cascaded regression architecture. In this work, instead of employing a deep regression network, a Globally Optimized Dual-Pathway (GoDP) deep architecture is proposed to identify the target pixels through solving a cascaded pixel labeling problem without resorting to high-level inference models or complex stacked architecture. The proposed end-to-end system relies on distance-aware softmax functions and dual-pathway proposal-refinement architecture. Results show that it outperforms the state-of-the-art cascaded regression-based methods on multiple in-the-wild face alignment databases. The model achieves 1.84 normalized mean error (NME) on the AFLW database, which outperforms 3DDFA by 61.8%. Experiments on face identification demonstrate that GoDP, coupled with DPM-headhunter, is able to improve rank-1 identification rate by 44.2% compared to Dlib toolbox on a challenging database.



