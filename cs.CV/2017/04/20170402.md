# Arxiv Papers in cs.CV on 2017-04-02
### A-Lamp: Adaptive Layout-Aware Multi-Patch Deep Convolutional Neural Network for Photo Aesthetic Assessment
- **Arxiv ID**: http://arxiv.org/abs/1704.00248v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00248v1)
- **Published**: 2017-04-02 03:12:18+00:00
- **Updated**: 2017-04-02 03:12:18+00:00
- **Authors**: Shuang Ma, Jing Liu, Chang Wen Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNN) have recently been shown to generate promising results for aesthetics assessment. However, the performance of these deep CNN methods is often compromised by the constraint that the neural network only takes the fixed-size input. To accommodate this requirement, input images need to be transformed via cropping, warping, or padding, which often alter image composition, reduce image resolution, or cause image distortion. Thus the aesthetics of the original images is impaired because of potential loss of fine grained details and holistic image layout. However, such fine grained details and holistic image layout is critical for evaluating an image's aesthetics. In this paper, we present an Adaptive Layout-Aware Multi-Patch Convolutional Neural Network (A-Lamp CNN) architecture for photo aesthetic assessment. This novel scheme is able to accept arbitrary sized images, and learn from both fined grained details and holistic image layout simultaneously. To enable training on these hybrid inputs, we extend the method by developing a dedicated double-subnet neural network structure, i.e. a Multi-Patch subnet and a Layout-Aware subnet. We further construct an aggregation layer to effectively combine the hybrid features from these two subnets. Extensive experiments on the large-scale aesthetics assessment benchmark (AVA) demonstrate significant performance improvement over the state-of-the-art in photo aesthetic assessment.



### Aligned Image-Word Representations Improve Inductive Transfer Across Vision-Language Tasks
- **Arxiv ID**: http://arxiv.org/abs/1704.00260v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.00260v2)
- **Published**: 2017-04-02 08:01:30+00:00
- **Updated**: 2017-10-16 05:34:24+00:00
- **Authors**: Tanmay Gupta, Kevin Shih, Saurabh Singh, Derek Hoiem
- **Comment**: Accepted in ICCV 2017. The arxiv version has an extra analysis on
  correlation with human attention
- **Journal**: None
- **Summary**: An important goal of computer vision is to build systems that learn visual representations over time that can be applied to many tasks. In this paper, we investigate a vision-language embedding as a core representation and show that it leads to better cross-task transfer than standard multi-task learning. In particular, the task of visual recognition is aligned to the task of visual question answering by forcing each to use the same word-region embeddings. We show this leads to greater inductive transfer from recognition to VQA than standard multitask learning. Visual recognition also improves, especially for categories that have relatively few recognition training labels but appear often in the VQA setting. Thus, our paper takes a small step towards creating more general vision systems by showing the benefit of interpretable, flexible, and trainable core representations.



### SAR image despeckling through convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1704.00275v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00275v2)
- **Published**: 2017-04-02 09:44:05+00:00
- **Updated**: 2017-05-03 15:53:02+00:00
- **Authors**: G. Chierchia, D. Cozzolino, G. Poggi, L. Verdoliva
- **Comment**: Accepted at 2017 IEEE International Geoscience and Remote Sensing
  Symposium, Fort Worth, Texas, July 23-28, 2017
- **Journal**: None
- **Summary**: In this paper we investigate the use of discriminative model learning through Convolutional Neural Networks (CNNs) for SAR image despeckling. The network uses a residual learning strategy, hence it does not recover the filtered image, but the speckle component, which is then subtracted from the noisy one. Training is carried out by considering a large multitemporal SAR image and its multilook version, in order to approximate a clean image. Experimental results, both on synthetic and real SAR data, show the method to achieve better performance with respect to state-of-the-art techniques.



### The Stixel world: A medium-level representation of traffic scenes
- **Arxiv ID**: http://arxiv.org/abs/1704.00280v1
- **DOI**: 10.1016/j.imavis.2017.01.009
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00280v1)
- **Published**: 2017-04-02 10:38:49+00:00
- **Updated**: 2017-04-02 10:38:49+00:00
- **Authors**: Marius Cordts, Timo Rehfeld, Lukas Schneider, David Pfeiffer, Markus Enzweiler, Stefan Roth, Marc Pollefeys, Uwe Franke
- **Comment**: Accepted for publication in Image and Vision Computing
- **Journal**: None
- **Summary**: Recent progress in advanced driver assistance systems and the race towards autonomous vehicles is mainly driven by two factors: (1) increasingly sophisticated algorithms that interpret the environment around the vehicle and react accordingly, and (2) the continuous improvements of sensor technology itself. In terms of cameras, these improvements typically include higher spatial resolution, which as a consequence requires more data to be processed. The trend to add multiple cameras to cover the entire surrounding of the vehicle is not conducive in that matter. At the same time, an increasing number of special purpose algorithms need access to the sensor input data to correctly interpret the various complex situations that can occur, particularly in urban traffic.   By observing those trends, it becomes clear that a key challenge for vision architectures in intelligent vehicles is to share computational resources. We believe this challenge should be faced by introducing a representation of the sensory data that provides compressed and structured access to all relevant visual content of the scene. The Stixel World discussed in this paper is such a representation. It is a medium-level model of the environment that is specifically designed to compress information about obstacles by leveraging the typical layout of outdoor traffic scenes. It has proven useful for a multitude of automotive vision applications, including object detection, tracking, segmentation, and mapping.   In this paper, we summarize the ideas behind the model and generalize it to take into account multiple dense input streams: the image itself, stereo depth maps, and semantic class probability maps that can be generated, e.g., by CNNs. Our generalization is embedded into a novel mathematical formulation for the Stixel model. We further sketch how the free parameters of the model can be learned using structured SVMs.



### Efficient Version-Space Reduction for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1704.00299v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00299v1)
- **Published**: 2017-04-02 14:00:48+00:00
- **Updated**: 2017-04-02 14:00:48+00:00
- **Authors**: Kourosh Meshgi, Shigeyuki Oba, Shin Ishii
- **Comment**: CRV'17 Conference
- **Journal**: None
- **Summary**: Discrminative trackers, employ a classification approach to separate the target from its background. To cope with variations of the target shape and appearance, the classifier is updated online with different samples of the target and the background. Sample selection, labeling and updating the classifier is prone to various sources of errors that drift the tracker. We introduce the use of an efficient version space shrinking strategy to reduce the labeling errors and enhance its sampling strategy by measuring the uncertainty of the tracker about the samples. The proposed tracker, utilize an ensemble of classifiers that represents different hypotheses about the target, diversify them using boosting to provide a larger and more consistent coverage of the version-space and tune the classifiers' weights in voting. The proposed system adjusts the model update rate by promoting the co-training of the short-memory ensemble with a long-memory oracle. The proposed tracker outperformed state-of-the-art trackers on different sequences bearing various tracking challenges.



### People Counting in Crowded and Outdoor Scenes using a Hybrid Multi-Camera Approach
- **Arxiv ID**: http://arxiv.org/abs/1704.00326v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00326v2)
- **Published**: 2017-04-02 16:38:04+00:00
- **Updated**: 2017-05-08 12:51:51+00:00
- **Authors**: Fabio Dittrich, Luiz E. S. de Oliveira, Alceu S. Britto Jr., Alessandro L. Koerich
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents two novel approaches for people counting in crowded and open environments that combine the information gathered by multiple views. Multiple camera are used to expand the field of view as well as to mitigate the problem of occlusion that commonly affects the performance of counting methods using single cameras. The first approach is regarded as a direct approach and it attempts to segment and count each individual in the crowd. For such an aim, two head detectors trained with head images are employed: one based on support vector machines and another based on Adaboost perceptron. The second approach, regarded as an indirect approach employs learning algorithms and statistical analysis on the whole crowd to achieve counting. For such an aim, corner points are extracted from groups of people in a foreground image and computed by a learning algorithm which estimates the number of people in the scene. Both approaches count the number of people on the scene and not only on a given image or video frame of the scene. The experimental results obtained on the benchmark PETS2009 video dataset show that proposed indirect method surpasses other methods with improvements of up to 46.7% and provides accurate counting results for the crowded scenes. On the other hand, the direct method shows high error rates due to the fact that the latter has much more complex problems to solve, such as segmentation of heads.



### Randomness in Deconvolutional Networks for Visual Representation
- **Arxiv ID**: http://arxiv.org/abs/1704.00330v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00330v3)
- **Published**: 2017-04-02 17:13:55+00:00
- **Updated**: 2018-02-20 07:19:05+00:00
- **Authors**: Kun He, Jingbo Wang, Haochuan Li, Yao Shu, Mengxiao Zhang, Man Zhu, Liwei Wang, John E. Hopcroft
- **Comment**: 15 pages, 10 Figures, submitted to a 2018 Conference
- **Journal**: None
- **Summary**: Toward a deeper understanding on the inner work of deep neural networks, we investigate CNN (convolutional neural network) using DCN (deconvolutional network) and randomization technique, and gain new insights for the intrinsic property of this network architecture. For the random representations of an untrained CNN, we train the corresponding DCN to reconstruct the input images. Compared with the image inversion on pre-trained CNN, our training converges faster and the yielding network exhibits higher quality for image reconstruction. It indicates there is rich information encoded in the random features; the pre-trained CNN may discard information irrelevant for classification and encode relevant features in a way favorable for classification but harder for reconstruction. We further explore the property of the overall random CNN-DCN architecture. Surprisingly, images can be inverted with satisfactory quality. Extensive empirical evidence as well as theoretical analysis are provided.



### Restoration of Images with Wavefront Aberrations
- **Arxiv ID**: http://arxiv.org/abs/1704.00331v1
- **DOI**: None
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.00331v1)
- **Published**: 2017-04-02 17:19:02+00:00
- **Updated**: 2017-04-02 17:19:02+00:00
- **Authors**: Claudius Zelenka, Reinhard Koch
- **Comment**: To appear in the proceedings of the 23rd International Conference on
  Pattern Recognition (ICPR 2016)
- **Journal**: None
- **Summary**: This contribution deals with image restoration in optical systems with coherent illumination, which is an important topic in astronomy, coherent microscopy and radar imaging. Such optical systems suffer from wavefront distortions, which are caused by imperfect imaging components and conditions. Known image restoration algorithms work well for incoherent imaging, they fail in case of coherent images. In this paper a novel wavefront correction algorithm is presented, which allows image restoration under coherent conditions. In most coherent imaging systems, especially in astronomy, the wavefront deformation is known. Using this information, the proposed algorithm allows a high quality restoration even in case of severe wavefront distortions. We present two versions of this algorithm, which are an evolution of the Gerchberg-Saxton and the Hybrid-Input-Output algorithm. The algorithm is verified on simulated and real microscopic images.



### Dense Multi-view 3D-reconstruction Without Dense Correspondences
- **Arxiv ID**: http://arxiv.org/abs/1704.00337v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00337v1)
- **Published**: 2017-04-02 17:56:47+00:00
- **Updated**: 2017-04-02 17:56:47+00:00
- **Authors**: Yvain Quéau, Jean Mélou, Jean-Denis Durou, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a variational method for multi-view shape-from-shading under natural illumination. The key idea is to couple PDE-based solutions for single-image based shape-from-shading problems across multiple images and multiple color channels by means of a variational formulation. Rather than alternatingly solving the individual SFS problems and optimizing the consistency across images and channels which is known to lead to suboptimal results, we propose an efficient solution of the coupled problem by means of an ADMM algorithm. In numerous experiments on both simulated and real imagery, we demonstrate that the proposed fusion of multiple-view reconstruction and shape-from-shading provides highly accurate dense reconstructions without the need to compute dense correspondences. With the proposed variational integration across multiple views shape-from-shading techniques become applicable to challenging real-world reconstruction problems, giving rise to highly detailed geometry even in areas of smooth brightness variation and lacking texture.



### Hidden Two-Stream Convolutional Networks for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.00389v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1704.00389v4)
- **Published**: 2017-04-02 23:39:51+00:00
- **Updated**: 2018-10-30 04:55:03+00:00
- **Authors**: Yi Zhu, Zhenzhong Lan, Shawn Newsam, Alexander G. Hauptmann
- **Comment**: Accepted at ACCV 2018, camera ready. Code available at
  https://github.com/bryanyzhu/Hidden-Two-Stream
- **Journal**: None
- **Summary**: Analyzing videos of human actions involves understanding the temporal relationships among video frames. State-of-the-art action recognition approaches rely on traditional optical flow estimation methods to pre-compute motion information for CNNs. Such a two-stage approach is computationally expensive, storage demanding, and not end-to-end trainable. In this paper, we present a novel CNN architecture that implicitly captures motion information between adjacent frames. We name our approach hidden two-stream CNNs because it only takes raw video frames as input and directly predicts action classes without explicitly computing optical flow. Our end-to-end approach is 10x faster than its two-stage baseline. Experimental results on four challenging action recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show that our approach significantly outperforms the previous best real-time approaches.



### Geometric Loss Functions for Camera Pose Regression with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1704.00390v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00390v2)
- **Published**: 2017-04-02 23:58:22+00:00
- **Updated**: 2017-05-23 13:45:48+00:00
- **Authors**: Alex Kendall, Roberto Cipolla
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Deep learning has shown to be effective for robust and real-time monocular image relocalisation. In particular, PoseNet is a deep convolutional neural network which learns to regress the 6-DOF camera pose from a single image. It learns to localize using high level features and is robust to difficult lighting, motion blur and unknown camera intrinsics, where point based SIFT registration fails. However, it was trained using a naive loss function, with hyper-parameters which require expensive tuning. In this paper, we give the problem a more fundamental theoretical treatment. We explore a number of novel loss functions for learning camera pose which are based on geometry and scene reprojection error. Additionally we show how to automatically learn an optimal weighting to simultaneously regress position and orientation. By leveraging geometry, we demonstrate that our technique significantly improves PoseNet's performance across datasets ranging from indoor rooms to a small city.



