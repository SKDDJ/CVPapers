# Arxiv Papers in cs.CV on 2017-04-27
### Single image depth estimation by dilated deep residual convolutional neural network and soft-weight-sum inference
- **Arxiv ID**: http://arxiv.org/abs/1705.00534v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.00534v1)
- **Published**: 2017-04-27 06:07:05+00:00
- **Updated**: 2017-04-27 06:07:05+00:00
- **Authors**: Bo Li, Yuchao Dai, Huahui Chen, Mingyi He
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a new residual convolutional neural network (CNN) architecture for single image depth estimation. Compared with existing deep CNN based methods, our method achieves much better results with fewer training examples and model parameters. The advantages of our method come from the usage of dilated convolution, skip connection architecture and soft-weight-sum inference. Experimental evaluation on the NYU Depth V2 dataset shows that our method outperforms other state-of-the-art methods by a margin.



### Locality Preserving Projections for Grassmann manifold
- **Arxiv ID**: http://arxiv.org/abs/1704.08458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08458v1)
- **Published**: 2017-04-27 07:24:35+00:00
- **Updated**: 2017-04-27 07:24:35+00:00
- **Authors**: Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Haoran Chen, Baocai Yin
- **Comment**: Accepted by IJCAI 2017
- **Journal**: None
- **Summary**: Learning on Grassmann manifold has become popular in many computer vision tasks, with the strong capability to extract discriminative information for imagesets and videos. However, such learning algorithms particularly on high-dimensional Grassmann manifold always involve with significantly high computational cost, which seriously limits the applicability of learning on Grassmann manifold in more wide areas. In this research, we propose an unsupervised dimensionality reduction algorithm on Grassmann manifold based on the Locality Preserving Projections (LPP) criterion. LPP is a commonly used dimensionality reduction algorithm for vector-valued data, aiming to preserve local structure of data in the dimension-reduced space. The strategy is to construct a mapping from higher dimensional Grassmann manifold into the one in a relative low-dimensional with more discriminative capability. The proposed method can be optimized as a basic eigenvalue problem. The performance of our proposed method is assessed on several classification and clustering tasks and the experimental results show its clear advantages over other Grassmann based algorithms.



### No More Discrimination: Cross City Adaptation of Road Scene Segmenters
- **Arxiv ID**: http://arxiv.org/abs/1704.08509v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1704.08509v1)
- **Published**: 2017-04-27 11:14:21+00:00
- **Updated**: 2017-04-27 11:14:21+00:00
- **Authors**: Yi-Hsin Chen, Wei-Yu Chen, Yu-Ting Chen, Bo-Cheng Tsai, Yu-Chiang Frank Wang, Min Sun
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: Despite the recent success of deep-learning based semantic segmentation, deploying a pre-trained road scene segmenter to a city whose images are not presented in the training set would not achieve satisfactory performance due to dataset biases. Instead of collecting a large number of annotated images of each city of interest to train or refine the segmenter, we propose an unsupervised learning approach to adapt road scene segmenters across different cities. By utilizing Google Street View and its time-machine feature, we can collect unannotated images for each road scene at different times, so that the associated static-object priors can be extracted accordingly. By advancing a joint global and class-specific domain adversarial learning framework, adaptation of pre-trained segmenters to that city can be achieved without the need of any user annotation or interaction. We show that our method improves the performance of semantic segmentation in multiple cities across continents, while it performs favorably against state-of-the-art approaches requiring annotated training data.



### ICNet for Real-Time Semantic Segmentation on High-Resolution Images
- **Arxiv ID**: http://arxiv.org/abs/1704.08545v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08545v2)
- **Published**: 2017-04-27 13:02:49+00:00
- **Updated**: 2018-08-20 03:34:25+00:00
- **Authors**: Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, Jiaya Jia
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: We focus on the challenging task of real-time semantic segmentation in this paper. It finds many practical applications and yet is with fundamental difficulty of reducing a large portion of computation for pixel-wise label inference. We propose an image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. We provide in-depth analysis of our framework and introduce the cascade feature fusion unit to quickly achieve high-quality segmentation. Our system yields real-time inference on a single GPU card with decent quality results evaluated on challenging datasets like Cityscapes, CamVid and COCO-Stuff.



### BAM! The Behance Artistic Media Dataset for Recognition Beyond Photography
- **Arxiv ID**: http://arxiv.org/abs/1704.08614v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08614v2)
- **Published**: 2017-04-27 15:05:30+00:00
- **Updated**: 2017-07-09 02:48:53+00:00
- **Authors**: Michael J. Wilber, Chen Fang, Hailin Jin, Aaron Hertzmann, John Collomosse, Serge Belongie
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision systems are designed to work well within the context of everyday photography. However, artists often render the world around them in ways that do not resemble photographs. Artwork produced by people is not constrained to mimic the physical world, making it more challenging for machines to recognize.   This work is a step toward teaching machines how to categorize images in ways that are valuable to humans. First, we collect a large-scale dataset of contemporary artwork from Behance, a website containing millions of portfolios from professional and commercial artists. We annotate Behance imagery with rich attribute labels for content, emotions, and artistic media. Furthermore, we carry out baseline experiments to show the value of this dataset for artistic style prediction, for improving the generality of existing object classifiers, and for the study of visual domain adaptation. We believe our Behance Artistic Media dataset will be a good starting point for researchers wishing to study artistic imagery and relevant problems.



### Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics
- **Arxiv ID**: http://arxiv.org/abs/1704.08615v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1704.08615v2)
- **Published**: 2017-04-27 15:07:42+00:00
- **Updated**: 2018-07-25 13:31:14+00:00
- **Authors**: Matthias Kümmerer, Thomas S. A. Wallis, Matthias Bethge
- **Comment**: published at ECCV 2018
- **Journal**: None
- **Summary**: Dozens of new models on fixation prediction are published every year and compared on open benchmarks such as MIT300 and LSUN. However, progress in the field can be difficult to judge because models are compared using a variety of inconsistent metrics. Here we show that no single saliency map can perform well under all metrics. Instead, we propose a principled approach to solve the benchmarking problem by separating the notions of saliency models, maps and metrics. Inspired by Bayesian decision theory, we define a saliency model to be a probabilistic model of fixation density prediction and a saliency map to be a metric-specific prediction derived from the model density which maximizes the expected performance on that metric given the model density. We derive these optimal saliency maps for the most commonly used saliency metrics (AUC, sAUC, NSS, CC, SIM, KL-Div) and show that they can be computed analytically or approximated with high precision. We show that this leads to consistent rankings in all metrics and avoids the penalties of using one saliency map for all metrics. Our method allows researchers to have their model compete on many different metrics with state-of-the-art in those metrics: "good" models will perform well in all metrics.



### End-to-End Multimodal Emotion Recognition using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.08619v1
- **DOI**: 10.1109/JSTSP.2017.2764438
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1704.08619v1)
- **Published**: 2017-04-27 15:14:33+00:00
- **Updated**: 2017-04-27 15:14:33+00:00
- **Authors**: Panagiotis Tzirakis, George Trigeorgis, Mihalis A. Nicolaou, Björn Schuller, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic affect recognition is a challenging task due to the various modalities emotions can be expressed with. Applications can be found in many domains including multimedia retrieval and human computer interaction. In recent years, deep neural networks have been used with great success in determining emotional states. Inspired by this success, we propose an emotion recognition system using auditory and visual modalities. To capture the emotional content for various styles of speaking, robust features need to be extracted. To this purpose, we utilize a Convolutional Neural Network (CNN) to extract features from the speech, while for the visual modality a deep residual network (ResNet) of 50 layers. In addition to the importance of feature extraction, a machine learning algorithm needs also to be insensitive to outliers while being able to model the context. To tackle this problem, Long Short-Term Memory (LSTM) networks are utilized. The system is then trained in an end-to-end fashion where - by also taking advantage of the correlations of the each of the streams - we manage to significantly outperform the traditional approaches based on auditory and visual handcrafted features for the prediction of spontaneous and natural emotions on the RECOLA database of the AVEC 2016 research challenge on emotion recognition.



### Full-Page Text Recognition: Learning Where to Start and When to Stop
- **Arxiv ID**: http://arxiv.org/abs/1704.08628v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08628v1)
- **Published**: 2017-04-27 15:50:37+00:00
- **Updated**: 2017-04-27 15:50:37+00:00
- **Authors**: Bastien Moysset, Christopher Kermorvant, Christian Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Text line detection and localization is a crucial step for full page document analysis, but still suffers from heterogeneity of real life documents. In this paper, we present a new approach for full page text recognition. Localization of the text lines is based on regressions with Fully Convolutional Neural Networks and Multidimensional Long Short-Term Memory as contextual layers. In order to increase the efficiency of this localization method, only the position of the left side of the text lines are predicted. The text recognizer is then in charge of predicting the end of the text to recognize. This method has shown good results for full page text recognition on the highly heterogeneous Maurdor dataset.



### Sparse Hierachical Extrapolated Parametric Methods for Cortical Data Analysis
- **Arxiv ID**: http://arxiv.org/abs/1704.08631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08631v1)
- **Published**: 2017-04-27 15:52:23+00:00
- **Updated**: 2017-04-27 15:52:23+00:00
- **Authors**: Nicolas Honnorat, Christos Davatzikos
- **Comment**: Technical report (ongoing work)
- **Journal**: None
- **Summary**: Many neuroimaging studies focus on the cortex, in order to benefit from better signal to noise ratios and reduced computational burden. Cortical data are usually projected onto a reference mesh, where subsequent analyses are carried out. Several multiscale approaches have been proposed for analyzing these surface data, such as spherical harmonics and graph wavelets. As far as we know, however, the hierarchical structure of the template icosahedral meshes used by most neuroimaging software has never been exploited for cortical data factorization. In this paper, we demonstrate how the structure of the ubiquitous icosahedral meshes can be exploited by data factorization methods such as sparse dictionary learning, and we assess the optimization speed-up offered by extrapolation methods in this context. By testing different sparsity-inducing norms, extrapolation methods, and factorization schemes, we compare the performances of eleven methods for analyzing four datasets: two structural and two functional MRI datasets obtained by processing the data publicly available for the hundred unrelated subjects of the Human Connectome Project. Our results demonstrate that, depending on the level of details requested, a speedup of several orders of magnitudes can be obtained.



### Deep Functional Maps: Structured Prediction for Dense Shape Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1704.08686v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08686v2)
- **Published**: 2017-04-27 17:56:20+00:00
- **Updated**: 2017-07-30 07:45:23+00:00
- **Authors**: Or Litany, Tal Remez, Emanuele Rodolà, Alex M. Bronstein, Michael M. Bronstein
- **Comment**: Accepted for publication at ICCV 2017
- **Journal**: None
- **Summary**: We introduce a new framework for learning dense correspondence between deformable 3D shapes. Existing learning based approaches model shape correspondence as a labelling problem, where each point of a query shape receives a label identifying a point on some reference domain; the correspondence is then constructed a posteriori by composing the label predictions of two input shapes. We propose a paradigm shift and design a structured prediction model in the space of functional maps, linear operators that provide a compact representation of the correspondence. We model the learning process via a deep residual network which takes dense descriptor fields defined on two shapes as input, and outputs a soft map between the two given objects. The resulting correspondence is shown to be accurate on several challenging benchmarks comprising multiple categories, synthetic models, real scans with acquisition artifacts, topological noise, and partiality.



### Action Understanding with Multiple Classes of Actors
- **Arxiv ID**: http://arxiv.org/abs/1704.08723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08723v1)
- **Published**: 2017-04-27 19:20:50+00:00
- **Updated**: 2017-04-27 19:20:50+00:00
- **Authors**: Chenliang Xu, Caiming Xiong, Jason J. Corso
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the rapid progress, existing works on action understanding focus strictly on one type of action agent, which we call actor---a human adult, ignoring the diversity of actions performed by other actors. To overcome this narrow viewpoint, our paper marks the first effort in the computer vision community to jointly consider algorithmic understanding of various types of actors undergoing various actions. To begin with, we collect a large annotated Actor-Action Dataset (A2D) that consists of 3782 short videos and 31 temporally untrimmed long videos. We formulate the general actor-action understanding problem and instantiate it at various granularities: video-level single- and multiple-label actor-action recognition, and pixel-level actor-action segmentation. We propose and examine a comprehensive set of graphical models that consider the various types of interplay among actors and actions. Our findings have led us to conclusive evidence that the joint modeling of actor and action improves performance over modeling each of them independently, and further improvement can be obtained by considering the multi-scale natural in video understanding. Hence, our paper concludes the argument of the value of explicit consideration of various actors in comprehensive action understanding and provides a dataset and a benchmark for later works exploring this new problem.



### Compressive Sensing Approaches for Autonomous Object Detection in Video Sequences
- **Arxiv ID**: http://arxiv.org/abs/1705.00002v1
- **DOI**: 10.1109/SDF.2015.7347706
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.00002v1)
- **Published**: 2017-04-27 20:24:33+00:00
- **Updated**: 2017-04-27 20:24:33+00:00
- **Authors**: Danil Kuzin, Olga Isupova, Lyudmila Mihaylova
- **Comment**: SDF 2015
- **Journal**: None
- **Summary**: Video analytics requires operating with large amounts of data. Compressive sensing allows to reduce the number of measurements required to represent the video using the prior knowledge of sparsity of the original signal, but it imposes certain conditions on the design matrix. The Bayesian compressive sensing approach relaxes the limitations of the conventional approach using the probabilistic reasoning and allows to include different prior knowledge about the signal structure. This paper presents two Bayesian compressive sensing methods for autonomous object detection in a video sequence from a static camera. Their performance is compared on the real datasets with the non-Bayesian greedy algorithm. It is shown that the Bayesian methods can provide the same accuracy as the greedy algorithm but much faster; or if the computational time is not critical they can provide more accurate results.



### Improving Facial Attribute Prediction using Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.08740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08740v1)
- **Published**: 2017-04-27 20:41:50+00:00
- **Updated**: 2017-04-27 20:41:50+00:00
- **Authors**: Mahdi M. Kalayeh, Boqing Gong, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Attributes are semantically meaningful characteristics whose applicability widely crosses category boundaries. They are particularly important in describing and recognizing concepts where no explicit training example is given, \textit{e.g., zero-shot learning}. Additionally, since attributes are human describable, they can be used for efficient human-computer interaction. In this paper, we propose to employ semantic segmentation to improve facial attribute prediction. The core idea lies in the fact that many facial attributes describe local properties. In other words, the probability of an attribute to appear in a face image is far from being uniform in the spatial domain. We build our facial attribute prediction model jointly with a deep semantic segmentation network. This harnesses the localization cues learned by the semantic segmentation to guide the attention of the attribute prediction to the regions where different attributes naturally show up. As a result of this approach, in addition to recognition, we are able to localize the attributes, despite merely having access to image level labels (weak supervision) during training. We evaluate our proposed method on CelebA and LFWA datasets and achieve superior results to the prior arts. Furthermore, we show that in the reverse problem, semantic face parsing improves when facial attributes are available. That reaffirms the need to jointly model these two interconnected tasks.



### Obstacle Avoidance through Deep Networks based Intermediate Perception
- **Arxiv ID**: http://arxiv.org/abs/1704.08759v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.08759v1)
- **Published**: 2017-04-27 21:55:07+00:00
- **Updated**: 2017-04-27 21:55:07+00:00
- **Authors**: Shichao Yang, Sandeep Konam, Chen Ma, Stephanie Rosenthal, Manuela Veloso, Sebastian Scherer
- **Comment**: None
- **Journal**: None
- **Summary**: Obstacle avoidance from monocular images is a challenging problem for robots. Though multi-view structure-from-motion could build 3D maps, it is not robust in textureless environments. Some learning based methods exploit human demonstration to predict a steering command directly from a single image. However, this method is usually biased towards certain tasks or demonstration scenarios and also biased by human understanding. In this paper, we propose a new method to predict a trajectory from images. We train our system on more diverse NYUv2 dataset. The ground truth trajectory is computed from the designed cost functions automatically. The Convolutional Neural Network perception is divided into two stages: first, predict depth map and surface normal from RGB images, which are two important geometric properties related to 3D obstacle representation. Second, predict the trajectory from the depth and normal. Results show that our intermediate perception increases the accuracy by 20% than the direct prediction. Our model generalizes well to other public indoor datasets and is also demonstrated for robot flights in simulation and experiments.



### GazeDirector: Fully Articulated Eye Gaze Redirection in Video
- **Arxiv ID**: http://arxiv.org/abs/1704.08763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08763v1)
- **Published**: 2017-04-27 22:23:53+00:00
- **Updated**: 2017-04-27 22:23:53+00:00
- **Authors**: Erroll Wood, Tadas Baltrusaitis, Louis-Philippe Morency, Peter Robinson, Andreas Bulling
- **Comment**: None
- **Journal**: None
- **Summary**: We present GazeDirector, a new approach for eye gaze redirection that uses model-fitting. Our method first tracks the eyes by fitting a multi-part eye region model to video frames using analysis-by-synthesis, thereby recovering eye region shape, texture, pose, and gaze simultaneously. It then redirects gaze by 1) warping the eyelids from the original image using a model-derived flow field, and 2) rendering and compositing synthesized 3D eyeballs onto the output image in a photorealistic manner. GazeDirector allows us to change where people are looking without person-specific training data, and with full articulation, i.e. we can precisely specify new gaze directions in 3D. Quantitatively, we evaluate both model-fitting and gaze synthesis, with experiments for gaze estimation and redirection on the Columbia gaze dataset. Qualitatively, we compare GazeDirector against recent work on gaze redirection, showing better results especially for large redirection angles. Finally, we demonstrate gaze redirection on YouTube videos by introducing new 3D gaze targets and by manipulating visual behavior.



### Deep Face Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1704.08772v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.08772v2)
- **Published**: 2017-04-27 23:01:45+00:00
- **Updated**: 2017-05-25 07:45:36+00:00
- **Authors**: Grigorios G. Chrysos, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Blind deblurring consists a long studied task, however the outcomes of generic methods are not effective in real world blurred images. Domain-specific methods for deblurring targeted object categories, e.g. text or faces, frequently outperform their generic counterparts, hence they are attracting an increasing amount of attention. In this work, we develop such a domain-specific method to tackle deblurring of human faces, henceforth referred to as face deblurring. Studying faces is of tremendous significance in computer vision, however face deblurring has yet to demonstrate some convincing results. This can be partly attributed to the combination of i) poor texture and ii) highly structure shape that yield the contour/gradient priors (that are typically used) sub-optimal. In our work instead of making assumptions over the prior, we adopt a learning approach by inserting weak supervision that exploits the well-documented structure of the face. Namely, we utilise a deep network to perform the deblurring and employ a face alignment technique to pre-process each face. We additionally surpass the requirement of the deep network for thousands training samples, by introducing an efficient framework that allows the generation of a large dataset. We utilised this framework to create 2MF2, a dataset of over two million frames. We conducted experiments with real world blurred facial images and report that our method returns a result close to the sharp natural latent image.



