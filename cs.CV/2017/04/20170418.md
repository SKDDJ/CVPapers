# Arxiv Papers in cs.CV on 2017-04-18
### Video Object Segmentation using Supervoxel-Based Gerrymandering
- **Arxiv ID**: http://arxiv.org/abs/1704.05165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05165v1)
- **Published**: 2017-04-18 01:11:35+00:00
- **Updated**: 2017-04-18 01:11:35+00:00
- **Authors**: Brent A. Griffin, Jason J. Corso
- **Comment**: None
- **Journal**: None
- **Summary**: Pixels operate locally. Superpixels have some potential to collect information across many pixels; supervoxels have more potential by implicitly operating across time. In this paper, we explore this well established notion thoroughly analyzing how supervoxels can be used in place of and in conjunction with other means of aggregating information across space-time. Focusing on the problem of strictly unsupervised video object segmentation, we devise a method called supervoxel gerrymandering that links masks of foregroundness and backgroundness via local and non-local consensus measures. We pose and answer a series of critical questions about the ability of supervoxels to adequately sway local voting; the questions regard type and scale of supervoxels as well as local versus non-local consensus, and the questions are posed in a general way so as to impact the broader knowledge of the use of supervoxels in video understanding. We work with the DAVIS dataset and find that our analysis yields an unsupervised method that outperforms all other known unsupervised methods and even many supervised ones.



### Joint Semantic and Motion Segmentation for dynamic scenes using Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.08331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.08331v1)
- **Published**: 2017-04-18 03:06:03+00:00
- **Updated**: 2017-04-18 03:06:03+00:00
- **Authors**: Nazrul Haque, N Dinesh Reddy, K. Madhava Krishna
- **Comment**: In Proceedings of the 12th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications - Volume 5:
  Visapp, (Visigrapp 2017)
- **Journal**: None
- **Summary**: Dynamic scene understanding is a challenging problem and motion segmentation plays a crucial role in solving it. Incorporating semantics and motion enhances the overall perception of the dynamic scene. For applications of outdoor robotic navigation, joint learning methods have not been extensively used for extracting spatio-temporal features or adding different priors into the formulation. The task becomes even more challenging without stereo information being incorporated. This paper proposes an approach to fuse semantic features and motion clues using CNNs, to address the problem of monocular semantic motion segmentation. We deduce semantic and motion labels by integrating optical flow as a constraint with semantic features into dilated convolution network. The pipeline consists of three main stages i.e Feature extraction, Feature amplification and Multi Scale Context Aggregation to fuse the semantics and flow features. Our joint formulation shows significant improvements in monocular motion segmentation over the state of the art methods on challenging KITTI tracking dataset.



### Deep Self-Taught Learning for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1704.05188v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05188v2)
- **Published**: 2017-04-18 03:30:28+00:00
- **Updated**: 2017-04-30 06:23:53+00:00
- **Authors**: Zequn Jie, Yunchao Wei, Xiaojie Jin, Jiashi Feng, Wei Liu
- **Comment**: Accepted as spotlight paper by CVPR 2017
- **Journal**: None
- **Summary**: Most existing weakly supervised localization (WSL) approaches learn detectors by finding positive bounding boxes based on features learned with image-level supervision. However, those features do not contain spatial location related information and usually provide poor-quality positive samples for training a detector. To overcome this issue, we propose a deep self-taught learning approach, which makes the detector learn the object-level features reliable for acquiring tight positive samples and afterwards re-train itself based on them. Consequently, the detector progressively improves its detection ability and localizes more informative positive samples. To implement such self-taught learning, we propose a seed sample acquisition method via image-to-object transferring and dense subgraph discovery to find reliable positive samples for initializing the detector. An online supportive sample harvesting scheme is further proposed to dynamically select the most confident tight positive samples and train the detector in a mutual boosting way. To prevent the detector from being trapped in poor optima due to overfitting, we propose a new relative improvement of predicted CNN scores for guiding the self-taught learning process. Extensive experiments on PASCAL 2007 and 2012 show that our approach outperforms the state-of-the-arts, strongly validating its effectiveness.



### Fast 2-D Complex Gabor Filter with Kernel Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1704.05231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05231v1)
- **Published**: 2017-04-18 08:34:33+00:00
- **Updated**: 2017-04-18 08:34:33+00:00
- **Authors**: Suhyuk Um, Jaeyoon Kim, Dongbo Min
- **Comment**: None
- **Journal**: None
- **Summary**: 2-D complex Gabor filtering has found numerous applications in the fields of computer vision and image processing. Especially, in some applications, it is often needed to compute 2-D complex Gabor filter bank consisting of the 2-D complex Gabor filtering outputs at multiple orientations and frequencies. Although several approaches for fast 2-D complex Gabor filtering have been proposed, they primarily focus on reducing the runtime of performing the 2-D complex Gabor filtering once at specific orientation and frequency. To obtain the 2-D complex Gabor filter bank output, existing methods are repeatedly applied with respect to multiple orientations and frequencies. In this paper, we propose a novel approach that efficiently computes the 2-D complex Gabor filter bank by reducing the computational redundancy that arises when performing the Gabor filtering at multiple orientations and frequencies. The proposed method first decomposes the Gabor basis kernels to allow a fast convolution with the Gaussian kernel in a separable manner. This enables reducing the runtime of the 2-D complex Gabor filter bank by reusing intermediate results of the 2-D complex Gabor filtering computed at a specific orientation. Furthermore, we extend this idea into 2-D localized sliding discrete Fourier transform (SDFT) using the Gaussian kernel in the DFT computation, which lends a spatial localization ability as in the 2-D complex Gabor filter. Experimental results demonstrate that our method runs faster than state-of-the-arts methods for fast 2-D complex Gabor filtering, while maintaining similar filtering quality.



### Robust Optical Flow Estimation in Rainy Scenes
- **Arxiv ID**: http://arxiv.org/abs/1704.05239v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05239v2)
- **Published**: 2017-04-18 09:04:02+00:00
- **Updated**: 2017-11-28 15:39:08+00:00
- **Authors**: Ruoteng Li, Robby T. Tan, Loong-Fah Cheong
- **Comment**: 9 pages, CVPR
- **Journal**: None
- **Summary**: Optical flow estimation in the rainy scenes is challenging due to background degradation introduced by rain streaks and rain accumulation effects in the scene. Rain accumulation effect refers to poor visibility of remote objects due to the intense rainfall. Most existing optical flow methods are erroneous when applied to rain sequences because the conventional brightness constancy constraint (BCC) and gradient constancy constraint (GCC) generally break down in this situation. Based on the observation that the RGB color channels receive raindrop radiance equally, we introduce a residue channel as a new data constraint to reduce the effect of rain streaks. To handle rain accumulation, our method decomposes the image into a piecewise-smooth background layer and a high-frequency detail layer. It also enforces the BCC on the background layer only. Results on both synthetic dataset and real images show that our algorithm outperforms existing methods on different types of rain sequences. To our knowledge, this is the first optical flow method specifically dealing with rain.



### Image Fusion With Cosparse Analysis Operator
- **Arxiv ID**: http://arxiv.org/abs/1704.05240v1
- **DOI**: 10.1109/LSP.2017.2696055
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1704.05240v1)
- **Published**: 2017-04-18 09:05:09+00:00
- **Updated**: 2017-04-18 09:05:09+00:00
- **Authors**: Rui Gao, Sergiy A. Vorobyov, Hong Zhao
- **Comment**: 12 pages, 4 figures, 1 table, Submitted to IEEE Signal Processing
  Letters on December 2016
- **Journal**: IEEE Signal Processing Letters, vol. 24, no. 7, pp. 943-947, July
  2017
- **Summary**: The paper addresses the image fusion problem, where multiple images captured with different focus distances are to be combined into a higher quality all-in-focus image. Most current approaches for image fusion strongly rely on the unrealistic noise-free assumption used during the image acquisition, and then yield limited robustness in fusion processing. In our approach, we formulate the multi-focus image fusion problem in terms of an analysis sparse model, and simultaneously perform the restoration and fusion of multi-focus images. Based on this model, we propose an analysis operator learning, and define a novel fusion function to generate an all-in-focus image. Experimental evaluations confirm the effectiveness of the proposed fusion approach both visually and quantitatively, and show that our approach outperforms state-of-the-art fusion methods.



### A Comment on "Analysis of Video Image Sequences Using Point and Line Correspondences"
- **Arxiv ID**: http://arxiv.org/abs/1704.05267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05267v1)
- **Published**: 2017-04-18 10:56:14+00:00
- **Updated**: 2017-04-18 10:56:14+00:00
- **Authors**: Mieczysław A. Kłopotek
- **Comment**: None
- **Journal**: preliminary version of: M.A. K{\l}opotek: A comment on "Analysis
  of video image sequences using point and line correspondences". Pattern
  Recognition 28(1995)2, pp. 283-292
- **Summary**: In this paper we would like to deny the results of Wang et al. raising two fundamental claims:   * A line does not contribute anything to recognition of motion parameters from two images   * Four traceable points are not sufficient to recover motion parameters from two perspective   To be constructive, however, we show that four traceable points are sufficient to recover motion parameters from two frames under orthogonal projection and that five points are sufficient to simplify the solution of the two-frame problem under orthogonal projection to solving a linear equation system.



### Unsupervised Learning by Predicting Noise
- **Arxiv ID**: http://arxiv.org/abs/1704.05310v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.05310v1)
- **Published**: 2017-04-18 12:51:47+00:00
- **Updated**: 2017-04-18 12:51:47+00:00
- **Authors**: Piotr Bojanowski, Armand Joulin
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, training these networks requires significant amounts of supervision. This paper introduces a generic framework to train deep networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with state-of-the-art unsupervised methods on ImageNet and Pascal VOC.



### Interactive Outlining of Pancreatic Cancer Liver Metastases in Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/1704.05368v1
- **DOI**: 10.1038/s41598-017-00940-z
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1704.05368v1)
- **Published**: 2017-04-18 14:45:20+00:00
- **Updated**: 2017-04-18 14:45:20+00:00
- **Authors**: Jan Egger, Dieter Schmalstieg, Xiaojun Chen, Wolfram G. Zoller, Alexander Hann
- **Comment**: 15 pages, 16 figures, 2 tables, 58 references
- **Journal**: Sci. Rep. 7, 892, 2017
- **Summary**: Ultrasound (US) is the most commonly used liver imaging modality worldwide. Due to its low cost, it is increasingly used in the follow-up of cancer patients with metastases localized in the liver. In this contribution, we present the results of an interactive segmentation approach for liver metastases in US acquisitions. A (semi-) automatic segmentation is still very challenging because of the low image quality and the low contrast between the metastasis and the surrounding liver tissue. Thus, the state of the art in clinical practice is still manual measurement and outlining of the metastases in the US images. We tackle the problem by providing an interactive segmentation approach providing real-time feedback of the segmentation results. The approach has been evaluated with typical US acquisitions from the clinical routine, and the datasets consisted of pancreatic cancer metastases. Even for difficult cases, satisfying segmentations results could be achieved because of the interactive real-time behavior of the approach. In total, 40 clinical images have been evaluated with our method by comparing the results against manual ground truth segmentations. This evaluation yielded to an average Dice Score of 85% and an average Hausdorff Distance of 13 pixels.



### Ranking to Learn: Feature Ranking and Selection via Eigenvector Centrality
- **Arxiv ID**: http://arxiv.org/abs/1704.05409v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.05409v1)
- **Published**: 2017-04-18 16:21:05+00:00
- **Updated**: 2017-04-18 16:21:05+00:00
- **Authors**: Giorgio Roffo, Simone Melzi
- **Comment**: Preprint version - Lecture Notes in Computer Science - Springer 2017
- **Journal**: New Frontiers in Mining Complex Patterns, Fifth International
  workshop, nfMCP2016. Lecture Notes in Computer Science - Springer
- **Summary**: In an era where accumulating data is easy and storing it inexpensive, feature selection plays a central role in helping to reduce the high-dimensionality of huge amounts of otherwise meaningless data. In this paper, we propose a graph-based method for feature selection that ranks features by identifying the most important ones into arbitrary set of cues. Mapping the problem on an affinity graph-where features are the nodes-the solution is given by assessing the importance of nodes through some indicators of centrality, in particular, the Eigen-vector Centrality (EC). The gist of EC is to estimate the importance of a feature as a function of the importance of its neighbors. Ranking central nodes individuates candidate features, which turn out to be effective from a classification point of view, as proved by a thoroughly experimental section. Our approach has been tested on 7 diverse datasets from recent literature (e.g., biological data and object recognition, among others), and compared against filter, embedded and wrappers methods. The results are remarkable in terms of accuracy, stability and low execution time.



### Light Field Blind Motion Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1704.05416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05416v1)
- **Published**: 2017-04-18 16:40:59+00:00
- **Updated**: 2017-04-18 16:40:59+00:00
- **Authors**: Pratul P. Srinivasan, Ren Ng, Ravi Ramamoorthi
- **Comment**: To be presented at CVPR 2017
- **Journal**: None
- **Summary**: We study the problem of deblurring light fields of general 3D scenes captured under 3D camera motion and present both theoretical and practical contributions. By analyzing the motion-blurred light field in the primal and Fourier domains, we develop intuition into the effects of camera motion on the light field, show the advantages of capturing a 4D light field instead of a conventional 2D image for motion deblurring, and derive simple methods of motion deblurring in certain cases. We then present an algorithm to blindly deblur light fields of general scenes without any estimation of scene geometry, and demonstrate that we can recover both the sharp light field and the 3D camera motion path of real and synthetically-blurred light fields.



### Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art
- **Arxiv ID**: http://arxiv.org/abs/1704.05519v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1704.05519v3)
- **Published**: 2017-04-18 20:33:50+00:00
- **Updated**: 2021-03-17 19:16:56+00:00
- **Authors**: Joel Janai, Fatma Güney, Aseem Behl, Andreas Geiger
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed enormous progress in AI-related fields such as computer vision, machine learning, and autonomous vehicles. As with any rapidly growing field, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several survey papers on particular sub-problems have appeared, no comprehensive survey on problems, datasets, and methods in computer vision for autonomous vehicles has been published. This book attempts to narrow this gap by providing a survey on the state-of-the-art datasets and techniques. Our survey includes both the historically most relevant literature as well as the current state of the art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding, and end-to-end learning for autonomous driving. Towards this goal, we analyze the performance of the state of the art on several challenging benchmarking datasets, including KITTI, MOT, and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we also provide a website that allows navigating topics as well as methods and provides additional information.



### Learning to Reason: End-to-End Module Networks for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1704.05526v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05526v3)
- **Published**: 2017-04-18 20:57:32+00:00
- **Updated**: 2017-09-11 22:22:59+00:00
- **Authors**: Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Kate Saenko
- **Comment**: None
- **Journal**: None
- **Summary**: Natural language questions are inherently compositional, and many are most easily answered by reasoning about their decomposition into modular sub-problems. For example, to answer "is there an equal number of balls and boxes?" we can look for balls, look for boxes, count them, and compare the results. The recently proposed Neural Module Network (NMN) architecture implements this approach to question answering by parsing questions into linguistic substructures and assembling question-specific deep networks from smaller modules that each solve one subtask. However, existing NMN implementations rely on brittle off-the-shelf parsers, and are restricted to the module configurations proposed by these parsers rather than learning them from data. In this paper, we propose End-to-End Module Networks (N2NMNs), which learn to reason by directly predicting instance-specific network layouts without the aid of a parser. Our model learns to generate network structures (by imitating expert demonstrations) while simultaneously learning network parameters (using the downstream task loss). Experimental results on the new CLEVR dataset targeted at compositional question answering show that N2NMNs achieve an error reduction of nearly 50% relative to state-of-the-art attentional approaches, while discovering interpretable network architectures specialized for each question.



### Annotating Object Instances with a Polygon-RNN
- **Arxiv ID**: http://arxiv.org/abs/1704.05548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05548v1)
- **Published**: 2017-04-18 22:17:28+00:00
- **Updated**: 2017-04-18 22:17:28+00:00
- **Authors**: Lluis Castrejon, Kaustav Kundu, Raquel Urtasun, Sanja Fidler
- **Comment**: None
- **Journal**: CVPR 2017
- **Summary**: We propose an approach for semi-automatic annotation of object instances. While most current methods treat object segmentation as a pixel-labeling problem, we here cast it as a polygon prediction task, mimicking how most current datasets have been annotated. In particular, our approach takes as input an image crop and sequentially produces vertices of the polygon outlining the object. This allows a human annotator to interfere at any time and correct a vertex if needed, producing as accurate segmentation as desired by the annotator. We show that our approach speeds up the annotation process by a factor of 4.7 across all classes in Cityscapes, while achieving 78.4% agreement in IoU with original ground-truth, matching the typical agreement between human annotators. For cars, our speed-up factor is 7.3 for an agreement of 82.2%. We further show generalization capabilities of our approach to unseen datasets.



