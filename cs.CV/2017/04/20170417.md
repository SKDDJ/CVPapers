# Arxiv Papers in cs.CV on 2017-04-17
### Quantum Mechanical Approach to Modelling Reliability of Sensor Reports
- **Arxiv ID**: http://arxiv.org/abs/1705.01013v1
- **DOI**: None
- **Categories**: **cs.OH**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.01013v1)
- **Published**: 2017-04-17 01:22:15+00:00
- **Updated**: 2017-04-17 01:22:15+00:00
- **Authors**: Zichang He, Wen Jiang
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: Dempster-Shafer evidence theory is wildly applied in multi-sensor data fusion. However, lots of uncertainty and interference exist in practical situation, especially in the battle field. It is still an open issue to model the reliability of sensor reports. Many methods are proposed based on the relationship among collected data. In this letter, we proposed a quantum mechanical approach to evaluate the reliability of sensor reports, which is based on the properties of a sensor itself. The proposed method is used to modify the combining of evidences.



### MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications
- **Arxiv ID**: http://arxiv.org/abs/1704.04861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04861v1)
- **Published**: 2017-04-17 03:57:34+00:00
- **Updated**: 2017-04-17 03:57:34+00:00
- **Authors**: Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam
- **Comment**: None
- **Journal**: None
- **Summary**: We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.



### Gang of GANs: Generative Adversarial Networks with Maximum Margin Ranking
- **Arxiv ID**: http://arxiv.org/abs/1704.04865v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.04865v1)
- **Published**: 2017-04-17 04:42:56+00:00
- **Updated**: 2017-04-17 04:42:56+00:00
- **Authors**: Felix Juefei-Xu, Vishnu Naresh Boddeti, Marios Savvides
- **Comment**: 16 pages. 11 figures
- **Journal**: None
- **Summary**: Traditional generative adversarial networks (GAN) and many of its variants are trained by minimizing the KL or JS-divergence loss that measures how close the generated data distribution is from the true data distribution. A recent advance called the WGAN based on Wasserstein distance can improve on the KL and JS-divergence based GANs, and alleviate the gradient vanishing, instability, and mode collapse issues that are common in the GAN training. In this work, we aim at improving on the WGAN by first generalizing its discriminator loss to a margin-based one, which leads to a better discriminator, and in turn a better generator, and then carrying out a progressive training paradigm involving multiple GANs to contribute to the maximum margin ranking loss so that the GAN at later stages will improve upon early stages. We call this method Gang of GANs (GoGAN). We have shown theoretically that the proposed GoGAN can reduce the gap between the true data distribution and the generated data distribution by at least half in an optimally trained WGAN. We have also proposed a new way of measuring GAN quality which is based on image completion tasks. We have evaluated our method on four visual datasets: CelebA, LSUN Bedroom, CIFAR-10, and 50K-SSFF, and have seen both visual and quantitative improvement over baseline WGAN.



### Least square ellipsoid fitting using iterative orthogonal transformations
- **Arxiv ID**: http://arxiv.org/abs/1704.04877v3
- **DOI**: 10.1016/j.amc.2017.07.025
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04877v3)
- **Published**: 2017-04-17 05:44:05+00:00
- **Updated**: 2017-05-01 13:08:03+00:00
- **Authors**: Amit Reza, Anand S. Sengupta
- **Comment**: Submitted to Applied Mathematics and Computation (Elsevier)
- **Journal**: None
- **Summary**: We describe a generalised method for ellipsoid fitting against a minimum set of data points. The proposed method is numerically stable and applies to a wide range of ellipsoidal shapes, including highly elongated and arbitrarily oriented ellipsoids. This new method also provides for the retrieval of rotational angle and length of semi-axes of the fitted ellipsoids accurately. We demonstrate the efficacy of this algorithm on simulated data sets and also indicate its potential use in gravitational wave data analysis.



### Multi-View Image Generation from a Single-View
- **Arxiv ID**: http://arxiv.org/abs/1704.04886v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1704.04886v4)
- **Published**: 2017-04-17 06:54:34+00:00
- **Updated**: 2018-02-27 02:36:32+00:00
- **Authors**: Bo Zhao, Xiao Wu, Zhi-Qi Cheng, Hao Liu, Zequn Jie, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses a challenging problem -- how to generate multi-view cloth images from only a single view input. To generate realistic-looking images with different views from the input, we propose a new image generation model termed VariGANs that combines the strengths of the variational inference and the Generative Adversarial Networks (GANs). Our proposed VariGANs model generates the target image in a coarse-to-fine manner instead of a single pass which suffers from severe artifacts. It first performs variational inference to model global appearance of the object (e.g., shape and color) and produce a coarse image with a different view. Conditioned on the generated low resolution images, it then proceeds to perform adversarial learning to fill details and generate images of consistent details with the input. Extensive experiments conducted on two clothing datasets, MVC and DeepFashion, have demonstrated that images of a novel view generated by our model are more plausible than those generated by existing approaches, in terms of more consistent global appearance as well as richer and sharper details.



### AMTnet: Action-Micro-Tube Regression by End-to-end Trainable Deep Architecture
- **Arxiv ID**: http://arxiv.org/abs/1704.04952v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.04952v2)
- **Published**: 2017-04-17 13:04:46+00:00
- **Updated**: 2017-08-06 14:18:44+00:00
- **Authors**: Suman Saha, Gurkirt Singh, Fabio Cuzzolin
- **Comment**: Update to version in ICCV 2017 proceedings
- **Journal**: None
- **Summary**: Dominant approaches to action detection can only provide sub-optimal solutions to the problem, as they rely on seeking frame-level detections, to later compose them into "action tubes" in a post-processing step. With this paper we radically depart from current practice, and take a first step towards the design and implementation of a deep network architecture able to classify and regress whole video subsets, so providing a truly optimal solution of the action detection problem. In this work, in particular, we propose a novel deep net framework able to regress and classify 3D region proposals spanning two successive video frames, whose core is an evolution of classical region proposal networks (RPNs). As such, our 3D-RPN net is able to effectively encode the temporal aspect of actions by purely exploiting appearance, as opposed to methods which heavily rely on expensive flow maps. The proposed model is end-to-end trainable and can be jointly optimised for action localisation and classification in a single step. At test time the network predicts "micro-tubes" encompassing two successive frames, which are linked up into complete action tubes via a new algorithm which exploits the temporal encoding learned by the network and cuts computation time by 50%. Promising results on the J-HMDB-21 and UCF-101 action detection datasets show that our model does outperform the state-of-the-art when relying purely on appearance.



### End-to-end 3D face reconstruction with deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1704.05020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05020v1)
- **Published**: 2017-04-17 16:31:12+00:00
- **Updated**: 2017-04-17 16:31:12+00:00
- **Authors**: Pengfei Dou, Shishir K. Shah, Ioannis A. Kakadiaris
- **Comment**: Accepted to CVPR17
- **Journal**: None
- **Summary**: Monocular 3D facial shape reconstruction from a single 2D facial image has been an active research area due to its wide applications. Inspired by the success of deep neural networks (DNN), we propose a DNN-based approach for End-to-End 3D FAce Reconstruction (UH-E2FAR) from a single 2D image. Different from recent works that reconstruct and refine the 3D face in an iterative manner using both an RGB image and an initial 3D facial shape rendering, our DNN model is end-to-end, and thus the complicated 3D rendering process can be avoided. Moreover, we integrate in the DNN architecture two components, namely a multi-task loss function and a fusion convolutional neural network (CNN) to improve facial expression reconstruction. With the multi-task loss function, 3D face reconstruction is divided into neutral 3D facial shape reconstruction and expressive 3D facial shape reconstruction. The neutral 3D facial shape is class-specific. Therefore, higher layer features are useful. In comparison, the expressive 3D facial shape favors lower or intermediate layer features. With the fusion-CNN, features from different intermediate layers are fused and transformed for predicting the 3D expressive facial shape. Through extensive experiments, we demonstrate the superiority of our end-to-end framework in improving the accuracy of 3D face reconstruction.



### A Nuclear-norm Model for Multi-Frame Super-Resolution Reconstruction from Video Clips
- **Arxiv ID**: http://arxiv.org/abs/1704.06196v1
- **DOI**: None
- **Categories**: **cs.CV**, 65K10, G.1.6
- **Links**: [PDF](http://arxiv.org/pdf/1704.06196v1)
- **Published**: 2017-04-17 17:17:34+00:00
- **Updated**: 2017-04-17 17:17:34+00:00
- **Authors**: Rui Zhao, Raymond H. Chan
- **Comment**: 12 pages, 7 numberical examples, 12 figure groups, 2 tables
- **Journal**: None
- **Summary**: We propose a variational approach to obtain super-resolution images from multiple low-resolution frames extracted from video clips. First the displacement between the low-resolution frames and the reference frame are computed by an optical flow algorithm. Then a low-rank model is used to construct the reference frame in high-resolution by incorporating the information of the low-resolution frames. The model has two terms: a 2-norm data fidelity term and a nuclear-norm regularization term. Alternating direction method of multipliers is used to solve the model. Comparison of our methods with other models on synthetic and real video clips show that our resulting images are more accurate with less artifacts. It also provides much finer and discernable details.



### A Gabor Filter Texture Analysis Approach for Histopathological Brain Tumor Subtype Discrimination
- **Arxiv ID**: http://arxiv.org/abs/1704.05122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05122v1)
- **Published**: 2017-04-17 21:06:09+00:00
- **Updated**: 2017-04-17 21:06:09+00:00
- **Authors**: Omar S. Al-Kadi
- **Comment**: 14 pages,4 figures, 2 tables
- **Journal**: ISESCO Journal of Science and Technology, vol. 12, no. 22, 2017
- **Summary**: Meningioma brain tumour discrimination is challenging as many histological patterns are mixed between the different subtypes. In clinical practice, dominant patterns are investigated for signs of specific meningioma pathology; however the simple observation could result in inter- and intra-observer variation due to the complexity of the histopathological patterns. Also employing a computerised feature extraction approach applied at a single resolution scale might not suffice in accurately delineating the mixture of histopathological patterns. In this work we propose a novel multiresolution feature extraction approach for characterising the textural properties of the different pathological patterns (i.e. mainly cell nuclei shape, orientation and spatial arrangement within the cytoplasm). The pattern textural properties are characterised at various scales and orientations for an improved separability between the different extracted features. The Gabor filter energy output of each magnitude response was combined with four other fixed-resolution texture signatures (2 model-based and 2 statistical-based) with and without cell nuclei segmentation. The highest classification accuracy of 95% was reported when combining the Gabor filters energy and the meningioma subimage fractal signature as a feature vector without performing any prior cell nuceli segmentation. This indicates that characterising the cell-nuclei self-similarity properties via Gabor filters can assists in achieving an improved meningioma subtype classification, which can assist in overcoming variations in reported diagnosis.



