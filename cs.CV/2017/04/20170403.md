# Arxiv Papers in cs.CV on 2017-04-03
### Sparse Autoencoder for Unsupervised Nucleus Detection and Representation in Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/1704.00406v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00406v2)
- **Published**: 2017-04-03 02:12:28+00:00
- **Updated**: 2017-04-10 16:37:46+00:00
- **Authors**: Le Hou, Vu Nguyen, Dimitris Samaras, Tahsin M. Kurc, Yi Gao, Tianhao Zhao, Joel H. Saltz
- **Comment**: None
- **Journal**: None
- **Summary**: Histopathology images are crucial to the study of complex diseases such as cancer. The histologic characteristics of nuclei play a key role in disease diagnosis, prognosis and analysis. In this work, we propose a sparse Convolutional Autoencoder (CAE) for fully unsupervised, simultaneous nucleus detection and feature extraction in histopathology tissue images. Our CAE detects and encodes nuclei in image patches in tissue images into sparse feature maps that encode both the location and appearance of nuclei. Our CAE is the first unsupervised detection network for computer vision applications. The pretrained nucleus detection and feature extraction modules in our CAE can be fine-tuned for supervised learning in an end-to-end fashion. We evaluate our method on four datasets and reduce the errors of state-of-the-art methods up to 42%. We are able to achieve comparable performance with only 5% of the fully-supervised annotation cost.



### A Good Practice Towards Top Performance of Face Recognition: Transferred Deep Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/1704.00438v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00438v2)
- **Published**: 2017-04-03 06:11:43+00:00
- **Updated**: 2018-02-09 05:37:41+00:00
- **Authors**: Lin Xiong, Jayashree Karlekar, Jian Zhao, Yi Cheng, Yan Xu, Jiashi Feng, Sugiri Pranata, Shengmei Shen
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: Unconstrained face recognition performance evaluations have traditionally focused on Labeled Faces in the Wild (LFW) dataset for imagery and the YouTubeFaces (YTF) dataset for videos in the last couple of years. Spectacular progress in this field has resulted in saturation on verification and identification accuracies for those benchmark datasets. In this paper, we propose a unified learning framework named Transferred Deep Feature Fusion (TDFF) targeting at the new IARPA Janus Benchmark A (IJB-A) face recognition dataset released by NIST face challenge. The IJB-A dataset includes real-world unconstrained faces from 500 subjects with full pose and illumination variations which are much harder than the LFW and YTF datasets. Inspired by transfer learning, we train two advanced deep convolutional neural networks (DCNN) with two different large datasets in source domain, respectively. By exploring the complementarity of two distinct DCNNs, deep feature fusion is utilized after feature extraction in target domain. Then, template specific linear SVMs is adopted to enhance the discrimination of framework. Finally, multiple matching scores corresponding different templates are merged as the final results. This simple unified framework exhibits excellent performance on IJB-A dataset. Based on the proposed approach, we have submitted our IJB-A results to National Institute of Standards and Technology (NIST) for official evaluation. Moreover, by introducing new data and advanced neural architecture, our method outperforms the state-of-the-art by a wide margin on IJB-A dataset.



### Learning a Variational Network for Reconstruction of Accelerated MRI Data
- **Arxiv ID**: http://arxiv.org/abs/1704.00447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00447v1)
- **Published**: 2017-04-03 06:49:46+00:00
- **Updated**: 2017-04-03 06:49:46+00:00
- **Authors**: Kerstin Hammernik, Teresa Klatzer, Erich Kobler, Michael P Recht, Daniel K Sodickson, Thomas Pock, Florian Knoll
- **Comment**: Submitted to Magnetic Resonance in Medicine
- **Journal**: None
- **Summary**: Purpose: To allow fast and high-quality reconstruction of clinical accelerated multi-coil MR data by learning a variational network that combines the mathematical structure of variational models with deep learning.   Theory and Methods: Generalized compressed sensing reconstruction formulated as a variational model is embedded in an unrolled gradient descent scheme. All parameters of this formulation, including the prior model defined by filter kernels and activation functions as well as the data term weights, are learned during an offline training procedure. The learned model can then be applied online to previously unseen data.   Results: The variational network approach is evaluated on a clinical knee imaging protocol. The variational network reconstructions outperform standard reconstruction algorithms in terms of image quality and residual artifacts for all tested acceleration factors and sampling patterns.   Conclusion: Variational network reconstructions preserve the natural appearance of MR images as well as pathologies that were not included in the training data set. Due to its high computational performance, i.e., reconstruction time of 193 ms on a single graphics card, and the omission of parameter tuning once the network is trained, this new approach to image reconstruction can easily be integrated into clinical workflow.



### Clustering in Hilbert simplex geometry
- **Arxiv ID**: http://arxiv.org/abs/1704.00454v7
- **DOI**: 10.1007/978-3-030-02520-5_11
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.00454v7)
- **Published**: 2017-04-03 07:23:37+00:00
- **Updated**: 2021-11-19 06:42:10+00:00
- **Authors**: Frank Nielsen, Ke Sun
- **Comment**: 48 pages
- **Journal**: Geometric Structures of Information, Springer, 2019 (pp. 297-331)
- **Summary**: Clustering categorical distributions in the finite-dimensional probability simplex is a fundamental task met in many applications dealing with normalized histograms. Traditionally, the differential-geometric structures of the probability simplex have been used either by (i) setting the Riemannian metric tensor to the Fisher information matrix of the categorical distributions, or (ii) defining the dualistic information-geometric structure induced by a smooth dissimilarity measure, the Kullback-Leibler divergence. In this work, we introduce for clustering tasks a novel computationally-friendly framework for modeling geometrically the probability simplex: The {\em Hilbert simplex geometry}. In the Hilbert simplex geometry, the distance is the non-separable Hilbert's metric distance which satisfies the property of information monotonicity with distance level set functions described by polytope boundaries. We show that both the Aitchison and Hilbert simplex distances are norm distances on normalized logarithmic representations with respect to the $\ell_2$ and variation norms, respectively. We discuss the pros and cons of those different statistical modelings, and benchmark experimentally these different kind of geometries for center-based $k$-means and $k$-center clustering. Furthermore, since a canonical Hilbert distance can be defined on any bounded convex subset of the Euclidean space, we also consider Hilbert's geometry of the elliptope of correlation matrices and study its clustering performances compared to Fr\"obenius and log-det divergences.



### A Comparison of Directional Distances for Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1704.00492v1
- **DOI**: 10.1007/978-3-642-40602-7_14
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00492v1)
- **Published**: 2017-04-03 09:31:01+00:00
- **Updated**: 2017-04-03 09:31:01+00:00
- **Authors**: Dimitrios Tzionas, Juergen Gall
- **Comment**: German Conference on Pattern Recognition (GCPR) 2013,
  http://files.is.tue.mpg.de/dtzionas/GCPR_2013.html
- **Journal**: None
- **Summary**: Benchmarking methods for 3d hand tracking is still an open problem due to the difficulty of acquiring ground truth data. We introduce a new dataset and benchmarking protocol that is insensitive to the accumulative error of other protocols. To this end, we create testing frame pairs of increasing difficulty and measure the pose estimation error separately for each of them. This approach gives new insights and allows to accurately study the performance of each feature or method without employing a full tracking pipeline. Following this protocol, we evaluate various directional distances in the context of silhouette-based 3d hand tracking, expressed as special cases of a generalized Chamfer distance form. An appropriate parameter setup is proposed for each of them, and a comparative study reveals the best performing method in this context.



### Convolutional neural networks for segmentation and object detection of human semen
- **Arxiv ID**: http://arxiv.org/abs/1704.00498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00498v1)
- **Published**: 2017-04-03 09:40:56+00:00
- **Updated**: 2017-04-03 09:40:56+00:00
- **Authors**: Malte Stær Nissen, Oswin Krause, Kristian Almstrup, Søren Kjærulff, Torben Trindkær Nielsen, Mads Nielsen
- **Comment**: Submitted for Scandinavian Conference on Image Analysis 2017
- **Journal**: None
- **Summary**: We compare a set of convolutional neural network (CNN) architectures for the task of segmenting and detecting human sperm cells in an image taken from a semen sample. In contrast to previous work, samples are not stained or washed to allow for full sperm quality analysis, making analysis harder due to clutter. Our results indicate that training on full images is superior to training on patches when class-skew is properly handled. Full image training including up-sampling during training proves to be beneficial in deep CNNs for pixel wise accuracy and detection performance. Predicted sperm cells are found by using connected components on the CNN predictions. We investigate optimization of a threshold parameter on the size of detected components. Our best network achieves 93.87% precision and 91.89% recall on our test dataset after thresholding outperforming a classical mage analysis approach.



### Truncating Wide Networks using Binary Tree Architectures
- **Arxiv ID**: http://arxiv.org/abs/1704.00509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00509v1)
- **Published**: 2017-04-03 10:11:10+00:00
- **Updated**: 2017-04-03 10:11:10+00:00
- **Authors**: Yan Zhang, Mete Ozay, Shuohao Li, Takayuki Okatani
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Recent study shows that a wide deep network can obtain accuracy comparable to a deeper but narrower network. Compared to narrower and deeper networks, wide networks employ relatively less number of layers and have various important benefits, such that they have less running time on parallel computing devices, and they are less affected by gradient vanishing problems. However, the parameter size of a wide network can be very large due to use of large width of each layer in the network. In order to keep the benefits of wide networks meanwhile improve the parameter size and accuracy trade-off of wide networks, we propose a binary tree architecture to truncate architecture of wide networks by reducing the width of the networks. More precisely, in the proposed architecture, the width is continuously reduced from lower layers to higher layers in order to increase the expressive capacity of network with a less increase on parameter size. Also, to ease the gradient vanishing problem, features obtained at different layers are concatenated to form the output of our architecture. By employing the proposed architecture on a baseline wide network, we can construct and train a new network with same depth but considerably less number of parameters. In our experimental analyses, we observe that the proposed architecture enables us to obtain better parameter size and accuracy trade-off compared to baseline networks using various benchmark image classification datasets. The results show that our model can decrease the classification error of baseline from 20.43% to 19.22% on Cifar-100 using only 28% of parameters that baseline has. Code is available at https://github.com/ZhangVision/bitnet.



### Capturing Hand Motion with an RGB-D Sensor, Fusing a Generative Model with Salient Points
- **Arxiv ID**: http://arxiv.org/abs/1704.00515v1
- **DOI**: 10.1007/978-3-319-11752-2_22
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00515v1)
- **Published**: 2017-04-03 10:26:45+00:00
- **Updated**: 2017-04-03 10:26:45+00:00
- **Authors**: Dimitrios Tzionas, Abhilash Srikantha, Pablo Aponte, Juergen Gall
- **Comment**: German Conference on Pattern Recognition (GCPR) 2014,
  http://files.is.tue.mpg.de/dtzionas/GCPR_2014.html
- **Journal**: None
- **Summary**: Hand motion capture has been an active research topic in recent years, following the success of full-body pose tracking. Despite similarities, hand tracking proves to be more challenging, characterized by a higher dimensionality, severe occlusions and self-similarity between fingers. For this reason, most approaches rely on strong assumptions, like hands in isolation or expensive multi-camera systems, that limit the practical use. In this work, we propose a framework for hand tracking that can capture the motion of two interacting hands using only a single, inexpensive RGB-D camera. Our approach combines a generative model with collision detection and discriminatively learned salient points. We quantitatively evaluate our approach on 14 new sequences with challenging interactions.



### Block-Matching Convolutional Neural Network for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1704.00524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00524v1)
- **Published**: 2017-04-03 11:01:47+00:00
- **Updated**: 2017-04-03 11:01:47+00:00
- **Authors**: Byeongyong Ahn, Nam Ik Cho
- **Comment**: 11 pages, 9 figures
- **Journal**: None
- **Summary**: There are two main streams in up-to-date image denoising algorithms: non-local self similarity (NSS) prior based methods and convolutional neural network (CNN) based methods. The NSS based methods are favorable on images with regular and repetitive patterns while the CNN based methods perform better on irregular structures. In this paper, we propose a block-matching convolutional neural network (BMCNN) method that combines NSS prior and CNN. Initially, similar local patches in the input image are integrated into a 3D block. In order to prevent the noise from messing up the block matching, we first apply an existing denoising algorithm on the noisy image. The denoised image is employed as a pilot signal for the block matching, and then denoising function for the block is learned by a CNN structure. Experimental results show that the proposed BMCNN algorithm achieves state-of-the-art performance. In detail, BMCNN can restore both repetitive and irregular structures.



### 3D Object Reconstruction from Hand-Object Interactions
- **Arxiv ID**: http://arxiv.org/abs/1704.00529v1
- **DOI**: 10.1109/ICCV.2015.90
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00529v1)
- **Published**: 2017-04-03 11:19:03+00:00
- **Updated**: 2017-04-03 11:19:03+00:00
- **Authors**: Dimitrios Tzionas, Juergen Gall
- **Comment**: International Conference on Computer Vision (ICCV) 2015,
  http://files.is.tue.mpg.de/dtzionas/In-Hand-Scanning
- **Journal**: None
- **Summary**: Recent advances have enabled 3d object reconstruction approaches using a single off-the-shelf RGB-D camera. Although these approaches are successful for a wide range of object classes, they rely on stable and distinctive geometric or texture features. Many objects like mechanical parts, toys, household or decorative articles, however, are textureless and characterized by minimalistic shapes that are simple and symmetric. Existing in-hand scanning systems and 3d reconstruction techniques fail for such symmetric objects in the absence of highly distinctive features. In this work, we show that extracting 3d hand motion for in-hand scanning effectively facilitates the reconstruction of even featureless and highly symmetric objects and we present an approach that fuses the rich additional information of hands into a 3d reconstruction pipeline, significantly contributing to the state-of-the-art of in-hand scanning.



### Spatiotemporal Networks for Video Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.00570v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00570v3)
- **Published**: 2017-04-03 13:21:38+00:00
- **Updated**: 2017-04-12 14:39:34+00:00
- **Authors**: Lijie Fan, Yunjie Ke
- **Comment**: The reason is that the article is just an experimental report without
  being reviewed carefully. There are some fatal drawbacks in this article and
  it may not be suitable for being published
- **Journal**: None
- **Summary**: Our experiment adapts several popular deep learning methods as well as some traditional methods on the problem of video emotion recognition. In our experiment, we use the CNN-LSTM architecture for visual information extraction and classification and utilize traditional methods such as for audio feature classification. For multimodal fusion, we use the traditional Support Vector Machine. Our experiment yields a good result on the AFEW 6.0 Dataset.



### Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.00616v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.MM, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1704.00616v2)
- **Published**: 2017-04-03 14:29:40+00:00
- **Updated**: 2017-05-26 18:40:14+00:00
- **Authors**: Mohammadreza Zolfaghari, Gabriel L. Oliveira, Nima Sedaghat, Thomas Brox
- **Comment**: 10 pages, 7 figures, ICCV 2017 submission
- **Journal**: None
- **Summary**: General human action recognition requires understanding of various visual cues. In this paper, we propose a network architecture that computes and integrates the most important visual cues for action recognition: pose, motion, and the raw images. For the integration, we introduce a Markov chain model which adds cues successively. The resulting approach is efficient and applicable to action classification as well as to spatial and temporal action localization. The two contributions clearly improve the performance over respective baselines. The overall approach achieves state-of-the-art action classification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover, it yields state-of-the-art spatio-temporal action localization results on UCF101 and J-HMDB.



### Local nearest neighbour classification with applications to semi-supervised learning
- **Arxiv ID**: http://arxiv.org/abs/1704.00642v3
- **DOI**: None
- **Categories**: **math.ST**, cs.CV, cs.LG, stat.ME, stat.TH, 62G20
- **Links**: [PDF](http://arxiv.org/pdf/1704.00642v3)
- **Published**: 2017-04-03 15:34:11+00:00
- **Updated**: 2019-05-18 10:49:46+00:00
- **Authors**: Timothy I. Cannings, Thomas B. Berrett, Richard J. Samworth
- **Comment**: 60 pages
- **Journal**: None
- **Summary**: We derive a new asymptotic expansion for the global excess risk of a local-$k$-nearest neighbour classifier, where the choice of $k$ may depend upon the test point. This expansion elucidates conditions under which the dominant contribution to the excess risk comes from the decision boundary of the optimal Bayes classifier, but we also show that if these conditions are not satisfied, then the dominant contribution may arise from the tails of the marginal distribution of the features. Moreover, we prove that, provided the $d$-dimensional marginal distribution of the features has a finite $\rho$th moment for some $\rho > 4$ (as well as other regularity conditions), a local choice of $k$ can yield a rate of convergence of the excess risk of $O(n^{-4/(d+4)})$, where $n$ is the sample size, whereas for the standard $k$-nearest neighbour classifier, our theory would require $d \geq 5$ and $\rho > 4d/(d-4)$ finite moments to achieve this rate. These results motivate a new $k$-nearest neighbour classifier for semi-supervised learning problems, where the unlabelled data are used to obtain an estimate of the marginal feature density, and fewer neighbours are used for classification when this density estimate is small. Our worst-case rates are complemented by a minimax lower bound, which reveals that the local, semi-supervised $k$-nearest neighbour classifier attains the minimax optimal rate over our classes for the excess risk, up to a subpolynomial factor in $n$. These theoretical improvements over the standard $k$-nearest neighbour classifier are also illustrated through a simulation study.



### Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations
- **Arxiv ID**: http://arxiv.org/abs/1704.00648v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.00648v2)
- **Published**: 2017-04-03 15:39:56+00:00
- **Updated**: 2017-06-08 09:18:22+00:00
- **Authors**: Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.



### The 2017 DAVIS Challenge on Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.00675v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00675v3)
- **Published**: 2017-04-03 16:44:46+00:00
- **Updated**: 2018-03-01 17:50:08+00:00
- **Authors**: Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, Luc Van Gool
- **Comment**: Challenge website: http://davischallenge.org
- **Journal**: None
- **Summary**: We present the 2017 DAVIS Challenge on Video Object Segmentation, a public dataset, benchmark, and competition specifically designed for the task of video object segmentation. Following the footsteps of other successful initiatives, such as ILSVRC and PASCAL VOC, which established the avenue of research in the fields of scene classification and semantic segmentation, the DAVIS Challenge comprises a dataset, an evaluation methodology, and a public competition with a dedicated workshop co-located with CVPR 2017. The DAVIS Challenge follows up on the recent publication of DAVIS (Densely-Annotated VIdeo Segmentation), which has fostered the development of several novel state-of-the-art video object segmentation techniques. In this paper we describe the scope of the benchmark, highlight the main characteristics of the dataset, define the evaluation metrics of the competition, and present a detailed analysis of the results of the participants to the challenge.



### Graph Partitioning with Acyclicity Constraints
- **Arxiv ID**: http://arxiv.org/abs/1704.00705v1
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1704.00705v1)
- **Published**: 2017-04-03 17:45:10+00:00
- **Updated**: 2017-04-03 17:45:10+00:00
- **Authors**: Orlando Moreira, Merten Popp, Christian Schulz
- **Comment**: None
- **Journal**: None
- **Summary**: Graphs are widely used to model execution dependencies in applications. In particular, the NP-complete problem of partitioning a graph under constraints receives enormous attention by researchers because of its applicability in multiprocessor scheduling. We identified the additional constraint of acyclic dependencies between blocks when mapping computer vision and imaging applications to a heterogeneous embedded multiprocessor. Existing algorithms and heuristics do not address this requirement and deliver results that are not applicable for our use-case. In this work, we show that this more constrained version of the graph partitioning problem is NP-complete and present heuristics that achieve a close approximation of the optimal solution found by an exhaustive search for small problem instances and much better scalability for larger instances. In addition, we can show a positive impact on the schedule of a real imaging application that improves communication volume and execution time.



### Hierarchical Surface Prediction for 3D Object Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1704.00710v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00710v2)
- **Published**: 2017-04-03 17:52:51+00:00
- **Updated**: 2017-11-06 19:11:08+00:00
- **Authors**: Christian Häne, Shubham Tulsiani, Jitendra Malik
- **Comment**: 3DV 2017
- **Journal**: None
- **Summary**: Recently, Convolutional Neural Networks have shown promising results for 3D geometry prediction. They can make predictions from very little input data such as a single color image. A major limitation of such approaches is that they only predict a coarse resolution voxel grid, which does not capture the surface of the objects well. We propose a general framework, called hierarchical surface prediction (HSP), which facilitates prediction of high resolution voxel grids. The main insight is that it is sufficient to predict high resolution voxels around the predicted surfaces. The exterior and interior of the objects can be represented with coarse resolution voxels. Our approach is not dependent on a specific input type. We show results for geometry prediction from color images, depth images and shape completion from partial voxel grids. Our analysis shows that our high resolution predictions are more accurate than low resolution predictions.



### It Takes Two to Tango: Towards Theory of AI's Mind
- **Arxiv ID**: http://arxiv.org/abs/1704.00717v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1704.00717v2)
- **Published**: 2017-04-03 17:58:07+00:00
- **Updated**: 2017-10-02 17:55:50+00:00
- **Authors**: Arjun Chandrasekaran, Deshraj Yadav, Prithvijit Chattopadhyay, Viraj Prabhu, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: Theory of Mind is the ability to attribute mental states (beliefs, intents, knowledge, perspectives, etc.) to others and recognize that these mental states may differ from one's own. Theory of Mind is critical to effective communication and to teams demonstrating higher collective performance. To effectively leverage the progress in Artificial Intelligence (AI) to make our lives more productive, it is important for humans and AI to work well together in a team. Traditionally, there has been much emphasis on research to make AI more accurate, and (to a lesser extent) on having it better understand human intentions, tendencies, beliefs, and contexts. The latter involves making AI more human-like and having it develop a theory of our minds. In this work, we argue that for human-AI teams to be effective, humans must also develop a theory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs, and quirks. We instantiate these ideas within the domain of Visual Question Answering (VQA). We find that using just a few examples (50), lay people can be trained to better predict responses and oncoming failures of a complex VQA model. We further evaluate the role existing explanation (or interpretability) modalities play in helping humans build ToAIM. Explainable AI has received considerable scientific and popular attention in recent times. Surprisingly, we find that having access to the model's internal states - its confidence in its top-k predictions, explicit or implicit attention maps which highlight regions in the image (and words in the question) the model is looking at (and listening to) while answering a question about an image - do not help people better predict its behavior.



### Unsupervised Action Proposal Ranking through Proposal Recombination
- **Arxiv ID**: http://arxiv.org/abs/1704.00758v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00758v1)
- **Published**: 2017-04-03 18:43:20+00:00
- **Updated**: 2017-04-03 18:43:20+00:00
- **Authors**: Waqas Sultani, Dong Zhang, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, action proposal methods have played an important role in action recognition tasks, as they reduce the search space dramatically. Most unsupervised action proposal methods tend to generate hundreds of action proposals which include many noisy, inconsistent, and unranked action proposals, while supervised action proposal methods take advantage of predefined object detectors (e.g., human detector) to refine and score the action proposals, but they require thousands of manual annotations to train.   Given the action proposals in a video, the goal of the proposed work is to generate a few better action proposals that are ranked properly. In our approach, we first divide action proposal into sub-proposal and then use Dynamic Programming based graph optimization scheme to select the optimal combinations of sub-proposals from different proposals and assign each new proposal a score. We propose a new unsupervised image-based actioness detector that leverages web images and employs it as one of the node scores in our graph formulation. Moreover, we capture motion information by estimating the number of motion contours within each action proposal patch. The proposed method is an unsupervised method that neither needs bounding box annotations nor video level labels, which is desirable with the current explosion of large-scale action datasets. Our approach is generic and does not depend on a specific action proposal method. We evaluate our approach on several publicly available trimmed and un-trimmed datasets and obtain better performance compared to several proposal ranking methods. In addition, we demonstrate that properly ranked proposals produce significantly better action detection as compared to state-of-the-art proposal based methods.



### AMC: Attention guided Multi-modal Correlation Learning for Image Search
- **Arxiv ID**: http://arxiv.org/abs/1704.00763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00763v1)
- **Published**: 2017-04-03 18:57:42+00:00
- **Updated**: 2017-04-03 18:57:42+00:00
- **Authors**: Kan Chen, Trung Bui, Fang Chen, Zhaowen Wang, Ram Nevatia
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Given a user's query, traditional image search systems rank images according to its relevance to a single modality (e.g., image content or surrounding text). Nowadays, an increasing number of images on the Internet are available with associated meta data in rich modalities (e.g., titles, keywords, tags, etc.), which can be exploited for better similarity measure with queries. In this paper, we leverage visual and textual modalities for image search by learning their correlation with input query. According to the intent of query, attention mechanism can be introduced to adaptively balance the importance of different modalities. We propose a novel Attention guided Multi-modal Correlation (AMC) learning method which consists of a jointly learned hierarchy of intra and inter-attention networks. Conditioned on query's intent, intra-attention networks (i.e., visual intra-attention network and language intra-attention network) attend on informative parts within each modality; a multi-modal inter-attention network promotes the importance of the most query-relevant modalities. In experiments, we evaluate AMC models on the search logs from two real world image search engines and show a significant boost on the ranking of user-clicked images in search results. Additionally, we extend AMC models to caption ranking task on COCO dataset and achieve competitive results compared with recent state-of-the-arts.



### Online deforestation detection
- **Arxiv ID**: http://arxiv.org/abs/1704.00829v1
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.00829v1)
- **Published**: 2017-04-03 22:40:48+00:00
- **Updated**: 2017-04-03 22:40:48+00:00
- **Authors**: Emiliano Diaz
- **Comment**: None
- **Journal**: None
- **Summary**: Deforestation detection using satellite images can make an important contribution to forest management. Current approaches can be broadly divided into those that compare two images taken at similar periods of the year and those that monitor changes by using multiple images taken during the growing season. The CMFDA algorithm described in Zhu et al. (2012) is an algorithm that builds on the latter category by implementing a year-long, continuous, time-series based approach to monitoring images. This algorithm was developed for 30m resolution, 16-day frequency reflectance data from the Landsat satellite. In this work we adapt the algorithm to 1km, 16-day frequency reflectance data from the modis sensor aboard the Terra satellite. The CMFDA algorithm is composed of two submodels which are fitted on a pixel-by-pixel basis. The first estimates the amount of surface reflectance as a function of the day of the year. The second estimates the occurrence of a deforestation event by comparing the last few predicted and real reflectance values. For this comparison, the reflectance observations for six different bands are first combined into a forest index. Real and predicted values of the forest index are then compared and high absolute differences for consecutive observation dates are flagged as deforestation events. Our adapted algorithm also uses the two model framework. However, since the modis 13A2 dataset used, includes reflectance data for different spectral bands than those included in the Landsat dataset, we cannot construct the forest index. Instead we propose two contrasting approaches: a multivariate and an index approach similar to that of CMFDA.



### Cascaded Segmentation-Detection Networks for Word-Level Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/1704.00834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00834v1)
- **Published**: 2017-04-03 23:55:13+00:00
- **Updated**: 2017-04-03 23:55:13+00:00
- **Authors**: Siyang Qin, Roberto Manduchi
- **Comment**: 7 pages, 8 figures
- **Journal**: None
- **Summary**: We introduce an algorithm for word-level text spotting that is able to accurately and reliably determine the bounding regions of individual words of text "in the wild". Our system is formed by the cascade of two convolutional neural networks. The first network is fully convolutional and is in charge of detecting areas containing text. This results in a very reliable but possibly inaccurate segmentation of the input image. The second network (inspired by the popular YOLO architecture) analyzes each segment produced in the first stage, and predicts oriented rectangular regions containing individual words. No post-processing (e.g. text line grouping) is necessary. With execution time of 450 ms for a 1000-by-560 image on a Titan X GPU, our system achieves the highest score to date among published algorithms on the ICDAR 2015 Incidental Scene Text dataset benchmark.



