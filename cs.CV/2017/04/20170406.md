# Arxiv Papers in cs.CV on 2017-04-06
### Generate To Adapt: Aligning Domains using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.01705v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01705v4)
- **Published**: 2017-04-06 04:52:48+00:00
- **Updated**: 2018-04-12 18:02:56+00:00
- **Authors**: Swami Sankaranarayanan, Yogesh Balaji, Carlos D. Castillo, Rama Chellappa
- **Comment**: Accepted as spotlight talk at CVPR 2018. Code available here:
  https://github.com/yogeshbalaji/Generate_To_Adapt
- **Journal**: None
- **Summary**: Domain Adaptation is an actively researched problem in Computer Vision. In this work, we propose an approach that leverages unsupervised data to bring the source and target distributions closer in a learned joint feature space. We accomplish this by inducing a symbiotic relationship between the learned embedding and a generative adversarial network. This is in contrast to methods which use the adversarial framework for realistic data generation and retraining deep models with such data. We demonstrate the strength and generality of our approach by performing experiments on three different tasks with varying levels of difficulty: (1) Digit classification (MNIST, SVHN and USPS datasets) (2) Object recognition using OFFICE dataset and (3) Domain adaptation from synthetic to real data. Our method achieves state-of-the art performance in most experimental settings and by far the only GAN-based method that has been shown to work well across different datasets such as OFFICE and DIGITS.



### Action Representation Using Classifier Decision Boundaries
- **Arxiv ID**: http://arxiv.org/abs/1704.01716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01716v1)
- **Published**: 2017-04-06 06:00:14+00:00
- **Updated**: 2017-04-06 06:00:14+00:00
- **Authors**: Jue Wang, Anoop Cherian, Fatih Porikli, Stephen Gould
- **Comment**: None
- **Journal**: None
- **Summary**: Most popular deep learning based models for action recognition are designed to generate separate predictions within their short temporal windows, which are often aggregated by heuristic means to assign an action label to the full video segment. Given that not all frames from a video characterize the underlying action, pooling schemes that impose equal importance to all frames might be unfavorable. In an attempt towards tackling this challenge, we propose a novel pooling scheme, dubbed SVM pooling, based on the notion that among the bag of features generated by a CNN on all temporal windows, there is at least one feature that characterizes the action. To this end, we learn a decision hyperplane that separates this unknown yet useful feature from the rest. Applying multiple instance learning in an SVM setup, we use the parameters of this separating hyperplane as a descriptor for the video. Since these parameters are directly related to the support vectors in a max-margin framework, they serve as robust representations for pooling of the CNN features. We devise a joint optimization objective and an efficient solver that learns these hyperplanes per video and the corresponding action classifiers over the hyperplanes. Showcased experiments on the standard HMDB and UCF101 datasets demonstrate state-of-the-art performance.



### Beyond triplet loss: a deep quadruplet network for person re-identification
- **Arxiv ID**: http://arxiv.org/abs/1704.01719v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01719v1)
- **Published**: 2017-04-06 06:09:55+00:00
- **Updated**: 2017-04-06 06:09:55+00:00
- **Authors**: Weihua Chen, Xiaotang Chen, Jianguo Zhang, Kaiqi Huang
- **Comment**: accepted to CVPR2017
- **Journal**: None
- **Summary**: Person re-identification (ReID) is an important task in wide area video surveillance which focuses on identifying people across different cameras. Recently, deep learning networks with a triplet loss become a common framework for person ReID. However, the triplet loss pays main attentions on obtaining correct orders on the training set. It still suffers from a weaker generalization capability from the training set to the testing set, thus resulting in inferior performance. In this paper, we design a quadruplet loss, which can lead to the model output with a larger inter-class variation and a smaller intra-class variation compared to the triplet loss. As a result, our model has a better generalization ability and can achieve a higher performance on the testing set. In particular, a quadruplet deep network using a margin-based online hard negative mining is proposed based on the quadruplet loss for the person ReID. In extensive experiments, the proposed network outperforms most of the state-of-the-art algorithms on representative datasets which clearly demonstrates the effectiveness of our proposed method.



### Object-Part Attention Model for Fine-grained Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1704.01740v2
- **DOI**: 10.1109/TIP.2017.2774041
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01740v2)
- **Published**: 2017-04-06 08:05:04+00:00
- **Updated**: 2017-09-25 02:27:29+00:00
- **Authors**: Yuxin Peng, Xiangteng He, Junjie Zhao
- **Comment**: 14 pages, submitted to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Fine-grained image classification is to recognize hundreds of subcategories belonging to the same basic-level category, such as 200 subcategories belonging to the bird, which is highly challenging due to large variance in the same subcategory and small variance among different subcategories. Existing methods generally first locate the objects or parts and then discriminate which subcategory the image belongs to. However, they mainly have two limitations: (1) Relying on object or part annotations which are heavily labor consuming. (2) Ignoring the spatial relationships between the object and its parts as well as among these parts, both of which are significantly helpful for finding discriminative parts. Therefore, this paper proposes the object-part attention model (OPAM) for weakly supervised fine-grained image classification, and the main novelties are: (1) Object-part attention model integrates two level attentions: object-level attention localizes objects of images, and part-level attention selects discriminative parts of object. Both are jointly employed to learn multi-view and multi-scale features to enhance their mutual promotions. (2) Object-part spatial constraint model combines two spatial constraints: object spatial constraint ensures selected parts highly representative, and part spatial constraint eliminates redundancy and enhances discrimination of selected parts. Both are jointly employed to exploit the subtle and local differences for distinguishing the subcategories. Importantly, neither object nor part annotations are used in our proposed approach, which avoids the heavy labor consumption of labeling. Comparing with more than 10 state-of-the-art methods on 4 widely-used datasets, our OPAM approach achieves the best performance.



### How to Make an Image More Memorable? A Deep Style Transfer Approach
- **Arxiv ID**: http://arxiv.org/abs/1704.01745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01745v1)
- **Published**: 2017-04-06 08:25:19+00:00
- **Updated**: 2017-04-06 08:25:19+00:00
- **Authors**: Aliaksandr Siarohin, Gloria Zen, Cveta Majtanovic, Xavier Alameda-Pineda, Elisa Ricci, Nicu Sebe
- **Comment**: Accepted at ACM ICMR 2017
- **Journal**: None
- **Summary**: Recent works have shown that it is possible to automatically predict intrinsic image properties like memorability. In this paper, we take a step forward addressing the question: "Can we make an image more memorable?". Methods for automatically increasing image memorability would have an impact in many application fields like education, gaming or advertising. Our work is inspired by the popular editing-by-applying-filters paradigm adopted in photo editing applications, like Instagram and Prisma. In this context, the problem of increasing image memorability maps to that of retrieving "memorabilizing" filters or style "seeds". Still, users generally have to go through most of the available filters before finding the desired solution, thus turning the editing process into a resource and time consuming task. In this work, we show that it is possible to automatically retrieve the best style seeds for a given image, thus remarkably reducing the number of human attempts needed to find a good match. Our approach leverages from recent advances in the field of image synthesis and adopts a deep architecture for generating a memorable picture from a given input image and a style seed. Importantly, to automatically select the best style a novel learning-based solution, also relying on deep models, is proposed. Our experimental evaluation, conducted on publicly available benchmarks, demonstrates the effectiveness of the proposed approach for generating memorable images through automatic style seed selection



### Enhance Feature Discrimination for Unsupervised Hashing
- **Arxiv ID**: http://arxiv.org/abs/1704.01754v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1704.01754v2)
- **Published**: 2017-04-06 08:58:39+00:00
- **Updated**: 2017-07-04 03:30:42+00:00
- **Authors**: Tuan Hoang, Thanh-Toan Do, Dang-Khoa Le Tan, Ngai-Man Cheung
- **Comment**: Accepted to ICIP 2017
- **Journal**: None
- **Summary**: We introduce a novel approach to improve unsupervised hashing. Specifically, we propose a very efficient embedding method: Gaussian Mixture Model embedding (Gemb). The proposed method, using Gaussian Mixture Model, embeds feature vector into a low-dimensional vector and, simultaneously, enhances the discriminative property of features before passing them into hashing. Our experiment shows that the proposed method boosts the hashing performance of many state-of-the-art, e.g. Binary Autoencoder (BA) [1], Iterative Quantization (ITQ) [2], in standard evaluation metrics for the three main benchmark datasets.



### Higher-Order Minimum Cost Lifted Multicuts for Motion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.01811v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01811v2)
- **Published**: 2017-04-06 12:55:11+00:00
- **Updated**: 2017-07-27 00:53:40+00:00
- **Authors**: Margret Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: Most state-of-the-art motion segmentation algorithms draw their potential from modeling motion differences of local entities such as point trajectories in terms of pairwise potentials in graphical models. Inference in instances of minimum cost multicut problems defined on such graphs al- lows to optimize the number of the resulting segments along with the segment assignment. However, pairwise potentials limit the discriminative power of the employed motion models to translational differences. More complex models such as Euclidean or affine transformations call for higher-order potentials and a tractable inference in the resulting higher-order graphical models. In this paper, we (1) introduce a generalization of the minimum cost lifted multicut problem to hypergraphs, and (2) propose a simple primal feasible heuristic that allows for a reasonably efficient inference in instances of higher-order lifted multicut problem instances defined on point trajectory hypergraphs for motion segmentation. The resulting motion segmentations improve over the state-of-the-art on the FBMS-59 dataset.



### A Convolution Tree with Deconvolution Branches: Exploiting Geometric Relationships for Single Shot Keypoint Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.01880v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01880v1)
- **Published**: 2017-04-06 15:08:59+00:00
- **Updated**: 2017-04-06 15:08:59+00:00
- **Authors**: Amit Kumar, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Deep Convolution Networks (DCNNs) have been applied to the task of face alignment and have shown potential for learning improved feature representations. Although deeper layers can capture abstract concepts like pose, it is difficult to capture the geometric relationships among the keypoints in DCNNs. In this paper, we propose a novel convolution-deconvolution network for facial keypoint detection. Our model predicts the 2D locations of the keypoints and their individual visibility along with 3D head pose, while exploiting the spatial relationships among different keypoints. Different from existing approaches of modeling these relationships, we propose learnable transform functions which captures the relationships between keypoints at feature level. However, due to extensive variations in pose, not all of these relationships act at once, and hence we propose, a pose-based routing function which implicitly models the active relationships. Both transform functions and the routing function are implemented through convolutions in a multi-task framework. Our approach presents a single-shot keypoint detection method, making it different from many existing cascade regression-based methods. We also show that learning these relationships significantly improve the accuracy of keypoint detections for in-the-wild face images from challenging datasets such as AFW and AFLW.



### Online Hashing
- **Arxiv ID**: http://arxiv.org/abs/1704.01897v1
- **DOI**: 10.1109/TNNLS.2017.2689242
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.01897v1)
- **Published**: 2017-04-06 15:44:29+00:00
- **Updated**: 2017-04-06 15:44:29+00:00
- **Authors**: Long-Kai Huang, Qiang Yang, Wei-Shi Zheng
- **Comment**: To appear in IEEE Transactions on Neural Networks and Learning
  Systems (DOI: 10.1109/TNNLS.2017.2689242)
- **Journal**: None
- **Summary**: Although hash function learning algorithms have achieved great success in recent years, most existing hash models are off-line, which are not suitable for processing sequential or online data. To address this problem, this work proposes an online hash model to accommodate data coming in stream for online learning. Specifically, a new loss function is proposed to measure the similarity loss between a pair of data samples in hamming space. Then, a structured hash model is derived and optimized in a passive-aggressive way. Theoretical analysis on the upper bound of the cumulative loss for the proposed online hash model is provided. Furthermore, we extend our online hashing from a single-model to a multi-model online hashing that trains multiple models so as to retain diverse online hashing models in order to avoid biased update. The competitive efficiency and effectiveness of the proposed online hash models are verified through extensive experiments on several large-scale datasets as compared to related hashing methods.



### Encoder Based Lifelong Learning
- **Arxiv ID**: http://arxiv.org/abs/1704.01920v1
- **DOI**: 10.1109/ICCV.2017.148
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.01920v1)
- **Published**: 2017-04-06 16:37:15+00:00
- **Updated**: 2017-04-06 16:37:15+00:00
- **Authors**: Amal Rannen Triki, Rahaf Aljundi, Mathew B. Blaschko, Tinne Tuytelaars
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a new lifelong learning solution where a single model is trained for a sequence of tasks. The main challenge that vision systems face in this context is catastrophic forgetting: as they tend to adapt to the most recently seen task, they lose performance on the tasks that were learned previously. Our method aims at preserving the knowledge of the previous tasks while learning a new one by using autoencoders. For each task, an under-complete autoencoder is learned, capturing the features that are crucial for its achievement. When a new task is presented to the system, we prevent the reconstructions of the features with these autoencoders from changing, which has the effect of preserving the information on which the previous tasks are mainly relying. At the same time, the features are given space to adjust to the most recent environment as only their projection into a low dimension submanifold is controlled. The proposed system is evaluated on image classification tasks and shows a reduction of forgetting over the state-of-the-art



### Automated Latent Fingerprint Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.01925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01925v1)
- **Published**: 2017-04-06 16:47:16+00:00
- **Updated**: 2017-04-06 16:47:16+00:00
- **Authors**: Kai Cao, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Latent fingerprints are one of the most important and widely used evidence in law enforcement and forensic agencies worldwide. Yet, NIST evaluations show that the performance of state-of-the-art latent recognition systems is far from satisfactory. An automated latent fingerprint recognition system with high accuracy is essential to compare latents found at crime scenes to a large collection of reference prints to generate a candidate list of possible mates. In this paper, we propose an automated latent fingerprint recognition algorithm that utilizes Convolutional Neural Networks (ConvNets) for ridge flow estimation and minutiae descriptor extraction, and extract complementary templates (two minutiae templates and one texture template) to represent the latent. The comparison scores between the latent and a reference print based on the three templates are fused to retrieve a short candidate list from the reference database. Experimental results show that the rank-1 identification accuracies (query latent is matched with its true mate in the reference database) are 64.7% for the NIST SD27 and 75.3% for the WVU latent databases, against a reference database of 100K rolled prints. These results are the best among published papers on latent recognition and competitive with the performance (66.7% and 70.8% rank-1 accuracies on NIST SD27 and WVU DB, respectively) of a leading COTS latent Automated Fingerprint Identification System (AFIS). By score-level (rank-level) fusion of our system with the commercial off-the-shelf (COTS) latent AFIS, the overall rank-1 identification performance can be improved from 64.7% and 75.3% to 73.3% (74.4%) and 76.6% (78.4%) on NIST SD27 and WVU latent databases, respectively.



### Semantically-Guided Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.01926v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01926v2)
- **Published**: 2017-04-06 16:47:56+00:00
- **Updated**: 2018-07-17 20:30:31+00:00
- **Authors**: Sergi Caelles, Yuhua Chen, Jordi Pont-Tuset, Luc Van Gool
- **Comment**: This paper has been incorporated in the following T-PAMI publication:
  arXiv:1709.06031
- **Journal**: None
- **Summary**: This paper tackles the problem of semi-supervised video object segmentation, that is, segmenting an object in a sequence given its mask in the first frame. One of the main challenges in this scenario is the change of appearance of the objects of interest. Their semantics, on the other hand, do not vary. This paper investigates how to take advantage of such invariance via the introduction of a semantic prior that guides the appearance model. Specifically, given the segmentation mask of the first frame of a sequence, we estimate the semantics of the object of interest, and propagate that knowledge throughout the sequence to improve the results based on an appearance model. We present Semantically-Guided Video Object Segmentation (SGV), which improves results over previous state of the art on two different datasets using a variety of evaluation metrics, while running in half a second per frame.



### Parallel Multi Channel Convolution using General Matrix Multiplication
- **Arxiv ID**: http://arxiv.org/abs/1704.04428v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1704.04428v2)
- **Published**: 2017-04-06 17:09:43+00:00
- **Updated**: 2017-07-03 12:43:10+00:00
- **Authors**: Aravind Vasudevan, Andrew Anderson, David Gregg
- **Comment**: Camera ready version to be published at ASAP 2017 - The 28th Annual
  IEEE International Conference on Application-specific Systems, Architectures
  and Processors. 6 pages
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have emerged as one of the most successful machine learning technologies for image and video processing. The most computationally intensive parts of CNNs are the convolutional layers, which convolve multi-channel images with multiple kernels. A common approach to implementing convolutional layers is to expand the image into a column matrix (im2col) and perform Multiple Channel Multiple Kernel (MCMK) convolution using an existing parallel General Matrix Multiplication (GEMM) library. This im2col conversion greatly increases the memory footprint of the input matrix and reduces data locality.   In this paper we propose a new approach to MCMK convolution that is based on General Matrix Multiplication (GEMM), but not on im2col. Our algorithm eliminates the need for data replication on the input thereby enabling us to apply the convolution kernels on the input images directly. We have implemented several variants of our algorithm on a CPU processor and an embedded ARM processor. On the CPU, our algorithm is faster than im2col in most cases.



