# Arxiv Papers in cs.CV on 2017-02-02
### Algorithmic Performance-Accuracy Trade-off in 3D Vision Applications Using HyperMapper
- **Arxiv ID**: http://arxiv.org/abs/1702.00505v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.LG, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1702.00505v2)
- **Published**: 2017-02-02 00:01:46+00:00
- **Updated**: 2017-03-21 21:58:41+00:00
- **Authors**: Luigi Nardi, Bruno Bodin, Sajad Saeedi, Emanuele Vespa, Andrew J. Davison, Paul H. J. Kelly
- **Comment**: 10 pages, Keywords: design space exploration, machine learning,
  computer vision, SLAM, embedded systems, GPU, crowd-sourcing
- **Journal**: 31st IEEE International Parallel and Distributed Processing
  Symposium May 29 - June 2, 2017 Orlando, Florida USA
- **Summary**: In this paper we investigate an emerging application, 3D scene understanding, likely to be significant in the mobile space in the near future. The goal of this exploration is to reduce execution time while meeting our quality of result objectives. In previous work we showed for the first time that it is possible to map this application to power constrained embedded systems, highlighting that decision choices made at the algorithmic design-level have the most impact.   As the algorithmic design space is too large to be exhaustively evaluated, we use a previously introduced multi-objective Random Forest Active Learning prediction framework dubbed HyperMapper, to find good algorithmic designs. We show that HyperMapper generalizes on a recent cutting edge 3D scene understanding algorithm and on a modern GPU-based computer architecture. HyperMapper is able to beat an expert human hand-tuning the algorithmic parameters of the class of Computer Vision applications taken under consideration in this paper automatically. In addition, we use crowd-sourcing using a 3D scene understanding Android app to show that the Pareto front obtained on an embedded system can be used to accelerate the same application on all the 83 smart-phones and tablets crowd-sourced with speedups ranging from 2 to over 12.



### Solving Uncalibrated Photometric Stereo Using Fewer Images by Jointly Optimizing Low-rank Matrix Completion and Integrability
- **Arxiv ID**: http://arxiv.org/abs/1702.00506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00506v1)
- **Published**: 2017-02-02 00:07:46+00:00
- **Updated**: 2017-02-02 00:07:46+00:00
- **Authors**: Soumyadip Sengupta, Hao Zhou, Walter Forkel, Ronen Basri, Tom Goldstein, David W. Jacobs
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new, integrated approach to uncalibrated photometric stereo. We perform 3D reconstruction of Lambertian objects using multiple images produced by unknown, directional light sources. We show how to formulate a single optimization that includes rank and integrability constraints, allowing also for missing data. We then solve this optimization using the Alternate Direction Method of Multipliers (ADMM). We conduct extensive experimental evaluation on real and synthetic data sets. Our integrated approach is particularly valuable when performing photometric stereo using as few as 4-6 images, since the integrability constraint is capable of improving estimation of the linear subspace of possible solutions. We show good improvements over prior work in these cases.



### Segmentation of optic disc, fovea and retinal vasculature using a single convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1702.00509v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1702.00509v1)
- **Published**: 2017-02-02 00:37:22+00:00
- **Updated**: 2017-02-02 00:37:22+00:00
- **Authors**: Jen Hong Tan, U. Rajendra Acharya, Sulatha V. Bhandary, Kuang Chua Chua, Sobha Sivaprasad
- **Comment**: None
- **Journal**: None
- **Summary**: We have developed and trained a convolutional neural network to automatically and simultaneously segment optic disc, fovea and blood vessels. Fundus images were normalised before segmentation was performed to enforce consistency in background lighting and contrast. For every effective point in the fundus image, our algorithm extracted three channels of input from the neighbourhood of the point and forward the response across the 7 layer network. In average, our segmentation achieved an accuracy of 92.68 percent on the testing set from Drive database.



### Deep Learning the Indus Script
- **Arxiv ID**: http://arxiv.org/abs/1702.00523v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, I.5.4; I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1702.00523v1)
- **Published**: 2017-02-02 01:56:22+00:00
- **Updated**: 2017-02-02 01:56:22+00:00
- **Authors**: Satish Palaniappan, Ronojoy Adhikari
- **Comment**: 17 pages, 10 figures, 7 supporting figures (2 pages)
- **Journal**: None
- **Summary**: Standardized corpora of undeciphered scripts, a necessary starting point for computational epigraphy, requires laborious human effort for their preparation from raw archaeological records. Automating this process through machine learning algorithms can be of significant aid to epigraphical research. Here, we take the first steps in this direction and present a deep learning pipeline that takes as input images of the undeciphered Indus script, as found in archaeological artifacts, and returns as output a string of graphemes, suitable for inclusion in a standard corpus. The image is first decomposed into regions using Selective Search and these regions are classified as containing textual and/or graphical information using a convolutional neural network. Regions classified as potentially containing text are hierarchically merged and trimmed to remove non-textual information. The remaining textual part of the image is segmented using standard image processing techniques to isolate individual graphemes. This set is finally passed to a second convolutional neural network to classify the graphemes, based on a standard corpus. The classifier can identify the presence or absence of the most frequent Indus grapheme, the "jar" sign, with an accuracy of 92%. Our results demonstrate the great potential of deep learning approaches in computational epigraphy and, more generally, in the digital humanities.



### Automating Image Analysis by Annotating Landmarks with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.00583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00583v1)
- **Published**: 2017-02-02 08:53:10+00:00
- **Updated**: 2017-02-02 08:53:10+00:00
- **Authors**: Mikhail Breslav, Tyson L. Hedrick, Stan Sclaroff, Margrit Betke
- **Comment**: 30 pages
- **Journal**: None
- **Summary**: Image and video analysis is often a crucial step in the study of animal behavior and kinematics. Often these analyses require that the position of one or more animal landmarks are annotated (marked) in numerous images. The process of annotating landmarks can require a significant amount of time and tedious labor, which motivates the need for algorithms that can automatically annotate landmarks. In the community of scientists that use image and video analysis to study the 3D flight of animals, there has been a trend of developing more automated approaches for annotating landmarks, yet they fall short of being generally applicable. Inspired by the success of Deep Neural Networks (DNNs) on many problems in the field of computer vision, we investigate how suitable DNNs are for accurate and automatic annotation of landmarks in video datasets representative of those collected by scientists studying animals.   Our work shows, through extensive experimentation on videos of hawkmoths, that DNNs are suitable for automatic and accurate landmark localization. In particular, we show that one of our proposed DNNs is more accurate than the current best algorithm for automatic localization of landmarks on hawkmoth videos. Moreover, we demonstrate how these annotations can be used to quantitatively analyze the 3D flight of a hawkmoth. To facilitate the use of DNNs by scientists from many different fields, we provide a self contained explanation of what DNNs are, how they work, and how to apply them to other datasets using the freely available library Caffe and supplemental code that we provide.



### A Fast and Compact Saliency Score Regression Network Based on Fully Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1702.00615v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00615v2)
- **Published**: 2017-02-02 11:07:51+00:00
- **Updated**: 2017-02-24 14:15:31+00:00
- **Authors**: Xuanyang Xi, Yongkang Luo, Fengfu Li, Peng Wang, Hong Qiao
- **Comment**: None
- **Journal**: None
- **Summary**: Visual saliency detection aims at identifying the most visually distinctive parts in an image, and serves as a pre-processing step for a variety of computer vision and image processing tasks. To this end, the saliency detection procedure must be as fast and compact as possible and optimally processes input images in a real time manner. It is an essential application requirement for the saliency detection task. However, contemporary detection methods often utilize some complicated procedures to pursue feeble improvements on the detection precession, which always take hundreds of milliseconds and make them not easy to be applied practically. In this paper, we tackle this problem by proposing a fast and compact saliency score regression network which employs fully convolutional network, a special deep convolutional neural network, to estimate the saliency of objects in images. It is an extremely simplified end-to-end deep neural network without any pre-processings and post-processings. When given an image, the network can directly predict a dense full-resolution saliency map (image-to-image prediction). It works like a compact pipeline which effectively simplifies the detection procedure. Our method is evaluated on six public datasets, and experimental results show that it can achieve comparable or better precision performance than the state-of-the-art methods while get a significant improvement in detection speed (35 FPS, processing in real time).



### Side Information in Robust Principal Component Analysis: Algorithms and Applications
- **Arxiv ID**: http://arxiv.org/abs/1702.00648v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00648v2)
- **Published**: 2017-02-02 12:42:50+00:00
- **Updated**: 2017-03-28 16:23:44+00:00
- **Authors**: Niannan Xue, Yannis Panagakis, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Robust Principal Component Analysis (RPCA) aims at recovering a low-rank subspace from grossly corrupted high-dimensional (often visual) data and is a cornerstone in many machine learning and computer vision applications. Even though RPCA has been shown to be very successful in solving many rank minimisation problems, there are still cases where degenerate or suboptimal solutions are obtained. This is likely to be remedied by taking into account of domain-dependent prior knowledge. In this paper, we propose two models for the RPCA problem with the aid of side information on the low-rank structure of the data. The versatility of the proposed methods is demonstrated by applying them to four applications, namely background subtraction, facial image denoising, face and facial expression recognition. Experimental results on synthetic and five real world datasets indicate the robustness and effectiveness of the proposed methods on these application domains, largely outperforming six previous approaches.



### Learning a time-dependent master saliency map from eye-tracking data in videos
- **Arxiv ID**: http://arxiv.org/abs/1702.00714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00714v1)
- **Published**: 2017-02-02 15:30:55+00:00
- **Updated**: 2017-02-02 15:30:55+00:00
- **Authors**: Antoine Coutrot, Nathalie Guyader
- **Comment**: None
- **Journal**: None
- **Summary**: To predict the most salient regions of complex natural scenes, saliency models commonly compute several feature maps (contrast, orientation, motion...) and linearly combine them into a master saliency map. Since feature maps have different spatial distribution and amplitude dynamic ranges, determining their contributions to overall saliency remains an open problem. Most state-of-the-art models do not take time into account and give feature maps constant weights across the stimulus duration. However, visual exploration is a highly dynamic process shaped by many time-dependent factors. For instance, some systematic viewing patterns such as the center bias are known to dramatically vary across the time course of the exploration. In this paper, we use maximum likelihood and shrinkage methods to dynamically and jointly learn feature map and systematic viewing pattern weights directly from eye-tracking data recorded on videos. We show that these weights systematically vary as a function of time, and heavily depend upon the semantic visual category of the videos being processed. Our fusion method allows taking these variations into account, and outperforms other state-of-the-art fusion schemes using constant weights over time. The code, videos and eye-tracking data we used for this study are available online: http://antoinecoutrot.magix.net/public/research.html



### Maritime situational awareness using adaptive multi-sensor management under hazy conditions
- **Arxiv ID**: http://arxiv.org/abs/1702.00754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00754v1)
- **Published**: 2017-02-02 17:19:16+00:00
- **Updated**: 2017-02-02 17:19:16+00:00
- **Authors**: D. K. Prasad, C. K. Prasath, D. Rajan, L. Rachmawati, E. Rajabally, C. Quek
- **Comment**: 11 pages, 2 figures, MTEC 2017
- **Journal**: None
- **Summary**: This paper presents a multi-sensor architecture with an adaptive multi-sensor management system suitable for control and navigation of autonomous maritime vessels in hazy and poor-visibility conditions. This architecture resides in the autonomous maritime vessels. It augments the data from on-board imaging sensors and weather sensors with the AIS data and weather data from sensors on other vessels and the on-shore vessel traffic surveillance system. The combined data is analyzed using computational intelligence and data analytics to determine suitable course of action while utilizing historically learnt knowledge and performing live learning from the current situation. Such framework is expected to be useful in diverse weather conditions and shall be a useful architecture to provide autonomy to maritime vessels.



### HashNet: Deep Learning to Hash by Continuation
- **Arxiv ID**: http://arxiv.org/abs/1702.00758v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.00758v4)
- **Published**: 2017-02-02 17:29:24+00:00
- **Updated**: 2017-07-29 17:55:50+00:00
- **Authors**: Zhangjie Cao, Mingsheng Long, Jianmin Wang, Philip S. Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval, due to its computation efficiency and retrieval quality. Deep learning to hash, which improves retrieval quality by end-to-end representation learning and hash encoding, has received increasing attention recently. Subject to the ill-posed gradient difficulty in the optimization with sign activations, existing deep learning to hash methods need to first learn continuous representations and then generate binary hash codes in a separated binarization step, which suffer from substantial loss of retrieval quality. This work presents HashNet, a novel deep architecture for deep learning to hash by continuation method with convergence guarantees, which learns exactly binary hash codes from imbalanced similarity data. The key idea is to attack the ill-posed gradient problem in optimizing deep networks with non-smooth binary activations by continuation method, in which we begin from learning an easier network with smoothed activation function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, deep network with the sign activation function. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art multimedia retrieval performance on standard benchmarks.



### Pixel Recursive Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1702.00783v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1702.00783v2)
- **Published**: 2017-02-02 18:59:17+00:00
- **Updated**: 2017-03-22 16:13:21+00:00
- **Authors**: Ryan Dahl, Mohammad Norouzi, Jonathon Shlens
- **Comment**: None
- **Journal**: None
- **Summary**: We present a pixel recursive super resolution model that synthesizes realistic details into images while enhancing their resolution. A low resolution image may correspond to multiple plausible high resolution images, thus modeling the super resolution process with a pixel independent conditional model often results in averaging different details--hence blurry edges. By contrast, our model is able to represent a multimodal conditional distribution by properly modeling the statistical dependencies among the high resolution image pixels, conditioned on a low resolution input. We employ a PixelCNN architecture to define a strong prior over natural images and jointly optimize this prior with a deep conditioning convolutional network. Human evaluations indicate that samples from our proposed model look more photo realistic than a strong L2 regression baseline.



### YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video
- **Arxiv ID**: http://arxiv.org/abs/1702.00824v5
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8; I.2.10; I.5.2; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1702.00824v5)
- **Published**: 2017-02-02 20:58:02+00:00
- **Updated**: 2017-03-24 18:52:39+00:00
- **Authors**: Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan, Vincent Vanhoucke
- **Comment**: Accepted at the Conference on Computer Vision and Pattern Recognition
  (CVPR) 2017
- **Journal**: None
- **Summary**: We introduce a new large-scale data set of video URLs with densely-sampled object bounding box annotations called YouTube-BoundingBoxes (YT-BB). The data set consists of approximately 380,000 video segments about 19s long, automatically selected to feature objects in natural settings without editing or post-processing, with a recording quality often akin to that of a hand-held cell phone camera. The objects represent a subset of the MS COCO label set. All video segments were human-annotated with high-precision classification labels and bounding boxes at 1 frame per second. The use of a cascade of increasingly precise human annotations ensures a label accuracy above 95% for every class and tight bounding boxes. Finally, we train and evaluate well-known deep network architectures and report baseline figures for per-frame classification and localization to provide a point of comparison for future work. We also demonstrate how the temporal contiguity of video can potentially be used to improve such inferences. Please see the PDF file to find the URL to download the data. We hope the availability of such large curated corpus will spur new advances in video object detection and tracking.



### Video Salient Object Detection via Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.00871v3
- **DOI**: 10.1109/TIP.2017.2754941
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00871v3)
- **Published**: 2017-02-02 23:44:28+00:00
- **Updated**: 2017-12-09 01:42:49+00:00
- **Authors**: Wenguan Wang, Jianbing Shen, Ling Shao
- **Comment**: W. Wang, J. Shen, and L. Shao, Video salient object detection via
  fully convolutional networks, IEEE Trans. on Image Processing, 27(1):38-49,
  2018 Code and results: https://github.com/wenguanwang/deepvideosaliency
- **Journal**: IEEE Transactions on Image Processing, Vol. 27, No. 1, pp 38-49,
  2018
- **Summary**: This paper proposes a deep learning model to efficiently detect salient regions in videos. It addresses two important issues: (1) deep video saliency model training with the absence of sufficiently large and pixel-wise annotated video data, and (2) fast video saliency training and detection. The proposed deep video saliency network consists of two modules, for capturing the spatial and temporal saliency information, respectively. The dynamic saliency model, explicitly incorporating saliency estimates from the static saliency model, directly produces spatiotemporal saliency inference without time-consuming optical flow computation. We further propose a novel data augmentation technique that simulates video training data from existing annotated image datasets, which enables our network to learn diverse saliency information and prevents overfitting with the limited number of training videos. Leveraging our synthetic video data (150K video sequences) and real videos, our deep video saliency model successfully learns both spatial and temporal saliency cues, thus producing accurate spatiotemporal saliency estimate. We advance the state-of-the-art on the DAVIS dataset (MAE of .06) and the FBMS dataset (MAE of .07), and do so with much improved speed (2fps with all steps).



