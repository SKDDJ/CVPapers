# Arxiv Papers in cs.CV on 2017-02-28
### Enabling Sparse Winograd Convolution by Native Pruning
- **Arxiv ID**: http://arxiv.org/abs/1702.08597v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08597v2)
- **Published**: 2017-02-28 01:37:57+00:00
- **Updated**: 2017-10-13 18:10:39+00:00
- **Authors**: Sheng Li, Jongsoo Park, Ping Tak Peter Tang
- **Comment**: 10 pages, 2 figures
- **Journal**: None
- **Summary**: Sparse methods and the use of Winograd convolutions are two orthogonal approaches, each of which significantly accelerates convolution computations in modern CNNs. Sparse Winograd merges these two and thus has the potential to offer a combined performance benefit. Nevertheless, training convolution layers so that the resulting Winograd kernels are sparse has not hitherto been very successful. By introducing a Winograd layer in place of a standard convolution layer, we can learn and prune Winograd coefficients "natively" and obtain sparsity level beyond 90% with only 0.1% accuracy loss with AlexNet on ImageNet dataset. Furthermore, we present a sparse Winograd convolution algorithm and implementation that exploits the sparsity, achieving up to 31.7 effective TFLOP/s in 32-bit precision on a latest Intel Xeon CPU, which corresponds to a 5.4x speedup over a state-of-the-art dense convolution implementation.



### Parallel Structure from Motion from Local Increment to Global Averaging
- **Arxiv ID**: http://arxiv.org/abs/1702.08601v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08601v3)
- **Published**: 2017-02-28 01:46:51+00:00
- **Updated**: 2017-06-05 03:05:15+00:00
- **Authors**: Siyu Zhu, Tianwei Shen, Lei Zhou, Runze Zhang, Jinglu Wang, Tian Fang, Long Quan
- **Comment**: Under review at the International Conference on Computer Vision
  (ICCV) 2017
- **Journal**: None
- **Summary**: In this paper, we tackle the accurate and consistent Structure from Motion (SfM) problem, in particular camera registration, far exceeding the memory of a single computer in parallel. Different from the previous methods which drastically simplify the parameters of SfM and sacrifice the accuracy of the final reconstruction, we try to preserve the connectivities among cameras by proposing a camera clustering algorithm to divide a large SfM problem into smaller sub-problems in terms of camera clusters with overlapping. We then exploit a hybrid formulation that applies the relative poses from local incremental SfM into a global motion averaging framework and produce accurate and consistent global camera poses. Our scalable formulation in terms of camera clusters is highly applicable to the whole SfM pipeline including track generation, local SfM, 3D point triangulation and bundle adjustment. We are even able to reconstruct the camera poses of a city-scale data-set containing more than one million high-resolution images with superior accuracy and robustness evaluated on benchmark, Internet, and sequential data-sets.



### The Active Atlas: Combining 3D Anatomical Models with Texture Detectors
- **Arxiv ID**: http://arxiv.org/abs/1702.08606v3
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1702.08606v3)
- **Published**: 2017-02-28 02:18:47+00:00
- **Updated**: 2018-01-15 18:33:24+00:00
- **Authors**: Yuncong Chen, Lauren McElvain, Alex Tolpygo, Daniel Ferrante, Harvey Karten, Partha Mitra, David Kleinfeld, Yoav Freund
- **Comment**: 8 pages, 10 figures, appeared in proceeding of MICCAI 2017
- **Journal**: None
- **Summary**: While modern imaging technologies such as fMRI have opened exciting new possibilities for studying the brain in vivo, histological sections remain the best way to study the anatomy of the brain at the level of single neurons. The histological atlas changed little since 1909 and localizing brain regions is a still a labor intensive process performed only by experienced neuro-anatomists. Existing digital atlases such as the Allen Brain atlas are limited to low resolution images which cannot identify the detailed structure of the neurons. We have developed a digital atlas methodology that combines information about the 3D organization of the brain and the detailed texture of neurons in different structures. Using the methodology we developed an atlas for the mouse brainstem and mid-brain, two regions for which there are currently no good atlases. Our atlas is "active" in that it can be used to automatically align a histological stack to the atlas, thus reducing the work of the neuroanatomist.



### Show, Attend and Interact: Perceivable Human-Robot Social Interaction through Neural Attention Q-Network
- **Arxiv ID**: http://arxiv.org/abs/1702.08626v1
- **DOI**: 10.1109/ICRA.2017.7989193
- **Categories**: **cs.RO**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1702.08626v1)
- **Published**: 2017-02-28 03:16:40+00:00
- **Updated**: 2017-02-28 03:16:40+00:00
- **Authors**: Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa, Hiroshi Ishiguro
- **Comment**: 7 pages, 5 figures, accepted by IEEE-RAS ICRA'17
- **Journal**: None
- **Summary**: For a safe, natural and effective human-robot social interaction, it is essential to develop a system that allows a robot to demonstrate the perceivable responsive behaviors to complex human behaviors. We introduce the Multimodal Deep Attention Recurrent Q-Network using which the robot exhibits human-like social interaction skills after 14 days of interacting with people in an uncontrolled real world. Each and every day during the 14 days, the system gathered robot interaction experiences with people through a hit-and-trial method and then trained the MDARQN on these experiences using end-to-end reinforcement learning approach. The results of interaction based learning indicate that the robot has learned to respond to complex human behaviors in a perceivable and socially acceptable manner.



### An Optimization Framework with Flexible Inexact Inner Iterations for Nonconvex and Nonsmooth Programming
- **Arxiv ID**: http://arxiv.org/abs/1702.08627v3
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1702.08627v3)
- **Published**: 2017-02-28 03:22:55+00:00
- **Updated**: 2017-06-30 02:28:54+00:00
- **Authors**: Yiyang Wang, Risheng Liu, Xiaoliang Song, Zhixun Su
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, numerous vision and learning tasks have been (re)formulated as nonconvex and nonsmooth programmings(NNPs). Although some algorithms have been proposed for particular problems, designing fast and flexible optimization schemes with theoretical guarantee is a challenging task for general NNPs. It has been investigated that performing inexact inner iterations often benefit to special applications case by case, but their convergence behaviors are still unclear. Motivated by these practical experiences, this paper designs a novel algorithmic framework, named inexact proximal alternating direction method (IPAD) for solving general NNPs. We demonstrate that any numerical algorithms can be incorporated into IPAD for solving subproblems and the convergence of the resulting hybrid schemes can be consistently guaranteed by a series of simple error conditions. Beyond the guarantee in theory, numerical experiments on both synthesized and real-world data further demonstrate the superiority and flexibility of our IPAD framework for practical use.



### Super-Trajectory for Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1702.08634v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08634v4)
- **Published**: 2017-02-28 03:51:42+00:00
- **Updated**: 2017-07-24 01:14:51+00:00
- **Authors**: Wenguan Wang, Jianbing Shen, Jianwen Xie, Fatih Porikli
- **Comment**: This paper has been published in ICCV 2017
- **Journal**: None
- **Summary**: We introduce a novel semi-supervised video segmentation approach based on an efficient video representation, called as "super-trajectory". Each super-trajectory corresponds to a group of compact trajectories that exhibit consistent motion patterns, similar appearance and close spatiotemporal relationships. We generate trajectories using a probabilistic model, which handles occlusions and drifts in a robust and natural way. To reliably group trajectories, we adopt a modified version of the density peaks based clustering algorithm that allows capturing rich spatiotemporal relations among trajectories in the clustering process. The presented video representation is discriminative enough to accurately propagate the initial annotations in the first frame onto the remaining video frames. Extensive experimental analysis on challenging benchmarks demonstrate our method is capable of distinguishing the target objects from complex backgrounds and even reidentifying them after long-term occlusions.



### Selective Video Object Cutout
- **Arxiv ID**: http://arxiv.org/abs/1702.08640v5
- **DOI**: 10.1109/TIP.2017.2745098
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08640v5)
- **Published**: 2017-02-28 04:33:00+00:00
- **Updated**: 2018-03-22 23:01:47+00:00
- **Authors**: Wenguan Wang, Jianbing Shen, Fatih Porikli
- **Comment**: W. Wang, J. Shen, and F. Porikli. "Selective video object cutout."
  IEEE Transactions on Image Processing 26.12 (2017): 5645-5655
- **Journal**: IEEE Transactions on Image Processing, Vol. 26, No. 12, pp
  5645-5655, 2017
- **Summary**: Conventional video segmentation approaches rely heavily on appearance models. Such methods often use appearance descriptors that have limited discriminative power under complex scenarios. To improve the segmentation performance, this paper presents a pyramid histogram based confidence map that incorporates structure information into appearance statistics. It also combines geodesic distance based dynamic models. Then, it employs an efficient measure of uncertainty propagation using local classifiers to determine the image regions where the object labels might be ambiguous. The final foreground cutout is obtained by refining on the uncertain regions. Additionally, to reduce manual labeling, our method determines the frames to be labeled by the human operator in a principled manner, which further boosts the segmentation performance and minimizes the labeling effort. Our extensive experimental analyses on two big benchmarks demonstrate that our solution achieves superior performance, favorable computational efficiency, and reduced manual labeling in comparison to the state-of-the-art.



### Boundary Flow: A Siamese Network that Predicts Boundary Motion without Training on Motion
- **Arxiv ID**: http://arxiv.org/abs/1702.08646v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08646v3)
- **Published**: 2017-02-28 04:57:12+00:00
- **Updated**: 2018-04-08 07:00:11+00:00
- **Authors**: Peng Lei, Fuxin Li, Sinisa Todorovic
- **Comment**: To appear at CVPR18
- **Journal**: None
- **Summary**: Using deep learning, this paper addresses the problem of joint object boundary detection and boundary motion estimation in videos, which we named boundary flow estimation. Boundary flow is an important mid-level visual cue as boundaries characterize objects spatial extents, and the flow indicates objects motions and interactions. Yet, most prior work on motion estimation has focused on dense object motion or feature points that may not necessarily reside on boundaries. For boundary flow estimation, we specify a new fully convolutional Siamese network (FCSN) that jointly estimates object-level boundaries in two consecutive frames. Boundary correspondences in the two frames are predicted by the same FCSN with a new, unconventional deconvolution approach. Finally, the boundary flow estimate is improved with an edgelet-based filtering. Evaluation is conducted on three tasks: boundary detection in videos, boundary flow estimation, and optical flow estimation. On boundary detection, we achieve the state-of-the-art performance on the benchmark VSB100 dataset. On boundary flow estimation, we present the first results on the Sintel training dataset. For optical flow estimation, we run the recent approach CPMFlow but on the augmented input with our boundary-flow matches, and achieve significant performance improvement on the Sintel benchmark.



### Scene Flow to Action Map: A New Representation for RGB-D based Action Recognition with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.08652v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08652v3)
- **Published**: 2017-02-28 05:39:25+00:00
- **Updated**: 2017-03-27 00:52:21+00:00
- **Authors**: Pichao Wang, Wanqing Li, Zhimin Gao, Yuyao Zhang, Chang Tang, Philip Ogunbona
- **Comment**: None
- **Journal**: None
- **Summary**: Scene flow describes the motion of 3D objects in real world and potentially could be the basis of a good feature for 3D action recognition. However, its use for action recognition, especially in the context of convolutional neural networks (ConvNets), has not been previously studied. In this paper, we propose the extraction and use of scene flow for action recognition from RGB-D data. Previous works have considered the depth and RGB modalities as separate channels and extract features for later fusion. We take a different approach and consider the modalities as one entity, thus allowing feature extraction for action recognition at the beginning. Two key questions about the use of scene flow for action recognition are addressed: how to organize the scene flow vectors and how to represent the long term dynamics of videos based on scene flow. In order to calculate the scene flow correctly on the available datasets, we propose an effective self-calibration method to align the RGB and depth data spatially without knowledge of the camera parameters. Based on the scene flow vectors, we propose a new representation, namely, Scene Flow to Action Map (SFAM), that describes several long term spatio-temporal dynamics for action recognition. We adopt a channel transform kernel to transform the scene flow vectors to an optimal color space analogous to RGB. This transformation takes better advantage of the trained ConvNets models over ImageNet. Experimental results indicate that this new representation can surpass the performance of state-of-the-art methods on two large public datasets.



### 3D Shape Segmentation via Shape Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.08675v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08675v3)
- **Published**: 2017-02-28 07:26:55+00:00
- **Updated**: 2018-05-26 01:45:21+00:00
- **Authors**: Pengyu Wang, Yuan Gan, Panpan Shui, Fenggen Yu, Yan Zhang, Songle Chen, Zhengxing Sun
- **Comment**: We update some missing references about intrinsic CNNs (2018.5.24)
- **Journal**: Computer & Graphics 70 (2),(2018), p. 128-139
- **Summary**: We desgin a novel fully convolutional network architecture for shapes, denoted by Shape Fully Convolutional Networks (SFCN). 3D shapes are represented as graph structures in the SFCN architecture, based on novel graph convolution and pooling operations, which are similar to convolution and pooling operations used on images. Meanwhile, to build our SFCN architecture in the original image segmentation fully convolutional network (FCN) architecture, we also design and implement a generating operation} with bridging function. This ensures that the convolution and pooling operation we have designed can be successfully applied in the original FCN architecture. In this paper, we also present a new shape segmentation approach based on SFCN. Furthermore, we allow more general and challenging input, such as mixed datasets of different categories of shapes} which can prove the ability of our generalisation. In our approach, SFCNs are trained triangles-to-triangles by using three low-level geometric features as input. Finally, the feature voting-based multi-label graph cuts is adopted to optimise the segmentation results obtained by SFCN prediction. The experiment results show that our method can effectively learn and predict mixed shape datasets of either similar or different characteristics, and achieve excellent segmentation results.



### MIML-FCN+: Multi-instance Multi-label Learning via Fully Convolutional Networks with Privileged Information
- **Arxiv ID**: http://arxiv.org/abs/1702.08681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08681v1)
- **Published**: 2017-02-28 07:54:22+00:00
- **Updated**: 2017-02-28 07:54:22+00:00
- **Authors**: Hao Yang, Joey Tianyi Zhou, Jianfei Cai, Yew Soon Ong
- **Comment**: Accepted in CVPR 2017
- **Journal**: None
- **Summary**: Multi-instance multi-label (MIML) learning has many interesting applications in computer visions, including multi-object recognition and automatic image tagging. In these applications, additional information such as bounding-boxes, image captions and descriptions is often available during training phrase, which is referred as privileged information (PI). However, as existing works on learning using PI only consider instance-level PI (privileged instances), they fail to make use of bag-level PI (privileged bags) available in MIML learning. Therefore, in this paper, we propose a two-stream fully convolutional network, named MIML-FCN+, unified by a novel PI loss to solve the problem of MIML learning with privileged bags. Compared to the previous works on PI, the proposed MIML-FCN+ utilizes the readily available privileged bags, instead of hard-to-obtain privileged instances, making the system more general and practical in real world applications. As the proposed PI loss is convex and SGD compatible and the framework itself is a fully convolutional network, MIML-FCN+ can be easily integrated with state of-the-art deep learning networks. Moreover, the flexibility of convolutional layers allows us to exploit structured correlations among instances to facilitate more effective training and testing. Experimental results on three benchmark datasets demonstrate the effectiveness of the proposed MIML-FCN+, outperforming state-of-the-art methods in the application of multi-object recognition.



### Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-tuning
- **Arxiv ID**: http://arxiv.org/abs/1702.08690v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1702.08690v2)
- **Published**: 2017-02-28 08:40:44+00:00
- **Updated**: 2017-06-06 11:51:03+00:00
- **Authors**: Weifeng Ge, Yizhou Yu
- **Comment**: To appear in 2017 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2017)
- **Journal**: None
- **Summary**: Deep neural networks require a large amount of labeled training data during supervised learning. However, collecting and labeling so much data might be infeasible in many cases. In this paper, we introduce a source-target selective joint fine-tuning scheme for improving the performance of deep learning tasks with insufficient training data. In this scheme, a target learning task with insufficient training data is carried out simultaneously with another source learning task with abundant training data. However, the source learning task does not use all existing training data. Our core idea is to identify and use a subset of training images from the original source learning task whose low-level characteristics are similar to those from the target learning task, and jointly fine-tune shared convolutional layers for both tasks. Specifically, we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task.   Experiments demonstrate that our selective joint fine-tuning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning. Such tasks include Caltech 256, MIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to fine-tuning without a source domain, the proposed method can improve the classification accuracy by 2% - 10% using a single model.



### Cascade one-vs-rest detection network for fine-grained recognition without part annotations
- **Arxiv ID**: http://arxiv.org/abs/1702.08692v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08692v2)
- **Published**: 2017-02-28 08:45:15+00:00
- **Updated**: 2017-08-23 07:05:42+00:00
- **Authors**: Long Chen, Junyu Dong, ShengKe Wang, Kin-Man Lam, Muwei Jian, Hua Zhang, XiaoChun Cao
- **Comment**: Part of authors has changed
- **Journal**: None
- **Summary**: Fine-grained recognition is a challenging task due to the small intra-category variances. Most of top-performing fine-grained recognition methods leverage parts of objects for better performance. Therefore, part annotations which are extremely computationally expensive are required. In this paper, we propose a novel cascaded deep CNN detection framework for fine-grained recognition which is trained to detect the whole object without considering parts. Nevertheless, most of current top-performing detection networks use the N+1 class (N object categories plus background) softmax loss, and the background category with much more training samples dominates the feature learning progress so that the features are not good for object categories with fewer samples. To bridge this gap, we introduce a cascaded structure to eliminate background and exploit a one-vs-rest loss to capture more minute variances among different subordinate categories. Experiments show that our proposed recognition framework achieves comparable performance with state-of-the-art, part-free, fine-grained recognition methods on the CUB-200-2011 Bird dataset. Moreover, our method even outperforms most of part-based methods while does not need part annotations at the training stage and is free from any annotations at test stage.



### II-FCN for skin lesion analysis towards melanoma detection
- **Arxiv ID**: http://arxiv.org/abs/1702.08699v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08699v2)
- **Published**: 2017-02-28 08:56:02+00:00
- **Updated**: 2017-03-14 13:40:27+00:00
- **Authors**: Hongdiao Wen
- **Comment**: 4 page abstract about our solution to the challenge of Lesion
  Segmentation in ISIC2017
- **Journal**: None
- **Summary**: Dermoscopy image detection stays a tough task due to the weak distinguishable property of the object.Although the deep convolution neural network signifigantly boosted the performance on prevelance computer vision tasks in recent years,there remains a room to explore more robust and precise models to the problem of low contrast image segmentation.Towards the challenge of Lesion Segmentation in ISBI 2017,we built a symmetrical identity inception fully convolution network which is based on only 10 reversible inception blocks,every block composed of four convolution branches with combination of different layer depth and kernel size to extract sundry semantic features.Then we proposed an approximate loss function for jaccard index metrics to train our model.To overcome the drawbacks of traditional convolution,we adopted the dilation convolution and conditional random field method to rectify our segmentation.We also introduced multiple ways to prevent the problem of overfitting.The experimental results shows that our model achived jaccard index of 0.82 and kept learning from epoch to epoch.



### An Extensive Technique to Detect and Analyze Melanoma: A Challenge at the International Symposium on Biomedical Imaging (ISBI) 2017
- **Arxiv ID**: http://arxiv.org/abs/1702.08717v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08717v1)
- **Published**: 2017-02-28 09:45:32+00:00
- **Updated**: 2017-02-28 09:45:32+00:00
- **Authors**: G Wiselin Jiji, P Johnson Durai Raj
- **Comment**: 4 Page Abstract for ISIC2017 Challenge
- **Journal**: None
- **Summary**: An automated method to detect and analyze the melanoma is presented to improve diagnosis which will leads to the exact treatment. Image processing techniques such as segmentation, feature descriptors and classification models are involved in this method. In the First phase the lesion region is segmented using CIELAB Color space Based Segmentation. Then feature descriptors such as shape, color and texture are extracted. Finally, in the third phase lesion region is classified as melanoma, seborrheic keratosis or nevus using multi class O-A SVM model. Experiment with ISIC 2017 Archive skin image database has been done and analyzed the results.



### Learning Deep Visual Object Models From Noisy Web Data: How to Make it Work
- **Arxiv ID**: http://arxiv.org/abs/1702.08513v1
- **DOI**: 10.1109/IROS.2017.8206444
- **Categories**: **cs.CV**, cs.DB, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1702.08513v1)
- **Published**: 2017-02-28 10:02:36+00:00
- **Updated**: 2017-02-28 10:02:36+00:00
- **Authors**: Nizar Massouh, Francesca Babiloni, Tatiana Tommasi, Jay Young, Nick Hawes, Barbara Caputo
- **Comment**: 8 pages, 7 figures, 3 tables
- **Journal**: 2017 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)
- **Summary**: Deep networks thrive when trained on large scale data collections. This has given ImageNet a central role in the development of deep architectures for visual object classification. However, ImageNet was created during a specific period in time, and as such it is prone to aging, as well as dataset bias issues. Moving beyond fixed training datasets will lead to more robust visual systems, especially when deployed on robots in new environments which must train on the objects they encounter there. To make this possible, it is important to break free from the need for manual annotators. Recent work has begun to investigate how to use the massive amount of images available on the Web in place of manual image annotations. We contribute to this research thread with two findings: (1) a study correlating a given level of noisily labels to the expected drop in accuracy, for two deep architectures, on two different types of noise, that clearly identifies GoogLeNet as a suitable architecture for learning from Web data; (2) a recipe for the creation of Web datasets with minimal noise and maximum visual variability, based on a visual and natural language processing concept expansion strategy. By combining these two results, we obtain a method for learning powerful deep object models automatically from the Web. We confirm the effectiveness of our approach through object categorization experiments using our Web-derived version of ImageNet on a popular robot vision benchmark database, and on a lifelong object discovery task on a mobile robot.



### Billion-scale similarity search with GPUs
- **Arxiv ID**: http://arxiv.org/abs/1702.08734v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB, cs.DS, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1702.08734v1)
- **Published**: 2017-02-28 10:42:31+00:00
- **Updated**: 2017-02-28 10:42:31+00:00
- **Authors**: Jeff Johnson, Matthijs Douze, Hervé Jégou
- **Comment**: None
- **Journal**: None
- **Summary**: Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy.   We propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.



### Weakly- and Semi-Supervised Object Detection with Expectation-Maximization Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1702.08740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08740v1)
- **Published**: 2017-02-28 11:03:39+00:00
- **Updated**: 2017-02-28 11:03:39+00:00
- **Authors**: Ziang Yan, Jian Liang, Weishen Pan, Jin Li, Changshui Zhang
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Object detection when provided image-level labels instead of instance-level labels (i.e., bounding boxes) during training is an important problem in computer vision, since large scale image datasets with instance-level labels are extremely costly to obtain. In this paper, we address this challenging problem by developing an Expectation-Maximization (EM) based object detection method using deep convolutional neural networks (CNNs). Our method is applicable to both the weakly-supervised and semi-supervised settings. Extensive experiments on PASCAL VOC 2007 benchmark show that (1) in the weakly supervised setting, our method provides significant detection performance improvement over current state-of-the-art methods, (2) having access to a small number of strongly (instance-level) annotated images, our method can almost match the performace of the fully supervised Fast RCNN. We share our source code at https://github.com/ZiangYan/EM-WSD.



### MILD: Multi-Index hashing for Loop closure Detection
- **Arxiv ID**: http://arxiv.org/abs/1702.08780v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08780v1)
- **Published**: 2017-02-28 13:30:06+00:00
- **Updated**: 2017-02-28 13:30:06+00:00
- **Authors**: Lei Han, Lu Fang
- **Comment**: 6 pages, 5 figures; accepted by IEEE ICME 2017
- **Journal**: None
- **Summary**: Loop Closure Detection (LCD) has been proved to be extremely useful in global consistent visual Simultaneously Localization and Mapping (SLAM) and appearance-based robot relocalization. Methods exploiting binary features in bag of words representation have recently gained a lot of popularity for their efficiency, but suffer from low recall due to the inherent drawback that high dimensional binary feature descriptors lack well-defined centroids. In this paper, we propose a realtime LCD approach called MILD (Multi-Index Hashing for Loop closure Detection), in which image similarity is measured by feature matching directly to achieve high recall without introducing extra computational complexity with the aid of Multi-Index Hashing (MIH). A theoretical analysis of the approximate image similarity measurement using MIH is presented, which reveals the trade-off between efficiency and accuracy from a probabilistic perspective. Extensive comparisons with state-of-the-art LCD methods demonstrate the superiority of MILD in both efficiency and accuracy.



### ShaResNet: reducing residual network parameter number by sharing weights
- **Arxiv ID**: http://arxiv.org/abs/1702.08782v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T05, 68T45, C.1.3; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1702.08782v2)
- **Published**: 2017-02-28 13:37:59+00:00
- **Updated**: 2017-03-06 13:49:15+00:00
- **Authors**: Alexandre Boulch
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Residual Networks have reached the state of the art in many image processing tasks such image classification. However, the cost for a gain in accuracy in terms of depth and memory is prohibitive as it requires a higher number of residual blocks, up to double the initial value. To tackle this problem, we propose in this paper a way to reduce the redundant information of the networks. We share the weights of convolutional layers between residual blocks operating at the same spatial scale. The signal flows multiple times in the same convolutional layer. The resulting architecture, called ShaResNet, contains block specific layers and shared layers. These ShaResNet are trained exactly in the same fashion as the commonly used residual networks. We show, on the one hand, that they are almost as efficient as their sequential counterparts while involving less parameters, and on the other hand that they are more efficient than a residual network with the same number of parameters. For example, a 152-layer-deep residual network can be reduced to 106 convolutional layers, i.e. a parameter gain of 39\%, while loosing less than 0.2\% accuracy on ImageNet.



### Unsupervised Triplet Hashing for Fast Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1702.08798v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08798v1)
- **Published**: 2017-02-28 14:26:14+00:00
- **Updated**: 2017-02-28 14:26:14+00:00
- **Authors**: Shanshan Huang, Yichao Xiong, Ya Zhang, Jia Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing has played a pivotal role in large-scale image retrieval. With the development of Convolutional Neural Network (CNN), hashing learning has shown great promise. But existing methods are mostly tuned for classification, which are not optimized for retrieval tasks, especially for instance-level retrieval. In this study, we propose a novel hashing method for large-scale image retrieval. Considering the difficulty in obtaining labeled datasets for image retrieval task in large scale, we propose a novel CNN-based unsupervised hashing method, namely Unsupervised Triplet Hashing (UTH). The unsupervised hashing network is designed under the following three principles: 1) more discriminative representations for image retrieval; 2) minimum quantization loss between the original real-valued feature descriptors and the learned hash codes; 3) maximum information entropy for the learned hash codes. Extensive experiments on CIFAR-10, MNIST and In-shop datasets have shown that UTH outperforms several state-of-the-art unsupervised hashing methods in terms of retrieval accuracy.



### Predicting Slice-to-Volume Transformation in Presence of Arbitrary Subject Motion
- **Arxiv ID**: http://arxiv.org/abs/1702.08891v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08891v2)
- **Published**: 2017-02-28 18:06:14+00:00
- **Updated**: 2017-03-04 12:32:52+00:00
- **Authors**: Benjamin Hou, Amir Alansary, Steven McDonagh, Alice Davidson, Mary Rutherford, Jo V. Hajnal, Daniel Rueckert, Ben Glocker, Bernhard Kainz
- **Comment**: 8 pages, 4 figures, 6 pages supplemental material, currently under
  review for MICCAI 2017
- **Journal**: None
- **Summary**: This paper aims to solve a fundamental problem in intensity-based 2D/3D registration, which concerns the limited capture range and need for very good initialization of state-of-the-art image registration methods. We propose a regression approach that learns to predict rotation and translations of arbitrary 2D image slices from 3D volumes, with respect to a learned canonical atlas co-ordinate system. To this end, we utilize Convolutional Neural Networks (CNNs) to learn the highly complex regression function that maps 2D image slices into their correct position and orientation in 3D space. Our approach is attractive in challenging imaging scenarios, where significant subject motion complicates reconstruction performance of 3D volumes from 2D slice data. We extensively evaluate the effectiveness of our approach quantitatively on simulated MRI brain data with extreme random motion. We further demonstrate qualitative results on fetal MRI where our method is integrated into a full reconstruction and motion compensation pipeline. With our CNN regression approach we obtain an average prediction error of 7mm on simulated data, and convincing reconstruction quality of images of very young fetuses where previous methods fail. We further discuss applications to Computed Tomography and X-ray projections. Our approach is a general solution to the 2D/3D initialization problem. It is computationally efficient, with prediction times per slice of a few milliseconds, making it suitable for real-time scenarios.



### Context-Sensitive Super-Resolution for Fast Fetal Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/1703.00035v3
- **DOI**: 10.1007/978-3-319-67564-0_12
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00035v3)
- **Published**: 2017-02-28 19:36:34+00:00
- **Updated**: 2017-09-23 10:21:06+00:00
- **Authors**: Steven McDonagh, Benjamin Hou, Konstantinos Kamnitsas, Ozan Oktay, Amir Alansary, Mary Rutherford, Jo V. Hajnal, Bernhard Kainz
- **Comment**: 11 pages, 6 figures, published in Proc MICCAI RAMBO'17
  https://link.springer.com/chapter/10.1007/978-3-319-67564-0_12
- **Journal**: Springer LNCS 10555 2017
- **Summary**: 3D Magnetic Resonance Imaging (MRI) is often a trade-off between fast but low-resolution image acquisition and highly detailed but slow image acquisition. Fast imaging is required for targets that move to avoid motion artefacts. This is in particular difficult for fetal MRI. Spatially independent upsampling techniques, which are the state-of-the-art to address this problem, are error prone and disregard contextual information. In this paper we propose a context-sensitive upsampling method based on a residual convolutional neural network model that learns organ specific appearance and adopts semantically to input data allowing for the generation of high resolution images with sharp edges and fine scale detail. By making contextual decisions about appearance and shape, present in different parts of an image, we gain a maximum of structural detail at a similar contrast as provided by high-resolution data. We experiment on $145$ fetal scans and show that our approach yields an increased PSNR of $1.25$ $dB$ when applied to under-sampled fetal data \emph{cf.} baseline upsampling. Furthermore, our method yields an increased PSNR of $1.73$ $dB$ when utilizing under-sampled fetal data to perform brain volume reconstruction on motion corrupted captured data.



### Deep Image Harmonization
- **Arxiv ID**: http://arxiv.org/abs/1703.00069v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00069v1)
- **Published**: 2017-02-28 21:58:45+00:00
- **Updated**: 2017-02-28 21:58:45+00:00
- **Authors**: Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Compositing is one of the most common operations in photo editing. To generate realistic composites, the appearances of foreground and background need to be adjusted to make them compatible. Previous approaches to harmonize composites have focused on learning statistical relationships between hand-crafted appearance features of the foreground and background, which is unreliable especially when the contents in the two layers are vastly different. In this work, we propose an end-to-end deep convolutional neural network for image harmonization, which can capture both the context and semantic information of the composite images during harmonization. We also introduce an efficient way to collect large-scale and high-quality training data that can facilitate the training process. Experiments on the synthesized dataset and real composite images show that the proposed network outperforms previous state-of-the-art methods.



### Discrete Wavelet Transform Based Algorithm for Recognition of QRS Complexes
- **Arxiv ID**: http://arxiv.org/abs/1703.00075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00075v1)
- **Published**: 2017-02-28 22:12:00+00:00
- **Updated**: 2017-02-28 22:12:00+00:00
- **Authors**: Rachid Haddadi, Elhassane Abdelmounim, Mustapha El Hanine, Abdelaziz Belaguid
- **Comment**: None
- **Journal**: World of Computer Science and Information Technology Journal
  (WCSIT) ; ISSN: 2221-0741; Vol. 4, No. 9, 127-132, 2014
- **Summary**: This paper proposes the application of Discrete Wavelet Transform (DWT) to detect the QRS (ECG is characterized by a recurrent wave sequence of P, QRS and T-wave) of an electrocardiogram (ECG) signal. Wavelet Transform provides localization in both time and frequency. In preprocessing stage, DWT is used to remove the baseline wander in the ECG signal. The performance of the algorithm of QRS detection is evaluated against the standard MIT BIH (Massachusetts Institute of Technology, Beth Israel Hospital) Arrhythmia database. The average QRS complexes detection rate of 98.1 % is achieved.



### Supervised Saliency Map Driven Segmentation of the Lesions in Dermoscopic Images
- **Arxiv ID**: http://arxiv.org/abs/1703.00087v4
- **DOI**: 10.1109/JBHI.2018.2839647
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00087v4)
- **Published**: 2017-02-28 23:27:25+00:00
- **Updated**: 2018-06-07 06:47:19+00:00
- **Authors**: Mostafa Jahanifar, Neda Zamani Tajeddin, Babak Mohammadzadeh Asl, Ali Gooya
- **Comment**: ISIC2017, JBHI
- **Journal**: None
- **Summary**: Lesion segmentation is the first step in most automatic melanoma recognition systems. Deficiencies and difficulties in dermoscopic images such as color inconstancy, hair occlusion, dark corners and color charts make lesion segmentation an intricate task. In order to detect the lesion in the presence of these problems, we propose a supervised saliency detection method tailored for dermoscopic images based on the discriminative regional feature integration (DRFI). DRFI method incorporates multi-level segmentation, regional contrast, property, background descriptors, and a random forest regressor to create saliency scores for each region in the image. In our improved saliency detection method, mDRFI, we have added some new features to regional property descriptors. Also, in order to achieve more robust regional background descriptors, a thresholding algorithm is proposed to obtain a new pseudo-background region. Findings reveal that mDRFI is superior to DRFI in detecting the lesion as the salient object in dermoscopic images. The proposed overall lesion segmentation framework uses detected saliency map to construct an initial mask of the lesion through thresholding and post-processing operations. The initial mask is then evolving in a level set framework to fit better on the lesion's boundaries. The results of evaluation tests on three public datasets show that our proposed segmentation method outperforms the other conventional state-of-the-art segmentation algorithms and its performance is comparable with most recent approaches that are based on deep convolutional neural networks.



