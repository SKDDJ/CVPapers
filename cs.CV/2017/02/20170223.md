# Arxiv Papers in cs.CV on 2017-02-23
### Learning Chained Deep Features and Classifiers for Cascade in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1702.07054v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07054v1)
- **Published**: 2017-02-23 00:40:29+00:00
- **Updated**: 2017-02-23 00:40:29+00:00
- **Authors**: Wanli Ouyang, Ku Wang, Xin Zhu, Xiaogang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Cascade is a widely used approach that rejects obvious negative samples at early stages for learning better classifier and faster inference. This paper presents chained cascade network (CC-Net). In this CC-Net, the cascaded classifier at a stage is aided by the classification scores in previous stages. Feature chaining is further proposed so that the feature learning for the current cascade stage uses the features in previous stages as the prior information. The chained ConvNet features and classifiers of multiple stages are jointly learned in an end-to-end network. In this way, features and classifiers at latter stages handle more difficult samples with the help of features and classifiers in previous stages. It yields consistent boost in detection performance on benchmarks like PASCAL VOC 2007 and ImageNet. Combined with better region proposal, CC-Net leads to state-of-the-art result of 81.1% mAP on PASCAL VOC 2007.



### Robust and fully automated segmentation of mandible from CT scans
- **Arxiv ID**: http://arxiv.org/abs/1702.07059v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07059v1)
- **Published**: 2017-02-23 01:23:45+00:00
- **Updated**: 2017-02-23 01:23:45+00:00
- **Authors**: Neslisah Torosdagli, Denise K. Liberton, Payal Verma, Murat Sincan Janice Lee, Sumanta Pattanaik, Ulas Bagci
- **Comment**: 4 pages, 5 figures, IEEE International Symposium on Biomedical
  Imaging (ISBI) 2017
- **Journal**: None
- **Summary**: Mandible bone segmentation from computed tomography (CT) scans is challenging due to mandible's structural irregularities, complex shape patterns, and lack of contrast in joints. Furthermore, connections of teeth to mandible and mandible to remaining parts of the skull make it extremely difficult to identify mandible boundary automatically. This study addresses these challenges by proposing a novel framework where we define the segmentation as two complementary tasks: recognition and delineation. For recognition, we use random forest regression to localize mandible in 3D. For delineation, we propose to use 3D gradient-based fuzzy connectedness (FC) image segmentation algorithm, operating on the recognized mandible sub-volume. Despite heavy CT artifacts and dental fillings, consisting half of the CT image data in our experiments, we have achieved highly accurate detection and delineation results. Specifically, detection accuracy more than 96% (measured by union of intersection (UoI)), the delineation accuracy of 91% (measured by dice similarity coefficient), and less than 1 mm in shape mismatch (Hausdorff Distance) were found.



### A Computational Model of a Single-Photon Avalanche Diode Sensor for Transient Imaging
- **Arxiv ID**: http://arxiv.org/abs/1703.02635v1
- **DOI**: None
- **Categories**: **physics.ins-det**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1703.02635v1)
- **Published**: 2017-02-23 10:17:34+00:00
- **Updated**: 2017-02-23 10:17:34+00:00
- **Authors**: Quercus Hernandez, Diego Gutierrez, Adrian Jarabo
- **Comment**: 7 pages, 11 figures
- **Journal**: None
- **Summary**: Single-Photon Avalanche Diodes (SPAD) are affordable photodetectors, capable to collect extremely fast low-energy events, due to their single-photon sensibility. This makes them very suitable for time-of-flight-based range imaging systems, allowing to reduce costs and power requirements, without sacrifizing much temporal resolution. In this work we describe a computational model to simulate the behaviour of SPAD sensors, aiming to provide a realistic camera model for time-resolved light transport simulation, with applications on prototyping new reconstructions techniques based on SPAD time-of-flight data. Our model accounts for the major effects of the sensor on the incoming signal. We compare our model against real-world measurements, and apply it to a variety of scenarios, including complex multiply-scattered light transport.



### Analyzing Learned Convnet Features with Dirichlet Process Gaussian Mixture Models
- **Arxiv ID**: http://arxiv.org/abs/1702.07189v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07189v1)
- **Published**: 2017-02-23 12:11:42+00:00
- **Updated**: 2017-02-23 12:11:42+00:00
- **Authors**: David Malmgren-Hansen, Allan Aasbjerg Nielsen, Rasmus Engholm
- **Comment**: Presented at NIPS 2016 Workshop: Practical Bayesian Nonparametrics
- **Journal**: None
- **Summary**: Convolutional Neural Networks (Convnets) have achieved good results in a range of computer vision tasks the recent years. Though given a lot of attention, visualizing the learned representations to interpret Convnets, still remains a challenging task. The high dimensionality of internal representations and the high abstractions of deep layers are the main challenges when visualizing Convnet functionality. We present in this paper a technique based on clustering internal Convnet representations with a Dirichlet Process Gaussian Mixture Model, for visualization of learned representations in Convnets. Our method copes with the high dimensionality of a Convnet by clustering representations across all nodes of each layer. We will discuss how this application is useful when considering transfer learning, i.e.\ transferring a model trained on one dataset to solve a task on a different one.



### ViP-CNN: Visual Phrase Guided Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1702.07191v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07191v2)
- **Published**: 2017-02-23 12:28:18+00:00
- **Updated**: 2017-04-10 16:45:53+00:00
- **Authors**: Yikang Li, Wanli Ouyang, Xiaogang Wang, Xiao'ou Tang
- **Comment**: 10 pages, 5 figures, accepted by CVPR 2017
- **Journal**: None
- **Summary**: As the intermediate level task connecting image captioning and object detection, visual relationship detection started to catch researchers' attention because of its descriptive power and clear structure. It detects the objects and captures their pair-wise interactions with a subject-predicate-object triplet, e.g. person-ride-horse. In this paper, each visual relationship is considered as a phrase with three components. We formulate the visual relationship detection as three inter-connected recognition problems and propose a Visual Phrase guided Convolutional Neural Network (ViP-CNN) to address them simultaneously. In ViP-CNN, we present a Phrase-guided Message Passing Structure (PMPS) to establish the connection among relationship components and help the model consider the three problems jointly. Corresponding non-maximum suppression method and model training strategy are also proposed. Experimental results show that our ViP-CNN outperforms the state-of-art method both in speed and accuracy. We further pretrain ViP-CNN on our cleansed Visual Genome Relationship dataset, which is found to perform better than the pretraining on the ImageNet for this task.



### k-Means Clustering and Ensemble of Regressions: An Algorithm for the ISIC 2017 Skin Lesion Segmentation Challenge
- **Arxiv ID**: http://arxiv.org/abs/1702.07333v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07333v1)
- **Published**: 2017-02-23 18:43:30+00:00
- **Updated**: 2017-02-23 18:43:30+00:00
- **Authors**: David Alvarez, Monica Iglesias
- **Comment**: Abstract submitted to arXiv as prerequisite to participate in the
  ISIC2017 Skin Lesion Segmentation Challenge
- **Journal**: None
- **Summary**: This abstract briefly describes a segmentation algorithm developed for the ISIC 2017 Skin Lesion Detection Competition hosted at [ref]. The objective of the competition is to perform a segmentation (in the form of a binary mask image) of skin lesions in dermoscopic images as close as possible to a segmentation performed by trained clinicians, which is taken as ground truth. This project only takes part in the segmentation phase of the challenge. The other phases of the competition (feature extraction and lesion identification) are not considered.   The proposed algorithm consists of 4 steps: (1) lesion image preprocessing, (2) image segmentation using k-means clustering of pixel colors, (3) calculation of a set of features describing the properties of each segmented region, and (4) calculation of a final score for each region, representing the likelihood of corresponding to a suitable lesion segmentation. The scores in step (4) are obtained by averaging the results of 2 different regression models using the scores of each region as input. Before using the algorithm these regression models must be trained using the training set of images and ground truth masks provided by the Competition. Steps 2 to 4 are repeated with an increasing number of clusters (and therefore the image is segmented into more regions) until there is no further improvement of the calculated scores.



### Feasibility of Principal Component Analysis in hand gesture recognition system
- **Arxiv ID**: http://arxiv.org/abs/1702.07371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07371v1)
- **Published**: 2017-02-23 19:34:25+00:00
- **Updated**: 2017-02-23 19:34:25+00:00
- **Authors**: Tanu Srivastava, Raj Shree Singh, Sunil Kumar, Pavan Chakraborty
- **Comment**: conference
- **Journal**: None
- **Summary**: Nowadays actions are increasingly being handled in electronic ways, instead of physical interaction. From earlier times biometrics is used in the authentication of a person. It recognizes a person by using a human trait associated with it like eyes (by calculating the distance between the eyes) and using hand gestures, fingerprint detection, face detection etc. Advantages of using these traits for identification are that they uniquely identify a person and cannot be forgotten or lost. These are unique features of a human being which are being used widely to make the human life simpler. Hand gesture recognition system is a powerful tool that supports efficient interaction between the user and the computer. The main moto of hand gesture recognition research is to create a system which can recognise specific hand gestures and use them to convey useful information for device control. This paper presents an experimental study over the feasibility of principal component analysis in hand gesture recognition system. PCA is a powerful tool for analyzing data. The primary goal of PCA is dimensionality reduction. Frames are extracted from the Sheffield KInect Gesture (SKIG) dataset. The implementation is done by creating a training set and then training the recognizer. It uses Eigen space by processing the eigenvalues and eigenvectors of the images in training set. Euclidean distance with the threshold value is used as similarity metric to recognize the gestures. The experimental results show that PCA is feasible to be used for hand gesture recognition system.



### Toward Streaming Synapse Detection with Compositional ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1702.07386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07386v1)
- **Published**: 2017-02-23 20:48:13+00:00
- **Updated**: 2017-02-23 20:48:13+00:00
- **Authors**: Shibani Santurkar, David Budden, Alexander Matveev, Heather Berlin, Hayk Saribekyan, Yaron Meirovitch, Nir Shavit
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Connectomics is an emerging field in neuroscience that aims to reconstruct the 3-dimensional morphology of neurons from electron microscopy (EM) images. Recent studies have successfully demonstrated the use of convolutional neural networks (ConvNets) for segmenting cell membranes to individuate neurons. However, there has been comparatively little success in high-throughput identification of the intercellular synaptic connections required for deriving connectivity graphs.   In this study, we take a compositional approach to segmenting synapses, modeling them explicitly as an intercellular cleft co-located with an asymmetric vesicle density along a cell membrane. Instead of requiring a deep network to learn all natural combinations of this compositionality, we train lighter networks to model the simpler marginal distributions of membranes, clefts and vesicles from just 100 electron microscopy samples. These feature maps are then combined with simple rules-based heuristics derived from prior biological knowledge.   Our approach to synapse detection is both more accurate than previous state-of-the-art (7% higher recall and 5% higher F1-score) and yields a 20-fold speed-up compared to the previous fastest implementations. We demonstrate by reconstructing the first complete, directed connectome from the largest available anisotropic microscopy dataset (245 GB) of mouse somatosensory cortex (S1) in just 9.7 hours on a single shared-memory CPU system. We believe that this work marks an important step toward the goal of a microscope-pace streaming connectomics pipeline.



### Continuous-Time Visual-Inertial Odometry for Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/1702.07389v2
- **DOI**: 10.1109/TRO.2018.2858287
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.07389v2)
- **Published**: 2017-02-23 20:56:09+00:00
- **Updated**: 2018-06-10 17:42:56+00:00
- **Authors**: Elias Mueggler, Guillermo Gallego, Henri Rebecq, Davide Scaramuzza
- **Comment**: 15 pages, 12 figures, 6 tables, IEEE Transactions on Robotics, 2018
- **Journal**: IEEE Transactions on Robotics, Vol. 34, No. 6, pp.1425-1440, Dec.
  2018
- **Summary**: Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, due to the fundamentally different structure of the sensor's output, new algorithms that exploit the high temporal resolution and the asynchronous nature of the sensor are required. Recent work has shown that a continuous-time representation of the event camera pose can deal with the high temporal resolution and asynchronous nature of this sensor in a principled way. In this paper, we leverage such a continuous-time representation to perform visual-inertial odometry with an event camera. This representation allows direct integration of the asynchronous events with micro-second accuracy and the inertial measurements at high frequency. The event camera trajectory is approximated by a smooth curve in the space of rigid-body motions using cubic splines. This formulation significantly reduces the number of variables in trajectory estimation problems. We evaluate our method on real data from several scenes and compare the results against ground truth from a motion-capture system. We show that our method provides improved accuracy over the result of a state-of-the-art visual odometry method for event cameras. We also show that both the map orientation and scale can be recovered accurately by fusing events and inertial data. To the best of our knowledge, this is the first work on visual-inertial fusion with event cameras using a continuous-time framework.



### WaterGAN: Unsupervised Generative Network to Enable Real-time Color Correction of Monocular Underwater Images
- **Arxiv ID**: http://arxiv.org/abs/1702.07392v3
- **DOI**: 10.1109/LRA.2017.2730363
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1702.07392v3)
- **Published**: 2017-02-23 21:06:51+00:00
- **Updated**: 2017-10-26 14:46:14+00:00
- **Authors**: Jie Li, Katherine A. Skinner, Ryan M. Eustice, Matthew Johnson-Roberson
- **Comment**: 8 pages, 16 figures, published by RA-letter 2018. Source code
  available at: https://github.com/kskin/WaterGAN
- **Journal**: IEEE Robotics and Automation Letters IEEE Robotics and Automation
  Letters IEEE Robotics and Automation Letters 387 - 394 (2018)
- **Summary**: This paper reports on WaterGAN, a generative adversarial network (GAN) for generating realistic underwater images from in-air image and depth pairings in an unsupervised pipeline used for color correction of monocular underwater images. Cameras onboard autonomous and remotely operated vehicles can capture high resolution images to map the seafloor, however, underwater image formation is subject to the complex process of light propagation through the water column. The raw images retrieved are characteristically different than images taken in air due to effects such as absorption and scattering, which cause attenuation of light at different rates for different wavelengths. While this physical process is well described theoretically, the model depends on many parameters intrinsic to the water column as well as the objects in the scene. These factors make recovery of these parameters difficult without simplifying assumptions or field calibration, hence, restoration of underwater images is a non-trivial problem. Deep learning has demonstrated great success in modeling complex nonlinear systems but requires a large amount of training data, which is difficult to compile in deep sea environments. Using WaterGAN, we generate a large training dataset of paired imagery, both raw underwater and true color in-air, as well as depth data. This data serves as input to a novel end-to-end network for color correction of monocular underwater images. Due to the depth-dependent water column effects inherent to underwater environments, we show that our end-to-end network implicitly learns a coarse depth estimate of the underwater scene from monocular underwater images. Our proposed pipeline is validated with testing on real data collected from both a pure water tank and from underwater surveys in field testing. Source code is made publicly available with sample datasets and pretrained models.



### Improving high-pass fusion method using wavelets
- **Arxiv ID**: http://arxiv.org/abs/1702.07343v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07343v1)
- **Published**: 2017-02-23 21:09:46+00:00
- **Updated**: 2017-02-23 21:09:46+00:00
- **Authors**: Hamid Reza Shahdoosti
- **Comment**: 7 pages, in Persian
- **Journal**: None
- **Summary**: In an appropriate image fusion method, spatial information of the panchromatic image is injected into the multispectral images such that the spectral information is not distorted. The high-pass modulation method is a successful method in image fusion. However, the main drawback of this method is that this technique uses the boxcar filter to extract the high frequency information of the panchromatic image. Using the boxcar filter introduces the ringing effect into the fused image. To cope with this problem, we use the wavelet transform instead of boxcar filters. Then, the results of the proposed method and those of other methods such as, Brovey, IHS, and PCA ones are compared. Experiments show the superiority of the proposed method in terms of correlation coefficient and mutual information.



### Building Usage Profiles Using Deep Neural Nets
- **Arxiv ID**: http://arxiv.org/abs/1702.07424v1
- **DOI**: 10.1109/ICSE-NIER.2017.12
- **Categories**: **cs.SE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.07424v1)
- **Published**: 2017-02-23 23:39:21+00:00
- **Updated**: 2017-02-23 23:39:21+00:00
- **Authors**: Domenic Curro, Konstantinos G. Derpanis, Andriy V. Miranskyy
- **Comment**: None
- **Journal**: Proceedings of the 39th International Conference on Software
  Engineering: New Ideas and Emerging Results Track (ICSE-NIER '17). IEEE
  Press, Piscataway, NJ, USA, 43-46, 2017
- **Summary**: To improve software quality, one needs to build test scenarios resembling the usage of a software product in the field. This task is rendered challenging when a product's customer base is large and diverse. In this scenario, existing profiling approaches, such as operational profiling, are difficult to apply. In this work, we consider publicly available video tutorials of a product to profile usage. Our goal is to construct an automatic approach to extract information about user actions from instructional videos. To achieve this goal, we use a Deep Convolutional Neural Network (DCNN) to recognize user actions. Our pilot study shows that a DCNN trained to recognize user actions in video can classify five different actions in a collection of 236 publicly available Microsoft Word tutorial videos (published on YouTube). In our empirical evaluation we report a mean average precision of 94.42% across all actions. This study demonstrates the efficacy of DCNN-based methods for extracting software usage information from videos. Moreover, this approach may aid in other software engineering activities that require information about customer usage of a product.



