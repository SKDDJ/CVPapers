# Arxiv Papers in cs.CV on 2017-02-12
### A Novel Weight-Shared Multi-Stage CNN for Scale Robustness
- **Arxiv ID**: http://arxiv.org/abs/1702.03505v3
- **DOI**: 10.1109/TCSVT.2018.2822773
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03505v3)
- **Published**: 2017-02-12 08:29:53+00:00
- **Updated**: 2019-04-12 02:44:54+00:00
- **Authors**: Ryo Takahashi, Takashi Matsubara, Kuniaki Uehara
- **Comment**: accepted version, 13 pages
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology,
  vol. 29, no. 4, 2019, pp. 1090-1101
- **Summary**: Convolutional neural networks (CNNs) have demonstrated remarkable results in image classification for benchmark tasks and practical applications. The CNNs with deeper architectures have achieved even higher performance recently thanks to their robustness to the parallel shift of objects in images as well as their numerous parameters and the resulting high expression ability. However, CNNs have a limited robustness to other geometric transformations such as scaling and rotation. This limits the performance improvement of the deep CNNs, but there is no established solution. This study focuses on scale transformation and proposes a network architecture called the weight-shared multi-stage network (WSMS-Net), which consists of multiple stages of CNNs. The proposed WSMS-Net is easily combined with existing deep CNNs such as ResNet and DenseNet and enables them to acquire robustness to object scaling. Experimental results on the CIFAR-10, CIFAR-100, and ImageNet datasets demonstrate that existing deep CNNs combined with the proposed WSMS-Net achieve higher accuracies for image classification tasks with only a minor increase in the number of parameters and computation time.



### Sparse Representation based Multi-sensor Image Fusion: A Review
- **Arxiv ID**: http://arxiv.org/abs/1702.03515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03515v1)
- **Published**: 2017-02-12 11:43:09+00:00
- **Updated**: 2017-02-12 11:43:09+00:00
- **Authors**: Qiang Zhang, Yi Liu, Rick S. Blum, Jungong Han, Dacheng Tao
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: As a result of several successful applications in computer vision and image processing, sparse representation (SR) has attracted significant attention in multi-sensor image fusion. Unlike the traditional multiscale transforms (MSTs) that presume the basis functions, SR learns an over-complete dictionary from a set of training images for image fusion, and it achieves more stable and meaningful representations of the source images. By doing so, the SR-based fusion methods generally outperform the traditional MST-based image fusion methods in both subjective and objective tests. In addition, they are less susceptible to mis-registration among the source images, thus facilitating the practical applications. This survey paper proposes a systematic review of the SR-based multi-sensor image fusion literature, highlighting the pros and cons of each category of approaches. Specifically, we start by performing a theoretical investigation of the entire system from three key algorithmic aspects, (1) sparse representation models; (2) dictionary learning methods; and (3) activity levels and fusion rules. Subsequently, we show how the existing works address these scientific problems and design the appropriate fusion rules for each application, such as multi-focus image fusion and multi-modality (e.g., infrared and visible) image fusion. At last, we carry out some experiments to evaluate the impact of these three algorithmic components on the fusion performance when dealing with different applications. This article is expected to serve as a tutorial and source of reference for researchers preparing to enter the field or who desire to employ the sparse representation theory in other fields.



