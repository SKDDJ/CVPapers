# Arxiv Papers in cs.CV on 2017-02-24
### Multi-Context Attention for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1702.07432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07432v1)
- **Published**: 2017-02-24 01:10:53+00:00
- **Updated**: 2017-02-24 01:10:53+00:00
- **Authors**: Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L. Yuille, Xiaogang Wang
- **Comment**: The first two authors contribute equally to this work
- **Journal**: None
- **Summary**: In this paper, we propose to incorporate convolutional neural networks with a multi-context attention mechanism into an end-to-end framework for human pose estimation. We adopt stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. The Conditional Random Field (CRF) is utilized to model the correlations among neighboring regions in the attention map. We further combine the holistic attention model, which focuses on the global consistency of the full human body, and the body part attention model, which focuses on the detailed description for different body parts. Hence our model has the ability to focus on different granularity from local salient regions to global semantic-consistent spaces. Additionally, we design novel Hourglass Residual Units (HRUs) to increase the receptive field of the network. These units are extensions of residual units with a side branch incorporating filters with larger receptive fields, hence features with various scales are learned and combined within the HRUs. The effectiveness of the proposed multi-context attention mechanism and the hourglass residual units is evaluated on two widely used human pose estimation benchmarks. Our approach outperforms all existing methods on both benchmarks over all the body parts.



### Viewpoint Adaptation for Rigid Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1702.07451v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07451v1)
- **Published**: 2017-02-24 02:37:15+00:00
- **Updated**: 2017-02-24 02:37:15+00:00
- **Authors**: Patrick Wang, Kenneth Morton, Peter Torrione, Leslie Collins
- **Comment**: None
- **Journal**: None
- **Summary**: An object detector performs suboptimally when applied to image data taken from a viewpoint different from the one with which it was trained. In this paper, we present a viewpoint adaptation algorithm that allows a trained single-view object detector to be adapted to a new, distinct viewpoint. We first illustrate how a feature space transformation can be inferred from a known homography between the source and target viewpoints. Second, we show that a variety of trained classifiers can be modified to behave as if that transformation were applied to each testing instance. The proposed algorithm is evaluated on a person detection task using images from the PETS 2007 and CAVIAR datasets, as well as from a new synthetic multi-view person detection dataset. It yields substantial performance improvements when adapting single-view person detectors to new viewpoints, and simultaneously reduces computational complexity. This work has the potential to improve detection performance for cameras viewing objects from arbitrary viewpoints, while simplifying data collection and feature extraction.



### Learning Non-local Image Diffusion for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1702.07472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07472v1)
- **Published**: 2017-02-24 06:12:55+00:00
- **Updated**: 2017-02-24 06:12:55+00:00
- **Authors**: Peng Qiao, Yong Dou, Wensen Feng, Yunjin Chen
- **Comment**: under review in a journal
- **Journal**: None
- **Summary**: Image diffusion plays a fundamental role for the task of image denoising. Recently proposed trainable nonlinear reaction diffusion (TNRD) model defines a simple but very effective framework for image denoising. However, as the TNRD model is a local model, the diffusion behavior of which is purely controlled by information of local patches, it is prone to create artifacts in the homogenous regions and over-smooth highly textured regions, especially in the case of strong noise levels. Meanwhile, it is widely known that the non-local self-similarity (NSS) prior stands as an effective image prior for image denoising, which has been widely exploited in many non-local methods. In this work, we are highly motivated to embed the NSS prior into the TNRD model to tackle its weaknesses. In order to preserve the expected property that end-to-end training is available, we exploit the NSS prior by a set of non-local filters, and derive our proposed trainable non-local reaction diffusion (TNLRD) model for image denoising. Together with the local filters and influence functions, the non-local filters are learned by employing loss-specific training. The experimental results show that the trained TNLRD model produces visually plausible recovered images with more textures and less artifacts, compared to its local versions. Moreover, the trained TNLRD model can achieve strongly competitive performance to recent state-of-the-art image denoising methods in terms of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM).



### Simultaneous Feature and Body-Part Learning for Real-Time Robot Awareness of Human Behaviors
- **Arxiv ID**: http://arxiv.org/abs/1702.07474v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1702.07474v1)
- **Published**: 2017-02-24 06:35:10+00:00
- **Updated**: 2017-02-24 06:35:10+00:00
- **Authors**: Fei Han, Xue Yang, Christopher Reardon, Yu Zhang, Hao Zhang
- **Comment**: 8 pages, 6 figures, accepted by ICRA'17
- **Journal**: None
- **Summary**: Robot awareness of human actions is an essential research problem in robotics with many important real-world applications, including human-robot collaboration and teaming. Over the past few years, depth sensors have become a standard device widely used by intelligent robots for 3D perception, which can also offer human skeletal data in 3D space. Several methods based on skeletal data were designed to enable robot awareness of human actions with satisfactory accuracy. However, previous methods treated all body parts and features equally important, without the capability to identify discriminative body parts and features. In this paper, we propose a novel simultaneous Feature And Body-part Learning (FABL) approach that simultaneously identifies discriminative body parts and features, and efficiently integrates all available information together to enable real-time robot awareness of human behaviors. We formulate FABL as a regression-like optimization problem with structured sparsity-inducing norms to model interrelationships of body parts and features. We also develop an optimization algorithm to solve the formulated problem, which possesses a theoretical guarantee to find the optimal solution. To evaluate FABL, three experiments were performed using public benchmark datasets, including the MSR Action3D and CAD-60 datasets, as well as a Baxter robot in practical assistive living applications. Experimental results show that our FABL approach obtains a high recognition accuracy with a processing speed of the order-of-magnitude of 10e4 Hz, which makes FABL a promising method to enable real-time robot awareness of human behaviors in practical robotics applications.



### Sequence-based Multimodal Apprenticeship Learning For Robot Perception and Decision Making
- **Arxiv ID**: http://arxiv.org/abs/1702.07475v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.07475v1)
- **Published**: 2017-02-24 06:37:06+00:00
- **Updated**: 2017-02-24 06:37:06+00:00
- **Authors**: Fei Han, Xue Yang, Yu Zhang, Hao Zhang
- **Comment**: 8 pages, 6 figures, accepted by ICRA'17
- **Journal**: None
- **Summary**: Apprenticeship learning has recently attracted a wide attention due to its capability of allowing robots to learn physical tasks directly from demonstrations provided by human experts. Most previous techniques assumed that the state space is known a priori or employed simple state representations that usually suffer from perceptual aliasing. Different from previous research, we propose a novel approach named Sequence-based Multimodal Apprenticeship Learning (SMAL), which is capable to simultaneously fusing temporal information and multimodal data, and to integrate robot perception with decision making. To evaluate the SMAL approach, experiments are performed using both simulations and real-world robots in the challenging search and rescue scenarios. The empirical study has validated that our SMAL approach can effectively learn plans for robots to make decisions using sequence of multimodal observations. Experimental results have also showed that SMAL outperforms the baseline methods using individual images.



### Speckle Reduction with Trained Nonlinear Diffusion Filtering
- **Arxiv ID**: http://arxiv.org/abs/1702.07482v1
- **DOI**: 10.1007/s10851-016-0697-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07482v1)
- **Published**: 2017-02-24 07:34:31+00:00
- **Updated**: 2017-02-24 07:34:31+00:00
- **Authors**: Wensen Feng, Yunjin Chen
- **Comment**: to appear in Journal of Mathematical Imaging and Vision. Demo codes
  are available from https://1drv.ms/u/s!ApXF85Oq1kvqgcscP8GqUvPE-dF7ig
- **Journal**: None
- **Summary**: Speckle reduction is a prerequisite for many image processing tasks in synthetic aperture radar (SAR) images, as well as all coherent images. In recent years, predominant state-of-the-art approaches for despeckling are usually based on nonlocal methods which mainly concentrate on achieving utmost image restoration quality, with relatively low computational efficiency. Therefore, in this study we aim to propose an efficient despeckling model with both high computational efficiency and high recovery quality. To this end, we exploit a newly-developed trainable nonlinear reaction diffusion(TNRD) framework which has proven a simple and effective model for various image restoration problems. {In the original TNRD applications, the diffusion network is usually derived based on the direct gradient descent scheme. However, this approach will encounter some problem for the task of multiplicative noise reduction exploited in this study. To solve this problem, we employed a new architecture derived from the proximal gradient descent method.} {Taking into account the speckle noise statistics, the diffusion process for the despeckling task is derived. We then retrain all the model parameters in the presence of speckle noise. Finally, optimized nonlinear diffusion filtering models are obtained, which are specialized for despeckling with various noise levels. Experimental results substantiate that the trained filtering models provide comparable or even better results than state-of-the-art nonlocal approaches. Meanwhile, our proposed model merely contains convolution of linear filters with an image, which offers high level parallelism on GPUs. As a consequence, for images of size $512 \times 512$, our GPU implementation takes less than 0.1 seconds to produce state-of-the-art despeckling performance.}



### Deep representation learning for human motion prediction and classification
- **Arxiv ID**: http://arxiv.org/abs/1702.07486v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07486v2)
- **Published**: 2017-02-24 07:51:29+00:00
- **Updated**: 2017-04-13 07:06:34+00:00
- **Authors**: Judith Bütepage, Michael Black, Danica Kragic, Hedvig Kjellström
- **Comment**: This paper is published at the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR), 2017
- **Journal**: None
- **Summary**: Generative models of 3D human motion are often restricted to a small number of activities and can therefore not generalize well to novel movements or applications. In this work we propose a deep learning framework for human motion capture data that learns a generic representation from a large corpus of motion capture data and generalizes well to new, unseen, motions. Using an encoding-decoding network that learns to predict future 3D poses from the most recent past, we extract a feature representation of human motion. Most work on deep learning for sequence prediction focuses on video and speech. Since skeletal data has a different structure, we present and evaluate different network architectures that make different assumptions about time dependencies and limb correlations. To quantify the learned features, we use the output of different layers for action classification and visualize the receptive fields of the network units. Our method outperforms the recent state of the art in skeletal motion prediction even though these use action specific training data. Our results show that deep feedforward networks, trained from a generic mocap database, can successfully be used for feature extraction from human motion data and that this representation can be used as a foundation for classification and prediction.



### Robot gains Social Intelligence through Multimodal Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1702.07492v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1702.07492v1)
- **Published**: 2017-02-24 08:30:43+00:00
- **Updated**: 2017-02-24 08:30:43+00:00
- **Authors**: Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa, Hiroshi Ishiguro
- **Comment**: The paper is published in IEEE-RAS International Conference on
  Humanoid Robots (Humanoids) 2016
- **Journal**: None
- **Summary**: For robots to coexist with humans in a social world like ours, it is crucial that they possess human-like social interaction skills. Programming a robot to possess such skills is a challenging task. In this paper, we propose a Multimodal Deep Q-Network (MDQN) to enable a robot to learn human-like interaction skills through a trial and error method. This paper aims to develop a robot that gathers data during its interaction with a human and learns human interaction behaviour from the high-dimensional sensory information using end-to-end reinforcement learning. This paper demonstrates that the robot was able to learn basic interaction skills successfully, after 14 days of interacting with people.



### Toward high-performance online HCCR: a CNN approach with DropDistortion, path signature and spatial stochastic max-pooling
- **Arxiv ID**: http://arxiv.org/abs/1702.07508v1
- **DOI**: 10.1016/j.patrec.2017.02.011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07508v1)
- **Published**: 2017-02-24 09:26:15+00:00
- **Updated**: 2017-02-24 09:26:15+00:00
- **Authors**: Songxuan Lai, Lianwen Jin, Weixin Yang
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: This paper presents an investigation of several techniques that increase the accuracy of online handwritten Chinese character recognition (HCCR). We propose a new training strategy named DropDistortion to train a deep convolutional neural network (DCNN) with distorted samples. DropDistortion gradually lowers the degree of character distortion during training, which allows the DCNN to better generalize. Path signature is used to extract effective features for online characters. Further improvement is achieved by employing spatial stochastic max-pooling as a method of feature map distortion and model averaging. Experiments were carried out on three publicly available datasets, namely CASIA-OLHWDB 1.0, CASIA-OLHWDB 1.1, and the ICDAR2013 online HCCR competition dataset. The proposed techniques yield state-of-the-art recognition accuracies of 97.67%, 97.30%, and 97.99%, respectively.



### How hard is it to cross the room? -- Training (Recurrent) Neural Networks to steer a UAV
- **Arxiv ID**: http://arxiv.org/abs/1702.07600v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07600v1)
- **Published**: 2017-02-24 14:29:35+00:00
- **Updated**: 2017-02-24 14:29:35+00:00
- **Authors**: Klaas Kelchtermans, Tinne Tuytelaars
- **Comment**: 12 pages, 30 figures
- **Journal**: None
- **Summary**: This work explores the feasibility of steering a drone with a (recurrent) neural network, based on input from a forward looking camera, in the context of a high-level navigation task. We set up a generic framework for training a network to perform navigation tasks based on imitation learning. It can be applied to both aerial and land vehicles. As a proof of concept we apply it to a UAV (Unmanned Aerial Vehicle) in a simulated environment, learning to cross a room containing a number of obstacles. So far only feedforward neural networks (FNNs) have been used to train UAV control. To cope with more complex tasks, we propose the use of recurrent neural networks (RNN) instead and successfully train an LSTM (Long-Short Term Memory) network for controlling UAVs. Vision based control is a sequential prediction problem, known for its highly correlated input data. The correlation makes training a network hard, especially an RNN. To overcome this issue, we investigate an alternative sampling method during training, namely window-wise truncated backpropagation through time (WW-TBPTT). Further, end-to-end training requires a lot of data which often is not available. Therefore, we compare the performance of retraining only the Fully Connected (FC) and LSTM control layers with networks which are trained end-to-end. Performing the relatively simple task of crossing a room already reveals important guidelines and good practices for training neural control networks. Different visualizations help to explain the behavior learned.



### Automatic segmentation of trees in dynamic outdoor environments
- **Arxiv ID**: http://arxiv.org/abs/1702.07611v3
- **DOI**: 10.1016/j.compind.2018.03.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07611v3)
- **Published**: 2017-02-24 14:46:55+00:00
- **Updated**: 2018-04-03 17:13:11+00:00
- **Authors**: Amy Tabb, Henry Medeiros
- **Comment**: 14 pages
- **Journal**: Computers in Industry 98, 90-99. 2018
- **Summary**: Segmentation in dynamic outdoor environments can be difficult when the illumination levels and other aspects of the scene cannot be controlled. Specifically in orchard and vineyard automation contexts, a background material is often used to shield a camera's field of view from other rows of crops. In this paper, we describe a method that uses superpixels to determine low texture regions of the image that correspond to the background material, and then show how this information can be integrated with the color distribution of the image to compute optimal segmentation parameters to segment objects of interest. Quantitative and qualitative experiments demonstrate the suitability of this approach for dynamic outdoor environments, specifically for tree reconstruction and apple flower detection applications.



### Fast and robust curve skeletonization for real-world elongated objects
- **Arxiv ID**: http://arxiv.org/abs/1702.07619v4
- **DOI**: 10.1109/WACV.2018.00214
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1702.07619v4)
- **Published**: 2017-02-24 15:01:22+00:00
- **Updated**: 2018-03-19 14:44:28+00:00
- **Authors**: Amy Tabb, Henry Medeiros
- **Comment**: 47 pages; IEEE WACV 2018, main paper and supplementary material
- **Journal**: None
- **Summary**: We consider the problem of extracting curve skeletons of three-dimensional, elongated objects given a noisy surface, which has applications in agricultural contexts such as extracting the branching structure of plants. We describe an efficient and robust method based on breadth-first search that can determine curve skeletons in these contexts. Our approach is capable of automatically detecting junction points as well as spurious segments and loops. All of that is accomplished with only one user-adjustable parameter. The run time of our method ranges from hundreds of milliseconds to less than four seconds on large, challenging datasets, which makes it appropriate for situations where real-time decision making is needed. Experiments on synthetic models as well as on data from real world objects, some of which were collected in challenging field conditions, show that our approach compares favorably to classical thinning algorithms as well as to recent contributions to the field.



### Inertia-Constrained Pixel-by-Pixel Nonnegative Matrix Factorisation: a Hyperspectral Unmixing Method Dealing with Intra-class Variability
- **Arxiv ID**: http://arxiv.org/abs/1702.07630v1
- **DOI**: 10.3390/rs10111706
- **Categories**: **stat.ME**, cs.CV, physics.data-an, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1702.07630v1)
- **Published**: 2017-02-24 15:43:10+00:00
- **Updated**: 2017-02-24 15:43:10+00:00
- **Authors**: Charlotte Revel, Yannick Deville, Véronique Achard, Xavier Briottet
- **Comment**: None
- **Journal**: Remote Sensing, Vol. 10, Issue 11, 1706, Nov. 2018
- **Summary**: Blind source separation is a common processing tool to analyse the constitution of pixels of hyperspectral images. Such methods usually suppose that pure pixel spectra (endmembers) are the same in all the image for each class of materials. In the framework of remote sensing, such an assumption is no more valid in the presence of intra-class variabilities due to illumination conditions, weathering, slight variations of the pure materials, etc... In this paper, we first describe the results of investigations highlighting intra-class variability measured in real images. Considering these results, a new formulation of the linear mixing model is presented leading to two new methods. Unconstrained Pixel-by-pixel NMF (UP-NMF) is a new blind source separation method based on the assumption of a linear mixing model, which can deal with intra-class variability. To overcome UP-NMF limitations an extended method is proposed, named Inertia-constrained Pixel-by-pixel NMF (IP-NMF). For each sensed spectrum, these extended versions of NMF extract a corresponding set of source spectra. A constraint is set to limit the spreading of each source's estimates in IP-NMF. The methods are tested on a semi-synthetic data set built with spectra extracted from a real hyperspectral image and then numerically mixed. We thus demonstrate the interest of our methods for realistic source variabilities. Finally, IP-NMF is tested on a real data set and it is shown to yield better performance than state of the art methods.



### How ConvNets model Non-linear Transformations
- **Arxiv ID**: http://arxiv.org/abs/1702.07664v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1702.07664v1)
- **Published**: 2017-02-24 17:09:22+00:00
- **Updated**: 2017-02-24 17:09:22+00:00
- **Authors**: Dipan K. Pal, Marios Savvides
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we theoretically address three fundamental problems involving deep convolutional networks regarding invariance, depth and hierarchy. We introduce the paradigm of Transformation Networks (TN) which are a direct generalization of Convolutional Networks (ConvNets). Theoretically, we show that TNs (and thereby ConvNets) are can be invariant to non-linear transformations of the input despite pooling over mere local translations. Our analysis provides clear insights into the increase in invariance with depth in these networks. Deeper networks are able to model much richer classes of transformations. We also find that a hierarchical architecture allows the network to generate invariance much more efficiently than a non-hierarchical network. Our results provide useful insight into these three fundamental problems in deep learning using ConvNets.



### A recommender system to restore images with impulse noise
- **Arxiv ID**: http://arxiv.org/abs/1702.07679v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1702.07679v1)
- **Published**: 2017-02-24 17:36:46+00:00
- **Updated**: 2017-02-24 17:36:46+00:00
- **Authors**: Alfredo Nava-Tudela
- **Comment**: 22 pages, 34 figures
- **Journal**: None
- **Summary**: We build a collaborative filtering recommender system to restore images with impulse noise for which the noisy pixels have been previously identified. We define this recommender system in terms of a new color image representation using three matrices that depend on the noise-free pixels of the image to restore, and two parameters: $k$, the number of features; and $\lambda$, the regularization factor. We perform experiments on a well known image database to test our algorithm and we provide image quality statistics for the results obtained. We discuss the roles of bias and variance in the performance of our algorithm as determined by the values of $k$ and $\lambda$, and provide guidance on how to choose the values of these parameters. Finally, we discuss the possibility of using our collaborative filtering recommender system to perform image inpainting and super-resolution.



### Unifying local and non-local signal processing with graph CNNs
- **Arxiv ID**: http://arxiv.org/abs/1702.07759v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07759v2)
- **Published**: 2017-02-24 20:56:26+00:00
- **Updated**: 2017-07-07 16:04:12+00:00
- **Authors**: Gilles Puy, Srdan Kitic, Patrick Pérez
- **Comment**: None
- **Journal**: None
- **Summary**: This paper deals with the unification of local and non-local signal processing on graphs within a single convolutional neural network (CNN) framework. Building upon recent works on graph CNNs, we propose to use convolutional layers that take as inputs two variables, a signal and a graph, allowing the network to adapt to changes in the graph structure. In this article, we explain how this framework allows us to design a novel method to perform style transfer.



### Video and Accelerometer-Based Motion Analysis for Automated Surgical Skills Assessment
- **Arxiv ID**: http://arxiv.org/abs/1702.07772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07772v1)
- **Published**: 2017-02-24 21:30:31+00:00
- **Updated**: 2017-02-24 21:30:31+00:00
- **Authors**: Aneeq Zia, Yachna Sharma, Vinay Bettadapura, Eric L. Sarin, Irfan Essa
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Basic surgical skills of suturing and knot tying are an essential part of medical training. Having an automated system for surgical skills assessment could help save experts time and improve training efficiency. There have been some recent attempts at automated surgical skills assessment using either video analysis or acceleration data. In this paper, we present a novel approach for automated assessment of OSATS based surgical skills and provide an analysis of different features on multi-modal data (video and accelerometer data). Methods: We conduct the largest study, to the best of our knowledge, for basic surgical skills assessment on a dataset that contained video and accelerometer data for suturing and knot-tying tasks. We introduce "entropy based" features - Approximate Entropy (ApEn) and Cross-Approximate Entropy (XApEn), which quantify the amount of predictability and regularity of fluctuations in time-series data. The proposed features are compared to existing methods of Sequential Motion Texture (SMT), Discrete Cosine Transform (DCT) and Discrete Fourier Transform (DFT), for surgical skills assessment. Results: We report average performance of different features across all applicable OSATS criteria for suturing and knot tying tasks. Our analysis shows that the proposed entropy based features out-perform previous state-of-the-art methods using video data. For accelerometer data, our method performs better for suturing only. We also show that fusion of video and acceleration features can improve overall performance with the proposed entropy features achieving highest accuracy. Conclusions: Automated surgical skills assessment can be achieved with high accuracy using the proposed entropy features. Such a system can significantly improve the efficiency of surgical training in medical schools and teaching hospitals.



