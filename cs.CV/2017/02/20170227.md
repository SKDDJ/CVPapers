# Arxiv Papers in cs.CV on 2017-02-27
### Bioplausible multiscale filtering in retino-cortical processing as a mechanism in perceptual grouping
- **Arxiv ID**: http://arxiv.org/abs/1702.08115v3
- **DOI**: 10.1007/s40708-017-0072-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08115v3)
- **Published**: 2017-02-27 00:38:33+00:00
- **Updated**: 2017-09-13 03:54:03+00:00
- **Authors**: Nasim Nematzadeh, David M. W. Powers, Trent W. Lewis
- **Comment**: 23 pages, 8 figures, Brain Informatics journal: Full text access:
  https://link.springer.com/article/10.1007/s40708-017-0072-8
- **Journal**: None
- **Summary**: Why does our visual system fail to reconstruct reality, when we look at certain patterns? Where do Geometrical illusions start to emerge in the visual pathway? How far should we take computational models of vision with the same visual ability to detect illusions as we do? This study addresses these questions, by focusing on a specific underlying neural mechanism involved in our visual experiences that affects our final perception. Among many types of visual illusion, Geometrical and, in particular, Tilt Illusions are rather important, being characterized by misperception of geometric patterns involving lines and tiles in combination with contrasting orientation, size or position. Over the last decade, many new neurophysiological experiments have led to new insights as to how, when and where retinal processing takes place, and the encoding nature of the retinal representation that is sent to the cortex for further processing. Based on these neurobiological discoveries, we provide computer simulation evidence from modelling retinal ganglion cells responses to some complex Tilt Illusions, suggesting that the emergence of tilt in these illusions is partially related to the interaction of multiscale visual processing performed in the retina. The output of our low-level filtering model is presented for several types of Tilt Illusion, predicting that the final tilt percept arises from multiple-scale processing of the Differences of Gaussians and the perceptual interaction of foreground and background elements. Our results suggest that this model has a high potential in revealing the underlying mechanism connecting low-level filtering approaches to mid- and high-level explanations such as Anchoring theory and Perceptual grouping.



### Multi-scale Image Fusion Between Pre-operative Clinical CT and X-ray Microtomography of Lung Pathology
- **Arxiv ID**: http://arxiv.org/abs/1702.08155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08155v1)
- **Published**: 2017-02-27 06:04:52+00:00
- **Updated**: 2017-02-27 06:04:52+00:00
- **Authors**: Holger R. Roth, Kai Nagara, Hirohisa Oda, Masahiro Oda, Tomoshi Sugiyama, Shota Nakamura, Kensaku Mori
- **Comment**: In proceedings of International Forum on Medical Imaging, IFMIA 2017,
  Okinawa, Japan
- **Journal**: None
- **Summary**: Computational anatomy allows the quantitative analysis of organs in medical images. However, most analysis is constrained to the millimeter scale because of the limited resolution of clinical computed tomography (CT). X-ray microtomography ($\mu$CT) on the other hand allows imaging of ex-vivo tissues at a resolution of tens of microns. In this work, we use clinical CT to image lung cancer patients before partial pneumonectomy (resection of pathological lung tissue). The resected specimen is prepared for $\mu$CT imaging at a voxel resolution of 50 $\mu$m (0.05 mm). This high-resolution image of the lung cancer tissue allows further insides into understanding of tumor growth and categorization. For making full use of this additional information, image fusion (registration) needs to be performed in order to re-align the $\mu$CT image with clinical CT. We developed a multi-scale non-rigid registration approach. After manual initialization using a few landmark points and rigid alignment, several levels of non-rigid registration between down-sampled (in the case of $\mu$CT) and up-sampled (in the case of clinical CT) representations of the image are performed. Any non-lung tissue is ignored during the computation of the similarity measure used to guide the registration during optimization. We are able to recover the volume differences introduced by the resection and preparation of the lung specimen. The average ($\pm$ std. dev.) minimum surface distance between $\mu$CT and clinical CT at the resected lung surface is reduced from 3.3 $\pm$ 2.9 (range: [0.1, 15.9]) to 2.3 mm $\pm$ 2.8 (range: [0.0, 15.3]) mm. The alignment of clinical CT with $\mu$CT will allow further registration with even finer resolutions of $\mu$CT (up to 10 $\mu$m resolution) and ultimately with histopathological microscopy images for further macro to micro image fusion that can aid medical image analysis.



### Segmentation of Objects by Hashing
- **Arxiv ID**: http://arxiv.org/abs/1702.08160v13
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08160v13)
- **Published**: 2017-02-27 06:40:22+00:00
- **Updated**: 2020-04-17 16:33:28+00:00
- **Authors**: J. D. Curtó, I. C. Zarza, Alex Smola, Luc van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach to address the problem of Simultaneous Detection and Segmentation introduced in [Hariharan et al 2014]. Using the hierarchical structures first presented in [Arbel\'aez et al 2011] we use an efficient and accurate procedure that exploits the feature information of the hierarchy using Locality Sensitive Hashing. We build on recent work that utilizes convolutional neural networks to detect bounding boxes in an image [Ren et al 2015] and then use the top similar hierarchical region that best fits each bounding box after hashing, we call this approach C&Z Segmentation. We then refine our final segmentation results by automatic hierarchical pruning. C&Z Segmentation introduces a train-free alternative to Hypercolumns [Hariharan et al 2015]. We conduct extensive experiments on PASCAL VOC 2012 segmentation dataset, showing that C&Z gives competitive state-of-the-art segmentations of objects.



### DeepNAT: Deep Convolutional Neural Network for Segmenting Neuroanatomy
- **Arxiv ID**: http://arxiv.org/abs/1702.08192v1
- **DOI**: 10.1016/j.neuroimage.2017.02.035
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1702.08192v1)
- **Published**: 2017-02-27 08:53:31+00:00
- **Updated**: 2017-02-27 08:53:31+00:00
- **Authors**: Christian Wachinger, Martin Reuter, Tassilo Klein
- **Comment**: Accepted for publication in NeuroImage, special issue "Brain
  Segmentation and Parcellation", 2017
- **Journal**: None
- **Summary**: We introduce DeepNAT, a 3D Deep convolutional neural network for the automatic segmentation of NeuroAnaTomy in T1-weighted magnetic resonance images. DeepNAT is an end-to-end learning-based approach to brain segmentation that jointly learns an abstract feature representation and a multi-class classification. We propose a 3D patch-based approach, where we do not only predict the center voxel of the patch but also neighbors, which is formulated as multi-task learning. To address a class imbalance problem, we arrange two networks hierarchically, where the first one separates foreground from background, and the second one identifies 25 brain structures on the foreground. Since patches lack spatial context, we augment them with coordinates. To this end, we introduce a novel intrinsic parameterization of the brain volume, formed by eigenfunctions of the Laplace-Beltrami operator. As network architecture, we use three convolutional layers with pooling, batch normalization, and non-linearities, followed by fully connected layers with dropout. The final segmentation is inferred from the probabilistic output of the network with a 3D fully connected conditional random field, which ensures label agreement between close voxels. The roughly 2.7 million parameters in the network are learned with stochastic gradient descent. Our results show that DeepNAT compares favorably to state-of-the-art methods. Finally, the purely learning-based method may have a high potential for the adaptation to young, old, or diseased brains by fine-tuning the pre-trained network with a small training sample on the target application, where the availability of larger datasets with manual annotations may boost the overall segmentation accuracy in the future.



### Anticipating many futures: Online human motion prediction and synthesis for human-robot collaboration
- **Arxiv ID**: http://arxiv.org/abs/1702.08212v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1702.08212v1)
- **Published**: 2017-02-27 10:02:40+00:00
- **Updated**: 2017-02-27 10:02:40+00:00
- **Authors**: Judith Bütepage, Hedvig Kjellström, Danica Kragic
- **Comment**: None
- **Journal**: None
- **Summary**: Fluent and safe interactions of humans and robots require both partners to anticipate the others' actions. A common approach to human intention inference is to model specific trajectories towards known goals with supervised classifiers. However, these approaches do not take possible future movements into account nor do they make use of kinematic cues, such as legible and predictable motion. The bottleneck of these methods is the lack of an accurate model of general human motion. In this work, we present a conditional variational autoencoder that is trained to predict a window of future human motion given a window of past frames. Using skeletal data obtained from RGB depth images, we show how this unsupervised approach can be used for online motion prediction for up to 1660 ms. Additionally, we demonstrate online target prediction within the first 300-500 ms after motion onset without the use of target specific training data. The advantage of our probabilistic approach is the possibility to draw samples of possible future motions. Finally, we investigate how movements and kinematic cues are represented on the learned low dimensional manifold.



### Low-Precision Batch-Normalized Activations
- **Arxiv ID**: http://arxiv.org/abs/1702.08231v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.08231v1)
- **Published**: 2017-02-27 11:10:54+00:00
- **Updated**: 2017-02-27 11:10:54+00:00
- **Authors**: Benjamin Graham
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Artificial neural networks can be trained with relatively low-precision floating-point and fixed-point arithmetic, using between one and 16 bits. Previous works have focused on relatively wide-but-shallow, feed-forward networks. We introduce a quantization scheme that is compatible with training very deep neural networks. Quantizing the network activations in the middle of each batch-normalization module can greatly reduce the amount of memory and computational power needed, with little loss in accuracy.



### Adaptive Ensemble Prediction for Deep Neural Networks based on Confidence Level
- **Arxiv ID**: http://arxiv.org/abs/1702.08259v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1702.08259v3)
- **Published**: 2017-02-27 12:54:54+00:00
- **Updated**: 2019-03-08 11:46:51+00:00
- **Authors**: Hiroshi Inoue
- **Comment**: None
- **Journal**: None
- **Summary**: Ensembling multiple predictions is a widely used technique for improving the accuracy of various machine learning tasks. One obvious drawback of ensembling is its higher execution cost during inference. In this paper, we first describe our insights on the relationship between the probability of prediction and the effect of ensembling with current deep neural networks; ensembling does not help mispredictions for inputs predicted with a high probability even when there is a non-negligible number of mispredicted inputs. This finding motivated us to develop a way to adaptively control the ensembling. If the prediction for an input reaches a high enough probability, i.e., the output from the softmax function, on the basis of the confidence level, we stop ensembling for this input to avoid wasting computation power. We evaluated the adaptive ensembling by using various datasets and showed that it reduces the computation cost significantly while achieving accuracy similar to that of static ensembling using a pre-defined number of local predictions. We also show that our statistically rigorous confidence-level-based early-exit condition reduces the burden of task-dependent threshold tuning better compared with naive early exit based on a pre-defined threshold in addition to yielding a better accuracy with the same cost.



### A Dataset for Developing and Benchmarking Active Vision
- **Arxiv ID**: http://arxiv.org/abs/1702.08272v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08272v2)
- **Published**: 2017-02-27 13:23:35+00:00
- **Updated**: 2017-03-03 20:06:58+00:00
- **Authors**: Phil Ammirato, Patrick Poirson, Eunbyung Park, Jana Kosecka, Alexander C. Berg
- **Comment**: To appear at ICRA 2017
- **Journal**: None
- **Summary**: We present a new public dataset with a focus on simulating robotic vision tasks in everyday indoor environments using real imagery. The dataset includes 20,000+ RGB-D images and 50,000+ 2D bounding boxes of object instances densely captured in 9 unique scenes. We train a fast object category detector for instance detection on our data. Using the dataset we show that, although increasingly accurate and fast, the state of the art for object detection is still severely impacted by object scale, occlusion, and viewing direction all of which matter for robotics applications. We next validate the dataset for simulating active vision, and use the dataset to develop and evaluate a deep-network-based system for next best move prediction for object classification using reinforcement learning. Our dataset is available for download at cs.unc.edu/~ammirato/active_vision_dataset_website/.



### Efficient Privacy Preserving Viola-Jones Type Object Detection via Random Base Image Representation
- **Arxiv ID**: http://arxiv.org/abs/1702.08318v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08318v2)
- **Published**: 2017-02-27 15:16:23+00:00
- **Updated**: 2017-03-30 14:06:21+00:00
- **Authors**: Xin Jin, Peng Yuan, Xiaodong Li, Chenggen Song, Shiming Ge, Geng Zhao, Yingya Chen
- **Comment**: 6 pages, 3 figures, To appear in the proceedings of the IEEE
  International Conference on Multimedia and Expo (ICME), Jul 10, 2017 - Jul
  14, 2017, Hong Kong, Hong Kong
- **Journal**: None
- **Summary**: A cloud server spent a lot of time, energy and money to train a Viola-Jones type object detector with high accuracy. Clients can upload their photos to the cloud server to find objects. However, the client does not want the leakage of the content of his/her photos. In the meanwhile, the cloud server is also reluctant to leak any parameters of the trained object detectors. 10 years ago, Avidan & Butman introduced Blind Vision, which is a method for securely evaluating a Viola-Jones type object detector. Blind Vision uses standard cryptographic tools and is painfully slow to compute, taking a couple of hours to scan a single image. The purpose of this work is to explore an efficient method that can speed up the process. We propose the Random Base Image (RBI) Representation. The original image is divided into random base images. Only the base images are submitted randomly to the cloud server. Thus, the content of the image can not be leaked. In the meanwhile, a random vector and the secure Millionaire protocol are leveraged to protect the parameters of the trained object detector. The RBI makes the integral-image enable again for the great acceleration. The experimental results reveal that our method can retain the detection accuracy of that of the plain vision algorithm and is significantly faster than the traditional blind vision, with only a very low probability of the information leakage theoretically.



### Visual Translation Embedding Network for Visual Relation Detection
- **Arxiv ID**: http://arxiv.org/abs/1702.08319v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1702.08319v1)
- **Published**: 2017-02-27 15:16:47+00:00
- **Updated**: 2017-02-27 15:16:47+00:00
- **Authors**: Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, Tat-Seng Chua
- **Comment**: None
- **Journal**: None
- **Summary**: Visual relations, such as "person ride bike" and "bike next to car", offer a comprehensive scene understanding of an image, and have already shown their great utility in connecting computer vision and natural language. However, due to the challenging combinatorial complexity of modeling subject-predicate-object relation triplets, very little work has been done to localize and predict visual relations. Inspired by the recent advances in relational representation learning of knowledge bases and convolutional object detection networks, we propose a Visual Translation Embedding network (VTransE) for visual relation detection. VTransE places objects in a low-dimensional relation space where a relation can be modeled as a simple vector translation, i.e., subject + predicate $\approx$ object. We propose a novel feature extraction layer that enables object-relation knowledge transfer in a fully-convolutional fashion that supports training and inference in a single forward/backward pass. To the best of our knowledge, VTransE is the first end-to-end relation detection network. We demonstrate the effectiveness of VTransE over other state-of-the-art methods on two large-scale datasets: Visual Relationship and Visual Genome. Note that even though VTransE is a purely visual model, it is still competitive to the Lu's multi-modal model with language priors.



### Multi-Label Segmentation via Residual-Driven Adaptive Regularization
- **Arxiv ID**: http://arxiv.org/abs/1702.08336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08336v1)
- **Published**: 2017-02-27 15:50:35+00:00
- **Updated**: 2017-02-27 15:50:35+00:00
- **Authors**: Byung-Woo Hong, Ja-Keoung Koo, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We present a variational multi-label segmentation algorithm based on a robust Huber loss for both the data and the regularizer, minimized within a convex optimization framework. We introduce a novel constraint on the common areas, to bias the solution towards mutually exclusive regions. We also propose a regularization scheme that is adapted to the spatial statistics of the residual at each iteration, resulting in a varying degree of regularization being applied as the algorithm proceeds: the effect of the regularizer is strongest at initialization, and wanes as the solution increasingly fits the data. This minimizes the bias induced by the regularizer at convergence. We design an efficient convex optimization algorithm based on the alternating direction method of multipliers using the equivalent relation between the Huber function and the proximal operator of the one-norm. We empirically validate our proposed algorithm on synthetic and real images and offer an information-theoretic derivation of the cost-function that highlights the modeling choices made.



### Revealing Hidden Potentials of the q-Space Signal in Breast Cancer
- **Arxiv ID**: http://arxiv.org/abs/1702.08379v3
- **DOI**: 10.1007/978-3-319-66182-7_76
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08379v3)
- **Published**: 2017-02-27 17:06:20+00:00
- **Updated**: 2017-05-22 08:14:59+00:00
- **Authors**: Paul Jaeger, Sebastian Bickelhaupt, Frederik Bernd Laun, Wolfgang Lederer, Daniel Heidi, Tristan Anselm Kuder, Daniel Paech, David Bonekamp, Alexander Radbruch, Stefan Delorme, Heinz-Peter Schlemmer, Franziska Steudle, Klaus H. Maier-Hein
- **Comment**: Accepted conference paper at MICCAI 2017
- **Journal**: None
- **Summary**: Mammography screening for early detection of breast lesions currently suffers from high amounts of false positive findings, which result in unnecessary invasive biopsies. Diffusion-weighted MR images (DWI) can help to reduce many of these false-positive findings prior to biopsy. Current approaches estimate tissue properties by means of quantitative parameters taken from generative, biophysical models fit to the q-space encoded signal under certain assumptions regarding noise and spatial homogeneity. This process is prone to fitting instability and partial information loss due to model simplicity. We reveal unexplored potentials of the signal by integrating all data processing components into a convolutional neural network (CNN) architecture that is designed to propagate clinical target information down to the raw input images. This approach enables simultaneous and target-specific optimization of image normalization, signal exploitation, global representation learning and classification. Using a multicentric data set of 222 patients, we demonstrate that our approach significantly improves clinical decision making with respect to the current state of the art.



### Asymmetric Tri-training for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1702.08400v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1702.08400v3)
- **Published**: 2017-02-27 17:48:17+00:00
- **Updated**: 2017-05-13 05:44:03+00:00
- **Authors**: Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: TBA on ICML2017
- **Journal**: None
- **Summary**: Deep-layered models trained on a large number of labeled samples boost the accuracy of many tasks. It is important to apply such models to different domains because collecting many labeled samples in various domains is expensive. In unsupervised domain adaptation, one needs to train a classifier that works well on a target domain when provided with labeled source samples and unlabeled target samples. Although many methods aim to match the distributions of source and target samples, simply matching the distribution cannot ensure accuracy on the target domain. To learn discriminative representations for the target domain, we assume that artificially labeling target samples can result in a good representation. Tri-training leverages three classifiers equally to give pseudo-labels to unlabeled samples, but the method does not assume labeling samples generated from a different domain.In this paper, we propose an asymmetric tri-training method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train neural networks as if they are true labels. In our work, we use three networks asymmetrically. By asymmetric, we mean that two networks are used to label unlabeled target samples and one network is trained by the samples to obtain target-discriminative representations. We evaluate our method on digit recognition and sentiment analysis datasets. Our proposed method achieves state-of-the-art performance on the benchmark digit recognition datasets of domain adaptation.



### Age Progression/Regression by Conditional Adversarial Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1702.08423v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08423v2)
- **Published**: 2017-02-27 18:28:58+00:00
- **Updated**: 2017-03-28 20:02:15+00:00
- **Authors**: Zhifei Zhang, Yang Song, Hairong Qi
- **Comment**: Accepted by The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2017)
- **Journal**: None
- **Summary**: "If I provide you a face image of mine (without telling you the actual age when I took the picture) and a large amount of face images that I crawled (containing labeled faces of different ages but not necessarily paired), can you show me what I would look like when I am 80 or what I was like when I was 5?" The answer is probably a "No." Most existing face aging works attempt to learn the transformation between age groups and thus would require the paired samples as well as the labeled query image. In this paper, we look at the problem from a generative modeling perspective such that no paired samples is required. In addition, given an unlabeled image, the generative model can directly produce the image with desired age attribute. We propose a conditional adversarial autoencoder (CAAE) that learns a face manifold, traversing on which smooth age progression and regression can be realized simultaneously. In CAAE, the face is first mapped to a latent vector through a convolutional encoder, and then the vector is projected to the face manifold conditional on age through a deconvolutional generator. The latent vector preserves personalized face features (i.e., personality) and the age condition controls progression vs. regression. Two adversarial networks are imposed on the encoder and generator, respectively, forcing to generate more photo-realistic faces. Experimental results demonstrate the appealing performance and flexibility of the proposed framework by comparing with the state-of-the-art and ground truth.



### Skin Lesion Classification Using Hybrid Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.08434v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08434v2)
- **Published**: 2017-02-27 18:54:41+00:00
- **Updated**: 2019-04-25 09:39:53+00:00
- **Authors**: Amirreza Mahbod, Gerald Schaefer, Chunliang Wang, Rupert Ecker, Isabella Ellinger
- **Comment**: Accepted for the 44th International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP 2019)
- **Journal**: None
- **Summary**: Skin cancer is one of the major types of cancers with an increasing incidence over the past decades. Accurately diagnosing skin lesions to discriminate between benign and malignant skin lesions is crucial to ensure appropriate patient treatment. While there are many computerised methods for skin lesion classification, convolutional neural networks (CNNs) have been shown to be superior over classical methods. In this work, we propose a fully automatic computerised method for skin lesion classification which employs optimised deep features from a number of well-established CNNs and from different abstraction levels. We use three pre-trained deep models, namely AlexNet, VGG16 and ResNet-18, as deep feature generators. The extracted features then are used to train support vector machine classifiers. In the final stage, the classifier outputs are fused to obtain a classification. Evaluated on the 150 validation images from the ISIC 2017 classification challenge, the proposed method is shown to achieve very good classification performance, yielding an area under receiver operating characteristic curve of 83.83% for melanoma classification and of 97.55% for seborrheic keratosis classification.



### Memory-Efficient Global Refinement of Decision-Tree Ensembles and its Application to Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1702.08481v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1702.08481v2)
- **Published**: 2017-02-27 19:25:53+00:00
- **Updated**: 2018-09-03 08:49:21+00:00
- **Authors**: Nenad Markuš, Ivan Gogić, Igor S. Pandžić, Jörgen Ahlberg
- **Comment**: BMVC Newcastle 2018
- **Journal**: None
- **Summary**: Ren et al. recently introduced a method for aggregating multiple decision trees into a strong predictor by interpreting a path taken by a sample down each tree as a binary vector and performing linear regression on top of these vectors stacked together. They provided experimental evidence that the method offers advantages over the usual approaches for combining decision trees (random forests and boosting). The method truly shines when the regression target is a large vector with correlated dimensions, such as a 2D face shape represented with the positions of several facial landmarks. However, we argue that their basic method is not applicable in many practical scenarios due to large memory requirements. This paper shows how this issue can be solved through the use of quantization and architectural changes of the predictor that maps decision tree-derived encodings to the desired output.



### Understanding Convolution for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1702.08502v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08502v3)
- **Published**: 2017-02-27 20:05:11+00:00
- **Updated**: 2018-06-01 01:15:23+00:00
- **Authors**: Panqu Wang, Pengfei Chen, Ye Yuan, Ding Liu, Zehua Huang, Xiaodi Hou, Garrison Cottrell
- **Comment**: WACV 2018. Updated acknowledgements. Source code:
  https://github.com/TuSimple/TuSimple-DUC
- **Journal**: None
- **Summary**: Recent advances in deep learning, especially deep convolutional neural networks (CNNs), have led to significant improvement over previous semantic segmentation systems. Here we show how to improve pixel-wise semantic segmentation by manipulating convolution-related operations that are of both theoretical and practical value. First, we design dense upsampling convolution (DUC) to generate pixel-level prediction, which is able to capture and decode more detailed information that is generally missing in bilinear upsampling. Second, we propose a hybrid dilated convolution (HDC) framework in the encoding phase. This framework 1) effectively enlarges the receptive fields (RF) of the network to aggregate global information; 2) alleviates what we call the "gridding issue" caused by the standard dilated convolution operation. We evaluate our approaches thoroughly on the Cityscapes dataset, and achieve a state-of-art result of 80.1% mIOU in the test set at the time of submission. We also have achieved state-of-the-art overall on the KITTI road estimation benchmark and the PASCAL VOC2012 segmentation task. Our source code can be found at https://github.com/TuSimple/TuSimple-DUC .



### Image Analysis Using a Dual-Tree $M$-Band Wavelet Transform
- **Arxiv ID**: http://arxiv.org/abs/1702.08534v1
- **DOI**: 10.1109/TIP.2006.875178
- **Categories**: **physics.data-an**, cs.CV, math.FA
- **Links**: [PDF](http://arxiv.org/pdf/1702.08534v1)
- **Published**: 2017-02-27 21:15:17+00:00
- **Updated**: 2017-02-27 21:15:17+00:00
- **Authors**: Caroline Chaux, Laurent Duval, Jean-Christophe Pesquet
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, August 2006, Volume 15,
  Issue 8, p. 2397-2412
- **Summary**: We propose a 2D generalization to the $M$-band case of the dual-tree decomposition structure (initially proposed by N. Kingsbury and further investigated by I. Selesnick) based on a Hilbert pair of wavelets. We particularly address (\textit{i}) the construction of the dual basis and (\textit{ii}) the resulting directional analysis. We also revisit the necessary pre-processing stage in the $M$-band case. While several reconstructions are possible because of the redundancy of the representation, we propose a new optimal signal reconstruction technique, which minimizes potential estimation errors. The effectiveness of the proposed $M$-band decomposition is demonstrated via denoising comparisons on several image types (natural, texture, seismics), with various $M$-band wavelets and thresholding strategies. Significant improvements in terms of both overall noise reduction and direction preservation are observed.



### DepthSynth: Real-Time Realistic Synthetic Data Generation from CAD Models for 2.5D Recognition
- **Arxiv ID**: http://arxiv.org/abs/1702.08558v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08558v2)
- **Published**: 2017-02-27 22:12:25+00:00
- **Updated**: 2017-11-28 17:34:22+00:00
- **Authors**: Benjamin Planche, Ziyan Wu, Kai Ma, Shanhui Sun, Stefan Kluckner, Terrence Chen, Andreas Hutter, Sergey Zakharov, Harald Kosch, Jan Ernst
- **Comment**: International Conference on 3D Vision 2017
- **Journal**: None
- **Summary**: Recent progress in computer vision has been dominated by deep neural networks trained over large amounts of labeled data. Collecting such datasets is however a tedious, often impossible task; hence a surge in approaches relying solely on synthetic data for their training. For depth images however, discrepancies with real scans still noticeably affect the end performance. We thus propose an end-to-end framework which simulates the whole mechanism of these devices, generating realistic depth data from 3D models by comprehensively modeling vital factors e.g. sensor noise, material reflectance, surface geometry. Not only does our solution cover a wider range of sensors and achieve more realistic results than previous methods, assessed through extended evaluation, but we go further by measuring the impact on the training of neural networks for various recognition tasks; demonstrating how our pipeline seamlessly integrates such architectures and consistently enhances their performance.



