# Arxiv Papers in cs.CV on 2017-02-10
### A New Rank Constraint on Multi-view Fundamental Matrices, and its Application to Camera Location Recovery
- **Arxiv ID**: http://arxiv.org/abs/1702.03023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03023v1)
- **Published**: 2017-02-10 00:31:17+00:00
- **Updated**: 2017-02-10 00:31:17+00:00
- **Authors**: Soumyadip Sengupta, Tal Amir, Meirav Galun, Tom Goldstein, David W. Jacobs, Amit Singer, Ronen Basri
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate estimation of camera matrices is an important step in structure from motion algorithms. In this paper we introduce a novel rank constraint on collections of fundamental matrices in multi-view settings. We show that in general, with the selection of proper scale factors, a matrix formed by stacking fundamental matrices between pairs of images has rank 6. Moreover, this matrix forms the symmetric part of a rank 3 matrix whose factors relate directly to the corresponding camera matrices. We use this new characterization to produce better estimations of fundamental matrices by optimizing an L1-cost function using Iterative Re-weighted Least Squares and Alternate Direction Method of Multiplier. We further show that this procedure can improve the recovery of camera locations, particularly in multi-view settings in which fewer images are available.



### Reconstruction-Based Disentanglement for Pose-invariant Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1702.03041v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03041v2)
- **Published**: 2017-02-10 02:03:31+00:00
- **Updated**: 2017-08-16 15:52:01+00:00
- **Authors**: Xi Peng, Xiang Yu, Kihyuk Sohn, Dimitris Metaxas, Manmohan Chandraker
- **Comment**: IEEE International Conference on Computer Vision (ICCV), 2017
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) trained on large-scale datasets have recently achieved impressive improvements in face recognition. But a persistent challenge remains to develop methods capable of handling large pose variations that are relatively underrepresented in training data. This paper presents a method for learning a feature representation that is invariant to pose, without requiring extensive pose coverage in training data. We first propose to generate non-frontal views from a single frontal face, in order to increase the diversity of training data while preserving accurate facial details that are critical for identity discrimination. Our next contribution is to seek a rich embedding that encodes identity features, as well as non-identity ones such as pose and landmark locations. Finally, we propose a new feature reconstruction metric learning to explicitly disentangle identity and pose, by demanding alignment between the feature reconstructions through various combinations of identity and pose features, which is obtained from two images of the same subject. Experiments on both controlled and in-the-wild face datasets, such as MultiPIE, 300WLP and the profile view database CFP, show that our method consistently outperforms the state-of-the-art, especially on images with large head pose variations. Detail results and resource are referred to https://sites.google.com/site/xipengcshomepage/iccv2017



### Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights
- **Arxiv ID**: http://arxiv.org/abs/1702.03044v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1702.03044v2)
- **Published**: 2017-02-10 02:30:22+00:00
- **Updated**: 2017-08-25 13:21:18+00:00
- **Authors**: Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, Yurong Chen
- **Comment**: Published by ICLR 2017, and the code is available at
  https://github.com/Zhouaojun/Incremental-Network-Quantization
- **Journal**: None
- **Summary**: This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization, our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. The code is available at https://github.com/Zhouaojun/Incremental-Network-Quantization.



### Online People Tracking and Identification with RFID and Kinect
- **Arxiv ID**: http://arxiv.org/abs/1702.03824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03824v1)
- **Published**: 2017-02-10 05:05:20+00:00
- **Updated**: 2017-02-10 05:05:20+00:00
- **Authors**: Xinyu Li, Yanyi Zhang, Ivan Marsic, Randall S. Burd
- **Comment**: 8 Pages, 8 Figures
- **Journal**: None
- **Summary**: We introduce a novel, accurate and practical system for real-time people tracking and identification. We used a Kinect V2 sensor for tracking that generates a body skeleton for up to six people in the view. We perform identification using both Kinect and passive RFID, by first measuring the velocity vector of person's skeleton and of their RFID tag using the position of the RFID reader antennas as reference points and then finding the best match between skeletons and tags. We introduce a method for synchronizing Kinect data, which is captured regularly, with irregular or missing RFID data readouts. Our experiments show centimeter-level people tracking resolution with 80% average identification accuracy for up to six people in indoor environments, which meets the needs of many applications. Our system can preserve user privacy and work with different lighting.



### Graph Fourier Transform with Negative Edges for Depth Image Coding
- **Arxiv ID**: http://arxiv.org/abs/1702.03105v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03105v2)
- **Published**: 2017-02-10 09:11:12+00:00
- **Updated**: 2017-06-08 06:28:34+00:00
- **Authors**: Weng-Tai Su, Gene Cheung, Chia-Wen Lin
- **Comment**: 5 pages, submitted to submitted to IEEE International Conference on
  Image Processing, Beijing, China, September, 2017
- **Journal**: None
- **Summary**: Recent advent in graph signal processing (GSP) has led to the development of new graph-based transforms and wavelets for image / video coding, where the underlying graph describes inter-pixel correlations. In this paper, we develop a new transform called signed graph Fourier transform (SGFT), where the underlying graph G contains negative edges that describe anti-correlations between pixel pairs. Specifically, we first construct a one-state Markov process that models both inter-pixel correlations and anti-correlations. We then derive the corresponding precision matrix, and show that the loopy graph Laplacian matrix Q of a graph G with a negative edge and two self-loops at its end nodes is approximately equivalent. This proves that the eigenvectors of Q - called SGFT - approximates the optimal Karhunen-Lo`eve Transform (KLT). We show the importance of the self-loops in G to ensure Q is positive semi-definite. We prove that the first eigenvector of Q is piecewise constant (PWC), and thus can well approximate a piecewise smooth (PWS) signal like a depth image. Experimental results show that a block-based coding scheme based on SGFT outperforms a previous scheme using graph transforms with only positive edges for several depth images.



### Texture Characterization by Using Shape Co-occurrence Patterns
- **Arxiv ID**: http://arxiv.org/abs/1702.03115v1
- **DOI**: 10.1109/TIP.2017.2726182
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03115v1)
- **Published**: 2017-02-10 09:37:33+00:00
- **Updated**: 2017-02-10 09:37:33+00:00
- **Authors**: Gui-Song Xia, Gang Liu, Xiang Bai, Liangpei Zhang
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, Vol. 26, No.10, pp. 5005 -
  5018, 2017
- **Summary**: Texture characterization is a key problem in image understanding and pattern recognition. In this paper, we present a flexible shape-based texture representation using shape co-occurrence patterns. More precisely, texture images are first represented by tree of shapes, each of which is associated with several geometrical and radiometric attributes. Then four typical kinds of shape co-occurrence patterns based on the hierarchical relationship of the shapes in the tree are learned as codewords. Three different coding methods are investigated to learn the codewords, with which, any given texture image can be encoded into a descriptive vector. In contrast with existing works, the proposed method not only inherits the strong ability to depict geometrical aspects of textures and the high robustness to variations of imaging conditions from the shape-based method, but also provides a flexible way to consider shape relationships and to compute high-order statistics on the tree. To our knowledge, this is the first time to use co-occurrence patterns of explicit shapes as a tool for texture analysis. Experiments on various texture datasets and scene datasets demonstrate the efficiency of the proposed method.



### A clustering approach to heterogeneous change detection
- **Arxiv ID**: http://arxiv.org/abs/1702.03176v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03176v1)
- **Published**: 2017-02-10 14:12:14+00:00
- **Updated**: 2017-02-10 14:12:14+00:00
- **Authors**: Luigi Tommaso Luppino, Stian Normann Anfinsen, Gabriele Moser, Robert Jenssen, Filippo Maria Bianchi, Sebastiano Serpico, Gregoire Mercier
- **Comment**: None
- **Journal**: None
- **Summary**: Change detection in heterogeneous multitemporal satellite images is a challenging and still not much studied topic in remote sensing and earth observation. This paper focuses on comparison of image pairs covering the same geographical area and acquired by two different sensors, one optical radiometer and one synthetic aperture radar, at two different times. We propose a clustering-based technique to detect changes, identified as clusters that split or merge in the different images. To evaluate potentials and limitations of our method, we perform experiments on real data. Preliminary results confirm the relationship between splits and merges of clusters and the occurrence of changes. However, it becomes evident that it is necessary to incorporate prior, ancillary, or application-specific information to improve the interpretation of clustering results and to identify unambiguously the areas of change.



### Dual-Tree Wavelet Scattering Network with Parametric Log Transformation for Object Classification
- **Arxiv ID**: http://arxiv.org/abs/1702.03267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03267v1)
- **Published**: 2017-02-10 18:02:05+00:00
- **Updated**: 2017-02-10 18:02:05+00:00
- **Authors**: Amarjot Singh, Nick Kingsbury
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a ScatterNet that uses a parametric log transformation with Dual-Tree complex wavelets to extract translation invariant representations from a multi-resolution image. The parametric transformation aids the OLS pruning algorithm by converting the skewed distributions into relatively mean-symmetric distributions while the Dual-Tree wavelets improve the computational efficiency of the network. The proposed network is shown to outperform Mallat's ScatterNet on two image datasets, both for classification accuracy and computational efficiency. The advantages of the proposed network over other supervised and some unsupervised methods are also presented using experiments performed on different training dataset sizes.



### Multi-Resolution Dual-Tree Wavelet Scattering Network for Signal Classification
- **Arxiv ID**: http://arxiv.org/abs/1702.03345v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03345v1)
- **Published**: 2017-02-10 22:52:13+00:00
- **Updated**: 2017-02-10 22:52:13+00:00
- **Authors**: Amarjot Singh, Nick Kingsbury
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a Deep Scattering network that utilizes Dual-Tree complex wavelets to extract translation invariant representations from an input signal. The computationally efficient Dual-Tree wavelets decompose the input signal into densely spaced representations over scales. Translation invariance is introduced in the representations by applying a non-linearity over a region followed by averaging. The discriminatory information in the densely spaced, locally smooth, signal representations aids the learning of the classifier. The proposed network is shown to outperform Mallat's ScatterNet on four datasets with different modalities on classification accuracy.



### Enhanced Local Binary Patterns for Automatic Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1702.03349v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03349v2)
- **Published**: 2017-02-10 23:10:14+00:00
- **Updated**: 2018-06-14 21:56:13+00:00
- **Authors**: Pavel Král, Ladislav Lenc, Antonín Vrba
- **Comment**: Submitted for VCIP 2018
- **Journal**: None
- **Summary**: This paper presents a novel automatic face recognition approach based on local binary patterns. This descriptor considers a local neighbourhood of a pixel to compute the feature vector values. This method is not very robust to handle image noise, variances and different illumination conditions. We address these issues by proposing a novel descriptor which considers more pixels and different neighbourhoods to compute the feature vector values. The proposed method is evaluated on two benchmark corpora, namely UFI and FERET face datasets. We experimentally show that our approach outperforms state-of-the-art methods and is efficient particularly in the real conditions where the above mentioned issues are obvious. We further show that the proposed method handles well one training sample issue and is also robust to the image resolution.



