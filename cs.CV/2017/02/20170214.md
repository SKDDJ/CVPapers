# Arxiv Papers in cs.CV on 2017-02-14
### Evolution-Preserving Dense Trajectory Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1702.04037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04037v1)
- **Published**: 2017-02-14 00:54:52+00:00
- **Updated**: 2017-02-14 00:54:52+00:00
- **Authors**: Yang Wang, Vinh Tran, Minh Hoai
- **Comment**: None
- **Journal**: None
- **Summary**: Recently Trajectory-pooled Deep-learning Descriptors were shown to achieve state-of-the-art human action recognition results on a number of datasets. This paper improves their performance by applying rank pooling to each trajectory, encoding the temporal evolution of deep learning features computed along the trajectory. This leads to Evolution-Preserving Trajectory (EPT) descriptors, a novel type of video descriptor that significantly outperforms Trajectory-pooled Deep-learning Descriptors. EPT descriptors are defined based on dense trajectories, and they provide complimentary benefits to video descriptors that are not based on trajectories. In particular, we show that the combination of EPT descriptors and VideoDarwin leads to state-of-the-art performance on Hollywood2 and UCF101 datasets.



### A Graphical Social Topology Model for Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1702.04040v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04040v2)
- **Published**: 2017-02-14 01:27:47+00:00
- **Updated**: 2017-09-29 11:39:20+00:00
- **Authors**: Shan Gao, Xiaogang Chen, Qixiang Ye, Junliang Xing, Arjan Kuijper, Xiangyang Ji
- **Comment**: there is an input error in experiments, so we should change the
  results in all results tables
- **Journal**: None
- **Summary**: Tracking multiple objects is a challenging task when objects move in groups and occlude each other. Existing methods have investigated the problems of group division and group energy-minimization; however, lacking overall object-group topology modeling limits their ability in handling complex object and group dynamics. Inspired with the social affinity property of moving objects, we propose a Graphical Social Topology (GST) model, which estimates the group dynamics by jointly modeling the group structure and the states of objects using a topological representation. With such topology representation, moving objects are not only assigned to groups, but also dynamically connected with each other, which enables in-group individuals to be correctly associated and the cohesion of each group to be precisely modeled. Using well-designed topology learning modules and topology training, we infer the birth/death and merging/splitting of dynamic groups. With the GST model, the proposed multi-object tracker can naturally facilitate the occlusion problem by treating the occluded object and other in-group members as a whole while leveraging overall state transition. Experiments on both RGB and RGB-D datasets confirm that the proposed multi-object tracker improves the state-of-the-arts especially in crowded scenes.



### SSPP-DAN: Deep Domain Adaptation Network for Face Recognition with Single Sample Per Person
- **Arxiv ID**: http://arxiv.org/abs/1702.04069v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04069v4)
- **Published**: 2017-02-14 04:02:07+00:00
- **Updated**: 2018-04-28 12:48:28+00:00
- **Authors**: Sungeun Hong, Woobin Im, Jongbin Ryu, Hyun S. Yang
- **Comment**: Accepted to ICIP 2017 Oral, Code is available at
  https://github.com/csehong/SSPP-DAN
- **Journal**: None
- **Summary**: Real-world face recognition using a single sample per person (SSPP) is a challenging task. The problem is exacerbated if the conditions under which the gallery image and the probe set are captured are completely different. To address these issues from the perspective of domain adaptation, we introduce an SSPP domain adaptation network (SSPP-DAN). In the proposed approach, domain adaptation, feature extraction, and classification are performed jointly using a deep architecture with domain-adversarial training. However, the SSPP characteristic of one training sample per class is insufficient to train the deep architecture. To overcome this shortage, we generate synthetic images with varying poses using a 3D face model. Experimental evaluations using a realistic SSPP dataset show that deep domain adaptation and image synthesis complement each other and dramatically improve accuracy. Experiments on a benchmark dataset using the proposed approach show state-of-the-art performance. All the dataset and the source code can be found in our online repository (https://github.com/csehong/SSPP-DAN).



### Efficient Algorithms for Moral Lineage Tracing
- **Arxiv ID**: http://arxiv.org/abs/1702.04111v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04111v2)
- **Published**: 2017-02-14 08:39:33+00:00
- **Updated**: 2017-08-25 09:33:06+00:00
- **Authors**: Markus Rempfler, Jan-Hendrik Lange, Florian Jug, Corinna Blasse, Eugene W. Myers, Bjoern H. Menze, Bjoern Andres
- **Comment**: Accepted at ICCV 2017
- **Journal**: None
- **Summary**: Lineage tracing, the joint segmentation and tracking of living cells as they move and divide in a sequence of light microscopy images, is a challenging task. Jug et al. have proposed a mathematical abstraction of this task, the moral lineage tracing problem (MLTP), whose feasible solutions define both a segmentation of every image and a lineage forest of cells. Their branch-and-cut algorithm, however, is prone to many cuts and slow convergence for large instances. To address this problem, we make three contributions: (i) we devise the first efficient primal feasible local search algorithms for the MLTP, (ii) we improve the branch-and-cut algorithm by separating tighter cutting planes and by incorporating our primal algorithms, (iii) we show in experiments that our algorithms find accurate solutions on the problem instances of Jug et al. and scale to larger instances, leveraging moral lineage tracing to practical significance.



### Graph Based Over-Segmentation Methods for 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1702.04114v1
- **DOI**: 10.1016/j.cviu.2018.06.004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04114v1)
- **Published**: 2017-02-14 08:53:13+00:00
- **Updated**: 2017-02-14 08:53:13+00:00
- **Authors**: Yizhak Ben-Shabat, Tamar Avraham, Michael Lindenbaum, Anath Fischer
- **Comment**: None
- **Journal**: None
- **Summary**: Over-segmentation, or super-pixel generation, is a common preliminary stage for many computer vision applications. New acquisition technologies enable the capturing of 3D point clouds that contain color and geometrical information. This 3D information introduces a new conceptual change that can be utilized to improve the results of over-segmentation, which uses mainly color information, and to generate clusters of points we call super-points. We consider a variety of possible 3D extensions of the Local Variation (LV) graph based over-segmentation algorithms, and compare them thoroughly. We consider different alternatives for constructing the connectivity graph, for assigning the edge weights, and for defining the merge criterion, which must now account for the geometric information and not only color. Following this evaluation, we derive a new generic algorithm for over-segmentation of 3D point clouds. We call this new algorithm Point Cloud Local Variation (PCLV). The advantages of the new over-segmentation algorithm are demonstrated on both outdoor and cluttered indoor scenes. Performance analysis of the proposed approach compared to state-of-the-art 2D and 3D over-segmentation algorithms shows significant improvement according to the common performance measures.



### One-Step Time-Dependent Future Video Frame Prediction with a Convolutional Encoder-Decoder Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1702.04125v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04125v2)
- **Published**: 2017-02-14 09:21:23+00:00
- **Updated**: 2017-07-24 11:11:25+00:00
- **Authors**: Vedran Vukotić, Silvia-Laura Pintea, Christian Raymond, Guillaume Gravier, Jan Van Gemert
- **Comment**: 11 pages, 1 figures, published in the International Conference of
  Image Analysis and Processing (ICIAP) 2017 and in the Netherlands Conference
  on Computer Vision (NCCV) 2016
- **Journal**: None
- **Summary**: There is an inherent need for autonomous cars, drones, and other robots to have a notion of how their environment behaves and to anticipate changes in the near future. In this work, we focus on anticipating future appearance given the current frame of a video. Existing work focuses on either predicting the future appearance as the next frame of a video, or predicting future motion as optical flow or motion trajectories starting from a single video frame. This work stretches the ability of CNNs (Convolutional Neural Networks) to predict an anticipation of appearance at an arbitrarily given future time, not necessarily the next video frame. We condition our predicted future appearance on a continuous time variable that allows us to anticipate future frames at a given temporal distance, directly from the input video frame. We show that CNNs can learn an intrinsic representation of typical appearance changes over time and successfully generate realistic predictions at a deliberate time difference in the near future.



### FERA 2017 - Addressing Head Pose in the Third Facial Expression Recognition and Analysis Challenge
- **Arxiv ID**: http://arxiv.org/abs/1702.04174v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04174v1)
- **Published**: 2017-02-14 12:22:30+00:00
- **Updated**: 2017-02-14 12:22:30+00:00
- **Authors**: Michel F. Valstar, Enrique Sánchez-Lozano, Jeffrey F. Cohn, László A. Jeni, Jeffrey M. Girard, Zheng Zhang, Lijun Yin, Maja Pantic
- **Comment**: FERA 2017 Baseline Paper
- **Journal**: None
- **Summary**: The field of Automatic Facial Expression Analysis has grown rapidly in recent years. However, despite progress in new approaches as well as benchmarking efforts, most evaluations still focus on either posed expressions, near-frontal recordings, or both. This makes it hard to tell how existing expression recognition approaches perform under conditions where faces appear in a wide range of poses (or camera views), displaying ecologically valid expressions. The main obstacle for assessing this is the availability of suitable data, and the challenge proposed here addresses this limitation. The FG 2017 Facial Expression Recognition and Analysis challenge (FERA 2017) extends FERA 2015 to the estimation of Action Units occurrence and intensity under different camera views. In this paper we present the third challenge in automatic recognition of facial expressions, to be held in conjunction with the 12th IEEE conference on Face and Gesture Recognition, May 2017, in Washington, United States. Two sub-challenges are defined: the detection of AU occurrence, and the estimation of AU intensity. In this work we outline the evaluation protocol, the data used, and the results of a baseline method for both sub-challenges.



### Structured Deep Hashing with Convolutional Neural Networks for Fast Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1702.04179v3
- **DOI**: 10.1016/j.cviu.2017.11.009
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04179v3)
- **Published**: 2017-02-14 12:35:53+00:00
- **Updated**: 2017-12-03 02:41:08+00:00
- **Authors**: Lin Wu, Yang Wang
- **Comment**: To appear at Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: Given a pedestrian image as a query, the purpose of person re-identification is to identify the correct match from a large collection of gallery images depicting the same person captured by disjoint camera views. The critical challenge is how to construct a robust yet discriminative feature representation to capture the compounded variations in pedestrian appearance. To this end, deep learning methods have been proposed to extract hierarchical features against extreme variability of appearance. However, existing methods in this category generally neglect the efficiency in the matching stage whereas the searching speed of a re-identification system is crucial in real-world applications. In this paper, we present a novel deep hashing framework with Convolutional Neural Networks (CNNs) for fast person re-identification. Technically, we simultaneously learn both CNN features and hash functions/codes to get robust yet discriminative features and similarity-preserving hash codes. Thereby, person re-identification can be resolved by efficiently computing and ranking the Hamming distances between images. A structured loss function defined over positive pairs and hard negatives is proposed to formulate a novel optimization problem so that fast convergence and more stable optimized solution can be obtained. Extensive experiments on two benchmarks CUHK03 \cite{FPNN} and Market-1501 \cite{Market1501} show that the proposed deep architecture is efficacy over state-of-the-arts.



### On Detecting Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1702.04267v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1702.04267v2)
- **Published**: 2017-02-14 15:44:26+00:00
- **Updated**: 2017-02-21 06:53:38+00:00
- **Authors**: Jan Hendrik Metzen, Tim Genewein, Volker Fischer, Bastian Bischoff
- **Comment**: Final version for ICLR2017 (see
  https://openreview.net/forum?id=SJzCSf9xg&noteId=SJzCSf9xg)
- **Journal**: None
- **Summary**: Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small "detector" subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.



### DAGER: Deep Age, Gender and Emotion Recognition Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1702.04280v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1702.04280v2)
- **Published**: 2017-02-14 16:34:05+00:00
- **Updated**: 2017-03-04 01:43:04+00:00
- **Authors**: Afshin Dehghan, Enrique G. Ortiz, Guang Shu, Syed Zain Masood
- **Comment**: 10 Pages
- **Journal**: None
- **Summary**: This paper describes the details of Sighthound's fully automated age, gender and emotion recognition system. The backbone of our system consists of several deep convolutional neural networks that are not only computationally inexpensive, but also provide state-of-the-art results on several competitive benchmarks. To power our novel deep networks, we collected large labeled datasets through a semi-supervised pipeline to reduce the annotation effort/time. We tested our system on several public benchmarks and report outstanding results. Our age, gender and emotion recognition models are available to developers through the Sighthound Cloud API at https://www.sighthound.com/products/cloud



### Integrating Three Mechanisms of Visual Attention for Active Visual Search
- **Arxiv ID**: http://arxiv.org/abs/1702.04292v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1702.04292v1)
- **Published**: 2017-02-14 17:06:37+00:00
- **Updated**: 2017-02-14 17:06:37+00:00
- **Authors**: Amir Rasouli, John K. Tsotsos
- **Comment**: Presented at International Symposium On Attention in Cognitive
  Systems (ISACS) in Association with IROS, 2015
- **Journal**: None
- **Summary**: Algorithms for robotic visual search can benefit from the use of visual attention methods in order to reduce computational costs. Here, we describe how three distinct mechanisms of visual attention can be integrated and productively used to improve search performance. The first is viewpoint selection as has been proposed earlier using a greedy search over a probabilistic occupancy grid representation. The second is top-down object-based attention using a histogram backprojection method, also previously described. The third is visual saliency. This is novel in the sense that it is not used as a region-of-interest method for the current image but rather as a noncombinatorial form of look-ahead in search for future viewpoint selection. Additionally, the integration of these three attentional schemes within a single framework is unique and not previously studied. We examine our proposed method in scenarios where little or no information regarding the environment is available. Through extensive experiments on a mobile robot, we show that our method improves visual search performance by reducing the time and number of actions required.



### The Effect of Color Space Selection on Detectability and Discriminability of Colored Objects
- **Arxiv ID**: http://arxiv.org/abs/1702.05421v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1702.05421v1)
- **Published**: 2017-02-14 17:13:30+00:00
- **Updated**: 2017-02-14 17:13:30+00:00
- **Authors**: Amir Rasouli, John K. Tsotsos
- **Comment**: submitted to CRV 2017
- **Journal**: None
- **Summary**: In this paper, we investigate the effect of color space selection on detectability and discriminability of colored objects under various conditions. 20 color spaces from the literature are evaluated on a large dataset of simulated and real images. We measure the suitability of color spaces from two different perspectives: detectability and discriminability of various color groups. Through experimental evaluation, we found that there is no single optimal color space suitable for all color groups. The color spaces have different levels of sensitivity to different color groups and they are useful depending on the color of the sought object. Overall, the best results were achieved in both simulated and real images using color spaces C1C2C3, UVW and XYZ. In addition, using a simulated environment, we show a practical application of color space selection in the context of top-down control in active visual search. The results indicate that on average color space C1C2C3 followed by HSI and XYZ achieve the best time in searching for objects of various colors. Here, the right choice of color space can improve time of search on average by 20%. As part of our contribution, we also introduce a large dataset of simulated 3D objects



### Enhanced Facial Recognition Framework based on Skin Tone and False Alarm Rejection
- **Arxiv ID**: http://arxiv.org/abs/1702.04377v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04377v1)
- **Published**: 2017-02-14 20:21:09+00:00
- **Updated**: 2017-02-14 20:21:09+00:00
- **Authors**: Ali Sharifara, Mohd Shafry Mohd Rahim, Farhad Navabifar, Dylan Ebert, Amir Ghaderi, Michalis Papakostas
- **Comment**: None
- **Journal**: None
- **Summary**: Face detection is one of the challenging tasks in computer vision. Human face detection plays an essential role in the first stage of face processing applications such as face recognition, face tracking, image database management, etc. In these applications, face objects often come from an inconsequential part of images that contain variations, namely different illumination, poses, and occlusion. These variations can decrease face detection rate noticeably. Most existing face detection approaches are not accurate, as they have not been able to resolve unstructured images due to large appearance variations and can only detect human faces under one particular variation. Existing frameworks of face detection need enhancements to detect human faces under the stated variations to improve detection rate and reduce detection time. In this study, an enhanced face detection framework is proposed to improve detection rate based on skin color and provide a validation process. A preliminary segmentation of the input images based on skin color can significantly reduce search space and accelerate the process of human face detection. The primary detection is based on Haar-like features and the Adaboost algorithm. A validation process is introduced to reject non-face objects, which might occur during the face detection process. The validation process is based on two-stage Extended Local Binary Patterns. The experimental results on the CMU-MIT and Caltech 10000 datasets over a wide range of facial variations in different colors, positions, scales, and lighting conditions indicated a successful face detection rate.



### ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/1702.04405v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04405v2)
- **Published**: 2017-02-14 22:08:03+00:00
- **Updated**: 2017-04-11 08:09:33+00:00
- **Authors**: Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nießner
- **Comment**: None
- **Journal**: None
- **Summary**: A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available -- current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval. The dataset is freely available at http://www.scan-net.org.



