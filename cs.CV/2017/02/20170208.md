# Arxiv Papers in cs.CV on 2017-02-08
### Automated Low-cost Terrestrial Laser Scanner for Measuring Diameters at Breast Height and Heights of Forest Trees
- **Arxiv ID**: http://arxiv.org/abs/1702.02235v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02235v2)
- **Published**: 2017-02-08 00:50:29+00:00
- **Updated**: 2017-05-09 03:52:49+00:00
- **Authors**: Pei Wang, Guochao Bu, Ronghao Li, Rui Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Terrestrial laser scanner is a kind of fast, high-precision data acquisition device, which had been more and more applied to the research areas of forest inventory. In this study, a kind of automated low-cost terrestrial laser scanner was designed and implemented based on a two-dimensional laser radar sensor SICK LMS-511 and a stepper motor. The new scanner was named as BEE, which can scan the forest trees in three dimension. The BEE scanner and its supporting software are specifically designed for forest inventory. The experiments have been performed by using the BEE scanner in an artificial ginkgo forest which was located in Haidian district of Beijing. Four square plots were selected to do the experiments. The BEE scanner scanned in the four plots and acquired the single scan data respectively. The DBH, tree height and tree position of trees in the four plots were estimated and analyzed. For comparison, the manual measured data was also collected in the four plots. The tree stem detection rate for all four plots was 92.75%; the root mean square error of the DBH estimation was 1.27cm; the root mean square error of the tree height estimation was 0.24m; the tree position estimation was in line with the actual position. Experimental results show that the BEE scanner can efficiently estimate the structure parameters of forest trees and has a good potential in practical application of forest inventory.



### Generating Multiple Diverse Hypotheses for Human 3D Pose Consistent with 2D Joint Detections
- **Arxiv ID**: http://arxiv.org/abs/1702.02258v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1702.02258v2)
- **Published**: 2017-02-08 02:54:25+00:00
- **Updated**: 2017-08-20 19:39:19+00:00
- **Authors**: Ehsan Jahangiri, Alan L. Yuille
- **Comment**: accepted to ICCV 2017 (PeopleCap)
- **Journal**: None
- **Summary**: We propose a method to generate multiple diverse and valid human pose hypotheses in 3D all consistent with the 2D detection of joints in a monocular RGB image. We use a novel generative model uniform (unbiased) in the space of anatomically plausible 3D poses. Our model is compositional (produces a pose by combining parts) and since it is restricted only by anatomical constraints it can generalize to every plausible human 3D pose. Removing the model bias intrinsically helps to generate more diverse 3D pose hypotheses. We argue that generating multiple pose hypotheses is more reasonable than generating only a single 3D pose based on the 2D joint detection given the depth ambiguity and the uncertainty due to occlusion and imperfect 2D joint detection. We hope that the idea of generating multiple consistent pose hypotheses can give rise to a new line of future work that has not received much attention in the literature. We used the Human3.6M dataset for empirical evaluation.



### Guided Optical Flow Learning
- **Arxiv ID**: http://arxiv.org/abs/1702.02295v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02295v2)
- **Published**: 2017-02-08 05:42:09+00:00
- **Updated**: 2017-07-01 18:12:09+00:00
- **Authors**: Yi Zhu, Zhenzhong Lan, Shawn Newsam, Alexander G. Hauptmann
- **Comment**: CVPR17 Workshop. Code available at
  https://github.com/bryanyzhu/GuidedNet
- **Journal**: None
- **Summary**: We study the unsupervised learning of CNNs for optical flow estimation using proxy ground truth data. Supervised CNNs, due to their immense learning capacity, have shown superior performance on a range of computer vision problems including optical flow prediction. They however require the ground truth flow which is usually not accessible except on limited synthetic data. Without the guidance of ground truth optical flow, unsupervised CNNs often perform worse as they are naturally ill-conditioned. We therefore propose a novel framework in which proxy ground truth data generated from classical approaches is used to guide the CNN learning. The models are further refined in an unsupervised fashion using an image reconstruction loss. Our guided learning approach is competitive with or superior to state-of-the-art approaches on three standard benchmark datasets yet is completely unsupervised and can run in real time.



### Multi-scale Convolutional Neural Networks for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1702.02359v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02359v1)
- **Published**: 2017-02-08 10:30:32+00:00
- **Updated**: 2017-02-08 10:30:32+00:00
- **Authors**: Lingke Zeng, Xiangmin Xu, Bolun Cai, Suo Qiu, Tong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd counting on static images is a challenging problem due to scale variations. Recently deep neural networks have been shown to be effective in this task. However, existing neural-networks-based methods often use the multi-column or multi-network model to extract the scale-relevant features, which is more complicated for optimization and computation wasting. To this end, we propose a novel multi-scale convolutional neural network (MSCNN) for single image crowd counting. Based on the multi-scale blobs, the network is able to generate scale-relevant features for higher crowd counting performances in a single-column architecture, which is both accuracy and cost effective for practical applications. Complemental results show that our method outperforms the state-of-the-art methods on both accuracy and robustness with far less number of parameters.



### An Adversarial Regularisation for Semi-Supervised Training of Structured Output Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.02382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02382v1)
- **Published**: 2017-02-08 11:40:15+00:00
- **Updated**: 2017-02-08 11:40:15+00:00
- **Authors**: Mateusz Koziński, Loïc Simon, Frédéric Jurie
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method for semi-supervised training of structured-output neural networks. Inspired by the framework of Generative Adversarial Networks (GAN), we train a discriminator network to capture the notion of a quality of network output. To this end, we leverage the qualitative difference between outputs obtained on the labelled training data and unannotated data. We then use the discriminator as a source of error signal for unlabelled data. This effectively boosts the performance of a network on a held out test set. Initial experiments in image segmentation demonstrate that the proposed framework enables achieving the same network performance as in a fully supervised scenario, while using two times less annotations.



### Scene-adapted plug-and-play algorithm with convergence guarantees
- **Arxiv ID**: http://arxiv.org/abs/1702.02445v2
- **DOI**: None
- **Categories**: **cs.CV**, 94A08, 68U10, 47N10, I.4.5; I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/1702.02445v2)
- **Published**: 2017-02-08 14:42:05+00:00
- **Updated**: 2017-11-08 14:02:14+00:00
- **Authors**: Afonso M. Teodoro, José M. Bioucas-Dias, Mário A. T. Figueiredo
- **Comment**: None
- **Journal**: None
- **Summary**: Recent frameworks, such as the so-called plug-and-play, allow us to leverage the developments in image denoising to tackle other, and more involved, problems in image processing. As the name suggests, state-of-the-art denoisers are plugged into an iterative algorithm that alternates between a denoising step and the inversion of the observation operator. While these tools offer flexibility, the convergence of the resulting algorithm may be difficult to analyse. In this paper, we plug a state-of-the-art denoiser, based on a Gaussian mixture model, in the iterations of an alternating direction method of multipliers and prove the algorithm is guaranteed to converge. Moreover, we build upon the concept of scene-adapted priors where we learn a model targeted to a specific scene being imaged, and apply the proposed method to address the hyperspectral sharpening problem.



### Region Ensemble Network: Improving Convolutional Network for Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1702.02447v2
- **DOI**: 10.1109/ICIP.2017.8297136
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02447v2)
- **Published**: 2017-02-08 14:44:31+00:00
- **Updated**: 2017-05-09 03:07:31+00:00
- **Authors**: Hengkai Guo, Guijin Wang, Xinghao Chen, Cairong Zhang, Fei Qiao, Huazhong Yang
- **Comment**: Accepted to ICIP 2017. Project:
  https://github.com/guohengkai/region-ensemble-network
- **Journal**: None
- **Summary**: Hand pose estimation from monocular depth images is an important and challenging problem for human-computer interaction. Recently deep convolutional networks (ConvNet) with sophisticated design have been employed to address it, but the improvement over traditional methods is not so apparent. To promote the performance of directly 3D coordinate regression, we propose a tree-structured Region Ensemble Network (REN), which partitions the convolution outputs into regions and integrates the results from multiple regressors on each regions. Compared with multi-model ensemble, our model is completely end-to-end training. The experimental results demonstrate that our approach achieves the best performance among state-of-the-arts on two public datasets.



### Video Frame Synthesis using Deep Voxel Flow
- **Arxiv ID**: http://arxiv.org/abs/1702.02463v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1702.02463v2)
- **Published**: 2017-02-08 15:20:14+00:00
- **Updated**: 2017-08-05 04:43:44+00:00
- **Authors**: Ziwei Liu, Raymond A. Yeh, Xiaoou Tang, Yiming Liu, Aseem Agarwala
- **Comment**: To appear in ICCV 2017 as an oral paper. More details at the project
  page: https://liuziwei7.github.io/projects/VoxelFlow.html
- **Journal**: None
- **Summary**: We address the problem of synthesizing new video frames in an existing video, either in-between existing frames (interpolation), or subsequent to them (extrapolation). This problem is challenging because video appearance and motion can be highly complex. Traditional optical-flow-based solutions often fail where flow estimation is challenging, while newer neural-network-based methods that hallucinate pixel values directly often produce blurry results. We combine the advantages of these two methods by training a deep network that learns to synthesize video frames by flowing pixel values from existing ones, which we call deep voxel flow. Our method requires no human supervision, and any video can be used as training data by dropping, and then learning to predict, existing frames. The technique is efficient, and can be applied at any video resolution. We demonstrate that our method produces results that both quantitatively and qualitatively improve upon the state-of-the-art.



### Monocular LSD-SLAM Integration within AR System
- **Arxiv ID**: http://arxiv.org/abs/1702.02514v1
- **DOI**: 10.13140/RG.2.2.10054.27205
- **Categories**: **cs.CV**, cs.GR, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/1702.02514v1)
- **Published**: 2017-02-08 16:52:19+00:00
- **Updated**: 2017-02-08 16:52:19+00:00
- **Authors**: Markus Höll, Vincent Lepetit
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we cover the process of integrating Large-Scale Direct Simultaneous Localization and Mapping (LSD-SLAM) algorithm into our existing AR stereo engine, developed for our modified "Augmented Reality Oculus Rift". With that, we are able to track one of our realworld cameras which are mounted on the rift, within a complete unknown environment. This makes it possible to achieve a constant and full augmentation, synchronizing our 3D movement (x, y, z) in both worlds, the real world and the virtual world. The development for the basic AR setup using the Oculus Rift DK1 and two fisheye cameras is fully documented in our previous paper. After an introduction to image-based registration, we detail the LSD-SLAM algorithm and document our code implementing our integration. The AR stereo engine with Oculus Rift support can be accessed via the GIT repository https://github.com/MaXvanHeLL/ARift.git and the modified LSD-SLAM project used for the integration is available here https://github.com/MaXvanHeLL/LSD-SLAM.git.



### Soft Biometrics: Gender Recognition from Unconstrained Face Images using Local Feature Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1702.02537v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02537v1)
- **Published**: 2017-02-08 17:34:53+00:00
- **Updated**: 2017-02-08 17:34:53+00:00
- **Authors**: Olasimbo Ayodeji Arigbabu, Sharifah Mumtazah Syed Ahmad, Wan Azizun Wan Adnan, Salman Yussof, Saif Mahmood
- **Comment**: None
- **Journal**: Journal of Information and Communication Technology (JICT), 2015
- **Summary**: Gender recognition from unconstrained face images is a challenging task due to the high degree of misalignment, pose, expression, and illumination variation. In previous works, the recognition of gender from unconstrained face images is approached by utilizing image alignment, exploiting multiple samples per individual to improve the learning ability of the classifier, or learning gender based on prior knowledge about pose and demographic distributions of the dataset. However, image alignment increases the complexity and time of computation, while the use of multiple samples or having prior knowledge about data distribution is unrealistic in practical applications. This paper presents an approach for gender recognition from unconstrained face images. Our technique exploits the robustness of local feature descriptor to photometric variations to extract the shape description of the 2D face image using a single sample image per individual. The results obtained from experiments on Labeled Faces in the Wild (LFW) dataset describe the effectiveness of the proposed method. The essence of this study is to investigate the most suitable functions and parameter settings for recognizing gender from unconstrained face images.



### Backpropagation Training for Fisher Vectors within Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.02549v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02549v1)
- **Published**: 2017-02-08 18:19:05+00:00
- **Updated**: 2017-02-08 18:19:05+00:00
- **Authors**: Patrick Wieschollek, Fabian Groh, Hendrik P. A. Lensch
- **Comment**: None
- **Journal**: None
- **Summary**: Fisher-Vectors (FV) encode higher-order statistics of a set of multiple local descriptors like SIFT features. They already show good performance in combination with shallow learning architectures on visual recognitions tasks. Current methods using FV as a feature descriptor in deep architectures assume that all original input features are static. We propose a framework to jointly learn the representation of original features, FV parameters and parameters of the classifier in the style of traditional neural networks. Our proof of concept implementation improves the performance of FV on the Pascal Voc 2007 challenge in a multi-GPU setting in comparison to a default SVM setting. We demonstrate that FV can be embedded into neural networks at arbitrary positions, allowing end-to-end training with back-propagation.



