# Arxiv Papers in cs.CV on 2017-02-13
### Underwater Optical Image Processing: A Comprehensive Review
- **Arxiv ID**: http://arxiv.org/abs/1702.03600v1
- **DOI**: None
- **Categories**: **cs.CV**, 78
- **Links**: [PDF](http://arxiv.org/pdf/1702.03600v1)
- **Published**: 2017-02-13 00:40:59+00:00
- **Updated**: 2017-02-13 00:40:59+00:00
- **Authors**: Huimin Lu, Yujie Li, Yudong Zhang, Min Chen, Seiichi Serikawa, Hyoungseop Kim
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Underwater cameras are widely used to observe the sea floor. They are usually included in autonomous underwater vehicles, unmanned underwater vehicles, and in situ ocean sensor networks. Despite being an important sensor for monitoring underwater scenes, there exist many issues with recent underwater camera sensors. Because of lights transportation characteristics in water and the biological activity at the sea floor, the acquired underwater images often suffer from scatters and large amounts of noise. Over the last five years, many methods have been proposed to overcome traditional underwater imaging problems. This paper aims to review the state-of-the-art techniques in underwater image processing by highlighting the contributions and challenges presented in over 40 papers. We present an overview of various underwater image processing approaches, such as underwater image descattering, underwater image color restoration, and underwater image quality assessments. Finally, we summarize the future trends and challenges in designing and processing underwater imaging sensors.



### Unsupervised temporal context learning using convolutional neural networks for laparoscopic workflow analysis
- **Arxiv ID**: http://arxiv.org/abs/1702.03684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03684v1)
- **Published**: 2017-02-13 09:29:50+00:00
- **Updated**: 2017-02-13 09:29:50+00:00
- **Authors**: Sebastian Bodenstedt, Martin Wagner, Darko Katić, Patrick Mietkowski, Benjamin Mayer, Hannes Kenngott, Beat Müller-Stich, Rüdiger Dillmann, Stefanie Speidel
- **Comment**: None
- **Journal**: None
- **Summary**: Computer-assisted surgery (CAS) aims to provide the surgeon with the right type of assistance at the right moment. Such assistance systems are especially relevant in laparoscopic surgery, where CAS can alleviate some of the drawbacks that surgeons incur. For many assistance functions, e.g. displaying the location of a tumor at the appropriate time or suggesting what instruments to prepare next, analyzing the surgical workflow is a prerequisite. Since laparoscopic interventions are performed via endoscope, the video signal is an obvious sensor modality to rely on for workflow analysis.   Image-based workflow analysis tasks in laparoscopy, such as phase recognition, skill assessment, video indexing or automatic annotation, require a temporal distinction between video frames. Generally computer vision based methods that generalize from previously seen data are used. For training such methods, large amounts of annotated data are necessary. Annotating surgical data requires expert knowledge, therefore collecting a sufficient amount of data is difficult, time-consuming and not always feasible.   In this paper, we address this problem by presenting an unsupervised method for training a convolutional neural network (CNN) to differentiate between laparoscopic video frames on a temporal basis. We extract video frames at regular intervals from 324 unlabeled laparoscopic interventions, resulting in a dataset of approximately 2.2 million images. From this dataset, we extract image pairs from the same video and train a CNN to determine their temporal order. To solve this problem, the CNN has to extract features that are relevant for comprehending laparoscopic workflow.   Furthermore, we demonstrate that such a CNN can be adapted for surgical workflow segmentation. We performed image-based workflow segmentation on a publicly available dataset of 7 cholecystectomies and 9 colorectal interventions.



### An Efficient Decomposition Framework for Discriminative Segmentation with Supermodular Losses
- **Arxiv ID**: http://arxiv.org/abs/1702.03690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03690v1)
- **Published**: 2017-02-13 09:49:35+00:00
- **Updated**: 2017-02-13 09:49:35+00:00
- **Authors**: Jiaqian Yu, Matthew B. Blaschko
- **Comment**: None
- **Journal**: None
- **Summary**: Several supermodular losses have been shown to improve the perceptual quality of image segmentation in a discriminative framework such as a structured output support vector machine (SVM). These loss functions do not necessarily have the same structure as the one used by the segmentation inference algorithm, and in general, we may have to resort to generic submodular minimization algorithms for loss augmented inference. Although these come with polynomial time guarantees, they are not practical to apply to image scale data. Many supermodular losses come with strong optimization guarantees, but are not readily incorporated in a loss augmented graph cuts procedure. This motivates our strategy of employing the alternating direction method of multipliers (ADMM) decomposition for loss augmented inference. In doing so, we create a new API for the structured SVM that separates the maximum a posteriori (MAP) inference of the model from the loss augmentation during training. In this way, we gain computational efficiency, making new choices of loss functions practical for the first time, while simultaneously making the inference algorithm employed during training closer to the test time procedure. We show improvement both in accuracy and computational performance on the Microsoft Research Grabcut database and a brain structure segmentation task, empirically validating the use of several supermodular loss functions during training, and the improved computational properties of the proposed ADMM approach over the Fujishige-Wolfe minimum norm point algorithm.



### Estimation of the volume of the left ventricle from MRI images using deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1702.03833v1
- **DOI**: 10.1109/TCYB.2017.2778799
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03833v1)
- **Published**: 2017-02-13 15:43:52+00:00
- **Updated**: 2017-02-13 15:43:52+00:00
- **Authors**: Fangzhou Liao, Xi Chen, Xiaolin Hu, Sen Song
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: Segmenting human left ventricle (LV) in magnetic resonance imaging (MRI) images and calculating its volume are important for diagnosing cardiac diseases. In 2016, Kaggle organized a competition to estimate the volume of LV from MRI images. The dataset consisted of a large number of cases, but only provided systole and diastole volumes as labels. We designed a system based on neural networks to solve this problem. It began with a detector combined with a neural network classifier for detecting regions of interest (ROIs) containing LV chambers. Then a deep neural network named hypercolumns fully convolutional network was used to segment LV in ROIs. The 2D segmentation results were integrated across different images to estimate the volume. With ground-truth volume labels, this model was trained end-to-end. To improve the result, an additional dataset with only segmentation label was used. The model was trained alternately on these two datasets with different types of teaching signals. We also proposed a variance estimation method for the final prediction. Our algorithm ranked the 4th on the test set in this competition.



### Cognitive Mapping and Planning for Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/1702.03920v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1702.03920v3)
- **Published**: 2017-02-13 18:52:04+00:00
- **Updated**: 2019-02-07 18:54:58+00:00
- **Authors**: Saurabh Gupta, Varun Tolani, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik
- **Comment**: Extended IJCV Version of the original paper at CVPR17. Project
  website with code, models, simulation environment and videos:
  https://sites.google.com/view/cognitive-mapping-and-planning/
- **Journal**: None
- **Summary**: We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the task, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. We train and test CMP on navigation problems in simulation environments derived from scans of real world buildings. Our experiments demonstrate that CMP outperforms alternate learning-based architectures, as well as, classical mapping and path planning approaches in many cases. Furthermore, it naturally extends to semantically specified goals, such as 'going to a chair'. We also deploy CMP on physical robots in indoor environments, where it achieves reasonable performance, even though it is trained entirely in simulation.



### End-to-End Interpretation of the French Street Name Signs Dataset
- **Arxiv ID**: http://arxiv.org/abs/1702.03970v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03970v1)
- **Published**: 2017-02-13 20:18:18+00:00
- **Updated**: 2017-02-13 20:18:18+00:00
- **Authors**: Raymond Smith, Chunhui Gu, Dar-Shyang Lee, Huiyi Hu, Ranjith Unnikrishnan, Julian Ibarz, Sacha Arnoud, Sophia Lin
- **Comment**: Presented at the IWRR workshop at ECCV 2016
- **Journal**: Computer Vision - ECCV 2016 Workshops Volume 9913 of the series
  Lecture Notes in Computer Science pp 411-426
- **Summary**: We introduce the French Street Name Signs (FSNS) Dataset consisting of more than a million images of street name signs cropped from Google Street View images of France. Each image contains several views of the same street name sign. Every image has normalized, title case folded ground-truth text as it would appear on a map. We believe that the FSNS dataset is large and complex enough to train a deep network of significant complexity to solve the street name extraction problem "end-to-end" or to explore the design trade-offs between a single complex engineered network and multiple sub-networks designed and trained to solve sub-problems. We present such an "end-to-end" network/graph for Tensor Flow and its results on the FSNS dataset.



