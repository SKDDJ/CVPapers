# Arxiv Papers in cs.CV on 2017-02-16
### Chord Angle Deviation using Tangent (CADT), an Efficient and Robust Contour-based Corner Detector
- **Arxiv ID**: http://arxiv.org/abs/1702.04843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04843v1)
- **Published**: 2017-02-16 02:52:36+00:00
- **Updated**: 2017-02-16 02:52:36+00:00
- **Authors**: Mohammad Asiful Hossain, Abdul Kawsar Tushar
- **Comment**: Conference Name - 2017 IEEE International Conference on Imaging,
  Vision & Pattern Recognition (icIVPR17); Conference Date - 13 Feb, 2017;
  Conference Venue - University of Dhaka, Dhaka, Bangladesh
- **Journal**: None
- **Summary**: Detection of corner is the most essential process in a large number of computer vision and image processing applications. We have mentioned a number of popular contour-based corner detectors in our paper. Among all these detectors chord to triangular arm angle (CTAA) has been demonstrated as the most dominant corner detector in terms of average repeatability. We introduce a new effective method to calculate the value of curvature in this paper. By demonstrating experimental results, our proposed technique outperforms CTAA and other detectors mentioned in this paper. The results exhibit that our proposed method is simple yet efficient at finding out corners more accurately and reliably.



### Deep Hybrid Similarity Learning for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1702.04858v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04858v2)
- **Published**: 2017-02-16 04:59:01+00:00
- **Updated**: 2017-02-17 10:36:38+00:00
- **Authors**: Jianqing Zhu, Huanqiang Zeng, Shengcai Liao, Zhen Lei, Canhui Cai, LiXin Zheng
- **Comment**: 10 pages, 12 figures, fixed typo errors in Fig.8
- **Journal**: None
- **Summary**: Person Re-IDentification (Re-ID) aims to match person images captured from two non-overlapping cameras. In this paper, a deep hybrid similarity learning (DHSL) method for person Re-ID based on a convolution neural network (CNN) is proposed. In our approach, a CNN learning feature pair for the input image pair is simultaneously extracted. Then, both the element-wise absolute difference and multiplication of the CNN learning feature pair are calculated. Finally, a hybrid similarity function is designed to measure the similarity between the feature pair, which is realized by learning a group of weight coefficients to project the element-wise absolute difference and multiplication into a similarity score. Consequently, the proposed DHSL method is able to reasonably assign parameters of feature learning and metric learning in a CNN so that the performance of person Re-ID is improved. Experiments on three challenging person Re-ID databases, QMUL GRID, VIPeR and CUHK03, illustrate that the proposed DHSL method is superior to multiple state-of-the-art person Re-ID methods.



### Improving automated multiple sclerosis lesion segmentation with a cascaded 3D convolutional neural network approach
- **Arxiv ID**: http://arxiv.org/abs/1702.04869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04869v1)
- **Published**: 2017-02-16 06:23:14+00:00
- **Updated**: 2017-02-16 06:23:14+00:00
- **Authors**: Sergi Valverde, Mariano Cabezas, Eloy Roura, Sandra González-Villà, Deborah Pareto, Joan-Carles Vilanova, LLuís Ramió-Torrentà, Àlex Rovira, Arnau Oliver, Xavier Lladó
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel automated method for White Matter (WM) lesion segmentation of Multiple Sclerosis (MS) patient images. Our approach is based on a cascade of two 3D patch-wise convolutional neural networks (CNN). The first network is trained to be more sensitive revealing possible candidate lesion voxels while the second network is trained to reduce the number of misclassified voxels coming from the first network. This cascaded CNN architecture tends to learn well from small sets of training data, which can be very interesting in practice, given the difficulty to obtain manual label annotations and the large amount of available unlabeled Magnetic Resonance Imaging (MRI) data. We evaluate the accuracy of the proposed method on the public MS lesion segmentation challenge MICCAI2008 dataset, comparing it with respect to other state-of-the-art MS lesion segmentation tools. Furthermore, the proposed method is also evaluated on two private MS clinical datasets, where the performance of our method is also compared with different recent public available state-of-the-art MS lesion segmentation methods. At the time of writing this paper, our method is the best ranked approach on the MICCAI2008 challenge, outperforming the rest of 60 participant methods when using all the available input modalities (T1-w, T2-w and FLAIR), while still in the top-rank (3rd position) when using only T1-w and FLAIR modalities. On clinical MS data, our approach exhibits a significant increase in the accuracy segmenting of WM lesions when compared with the rest of evaluated methods, highly correlating ($r \ge 0.97$) also with the expected lesion volume.



### Discovering objects and their relations from entangled scene representations
- **Arxiv ID**: http://arxiv.org/abs/1702.05068v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.05068v1)
- **Published**: 2017-02-16 18:08:27+00:00
- **Updated**: 2017-02-16 18:08:27+00:00
- **Authors**: David Raposo, Adam Santoro, David Barrett, Razvan Pascanu, Timothy Lillicrap, Peter Battaglia
- **Comment**: ICLR Workshop 2017
- **Journal**: None
- **Summary**: Our world can be succinctly and compactly described as structured scenes of objects and relations. A typical room, for example, contains salient objects such as tables, chairs and books, and these objects typically relate to each other by their underlying causes and semantics. This gives rise to correlated features, such as position, function and shape. Humans exploit knowledge of objects and their relations for learning a wide spectrum of tasks, and more generally when learning the structure underlying observed data. In this work, we introduce relation networks (RNs) - a general purpose neural network architecture for object-relation reasoning. We show that RNs are capable of learning object relations from scene description data. Furthermore, we show that RNs can act as a bottleneck that induces the factorization of objects from entangled scene description inputs, and from distributed deep representations of scene images provided by a variational autoencoder. The model can also be used in conjunction with differentiable memory mechanisms for implicit relation discovery in one-shot learning tasks. Our results suggest that relation networks are a potentially powerful architecture for solving a variety of problems that require object relation reasoning.



### KEPLER: Keypoint and Pose Estimation of Unconstrained Faces by Learning Efficient H-CNN Regressors
- **Arxiv ID**: http://arxiv.org/abs/1702.05085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05085v1)
- **Published**: 2017-02-16 18:44:59+00:00
- **Updated**: 2017-02-16 18:44:59+00:00
- **Authors**: Amit Kumar, Azadeh Alavi, Rama Chellappa
- **Comment**: Accept as Oral FG'17
- **Journal**: None
- **Summary**: Keypoint detection is one of the most important pre-processing steps in tasks such as face modeling, recognition and verification. In this paper, we present an iterative method for Keypoint Estimation and Pose prediction of unconstrained faces by Learning Efficient H-CNN Regressors (KEPLER) for addressing the face alignment problem. Recent state of the art methods have shown improvements in face keypoint detection by employing Convolution Neural Networks (CNNs). Although a simple feed forward neural network can learn the mapping between input and output spaces, it cannot learn the inherent structural dependencies. We present a novel architecture called H-CNN (Heatmap-CNN) which captures structured global and local features and thus favors accurate keypoint detecion. HCNN is jointly trained on the visibility, fiducials and 3D-pose of the face. As the iterations proceed, the error decreases making the gradients small and thus requiring efficient training of DCNNs to mitigate this. KEPLER performs global corrections in pose and fiducials for the first four iterations followed by local corrections in the subsequent stage. As a by-product, KEPLER also provides 3D pose (pitch, yaw and roll) of the face accurately. In this paper, we show that without using any 3D information, KEPLER outperforms state of the art methods for alignment on challenging datasets such as AFW and AFLW.



### Improving Text Proposals for Scene Images with Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.05089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05089v1)
- **Published**: 2017-02-16 18:56:53+00:00
- **Updated**: 2017-02-16 18:56:53+00:00
- **Authors**: Dena Bazazian, Raul Gomez, Anguelos Nicolaou, Lluis Gomez, Dimosthenis Karatzas, Andrew D. Bagdanov
- **Comment**: 6 pages, 8 figures, International Conference on Pattern Recognition
  (ICPR) - DLPR (Deep Learning for Pattern Recognition) workshop
- **Journal**: None
- **Summary**: Text Proposals have emerged as a class-dependent version of object proposals - efficient approaches to reduce the search space of possible text object locations in an image. Combined with strong word classifiers, text proposals currently yield top state of the art results in end-to-end scene text recognition. In this paper we propose an improvement over the original Text Proposals algorithm of Gomez and Karatzas (2016), combining it with Fully Convolutional Networks to improve the ranking of proposals. Results on the ICDAR RRC and the COCO-text datasets show superior performance over current state-of-the-art.



### Automatic Handgun Detection Alarm in Videos Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1702.05147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05147v1)
- **Published**: 2017-02-16 20:30:33+00:00
- **Updated**: 2017-02-16 20:30:33+00:00
- **Authors**: Roberto Olmos, Siham Tabik, Francisco Herrera
- **Comment**: None
- **Journal**: None
- **Summary**: Current surveillance and control systems still require human supervision and intervention. This work presents a novel automatic handgun detection system in videos appropriate for both, surveillance and control purposes. We reformulate this detection problem into the problem of minimizing false positives and solve it by building the key training data-set guided by the results of a deep Convolutional Neural Networks (CNN) classifier, then assessing the best classification model under two approaches, the sliding window approach and region proposal approach. The most promising results are obtained by Faster R-CNN based model trained on our new database. The best detector show a high potential even in low quality youtube videos and provides satisfactory results as automatic alarm system. Among 30 scenes, it successfully activates the alarm after five successive true positives in less than 0.2 seconds, in 27 scenes. We also define a new metric, Alarm Activation per Interval (AApI), to assess the performance of a detection model as an automatic detection system in videos.



### BubbleView: an interface for crowdsourcing image importance maps and tracking visual attention
- **Arxiv ID**: http://arxiv.org/abs/1702.05150v3
- **DOI**: 10.1145/3131275
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.05150v3)
- **Published**: 2017-02-16 20:49:26+00:00
- **Updated**: 2017-08-09 14:23:54+00:00
- **Authors**: Nam Wook Kim, Zoya Bylinskii, Michelle A. Borkin, Krzysztof Z. Gajos, Aude Oliva, Fredo Durand, Hanspeter Pfister
- **Comment**: None
- **Journal**: TOCHI 2017
- **Summary**: In this paper, we present BubbleView, an alternative methodology for eye tracking using discrete mouse clicks to measure which information people consciously choose to examine. BubbleView is a mouse-contingent, moving-window interface in which participants are presented with a series of blurred images and click to reveal "bubbles" - small, circular areas of the image at original resolution, similar to having a confined area of focus like the eye fovea. Across 10 experiments with 28 different parameter combinations, we evaluated BubbleView on a variety of image types: information visualizations, natural images, static webpages, and graphic designs, and compared the clicks to eye fixations collected with eye-trackers in controlled lab settings. We found that BubbleView clicks can both (i) successfully approximate eye fixations on different images, and (ii) be used to rank image and design elements by importance. BubbleView is designed to collect clicks on static images, and works best for defined tasks such as describing the content of an information visualization or measuring image importance. BubbleView data is cleaner and more consistent than related methodologies that use continuous mouse movements. Our analyses validate the use of mouse-contingent, moving-window methodologies as approximating eye fixations for different image and task types.



### An Analysis of Parallelized Motion Masking Using Dual-Mode Single Gaussian Models
- **Arxiv ID**: http://arxiv.org/abs/1702.05156v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1702.05156v1)
- **Published**: 2017-02-16 21:10:13+00:00
- **Updated**: 2017-02-16 21:10:13+00:00
- **Authors**: Peter Henderson, Matthew Vertescher
- **Comment**: None
- **Journal**: None
- **Summary**: Motion detection in video is important for a number of applications and fields. In video surveillance, motion detection is an essential accompaniment to activity recognition for early warning systems. Robotics also has much to gain from motion detection and segmentation, particularly in high speed motion tracking for tactile systems. There are a myriad of techniques for detecting and masking motion in an image. Successful systems have used Gaussian Models to discern background from foreground in an image (motion from static imagery). However, particularly in the case of a moving camera or frame of reference, it is necessary to compensate for the motion of the camera when attempting to discern objects moving in the foreground. For example, it is possible to estimate motion of the camera through optical flow methods or temporal differencing and then compensate for this motion in a background subtraction model. We selection a method by Yi et al. using Dual-Mode Single Gaussian Models which does just this. We implement the technique in Intel's Thread Building Blocks (TBB) and NVIDIA's CUDA libraries. We then compare parallelization improvements with a theoretical analysis of speedups based on the characteristics of our selected model and attributes of both TBB and CUDA. We make our implementation available to the public.



### Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1702.05174v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05174v1)
- **Published**: 2017-02-16 22:10:37+00:00
- **Updated**: 2017-02-16 22:10:37+00:00
- **Authors**: Michal Drozdzal, Gabriel Chartrand, Eugene Vorontsov, Lisa Di Jorio, An Tang, Adriana Romero, Yoshua Bengio, Chris Pal, Samuel Kadoury
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a simple, yet powerful pipeline for medical image segmentation that combines Fully Convolutional Networks (FCNs) with Fully Convolutional Residual Networks (FC-ResNets). We propose and examine a design that takes particular advantage of recent advances in the understanding of both Convolutional Neural Networks as well as ResNets. Our approach focuses upon the importance of a trainable pre-processing when using FC-ResNets and we show that a low-capacity FCN model can serve as a pre-processor to normalize medical input data. In our image segmentation pipeline, we use FCNs to obtain normalized images, which are then iteratively refined by means of a FC-ResNet to generate a segmentation prediction. As in other fully convolutional approaches, our pipeline can be used off-the-shelf on different image modalities. We show that using this pipeline, we exhibit state-of-the-art performance on the challenging Electron Microscopy benchmark, when compared to other 2D methods. We improve segmentation results on CT images of liver lesions, when contrasting with standard FCN methods. Moreover, when applying our 2D pipeline on a challenging 3D MRI prostate segmentation challenge we reach results that are competitive even when compared to 3D methods. The obtained results illustrate the strong potential and versatility of the pipeline by achieving highly accurate results on multi-modality images from different anatomical regions and organs.



