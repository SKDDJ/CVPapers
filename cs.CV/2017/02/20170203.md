# Arxiv Papers in cs.CV on 2017-02-03
### Seeded Laplaican: An Eigenfunction Solution for Scribble Based Interactive Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1702.00882v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00882v2)
- **Published**: 2017-02-03 01:13:07+00:00
- **Updated**: 2017-08-27 20:12:37+00:00
- **Authors**: Ahmed Taha, Marwan Torki
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we cast the scribble-based interactive image segmentation as a semi-supervised learning problem. Our novel approach alleviates the need to solve an expensive generalized eigenvector problem by approximating the eigenvectors using efficiently computed eigenfunctions. The smoothness operator defined on feature densities at the limit n tends to infinity recovers the exact eigenvectors of the graph Laplacian, where n is the number of nodes in the graph. To further reduce the computational complexity without scarifying our accuracy, we select pivots pixels from user annotations. In our experiments, we evaluate our approach using both human scribble and "robot user" annotations to guide the foreground/background segmentation. We developed a new unbiased collection of five annotated images datasets to standardize the evaluation procedure for any scribble-based segmentation method. We experimented with several variations, including different feature vectors, pivot count and the number of eigenvectors. Experiments are carried out on datasets that contain a wide variety of natural images. We achieve better qualitative and quantitative results compared to state-of-the-art interactive segmentation algorithms.



### FCSS: Fully Convolutional Self-Similarity for Dense Semantic Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1702.00926v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00926v1)
- **Published**: 2017-02-03 07:36:44+00:00
- **Updated**: 2017-02-03 07:36:44+00:00
- **Authors**: Seungryong Kim, Dongbo Min, Bumsub Ham, Sangryul Jeon, Stephen Lin, Kwanghoon Sohn
- **Comment**: None
- **Journal**: None
- **Summary**: We present a descriptor, called fully convolutional self-similarity (FCSS), for dense semantic correspondence. To robustly match points among different instances within the same object class, we formulate FCSS using local self-similarity (LSS) within a fully convolutional network. In contrast to existing CNN-based descriptors, FCSS is inherently insensitive to intra-class appearance variations because of its LSS-based structure, while maintaining the precise localization ability of deep neural networks. The sampling patterns of local structure and the self-similarity measure are jointly learned within the proposed network in an end-to-end and multi-scale manner. As training data for semantic correspondence is rather limited, we propose to leverage object candidate priors provided in existing image datasets and also correspondence consistency between object pairs to enable weakly-supervised learning. Experiments demonstrate that FCSS outperforms conventional handcrafted descriptors and CNN-based descriptors on various benchmarks.



### A method of limiting performance loss of CNNs in noisy environments
- **Arxiv ID**: http://arxiv.org/abs/1702.00932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00932v1)
- **Published**: 2017-02-03 08:21:46+00:00
- **Updated**: 2017-02-03 08:21:46+00:00
- **Authors**: James R. Geraci, Parichay Kapoor
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) recognition rates drop in the presence of noise. We demonstrate a novel method of counteracting this drop in recognition rate by adjusting the biases of the neurons in the convolutional layers according to the noise conditions encountered at runtime. We compare our technique to training one network for all possible noise levels, dehazing via preprocessing a signal with a denoising autoencoder, and training a network specifically for each noise level. Our system compares favorably in terms of robustness, computational complexity and recognition rate.



### Deep Learning with Low Precision by Half-wave Gaussian Quantization
- **Arxiv ID**: http://arxiv.org/abs/1702.00953v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1702.00953v1)
- **Published**: 2017-02-03 10:11:40+00:00
- **Updated**: 2017-02-03 10:11:40+00:00
- **Authors**: Zhaowei Cai, Xiaodong He, Jian Sun, Nuno Vasconcelos
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of quantizing the activations of a deep neural network is considered. An examination of the popular binary quantization approach shows that this consists of approximating a classical non-linearity, the hyperbolic tangent, by two functions: a piecewise constant sign function, which is used in feedforward network computations, and a piecewise linear hard tanh function, used in the backpropagation step during network learning. The problem of approximating the ReLU non-linearity, widely used in the recent deep learning literature, is then considered. An half-wave Gaussian quantizer (HWGQ) is proposed for forward approximation and shown to have efficient implementation, by exploiting the statistics of of network activations and batch normalization operations commonly used in the literature. To overcome the problem of gradient mismatch, due to the use of different forward and backward approximations, several piece-wise backward approximators are then investigated. The implementation of the resulting quantized network, denoted as HWGQ-Net, is shown to achieve much closer performance to full precision networks, such as AlexNet, ResNet, GoogLeNet and VGG-Net, than previously available low-precision networks, with 1-bit binary weights and 2-bit quantized activations.



### Intrinsic Grassmann Averages for Online Linear, Robust and Nonlinear Subspace Learning
- **Arxiv ID**: http://arxiv.org/abs/1702.01005v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.01005v2)
- **Published**: 2017-02-03 13:44:44+00:00
- **Updated**: 2018-07-10 02:29:39+00:00
- **Authors**: Rudrasis Chakraborty, SÃ¸ren Hauberg, Baba C. Vemuri
- **Comment**: None
- **Journal**: None
- **Summary**: Principal Component Analysis (PCA) and Kernel Principal Component Analysis (KPCA) are fundamental methods in machine learning for dimensionality reduction. The former is a technique for finding this approximation in finite dimensions and the latter is often in an infinite dimensional Reproducing Kernel Hilbert-space (RKHS). In this paper, we present a geometric framework for computing the principal linear subspaces in both situations as well as for the robust PCA case, that amounts to computing the intrinsic average on the space of all subspaces: the Grassmann manifold. Points on this manifold are defined as the subspaces spanned by $K$-tuples of observations. The intrinsic Grassmann average of these subspaces are shown to coincide with the principal components of the observations when they are drawn from a Gaussian distribution. We show similar results in the RKHS case and provide an efficient algorithm for computing the projection onto the this average subspace. The result is a method akin to KPCA which is substantially faster. Further, we present a novel online version of the KPCA using our geometric framework. Competitive performance of all our algorithms are demonstrated on a variety of real and synthetic data sets.



### Joint 2D-3D-Semantic Data for Indoor Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1702.01105v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1702.01105v2)
- **Published**: 2017-02-03 18:28:33+00:00
- **Updated**: 2017-04-06 01:46:13+00:00
- **Authors**: Iro Armeni, Sasha Sax, Amir R. Zamir, Silvio Savarese
- **Comment**: The dataset is available http://3Dsemantics.stanford.edu/
- **Journal**: None
- **Summary**: We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000m2 and contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360{\deg} equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces. The dataset is available here: http://3Dsemantics.stanford.edu/



### An Analysis of 1-to-First Matching in Iris Recognition
- **Arxiv ID**: http://arxiv.org/abs/1702.01167v1
- **DOI**: 10.1109/WACV.2016.7477687
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.01167v1)
- **Published**: 2017-02-03 21:24:10+00:00
- **Updated**: 2017-02-03 21:24:10+00:00
- **Authors**: Andrey Kuehlkamp, Kevin W. Bowyer
- **Comment**: 2016 IEEE Winter Conference on Applications of Computer Vision (WACV)
- **Journal**: None
- **Summary**: Iris recognition systems are a mature technology that is widely used throughout the world. In identification (as opposed to verification) mode, an iris to be recognized is typically matched against all N enrolled irises. This is the classic "1-to-N search". In order to improve the speed of large-scale identification, a modified "1-to-First" search has been used in some operational systems. A 1-to-First search terminates with the first below-threshold match that is found, whereas a 1-to-N search always finds the best match across all enrollments. We know of no previous studies that evaluate how the accuracy of 1-to-First search differs from that of 1-to-N search. Using a dataset of over 50,000 iris images from 2,800 different irises, we perform experiments to evaluate the relative accuracy of 1-to-First and 1-to-N search. We evaluate how the accuracy difference changes with larger numbers of enrolled irises, and with larger ranges of rotational difference allowed between iris images. We find that False Match error rate for 1-to-First is higher than for 1-to-N, and the the difference grows with larger number of enrolled irises and with larger range of rotation.



