# Arxiv Papers in cs.CV on 2017-02-01
### Denoising Hyperspectral Image with Non-i.i.d. Noise Structure
- **Arxiv ID**: http://arxiv.org/abs/1702.00098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00098v1)
- **Published**: 2017-02-01 00:52:01+00:00
- **Updated**: 2017-02-01 00:52:01+00:00
- **Authors**: Yang Chen, Xiangyong Cao, Qian Zhao, Deyu Meng, Zongben Xu
- **Comment**: 13 pages, 14 figures
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) denoising has been attracting much research attention in remote sensing area due to its importance in improving the HSI qualities. The existing HSI denoising methods mainly focus on specific spectral and spatial prior knowledge in HSIs, and share a common underlying assumption that the embedded noise in HSI is independent and identically distributed (i.i.d.). In real scenarios, however, the noise existed in a natural HSI is always with much more complicated non-i.i.d. statistical structures and the under-estimation to this noise complexity often tends to evidently degenerate the robustness of current methods. To alleviate this issue, this paper attempts the first effort to model the HSI noise using a non-i.i.d. mixture of Gaussians (NMoG) noise assumption, which is finely in accordance with the noise characteristics possessed by a natural HSI and thus is capable of adapting various noise shapes encountered in real applications. Then we integrate such noise modeling strategy into the low-rank matrix factorization (LRMF) model and propose a NMoG-LRMF model in the Bayesian framework. A variational Bayes algorithm is designed to infer the posterior of the proposed model. All involved parameters can be recursively updated in closed-form. Compared with the current techniques, the proposed method performs more robust beyond the state-of-the-arts, as substantiated by our experiments implemented on synthetic and real noisy HSIs.



### Stochastic Graphlet Embedding
- **Arxiv ID**: http://arxiv.org/abs/1702.00156v3
- **DOI**: 10.1109/TNNLS.2018.2884700
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1702.00156v3)
- **Published**: 2017-02-01 08:16:03+00:00
- **Updated**: 2018-12-05 01:59:34+00:00
- **Authors**: Anjan Dutta, Hichem Sahbi
- **Comment**: Accepted in IEEE TNNLS (14 pages, 7 figures, 10 tables)
- **Journal**: IEEE TNNLS, pages 1-14, 2018
- **Summary**: Graph-based methods are known to be successful in many machine learning and pattern classification tasks. These methods consider semi-structured data as graphs where nodes correspond to primitives (parts, interest points, segments, etc.) and edges characterize the relationships between these primitives. However, these non-vectorial graph data cannot be straightforwardly plugged into off-the-shelf machine learning algorithms without a preliminary step of -- explicit/implicit -- graph vectorization and embedding. This embedding process should be resilient to intra-class graph variations while being highly discriminant. In this paper, we propose a novel high-order stochastic graphlet embedding (SGE) that maps graphs into vector spaces. Our main contribution includes a new stochastic search procedure that efficiently parses a given graph and extracts/samples unlimitedly high-order graphlets. We consider these graphlets, with increasing orders, to model local primitives as well as their increasingly complex interactions. In order to build our graph representation, we measure the distribution of these graphlets into a given graph, using particular hash functions that efficiently assign sampled graphlets into isomorphic sets with a very low probability of collision. When combined with maximum margin classifiers, these graphlet-based representations have positive impact on the performance of pattern comparison and recognition as corroborated through extensive experiments using standard benchmark databases.



### Design, Analysis and Application of A Volumetric Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1702.00158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00158v1)
- **Published**: 2017-02-01 08:32:11+00:00
- **Updated**: 2017-02-01 08:32:11+00:00
- **Authors**: Xiaqing Pan, Yueru Chen, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: The design, analysis and application of a volumetric convolutional neural network (VCNN) are studied in this work. Although many CNNs have been proposed in the literature, their design is empirical. In the design of the VCNN, we propose a feed-forward K-means clustering algorithm to determine the filter number and size at each convolutional layer systematically. For the analysis of the VCNN, the cause of confusing classes in the output of the VCNN is explained by analyzing the relationship between the filter weights (also known as anchor vectors) from the last fully-connected layer to the output. Furthermore, a hierarchical clustering method followed by a random forest classification method is proposed to boost the classification performance among confusing classes. For the application of the VCNN, we examine the 3D shape classification problem and conduct experiments on a popular ModelNet40 dataset. The proposed VCNN offers the state-of-the-art performance among all volume-based CNN methods.



### A Kinematic Chain Space for Monocular Motion Capture
- **Arxiv ID**: http://arxiv.org/abs/1702.00186v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00186v1)
- **Published**: 2017-02-01 10:14:24+00:00
- **Updated**: 2017-02-01 10:14:24+00:00
- **Authors**: Bastian Wandt, Hanno Ackermann, Bodo Rosenhahn
- **Comment**: None
- **Journal**: None
- **Summary**: This paper deals with motion capture of kinematic chains (e.g. human skeletons) from monocular image sequences taken by uncalibrated cameras. We present a method based on projecting an observation into a kinematic chain space (KCS). An optimization of the nuclear norm is proposed that implicitly enforces structural properties of the kinematic chain. Unlike other approaches our method does not require specific camera or object motion and is not relying on training data or previously determined constraints such as particular body lengths. The proposed algorithm is able to reconstruct scenes with limited camera motion and previously unseen motions. It is not only applicable to human skeletons but also to other kinematic chains for instance animals or industrial robots. We achieve state-of-the-art results on different benchmark data bases and real world scenes.



### ImageNet MPEG-7 Visual Descriptors - Technical Report
- **Arxiv ID**: http://arxiv.org/abs/1702.00187v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1702.00187v1)
- **Published**: 2017-02-01 10:15:13+00:00
- **Updated**: 2017-02-01 10:15:13+00:00
- **Authors**: Frédéric Rayar
- **Comment**: None
- **Journal**: None
- **Summary**: ImageNet is a large scale and publicly available image database. It currently offers more than 14 millions of images, organised according to the WordNet hierarchy. One of the main objective of the creators is to provide to the research community a relevant database for visual recognition applications such as object recognition, image classification or object localisation. However, only a few visual descriptors of the images are available to be used by the researchers. Only SIFT-based features have been extracted from a subset of the collection. This technical report presents the extraction of some MPEG-7 visual descriptors from the ImageNet database. These descriptors are made publicly available in an effort towards open research.



### Evolving Boxes for Fast Vehicle Detection
- **Arxiv ID**: http://arxiv.org/abs/1702.00254v3
- **DOI**: 10.1109/ICME.2017.8019461
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00254v3)
- **Published**: 2017-02-01 13:53:12+00:00
- **Updated**: 2017-03-29 08:46:08+00:00
- **Authors**: Li Wang, Yao Lu, Hong Wang, Yingbin Zheng, Hao Ye, Xiangyang Xue
- **Comment**: None
- **Journal**: IEEE International Conference on Multimedia and Expo (ICME), 2017,
  pp. 1135-1140
- **Summary**: We perform fast vehicle detection from traffic surveillance cameras. A novel deep learning framework, namely Evolving Boxes, is developed that proposes and refines the object boxes under different feature representations. Specifically, our framework is embedded with a light-weight proposal network to generate initial anchor boxes as well as to early discard unlikely regions; a fine-turning network produces detailed features for these candidate boxes. We show intriguingly that by applying different feature fusion techniques, the initial boxes can be refined for both localization and recognition. We evaluate our network on the recent DETRAC benchmark and obtain a significant improvement over the state-of-the-art Faster RCNN by 9.5% mAP. Further, our network achieves 9-13 FPS detection speed on a moderate commercial GPU.



### Pixel-wise Ear Detection with Convolutional Encoder-Decoder Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.00307v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00307v2)
- **Published**: 2017-02-01 15:16:50+00:00
- **Updated**: 2019-02-01 07:56:10+00:00
- **Authors**: Žiga Emeršič, Luka Lan Gabriel, Vitomir Štruc, Peter Peer
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Object detection and segmentation represents the basis for many tasks in computer and machine vision. In biometric recognition systems the detection of the region-of-interest (ROI) is one of the most crucial steps in the overall processing pipeline, significantly impacting the performance of the entire recognition system. Existing approaches to ear detection, for example, are commonly susceptible to the presence of severe occlusions, ear accessories or variable illumination conditions and often deteriorate in their performance if applied on ear images captured in unconstrained settings. To address these shortcomings, we present in this paper a novel ear detection technique based on convolutional encoder-decoder networks (CEDs). For our technique, we formulate the problem of ear detection as a two-class segmentation problem and train a convolutional encoder-decoder network based on the SegNet architecture to distinguish between image-pixels belonging to either the ear or the non-ear class. The output of the network is then post-processed to further refine the segmentation result and return the final locations of the ears in the input image. Different from competing techniques from the literature, our approach does not simply return a bounding box around the detected ear, but provides detailed, pixel-wise information about the location of the ears in the image. Our experiments on a dataset gathered from the web (a.k.a. in the wild) show that the proposed technique ensures good detection results in the presence of various covariate factors and significantly outperforms the existing state-of-the-art.



### Siamese Network of Deep Fisher-Vector Descriptors for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1702.00338v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00338v1)
- **Published**: 2017-02-01 16:20:00+00:00
- **Updated**: 2017-02-01 16:20:00+00:00
- **Authors**: Eng-Jon Ong, Sameed Husain, Miroslaw Bober
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of large scale image retrieval, with the aim of accurately ranking the similarity of a large number of images to a given query image. To achieve this, we propose a novel Siamese network. This network consists of two computational strands, each comprising of a CNN component followed by a Fisher vector component. The CNN component produces dense, deep convolutional descriptors that are then aggregated by the Fisher Vector method. Crucially, we propose to simultaneously learn both the CNN filter weights and Fisher Vector model parameters. This allows us to account for the evolving distribution of deep descriptors over the course of the learning process. We show that the proposed approach gives significant improvements over the state-of-the-art methods on the Oxford and Paris image retrieval datasets. Additionally, we provide a baseline performance measure for both these datasets with the inclusion of 1 million distractors.



### Visual Saliency Prediction Using a Mixture of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.00372v1
- **DOI**: 10.1109/TIP.2018.2834826
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00372v1)
- **Published**: 2017-02-01 17:45:55+00:00
- **Updated**: 2017-02-01 17:45:55+00:00
- **Authors**: Samuel Dodge, Lina Karam
- **Comment**: None
- **Journal**: None
- **Summary**: Visual saliency models have recently begun to incorporate deep learning to achieve predictive capacity much greater than previous unsupervised methods. However, most existing models predict saliency using local mechanisms limited to the receptive field of the network. We propose a model that incorporates global scene semantic information in addition to local information gathered by a convolutional neural network. Our model is formulated as a mixture of experts. Each expert network is trained to predict saliency for a set of closely related images. The final saliency map is computed as a weighted mixture of the expert networks' output, with weights determined by a separate gating network. This gating network is guided by global scene information to predict weights. The expert networks and the gating network are trained simultaneously in an end-to-end manner. We show that our mixture formulation leads to improvement in performance over an otherwise identical non-mixture model that does not incorporate global scene information.



### Understanding trained CNNs by indexing neuron selectivity
- **Arxiv ID**: http://arxiv.org/abs/1702.00382v2
- **DOI**: 10.1016/j.patrec.2019.10.013
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00382v2)
- **Published**: 2017-02-01 18:19:37+00:00
- **Updated**: 2019-05-21 15:37:50+00:00
- **Authors**: Ivet Rafegas, Maria Vanrell, Luis A. Alexandre, Guillem Arias
- **Comment**: Under review on Pattern Recognition Letters
- **Journal**: None
- **Summary**: The impressive performance of Convolutional Neural Networks (CNNs) when solving different vision problems is shadowed by their black-box nature and our consequent lack of understanding of the representations they build and how these representations are organized. To help understanding these issues, we propose to describe the activity of individual neurons by their Neuron Feature visualization and quantify their inherent selectivity with two specific properties. We explore selectivity indexes for: an image feature (color); and an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer Conv4 or class selective neurons such as dog-face neurons in layer Conv5 in VGG-M, and establishes a methodology to derive other selectivity properties. Indexing on neuron selectivity can statistically draw how features and classes are represented through layers in a moment when the size of trained nets is growing and automatic tools to index neurons can be helpful.



### Handwritten Recognition Using SVM, KNN and Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1702.00723v1
- **DOI**: None
- **Categories**: **cs.CV**, 68Txx
- **Links**: [PDF](http://arxiv.org/pdf/1702.00723v1)
- **Published**: 2017-02-01 18:32:12+00:00
- **Updated**: 2017-02-01 18:32:12+00:00
- **Authors**: Norhidayu Abdul Hamid, Nilam Nur Amir Sjarif
- **Comment**: 11 pages ; 22 Figures
- **Journal**: None
- **Summary**: Handwritten recognition (HWR) is the ability of a computer to receive and interpret intelligible handwritten input from source such as paper documents, photographs, touch-screens and other devices. In this paper we will using three (3) classification t o re cognize the handwritten which is SVM, KNN and Neural Network.



### Product Graph-based Higher Order Contextual Similarities for Inexact Subgraph Matching
- **Arxiv ID**: http://arxiv.org/abs/1702.00391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00391v1)
- **Published**: 2017-02-01 18:53:53+00:00
- **Updated**: 2017-02-01 18:53:53+00:00
- **Authors**: Anjan Dutta, Josep Lladós, Horst Bunke, Umapada Pal
- **Comment**: None
- **Journal**: None
- **Summary**: Many algorithms formulate graph matching as an optimization of an objective function of pairwise quantification of nodes and edges of two graphs to be matched. Pairwise measurements usually consider local attributes but disregard contextual information involved in graph structures. We address this issue by proposing contextual similarities between pairs of nodes. This is done by considering the tensor product graph (TPG) of two graphs to be matched, where each node is an ordered pair of nodes of the operand graphs. Contextual similarities between a pair of nodes are computed by accumulating weighted walks (normalized pairwise similarities) terminating at the corresponding paired node in TPG. Once the contextual similarities are obtained, we formulate subgraph matching as a node and edge selection problem in TPG. We use contextual similarities to construct an objective function and optimize it with a linear programming approach. Since random walk formulation through TPG takes into account higher order information, it is not a surprise that we obtain more reliable similarities and better discrimination among the nodes and edges. Experimental results shown on synthetic as well as real benchmarks illustrate that higher order contextual similarities add discriminating power and allow one to find approximate solutions to the subgraph matching problem.



### Learning to Compose with Professional Photographs on the Web
- **Arxiv ID**: http://arxiv.org/abs/1702.00503v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00503v2)
- **Published**: 2017-02-01 23:49:36+00:00
- **Updated**: 2017-07-18 02:12:17+00:00
- **Authors**: Yi-Ling Chen, Jan Klopp, Min Sun, Shao-Yi Chien, Kwan-Liu Ma
- **Comment**: Scripts and pre-trained models available at
  https://github.com/yiling-chen/view-finding-network
- **Journal**: None
- **Summary**: Photo composition is an important factor affecting the aesthetics in photography. However, it is a highly challenging task to model the aesthetic properties of good compositions due to the lack of globally applicable rules to the wide variety of photographic styles. Inspired by the thinking process of photo taking, we formulate the photo composition problem as a view finding process which successively examines pairs of views and determines their aesthetic preferences. We further exploit the rich professional photographs on the web to mine unlimited high-quality ranking samples and demonstrate that an aesthetics-aware deep ranking network can be trained without explicitly modeling any photographic rules. The resulting model is simple and effective in terms of its architectural design and data sampling method. It is also generic since it naturally learns any photographic rules implicitly encoded in professional photographs. The experiments show that the proposed view finding network achieves state-of-the-art performance with sliding window search strategy on two image cropping datasets.



