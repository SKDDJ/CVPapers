# Arxiv Papers in cs.CV on 2017-02-21
### RenderMap: Exploiting the Link Between Perception and Rendering for Dense Mapping
- **Arxiv ID**: http://arxiv.org/abs/1702.06813v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1702.06813v1)
- **Published**: 2017-02-21 00:12:32+00:00
- **Updated**: 2017-02-21 00:12:32+00:00
- **Authors**: Julian Ryde, Xuchu, Ding
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an approach for the real-time (2Hz) creation of a dense map and alignment of a moving robotic agent within that map by rendering using a Graphics Processing Unit (GPU). This is done by recasting the scan alignment part of the dense mapping process as a rendering task. Alignment errors are computed from rendering the scene, comparing with range data from the sensors, and minimized by an optimizer. The proposed approach takes advantage of the advances in rendering techniques for computer graphics and GPU hardware to accelerate the algorithm. Moreover, it allows one to exploit information not used in classic dense mapping algorithms such as Iterative Closest Point (ICP) by rendering interfaces between the free space, occupied space and the unknown. The proposed approach leverages directly the rendering capabilities of the GPU, in contrast to other GPU-based approaches that deploy the GPU as a general purpose parallel computation platform.   We argue that the proposed concept is a general consequence of treating perception problems as inverse problems of rendering. Many perception problems can be recast into a form where much of the computation is replaced by render operations. This is not only efficient since rendering is fast, but also simpler to implement and will naturally benefit from future advancements in GPU speed and rendering techniques. Furthermore, this general concept can go beyond addressing perception problems and can be used for other problem domains such as path planning.



### Learning to Generate Posters of Scientific Papers by Probabilistic Graphical Models
- **Arxiv ID**: http://arxiv.org/abs/1702.06228v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1702.06228v1)
- **Published**: 2017-02-21 01:02:56+00:00
- **Updated**: 2017-02-21 01:02:56+00:00
- **Authors**: Yu-ting Qiang, Yanwei Fu, Xiao Yu, Yanwen Guo, Zhi-Hua Zhou, Leonid Sigal
- **Comment**: 10 pages, submission to IEEE TPAMI. arXiv admin note: text overlap
  with arXiv:1604.01219
- **Journal**: None
- **Summary**: Researchers often summarize their work in the form of scientific posters. Posters provide a coherent and efficient way to convey core ideas expressed in scientific papers. Generating a good scientific poster, however, is a complex and time consuming cognitive task, since such posters need to be readable, informative, and visually aesthetic. In this paper, for the first time, we study the challenging problem of learning to generate posters from scientific papers. To this end, a data-driven framework, that utilizes graphical models, is proposed. Specifically, given content to display, the key elements of a good poster, including attributes of each panel and arrangements of graphical elements are learned and inferred from data. During the inference stage, an MAP inference framework is employed to incorporate some design principles. In order to bridge the gap between panel attributes and the composition within each panel, we also propose a recursive page splitting algorithm to generate the panel layout for a poster. To learn and validate our model, we collect and release a new benchmark dataset, called NJU-Fudan Paper-Poster dataset, which consists of scientific papers and corresponding posters with exhaustively labelled panels and attributes. Qualitative and quantitative results indicate the effectiveness of our approach.



### The Power of Sparsity in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.06257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06257v1)
- **Published**: 2017-02-21 04:10:31+00:00
- **Updated**: 2017-02-21 04:10:31+00:00
- **Authors**: Soravit Changpinyo, Mark Sandler, Andrey Zhmoginov
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional networks are well-known for their high computational and memory demands. Given limited resources, how does one design a network that balances its size, training time, and prediction accuracy? A surprisingly effective approach to trade accuracy for size and speed is to simply reduce the number of channels in each convolutional layer by a fixed fraction and retrain the network. In many cases this leads to significantly smaller networks with only minimal changes to accuracy. In this paper, we take a step further by empirically examining a strategy for deactivating connections between filters in convolutional layers in a way that allows us to harvest savings both in run-time and memory for many network architectures. More specifically, we generalize 2D convolution to use a channel-wise sparse connection structure and show that this leads to significantly better results than the baseline approach for large networks including VGG and Inception V3.



### Weighted Motion Averaging for the Registration of Multi-View Range Scans
- **Arxiv ID**: http://arxiv.org/abs/1702.06264v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06264v3)
- **Published**: 2017-02-21 04:53:01+00:00
- **Updated**: 2017-04-09 08:13:09+00:00
- **Authors**: Rui Guo, Jihua Zhu, Yaochen Li, Dapeng Chen, Zhongyu Li, Yongqin Zhang
- **Comment**: 9 pages, 6 figures, 2tables
- **Journal**: None
- **Summary**: Multi-view registration is a fundamental but challenging problem in 3D reconstruction and robot vision. Although the original motion averaging algorithm has been introduced as an effective means to solve the multi-view registration problem, it does not consider the reliability and accuracy of each relative motion. Accordingly, this paper proposes a novel motion averaging algorithm for multi-view registration. Firstly, it utilizes the pair-wise registration algorithm to estimate the relative motion and overlapping percentage of each scan pair with a certain degree of overlap. With the overlapping percentage available, it views the overlapping percentage as the corresponding weight of each scan pair and proposes the weight motion averaging algorithm, which can pay more attention to reliable and accurate relative motions. By treating each relative motion distinctively, more accurate registration can be achieved by applying the weighted motion averaging to multi-view range scans. Experimental results demonstrate the superiority of our proposed approach compared with the state-of-the-art methods in terms of accuracy, robustness and efficiency.



### Projection based advanced motion model for cubic mapping for 360-degree video
- **Arxiv ID**: http://arxiv.org/abs/1702.06277v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.06277v1)
- **Published**: 2017-02-21 06:35:01+00:00
- **Updated**: 2017-02-21 06:35:01+00:00
- **Authors**: Li Li, Zhu Li, Madhukar Budagavi, Houqiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel advanced motion model to handle the irregular motion for the cubic map projection of 360-degree video. Since the irregular motion is mainly caused by the projection from the sphere to the cube map, we first try to project the pixels in both the current picture and reference picture from unfolding cube back to the sphere. Then through utilizing the characteristic that most of the motions in the sphere are uniform, we can derive the relationship between the motion vectors of various pixels in the unfold cube. The proposed advanced motion model is implemented in the High Efficiency Video Coding reference software. Experimental results demonstrate that quite obvious performance improvement can be achieved for the sequences with obvious motions.



### Scene Recognition by Combining Local and Global Image Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1702.06850v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1702.06850v1)
- **Published**: 2017-02-21 06:57:37+00:00
- **Updated**: 2017-02-21 06:57:37+00:00
- **Authors**: Jobin Wilson, Muhammad Arif
- **Comment**: A full implementation of our model is available at
  https://github.com/flytxtds/scene-recognition
- **Journal**: None
- **Summary**: Object recognition is an important problem in computer vision, having diverse applications. In this work, we construct an end-to-end scene recognition pipeline consisting of feature extraction, encoding, pooling and classification. Our approach simultaneously utilize global feature descriptors as well as local feature descriptors from images, to form a hybrid feature descriptor corresponding to each image. We utilize DAISY features associated with key points within images as our local feature descriptor and histogram of oriented gradients (HOG) corresponding to an entire image as a global descriptor. We make use of a bag-of-visual-words encoding and apply Mini- Batch K-Means algorithm to reduce the complexity of our feature encoding scheme. A 2-level pooling procedure is used to combine DAISY and HOG features corresponding to each image. Finally, we experiment with a multi-class SVM classifier with several kernels, in a cross-validation setting, and tabulate our results on the fifteen scene categories dataset. The average accuracy of our model was 76.4% in the case of a 40%-60% random split of images into training and testing datasets respectively. The primary objective of this work is to clearly outline the practical implementation of a basic screne-recognition pipeline having a reasonable accuracy, in python, using open-source libraries. A full implementation of the proposed model is available in our github repository.



### Real-time visual tracking by deep reinforced decision making
- **Arxiv ID**: http://arxiv.org/abs/1702.06291v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06291v2)
- **Published**: 2017-02-21 08:15:51+00:00
- **Updated**: 2018-08-17 05:21:47+00:00
- **Authors**: Janghoon Choi, Junseok Kwon, Kyoung Mu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: One of the major challenges of model-free visual tracking problem has been the difficulty originating from the unpredictable and drastic changes in the appearance of objects we target to track. Existing methods tackle this problem by updating the appearance model on-line in order to adapt to the changes in the appearance. Despite the success of these methods however, inaccurate and erroneous updates of the appearance model result in a tracker drift. In this paper, we introduce a novel real-time visual tracking algorithm based on a template selection strategy constructed by deep reinforcement learning methods. The tracking algorithm utilizes this strategy to choose the appropriate template for tracking a given frame. The template selection strategy is self-learned by utilizing a simple policy gradient method on numerous training episodes randomly generated from a tracking benchmark dataset. Our proposed reinforcement learning framework is generally applicable to other confidence map based tracking algorithms. The experiment shows that our tracking algorithm runs in real-time speed of 43 fps and the proposed policy network effectively decides the appropriate template for successful visual tracking.



### Learning Compact Appearance Representation for Video-based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1702.06294v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06294v2)
- **Published**: 2017-02-21 08:58:28+00:00
- **Updated**: 2019-09-20 06:40:28+00:00
- **Authors**: Wei Zhang, Shengnan Hu, Kan Liu, Zhengjun Zha
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel approach for video-based person re-identification using multiple Convolutional Neural Networks (CNNs). Unlike previous work, we intend to extract a compact yet discriminative appearance representation from several frames rather than the whole sequence. Specifically, given a video, the representative frames are selected based on the walking profile of consecutive frames. A multiple CNN architecture incorporated with feature pooling is proposed to learn and compile the features of the selected representative frames into a compact description about the pedestrian for identification. Experiments are conducted on benchmark datasets to demonstrate the superiority of the proposed method over existing person re-identification approaches.



### Is Saki #delicious? The Food Perception Gap on Instagram and Its Relation to Health
- **Arxiv ID**: http://arxiv.org/abs/1702.06318v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1702.06318v1)
- **Published**: 2017-02-21 10:36:28+00:00
- **Updated**: 2017-02-21 10:36:28+00:00
- **Authors**: Ferda Ofli, Yusuf Aytar, Ingmar Weber, Raggi al Hammouri, Antonio Torralba
- **Comment**: This is a pre-print of our paper accepted to appear in the
  Proceedings of 2017 International World Wide Web Conference (WWW'17)
- **Journal**: None
- **Summary**: Food is an integral part of our life and what and how much we eat crucially affects our health. Our food choices largely depend on how we perceive certain characteristics of food, such as whether it is healthy, delicious or if it qualifies as a salad. But these perceptions differ from person to person and one person's "single lettuce leaf" might be another person's "side salad". Studying how food is perceived in relation to what it actually is typically involves a laboratory setup. Here we propose to use recent advances in image recognition to tackle this problem. Concretely, we use data for 1.9 million images from Instagram from the US to look at systematic differences in how a machine would objectively label an image compared to how a human subjectively does. We show that this difference, which we call the "perception gap", relates to a number of health outcomes observed at the county level. To the best of our knowledge, this is the first time that image recognition is being used to study the "misalignment" of how people describe food images vs. what they actually depict.



### Just DIAL: DomaIn Alignment Layers for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1702.06332v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06332v3)
- **Published**: 2017-02-21 11:23:48+00:00
- **Updated**: 2017-04-27 08:45:08+00:00
- **Authors**: Fabio Maria Carlucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, Samuel Rota Bulò
- **Comment**: None
- **Journal**: None
- **Summary**: The empirical fact that classifiers, trained on given data collections, perform poorly when tested on data acquired in different settings is theoretically explained in domain adaptation through a shift among distributions of the source and target domains. Alleviating the domain shift problem, especially in the challenging setting where no labeled data are available for the target domain, is paramount for having visual recognition systems working in the wild. As the problem stems from a shift among distributions, intuitively one should try to align them. In the literature, this has resulted in a stream of works attempting to align the feature representations learned from the source and target domains. Here we take a different route. Rather than introducing regularization terms aiming to promote the alignment of the two representations, we act at the distribution level through the introduction of \emph{DomaIn Alignment Layers} (\DIAL), able to match the observed source and target data distributions to a reference one. Thorough experiments on three different public benchmarks we confirm the power of our approach.



### Object Detection in Videos with Tubelet Proposal Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.06355v2
- **DOI**: 10.1109/CVPR.2017.101
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06355v2)
- **Published**: 2017-02-21 12:38:11+00:00
- **Updated**: 2017-04-10 08:39:06+00:00
- **Authors**: Kai Kang, Hongsheng Li, Tong Xiao, Wanli Ouyang, Junjie Yan, Xihui Liu, Xiaogang Wang
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Object detection in videos has drawn increasing attention recently with the introduction of the large-scale ImageNet VID dataset. Different from object detection in static images, temporal information in videos is vital for object detection. To fully utilize temporal information, state-of-the-art methods are based on spatiotemporal tubelets, which are essentially sequences of associated bounding boxes across time. However, the existing methods have major limitations in generating tubelets in terms of quality and efficiency. Motion-based methods are able to obtain dense tubelets efficiently, but the lengths are generally only several frames, which is not optimal for incorporating long-term temporal information. Appearance-based methods, usually involving generic object tracking, could generate long tubelets, but are usually computationally expensive. In this work, we propose a framework for object detection in videos, which consists of a novel tubelet proposal network to efficiently generate spatiotemporal proposals, and a Long Short-term Memory (LSTM) network that incorporates temporal information from tubelet proposals for achieving high object detection accuracy in videos. Experiments on the large-scale ImageNet VID dataset demonstrate the effectiveness of the proposed framework for object detection in videos.



### Mimicking Ensemble Learning with Deep Branched Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.06376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06376v1)
- **Published**: 2017-02-21 13:33:37+00:00
- **Updated**: 2017-02-21 13:33:37+00:00
- **Authors**: Byungju Kim, Youngsoo Kim, Yeakang Lee, Junmo Kim
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a branched residual network for image classification. It is known that high-level features of deep neural network are more representative than lower-level features. By sharing the low-level features, the network can allocate more memory to high-level features. The upper layers of our proposed network are branched, so that it mimics the ensemble learning. By mimicking ensemble learning with single network, we have achieved better performance on ImageNet classification task.



### Differential Geometric Retrieval of Deep Features
- **Arxiv ID**: http://arxiv.org/abs/1702.06383v2
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.06383v2)
- **Published**: 2017-02-21 13:55:47+00:00
- **Updated**: 2017-11-12 09:01:35+00:00
- **Authors**: Y Qian, E Vazquez, B Sengupta
- **Comment**: 5th ICDM Workshop on High Dimensional Data Mining (HDM 2017)
- **Journal**: None
- **Summary**: Comparing images to recommend items from an image-inventory is a subject of continued interest. Added with the scalability of deep-learning architectures the once `manual' job of hand-crafting features have been largely alleviated, and images can be compared according to features generated from a deep convolutional neural network. In this paper, we compare distance metrics (and divergences) to rank features generated from a neural network, for content-based image retrieval. Specifically, after modelling individual images using approximations of mixture models or sparse covariance estimators, we resort to their information-theoretic and Riemann geometric comparisons. We show that using approximations of mixture models enable us to compute a distance measure based on the Wasserstein metric that requires less effort than other computationally intensive optimal transport plans; finally, an affine invariant metric is used to compare the optimal transport metric to its Riemann geometric counterpart -- we conclude that although expensive, retrieval metric based on Wasserstein geometry is more suitable than information theoretic comparison of images. In short, we combine GPU scalability in learning deep feature vectors with statistically efficient metrics that we foresee being utilised in a commercial setting.



### A Discriminative Event Based Model for Alzheimer's Disease Progression Modeling
- **Arxiv ID**: http://arxiv.org/abs/1702.06408v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1702.06408v1)
- **Published**: 2017-02-21 14:41:15+00:00
- **Updated**: 2017-02-21 14:41:15+00:00
- **Authors**: Vikram Venkatraghavan, Esther Bron, Wiro Niessen, Stefan Klein
- **Comment**: Information Processing in Medical Imaging (IPMI), 2017
- **Journal**: None
- **Summary**: The event-based model (EBM) for data-driven disease progression modeling estimates the sequence in which biomarkers for a disease become abnormal. This helps in understanding the dynamics of disease progression and facilitates early diagnosis by staging patients on a disease progression timeline. Existing EBM methods are all generative in nature. In this work we propose a novel discriminative approach to EBM, which is shown to be more accurate as well as computationally more efficient than existing state-of-the art EBM methods. The method first estimates for each subject an approximate ordering of events, by ranking the posterior probabilities of individual biomarkers being abnormal. Subsequently, the central ordering over all subjects is estimated by fitting a generalized Mallows model to these approximate subject-specific orderings based on a novel probabilistic Kendall's Tau distance. To evaluate the accuracy, we performed extensive experiments on synthetic data simulating the progression of Alzheimer's disease. Subsequently, the method was applied to the Alzheimer's Disease Neuroimaging Initiative (ADNI) data to estimate the central event ordering in the dataset. The experiments benchmark the accuracy of the new model under various conditions and compare it with existing state-of-the-art EBM methods. The results indicate that discriminative EBM could be a simple and elegant approach to disease progression modeling.



### Comprehensive Data Set for Automatic Single Camera Visual Speed Measurement
- **Arxiv ID**: http://arxiv.org/abs/1702.06441v2
- **DOI**: 10.1109/TITS.2018.2825609
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06441v2)
- **Published**: 2017-02-21 15:34:20+00:00
- **Updated**: 2018-05-09 07:51:55+00:00
- **Authors**: Jakub Sochor, Roman Juránek, Jakub Špaňhel, Lukáš Maršík, Adam Široký, Adam Herout, Pavel Zemčík
- **Comment**: None
- **Journal**: IEEE Transactions on Intelligent Transportation Systems 2018
- **Summary**: In this paper, we focus on traffic camera calibration and a visual speed measurement from a single monocular camera, which is an important task of visual traffic surveillance. Existing methods addressing this problem are difficult to compare due to a lack of a common data set with reliable ground truth. Therefore, it is not clear how the methods compare in various aspects and what factors are affecting their performance. We captured a new data set of 18 full-HD videos, each around 1 hr long, captured at six different locations. Vehicles in the videos (20865 instances in total) are annotated with the precise speed measurements from optical gates using LiDAR and verified with several reference GPS tracks. We made the data set available for download and it contains the videos and metadata (calibration, lengths of features in image, annotations, and so on) for future comparison and evaluation. Camera calibration is the most crucial part of the speed measurement; therefore, we provide a brief overview of the methods and analyze a recently published method for fully automatic camera calibration and vehicle speed measurement and report the results on this data set in detail.



### Traffic Surveillance Camera Calibration by 3D Model Bounding Box Alignment for Accurate Vehicle Speed Measurement
- **Arxiv ID**: http://arxiv.org/abs/1702.06451v2
- **DOI**: 10.1016/j.cviu.2017.05.015
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06451v2)
- **Published**: 2017-02-21 15:50:10+00:00
- **Updated**: 2017-06-01 11:57:49+00:00
- **Authors**: Jakub Sochor, Roman Juránek, Adam Herout
- **Comment**: accepted to CVIU
- **Journal**: None
- **Summary**: In this paper, we focus on fully automatic traffic surveillance camera calibration, which we use for speed measurement of passing vehicles. We improve over a recent state-of-the-art camera calibration method for traffic surveillance based on two detected vanishing points. More importantly, we propose a novel automatic scene scale inference method. The method is based on matching bounding boxes of rendered 3D models of vehicles with detected bounding boxes in the image. The proposed method can be used from arbitrary viewpoints, since it has no constraints on camera placement. We evaluate our method on the recent comprehensive dataset for speed measurement BrnoCompSpeed. Experiments show that our automatic camera calibration method by detection of two vanishing points reduces error by 50% (mean distance ratio error reduced from 0.18 to 0.09) compared to the previous state-of-the-art method. We also show that our scene scale inference method is more precise, outperforming both state-of-the-art automatic calibration method for speed measurement (error reduction by 86% -- 7.98km/h to 1.10km/h) and manual calibration (error reduction by 19% -- 1.35km/h to 1.10km/h). We also present qualitative results of the proposed automatic camera calibration method on video sequences obtained from real surveillance cameras in various places, and under different lighting conditions (night, dawn, day).



### Online Representation Learning with Single and Multi-layer Hebbian Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1702.06456v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1702.06456v3)
- **Published**: 2017-02-21 16:01:28+00:00
- **Updated**: 2018-01-29 12:03:51+00:00
- **Authors**: Yanis Bahroun, Andrea Soltoggio
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Unsupervised learning permits the development of algorithms that are able to adapt to a variety of different data sets using the same underlying rules thanks to the autonomous discovery of discriminating features during training. Recently, a new class of Hebbian-like and local unsupervised learning rules for neural networks have been developed that minimise a similarity matching cost-function. These have been shown to perform sparse representation learning. This study tests the effectiveness of one such learning rule for learning features from images. The rule implemented is derived from a nonnegative classical multidimensional scaling cost-function, and is applied to both single and multi-layer architectures. The features learned by the algorithm are then used as input to an SVM to test their effectiveness in classification on the established CIFAR-10 image dataset. The algorithm performs well in comparison to other unsupervised learning algorithms and multi-layer networks, thus suggesting its validity in the design of a new class of compact, online learning networks.



### Crowd Sourcing Image Segmentation with iaSTAPLE
- **Arxiv ID**: http://arxiv.org/abs/1702.06461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06461v1)
- **Published**: 2017-02-21 16:12:18+00:00
- **Updated**: 2017-02-21 16:12:18+00:00
- **Authors**: Dmitrij Schlesinger, Florian Jug, Gene Myers, Carsten Rother, Dagmar Kainmüller
- **Comment**: Accepted to ISBI2017
- **Journal**: None
- **Summary**: We propose a novel label fusion technique as well as a crowdsourcing protocol to efficiently obtain accurate epithelial cell segmentations from non-expert crowd workers. Our label fusion technique simultaneously estimates the true segmentation, the performance levels of individual crowd workers, and an image segmentation model in the form of a pairwise Markov random field. We term our approach image-aware STAPLE (iaSTAPLE) since our image segmentation model seamlessly integrates into the well-known and widely used STAPLE approach. In an evaluation on a light microscopy dataset containing more than 5000 membrane labeled epithelial cells of a fly wing, we show that iaSTAPLE outperforms STAPLE in terms of segmentation accuracy as well as in terms of the accuracy of estimated crowd worker performance levels, and is able to correctly segment 99% of all cells when compared to expert segmentations. These results show that iaSTAPLE is a highly useful tool for crowd sourcing image segmentation.



### PixelNet: Representation of the pixels, by the pixels, and for the pixels
- **Arxiv ID**: http://arxiv.org/abs/1702.06506v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1702.06506v1)
- **Published**: 2017-02-21 18:20:30+00:00
- **Updated**: 2017-02-21 18:20:30+00:00
- **Authors**: Aayush Bansal, Xinlei Chen, Bryan Russell, Abhinav Gupta, Deva Ramanan
- **Comment**: Project Page: http://www.cs.cmu.edu/~aayushb/pixelNet/. arXiv admin
  note: substantial text overlap with arXiv:1609.06694
- **Journal**: None
- **Summary**: We explore design principles for general pixel-level prediction problems, from low-level edge detection to mid-level surface normal estimation to high-level semantic segmentation. Convolutional predictors, such as the fully-convolutional network (FCN), have achieved remarkable success by exploiting the spatial redundancy of neighboring pixels through convolutional processing. Though computationally efficient, we point out that such approaches are not statistically efficient during learning precisely because spatial redundancy limits the information learned from neighboring pixels. We demonstrate that stratified sampling of pixels allows one to (1) add diversity during batch updates, speeding up learning; (2) explore complex nonlinear predictors, improving accuracy; and (3) efficiently train state-of-the-art models tabula rasa (i.e., "from scratch") for diverse pixel-labeling tasks. Our single architecture produces state-of-the-art results for semantic segmentation on PASCAL-Context dataset, surface normal estimation on NYUDv2 depth dataset, and edge detection on BSDS.



### VidLoc: A Deep Spatio-Temporal Model for 6-DoF Video-Clip Relocalization
- **Arxiv ID**: http://arxiv.org/abs/1702.06521v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06521v2)
- **Published**: 2017-02-21 18:49:26+00:00
- **Updated**: 2017-07-31 10:18:08+00:00
- **Authors**: Ronald Clark, Sen Wang, Andrew Markham, Niki Trigoni, Hongkai Wen
- **Comment**: To appear at CVPR 2017
- **Journal**: None
- **Summary**: Machine learning techniques, namely convolutional neural networks (CNN) and regression forests, have recently shown great promise in performing 6-DoF localization of monocular images. However, in most cases image-sequences, rather only single images, are readily available. To this extent, none of the proposed learning-based approaches exploit the valuable constraint of temporal smoothness, often leading to situations where the per-frame error is larger than the camera motion. In this paper we propose a recurrent model for performing 6-DoF localization of video-clips. We find that, even by considering only short sequences (20 frames), the pose estimates are smoothed and the localization error can be drastically reduced. Finally, we consider means of obtaining probabilistic pose estimates from our model. We evaluate our method on openly-available real-world autonomous driving and indoor localization datasets.



### Lensless Photography with only an image sensor
- **Arxiv ID**: http://arxiv.org/abs/1702.06619v1
- **DOI**: 10.1364/ao.56.006450
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1702.06619v1)
- **Published**: 2017-02-21 23:20:54+00:00
- **Updated**: 2017-02-21 23:20:54+00:00
- **Authors**: Ganghun Kim, Kyle Isaacson, Racheal Palmer, Rajesh Menon
- **Comment**: None
- **Journal**: None
- **Summary**: Photography usually requires optics in conjunction with a recording device (an image sensor). Eliminating the optics could lead to new form factors for cameras. Here, we report a simple demonstration of imaging using a bare CMOS sensor that utilizes computation. The technique relies on the space variant point-spread functions resulting from the interaction of a point source in the field of view with the image sensor. These space-variant point-spread functions are combined with a reconstruction algorithm in order to image simple objects displayed on a discrete LED array as well as on an LCD screen. We extended the approach to video imaging at the native frame rate of the sensor. Finally, we performed experiments to analyze the parametric impact of the object distance. Improving the sensor designs and reconstruction algorithms can lead to useful cameras without optics.



