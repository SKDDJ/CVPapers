# Arxiv Papers in cs.CV on 2017-02-26
### Supervised Learning of Labeled Pointcloud Differences via Cover-Tree Entropy Reduction
- **Arxiv ID**: http://arxiv.org/abs/1702.07959v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 62H30, 60G55
- **Links**: [PDF](http://arxiv.org/pdf/1702.07959v3)
- **Published**: 2017-02-26 00:17:42+00:00
- **Updated**: 2018-01-19 19:30:37+00:00
- **Authors**: Abraham Smith, Paul Bendich, John Harer, Alex Pieloch, Jay Hineman
- **Comment**: Distribution Statement A - Approved for public release, distribution
  is unlimited. Version 2: added link to code, and some minor improvements.
  Version 3: updated authors and thanks
- **Journal**: None
- **Summary**: We introduce a new algorithm, called CDER, for supervised machine learning that merges the multi-scale geometric properties of Cover Trees with the information-theoretic properties of entropy. CDER applies to a training set of labeled pointclouds embedded in a common Euclidean space. If typical pointclouds corresponding to distinct labels tend to differ at any scale in any sub-region, CDER can identify these differences in (typically) linear time, creating a set of distributional coordinates which act as a feature extraction mechanism for supervised learning. We describe theoretical properties and implementation details of CDER, and illustrate its benefits on several synthetic examples.



### Spatially Aware Melanoma Segmentation Using Hybrid Deep Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/1702.07963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07963v1)
- **Published**: 2017-02-26 00:56:25+00:00
- **Updated**: 2017-02-26 00:56:25+00:00
- **Authors**: M. Attia, M. Hossny, S. Nahavandi, A. Yazdabadi
- **Comment**: ISIC2017
- **Journal**: None
- **Summary**: In this paper, we proposed using a hybrid method that utilises deep convolutional and recurrent neural networks for accurate delineation of skin lesion of images supplied with ISBI 2017 lesion segmentation challenge. The proposed method was trained using 1800 images and tested on 150 images from ISBI 2017 challenge.



### Seeing What Is Not There: Learning Context to Determine Where Objects Are Missing
- **Arxiv ID**: http://arxiv.org/abs/1702.07971v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07971v1)
- **Published**: 2017-02-26 01:56:38+00:00
- **Updated**: 2017-02-26 01:56:38+00:00
- **Authors**: Jin Sun, David W. Jacobs
- **Comment**: None
- **Journal**: None
- **Summary**: Most of computer vision focuses on what is in an image. We propose to train a standalone object-centric context representation to perform the opposite task: seeing what is not there. Given an image, our context model can predict where objects should exist, even when no object instances are present. Combined with object detection results, we can perform a novel vision task: finding where objects are missing in an image. Our model is based on a convolutional neural network structure. With a specially designed training strategy, the model learns to ignore objects and focus on context only. It is fully convolutional thus highly efficient. Experiments show the effectiveness of the proposed approach in one important accessibility task: finding city street regions where curb ramps are missing, which could help millions of people with mobility disabilities.



### Building Fast and Compact Convolutional Neural Networks for Offline Handwritten Chinese Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1702.07975v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07975v1)
- **Published**: 2017-02-26 02:13:20+00:00
- **Updated**: 2017-02-26 02:13:20+00:00
- **Authors**: Xuefeng Xiao, Lianwen Jin, Yafeng Yang, Weixin Yang, Jun Sun, Tianhai Chang
- **Comment**: 15 pages, 7 figures, 5 tables
- **Journal**: None
- **Summary**: Like other problems in computer vision, offline handwritten Chinese character recognition (HCCR) has achieved impressive results using convolutional neural network (CNN)-based methods. However, larger and deeper networks are needed to deliver state-of-the-art results in this domain. Such networks intuitively appear to incur high computational cost, and require the storage of a large number of parameters, which renders them unfeasible for deployment in portable devices. To solve this problem, we propose a Global Supervised Low-rank Expansion (GSLRE) method and an Adaptive Drop-weight (ADW) technique to solve the problems of speed and storage capacity. We design a nine-layer CNN for HCCR consisting of 3,755 classes, and devise an algorithm that can reduce the networks computational cost by nine times and compress the network to 1/18 of the original size of the baseline model, with only a 0.21% drop in accuracy. In tests, the proposed algorithm surpassed the best single-network performance reported thus far in the literature while requiring only 2.3 MB for storage. Furthermore, when integrated with our effective forward implementation, the recognition of an offline character image took only 9.7 ms on a CPU. Compared with the state-of-the-art CNN model for HCCR, our approach is approximately 30 times faster, yet 10 times more cost efficient.



### A multi-task convolutional neural network for mega-city analysis using very high resolution satellite imagery and geospatial data
- **Arxiv ID**: http://arxiv.org/abs/1702.07985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07985v1)
- **Published**: 2017-02-26 04:23:36+00:00
- **Updated**: 2017-02-26 04:23:36+00:00
- **Authors**: Fan Zhang, Bo Du, Liangpei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Mega-city analysis with very high resolution (VHR) satellite images has been drawing increasing interest in the fields of city planning and social investigation. It is known that accurate land-use, urban density, and population distribution information is the key to mega-city monitoring and environmental studies. Therefore, how to generate land-use, urban density, and population distribution maps at a fine scale using VHR satellite images has become a hot topic. Previous studies have focused solely on individual tasks with elaborate hand-crafted features and have ignored the relationship between different tasks. In this study, we aim to propose a universal framework which can: 1) automatically learn the internal feature representation from the raw image data; and 2) simultaneously produce fine-scale land-use, urban density, and population distribution maps. For the first target, a deep convolutional neural network (CNN) is applied to learn the hierarchical feature representation from the raw image data. For the second target, a novel CNN-based universal framework is proposed to process the VHR satellite images and generate the land-use, urban density, and population distribution maps. To the best of our knowledge, this is the first CNN-based mega-city analysis method which can process a VHR remote sensing image with such a large data volume. A VHR satellite image (1.2 m spatial resolution) of the center of Wuhan covering an area of 2606 km2 was used to evaluate the proposed method. The experimental results confirm that the proposed method can achieve a promising accuracy for land-use, urban density, and population distribution maps.



### Bayesian Nonparametric Feature and Policy Learning for Decision-Making
- **Arxiv ID**: http://arxiv.org/abs/1702.08001v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.08001v1)
- **Published**: 2017-02-26 08:34:26+00:00
- **Updated**: 2017-02-26 08:34:26+00:00
- **Authors**: Jürgen Hahn, Abdelhak M. Zoubir
- **Comment**: None
- **Journal**: None
- **Summary**: Learning from demonstrations has gained increasing interest in the recent past, enabling an agent to learn how to make decisions by observing an experienced teacher. While many approaches have been proposed to solve this problem, there is only little work that focuses on reasoning about the observed behavior. We assume that, in many practical problems, an agent makes its decision based on latent features, indicating a certain action. Therefore, we propose a generative model for the states and actions. Inference reveals the number of features, the features, and the policies, allowing us to learn and to analyze the underlying structure of the observed behavior. Further, our approach enables prediction of actions for new states. Simulations are used to assess the performance of the algorithm based upon this model. Moreover, the problem of learning a driver's behavior is investigated, demonstrating the performance of the proposed model in a real-world scenario.



### Bayesian Nonparametric Unmixing of Hyperspectral Images
- **Arxiv ID**: http://arxiv.org/abs/1702.08007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08007v1)
- **Published**: 2017-02-26 09:10:45+00:00
- **Updated**: 2017-02-26 09:10:45+00:00
- **Authors**: Jürgen Hahn, Abdelhak M. Zoubir
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral imaging is an important tool in remote sensing, allowing for accurate analysis of vast areas. Due to a low spatial resolution, a pixel of a hyperspectral image rarely represents a single material, but rather a mixture of different spectra. HSU aims at estimating the pure spectra present in the scene of interest, referred to as endmembers, and their fractions in each pixel, referred to as abundances. Today, many HSU algorithms have been proposed, based either on a geometrical or statistical model. While most methods assume that the number of endmembers present in the scene is known, there is only little work about estimating this number from the observed data. In this work, we propose a Bayesian nonparametric framework that jointly estimates the number of endmembers, the endmembers itself, and their abundances, by making use of the Indian Buffet Process as a prior for the endmembers. Simulation results and experiments on real data demonstrate the effectiveness of the proposed algorithm, yielding results comparable with state-of-the-art methods while being able to reliably infer the number of endmembers. In scenarios with strong noise, where other algorithms provide only poor results, the proposed approach tends to overestimate the number of endmembers slightly. The additional endmembers, however, often simply represent noisy replicas of present endmembers and could easily be merged in a post-processing step.



### Analyzing Modular CNN Architectures for Joint Depth Prediction and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1702.08009v1
- **DOI**: 10.1109/ICRA.2017.7989537
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1702.08009v1)
- **Published**: 2017-02-26 09:30:08+00:00
- **Updated**: 2017-02-26 09:30:08+00:00
- **Authors**: Omid Hosseini Jafari, Oliver Groth, Alexander Kirillov, Michael Ying Yang, Carsten Rother
- **Comment**: Accepted to ICRA 2017
- **Journal**: None
- **Summary**: This paper addresses the task of designing a modular neural network architecture that jointly solves different tasks. As an example we use the tasks of depth estimation and semantic segmentation given a single RGB image. The main focus of this work is to analyze the cross-modality influence between depth and semantic prediction maps on their joint refinement. While most previous works solely focus on measuring improvements in accuracy, we propose a way to quantify the cross-modality influence. We show that there is a relationship between final accuracy and cross-modality influence, although not a simple linear one. Hence a larger cross-modality influence does not necessarily translate into an improved accuracy. We find that a beneficial balance between the cross-modality influences can be achieved by network architecture and conjecture that this relationship can be utilized to understand different network design choices. Towards this end we propose a Convolutional Neural Network (CNN) architecture that fuses the state of the state-of-the-art results for depth estimation and semantic labeling. By balancing the cross-modality influences between depth and semantic prediction, we achieve improved results for both tasks using the NYU-Depth v2 benchmark.



### Adversarial Networks for the Detection of Aggressive Prostate Cancer
- **Arxiv ID**: http://arxiv.org/abs/1702.08014v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08014v1)
- **Published**: 2017-02-26 10:08:49+00:00
- **Updated**: 2017-02-26 10:08:49+00:00
- **Authors**: Simon Kohl, David Bonekamp, Heinz-Peter Schlemmer, Kaneschka Yaqubi, Markus Hohenfellner, Boris Hadaschik, Jan-Philipp Radtke, Klaus Maier-Hein
- **Comment**: 8 pages, 3 figures; under review as a conference paper at MICCAI 2017
- **Journal**: None
- **Summary**: Semantic segmentation constitutes an integral part of medical image analyses for which breakthroughs in the field of deep learning were of high relevance. The large number of trainable parameters of deep neural networks however renders them inherently data hungry, a characteristic that heavily challenges the medical imaging community. Though interestingly, with the de facto standard training of fully convolutional networks (FCNs) for semantic segmentation being agnostic towards the `structure' of the predicted label maps, valuable complementary information about the global quality of the segmentation lies idle. In order to tap into this potential, we propose utilizing an adversarial network which discriminates between expert and generated annotations in order to train FCNs for semantic segmentation. Because the adversary constitutes a learned parametrization of what makes a good segmentation at a global level, we hypothesize that the method holds particular advantages for segmentation tasks on complex structured, small datasets. This holds true in our experiments: We learn to segment aggressive prostate cancer utilizing MRI images of 152 patients and show that the proposed scheme is superior over the de facto standard in terms of the detection sensitivity and the dice-score for aggressive prostate cancer. The achieved relative gains are shown to be particularly pronounced in the small dataset limit.



### 3D Scanning System for Automatic High-Resolution Plant Phenotyping
- **Arxiv ID**: http://arxiv.org/abs/1702.08112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.08112v1)
- **Published**: 2017-02-26 23:52:00+00:00
- **Updated**: 2017-02-26 23:52:00+00:00
- **Authors**: Chuong V Nguyen, Jurgen Fripp, David R Lovell, Robert Furbank, Peter Kuffner, Helen Daily, Xavier Sirault
- **Comment**: 8 papes, DICTA 2016
- **Journal**: In Digital Image Computing: Techniques and Applications (DICTA),
  2016 International Conference on, pp. 1-8. IEEE, 2016
- **Summary**: Thin leaves, fine stems, self-occlusion, non-rigid and slowly changing structures make plants difficult for three-dimensional (3D) scanning and reconstruction -- two critical steps in automated visual phenotyping. Many current solutions such as laser scanning, structured light, and multiview stereo can struggle to acquire usable 3D models because of limitations in scanning resolution and calibration accuracy. In response, we have developed a fast, low-cost, 3D scanning platform to image plants on a rotating stage with two tilting DSLR cameras centred on the plant. This uses new methods of camera calibration and background removal to achieve high-accuracy 3D reconstruction. We assessed the system's accuracy using a 3D visual hull reconstruction algorithm applied on 2 plastic models of dicotyledonous plants, 2 sorghum plants and 2 wheat plants across different sets of tilt angles. Scan times ranged from 3 minutes (to capture 72 images using 2 tilt angles), to 30 minutes (to capture 360 images using 10 tilt angles). The leaf lengths, widths, areas and perimeters of the plastic models were measured manually and compared to measurements from the scanning system: results were within 3-4% of each other. The 3D reconstructions obtained with the scanning system show excellent geometric agreement with all six plant specimens, even plants with thin leaves and fine stems.



