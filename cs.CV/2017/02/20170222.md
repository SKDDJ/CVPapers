# Arxiv Papers in cs.CV on 2017-02-22
### Unsupervised Diverse Colorization via Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.06674v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1702.06674v2)
- **Published**: 2017-02-22 04:34:31+00:00
- **Updated**: 2017-07-01 10:57:03+00:00
- **Authors**: Yun Cao, Zhiming Zhou, Weinan Zhang, Yong Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Colorization of grayscale images has been a hot topic in computer vision. Previous research mainly focuses on producing a colored image to match the original one. However, since many colors share the same gray value, an input grayscale image could be diversely colored while maintaining its reality. In this paper, we design a novel solution for unsupervised diverse colorization. Specifically, we leverage conditional generative adversarial networks to model the distribution of real-world item colors, in which we develop a fully convolutional generator with multi-layer noise to enhance diversity, with multi-layer condition concatenation to maintain reality, and with stride 1 to keep spatial information. With such a novel network architecture, the model yields highly competitive performance on the open LSUN bedroom dataset. The Turing test of 80 humans further indicates our generated color schemes are highly convincible.



### Using Deep Learning and Google Street View to Estimate the Demographic Makeup of the US
- **Arxiv ID**: http://arxiv.org/abs/1702.06683v2
- **DOI**: 10.1073/pnas.1700035114
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06683v2)
- **Published**: 2017-02-22 06:20:13+00:00
- **Updated**: 2017-03-02 05:11:11+00:00
- **Authors**: Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei
- **Comment**: 41 pages including supplementary material. Under review at PNAS
- **Journal**: None
- **Summary**: The United States spends more than $1B each year on initiatives such as the American Community Survey (ACS), a labor-intensive door-to-door study that measures statistics relating to race, gender, education, occupation, unemployment, and other demographic factors. Although a comprehensive source of data, the lag between demographic changes and their appearance in the ACS can exceed half a decade. As digital imagery becomes ubiquitous and machine vision techniques improve, automated data analysis may provide a cheaper and faster alternative. Here, we present a method that determines socioeconomic trends from 50 million images of street scenes, gathered in 200 American cities by Google Street View cars. Using deep learning-based computer vision techniques, we determined the make, model, and year of all motor vehicles encountered in particular neighborhoods. Data from this census of motor vehicles, which enumerated 22M automobiles in total (8% of all automobiles in the US), was used to accurately estimate income, race, education, and voting patterns, with single-precinct resolution. (The average US precinct contains approximately 1000 people.) The resulting associations are surprisingly simple and powerful. For instance, if the number of sedans encountered during a 15-minute drive through a city is higher than the number of pickup trucks, the city is likely to vote for a Democrat during the next Presidential election (88% chance); otherwise, it is likely to vote Republican (82%). Our results suggest that automated systems for monitoring demographic trends may effectively complement labor-intensive approaches, with the potential to detect trends with fine spatial resolution, in close to real time.



### Task-driven Visual Saliency and Attention-based Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1702.06700v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1702.06700v1)
- **Published**: 2017-02-22 08:19:38+00:00
- **Updated**: 2017-02-22 08:19:38+00:00
- **Authors**: Yuetan Lin, Zhangyang Pang, Donghui Wang, Yueting Zhuang
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Visual question answering (VQA) has witnessed great progress since May, 2015 as a classic problem unifying visual and textual data into a system. Many enlightening VQA works explore deep into the image and question encodings and fusing methods, of which attention is the most effective and infusive mechanism. Current attention based methods focus on adequate fusion of visual and textual features, but lack the attention to where people focus to ask questions about the image. Traditional attention based methods attach a single value to the feature at each spatial location, which losses many useful information. To remedy these problems, we propose a general method to perform saliency-like pre-selection on overlapped region features by the interrelation of bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication based attention method to capture more competent correlation information between visual and textual features. We conduct experiments on the large-scale COCO-VQA dataset and analyze the effectiveness of our model demonstrated by strong empirical results.



### 3D Reconstruction of Temples in the Special Region of Yogyakarta By Using Close-Range Photogrammetry
- **Arxiv ID**: http://arxiv.org/abs/1702.06722v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06722v1)
- **Published**: 2017-02-22 09:32:09+00:00
- **Updated**: 2017-02-22 09:32:09+00:00
- **Authors**: Adityo Priyandito Utomo, Canggih Puspo Wibowo
- **Comment**: Semnasteknomedia 2017, 5 pages
- **Journal**: None
- **Summary**: Object reconstruction is one of the main problems in cultural heritage preservation. This problem is due to lack of data in documentation. Thus in this research we presented a method of 3D reconstruction using close-range photogrammetry. We collected 1319 photos from five temples in Yogyakarta. Using A-KAZE algorithm, keypoints of each image were obtained. Then we employed LIOP to create feature descriptor from it. After performing feature matching, L1RA was utilized to create sparse point clouds. In order to generate the geometry shape, MVS was used. Finally, FSSR and Large Scale Texturing were employed to deal with the surface and texture of the object. The quality of the reconstructed 3D model was measured by comparing the 3D images of the model with the original photos utilizing SSIM. The results showed that in terms of quality, our method was on par with other commercial method such as PhotoModeler and PhotoScan.



### MomentsNet: a simple learning-free method for binary image recognition
- **Arxiv ID**: http://arxiv.org/abs/1702.06767v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06767v1)
- **Published**: 2017-02-22 12:08:09+00:00
- **Updated**: 2017-02-22 12:08:09+00:00
- **Authors**: Jiasong Wu, Shijie Qiu, Youyong Kong, Yang Chen, Lotfi Senhadji, Huazhong Shu
- **Comment**: 5 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: In this paper, we propose a new simple and learning-free deep learning network named MomentsNet, whose convolution layer, nonlinear processing layer and pooling layer are constructed by Moments kernels, binary hashing and block-wise histogram, respectively. Twelve typical moments (including geometrical moment, Zernike moment, Tchebichef moment, etc.) are used to construct the MomentsNet whose recognition performance for binary image is studied. The results reveal that MomentsNet has better recognition performance than its corresponding moments in almost all cases and ZernikeNet achieves the best recognition performance among MomentsNet constructed by twelve moments. ZernikeNet also shows better recognition performance on binary image database than that of PCANet, which is a learning-based deep learning network.



### Boosted Multiple Kernel Learning for First-Person Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1702.06799v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.06799v2)
- **Published**: 2017-02-22 13:56:54+00:00
- **Updated**: 2017-06-05 08:23:41+00:00
- **Authors**: Fatih Ozkan, Mehmet Ali Arabaci, Elif Surer, Alptekin Temizel
- **Comment**: First published in the Proceedings of the 25th European Signal
  Processing Conference (EUSIPCO-2017) in 2017, published by EURASIP
- **Journal**: None
- **Summary**: Activity recognition from first-person (ego-centric) videos has recently gained attention due to the increasing ubiquity of the wearable cameras. There has been a surge of efforts adapting existing feature descriptors and designing new descriptors for the first-person videos. An effective activity recognition system requires selection and use of complementary features and appropriate kernels for each feature. In this study, we propose a data-driven framework for first-person activity recognition which effectively selects and combines features and their respective kernels during the training. Our experimental results show that use of Multiple Kernel Learning (MKL) and Boosted MKL in first-person activity recognition problem exhibits improved results in comparison to the state-of-the-art. In addition, these techniques enable the expansion of the framework with new features in an efficient and convenient way.



### Learning Deep Features via Congenerous Cosine Loss for Person Recognition
- **Arxiv ID**: http://arxiv.org/abs/1702.06890v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1702.06890v2)
- **Published**: 2017-02-22 16:45:48+00:00
- **Updated**: 2017-03-31 17:27:50+00:00
- **Authors**: Yu Liu, Hongyang Li, Xiaogang Wang
- **Comment**: Post-rebuttal update. Add some comparison results; correct some
  technical part; rewrite some sections to make it more readable; code link
  available
- **Journal**: None
- **Summary**: Person recognition aims at recognizing the same identity across time and space with complicated scenes and similar appearance. In this paper, we propose a novel method to address this task by training a network to obtain robust and representative features. The intuition is that we directly compare and optimize the cosine distance between two features - enlarging inter-class distinction as well as alleviating inner-class variance. We propose a congenerous cosine loss by minimizing the cosine distance between samples and their cluster centroid in a cooperative way. Such a design reduces the complexity and could be implemented via softmax with normalized inputs. Our method also differs from previous work in person recognition that we do not conduct a second training on the test subset. The identity of a person is determined by measuring the similarity from several body regions in the reference set. Experimental results show that the proposed approach achieves better classification accuracy against previous state-of-the-arts.



### Regularizing Face Verification Nets For Pain Intensity Regression
- **Arxiv ID**: http://arxiv.org/abs/1702.06925v3
- **DOI**: 10.13140/RG.2.2.20841.49765
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1702.06925v3)
- **Published**: 2017-02-22 18:15:42+00:00
- **Updated**: 2017-06-01 17:49:56+00:00
- **Authors**: Feng Wang, Xiang Xiang, Chang Liu, Trac D. Tran, Austin Reiter, Gregory D. Hager, Harry Quon, Jian Cheng, Alan L. Yuille
- **Comment**: 5 pages, 3 figure; Camera-ready version to appear at IEEE ICIP 2017
- **Journal**: None
- **Summary**: Limited labeled data are available for the research of estimating facial expression intensities. For instance, the ability to train deep networks for automated pain assessment is limited by small datasets with labels of patient-reported pain intensities. Fortunately, fine-tuning from a data-extensive pre-trained domain, such as face verification, can alleviate this problem. In this paper, we propose a network that fine-tunes a state-of-the-art face verification network using a regularized regression loss and additional data with expression labels. In this way, the expression intensity regression task can benefit from the rich feature representations trained on a huge amount of data for face verification. The proposed regularized deep regressor is applied to estimate the pain expression intensity and verified on the widely-used UNBC-McMaster Shoulder-Pain dataset, achieving the state-of-the-art performance. A weighted evaluation metric is also proposed to address the imbalance issue of different pain intensities.



### Lensless computational imaging through deep learning
- **Arxiv ID**: http://arxiv.org/abs/1702.08516v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1702.08516v2)
- **Published**: 2017-02-22 20:55:26+00:00
- **Updated**: 2017-06-26 05:49:50+00:00
- **Authors**: Ayan Sinha, Justin Lee, Shuai Li, George Barbastathis
- **Comment**: 8 pages, 13 figures
- **Journal**: None
- **Summary**: Deep learning has been proven to yield reliably generalizable answers to numerous classification and decision tasks. Here, we demonstrate for the first time, to our knowledge, that deep neural networks (DNNs) can be trained to solve inverse problems in computational imaging. We experimentally demonstrate a lens-less imaging system where a DNN was trained to recover a phase object given a raw intensity image recorded some distance away.



### Synthesising Dynamic Textures using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.07006v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07006v1)
- **Published**: 2017-02-22 21:03:49+00:00
- **Updated**: 2017-02-22 21:03:49+00:00
- **Authors**: Christina M. Funke, Leon A. Gatys, Alexander S. Ecker, Matthias Bethge
- **Comment**: None
- **Journal**: None
- **Summary**: Here we present a parametric model for dynamic textures. The model is based on spatiotemporal summary statistics computed from the feature representations of a Convolutional Neural Network (CNN) trained on object recognition. We demonstrate how the model can be used to synthesise new samples of dynamic textures and to predict motion in simple movies.



### CT Image Denoising with Perceptive Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.07019v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07019v1)
- **Published**: 2017-02-22 21:50:55+00:00
- **Updated**: 2017-02-22 21:50:55+00:00
- **Authors**: Qingsong Yang, Pingkun Yan, Mannudeep K. Kalra, Ge Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Increasing use of CT in modern medical practice has raised concerns over associated radiation dose. Reduction of radiation dose associated with CT can increase noise and artifacts, which can adversely affect diagnostic confidence. Denoising of low-dose CT images on the other hand can help improve diagnostic confidence, which however is a challenging problem due to its ill-posed nature, since one noisy image patch may correspond to many different output patches. In the past decade, machine learning based approaches have made quite impressive progress in this direction. However, most of those methods, including the recently popularized deep learning techniques, aim for minimizing mean-squared-error (MSE) between a denoised CT image and the ground truth, which results in losing important structural details due to over-smoothing, although the PSNR based performance measure looks great. In this work, we introduce a new perceptual similarity measure as the objective function for a deep convolutional neural network to facilitate CT image denoising. Instead of directly computing MSE for pixel-to-pixel intensity loss, we compare the perceptual features of a denoised output against those of the ground truth in a feature space. Therefore, our proposed method is capable of not only reducing the image noise levels, but also keeping the critical structural information at the same time. Promising results have been obtained in our experiments with a large number of CT images.



### Convolutional Neural Network Committees for Melanoma Classification with Classical And Expert Knowledge Based Image Transforms Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1702.07025v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07025v2)
- **Published**: 2017-02-22 22:17:13+00:00
- **Updated**: 2017-03-15 11:50:58+00:00
- **Authors**: Cristina Nader Vasconcelos, Bárbara Nader Vasconcelos
- **Comment**: None
- **Journal**: None
- **Summary**: Skin cancer is a major public health problem, as is the most common type of cancer and represents more than half of cancer diagnoses worldwide. Early detection influences the outcome of the disease and motivates our work. We investigate the composition of CNN committees and data augmentation for the the ISBI 2017 Melanoma Classification Challenge (named Skin Lesion Analysis towards Melanoma Detection) facing the peculiarities of dealing with such a small, unbalanced, biological database. For that, we explore committees of Convolutional Neural Networks trained over the ISBI challenge training dataset artificially augmented by both classical image processing transforms and image warping guided by specialist knowledge about the lesion axis and improve the final classifier invariance to common melanoma variations.



