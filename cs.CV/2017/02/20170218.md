# Arxiv Papers in cs.CV on 2017-02-18
### Soft + Hardwired Attention: An LSTM Framework for Human Trajectory Prediction and Abnormal Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1702.05552v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1702.05552v1)
- **Published**: 2017-02-18 01:08:18+00:00
- **Updated**: 2017-02-18 01:08:18+00:00
- **Authors**: Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: None
- **Journal**: None
- **Summary**: As humans we possess an intuitive ability for navigation which we master through years of practice; however existing approaches to model this trait for diverse tasks including monitoring pedestrian flow and detecting abnormal events have been limited by using a variety of hand-crafted features. Recent research in the area of deep-learning has demonstrated the power of learning features directly from the data; and related research in recurrent neural networks has shown exemplary results in sequence-to-sequence problems such as neural machine translation and neural image caption generation. Motivated by these approaches, we propose a novel method to predict the future motion of a pedestrian given a short history of their, and their neighbours, past behaviour. The novelty of the proposed method is the combined attention model which utilises both "soft attention" as well as "hard-wired" attention in order to map the trajectory information from the local neighbourhood to the future positions of the pedestrian of interest. We illustrate how a simple approximation of attention weights (i.e hard-wired) can be merged together with soft attention weights in order to make our model applicable for challenging real world scenarios with hundreds of neighbours. The navigational capability of the proposed method is tested on two challenging publicly available surveillance databases where our model outperforms the current-state-of-the-art methods. Additionally, we illustrate how the proposed architecture can be directly applied for the task of abnormal event detection without handcrafting the features.



### Defect detection for patterned fabric images based on GHOG and low-rank decomposition
- **Arxiv ID**: http://arxiv.org/abs/1702.05555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05555v1)
- **Published**: 2017-02-18 01:24:49+00:00
- **Updated**: 2017-02-18 01:24:49+00:00
- **Authors**: Chunlei Li, Guangshuai Gao, Zhoufeng Liu, Di Huang, Sheng Liu, Miao Yu
- **Comment**: None
- **Journal**: None
- **Summary**: In order to accurately detect defects in patterned fabric images, a novel detection algorithm based on Gabor-HOG (GHOG) and low-rank decomposition is proposed in this paper. Defect-free pattern fabric images have the specified direction, while defects damage their regularity of direction. Therefore, a direction-aware descriptor is designed, denoted as GHOG, a combination of Gabor and HOG, which is extremely valuable for localizing the defect region. Upon devising a powerful directional descriptor, an efficient low-rank decomposition model is constructed to divide the matrix generated by the directional feature extracted from image blocks into a low-rank matrix (background information) and a sparse matrix (defect information). A nonconvex log det(.) as a smooth surrogate function for the rank instead of the nuclear norm is also exploited to improve the efficiency of the low-rank model. Moreover, the computational efficiency is further improved by utilizing the alternative direction method of multipliers (ADMM). Thereafter, the saliency map generated by the sparse matrix is segmented via the optimal threshold algorithm to locate the defect regions. Experimental results show that the proposed method can effectively detect patterned fabric defects and outperform the state-of-the-art methods.



### The Ciona17 Dataset for Semantic Segmentation of Invasive Species in a Marine Aquaculture Environment
- **Arxiv ID**: http://arxiv.org/abs/1702.05564v1
- **DOI**: 10.5683/SP/NTUOK9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05564v1)
- **Published**: 2017-02-18 03:40:33+00:00
- **Updated**: 2017-02-18 03:40:33+00:00
- **Authors**: Angus Galloway, Graham W. Taylor, Aaron Ramsay, Medhat Moussa
- **Comment**: Submitted to the Conference on Computer and Robot Vision (CRV) 2017
- **Journal**: None
- **Summary**: An original dataset for semantic segmentation, Ciona17, is introduced, which to the best of the authors' knowledge, is the first dataset of its kind with pixel-level annotations pertaining to invasive species in a marine environment. Diverse outdoor illumination, a range of object shapes, colour, and severe occlusion provide a significant real world challenge for the computer vision community. An accompanying ground-truthing tool for superpixel labeling, Truth and Crop, is also introduced. Finally, we provide a baseline using a variant of Fully Convolutional Networks, and report results in terms of the standard mean intersection over union (mIoU) metric.



### Collaborative Deep Reinforcement Learning for Joint Object Search
- **Arxiv ID**: http://arxiv.org/abs/1702.05573v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05573v1)
- **Published**: 2017-02-18 06:00:45+00:00
- **Updated**: 2017-02-18 06:00:45+00:00
- **Authors**: Xiangyu Kong, Bo Xin, Yizhou Wang, Gang Hua
- **Comment**: None
- **Journal**: None
- **Summary**: We examine the problem of joint top-down active search of multiple objects under interaction, e.g., person riding a bicycle, cups held by the table, etc.. Such objects under interaction often can provide contextual cues to each other to facilitate more efficient search. By treating each detector as an agent, we present the first collaborative multi-agent deep reinforcement learning algorithm to learn the optimal policy for joint active object localization, which effectively exploits such beneficial contextual information. We learn inter-agent communication through cross connections with gates between the Q-networks, which is facilitated by a novel multi-agent deep Q-learning algorithm with joint exploitation sampling. We verify our proposed method on multiple object detection benchmarks. Not only does our model help to improve the performance of state-of-the-art active localization models, it also reveals interesting co-detection patterns that are intuitively interpretable.



### Brain Inspired Cognitive Model with Attention for Self-Driving Cars
- **Arxiv ID**: http://arxiv.org/abs/1702.05596v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1702.05596v1)
- **Published**: 2017-02-18 10:47:16+00:00
- **Updated**: 2017-02-18 10:47:16+00:00
- **Authors**: Shitao Chen, Songyi Zhang, Jinghao Shang, Badong Chen, Nanning Zheng
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: Perception-driven approach and end-to-end system are two major vision-based frameworks for self-driving cars. However, it is difficult to introduce attention and historical information of autonomous driving process, which are the essential factors for achieving human-like driving into these two methods. In this paper, we propose a novel model for self-driving cars named brain-inspired cognitive model with attention (CMA). This model consists of three parts: a convolutional neural network for simulating human visual cortex, a cognitive map built to describe relationships between objects in complex traffic scene and a recurrent neural network that combines with the real-time updated cognitive map to implement attention mechanism and long-short term memory. The benefit of our model is that can accurately solve three tasks simultaneously:1) detection of the free space and boundaries of the current and adjacent lanes. 2)estimation of obstacle distance and vehicle attitude, and 3) learning of driving behavior and decision making from human driver. More significantly, the proposed model could accept external navigating instructions during an end-to-end driving process. For evaluation, we build a large-scale road-vehicle dataset which contains more than forty thousand labeled road images captured by three cameras on our self-driving car. Moreover, human driving activities and vehicle states are recorded in the meanwhile.



### 3D Face Reconstruction with Geometry Details from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1702.05619v2
- **DOI**: 10.1109/TIP.2018.2845697
- **Categories**: **cs.CV**, I.4.5; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1702.05619v2)
- **Published**: 2017-02-18 15:03:14+00:00
- **Updated**: 2018-06-11 11:47:34+00:00
- **Authors**: Luo Jiang, Juyong Zhang, Bailin Deng, Hao Li, Ligang Liu
- **Comment**: Accepted by IEEE Transactions on Image Processing, 2018
- **Journal**: None
- **Summary**: 3D face reconstruction from a single image is a classical and challenging problem, with wide applications in many areas. Inspired by recent works in face animation from RGB-D or monocular video inputs, we develop a novel method for reconstructing 3D faces from unconstrained 2D images, using a coarse-to-fine optimization strategy. First, a smooth coarse 3D face is generated from an example-based bilinear face model, by aligning the projection of 3D face landmarks with 2D landmarks detected from the input image. Afterwards, using local corrective deformation fields, the coarse 3D face is refined using photometric consistency constraints, resulting in a medium face shape. Finally, a shape-from-shading method is applied on the medium face to recover fine geometric details. Our method outperforms state-of-the-art approaches in terms of accuracy and detail recovery, which is demonstrated in extensive experiments using real world models and publicly available datasets.



### Revisiting Graph Construction for Fast Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1702.05650v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05650v2)
- **Published**: 2017-02-18 20:37:42+00:00
- **Updated**: 2017-12-02 22:40:39+00:00
- **Authors**: Zizhao Zhang, Fuyong Xing, Hanzi Wang, Yan Yan, Ying Huang, Xiaoshuang Shi, Lin Yang
- **Comment**: To appear in PR
- **Journal**: None
- **Summary**: In this paper, we propose a simple but effective method for fast image segmentation. We re-examine the locality-preserving character of spectral clustering by constructing a graph over image regions with both global and local connections. Our novel approach to build graph connections relies on two key observations: 1) local region pairs that co-occur frequently will have a high probability to reside on a common object; 2) spatially distant regions in a common object often exhibit similar visual saliency, which implies their neighborship in a manifold. We present a novel energy function to efficiently conduct graph partitioning. Based on multiple high quality partitions, we show that the generated eigenvector histogram based representation can automatically drive effective unary potentials for a hierarchical random field model to produce multi-class segmentation. Sufficient experiments, on the BSDS500 benchmark, large-scale PASCAL VOC and COCO datasets, demonstrate the competitive segmentation accuracy and significantly improved efficiency of our proposed method compared with other state of the arts.



### MAT: A Multimodal Attentive Translator for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1702.05658v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05658v3)
- **Published**: 2017-02-18 21:35:06+00:00
- **Updated**: 2017-08-10 14:29:19+00:00
- **Authors**: Chang Liu, Fuchun Sun, Changhu Wang, Feng Wang, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we formulate the problem of image captioning as a multimodal translation task. Analogous to machine translation, we present a sequence-to-sequence recurrent neural networks (RNN) model for image caption generation. Different from most existing work where the whole image is represented by convolutional neural network (CNN) feature, we propose to represent the input image as a sequence of detected objects which feeds as the source sequence of the RNN model. In this way, the sequential representation of an image can be naturally translated to a sequence of words, as the target sequence of the RNN model. To represent the image in a sequential way, we extract the objects features in the image and arrange them in a order using convolutional neural networks. To further leverage the visual information from the encoded objects, a sequential attention layer is introduced to selectively attend to the objects that are related to generate corresponding words in the sentences. Extensive experiments are conducted to validate the proposed approach on popular benchmark dataset, i.e., MS COCO, and the proposed model surpasses the state-of-the-art methods in all metrics following the dataset splits of previous work. The proposed approach is also evaluated by the evaluation server of MS COCO captioning challenge, and achieves very competitive results, e.g., a CIDEr of 1.029 (c5) and 1.064 (c40).



### The Game Imitation: Deep Supervised Convolutional Networks for Quick Video Game AI
- **Arxiv ID**: http://arxiv.org/abs/1702.05663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05663v1)
- **Published**: 2017-02-18 22:15:25+00:00
- **Updated**: 2017-02-18 22:15:25+00:00
- **Authors**: Zhao Chen, Darvin Yi
- **Comment**: 11 pages, 12 figures
- **Journal**: None
- **Summary**: We present a vision-only model for gaming AI which uses a late integration deep convolutional network architecture trained in a purely supervised imitation learning context. Although state-of-the-art deep learning models for video game tasks generally rely on more complex methods such as deep-Q learning, we show that a supervised model which requires substantially fewer resources and training time can already perform well at human reaction speeds on the N64 classic game Super Smash Bros. We frame our learning task as a 30-class classification problem, and our CNN model achieves 80% top-1 and 95% top-3 validation accuracy. With slight test-time fine-tuning, our model is also competitive during live simulation with the highest-level AI built into the game. We will further show evidence through network visualizations that the network is successfully leveraging temporal information during inference to aid in decision making. Our work demonstrates that supervised CNN models can provide good performance in challenging policy prediction tasks while being significantly simpler and more lightweight than alternatives.



### Robust Shape Registration using Fuzzy Correspondences
- **Arxiv ID**: http://arxiv.org/abs/1702.05664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05664v1)
- **Published**: 2017-02-18 22:22:57+00:00
- **Updated**: 2017-02-18 22:22:57+00:00
- **Authors**: Abhishek Kolagunda, Scott Sorensen, Philip Saponaro, Wayne Treible, Chandra Kambhamettu
- **Comment**: None
- **Journal**: None
- **Summary**: Shape registration is the process of aligning one 3D model to another. Most previous methods to align shapes with no known correspondences attempt to solve for both the transformation and correspondences iteratively. We present a shape registration approach that solves for the transformation using fuzzy correspondences to maximize the overlap between the given shape and the target shape. A coarse to fine approach with Levenberg-Marquardt method is used for optimization. Real and synthetic experiments show our approach is robust and outperforms other state of the art methods when point clouds are noisy, sparse, and have non-uniform density. Experiments show our method is more robust to initialization and can handle larger scale changes and rotation than other methods. We also show that the approach can be used for 2D-3D alignment via ray-point alignment.



