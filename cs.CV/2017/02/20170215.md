# Arxiv Papers in cs.CV on 2017-02-15
### Learning from Ambiguously Labeled Face Images
- **Arxiv ID**: http://arxiv.org/abs/1702.04455v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04455v2)
- **Published**: 2017-02-15 03:31:17+00:00
- **Updated**: 2017-07-01 05:35:22+00:00
- **Authors**: Ching-Hui Chen, Vishal M. Patel, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Learning a classifier from ambiguously labeled face images is challenging since training images are not always explicitly-labeled. For instance, face images of two persons in a news photo are not explicitly labeled by their names in the caption. We propose a Matrix Completion for Ambiguity Resolution (MCar) method for predicting the actual labels from ambiguously labeled images. This step is followed by learning a standard supervised classifier from the disambiguated labels to classify new images. To prevent the majority labels from dominating the result of MCar, we generalize MCar to a weighted MCar (WMCar) that handles label imbalance. Since WMCar outputs a soft labeling vector of reduced ambiguity for each instance, we can iteratively refine it by feeding it as the input to WMCar. Nevertheless, such an iterative implementation can be affected by the noisy soft labeling vectors, and thus the performance may degrade. Our proposed Iterative Candidate Elimination (ICE) procedure makes the iterative ambiguity resolution possible by gradually eliminating a portion of least likely candidates in ambiguously labeled face. We further extend MCar to incorporate the labeling constraints between instances when such prior knowledge is available. Compared to existing methods, our approach demonstrates improvement on several ambiguously labeled datasets.



### Analyzing the Weighted Nuclear Norm Minimization and Nuclear Norm Minimization based on Group Sparse Representation
- **Arxiv ID**: http://arxiv.org/abs/1702.04463v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04463v5)
- **Published**: 2017-02-15 04:28:52+00:00
- **Updated**: 2018-07-19 03:11:49+00:00
- **Authors**: Zhiyuan Zha, Xin Yuan, Bei Li, Xinggan Zhang, Xin Liu, Lan Tang, Ying-Chang Liang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1611.08983
- **Journal**: None
- **Summary**: Rank minimization methods have attracted considerable interest in various areas, such as computer vision and machine learning. The most representative work is nuclear norm minimization (NNM), which can recover the matrix rank exactly under some restricted and theoretical guarantee conditions. However, for many real applications, NNM is not able to approximate the matrix rank accurately, since it often tends to over-shrink the rank components. To rectify the weakness of NNM, recent advances have shown that weighted nuclear norm minimization (WNNM) can achieve a better matrix rank approximation than NNM, which heuristically set the weight being inverse to the singular values. However, it still lacks a sound mathematical explanation on why WNNM is more feasible than NNM. In this paper, we propose a scheme to analyze WNNM and NNM from the perspective of the group sparse representation. Specifically, we design an adaptive dictionary to bridge the gap between the group sparse representation and the rank minimization models. Based on this scheme, we provide a mathematical derivation to explain why WNNM is more feasible than NNM. Moreover, due to the heuristical set of the weight, WNNM sometimes pops out error in the operation of SVD, and thus we present an adaptive weight setting scheme to avoid this error. We then employ the proposed scheme on two low-level vision tasks including image denoising and image inpainting. Experimental results demonstrate that WNNM is more feasible than NNM and the proposed scheme outperforms many current state-of-the-art methods.



### Deep Heterogeneous Feature Fusion for Template-Based Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1702.04471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04471v1)
- **Published**: 2017-02-15 06:23:05+00:00
- **Updated**: 2017-02-15 06:23:05+00:00
- **Authors**: Navaneeth Bodla, Jingxiao Zheng, Hongyu Xu, Jun-Cheng Chen, Carlos Castillo, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep learning has yielded impressive performance for face recognition, many studies have shown that different networks learn different feature maps: while some networks are more receptive to pose and illumination others appear to capture more local information. Thus, in this work, we propose a deep heterogeneous feature fusion network to exploit the complementary information present in features generated by different deep convolutional neural networks (DCNNs) for template-based face recognition, where a template refers to a set of still face images or video frames from different sources which introduces more blur, pose, illumination and other variations than traditional face datasets. The proposed approach efficiently fuses the discriminative information of different deep features by 1) jointly learning the non-linear high-dimensional projection of the deep features and 2) generating a more discriminative template representation which preserves the inherent geometry of the deep features in the feature space. Experimental results on the IARPA Janus Challenge Set 3 (Janus CS3) dataset demonstrate that the proposed method can effectively improve the recognition performance. In addition, we also present a series of covariate experiments on the face verification task for in-depth qualitative evaluations for the proposed approach.



### Recognizing Dynamic Scenes with Deep Dual Descriptor based on Key Frames and Key Segments
- **Arxiv ID**: http://arxiv.org/abs/1702.04479v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04479v2)
- **Published**: 2017-02-15 06:59:01+00:00
- **Updated**: 2017-02-16 07:14:19+00:00
- **Authors**: Sungeun Hong, Jongbin Ryu, Woobin Im, Hyun S. Yang
- **Comment**: 10 pages, 7 figures, 8 tables
- **Journal**: None
- **Summary**: Recognizing dynamic scenes is one of the fundamental problems in scene understanding, which categorizes moving scenes such as a forest fire, landslide, or avalanche. While existing methods focus on reliable capturing of static and dynamic information, few works have explored frame selection from a dynamic scene sequence. In this paper, we propose dynamic scene recognition using a deep dual descriptor based on `key frames' and `key segments.' Key frames that reflect the feature distribution of the sequence with a small number are used for capturing salient static appearances. Key segments, which are captured from the area around each key frame, provide an additional discriminative power by dynamic patterns within short time intervals. To this end, two types of transferred convolutional neural network features are used in our approach. A fully connected layer is used to select the key frames and key segments, while the convolutional layer is used to describe them. We conducted experiments using public datasets as well as a new dataset comprised of 23 dynamic scene classes with 10 videos per class. The evaluation results demonstrated the state-of-the-art performance of the proposed method.



### Application of Multi-channel 3D-cube Successive Convolution Network for Convective Storm Nowcasting
- **Arxiv ID**: http://arxiv.org/abs/1702.04517v5
- **DOI**: 10.1109/BigData47090.2019.9005568
- **Categories**: **cs.CV**, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/1702.04517v5)
- **Published**: 2017-02-15 09:35:45+00:00
- **Updated**: 2019-11-05 00:27:48+00:00
- **Authors**: Wei Zhang, Lei Han, Juanzhen Sun, Hanyang Guo, Jie Dai
- **Comment**: 9 pages, 9 figures, 3 tables, This is an expanded version of the
  paper accepted by 2019 IEEE International Conference on Big Data. The
  copyright of this paper has been transferred to the IEEE, please comply with
  the copyright of the IEEE
- **Journal**: 2019 IEEE International Conference on Big Data (Big Data), Los
  Angeles, CA, USA, 2019, pp. 1705-1710
- **Summary**: Convective storm nowcasting has attracted substantial attention in various fields. Existing methods under a deep learning framework rely primarily on radar data. Although they perform nowcast storm advection well, it is still challenging to nowcast storm initiation and growth, due to the limitations of the radar observations. This paper describes the first attempt to nowcast storm initiation, growth, and advection simultaneously under a deep learning framework using multi-source meteorological data. To this end, we present a multi-channel 3D-cube successive convolution network (3D-SCN). As real-time re-analysis meteorological data can now provide valuable atmospheric boundary layer thermal dynamic information, which is essential to predict storm initiation and growth, both raw 3D radar and re-analysis data are used directly without any handcraft feature engineering. These data are formulated as multi-channel 3D cubes, to be fed into our network, which are convolved by cross-channel 3D convolutions. By stacking successive convolutional layers without pooling, we build an end-to-end trainable model for nowcasting. Experimental results show that deep learning methods achieve better performance than traditional extrapolation methods. The qualitative analyses of 3D-SCN show encouraging results of nowcasting of storm initiation, growth, and advection.



### A deep learning model integrating FCNNs and CRFs for brain tumor segmentation
- **Arxiv ID**: http://arxiv.org/abs/1702.04528v3
- **DOI**: 10.1016/j.media.2017.10.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04528v3)
- **Published**: 2017-02-15 10:06:13+00:00
- **Updated**: 2017-11-10 02:49:49+00:00
- **Authors**: Xiaomei Zhao, Yihong Wu, Guidong Song, Zhenye Li, Yazhuo Zhang, Yong Fan
- **Comment**: This version was accepted in the journal Medical Image Analysis
- **Journal**: None
- **Summary**: Accurate and reliable brain tumor segmentation is a critical component in cancer diagnosis, treatment planning, and treatment outcome evaluation. Build upon successful deep learning techniques, a novel brain tumor segmentation method is developed by integrating fully convolutional neural networks (FCNNs) and Conditional Random Fields (CRFs) in a unified framework to obtain segmentation results with appearance and spatial consistency. We train a deep learning based segmentation model using 2D image patches and image slices in following steps: 1) training FCNNs using image patches; 2) training CRFs as Recurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNs fixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices. Particularly, we train 3 segmentation models using 2D image patches and slices obtained in axial, coronal and sagittal views respectively, and combine them to segment brain tumors using a voting based fusion strategy. Our method could segment brain images slice-by-slice, much faster than those based on image patches. We have evaluated our method based on imaging data provided by the Multimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015 and BRATS 2016. The experimental results have demonstrated that our method could build a segmentation model with Flair, T1c, and T2 scans and achieve competitive performance as those built with Flair, T1, T1c, and T2 scans.



### Normalized Total Gradient: A New Measure for Multispectral Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1702.04562v1
- **DOI**: 10.1109/TIP.2017.2776753
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04562v1)
- **Published**: 2017-02-15 11:52:38+00:00
- **Updated**: 2017-02-15 11:52:38+00:00
- **Authors**: Shu-Jie Chen, Hui-Liang Shen
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: Image registration is a fundamental issue in multispectral image processing. In filter wheel based multispectral imaging systems, the non-coplanar placement of the filters always causes the misalignment of multiple channel images. The selective characteristic of spectral response in multispectral imaging raises two challenges to image registration. First, the intensity levels of a local region may be different in individual channel images. Second, the local intensity may vary rapidly in some channel images while keeps stationary in others. Conventional multimodal measures, such as mutual information, correlation coefficient, and correlation ratio, can register images with different regional intensity levels, but will fail in the circumstance of severe local intensity variation. In this paper, a new measure, namely normalized total gradient (NTG), is proposed for multispectral image registration. The NTG is applied on the difference between two channel images. This measure is based on the key assumption (observation) that the gradient of difference image between two aligned channel images is sparser than that between two misaligned ones. A registration framework, which incorporates image pyramid and global/local optimization, is further introduced for rigid transform. Experimental results validate that the proposed method is effective for multispectral image registration and performs better than conventional methods.



### Deep Multi-camera People Detection
- **Arxiv ID**: http://arxiv.org/abs/1702.04593v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04593v3)
- **Published**: 2017-02-15 13:16:41+00:00
- **Updated**: 2017-07-23 19:14:01+00:00
- **Authors**: Tatjana Chavdarova, François Fleuret
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of multi-view people occupancy map estimation. Existing solutions for this problem either operate per-view, or rely on a background subtraction pre-processing. Both approaches lessen the detection performance as scenes become more crowded. The former does not exploit joint information, whereas the latter deals with ambiguous input due to the foreground blobs becoming more and more interconnected as the number of targets increases.   Although deep learning algorithms have proven to excel on remarkably numerous computer vision tasks, such a method has not been applied yet to this problem. In large part this is due to the lack of large-scale multi-camera data-set.   The core of our method is an architecture which makes use of monocular pedestrian data-set, available at larger scale then the multi-view ones, applies parallel processing to the multiple video streams, and jointly utilises it. Our end-to-end deep learning method outperforms existing methods by large margins on the commonly used PETS 2009 data-set. Furthermore, we make publicly available a new three-camera HD data-set. Our source code and trained models will be made available under an open-source license.



### Visualizing Deep Neural Network Decisions: Prediction Difference Analysis
- **Arxiv ID**: http://arxiv.org/abs/1702.04595v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1702.04595v1)
- **Published**: 2017-02-15 13:25:26+00:00
- **Updated**: 2017-02-15 13:25:26+00:00
- **Authors**: Luisa M Zintgraf, Taco S Cohen, Tameem Adel, Max Welling
- **Comment**: ICLR2017
- **Journal**: None
- **Summary**: This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).



### Filling missing data in point clouds by merging structured and unstructured point clouds
- **Arxiv ID**: http://arxiv.org/abs/1702.04641v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, cs.DM, 53A05, F.2.2; G.2.1; I.3.5
- **Links**: [PDF](http://arxiv.org/pdf/1702.04641v1)
- **Published**: 2017-02-15 14:59:10+00:00
- **Updated**: 2017-02-15 14:59:10+00:00
- **Authors**: Franziska Lippoldt, Hartmut Schwandt
- **Comment**: 6 pages, 1 figure, in preparation
- **Journal**: None
- **Summary**: Point clouds arising from structured data, mainly as a result of CT scans, provides special properties on the distribution of points and the distances between those. Yet often, the amount of data provided can not compare to unstructured point clouds, i.e. data that arises from 3D light scans or laser scans. This article hereby proposes an approach to extend structured data and enhance the quality by inserting selected points from an unstructured point cloud. The resulting point cloud still has a partial structure that is called "half-structure". In this way, missing data that can not be optimally recovered through other surface reconstruction methods can be completed.



### Computational Model for Predicting Visual Fixations from Childhood to Adulthood
- **Arxiv ID**: http://arxiv.org/abs/1702.04657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04657v1)
- **Published**: 2017-02-15 15:48:45+00:00
- **Updated**: 2017-02-15 15:48:45+00:00
- **Authors**: Olivier Le Meur, Antoine Coutrot, Zhi Liu, Adrien Le Roch, Andrea Helo, Pia Rama
- **Comment**: None
- **Journal**: None
- **Summary**: How people look at visual information reveals fundamental information about themselves, their interests and their state of mind. While previous visual attention models output static 2-dimensional saliency maps, saccadic models aim to predict not only where observers look at but also how they move their eyes to explore the scene. Here we demonstrate that saccadic models are a flexible framework that can be tailored to emulate observer's viewing tendencies. More specifically, we use the eye data from 101 observers split in 5 age groups (adults, 8-10 y.o., 6-8 y.o., 4-6 y.o. and 2 y.o.) to train our saccadic model for different stages of the development of the human visual system. We show that the joint distribution of saccade amplitude and orientation is a visual signature specific to each age group, and can be used to generate age-dependent scanpaths. Our age-dependent saccadic model not only outputs human-like, age-specific visual scanpath, but also significantly outperforms other state-of-the-art saliency models. In this paper, we demonstrate that the computational modelling of visual attention, through the use of saccadic model, can be efficiently adapted to emulate the gaze behavior of a specific group of observers.



### Handwritten Arabic Numeral Recognition using Deep Learning Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.04663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04663v1)
- **Published**: 2017-02-15 16:06:15+00:00
- **Updated**: 2017-02-15 16:06:15+00:00
- **Authors**: Akm Ashiquzzaman, Abdul Kawsar Tushar
- **Comment**: Conference Name - 2017 IEEE International Conference on Imaging,
  Vision & Pattern Recognition (icIVPR17) 4 pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: Handwritten character recognition is an active area of research with applications in numerous fields. Past and recent works in this field have concentrated on various languages. Arabic is one language where the scope of research is still widespread, with it being one of the most popular languages in the world and being syntactically different from other major languages. Das et al. \cite{DBLP:journals/corr/abs-1003-1891} has pioneered the research for handwritten digit recognition in Arabic. In this paper, we propose a novel algorithm based on deep learning neural networks using appropriate activation function and regularization layer, which shows significantly improved accuracy compared to the existing Arabic numeral recognition methods. The proposed model gives 97.4 percent accuracy, which is the recorded highest accuracy of the dataset used in the experiment. We also propose a modification of the method described in \cite{DBLP:journals/corr/abs-1003-1891}, where our method scores identical accuracy as that of \cite{DBLP:journals/corr/abs-1003-1891}, with the value of 93.8 percent.



### Visual Discovery at Pinterest
- **Arxiv ID**: http://arxiv.org/abs/1702.04680v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04680v2)
- **Published**: 2017-02-15 16:52:52+00:00
- **Updated**: 2017-03-25 18:31:39+00:00
- **Authors**: Andrew Zhai, Dmitry Kislyuk, Yushi Jing, Michael Feng, Eric Tzeng, Jeff Donahue, Yue Li Du, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past three years Pinterest has experimented with several visual search and recommendation services, including Related Pins (2014), Similar Looks (2015), Flashlight (2016) and Lens (2017). This paper presents an overview of our visual discovery engine powering these services, and shares the rationales behind our technical and product decisions such as the use of object detection and interactive user interfaces. We conclude that this visual discovery engine significantly improves engagement in both search and recommendation tasks.



### Multi-Task Convolutional Neural Network for Pose-Invariant Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1702.04710v2
- **DOI**: 10.1109/TIP.2017.2765830
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.04710v2)
- **Published**: 2017-02-15 18:41:21+00:00
- **Updated**: 2017-05-09 03:09:04+00:00
- **Authors**: Xi Yin, Xiaoming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper explores multi-task learning (MTL) for face recognition. We answer the questions of how and why MTL can improve the face recognition performance. First, we propose a multi-task Convolutional Neural Network (CNN) for face recognition where identity classification is the main task and pose, illumination, and expression estimations are the side tasks. Second, we develop a dynamic-weighting scheme to automatically assign the loss weight to each side task, which is a crucial problem in MTL. Third, we propose a pose-directed multi-task CNN by grouping different poses to learn pose-specific identity features, simultaneously across all poses. Last but not least, we propose an energy-based weight analysis method to explore how CNN-based MTL works. We observe that the side tasks serve as regularizations to disentangle the variations from the learnt identity features. Extensive experiments on the entire Multi-PIE dataset demonstrate the effectiveness of the proposed approach. To the best of our knowledge, this is the first work using all data in Multi-PIE for face recognition. Our approach is also applicable to in-the-wild datasets for pose-invariant face recognition and achieves comparable or better performance than state of the art on LFW, CFP, and IJB-A datasets.



