# Arxiv Papers in cs.CV on 2017-02-09
### Manifold Based Low-rank Regularization for Image Restoration and Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1702.02680v1
- **DOI**: None
- **Categories**: **cs.CV**, math.NA, 65D18, 65J22, 68U10, 68Q32
- **Links**: [PDF](http://arxiv.org/pdf/1702.02680v1)
- **Published**: 2017-02-09 02:19:24+00:00
- **Updated**: 2017-02-09 02:19:24+00:00
- **Authors**: Rongjie Lai, Jia Li
- **Comment**: 23 pages, 13 figures
- **Journal**: None
- **Summary**: Low-rank structures play important role in recent advances of many problems in image science and data science. As a natural extension of low-rank structures for data with nonlinear structures, the concept of the low-dimensional manifold structure has been considered in many data processing problems. Inspired by this concept, we consider a manifold based low-rank regularization as a linear approximation of manifold dimension. This regularization is less restricted than the global low-rank regularization, and thus enjoy more flexibility to handle data with nonlinear structures. As applications, we demonstrate the proposed regularization to classical inverse problems in image sciences and data sciences including image inpainting, image super-resolution, X-ray computer tomography (CT) image reconstruction and semi-supervised learning. We conduct intensive numerical experiments in several image restoration problems and a semi-supervised learning problem of classifying handwritten digits using the MINST data. Our numerical tests demonstrate the effectiveness of the proposed methods and illustrate that the new regularization methods produce outstanding results by comparing with many existing methods.



### Semi-Supervised Deep Learning for Monocular Depth Map Prediction
- **Arxiv ID**: http://arxiv.org/abs/1702.02706v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02706v3)
- **Published**: 2017-02-09 05:08:22+00:00
- **Updated**: 2017-05-09 21:58:52+00:00
- **Authors**: Yevhen Kuznietsov, Jörg Stückler, Bastian Leibe
- **Comment**: CVPR 2017 Spotlight
- **Journal**: None
- **Summary**: Supervised deep learning often suffers from the lack of sufficient training data. Specifically in the context of monocular depth map prediction, it is barely possible to determine dense ground truth depth images in realistic dynamic outdoor environments. When using LiDAR sensors, for instance, noise is present in the distance measurements, the calibration between sensors cannot be perfect, and the measurements are typically much sparser than the camera images. In this paper, we propose a novel approach to depth map prediction from monocular images that learns in a semi-supervised way. While we use sparse ground-truth depth for supervised learning, we also enforce our deep network to produce photoconsistent dense depth maps in a stereo setup using a direct image alignment loss. In experiments we demonstrate superior performance in depth map prediction from single images compared to the state-of-the-art methods.



### Predicting Privileged Information for Height Estimation
- **Arxiv ID**: http://arxiv.org/abs/1702.02709v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02709v1)
- **Published**: 2017-02-09 05:30:26+00:00
- **Updated**: 2017-02-09 05:30:26+00:00
- **Authors**: Nikolaos Sarafianos, Christophoros Nikou, Ioannis A. Kakadiaris
- **Comment**: Published in ICPR 2016
- **Journal**: None
- **Summary**: In this paper, we propose a novel regression-based method for employing privileged information to estimate the height using human metrology. The actual values of the anthropometric measurements are difficult to estimate accurately using state-of-the-art computer vision algorithms. Hence, we use ratios of anthropometric measurements as features. Since many anthropometric measurements are not available at test time in real-life scenarios, we employ a learning using privileged information (LUPI) framework in a regression setup. Instead of using the LUPI paradigm for regression in its original form (i.e., \epsilon-SVR+), we train regression models that predict the privileged information at test time. The predictions are then used, along with observable features, to perform height estimation. Once the height is estimated, a mapping to classes is performed. We demonstrate that the proposed approach can estimate the height better and faster than the \epsilon-SVR+ algorithm and report results for different genders and quartiles of humans.



### Effective face landmark localization via single deep network
- **Arxiv ID**: http://arxiv.org/abs/1702.02719v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02719v1)
- **Published**: 2017-02-09 06:25:54+00:00
- **Updated**: 2017-02-09 06:25:54+00:00
- **Authors**: Zongping Deng, Ke Li, Qijun Zhao, Yi Zhang, Hu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel face alignment method using single deep network (SDN) on existing limited training data. Rather than using a max-pooling layer followed one convolutional layer in typical convolutional neural networks (CNN), SDN adopts a stack of 3 layer groups instead. Each group layer contains two convolutional layers and a max-pooling layer, which can extract the features hierarchically. Moreover, an effective data augmentation strategy and corresponding training skills are also proposed to over-come the lack of training images on COFW and 300-W da-tasets. The experiment results show that our method outper-forms state-of-the-art methods in both detection accuracy and speed.



### Joint Discovery of Object States and Manipulation Actions
- **Arxiv ID**: http://arxiv.org/abs/1702.02738v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.5.1; I.5.4; I.2
- **Links**: [PDF](http://arxiv.org/pdf/1702.02738v3)
- **Published**: 2017-02-09 08:04:33+00:00
- **Updated**: 2017-08-28 08:04:18+00:00
- **Authors**: Jean-Baptiste Alayrac, Josev Sivic, Ivan Laptev, Simon Lacoste-Julien
- **Comment**: Appears in: International Conference on Computer Vision 2017 (ICCV
  2017). 15 pages
- **Journal**: None
- **Summary**: Many human activities involve object manipulations aiming to modify the object state. Examples of common state changes include full/empty bottle, open/closed door, and attached/detached car wheel. In this work, we seek to automatically discover the states of objects and the associated manipulation actions. Given a set of videos for a particular task, we propose a joint model that learns to identify object states and to localize state-modifying actions. Our model is formulated as a discriminative clustering cost with constraints. We assume a consistent temporal order for the changes in object states and manipulation actions, and introduce new optimization techniques to learn model parameters without additional supervision. We demonstrate successful discovery of seven manipulation actions and corresponding object states on a new dataset of videos depicting real-life object manipulations. We show that our joint formulation results in an improvement of object state discovery by action recognition and vice versa.



### Automatic Estimation of Fetal Abdominal Circumference from Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/1702.02741v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1702.02741v2)
- **Published**: 2017-02-09 08:18:32+00:00
- **Updated**: 2017-11-02 05:59:49+00:00
- **Authors**: Jaeseong Jang, Yejin Park, Bukweon Kim, Sung Min Lee, Ja-Young Kwon, Jin Keun Seo
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound diagnosis is routinely used in obstetrics and gynecology for fetal biometry, and owing to its time-consuming process, there has been a great demand for automatic estimation. However, the automated analysis of ultrasound images is complicated because they are patient-specific, operator-dependent, and machine-specific. Among various types of fetal biometry, the accurate estimation of abdominal circumference (AC) is especially difficult to perform automatically because the abdomen has low contrast against surroundings, non-uniform contrast, and irregular shape compared to other parameters.We propose a method for the automatic estimation of the fetal AC from 2D ultrasound data through a specially designed convolutional neural network (CNN), which takes account of doctors' decision process, anatomical structure, and the characteristics of the ultrasound image. The proposed method uses CNN to classify ultrasound images (stomach bubble, amniotic fluid, and umbilical vein) and Hough transformation for measuring AC. We test the proposed method using clinical ultrasound data acquired from 56 pregnant women. Experimental results show that, with relatively small training samples, the proposed CNN provides sufficient classification results for AC estimation through the Hough transformation. The proposed method automatically estimates AC from ultrasound images. The method is quantitatively evaluated, and shows stable performance in most cases and even for ultrasound images deteriorated by shadowing artifacts. As a result of experiments for our acceptance check, the accuracies are 0.809 and 0.771 with the expert 1 and expert 2, respectively, while the accuracy between the two experts is 0.905. However, for cases of oversized fetus, when the amniotic fluid is not observed or the abdominal area is distorted, it could not correctly estimate AC.



### Incorporation of prior knowledge of the signal behavior into the reconstruction to accelerate the acquisition of MR diffusion data
- **Arxiv ID**: http://arxiv.org/abs/1702.02743v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.02743v1)
- **Published**: 2017-02-09 08:26:53+00:00
- **Updated**: 2017-02-09 08:26:53+00:00
- **Authors**: Juan F P J Abascal, Manuel Desco, Juan Parra-Robles
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion MRI measurements using hyperpolarized gases are generally acquired during patient breath hold, which yields a compromise between achievable image resolution, lung coverage and number of b-values. In this work, we propose a novel method that accelerates the acquisition of MR diffusion data by undersampling in both spatial and b-value dimensions, thanks to incorporating knowledge about the signal decay into the reconstruction (SIDER). SIDER is compared to total variation (TV) reconstruction by assessing their effect on both the recovery of ventilation images and estimated mean alveolar dimensions (MAD). Both methods are assessed by retrospectively undersampling diffusion datasets of normal volunteers and COPD patients (n=8) for acceleration factors between x2 and x10. TV led to large errors and artefacts for acceleration factors equal or larger than x5. SIDER improved TV, presenting lower errors and histograms of MAD closer to those obtained from fully sampled data for accelerations factors up to x10. SIDER preserved image quality at all acceleration factors but images were slightly smoothed and some details were lost at x10. In conclusion, we have developed and validated a novel compressed sensing method for lung MRI imaging and achieved high acceleration factors, which can be used to increase the amount of data acquired during a breath-hold. This methodology is expected to improve the accuracy of estimated lung microstructure dimensions and widen the possibilities of studying lung diseases with MRI.



### L1-regularized Reconstruction Error as Alpha Matte
- **Arxiv ID**: http://arxiv.org/abs/1702.02744v1
- **DOI**: 10.1109/LSP.2017.2666180
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02744v1)
- **Published**: 2017-02-09 08:29:58+00:00
- **Updated**: 2017-02-09 08:29:58+00:00
- **Authors**: Jubin Johnson, Hisham Cholakkal, Deepu Rajan
- **Comment**: 5 pages, 5 figure, Accepted in IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: Sampling-based alpha matting methods have traditionally followed the compositing equation to estimate the alpha value at a pixel from a pair of foreground (F) and background (B) samples. The (F,B) pair that produces the least reconstruction error is selected, followed by alpha estimation. The significance of that residual error has been left unexamined. In this letter, we propose a video matting algorithm that uses L1-regularized reconstruction error of F and B samples as a measure of the alpha matte. A multi-frame non-local means framework using coherency sensitive hashing is utilized to ensure temporal coherency in the video mattes. Qualitative and quantitative evaluations on a dataset exclusively for video matting demonstrate the effectiveness of the proposed matting algorithm.



### On-the-Fly Adaptation of Regression Forests for Online Camera Relocalisation
- **Arxiv ID**: http://arxiv.org/abs/1702.02779v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02779v2)
- **Published**: 2017-02-09 10:39:05+00:00
- **Updated**: 2017-06-26 10:11:30+00:00
- **Authors**: Tommaso Cavallari, Stuart Golodetz, Nicholas A. Lord, Julien Valentin, Luigi Di Stefano, Philip H. S. Torr
- **Comment**: To appear in the proceedings of CVPR 2017
- **Journal**: None
- **Summary**: Camera relocalisation is an important problem in computer vision, with applications in simultaneous localisation and mapping, virtual/augmented reality and navigation. Common techniques either match the current image against keyframes with known poses coming from a tracker, or establish 2D-to-3D correspondences between keypoints in the current image and points in the scene in order to estimate the camera pose. Recently, regression forests have become a popular alternative to establish such correspondences. They achieve accurate results, but must be trained offline on the target scene, preventing relocalisation in new environments. In this paper, we show how to circumvent this limitation by adapting a pre-trained forest to a new scene on the fly. Our adapted forests achieve relocalisation performance that is on par with that of offline forests, and our approach runs in under 150ms, making it desirable for real-time systems that require online relocalisation.



### Attribute-controlled face photo synthesis from simple line drawing
- **Arxiv ID**: http://arxiv.org/abs/1702.02805v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02805v1)
- **Published**: 2017-02-09 12:21:36+00:00
- **Updated**: 2017-02-09 12:21:36+00:00
- **Authors**: Qi Guo, Ce Zhu, Zhiqiang Xia, Zhengtao Wang, Yipeng Liu
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: Face photo synthesis from simple line drawing is a one-to-many task as simple line drawing merely contains the contour of human face. Previous exemplar-based methods are over-dependent on the datasets and are hard to generalize to complicated natural scenes. Recently, several works utilize deep neural networks to increase the generalization, but they are still limited in the controllability of the users. In this paper, we propose a deep generative model to synthesize face photo from simple line drawing controlled by face attributes such as hair color and complexion. In order to maximize the controllability of face attributes, an attribute-disentangled variational auto-encoder (AD-VAE) is firstly introduced to learn latent representations disentangled with respect to specified attributes. Then we conduct photo synthesis from simple line drawing based on AD-VAE. Experiments show that our model can well disentangle the variations of attributes from other variations of face photos and synthesize detailed photorealistic face images with desired attributes. Regarding background and illumination as the style and human face as the content, we can also synthesize face photos with the target style of a style photo.



### EAC-Net: A Region-based Deep Enhancing and Cropping Approach for Facial Action Unit Detection
- **Arxiv ID**: http://arxiv.org/abs/1702.02925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02925v1)
- **Published**: 2017-02-09 18:16:44+00:00
- **Updated**: 2017-02-09 18:16:44+00:00
- **Authors**: Wei Li, Farnaz Abtahi, Zhigang Zhu, Lijun Yin
- **Comment**: The paper is accepted by FG 2017
- **Journal**: None
- **Summary**: In this paper, we propose a deep learning based approach for facial action unit detection by enhancing and cropping the regions of interest. The approach is implemented by adding two novel nets (layers): the enhancing layers and the cropping layers, to a pretrained CNN model. For the enhancing layers, we designed an attention map based on facial landmark features and applied it to a pretrained neural network to conduct enhanced learning (The E-Net). For the cropping layers, we crop facial regions around the detected landmarks and design convolutional layers to learn deeper features for each facial region (C-Net). We then fuse the E-Net and the C-Net to obtain our Enhancing and Cropping (EAC) Net, which can learn both feature enhancing and region cropping functions. Our approach shows significant improvement in performance compared to the state-of-the-art methods applied to BP4D and DISFA AU datasets.



### A large comparison of feature-based approaches for buried target classification in forward-looking ground-penetrating radar
- **Arxiv ID**: http://arxiv.org/abs/1702.03000v1
- **DOI**: 10.1109/TGRS.2017.2751461
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.03000v1)
- **Published**: 2017-02-09 22:06:04+00:00
- **Updated**: 2017-02-09 22:06:04+00:00
- **Authors**: Joseph A. Camilo, Leslie M. Collins, Jordan M. Malof
- **Comment**: 11 pages, 14 figures, for submission to IEEE TGARS
- **Journal**: None
- **Summary**: Forward-looking ground-penetrating radar (FLGPR) has recently been investigated as a remote sensing modality for buried target detection (e.g., landmines). In this context, raw FLGPR data is beamformed into images and then computerized algorithms are applied to automatically detect subsurface buried targets. Most existing algorithms are supervised, meaning they are trained to discriminate between labeled target and non-target imagery, usually based on features extracted from the imagery. A large number of features have been proposed for this purpose, however thus far it is unclear which are the most effective. The first goal of this work is to provide a comprehensive comparison of detection performance using existing features on a large collection of FLGPR data. Fusion of the decisions resulting from processing each feature is also considered. The second goal of this work is to investigate two modern feature learning approaches from the object recognition literature: the bag-of-visual-words and the Fisher vector for FLGPR processing. The results indicate that the new feature learning approaches outperform existing methods. Results also show that fusion between existing features and new features yields little additional performance improvements.



