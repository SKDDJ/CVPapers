# Arxiv Papers in cs.CV on 2017-02-20
### Progressively Diffused Networks for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1702.05839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05839v1)
- **Published**: 2017-02-20 02:34:26+00:00
- **Updated**: 2017-02-20 02:34:26+00:00
- **Authors**: Ruimao Zhang, Wei Yang, Zhanglin Peng, Xiaogang Wang, Liang Lin
- **Comment**: Sun Yat-sen University, The Chinese University of Hong Kong
- **Journal**: None
- **Summary**: This paper introduces Progressively Diffused Networks (PDNs) for unifying multi-scale context modeling with deep feature learning, by taking semantic image segmentation as an exemplar application. Prior neural networks, such as ResNet, tend to enhance representational power by increasing the depth of architectures and driving the training objective across layers. However, we argue that spatial dependencies in different layers, which generally represent the rich contexts among data elements, are also critical to building deep and discriminative representations. To this end, our PDNs enables to progressively broadcast information over the learned feature maps by inserting a stack of information diffusion layers, each of which exploits multi-dimensional convolutional LSTMs (Long-Short-Term Memory Structures). In each LSTM unit, a special type of atrous filters are designed to capture the short range and long range dependencies from various neighbors to a certain site of the feature map and pass the accumulated information to the next layer. From the extensive experiments on semantic image segmentation benchmarks (e.g., ImageNet Parsing, PASCAL VOC2012 and PASCAL-Part), our framework demonstrates the effectiveness to substantially improve the performances over the popular existing neural network models, and achieves state-of-the-art on ImageNet Parsing for large scale semantic segmentation.



### A GPU-Outperforming FPGA Accelerator Architecture for Binary Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.06392v2
- **DOI**: None
- **Categories**: **cs.DC**, cs.AR, cs.CV, cs.LG, C.3
- **Links**: [PDF](http://arxiv.org/pdf/1702.06392v2)
- **Published**: 2017-02-20 05:21:34+00:00
- **Updated**: 2017-06-08 16:09:55+00:00
- **Authors**: Yixing Li, Zichuan Liu, Kai Xu, Hao Yu, Fengbo Ren
- **Comment**: None
- **Journal**: None
- **Summary**: FPGA-based hardware accelerators for convolutional neural networks (CNNs) have obtained great attentions due to their higher energy efficiency than GPUs. However, it is challenging for FPGA-based solutions to achieve a higher throughput than GPU counterparts. In this paper, we demonstrate that FPGA acceleration can be a superior solution in terms of both throughput and energy efficiency when a CNN is trained with binary constraints on weights and activations. Specifically, we propose an optimized FPGA accelerator architecture tailored for bitwise convolution and normalization that features massive spatial parallelism with deep pipelines stages. A key advantage of the FPGA accelerator is that its performance is insensitive to data batch size, while the performance of GPU acceleration varies largely depending on the batch size of the data. Experiment results show that the proposed accelerator architecture for binary CNNs running on a Virtex-7 FPGA is 8.3x faster and 75x more energy-efficient than a Titan X GPU for processing online individual requests in small batch sizes. For processing static data in large batch sizes, the proposed solution is on a par with a Titan X GPU in terms of throughput while delivering 9.5x higher energy efficiency.



### From Photo Streams to Evolving Situations
- **Arxiv ID**: http://arxiv.org/abs/1702.05878v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.05878v1)
- **Published**: 2017-02-20 06:53:21+00:00
- **Updated**: 2017-02-20 06:53:21+00:00
- **Authors**: Mengfan Tang, Feiping Nie, Siripen Pongpaichet, Ramesh Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Photos are becoming spontaneous, objective, and universal sources of information. This paper develops evolving situation recognition using photo streams coming from disparate sources combined with the advances of deep learning. Using visual concepts in photos together with space and time information, we formulate the situation detection into a semi-supervised learning framework and propose new graph-based models to solve the problem. To extend the method for unknown situations, we introduce a soft label method which enables the traditional semi-supervised learning framework to accurately predict predefined labels as well as effectively form new clusters. To overcome the noisy data which degrades graph quality, leading to poor recognition results, we take advantage of two kinds of noise-robust norms which can eliminate the adverse effects of outliers in visual concepts and improve the accuracy of situation recognition. Finally, we demonstrate the idea and the effectiveness of the proposed model on Yahoo Flickr Creative Commons 100 Million.



### Memory Efficient Max Flow for Multi-label Submodular MRFs
- **Arxiv ID**: http://arxiv.org/abs/1702.05888v1
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV, G.2.2; F.2.2; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/1702.05888v1)
- **Published**: 2017-02-20 08:09:09+00:00
- **Updated**: 2017-02-20 08:09:09+00:00
- **Authors**: Thalaiyasingam Ajanthan, Richard Hartley, Mathieu Salzmann
- **Comment**: 15 Pages, 13 Figures and 3 Tables
- **Journal**: None
- **Summary**: Multi-label submodular Markov Random Fields (MRFs) have been shown to be solvable using max-flow based on an encoding of the labels proposed by Ishikawa, in which each variable $X_i$ is represented by $\ell$ nodes (where $\ell$ is the number of labels) arranged in a column. However, this method in general requires $2\,\ell^2$ edges for each pair of neighbouring variables. This makes it inapplicable to realistic problems with many variables and labels, due to excessive memory requirement. In this paper, we introduce a variant of the max-flow algorithm that requires much less storage. Consequently, our algorithm makes it possible to optimally solve multi-label submodular problems involving large numbers of variables and labels on a standard computer.



### Learning Spatial Regularization with Image-level Supervisions for Multi-label Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1702.05891v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05891v2)
- **Published**: 2017-02-20 08:21:58+00:00
- **Updated**: 2017-03-31 08:49:43+00:00
- **Authors**: Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, Xiaogang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-label image classification is a fundamental but challenging task in computer vision. Great progress has been achieved by exploiting semantic relations between labels in recent years. However, conventional approaches are unable to model the underlying spatial relations between labels in multi-label images, because spatial annotations of the labels are generally not provided. In this paper, we propose a unified deep neural network that exploits both semantic and spatial relations between labels with only image-level supervisions. Given a multi-label image, our proposed Spatial Regularization Network (SRN) generates attention maps for all labels and captures the underlying relations between them via learnable convolutions. By aggregating the regularized classification results with original results by a ResNet-101 network, the classification performance can be consistently improved. The whole deep neural network is trained end-to-end with only image-level annotations, thus requires no additional efforts on image annotations. Extensive evaluations on 3 public datasets with different types of labels show that our approach significantly outperforms state-of-the-arts and has strong generalization capability. Analysis of the learned SRN model demonstrates that it can effectively capture both semantic and spatial relations of labels for improving classification performance.



### Efficient Large-scale Approximate Nearest Neighbor Search on the GPU
- **Arxiv ID**: http://arxiv.org/abs/1702.05911v1
- **DOI**: 10.1109/CVPR.2016.223
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05911v1)
- **Published**: 2017-02-20 09:57:11+00:00
- **Updated**: 2017-02-20 09:57:11+00:00
- **Authors**: Patrick Wieschollek, Oliver Wang, Alexander Sorkine-Hornung, Hendrik P. A. Lensch
- **Comment**: None
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), pp. 2027 - 2035 (2016)
- **Summary**: We present a new approach for efficient approximate nearest neighbor (ANN) search in high dimensional spaces, extending the idea of Product Quantization. We propose a two-level product and vector quantization tree that reduces the number of vector comparisons required during tree traversal. Our approach also includes a novel highly parallelizable re-ranking method for candidate vectors by efficiently reusing already computed intermediate values. Due to its small memory footprint during traversal, the method lends itself to an efficient, parallel GPU implementation. This Product Quantization Tree (PQT) approach significantly outperforms recent state of the art methods for high dimensional nearest neighbor queries on standard reference datasets. Ours is the first work that demonstrates GPU performance superior to CPU performance on high dimensional, large scale ANN problems in time-critical real-world applications, like loop-closing in videos.



### The importance of stain normalization in colorectal tissue classification with convolutional networks
- **Arxiv ID**: http://arxiv.org/abs/1702.05931v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1702.05931v2)
- **Published**: 2017-02-20 11:11:50+00:00
- **Updated**: 2017-05-23 12:34:17+00:00
- **Authors**: Francesco Ciompi, Oscar Geessink, Babak Ehteshami Bejnordi, Gabriel Silva de Souza, Alexi Baidoshvili, Geert Litjens, Bram van Ginneken, Iris Nagtegaal, Jeroen van der Laak
- **Comment**: Published in Proceedings of IEEE International Symposium on
  Biomedical Imaging (ISBI) 2017
- **Journal**: None
- **Summary**: The development of reliable imaging biomarkers for the analysis of colorectal cancer (CRC) in hematoxylin and eosin (H&E) stained histopathology images requires an accurate and reproducible classification of the main tissue components in the image. In this paper, we propose a system for CRC tissue classification based on convolutional networks (ConvNets). We investigate the importance of stain normalization in tissue classification of CRC tissue samples in H&E-stained images. Furthermore, we report the performance of ConvNets on a cohort of rectal cancer samples and on an independent publicly available dataset of colorectal H&E images.



### SurvivalNet: Predicting patient survival from diffusion weighted magnetic resonance images using cascaded fully convolutional and 3D convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1702.05941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05941v1)
- **Published**: 2017-02-20 12:05:30+00:00
- **Updated**: 2017-02-20 12:05:30+00:00
- **Authors**: Patrick Ferdinand Christ, Florian Ettlinger, Georgios Kaissis, Sebastian Schlecht, Freba Ahmaddy, Felix Grün, Alexander Valentinitsch, Seyed-Ahmad Ahmadi, Rickmer Braren, Bjoern Menze
- **Comment**: Accepted at IEEE ISBI 2017
- **Journal**: None
- **Summary**: Automatic non-invasive assessment of hepatocellular carcinoma (HCC) malignancy has the potential to substantially enhance tumor treatment strategies for HCC patients. In this work we present a novel framework to automatically characterize the malignancy of HCC lesions from DWI images. We predict HCC malignancy in two steps: As a first step we automatically segment HCC tumor lesions using cascaded fully convolutional neural networks (CFCN). A 3D neural network (SurvivalNet) then predicts the HCC lesions' malignancy from the HCC tumor segmentation. We formulate this task as a classification problem with classes being "low risk" and "high risk" represented by longer or shorter survival times than the median survival. We evaluated our method on DWI of 31 HCC patients. Our proposed framework achieves an end-to-end accuracy of 65% with a Dice score for the automatic lesion segmentation of 69% and an accuracy of 68% for tumor malignancy classification based on expert annotations. We compared the SurvivalNet to classical handcrafted features such as Histogram and Haralick and show experimentally that SurvivalNet outperforms the handcrafted features in HCC malignancy classification. End-to-end assessment of tumor malignancy based on our proposed fully automatic framework corresponds to assessment based on expert annotations with high significance (p>0.95).



### Reflection Separation Using Guided Annotation
- **Arxiv ID**: http://arxiv.org/abs/1702.05958v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.05958v2)
- **Published**: 2017-02-20 13:21:20+00:00
- **Updated**: 2017-05-10 10:45:49+00:00
- **Authors**: Ofer Springer, Yair Weiss
- **Comment**: To be presented at ICIP 2017, 6 pages, 7 figures
- **Journal**: None
- **Summary**: Photographs taken through a glass surface often contain an approximately linear superposition of reflected and transmitted layers. Decomposing an image into these layers is generally an ill-posed task and the use of an additional image prior and user provided cues is presently necessary in order to obtain good results. Current annotation approaches rely on a strong sparsity assumption. For images with significant texture this assumption does not typically hold, thus rendering the annotation process unviable. In this paper we show that using a Gaussian Mixture Model patch prior, the correct local decomposition can almost always be found as one of 100 likely modes of the posterior. Thus, the user need only choose one of these modes in a sparse set of patches and the decomposition may then be completed automatically. We demonstrate the performance of our method using synthesized and real reflection images.



### Automatic Liver and Tumor Segmentation of CT and MRI Volumes using Cascaded Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.05970v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1702.05970v2)
- **Published**: 2017-02-20 13:52:57+00:00
- **Updated**: 2017-02-23 15:02:59+00:00
- **Authors**: Patrick Ferdinand Christ, Florian Ettlinger, Felix Grün, Mohamed Ezzeldin A. Elshaera, Jana Lipkova, Sebastian Schlecht, Freba Ahmaddy, Sunil Tatavarty, Marc Bickel, Patrick Bilic, Markus Rempfler, Felix Hofmann, Melvin D Anastasi, Seyed-Ahmad Ahmadi, Georgios Kaissis, Julian Holch, Wieland Sommer, Rickmer Braren, Volker Heinemann, Bjoern Menze
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Automatic segmentation of the liver and hepatic lesions is an important step towards deriving quantitative biomarkers for accurate clinical diagnosis and computer-aided decision support systems. This paper presents a method to automatically segment liver and lesions in CT and MRI abdomen images using cascaded fully convolutional neural networks (CFCNs) enabling the segmentation of a large-scale medical trial or quantitative image analysis. We train and cascade two FCNs for a combined segmentation of the liver and its lesions. In the first step, we train a FCN to segment the liver as ROI input for a second FCN. The second FCN solely segments lesions within the predicted liver ROIs of step 1. CFCN models were trained on an abdominal CT dataset comprising 100 hepatic tumor volumes. Validations on further datasets show that CFCN-based semantic liver and lesion segmentation achieves Dice scores over 94% for liver with computation times below 100s per volume. We further experimentally demonstrate the robustness of the proposed method on an 38 MRI liver tumor volumes and the public 3DIRCAD dataset.



### An Extended Framework for Marginalized Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1702.05993v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1702.05993v1)
- **Published**: 2017-02-20 15:00:13+00:00
- **Updated**: 2017-02-20 15:00:13+00:00
- **Authors**: Gabriela Csurka, Boris Chidlovski, Stephane Clinchant, Sophia Michel
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an extended framework for marginalized domain adaptation, aimed at addressing unsupervised, supervised and semi-supervised scenarios. We argue that the denoising principle should be extended to explicitly promote domain-invariant features as well as help the classification task. Therefore we propose to jointly learn the data auto-encoders and the target classifiers. First, in order to make the denoised features domain-invariant, we propose a domain regularization that may be either a domain prediction loss or a maximum mean discrepancy between the source and target data. The noise marginalization in this case is reduced to solving the linear matrix system $AX=B$ which has a closed-form solution. Second, in order to help the classification, we include a class regularization term. Adding this component reduces the learning problem to solving a Sylvester linear matrix equation $AX+BX=C$, for which an efficient iterative procedure exists as well. We did an extensive study to assess how these regularization terms improve the baseline performance in the three domain adaptation scenarios and present experimental results on two image and one text benchmark datasets, conventionally used for validating domain adaptation methods. We report our findings and comparison with state-of-the-art methods.



### Synthesis versus analysis in patch-based image priors
- **Arxiv ID**: http://arxiv.org/abs/1702.06085v1
- **DOI**: None
- **Categories**: **cs.CV**, 94A08, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/1702.06085v1)
- **Published**: 2017-02-20 17:59:38+00:00
- **Updated**: 2017-02-20 17:59:38+00:00
- **Authors**: Mario A. T. Figueiredo
- **Comment**: To appear in ICASSP 2017
- **Journal**: None
- **Summary**: In global models/priors (for example, using wavelet frames), there is a well known analysis vs synthesis dichotomy in the way signal/image priors are formulated. In patch-based image models/priors, this dichotomy is also present in the choice of how each patch is modeled. This paper shows that there is another analysis vs synthesis dichotomy, in terms of how the whole image is related to the patches, and that all existing patch-based formulations that provide a global image prior belong to the analysis category. We then propose a synthesis formulation, where the image is explicitly modeled as being synthesized by additively combining a collection of independent patches. We formally establish that these analysis and synthesis formulations are not equivalent in general and that both formulations are compatible with analysis and synthesis formulations at the patch level. Finally, we present an instance of the alternating direction method of multipliers (ADMM) that can be used to perform image denoising under the proposed synthesis formulation, showing its computational feasibility. Rather than showing the superiority of the synthesis or analysis formulations, the contributions of this paper is to establish the existence of both alternatives, thus closing the corresponding gap in the field of patch-based image processing.



### Label Distribution Learning Forests
- **Arxiv ID**: http://arxiv.org/abs/1702.06086v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.06086v4)
- **Published**: 2017-02-20 18:04:31+00:00
- **Updated**: 2017-10-16 21:05:45+00:00
- **Authors**: Wei Shen, Kai Zhao, Yilu Guo, Alan Yuille
- **Comment**: Accepted by NIPS2017
- **Journal**: None
- **Summary**: Label distribution learning (LDL) is a general learning framework, which assigns to an instance a distribution over a set of labels rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning, e.g., to learn deep features in an end-to-end manner. This paper presents label distribution learning forests (LDLFs) - a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by a mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning. We define a distribution-based loss function for a forest, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on several LDL tasks and a computer vision application, showing significant improvements to the state-of-the-art LDL methods.



### Developing a comprehensive framework for multimodal feature extraction
- **Arxiv ID**: http://arxiv.org/abs/1702.06151v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1702.06151v1)
- **Published**: 2017-02-20 19:22:21+00:00
- **Updated**: 2017-02-20 19:22:21+00:00
- **Authors**: Quinten McNamara, Alejandro de la Vega, Tal Yarkoni
- **Comment**: None
- **Journal**: None
- **Summary**: Feature extraction is a critical component of many applied data science workflows. In recent years, rapid advances in artificial intelligence and machine learning have led to an explosion of feature extraction tools and services that allow data scientists to cheaply and effectively annotate their data along a vast array of dimensions---ranging from detecting faces in images to analyzing the sentiment expressed in coherent text. Unfortunately, the proliferation of powerful feature extraction services has been mirrored by a corresponding expansion in the number of distinct interfaces to feature extraction services. In a world where nearly every new service has its own API, documentation, and/or client library, data scientists who need to combine diverse features obtained from multiple sources are often forced to write and maintain ever more elaborate feature extraction pipelines. To address this challenge, we introduce a new open-source framework for comprehensive multimodal feature extraction. Pliers is an open-source Python package that supports standardized annotation of diverse data types (video, images, audio, and text), and is expressly with both ease-of-use and extensibility in mind. Users can apply a wide range of pre-existing feature extraction tools to their data in just a few lines of Python code, and can also easily add their own custom extractors by writing modular classes. A graph-based API enables rapid development of complex feature extraction pipelines that output results in a single, standardized format. We describe the package's architecture, detail its major advantages over previous feature extraction toolboxes, and use a sample application to a large functional MRI dataset to illustrate how pliers can significantly reduce the time and effort required to construct sophisticated feature extraction workflows while increasing code clarity and maintainability.



### Efficient Dense Labeling of Human Activity Sequences from Wearables using Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1702.06212v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1702.06212v1)
- **Published**: 2017-02-20 23:44:54+00:00
- **Updated**: 2017-02-20 23:44:54+00:00
- **Authors**: Rui Yao, Guosheng Lin, Qinfeng Shi, Damith Ranasinghe
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Recognizing human activities in a sequence is a challenging area of research in ubiquitous computing. Most approaches use a fixed size sliding window over consecutive samples to extract features---either handcrafted or learned features---and predict a single label for all samples in the window. Two key problems emanate from this approach: i) the samples in one window may not always share the same label. Consequently, using one label for all samples within a window inevitably lead to loss of information; ii) the testing phase is constrained by the window size selected during training while the best window size is difficult to tune in practice. We propose an efficient algorithm that can predict the label of each sample, which we call dense labeling, in a sequence of human activities of arbitrary length using a fully convolutional network. In particular, our approach overcomes the problems posed by the sliding window step. Additionally, our algorithm learns both the features and classifier automatically. We release a new daily activity dataset based on a wearable sensor with hospitalized patients. We conduct extensive experiments and demonstrate that our proposed approach is able to outperform the state-of-the-arts in terms of classification and label misalignment measures on three challenging datasets: Opportunity, Hand Gesture, and our new dataset.



