# Arxiv Papers in cs.CV on 2017-02-06
### Semi-Dense Visual Odometry for RGB-D Cameras Using Approximate Nearest Neighbour Fields
- **Arxiv ID**: http://arxiv.org/abs/1702.02512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02512v1)
- **Published**: 2017-02-06 00:12:37+00:00
- **Updated**: 2017-02-06 00:12:37+00:00
- **Authors**: Yi Zhou, Laurent Kneip, Hongdong Li
- **Comment**: ICRA 2017
- **Journal**: None
- **Summary**: This paper presents a robust and efficient semi-dense visual odometry solution for RGB-D cameras. The core of our method is a 2D-3D ICP pipeline which estimates the pose of the sensor by registering the projection of a 3D semi-dense map of the reference frame with the 2D semi-dense region extracted in the current frame. The processing is speeded up by efficiently implemented approximate nearest neighbour fields under the Euclidean distance criterion, which permits the use of compact Gauss-Newton updates in the optimization. The registration is formulated as a maximum a posterior problem to deal with outliers and sensor noises, and consequently the equivalent weighted least squares problem is solved by iteratively reweighted least squares method. A variety of robust weight functions are tested and the optimum is determined based on the characteristics of the sensor model. Extensive evaluation on publicly available RGB-D datasets shows that the proposed method predominantly outperforms existing state-of-the-art methods.



### Attentional Network for Visual Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1702.01478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.01478v1)
- **Published**: 2017-02-06 00:50:36+00:00
- **Updated**: 2017-02-06 00:50:36+00:00
- **Authors**: Kota Hara, Ming-Yu Liu, Oncel Tuzel, Amir-massoud Farahmand
- **Comment**: None
- **Journal**: None
- **Summary**: We propose augmenting deep neural networks with an attention mechanism for the visual object detection task. As perceiving a scene, humans have the capability of multiple fixation points, each attended to scene content at different locations and scales. However, such a mechanism is missing in the current state-of-the-art visual object detection methods. Inspired by the human vision system, we propose a novel deep network architecture that imitates this attention mechanism. As detecting objects in an image, the network adaptively places a sequence of glimpses of different shapes at different locations in the image. Evidences of the presence of an object and its location are extracted from these glimpses, which are then fused for estimating the object class and bounding box coordinates. Due to lacks of ground truth annotations of the visual attention mechanism, we train our network using a reinforcement learning algorithm with policy gradients. Experiment results on standard object detection benchmarks show that the proposed network consistently outperforms the baseline networks that does not model the attention mechanism.



### Detailed Surface Geometry and Albedo Recovery from RGB-D Video Under Natural Illumination
- **Arxiv ID**: http://arxiv.org/abs/1702.01486v4
- **DOI**: 10.1109/TPAMI.2019.2955459
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.01486v4)
- **Published**: 2017-02-06 02:28:25+00:00
- **Updated**: 2019-11-20 05:06:40+00:00
- **Authors**: Xinxin Zuo, Sen Wang, Jiangbin Zheng, Ruigang Yang
- **Comment**: TPAMI accepted
- **Journal**: None
- **Summary**: In this paper we present a novel approach for depth map enhancement from an RGB-D video sequence. The basic idea is to exploit the shading information in the color image. Instead of making assumption about surface albedo or controlled object motion and lighting, we use the lighting variations introduced by casual object movement. We are effectively calculating photometric stereo from a moving object under natural illuminations. The key technical challenge is to establish correspondences over the entire image set. We therefore develop a lighting insensitive robust pixel matching technique that out-performs optical flow method in presence of lighting variations. In addition we present an expectation-maximization framework to recover the surface normal and albedo simultaneously, without any regularization term. We have validated our method on both synthetic and real datasets to show its superior performance on both surface details recovery and intrinsic decomposition.



### Designing Deep Convolutional Neural Networks for Continuous Object Orientation Estimation
- **Arxiv ID**: http://arxiv.org/abs/1702.01499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.01499v1)
- **Published**: 2017-02-06 05:33:45+00:00
- **Updated**: 2017-02-06 05:33:45+00:00
- **Authors**: Kota Hara, Raviteja Vemulapalli, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (DCNN) have been proven to be effective for various computer vision problems. In this work, we demonstrate its effectiveness on a continuous object orientation estimation task, which requires prediction of 0 to 360 degrees orientation of the objects. We do so by proposing and comparing three continuous orientation prediction approaches designed for the DCNNs. The first two approaches work by representing an orientation as a point on a unit circle and minimizing either L2 loss or angular difference loss. The third method works by first converting the continuous orientation estimation task into a set of discrete orientation estimation tasks and then converting the discrete orientation outputs back to the continuous orientation using a mean-shift algorithm. By evaluating on a vehicle orientation estimation task and a pedestrian orientation estimation task, we demonstrate that the discretization-based approach not only works better than the other two approaches but also achieves state-of-the-art performance. We also demonstrate that finding an appropriate feature representation is critical to achieve a good performance when adapting a DCNN trained for an image recognition task.



### Challenge of Multi-Camera Tracking
- **Arxiv ID**: http://arxiv.org/abs/1702.01507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.01507v1)
- **Published**: 2017-02-06 06:32:14+00:00
- **Updated**: 2017-02-06 06:32:14+00:00
- **Authors**: Yong Wang, Ke Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-camera tracking is quite different from single camera tracking, and it faces new technology and system architecture challenges. By analyzing the corresponding characteristics and disadvantages of the existing algorithms, problems in multi-camera tracking are summarized and some new directions for future work are also generalized.



### Contextually Customized Video Summaries via Natural Language
- **Arxiv ID**: http://arxiv.org/abs/1702.01528v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1702.01528v3)
- **Published**: 2017-02-06 08:31:44+00:00
- **Updated**: 2018-03-02 06:13:45+00:00
- **Authors**: Jinsoo Choi, Tae-Hyun Oh, In So Kweon
- **Comment**: To appear in WACV 2018
- **Journal**: None
- **Summary**: The best summary of a long video differs among different people due to its highly subjective nature. Even for the same person, the best summary may change with time or mood. In this paper, we introduce the task of generating customized video summaries through simple text. First, we train a deep architecture to effectively learn semantic embeddings of video frames by leveraging the abundance of image-caption data via a progressive and residual manner. Given a user-specific text description, our algorithm is able to select semantically relevant video segments and produce a temporally aligned video summary. In order to evaluate our textually customized video summaries, we conduct experimental comparison with baseline methods that utilize ground-truth information. Despite the challenging baselines, our method still manages to show comparable or even exceeding performance. We also show that our method is able to generate semantically diverse video summaries by only utilizing the learned visual embeddings.



### Slice-to-volume medical image registration: a survey
- **Arxiv ID**: http://arxiv.org/abs/1702.01636v2
- **DOI**: 10.1016/j.media.2017.04.010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.01636v2)
- **Published**: 2017-02-06 14:51:29+00:00
- **Updated**: 2017-04-27 14:49:15+00:00
- **Authors**: Enzo Ferrante, Nikos Paragios
- **Comment**: Accepted for publication in Medical Image Analysis
- **Journal**: None
- **Summary**: During the last decades, the research community of medical imaging has witnessed continuous advances in image registration methods, which pushed the limits of the state-of-the-art and enabled the development of novel medical procedures. A particular type of image registration problem, known as slice-to-volume registration, played a fundamental role in areas like image guided surgeries and volumetric image reconstruction. However, to date, and despite the extensive literature available on this topic, no survey has been written to discuss this challenging problem. This paper introduces the first comprehensive survey of the literature about slice-to-volume registration, presenting a categorical study of the algorithms according to an ad-hoc taxonomy and analyzing advantages and disadvantages of every category. We draw some general conclusions from this analysis and present our perspectives on the future of the field.



### Concurrent Activity Recognition with Multimodal CNN-LSTM Structure
- **Arxiv ID**: http://arxiv.org/abs/1702.01638v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.01638v1)
- **Published**: 2017-02-06 15:01:45+00:00
- **Updated**: 2017-02-06 15:01:45+00:00
- **Authors**: Xinyu Li, Yanyi Zhang, Jianyu Zhang, Shuhong Chen, Ivan Marsic, Richard A. Farneth, Randall S. Burd
- **Comment**: 14 pages, 12 figures, under review
- **Journal**: None
- **Summary**: We introduce a system that recognizes concurrent activities from real-world data captured by multiple sensors of different types. The recognition is achieved in two steps. First, we extract spatial and temporal features from the multimodal data. We feed each datatype into a convolutional neural network that extracts spatial features, followed by a long-short term memory network that extracts temporal information in the sensory data. The extracted features are then fused for decision making in the second step. Second, we achieve concurrent activity recognition with a single classifier that encodes a binary output vector in which elements indicate whether the corresponding activity types are currently in progress. We tested our system with three datasets from different domains recorded using different sensors and achieved performance comparable to existing systems designed specifically for those domains. Our system is the first to address the concurrent activity recognition with multisensory data using a single model, which is scalable, simple to train and easy to deploy.



### View Independent Vehicle Make, Model and Color Recognition Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1702.01721v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1702.01721v1)
- **Published**: 2017-02-06 17:47:08+00:00
- **Updated**: 2017-02-06 17:47:08+00:00
- **Authors**: Afshin Dehghan, Syed Zain Masood, Guang Shu, Enrique. G. Ortiz
- **Comment**: 7 Pages
- **Journal**: None
- **Summary**: This paper describes the details of Sighthound's fully automated vehicle make, model and color recognition system. The backbone of our system is a deep convolutional neural network that is not only computationally inexpensive, but also provides state-of-the-art results on several competitive benchmarks. Additionally, our deep network is trained on a large dataset of several million images which are labeled through a semi-automated process. Finally we test our system on several public datasets as well as our own internal test dataset. Our results show that we outperform other methods on all benchmarks by significant margins. Our model is available to developers through the Sighthound Cloud API at https://www.sighthound.com/products/cloud



### A Deep Convolutional Neural Network for Background Subtraction
- **Arxiv ID**: http://arxiv.org/abs/1702.01731v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.01731v1)
- **Published**: 2017-02-06 18:24:04+00:00
- **Updated**: 2017-02-06 18:24:04+00:00
- **Authors**: Mohammadreza Babaee, Duc Tung Dinh, Gerhard Rigoll
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a novel background subtraction system that uses a deep Convolutional Neural Network (CNN) to perform the segmentation. With this approach, feature engineering and parameter tuning become unnecessary since the network parameters can be learned from data by training a single CNN that can handle various video scenes. Additionally, we propose a new approach to estimate background model from video. For the training of the CNN, we employed randomly 5 percent video frames and their ground truth segmentations taken from the Change Detection challenge 2014(CDnet 2014). We also utilized spatial-median filtering as the post-processing of the network outputs. Our method is evaluated with different data-sets, and the network outperforms the existing algorithms with respect to the average ranking over different evaluation metrics. Furthermore, due to the network architecture, our CNN is capable of real time processing.



