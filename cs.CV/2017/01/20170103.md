# Arxiv Papers in cs.CV on 2017-01-03
### Robust and Real-time Deep Tracking Via Multi-Scale Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1701.00561v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.00561v1)
- **Published**: 2017-01-03 01:10:51+00:00
- **Updated**: 2017-01-03 01:10:51+00:00
- **Authors**: Xinyu Wang, Hanxi Li, Yi Li, Fumin Shen, Fatih Porikli
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Visual tracking is a fundamental problem in computer vision. Recently, some deep-learning-based tracking algorithms have been achieving record-breaking performances. However, due to the high complexity of deep learning, most deep trackers suffer from low tracking speed, and thus are impractical in many real-world applications. Some new deep trackers with smaller network structure achieve high efficiency while at the cost of significant decrease on precision. In this paper, we propose to transfer the feature for image classification to the visual tracking domain via convolutional channel reductions. The channel reduction could be simply viewed as an additional convolutional layer with the specific task. It not only extracts useful information for object tracking but also significantly increases the tracking speed. To better accommodate the useful feature of the target in different scales, the adaptation filters are designed with different sizes. The yielded visual tracker is real-time and also illustrates the state-of-the-art accuracies in the experiment involving two well-adopted benchmarks with more than 100 test videos.



### AENet: Learning Deep Audio Features for Video Analysis
- **Arxiv ID**: http://arxiv.org/abs/1701.00599v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1701.00599v2)
- **Published**: 2017-01-03 07:35:54+00:00
- **Updated**: 2017-01-04 04:07:11+00:00
- **Authors**: Naoya Takahashi, Michael Gygli, Luc Van Gool
- **Comment**: 12 pages, 9 figures. arXiv admin note: text overlap with
  arXiv:1604.07160
- **Journal**: None
- **Summary**: We propose a new deep network for audio event recognition, called AENet. In contrast to speech, sounds coming from audio events may be produced by a wide variety of sources. Furthermore, distinguishing them often requires analyzing an extended time period due to the lack of clear sub-word units that are present in speech. In order to incorporate this long-time frequency structure of audio events, we introduce a convolutional neural network (CNN) operating on a large temporal input. In contrast to previous works this allows us to train an audio event detection system end-to-end. The combination of our network architecture and a novel data augmentation outperforms previous methods for audio event detection by 16%. Furthermore, we perform transfer learning and show that our model learnt generic audio features, similar to the way CNNs learn generic features on vision tasks. In video analysis, combining visual features and traditional audio features such as MFCC typically only leads to marginal improvements. Instead, combining visual features with our AENet features, which can be computed efficiently on a GPU, leads to significant performance improvements on action recognition and video highlight detection. In video highlight detection, our audio features improve the performance by more than 8% over visual features alone.



### Product Manifold Filter: Non-Rigid Shape Correspondence via Kernel Density Estimation in the Product Space
- **Arxiv ID**: http://arxiv.org/abs/1701.00669v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.00669v2)
- **Published**: 2017-01-03 11:43:44+00:00
- **Updated**: 2017-04-07 11:40:41+00:00
- **Authors**: Matthias Vestner, Roee Litman, Emanuele Rodol√†, Alex Bronstein, Daniel Cremers
- **Comment**: To appear at CVPR 2017
- **Journal**: None
- **Summary**: Many algorithms for the computation of correspondences between deformable shapes rely on some variant of nearest neighbor matching in a descriptor space. Such are, for example, various point-wise correspondence recovery algorithms used as a post-processing stage in the functional correspondence framework. Such frequently used techniques implicitly make restrictive assumptions (e.g., near-isometry) on the considered shapes and in practice suffer from lack of accuracy and result in poor surjectivity. We propose an alternative recovery technique capable of guaranteeing a bijective correspondence and producing significantly higher accuracy and smoothness. Unlike other methods our approach does not depend on the assumption that the analyzed shapes are isometric. We derive the proposed method from the statistical framework of kernel density estimation and demonstrate its performance on several challenging deformable 3D shape matching datasets.



### Mixed one-bit compressive sensing with applications to overexposure correction for CT reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1701.00694v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1701.00694v1)
- **Published**: 2017-01-03 14:35:33+00:00
- **Updated**: 2017-01-03 14:35:33+00:00
- **Authors**: Xiaolin Huang, Yan Xia, Lei Shi, Yixing Huang, Ming Yan, Joachim Hornegger, Andreas Maier
- **Comment**: None
- **Journal**: None
- **Summary**: When a measurement falls outside the quantization or measurable range, it becomes saturated and cannot be used in classical reconstruction methods. For example, in C-arm angiography systems, which provide projection radiography, fluoroscopy, digital subtraction angiography, and are widely used for medical diagnoses and interventions, the limited dynamic range of C-arm flat detectors leads to overexposure in some projections during an acquisition, such as imaging relatively thin body parts (e.g., the knee). Aiming at overexposure correction for computed tomography (CT) reconstruction, we in this paper propose a mixed one-bit compressive sensing (M1bit-CS) to acquire information from both regular and saturated measurements. This method is inspired by the recent progress on one-bit compressive sensing, which deals with only sign observations. Its successful applications imply that information carried by saturated measurements is useful to improve recovery quality. For the proposed M1bit-CS model, alternating direction methods of multipliers is developed and an iterative saturation detection scheme is established. Then we evaluate M1bit-CS on one-dimensional signal recovery tasks. In some experiments, the performance of the proposed algorithms on mixed measurements is almost the same as recovery on unsaturated ones with the same amount of measurements. Finally, we apply the proposed method to overexposure correction for CT reconstruction on a phantom and a simulated clinical image. The results are promising, as the typical streaking artifacts and capping artifacts introduced by saturated projection data are effectively reduced, yielding significant error reduction compared with existing algorithms based on extrapolation.



### Image denoising using group sparsity residual and external nonlocal self-similarity prior
- **Arxiv ID**: http://arxiv.org/abs/1701.00723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.00723v1)
- **Published**: 2017-01-03 15:32:16+00:00
- **Updated**: 2017-01-03 15:32:16+00:00
- **Authors**: Zhiyuan Zha, Xinggan Zhang, Qiong Wang, Yechao Bai, Lan Tang
- **Comment**: arXiv admin note: text overlap with arXiv:1609.03302
- **Journal**: None
- **Summary**: Nonlocal image representation has been successfully used in many image-related inverse problems including denoising, deblurring and deblocking. However, a majority of reconstruction methods only exploit the nonlocal self-similarity (NSS) prior of the degraded observation image, it is very challenging to reconstruct the latent clean image. In this paper we propose a novel model for image denoising via group sparsity residual and external NSS prior. To boost the performance of image denoising, the concept of group sparsity residual is proposed, and thus the problem of image denoising is transformed into one that reduces the group sparsity residual. Due to the fact that the groups contain a large amount of NSS information of natural images, we obtain a good estimation of the group sparse coefficients of the original image by the external NSS prior based on Gaussian Mixture model (GMM) learning and the group sparse coefficients of noisy image is used to approximate the estimation. Experimental results have demonstrated that the proposed method not only outperforms many state-of-the-art methods, but also delivers the best qualitative denoising results with finer details and less ringing artifacts.



### Constrained Deep Weak Supervision for Histopathology Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1701.00794v1
- **DOI**: 10.1109/TMI.2017.2724070
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.00794v1)
- **Published**: 2017-01-03 19:29:41+00:00
- **Updated**: 2017-01-03 19:29:41+00:00
- **Authors**: Zhipeng Jia, Xingyi Huang, Eric I-Chao Chang, Yan Xu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we develop a new weakly-supervised learning algorithm to learn to segment cancerous regions in histopathology images. Our work is under a multiple instance learning framework (MIL) with a new formulation, deep weak supervision (DWS); we also propose an effective way to introduce constraints to our neural networks to assist the learning process. The contributions of our algorithm are threefold: (1) We build an end-to-end learning system that segments cancerous regions with fully convolutional networks (FCN) in which image-to-image weakly-supervised learning is performed. (2) We develop a deep week supervision formulation to exploit multi-scale learning under weak supervision within fully convolutional networks. (3) Constraints about positive instances are introduced in our approach to effectively explore additional weakly-supervised information that is easy to obtain and enjoys a significant boost to the learning process. The proposed algorithm, abbreviated as DWS-MIL, is easy to implement and can be trained efficiently. Our system demonstrates state-of-the-art results on large-scale histopathology image datasets and can be applied to various applications in medical imaging beyond histopathology images such as MRI, CT, and ultrasound images.



### Semi-Supervised Endmember Identification In Nonlinear Spectral Mixtures Via Semantic Representation
- **Arxiv ID**: http://arxiv.org/abs/1701.00804v1
- **DOI**: 10.1109/TGRS.2017.2667226
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.00804v1)
- **Published**: 2017-01-03 19:59:15+00:00
- **Updated**: 2017-01-03 19:59:15+00:00
- **Authors**: Yuki Itoh, Siwei Feng, Marco F. Duarte, Mario Parente
- **Comment**: 15 pages, 11 figures, 4 tables
- **Journal**: None
- **Summary**: This paper proposes a new hyperspectral unmixing method for nonlinearly mixed hyperspectral data using a semantic representation in a semi-supervised fashion, assuming the availability of a spectral reference library. Existing semi-supervised unmixing algorithms select members from an endmember library that are present at each of the pixels; most such methods assume a linear mixing model. However, those methods will fail in the presence of nonlinear mixing among the observed spectra. To address this issue, we develop an endmember selection method using a recently proposed semantic spectral representation obtained via non-homogeneous hidden Markov chain (NHMC) model for a wavelet transform of the spectra. The semantic representation can encode spectrally discriminative features for any observed spectrum and, therefore, our proposed method can perform endmember selection without any assumption on the mixing model. Experimental results show that in the presence of sufficiently nonlinear mixing our proposed method outperforms dictionary-based sparse unmixing approaches based on linear models.



### Learning a Mixture of Deep Networks for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1701.00823v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.00823v1)
- **Published**: 2017-01-03 20:41:46+00:00
- **Updated**: 2017-01-03 20:41:46+00:00
- **Authors**: Ding Liu, Zhaowen Wang, Nasser Nasrabadi, Thomas Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Single image super-resolution (SR) is an ill-posed problem which aims to recover high-resolution (HR) images from their low-resolution (LR) observations. The crux of this problem lies in learning the complex mapping between low-resolution patches and the corresponding high-resolution patches. Prior arts have used either a mixture of simple regression models or a single non-linear neural network for this propose. This paper proposes the method of learning a mixture of SR inference modules in a unified framework to tackle this problem. Specifically, a number of SR inference modules specialized in different image local patterns are first independently applied on the LR image to obtain various HR estimates, and the resultant HR estimates are adaptively aggregated to form the final HR image. By selecting neural networks as the SR inference module, the whole procedure can be incorporated into a unified network and be optimized jointly. Extensive experiments are conducted to investigate the relation between restoration performance and different network architectures. Compared with other current image SR approaches, our proposed method achieves state-of-the-arts restoration results on a wide range of images consistently while allowing more flexible design choices. The source codes are available in http://www.ifp.illinois.edu/~dingliu2/accv2016.



