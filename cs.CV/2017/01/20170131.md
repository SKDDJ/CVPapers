# Arxiv Papers in cs.CV on 2017-01-31
### SenseGen: A Deep Learning Architecture for Synthetic Sensor Data Generation
- **Arxiv ID**: http://arxiv.org/abs/1701.08886v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1701.08886v1)
- **Published**: 2017-01-31 01:59:58+00:00
- **Updated**: 2017-01-31 01:59:58+00:00
- **Authors**: Moustafa Alzantot, Supriyo Chakraborty, Mani B. Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Our ability to synthesize sensory data that preserves specific statistical properties of the real data has had tremendous implications on data privacy and big data analytics. The synthetic data can be used as a substitute for selective real data segments,that are sensitive to the user, thus protecting privacy and resulting in improved analytics.However, increasingly adversarial roles taken by data recipients such as mobile apps, or other cloud-based analytics services, mandate that the synthetic data, in addition to preserving statistical properties, should also be difficult to distinguish from the real data. Typically, visual inspection has been used as a test to distinguish between datasets. But more recently, sophisticated classifier models (discriminators), corresponding to a set of events, have also been employed to distinguish between synthesized and real data. The model operates on both datasets and the respective event outputs are compared for consistency. In this paper, we take a step towards generating sensory data that can pass a deep learning based discriminator model test, and make two specific contributions: first, we present a deep learning based architecture for synthesizing sensory data. This architecture comprises of a generator model, which is a stack of multiple Long-Short-Term-Memory (LSTM) networks and a Mixture Density Network. second, we use another LSTM network based discriminator model for distinguishing between the true and the synthesized data. Using a dataset of accelerometer traces, collected using smartphones of users doing their daily activities, we show that the deep learning based discriminator model can only distinguish between the real and synthesized traces with an accuracy in the neighborhood of 50%.



### Stable and Controllable Neural Texture Synthesis and Style Transfer Using Histogram Losses
- **Arxiv ID**: http://arxiv.org/abs/1701.08893v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1701.08893v2)
- **Published**: 2017-01-31 02:37:19+00:00
- **Updated**: 2017-02-01 23:30:20+00:00
- **Authors**: Eric Risser, Pierre Wilmot, Connelly Barnes
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, methods have been proposed that perform texture synthesis and style transfer by using convolutional neural networks (e.g. Gatys et al. [2015,2016]). These methods are exciting because they can in some cases create results with state-of-the-art quality. However, in this paper, we show these methods also have limitations in texture quality, stability, requisite parameter tuning, and lack of user controls. This paper presents a multiscale synthesis pipeline based on convolutional neural networks that ameliorates these issues. We first give a mathematical explanation of the source of instabilities in many previous approaches. We then improve these instabilities by using histogram losses to synthesize textures that better statistically match the exemplar. We also show how to integrate localized style losses in our multiscale framework. These losses can improve the quality of large features, improve the separation of content and style, and offer artistic controls such as paint by numbers. We demonstrate that our approach offers improved quality, convergence in fewer iterations, and more stability over the optimization.



### Feature Selection based on PCA and PSO for Multimodal Medical Image Fusion using DTCWT
- **Arxiv ID**: http://arxiv.org/abs/1701.08918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.08918v1)
- **Published**: 2017-01-31 05:12:34+00:00
- **Updated**: 2017-01-31 05:12:34+00:00
- **Authors**: Padmavathi K, Mahima Bhat, Maya V Karki
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Multimodal medical image fusion helps to increase efficiency in medical diagnosis. This paper presents multimodal medical image fusion by selecting relevant features using Principle Component Analysis (PCA) and Particle Swarm Optimization techniques (PSO). DTCWT is used for decomposition of the images into low and high frequency coefficients. Fusion rules such as combination of minimum, maximum and simple averaging are applied to approximate and detailed coefficients. The fused image is reconstructed by inverse DTCWT. Performance metrics are evaluated and it shows that DTCWT-PCA performs better than DTCWT-PSO in terms of Structural Similarity Index Measure (SSIM) and Cross Correlation (CC). Computation time and feature vector size is reduced in DTCWT-PCA compared to DTCWT-PSO for feature selection which proves robustness and storage capacity.



### Co-segmentation for Space-Time Co-located Collections
- **Arxiv ID**: http://arxiv.org/abs/1701.08931v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.08931v1)
- **Published**: 2017-01-31 07:05:58+00:00
- **Updated**: 2017-01-31 07:05:58+00:00
- **Authors**: Hadar Averbuch-Elor, Johannes Kopf, Tamir Hazan, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: We present a co-segmentation technique for space-time co-located image collections. These prevalent collections capture various dynamic events, usually by multiple photographers, and may contain multiple co-occurring objects which are not necessarily part of the intended foreground object, resulting in ambiguities for traditional co-segmentation techniques. Thus, to disambiguate what the common foreground object is, we introduce a weakly-supervised technique, where we assume only a small seed, given in the form of a single segmented image. We take a distributed approach, where local belief models are propagated and reinforced with similar images. Our technique progressively expands the foreground and background belief models across the entire collection. The technique exploits the power of the entire set of image without building a global model, and thus successfully overcomes large variability in appearance of the common foreground object. We demonstrate that our method outperforms previous co-segmentation techniques on challenging space-time co-located collections, including dense benchmark datasets which were adapted for our novel problem setting.



### Deep Reinforcement Learning for Visual Object Tracking in Videos
- **Arxiv ID**: http://arxiv.org/abs/1701.08936v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1701.08936v2)
- **Published**: 2017-01-31 07:48:56+00:00
- **Updated**: 2017-04-10 20:34:43+00:00
- **Authors**: Da Zhang, Hamid Maei, Xin Wang, Yuan-Fang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce a fully end-to-end approach for visual tracking in videos that learns to predict the bounding box locations of a target object at every frame. An important insight is that the tracking problem can be considered as a sequential decision-making process and historical semantics encode highly relevant information for future decisions. Based on this intuition, we formulate our model as a recurrent convolutional neural network agent that interacts with a video overtime, and our model can be trained with reinforcement learning (RL) algorithms to learn good tracking policies that pay attention to continuous, inter-frame correlation and maximize tracking performance in the long run. The proposed tracking algorithm achieves state-of-the-art performance in an existing tracking benchmark and operates at frame-rates faster than real-time. To the best of our knowledge, our tracker is the first neural-network tracker that combines convolutional and recurrent networks with RL algorithms.



### Supervised Learning in Automatic Channel Selection for Epileptic Seizure Detection
- **Arxiv ID**: http://arxiv.org/abs/1701.08968v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.08968v1)
- **Published**: 2017-01-31 10:01:45+00:00
- **Updated**: 2017-01-31 10:01:45+00:00
- **Authors**: Nhan Truong, Levin Kuhlmann, Mohammad Reza Bonyadi, Jiawei Yang, Andrew Faulks, Omid Kavehei
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting seizure using brain neuroactivations recorded by intracranial electroencephalogram (iEEG) has been widely used for monitoring, diagnosing, and closed-loop therapy of epileptic patients, however, computational efficiency gains are needed if state-of-the-art methods are to be implemented in implanted devices. We present a novel method for automatic seizure detection based on iEEG data that outperforms current state-of-the-art seizure detection methods in terms of computational efficiency while maintaining the accuracy. The proposed algorithm incorporates an automatic channel selection (ACS) engine as a pre-processing stage to the seizure detection procedure. The ACS engine consists of supervised classifiers which aim to find iEEGchannelswhich contribute the most to a seizure. Seizure detection stage involves feature extraction and classification. Feature extraction is performed in both frequency and time domains where spectral power and correlation between channel pairs are calculated. Random Forest is used in classification of interictal, ictal and early ictal periods of iEEG signals. Seizure detection in this paper is retrospective and patient-specific. iEEG data is accessed via Kaggle, provided by International Epilepsy Electro-physiology Portal. The dataset includes a training set of 6.5 hours of interictal data and 41 minin ictal data and a test set of 9.14 hours. Compared to the state-of-the-art on the same dataset, we achieve 49.4% increase in computational efficiency and 400 mins better in average for detection delay. The proposed model is able to detect a seizure onset at 91.95% sensitivity and 94.05% specificity with a mean detection delay of 2.77 s. The area under the curve (AUC) is 96.44%, that is comparable to the current state-of-the-art with AUC of 96.29%.



### Towards Adversarial Retinal Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1701.08974v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1701.08974v1)
- **Published**: 2017-01-31 10:17:13+00:00
- **Updated**: 2017-01-31 10:17:13+00:00
- **Authors**: Pedro Costa, Adrian Galdran, Maria Inês Meyer, Michael David Abràmoff, Meindert Niemeijer, Ana Maria Mendonça, Aurélio Campilho
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesizing images of the eye fundus is a challenging task that has been previously approached by formulating complex models of the anatomy of the eye. New images can then be generated by sampling a suitable parameter space. In this work, we propose a method that learns to synthesize eye fundus images directly from data. For that, we pair true eye fundus images with their respective vessel trees, by means of a vessel segmentation technique. These pairs are then used to learn a mapping from a binary vessel tree to a new retinal image. For this purpose, we use a recent image-to-image translation technique, based on the idea of adversarial learning. Experimental results show that the original and the generated images are visually different in terms of their global appearance, in spite of sharing the same vessel tree. Additionally, a quantitative quality analysis of the synthetic retinal images confirms that the produced images retain a high proportion of the true image set quality.



### Deep Multitask Architecture for Integrated 2D and 3D Human Sensing
- **Arxiv ID**: http://arxiv.org/abs/1701.08985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.08985v1)
- **Published**: 2017-01-31 10:52:48+00:00
- **Updated**: 2017-01-31 10:52:48+00:00
- **Authors**: Alin-Ionut Popa, Mihai Zanfir, Cristian Sminchisescu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deep multitask architecture for \emph{fully automatic 2d and 3d human sensing} (DMHS), including \emph{recognition and reconstruction}, in \emph{monocular images}. The system computes the figure-ground segmentation, semantically identifies the human body parts at pixel level, and estimates the 2d and 3d pose of the person. The model supports the joint training of all components by means of multi-task losses where early processing stages recursively feed into advanced ones for increasingly complex calculations, accuracy and robustness. The design allows us to tie a complete training protocol, by taking advantage of multiple datasets that would otherwise restrictively cover only some of the model components: complex 2d image data with no body part labeling and without associated 3d ground truth, or complex 3d data with limited 2d background variability. In detailed experiments based on several challenging 2d and 3d datasets (LSP, HumanEva, Human3.6M), we evaluate the sub-structures of the model, the effect of various types of training data in the multitask loss, and demonstrate that state-of-the-art results can be achieved at all processing levels. We also show that in the wild our monocular RGB architecture is perceptually competitive to a state-of-the art (commercial) Kinect system based on RGB-D data.



### A novel method for automatic localization of joint area on knee plain radiographs
- **Arxiv ID**: http://arxiv.org/abs/1701.08991v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.08991v3)
- **Published**: 2017-01-31 11:06:12+00:00
- **Updated**: 2017-04-05 20:05:02+00:00
- **Authors**: Aleksei Tiulpin, Jérôme Thevenot, Esa Rahtu, Simo Saarakkala
- **Comment**: Accepted to Scandinavian Conference on Image Analysis (SCIA) 2017
- **Journal**: None
- **Summary**: Osteoarthritis (OA) is a common musculoskeletal condition typically diagnosed from radiographic assessment after clinical examination. However, a visual evaluation made by a practitioner suffers from subjectivity and is highly dependent on the experience. Computer-aided diagnostics (CAD) could improve the objectivity of knee radiographic examination. The first essential step of knee OA CAD is to automatically localize the joint area. However, according to the literature this task itself remains challenging. The aim of this study was to develop novel and computationally efficient method to tackle the issue. Here, three different datasets of knee radiographs were used (n = 473/93/77) to validate the overall performance of the method. Our pipeline consists of two parts: anatomically-based joint area proposal and their evaluation using Histogram of Oriented Gradients and the pre-trained Support Vector Machine classifier scores. The obtained results for the used datasets show the mean intersection over the union equal to: 0.84, 0.79 and 0.78. Using a high-end computer, the method allows to automatically annotate conventional knee radiographs within 14-16ms and high resolution ones within 170ms. Our results demonstrate that the developed method is suitable for large-scale analyses.



### Computational Techniques in Multispectral Image Processing: Application to the Syriac Galen Palimpsest
- **Arxiv ID**: http://arxiv.org/abs/1702.02508v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.02508v1)
- **Published**: 2017-01-31 13:03:20+00:00
- **Updated**: 2017-01-31 13:03:20+00:00
- **Authors**: Corneliu Arsene, Peter Pormann, William Sellers, Siam Bhayro
- **Comment**: 29 February - 2 March 2016, Second International Conference on
  Natural Sciences and Technology in Manuscript Analysis, Centre for the study
  of Manuscript Cultures, Hamburg, Germany
- **Journal**: None
- **Summary**: Multispectral and hyperspectral image analysis has experienced much development in the last decade. The application of these methods to palimpsests has produced significant results, enabling researchers to recover texts that would be otherwise lost under the visible overtext, by improving the contrast between the undertext and the overtext. In this paper we explore an extended number of multispectral and hyperspectral image analysis methods, consisting of supervised and unsupervised dimensionality reduction techniques, on a part of the Syriac Galen Palimpsest dataset (www.digitalgalen.net). Of this extended set of methods, eight methods gave good results: three were supervised methods Generalized Discriminant Analysis (GDA), Linear Discriminant Analysis (LDA), and Neighborhood Component Analysis (NCA); and the other five methods were unsupervised methods (but still used in a supervised way) Gaussian Process Latent Variable Model (GPLVM), Isomap, Landmark Isomap, Principal Component Analysis (PCA), and Probabilistic Principal Component Analysis (PPCA). The relative success of these methods was determined visually, using color pictures, on the basis of whether the undertext was distinguishable from the overtext, resulting in the following ranking of the methods: LDA, NCA, GDA, Isomap, Landmark Isomap, PPCA, PCA, and GPLVM. These results were compared with those obtained using the Canonical Variates Analysis (CVA) method on the same dataset, which showed remarkably accuracy (LDA is a particular case of CVA where the objects are classified to two classes).



### A New Method for Removing the Moire' Pattern from Images
- **Arxiv ID**: http://arxiv.org/abs/1701.09037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.09037v1)
- **Published**: 2017-01-31 13:52:31+00:00
- **Updated**: 2017-01-31 13:52:31+00:00
- **Authors**: Seyede Mahya Hazavei, Hamid Reza Shahdoosti
- **Comment**: 6 pages, 12 figures, conference
- **Journal**: None
- **Summary**: During the last decades, denoising methods have attracted much attention of researchers. The conventional method for removing the Moire' pattern from images is using notch filters in the Frequency-domain. In this paper a new method is proposed that can achieve a better performance in comparison with the traditional method. Median filter is used in some part of spectrum of noisy images to reduce the noise. At the second part of this paper, to demonstrate the robustness of the proposed method, it is implemented for some noisy images that have moire' pattern. Experiments on noisy images with different characteristics show that the proposed method increases the PSNR values compared with previous methods.



### DeepNav: Learning to Navigate Large Cities
- **Arxiv ID**: http://arxiv.org/abs/1701.09135v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.09135v2)
- **Published**: 2017-01-31 17:14:24+00:00
- **Updated**: 2017-05-20 22:40:26+00:00
- **Authors**: Samarth Brahmbhatt, James Hays
- **Comment**: CVPR 2017 camera ready version
- **Journal**: None
- **Summary**: We present DeepNav, a Convolutional Neural Network (CNN) based algorithm for navigating large cities using locally visible street-view images. The DeepNav agent learns to reach its destination quickly by making the correct navigation decisions at intersections. We collect a large-scale dataset of street-view images organized in a graph where nodes are connected by roads. This dataset contains 10 city graphs and more than 1 million street-view images. We propose 3 supervised learning approaches for the navigation task and show how A* search in the city graph can be used to generate supervision for the learning. Our annotation process is fully automated using publicly available mapping services and requires no human input. We evaluate the proposed DeepNav models on 4 held-out cities for navigating to 5 different types of destinations. Our algorithms outperform previous work that uses hand-crafted features and Support Vector Regression (SVR)[19].



### Spatial Aggregation of Holistically-Nested Convolutional Neural Networks for Automated Pancreas Localization and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1702.00045v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00045v1)
- **Published**: 2017-01-31 20:22:15+00:00
- **Updated**: 2017-01-31 20:22:15+00:00
- **Authors**: Holger R. Roth, Le Lu, Nathan Lay, Adam P. Harrison, Amal Farag, Andrew Sohn, Ronald M. Summers
- **Comment**: This version was submitted to IEEE Trans. on Medical Imaging on Dec.
  18th, 2016. The content of this article is covered by US Patent Applications
  of 62/345,606# and 62/450,681#
- **Journal**: None
- **Summary**: Accurate and automatic organ segmentation from 3D radiological scans is an important yet challenging problem for medical image analysis. Specifically, the pancreas demonstrates very high inter-patient anatomical variability in both its shape and volume. In this paper, we present an automated system using 3D computed tomography (CT) volumes via a two-stage cascaded approach: pancreas localization and segmentation. For the first step, we localize the pancreas from the entire 3D CT scan, providing a reliable bounding box for the more refined segmentation step. We introduce a fully deep-learning approach, based on an efficient application of holistically-nested convolutional networks (HNNs) on the three orthogonal axial, sagittal, and coronal views. The resulting HNN per-pixel probability maps are then fused using pooling to reliably produce a 3D bounding box of the pancreas that maximizes the recall. We show that our introduced localizer compares favorably to both a conventional non-deep-learning method and a recent hybrid approach based on spatial aggregation of superpixels using random forest classification. The second, segmentation, phase operates within the computed bounding box and integrates semantic mid-level cues of deeply-learned organ interior and boundary maps, obtained by two additional and separate realizations of HNNs. By integrating these two mid-level cues, our method is capable of generating boundary-preserving pixel-wise class label maps that result in the final pancreas segmentation. Quantitative evaluation is performed on a publicly available dataset of 82 patient CT scans using 4-fold cross-validation (CV). We achieve a Dice similarity coefficient (DSC) of 81.27+/-6.27% in validation, which significantly outperforms previous state-of-the art methods that report DSCs of 71.80+/-10.70% and 78.01+/-8.20%, respectively, using the same dataset.



### Vertical Landing for Micro Air Vehicles using Event-Based Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1702.00061v3
- **DOI**: 10.1002/rob.21764
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.00061v3)
- **Published**: 2017-01-31 21:43:23+00:00
- **Updated**: 2017-11-13 09:33:33+00:00
- **Authors**: Bas J. Pijnacker Hordijk, Kirk Y. W. Scheper, Guido C. H. E. de Croon
- **Comment**: 29 pages, 14 figures, under peer review
- **Journal**: None
- **Summary**: Small flying robots can perform landing maneuvers using bio-inspired optical flow by maintaining a constant divergence. However, optical flow is typically estimated from frame sequences recorded by standard miniature cameras. This requires processing full images on-board, limiting the update rate of divergence measurements, and thus the speed of the control loop and the robot. Event-based cameras overcome these limitations by only measuring pixel-level brightness changes at microsecond temporal accuracy, hence providing an efficient mechanism for optical flow estimation. This paper presents, to the best of our knowledge, the first work integrating event-based optical flow estimation into the control loop of a flying robot. We extend an existing 'local plane fitting' algorithm to obtain an improved and more computationally efficient optical flow estimation method, valid for a wide range of optical flow velocities. This method is validated for real event sequences. In addition, a method for estimating the divergence from event-based optical flow is introduced, which accounts for the aperture problem. The developed algorithms are implemented in a constant divergence landing controller on-board of a quadrotor. Experiments show that, using event-based optical flow, accurate divergence estimates can be obtained over a wide range of speeds. This enables the quadrotor to perform very fast landing maneuvers.



