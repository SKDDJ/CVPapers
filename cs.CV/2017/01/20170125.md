# Arxiv Papers in cs.CV on 2017-01-25
### Learning Multi-level Region Consistency with Dense Multi-label Networks for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1701.07122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.07122v1)
- **Published**: 2017-01-25 01:11:43+00:00
- **Updated**: 2017-01-25 01:11:43+00:00
- **Authors**: Tong Shen, Guosheng Lin, Chunhua Shen, Ian Reid
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic image segmentation is a fundamental task in image understanding. Per-pixel semantic labelling of an image benefits greatly from the ability to consider region consistency both locally and globally. However, many Fully Convolutional Network based methods do not impose such consistency, which may give rise to noisy and implausible predictions. We address this issue by proposing a dense multi-label network module that is able to encourage the region consistency at different levels. This simple but effective module can be easily integrated into any semantic segmentation systems. With comprehensive experiments, we show that the dense multi-label can successfully remove the implausible labels and clear the confusion so as to boost the performance of semantic segmentation systems.



### An Edge Driven Wavelet Frame Model for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1701.07158v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, math.FA
- **Links**: [PDF](http://arxiv.org/pdf/1701.07158v1)
- **Published**: 2017-01-25 04:56:10+00:00
- **Updated**: 2017-01-25 04:56:10+00:00
- **Authors**: Jae Kyu Choi, Bin Dong, Xiaoqun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Wavelet frame systems are known to be effective in capturing singularities from noisy and degraded images. In this paper, we introduce a new edge driven wavelet frame model for image restoration by approximating images as piecewise smooth functions. With an implicit representation of image singularities sets, the proposed model inflicts different strength of regularization on smooth and singular image regions and edges. The proposed edge driven model is robust to both image approximation and singularity estimation. The implicit formulation also enables an asymptotic analysis of the proposed models and a rigorous connection between the discrete model and a general continuous variational model. Finally, numerical results on image inpainting and deblurring show that the proposed model is compared favorably against several popular image restoration models.



### Historic Emergence of Diversity in Painting: Heterogeneity in Chromatic Distance in Images and Characterization of Massive Painting Data Set
- **Arxiv ID**: http://arxiv.org/abs/1701.07164v2
- **DOI**: 10.1371/journal.pone.0204430
- **Categories**: **cs.CV**, physics.soc-ph
- **Links**: [PDF](http://arxiv.org/pdf/1701.07164v2)
- **Published**: 2017-01-25 05:15:09+00:00
- **Updated**: 2018-09-14 02:41:53+00:00
- **Authors**: Byunghwee Lee, Daniel Kim, Seunghye Sun, Hawoong Jeong, Juyong Park
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Painting is an art form that has long functioned as a major channel for the creative expression and communication of humans, its evolution taking place under an interplay with the science, technology, and social environments of the times. Therefore, understanding the process based on comprehensive data could shed light on how humans acted and manifested creatively under changing conditions. Yet, there exist few systematic frameworks that characterize the process for painting, which would require robust statistical methods for defining painting characteristics and identifying human's creative developments, and data of high quality and sufficient quantity. Here we propose that the color contrast of a painting image signifying the heterogeneity in inter-pixel chromatic distance can be a useful representation of its style, integrating both the color and geometry. From the color contrasts of paintings from a large-scale, comprehensive archive of 179,853 high-quality images spanning several centuries we characterize the temporal evolutionary patterns of paintings, and present a deep study of an extraordinary expansion in creative diversity and individuality that came to define the modern era.



### Towards End-to-End Face Recognition through Alignment Learning
- **Arxiv ID**: http://arxiv.org/abs/1701.07174v1
- **DOI**: 10.1109/LSP.2017.2715076
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.07174v1)
- **Published**: 2017-01-25 06:10:41+00:00
- **Updated**: 2017-01-25 06:10:41+00:00
- **Authors**: Yuanyi Zhong, Jiansheng Chen, Bo Huang
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Plenty of effective methods have been proposed for face recognition during the past decade. Although these methods differ essentially in many aspects, a common practice of them is to specifically align the facial area based on the prior knowledge of human face structure before feature extraction. In most systems, the face alignment module is implemented independently. This has actually caused difficulties in the designing and training of end-to-end face recognition models. In this paper we study the possibility of alignment learning in end-to-end face recognition, in which neither prior knowledge on facial landmarks nor artificially defined geometric transformations are required. Specifically, spatial transformer layers are inserted in front of the feature extraction layers in a Convolutional Neural Network (CNN) for face recognition. Only human identity clues are used for driving the neural network to automatically learn the most suitable geometric transformation and the most appropriate facial area for the recognition task. To ensure reproducibility, our model is trained purely on the publicly available CASIA-WebFace dataset, and is tested on the Labeled Face in the Wild (LFW) dataset. We have achieved a verification accuracy of 99.08\% which is comparable to state-of-the-art single model based methods.



### Distributed methods for synchronization of orthogonal matrices over graphs
- **Arxiv ID**: http://arxiv.org/abs/1701.07248v3
- **DOI**: 10.1016/j.automatica.2017.02.025
- **Categories**: **math.OC**, cs.CV, cs.DC, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/1701.07248v3)
- **Published**: 2017-01-25 10:34:45+00:00
- **Updated**: 2017-04-07 12:11:11+00:00
- **Authors**: Johan Thunberg, Florian Bernard, Jorge Goncalves
- **Comment**: 18 pages, 2 figures
- **Journal**: None
- **Summary**: This paper addresses the problem of synchronizing orthogonal matrices over directed graphs. For synchronized transformations (or matrices), composite transformations over loops equal the identity. We formulate the synchronization problem as a least-squares optimization problem with nonlinear constraints. The synchronization problem appears as one of the key components in applications ranging from 3D-localization to image registration. The main contributions of this work can be summarized as the introduction of two novel algorithms; one for symmetric graphs and one for graphs that are possibly asymmetric. Under general conditions, the former has guaranteed convergence to the solution of a spectral relaxation to the synchronization problem. The latter is stable for small step sizes when the graph is quasi-strongly connected. The proposed methods are verified in numerical simulations.



### Universal representations:The missing link between faces, text, planktons, and cat breeds
- **Arxiv ID**: http://arxiv.org/abs/1701.07275v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1701.07275v1)
- **Published**: 2017-01-25 12:07:15+00:00
- **Updated**: 2017-01-25 12:07:15+00:00
- **Authors**: Hakan Bilen, Andrea Vedaldi
- **Comment**: 10 pages, 4 figures, 5 tables
- **Journal**: None
- **Summary**: With the advent of large labelled datasets and high-capacity models, the performance of machine vision systems has been improving rapidly. However, the technology has still major limitations, starting from the fact that different vision problems are still solved by different models, trained from scratch or fine-tuned on the target data. The human visual system, in stark contrast, learns a universal representation for vision in the early life of an individual. This representation works well for an enormous variety of vision problems, with little or no change, with the major advantage of requiring little training data to solve any of them.



### Photographic dataset: playing cards
- **Arxiv ID**: http://arxiv.org/abs/1701.07354v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1701.07354v1)
- **Published**: 2017-01-25 15:35:09+00:00
- **Updated**: 2017-01-25 15:35:09+00:00
- **Authors**: David Villacis, Santeri Kaupinm√§ki, Samuli Siltanen, Teemu Helenius
- **Comment**: 9 pages, 12 figures
- **Journal**: None
- **Summary**: This is a photographic dataset collected for testing image processing algorithms. The idea is to have images that can exploit the properties of total variation, therefore a set of playing cards was distributed on the scene. The dataset is made available at www.fips.fi/photographic_dataset2.php



### Deep Local Video Feature for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1701.07368v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.07368v2)
- **Published**: 2017-01-25 16:23:17+00:00
- **Updated**: 2017-01-28 13:50:09+00:00
- **Authors**: Zhenzhong Lan, Yi Zhu, Alexander G. Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate the problem of representing an entire video using CNN features for human action recognition. Currently, limited by GPU memory, we have not been able to feed a whole video into CNN/RNNs for end-to-end learning. A common practice is to use sampled frames as inputs and video labels as supervision. One major problem of this popular approach is that the local samples may not contain the information indicated by global labels. To deal with this problem, we propose to treat the deep networks trained on local inputs as local feature extractors. After extracting local features, we aggregate them into global features and train another mapping function on the same training data to map the global features into global labels. We study a set of problems regarding this new type of local features such as how to aggregate them into global features. Experimental results on HMDB51 and UCF101 datasets show that, for these new local features, a simple maximum pooling on the sparsely sampled features lead to significant performance improvement.



### A Multi-view RGB-D Approach for Human Pose Estimation in Operating Rooms
- **Arxiv ID**: http://arxiv.org/abs/1701.07372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.07372v1)
- **Published**: 2017-01-25 16:43:41+00:00
- **Updated**: 2017-01-25 16:43:41+00:00
- **Authors**: Abdolrahim Kadkhodamohammadi, Afshin Gangi, Michel de Mathelin, Nicolas Padoy
- **Comment**: WACV 2017. Supplementary material video: https://youtu.be/L3A0BzT0FKQ
- **Journal**: None
- **Summary**: Many approaches have been proposed for human pose estimation in single and multi-view RGB images. However, some environments, such as the operating room, are still very challenging for state-of-the-art RGB methods. In this paper, we propose an approach for multi-view 3D human pose estimation from RGB-D images and demonstrate the benefits of using the additional depth channel for pose refinement beyond its use for the generation of improved features. The proposed method permits the joint detection and estimation of the poses without knowing a priori the number of persons present in the scene. We evaluate this approach on a novel multi-view RGB-D dataset acquired during live surgeries and annotated with ground truth 3D poses.



### Recovering 3D Planar Arrangements from Videos
- **Arxiv ID**: http://arxiv.org/abs/1701.07393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.07393v1)
- **Published**: 2017-01-25 17:33:52+00:00
- **Updated**: 2017-01-25 17:33:52+00:00
- **Authors**: Shuai Du, Youyi Zheng
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Acquiring 3D geometry of real world objects has various applications in 3D digitization, such as navigation and content generation in virtual environments. Image remains one of the most popular media for such visual tasks due to its simplicity of acquisition. Traditional image-based 3D reconstruction approaches heavily exploit point-to-point correspondence among multiple images to estimate camera motion and 3D geometry. Establishing point-to-point correspondence lies at the center of the 3D reconstruction pipeline, which however is easily prone to errors. In this paper, we propose an optimization framework which traces image points using a novel structure-guided dynamic tracking algorithm and estimates both the camera motion and a 3D structure model by enforcing a set of planar constraints. The key to our method is a structure model represented as a set of planes and their arrangements. Constraints derived from the structure model is used both in the correspondence establishment stage and the bundle adjustment stage in our reconstruction pipeline. Experiments show that our algorithm can effectively localize structure correspondence across dense image frames while faithfully reconstructing the camera motion and the underlying structured 3D model.



### Learning Word-Like Units from Joint Audio-Visual Analysis
- **Arxiv ID**: http://arxiv.org/abs/1701.07481v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1701.07481v3)
- **Published**: 2017-01-25 20:40:56+00:00
- **Updated**: 2017-05-24 22:10:25+00:00
- **Authors**: David Harwath, James R. Glass
- **Comment**: None
- **Journal**: None
- **Summary**: Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the word 'lighthouse' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.



