# Arxiv Papers in cs.CV on 2017-01-05
### Overlapping Cover Local Regression Machines
- **Arxiv ID**: http://arxiv.org/abs/1701.01218v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1701.01218v1)
- **Published**: 2017-01-05 06:04:53+00:00
- **Updated**: 2017-01-05 06:04:53+00:00
- **Authors**: Mohamed Elhoseiny, Ahmed Elgammal
- **Comment**: Long Article with more experiments and analysis of conference paper
  "Overlapping Domain Cover for Scalable and Accurate Regression Kernel
  Machines", presented orally 2015 at the British Machine Vision Conference
  2015 (BMVC)
- **Journal**: None
- **Summary**: We present the Overlapping Domain Cover (ODC) notion for kernel machines, as a set of overlapping subsets of the data that covers the entire training set and optimized to be spatially cohesive as possible. We show how this notion benefit the speed of local kernel machines for regression in terms of both speed while achieving while minimizing the prediction error. We propose an efficient ODC framework, which is applicable to various regression models and in particular reduces the complexity of Twin Gaussian Processes (TGP) regression from cubic to quadratic. Our notion is also applicable to several kernel methods (e.g., Gaussian Process Regression(GPR) and IWTGP regression, as shown in our experiments). We also theoretically justified the idea behind our method to improve local prediction by the overlapping cover. We validated and analyzed our method on three benchmark human pose estimation datasets and interesting findings are discussed.



### Autoencoder Regularized Network For Driving Style Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1701.01272v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1701.01272v1)
- **Published**: 2017-01-05 10:38:07+00:00
- **Updated**: 2017-01-05 10:38:07+00:00
- **Authors**: Weishan Dong, Ting Yuan, Kai Yang, Changsheng Li, Shilei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study learning generalized driving style representations from automobile GPS trip data. We propose a novel Autoencoder Regularized deep neural Network (ARNet) and a trip encoding framework trip2vec to learn drivers' driving styles directly from GPS records, by combining supervised and unsupervised feature learning in a unified architecture. Experiments on a challenging driver number estimation problem and the driver identification problem show that ARNet can learn a good generalized driving style representation: It significantly outperforms existing methods and alternative architectures by reaching the least estimation error on average (0.68, less than one driver) and the highest identification accuracy (by at least 3% improvement) compared with traditional supervised learning methods.



### Learning from Synthetic Humans
- **Arxiv ID**: http://arxiv.org/abs/1701.01370v3
- **DOI**: 10.1109/CVPR.2017.492
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01370v3)
- **Published**: 2017-01-05 16:27:46+00:00
- **Updated**: 2018-01-19 12:34:53+00:00
- **Authors**: GÃ¼l Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black, Ivan Laptev, Cordelia Schmid
- **Comment**: Appears in: 2017 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2017). 9 pages
- **Journal**: None
- **Summary**: Estimating human pose, shape, and motion from images and videos are fundamental challenges with many applications. Recent advances in 2D human pose estimation use large amounts of manually-labeled training data for learning convolutional neural networks (CNNs). Such data is time consuming to acquire and difficult to extend. Moreover, manual labeling of 3D pose, depth and motion is impractical. In this work we present SURREAL (Synthetic hUmans foR REAL tasks): a new large-scale dataset with synthetically-generated but realistic images of people rendered from 3D sequences of human motion capture data. We generate more than 6 million frames together with ground truth pose, depth maps, and segmentation masks. We show that CNNs trained on our synthetic dataset allow for accurate human depth estimation and human part segmentation in real RGB images. Our results and the new dataset open up new possibilities for advancing person analysis using cheap and large-scale synthetic data.



### Quantitative Analysis of Automatic Image Cropping Algorithms: A Dataset and Comparative Study
- **Arxiv ID**: http://arxiv.org/abs/1701.01480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01480v1)
- **Published**: 2017-01-05 21:22:22+00:00
- **Updated**: 2017-01-05 21:22:22+00:00
- **Authors**: Yi-Ling Chen, Tzu-Wei Huang, Kai-Han Chang, Yu-Chen Tsai, Hwann-Tzong Chen, Bing-Yu Chen
- **Comment**: The dataset presented in this article can be found on <a
  href="https://github.com/yiling-chen/flickr-cropping-dataset">Github</a>
- **Journal**: None
- **Summary**: Automatic photo cropping is an important tool for improving visual quality of digital photos without resorting to tedious manual selection. Traditionally, photo cropping is accomplished by determining the best proposal window through visual quality assessment or saliency detection. In essence, the performance of an image cropper highly depends on the ability to correctly rank a number of visually similar proposal windows. Despite the ranking nature of automatic photo cropping, little attention has been paid to learning-to-rank algorithms in tackling such a problem. In this work, we conduct an extensive study on traditional approaches as well as ranking-based croppers trained on various image features. In addition, a new dataset consisting of high quality cropping and pairwise ranking annotations is presented to evaluate the performance of various baselines. The experimental results on the new dataset provide useful insights into the design of better photo cropping algorithms.



### Motion Deblurring in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1701.01486v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01486v2)
- **Published**: 2017-01-05 21:39:03+00:00
- **Updated**: 2017-08-29 13:51:31+00:00
- **Authors**: Mehdi Noroozi, Paramanand Chandramouli, Paolo Favaro
- **Comment**: None
- **Journal**: None
- **Summary**: The task of image deblurring is a very ill-posed problem as both the image and the blur are unknown. Moreover, when pictures are taken in the wild, this task becomes even more challenging due to the blur varying spatially and the occlusions between the object. Due to the complexity of the general image model we propose a novel convolutional network architecture which directly generates the sharp image.This network is built in three stages, and exploits the benefits of pyramid schemes often used in blind deconvolution. One of the main difficulties in training such a network is to design a suitable dataset. While useful data can be obtained by synthetically blurring a collection of images, more realistic data must be collected in the wild. To obtain such data we use a high frame rate video camera and keep one frame as the sharp image and frame average as the corresponding blurred image. We show that this realistic dataset is key in achieving state-of-the-art performance and dealing with occlusions.



