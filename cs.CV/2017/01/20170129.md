# Arxiv Papers in cs.CV on 2017-01-29
### Pooling Facial Segments to Face: The Shallow and Deep Ends
- **Arxiv ID**: http://arxiv.org/abs/1701.08341v1
- **DOI**: 10.1109/FG.2017.80
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.08341v1)
- **Published**: 2017-01-29 00:30:09+00:00
- **Updated**: 2017-01-29 00:30:09+00:00
- **Authors**: Upal Mahbub, Sayantan Sarkar, Rama Chellappa
- **Comment**: 8 pages, 7 figures, 3 tables, accepted for publication in FG2017
- **Journal**: 2017 12th IEEE International Conference on Automatic Face &
  Gesture Recognition (FG 2017)
- **Summary**: Generic face detection algorithms do not perform very well in the mobile domain due to significant presence of occluded and partially visible faces. One promising technique to handle the challenge of partial faces is to design face detectors based on facial segments. In this paper two such face detectors namely, SegFace and DeepSegFace, are proposed that detect the presence of a face given arbitrary combinations of certain face segments. Both methods use proposals from facial segments as input that are found using weak boosted classifiers. SegFace is a shallow and fast algorithm using traditional features, tailored for situations where real time constraints must be satisfied. On the other hand, DeepSegFace is a more powerful algorithm based on a deep convolutional neutral network (DCNN) architecture. DeepSegFace offers certain advantages over other DCNN-based face detectors as it requires relatively little amount of data to train by utilizing a novel data augmentation scheme and is very robust to occlusion by design. Extensive experiments show the superiority of the proposed methods, specially DeepSegFace, over other state-of-the-art face detectors in terms of precision-recall and ROC curve on two mobile face datasets.



### Supervised Deep Sparse Coding Networks
- **Arxiv ID**: http://arxiv.org/abs/1701.08349v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.08349v3)
- **Published**: 2017-01-29 04:03:39+00:00
- **Updated**: 2017-05-23 01:31:04+00:00
- **Authors**: Xiaoxia Sun, Nasser M. Nasrabadi, Trac D. Tran
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we describe the deep sparse coding network (SCN), a novel deep network that encodes intermediate representations with nonnegative sparse coding. The SCN is built upon a number of cascading bottleneck modules, where each module consists of two sparse coding layers with relatively wide and slim dictionaries that are specialized to produce high dimensional discriminative features and low dimensional representations for clustering, respectively. During training, both the dictionaries and regularization parameters are optimized with an end-to-end supervised learning algorithm based on multilevel optimization. Effectiveness of an SCN with seven bottleneck modules is verified on several popular benchmark datasets. Remarkably, with few parameters to learn, our SCN achieves 5.81% and 19.93% classification error rate on CIFAR-10 and CIFAR-100, respectively.



### Feature base fusion for splicing forgery detection based on neuro fuzzy
- **Arxiv ID**: http://arxiv.org/abs/1701.08374v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1701.08374v1)
- **Published**: 2017-01-29 13:19:07+00:00
- **Updated**: 2017-01-29 13:19:07+00:00
- **Authors**: Habib Ghaffari Hadigheh, Ghazali bin sulong
- **Comment**: None
- **Journal**: None
- **Summary**: Most of researches on image forensics have been mainly focused on detection of artifacts introduced by a single processing tool. They lead in the development of many specialized algorithms looking for one or more particular footprints under specific settings. Naturally, the performance of such algorithms are not perfect, and accordingly the provided output might be noisy, inaccurate and only partially correct. Furthermore, a forged image in practical scenarios is often the result of utilizing several tools available by image-processing software systems. Therefore, reliable tamper detection requires developing more poweful tools to deal with various tempering scenarios. Fusion of forgery detection tools based on Fuzzy Inference System has been used before for addressing this problem. Adjusting the membership functions and defining proper fuzzy rules for attaining to better results are time-consuming processes. This can be accounted as main disadvantage of fuzzy inference systems. In this paper, a Neuro-Fuzzy inference system for fusion of forgery detection tools is developed. The neural network characteristic of these systems provides appropriate tool for automatically adjusting the membership functions. Moreover, initial fuzzy inference system is generated based on fuzzy clustering techniques. The proposed framework is implemented and validated on a benchmark image splicing data set in which three forgery detection tools are fused based on adaptive Neuro-Fuzzy inference system. The outcome of the proposed method reveals that applying Neuro Fuzzy inference systems could be a better approach for fusion of forgery detection tools.



### VINet: Visual-Inertial Odometry as a Sequence-to-Sequence Learning Problem
- **Arxiv ID**: http://arxiv.org/abs/1701.08376v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.08376v2)
- **Published**: 2017-01-29 13:34:22+00:00
- **Updated**: 2017-04-02 17:11:53+00:00
- **Authors**: Ronald Clark, Sen Wang, Hongkai Wen, Andrew Markham, Niki Trigoni
- **Comment**: AAAI-17
- **Journal**: None
- **Summary**: In this paper we present an on-manifold sequence-to-sequence learning approach to motion estimation using visual and inertial sensors. It is to the best of our knowledge the first end-to-end trainable method for visual-inertial odometry which performs fusion of the data at an intermediate feature-representation level. Our method has numerous advantages over traditional approaches. Specifically, it eliminates the need for tedious manual synchronization of the camera and IMU as well as eliminating the need for manual calibration between the IMU and camera. A further advantage is that our model naturally and elegantly incorporates domain specific information which significantly mitigates drift. We show that our approach is competitive with state-of-the-art traditional methods when accurate calibration data is available and can be trained to outperform them in the presence of calibration and synchronization errors.



### MSCM-LiFe: Multi-scale cross modal linear feature for horizon detection in maritime images
- **Arxiv ID**: http://arxiv.org/abs/1701.08378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.08378v1)
- **Published**: 2017-01-29 13:35:47+00:00
- **Updated**: 2017-01-29 13:35:47+00:00
- **Authors**: D. K. Prasad, D. Rajan, C. K. Prasath, L. Rachmawati, E. Rajabaly, C. Quek
- **Comment**: 5 pages, 4 figures, IEEE TENCON 2016
- **Journal**: None
- **Summary**: This paper proposes a new method for horizon detection called the multi-scale cross modal linear feature. This method integrates three different concepts related to the presence of horizon in maritime images to increase the accuracy of horizon detection. Specifically it uses the persistence of horizon in multi-scale median filtering, and its detection as a linear feature commonly detected by two different methods, namely the Hough transform of edgemap and the intensity gradient. We demonstrate the performance of the method over 13 videos comprising of more than 3000 frames and show that the proposed method detects horizon with small error in most of the cases, outperforming three state-of-the-art methods.



### The HASYv2 dataset
- **Arxiv ID**: http://arxiv.org/abs/1701.08380v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.08380v1)
- **Published**: 2017-01-29 13:42:14+00:00
- **Updated**: 2017-01-29 13:42:14+00:00
- **Authors**: Martin Thoma
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes the HASYv2 dataset. HASY is a publicly available, free of charge dataset of single symbols similar to MNIST. It contains 168233 instances of 369 classes. HASY contains two challenges: A classification challenge with 10 pre-defined folds for 10-fold cross-validation and a verification challenge.



### Faceness-Net: Face Detection through Deep Facial Part Responses
- **Arxiv ID**: http://arxiv.org/abs/1701.08393v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.08393v3)
- **Published**: 2017-01-29 16:11:27+00:00
- **Updated**: 2017-08-25 12:36:25+00:00
- **Authors**: Shuo Yang, Ping Luo, Chen Change Loy, Xiaoou Tang
- **Comment**: Will appear in TPAMI. arXiv admin note: substantial text overlap with
  arXiv:1509.06451
- **Journal**: None
- **Summary**: We propose a deep convolutional neural network (CNN) for face detection leveraging on facial attributes based supervision. We observe a phenomenon that part detectors emerge within CNN trained to classify attributes from uncropped face images, without any explicit part supervision. The observation motivates a new method for finding faces through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is data-driven, and carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variations. Our method achieves promising performance on popular benchmarks including FDDB, PASCAL Faces, AFW, and WIDER FACE.



### Re-ranking Person Re-identification with k-reciprocal Encoding
- **Arxiv ID**: http://arxiv.org/abs/1701.08398v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.08398v4)
- **Published**: 2017-01-29 16:31:51+00:00
- **Updated**: 2017-05-05 02:46:47+00:00
- **Authors**: Zhun Zhong, Liang Zheng, Donglin Cao, Shaozi Li
- **Comment**: To appear in CVPR 2017
- **Journal**: None
- **Summary**: When considering person re-identification (re-ID) as a retrieval process, re-ranking is a critical step to improve its accuracy. Yet in the re-ID community, limited effort has been devoted to re-ranking, especially those fully automatic, unsupervised solutions. In this paper, we propose a k-reciprocal encoding method to re-rank the re-ID results. Our hypothesis is that if a gallery image is similar to the probe in the k-reciprocal nearest neighbors, it is more likely to be a true match. Specifically, given an image, a k-reciprocal feature is calculated by encoding its k-reciprocal nearest neighbors into a single vector, which is used for re-ranking under the Jaccard distance. The final distance is computed as the combination of the original distance and the Jaccard distance. Our re-ranking method does not require any human interaction or any labeled data, so it is applicable to large-scale datasets. Experiments on the large-scale Market-1501, CUHK03, MARS, and PRW datasets confirm the effectiveness of our method.



### When Slepian Meets Fiedler: Putting a Focus on the Graph Spectrum
- **Arxiv ID**: http://arxiv.org/abs/1701.08401v2
- **DOI**: 10.1109/LSP.2017.2704359
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1701.08401v2)
- **Published**: 2017-01-29 17:11:13+00:00
- **Updated**: 2017-03-21 12:53:26+00:00
- **Authors**: Dimitri Van De Ville, Robin Demesmaeker, Maria Giulia Preti
- **Comment**: 4 pages, 5 figures, submitted to IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: The study of complex systems benefits from graph models and their analysis. In particular, the eigendecomposition of the graph Laplacian lets emerge properties of global organization from local interactions; e.g., the Fiedler vector has the smallest non-zero eigenvalue and plays a key role for graph clustering. Graph signal processing focusses on the analysis of signals that are attributed to the graph nodes. The eigendecomposition of the graph Laplacian allows to define the graph Fourier transform and extend conventional signal-processing operations to graphs. Here, we introduce the design of Slepian graph signals, by maximizing energy concentration in a predefined subgraph for a graph spectral bandlimit. We establish a novel link with classical Laplacian embedding and graph clustering, which provides a meaning to localized graph frequencies.



### Transformation-Based Models of Video Sequences
- **Arxiv ID**: http://arxiv.org/abs/1701.08435v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1701.08435v3)
- **Published**: 2017-01-29 21:39:05+00:00
- **Updated**: 2023-02-06 14:49:05+00:00
- **Authors**: Joost van Amersfoort, Anitha Kannan, Marc'Aurelio Ranzato, Arthur Szlam, Du Tran, Soumith Chintala
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model. In order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discriminative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.



### SafeDrive: A Robust Lane Tracking System for Autonomous and Assisted Driving Under Limited Visibility
- **Arxiv ID**: http://arxiv.org/abs/1701.08449v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1701.08449v1)
- **Published**: 2017-01-29 23:17:21+00:00
- **Updated**: 2017-01-29 23:17:21+00:00
- **Authors**: Junaed Sattar, Jiawei Mo
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach towards robust lane tracking for assisted and autonomous driving, particularly under poor visibility. Autonomous detection of lane markers improves road safety, and purely visual tracking is desirable for widespread vehicle compatibility and reducing sensor intrusion, cost, and energy consumption. However, visual approaches are often ineffective because of a number of factors, including but not limited to occlusion, poor weather conditions, and paint wear-off. Our method, named SafeDrive, attempts to improve visual lane detection approaches in drastically degraded visual conditions without relying on additional active sensors. In scenarios where visual lane detection algorithms are unable to detect lane markers, the proposed approach uses location information of the vehicle to locate and access alternate imagery of the road and attempts detection on this secondary image. Subsequently, by using a combination of feature-based and pixel-based alignment, an estimated location of the lane marker is found in the current scene. We demonstrate the effectiveness of our system on actual driving data from locations in the United States with Google Street View as the source of alternate imagery.



