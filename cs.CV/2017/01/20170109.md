# Arxiv Papers in cs.CV on 2017-01-09
### Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1701.02096v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02096v2)
- **Published**: 2017-01-09 08:54:14+00:00
- **Updated**: 2017-11-06 14:47:29+00:00
- **Authors**: Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: The recent work of Gatys et al., who characterized the style of an image by the statistics of convolutional neural network filters, ignited a renewed interest in the texture generation and image stylization problems. While their image generation technique uses a slow optimization process, recently several authors have proposed to learn generator neural networks that can produce similar outputs in one quick forward pass. While generator networks are promising, they are still inferior in visual quality and diversity compared to generation-by-optimization. In this work, we advance them in two significant ways. First, we introduce an instance normalization module to replace batch normalization with significant improvements to the quality of image stylization. Second, we improve diversity by introducing a new learning formulation that encourages generators to sample unbiasedly from the Julesz texture ensemble, which is the equivalence class of all images characterized by certain filter responses. Together, these two improvements take feed forward texture synthesis and image stylization much closer to the quality of generation-via-optimization, while retaining the speed advantage.



### Temporal scale selection in time-causal scale space
- **Arxiv ID**: http://arxiv.org/abs/1701.05088v1
- **DOI**: 10.1007/s10851-016-0691-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05088v1)
- **Published**: 2017-01-09 09:48:49+00:00
- **Updated**: 2017-01-09 09:48:49+00:00
- **Authors**: Tony Lindeberg
- **Comment**: 44 pages, 15 figures, 10 tables in Journal of Mathematical Imaging
  and Vision, 2017
- **Journal**: None
- **Summary**: When designing and developing scale selection mechanisms for generating hypotheses about characteristic scales in signals, it is essential that the selected scale levels reflect the extent of the underlying structures in the signal.   This paper presents a theory and in-depth theoretical analysis about the scale selection properties of methods for automatically selecting local temporal scales in time-dependent signals based on local extrema over temporal scales of scale-normalized temporal derivative responses. Specifically, this paper develops a novel theoretical framework for performing such temporal scale selection over a time-causal and time-recursive temporal domain as is necessary when processing continuous video or audio streams in real time or when modelling biological perception.   For a recently developed time-causal and time-recursive scale-space concept defined by convolution with a scale-invariant limit kernel, we show that it is possible to transfer a large number of the desirable scale selection properties that hold for the Gaussian scale-space concept over a non-causal temporal domain to this temporal scale-space concept over a truly time-causal domain. Specifically, we show that for this temporal scale-space concept, it is possible to achieve true temporal scale invariance although the temporal scale levels have to be discrete, which is a novel theoretical construction.



### Green-Blue Stripe Pattern for Range Sensing from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1701.02123v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, I.2.10; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1701.02123v2)
- **Published**: 2017-01-09 10:16:11+00:00
- **Updated**: 2021-07-21 02:31:41+00:00
- **Authors**: Changsoo Je, Kyuhyoung Choi, Sang Wook Lee
- **Comment**: 7 pages, 5 figures. Updated version of a conference paper
- **Journal**: Proc. 30th Fall Semiannual Conference of Korea Information Science
  Society, vol. 2, pp. 661-663, Seoul, Korea, October, 2003
- **Summary**: In this paper, we present a novel method for rapid high-resolution range sensing using green-blue stripe pattern. We use green and blue for designing high-frequency stripe projection pattern. For accurate and reliable range recovery, we identify the stripe patterns by our color-stripe segmentation and unwrapping algorithms. The experimental result for a naked human face shows the effectiveness of our method.



### Discrete approximations of the affine Gaussian derivative model for visual receptive fields
- **Arxiv ID**: http://arxiv.org/abs/1701.02127v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02127v5)
- **Published**: 2017-01-09 10:42:03+00:00
- **Updated**: 2017-12-20 15:39:39+00:00
- **Authors**: Tony Lindeberg
- **Comment**: 41 pages, 11 figures
- **Journal**: None
- **Summary**: The affine Gaussian derivative model can in several respects be regarded as a canonical model for receptive fields over a spatial image domain: (i) it can be derived by necessity from scale-space axioms that reflect structural properties of the world, (ii) it constitutes an excellent model for the receptive fields of simple cells in the primary visual cortex and (iii) it is covariant under affine image deformations, which enables more accurate modelling of image measurements under the local image deformations caused by the perspective mapping, compared to the more commonly used Gaussian derivative model based on derivatives of the rotationally symmetric Gaussian kernel.   This paper presents a theory for discretizing the affine Gaussian scale-space concept underlying the affine Gaussian derivative model, so that scale-space properties hold also for the discrete implementation.   Two ways of discretizing spatial smoothing with affine Gaussian kernels are presented: (i) by solving semi-discretized affine diffusion equation, which has derived by necessity from the requirements of a semi-group structure over scale as parameterized by a family of spatial covariance matrices and obeying non-creation of new structures from any finer to any coarser scale in terms of non-enhancement of local extrema and (ii) approximating these semi-discrete affine receptive fields by parameterized families of 3x3-kernels as obtained from an additional discretization along the scale direction. The latter discrete approach can be optionally complemented by spatial subsampling at coarser scales, leading to the notion of affine hybrid pyramids.   Using these theoretical results, we outline hybrid architectures for discrete approximations of affine covariant receptive field families, to be used as a first processing layer for affine covariant and affine invariant visual operations at higher levels.



### Light Field Super-Resolution Via Graph-Based Regularization
- **Arxiv ID**: http://arxiv.org/abs/1701.02141v2
- **DOI**: 10.1109/TIP.2018.2828983
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1701.02141v2)
- **Published**: 2017-01-09 11:32:30+00:00
- **Updated**: 2017-07-31 17:17:15+00:00
- **Authors**: Mattia Rossi, Pascal Frossard
- **Comment**: This new version includes more material. In particular, we added: a
  new section on the computational complexity of the proposed algorithm,
  experimental comparisons with a CNN-based super-resolution algorithm, and new
  experiments on a third dataset
- **Journal**: None
- **Summary**: Light field cameras capture the 3D information in a scene with a single exposure. This special feature makes light field cameras very appealing for a variety of applications: from post-capture refocus, to depth estimation and image-based rendering. However, light field cameras suffer by design from strong limitations in their spatial resolution, which should therefore be augmented by computational methods. On the one hand, off-the-shelf single-frame and multi-frame super-resolution algorithms are not ideal for light field data, as they do not consider its particular structure. On the other hand, the few super-resolution algorithms explicitly tailored for light field data exhibit significant limitations, such as the need to estimate an explicit disparity map at each view. In this work we propose a new light field super-resolution algorithm meant to address these limitations. We adopt a multi-frame alike super-resolution approach, where the complementary information in the different light field views is used to augment the spatial resolution of the whole light field. We show that coupling the multi-frame approach with a graph regularizer, that enforces the light field structure via nonlocal self similarities, permits to avoid the costly and challenging disparity estimation step for all the views. Extensive experiments show that the new algorithm compares favorably to the other state-of-the-art methods for light field super-resolution, both in terms of PSNR and visual quality.



### A Learning-based Variable Size Part Extraction Architecture for 6D Object Pose Recovery in Depth
- **Arxiv ID**: http://arxiv.org/abs/1701.02166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02166v1)
- **Published**: 2017-01-09 13:20:32+00:00
- **Updated**: 2017-01-09 13:20:32+00:00
- **Authors**: Caner Sahin, Rigas Kouskouridas, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art techniques for 6D object pose recovery depend on occlusion-free point clouds to accurately register objects in 3D space. To deal with this shortcoming, we introduce a novel architecture called Iterative Hough Forest with Histogram of Control Points that is capable of estimating the 6D pose of occluded and cluttered objects given a candidate 2D bounding box. Our Iterative Hough Forest (IHF) is learnt using parts extracted only from the positive samples. These parts are represented with Histogram of Control Points (HoCP), a "scale-variant" implicit volumetric description, which we derive from recently introduced Implicit B-Splines (IBS). The rich discriminative information provided by the scale-variant HoCP features is leveraged during inference. An automatic variable size part extraction framework iteratively refines the object's initial pose that is roughly aligned due to the extraction of coarsest parts, the ones occupying the largest area in image pixels. The iterative refinement is accomplished based on finer (smaller) parts that are represented with more discriminative control point descriptors by using our Iterative Hough Forest. Experiments conducted on a publicly available dataset report that our approach show better registration performance than the state-of-the-art methods.



### Multiple Instance Hybrid Estimator for Learning Target Signatures
- **Arxiv ID**: http://arxiv.org/abs/1701.02258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02258v1)
- **Published**: 2017-01-09 16:59:01+00:00
- **Updated**: 2017-01-09 16:59:01+00:00
- **Authors**: Changzhe Jiao, Alina Zare
- **Comment**: None
- **Journal**: None
- **Summary**: Signature-based detectors for hyperspectral target detection rely on knowing the specific target signature in advance. However, target signature are often difficult or impossible to obtain. Furthermore, common methods for obtaining target signatures, such as from laboratory measurements or manual selection from an image scene, usually do not capture the discriminative features of target class. In this paper, an approach for estimating a discriminative target signature from imprecise labels is presented. The proposed approach maximizes the response of the hybrid sub-pixel detector within a multiple instance learning framework and estimates a set of discriminative target signatures. After learning target signatures, any signature based detector can then be applied on test data. Both simulated and real hyperspectral target detection experiments are shown to illustrate the effectiveness of the method.



### Visual Multiple-Object Tracking for Unknown Clutter Rate
- **Arxiv ID**: http://arxiv.org/abs/1701.02273v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02273v3)
- **Published**: 2017-01-09 17:40:40+00:00
- **Updated**: 2017-11-30 15:38:10+00:00
- **Authors**: Du Yong Kim
- **Comment**: 6 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: In multi-object tracking applications, model parameter tuning is a prerequisite for reliable performance. In particular, it is difficult to know statistics of false measurements due to various sensing conditions and changes in the field of views. In this paper we are interested in designing a multi-object tracking algorithm that handles unknown false measurement rate. Recently proposed robust multi-Bernoulli filter is employed for clutter estimation while generalized labeled multi-Bernoulli filter is considered for target tracking. Performance evaluation with real videos demonstrates the effectiveness of the tracking algorithm for real-world scenarios.



### Information Pursuit: A Bayesian Framework for Sequential Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/1701.02343v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1701.02343v1)
- **Published**: 2017-01-09 20:39:12+00:00
- **Updated**: 2017-01-09 20:39:12+00:00
- **Authors**: Ehsan Jahangiri, Erdem Yoruk, Rene Vidal, Laurent Younes, Donald Geman
- **Comment**: None
- **Journal**: None
- **Summary**: Despite enormous progress in object detection and classification, the problem of incorporating expected contextual relationships among object instances into modern recognition systems remains a key challenge. In this work we propose Information Pursuit, a Bayesian framework for scene parsing that combines prior models for the geometry of the scene and the spatial arrangement of objects instances with a data model for the output of high-level image classifiers trained to answer specific questions about the scene. In the proposed framework, the scene interpretation is progressively refined as evidence accumulates from the answers to a sequence of questions. At each step, we choose the question to maximize the mutual information between the new answer and the full interpretation given the current evidence obtained from previous inquiries. We also propose a method for learning the parameters of the model from synthesized, annotated scenes obtained by top-down sampling from an easy-to-learn generative scene model. Finally, we introduce a database of annotated indoor scenes of dining room tables, which we use to evaluate the proposed approach.



### MonoCap: Monocular Human Motion Capture using a CNN Coupled with a Geometric Prior
- **Arxiv ID**: http://arxiv.org/abs/1701.02354v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02354v2)
- **Published**: 2017-01-09 21:25:06+00:00
- **Updated**: 2018-03-09 09:28:11+00:00
- **Authors**: Xiaowei Zhou, Menglong Zhu, Georgios Pavlakos, Spyridon Leonardos, Kostantinos G. Derpanis, Kostas Daniilidis
- **Comment**: Accepted by PAMI. Extended version of the following paper: Sparseness
  Meets Deepness: 3D Human Pose Estimation from Monocular Video. X Zhou, M Zhu,
  S Leonardos, K Derpanis, K Daniilidis. CVPR 2016. arXiv admin note:
  substantial text overlap with arXiv:1511.09439
- **Journal**: None
- **Summary**: Recovering 3D full-body human pose is a challenging problem with many applications. It has been successfully addressed by motion capture systems with body worn markers and multiple cameras. In this paper, we address the more challenging case of not only using a single camera but also not leveraging markers: going directly from 2D appearance to 3D geometry. Deep learning approaches have shown remarkable abilities to discriminatively learn 2D appearance features. The missing piece is how to integrate 2D, 3D and temporal information to recover 3D geometry and account for the uncertainties arising from the discriminative model. We introduce a novel approach that treats 2D joint locations as latent variables whose uncertainty distributions are given by a deep fully convolutional neural network. The unknown 3D poses are modeled by a sparse representation and the 3D parameter estimates are realized via an Expectation-Maximization algorithm, where it is shown that the 2D joint location uncertainties can be conveniently marginalized out during inference. Extensive evaluation on benchmark datasets shows that the proposed approach achieves greater accuracy over state-of-the-art baselines. Notably, the proposed approach does not require synchronized 2D-3D data for training and is applicable to "in-the-wild" images, which is demonstrated with the MPII dataset.



### Son of Zorn's Lemma: Targeted Style Transfer Using Instance-aware Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1701.02357v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1701.02357v1)
- **Published**: 2017-01-09 21:30:03+00:00
- **Updated**: 2017-01-09 21:30:03+00:00
- **Authors**: Carlos Castillo, Soham De, Xintong Han, Bharat Singh, Abhay Kumar Yadav, Tom Goldstein
- **Comment**: None
- **Journal**: ICASSP 2017
- **Summary**: Style transfer is an important task in which the style of a source image is mapped onto that of a target image. The method is useful for synthesizing derivative works of a particular artist or specific painting. This work considers targeted style transfer, in which the style of a template image is used to alter only part of a target image. For example, an artist may wish to alter the style of only one particular object in a target image without altering the object's general morphology or surroundings. This is useful, for example, in augmented reality applications (such as the recently released Pokemon GO), where one wants to alter the appearance of a single real-world object in an image frame to make it appear as a cartoon. Most notably, the rendering of real-world objects into cartoon characters has been used in a number of films and television show, such as the upcoming series Son of Zorn. We present a method for targeted style transfer that simultaneously segments and stylizes single objects selected by the user. The method uses a Markov random field model to smooth and anti-alias outlier pixels near object boundaries, so that stylized objects naturally blend into their surroundings.



### Visualizing Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1701.02362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02362v1)
- **Published**: 2017-01-09 21:42:46+00:00
- **Updated**: 2017-01-09 21:42:46+00:00
- **Authors**: Brian Chu, Daylen Yang, Ravi Tadinada
- **Comment**: UC Berkeley CS 280 final project report
- **Journal**: None
- **Summary**: Residual networks are the current state of the art on ImageNet. Similar work in the direction of utilizing shortcut connections has been done extremely recently with derivatives of residual networks and with highway networks. This work potentially challenges our understanding that CNNs learn layers of local features that are followed by increasingly global features. Through qualitative visualization and empirical analysis, we explore the purpose that residual skip connections serve. Our assessments show that the residual shortcut connections force layers to refine features, as expected. We also provide alternate visualizations that confirm that residual networks learn what is already intuitively known about CNNs in general.



