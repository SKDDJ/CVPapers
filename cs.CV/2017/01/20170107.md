# Arxiv Papers in cs.CV on 2017-01-07
### Large-scale Isolated Gesture Recognition Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1701.01814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01814v1)
- **Published**: 2017-01-07 10:31:58+00:00
- **Updated**: 2017-01-07 10:31:58+00:00
- **Authors**: Pichao Wang, Wanqing Li, Song Liu, Zhimin Gao, Chang Tang, Philip Ogunbona
- **Comment**: arXiv admin note: text overlap with arXiv:1608.06338
- **Journal**: None
- **Summary**: This paper proposes three simple, compact yet effective representations of depth sequences, referred to respectively as Dynamic Depth Images (DDI), Dynamic Depth Normal Images (DDNI) and Dynamic Depth Motion Normal Images (DDMNI). These dynamic images are constructed from a sequence of depth maps using bidirectional rank pooling to effectively capture the spatial-temporal information. Such image-based representations enable us to fine-tune the existing ConvNets models trained on image data for classification of depth sequences, without introducing large parameters to learn. Upon the proposed representations, a convolutional Neural networks (ConvNets) based method is developed for gesture recognition and evaluated on the Large-scale Isolated Gesture Recognition at the ChaLearn Looking at People (LAP) challenge 2016. The method achieved 55.57\% classification accuracy and ranked $2^{nd}$ place in this challenge but was very close to the best performance even though we only used depth data.



### Unsupervised Learning of Long-Term Motion Dynamics for Videos
- **Arxiv ID**: http://arxiv.org/abs/1701.01821v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01821v3)
- **Published**: 2017-01-07 12:03:11+00:00
- **Updated**: 2017-04-11 22:09:03+00:00
- **Authors**: Zelun Luo, Boya Peng, De-An Huang, Alexandre Alahi, Li Fei-Fei
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: We present an unsupervised representation learning approach that compactly encodes the motion dependencies in videos. Given a pair of images from a video clip, our framework learns to predict the long-term 3D motions. To reduce the complexity of the learning framework, we propose to describe the motion as a sequence of atomic 3D flows computed with RGB-D modality. We use a Recurrent Neural Network based Encoder-Decoder framework to predict these sequences of flows. We argue that in order for the decoder to reconstruct these sequences, the encoder must learn a robust video representation that captures long-term motion dependencies and spatial-temporal relations. We demonstrate the effectiveness of our learned temporal representations on activity classification across multiple modalities and datasets such as NTU RGB+D and MSR Daily Activity 3D. Our framework is generic to any input modality, i.e., RGB, Depth, and RGB-D videos.



### Oriented Response Networks
- **Arxiv ID**: http://arxiv.org/abs/1701.01833v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01833v2)
- **Published**: 2017-01-07 14:18:01+00:00
- **Updated**: 2017-07-13 02:32:23+00:00
- **Authors**: Yanzhao Zhou, Qixiang Ye, Qiang Qiu, Jianbin Jiao
- **Comment**: Accepted in CVPR 2017. Source code available at http://yzhou.work/ORN
- **Journal**: None
- **Summary**: Deep Convolution Neural Networks (DCNNs) are capable of learning unprecedentedly effective image representations. However, their ability in handling significant local and global image rotations remains limited. In this paper, we propose Active Rotating Filters (ARFs) that actively rotate during convolution and produce feature maps with location and orientation explicitly encoded. An ARF acts as a virtual filter bank containing the filter itself and its multiple unmaterialised rotated versions. During back-propagation, an ARF is collectively updated using errors from all its rotated versions. DCNNs using ARFs, referred to as Oriented Response Networks (ORNs), can produce within-class rotation-invariant deep features while maintaining inter-class discrimination for classification tasks. The oriented response produced by ORNs can also be used for image and object orientation estimation tasks. Over multiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, we consistently observe that replacing regular filters with the proposed ARFs leads to significant reduction in the number of network parameters and improvement in classification performance. We report the best results on several commonly used benchmarks.



### Sign Language Recognition Using Temporal Classification
- **Arxiv ID**: http://arxiv.org/abs/1701.01875v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01875v1)
- **Published**: 2017-01-07 20:09:52+00:00
- **Updated**: 2017-01-07 20:09:52+00:00
- **Authors**: Hardie Cate, Fahim Dalvi, Zeshan Hussain
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Devices like the Myo armband available in the market today enable us to collect data about the position of a user's hands and fingers over time. We can use these technologies for sign language translation since each sign is roughly a combination of gestures across time. In this work, we utilize a dataset collected by a group at the University of South Wales, which contains parameters, such as hand position, hand rotation, and finger bend, for 95 unique signs. For each input stream representing a sign, we predict which sign class this stream falls into. We begin by implementing baseline SVM and logistic regression models, which perform reasonably well on high quality data. Lower quality data requires a more sophisticated approach, so we explore different methods in temporal classification, including long short term memory architectures and sequential pattern mining methods.



### DeepFace: Face Generation using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1701.01876v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01876v1)
- **Published**: 2017-01-07 20:22:05+00:00
- **Updated**: 2017-01-07 20:22:05+00:00
- **Authors**: Hardie Cate, Fahim Dalvi, Zeshan Hussain
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: We use CNNs to build a system that both classifies images of faces based on a variety of different facial attributes and generates new faces given a set of desired facial characteristics. After introducing the problem and providing context in the first section, we discuss recent work related to image generation in Section 2. In Section 3, we describe the methods used to fine-tune our CNN and generate new images using a novel approach inspired by a Gaussian mixture model. In Section 4, we discuss our working dataset and describe our preprocessing steps and handling of facial attributes. Finally, in Sections 5, 6 and 7, we explain our experiments and results and conclude in the following section. Our classification system has 82\% test accuracy. Furthermore, our generation pipeline successfully creates well-formed faces.



### Greedy Search for Descriptive Spatial Face Features
- **Arxiv ID**: http://arxiv.org/abs/1701.01879v2
- **DOI**: 10.1109/ICASSP.2017.7952406
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01879v2)
- **Published**: 2017-01-07 20:36:18+00:00
- **Updated**: 2017-07-03 20:03:32+00:00
- **Authors**: Caner Gacav, Burak Benligiray, Cihan Topal
- **Comment**: International Conference on Acoustics, Speech and Signal Processing
  (ICASSP), 2017
- **Journal**: None
- **Summary**: Facial expression recognition methods use a combination of geometric and appearance-based features. Spatial features are derived from displacements of facial landmarks, and carry geometric information. These features are either selected based on prior knowledge, or dimension-reduced from a large pool. In this study, we produce a large number of potential spatial features using two combinations of facial landmarks. Among these, we search for a descriptive subset of features using sequential forward selection. The chosen feature subset is used to classify facial expressions in the extended Cohn-Kanade dataset (CK+), and delivered 88.7% recognition accuracy without using any appearance-based features.



### Group Visual Sentiment Analysis
- **Arxiv ID**: http://arxiv.org/abs/1701.01885v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01885v1)
- **Published**: 2017-01-07 21:12:24+00:00
- **Updated**: 2017-01-07 21:12:24+00:00
- **Authors**: Zeshan Hussain, Tariq Patanam, Hardie Cate
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: In this paper, we introduce a framework for classifying images according to high-level sentiment. We subdivide the task into three primary problems: emotion classification on faces, human pose estimation, and 3D estimation and clustering of groups of people. We introduce novel algorithms for matching body parts to a common individual and clustering people in images based on physical location and orientation. Our results outperform several baseline approaches.



### Urban Scene Segmentation with Laser-Constrained CRFs
- **Arxiv ID**: http://arxiv.org/abs/1701.01892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01892v1)
- **Published**: 2017-01-07 22:58:26+00:00
- **Updated**: 2017-01-07 22:58:26+00:00
- **Authors**: Charika De Alvis, Lionel Ott, Fabio Ramos
- **Comment**: In International Conference On Intelligent Robots and Systems, 2016
- **Journal**: None
- **Summary**: Robots typically possess sensors of different modalities, such as colour cameras, inertial measurement units, and 3D laser scanners. Often, solving a particular problem becomes easier when more than one modality is used. However, while there are undeniable benefits to combine sensors of different modalities the process tends to be complicated. Segmenting scenes observed by the robot into a discrete set of classes is a central requirement for autonomy as understanding the scene is the first step to reason about future situations. Scene segmentation is commonly performed using either image data or 3D point cloud data. In computer vision many successful methods for scene segmentation are based on conditional random fields (CRF) where the maximum a posteriori (MAP) solution to the segmentation can be obtained by inference. In this paper we devise a new CRF inference method for scene segmentation that incorporates global constraints, enforcing the sets of nodes are assigned the same class label. To do this efficiently, the CRF is formulated as a relaxed quadratic program whose MAP solution is found using a gradient-based optimisation approach. The proposed method is evaluated on images and 3D point cloud data gathered in urban environments where image data provides the appearance features needed by the CRF, while the 3D point cloud data provides global spatial constraints over sets of nodes. Comparisons with belief propagation, conventional quadratic programming relaxation, and higher order potential CRF show the benefits of the proposed method.



