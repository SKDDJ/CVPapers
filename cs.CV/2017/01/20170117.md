# Arxiv Papers in cs.CV on 2017-01-17
### Systematic study of color spaces and components for the segmentation of sky/cloud images
- **Arxiv ID**: http://arxiv.org/abs/1701.04520v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.04520v1)
- **Published**: 2017-01-17 03:27:56+00:00
- **Updated**: 2017-01-17 03:27:56+00:00
- **Authors**: Soumyabrata Dev, Yee Hui Lee, Stefan Winkler
- **Comment**: Published in Proc. IEEE International Conference on Image Processing
  (ICIP), Oct. 2014
- **Journal**: None
- **Summary**: Sky/cloud imaging using ground-based Whole Sky Imagers (WSI) is a cost-effective means to understanding cloud cover and weather patterns. The accurate segmentation of clouds in these images is a challenging task, as clouds do not possess any clear structure. Several algorithms using different color models have been proposed in the literature. This paper presents a systematic approach for the selection of color spaces and components for optimal segmentation of sky/cloud images. Using mainly principal component analysis (PCA) and fuzzy clustering for evaluation, we identify the most suitable color components for this task.



### Fusing Deep Learned and Hand-Crafted Features of Appearance, Shape, and Dynamics for Automatic Pain Estimation
- **Arxiv ID**: http://arxiv.org/abs/1701.04540v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.04540v1)
- **Published**: 2017-01-17 06:05:48+00:00
- **Updated**: 2017-01-17 06:05:48+00:00
- **Authors**: Joy Egede, Michel Valstar, Brais Martinez
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Automatic continuous time, continuous value assessment of a patient's pain from face video is highly sought after by the medical profession. Despite the recent advances in deep learning that attain impressive results in many domains, pain estimation risks not being able to benefit from this due to the difficulty in obtaining data sets of considerable size. In this work we propose a combination of hand-crafted and deep-learned features that makes the most of deep learning techniques in small sample settings. Encoding shape, appearance, and dynamics, our method significantly outperforms the current state of the art, attaining a RMSE error of less than 1 point on a 16-level pain scale, whilst simultaneously scoring a 67.3% Pearson correlation coefficient between our predicted pain level time series and the ground truth.



### Image Generation and Editing with Variational Info Generative AdversarialNetworks
- **Arxiv ID**: http://arxiv.org/abs/1701.04568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.04568v1)
- **Published**: 2017-01-17 08:48:28+00:00
- **Updated**: 2017-01-17 08:48:28+00:00
- **Authors**: Mahesh Gorijala, Ambedkar Dukkipati
- **Comment**: None
- **Journal**: None
- **Summary**: Recently there has been an enormous interest in generative models for images in deep learning. In pursuit of this, Generative Adversarial Networks (GAN) and Variational Auto-Encoder (VAE) have surfaced as two most prominent and popular models. While VAEs tend to produce excellent reconstructions but blurry samples, GANs generate sharp but slightly distorted images. In this paper we propose a new model called Variational InfoGAN (ViGAN). Our aim is two fold: (i) To generated new images conditioned on visual descriptions, and (ii) modify the image, by fixing the latent representation of image and varying the visual description. We evaluate our model on Labeled Faces in the Wild (LFW), celebA and a modified version of MNIST datasets and demonstrate the ability of our model to generate new images as well as to modify a given image by changing attributes.



### Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks
- **Arxiv ID**: http://arxiv.org/abs/1701.04658v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.04658v2)
- **Published**: 2017-01-17 13:04:33+00:00
- **Updated**: 2017-04-28 17:08:42+00:00
- **Authors**: Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbel√°ez, Luc Van Gool
- **Comment**: Accepted by T-PAMI. Extended version of "Convolutional Oriented
  Boundaries", ECCV 2016 (arXiv:1608.02755). Project page:
  http://www.vision.ee.ethz.ch/~cvlsegmentation/cob/
- **Journal**: None
- **Summary**: We present Convolutional Oriented Boundaries (COB), which produces multiscale oriented contours and region hierarchies starting from generic image classification Convolutional Neural Networks (CNNs). COB is computationally efficient, because it requires a single CNN forward pass for multi-scale contour detection and it uses a novel sparse boundary representation for hierarchical segmentation; it gives a significant leap in performance over the state-of-the-art, and it generalizes very well to unseen categories and datasets. Particularly, we show that learning to estimate not only contour strength but also orientation provides more accurate results. We perform extensive experiments for low-level applications on BSDS, PASCAL Context, PASCAL Segmentation, and NYUD to evaluate boundary detection performance, showing that COB provides state-of-the-art contours and region hierarchies in all datasets. We also evaluate COB on high-level tasks when coupled with multiple pipelines for object proposals, semantic contours, semantic segmentation, and object detection on MS-COCO, SBD, and PASCAL; showing that COB also improves the results for all tasks.



### Human perception in computer vision
- **Arxiv ID**: http://arxiv.org/abs/1701.04674v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1701.04674v1)
- **Published**: 2017-01-17 14:00:30+00:00
- **Updated**: 2017-01-17 14:00:30+00:00
- **Authors**: Ron Dekel
- **Comment**: Under review as a conference paper at ICLR 2017
- **Journal**: None
- **Summary**: Computer vision has made remarkable progress in recent years. Deep neural network (DNN) models optimized to identify objects in images exhibit unprecedented task-trained accuracy and, remarkably, some generalization ability: new visual problems can now be solved more easily based on previous learning. Biological vision (learned in life and through evolution) is also accurate and general-purpose. Is it possible that these different learning regimes converge to similar problem-dependent optimal computations? We therefore asked whether the human system-level computation of visual perception has DNN correlates and considered several anecdotal test cases. We found that perceptual sensitivity to image changes has DNN mid-computation correlates, while sensitivity to segmentation, crowding and shape has DNN end-computation correlates. Our results quantify the applicability of using DNN computation to estimate perceptual loss, and are consistent with the fascinating theoretical view that properties of human perception are a consequence of architecture-independent visual learning.



### Computing Egomotion with Local Loop Closures for Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/1701.04743v1
- **DOI**: 10.1109/WACV.2017.57
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.04743v1)
- **Published**: 2017-01-17 16:08:02+00:00
- **Updated**: 2017-01-17 16:08:02+00:00
- **Authors**: Suvam Patra, Himanshu Aggarwal, Himani Arora, Chetan Arora, Subhashis Banerjee
- **Comment**: Accepted in WACV 2017
- **Journal**: None
- **Summary**: Finding the camera pose is an important step in many egocentric video applications. It has been widely reported that, state of the art SLAM algorithms fail on egocentric videos. In this paper, we propose a robust method for camera pose estimation, designed specifically for egocentric videos. In an egocentric video, the camera views the same scene point multiple times as the wearer's head sweeps back and forth. We use this specific motion profile to perform short loop closures aligned with wearer's footsteps. For egocentric videos, depth estimation is usually noisy. In an important departure, we use 2D computations for rotation averaging which do not rely upon depth estimates. The two modification results in much more stable algorithm as is evident from our experiments on various egocentric video datasets for different egocentric applications. The proposed algorithm resolves a long standing problem in egocentric vision and unlocks new usage scenarios for future applications.



### 3D Reconstruction of Simple Objects from A Single View Silhouette Image
- **Arxiv ID**: http://arxiv.org/abs/1701.04752v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.04752v1)
- **Published**: 2017-01-17 16:39:32+00:00
- **Updated**: 2017-01-17 16:39:32+00:00
- **Authors**: Xinhan Di, Pengqian Yu
- **Comment**: Submitted Nov 2016
- **Journal**: None
- **Summary**: While recent deep neural networks have achieved promising results for 3D reconstruction from a single-view image, these rely on the availability of RGB textures in images and extra information as supervision. In this work, we propose novel stacked hierarchical networks and an end to end training strategy to tackle a more challenging task for the first time, 3D reconstruction from a single-view 2D silhouette image. We demonstrate that our model is able to conduct 3D reconstruction from a single-view silhouette image both qualitatively and quantitatively. Evaluation is performed using Shapenet for the single-view reconstruction and results are presented in comparison with a single network, to highlight the improvements obtained with the proposed stacked networks and the end to end training strategy. Furthermore, 3D re- construction in forms of IoU is compared with the state of art 3D reconstruction from a single-view RGB image, and the proposed model achieves higher IoU than the state of art of reconstruction from a single view RGB image.



### Complex Event Recognition from Images with Few Training Examples
- **Arxiv ID**: http://arxiv.org/abs/1701.04769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.04769v1)
- **Published**: 2017-01-17 17:16:55+00:00
- **Updated**: 2017-01-17 17:16:55+00:00
- **Authors**: Unaiza Ahsan, Chen Sun, James Hays, Irfan Essa
- **Comment**: Accepted to Winter Applications of Computer Vision (WACV'17)
- **Journal**: None
- **Summary**: We propose to leverage concept-level representations for complex event recognition in photographs given limited training examples. We introduce a novel framework to discover event concept attributes from the web and use that to extract semantic features from images and classify them into social event categories with few training examples. Discovered concepts include a variety of objects, scenes, actions and event sub-types, leading to a discriminative and compact representation for event images. Web images are obtained for each discovered event concept and we use (pretrained) CNN features to train concept classifiers. Extensive experiments on challenging event datasets demonstrate that our proposed method outperforms several baselines using deep CNN features directly in classifying images into events with limited training examples. We also demonstrate that our method achieves the best overall accuracy on a dataset with unseen event categories using a single training example.



### Synthesizing Normalized Faces from Facial Identity Features
- **Arxiv ID**: http://arxiv.org/abs/1701.04851v4
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1701.04851v4)
- **Published**: 2017-01-17 20:03:46+00:00
- **Updated**: 2017-10-17 15:27:21+00:00
- **Authors**: Forrester Cole, David Belanger, Dilip Krishnan, Aaron Sarna, Inbar Mosseri, William T. Freeman
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for synthesizing a frontal, neutral-expression image of a person's face given an input face photograph. This is achieved by learning to generate facial landmarks and textures from features extracted from a facial-recognition network. Unlike previous approaches, our encoding feature vector is largely invariant to lighting, pose, and facial expression. Exploiting this invariance, we train our decoder network using only frontal, neutral-expression photographs. Since these photographs are well aligned, we can decompose them into a sparse set of landmark points and aligned texture maps. The decoder then predicts landmarks and textures independently and combines them using a differentiable image warping operation. The resulting images can be used for a number of applications, such as analyzing facial attributes, exposure and white balance adjustment, or creating a 3-D avatar.



