# Arxiv Papers in cs.CV on 2017-01-16
### 3D tracking of water hazards with polarized stereo cameras
- **Arxiv ID**: http://arxiv.org/abs/1701.04175v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1701.04175v2)
- **Published**: 2017-01-16 05:47:30+00:00
- **Updated**: 2017-02-26 07:36:42+00:00
- **Authors**: Chuong V. Nguyen, Michael Milford, Robert Mahony
- **Comment**: 7 pages, ICRA 2017
- **Journal**: None
- **Summary**: Current self-driving car systems operate well in sunny weather but struggle in adverse conditions. One of the most commonly encountered adverse conditions involves water on the road caused by rain, sleet, melting snow or flooding. While some advances have been made in using conventional RGB camera and LIDAR technology for detecting water hazards, other sources of information such as polarization offer a promising and potentially superior approach to this problem in terms of performance and cost. In this paper, we present a novel stereo-polarization system for detecting and tracking water hazards based on polarization and color variation of reflected light, with consideration of the effect of polarized light from sky as function of reflection and azimuth angles. To evaluate this system, we present a new large `water on road' datasets spanning approximately 2 km of driving in various on-road and off-road conditions and demonstrate for the first time reliable water detection and tracking over a wide range of realistic car driving water conditions using polarized vision as the primary sensing modality. Our system successfully detects water hazards up to more than 100m. Finally, we discuss several interesting challenges and propose future research directions for further improving robust autonomous car perception in hazardous wet conditions using polarization sensors.



### A Watermarking Technique Using Discrete Curvelet Transform for Security of Multiple Biometric Features
- **Arxiv ID**: http://arxiv.org/abs/1701.04185v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1701.04185v1)
- **Published**: 2017-01-16 06:41:21+00:00
- **Updated**: 2017-01-16 06:41:21+00:00
- **Authors**: Rohit M. Thanki, Ved Vyas Dwivedi, Komal R. Borisagar
- **Comment**: None
- **Journal**: International Journal of Information Processing,volume 10, issue
  1, pp. 103 - 114 (2016)
- **Summary**: The robustness and security of the biometric watermarking approach can be improved by using a multiple watermarking. This multiple watermarking proposed for improving security of biometric features and data. When the imposter tries to create the spoofed biometric feature, the invisible biometric watermark features can provide appropriate protection to multimedia data. In this paper, a biometric watermarking technique with multiple biometric watermarks are proposed in which biometric features of fingerprint, face, iris and signature is embedded in the image. Before embedding, fingerprint, iris, face and signature features are extracted using Shen-Castan edge detection and Principal Component Analysis. These all biometric watermark features are embedded into various mid band frequency curvelet coefficients of host image. All four fingerprint features, iris features, facial features and signature features are the biometric characteristics of the individual and they are used for cross verification and copyright protection if any manipulation occurs. The proposed technique is fragile enough; features cannot be extracted from the watermarked image when an imposter tries to remove watermark features illegally. It can use for multiple copyright authentication and verification.



### Bandwidth limited object recognition in high resolution imagery
- **Arxiv ID**: http://arxiv.org/abs/1701.04210v1
- **DOI**: 10.1109/WACV.2017.138
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.04210v1)
- **Published**: 2017-01-16 09:16:35+00:00
- **Updated**: 2017-01-16 09:16:35+00:00
- **Authors**: Laura Lopez-Fuentes, Andrew D. Bagdanov, Joost van de Weijer, Harald Skinnemoen
- **Comment**: 9 pages, 9 figures, accepted in WACV
- **Journal**: Applications of Computer Vision (WACV), 2017 IEEE Winter
  Conference on. IEEE, 2017. p. 1197-1205
- **Summary**: This paper proposes a novel method to optimize bandwidth usage for object detection in critical communication scenarios. We develop two operating models of active information seeking. The first model identifies promising regions in low resolution imagery and progressively requests higher resolution regions on which to perform recognition of higher semantic quality. The second model identifies promising regions in low resolution imagery while simultaneously predicting the approximate location of the object of higher semantic quality. From this general framework, we develop a car recognition system via identification of its license plate and evaluate the performance of both models on a car dataset that we introduce. Results are compared with traditional JPEG compression and demonstrate that our system saves up to one order of magnitude of bandwidth while sacrificing little in terms of recognition performance.



### Auxiliary Multimodal LSTM for Audio-visual Speech Recognition and Lipreading
- **Arxiv ID**: http://arxiv.org/abs/1701.04224v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.04224v2)
- **Published**: 2017-01-16 10:08:22+00:00
- **Updated**: 2017-03-17 14:57:06+00:00
- **Authors**: Chunlin Tian, Weijun Ji
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: The Aduio-visual Speech Recognition (AVSR) which employs both the video and audio information to do Automatic Speech Recognition (ASR) is one of the application of multimodal leaning making ASR system more robust and accuracy. The traditional models usually treated AVSR as inference or projection but strict prior limits its ability. As the revival of deep learning, Deep Neural Networks (DNN) becomes an important toolkit in many traditional classification tasks including ASR, image classification, natural language processing. Some DNN models were used in AVSR like Multimodal Deep Autoencoders (MDAEs), Multimodal Deep Belief Network (MDBN) and Multimodal Deep Boltzmann Machine (MDBM) that actually work better than traditional methods. However, such DNN models have several shortcomings: (1) They don't balance the modal fusion and temporal fusion, or even haven't temporal fusion; (2)The architecture of these models isn't end-to-end, the training and testing getting cumbersome. We propose a DNN model, Auxiliary Multimodal LSTM (am-LSTM), to overcome such weakness. The am-LSTM could be trained and tested once, moreover easy to train and preventing overfitting automatically. The extensibility and flexibility are also take into consideration. The experiments show that am-LSTM is much better than traditional methods and other DNN models in three datasets.



### Geometric features for voxel-based surface recognition
- **Arxiv ID**: http://arxiv.org/abs/1701.04249v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1701.04249v1)
- **Published**: 2017-01-16 11:30:31+00:00
- **Updated**: 2017-01-16 11:30:31+00:00
- **Authors**: Dmitry Yarotsky
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: We introduce a library of geometric voxel features for CAD surface recognition/retrieval tasks. Our features include local versions of the intrinsic volumes (the usual 3D volume, surface area, integrated mean and Gaussian curvature) and a few closely related quantities. We also compute Haar wavelet and statistical distribution features by aggregating raw voxel features. We apply our features to object classification on the ESB data set and demonstrate accurate results with a small number of shallow decision trees.



### Automatic Spatial Context-Sensitive Cloud/Cloud-Shadow Detection in Multi-Source Multi-Spectral Earth Observation Images: AutoCloud+
- **Arxiv ID**: http://arxiv.org/abs/1701.04256v1
- **DOI**: 10.13140/RG.2.2.34162.71363
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.04256v1)
- **Published**: 2017-01-16 11:58:40+00:00
- **Updated**: 2017-01-16 11:58:40+00:00
- **Authors**: Andrea Baraldi
- **Comment**: Invitation to tender ESA/AO/1 8373/15/I NB, VAE: Next Generation EO
  based Information Services
- **Journal**: None
- **Summary**: The proposed Earth observation (EO) based value adding system (EO VAS), hereafter identified as AutoCloud+, consists of an innovative EO image understanding system (EO IUS) design and implementation capable of automatic spatial context sensitive cloud/cloud shadow detection in multi source multi spectral (MS) EO imagery, whether or not radiometrically calibrated, acquired by multiple platforms, either spaceborne or airborne, including unmanned aerial vehicles (UAVs). It is worth mentioning that the same EO IUS architecture is suitable for a large variety of EO based value adding products and services, including: (i) low level image enhancement applications, such as automatic MS image topographic correction, co registration, mosaicking and compositing, (ii) high level MS image land cover (LC) and LC change (LCC) classification and (iii) content based image storage/retrieval in massive multi source EO image databases (big data mining).



### Hierarchical Salient Object Detection for Assisted Grasping
- **Arxiv ID**: http://arxiv.org/abs/1701.04284v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1701.04284v2)
- **Published**: 2017-01-16 13:21:35+00:00
- **Updated**: 2017-01-17 12:29:05+00:00
- **Authors**: Dominik Alexander Klein, Boris Illing, Bastian Gaspers, Dirk Schulz, Armin Bernd Cremers
- **Comment**: Accepted for ICRA 2017
- **Journal**: None
- **Summary**: Visual scene decomposition into semantic entities is one of the major challenges when creating a reliable object grasping system. Recently, we introduced a bottom-up hierarchical clustering approach which is able to segment objects and parts in a scene. In this paper, we introduce a transform from such a segmentation into a corresponding, hierarchical saliency function. In comprehensive experiments we demonstrate its ability to detect salient objects in a scene. Furthermore, this hierarchical saliency defines a most salient corresponding region (scale) for every point in an image. Based on this, an easy-to-use pick and place manipulation system was developed and tested exemplarily.



