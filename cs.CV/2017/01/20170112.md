# Arxiv Papers in cs.CV on 2017-01-12
### Light Source Point Cluster Selection Based Atmosphere Light Estimation
- **Arxiv ID**: http://arxiv.org/abs/1701.03244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.03244v1)
- **Published**: 2017-01-12 05:56:28+00:00
- **Updated**: 2017-01-12 05:56:28+00:00
- **Authors**: Wenbo Zhang, Xiaorong Hou
- **Comment**: None
- **Journal**: None
- **Summary**: Atmosphere light value is a highly critical parameter in defogging algorithms that are based on an atmosphere scattering model. Any error in atmosphere light value will produce a direct impact on the accuracy of scattering computation and thus bring chromatic distortion to restored images. To address this problem, this paper propose a method that relies on clustering statistics to estimate atmosphere light value. It starts by selecting in the original image some potential atmosphere light source points, which are grouped into point clusters by means of clustering technique. From these clusters, a number of clusters containing candidate atmosphere light source points are selected, the points are then analyzed statistically, and the cluster containing the most candidate points is used for estimating atmosphere light value. The mean brightness vector of the candidate atmosphere light points in the chosen point cluster is taken as the estimate of atmosphere light value, while their geometric center in the image is accepted as the location of atmosphere light. Experimental results suggest that this statistics clustering method produces more accurate atmosphere brightness vectors and light source locations. This accuracy translates to, from a subjective perspective, more natural defogging effect on the one hand and to the improvement in various objective image quality indicators on the other hand.



### Ordered Pooling of Optical Flow Sequences for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1701.03246v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.03246v2)
- **Published**: 2017-01-12 06:08:18+00:00
- **Updated**: 2017-04-06 05:27:03+00:00
- **Authors**: Jue Wang, Anoop Cherian, Fatih Porikli
- **Comment**: Accepted in WACV 2017
- **Journal**: None
- **Summary**: Training of Convolutional Neural Networks (CNNs) on long video sequences is computationally expensive due to the substantial memory requirements and the massive number of parameters that deep architectures demand. Early fusion of video frames is thus a standard technique, in which several consecutive frames are first agglomerated into a compact representation, and then fed into the CNN as an input sample. For this purpose, a summarization approach that represents a set of consecutive RGB frames by a single dynamic image to capture pixel dynamics is proposed recently. In this paper, we introduce a novel ordered representation of consecutive optical flow frames as an alternative and argue that this representation captures the action dynamics more effectively than RGB frames. We provide intuitions on why such a representation is better for action recognition. We validate our claims on standard benchmark datasets and demonstrate that using summaries of flow images lead to significant improvements over RGB frames while achieving accuracy comparable to the state-of-the-art on UCF101 and HMDB datasets.



### Probabilistic Diffeomorphic Registration: Representing Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1701.03266v1
- **DOI**: 10.1007/978-3-319-08554-8_8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.03266v1)
- **Published**: 2017-01-12 08:29:31+00:00
- **Updated**: 2017-01-12 08:29:31+00:00
- **Authors**: Demian Wassermann, Matt Toews, Marc Niethammer, William Wells Iii
- **Comment**: None
- **Journal**: Biomedical Image Registration, 2014, London, United Kingdom. pp.72
  - 82, 2014
- **Summary**: This paper presents a novel mathematical framework for representing uncertainty in large deformation diffeomorphic image registration. The Bayesian posterior distribution over the deformations aligning a moving and a fixed image is approximated via a variational formulation. A stochastic differential equation (SDE) modeling the deformations as the evolution of a time-varying velocity field leads to a prior density over deformations in the form of a Gaussian process. This permits estimating the full posterior distribution in order to represent uncertainty, in contrast to methods in which the posterior is approximated via Monte Carlo sampling or maximized in maximum a-posteriori (MAP) estimation. The frame-work is demonstrated in the case of landmark-based image registration, including simulated data and annotated pre and intra-operative 3D images.



### Two-view 3D Reconstruction for Food Volume Estimation
- **Arxiv ID**: http://arxiv.org/abs/1701.03330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.03330v1)
- **Published**: 2017-01-12 13:10:22+00:00
- **Updated**: 2017-01-12 13:10:22+00:00
- **Authors**: Joachim Dehais, Marios Anthimopoulos, Sergey Shevchik, Stavroula Mougiakakou
- **Comment**: 10 pages, 7 Tables, 8 Figures in IEEE Transactions on Multimedia,
  2016
- **Journal**: None
- **Summary**: The increasing prevalence of diet-related chronic diseases coupled with the ineffectiveness of traditional diet management methods have resulted in a need for novel tools to accurately and automatically assess meals. Recently, computer vision based systems that use meal images to assess their content have been proposed. Food portion estimation is the most difficult task for individuals assessing their meals and it is also the least studied area. The present paper proposes a three-stage system to calculate portion sizes using two images of a dish acquired by mobile devices. The first stage consists in understanding the configuration of the different views, after which a dense 3D model is built from the two images; finally, this 3D model serves to extract the volume of the different items. The system was extensively tested on 77 real dishes of known volume, and achieved an average error of less than 10% in 5.5 seconds per dish. The proposed pipeline is computationally tractable and requires no user input, making it a viable option for fully automated dietary assessment.



### A Digital Fuzzy Edge Detector for Color Images
- **Arxiv ID**: http://arxiv.org/abs/1701.03364v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.03364v2)
- **Published**: 2017-01-12 14:52:32+00:00
- **Updated**: 2017-01-13 05:17:13+00:00
- **Authors**: Yuan-Hang Zhang, Xie Li, Jing-Yun Xiao
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Edge detection is a classic problem in the field of image processing, which lays foundations for other tasks such as image segmentation. Conventionally, this operation is performed using gradient operators such as the Roberts or Sobel operator, which can discover local changes in intensity levels. These operators, however, perform poorly on low contrast images. In this paper, we propose an edge detector architecture for color images based on fuzzy theory and the Sobel operator. First, the R, G and B channels are extracted from an image and enhanced using fuzzy methods, in order to suppress noise and improve the contrast between the background and the objects. The Sobel operator is then applied to each of the channels, which are finally combined into an edge map of the origin image. Experimental results obtained through an FPGA-based implementation have proved the proposed method effective.



### Scaling Binarized Neural Networks on Reconfigurable Logic
- **Arxiv ID**: http://arxiv.org/abs/1701.03400v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1701.03400v2)
- **Published**: 2017-01-12 16:42:47+00:00
- **Updated**: 2017-01-27 09:12:48+00:00
- **Authors**: Nicholas J. Fraser, Yaman Umuroglu, Giulio Gambardella, Michaela Blott, Philip Leong, Magnus Jahre, Kees Vissers
- **Comment**: To appear in the PARMA-DITAM workshop at HiPEAC 2017, January 2017
- **Journal**: None
- **Summary**: Binarized neural networks (BNNs) are gaining interest in the deep learning community due to their significantly lower computational and memory cost. They are particularly well suited to reconfigurable logic devices, which contain an abundance of fine-grained compute resources and can result in smaller, lower power implementations, or conversely in higher classification rates. Towards this end, the Finn framework was recently proposed for building fast and flexible field programmable gate array (FPGA) accelerators for BNNs. Finn utilized a novel set of optimizations that enable efficient mapping of BNNs to hardware and implemented fully connected, non-padded convolutional and pooling layers, with per-layer compute resources being tailored to user-provided throughput requirements. However, FINN was not evaluated on larger topologies due to the size of the chosen FPGA, and exhibited decreased accuracy due to lack of padding. In this paper, we improve upon Finn to show how padding can be employed on BNNs while still maintaining a 1-bit datapath and high accuracy. Based on this technique, we demonstrate numerous experiments to illustrate flexibility and scalability of the approach. In particular, we show that a large BNN requiring 1.2 billion operations per frame running on an ADM-PCIE-8K5 platform can classify images at 12 kFPS with 671 us latency while drawing less than 41 W board power and classifying CIFAR-10 images at 88.7% accuracy. Our implementation of this network achieves 14.8 trillion operations per second. We believe this is the fastest classification rate reported to date on this benchmark at this level of accuracy.



### Joint Dictionary Learning for Example-based Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1701.03420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.03420v1)
- **Published**: 2017-01-12 17:21:46+00:00
- **Updated**: 2017-01-12 17:21:46+00:00
- **Authors**: Mojtaba Sahraee-Ardakan, Mohsen Joneidi
- **Comment**: 5 pages, 1 figure, 1 table
- **Journal**: None
- **Summary**: In this paper, we propose a new joint dictionary learning method for example-based image super-resolution (SR), using sparse representation. The low-resolution (LR) dictionary is trained from a set of LR sample image patches. Using the sparse representation coefficients of these LR patches over the LR dictionary, the high-resolution (HR) dictionary is trained by minimizing the reconstruction error of HR sample patches. The error criterion used here is the mean square error. In this way we guarantee that the HR patches have the same sparse representation over HR dictionary as the LR patches over the LR dictionary, and at the same time, these sparse representations can well reconstruct the HR patches. Simulation results show the effectiveness of our method compared to the state-of-art SR algorithms.



### Comprehension-guided referring expressions
- **Arxiv ID**: http://arxiv.org/abs/1701.03439v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.03439v1)
- **Published**: 2017-01-12 18:03:52+00:00
- **Updated**: 2017-01-12 18:03:52+00:00
- **Authors**: Ruotian Luo, Gregory Shakhnarovich
- **Comment**: None
- **Journal**: None
- **Summary**: We consider generation and comprehension of natural language referring expression for objects in an image. Unlike generic "image captioning" which lacks natural standard evaluation criteria, quality of a referring expression may be measured by the receiver's ability to correctly infer which object is being described. Following this intuition, we propose two approaches to utilize models trained for comprehension task to generate better expressions. First, we use a comprehension module trained on human-generated expressions, as a "critic" of referring expression generator. The comprehension module serves as a differentiable proxy of human evaluation, providing training signal to the generation module. Second, we use the comprehension module in a generate-and-rerank pipeline, which chooses from candidate expressions generated by a model according to their performance on the comprehension task. We show that both approaches lead to improved referring expression generation on multiple benchmark datasets.



