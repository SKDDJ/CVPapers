# Arxiv Papers in cs.CV on 2017-01-01
### Video-based Person Re-identification with Accumulative Motion Context
- **Arxiv ID**: http://arxiv.org/abs/1701.00193v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.00193v2)
- **Published**: 2017-01-01 04:20:20+00:00
- **Updated**: 2017-06-13 03:27:01+00:00
- **Authors**: Hao Liu, Zequn Jie, Karlekar Jayashree, Meibin Qi, Jianguo Jiang, Shuicheng Yan, Jiashi Feng
- **Comment**: accepted by TCSVT
- **Journal**: None
- **Summary**: Video based person re-identification plays a central role in realistic security and video surveillance. In this paper we propose a novel Accumulative Motion Context (AMOC) network for addressing this important problem, which effectively exploits the long-range motion context for robustly identifying the same person under challenging conditions. Given a video sequence of the same or different persons, the proposed AMOC network jointly learns appearance representation and motion context from a collection of adjacent frames using a two-stream convolutional architecture. Then AMOC accumulates clues from motion context by recurrent aggregation, allowing effective information flow among adjacent frames and capturing dynamic gist of the persons. The architecture of AMOC is end-to-end trainable and thus motion context can be adapted to complement appearance clues under unfavorable conditions (e.g. occlusions). Extensive experiments are conduced on three public benchmark datasets, i.e., the iLIDS-VID, PRID-2011 and MARS datasets, to investigate the performance of AMOC. The experimental results demonstrate that the proposed AMOC network outperforms state-of-the-arts for video-based re-identification significantly and confirm the advantage of exploiting long-range motion context for video based person re-identification, validating our motivation evidently.



### A robust approach for tree segmentation in deciduous forests using small-footprint airborne LiDAR data
- **Arxiv ID**: http://arxiv.org/abs/1701.00198v1
- **DOI**: 10.1016/j.jag.2016.07.006
- **Categories**: **cs.CV**, cs.CE, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1701.00198v1)
- **Published**: 2017-01-01 04:49:47+00:00
- **Updated**: 2017-01-01 04:49:47+00:00
- **Authors**: Hamid Hamraz, Marco A. Contreras, Jun Zhang
- **Comment**: None
- **Journal**: International Journal of Applied Earth Observation and
  Geoinformation 52 (pp. 532-541): Elsevier (2016)
- **Summary**: This paper presents a non-parametric approach for segmenting trees from airborne LiDAR data in deciduous forests. Based on the LiDAR point cloud, the approach collects crown information such as steepness and height on-the-fly to delineate crown boundaries, and most importantly, does not require a priori assumptions of crown shape and size. The approach segments trees iteratively starting from the tallest within a given area to the smallest until all trees have been segmented. To evaluate its performance, the approach was applied to the University of Kentucky Robinson Forest, a deciduous closed-canopy forest with complex terrain and vegetation conditions. The approach identified 94% of dominant and co-dominant trees with a false detection rate of 13%. About 62% of intermediate, overtopped, and dead trees were also detected with a false detection rate of 15%. The overall segmentation accuracy was 77%. Correlations of the segmentation scores of the proposed approach with local terrain and stand metrics was not significant, which is likely an indication of the robustness of the approach as results are not sensitive to the differences in terrain and stand structures.



### The Geodesic Distance between $\mathcal{G}_I^0$ Models and its Application to Region Discrimination
- **Arxiv ID**: http://arxiv.org/abs/1701.00294v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1701.00294v1)
- **Published**: 2017-01-01 22:37:13+00:00
- **Updated**: 2017-01-01 22:37:13+00:00
- **Authors**: Jos√© Naranjo-Torres, Juliana Gambini, Alejandro C. Frery
- **Comment**: Accepted for publication in the IEEE Journal of Selected Topics in
  Applied Earth Observations and Remote Sensing (J-STARS), 1 January 2017
- **Journal**: None
- **Summary**: The $\mathcal{G}_I^0$ distribution is able to characterize different regions in monopolarized SAR imagery. It is indexed by three parameters: the number of looks (which can be estimated in the whole image), a scale parameter and a texture parameter. This paper presents a new proposal for feature extraction and region discrimination in SAR imagery, using the geodesic distance as a measure of dissimilarity between $\mathcal{G}_I^0$ models. We derive geodesic distances between models that describe several practical situations, assuming the number of looks known, for same and different texture and for same and different scale. We then apply this new tool to the problems of (i)~identifying edges between regions with different texture, and (ii)~quantify the dissimilarity between pairs of samples in actual SAR data. We analyze the advantages of using the geodesic distance when compared to stochastic distances.



### Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1701.00295v4
- **DOI**: 10.1109/CVPR.2017.603
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.00295v4)
- **Published**: 2017-01-01 22:50:51+00:00
- **Updated**: 2017-10-11 12:51:11+00:00
- **Authors**: Denis Tome, Chris Russell, Lourdes Agapito
- **Comment**: Paper presented at CVPR 17
- **Journal**: None
- **Summary**: We propose a unified formulation for the problem of 3D human pose estimation from a single raw RGB image that reasons jointly about 2D joint estimation and 3D pose reconstruction to improve both tasks. We take an integrated approach that fuses probabilistic knowledge of 3D human pose with a multi-stage CNN architecture and uses the knowledge of plausible 3D landmark locations to refine the search for better 2D locations. The entire process is trained end-to-end, is extremely efficient and obtains state- of-the-art results on Human3.6M outperforming previous approaches both on 2D and 3D errors.



