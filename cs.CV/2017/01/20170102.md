# Arxiv Papers in cs.CV on 2017-01-02
### Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices
- **Arxiv ID**: http://arxiv.org/abs/1701.00485v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1701.00485v2)
- **Published**: 2017-01-02 04:28:16+00:00
- **Updated**: 2017-01-04 13:54:51+00:00
- **Authors**: Wenjia Meng, Zonghua Gu, Ming Zhang, Zhaohui Wu
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid proliferation of Internet of Things and intelligent edge devices, there is an increasing need for implementing machine learning algorithms, including deep learning, on resource-constrained mobile embedded devices with limited memory and computation power. Typical large Convolutional Neural Networks (CNNs) need large amounts of memory and computational power, and cannot be deployed on embedded devices efficiently. We present Two-Bit Networks (TBNs) for model compression of CNNs with edge weights constrained to (-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the memory usage and improve computational efficiency significantly while achieving good performance in terms of classification accuracy, thus representing a reasonable tradeoff between model size and performance.



### Challenges ahead Electron Microscopy for Structural Biology from the Image Processing point of view
- **Arxiv ID**: http://arxiv.org/abs/1701.00326v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1701.00326v1)
- **Published**: 2017-01-02 07:34:38+00:00
- **Updated**: 2017-01-02 07:34:38+00:00
- **Authors**: Carlos Oscar S. Sorzano, Jose Maria Carazo
- **Comment**: None
- **Journal**: None
- **Summary**: Since the introduction of Direct Electron Detectors (DEDs), the resolution and range of macromolecules amenable to this technique has significantly widened, generating a broad interest that explains the well over a dozen reviews in top journal in the last two years. Similarly, the number of job offers to lead EM groups and/or coordinate EM facilities has exploded, and FEI (the main microscope manufacturer for Life Sciences) has received more than 100 orders of high-end electron microscopes by summer 2016. Strategic corporate movements are also happening, with very big players entering the market through key acquisitions (Thermo Fisher has recently bought FEI for \$4.2B), partly attracted by new Pharma interest in the field, now perceived to be in a position to impact structure-based drug design. The scientific perspectives are indeed extremely positive but, in these moments of well-founded generalized optimists, we want to make a reflection on some of the hurdles ahead us, since they certainly exist and they indeed limit the informational content of cryoEM projects. Here we focus on image processing aspects, particularly in the so-called area of Single Particle Analysis, discussing some of the current resolution and high-throughput limiting factors.



### Assessing Uncertainties in X-ray Single-particle Three-dimensional reconstructions
- **Arxiv ID**: http://arxiv.org/abs/1701.00338v1
- **DOI**: 10.1103/PhysRevE.98.013303
- **Categories**: **stat.ME**, cs.CV, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1701.00338v1)
- **Published**: 2017-01-02 08:58:19+00:00
- **Updated**: 2017-01-02 08:58:19+00:00
- **Authors**: Stefan Engblom, Carl Nettelblad, Jing Liu
- **Comment**: 21 pages
- **Journal**: Phys. Rev. E 98, 013303 (2018)
- **Summary**: Modern technology for producing extremely bright and coherent X-ray laser pulses provides the possibility to acquire a large number of diffraction patterns from individual biological nanoparticles, including proteins, viruses, and DNA. These two-dimensional diffraction patterns can be practically reconstructed and retrieved down to a resolution of a few \angstrom. In principle, a sufficiently large collection of diffraction patterns will contain the required information for a full three-dimensional reconstruction of the biomolecule. The computational methodology for this reconstruction task is still under development and highly resolved reconstructions have not yet been produced.   We analyze the Expansion-Maximization-Compression scheme, the current state of the art approach for this very challenging application, by isolating different sources of uncertainty. Through numerical experiments on synthetic data we evaluate their respective impact. We reach conclusions of relevance for handling actual experimental data, as well as pointing out certain improvements to the underlying estimation algorithm.   We also introduce a practically applicable computational methodology in the form of bootstrap procedures for assessing reconstruction uncertainty in the real data case. We evaluate the sharpness of this approach and argue that this type of procedure will be critical in the near future when handling the increasing amount of data.



### Weakly Supervised Semantic Segmentation using Web-Crawled Videos
- **Arxiv ID**: http://arxiv.org/abs/1701.00352v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.00352v3)
- **Published**: 2017-01-02 10:12:03+00:00
- **Updated**: 2018-01-07 11:03:34+00:00
- **Authors**: Seunghoon Hong, Donghun Yeo, Suha Kwak, Honglak Lee, Bohyung Han
- **Comment**: CVPR 2017 (Spotlight)
- **Journal**: None
- **Summary**: We propose a novel algorithm for weakly supervised semantic segmentation based on image-level class labels only. In weakly supervised setting, it is commonly observed that trained model overly focuses on discriminative parts rather than the entire object area. Our goal is to overcome this limitation with no additional human intervention by retrieving videos relevant to target class labels from web repository, and generating segmentation labels from the retrieved videos to simulate strong supervision for semantic segmentation. During this process, we take advantage of image classification with discriminative localization technique to reject false alarms in retrieved videos and identify relevant spatio-temporal volumes within retrieved videos. Although the entire procedure does not require any additional supervision, the segmentation annotations obtained from videos are sufficiently strong to learn a model for semantic segmentation. The proposed algorithm substantially outperforms existing methods based on the same level of supervision and is even as competitive as the approaches relying on extra annotations.



### Adversarially Tuned Scene Generation
- **Arxiv ID**: http://arxiv.org/abs/1701.00405v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.00405v2)
- **Published**: 2017-01-02 14:36:20+00:00
- **Updated**: 2017-07-07 14:28:40+00:00
- **Authors**: V S R Veeravasarapu, Constantin Rothkopf, Ramesh Visvanathan
- **Comment**: 9 pages, accepted at CVPR 2017
- **Journal**: None
- **Summary**: Generalization performance of trained computer vision systems that use computer graphics (CG) generated data is not yet effective due to the concept of 'domain-shift' between virtual and real data. Although simulated data augmented with a few real world samples has been shown to mitigate domain shift and improve transferability of trained models, guiding or bootstrapping the virtual data generation with the distributions learnt from target real world domain is desired, especially in the fields where annotating even few real images is laborious (such as semantic labeling, and intrinsic images etc.). In order to address this problem in an unsupervised manner, our work combines recent advances in CG (which aims to generate stochastic scene layouts coupled with large collections of 3D object models) and generative adversarial training (which aims train generative models by measuring discrepancy between generated and real data in terms of their separability in the space of a deep discriminatively-trained classifier). Our method uses iterative estimation of the posterior density of prior distributions for a generative graphical model. This is done within a rejection sampling framework. Initially, we assume uniform distributions as priors on the parameters of a scene described by a generative graphical model. As iterations proceed the prior distributions get updated to distributions that are closer to the (unknown) distributions of target data. We demonstrate the utility of adversarially tuned scene generation on two real-world benchmark datasets (CityScapes and CamVid) for traffic scene semantic labeling with a deep convolutional net (DeepLab). We realized performance improvements by 2.28 and 3.14 points (using the IoU metric) between the DeepLab models trained on simulated sets prepared from the scene generation models before and after tuning to CityScapes and CamVid respectively.



### Retrieving Similar X-Ray Images from Big Image Data Using Radon Barcodes with Single Projections
- **Arxiv ID**: http://arxiv.org/abs/1701.00449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.00449v1)
- **Published**: 2017-01-02 17:00:53+00:00
- **Updated**: 2017-01-02 17:00:53+00:00
- **Authors**: Morteza Babaie, H. R. Tizhoosh, Shujin Zhu, M. E. Shiri
- **Comment**: Accepted for publication in ICPRAM 2017: The International Conference
  on Pattern Recognition Applications and Methods, Porto, Portugal, 2017
- **Journal**: None
- **Summary**: The idea of Radon barcodes (RBC) has been introduced recently. In this paper, we propose a content-based image retrieval approach for big datasets based on Radon barcodes. Our method (Single Projection Radon Barcode, or SP-RBC) uses only a few Radon single projections for each image as global features that can serve as a basis for weak learners. This is our most important contribution in this work, which improves the results of the RBC considerably. As a matter of fact, only one projection of an image, as short as a single SURF feature vector, can already achieve acceptable results. Nevertheless, using multiple projections in a long vector will not deliver anticipated improvements. To exploit the information inherent in each projection, our method uses the outcome of each projection separately and then applies more precise local search on the small subset of retrieved images. We have tested our method using IRMA 2009 dataset a with 14,400 x-ray images as part of imageCLEF initiative. Our approach leads to a substantial decrease in the error rate in comparison with other non-learning methods.



### Deep-HiTS: Rotation Invariant Convolutional Neural Network for Transient Detection
- **Arxiv ID**: http://arxiv.org/abs/1701.00458v1
- **DOI**: 10.3847/1538-4357/836/1/97
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1701.00458v1)
- **Published**: 2017-01-02 17:18:09+00:00
- **Updated**: 2017-01-02 17:18:09+00:00
- **Authors**: Guillermo Cabrera-Vives, Ignacio Reyes, Francisco Förster, Pablo A. Estévez, Juan-Carlos Maureira
- **Comment**: None
- **Journal**: The Astrophysical Journal, 2017
- **Summary**: We introduce Deep-HiTS, a rotation invariant convolutional neural network (CNN) model for classifying images of transients candidates into artifacts or real sources for the High cadence Transient Survey (HiTS). CNNs have the advantage of learning the features automatically from the data while achieving high performance. We compare our CNN model against a feature engineering approach using random forests (RF). We show that our CNN significantly outperforms the RF model reducing the error by almost half. Furthermore, for a fixed number of approximately 2,000 allowed false transient candidates per night we are able to reduce the miss-classified real transients by approximately 1/5. To the best of our knowledge, this is the first time CNNs have been used to detect astronomical transient events. Our approach will be very useful when processing images from next generation instruments such as the Large Synoptic Survey Telescope (LSST). We have made all our code and data available to the community for the sake of allowing further developments and comparisons at https://github.com/guille-c/Deep-HiTS.



### Vid2speech: Speech Reconstruction from Silent Video
- **Arxiv ID**: http://arxiv.org/abs/1701.00495v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1701.00495v2)
- **Published**: 2017-01-02 19:00:22+00:00
- **Updated**: 2017-01-09 17:35:17+00:00
- **Authors**: Ariel Ephrat, Shmuel Peleg
- **Comment**: Accepted for publication at ICASSP 2017
- **Journal**: None
- **Summary**: Speechreading is a notoriously difficult task for humans to perform. In this paper we present an end-to-end model based on a convolutional neural network (CNN) for generating an intelligible acoustic speech signal from silent video frames of a speaking person. The proposed CNN generates sound features for each frame based on its neighboring frames. Waveforms are then synthesized from the learned speech features to produce intelligible speech. We show that by leveraging the automatic feature learning capabilities of a CNN, we can obtain state-of-the-art word intelligibility on the GRID dataset, and show promising results for learning out-of-vocabulary (OOV) words.



