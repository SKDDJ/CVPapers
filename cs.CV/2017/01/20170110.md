# Arxiv Papers in cs.CV on 2017-01-10
### Scene Graph Generation by Iterative Message Passing
- **Arxiv ID**: http://arxiv.org/abs/1701.02426v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02426v2)
- **Published**: 2017-01-10 03:06:58+00:00
- **Updated**: 2017-04-12 04:11:32+00:00
- **Authors**: Danfei Xu, Yuke Zhu, Christopher B. Choy, Li Fei-Fei
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Understanding a visual scene goes beyond recognizing individual objects in isolation. Relationships between objects also constitute rich semantic information about the scene. In this work, we explicitly model the objects and their relationships using scene graphs, a visually-grounded graphical structure of an image. We propose a novel end-to-end model that generates such structured scene representation from an input image. The model solves the scene graph inference problem using standard RNNs and learns to iteratively improves its predictions via message passing. Our joint inference model can take advantage of contextual cues to make better predictions on objects and their relationships. The experiments show that our model significantly outperforms previous methods for generating scene graphs using Visual Genome dataset and inferring support relations with NYU Depth v2 dataset.



### Unite the People: Closing the Loop Between 3D and 2D Human Representations
- **Arxiv ID**: http://arxiv.org/abs/1701.02468v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02468v3)
- **Published**: 2017-01-10 08:14:58+00:00
- **Updated**: 2017-07-25 01:58:38+00:00
- **Authors**: Christoph Lassner, Javier Romero, Martin Kiefel, Federica Bogo, Michael J. Black, Peter V. Gehler
- **Comment**: None
- **Journal**: None
- **Summary**: 3D models provide a common ground for different representations of human bodies. In turn, robust 2D estimation has proven to be a powerful tool to obtain 3D fits "in-the- wild". However, depending on the level of detail, it can be hard to impossible to acquire labeled data for training 2D estimators on large scale. We propose a hybrid approach to this problem: with an extended version of the recently introduced SMPLify method, we obtain high quality 3D body model fits for multiple human pose datasets. Human annotators solely sort good and bad fits. This procedure leads to an initial dataset, UP-3D, with rich annotations. With a comprehensive set of experiments, we show how this data can be used to train discriminative models that produce results with an unprecedented level of detail: our models predict 31 segments and 91 landmark locations on the body. Using the 91 landmark pose estimator, we present state-of-the art results for 3D human pose and shape estimation using an order of magnitude less training data and without assumptions about gender or pose in the fitting procedure. We show that UP-3D can be enhanced with these improved fits to grow in quantity and quality, which makes the system deployable on large scale. The data, code and models are available for research purposes.



### Methods for Mapping Forest Disturbance and Degradation from Optical Earth Observation Data: a Review
- **Arxiv ID**: http://arxiv.org/abs/1701.02470v4
- **DOI**: 10.1007/s40725-017-0047-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02470v4)
- **Published**: 2017-01-10 08:32:04+00:00
- **Updated**: 2017-03-22 10:17:06+00:00
- **Authors**: Manuela Hirschmugl, Heinz Gallaun, Matthias Dees, Pawan Datta, Janik Deutscher, Nikos Koutsias, Mathias Schardt
- **Comment**: This is the Authors' accepted version only! The final version of this
  paper can be located at Springer.com as part of the Current Forestry Reports
  (2017) 3: 32. doi:10.1007/s40725-017-0047-2
- **Journal**: Current Forestry Reports 2017
- **Summary**: Purpose of review: This paper presents a review of the current state of the art in remote sensing based monitoring of forest disturbances and forest degradation from optical Earth Observation data. Part one comprises an overview of currently available optical remote sensing sensors, which can be used for forest disturbance and degradation mapping. Part two reviews the two main categories of existing approaches: classical image-to-image change detection and time series analysis. Recent findings: With the launch of the Sentinel-2a satellite and available Landsat imagery, time series analysis has become the most promising but also most demanding category of degradation mapping approaches. Four time series classification methods are distinguished. The methods are explained and their benefits and drawbacks are discussed. A separate chapter presents a number of recent forest degradation mapping studies for two different ecosystems: temperate forests with a geographical focus on Europe and tropical forests with a geographical focus on Africa. Summary: The review revealed that a wide variety of methods for the detection of forest degradation is already available. Today, the main challenge is to transfer these approaches to high resolution time series data from multiple sensors. Future research should also focus on the classification of disturbance types and the development of robust up-scalable methods to enable near real time disturbance mapping in support of operational reactive measures.



### Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/1701.02477v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1701.02477v1)
- **Published**: 2017-01-10 08:47:56+00:00
- **Updated**: 2017-01-10 08:47:56+00:00
- **Authors**: Abhinav Thanda, Shankar M Venkatesan
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-task learning (MTL) involves the simultaneous training of two or more related tasks over shared representations. In this work, we apply MTL to audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic GMM/HMM model. This is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual GMM/HMM model. The MTL model is tested at various levels of babble noise and the results are compared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate that MTL is especially useful at higher level of noise. Compared to base-line, upto 7\% relative improvement in WER is reported at -3 SNR dB



### Efficient Image Set Classification using Linear Regression based Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1701.02485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02485v1)
- **Published**: 2017-01-10 09:17:29+00:00
- **Updated**: 2017-01-10 09:17:29+00:00
- **Authors**: Syed Afaq Ali Shah, Uzair Nadeem, Mohammed Bennamoun, Ferdous Sohel, Roberto Togneri
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel image set classification technique using linear regression models. Downsampled gallery image sets are interpreted as subspaces of a high dimensional space to avoid the computationally expensive training step. We estimate regression models for each test image using the class specific gallery subspaces. Images of the test set are then reconstructed using the regression models. Based on the minimum reconstruction error between the reconstructed and the original images, a weighted voting strategy is used to classify the test set. We performed extensive evaluation on the benchmark UCSD/Honda, CMU Mobo and YouTube Celebrity datasets for face classification, and ETH-80 dataset for object classification. The results demonstrate that by using only a small amount of training data, our technique achieved competitive classification accuracy and superior computational speed compared with the state-of-the-art methods.



### Reconstructing Subject-Specific Effect Maps
- **Arxiv ID**: http://arxiv.org/abs/1701.02610v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02610v3)
- **Published**: 2017-01-10 14:25:20+00:00
- **Updated**: 2018-07-17 10:18:58+00:00
- **Authors**: Ender Konukoglu, Ben Glocker
- **Comment**: 29 pages, 16 figures, will appear in Neuroimage
- **Journal**: None
- **Summary**: Predictive models allow subject-specific inference when analyzing disease related alterations in neuroimaging data. Given a subject's data, inference can be made at two levels: global, i.e. identifiying condition presence for the subject, and local, i.e. detecting condition effect on each individual measurement extracted from the subject's data. While global inference is widely used, local inference, which can be used to form subject-specific effect maps, is rarely used because existing models often yield noisy detections composed of dispersed isolated islands. In this article, we propose a reconstruction method, named RSM, to improve subject-specific detections of predictive modeling approaches and in particular, binary classifiers. RSM specifically aims to reduce noise due to sampling error associated with using a finite sample of examples to train classifiers. The proposed method is a wrapper-type algorithm that can be used with different binary classifiers in a diagnostic manner, i.e. without information on condition presence. Reconstruction is posed as a Maximum-A-Posteriori problem with a prior model whose parameters are estimated from training data in a classifier-specific fashion. Experimental evaluation is performed on synthetically generated data and data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on synthetic data demonstrate that using RSM yields higher detection accuracy compared to using models directly or with bootstrap averaging. Analyses on the ADNI dataset show that RSM can also improve correlation between subject-specific detections in cortical thickness data and non-imaging markers of Alzheimer's Disease (AD), such as the Mini Mental State Examination Score and Cerebrospinal Fluid amyloid-$\beta$ levels. Further reliability studies on the longitudinal ADNI dataset show improvement on detection reliability when RSM is used.



### Deep Learning for Logo Recognition
- **Arxiv ID**: http://arxiv.org/abs/1701.02620v2
- **DOI**: 10.1016/j.neucom.2017.03.051
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02620v2)
- **Published**: 2017-01-10 14:51:39+00:00
- **Updated**: 2017-05-03 07:30:48+00:00
- **Authors**: Simone Bianco, Marco Buzzelli, Davide Mazzini, Raimondo Schettini
- **Comment**: Preprint accepted in Neurocomputing
- **Journal**: Neurocomputing 245, 23-30 (2017)
- **Summary**: In this paper we propose a method for logo recognition using deep learning. Our recognition pipeline is composed of a logo region proposal followed by a Convolutional Neural Network (CNN) specifically trained for logo classification, even if they are not precisely localized. Experiments are carried out on the FlickrLogos-32 database, and we evaluate the effect on recognition performance of synthetic versus real data augmentation, and image pre-processing. Moreover, we systematically investigate the benefits of different training choices such as class-balancing, sample-weighting and explicit modeling the background class (i.e. no-logo regions). Experimental results confirm the feasibility of the proposed method, that outperforms the methods in the state of the art.



### Midgar: Detection of people through computer vision in the Internet of Things scenarios to improve the security in Smart Cities, Smart Towns, and Smart Homes
- **Arxiv ID**: http://arxiv.org/abs/1701.02632v3
- **DOI**: 10.1016/j.future.2016.12.033
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02632v3)
- **Published**: 2017-01-10 15:19:14+00:00
- **Updated**: 2017-04-05 06:58:50+00:00
- **Authors**: Cristian González García, Daniel Meana-Llorián, B. Cristina Pelayo G-Bustelo, Juan Manuel Cueva Lovelle, Néstor Garcia-Fernandez
- **Comment**: None
- **Journal**: None
- **Summary**: Could we use Computer Vision in the Internet of Things for using pictures as sensors? This is the principal hypothesis that we want to resolve. Currently, in order to create safety areas, cities, or homes, people use IP cameras. Nevertheless, this system needs people who watch the camera images, watch the recording after something occurred, or watch when the camera notifies them of any movement. These are the disadvantages. Furthermore, there are many Smart Cities and Smart Homes around the world. This is why we thought of using the idea of the Internet of Things to add a way of automating the use of IP cameras. In our case, we propose the analysis of pictures through Computer Vision to detect people in the analysed pictures. With this analysis, we are able to obtain if these pictures contain people and handle the pictures as if they were sensors with two possible states. Notwithstanding, Computer Vision is a very complicated field. This is why we needed a second hypothesis: Could we work with Computer Vision in the Internet of Things with a good accuracy to automate or semi-automate this kind of events? The demonstration of these hypotheses required a testing over our Computer Vision module to check the possibilities that we have to use this module in a possible real environment with a good accuracy. Our proposal, as a possible solution, is the analysis of entire sequence instead of isolated pictures for using pictures as sensors in the Internet of Things.



### ChaLearn Looking at People: A Review of Events and Resources
- **Arxiv ID**: http://arxiv.org/abs/1701.02664v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02664v2)
- **Published**: 2017-01-10 16:21:42+00:00
- **Updated**: 2017-02-15 17:08:31+00:00
- **Authors**: Sergio Escalera, Xavier Baró, Hugo Jair Escalante, Isabelle Guyon
- **Comment**: Paper to appear in proceedings of IJCNN 2017 - IEEE - Associated
  website: http://chalearnlap.cvc.uab.es
- **Journal**: None
- **Summary**: This paper reviews the historic of ChaLearn Looking at People (LAP) events. We started in 2011 (with the release of the first Kinect device) to run challenges related to human action/activity and gesture recognition. Since then we have regularly organized events in a series of competitions covering all aspects of visual analysis of humans. So far we have organized more than 10 international challenges and events in this field. This paper reviews associated events, and introduces the ChaLearn LAP platform where public resources (including code, data and preprints of papers) related to the organized events are available. We also provide a discussion on perspectives of ChaLearn LAP activities.



### Unsupervised Image-to-Image Translation with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1701.02676v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1701.02676v1)
- **Published**: 2017-01-10 16:43:03+00:00
- **Updated**: 2017-01-10 16:43:03+00:00
- **Authors**: Hao Dong, Paarth Neekhara, Chao Wu, Yike Guo
- **Comment**: None
- **Journal**: None
- **Summary**: It's useful to automatically transform an image from its original form to some synthetic form (style, partial contents, etc.), while keeping the original structure or semantics. We define this requirement as the "image-to-image translation" problem, and propose a general approach to achieve it, based on deep convolutional and conditional generative adversarial networks (GANs), which has gained a phenomenal success to learn mapping images from noise input since 2014. In this work, we develop a two step (unsupervised) learning method to translate images between different domains by using unlabeled images without specifying any correspondence between them, so that to avoid the cost of acquiring labeled data. Compared with prior works, we demonstrated the capacity of generality in our model, by which variance of translations can be conduct by a single type of model. Such capability is desirable in applications like bidirectional translation



### What are the visual features underlying human versus machine vision?
- **Arxiv ID**: http://arxiv.org/abs/1701.02704v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02704v2)
- **Published**: 2017-01-10 17:43:32+00:00
- **Updated**: 2017-11-07 19:19:31+00:00
- **Authors**: Drew Linsley, Sven Eberhardt, Tarun Sharma, Pankaj Gupta, Thomas Serre
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Although Deep Convolutional Networks (DCNs) are approaching the accuracy of human observers at object recognition, it is unknown whether they leverage similar visual representations to achieve this performance. To address this, we introduce Clicktionary, a web-based game for identifying visual features used by human observers during object recognition. Importance maps derived from the game are consistent across participants and uncorrelated with image saliency measures. These results suggest that Clicktionary identifies image regions that are meaningful and diagnostic for object recognition but different than those driving eye movements. Surprisingly, Clicktionary importance maps are only weakly correlated with relevance maps derived from DCNs trained for object recognition. Our study demonstrates that the narrowing gap between the object recognition accuracy of human observers and DCNs obscures distinct visual strategies used by each to achieve this performance.



### See the Glass Half Full: Reasoning about Liquid Containers, their Volume and Content
- **Arxiv ID**: http://arxiv.org/abs/1701.02718v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02718v2)
- **Published**: 2017-01-10 18:25:15+00:00
- **Updated**: 2017-09-06 21:42:02+00:00
- **Authors**: Roozbeh Mottaghi, Connor Schenck, Dieter Fox, Ali Farhadi
- **Comment**: None
- **Journal**: None
- **Summary**: Humans have rich understanding of liquid containers and their contents; for example, we can effortlessly pour water from a pitcher to a cup. Doing so requires estimating the volume of the cup, approximating the amount of water in the pitcher, and predicting the behavior of water when we tilt the pitcher. Very little attention in computer vision has been made to liquids and their containers. In this paper, we study liquid containers and their contents, and propose methods to estimate the volume of containers, approximate the amount of liquid in them, and perform comparative volume estimations all from a single RGB image. Furthermore, we show the results of the proposed model for predicting the behavior of liquids inside containers when one tilts the containers. We also introduce a new dataset of Containers Of liQuid contEnt (COQE) that contains more than 5,000 images of 10,000 liquid containers in context labelled with volume, amount of content, bounding box annotation, and corresponding similar 3D CAD models.



### Full-reference image quality assessment-based B-mode ultrasound image similarity measure
- **Arxiv ID**: http://arxiv.org/abs/1701.02797v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.02797v2)
- **Published**: 2017-01-10 21:54:02+00:00
- **Updated**: 2017-01-17 20:45:49+00:00
- **Authors**: Kele Xu, Xi Liu, Hengxing Cai, Zhifeng Gao
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: During the last decades, the number of new full-reference image quality assessment algorithms has been increasing drastically. Yet, despite of the remarkable progress that has been made, the medical ultrasound image similarity measurement remains largely unsolved due to a high level of speckle noise contamination. Potential applications of the ultrasound image similarity measurement seem evident in several aspects. To name a few, ultrasound imaging quality assessment, abnormal function region detection, etc. In this paper, a comparative study was made on full-reference image quality assessment methods for ultrasound image visual structural similarity measure. Moreover, based on the image similarity index, a generic ultrasound motion tracking re-initialization framework is given in this work. The experiments are conducted on synthetic data and real-ultrasound liver data and the results demonstrate that, with proposed similarity-based tracking re-initialization, the mean error of landmarks tracking can be decreased from 2 mm to about 1.5 mm in the ultrasound liver sequence.



