# Arxiv Papers in cs.CV on 2017-01-06
### Abnormal Event Detection in Videos using Spatiotemporal Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1701.01546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01546v1)
- **Published**: 2017-01-06 05:25:50+00:00
- **Updated**: 2017-01-06 05:25:50+00:00
- **Authors**: Yong Shean Chong, Yong Haur Tay
- **Comment**: None
- **Journal**: None
- **Summary**: We present an efficient method for detecting anomalies in videos. Recent applications of convolutional neural networks have shown promises of convolutional layers for object detection and recognition, especially in images. However, convolutional neural networks are supervised and require labels as learning signals. We propose a spatiotemporal architecture for anomaly detection in videos including crowded scenes. Our architecture includes two main components, one for spatial feature representation, and one for learning the temporal evolution of the spatial features. Experimental results on Avenue, Subway and UCSD benchmarks confirm that the detection accuracy of our method is comparable to state-of-the-art methods at a considerable speed of up to 140 fps.



### Distinguishing Posed and Spontaneous Smiles by Facial Dynamics
- **Arxiv ID**: http://arxiv.org/abs/1701.01573v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01573v3)
- **Published**: 2017-01-06 08:41:01+00:00
- **Updated**: 2017-02-17 06:00:13+00:00
- **Authors**: Bappaditya Mandal, David Lee, Nizar Ouarti
- **Comment**: 16 pages, 8 figures, ACCV 2016, Second Workshop on Spontaneous Facial
  Behavior Analysis
- **Journal**: None
- **Summary**: Smile is one of the key elements in identifying emotions and present state of mind of an individual. In this work, we propose a cluster of approaches to classify posed and spontaneous smiles using deep convolutional neural network (CNN) face features, local phase quantization (LPQ), dense optical flow and histogram of gradient (HOG). Eulerian Video Magnification (EVM) is used for micro-expression smile amplification along with three normalization procedures for distinguishing posed and spontaneous smiles. Although the deep CNN face model is trained with large number of face images, HOG features outperforms this model for overall face smile classification task. Using EVM to amplify micro-expressions did not have a significant impact on classification accuracy, while the normalizing facial features improved classification accuracy. Unlike many manual or semi-automatic methodologies, our approach aims to automatically classify all smiles into either `spontaneous' or `posed' categories, by using support vector machines (SVM). Experimental results on large UvA-NEMO smile database show promising results as compared to other relevant methods.



### Learning From Noisy Large-Scale Datasets With Minimal Supervision
- **Arxiv ID**: http://arxiv.org/abs/1701.01619v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01619v2)
- **Published**: 2017-01-06 12:38:57+00:00
- **Updated**: 2017-04-10 01:25:42+00:00
- **Authors**: Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, Serge Belongie
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: We present an approach to effectively use millions of images with noisy annotations in conjunction with a small subset of cleanly-annotated images to learn powerful image representations. One common approach to combine clean and noisy data is to first pre-train a network using the large noisy dataset and then fine-tune with the clean dataset. We show this approach does not fully leverage the information contained in the clean set. Thus, we demonstrate how to use the clean annotations to reduce the noise in the large dataset before fine-tuning the network using both the clean set and the full set with reduced noise. The approach comprises a multi-task network that jointly learns to clean noisy annotations and to accurately classify images. We evaluate our approach on the recently released Open Images dataset, containing ~9 million images, multiple annotations per image and over 6000 unique classes. For the small clean set of annotations we use a quarter of the validation set with ~40k images. Our results demonstrate that the proposed approach clearly outperforms direct fine-tuning across all major categories of classes in the Open Image dataset. Further, our approach is particularly effective for a large number of classes with wide range of noise in annotations (20-80% false positive annotations).



### Deep Convolutional Denoising of Low-Light Images
- **Arxiv ID**: http://arxiv.org/abs/1701.01687v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01687v1)
- **Published**: 2017-01-06 16:35:54+00:00
- **Updated**: 2017-01-06 16:35:54+00:00
- **Authors**: Tal Remez, Or Litany, Raja Giryes, Alex M. Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: Poisson distribution is used for modeling noise in photon-limited imaging. While canonical examples include relatively exotic types of sensing like spectral imaging or astronomy, the problem is relevant to regular photography now more than ever due to the booming market for mobile cameras. Restricted form factor limits the amount of absorbed light, thus computational post-processing is called for. In this paper, we make use of the powerful framework of deep convolutional neural networks for Poisson denoising. We demonstrate how by training the same network with images having a specific peak value, our denoiser outperforms previous state-of-the-art by a large margin both visually and quantitatively. Being flexible and data-driven, our solution resolves the heavy ad hoc engineering used in previous methods and is an order of magnitude faster. We further show that by adding a reasonable prior on the class of the image being processed, another significant boost in performance is achieved.



### To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1701.01692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01692v1)
- **Published**: 2017-01-06 16:51:32+00:00
- **Updated**: 2017-01-06 16:51:32+00:00
- **Authors**: Eshed Ohn-Bar, Mohan M. Trivedi
- **Comment**: ICPR, December 2016. Added WIDER FACE test results (Fig. 5)
- **Journal**: None
- **Summary**: We aim to study the modeling limitations of the commonly employed boosted decision trees classifier. Inspired by the success of large, data-hungry visual recognition models (e.g. deep convolutional neural networks), this paper focuses on the relationship between modeling capacity of the weak learners, dataset size, and dataset properties. A set of novel experiments on the Caltech Pedestrian Detection benchmark results in the best known performance among non-CNN techniques while operating at fast run-time speed. Furthermore, the performance is on par with deep architectures (9.71% log-average miss rate), while using only HOG+LUV channels as features. The conclusions from this study are shown to generalize over different object detection domains as demonstrated on the FDDB face detection benchmark (93.37% accuracy). Despite the impressive performance, this study reveals the limited modeling capacity of the common boosted trees model, motivating a need for architectural changes in order to compete with multi-level and very deep architectures.



### Deep Class Aware Denoising
- **Arxiv ID**: http://arxiv.org/abs/1701.01698v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01698v2)
- **Published**: 2017-01-06 17:13:42+00:00
- **Updated**: 2017-02-27 20:25:59+00:00
- **Authors**: Tal Remez, Or Litany, Raja Giryes, Alex M. Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: The increasing demand for high image quality in mobile devices brings forth the need for better computational enhancement techniques, and image denoising in particular. At the same time, the images captured by these devices can be categorized into a small set of semantic classes. However simple, this observation has not been exploited in image denoising until now. In this paper, we demonstrate how the reconstruction quality improves when a denoiser is aware of the type of content in the image. To this end, we first propose a new fully convolutional deep neural network architecture which is simple yet powerful as it achieves state-of-the-art performance even without being class-aware. We further show that a significant boost in performance of up to $0.4$ dB PSNR can be achieved by making our network class-aware, namely, by fine-tuning it for images belonging to a specific semantic class. Relying on the hugely successful existing image classifiers, this research advocates for using a class-aware approach in all image enhancement tasks.



### Map-guided Hyperspectral Image Superpixel Segmentation Using Proportion Maps
- **Arxiv ID**: http://arxiv.org/abs/1701.01745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01745v1)
- **Published**: 2017-01-06 19:52:10+00:00
- **Updated**: 2017-01-06 19:52:10+00:00
- **Authors**: Hao Sun, Alina Zare
- **Comment**: None
- **Journal**: None
- **Summary**: A map-guided superpixel segmentation method for hyperspectral imagery is developed and introduced. The proposed approach develops a hyperspectral-appropriate version of the SLIC superpixel segmentation algorithm, leverages map information to guide segmentation, and incorporates the semi-supervised Partial Membership Latent Dirichlet Allocation (sPM-LDA) to obtain a final superpixel segmentation. The proposed method is applied to two real hyperspectral data sets and quantitative cluster validity metrics indicate that the proposed approach outperforms existing hyperspectral superpixel segmentation methods.



### Towards Accurate Multi-person Pose Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1701.01779v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01779v2)
- **Published**: 2017-01-06 23:56:02+00:00
- **Updated**: 2017-04-14 18:30:58+00:00
- **Authors**: George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chris Bregler, Kevin Murphy
- **Comment**: Paper describing an improved version of the G-RMI entry to the 2016
  COCO keypoints challenge (http://image-net.org/challenges/ilsvrc+coco2016).
  Camera ready version to appear in the Proceedings of CVPR 2017
- **Journal**: None
- **Summary**: We propose a method for multi-person detection and 2-D pose estimation that achieves state-of-art results on the challenging COCO keypoints task. It is a simple, yet powerful, top-down approach consisting of two stages.   In the first stage, we predict the location and scale of boxes which are likely to contain people; for this we use the Faster RCNN detector. In the second stage, we estimate the keypoints of the person potentially contained in each proposed bounding box. For each keypoint type we predict dense heatmaps and offsets using a fully convolutional ResNet. To combine these outputs we introduce a novel aggregation procedure to obtain highly localized keypoint predictions. We also use a novel form of keypoint-based Non-Maximum-Suppression (NMS), instead of the cruder box-level NMS, and a novel form of keypoint-based confidence score estimation, instead of box-level scoring.   Trained on COCO data alone, our final system achieves average precision of 0.649 on the COCO test-dev set and the 0.643 test-standard sets, outperforming the winner of the 2016 COCO keypoints challenge and other recent state-of-art. Further, by using additional in-house labeled data we obtain an even higher average precision of 0.685 on the test-dev set and 0.673 on the test-standard set, more than 5% absolute improvement compared to the previous best performing method on the same dataset.



