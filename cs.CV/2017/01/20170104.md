# Arxiv Papers in cs.CV on 2017-01-04
### A Hierarchical Image Matting Model for Blood Vessel Segmentation in Fundus images
- **Arxiv ID**: http://arxiv.org/abs/1701.00892v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.00892v3)
- **Published**: 2017-01-04 03:24:29+00:00
- **Updated**: 2017-10-09 14:52:55+00:00
- **Authors**: Zhun Fan, Jiewei Lu, Wenji Li, Caimin Wei, Han Huang, Xinye Cai, Xinjian Chen
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: In this paper, a hierarchical image matting model is proposed to extract blood vessels from fundus images. More specifically, a hierarchical strategy utilizing the continuity and extendibility of retinal blood vessels is integrated into the image matting model for blood vessel segmentation. Normally the matting models require the user specified trimap, which separates the input image into three regions manually: the foreground, background and unknown regions. However, since creating a user specified trimap is a tedious and time-consuming task, region features of blood vessels are used to generate the trimap automatically in this paper. The proposed model has low computational complexity and outperforms many other state-ofart supervised and unsupervised methods in terms of accuracy, which achieves a vessel segmentation accuracy of 96:0%, 95:7% and 95:1% in an average time of 10:72s, 15:74s and 50:71s on images from three publicly available fundus image datasets DRIVE, STARE, and CHASE DB1, respectively.



### Dense Associative Memory is Robust to Adversarial Inputs
- **Arxiv ID**: http://arxiv.org/abs/1701.00939v1
- **DOI**: 10.1162/neco_a_01143
- **Categories**: **cs.LG**, cs.CR, cs.CV, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1701.00939v1)
- **Published**: 2017-01-04 09:40:09+00:00
- **Updated**: 2017-01-04 09:40:09+00:00
- **Authors**: Dmitry Krotov, John J Hopfield
- **Comment**: None
- **Journal**: Neural Computation Volume 30, Issue 12, December 2018 p.3151-3167
- **Summary**: Deep neural networks (DNN) trained in a supervised way suffer from two known problems. First, the minima of the objective function used in learning correspond to data points (also known as rubbish examples or fooling images) that lack semantic similarity with the training data. Second, a clean input can be changed by a small, and often imperceptible for human vision, perturbation, so that the resulting deformed input is misclassified by the network. These findings emphasize the differences between the ways DNN and humans classify patterns, and raise a question of designing learning algorithms that more accurately mimic human perception compared to the existing methods.   Our paper examines these questions within the framework of Dense Associative Memory (DAM) models. These models are defined by the energy function, with higher order (higher than quadratic) interactions between the neurons. We show that in the limit when the power of the interaction vertex in the energy function is sufficiently large, these models have the following three properties. First, the minima of the objective function are free from rubbish images, so that each minimum is a semantically meaningful pattern. Second, artificial patterns poised precisely at the decision boundary look ambiguous to human subjects and share aspects of both classes that are separated by that decision boundary. Third, adversarial images constructed by models with small power of the interaction vertex, which are equivalent to DNN with rectified linear units (ReLU), fail to transfer to and fool the models with higher order interactions. This opens up a possibility to use higher order models for detecting and stopping malicious adversarial attacks. The presented results suggest that DAM with higher order energy functions are closer to human visual perception than DNN with ReLUs.



### A Concave Optimization Algorithm for Matching Partially Overlapping Point Sets
- **Arxiv ID**: http://arxiv.org/abs/1701.00951v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.00951v1)
- **Published**: 2017-01-04 10:28:10+00:00
- **Updated**: 2017-01-04 10:28:10+00:00
- **Authors**: Wei Lian, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Point matching refers to the process of finding spatial transformation and correspondences between two sets of points. In this paper, we focus on the case that there is only partial overlap between two point sets. Following the approach of the robust point matching method, we model point matching as a mixed linear assignment-least square problem and show that after eliminating the transformation variable, the resulting problem of minimization with respect to point correspondence is a concave optimization problem. Furthermore, this problem has the property that the objective function can be converted into a form with few nonlinear terms via a linear transformation. Based on these properties, we employ the branch-and-bound (BnB) algorithm to optimize the resulting problem where the dimension of the search space is small. To further improve efficiency of the BnB algorithm where computation of the lower bound is the bottleneck, we propose a new lower bounding scheme which has a k-cardinality linear assignment formulation and can be efficiently solved. Experimental results show that the proposed algorithm outperforms state-of-the-art methods in terms of robustness to disturbances and point matching accuracy.



### An Evaluation Framework and Database for MoCap-Based Gait Recognition Methods
- **Arxiv ID**: http://arxiv.org/abs/1701.00995v4
- **DOI**: 10.1007/978-3-319-56414-2_3
- **Categories**: **cs.CV**, 68T05, 68T10, I.5
- **Links**: [PDF](http://arxiv.org/pdf/1701.00995v4)
- **Published**: 2017-01-04 13:17:49+00:00
- **Updated**: 2022-12-07 22:15:57+00:00
- **Authors**: Michal Balazia, Petr Sojka
- **Comment**: Preprint. Full paper published at the 1st IAPR Workshop on
  Proceedings of Reproducible Research in Pattern Recognition (RRPR), Cancun,
  Mexico, December 2016. 13 pages. arXiv admin note: text overlap with
  arXiv:1609.06936
- **Journal**: None
- **Summary**: As a contribution to reproducible research, this paper presents a framework and a database to improve the development, evaluation and comparison of methods for gait recognition from motion capture (MoCap) data. The evaluation framework provides implementation details and source codes of state-of-the-art human-interpretable geometric features as well as our own approaches where gait features are learned by a modification of Fisher's Linear Discriminant Analysis with the Maximum Margin Criterion, and by a combination of Principal Component Analysis and Linear Discriminant Analysis. It includes a description and source codes of a mechanism for evaluating four class separability coefficients of feature space and four rank-based classifier performance metrics. This framework also contains a tool for learning a custom classifier and for classifying a custom query on a custom gallery. We provide an experimental database along with source codes for its extraction from the general CMU MoCap database.



### Path-following based Point Matching using Similarity Transformation
- **Arxiv ID**: http://arxiv.org/abs/1701.01035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01035v1)
- **Published**: 2017-01-04 14:53:07+00:00
- **Updated**: 2017-01-04 14:53:07+00:00
- **Authors**: Wei Lian
- **Comment**: None
- **Journal**: None
- **Summary**: To address the problem of 3D point matching where the poses of two point sets are unknown, we adapt a recently proposed path following based method to use similarity transformation instead of the original affine transformation. The reduced number of transformation parameters leads to more constrained and desirable matching results. Experimental results demonstrate better robustness of the proposed method over state-of-the-art methods.



### Demystifying Neural Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1701.01036v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1701.01036v2)
- **Published**: 2017-01-04 14:54:20+00:00
- **Updated**: 2017-07-01 13:21:11+00:00
- **Authors**: Yanghao Li, Naiyan Wang, Jiaying Liu, Xiaodi Hou
- **Comment**: Accepted by IJCAI 2017
- **Journal**: None
- **Summary**: Neural Style Transfer has recently demonstrated very exciting results which catches eyes in both academia and industry. Despite the amazing results, the principle of neural style transfer, especially why the Gram matrices could represent style remains unclear. In this paper, we propose a novel interpretation of neural style transfer by treating it as a domain adaptation problem. Specifically, we theoretically show that matching the Gram matrices of feature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with the second order polynomial kernel. Thus, we argue that the essence of neural style transfer is to match the feature distributions between the style images and the generated images. To further support our standpoint, we experiment with several other distribution alignment methods, and achieve appealing results. We believe this novel interpretation connects these two important research fields, and could enlighten future researches.



### Transforming Sensor Data to the Image Domain for Deep Learning - an Application to Footstep Detection
- **Arxiv ID**: http://arxiv.org/abs/1701.01077v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01077v3)
- **Published**: 2017-01-04 17:12:43+00:00
- **Updated**: 2017-07-14 18:07:03+00:00
- **Authors**: Monit Shah Singh, Vinaychandran Pondenkandath, Bo Zhou, Paul Lukowicz, Marcus Liwicki
- **Comment**: 8 pages, 8 figures, Published in IJCNN, 2017 Copyright: IEEE 2017
  DOI: 10.1109/IJCNN.2017.7966182
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have become the state-of-the-art in various computer vision tasks, but they are still premature for most sensor data, especially in pervasive and wearable computing. A major reason for this is the limited amount of annotated training data. In this paper, we propose the idea of leveraging the discriminative power of pre-trained deep CNNs on 2-dimensional sensor data by transforming the sensor modality to the visual domain. By three proposed strategies, 2D sensor output is converted into pressure distribution imageries. Then we utilize a pre-trained CNN for transfer learning on the converted imagery data. We evaluate our method on a gait dataset of floor surface pressure mapping. We obtain a classification accuracy of 87.66%, which outperforms the conventional machine learning methods by over 10%.



### SalGAN: Visual Saliency Prediction with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1701.01081v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01081v3)
- **Published**: 2017-01-04 17:16:54+00:00
- **Updated**: 2018-07-01 18:52:31+00:00
- **Authors**: Junting Pan, Cristian Canton Ferrer, Kevin McGuinness, Noel E. O'Connor, Jordi Torres, Elisa Sayrol, Xavier Giro-i-Nieto
- **Comment**: Submitted for review to Computer Vision and Image Understanding
  (CVIU)
- **Journal**: None
- **Summary**: We introduce SalGAN, a deep convolutional neural network for visual saliency prediction trained with adversarial examples. The first stage of the network consists of a generator model whose weights are learned by back-propagation computed from a binary cross entropy (BCE) loss over downsampled versions of the saliency maps. The resulting prediction is processed by a discriminator network trained to solve a binary classification task between the saliency maps generated by the generative stage and the ground truth ones. Our experiments show how adversarial training allows reaching state-of-the-art performance across different metrics when combined with a widely-used loss function like BCE. Our results can be reproduced with the source code and trained models available at https://imatge-upc.github.io/saliency-salgan-2017/.



