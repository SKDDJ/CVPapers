# Arxiv Papers in cs.CV on 2017-01-19
### Accurate Motion Estimation through Random Sample Aggregated Consensus
- **Arxiv ID**: http://arxiv.org/abs/1701.05268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05268v1)
- **Published**: 2017-01-19 01:13:41+00:00
- **Updated**: 2017-01-19 01:13:41+00:00
- **Authors**: Martin Rais, Gabriele Facciolo, Enric Meinhardt-Llopis, Jean-Michel Morel, Antoni Buades, Bartomeu Coll
- **Comment**: None
- **Journal**: None
- **Summary**: We reconsider the classic problem of estimating accurately a 2D transformation from point matches between images containing outliers. RANSAC discriminates outliers by randomly generating minimalistic sampled hypotheses and verifying their consensus over the input data. Its response is based on the single hypothesis that obtained the largest inlier support. In this article we show that the resulting accuracy can be improved by aggregating all generated hypotheses. This yields RANSAAC, a framework that improves systematically over RANSAC and its state-of-the-art variants by statistically aggregating hypotheses. To this end, we introduce a simple strategy that allows to rapidly average 2D transformations, leading to an almost negligible extra computational cost. We give practical applications on projective transforms and homography+distortion models and demonstrate a significant performance gain in both cases.



### Pixel Objectness
- **Arxiv ID**: http://arxiv.org/abs/1701.05349v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05349v2)
- **Published**: 2017-01-19 09:48:45+00:00
- **Updated**: 2017-04-12 07:36:44+00:00
- **Authors**: Suyog Dutt Jain, Bo Xiong, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an end-to-end learning framework for generating foreground object segmentations. Given a single novel image, our approach produces pixel-level masks for all "object-like" regions---even for object categories never seen during training. We formulate the task as a structured prediction problem of assigning foreground/background labels to all pixels, implemented using a deep fully convolutional network. Key to our idea is training with a mix of image-level object category examples together with relatively few images with boundary-level annotations. Our method substantially improves the state-of-the-art on foreground segmentation for ImageNet and MIT Object Discovery datasets. Furthermore, on over 1 million images, we show that it generalizes well to segment object categories unseen in the foreground maps used for training. Finally, we demonstrate how our approach benefits image retrieval and image retargeting, both of which flourish when given our high-quality foreground maps.



### 3D Face Morphable Models "In-the-Wild"
- **Arxiv ID**: http://arxiv.org/abs/1701.05360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05360v1)
- **Published**: 2017-01-19 10:27:38+00:00
- **Updated**: 2017-01-19 10:27:38+00:00
- **Authors**: James Booth, Epameinondas Antonakos, Stylianos Ploumpis, George Trigeorgis, Yannis Panagakis, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: 3D Morphable Models (3DMMs) are powerful statistical models of 3D facial shape and texture, and among the state-of-the-art methods for reconstructing facial shape from single images. With the advent of new 3D sensors, many 3D facial datasets have been collected containing both neutral as well as expressive faces. However, all datasets are captured under controlled conditions. Thus, even though powerful 3D facial shape models can be learnt from such data, it is difficult to build statistical texture models that are sufficient to reconstruct faces captured in unconstrained conditions ("in-the-wild"). In this paper, we propose the first, to the best of our knowledge, "in-the-wild" 3DMM by combining a powerful statistical model of facial shape, which describes both identity and expression, with an "in-the-wild" texture model. We show that the employment of such an "in-the-wild" texture model greatly simplifies the fitting procedure, because there is no need to optimize with regards to the illumination parameters. Furthermore, we propose a new fast algorithm for fitting the 3DMM in arbitrary images. Finally, we have captured the first 3D facial database with relatively unconstrained conditions and report quantitative evaluations with state-of-the-art performance. Complementary qualitative reconstruction results are demonstrated on standard "in-the-wild" facial databases. An open source implementation of our technique is released as part of the Menpo Project.



### Profiling of OCR'ed Historical Texts Revisited
- **Arxiv ID**: http://arxiv.org/abs/1701.05377v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DL
- **Links**: [PDF](http://arxiv.org/pdf/1701.05377v1)
- **Published**: 2017-01-19 11:31:14+00:00
- **Updated**: 2017-01-19 11:31:14+00:00
- **Authors**: Florian Fink, Klaus-U. Schulz, Uwe Springmann
- **Comment**: None
- **Journal**: None
- **Summary**: In the absence of ground truth it is not possible to automatically determine the exact spectrum and occurrences of OCR errors in an OCR'ed text. Yet, for interactive postcorrection of OCR'ed historical printings it is extremely useful to have a statistical profile available that provides an estimate of error classes with associated frequencies, and that points to conjectured errors and suspicious tokens. The method introduced in Reffle (2013) computes such a profile, combining lexica, pattern sets and advanced matching techniques in a specialized Expectation Maximization (EM) procedure. Here we improve this method in three respects: First, the method in Reffle (2013) is not adaptive: user feedback obtained by actual postcorrection steps cannot be used to compute refined profiles. We introduce a variant of the method that is open for adaptivity, taking correction steps of the user into account. This leads to higher precision with respect to recognition of erroneous OCR tokens. Second, during postcorrection often new historical patterns are found. We show that adding new historical patterns to the linguistic background resources leads to a second kind of improvement, enabling even higher precision by telling historical spellings apart from OCR errors. Third, the method in Reffle (2013) does not make any active use of tokens that cannot be interpreted in the underlying channel model. We show that adding these uninterpretable tokens to the set of conjectured errors leads to a significant improvement of the recall for error detection, at the same time improving precision.



### FusionSeg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos
- **Arxiv ID**: http://arxiv.org/abs/1701.05384v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05384v2)
- **Published**: 2017-01-19 12:16:30+00:00
- **Updated**: 2017-04-12 04:09:46+00:00
- **Authors**: Suyog Dutt Jain, Bo Xiong, Kristen Grauman
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: We propose an end-to-end learning framework for segmenting generic objects in videos. Our method learns to combine appearance and motion information to produce pixel level segmentation masks for all prominent objects in videos. We formulate this task as a structured prediction problem and design a two-stream fully convolutional neural network which fuses together motion and appearance in a unified framework. Since large-scale video datasets with pixel level segmentations are problematic, we show how to bootstrap weakly annotated videos together with existing image recognition datasets for training. Through experiments on three challenging video segmentation benchmarks, our method substantially improves the state-of-the-art for segmenting generic (unseen) objects. Code and pre-trained models are available on the project website.



### Block-wise Lensless Compressive Camera
- **Arxiv ID**: http://arxiv.org/abs/1701.05412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05412v1)
- **Published**: 2017-01-19 13:45:37+00:00
- **Updated**: 2017-01-19 13:45:37+00:00
- **Authors**: Xin Yuan, Gang Huang, Hong Jiang, Paul Wilford
- **Comment**: 5 pages, 10 figures
- **Journal**: None
- **Summary**: The existing lensless compressive camera ($\text{L}^2\text{C}^2$)~\cite{Huang13ICIP} suffers from low capture rates, resulting in low resolution images when acquired over a short time. In this work, we propose a new regime to mitigate these drawbacks. We replace the global-based compressive sensing used in the existing $\text{L}^2\text{C}^2$ by the local block (patch) based compressive sensing. We use a single sensor for each block, rather than for the entire image, thus forming a multiple but spatially parallel sensor $\text{L}^2\text{C}^2$. This new camera retains the advantages of existing $\text{L}^2\text{C}^2$ while leading to the following additional benefits: 1) Since each block can be very small, {\em e.g.}$~8\times 8$ pixels, we only need to capture $\sim 10$ measurements to achieve reasonable reconstruction. Therefore the capture time can be reduced significantly. 2) The coding patterns used in each block can be the same, therefore the sensing matrix is only of the block size compared to the entire image size in existing $\text{L}^2\text{C}^2$. This saves the memory requirement of the sensing matrix as well as speeds up the reconstruction. 3) Patch based image reconstruction is fast and since real time stitching algorithms exist, we can perform real time reconstruction. 4) These small blocks can be integrated to any desirable number, leading to ultra high resolution images while retaining fast capture rate and fast reconstruction. We develop multiple geometries of this block-wise $\text{L}^2\text{C}^2$ in this paper. We have built prototypes of the proposed block-wise $\text{L}^2\text{C}^2$ and demonstrated excellent results of real data.



### Moving to VideoKifu: the last steps toward a fully automatic record-keeping of a Go game
- **Arxiv ID**: http://arxiv.org/abs/1701.05419v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.8; I.5.5
- **Links**: [PDF](http://arxiv.org/pdf/1701.05419v1)
- **Published**: 2017-01-19 14:06:08+00:00
- **Updated**: 2017-01-19 14:06:08+00:00
- **Authors**: Mario Corsolini, Andrea Carta
- **Comment**: 20 pages, 14 figures. Accepted for publication in the "Journal of
  Baduk Studies", datasets available from http://www.oipaz.net/PhotoKifu.html
- **Journal**: Journal of Baduk Studies, 13(2):45-63, December 2016
- **Summary**: In a previous paper [ arXiv:1508.03269 ] we described the techniques we successfully employed for automatically reconstructing the whole move sequence of a Go game by means of a set of pictures. Now we describe how it is possible to reconstruct the move sequence by means of a video stream (which may be provided by an unattended webcam), possibly in real-time. Although the basic algorithms remain the same, we will discuss the new problems that arise when dealing with videos, with special care for the ones that could block a real-time analysis and require an improvement of our previous techniques or even a completely brand new approach. Eventually we present a number of preliminary but positive experimental results supporting the effectiveness of the software we are developing, built on the ideas here outlined.



### Higher-order Pooling of CNN Features via Kernel Linearization for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1701.05432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05432v1)
- **Published**: 2017-01-19 14:30:49+00:00
- **Updated**: 2017-01-19 14:30:49+00:00
- **Authors**: Anoop Cherian, Piotr Koniusz, Stephen Gould
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Most successful deep learning algorithms for action recognition extend models designed for image-based tasks such as object recognition to video. Such extensions are typically trained for actions on single video frames or very short clips, and then their predictions from sliding-windows over the video sequence are pooled for recognizing the action at the sequence level. Usually this pooling step uses the first-order statistics of frame-level action predictions. In this paper, we explore the advantages of using higher-order correlations; specifically, we introduce Higher-order Kernel (HOK) descriptors generated from the late fusion of CNN classifier scores from all the frames in a sequence. To generate these descriptors, we use the idea of kernel linearization. Specifically, a similarity kernel matrix, which captures the temporal evolution of deep classifier scores, is first linearized into kernel feature maps. The HOK descriptors are then generated from the higher-order co-occurrences of these feature maps, and are then used as input to a video-level classifier. We provide experiments on two fine-grained action recognition datasets and show that our scheme leads to state-of-the-art results.



### T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects
- **Arxiv ID**: http://arxiv.org/abs/1701.05498v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1701.05498v1)
- **Published**: 2017-01-19 16:16:36+00:00
- **Updated**: 2017-01-19 16:16:36+00:00
- **Authors**: Tomas Hodan, Pavel Haluza, Stepan Obdrzalek, Jiri Matas, Manolis Lourakis, Xenophon Zabulis
- **Comment**: WACV 2017
- **Journal**: None
- **Summary**: We introduce T-LESS, a new public dataset for estimating the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit symmetries and mutual similarities in shape and/or size. Compared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are provided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training images depict individual objects against a black background. Test images originate from twenty test scenes having varying complexity, which increases from simple scenes with several isolated objects to very challenging ones with multiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a systematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects. Initial evaluation results indicate that the state of the art in 6D object pose estimation has ample room for improvement, especially in difficult cases with significant occlusion. The T-LESS dataset is available online at cmp.felk.cvut.cz/t-less.



### Synthetic to Real Adaptation with Generative Correlation Alignment Networks
- **Arxiv ID**: http://arxiv.org/abs/1701.05524v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05524v3)
- **Published**: 2017-01-19 17:42:00+00:00
- **Updated**: 2017-03-18 12:56:45+00:00
- **Authors**: Xingchao Peng, Kate Saenko
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: Synthetic images rendered from 3D CAD models are useful for augmenting training data for object recognition algorithms. However, the generated images are non-photorealistic and do not match real image statistics. This leads to a large domain discrepancy, causing models trained on synthetic data to perform poorly on real domains. Recent work has shown the great potential of deep convolutional neural networks to generate realistic images, but has not utilized generative models to address synthetic-to-real domain adaptation. In this work, we propose a Deep Generative Correlation Alignment Network (DGCAN) to synthesize images using a novel domain adaption algorithm. DGCAN leverages a shape preserving loss and a low level statistic matching loss to minimize the domain discrepancy between synthetic and real images in deep feature space. Experimentally, we show training off-the-shelf classifiers on the newly generated data can significantly boost performance when testing on the real image domains (PASCAL VOC 2007 benchmark and Office dataset), improving upon several existing methods.



### Deep Neural Networks - A Brief History
- **Arxiv ID**: http://arxiv.org/abs/1701.05549v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1701.05549v1)
- **Published**: 2017-01-19 18:43:56+00:00
- **Updated**: 2017-01-19 18:43:56+00:00
- **Authors**: Krzysztof J. Cios
- **Comment**: 14 pages, 14 figures
- **Journal**: None
- **Summary**: Introduction to deep neural networks and their history.



### High Performance Novel Skin Segmentation Algorithm for Images With Complex Background
- **Arxiv ID**: http://arxiv.org/abs/1701.05588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05588v1)
- **Published**: 2017-01-19 20:22:58+00:00
- **Updated**: 2017-01-19 20:22:58+00:00
- **Authors**: Mohammad Reza Mahmoodi
- **Comment**: None
- **Journal**: None
- **Summary**: Skin Segmentation is widely used in biometric applications such as face detection, face recognition, face tracking, and hand gesture recognition. However, several challenges such as nonlinear illumination, equipment effects, personal interferences, ethnicity variations, etc., are involved in detection process that result in the inefficiency of color based methods. Even though many ideas have already been proposed, the problem has not been satisfactorily solved yet. This paper introduces a technique that addresses some limitations of the previous works. The proposed algorithm consists of three main steps including initial seed generation of skin map, Otsu segmentation in color images, and finally a two-stage diffusion. The initial seed of skin pixels is provided based on the idea of ternary image as there are certain pixels in images which are associated to human complexion with very high probability. The Otsu segmentation is performed on several color channels in order to identify homogeneous regions. The result accompanying with the edge map of the image is utilized in two consecutive diffusion steps in order to annex initially unidentified skin pixels to the seed. Both quantitative and qualitative results demonstrate the effectiveness of the proposed system in compare with the state-of-the-art works.



### Fast and Efficient Skin Detection for Facial Detection
- **Arxiv ID**: http://arxiv.org/abs/1701.05595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05595v1)
- **Published**: 2017-01-19 20:43:27+00:00
- **Updated**: 2017-01-19 20:43:27+00:00
- **Authors**: Mohammad Reza Mahmoodi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, an efficient skin detection system is proposed. The algorithm is based on a very fast efficient pre-processing step utilizing the concept of ternary conversion in order to identify candidate windows and subsequently, a novel local two-stage diffusion method which has F-score accuracy of 0.5978 on SDD dataset. The pre-processing step has been proven to be useful to boost the speed of the system by eliminating 82% of an image in average. This is obtained by keeping the true positive rate above 98%. In addition, a novel segmentation algorithm is also designed to process candidate windows which is quantitatively and qualitatively proven to be very efficient in term of accuracy. The algorithm has been implemented in FPGA to obtain real-time processing speed. The system is designed fully pipeline and the inherent parallel structure of the algorithm is fully exploited to maximize the performance. The system is implemented on a Spartan-6 LXT45 Xilinx FPGA and it is capable of processing 98 frames of 640*480 24-bit color images per second.



### Holistic Interstitial Lung Disease Detection using Deep Convolutional Neural Networks: Multi-label Learning and Unordered Pooling
- **Arxiv ID**: http://arxiv.org/abs/1701.05616v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05616v1)
- **Published**: 2017-01-19 21:52:21+00:00
- **Updated**: 2017-01-19 21:52:21+00:00
- **Authors**: Mingchen Gao, Ziyue Xu, Le Lu, Adam P. Harrison, Ronald M. Summers, Daniel J. Mollura
- **Comment**: None
- **Journal**: None
- **Summary**: Accurately predicting and detecting interstitial lung disease (ILD) patterns given any computed tomography (CT) slice without any pre-processing prerequisites, such as manually delineated regions of interest (ROIs), is a clinically desirable, yet challenging goal. The majority of existing work relies on manually-provided ILD ROIs to extract sampled 2D image patches from CT slices and, from there, performs patch-based ILD categorization. Acquiring manual ROIs is labor intensive and serves as a bottleneck towards fully-automated CT imaging ILD screening over large-scale populations. Furthermore, despite the considerable high frequency of more than one ILD pattern on a single CT slice, previous works are only designed to detect one ILD pattern per slice or patch.   To tackle these two critical challenges, we present multi-label deep convolutional neural networks (CNNs) for detecting ILDs from holistic CT slices (instead of ROIs or sub-images). Conventional single-labeled CNN models can be augmented to cope with the possible presence of multiple ILD pattern labels, via 1) continuous-valued deep regression based robust norm loss functions or 2) a categorical objective as the sum of element-wise binary logistic losses. Our methods are evaluated and validated using a publicly available database of 658 patient CT scans under five-fold cross-validation, achieving promising performance on detecting four major ILD patterns: Ground Glass, Reticular, Honeycomb, and Emphysema. We also investigate the effectiveness of a CNN activation-based deep-feature encoding scheme using Fisher vector encoding, which treats ILD detection as spatially-unordered deep texture classification.



