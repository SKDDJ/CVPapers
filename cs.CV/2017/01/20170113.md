# Arxiv Papers in cs.CV on 2017-01-13
### An OpenCL(TM) Deep Learning Accelerator on Arria 10
- **Arxiv ID**: http://arxiv.org/abs/1701.03534v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1701.03534v1)
- **Published**: 2017-01-13 00:31:15+00:00
- **Updated**: 2017-01-13 00:31:15+00:00
- **Authors**: Utku Aydonat, Shane O'Connell, Davor Capalija, Andrew C. Ling, Gordon R. Chiu
- **Comment**: To be published at FPGA 2017
- **Journal**: None
- **Summary**: Convolutional neural nets (CNNs) have become a practical means to perform vision tasks, particularly in the area of image classification. FPGAs are well known to be able to perform convolutions efficiently, however, most recent efforts to run CNNs on FPGAs have shown limited advantages over other devices such as GPUs. Previous approaches on FPGAs have often been memory bound due to the limited external memory bandwidth on the FPGA device. We show a novel architecture written in OpenCL(TM), which we refer to as a Deep Learning Accelerator (DLA), that maximizes data reuse and minimizes external memory bandwidth. Furthermore, we show how we can use the Winograd transform to significantly boost the performance of the FPGA. As a result, when running our DLA on Intel's Arria 10 device we can achieve a performance of 1020 img/s, or 23 img/s/W when running the AlexNet CNN benchmark. This comes to 1382 GFLOPs and is 10x faster with 8.4x more GFLOPS and 5.8x better efficiency than the state-of-the-art on FPGAs. Additionally, 23 img/s/W is competitive against the best publicly known implementation of AlexNet on nVidia's TitanX GPU.



### Cost-Effective Active Learning for Deep Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1701.03551v1
- **DOI**: 10.1109/TCSVT.2016.2589879
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.03551v1)
- **Published**: 2017-01-13 03:07:45+00:00
- **Updated**: 2017-01-13 03:07:45+00:00
- **Authors**: Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, Liang Lin
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT) 2016
- **Journal**: None
- **Summary**: Recent successes in learning-based image classification, however, heavily rely on the large number of annotated training samples, which may require considerable human efforts. In this paper, we propose a novel active learning framework, which is capable of building a competitive classifier with optimal feature representation via a limited amount of labeled training instances in an incremental learning manner. Our approach advances the existing active learning methods in two aspects. First, we incorporate deep convolutional neural networks into active learning. Through the properly designed framework, the feature representation and the classifier can be simultaneously updated with progressively annotated informative samples. Second, we present a cost-effective sample selection strategy to improve the classification performance with less manual annotations. Unlike traditional methods focusing on only the uncertain samples of low prediction confidence, we especially discover the large amount of high confidence samples from the unlabeled set for feature learning. Specifically, these high confidence samples are automatically selected and iteratively assigned pseudo-labels. We thus call our framework "Cost-Effective Active Learning" (CEAL) standing for the two advantages.Extensive experiments demonstrate that the proposed CEAL framework can achieve promising results on two challenging image classification datasets, i.e., face recognition on CACD database [1] and object categorization on Caltech-256 [2].



### Active Self-Paced Learning for Cost-Effective and Progressive Face Identification
- **Arxiv ID**: http://arxiv.org/abs/1701.03555v2
- **DOI**: 10.1109/TPAMI.2017.2652459
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.03555v2)
- **Published**: 2017-01-13 03:30:42+00:00
- **Updated**: 2017-07-03 02:18:47+00:00
- **Authors**: Liang Lin, Keze Wang, Deyu Meng, Wangmeng Zuo, Lei Zhang
- **Comment**: To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence 2017
- **Journal**: None
- **Summary**: This paper aims to develop a novel cost-effective framework for face identification, which progressively maintains a batch of classifiers with the increasing face images of different individuals. By naturally combining two recently rising techniques: active learning (AL) and self-paced learning (SPL), our framework is capable of automatically annotating new instances and incorporating them into training under weak expert re-certification. We first initialize the classifier using a few annotated samples for each individual, and extract image features using the convolutional neural nets. Then, a number of candidates are selected from the unannotated samples for classifier updating, in which we apply the current classifiers ranking the samples by the prediction confidence. In particular, our approach utilizes the high-confidence and low-confidence samples in the self-paced and the active user-query way, respectively. The neural nets are later fine-tuned based on the updated classifiers. Such heuristic implementation is formulated as solving a concise active SPL optimization problem, which also advances the SPL development by supplementing a rational dynamic curriculum constraint. The new model finely accords with the "instructor-student-collaborative" learning mode in human education. The advantages of this proposed framework are two-folds: i) The required number of annotated samples is significantly decreased while the comparable performance is guaranteed. A dramatic reduction of user effort is also achieved over other state-of-the-art active learning techniques. ii) The mixture of SPL and AL effectively improves not only the classifier accuracy compared to existing AL/SPL methods but also the robustness against noisy data. We evaluate our framework on two challenging datasets, and demonstrate very promising results. (http://hcp.sysu.edu.cn/projects/aspl/)



### Real-Time Optical flow-based Video Stabilization for Unmanned Aerial Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1701.03572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.03572v1)
- **Published**: 2017-01-13 06:51:57+00:00
- **Updated**: 2017-01-13 06:51:57+00:00
- **Authors**: Anli Lim, Bharath Ramesh, Yue Yang, Cheng Xiang, Zhi Gao, Feng Lin
- **Comment**: Journal Paper
- **Journal**: None
- **Summary**: This paper describes the development of a novel algorithm to tackle the problem of real-time video stabilization for unmanned aerial vehicles (UAVs). There are two main components in the algorithm: (1) By designing a suitable model for the global motion of UAV, the proposed algorithm avoids the necessity of estimating the most general motion model, projective transformation, and considers simpler motion models, such as rigid transformation and similarity transformation. (2) To achieve a high processing speed, optical-flow based tracking is employed in lieu of conventional tracking and matching methods used by state-of-the-art algorithms. These two new ideas resulted in a real-time stabilization algorithm, developed over two phases. Stage I considers processing the whole sequence of frames in the video while achieving an average processing speed of 50fps on several publicly available benchmark videos. Next, Stage II undertakes the task of real-time video stabilization using a multi-threading implementation of the algorithm designed in Stage I.



### Tumour Ellipsification in Ultrasound Images for Treatment Prediction in Breast Cancer
- **Arxiv ID**: http://arxiv.org/abs/1701.03779v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.03779v1)
- **Published**: 2017-01-13 18:53:59+00:00
- **Updated**: 2017-01-13 18:53:59+00:00
- **Authors**: Mehrdad J. Gangeh, Hamid R. Tizhoosh, Kan Wu, Dun Huang, Hadi Tadayyon, Gregory J. Czarnota
- **Comment**: Accepted at BHI 2017
- **Journal**: None
- **Summary**: Recent advances in using quantitative ultrasound (QUS) methods have provided a promising framework to non-invasively and inexpensively monitor or predict the effectiveness of therapeutic cancer responses. One of the earliest steps in using QUS methods is contouring a region of interest (ROI) inside the tumour in ultrasound B-mode images. While manual segmentation is a very time-consuming and tedious task for human experts, auto-contouring is also an extremely difficult task for computers due to the poor quality of ultrasound B-mode images. However, for the purpose of cancer response prediction, a rough boundary of the tumour as an ROI is only needed. In this research, a semi-automated tumour localization approach is proposed for ROI estimation in ultrasound B-mode images acquired from patients with locally advanced breast cancer (LABC). The proposed approach comprised several modules, including 1) feature extraction using keypoint descriptors, 2) augmenting the feature descriptors with the distance of the keypoints to the user-input pixel as the centre of the tumour, 3) supervised learning using a support vector machine (SVM) to classify keypoints as "tumour" or "non-tumour", and 4) computation of an ellipse as an outline of the ROI representing the tumour. Experiments with 33 B-mode images from 10 LABC patients yielded promising results with an accuracy of 76.7% based on the Dice coefficient performance measure. The results demonstrated that the proposed method can potentially be used as the first stage in a computer-assisted cancer response prediction system for semi-automated contouring of breast tumours.



