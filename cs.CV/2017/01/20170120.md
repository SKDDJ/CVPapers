# Arxiv Papers in cs.CV on 2017-01-20
### Dual Recovery Network with Online Compensation for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1701.05652v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05652v3)
- **Published**: 2017-01-20 01:44:21+00:00
- **Updated**: 2018-06-18 07:31:53+00:00
- **Authors**: Sifeng Xia, Wenhan Yang, Jiaying Liu, Zongming Guo
- **Comment**: ISCAS 2018
- **Journal**: None
- **Summary**: Image super-resolution (SR) methods essentially lead to a loss of some high-frequency (HF) information when predicting high-resolution (HR) images from low-resolution (LR) images without using external references. To address this issue, we additionally utilize online retrieved data to facilitate image SR in a unified deep framework. A novel dual high-frequency recovery network (DHN) is proposed to predict an HR image with three parts: an LR image, an internal inferred HF (IHF) map (HF missing part inferred solely from the LR image) and an external extracted HF (EHF) map. In particular, we infer the HF information based on both the LR image and similar HR references which are retrieved online. For the EHF map, we align the references with affine transformation and then in the aligned references, part of HF signals are extracted by the proposed DHN to compensate for the HF loss. Extensive experimental results demonstrate that our DHN achieves notably better performance than state-of-the-art SR methods.



### Efficient Feature Matching by Progressive Candidate Search
- **Arxiv ID**: http://arxiv.org/abs/1701.05676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05676v1)
- **Published**: 2017-01-20 03:52:48+00:00
- **Updated**: 2017-01-20 03:52:48+00:00
- **Authors**: Sehyung Lee, Jongwoo Lim, Il Hong Suh
- **Comment**: 9 pages including 1 references, 9 figures, 2 tables
- **Journal**: None
- **Summary**: We present a novel feature matching algorithm that systematically utilizes the geometric properties of features such as position, scale, and orientation, in addition to the conventional descriptor vectors. In challenging scenes with the presence of repetitive patterns or with a large viewpoint change, it is hard to find the correct correspondences using feature descriptors only, since the descriptor distances of the correct matches may not be the least among the candidates due to appearance changes. Assuming that the layout of the nearby features does not changed much, we propose the bidirectional transfer measure to gauge the geometric consistency of a pair of feature correspondences. The feature matching problem is formulated as a Markov random field (MRF) which uses descriptor distances and relative geometric similarities together. The unmatched features are explicitly modeled in the MRF to minimize its negative impact. For speed and stability, instead of solving the MRF on the entire features at once, we start with a small set of confident feature matches, and then progressively search the candidates in nearby features and expand the MRF with them. Experimental comparisons show that the proposed algorithm finds better feature correspondences, i.e. more matches with higher inlier ratio, in many challenging scenes with much lower computational cost than the state-of-the-art algorithms.



### Automatic Generation of Typographic Font from a Small Font Subset
- **Arxiv ID**: http://arxiv.org/abs/1701.05703v1
- **DOI**: 10.1109/MCG.2019.2931431
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05703v1)
- **Published**: 2017-01-20 06:14:22+00:00
- **Updated**: 2017-01-20 06:14:22+00:00
- **Authors**: Tomo Miyazaki, Tatsunori Tsuchiya, Yoshihiro Sugaya, Shinichiro Omachi, Masakazu Iwamura, Seiichi Uchida, Koichi Kise
- **Comment**: 12 pages, 17 figures
- **Journal**: IEEE Computer Graphics and Applications, 2019
- **Summary**: This paper addresses the automatic generation of a typographic font from a subset of characters. Specifically, we use a subset of a typographic font to extrapolate additional characters. Consequently, we obtain a complete font containing a number of characters sufficient for daily use. The automated generation of Japanese fonts is in high demand because a Japanese font requires over 1,000 characters. Unfortunately, professional typographers create most fonts, resulting in significant financial and time investments for font generation. The proposed method can be a great aid for font creation because designers do not need to create the majority of the characters for a new font. The proposed method uses strokes from given samples for font generation. The strokes, from which we construct characters, are extracted by exploiting a character skeleton dataset. This study makes three main contributions: a novel method of extracting strokes from characters, which is applicable to both standard fonts and their variations; a fully automated approach for constructing characters; and a selection method for sample characters. We demonstrate our proposed method by generating 2,965 characters in 47 fonts. Objective and subjective evaluations verify that the generated characters are similar to handmade characters.



### LAREX - A semi-automatic open-source Tool for Layout Analysis and Region Extraction on Early Printed Books
- **Arxiv ID**: http://arxiv.org/abs/1701.07396v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1701.07396v1)
- **Published**: 2017-01-20 09:48:59+00:00
- **Updated**: 2017-01-20 09:48:59+00:00
- **Authors**: Christian Reul, Uwe Springmann, Frank Puppe
- **Comment**: None
- **Journal**: None
- **Summary**: A semi-automatic open-source tool for layout analysis on early printed books is presented. LAREX uses a rule based connected components approach which is very fast, easily comprehensible for the user and allows an intuitive manual correction if necessary. The PageXML format is used to support integration into existing OCR workflows. Evaluations showed that LAREX provides an efficient and flexible way to segment pages of early printed books.



### Robust Intrinsic and Extrinsic Calibration of RGB-D Cameras
- **Arxiv ID**: http://arxiv.org/abs/1701.05748v2
- **DOI**: 10.1109/TRO.2018.2853742
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1701.05748v2)
- **Published**: 2017-01-20 10:32:03+00:00
- **Updated**: 2018-10-19 11:01:51+00:00
- **Authors**: Filippo Basso, Emanuele Menegatti, Alberto Pretto
- **Comment**: None
- **Journal**: Published in IEEE Transactions on Robotics, vol. 34, no. 5, 2018
- **Summary**: Color-depth cameras (RGB-D cameras) have become the primary sensors in most robotics systems, from service robotics to industrial robotics applications. Typical consumer-grade RGB-D cameras are provided with a coarse intrinsic and extrinsic calibration that generally does not meet the accuracy requirements needed by many robotics applications (e.g., highly accurate 3D environment reconstruction and mapping, high precision object recognition and localization, ...). In this paper, we propose a human-friendly, reliable and accurate calibration framework that enables to easily estimate both the intrinsic and extrinsic parameters of a general color-depth sensor couple. Our approach is based on a novel two components error model. This model unifies the error sources of RGB-D pairs based on different technologies, such as structured-light 3D cameras and time-of-flight cameras. Our method provides some important advantages compared to other state-of-the-art systems: it is general (i.e., well suited for different types of sensors), based on an easy and stable calibration protocol, provides a greater calibration accuracy, and has been implemented within the ROS robotics framework. We report detailed experimental validations and performance comparisons to support our statements.



### A Large-scale Dataset and Benchmark for Similar Trademark Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1701.05766v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05766v2)
- **Published**: 2017-01-20 11:36:30+00:00
- **Updated**: 2017-10-14 11:44:56+00:00
- **Authors**: Osman Tursun, Cemal Aker, Sinan Kalkan
- **Comment**: None
- **Journal**: None
- **Summary**: Trademark retrieval (TR) has become an important yet challenging problem due to an ever increasing trend in trademark applications and infringement incidents. There have been many promising attempts for the TR problem, which, however, fell impracticable since they were evaluated with limited and mostly trivial datasets. In this paper, we provide a large-scale dataset with benchmark queries with which different TR approaches can be evaluated systematically. Moreover, we provide a baseline on this benchmark using the widely-used methods applied to TR in the literature. Furthermore, we identify and correct two important issues in TR approaches that were not addressed before: reversal of contrast, and presence of irrelevant text in trademarks severely affect the TR methods. Lastly, we applied deep learning, namely, several popular Convolutional Neural Network models, to the TR problem. To the best of the authors, this is the first attempt to do so.



### Fusion of Heterogeneous Data in Convolutional Networks for Urban Semantic Labeling (Invited Paper)
- **Arxiv ID**: http://arxiv.org/abs/1701.05818v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1701.05818v1)
- **Published**: 2017-01-20 15:10:09+00:00
- **Updated**: 2017-01-20 15:10:09+00:00
- **Authors**: Nicolas Audebert, Bertrand Le Saux, Sébastien Lefèvre
- **Comment**: Joint Urban Remote Sensing Event (JURSE), Mar 2017, Dubai, United
  Arab Emirates. Joint Urban Remote Sensing Event 2017
- **Journal**: None
- **Summary**: In this work, we present a novel module to perform fusion of heterogeneous data using fully convolutional networks for semantic labeling. We introduce residual correction as a way to learn how to fuse predictions coming out of a dual stream architecture. Especially, we perform fusion of DSM and IRRG optical data on the ISPRS Vaihingen dataset over a urban area and obtain new state-of-the-art results.



### Case Study of a highly automated Layout Analysis and OCR of an incunabulum: 'Der Heiligen Leben' (1488)
- **Arxiv ID**: http://arxiv.org/abs/1701.07395v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.07395v1)
- **Published**: 2017-01-20 15:37:01+00:00
- **Updated**: 2017-01-20 15:37:01+00:00
- **Authors**: Christian Reul, Marco Dittrich, Martin Gruner
- **Comment**: None
- **Journal**: None
- **Summary**: This paper provides the first thorough documentation of a high quality digitization process applied to an early printed book from the incunabulum period (1450-1500). The entire OCR related workflow including preprocessing, layout analysis and text recognition is illustrated in detail using the example of 'Der Heiligen Leben', printed in Nuremberg in 1488. For each step the required time expenditure was recorded. The character recognition yielded excellent results both on character (97.57%) and word (92.19%) level. Furthermore, a comparison of a highly automated (LAREX) and a manual (Aletheia) method for layout analysis was performed. By considerably automating the segmentation the required human effort was reduced significantly from over 100 hours to less than six hours, resulting in only a slight drop in OCR accuracy. Realistic estimates for the human effort necessary for full text extraction from incunabula can be derived from this study. The printed pages of the complete work together with the OCR result is available online ready to be inspected and downloaded.



### End-To-End Visual Speech Recognition With LSTMs
- **Arxiv ID**: http://arxiv.org/abs/1701.05847v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1701.05847v1)
- **Published**: 2017-01-20 16:36:09+00:00
- **Updated**: 2017-01-20 16:36:09+00:00
- **Authors**: Stavros Petridis, Zuwei Li, Maja Pantic
- **Comment**: Accepted for publication, ICASSP 2017
- **Journal**: None
- **Summary**: Traditional visual speech recognition systems consist of two stages, feature extraction and classification. Recently, several deep learning approaches have been presented which automatically extract features from the mouth images and aim to replace the feature extraction stage. However, research on joint learning of features and classification is very limited. In this work, we present an end-to-end visual speech recognition system based on Long-Short Memory (LSTM) networks. To the best of our knowledge, this is the first model which simultaneously learns to extract features directly from the pixels and perform classification and also achieves state-of-the-art performance in visual speech classification. The model consists of two streams which extract features directly from the mouth and difference images, respectively. The temporal dynamics in each stream are modelled by an LSTM and the fusion of the two streams takes place via a Bidirectional LSTM (BLSTM). An absolute improvement of 9.7% over the base line is reported on the OuluVS2 database, and 1.5% on the CUAVE database when compared with other methods which use a similar visual front-end.



