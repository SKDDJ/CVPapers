# Arxiv Papers in cs.CV on 2017-01-23
### Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities
- **Arxiv ID**: http://arxiv.org/abs/1701.06264v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.06264v6)
- **Published**: 2017-01-23 04:46:22+00:00
- **Updated**: 2018-03-19 03:54:50+00:00
- **Authors**: Guo-Jun Qi
- **Comment**: The source codes for both LS-GAN and GLS-GAN are available at
  \url{https://github.com/maple-research-lab}. LS-GAN is also supported by
  Microsoft CNTK at
  \url{https://www.cntk.ai/pythondocs/CNTK_206C_WGAN_LSGAN.html}. The original
  codes of LS-GAN and GLS-GAN are also available at
  https://github.com/guojunq/lsgan/ and https://github.com/guojunq/glsgan/
- **Journal**: in International Journal of Computer Vision (IJCV), 2019
- **Summary**: In this paper, we present the Lipschitz regularization theory and algorithms for a novel Loss-Sensitive Generative Adversarial Network (LS-GAN). Specifically, it trains a loss function to distinguish between real and fake samples by designated margins, while learning a generator alternately to produce realistic samples by minimizing their losses. The LS-GAN further regularizes its loss function with a Lipschitz regularity condition on the density of real data, yielding a regularized model that can better generalize to produce new data from a reasonable number of training examples than the classic GAN. We will further present a Generalized LS-GAN (GLS-GAN) and show it contains a large family of regularized GAN models, including both LS-GAN and Wasserstein GAN, as its special cases. Compared with the other GAN models, we will conduct experiments to show both LS-GAN and GLS-GAN exhibit competitive ability in generating new images in terms of the Minimum Reconstruction Error (MRE) assessed on a separate test set. We further extend the LS-GAN to a conditional form for supervised and semi-supervised learning problems, and demonstrate its outstanding performance on image classification tasks.



### Normative theory of visual receptive fields
- **Arxiv ID**: http://arxiv.org/abs/1701.06333v4
- **DOI**: 10.1016/j.heliyon.2021.e05897
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1701.06333v4)
- **Published**: 2017-01-23 11:13:07+00:00
- **Updated**: 2017-02-16 13:52:12+00:00
- **Authors**: Tony Lindeberg
- **Comment**: 15 pages, 17 figures. arXiv admin note: text overlap with
  arXiv:1210.0754
- **Journal**: Substantially revised version in Heliyon 7(1): e05897: 1-20, 2021
- **Summary**: This article gives an overview of a normative computational theory of visual receptive fields, by which idealized functional models of early spatial, spatio-chromatic and spatio-temporal receptive fields can be derived in an axiomatic way based on structural properties of the environment in combination with assumptions about the internal structure of a vision system to guarantee consistent handling of image representations over multiple spatial and temporal scales. Interestingly, this theory leads to predictions about visual receptive field shapes with qualitatively very good similarity to biological receptive fields measured in the retina, the LGN and the primary visual cortex (V1) of mammals.



### Person Re-Identification via Recurrent Feature Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1701.06351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.06351v1)
- **Published**: 2017-01-23 12:05:16+00:00
- **Updated**: 2017-01-23 12:05:16+00:00
- **Authors**: Yichao Yan, Bingbing Ni, Zhichao Song, Chao Ma, Yan Yan, Xiaokang Yang
- **Comment**: 14 pages, 4 figures, in ECCV 2016
- **Journal**: None
- **Summary**: We address the person re-identification problem by effectively exploiting a globally discriminative feature representation from a sequence of tracked human regions/patches. This is in contrast to previous person re-id works, which rely on either single frame based person to person patch matching, or graph based sequence to sequence matching. We show that a progressive/sequential fusion framework based on long short term memory (LSTM) network aggregates the frame-wise human region representation at each time stamp and yields a sequence level human feature representation. Since LSTM nodes can remember and propagate previously accumulated good features and forget newly input inferior ones, even with simple hand-crafted features, the proposed recurrent feature aggregation network (RFA-Net) is effective in generating highly discriminative sequence level human representations. Extensive experimental results on two person re-identification benchmarks demonstrate that the proposed method performs favorably against state-of-the-art person re-identification methods.



### Nonsmooth Analysis and Subgradient Methods for Averaging in Dynamic Time Warping Spaces
- **Arxiv ID**: http://arxiv.org/abs/1701.06393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.06393v1)
- **Published**: 2017-01-23 13:58:44+00:00
- **Updated**: 2017-01-23 13:58:44+00:00
- **Authors**: David Schultz, Brijnesh Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Time series averaging in dynamic time warping (DTW) spaces has been successfully applied to improve pattern recognition systems. This article proposes and analyzes subgradient methods for the problem of finding a sample mean in DTW spaces. The class of subgradient methods generalizes existing sample mean algorithms such as DTW Barycenter Averaging (DBA). We show that DBA is a majorize-minimize algorithm that converges to necessary conditions of optimality after finitely many iterations. Empirical results show that for increasing sample sizes the proposed stochastic subgradient (SSG) algorithm is more stable and finds better solutions in shorter time than the DBA algorithm on average. Therefore, SSG is useful in online settings and for non-small sample sizes. The theoretical and empirical results open new paths for devising sample mean algorithms: nonsmooth optimization methods and modified variants of pairwise averaging methods.



### Segmentation-free Vehicle License Plate Recognition using ConvNet-RNN
- **Arxiv ID**: http://arxiv.org/abs/1701.06439v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.0, I.5.1, I.5.4, I.7.5
- **Links**: [PDF](http://arxiv.org/pdf/1701.06439v1)
- **Published**: 2017-01-23 15:11:12+00:00
- **Updated**: 2017-01-23 15:11:12+00:00
- **Authors**: Teik Koon Cheang, Yong Shean Chong, Yong Haur Tay
- **Comment**: 5 pages, 3 figures, International Workshop on Advanced Image
  Technology, January, 8-10, 2017. Penang, Malaysia. Proceeding IWAIT2017
- **Journal**: None
- **Summary**: While vehicle license plate recognition (VLPR) is usually done with a sliding window approach, it can have limited performance on datasets with characters that are of variable width. This can be solved by hand-crafting algorithms to prescale the characters. While this approach can work fairly well, the recognizer is only aware of the pixels within each detector window, and fails to account for other contextual information that might be present in other parts of the image. A sliding window approach also requires training data in the form of presegmented characters, which can be more difficult to obtain. In this paper, we propose a unified ConvNet-RNN model to recognize real-world captured license plate photographs. By using a Convolutional Neural Network (ConvNet) to perform feature extraction and using a Recurrent Neural Network (RNN) for sequencing, we address the problem of sliding window approaches being unable to access the context of the entire image by feeding the entire image as input to the ConvNet. This has the added benefit of being able to perform end-to-end training of the entire model on labelled, full license plate images. Experimental results comparing the ConvNet-RNN architecture to a sliding window-based approach shows that the ConvNet-RNN architecture performs significantly better.



### Learning what to look in chest X-rays with a recurrent visual attention model
- **Arxiv ID**: http://arxiv.org/abs/1701.06452v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1701.06452v1)
- **Published**: 2017-01-23 15:29:47+00:00
- **Updated**: 2017-01-23 15:29:47+00:00
- **Authors**: Petros-Pavlos Ypsilantis, Giovanni Montana
- **Comment**: NIPS 2016 Workshop on Machine Learning for Health
- **Journal**: None
- **Summary**: X-rays are commonly performed imaging tests that use small amounts of radiation to produce pictures of the organs, tissues, and bones of the body. X-rays of the chest are used to detect abnormalities or diseases of the airways, blood vessels, bones, heart, and lungs. In this work we present a stochastic attention-based model that is capable of learning what regions within a chest X-ray scan should be visually explored in order to conclude that the scan contains a specific radiological abnormality. The proposed model is a recurrent neural network (RNN) that learns to sequentially sample the entire X-ray and focus only on informative areas that are likely to contain the relevant information. We report on experiments carried out with more than $100,000$ X-rays containing enlarged hearts or medical devices. The model has been trained using reinforcement learning methods to learn task-specific policies.



### Using Convolutional Neural Networks to Count Palm Trees in Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1701.06462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.06462v1)
- **Published**: 2017-01-23 15:38:52+00:00
- **Updated**: 2017-01-23 15:38:52+00:00
- **Authors**: Eu Koon Cheang, Teik Koon Cheang, Yong Haur Tay
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a supervised learning system for counting and localizing palm trees in high-resolution, panchromatic satellite imagery (40cm/pixel to 1.5m/pixel). A convolutional neural network classifier trained on a set of palm and no-palm images is applied across a satellite image scene in a sliding window fashion. The resultant confidence map is smoothed with a uniform filter. A non-maximal suppression is applied onto the smoothed confidence map to obtain peaks. Trained with a small dataset of 500 images of size 40x40 cropped from satellite images, the system manages to achieve a tree count accuracy of over 99%.



### Dirty Pixels: Towards End-to-End Image Processing and Perception
- **Arxiv ID**: http://arxiv.org/abs/1701.06487v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.06487v2)
- **Published**: 2017-01-23 16:46:12+00:00
- **Updated**: 2021-05-08 02:14:41+00:00
- **Authors**: Steven Diamond, Vincent Sitzmann, Frank Julca-Aguilar, Stephen Boyd, Gordon Wetzstein, Felix Heide
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world imaging systems acquire measurements that are degraded by noise, optical aberrations, and other imperfections that make image processing for human viewing and higher-level perception tasks challenging. Conventional cameras address this problem by compartmentalizing imaging from high-level task processing. As such, conventional imaging involves processing the RAW sensor measurements in a sequential pipeline of steps, such as demosaicking, denoising, deblurring, tone-mapping and compression. This pipeline is optimized to obtain a visually pleasing image. High-level processing, on the other hand, involves steps such as feature extraction, classification, tracking, and fusion. While this siloed design approach allows for efficient development, it also dictates compartmentalized performance metrics, without knowledge of the higher-level task of the camera system. For example, today's demosaicking and denoising algorithms are designed using perceptual image quality metrics but not with domain-specific tasks such as object detection in mind. We propose an end-to-end differentiable architecture that jointly performs demosaicking, denoising, deblurring, tone-mapping, and classification. The architecture learns processing pipelines whose outputs differ from those of existing ISPs optimized for perceptual quality, preserving fine detail at the cost of increased noise and artifacts. We demonstrate on captured and simulated data that our model substantially improves perception in low light and other challenging conditions, which is imperative for real-world applications. Finally, we found that the proposed model also achieves state-of-the-art accuracy when optimized for image reconstruction in low-light conditions, validating the architecture itself as a potentially useful drop-in network for reconstruction and analysis tasks beyond the applications demonstrated in this work.



### Unsupervised Joint Mining of Deep Features and Image Labels for Large-scale Radiology Image Categorization and Scene Recognition
- **Arxiv ID**: http://arxiv.org/abs/1701.06599v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.06599v2)
- **Published**: 2017-01-23 19:34:22+00:00
- **Updated**: 2017-12-27 19:09:02+00:00
- **Authors**: Xiaosong Wang, Le Lu, Hoo-chang Shin, Lauren Kim, Mohammadhadi Bagheri, Isabella Nogues, Jianhua Yao, Ronald M. Summers
- **Comment**: WACV 2017. arXiv admin note: text overlap with arXiv:1603.07965 V2:
  supplementary material appended
- **Journal**: None
- **Summary**: The recent rapid and tremendous success of deep convolutional neural networks (CNN) on many challenging computer vision tasks largely derives from the accessibility of the well-annotated ImageNet and PASCAL VOC datasets. Nevertheless, unsupervised image categorization (i.e., without the ground-truth labeling) is much less investigated, yet critically important and difficult when annotations are extremely hard to obtain in the conventional way of "Google Search" and crowd sourcing. We address this problem by presenting a looped deep pseudo-task optimization (LDPO) framework for joint mining of deep CNN features and image labels. Our method is conceptually simple and rests upon the hypothesized "convergence" of better labels leading to better trained CNN models which in turn feed more discriminative image representations to facilitate more meaningful clusters/labels. Our proposed method is validated in tackling two important applications: 1) Large-scale medical image annotation has always been a prohibitively expensive and easily-biased task even for well-trained radiologists. Significantly better image categorization results are achieved via our proposed approach compared to the previous state-of-the-art method. 2) Unsupervised scene recognition on representative and publicly available datasets with our proposed technique is examined. The LDPO achieves excellent quantitative scene classification results. On the MIT indoor scene dataset, it attains a clustering accuracy of 75.3%, compared to the state-of-the-art supervised classification accuracy of 81.0% (when both are based on the VGG-VD model).



### Perceptually Optimized Image Rendering
- **Arxiv ID**: http://arxiv.org/abs/1701.06641v1
- **DOI**: 10.1364/JOSAA.34.001511
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1701.06641v1)
- **Published**: 2017-01-23 21:38:52+00:00
- **Updated**: 2017-01-23 21:38:52+00:00
- **Authors**: Valero Laparra, Alex Berardino, Johannes Ballé, Eero P. Simoncelli
- **Comment**: None
- **Journal**: J. Optical Society of America, A. 34(9):1511-1525. Sep 2017
- **Summary**: We develop a framework for rendering photographic images, taking into account display limitations, so as to optimize perceptual similarity between the rendered image and the original scene. We formulate this as a constrained optimization problem, in which we minimize a measure of perceptual dissimilarity, the Normalized Laplacian Pyramid Distance (NLPD), which mimics the early stage transformations of the human visual system. When rendering images acquired with higher dynamic range than that of the display, we find that the optimized solution boosts the contrast of low-contrast features without introducing significant artifacts, yielding results of comparable visual quality to current state-of-the art methods with no manual intervention or parameter settings. We also examine a variety of other display constraints, including limitations on minimum luminance (black point), mean luminance (as a proxy for energy consumption), and quantized luminance levels (halftoning). Finally, we show that the method may be used to enhance details and contrast of images degraded by optical scattering (e.g. fog).



### Residual and Plain Convolutional Neural Networks for 3D Brain MRI Classification
- **Arxiv ID**: http://arxiv.org/abs/1701.06643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.06643v1)
- **Published**: 2017-01-23 21:54:50+00:00
- **Updated**: 2017-01-23 21:54:50+00:00
- **Authors**: Sergey Korolev, Amir Safiullin, Mikhail Belyaev, Yulia Dodonova
- **Comment**: IEEE International Symposium on Biomedical Imaging 2017
- **Journal**: None
- **Summary**: In the recent years there have been a number of studies that applied deep learning algorithms to neuroimaging data. Pipelines used in those studies mostly require multiple processing steps for feature extraction, although modern advancements in deep learning for image classification can provide a powerful framework for automatic feature generation and more straightforward analysis. In this paper, we show how similar performance can be achieved skipping these feature extraction steps with the residual and plain 3D convolutional neural network architectures. We demonstrate the performance of the proposed approach for classification of Alzheimer's disease versus mild cognitive impairment and normal controls on the Alzheimer's Disease National Initiative (ADNI) dataset of 3D structural MRI brain scans.



### DSSD : Deconvolutional Single Shot Detector
- **Arxiv ID**: http://arxiv.org/abs/1701.06659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.06659v1)
- **Published**: 2017-01-23 22:33:35+00:00
- **Updated**: 2017-01-23 22:33:35+00:00
- **Authors**: Cheng-Yang Fu, Wei Liu, Ananth Ranga, Ambrish Tyagi, Alexander C. Berg
- **Comment**: None
- **Journal**: None
- **Summary**: The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection. To achieve this we first combine a state-of-the-art classifier (Residual-101[14]) with a fast detection framework (SSD[18]). We then augment SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects, calling our resulting system DSSD for deconvolutional single shot detector. While these two contributions are easily described at a high-level, a naive implementation does not succeed. Instead we show that carefully adding additional stages of learned transformations, specifically a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research. Results are shown on both PASCAL VOC and COCO detection. Our DSSD with $513 \times 513$ input achieves 81.5% mAP on VOC2007 test, 80.0% mAP on VOC2012 test, and 33.2% mAP on COCO, outperforming a state-of-the-art method R-FCN[3] on each dataset.



