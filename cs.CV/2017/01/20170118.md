# Arxiv Papers in cs.CV on 2017-01-18
### Compression of Deep Neural Networks for Image Instance Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1701.04923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.04923v1)
- **Published**: 2017-01-18 01:57:55+00:00
- **Updated**: 2017-01-18 01:57:55+00:00
- **Authors**: Vijay Chandrasekhar, Jie Lin, Qianli Liao, Olivier Morère, Antoine Veillard, Lingyu Duan, Tomaso Poggio
- **Comment**: 10 pages, accepted by DCC 2017
- **Journal**: None
- **Summary**: Image instance retrieval is the problem of retrieving images from a database which contain the same object. Convolutional Neural Network (CNN) based descriptors are becoming the dominant approach for generating {\it global image descriptors} for the instance retrieval problem. One major drawback of CNN-based {\it global descriptors} is that uncompressed deep neural network models require hundreds of megabytes of storage making them inconvenient to deploy in mobile applications or in custom hardware. In this work, we study the problem of neural network model compression focusing on the image instance retrieval task. We study quantization, coding, pruning and weight sharing techniques for reducing model size for the instance retrieval problem. We provide extensive experimental results on the trade-off between retrieval performance and model size for different types of networks on several data sets providing the most comprehensive study on this topic. We compress models to the order of a few MBs: two orders of magnitude smaller than the uncompressed models while achieving negligible loss in retrieval performance.



### Action Recognition: From Static Datasets to Moving Robots
- **Arxiv ID**: http://arxiv.org/abs/1701.04925v1
- **DOI**: 10.1109/ICRA.2017.7989361
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1701.04925v1)
- **Published**: 2017-01-18 02:10:56+00:00
- **Updated**: 2017-01-18 02:10:56+00:00
- **Authors**: Fahimeh Rezazadegan, Sareh Shirazi, Ben Upcroft, Michael Milford
- **Comment**: None
- **Journal**: Robotics and Automation (ICRA), 2017 IEEE International Conference
  on
- **Summary**: Deep learning models have achieved state-of-the- art performance in recognizing human activities, but often rely on utilizing background cues present in typical computer vision datasets that predominantly have a stationary camera. If these models are to be employed by autonomous robots in real world environments, they must be adapted to perform independently of background cues and camera motion effects. To address these challenges, we propose a new method that firstly generates generic action region proposals with good potential to locate one human action in unconstrained videos regardless of camera motion and then uses action proposals to extract and classify effective shape and motion features by a ConvNet framework. In a range of experiments, we demonstrate that by actively proposing action regions during both training and testing, state-of-the-art or better performance is achieved on benchmarks. We show the outperformance of our approach compared to the state-of-the-art in two new datasets; one emphasizes on irrelevant background, the other highlights the camera motion. We also validate our action recognition method in an abnormal behavior detection scenario to improve workplace safety. The results verify a higher success rate for our method due to the ability of our system to recognize human actions regardless of environment and camera motion.



### Bringing Impressionism to Life with Neural Style Transfer in Come Swim
- **Arxiv ID**: http://arxiv.org/abs/1701.04928v1
- **DOI**: None
- **Categories**: **cs.CV**, I.3.3; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/1701.04928v1)
- **Published**: 2017-01-18 02:37:48+00:00
- **Updated**: 2017-01-18 02:37:48+00:00
- **Authors**: Bhautik Joshi, Kristen Stewart, David Shapiro
- **Comment**: 3 pages, 6 figures, paper is a case study of how Neural Style
  Transfer can be used in a movie production context
- **Journal**: None
- **Summary**: Neural Style Transfer is a striking, recently-developed technique that uses neural networks to artistically redraw an image in the style of a source style image. This paper explores the use of this technique in a production setting, applying Neural Style Transfer to redraw key scenes in 'Come Swim' in the style of the impressionistic painting that inspired the film. We document how the technique can be driven within the framework of an iterative creative process to achieve a desired look, and propose a mapping of the broad parameter space to a key set of creative controls. We hope that this mapping can provide insights into priorities for future research.



### A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in Caffe
- **Arxiv ID**: http://arxiv.org/abs/1701.04949v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1701.04949v1)
- **Published**: 2017-01-18 05:24:24+00:00
- **Updated**: 2017-01-18 05:24:24+00:00
- **Authors**: Volodymyr Turchenko, Eric Chalmers, Artur Luczak
- **Comment**: 21 pages, 11 figures, 5 tables, 62 references
- **Journal**: None
- **Summary**: This paper presents the development of several models of a deep convolutional auto-encoder in the Caffe deep learning framework and their experimental evaluation on the example of MNIST dataset. We have created five models of a convolutional auto-encoder which differ architecturally by the presence or absence of pooling and unpooling layers in the auto-encoder's encoder and decoder parts. Our results show that the developed models provide very good results in dimensionality reduction and unsupervised clustering tasks, and small classification errors when we used the learned internal code as an input of a supervised linear classifier and multi-layer perceptron. The best results were provided by a model where the encoder part contains convolutional and pooling layers, followed by an analogous decoder part with deconvolution and unpooling layers without the use of switch variables in the decoder part. The paper also discusses practical details of the creation of a deep convolutional auto-encoder in the very popular Caffe deep learning framework. We believe that our approach and results presented in this paper could help other researchers to build efficient deep neural network architectures in the future.



### Effective Multi-Query Expansions: Collaborative Deep Networks for Robust Landmark Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1701.05003v1
- **DOI**: 10.1109/TIP.2017.2655449
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05003v1)
- **Published**: 2017-01-18 10:48:00+00:00
- **Updated**: 2017-01-18 10:48:00+00:00
- **Authors**: Yang Wang, Xuemin Lin, Lin Wu, Wenjie Zhang
- **Comment**: Accepted to Appear in IEEE Trans on Image Processing
- **Journal**: None
- **Summary**: Given a query photo issued by a user (q-user), the landmark retrieval is to return a set of photos with their landmarks similar to those of the query, while the existing studies on the landmark retrieval focus on exploiting geometries of landmarks for similarity matches between candidate photos and a query photo. We observe that the same landmarks provided by different users over social media community may convey different geometry information depending on the viewpoints and/or angles, and may subsequently yield very different results. In fact, dealing with the landmarks with \illshapes caused by the photography of q-users is often nontrivial and has seldom been studied. In this paper we propose a novel framework, namely multi-query expansions, to retrieve semantically robust landmarks by two steps. Firstly, we identify the top-$k$ photos regarding the latent topics of a query landmark to construct multi-query set so as to remedy its possible \illshape. For this purpose, we significantly extend the techniques of Latent Dirichlet Allocation. Then, motivated by the typical \emph{collaborative filtering} methods, we propose to learn a \emph{collaborative} deep networks based semantically, nonlinear and high-level features over the latent factor for landmark photo as the training set, which is formed by matrix factorization over \emph{collaborative} user-photo matrix regarding the multi-query set. The learned deep network is further applied to generate the features for all the other photos, meanwhile resulting into a compact multi-query set within such space. Extensive experiments are conducted on real-world social media data with both landmark photos together with their user information to show the superior performance over the existing methods.



### Transfer learning for multi-center classification of chronic obstructive pulmonary disease
- **Arxiv ID**: http://arxiv.org/abs/1701.05013v2
- **DOI**: 10.1109/JBHI.2017.2769800
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.05013v2)
- **Published**: 2017-01-18 11:13:01+00:00
- **Updated**: 2017-11-23 14:10:34+00:00
- **Authors**: Veronika Cheplygina, Isabel Pino Peña, Jesper Holst Pedersen, David A. Lynch, Lauge Sørensen, Marleen de Bruijne
- **Comment**: Accepted at Journal of Biomedical and Health Informatics
- **Journal**: None
- **Summary**: Chronic obstructive pulmonary disease (COPD) is a lung disease which can be quantified using chest computed tomography (CT) scans. Recent studies have shown that COPD can be automatically diagnosed using weakly supervised learning of intensity and texture distributions. However, up till now such classifiers have only been evaluated on scans from a single domain, and it is unclear whether they would generalize across domains, such as different scanners or scanning protocols. To address this problem, we investigate classification of COPD in a multi-center dataset with a total of 803 scans from three different centers, four different scanners, with heterogenous subject distributions. Our method is based on Gaussian texture features, and a weighted logistic classifier, which increases the weights of samples similar to the test data. We show that Gaussian texture features outperform intensity features previously used in multi-center classification tasks. We also show that a weighting strategy based on a classifier that is trained to discriminate between scans from different domains, can further improve the results. To encourage further research into transfer learning methods for classification of COPD, upon acceptance of the paper we will release two feature datasets used in this study on http://bigr.nl/research/projects/copd



### Deep Learning Features at Scale for Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1701.05105v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1701.05105v1)
- **Published**: 2017-01-18 16:28:03+00:00
- **Updated**: 2017-01-18 16:28:03+00:00
- **Authors**: Zetao Chen, Adam Jacobson, Niko Sunderhauf, Ben Upcroft, Lingqiao Liu, Chunhua Shen, Ian Reid, Michael Milford
- **Comment**: 8 pages, 10 figures. Accepted by International Conference on Robotics
  and Automation (ICRA) 2017. This is the submitted version. The final
  published version may be slightly different
- **Journal**: None
- **Summary**: The success of deep learning techniques in the computer vision domain has triggered a range of initial investigations into their utility for visual place recognition, all using generic features from networks that were trained for other types of recognition tasks. In this paper, we train, at large scale, two CNN architectures for the specific place recognition task and employ a multi-scale feature encoding method to generate condition- and viewpoint-invariant features. To enable this training to occur, we have developed a massive Specific PlacEs Dataset (SPED) with hundreds of examples of place appearance change at thousands of different places, as opposed to the semantic place type datasets currently available. This new dataset enables us to set up a training regime that interprets place recognition as a classification problem. We comprehensively evaluate our trained networks on several challenging benchmark place recognition datasets and demonstrate that they achieve an average 10% increase in performance over other place recognition algorithms and pre-trained CNNs. By analyzing the network responses and their differences from pre-trained networks, we provide insights into what a network learns when training for place recognition, and what these results signify for future research in this area.



### Parsimonious Inference on Convolutional Neural Networks: Learning and applying on-line kernel activation rules
- **Arxiv ID**: http://arxiv.org/abs/1701.05221v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, 68T10, 62H30, 68Q32, 68T05, 68Q32, 91E40, I.5; F.1.1; F.4.1; K.3.2; I.4; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1701.05221v5)
- **Published**: 2017-01-18 20:03:12+00:00
- **Updated**: 2017-01-31 12:15:43+00:00
- **Authors**: I. Theodorakopoulos, V. Pothos, D. Kastaniotis, N. Fragoulis
- **Comment**: 17 pages, 10 figures, 5 tables
- **Journal**: None
- **Summary**: A new, radical CNN design approach is presented in this paper, considering the reduction of the total computational load during inference. This is achieved by a new holistic intervention on both the CNN architecture and the training procedure, which targets to the parsimonious inference by learning to exploit or remove the redundant capacity of a CNN architecture. This is accomplished, by the introduction of a new structural element that can be inserted as an add-on to any contemporary CNN architecture, whilst preserving or even improving its recognition accuracy. Our approach formulates a systematic and data-driven method for developing CNNs that are trained to eventually change size and form in real-time during inference, targeting to the smaller possible computational footprint. Results are provided for the optimal implementation on a few modern, high-end mobile computing platforms indicating a significant speed-up of up to x3 times.



