# Arxiv Papers in cs.CV on 2017-03-24
### Perception Driven Texture Generation
- **Arxiv ID**: http://arxiv.org/abs/1703.09784v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.09784v1)
- **Published**: 2017-03-24 01:25:30+00:00
- **Updated**: 2017-03-24 01:25:30+00:00
- **Authors**: Yanhai Gan, Huifang Chi, Ying Gao, Jun Liu, Guoqiang Zhong, Junyu Dong
- **Comment**: 7 pages, 4 figures, icme2017
- **Journal**: None
- **Summary**: This paper investigates a novel task of generating texture images from perceptual descriptions. Previous work on texture generation focused on either synthesis from examples or generation from procedural models. Generating textures from perceptual attributes have not been well studied yet. Meanwhile, perceptual attributes, such as directionality, regularity and roughness are important factors for human observers to describe a texture. In this paper, we propose a joint deep network model that combines adversarial training and perceptual feature regression for texture generation, while only random noise and user-defined perceptual attributes are required as input. In this model, a preliminary trained convolutional neural network is essentially integrated with the adversarial framework, which can drive the generated textures to possess given perceptual attributes. An important aspect of the proposed model is that, if we change one of the input perceptual features, the corresponding appearance of the generated textures will also be changed. We design several experiments to validate the effectiveness of the proposed method. The results show that the proposed method can produce high quality texture images with desired perceptual properties.



### View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data
- **Arxiv ID**: http://arxiv.org/abs/1703.08274v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08274v2)
- **Published**: 2017-03-24 03:01:29+00:00
- **Updated**: 2017-08-04 10:21:13+00:00
- **Authors**: Pengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng, Jianru Xue, Nanning Zheng
- **Comment**: ICCV2017
- **Journal**: None
- **Summary**: Skeleton-based human action recognition has recently attracted increasing attention due to the popularity of 3D skeleton data. One main challenge lies in the large view variations in captured human actions. We propose a novel view adaptation scheme to automatically regulate observation viewpoints during the occurrence of an action. Rather than re-positioning the skeletons based on a human defined prior criterion, we design a view adaptive recurrent neural network (RNN) with LSTM architecture, which enables the network itself to adapt to the most suitable observation viewpoints from end to end. Extensive experiment analyses show that the proposed view adaptive RNN model strives to (1) transform the skeletons of various views to much more consistent viewpoints and (2) maintain the continuity of the action rather than transforming every frame to the same position with the same body orientation. Our model achieves significant improvement over the state-of-the-art approaches on three benchmark datasets.



### Deep Direct Regression for Multi-Oriented Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1703.08289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08289v1)
- **Published**: 2017-03-24 05:54:11+00:00
- **Updated**: 2017-03-24 05:54:11+00:00
- **Authors**: Wenhao He, Xu-Yao Zhang, Fei Yin, Cheng-Lin Liu
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: In this paper, we first provide a new perspective to divide existing high performance object detection methods into direct and indirect regressions. Direct regression performs boundary regression by predicting the offsets from a given point, while indirect regression predicts the offsets from some bounding box proposals. Then we analyze the drawbacks of the indirect regression, which the recent state-of-the-art detection structures like Faster-RCNN and SSD follows, for multi-oriented scene text detection, and point out the potential superiority of direct regression. To verify this point of view, we propose a deep direct regression based method for multi-oriented scene text detection. Our detection framework is simple and effective with a fully convolutional network and one-step post processing. The fully convolutional network is optimized in an end-to-end way and has bi-task outputs where one is pixel-wise classification between text and non-text, and the other is direct regression to determine the vertex coordinates of quadrilateral text boundaries. The proposed method is particularly beneficial for localizing incidental scene texts. On the ICDAR2015 Incidental Scene Text benchmark, our method achieves the F1-measure of 81%, which is a new state-of-the-art and significantly outperforms previous approaches. On other standard datasets with focused scene texts, our method also reaches the state-of-the-art performance.



### Improving Classification by Improving Labelling: Introducing Probabilistic Multi-Label Object Interaction Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.08338v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08338v2)
- **Published**: 2017-03-24 10:11:03+00:00
- **Updated**: 2017-04-21 16:29:22+00:00
- **Authors**: Michael Wray, Davide Moltisanti, Walterio Mayol-Cuevas, Dima Damen
- **Comment**: None
- **Journal**: None
- **Summary**: This work deviates from easy-to-define class boundaries for object interactions. For the task of object interaction recognition, often captured using an egocentric view, we show that semantic ambiguities in verbs and recognising sub-interactions along with concurrent interactions result in legitimate class overlaps (Figure 1). We thus aim to model the mapping between observations and interaction classes, as well as class overlaps, towards a probabilistic multi-label classifier that emulates human annotators. Given a video segment containing an object interaction, we model the probability for a verb, out of a list of possible verbs, to be used to annotate that interaction. The proba- bility is learnt from crowdsourced annotations, and is tested on two public datasets, comprising 1405 video sequences for which we provide annotations on 90 verbs. We outper- form conventional single-label classification by 11% and 6% on the two datasets respectively, and show that learning from annotation probabilities outperforms majority voting and enables discovery of co-occurring labels.



### Scalable Person Re-identification on Supervised Smoothed Manifold
- **Arxiv ID**: http://arxiv.org/abs/1703.08359v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08359v1)
- **Published**: 2017-03-24 11:17:00+00:00
- **Updated**: 2017-03-24 11:17:00+00:00
- **Authors**: Song Bai, Xiang Bai, Qi Tian
- **Comment**: Accepted as spotlight by CVPR 2017
- **Journal**: None
- **Summary**: Most existing person re-identification algorithms either extract robust visual features or learn discriminative metrics for person images. However, the underlying manifold which those images reside on is rarely investigated. That raises a problem that the learned metric is not smooth with respect to the local geometry structure of the data manifold.   In this paper, we study person re-identification with manifold-based affinity learning, which did not receive enough attention from this area. An unconventional manifold-preserving algorithm is proposed, which can 1) make the best use of supervision from training data, whose label information is given as pairwise constraints; 2) scale up to large repositories with low on-line time complexity; and 3) be plunged into most existing algorithms, serving as a generic postprocessing procedure to further boost the identification accuracies. Extensive experimental results on five popular person re-identification benchmarks consistently demonstrate the effectiveness of our method. Especially, on the largest CUHK03 and Market-1501, our method outperforms the state-of-the-art alternatives by a large margin with high efficiency, which is more appropriate for practical applications.



### A Hybrid Deep Learning Approach for Texture Analysis
- **Arxiv ID**: http://arxiv.org/abs/1703.08366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08366v1)
- **Published**: 2017-03-24 11:39:26+00:00
- **Updated**: 2017-03-24 11:39:26+00:00
- **Authors**: Hussein Adly, Mohamed Moustafa
- **Comment**: None
- **Journal**: None
- **Summary**: Texture classification is a problem that has various applications such as remote sensing and forest species recognition. Solutions tend to be custom fit to the dataset used but fails to generalize. The Convolutional Neural Network (CNN) in combination with Support Vector Machine (SVM) form a robust selection between powerful invariant feature extractor and accurate classifier. The fusion of experts provides stability in classification rates among different datasets.



### Feature Fusion using Extended Jaccard Graph and Stochastic Gradient Descent for Robot
- **Arxiv ID**: http://arxiv.org/abs/1703.08378v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1703.08378v1)
- **Published**: 2017-03-24 11:58:14+00:00
- **Updated**: 2017-03-24 11:58:14+00:00
- **Authors**: Shenglan Liu, Muxin Sun, Wei Wang, Feilong Wang
- **Comment**: Assembly Automation
- **Journal**: None
- **Summary**: Robot vision is a fundamental device for human-robot interaction and robot complex tasks. In this paper, we use Kinect and propose a feature graph fusion (FGF) for robot recognition. Our feature fusion utilizes RGB and depth information to construct fused feature from Kinect. FGF involves multi-Jaccard similarity to compute a robust graph and utilize word embedding method to enhance the recognition results. We also collect DUT RGB-D face dataset and a benchmark datset to evaluate the effectiveness and efficiency of our method. The experimental results illustrate FGF is robust and effective to face and object datasets in robot applications.



### DeepVisage: Making face recognition simple yet with powerful generalization skills
- **Arxiv ID**: http://arxiv.org/abs/1703.08388v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08388v2)
- **Published**: 2017-03-24 12:41:38+00:00
- **Updated**: 2017-04-07 11:37:21+00:00
- **Authors**: Abul Hasnat, Julien Bohné, Jonathan Milgram, Stéphane Gentric, Liming Chen
- **Comment**: Second version (12 pages), under review
- **Journal**: None
- **Summary**: Face recognition (FR) methods report significant performance by adopting the convolutional neural network (CNN) based learning methods. Although CNNs are mostly trained by optimizing the softmax loss, the recent trend shows an improvement of accuracy with different strategies, such as task-specific CNN learning with different loss functions, fine-tuning on target dataset, metric learning and concatenating features from multiple CNNs. Incorporating these tasks obviously requires additional efforts. Moreover, it demotivates the discovery of efficient CNN models for FR which are trained only with identity labels. We focus on this fact and propose an easily trainable and single CNN based FR method. Our CNN model exploits the residual learning framework. Additionally, it uses normalized features to compute the loss. Our extensive experiments show excellent generalization on different datasets. We obtain very competitive and state-of-the-art results on the LFW, IJB-A, YouTube faces and CACD datasets.



### Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach
- **Arxiv ID**: http://arxiv.org/abs/1703.08448v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08448v3)
- **Published**: 2017-03-24 15:05:38+00:00
- **Updated**: 2018-05-28 01:19:51+00:00
- **Authors**: Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan
- **Comment**: Accepted to appear in CVPR 2017 (oral)
- **Journal**: None
- **Summary**: We investigate a principle way to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation problems. Classification networks are only responsive to small and sparse discriminative regions from the object of interest, which deviates from the requirement of the segmentation task that needs to localize dense, interior and integral regions for pixel-wise inference. To mitigate this gap, we propose a new adversarial erasing approach for localizing and expanding object regions progressively. Starting with a single small object region, our proposed approach drives the classification network to sequentially discover new and complement object regions by erasing the current mined regions in an adversarial manner. These localized regions eventually constitute a dense and complete object region for learning semantic segmentation. To further enhance the quality of the discovered regions by adversarial erasing, an online prohibitive segmentation learning approach is developed to collaborate with adversarial erasing by providing auxiliary segmentation supervision modulated by the more reliable classification scores. Despite its apparent simplicity, the proposed approach achieves 55.0% and 55.7% mean Intersection-over-Union (mIoU) scores on PASCAL VOC 2012 val and test sets, which are the new state-of-the-arts.



### Medical Image Retrieval using Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1703.08472v1
- **DOI**: 10.1016/j.neucom.2017.05.025
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08472v1)
- **Published**: 2017-03-24 15:41:01+00:00
- **Updated**: 2017-03-24 15:41:01+00:00
- **Authors**: Adnan Qayyum, Syed Muhammad Anwar, Muhammad Awais, Muhammad Majid
- **Comment**: Submitted to Neurocomputing
- **Journal**: Neurocomputing 2017
- **Summary**: With a widespread use of digital imaging data in hospitals, the size of medical image repositories is increasing rapidly. This causes difficulty in managing and querying these large databases leading to the need of content based medical image retrieval (CBMIR) systems. A major challenge in CBMIR systems is the semantic gap that exists between the low level visual information captured by imaging devices and high level semantic information perceived by human. The efficacy of such systems is more crucial in terms of feature representations that can characterize the high-level information completely. In this paper, we propose a framework of deep learning for CBMIR system by using deep Convolutional Neural Network (CNN) that is trained for classification of medical images. An intermodal dataset that contains twenty four classes and five modalities is used to train the network. The learned features and the classification results are used to retrieve medical images. For retrieval, best results are achieved when class based predictions are used. An average classification accuracy of 99.77% and a mean average precision of 0.69 is achieved for retrieval task. The proposed method is best suited to retrieve multimodal medical images for different body organs.



### Content-Based Image Retrieval Based on Late Fusion of Binary and Local Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1703.08492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08492v1)
- **Published**: 2017-03-24 16:27:57+00:00
- **Updated**: 2017-03-24 16:27:57+00:00
- **Authors**: Nouman Ali, Danish Ali Mazhar, Zeshan Iqbal, Rehan Ashraf, Jawad Ahmed, Farrukh Zeeshan Khan
- **Comment**: None
- **Journal**: International Journal of Computer Science and Information Security
  (IJCSIS), Volume 14, Issue 11, Publication date 2016/11
- **Summary**: One of the challenges in Content-Based Image Retrieval (CBIR) is to reduce the semantic gaps between low-level features and high-level semantic concepts. In CBIR, the images are represented in the feature space and the performance of CBIR depends on the type of selected feature representation. Late fusion also known as visual words integration is applied to enhance the performance of image retrieval. The recent advances in image retrieval diverted the focus of research towards the use of binary descriptors as they are reported computationally efficient. In this paper, we aim to investigate the late fusion of Fast Retina Keypoint (FREAK) and Scale Invariant Feature Transform (SIFT). The late fusion of binary and local descriptor is selected because among binary descriptors, FREAK has shown good results in classification-based problems while SIFT is robust to translation, scaling, rotation and small distortions. The late fusion of FREAK and SIFT integrates the performance of both feature descriptors for an effective image retrieval. Experimental results and comparisons show that the proposed late fusion enhances the performances of image retrieval.



### Multi-stage Multi-recursive-input Fully Convolutional Networks for Neuronal Boundary Detection
- **Arxiv ID**: http://arxiv.org/abs/1703.08493v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08493v2)
- **Published**: 2017-03-24 16:28:57+00:00
- **Updated**: 2017-07-31 16:10:54+00:00
- **Authors**: Wei Shen, Bin Wang, Yuan Jiang, Yan Wang, Alan Yuille
- **Comment**: Accepted by ICCV2017
- **Journal**: None
- **Summary**: In the field of connectomics, neuroscientists seek to identify cortical connectivity comprehensively. Neuronal boundary detection from the Electron Microscopy (EM) images is often done to assist the automatic reconstruction of neuronal circuit. But the segmentation of EM images is a challenging problem, as it requires the detector to be able to detect both filament-like thin and blob-like thick membrane, while suppressing the ambiguous intracellular structure. In this paper, we propose multi-stage multi-recursive-input fully convolutional networks to address this problem. The multiple recursive inputs for one stage, i.e., the multiple side outputs with different receptive field sizes learned from the lower stage, provide multi-scale contextual boundary information for the consecutive learning. This design is biologically-plausible, as it likes a human visual system to compare different possible segmentation solutions to address the ambiguous boundary issue. Our multi-stage networks are trained end-to-end. It achieves promising results on two public available EM segmentation datasets, the mouse piriform cortex dataset and the ISBI 2012 EM dataset.



### Local Deep Neural Networks for Age and Gender Classification
- **Arxiv ID**: http://arxiv.org/abs/1703.08497v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08497v1)
- **Published**: 2017-03-24 16:41:19+00:00
- **Updated**: 2017-03-24 16:41:19+00:00
- **Authors**: Zukang Liao, Stavros Petridis, Maja Pantic
- **Comment**: None
- **Journal**: None
- **Summary**: Local deep neural networks have been recently introduced for gender recognition. Although, they achieve very good performance they are very computationally expensive to train. In this work, we introduce a simplified version of local deep neural networks which significantly reduces the training time. Instead of using hundreds of patches per image, as suggested by the original method, we propose to use 9 overlapping patches per image which cover the entire face region. This results in a much reduced training time, since just 9 patches are extracted per image instead of hundreds, at the expense of a slightly reduced performance. We tested the proposed modified local deep neural networks approach on the LFW and Adience databases for the task of gender and age classification. For both tasks and both databases the performance is up to 1% lower compared to the original version of the algorithm. We have also investigated which patches are more discriminative for age and gender classification. It turns out that the mouth and eyes regions are useful for age classification, whereas just the eye region is useful for gender classification.



### Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer
- **Arxiv ID**: http://arxiv.org/abs/1703.08516v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.1; I.2.10; I.4.7; I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1703.08516v1)
- **Published**: 2017-03-24 17:14:58+00:00
- **Updated**: 2017-03-24 17:14:58+00:00
- **Authors**: Martin Vallières, Emily Kay-Rivest, Léo Jean Perrin, Xavier Liem, Christophe Furstoss, Hugo J. W. L. Aerts, Nader Khaouam, Phuc Felix Nguyen-Tan, Chang-Shu Wang, Khalil Sultanem, Jan Seuntjens, Issam El Naqa
- **Comment**: (1) Paper: 33 pages, 4 figures, 1 table; (2) SUPP info: 41 pages, 7
  figures, 8 tables
- **Journal**: None
- **Summary**: Quantitative extraction of high-dimensional mineable data from medical images is a process known as radiomics. Radiomics is foreseen as an essential prognostic tool for cancer risk assessment and the quantification of intratumoural heterogeneity. In this work, 1615 radiomic features (quantifying tumour image intensity, shape, texture) extracted from pre-treatment FDG-PET and CT images of 300 patients from four different cohorts were analyzed for the risk assessment of locoregional recurrences (LR) and distant metastases (DM) in head-and-neck cancer. Prediction models combining radiomic and clinical variables were constructed via random forests and imbalance-adjustment strategies using two of the four cohorts. Independent validation of the prediction and prognostic performance of the models was carried out on the other two cohorts (LR: AUC = 0.69 and CI = 0.67; DM: AUC = 0.86 and CI = 0.88). Furthermore, the results obtained via Kaplan-Meier analysis demonstrated the potential of radiomics for assessing the risk of specific tumour outcomes using multiple stratification groups. This could have important clinical impact, notably by allowing for a better personalization of chemo-radiation treatments for head-and-neck cancer patients from different risk groups.



### Deep Residual Learning for Instrument Segmentation in Robotic Surgery
- **Arxiv ID**: http://arxiv.org/abs/1703.08580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08580v1)
- **Published**: 2017-03-24 19:43:20+00:00
- **Updated**: 2017-03-24 19:43:20+00:00
- **Authors**: Daniil Pakhomov, Vittal Premachandran, Max Allan, Mahdi Azizian, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Detection, tracking, and pose estimation of surgical instruments are crucial tasks for computer assistance during minimally invasive robotic surgery. In the majority of cases, the first step is the automatic segmentation of surgical tools. Prior work has focused on binary segmentation, where the objective is to label every pixel in an image as tool or background. We improve upon previous work in two major ways. First, we leverage recent techniques such as deep residual learning and dilated convolutions to advance binary-segmentation performance. Second, we extend the approach to multi-class segmentation, which lets us segment different parts of the tool, in addition to background. We demonstrate the performance of this method on the MICCAI Endoscopic Vision Challenge Robotic Instruments dataset.



### Adversarial Examples for Semantic Segmentation and Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1703.08603v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08603v3)
- **Published**: 2017-03-24 21:26:16+00:00
- **Updated**: 2017-07-21 17:27:17+00:00
- **Authors**: Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, Alan Yuille
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: It has been well demonstrated that adversarial examples, i.e., natural images with visually imperceptible perturbations added, generally exist for deep networks to fail on image classification. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difficult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (e.g., the basic target is a pixel or a receptive field in segmentation, and an object proposal in detection), which inspires us to optimize a loss function over a set of pixels/proposals for generating adversarial perturbations. Based on this idea, we propose a novel algorithm named Dense Adversary Generation (DAG), which generates a large family of adversarial examples, and applies to a wide range of state-of-the-art deep networks for segmentation and detection. We also find that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transferability across networks with the same architecture is more significant than in other cases. Besides, summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of black-box adversarial attack.



### Temporal Non-Volume Preserving Approach to Facial Age-Progression and Age-Invariant Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.08617v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08617v1)
- **Published**: 2017-03-24 22:43:05+00:00
- **Updated**: 2017-03-24 22:43:05+00:00
- **Authors**: Chi Nhan Duong, Kha Gia Quach, Khoa Luu, T. Hoang Ngan le, Marios Savvides
- **Comment**: None
- **Journal**: None
- **Summary**: Modeling the long-term facial aging process is extremely challenging due to the presence of large and non-linear variations during the face development stages. In order to efficiently address the problem, this work first decomposes the aging process into multiple short-term stages. Then, a novel generative probabilistic model, named Temporal Non-Volume Preserving (TNVP) transformation, is presented to model the facial aging process at each stage. Unlike Generative Adversarial Networks (GANs), which requires an empirical balance threshold, and Restricted Boltzmann Machines (RBM), an intractable model, our proposed TNVP approach guarantees a tractable density function, exact inference and evaluation for embedding the feature transformations between faces in consecutive stages. Our model shows its advantages not only in capturing the non-linear age related variance in each stage but also producing a smooth synthesis in age progression across faces. Our approach can model any face in the wild provided with only four basic landmark points. Moreover, the structure can be transformed into a deep convolutional network while keeping the advantages of probabilistic models with tractable log-likelihood density estimation. Our method is evaluated in both terms of synthesizing age-progressed faces and cross-age face verification and consistently shows the state-of-the-art results in various face aging databases, i.e. FG-NET, MORPH, AginG Faces in the Wild (AGFW), and Cross-Age Celebrity Dataset (CACD). A large-scale face verification on Megaface challenge 1 is also performed to further show the advantages of our proposed approach.



### AMAT: Medial Axis Transform for Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1703.08628v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08628v2)
- **Published**: 2017-03-24 23:50:52+00:00
- **Updated**: 2017-08-02 23:21:18+00:00
- **Authors**: Stavros Tsogkas, Sven Dickinson
- **Comment**: 10 pages (including references), 5 figures, accepted at ICCV 2017
- **Journal**: None
- **Summary**: We introduce Appearance-MAT (AMAT), a generalization of the medial axis transform for natural images, that is framed as a weighted geometric set cover problem. We make the following contributions: i) we extend previous medial point detection methods for color images, by associating each medial point with a local scale; ii) inspired by the invertibility property of the binary MAT, we also associate each medial point with a local encoding that allows us to invert the AMAT, reconstructing the input image; iii) we describe a clustering scheme that takes advantage of the additional scale and appearance information to group individual points into medial branches, providing a shape decomposition of the underlying image regions. In our experiments, we show state-of-the-art performance in medial point detection on Berkeley Medial AXes (BMAX500), a new dataset of medial axes based on the BSDS500 database, and good generalization on the SK506 and WH-SYMMAX datasets. We also measure the quality of reconstructed images from BMAX500, obtained by inverting their computed AMAT. Our approach delivers significantly better reconstruction quality with respect to three baselines, using just 10% of the image pixels. Our code and annotations are available at https://github.com/tsogkas/amat .



