# Arxiv Papers in cs.CV on 2017-03-15
### Skin lesion segmentation based on preprocessing, thresholding and neural networks
- **Arxiv ID**: http://arxiv.org/abs/1703.04845v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04845v1)
- **Published**: 2017-03-15 00:15:01+00:00
- **Updated**: 2017-03-15 00:15:01+00:00
- **Authors**: Juana M. Gutiérrez-Arriola, Marta Gómez-Álvarez, Victor Osma-Ruiz, Nicolás Sáenz-Lechón, Rubén Fraile
- **Comment**: 4 pages, 4 figures, abstract submitted to participate in the
  challenge ISIC 2017
- **Journal**: None
- **Summary**: This abstract describes the segmentation system used to participate in the challenge ISIC 2017: Skin Lesion Analysis Towards Melanoma Detection. Several preprocessing techniques have been tested for three color representations (RGB, YCbCr and HSV) of 392 images. Results have been used to choose the better preprocessing for each channel. In each case a neural network is trained to predict the Jaccard Index based on object characteristics. The system includes black frames and reference circle detection algorithms but no special treatment is done for hair removal. Segmentation is performed in two steps first the best channel to be segmented is chosen by selecting the best neural network output. If this output does not predict a Jaccard Index over 0.5 a more aggressive preprocessing is performed using open and close morphological operations and the segmentation of the channel that obtains the best output from the neural networks is selected as the lesion.



### Face Recognition using Multi-Modal Low-Rank Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.04853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04853v1)
- **Published**: 2017-03-15 00:38:01+00:00
- **Updated**: 2017-03-15 00:38:01+00:00
- **Authors**: Homa Foroughi, Moein Shakeri, Nilanjan Ray, Hong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition has been widely studied due to its importance in different applications; however, most of the proposed methods fail when face images are occluded or captured under illumination and pose variations. Recently several low-rank dictionary learning methods have been proposed and achieved promising results for noisy observations. While these methods are mostly developed for single-modality scenarios, recent studies demonstrated the advantages of feature fusion from multiple inputs. We propose a multi-modal structured low-rank dictionary learning method for robust face recognition, using raw pixels of face images and their illumination invariant representation. The proposed method learns robust and discriminative representations from contaminated face images, even if there are few training samples with large intra-class variations. Extensive experiments on different datasets validate the superior performance and robustness of our method to severe illumination variations and occlusion.



### Source Camera Identification Based On Content-Adaptive Fusion Network
- **Arxiv ID**: http://arxiv.org/abs/1703.04856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04856v1)
- **Published**: 2017-03-15 00:52:50+00:00
- **Updated**: 2017-03-15 00:52:50+00:00
- **Authors**: Pengpeng Yang, Wei Zhao, Rongrong Ni, Yao Zhao
- **Comment**: This article has been submitted to the 2017 IEEE International
  Conference on Image Processing
- **Journal**: None
- **Summary**: Source camera identification is still a hard task in forensics community, especially for the case of the small query image size. In this paper, we propose a solution to identify the source camera of the small-size images: content-adaptive fusion network. In order to learn better feature representation from the input data, content-adaptive convolutional neural networks(CA-CNN) are constructed. We add a convolutional layer in preprocessing stage. Moreover, with the purpose of capturing more comprehensive information, we parallel three CA-CNNs: CA3-CNN, CA5-CNN, CA7-CNN to get the content-adaptive fusion network. The difference of three CA-CNNs lies in the convolutional kernel size of pre-processing layer. The experimental results show that the proposed method is practicable and satisfactory.



### Robust Non-Rigid Registration with Reweighted Position and Transformation Sparsity
- **Arxiv ID**: http://arxiv.org/abs/1703.04861v2
- **DOI**: 10.1109/TVCG.2018.2832136
- **Categories**: **cs.CV**, cs.CG, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1703.04861v2)
- **Published**: 2017-03-15 01:00:44+00:00
- **Updated**: 2019-06-19 03:21:05+00:00
- **Authors**: Kun Li, Jingyu Yang, Yu-Kun Lai, Daoliang Guo
- **Comment**: IEEE Transactions on Visualization and Computer Graphics
- **Journal**: IEEE Transactions on Visualization and Computer Graphics ( Volume:
  25 , Issue: 6 , June 1 2019 )
- **Summary**: Non-rigid registration is challenging because it is ill-posed with high degrees of freedom and is thus sensitive to noise and outliers. We propose a robust non-rigid registration method using reweighted sparsities on position and transformation to estimate the deformations between 3-D shapes. We formulate the energy function with position and transformation sparsity on both the data term and the smoothness term, and define the smoothness constraint using local rigidity. The double sparsity based non-rigid registration model is enhanced with a reweighting scheme, and solved by transferring the model into four alternately-optimized subproblems which have exact solutions and guaranteed convergence. Experimental results on both public datasets and real scanned datasets show that our method outperforms the state-of-the-art methods and is more robust to noise and outliers than conventional non-rigid registration methods.



### Real-time 3D Human Tracking for Mobile Robots with Multisensors
- **Arxiv ID**: http://arxiv.org/abs/1703.04877v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.04877v1)
- **Published**: 2017-03-15 01:48:30+00:00
- **Updated**: 2017-03-15 01:48:30+00:00
- **Authors**: Mengmeng Wang, Daobilige Su, Lei Shi, Yong Liu, Jaime Valls Miro
- **Comment**: None
- **Journal**: None
- **Summary**: Acquiring the accurate 3-D position of a target person around a robot provides fundamental and valuable information that is applicable to a wide range of robotic tasks, including home service, navigation and entertainment. This paper presents a real-time robotic 3-D human tracking system which combines a monocular camera with an ultrasonic sensor by the extended Kalman filter (EKF). The proposed system consists of three sub-modules: monocular camera sensor tracking model, ultrasonic sensor tracking model and multi-sensor fusion. An improved visual tracking algorithm is presented to provide partial location estimation (2-D). The algorithm is designed to overcome severe occlusions, scale variation, target missing and achieve robust re-detection. The scale accuracy is further enhanced by the estimated 3-D information. An ultrasonic sensor array is employed to provide the range information from the target person to the robot and Gaussian Process Regression is used for partial location estimation (2-D). EKF is adopted to sequentially process multiple, heterogeneous measurements arriving in an asynchronous order from the vision sensor and the ultrasonic sensor separately. In the experiments, the proposed tracking system is tested in both simulation platform and actual mobile robot for various indoor and outdoor scenes. The experimental results show the superior performance of the 3-D tracking system in terms of both the accuracy and robustness.



### Cloud Radiative Effect Study Using Sky Camera
- **Arxiv ID**: http://arxiv.org/abs/1703.05591v1
- **DOI**: None
- **Categories**: **physics.ao-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.05591v1)
- **Published**: 2017-03-15 02:47:50+00:00
- **Updated**: 2017-03-15 02:47:50+00:00
- **Authors**: Soumyabrata Dev, Shilpa Manandhar, Feng Yuan, Yee Hui Lee, Stefan Winkler
- **Comment**: Accepted in Proc. IEEE AP-S Symposium on Antennas and Propagation and
  USNC-URSI Radio Science Meeting, 2017
- **Journal**: None
- **Summary**: The analysis of clouds in the earth's atmosphere is important for a variety of applications, viz. weather reporting, climate forecasting, and solar energy generation. In this paper, we focus our attention on the impact of cloud on the total solar irradiance reaching the earth's surface. We use weather station to record the total solar irradiance. Moreover, we employ collocated ground-based sky camera to automatically compute the instantaneous cloud coverage. We analyze the relationship between measured solar irradiance and computed cloud coverage value, and conclude that higher cloud coverage greatly impacts the total solar irradiance. Such studies will immensely help in solar energy generation and forecasting.



### End-to-end Binary Representation Learning via Direct Binary Embedding
- **Arxiv ID**: http://arxiv.org/abs/1703.04960v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1703.04960v2)
- **Published**: 2017-03-15 06:41:31+00:00
- **Updated**: 2017-06-04 04:44:40+00:00
- **Authors**: Liu Liu, Alireza Rahimpour, Ali Taalimi, Hairong Qi
- **Comment**: Accepted by ICIP'17
- **Journal**: None
- **Summary**: Learning binary representation is essential to large-scale computer vision tasks. Most existing algorithms require a separate quantization constraint to learn effective hashing functions. In this work, we present Direct Binary Embedding (DBE), a simple yet very effective algorithm to learn binary representation in an end-to-end fashion. By appending an ingeniously designed DBE layer to the deep convolutional neural network (DCNN), DBE learns binary code directly from the continuous DBE layer activation without quantization error. By employing the deep residual network (ResNet) as DCNN component, DBE captures rich semantics from images. Furthermore, in the effort of handling multilabel images, we design a joint cross entropy loss that includes both softmax cross entropy and weighted binary cross entropy in consideration of the correlation and independence of labels, respectively. Extensive experiments demonstrate the significant superiority of DBE over state-of-the-art methods on tasks of natural object recognition, image retrieval and image annotation.



### Comparison of the Deep-Learning-Based Automated Segmentation Methods for the Head Sectioned Images of the Virtual Korean Human Project
- **Arxiv ID**: http://arxiv.org/abs/1703.04967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04967v1)
- **Published**: 2017-03-15 06:49:01+00:00
- **Updated**: 2017-03-15 06:49:01+00:00
- **Authors**: Mohammad Eshghi, Holger R. Roth, Masahiro Oda, Min Suk Chung, Kensaku Mori
- **Comment**: Accepted for presentation at the 15th IAPR Conference on Machine
  Vision Applications (MVA2017), Nagoya, Japan
- **Journal**: None
- **Summary**: This paper presents an end-to-end pixelwise fully automated segmentation of the head sectioned images of the Visible Korean Human (VKH) project based on Deep Convolutional Neural Networks (DCNNs). By converting classification networks into Fully Convolutional Networks (FCNs), a coarse prediction map, with smaller size than the original input image, can be created for segmentation purposes. To refine this map and to obtain a dense pixel-wise output, standard FCNs use deconvolution layers to upsample the coarse map. However, upsampling based on deconvolution increases the number of network parameters and causes loss of detail because of interpolation. On the other hand, dilated convolution is a new technique introduced recently that attempts to capture multi-scale contextual information without increasing the network parameters while keeping the resolution of the prediction maps high. We used both a standard FCN and a dilated convolution based FCN for semantic segmentation of the head sectioned images of the VKH dataset. Quantitative results showed approximately 20% improvement in the segmentation accuracy when using FCNs with dilated convolutions.



### What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?
- **Arxiv ID**: http://arxiv.org/abs/1703.04977v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04977v2)
- **Published**: 2017-03-15 07:27:12+00:00
- **Updated**: 2017-10-05 13:04:51+00:00
- **Authors**: Alex Kendall, Yarin Gal
- **Comment**: NIPS 2017
- **Journal**: None
- **Summary**: There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model -- uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.



### Classification of COPD with Multiple Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.04980v1
- **DOI**: 10.1109/ICPR.2014.268
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.04980v1)
- **Published**: 2017-03-15 07:41:49+00:00
- **Updated**: 2017-03-15 07:41:49+00:00
- **Authors**: Veronika Cheplygina, Lauge Sørensen, David M. J. Tax, Jesper Holst Pedersen, Marco Loog, Marleen de Bruijne
- **Comment**: Published at International Conference on Pattern Recognition (ICPR)
  2014
- **Journal**: None
- **Summary**: Chronic obstructive pulmonary disease (COPD) is a lung disease where early detection benefits the survival rate. COPD can be quantified by classifying patches of computed tomography images, and combining patch labels into an overall diagnosis for the image. As labeled patches are often not available, image labels are propagated to the patches, incorrectly labeling healthy patches in COPD patients as being affected by the disease. We approach quantification of COPD from lung images as a multiple instance learning (MIL) problem, which is more suitable for such weakly labeled data. We investigate various MIL assumptions in the context of COPD and show that although a concept region with COPD-related disease patterns is present, considering the whole distribution of lung tissue patches improves the performance. The best method is based on averaging instances and obtains an AUC of 0.742, which is higher than the previously reported best of 0.713 on the same dataset. Using the full training set further increases performance to 0.776, which is significantly higher (DeLong test) than previous results.



### Transfer Learning by Asymmetric Image Weighting for Segmentation across Scanners
- **Arxiv ID**: http://arxiv.org/abs/1703.04981v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.04981v1)
- **Published**: 2017-03-15 07:43:10+00:00
- **Updated**: 2017-03-15 07:43:10+00:00
- **Authors**: Veronika Cheplygina, Annegreet van Opbroek, M. Arfan Ikram, Meike W. Vernooij, Marleen de Bruijne
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised learning has been very successful for automatic segmentation of images from a single scanner. However, several papers report deteriorated performances when using classifiers trained on images from one scanner to segment images from other scanners. We propose a transfer learning classifier that adapts to differences between training and test images. This method uses a weighted ensemble of classifiers trained on individual images. The weight of each classifier is determined by the similarity between its training image and the test image.   We examine three unsupervised similarity measures, which can be used in scenarios where no labeled data from a newly introduced scanner or scanning protocol is available. The measures are based on a divergence, a bag distance, and on estimating the labels with a clustering procedure. These measures are asymmetric. We study whether the asymmetry can improve classification. Out of the three similarity measures, the bag similarity measure is the most robust across different studies and achieves excellent results on four brain tissue segmentation datasets and three white matter lesion segmentation datasets, acquired at different centers and with different scanners and scanning protocols. We show that the asymmetry can indeed be informative, and that computing the similarity from the test image to the training images is more appropriate than the opposite direction.



### Label Stability in Multiple Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.04986v1
- **DOI**: 10.1007/978-3-319-24553-9_66
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.04986v1)
- **Published**: 2017-03-15 07:46:18+00:00
- **Updated**: 2017-03-15 07:46:18+00:00
- **Authors**: Veronika Cheplygina, Lauge Sørensen, David M. J. Tax, Marleen de Bruijne, Marco Loog
- **Comment**: Published at MICCAI 2015
- **Journal**: None
- **Summary**: We address the problem of \emph{instance label stability} in multiple instance learning (MIL) classifiers. These classifiers are trained only on globally annotated images (bags), but often can provide fine-grained annotations for image pixels or patches (instances). This is interesting for computer aided diagnosis (CAD) and other medical image analysis tasks for which only a coarse labeling is provided. Unfortunately, the instance labels may be unstable. This means that a slight change in training data could potentially lead to abnormalities being detected in different parts of the image, which is undesirable from a CAD point of view. Despite MIL gaining popularity in the CAD literature, this issue has not yet been addressed. We investigate the stability of instance labels provided by several MIL classifiers on 5 different datasets, of which 3 are medical image datasets (breast histopathology, diabetic retinopathy and computed tomography lung images). We propose an unsupervised measure to evaluate instance stability, and demonstrate that a performance-stability trade-off can be made when comparing MIL classifiers.



### Zero-Shot Recognition using Dual Visual-Semantic Mapping Paths
- **Arxiv ID**: http://arxiv.org/abs/1703.05002v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05002v2)
- **Published**: 2017-03-15 08:28:58+00:00
- **Updated**: 2017-03-20 01:46:27+00:00
- **Authors**: Yanan Li, Donghui Wang, Huanhang Hu, Yuetan Lin, Yueting Zhuang
- **Comment**: Accepted as a full paper in IEEE Computer Vision and Pattern
  Recognition (CVPR) 2017
- **Journal**: None
- **Summary**: Zero-shot recognition aims to accurately recognize objects of unseen classes by using a shared visual-semantic mapping between the image feature space and the semantic embedding space. This mapping is learned on training data of seen classes and is expected to have transfer ability to unseen classes. In this paper, we tackle this problem by exploiting the intrinsic relationship between the semantic space manifold and the transfer ability of visual-semantic mapping. We formalize their connection and cast zero-shot recognition as a joint optimization problem. Motivated by this, we propose a novel framework for zero-shot recognition, which contains dual visual-semantic mapping paths. Our analysis shows this framework can not only apply prior semantic knowledge to infer underlying semantic manifold in the image feature space, but also generate optimized semantic embedding space, which can enhance the transfer ability of the visual-semantic mapping to unseen classes. The proposed method is evaluated for zero-shot recognition on four benchmark datasets, achieving outstanding results.



### Large Margin Object Tracking with Circulant Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/1703.05020v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05020v2)
- **Published**: 2017-03-15 09:02:11+00:00
- **Updated**: 2017-03-20 02:56:31+00:00
- **Authors**: Mengmeng Wang, Yong Liu, Zeyi Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Structured output support vector machine (SVM) based tracking algorithms have shown favorable performance recently. Nonetheless, the time-consuming candidate sampling and complex optimization limit their real-time applications. In this paper, we propose a novel large margin object tracking method which absorbs the strong discriminative ability from structured output SVM and speeds up by the correlation filter algorithm significantly. Secondly, a multimodal target detection technique is proposed to improve the target localization precision and prevent model drift introduced by similar objects or background noise. Thirdly, we exploit the feedback from high-confidence tracking results to avoid the model corruption problem. We implement two versions of the proposed tracker with the representations from both conventional hand-crafted and deep convolution neural networks (CNNs) based features to validate the strong compatibility of the algorithm. The experimental results demonstrate that the proposed tracker performs superiorly against several state-of-the-art algorithms on the challenging benchmark sequences while runs at speed in excess of 80 frames per second. The source code and experimental results will be made publicly available.



### Learning Rank Reduced Interpolation with Principal Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/1703.05061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05061v1)
- **Published**: 2017-03-15 10:22:21+00:00
- **Updated**: 2017-03-15 10:22:21+00:00
- **Authors**: Matthias Ochs, Henry Bradler, Rudolf Mester
- **Comment**: Accepted at Intelligent Vehicles Symposium (IV), Los Angeles, USA,
  June 2017
- **Journal**: None
- **Summary**: In computer vision most iterative optimization algorithms, both sparse and dense, rely on a coarse and reliable dense initialization to bootstrap their optimization procedure. For example, dense optical flow algorithms profit massively in speed and robustness if they are initialized well in the basin of convergence of the used loss function. The same holds true for methods as sparse feature tracking when initial flow or depth information for new features at arbitrary positions is needed. This makes it extremely important to have techniques at hand that allow to obtain from only very few available measurements a dense but still approximative sketch of a desired 2D structure (e.g. depth maps, optical flow, disparity maps, etc.). The 2D map is regarded as sample from a 2D random process. The method presented here exploits the complete information given by the principal component analysis (PCA) of that process, the principal basis and its prior distribution. The method is able to determine a dense reconstruction from sparse measurement. When facing situations with only very sparse measurements, typically the number of principal components is further reduced which results in a loss of expressiveness of the basis. We overcome this problem and inject prior knowledge in a maximum a posterior (MAP) approach. We test our approach on the KITTI and the virtual KITTI datasets and focus on the interpolation of depth maps for driving scenes. The evaluation of the results show good agreement to the ground truth and are clearly better than results of interpolation by the nearest neighbor method which disregards statistical information.



### Joint Epipolar Tracking (JET): Simultaneous optimization of epipolar geometry and feature correspondences
- **Arxiv ID**: http://arxiv.org/abs/1703.05065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05065v1)
- **Published**: 2017-03-15 10:30:21+00:00
- **Updated**: 2017-03-15 10:30:21+00:00
- **Authors**: Henry Bradler, Matthias Ochs, Rudolf Mester
- **Comment**: Accepted at IEEE Winter Conference on Applications of Computer Vision
  (WACV), Santa Rosa, USA, March 2017
- **Journal**: None
- **Summary**: Traditionally, pose estimation is considered as a two step problem. First, feature correspondences are determined by direct comparison of image patches, or by associating feature descriptors. In a second step, the relative pose and the coordinates of corresponding points are estimated, most often by minimizing the reprojection error (RPE). RPE optimization is based on a loss function that is merely aware of the feature pixel positions but not of the underlying image intensities. In this paper, we propose a sparse direct method which introduces a loss function that allows to simultaneously optimize the unscaled relative pose, as well as the set of feature correspondences directly considering the image intensity values. Furthermore, we show how to integrate statistical prior information on the motion into the optimization process. This constructive inclusion of a Bayesian bias term is particularly efficient in application cases with a strongly predictable (short term) dynamic, e.g. in a driving scenario. In our experiments, we demonstrate that the JET algorithm we propose outperforms the classical reprojection error optimization on two synthetic datasets and on the KITTI dataset. The JET algorithm runs in real-time on a single CPU thread.



### A Data Driven Approach for Compound Figure Separation Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.05105v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05105v2)
- **Published**: 2017-03-15 12:10:36+00:00
- **Updated**: 2017-08-21 23:01:35+00:00
- **Authors**: Satoshi Tsutsui, David Crandall
- **Comment**: Accepted to The International Conference on Document Analysis and
  Recognition (ICDAR) 2017
- **Journal**: None
- **Summary**: A key problem in automatic analysis and understanding of scientific papers is to extract semantic information from non-textual paper components like figures, diagrams, tables, etc. Much of this work requires a very first preprocessing step: decomposing compound multi-part figures into individual subfigures. Previous work in compound figure separation has been based on manually designed features and separation rules, which often fail for less common figure types and layouts. Moreover, few implementations for compound figure decomposition are publicly available.   This paper proposes a data driven approach to separate compound figures using modern deep Convolutional Neural Networks (CNNs) to train the separator in an end-to-end manner. CNNs eliminate the need for manually designing features and separation rules, but require a large amount of annotated training data. We overcome this challenge using transfer learning as well as automatically synthesizing training exemplars. We evaluate our technique on the ImageCLEF Medical dataset, achieving 85.9% accuracy and outperforming previous techniques. We have released our implementation as an easy-to-use Python library, aiming to promote further research in scientific figure mining.



### DeepVel: deep learning for the estimation of horizontal velocities at the solar surface
- **Arxiv ID**: http://arxiv.org/abs/1703.05128v2
- **DOI**: 10.1051/0004-6361/201730783
- **Categories**: **astro-ph.SR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.05128v2)
- **Published**: 2017-03-15 12:49:07+00:00
- **Updated**: 2017-04-24 16:03:35+00:00
- **Authors**: A. Asensio Ramos, I. S. Requerey, N. Vitas
- **Comment**: 9 pages, 5 figures, accepted for publication in A&A
- **Journal**: None
- **Summary**: Many phenomena taking place in the solar photosphere are controlled by plasma motions. Although the line-of-sight component of the velocity can be estimated using the Doppler effect, we do not have direct spectroscopic access to the components that are perpendicular to the line-of-sight. These components are typically estimated using methods based on local correlation tracking. We have designed DeepVel, an end-to-end deep neural network that produces an estimation of the velocity at every single pixel and at every time step and at three different heights in the atmosphere from just two consecutive continuum images. We confront DeepVel with local correlation tracking, pointing out that they give very similar results in the time- and spatially-averaged cases. We use the network to study the evolution in height of the horizontal velocity field in fragmenting granules, supporting the buoyancy-braking mechanism for the formation of integranular lanes in these granules. We also show that DeepVel can capture very small vortices, so that we can potentially expand the scaling cascade of vortices to very small sizes and durations.



### Block Compressive Sensing of Image and Video with Nonlocal Lagrangian Multiplier and Patch-based Sparse Representation
- **Arxiv ID**: http://arxiv.org/abs/1703.05130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05130v1)
- **Published**: 2017-03-15 12:56:25+00:00
- **Updated**: 2017-03-15 12:56:25+00:00
- **Authors**: Trinh Van Chien, Khanh Quoc Dinh, Byeungwoo Jeon, Martin Burger
- **Comment**: 30 pages, 13 figures, 2 tables; Accepted by Elsevier Signal
  Processing:Image Communication
- **Journal**: None
- **Summary**: Although block compressive sensing (BCS) makes it tractable to sense large-sized images and video, its recovery performance has yet to be significantly improved because its recovered images or video usually suffer from blurred edges, loss of details, and high-frequency oscillatory artifacts, especially at a low subrate. This paper addresses these problems by designing a modified total variation technique that employs multi-block gradient processing, a denoised Lagrangian multiplier, and patch-based sparse representation. In the case of video, the proposed recovery method is able to exploit both spatial and temporal similarities. Simulation results confirm the improved performance of the proposed method for compressive sensing of images and video in terms of both objective and subjective qualities.



### Random Forests and VGG-NET: An Algorithm for the ISIC 2017 Skin Lesion Classification Challenge
- **Arxiv ID**: http://arxiv.org/abs/1703.05148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05148v1)
- **Published**: 2017-03-15 13:33:55+00:00
- **Updated**: 2017-03-15 13:33:55+00:00
- **Authors**: Songtao Guo, Yixin Luo, Yanzhi Song
- **Comment**: ISIC2017
- **Journal**: None
- **Summary**: This manuscript briefly describes an algorithm developed for the ISIC 2017 Skin Lesion Classification Competition. In this task, participants are asked to complete two independent binary image classification tasks that involve three unique diagnoses of skin lesions (melanoma, nevus, and seborrheic keratosis). In the first binary classification task, participants are asked to distinguish between (a) melanoma and (b) nevus and seborrheic keratosis. In the second binary classification task, participants are asked to distinguish between (a) seborrheic keratosis and (b) nevus and melanoma. The other phases of the competition are not considered. Our proposed algorithm consists of three steps: preprocessing, classification using VGG-NET and Random Forests, and calculation of a final score.



### Real-Time Panoramic Tracking for Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/1703.05161v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05161v2)
- **Published**: 2017-03-15 14:03:47+00:00
- **Updated**: 2017-03-21 13:08:49+00:00
- **Authors**: Christian Reinbacher, Gottfried Munda, Thomas Pock
- **Comment**: Accepted to International Conference on Computational Photography
  2017
- **Journal**: None
- **Summary**: Event cameras are a paradigm shift in camera technology. Instead of full frames, the sensor captures a sparse set of events caused by intensity changes. Since only the changes are transferred, those cameras are able to capture quick movements of objects in the scene or of the camera itself. In this work we propose a novel method to perform camera tracking of event cameras in a panoramic setting with three degrees of freedom. We propose a direct camera tracking formulation, similar to state-of-the-art in visual odometry. We show that the minimal information needed for simultaneous tracking and mapping is the spatial position of events, without using the appearance of the imaged scene point. We verify the robustness to fast camera movements and dynamic objects in the scene on a recently proposed dataset and self-recorded sequences.



### Automatic skin lesion segmentation with fully convolutional-deconvolutional networks
- **Arxiv ID**: http://arxiv.org/abs/1703.05165v2
- **DOI**: 10.1109/JBHI.2017.2787487
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05165v2)
- **Published**: 2017-03-15 14:18:23+00:00
- **Updated**: 2017-09-28 02:26:32+00:00
- **Authors**: Yading Yuan
- **Comment**: ISIC2017 challenge, 4 pages
- **Journal**: IEEE Journal of Biomedical and Health Informatics, 2018
- **Summary**: This paper summarizes our method and validation results for the ISBI Challenge 2017 - Skin Lesion Analysis Towards Melanoma Detection - Part I: Lesion Segmentation



### Learning to Discover Cross-Domain Relations with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.05192v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05192v2)
- **Published**: 2017-03-15 14:53:15+00:00
- **Updated**: 2017-05-15 05:04:38+00:00
- **Authors**: Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, Jiwon Kim
- **Comment**: Accepted to International Conference on Machine Learning (ICML) 2017
- **Journal**: None
- **Summary**: While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity. Source code for official implementation is publicly available https://github.com/SKTBrain/DiscoGAN



### Texture segmentation with Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.05230v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05230v1)
- **Published**: 2017-03-15 16:14:52+00:00
- **Updated**: 2017-03-15 16:14:52+00:00
- **Authors**: Vincent Andrearczyk, Paul F. Whelan
- **Comment**: 13 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: In the last decade, deep learning has contributed to advances in a wide range computer vision tasks including texture analysis. This paper explores a new approach for texture segmentation using deep convolutional neural networks, sharing important ideas with classic filter bank based texture segmentation methods. Several methods are developed to train Fully Convolutional Networks to segment textures in various applications. We show in particular that these networks can learn to recognize and segment a type of texture, e.g. wood and grass from texture recognition datasets (no training segmentation). We demonstrate that Fully Convolutional Networks can learn from repetitive patterns to segment a particular texture from a single image or even a part of an image. We take advantage of these findings to develop a method that is evaluated on a series of supervised and unsupervised experiments and improve the state of the art on the Prague texture segmentation datasets.



### Transfer Learning for Melanoma Detection: Participation in ISIC 2017 Skin Lesion Classification Challenge
- **Arxiv ID**: http://arxiv.org/abs/1703.05235v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05235v1)
- **Published**: 2017-03-15 16:31:16+00:00
- **Updated**: 2017-03-15 16:31:16+00:00
- **Authors**: Dennis H. Murphree, Che Ngufor
- **Comment**: None
- **Journal**: None
- **Summary**: This manuscript describes our participation in the International Skin Imaging Collaboration's 2017 Skin Lesion Analysis Towards Melanoma Detection competition. We participated in Part 3: Lesion Classification. The two stated goals of this binary image classification challenge were to distinguish between (a) melanoma and (b) nevus and seborrheic keratosis, followed by distinguishing between (a) seborrheic keratosis and (b) nevus and melanoma. We chose a deep neural network approach with a transfer learning strategy, using a pre-trained Inception V3 network as both a feature extractor to provide input for a multi-layer perceptron as well as fine-tuning an augmented Inception network. This approach yielded validation set AUC's of 0.84 on the second task and 0.76 on the first task, for an average AUC of 0.80. We joined the competition unfortunately late, and we look forward to improving on these results.



### A Hybrid Supervised-unsupervised Method on Image Topic Visualization with Convolutional Neural Network and LDA
- **Arxiv ID**: http://arxiv.org/abs/1703.05243v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05243v2)
- **Published**: 2017-03-15 16:35:31+00:00
- **Updated**: 2017-04-09 17:42:47+00:00
- **Authors**: Kai Zhen, Mridul Birla, David Crandall, Bingjing Zhang, Judy Qiu
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: Given the progress in image recognition with recent data driven paradigms, it's still expensive to manually label a large training data to fit a convolutional neural network (CNN) model. This paper proposes a hybrid supervised-unsupervised method combining a pre-trained AlexNet with Latent Dirichlet Allocation (LDA) to extract image topics from both an unlabeled life-logging dataset and the COCO dataset. We generate the bag-of-words representations of an egocentric dataset from the softmax layer of AlexNet and use LDA to visualize the subject's living genre with duplicated images. We use a subset of COCO on 4 categories as ground truth, and define consistent rate to quantitatively analyze the performance of the method, it achieves 84% for consistent rate on average comparing to 18.75% from a raw CNN model. The method is capable of detecting false labels and multi-labels from COCO dataset. For scalability test, parallelization experiments are conducted with Harp-LDA on a Intel Knights Landing cluster: to extract 1,000 topic assignments for 241,035 COCO images, it takes 10 minutes with 60 threads.



### A clever elimination strategy for efficient minimal solvers
- **Arxiv ID**: http://arxiv.org/abs/1703.05289v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SC
- **Links**: [PDF](http://arxiv.org/pdf/1703.05289v1)
- **Published**: 2017-03-15 17:44:37+00:00
- **Updated**: 2017-03-15 17:44:37+00:00
- **Authors**: Zuzana Kukelova, Joe Kileel, Bernd Sturmfels, Tomas Pajdla
- **Comment**: 13 pages, 7 figures
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition 2017
  (CVPR 2017)
- **Summary**: We present a new insight into the systematic generation of minimal solvers in computer vision, which leads to smaller and faster solvers. Many minimal problem formulations are coupled sets of linear and polynomial equations where image measurements enter the linear equations only. We show that it is useful to solve such systems by first eliminating all the unknowns that do not appear in the linear equations and then extending solutions to the rest of unknowns. This can be generalized to fully non-linear systems by linearization via lifting. We demonstrate that this approach leads to more efficient solvers in three problems of partially calibrated relative camera pose computation with unknown focal length and/or radial distortion. Our approach also generates new interesting constraints on the fundamental matrices of partially calibrated cameras, which were not known before.



### Illuminant Estimation using Ensembles of Multivariate Regression Trees
- **Arxiv ID**: http://arxiv.org/abs/1703.05354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05354v1)
- **Published**: 2017-03-15 18:59:46+00:00
- **Updated**: 2017-03-15 18:59:46+00:00
- **Authors**: Peter van Beek, R. Wayne Oldford
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: White balancing is a fundamental step in the image processing pipeline. The process involves estimating the chromaticity of the illuminant or light source and using the estimate to correct the image to remove any color cast. Given the importance of the problem, there has been much previous work on illuminant estimation. Recently, an approach based on ensembles of univariate regression trees that are fit using the squared-error loss function has been proposed and shown to give excellent performance. In this paper, we show that a simpler and more accurate ensemble model can be learned by (i) using multivariate regression trees to take into account that the chromaticity components of the illuminant are correlated and constrained, and (ii) fitting each tree by directly minimizing a loss function of interest---such as recovery angular error or reproduction angular error---rather than indirectly using the squared-error loss function as a surrogate. We show empirically that overall our method leads to improved performance on diverse image sets.



### 3D Vision Guided Robotic Charging Station for Electric and Plug-in Hybrid Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1703.05381v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.05381v1)
- **Published**: 2017-03-15 20:49:14+00:00
- **Updated**: 2017-03-15 20:49:14+00:00
- **Authors**: Justinas Miseikis, Matthias Ruther, Bernhard Walzel, Mario Hirz, Helmut Brunner
- **Comment**: 6 pages, 8 figures, OAGM and ARW Joint Workshop 2017 on Vision,
  Automation and Robotics
- **Journal**: None
- **Summary**: Electric vehicles (EVs) and plug-in hybrid vehicles (PHEVs) are rapidly gaining popularity on our roads. Besides a comparatively high purchasing price, the main two problems limiting their use are the short driving range and inconvenient charging process. In this paper we address the following by presenting an automatic robot-based charging station with 3D vision guidance for plugging and unplugging the charger. First of all, the whole system concept consisting of a 3D vision system, an UR10 robot and a charging station is presented. Then we show the shape-based matching methods used to successfully identify and get the exact pose of the charging port. The same approach is used to calibrate the camera-robot system by using just known structure of the connector plug and no additional markers. Finally, a three-step robot motion planning procedure for plug-in is presented and functionality is demonstrated in a series of successful experiments.



### Convolutional Low-Resolution Fine-Grained Classification
- **Arxiv ID**: http://arxiv.org/abs/1703.05393v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05393v3)
- **Published**: 2017-03-15 21:40:48+00:00
- **Updated**: 2017-10-16 11:53:48+00:00
- **Authors**: Dingding Cai, Ke Chen, Yanlin Qian, Joni-Kristian Kämäräinen
- **Comment**: None
- **Journal**: None
- **Summary**: Successful fine-grained image classification methods learn subtle details between visually similar (sub-)classes, but the problem becomes significantly more challenging if the details are missing due to low resolution. Encouraged by the recent success of Convolutional Neural Network (CNN) architectures in image classification, we propose a novel resolution-aware deep model which combines convolutional image super-resolution and convolutional fine-grained classification into a single model in an end-to-end manner. Extensive experiments on the Stanford Cars and Caltech-UCSD Birds 200-2011 benchmarks demonstrate that the proposed model consistently performs better than conventional convolutional net on classifying fine-grained object classes in low-resolution images.



