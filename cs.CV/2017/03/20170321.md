# Arxiv Papers in cs.CV on 2017-03-21
### Recurrent Topic-Transition GAN for Visual Paragraph Generation
- **Arxiv ID**: http://arxiv.org/abs/1703.07022v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.07022v2)
- **Published**: 2017-03-21 01:43:12+00:00
- **Updated**: 2017-03-23 20:06:15+00:00
- **Authors**: Xiaodan Liang, Zhiting Hu, Hao Zhang, Chuang Gan, Eric P. Xing
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: A natural image usually conveys rich semantic content and can be viewed from different angles. Existing image description methods are largely restricted by small sets of biased visual paragraph annotations, and fail to cover rich underlying semantics. In this paper, we investigate a semi-supervised paragraph generative framework that is able to synthesize diverse and semantically coherent paragraph descriptions by reasoning over local semantic regions and exploiting linguistic knowledge. The proposed Recurrent Topic-Transition Generative Adversarial Network (RTT-GAN) builds an adversarial framework between a structured paragraph generator and multi-level paragraph discriminators. The paragraph generator generates sentences recurrently by incorporating region-based visual and language attention mechanisms at each step. The quality of generated paragraph sentences is assessed by multi-level adversarial discriminators from two aspects, namely, plausibility at sentence level and topic-transition coherence at paragraph level. The joint adversarial training of RTT-GAN drives the model to generate realistic paragraphs with smooth logical transition between sentence topics. Extensive quantitative experiments on image and video paragraph datasets demonstrate the effectiveness of our RTT-GAN in both supervised and semi-supervised settings. Qualitative results on telling diverse stories for an image also verify the interpretability of RTT-GAN.



### Encouraging LSTMs to Anticipate Actions Very Early
- **Arxiv ID**: http://arxiv.org/abs/1703.07023v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07023v3)
- **Published**: 2017-03-21 01:50:53+00:00
- **Updated**: 2017-08-14 01:59:04+00:00
- **Authors**: Mohammad Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Basura Fernando, Lars Petersson, Lars Andersson
- **Comment**: 13 Pages, 7 Figures, 11 Tables. Accepted in ICCV 2017. arXiv admin
  note: text overlap with arXiv:1611.05520
- **Journal**: None
- **Summary**: In contrast to the widely studied problem of recognizing an action given a complete sequence, action anticipation aims to identify the action from only partially available videos. As such, it is therefore key to the success of computer vision applications requiring to react as early as possible, such as autonomous navigation. In this paper, we propose a new action anticipation method that achieves high prediction accuracy even in the presence of a very small percentage of a video sequence. To this end, we develop a multi-stage LSTM architecture that leverages context-aware and action-aware features, and introduce a novel loss function that encourages the model to predict the correct class as early as possible. Our experiments on standard benchmark datasets evidence the benefits of our approach; We outperform the state-of-the-art action anticipation methods for early prediction by a relative increase in accuracy of 22.0% on JHMDB-21, 14.0% on UT-Interaction and 49.9% on UCF-101.



### Cross-modal Deep Metric Learning with Multi-task Regularization
- **Arxiv ID**: http://arxiv.org/abs/1703.07026v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.07026v2)
- **Published**: 2017-03-21 02:04:30+00:00
- **Updated**: 2017-04-05 05:02:20+00:00
- **Authors**: Xin Huang, Yuxin Peng
- **Comment**: Revision: Added reference [7] 6 pages, 1 figure, to appear in the
  proceedings of the IEEE International Conference on Multimedia and Expo
  (ICME), Jul 10, 2017 - Jul 14, 2017, Hong Kong, Hong Kong
- **Journal**: None
- **Summary**: DNN-based cross-modal retrieval has become a research hotspot, by which users can search results across various modalities like image and text. However, existing methods mainly focus on the pairwise correlation and reconstruction error of labeled data. They ignore the semantically similar and dissimilar constraints between different modalities, and cannot take advantage of unlabeled data. This paper proposes Cross-modal Deep Metric Learning with Multi-task Regularization (CDMLMR), which integrates quadruplet ranking loss and semi-supervised contrastive loss for modeling cross-modal semantic similarity in a unified multi-task learning architecture. The quadruplet ranking loss can model the semantically similar and dissimilar constraints to preserve cross-modal relative similarity ranking information. The semi-supervised contrastive loss is able to maximize the semantic similarity on both labeled and unlabeled data. Compared to the existing methods, CDMLMR exploits not only the similarity ranking information but also unlabeled cross-modal data, and thus boosts cross-modal retrieval accuracy.



### High-Resolution Breast Cancer Screening with Multi-View Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.07047v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.07047v3)
- **Published**: 2017-03-21 04:11:13+00:00
- **Updated**: 2018-06-28 01:21:51+00:00
- **Authors**: Krzysztof J. Geras, Stacey Wolfson, Yiqiu Shen, Nan Wu, S. Gene Kim, Eric Kim, Laura Heacock, Ujas Parikh, Linda Moy, Kyunghyun Cho
- **Comment**: None
- **Journal**: None
- **Summary**: Advances in deep learning for natural images have prompted a surge of interest in applying similar techniques to medical images. The majority of the initial attempts focused on replacing the input of a deep convolutional neural network with a medical image, which does not take into consideration the fundamental differences between these two types of images. Specifically, fine details are necessary for detection in medical images, unlike in natural images where coarse structures matter most. This difference makes it inadequate to use the existing network architectures developed for natural images, because they work on heavily downscaled images to reduce the memory requirements. This hides details necessary to make accurate predictions. Additionally, a single exam in medical imaging often comes with a set of views which must be fused in order to reach a correct conclusion. In our work, we propose to use a multi-view deep convolutional neural network that handles a set of high-resolution medical images. We evaluate it on large-scale mammography-based breast cancer screening (BI-RADS prediction) using 886,000 images. We focus on investigating the impact of the training set size and image size on the prediction accuracy. Our results highlight that performance increases with the size of training set, and that the best performance can only be achieved using the original resolution. In the reader study, performed on a random subset of the test set, we confirmed the efficacy of our model, which achieved performance comparable to a committee of radiologists when presented with the same data.



### On the Interplay between Strong Regularity and Graph Densification
- **Arxiv ID**: http://arxiv.org/abs/1703.07107v1
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.07107v1)
- **Published**: 2017-03-21 09:37:16+00:00
- **Updated**: 2017-03-21 09:37:16+00:00
- **Authors**: Marco Fiorucci, Alessandro Torcinovich, Manuel Curado, Francisco Escolano, Marcello Pelillo
- **Comment**: GbR2017 to appear in Lecture Notes in Computer Science (LNCS)
- **Journal**: None
- **Summary**: In this paper we analyze the practical implications of Szemer\'edi's regularity lemma in the preservation of metric information contained in large graphs. To this end, we present a heuristic algorithm to find regular partitions. Our experiments show that this method is quite robust to the natural sparsification of proximity graphs. In addition, this robustness can be enforced by graph densification.



### Knowledge distillation using unlabeled mismatched images
- **Arxiv ID**: http://arxiv.org/abs/1703.07131v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.07131v1)
- **Published**: 2017-03-21 10:34:59+00:00
- **Updated**: 2017-03-21 10:34:59+00:00
- **Authors**: Mandar Kulkarni, Kalpesh Patil, Shirish Karande
- **Comment**: None
- **Journal**: None
- **Summary**: Current approaches for Knowledge Distillation (KD) either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance. Our examples include use of various datasets for stimulating MNIST and CIFAR teachers.



### Deep generative-contrastive networks for facial expression recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.07140v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07140v3)
- **Published**: 2017-03-21 10:52:02+00:00
- **Updated**: 2019-05-08 06:41:05+00:00
- **Authors**: Youngsung Kim, ByungIn Yoo, Youngjun Kwak, Changkyu Choi, Junmo Kim
- **Comment**: None
- **Journal**: None
- **Summary**: As the expressive depth of an emotional face differs with individuals or expressions, recognizing an expression using a single facial image at a moment is difficult. A relative expression of a query face compared to a reference face might alleviate this difficulty. In this paper, we propose to utilize contrastive representation that embeds a distinctive expressive factor for a discriminative purpose. The contrastive representation is calculated at the embedding layer of deep networks by comparing a given (query) image with the reference image. We attempt to utilize a generative reference image that is estimated based on the given image. Consequently, we deploy deep neural networks that embed a combination of a generative model, a contrastive model, and a discriminative model with an end-to-end training manner. In our proposed networks, we attempt to disentangle a facial expressive factor in two steps including learning of a generator network and a contrastive encoder network. We conducted extensive experiments on publicly available face expression databases (CK+, MMI, Oulu-CASIA, and in-the-wild databases) that have been widely adopted in the recent literatures. The proposed method outperforms the known state-of-the art methods in terms of the recognition accuracy.



### Proposal Flow: Semantic Correspondences from Object Proposals
- **Arxiv ID**: http://arxiv.org/abs/1703.07144v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07144v1)
- **Published**: 2017-03-21 10:57:27+00:00
- **Updated**: 2017-03-21 10:57:27+00:00
- **Authors**: Bumsub Ham, Minsu Cho, Cordelia Schmid, Jean Ponce
- **Comment**: arXiv admin note: text overlap with arXiv:1511.05065
- **Journal**: None
- **Summary**: Finding image correspondences remains a challenging problem in the presence of intra-class variations and large changes in scene layout. Semantic flow methods are designed to handle images depicting different instances of the same object or scene category. We introduce a novel approach to semantic flow, dubbed proposal flow, that establishes reliable correspondences using object proposals. Unlike prevailing semantic flow approaches that operate on pixels or regularly sampled local regions, proposal flow benefits from the characteristics of modern object proposals, that exhibit high repeatability at multiple scales, and can take advantage of both local and geometric consistency constraints among proposals. We also show that the corresponding sparse proposal flow can effectively be transformed into a conventional dense flow field. We introduce two new challenging datasets that can be used to evaluate both general semantic flow techniques and region-based approaches such as proposal flow. We use these benchmarks to compare different matching algorithms, object proposals, and region features within proposal flow, to the state of the art in semantic flow. This comparison, along with experiments on standard datasets, demonstrates that proposal flow significantly outperforms existing semantic flow methods in various settings.



### GP-GAN: Towards Realistic High-Resolution Image Blending
- **Arxiv ID**: http://arxiv.org/abs/1703.07195v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07195v3)
- **Published**: 2017-03-21 12:57:58+00:00
- **Updated**: 2019-08-05 09:20:50+00:00
- **Authors**: Huikai Wu, Shuai Zheng, Junge Zhang, Kaiqi Huang
- **Comment**: Accepted by ACMMM 2019. The source code is available in
  https://github.com/wuhuikai/GP-GAN and there's also an online demo in
  http://wuhuikai.me/DeepJS
- **Journal**: None
- **Summary**: It is common but challenging to address high-resolution image blending in the automatic photo editing application. In this paper, we would like to focus on solving the problem of high-resolution image blending, where the composite images are provided. We propose a framework called Gaussian-Poisson Generative Adversarial Network (GP-GAN) to leverage the strengths of the classical gradient-based approach and Generative Adversarial Networks. To the best of our knowledge, it's the first work that explores the capability of GANs in high-resolution image blending task. Concretely, we propose Gaussian-Poisson Equation to formulate the high-resolution image blending problem, which is a joint optimization constrained by the gradient and color information. Inspired by the prior works, we obtain gradient information via applying gradient filters. To generate the color information, we propose a Blending GAN to learn the mapping between the composite images and the well-blended ones. Compared to the alternative methods, our approach can deliver high-resolution, realistic images with fewer bleedings and unpleasant artifacts. Experiments confirm that our approach achieves the state-of-the-art performance on Transient Attributes dataset. A user study on Amazon Mechanical Turk finds that the majority of workers are in favor of the proposed method.



### INTEL-TUT Dataset for Camera Invariant Color Constancy Research
- **Arxiv ID**: http://arxiv.org/abs/1703.09778v2
- **DOI**: 10.1109/TIP.2017.2764264
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09778v2)
- **Published**: 2017-03-21 13:07:45+00:00
- **Updated**: 2017-03-31 07:29:34+00:00
- **Authors**: Caglar Aytekin, Jarno Nikkanen, Moncef Gabbouj
- **Comment**: Download Link for the Dataset:
  https://etsin.avointiede.fi/dataset/urn-nbn-fi-csc-kata20170321084219004008
  Submission Info: Submitted to IEEE TIP
- **Journal**: Published in: IEEE Transactions on Image Processing ( Volume: 27,
  Issue: 2, Feb. 2018 )
- **Summary**: In this paper, we provide a novel dataset designed for camera invariant color constancy research. Camera invariance corresponds to the robustness of an algorithm's performance when run on images of the same scene taken by different cameras. Accordingly, images in the database correspond to several lab and field scenes each of which are captured by three different cameras with minimal registration errors. The lab scenes are also captured under five different illuminations. The spectral responses of cameras and the spectral power distributions of the lab light sources are also provided, as they may prove beneficial for training future algorithms to achieve color constancy. For a fair evaluation of future methods, we provide guidelines for supervised methods with indicated training, validation and testing partitions. Accordingly, we evaluate a recently proposed convolutional neural network based color constancy algorithm as a baseline for future research. As a side contribution, this dataset also includes images taken by a mobile camera with color shading corrected and uncorrected results. This allows research on the effect of color shading as well.



### Improving Person Re-identification by Attribute and Identity Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.07220v3
- **DOI**: 10.1016/j.patcog.2019.06.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07220v3)
- **Published**: 2017-03-21 13:44:42+00:00
- **Updated**: 2019-06-09 18:22:47+00:00
- **Authors**: Yutian Lin, Liang Zheng, Zhedong Zheng, Yu Wu, Zhilan Hu, Chenggang Yan, Yi Yang
- **Comment**: Accepted to Pattern Recognition (PR)
- **Journal**: None
- **Summary**: Person re-identification (re-ID) and attribute recognition share a common target at learning pedestrian descriptions. Their difference consists in the granularity. Most existing re-ID methods only take identity labels of pedestrians into consideration. However, we find the attributes, containing detailed local descriptions, are beneficial in allowing the re-ID model to learn more discriminative feature representations. In this paper, based on the complementarity of attribute labels and ID labels, we propose an attribute-person recognition (APR) network, a multi-task network which learns a re-ID embedding and at the same time predicts pedestrian attributes. We manually annotate attribute labels for two large-scale re-ID datasets, and systematically investigate how person re-ID and attribute recognition benefit from each other. In addition, we re-weight the attribute predictions considering the dependencies and correlations among the attributes. The experimental results on two large-scale re-ID benchmarks demonstrate that by learning a more discriminative representation, APR achieves competitive re-ID performance compared with the state-of-the-art methods. We use APR to speed up the retrieval process by ten times with a minor accuracy drop of 2.92% on Market-1501. Besides, we also apply APR on the attribute recognition task and demonstrate improvement over the baselines.



### ZM-Net: Real-time Zero-shot Image Manipulation Network
- **Arxiv ID**: http://arxiv.org/abs/1703.07255v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.07255v2)
- **Published**: 2017-03-21 15:01:59+00:00
- **Updated**: 2017-03-22 17:08:40+00:00
- **Authors**: Hao Wang, Xiaodan Liang, Hao Zhang, Dit-Yan Yeung, Eric P. Xing
- **Comment**: None
- **Journal**: None
- **Summary**: Many problems in image processing and computer vision (e.g. colorization, style transfer) can be posed as 'manipulating' an input image into a corresponding output image given a user-specified guiding signal. A holy-grail solution towards generic image manipulation should be able to efficiently alter an input image with any personalized signals (even signals unseen during training), such as diverse paintings and arbitrary descriptive attributes. However, existing methods are either inefficient to simultaneously process multiple signals (let alone generalize to unseen signals), or unable to handle signals from other modalities. In this paper, we make the first attempt to address the zero-shot image manipulation task. We cast this problem as manipulating an input image according to a parametric model whose key parameters can be conditionally generated from any guiding signal (even unseen ones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a fully-differentiable architecture that jointly optimizes an image-transformation network (TNet) and a parameter network (PNet). The PNet learns to generate key transformation parameters for the TNet given any guiding signal while the TNet performs fast zero-shot image manipulation according to both signal-dependent parameters from the PNet and signal-invariant parameters from the TNet itself. Extensive experiments show that our ZM-Net can perform high-quality image manipulation conditioned on different forms of guiding signals (e.g. style images and attributes) in real-time (tens of milliseconds per image) even for unseen signals. Moreover, a large-scale style dataset with over 20,000 style images is also constructed to promote further research.



### On the use of convolutional neural networks for robust classification of multiple fingerprint captures
- **Arxiv ID**: http://arxiv.org/abs/1703.07270v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07270v3)
- **Published**: 2017-03-21 15:22:26+00:00
- **Updated**: 2017-05-15 09:17:35+00:00
- **Authors**: Daniel Peralta, Isaac Triguero, Salvador García, Yvan Saeys, Jose M. Benitez, Francisco Herrera
- **Comment**: None
- **Journal**: None
- **Summary**: Fingerprint classification is one of the most common approaches to accelerate the identification in large databases of fingerprints. Fingerprints are grouped into disjoint classes, so that an input fingerprint is compared only with those belonging to the predicted class, reducing the penetration rate of the search. The classification procedure usually starts by the extraction of features from the fingerprint image, frequently based on visual characteristics. In this work, we propose an approach to fingerprint classification using convolutional neural networks, which avoid the necessity of an explicit feature extraction process by incorporating the image processing within the training of the classifier. Furthermore, such an approach is able to predict a class even for low-quality fingerprints that are rejected by commonly used algorithms, such as FingerCode. The study gives special importance to the robustness of the classification for different impressions of the same fingerprint, aiming to minimize the penetration in the database. In our experiments, convolutional neural networks yielded better accuracy and penetration rate than state-of-the-art classifiers based on explicit feature extraction. The tested networks also improved on the runtime, as a result of the joint optimization of both feature extraction and classification.



### License Plate Detection and Recognition Using Deeply Learned Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.07330v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07330v2)
- **Published**: 2017-03-21 17:32:31+00:00
- **Updated**: 2017-03-28 19:41:30+00:00
- **Authors**: Syed Zain Masood, Guang Shu, Afshin Dehghan, Enrique G. Ortiz
- **Comment**: 10 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: This work details Sighthounds fully automated license plate detection and recognition system. The core technology of the system is built using a sequence of deep Convolutional Neural Networks (CNNs) interlaced with accurate and efficient algorithms. The CNNs are trained and fine-tuned so that they are robust under different conditions (e.g. variations in pose, lighting, occlusion, etc.) and can work across a variety of license plate templates (e.g. sizes, backgrounds, fonts, etc). For quantitative analysis, we show that our system outperforms the leading license plate detection and recognition technology i.e. ALPR on several benchmarks. Our system is available to developers through the Sighthound Cloud API at https://www.sighthound.com/products/cloud



### How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)
- **Arxiv ID**: http://arxiv.org/abs/1703.07332v3
- **DOI**: 10.1109/ICCV.2017.116
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.07332v3)
- **Published**: 2017-03-21 17:37:36+00:00
- **Updated**: 2017-09-07 16:21:37+00:00
- **Authors**: Adrian Bulat, Georgios Tzimiropoulos
- **Comment**: accepted to ICCV 2017
- **Journal**: None
- **Summary**: This paper investigates how far a very deep neural network is from attaining close to saturating performance on existing 2D and 3D face alignment datasets. To this end, we make the following 5 contributions: (a) we construct, for the first time, a very strong baseline by combining a state-of-the-art architecture for landmark localization with a state-of-the-art residual block, train it on a very large yet synthetically expanded 2D facial landmark dataset and finally evaluate it on all other 2D facial landmark datasets. (b) We create a guided by 2D landmarks network which converts 2D landmark annotations to 3D and unifies all existing datasets, leading to the creation of LS3D-W, the largest and most challenging 3D facial landmark dataset to date ~230,000 images. (c) Following that, we train a neural network for 3D face alignment and evaluate it on the newly introduced LS3D-W. (d) We further look into the effect of all "traditional" factors affecting face alignment performance like large pose, initialization and resolution, and introduce a "new" one, namely the size of the network. (e) We show that both 2D and 3D face alignment networks achieve performance of remarkable accuracy which is probably close to saturating the datasets used. Training and testing code as well as the dataset can be downloaded from https://www.adrianbulat.com/face-alignment/



### A Holistic Approach for Optimizing DSP Block Utilization of a CNN implementation on FPGA
- **Arxiv ID**: http://arxiv.org/abs/1703.09779v1
- **DOI**: 10.1145/2967413.2967430
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09779v1)
- **Published**: 2017-03-21 17:41:37+00:00
- **Updated**: 2017-03-21 17:41:37+00:00
- **Authors**: Kamel Abdelouahab, Cedric Bourrasset, Maxime Pelcat, François Berry, Jean-Charles Quinton, Jocelyn Serot
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Deep Neural Networks are becoming the de-facto standard models for image understanding, and more generally for computer vision tasks. As they involve highly parallelizable computations, CNN are well suited to current fine grain programmable logic devices. Thus, multiple CNN accelerators have been successfully implemented on FPGAs. Unfortunately, FPGA resources such as logic elements or DSP units remain limited. This work presents a holistic method relying on approximate computing and design space exploration to optimize the DSP block utilization of a CNN implementation on an FPGA. This method was tested when implementing a reconfigurable OCR convolutional neural network on an Altera Stratix V device and varying both data representation and CNN topology in order to find the best combination in terms of DSP block utilization and classification accuracy. This exploration generated dataflow architectures of 76 CNN topologies with 5 different fixed point representation. Most efficient implementation performs 883 classifications/sec at 256 x 256 resolution using 8% of the available DSP blocks.



### Pop-up SLAM: Semantic Monocular Plane SLAM for Low-texture Environments
- **Arxiv ID**: http://arxiv.org/abs/1703.07334v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1703.07334v1)
- **Published**: 2017-03-21 17:41:46+00:00
- **Updated**: 2017-03-21 17:41:46+00:00
- **Authors**: Shichao Yang, Yu Song, Michael Kaess, Sebastian Scherer
- **Comment**: International Conference on Intelligent Robots and Systems (IROS)
  2016
- **Journal**: None
- **Summary**: Existing simultaneous localization and mapping (SLAM) algorithms are not robust in challenging low-texture environments because there are only few salient features. The resulting sparse or semi-dense map also conveys little information for motion planning. Though some work utilize plane or scene layout for dense map regularization, they require decent state estimation from other sources. In this paper, we propose real-time monocular plane SLAM to demonstrate that scene understanding could improve both state estimation and dense mapping especially in low-texture environments. The plane measurements come from a pop-up 3D plane model applied to each single image. We also combine planes with point based SLAM to improve robustness. On a public TUM dataset, our algorithm generates a dense semantic 3D model with pixel depth error of 6.2 cm while existing SLAM algorithms fail. On a 60 m long dataset with loops, our method creates a much better 3D model with state estimation error of 0.67%.



### Simple Online and Realtime Tracking with a Deep Association Metric
- **Arxiv ID**: http://arxiv.org/abs/1703.07402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07402v1)
- **Published**: 2017-03-21 19:40:25+00:00
- **Updated**: 2017-03-21 19:40:25+00:00
- **Authors**: Nicolai Wojke, Alex Bewley, Dietrich Paulus
- **Comment**: 5 pages, 1 figure
- **Journal**: None
- **Summary**: Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45%, achieving overall competitive performance at high frame rates.



### IOD-CNN: Integrating Object Detection Networks for Event Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.07431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07431v1)
- **Published**: 2017-03-21 21:05:21+00:00
- **Updated**: 2017-03-21 21:05:21+00:00
- **Authors**: Sungmin Eum, Hyungtae Lee, Heesung Kwon, David Doermann
- **Comment**: submitted to IEEE International Conference on Image Processing 2017
- **Journal**: None
- **Summary**: Many previous methods have showed the importance of considering semantically relevant objects for performing event recognition, yet none of the methods have exploited the power of deep convolutional neural networks to directly integrate relevant object information into a unified network. We present a novel unified deep CNN architecture which integrates architecturally different, yet semantically-related object detection networks to enhance the performance of the event recognition task. Our architecture allows the sharing of the convolutional layers and a fully connected layer which effectively integrates event recognition, rigid object detection and non-rigid object detection.



### No Fuss Distance Metric Learning using Proxies
- **Arxiv ID**: http://arxiv.org/abs/1703.07464v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07464v3)
- **Published**: 2017-03-21 23:11:56+00:00
- **Updated**: 2017-08-01 19:52:13+00:00
- **Authors**: Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Leung, Sergey Ioffe, Saurabh Singh
- **Comment**: To be presented in ICCV 2017
- **Journal**: None
- **Summary**: We address the problem of distance metric learning (DML), defined as learning a distance consistent with a notion of semantic similarity. Traditionally, for this problem supervision is expressed in the form of sets of points that follow an ordinal relationship -- an anchor point $x$ is similar to a set of positive points $Y$, and dissimilar to a set of negative points $Z$, and a loss defined over these distances is minimized. While the specifics of the optimization differ, in this work we collectively call this type of supervision Triplets and all methods that follow this pattern Triplet-Based methods. These methods are challenging to optimize. A main issue is the need for finding informative triplets, which is usually achieved by a variety of tricks such as increasing the batch size, hard or semi-hard triplet mining, etc. Even with these tricks, the convergence rate of such methods is slow. In this paper we propose to optimize the triplet loss on a different space of triplets, consisting of an anchor data point and similar and dissimilar proxy points which are learned as well. These proxies approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss. This proxy-based loss is empirically better behaved. As a result, the proxy-loss improves on state-of-art results for three standard zero-shot learning datasets, by up to 15% points, while converging three times as fast as other triplet-based losses.



### Episode-Based Active Learning with Bayesian Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.07473v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.07473v1)
- **Published**: 2017-03-21 23:56:51+00:00
- **Updated**: 2017-03-21 23:56:51+00:00
- **Authors**: Feras Dayoub, Niko Sünderhauf, Peter Corke
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate different strategies for active learning with Bayesian deep neural networks. We focus our analysis on scenarios where new, unlabeled data is obtained episodically, such as commonly encountered in mobile robotics applications. An evaluation of different strategies for acquisition, updating, and final training on the CIFAR-10 dataset shows that incremental network updates with final training on the accumulated acquisition set are essential for best performance, while limiting the amount of required human labeling labor.



