# Arxiv Papers in cs.CV on 2017-03-10
### A New Evaluation Protocol and Benchmarking Results for Extendable Cross-media Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1703.03567v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03567v1)
- **Published**: 2017-03-10 07:56:01+00:00
- **Updated**: 2017-03-10 07:56:01+00:00
- **Authors**: Ruoyu Liu, Yao Zhao, Liang Zheng, Shikui Wei, Yi Yang
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: This paper proposes a new evaluation protocol for cross-media retrieval which better fits the real-word applications. Both image-text and text-image retrieval modes are considered. Traditionally, class labels in the training and testing sets are identical. That is, it is usually assumed that the query falls into some pre-defined classes. However, in practice, the content of a query image/text may vary extensively, and the retrieval system does not necessarily know in advance the class label of a query. Considering the inconsistency between the real-world applications and laboratory assumptions, we think that the existing protocol that works under identical train/test classes can be modified and improved.   This work is dedicated to addressing this problem by considering the protocol under an extendable scenario, \ie, the training and testing classes do not overlap. We provide extensive benchmarking results obtained by the existing protocol and the proposed new protocol on several commonly used datasets. We demonstrate a noticeable performance drop when the testing classes are unseen during training. Additionally, a trivial solution, \ie, directly using the predicted class label for cross-media retrieval, is tested. We show that the trivial solution is very competitive in traditional non-extendable retrieval, but becomes less so under the new settings. The train/test split, evaluation code, and benchmarking results are publicly available on our website.



### Multi-frequency image reconstruction for radio-interferometry with self-tuned regularization parameters
- **Arxiv ID**: http://arxiv.org/abs/1703.03608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03608v1)
- **Published**: 2017-03-10 10:17:10+00:00
- **Updated**: 2017-03-10 10:17:10+00:00
- **Authors**: Rita Ammanouil, André Ferrari, Rémi Flamary, Chiara Ferrari, David Mary
- **Comment**: None
- **Journal**: None
- **Summary**: As the world's largest radio telescope, the Square Kilometer Array (SKA) will provide radio interferometric data with unprecedented detail. Image reconstruction algorithms for radio interferometry are challenged to scale well with TeraByte image sizes never seen before. In this work, we investigate one such 3D image reconstruction algorithm known as MUFFIN (MUlti-Frequency image reconstruction For radio INterferometry). In particular, we focus on the challenging task of automatically finding the optimal regularization parameter values. In practice, finding the regularization parameters using classical grid search is computationally intensive and nontrivial due to the lack of ground- truth. We adopt a greedy strategy where, at each iteration, the optimal parameters are found by minimizing the predicted Stein unbiased risk estimate (PSURE). The proposed self-tuned version of MUFFIN involves parallel and computationally efficient steps, and scales well with large- scale data. Finally, numerical results on a 3D image are presented to showcase the performance of the proposed approach.



### Fast LIDAR-based Road Detection Using Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.03613v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03613v2)
- **Published**: 2017-03-10 10:26:24+00:00
- **Updated**: 2017-03-29 07:30:07+00:00
- **Authors**: Luca Caltagirone, Samuel Scheidegger, Lennart Svensson, Mattias Wahde
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, a deep learning approach has been developed to carry out road detection using only LIDAR data. Starting from an unstructured point cloud, top-view images encoding several basic statistics such as mean elevation and density are generated. By considering a top-view representation, road detection is reduced to a single-scale problem that can be addressed with a simple and fast fully convolutional neural network (FCN). The FCN is specifically designed for the task of pixel-wise semantic segmentation by combining a large receptive field with high-resolution feature maps. The proposed system achieved excellent performance and it is among the top-performing algorithms on the KITTI road benchmark. Its fast inference makes it particularly suitable for real-time applications.



### From Depth Data to Head Pose Estimation: a Siamese approach
- **Arxiv ID**: http://arxiv.org/abs/1703.03624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03624v1)
- **Published**: 2017-03-10 11:08:50+00:00
- **Updated**: 2017-03-10 11:08:50+00:00
- **Authors**: Marco Venturelli, Guido Borghi, Roberto Vezzani, Rita Cucchiara
- **Comment**: VISAPP 2017. arXiv admin note: text overlap with arXiv:1703.01883
- **Journal**: None
- **Summary**: The correct estimation of the head pose is a problem of the great importance for many applications. For instance, it is an enabling technology in automotive for driver attention monitoring. In this paper, we tackle the pose estimation problem through a deep learning network working in regression manner. Traditional methods usually rely on visual facial features, such as facial landmarks or nose tip position. In contrast, we exploit a Convolutional Neural Network (CNN) to perform head pose estimation directly from depth data. We exploit a Siamese architecture and we propose a novel loss function to improve the learning of the regression network layer. The system has been tested on two public datasets, Biwi Kinect Head Pose and ICT-3DHP database. The reported results demonstrate the improvement in accuracy with respect to current state-of-the-art approaches and the real time capabilities of the overall framework.



### Parallel Multiscale Autoregressive Density Estimation
- **Arxiv ID**: http://arxiv.org/abs/1703.03664v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1703.03664v1)
- **Published**: 2017-03-10 12:58:23+00:00
- **Updated**: 2017-03-10 12:58:23+00:00
- **Authors**: Scott Reed, Aäron van den Oord, Nal Kalchbrenner, Sergio Gómez Colmenarejo, Ziyu Wang, Dan Belov, Nando de Freitas
- **Comment**: None
- **Journal**: None
- **Summary**: PixelCNN achieves state-of-the-art results in density estimation for natural images. Although training is fast, inference is costly, requiring one network evaluation per pixel; O(N) for N pixels. This can be sped up by caching activations, but still involves generating each pixel sequentially. In this work, we propose a parallelized PixelCNN that allows more efficient inference by modeling certain pixel groups as conditionally independent. Our new PixelCNN model achieves competitive density estimation and orders of magnitude speedup - O(log N) sampling instead of O(N) - enabling the practical generation of 512x512 images. We evaluate the model on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, showing that our model achieves the best results among non-pixel-autoregressive density models that allow efficient sampling.



### Data-Driven Color Augmentation Techniques for Deep Skin Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1703.03702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03702v1)
- **Published**: 2017-03-10 14:39:11+00:00
- **Updated**: 2017-03-10 14:39:11+00:00
- **Authors**: Adrian Galdran, Aitor Alvarez-Gila, Maria Ines Meyer, Cristina L. Saratxaga, Teresa Araújo, Estibaliz Garrote, Guilherme Aresta, Pedro Costa, A. M. Mendonça, Aurélio Campilho
- **Comment**: None
- **Journal**: None
- **Summary**: Dermoscopic skin images are often obtained with different imaging devices, under varying acquisition conditions. In this work, instead of attempting to perform intensity and color normalization, we propose to leverage computational color constancy techniques to build an artificial data augmentation technique suitable for this kind of images. Specifically, we apply the \emph{shades of gray} color constancy technique to color-normalize the entire training set of images, while retaining the estimated illuminants. We then draw one sample from the distribution of training set illuminants and apply it on the normalized image. We employ this technique for training two deep convolutional neural networks for the tasks of skin lesion segmentation and skin lesion classification, in the context of the ISIC 2017 challenge and without using any external dermatologic image set. Our results on the validation set are promising, and will be supplemented with extended results on the hidden test set when available.



### Neural Networks for Beginners. A fast implementation in Matlab, Torch, TensorFlow
- **Arxiv ID**: http://arxiv.org/abs/1703.05298v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MS, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.05298v2)
- **Published**: 2017-03-10 18:01:20+00:00
- **Updated**: 2017-03-16 08:32:19+00:00
- **Authors**: Francesco Giannini, Vincenzo Laveglia, Alessandro Rossi, Dario Zanca, Andrea Zugarini
- **Comment**: None
- **Journal**: None
- **Summary**: This report provides an introduction to some Machine Learning tools within the most common development environments. It mainly focuses on practical problems, skipping any theoretical introduction. It is oriented to both students trying to approach Machine Learning and experts looking for new frameworks.



### Development of An Android Application for Object Detection Based on Color, Shape, or Local Features
- **Arxiv ID**: http://arxiv.org/abs/1703.03848v1
- **DOI**: 10.5121/ijma.2017.9103
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03848v1)
- **Published**: 2017-03-10 21:39:49+00:00
- **Updated**: 2017-03-10 21:39:49+00:00
- **Authors**: Lamiaa A. Elrefaei, Mona Omar Al-musawa, Norah Abdullah Al-gohany
- **Comment**: None
- **Journal**: The International Journal of Multimedia & Its Applications (IJMA)
  Vol.9, No.1, February 2017
- **Summary**: Object detection and recognition is an important task in many computer vision applications. In this paper an Android application was developed using Eclipse IDE and OpenCV3 Library. This application is able to detect objects in an image that is loaded from the mobile gallery, based on its color, shape, or local features. The image is processed in the HSV color domain for better color detection. Circular shapes are detected using Circular Hough Transform and other shapes are detected using Douglas-Peucker algorithm. BRISK (binary robust invariant scalable keypoints) local features were applied in the developed Android application for matching an object image in another scene image. The steps of the proposed detection algorithms are described, and the interfaces of the application are illustrated. The application is ported and tested on Galaxy S3, S6, and Note1 Smartphones. Based on the experimental results, the application is capable of detecting eleven different colors, detecting two dimensional geometrical shapes including circles, rectangles, triangles, and squares, and correctly match local features of object and scene images for different conditions. The application could be used as a standalone application, or as a part of another application such as Robot systems, traffic systems, e-learning applications, information retrieval and many others.



### Convolutional Spike Timing Dependent Plasticity based Feature Learning in Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.03854v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.03854v2)
- **Published**: 2017-03-10 22:09:20+00:00
- **Updated**: 2017-03-20 15:06:10+00:00
- **Authors**: Priyadarshini Panda, Gopalakrishnan Srinivasan, Kaushik Roy
- **Comment**: 11 pages, 10 figures, Under Consideration in Scientific Reports
- **Journal**: None
- **Summary**: Brain-inspired learning models attempt to mimic the cortical architecture and computations performed in the neurons and synapses constituting the human brain to achieve its efficiency in cognitive tasks. In this work, we present convolutional spike timing dependent plasticity based feature learning with biologically plausible leaky-integrate-and-fire neurons in Spiking Neural Networks (SNNs). We use shared weight kernels that are trained to encode representative features underlying the input patterns thereby improving the sparsity as well as the robustness of the learning model. We demonstrate that the proposed unsupervised learning methodology learns several visual categories for object recognition with fewer number of examples and outperforms traditional fully-connected SNN architectures while yielding competitive accuracy. Additionally, we observe that the learning model performs out-of-set generalization further making the proposed biologically plausible framework a viable and efficient architecture for future neuromorphic applications.



### Depth from Monocular Images using a Semi-Parallel Deep Neural Network (SPDNN) Hybrid Architecture
- **Arxiv ID**: http://arxiv.org/abs/1703.03867v3
- **DOI**: 10.1117/1.JEI.27.4.043041
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03867v3)
- **Published**: 2017-03-10 23:10:11+00:00
- **Updated**: 2018-04-18 14:31:24+00:00
- **Authors**: S. Bazrafkan, H. Javidnia, J. Lemley, P. Corcoran
- **Comment**: 44 pages, 25 figures
- **Journal**: J. of Electronic Imaging, 27(4), 043041 (2018)
- **Summary**: Deep neural networks are applied to a wide range of problems in recent years. In this work, Convolutional Neural Network (CNN) is applied to the problem of determining the depth from a single camera image (monocular depth). Eight different networks are designed to perform depth estimation, each of them suitable for a feature level. Networks with different pooling sizes determine different feature levels. After designing a set of networks, these models may be combined into a single network topology using graph optimization techniques. This "Semi Parallel Deep Neural Network (SPDNN)" eliminates duplicated common network layers, and can be further optimized by retraining to achieve an improved model compared to the individual topologies. In this study, four SPDNN models are trained and have been evaluated at 2 stages on the KITTI dataset. The ground truth images in the first part of the experiment are provided by the benchmark, and for the second part, the ground truth images are the depth map results from applying a state-of-the-art stereo matching method. The results of this evaluation demonstrate that using post-processing techniques to refine the target of the network increases the accuracy of depth estimation on individual mono images. The second evaluation shows that using segmentation data alongside the original data as the input can improve the depth estimation results to a point where performance is comparable with stereo depth estimation. The computational time is also discussed in this study.



### Deep Image Matting
- **Arxiv ID**: http://arxiv.org/abs/1703.03872v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03872v3)
- **Published**: 2017-03-10 23:43:17+00:00
- **Updated**: 2017-04-11 00:57:05+00:00
- **Authors**: Ning Xu, Brian Price, Scott Cohen, Thomas Huang
- **Comment**: Computer Vision and Pattern Recognition 2017
- **Journal**: None
- **Summary**: Image matting is a fundamental computer vision problem and has many applications. Previous algorithms have poor performance when an image has similar foreground and background colors or complicated textures. The main reasons are prior methods 1) only use low-level features and 2) lack high-level context. In this paper, we propose a novel deep learning based algorithm that can tackle both these problems. Our deep model has two parts. The first part is a deep convolutional encoder-decoder network that takes an image and the corresponding trimap as inputs and predict the alpha matte of the image. The second part is a small convolutional network that refines the alpha matte predictions of the first network to have more accurate alpha values and sharper edges. In addition, we also create a large-scale image matting dataset including 49300 training images and 1000 testing images. We evaluate our algorithm on the image matting benchmark, our testing set, and a wide variety of real images. Experimental results clearly demonstrate the superiority of our algorithm over previous methods.



