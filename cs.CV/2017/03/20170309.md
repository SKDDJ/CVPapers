# Arxiv Papers in cs.CV on 2017-03-09
### DA-RNN: Semantic Mapping with Data Associated Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.03098v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1703.03098v2)
- **Published**: 2017-03-09 01:29:23+00:00
- **Updated**: 2017-05-30 20:12:35+00:00
- **Authors**: Yu Xiang, Dieter Fox
- **Comment**: Published in RSS 2017
- **Journal**: None
- **Summary**: 3D scene understanding is important for robots to interact with the 3D world in a meaningful way. Most previous works on 3D scene understanding focus on recognizing geometrical or semantic properties of the scene independently. In this work, we introduce Data Associated Recurrent Neural Networks (DA-RNNs), a novel framework for joint 3D scene mapping and semantic labeling. DA-RNNs use a new recurrent neural network architecture for semantic labeling on RGB-D videos. The output of the network is integrated with mapping techniques such as KinectFusion in order to inject semantic information into the reconstructed 3D scene. Experiments conducted on a real world dataset and a synthetic dataset with RGB-D videos demonstrate the ability of our method in semantic 3D scene mapping.



### Image Classification of Melanoma, Nevus and Seborrheic Keratosis by Deep Neural Network Ensemble
- **Arxiv ID**: http://arxiv.org/abs/1703.03108v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03108v1)
- **Published**: 2017-03-09 02:35:59+00:00
- **Updated**: 2017-03-09 02:35:59+00:00
- **Authors**: Kazuhisa Matsunaga, Akira Hamada, Akane Minagawa, Hiroshi Koga
- **Comment**: 4 pages. 3 figures. ISIC2017
- **Journal**: None
- **Summary**: This short paper reports the method and the evaluation results of Casio and Shinshu University joint team for the ISBI Challenge 2017 - Skin Lesion Analysis Towards Melanoma Detection - Part 3: Lesion Classification hosted by ISIC. Our online validation score was 0.958 with melanoma classifier AUC 0.924 and seborrheic keratosis classifier AUC 0.993.



### DeepSD: Generating High Resolution Climate Change Projections through Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1703.03126v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03126v1)
- **Published**: 2017-03-09 04:19:17+00:00
- **Updated**: 2017-03-09 04:19:17+00:00
- **Authors**: Thomas Vandal, Evan Kodra, Sangram Ganguly, Andrew Michaelis, Ramakrishna Nemani, Auroop R Ganguly
- **Comment**: 9 pages, 5 Figures, 2 Tables
- **Journal**: None
- **Summary**: The impacts of climate change are felt by most critical systems, such as infrastructure, ecological systems, and power-plants. However, contemporary Earth System Models (ESM) are run at spatial resolutions too coarse for assessing effects this localized. Local scale projections can be obtained using statistical downscaling, a technique which uses historical climate observations to learn a low-resolution to high-resolution mapping. Depending on statistical modeling choices, downscaled projections have been shown to vary significantly terms of accuracy and reliability. The spatio-temporal nature of the climate system motivates the adaptation of super-resolution image processing techniques to statistical downscaling. In our work, we present DeepSD, a generalized stacked super resolution convolutional neural network (SRCNN) framework for statistical downscaling of climate variables. DeepSD augments SRCNN with multi-scale input channels to maximize predictability in statistical downscaling. We provide a comparison with Bias Correction Spatial Disaggregation as well as three Automated-Statistical Downscaling approaches in downscaling daily precipitation from 1 degree (~100km) to 1/8 degrees (~12.5km) over the Continental United States. Furthermore, a framework using the NASA Earth Exchange (NEX) platform is discussed for downscaling more than 20 ESM models with multiple emission scenarios.



### Face-to-BMI: Using Computer Vision to Infer Body Mass Index on Social Media
- **Arxiv ID**: http://arxiv.org/abs/1703.03156v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1703.03156v1)
- **Published**: 2017-03-09 06:48:47+00:00
- **Updated**: 2017-03-09 06:48:47+00:00
- **Authors**: Enes Kocabey, Mustafa Camurcu, Ferda Ofli, Yusuf Aytar, Javier Marin, Antonio Torralba, Ingmar Weber
- **Comment**: This is a preprint of a short paper accepted at ICWSM'17. Please cite
  that version instead
- **Journal**: None
- **Summary**: A person's weight status can have profound implications on their life, ranging from mental health, to longevity, to financial income. At the societal level, "fat shaming" and other forms of "sizeism" are a growing concern, while increasing obesity rates are linked to ever raising healthcare costs. For these reasons, researchers from a variety of backgrounds are interested in studying obesity from all angles. To obtain data, traditionally, a person would have to accurately self-report their body-mass index (BMI) or would have to see a doctor to have it measured. In this paper, we show how computer vision can be used to infer a person's BMI from social media images. We hope that our tool, which we release, helps to advance the study of social aspects related to body weight.



### Segmenting Dermoscopic Images
- **Arxiv ID**: http://arxiv.org/abs/1703.03186v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03186v1)
- **Published**: 2017-03-09 09:14:40+00:00
- **Updated**: 2017-03-09 09:14:40+00:00
- **Authors**: Mario Rosario Guarracino, Lucia Maddalena
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: We propose an automatic algorithm, named SDI, for the segmentation of skin lesions in dermoscopic images, articulated into three main steps: selection of the image ROI, selection of the segmentation band, and segmentation. We present extensive experimental results achieved by the SDI algorithm on the lesion segmentation dataset made available for the ISIC 2017 challenge on Skin Lesion Analysis Towards Melanoma Detection, highlighting its advantages and disadvantages.



### Prior-based Hierarchical Segmentation Highlighting Structures of Interest
- **Arxiv ID**: http://arxiv.org/abs/1703.03196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03196v1)
- **Published**: 2017-03-09 09:41:28+00:00
- **Updated**: 2017-03-09 09:41:28+00:00
- **Authors**: Amin Fehri, Santiago Velasco-Forero, Fernand Meyer
- **Comment**: None
- **Journal**: None
- **Summary**: Image segmentation is the process of partitioning an image into a set of meaningful regions according to some criteria. Hierarchical segmentation has emerged as a major trend in this regard as it favors the emergence of important regions at different scales. On the other hand, many methods allow us to have prior information on the position of structures of interest in the images. In this paper, we present a versatile hierarchical segmentation method that takes into account any prior spatial information and outputs a hierarchical segmentation that emphasizes the contours or regions of interest while preserving the important structures in the image. Several applications are presented that illustrate the method versatility and efficiency.



### WebCaricature: a benchmark for caricature recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.03230v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03230v4)
- **Published**: 2017-03-09 11:27:26+00:00
- **Updated**: 2018-08-09 13:22:59+00:00
- **Authors**: Jing Huo, Wenbin Li, Yinghuan Shi, Yang Gao, Hujun Yin
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Studying caricature recognition is fundamentally important to understanding of face perception. However, little research has been conducted in the computer vision community, largely due to the shortage of suitable datasets. In this paper, a new caricature dataset is built, with the objective to facilitate research in caricature recognition. All the caricatures and face images were collected from the Web. Compared with two existing datasets, this dataset is much more challenging, with a much greater number of available images, artistic styles and larger intra-personal variations. Evaluation protocols are also offered together with their baseline performances on the dataset to allow fair comparisons. Besides, a framework for caricature face recognition is presented to make a thorough analyze of the challenges of caricature recognition. By analyzing the challenges, the goal is to show problems that worth to be further investigated. Additionally, based on the evaluation protocols and the framework, baseline performances of various state-of-the-art algorithms are provided. A conclusion is that there is still a large space for performance improvement and the analyzed problems still need further investigation.



### End-to-end semantic face segmentation with conditional random fields as convolutional, recurrent and adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1703.03305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03305v1)
- **Published**: 2017-03-09 15:48:22+00:00
- **Updated**: 2017-03-09 15:48:22+00:00
- **Authors**: Umut Güçlü, Yağmur Güçlütürk, Meysam Madadi, Sergio Escalera, Xavier Baró, Jordi González, Rob van Lier, Marcel A. J. van Gerven
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have seen a sharp increase in the number of related yet distinct advances in semantic segmentation. Here, we tackle this problem by leveraging the respective strengths of these advances. That is, we formulate a conditional random field over a four-connected graph as end-to-end trainable convolutional and recurrent networks, and estimate them via an adversarial process. Importantly, our model learns not only unary potentials but also pairwise potentials, while aggregating multi-scale contexts and controlling higher-order inconsistencies. We evaluate our model on two standard benchmark datasets for semantic face segmentation, achieving state-of-the-art results on both of them.



### UntrimmedNets for Weakly Supervised Action Recognition and Detection
- **Arxiv ID**: http://arxiv.org/abs/1703.03329v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03329v2)
- **Published**: 2017-03-09 16:29:39+00:00
- **Updated**: 2017-05-22 12:38:02+00:00
- **Authors**: Limin Wang, Yuanjun Xiong, Dahua Lin, Luc Van Gool
- **Comment**: camera-ready version to appear in CVPR2017
- **Journal**: None
- **Summary**: Current action recognition methods heavily rely on trimmed videos for model training. However, it is expensive and time-consuming to acquire a large-scale trimmed video dataset. This paper presents a new weakly supervised architecture, called UntrimmedNet, which is able to directly learn action recognition models from untrimmed videos without the requirement of temporal annotations of action instances. Our UntrimmedNet couples two important components, the classification module and the selection module, to learn the action models and reason about the temporal duration of action instances, respectively. These two components are implemented with feed-forward networks, and UntrimmedNet is therefore an end-to-end trainable architecture. We exploit the learned models for action recognition (WSR) and detection (WSD) on the untrimmed video datasets of THUMOS14 and ActivityNet. Although our UntrimmedNet only employs weak supervision, our method achieves performance superior or comparable to that of those strongly supervised approaches on these two datasets.



### A Self-supervised Learning System for Object Detection using Physics Simulation and Multi-view Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1703.03347v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.03347v2)
- **Published**: 2017-03-09 17:14:21+00:00
- **Updated**: 2017-08-03 15:04:14+00:00
- **Authors**: Chaitanya Mitash, Kostas E. Bekris, Abdeslam Boularias
- **Comment**: 7 pages, 6 figures, accepted at the IEEE International Conference on
  Intelligent Robots and Systems (IROS), Vancouver, Canada, 2017
- **Journal**: None
- **Summary**: Progress has been achieved recently in object detection given advancements in deep learning. Nevertheless, such tools typically require a large amount of training data and significant manual effort to label objects. This limits their applicability in robotics, where solutions must scale to a large number of objects and variety of conditions. This work proposes an autonomous process for training a Convolutional Neural Network (CNN) for object detection and pose estimation in robotic setups. The focus is on detecting objects placed in cluttered, tight environments, such as a shelf with multiple objects. In particular, given access to 3D object models, several aspects of the environment are physically simulated. The models are placed in physically realistic poses with respect to their environment to generate a labeled synthetic dataset. To further improve object detection, the network self-trains over real images that are labeled using a robust multi-view pose estimation process. The proposed training process is evaluated on several existing datasets and on a dataset collected for this paper with a Motoman robotic arm. Results show that the proposed approach outperforms popular training processes relying on synthetic - but not physically realistic - data and manual annotation. The key contributions are the incorporation of physical reasoning in the synthetic data generation process and the automation of the annotation process over real images.



### Fast and Robust Detection of Fallen People from a Mobile Robot
- **Arxiv ID**: http://arxiv.org/abs/1703.03349v1
- **DOI**: 10.1109/IROS.2017.8206276
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.03349v1)
- **Published**: 2017-03-09 17:15:18+00:00
- **Updated**: 2017-03-09 17:15:18+00:00
- **Authors**: Morris Antonello, Marco Carraro, Marco Pierobon, Emanuele Menegatti
- **Comment**: None
- **Journal**: None
- **Summary**: This paper deals with the problem of detecting fallen people lying on the floor by means of a mobile robot equipped with a 3D depth sensor. In the proposed algorithm, inspired by semantic segmentation techniques, the 3D scene is over-segmented into small patches. Fallen people are then detected by means of two SVM classifiers: the first one labels each patch, while the second one captures the spatial relations between them. This novel approach showed to be robust and fast. Indeed, thanks to the use of small patches, fallen people in real cluttered scenes with objects side by side are correctly detected. Moreover, the algorithm can be executed on a mobile robot fitted with a standard laptop making it possible to exploit the 2D environmental map built by the robot and the multiple points of view obtained during the robot navigation. Additionally, this algorithm is robust to illumination changes since it does not rely on RGB data but on depth data. All the methods have been thoroughly validated on the IASLAB-RGBD Fallen Person Dataset, which is published online as a further contribution. It consists of several static and dynamic sequences with 15 different people and 2 different environments.



### LesionSeg: Semantic segmentation of skin lesions using Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1703.03372v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1703.03372v3)
- **Published**: 2017-03-09 17:52:28+00:00
- **Updated**: 2017-03-15 01:37:18+00:00
- **Authors**: Dhanesh Ramachandram, Terrance DeVries
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for skin lesion segmentation for the ISIC 2017 Skin Lesion Segmentation Challenge. Our approach is based on a Fully Convolutional Network architecture which is trained end to end, from scratch, on a limited dataset. Our semantic segmentation architecture utilizes several recent innovations in particularly in the combined use of (i) use of atrous convolutions to increase the effective field of view of the network's receptive field without increasing the number of parameters, (ii) the use of network-in-network $1\times1$ convolution layers to add capacity to the network and (iii) state-of-art super-resolution upsampling of predictions using subpixel CNN layers. We reported a mean IOU score of 0.642 on the validation set provided by the organisers.



### Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.03400v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1703.03400v3)
- **Published**: 2017-03-09 18:58:03+00:00
- **Updated**: 2017-07-18 16:45:29+00:00
- **Authors**: Chelsea Finn, Pieter Abbeel, Sergey Levine
- **Comment**: ICML 2017. Code at https://github.com/cbfinn/maml, Videos of RL
  results at https://sites.google.com/view/maml, Blog post at
  http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/
- **Journal**: None
- **Summary**: We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.



### Position Tracking for Virtual Reality Using Commodity WiFi
- **Arxiv ID**: http://arxiv.org/abs/1703.03468v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/1703.03468v2)
- **Published**: 2017-03-09 21:19:48+00:00
- **Updated**: 2017-07-12 18:12:40+00:00
- **Authors**: Manikanta Kotaru, Sachin Katti
- **Comment**: None
- **Journal**: None
- **Summary**: Today, experiencing virtual reality (VR) is a cumbersome experience which either requires dedicated infrastructure like infrared cameras to track the headset and hand-motion controllers (e.g., Oculus Rift, HTC Vive), or provides only 3-DoF (Degrees of Freedom) tracking which severely limits the user experience (e.g., Samsung Gear). To truly enable VR everywhere, we need position tracking to be available as a ubiquitous service. This paper presents WiCapture, a novel approach which leverages commodity WiFi infrastructure, which is ubiquitous today, for tracking purposes. We prototype WiCapture using off-the-shelf WiFi radios and show that it achieves an accuracy of 0.88 cm compared to sophisticated infrared based tracking systems like the Oculus, while providing much higher range, resistance to occlusion, ubiquity and ease of deployment.



### A New Representation of Skeleton Sequences for 3D Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.03492v3
- **DOI**: 10.1109/CVPR.2017.486
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.03492v3)
- **Published**: 2017-03-09 23:47:27+00:00
- **Updated**: 2017-06-05 01:29:39+00:00
- **Authors**: Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous Sohel, Farid Boussaid
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: This paper presents a new method for 3D action recognition with skeleton sequences (i.e., 3D trajectories of human skeleton joints). The proposed method first transforms each skeleton sequence into three clips each consisting of several frames for spatial temporal feature learning using deep neural networks. Each clip is generated from one channel of the cylindrical coordinates of the skeleton sequence. Each frame of the generated clips represents the temporal information of the entire skeleton sequence, and incorporates one particular spatial relationship between the joints. The entire clips include multiple frames with different spatial relationships, which provide useful spatial structural information of the human skeleton. We propose to use deep convolutional neural networks to learn long-term temporal information of the skeleton sequence from the frames of the generated clips, and then use a Multi-Task Learning Network (MTLN) to jointly process all frames of the generated clips in parallel to incorporate spatial structural information for action recognition. Experimental results clearly show the effectiveness of the proposed new representation and feature learning method for 3D action recognition.



