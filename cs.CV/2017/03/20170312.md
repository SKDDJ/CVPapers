# Arxiv Papers in cs.CV on 2017-03-12
### Multi-Pose Face Recognition Using Hybrid Face Features Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1703.04062v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04062v1)
- **Published**: 2017-03-12 02:58:07+00:00
- **Updated**: 2017-03-12 02:58:07+00:00
- **Authors**: I Gede Pasek Suta Wijaya, Keiichi Uchimura, Gou Koutaki
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a multi-pose face recognition approach using hybrid face features descriptors (HFFD). The HFFD is a face descriptor containing of rich discriminant information that is created by fusing some frequency-based features extracted using both wavelet and DCT analysis of several different poses of 2D face images. The main aim of this method is to represent the multi-pose face images using a dominant frequency component with still having reasonable achievement compared to the recent multi-pose face recognition methods. The HFFD based face recognition tends to achieve better performance than that of the recent 2D-based face recognition method. In addition, the HFFD-based face recognition also is sufficiently to handle large face variability due to face pose variations .



### A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification and Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1703.04071v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1703.04071v4)
- **Published**: 2017-03-12 05:07:00+00:00
- **Updated**: 2017-04-03 05:17:42+00:00
- **Authors**: Chunpeng Wu, Wei Wen, Tariq Afzal, Yongmei Zhang, Yiran Chen, Hai Li
- **Comment**: 2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR'17)
- **Journal**: None
- **Summary**: Recently, DNN model compression based on network architecture design, e.g., SqueezeNet, attracted a lot attention. No accuracy drop on image classification is observed on these extremely compact networks, compared to well-known models. An emerging question, however, is whether these model compression techniques hurt DNN's learning ability other than classifying images on a single dataset. Our preliminary experiment shows that these compression methods could degrade domain adaptation (DA) ability, though the classification performance is preserved. Therefore, we propose a new compact network architecture and unsupervised DA method in this paper. The DNN is built on a new basic module Conv-M which provides more diverse feature extractors without significantly increasing parameters. The unified framework of our DA method will simultaneously learn invariance across domains, reduce divergence of feature representations, and adapt label prediction. Our DNN has 4.1M parameters, which is only 6.7% of AlexNet or 59% of GoogLeNet. Experiments show that our DNN obtains GoogLeNet-level accuracy both on classification and DA, and our DA method slightly outperforms previous competitive ones. Put all together, our DA strategy based on our DNN achieves state-of-the-art on sixteen of total eighteen DA tasks on popular Office-31 and Office-Caltech datasets.



### Prostate Cancer Diagnosis using Deep Learning with 3D Multiparametric MRI
- **Arxiv ID**: http://arxiv.org/abs/1703.04078v1
- **DOI**: 10.1117/12.2277121
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.04078v1)
- **Published**: 2017-03-12 07:19:55+00:00
- **Updated**: 2017-03-12 07:19:55+00:00
- **Authors**: Saifeng Liu, Huaixiu Zheng, Yesu Feng, Wei Li
- **Comment**: 4 pages, 4 figures, Proc. SPIE 10134, Medical Imaging 2017
- **Journal**: None
- **Summary**: A novel deep learning architecture (XmasNet) based on convolutional neural networks was developed for the classification of prostate cancer lesions, using the 3D multiparametric MRI data provided by the PROSTATEx challenge. End-to-end training was performed for XmasNet, with data augmentation done through 3D rotation and slicing, in order to incorporate the 3D information of the lesion. XmasNet outperformed traditional machine learning models based on engineered features, for both train and test data. For the test data, XmasNet outperformed 69 methods from 33 participating groups and achieved the second highest AUC (0.84) in the PROSTATEx challenge. This study shows the great potential of deep learning for cancer imaging.



### SurfNet: Generating 3D shape surfaces using deep residual networks
- **Arxiv ID**: http://arxiv.org/abs/1703.04079v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1703.04079v1)
- **Published**: 2017-03-12 07:21:50+00:00
- **Updated**: 2017-03-12 07:21:50+00:00
- **Authors**: Ayan Sinha, Asim Unmesh, Qixing Huang, Karthik Ramani
- **Comment**: CVPR 2017 paper
- **Journal**: None
- **Summary**: 3D shape models are naturally parameterized using vertices and faces, \ie, composed of polygons forming a surface. However, current 3D learning paradigms for predictive and generative tasks using convolutional neural networks focus on a voxelized representation of the object. Lifting convolution operators from the traditional 2D to 3D results in high computational overhead with little additional benefit as most of the geometry information is contained on the surface boundary. Here we study the problem of directly generating the 3D shape surface of rigid and non-rigid shapes using deep convolutional neural networks. We develop a procedure to create consistent `geometry images' representing the shape surface of a category of 3D objects. We then use this consistent representation for category-specific shape surface generation from a parametric representation or an image by developing novel extensions of deep residual networks for the task of geometry image generation. Our experiments indicate that our network learns a meaningful representation of shape surfaces allowing it to interpolate between shape orientations and poses, invent new shape surfaces and reconstruct 3D shape surfaces from previously unseen images.



### Local Patch Encoding-Based Method for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1703.04088v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04088v2)
- **Published**: 2017-03-12 09:47:51+00:00
- **Updated**: 2018-07-04 01:45:24+00:00
- **Authors**: Yang Zhao, Ronggang Wang, Wei Jia, Jianchao Yang, Wenmin Wang, Wen Gao
- **Comment**: 20 pages, 8 figures
- **Journal**: Y. Zhao, R. Wang, W. Jia, J. Yang, W. Wang , W. Gao, Local patch
  encoding-based method for single image super-resolution, Information
  Sciences, vol.433, pp.292-305, 2018
- **Summary**: Recent learning-based super-resolution (SR) methods often focus on dictionary learning or network training. In this paper, we discuss in detail a new SR method based on local patch encoding (LPE) instead of traditional dictionary learning. The proposed method consists of a learning stage and a reconstructing stage. In the learning stage, image patches are classified into different classes by means of the proposed LPE, and then a projection matrix is computed for each class by utilizing a simple constraint. In the reconstructing stage, an input LR patch can be simply reconstructed by computing its LPE code and then multiplying the corresponding projection matrix. Furthermore, we discuss the relationship between the proposed method and the anchored neighborhood regression methods; we also analyze the extendibility of the proposed method. The experimental results on several image sets demonstrate the effectiveness of the LPE-based methods.



### Improving Interpretability of Deep Neural Networks with Semantic Information
- **Arxiv ID**: http://arxiv.org/abs/1703.04096v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04096v2)
- **Published**: 2017-03-12 10:38:10+00:00
- **Updated**: 2017-03-30 11:48:31+00:00
- **Authors**: Yinpeng Dong, Hang Su, Jun Zhu, Bo Zhang
- **Comment**: To appear in CVPR 2017
- **Journal**: None
- **Summary**: Interpretability of deep neural networks (DNNs) is essential since it enables users to understand the overall strengths and weaknesses of the models, conveys an understanding of how the models will behave in the future, and how to diagnose and correct potential problems. However, it is challenging to reason about what a DNN actually does due to its opaque or black-box nature. To address this issue, we propose a novel technique to improve the interpretability of DNNs by leveraging the rich semantic information embedded in human descriptions. By concentrating on the video captioning task, we first extract a set of semantically meaningful topics from the human descriptions that cover a wide range of visual concepts, and integrate them into the model with an interpretive loss. We then propose a prediction difference maximization algorithm to interpret the learned features of each neuron. Experimental results demonstrate its effectiveness in video captioning using the interpretable features, which can also be transferred to video action recognition. By clearly understanding the learned features, users can easily revise false predictions via a human-in-the-loop procedure.



### Evaluating Deep Convolutional Neural Networks for Material Classification
- **Arxiv ID**: http://arxiv.org/abs/1703.04101v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04101v2)
- **Published**: 2017-03-12 11:29:00+00:00
- **Updated**: 2017-03-16 10:36:15+00:00
- **Authors**: Grigorios Kalliatakis, Georgios Stamatiadis, Shoaib Ehsan, Ales Leonardis, Juergen Gall, Anca Sticlaru, Klaus D. McDonald-Maier
- **Comment**: In Proceedings of the 12th International Conference on Computer
  Vision Theory and Applications (VISAPP 2017), 7 pages
- **Journal**: None
- **Summary**: Determining the material category of a surface from an image is a demanding task in perception that is drawing increasing attention. Following the recent remarkable results achieved for image classification and object detection utilising Convolutional Neural Networks (CNNs), we empirically study material classification of everyday objects employing these techniques. More specifically, we conduct a rigorous evaluation of how state-of-the art CNN architectures compare on a common ground over widely used material databases. Experimental results on three challenging material databases show that the best performing CNN architectures can achieve up to 94.99\% mean average precision when classifying materials.



### Detection of Human Rights Violations in Images: Can Convolutional Neural Networks help?
- **Arxiv ID**: http://arxiv.org/abs/1703.04103v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04103v2)
- **Published**: 2017-03-12 11:39:41+00:00
- **Updated**: 2017-03-16 10:37:25+00:00
- **Authors**: Grigorios Kalliatakis, Shoaib Ehsan, Maria Fasli, Ales Leonardis, Juergen Gall, Klaus D. McDonald-Maier
- **Comment**: In Proceedings of the 12th International Conference on Computer
  Vision Theory and Applications (VISAPP 2017), 8 pages
- **Journal**: None
- **Summary**: After setting the performance benchmarks for image, video, speech and audio processing, deep convolutional networks have been core to the greatest advances in image recognition tasks in recent times. This raises the question of whether there are any benefit in targeting these remarkable deep architectures with the unattempted task of recognising human rights violations through digital images. Under this perspective, we introduce a new, well-sampled human rights-centric dataset called Human Rights Understanding (HRUN). We conduct a rigorous evaluation on a common ground by combining this dataset with different state-of-the-art deep convolutional architectures in order to achieve recognition of human rights violations. Experimental results on the HRUN dataset have shown that the best performing CNN architectures can achieve up to 88.10\% mean average precision. Additionally, our experiments demonstrate that increasing the size of the training samples is crucial for achieving an improvement on mean average precision principally when utilising very deep networks.



### Combining Residual Networks with LSTMs for Lipreading
- **Arxiv ID**: http://arxiv.org/abs/1703.04105v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04105v4)
- **Published**: 2017-03-12 12:06:04+00:00
- **Updated**: 2017-09-08 10:08:34+00:00
- **Authors**: Themos Stafylakis, Georgios Tzimiropoulos
- **Comment**: Submitted to Interspeech 2017
- **Journal**: None
- **Summary**: We propose an end-to-end deep learning architecture for word-level visual speech recognition. The system is a combination of spatiotemporal convolutional, residual and bidirectional Long Short-Term Memory networks. We train and evaluate it on the Lipreading In-The-Wild benchmark, a challenging database of 500-size target-words consisting of 1.28sec video excerpts from BBC TV broadcasts. The proposed network attains word accuracy equal to 83.0, yielding 6.8 absolute improvement over the current state-of-the-art, without using information about word boundaries during training or testing.



### Co-occurrence Filter
- **Arxiv ID**: http://arxiv.org/abs/1703.04111v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04111v2)
- **Published**: 2017-03-12 12:41:16+00:00
- **Updated**: 2017-12-24 06:52:54+00:00
- **Authors**: Roy J Jevnisek, Shai Avidan
- **Comment**: accepted to CVPR 2017
- **Journal**: None
- **Summary**: Co-occurrence Filter (CoF) is a boundary preserving filter. It is based on the Bilateral Filter (BF) but instead of using a Gaussian on the range values to preserve edges it relies on a co-occurrence matrix. Pixel values that co-occur frequently in the image (i.e., inside textured regions) will have a high weight in the co-occurrence matrix. This, in turn, means that such pixel pairs will be averaged and hence smoothed, regardless of their intensity differences. On the other hand, pixel values that rarely co-occur (i.e., across texture boundaries) will have a low weight in the co-occurrence matrix. As a result, they will not be averaged and the boundary between them will be preserved. The CoF therefore extends the BF to deal with boundaries, not just edges. It learns co-occurrences directly from the image. We can achieve various filtering results by directing it to learn the co-occurrence matrix from a part of the image, or a different image. We give the definition of the filter, discuss how to use it with color images and show several use cases.



### Hardware-Driven Nonlinear Activation for Stochastic Computing Based Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.04135v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04135v1)
- **Published**: 2017-03-12 15:27:23+00:00
- **Updated**: 2017-03-12 15:27:23+00:00
- **Authors**: Ji Li, Zihao Yuan, Zhe Li, Caiwen Ding, Ao Ren, Qinru Qiu, Jeffrey Draper, Yanzhi Wang
- **Comment**: This paper is accepted by 2017 International Joint Conference on
  Neural Networks (IJCNN)
- **Journal**: None
- **Summary**: Recently, Deep Convolutional Neural Networks (DCNNs) have made unprecedented progress, achieving the accuracy close to, or even better than human-level perception in various tasks. There is a timely need to map the latest software DCNNs to application-specific hardware, in order to achieve orders of magnitude improvement in performance, energy efficiency and compactness. Stochastic Computing (SC), as a low-cost alternative to the conventional binary computing paradigm, has the potential to enable massively parallel and highly scalable hardware implementation of DCNNs. One major challenge in SC based DCNNs is designing accurate nonlinear activation functions, which have a significant impact on the network-level accuracy but cannot be implemented accurately by existing SC computing blocks. In this paper, we design and optimize SC based neurons, and we propose highly accurate activation designs for the three most frequently used activation functions in software DCNNs, i.e, hyperbolic tangent, logistic, and rectified linear units. Experimental results on LeNet-5 using MNIST dataset demonstrate that compared with a binary ASIC hardware DCNN, the DCNN with the proposed SC neurons can achieve up to 61X, 151X, and 2X improvement in terms of area, power, and energy, respectively, at the cost of small precision degradation.In addition, the SC approach achieves up to 21X and 41X of the area, 41X and 72X of the power, and 198200X and 96443X of the energy, compared with CPU and GPU approaches, respectively, while the error is increased by less than 3.07%. ReLU activation is suggested for future SC based DCNNs considering its superior performance under a small bit stream length.



### Tree Memory Networks for Modelling Long-term Temporal Dependencies
- **Arxiv ID**: http://arxiv.org/abs/1703.04706v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1703.04706v2)
- **Published**: 2017-03-12 21:13:28+00:00
- **Updated**: 2018-05-20 05:18:59+00:00
- **Authors**: Tharindu Fernando, Simon Denman, Aaron McFadyen, Sridha Sridharan, Clinton Fookes
- **Comment**: None
- **Journal**: Neurocomputing, Volume 304, 23 August 2018, Pages 64-81
- **Summary**: In the domain of sequence modelling, Recurrent Neural Networks (RNN) have been capable of achieving impressive results in a variety of application areas including visual question answering, part-of-speech tagging and machine translation. However this success in modelling short term dependencies has not successfully transitioned to application areas such as trajectory prediction, which require capturing both short term and long term relationships. In this paper, we propose a Tree Memory Network (TMN) for modelling long term and short term relationships in sequence-to-sequence mapping problems. The proposed network architecture is composed of an input module, controller and a memory module. In contrast to related literature, which models the memory as a sequence of historical states, we model the memory as a recursive tree structure. This structure more effectively captures temporal dependencies across both short term and long term sequences using its hierarchical structure. We demonstrate the effectiveness and flexibility of the proposed TMN in two practical problems, aircraft trajectory modelling and pedestrian trajectory modelling in a surveillance setting, and in both cases we outperform the current state-of-the-art. Furthermore, we perform an in depth analysis on the evolution of the memory module content over time and provide visual evidence on how the proposed TMN is able to map both long term and short term relationships efficiently via a hierarchical structure.



### Automatic Skin Lesion Analysis using Large-scale Dermoscopy Images and Deep Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.04197v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04197v2)
- **Published**: 2017-03-12 23:32:18+00:00
- **Updated**: 2017-03-17 01:09:23+00:00
- **Authors**: Lei Bi, Jinman Kim, Euijoon Ahn, Dagan Feng
- **Comment**: Submission for ISIC2017 Challenge
- **Journal**: None
- **Summary**: Malignant melanoma has one of the most rapidly increasing incidences in the world and has a considerable mortality rate. Early diagnosis is particularly important since melanoma can be cured with prompt excision. Dermoscopy images play an important role in the non-invasive early detection of melanoma [1]. However, melanoma detection using human vision alone can be subjective, inaccurate and poorly reproducible even among experienced dermatologists. This is attributed to the challenges in interpreting images with diverse characteristics including lesions of varying sizes and shapes, lesions that may have fuzzy boundaries, different skin colors and the presence of hair [2]. Therefore, the automatic analysis of dermoscopy images is a valuable aid for clinical decision making and for image-based diagnosis to identify diseases such as melanoma [1-4]. Deep residual networks (ResNets) has achieved state-of-the-art results in image classification and detection related problems [5-8]. In this ISIC 2017 skin lesion analysis challenge [9], we propose to exploit the deep ResNets for robust visual features learning and representations.



