# Arxiv Papers in cs.CV on 2017-03-26
### Structured Learning of Tree Potentials in CRF for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1703.08764v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08764v1)
- **Published**: 2017-03-26 04:15:10+00:00
- **Updated**: 2017-03-26 04:15:10+00:00
- **Authors**: Fayao Liu, Guosheng Lin, Ruizhi Qiao, Chunhua Shen
- **Comment**: 10 pages. Appearing in IEEE Transactions on Neural Networks and
  Learning Systems
- **Journal**: None
- **Summary**: We propose a new approach to image segmentation, which exploits the advantages of both conditional random fields (CRFs) and decision trees. In the literature, the potential functions of CRFs are mostly defined as a linear combination of some pre-defined parametric models, and then methods like structured support vector machines (SSVMs) are applied to learn those linear coefficients. We instead formulate the unary and pairwise potentials as nonparametric forests---ensembles of decision trees, and learn the ensemble parameters and the trees in a unified optimization problem within the large-margin framework. In this fashion, we easily achieve nonlinear learning of potential functions on both unary and pairwise terms in CRFs. Moreover, we learn class-wise decision trees for each object that appears in the image. Due to the rich structure and flexibility of decision trees, our approach is powerful in modelling complex data likelihoods and label relationships. The resulting optimization problem is very challenging because it can have exponentially many variables and constraints. We show that this challenging optimization can be efficiently solved by combining a modified column generation and cutting-planes techniques. Experimental results on both binary (Graz-02, Weizmann horse, Oxford flower) and multi-class (MSRC-21, PASCAL VOC 2012) segmentation datasets demonstrate the power of the learned nonlinear nonparametric potentials.



### Open Vocabulary Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/1703.08769v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1703.08769v2)
- **Published**: 2017-03-26 05:44:56+00:00
- **Updated**: 2017-04-04 18:28:20+00:00
- **Authors**: Hang Zhao, Xavier Puig, Bolei Zhou, Sanja Fidler, Antonio Torralba
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scenes with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.



### SCAN: Structure Correcting Adversarial Network for Organ Segmentation in Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/1703.08770v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08770v2)
- **Published**: 2017-03-26 05:48:38+00:00
- **Updated**: 2017-04-10 16:18:36+00:00
- **Authors**: Wei Dai, Joseph Doyle, Xiaodan Liang, Hao Zhang, Nanqing Dong, Yuan Li, Eric P. Xing
- **Comment**: 10 pages, 7 figures, submitted to ICCV 2017
- **Journal**: None
- **Summary**: Chest X-ray (CXR) is one of the most commonly prescribed medical imaging procedures, often with over 2-10x more scans than other imaging modalities such as MRI, CT scan, and PET scans. These voluminous CXR scans place significant workloads on radiologists and medical practitioners. Organ segmentation is a crucial step to obtain effective computer-aided detection on CXR. In this work, we propose Structure Correcting Adversarial Network (SCAN) to segment lung fields and the heart in CXR images. SCAN incorporates a critic network to impose on the convolutional segmentation network the structural regularities emerging from human physiology. During training, the critic network learns to discriminate between the ground truth organ annotations from the masks synthesized by the segmentation network. Through this adversarial process the critic network learns the higher order structures and guides the segmentation model to achieve realistic segmentation outcomes. Extensive experiments show that our method produces highly accurate and natural segmentation. Using only very limited training data available, our model reaches human-level performance without relying on any existing trained model or dataset. Our method also generalizes well to CXR images from a different patient population and disease profiles, surpassing the current state-of-the-art.



### Multivariate Regression with Gross Errors on Manifold-valued Data
- **Arxiv ID**: http://arxiv.org/abs/1703.08772v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1703.08772v2)
- **Published**: 2017-03-26 05:53:39+00:00
- **Updated**: 2017-09-11 14:27:35+00:00
- **Authors**: Xiaowei Zhang, Xudong Shi, Yu Sun, Li Cheng
- **Comment**: 14 pages, submitted to an IEEE journal
- **Journal**: None
- **Summary**: We consider the topic of multivariate regression on manifold-valued output, that is, for a multivariate observation, its output response lies on a manifold. Moreover, we propose a new regression model to deal with the presence of grossly corrupted manifold-valued responses, a bottleneck issue commonly encountered in practical scenarios. Our model first takes a correction step on the grossly corrupted responses via geodesic curves on the manifold, and then performs multivariate linear regression on the corrected data. This results in a nonconvex and nonsmooth optimization problem on manifolds. To this end, we propose a dedicated approach named PALMR, by utilizing and extending the proximal alternating linearized minimization techniques. Theoretically, we investigate its convergence property, where it is shown to converge to a critical point under mild conditions. Empirically, we test our model on both synthetic and real diffusion tensor imaging data, and show that our model outperforms other multivariate regression models when manifold-valued responses contain gross errors, and is effective in identifying gross errors.



### Who Said What: Modeling Individual Labelers Improves Classification
- **Arxiv ID**: http://arxiv.org/abs/1703.08774v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.08774v2)
- **Published**: 2017-03-26 06:34:45+00:00
- **Updated**: 2018-01-04 21:46:22+00:00
- **Authors**: Melody Y. Guan, Varun Gulshan, Andrew M. Dai, Geoffrey E. Hinton
- **Comment**: AAAI 2018
- **Journal**: None
- **Summary**: Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona (2010), and by Mnih and Hinton (2012). Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training.



### Learned Multi-Patch Similarity
- **Arxiv ID**: http://arxiv.org/abs/1703.08836v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.08836v2)
- **Published**: 2017-03-26 16:17:55+00:00
- **Updated**: 2017-08-21 13:10:39+00:00
- **Authors**: Wilfried Hartmann, Silvano Galliani, Michal Havlena, Luc Van Gool, Konrad Schindler
- **Comment**: 10 pages, 7 figures, Accepted at ICCV 2017
- **Journal**: None
- **Summary**: Estimating a depth map from multiple views of a scene is a fundamental task in computer vision. As soon as more than two viewpoints are available, one faces the very basic question how to measure similarity across >2 image patches. Surprisingly, no direct solution exists, instead it is common to fall back to more or less robust averaging of two-view similarities. Encouraged by the success of machine learning, and in particular convolutional neural networks, we propose to learn a matching function which directly maps multiple image patches to a scalar similarity score. Experiments on several multi-view datasets demonstrate that this approach has advantages over methods based on pairwise patch similarity.



### Person Re-Identification by Camera Correlation Aware Feature Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1703.08837v1
- **DOI**: 10.1109/TPAMI.2017.2666805
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08837v1)
- **Published**: 2017-03-26 16:18:48+00:00
- **Updated**: 2017-03-26 16:18:48+00:00
- **Authors**: Ying-Cong Chen, Xiatian Zhu, Wei-Shi Zheng, Jian-Huang Lai
- **Comment**: To Appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence, 2017
- **Journal**: None
- **Summary**: The challenge of person re-identification (re-id) is to match individual images of the same person captured by different non-overlapping camera views against significant and unknown cross-view feature distortion. While a large number of distance metric/subspace learning models have been developed for re-id, the cross-view transformations they learned are view-generic and thus potentially less effective in quantifying the feature distortion inherent to each camera view. Learning view-specific feature transformations for re-id (i.e., view-specific re-id), an under-studied approach, becomes an alternative resort for this problem. In this work, we formulate a novel view-specific person re-identification framework from the feature augmentation point of view, called Camera coRrelation Aware Feature augmenTation (CRAFT). Specifically, CRAFT performs cross-view adaptation by automatically measuring camera correlation from cross-view visual data distribution and adaptively conducting feature augmentation to transform the original features into a new adaptive space. Through our augmentation framework, view-generic learning algorithms can be readily generalized to learn and optimize view-specific sub-models whilst simultaneously modelling view-generic discrimination information. Therefore, our framework not only inherits the strength of view-generic model learning but also provides an effective way to take into account view specific characteristics. Our CRAFT framework can be extended to jointly learn view-specific feature transformations for person re-id across a large network with more than two cameras, a largely under-investigated but realistic re-id setting. Additionally, we present a domain-generic deep person appearance representation which is designed particularly to be towards view invariant for facilitating cross-view adaptation by CRAFT.



### InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations
- **Arxiv ID**: http://arxiv.org/abs/1703.08840v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.08840v2)
- **Published**: 2017-03-26 16:20:36+00:00
- **Updated**: 2017-11-14 21:51:21+00:00
- **Authors**: Yunzhu Li, Jiaming Song, Stefano Ermon
- **Comment**: 14 pages, NIPS 2017
- **Journal**: None
- **Summary**: The goal of imitation learning is to mimic expert behavior without access to an explicit reward signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are typically not explicitly modeled. In this paper, we propose a new algorithm that can infer the latent structure of expert demonstrations in an unsupervised way. Our method, built on top of Generative Adversarial Imitation Learning, can not only imitate complex behaviors, but also learn interpretable and meaningful representations of complex behavioral data, including visual demonstrations. In the driving domain, we show that a model learned from human demonstrations is able to both accurately reproduce a variety of behaviors and accurately anticipate human actions using raw visual inputs. Compared with various baselines, our method can better capture the latent structure underlying expert demonstrations, often recovering semantically meaningful factors of variation in the data.



### Multi-View Deep Learning for Consistent Semantic Mapping with RGB-D Cameras
- **Arxiv ID**: http://arxiv.org/abs/1703.08866v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08866v2)
- **Published**: 2017-03-26 20:28:02+00:00
- **Updated**: 2017-12-04 19:01:11+00:00
- **Authors**: Lingni Ma, Jörg Stückler, Christian Kerl, Daniel Cremers
- **Comment**: the 2017 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS 2017)
- **Journal**: None
- **Summary**: Visual scene understanding is an important capability that enables robots to purposefully act in their environment. In this paper, we propose a novel approach to object-class segmentation from multiple RGB-D views using deep learning. We train a deep neural network to predict object-class semantics that is consistent from several view points in a semi-supervised way. At test time, the semantics predictions of our network can be fused more consistently in semantic keyframe maps than predictions of a network trained on individual views. We base our network architecture on a recent single-view deep learning approach to RGB and depth fusion for semantic object-class segmentation and enhance it with multi-scale loss minimization. We obtain the camera trajectory using RGB-D SLAM and warp the predictions of RGB-D images into ground-truth annotated frames in order to enforce multi-view consistency during training. At test time, predictions from multiple views are fused into keyframes. We propose and analyze several methods for enforcing multi-view consistency during training and testing. We evaluate the benefit of multi-view consistency training and demonstrate that pooling of deep features and fusion over multiple views outperforms single-view baselines on the NYUDv2 benchmark for semantic segmentation. Our end-to-end trained network achieves state-of-the-art performance on the NYUDv2 dataset in single-view segmentation as well as multi-view semantic fusion.



### Deceiving Google's Cloud Video Intelligence API Built for Summarizing Videos
- **Arxiv ID**: http://arxiv.org/abs/1703.09793v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.09793v2)
- **Published**: 2017-03-26 20:52:43+00:00
- **Updated**: 2017-03-31 05:25:36+00:00
- **Authors**: Hossein Hosseini, Baicen Xiao, Radha Poovendran
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the rapid progress of the techniques for image classification, video annotation has remained a challenging task. Automated video annotation would be a breakthrough technology, enabling users to search within the videos. Recently, Google introduced the Cloud Video Intelligence API for video analysis. As per the website, the system can be used to "separate signal from noise, by retrieving relevant information at the video, shot or per frame" level. A demonstration website has been also launched, which allows anyone to select a video for annotation. The API then detects the video labels (objects within the video) as well as shot labels (description of the video events over time). In this paper, we examine the usability of the Google's Cloud Video Intelligence API in adversarial environments. In particular, we investigate whether an adversary can subtly manipulate a video in such a way that the API will return only the adversary-desired labels. For this, we select an image, which is different from the video content, and insert it, periodically and at a very low rate, into the video. We found that if we insert one image every two seconds, the API is deceived into annotating the video as if it only contained the inserted image. Note that the modification to the video is hardly noticeable as, for instance, for a typical frame rate of 25, we insert only one image per 50 video frames. We also found that, by inserting one image per second, all the shot labels returned by the API are related to the inserted image. We perform the experiments on the sample videos provided by the API demonstration website and show that our attack is successful with different videos and images.



