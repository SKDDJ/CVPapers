# Arxiv Papers in cs.CV on 2017-03-03
### A Novel Multi-task Deep Learning Model for Skin Lesion Segmentation and Classification
- **Arxiv ID**: http://arxiv.org/abs/1703.01025v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01025v1)
- **Published**: 2017-03-03 03:22:16+00:00
- **Updated**: 2017-03-03 03:22:16+00:00
- **Authors**: Xulei Yang, Zeng Zeng, Si Yong Yeo, Colin Tan, Hong Liang Tey, Yi Su
- **Comment**: Submission to support ISIC 2017 challenge results
- **Journal**: None
- **Summary**: In this study, a multi-task deep neural network is proposed for skin lesion analysis. The proposed multi-task learning model solves different tasks (e.g., lesion segmentation and two independent binary lesion classifications) at the same time by exploiting commonalities and differences across tasks. This results in improved learning efficiency and potential prediction accuracy for the task-specific models, when compared to training the individual models separately. The proposed multi-task deep learning model is trained and evaluated on the dermoscopic image sets from the International Skin Imaging Collaboration (ISIC) 2017 Challenge - Skin Lesion Analysis towards Melanoma Detection, which consists of 2000 training samples and 150 evaluation samples. The experimental results show that the proposed multi-task deep learning model achieves promising performances on skin lesion segmentation and classification. The average value of Jaccard index for lesion segmentation is 0.724, while the average values of area under the receiver operating characteristic curve (AUC) on two individual lesion classifications are 0.880 and 0.972, respectively.



### Outlier Cluster Formation in Spectral Clustering
- **Arxiv ID**: http://arxiv.org/abs/1703.01028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01028v1)
- **Published**: 2017-03-03 04:02:11+00:00
- **Updated**: 2017-03-03 04:02:11+00:00
- **Authors**: Takuro Ina, Atsushi Hashimoto, Masaaki Iiyama, Hidekazu Kasahara, Mikihiko Mori, Michihiko Minoh
- **Comment**: 10 pages, 2 figures, 2 tables
- **Journal**: None
- **Summary**: Outlier detection and cluster number estimation is an important issue for clustering real data. This paper focuses on spectral clustering, a time-tested clustering method, and reveals its important properties related to outliers. The highlights of this paper are the following two mathematical observations: first, spectral clustering's intrinsic property of an outlier cluster formation, and second, the singularity of an outlier cluster with a valid cluster number. Based on these observations, we designed a function that evaluates clustering and outlier detection results. In experiments, we prepared two scenarios, face clustering in photo album and person re-identification in a camera network. We confirmed that the proposed method detects outliers and estimates the number of clusters properly in both problems. Our method outperforms state-of-the-art methods in both the 128-dimensional sparse space for face clustering and the 4,096-dimensional non-sparse space for person re-identification.



### Learning Robot Activities from First-Person Human Videos Using Convolutional Future Regression
- **Arxiv ID**: http://arxiv.org/abs/1703.01040v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.01040v2)
- **Published**: 2017-03-03 05:27:50+00:00
- **Updated**: 2017-07-24 08:02:11+00:00
- **Authors**: Jangwon Lee, Michael S. Ryoo
- **Comment**: None
- **Journal**: None
- **Summary**: We design a new approach that allows robot learning of new activities from unlabeled human example videos. Given videos of humans executing the same activity from a human's viewpoint (i.e., first-person videos), our objective is to make the robot learn the temporal structure of the activity as its future regression network, and learn to transfer such model for its own motor execution. We present a new deep learning model: We extend the state-of-the-art convolutional object detection network for the representation/estimation of human hands in training videos, and newly introduce the concept of using a fully convolutional network to regress (i.e., predict) the intermediate scene representation corresponding to the future frame (e.g., 1-2 seconds later). Combining these allows direct prediction of future locations of human hands and objects, which enables the robot to infer the motor control plan using our manipulation network. We experimentally confirm that our approach makes learning of robot activities from unlabeled human interaction videos possible, and demonstrate that our robot is able to execute the learned collaborative activities in real-time directly based on its camera input.



### Large-Scale Evolution of Image Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1703.01041v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.DC, I.2.6; I.5.1; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1703.01041v2)
- **Published**: 2017-03-03 05:41:30+00:00
- **Updated**: 2017-06-11 08:42:28+00:00
- **Authors**: Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc Le, Alex Kurakin
- **Comment**: Accepted for publication at ICML 2017 (34th International Conference
  on Machine Learning)
- **Journal**: None
- **Summary**: Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6% (95.6% for ensemble) and 77.0%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.



### Skin Lesion Classification using Class Activation Map
- **Arxiv ID**: http://arxiv.org/abs/1703.01053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01053v1)
- **Published**: 2017-03-03 06:38:30+00:00
- **Updated**: 2017-03-03 06:38:30+00:00
- **Authors**: Xi Jia, Linlin Shen
- **Comment**: None
- **Journal**: None
- **Summary**: We proposed a two stage framework with only one network to analyze skin lesion images, we firstly trained a convolutional network to classify these images, and cropped the import regions which the network has the maximum activation value. In the second stage, we retrained this CNN with the image regions extracted from stage one and output the final probabilities. The two stage framework achieved a mean AUC of 0.857 in ISIC-2017 skin lesion validation set and is 0.04 higher than that of the original inputs, 0.821.



### Arbitrary-Oriented Scene Text Detection via Rotation Proposals
- **Arxiv ID**: http://arxiv.org/abs/1703.01086v3
- **DOI**: 10.1109/TMM.2018.2818020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01086v3)
- **Published**: 2017-03-03 09:24:41+00:00
- **Updated**: 2018-03-15 07:07:36+00:00
- **Authors**: Jianqi Ma, Weiyuan Shao, Hao Ye, Li Wang, Hong Wang, Yingbin Zheng, Xiangyang Xue
- **Comment**: Code is available at: https://github.com/mjq11302010044/RRPN
- **Journal**: IEEE Transactions on Multimedia, vol. 20, no. 11, pp. 3111-3122,
  2018
- **Summary**: This paper introduces a novel rotation-based framework for arbitrary-oriented text detection in natural scene images. We present the Rotation Region Proposal Networks (RRPN), which are designed to generate inclined proposals with text orientation angle information. The angle information is then adapted for bounding box regression to make the proposals more accurately fit into the text region in terms of the orientation. The Rotation Region-of-Interest (RRoI) pooling layer is proposed to project arbitrary-oriented proposals to a feature map for a text region classifier. The whole framework is built upon a region-proposal-based architecture, which ensures the computational efficiency of the arbitrary-oriented text detection compared with previous text detection systems. We conduct experiments using the rotation-based framework on three real-world scene text detection datasets and demonstrate its superiority in terms of effectiveness and efficiency over previous approaches.



### Adversarial Examples for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1703.01101v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CR, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1703.01101v1)
- **Published**: 2017-03-03 10:27:16+00:00
- **Updated**: 2017-03-03 10:27:16+00:00
- **Authors**: Volker Fischer, Mummadi Chaithanya Kumar, Jan Hendrik Metzen, Thomas Brox
- **Comment**: ICLR 2017 workshop submission
- **Journal**: None
- **Summary**: Machine learning methods in general and Deep Neural Networks in particular have shown to be vulnerable to adversarial perturbations. So far this phenomenon has mainly been studied in the context of whole-image classification. In this contribution, we analyse how adversarial perturbations can affect the task of semantic segmentation. We show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible adversarial perturbations that lead a deep network to misclassify almost all pixels of a chosen class while leaving network prediction nearly unchanged outside this class.



### Deep artifact learning for compressed sensing and parallel MRI
- **Arxiv ID**: http://arxiv.org/abs/1703.01120v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01120v1)
- **Published**: 2017-03-03 12:02:32+00:00
- **Updated**: 2017-03-03 12:02:32+00:00
- **Authors**: Dongwook Lee, Jaejun Yoo, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Compressed sensing MRI (CS-MRI) from single and parallel coils is one of the powerful ways to reduce the scan time of MR imaging with performance guarantee. However, the computational costs are usually expensive. This paper aims to propose a computationally fast and accurate deep learning algorithm for the reconstruction of MR images from highly down-sampled k-space data.   Theory: Based on the topological analysis, we show that the data manifold of the aliasing artifact is easier to learn from a uniform subsampling pattern with additional low-frequency k-space data. Thus, we develop deep aliasing artifact learning networks for the magnitude and phase images to estimate and remove the aliasing artifacts from highly accelerated MR acquisition.   Methods: The aliasing artifacts are directly estimated from the distorted magnitude and phase images reconstructed from subsampled k-space data so that we can get an aliasing-free images by subtracting the estimated aliasing artifact from corrupted inputs. Moreover, to deal with the globally distributed aliasing artifact, we develop a multi-scale deep neural network with a large receptive field.   Results: The experimental results confirm that the proposed deep artifact learning network effectively estimates and removes the aliasing artifacts. Compared to existing CS methods from single and multi-coli data, the proposed network shows minimal errors by removing the coherent aliasing artifacts. Furthermore, the computational time is by order of magnitude faster.   Conclusion: As the proposed deep artifact learning network immediately generates accurate reconstruction, it has great potential for clinical applications.



### Deep Learning with Domain Adaptation for Accelerated Projection-Reconstruction MR
- **Arxiv ID**: http://arxiv.org/abs/1703.01135v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01135v2)
- **Published**: 2017-03-03 12:49:36+00:00
- **Updated**: 2018-01-09 04:28:28+00:00
- **Authors**: Yo Seob Han, Jaejun Yoo, Jong Chul Ye
- **Comment**: This paper has been accepted and will soon appear in Magnetic
  Resonance in Medicine
- **Journal**: None
- **Summary**: Purpose: The radial k-space trajectory is a well-established sampling trajectory used in conjunction with magnetic resonance imaging. However, the radial k-space trajectory requires a large number of radial lines for high-resolution reconstruction. Increasing the number of radial lines causes longer acquisition time, making it more difficult for routine clinical use. On the other hand, if we reduce the number of radial lines, streaking artifact patterns are unavoidable. To solve this problem, we propose a novel deep learning approach with domain adaptation to restore high-resolution MR images from under-sampled k-space data.   Methods: The proposed deep network removes the streaking artifacts from the artifact corrupted images. To address the situation given the limited available data, we propose a domain adaptation scheme that employs a pre-trained network using a large number of x-ray computed tomography (CT) or synthesized radial MR datasets, which is then fine-tuned with only a few radial MR datasets.   Results: The proposed method outperforms existing compressed sensing algorithms, such as the total variation and PR-FOCUSS methods. In addition, the calculation time is several orders of magnitude faster than the total variation and PR-FOCUSS methods.Moreover, we found that pre-training using CT or MR data from similar organ data is more important than pre-training using data from the same modality for different organ.   Conclusion: We demonstrate the possibility of a domain-adaptation when only a limited amount of MR data is available. The proposed method surpasses the existing compressed sensing algorithms in terms of the image quality and computation time.



### A Survey on Content-Aware Video Analysis for Sports
- **Arxiv ID**: http://arxiv.org/abs/1703.01170v1
- **DOI**: 10.1109/TCSVT.2017.2655624
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1703.01170v1)
- **Published**: 2017-03-03 14:28:03+00:00
- **Updated**: 2017-03-03 14:28:03+00:00
- **Authors**: Huang-Chia Shih
- **Comment**: Accepted for publication in IEEE Transactions on Circuits and Systems
  for Video Technology (TCSVT)
- **Journal**: None
- **Summary**: Sports data analysis is becoming increasingly large-scale, diversified, and shared, but difficulty persists in rapidly accessing the most crucial information. Previous surveys have focused on the methodologies of sports video analysis from the spatiotemporal viewpoint instead of a content-based viewpoint, and few of these studies have considered semantics. This study develops a deeper interpretation of content-aware sports video analysis by examining the insight offered by research into the structure of content under different scenarios. On the basis of this insight, we provide an overview of the themes particularly relevant to the research on content-aware systems for broadcast sports. Specifically, we focus on the video content analysis techniques applied in sportscasts over the past decade from the perspectives of fundamentals and general review, a content hierarchical model, and trends and challenges. Content-aware analysis methods are discussed with respect to object-, event-, and context-oriented groups. In each group, the gap between sensation and content excitement must be bridged using proper strategies. In this regard, a content-aware approach is required to determine user demands. Finally, the paper summarizes the future trends and challenges for sports video analysis. We believe that our findings can advance the field of research on content-aware video analysis for broadcast sports.



### EmotioNet Challenge: Recognition of facial expressions of emotion in the wild
- **Arxiv ID**: http://arxiv.org/abs/1703.01210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01210v1)
- **Published**: 2017-03-03 15:35:52+00:00
- **Updated**: 2017-03-03 15:35:52+00:00
- **Authors**: C. Fabian Benitez-Quiroz, Ramprakash Srinivasan, Qianli Feng, Yan Wang, Aleix M. Martinez
- **Comment**: None
- **Journal**: None
- **Summary**: This paper details the methodology and results of the EmotioNet challenge. This challenge is the first to test the ability of computer vision algorithms in the automatic analysis of a large number of images of facial expressions of emotion in the wild. The challenge was divided into two tracks. The first track tested the ability of current computer vision algorithms in the automatic detection of action units (AUs). Specifically, we tested the detection of 11 AUs. The second track tested the algorithms' ability to recognize emotion categories in images of facial expressions. Specifically, we tested the recognition of 16 basic and compound emotion categories. The results of the challenge suggest that current computer vision and machine learning algorithms are unable to reliably solve these two tasks. The limitations of current algorithms are more apparent when trying to recognize emotion. We also show that current algorithms are not affected by mild resolution changes, small occluders, gender or age, but that 3D pose is a major limiting factor on performance. We provide an in-depth discussion of the points that need special attention moving forward.



### Denoising Adversarial Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1703.01220v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.01220v4)
- **Published**: 2017-03-03 15:59:16+00:00
- **Updated**: 2018-01-04 17:18:16+00:00
- **Authors**: Antonia Creswell, Anil Anthony Bharath
- **Comment**: submitted to journal
- **Journal**: None
- **Summary**: Unsupervised learning is of growing interest because it unlocks the potential held in vast amounts of unlabelled data to learn useful representations for inference. Autoencoders, a form of generative model, may be trained by learning to reconstruct unlabelled input data from a latent representation space. More robust representations may be produced by an autoencoder if it learns to recover clean input samples from corrupted ones. Representations may be further improved by introducing regularisation during training to shape the distribution of the encoded data in latent space. We suggest denoising adversarial autoencoders, which combine denoising and regularisation, shaping the distribution of latent space using adversarial training. We introduce a novel analysis that shows how denoising may be incorporated into the training and sampling of adversarial autoencoders. Experiments are performed to assess the contributions that denoising makes to the learning of representations for classification and sample synthesis. Our results suggest that autoencoders trained using a denoising criterion achieve higher classification performance, and can synthesise samples that are more consistent with the input data than those trained without a corruption process.



### Context Aware Query Image Representation for Particular Object Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1703.01226v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1703.01226v1)
- **Published**: 2017-03-03 16:14:53+00:00
- **Updated**: 2017-03-03 16:14:53+00:00
- **Authors**: Zakaria Laskar, Juho Kannala
- **Comment**: 14 pages, Extended version of a manuscript submitted to SCIA 2017
- **Journal**: None
- **Summary**: The current models of image representation based on Convolutional Neural Networks (CNN) have shown tremendous performance in image retrieval. Such models are inspired by the information flow along the visual pathway in the human visual cortex. We propose that in the field of particular object retrieval, the process of extracting CNN representations from query images with a given region of interest (ROI) can also be modelled by taking inspiration from human vision. Particularly, we show that by making the CNN pay attention on the ROI while extracting query image representation leads to significant improvement over the baseline methods on challenging Oxford5k and Paris6k datasets. Furthermore, we propose an extension to a recently introduced encoding method for CNN representations, regional maximum activations of convolutions (R-MAC). The proposed extension weights the regional representations using a novel saliency measure prior to aggregation. This leads to further improvement in retrieval accuracy.



### Deep Collaborative Learning for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.01229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01229v1)
- **Published**: 2017-03-03 16:17:45+00:00
- **Updated**: 2017-03-03 16:17:45+00:00
- **Authors**: Yan Wang, Lingxi Xie, Ya Zhang, Wenjun Zhang, Alan Yuille
- **Comment**: Submitted to CVPR 2017 (10 pages, 5 figures)
- **Journal**: None
- **Summary**: Deep neural networks are playing an important role in state-of-the-art visual recognition. To represent high-level visual concepts, modern networks are equipped with large convolutional layers, which use a large number of filters and contribute significantly to model complexity. For example, more than half of the weights of AlexNet are stored in the first fully-connected layer (4,096 filters).   We formulate the function of a convolutional layer as learning a large visual vocabulary, and propose an alternative way, namely Deep Collaborative Learning (DCL), to reduce the computational complexity. We replace a convolutional layer with a two-stage DCL module, in which we first construct a couple of smaller convolutional layers individually, and then fuse them at each spatial position to consider feature co-occurrence. In mathematics, DCL can be explained as an efficient way of learning compositional visual concepts, in which the vocabulary size increases exponentially while the model complexity only increases linearly. We evaluate DCL on a wide range of visual recognition tasks, including a series of multi-digit number classification datasets, and some generic image classification datasets such as SVHN, CIFAR and ILSVRC2012. We apply DCL to several state-of-the-art network structures, improving the recognition accuracy meanwhile reducing the number of parameters (16.82% fewer in AlexNet).



### Incident Light Frequency-based Image Defogging Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1703.01248v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01248v1)
- **Published**: 2017-03-03 17:03:17+00:00
- **Updated**: 2017-03-03 17:03:17+00:00
- **Authors**: Wenbo Zhang, Xiaorong Hou
- **Comment**: None
- **Journal**: None
- **Summary**: Considering the problem of color distortion caused by the defogging algorithm based on dark channel prior, an improved algorithm was proposed to calculate the transmittance of all channels respectively. First, incident light frequency's effect on the transmittance of various color channels was analyzed according to the Beer-Lambert's Law, from which a proportion among various channel transmittances was derived; afterwards, images were preprocessed by down-sampling to refine transmittance, and then the original size was restored to enhance the operational efficiency of the algorithm; finally, the transmittance of all color channels was acquired in accordance with the proportion, and then the corresponding transmittance was used for image restoration in each channel. The experimental results show that compared with the existing algorithm, this improved image defogging algorithm could make image colors more natural, solve the problem of slightly higher color saturation caused by the existing algorithm, and shorten the operation time by four to nine times.



### Instance Flow Based Online Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1703.01289v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01289v2)
- **Published**: 2017-03-03 18:54:55+00:00
- **Updated**: 2017-05-15 14:14:30+00:00
- **Authors**: Sebastian Bullinger, Christoph Bodensteiner, Michael Arens
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method to perform online Multiple Object Tracking (MOT) of known object categories in monocular video data. Current Tracking-by-Detection MOT approaches build on top of 2D bounding box detections. In contrast, we exploit state-of-the-art instance aware semantic segmentation techniques to compute 2D shape representations of target objects in each frame. We predict position and shape of segmented instances in subsequent frames by exploiting optical flow cues. We define an affinity matrix between instances of subsequent frames which reflects locality and visual similarity. The instance association is solved by applying the Hungarian method. We evaluate different configurations of our algorithm using the MOT 2D 2015 train dataset. The evaluation shows that our tracking approach is able to track objects with high relative motions. In addition, we provide results of our approach on the MOT 2D 2015 test set for comparison with previous works. We achieve a MOTA score of 32.1.



### Bridging Saliency Detection to Weakly Supervised Object Detection Based on Self-paced Curriculum Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.01290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01290v1)
- **Published**: 2017-03-03 18:55:10+00:00
- **Updated**: 2017-03-03 18:55:10+00:00
- **Authors**: Dingwen Zhang, Deyu Meng, Long Zhao, Junwei Han
- **Comment**: Has published in IJCAI 16
- **Journal**: None
- **Summary**: Weakly-supervised object detection (WOD) is a challenging problems in computer vision. The key problem is to simultaneously infer the exact object locations in the training images and train the object detectors, given only the training images with weak image-level labels. Intuitively, by simulating the selective attention mechanism of human visual system, saliency detection technique can select attractive objects in scenes and thus is a potential way to provide useful priors for WOD. However, the way to adopt saliency detection in WOD is not trivial since the detected saliency region might be possibly highly ambiguous in complex cases. To this end, this paper first comprehensively analyzes the challenges in applying saliency detection to WOD. Then, we make one of the earliest efforts to bridge saliency detection to WOD via the self-paced curriculum learning, which can guide the learning procedure to gradually achieve faithful knowledge of multi-class objects from easy to hard. The experimental results demonstrate that the proposed approach can successfully bridge saliency detection and WOD tasks and achieve the state-of-the-art object detection results under the weak supervision.



### Detecting Cancer Metastases on Gigapixel Pathology Images
- **Arxiv ID**: http://arxiv.org/abs/1703.02442v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02442v2)
- **Published**: 2017-03-03 19:03:08+00:00
- **Updated**: 2017-03-08 04:47:57+00:00
- **Authors**: Yun Liu, Krishna Gadepalli, Mohammad Norouzi, George E. Dahl, Timo Kohlberger, Aleksey Boyko, Subhashini Venugopalan, Aleksei Timofeev, Philip Q. Nelson, Greg S. Corrado, Jason D. Hipp, Lily Peng, Martin C. Stumpe
- **Comment**: Fig 1: normal and tumor patches were accidentally reversed - now
  fixed. Minor grammatical corrections in appendix, section "Image Color
  Normalization"
- **Journal**: MICCAI Tutorial (2017)
- **Summary**: Each year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection.



