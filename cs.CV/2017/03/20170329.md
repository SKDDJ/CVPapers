# Arxiv Papers in cs.CV on 2017-03-29
### Automatic Detection of Knee Joints and Quantification of Knee Osteoarthritis Severity using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.09856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09856v1)
- **Published**: 2017-03-29 01:29:32+00:00
- **Updated**: 2017-03-29 01:29:32+00:00
- **Authors**: Joseph Antony, Kevin McGuinness, Kieran Moran, Noel E O'Connor
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a new approach to automatically quantify the severity of knee OA using X-ray images. Automatically quantifying knee OA severity involves two steps: first, automatically localizing the knee joints; next, classifying the localized knee joint images. We introduce a new approach to automatically detect the knee joints using a fully convolutional neural network (FCN). We train convolutional neural networks (CNN) from scratch to automatically quantify the knee OA severity optimizing a weighted ratio of two loss functions: categorical cross-entropy and mean-squared loss. This joint training further improves the overall quantification of knee OA severity, with the added benefit of naturally producing simultaneous multi-class classification and regression outputs. Two public datasets are used to evaluate our approach, the Osteoarthritis Initiative (OAI) and the Multicenter Osteoarthritis Study (MOST), with extremely promising results that outperform existing approaches.



### Click Here: Human-Localized Keypoints as Guidance for Viewpoint Estimation
- **Arxiv ID**: http://arxiv.org/abs/1703.09859v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09859v2)
- **Published**: 2017-03-29 02:08:08+00:00
- **Updated**: 2017-08-04 23:01:39+00:00
- **Authors**: Ryan Szeto, Jason J. Corso
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: We motivate and address a human-in-the-loop variant of the monocular viewpoint estimation task in which the location and class of one semantic object keypoint is available at test time. In order to leverage the keypoint information, we devise a Convolutional Neural Network called Click-Here CNN (CH-CNN) that integrates the keypoint information with activations from the layers that process the image. It transforms the keypoint information into a 2D map that can be used to weigh features from certain parts of the image more heavily. The weighted sum of these spatial features is combined with global image features to provide relevant information to the prediction layers. To train our network, we collect a novel dataset of 3D keypoint annotations on thousands of CAD models, and synthetically render millions of images with 2D keypoint information. On test instances from PASCAL 3D+, our model achieves a mean class accuracy of 90.7%, whereas the state-of-the-art baseline only obtains 85.7% mean class accuracy, justifying our argument for human-in-the-loop inference.



### Novel Structured Low-rank algorithm to recover spatially smooth exponential image time series
- **Arxiv ID**: http://arxiv.org/abs/1703.09880v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09880v1)
- **Published**: 2017-03-29 04:12:10+00:00
- **Updated**: 2017-03-29 04:12:10+00:00
- **Authors**: Arvind Balachandrasekaran, Mathews Jacob
- **Comment**: 4 pages, 3 figures, accepted at ISBI 2017, Melbourne, Australia
- **Journal**: None
- **Summary**: We propose a structured low rank matrix completion algorithm to recover a time series of images consisting of linear combination of exponential parameters at every pixel, from under-sampled Fourier measurements. The spatial smoothness of these parameters is exploited along with the exponential structure of the time series at every pixel, to derive an annihilation relation in the $k-t$ domain. This annihilation relation translates into a structured low rank matrix formed from the $k-t$ samples. We demonstrate the algorithm in the parameter mapping setting and show significant improvement over state of the art methods.



### LabelBank: Revisiting Global Perspectives for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1703.09891v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.09891v1)
- **Published**: 2017-03-29 05:58:21+00:00
- **Updated**: 2017-03-29 05:58:21+00:00
- **Authors**: Hexiang Hu, Zhiwei Deng, Guang-Tong Zhou, Fei Sha, Greg Mori
- **Comment**: Pre-prints
- **Journal**: None
- **Summary**: Semantic segmentation requires a detailed labeling of image pixels by object category. Information derived from local image patches is necessary to describe the detailed shape of individual objects. However, this information is ambiguous and can result in noisy labels. Global inference of image content can instead capture the general semantic concepts present. We advocate that holistic inference of image concepts provides valuable information for detailed pixel labeling. We propose a generic framework to leverage holistic information in the form of a LabelBank for pixel-level segmentation.   We show the ability of our framework to improve semantic segmentation performance in a variety of settings. We learn models for extracting a holistic LabelBank from visual cues, attributes, and/or textual descriptions. We demonstrate improvements in semantic segmentation accuracy on standard datasets across a range of state-of-the-art segmentation architectures and holistic inference approaches.



### Learning with Privileged Information for Multi-Label Classification
- **Arxiv ID**: http://arxiv.org/abs/1703.09911v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09911v1)
- **Published**: 2017-03-29 07:17:52+00:00
- **Updated**: 2017-03-29 07:17:52+00:00
- **Authors**: Shiyu Chen, Shangfei Wang, Tanfang Chen, Xiaoxiao Shi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach for learning multi-label classifiers with the help of privileged information. Specifically, we use similarity constraints to capture the relationship between available information and privileged information, and use ranking constraints to capture the dependencies among multiple labels. By integrating similarity constraints and ranking constraints into the learning process of classifiers, the privileged information and the dependencies among multiple labels are exploited to construct better classifiers during training. A maximum margin classifier is adopted, and an efficient learning algorithm of the proposed method is also developed. We evaluate the proposed method on two applications: multiple object recognition from images with the help of implicit information about object importance conveyed by the list of manually annotated image tags; and multiple facial action unit detection from low-resolution images augmented by high-resolution images. Experimental results demonstrate that the proposed method can effectively take full advantage of privileged information and dependencies among multiple labels for better object recognition and better facial action unit detection.



### One Network to Solve Them All --- Solving Linear Inverse Problems using Deep Projection Models
- **Arxiv ID**: http://arxiv.org/abs/1703.09912v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/1703.09912v1)
- **Published**: 2017-03-29 07:20:10+00:00
- **Updated**: 2017-03-29 07:20:10+00:00
- **Authors**: J. H. Rick Chang, Chun-Liang Li, Barnabas Poczos, B. V. K. Vijaya Kumar, Aswin C. Sankaranarayanan
- **Comment**: None
- **Journal**: None
- **Summary**: While deep learning methods have achieved state-of-the-art performance in many challenging inverse problems like image inpainting and super-resolution, they invariably involve problem-specific training of the networks. Under this approach, different problems require different networks. In scenarios where we need to solve a wide variety of problems, e.g., on a mobile camera, it is inefficient and costly to use these specially-trained networks. On the other hand, traditional methods using signal priors can be used in all linear inverse problems but often have worse performance on challenging tasks. In this work, we provide a middle ground between the two kinds of methods --- we propose a general framework to train a single deep neural network that solves arbitrary linear inverse problems. The proposed network acts as a proximal operator for an optimization algorithm and projects non-image signals onto the set of natural images defined by the decision boundary of a classifier. In our experiments, the proposed framework demonstrates superior performance over traditional methods using a wavelet sparsity prior and achieves comparable performance of specially-trained networks on tasks including compressive sensing and pixel-wise inpainting.



### Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination
- **Arxiv ID**: http://arxiv.org/abs/1703.09913v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09913v2)
- **Published**: 2017-03-29 07:25:33+00:00
- **Updated**: 2018-03-29 11:18:54+00:00
- **Authors**: Hazel Doughty, Dima Damen, Walterio Mayol-Cuevas
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: We present a method for assessing skill from video, applicable to a variety of tasks, ranging from surgery to drawing and rolling pizza dough. We formulate the problem as pairwise (who's better?) and overall (who's best?) ranking of video collections, using supervised deep ranking. We propose a novel loss function that learns discriminative features when a pair of videos exhibit variance in skill, and learns shared features when a pair of videos exhibit comparable skill levels. Results demonstrate our method is applicable across tasks, with the percentage of correctly ordered pairs of videos ranging from 70% to 83% for four datasets. We demonstrate the robustness of our approach via sensitivity analysis of its parameters. We see this work as effort toward the automated organization of how-to video collections and overall, generic skill determination in video.



### Towards thinner convolutional neural networks through Gradually Global Pruning
- **Arxiv ID**: http://arxiv.org/abs/1703.09916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09916v1)
- **Published**: 2017-03-29 07:31:46+00:00
- **Updated**: 2017-03-29 07:31:46+00:00
- **Authors**: Zhengtao Wang, Ce Zhu, Zhiqiang Xia, Qi Guo, Yipeng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep network pruning is an effective method to reduce the storage and computation cost of deep neural networks when applying them to resource-limited devices. Among many pruning granularities, neuron level pruning will remove redundant neurons and filters in the model and result in thinner networks. In this paper, we propose a gradually global pruning scheme for neuron level pruning. In each pruning step, a small percent of neurons were selected and dropped across all layers in the model. We also propose a simple method to eliminate the biases in evaluating the importance of neurons to make the scheme feasible. Compared with layer-wise pruning scheme, our scheme avoid the difficulty in determining the redundancy in each layer and is more effective for deep networks. Our scheme would automatically find a thinner sub-network in original network under a given performance.



### Bundle Optimization for Multi-aspect Embedding
- **Arxiv ID**: http://arxiv.org/abs/1703.09928v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09928v3)
- **Published**: 2017-03-29 08:29:55+00:00
- **Updated**: 2017-09-16 03:16:06+00:00
- **Authors**: Qiong Zeng, Baoquan Chen, Yanir Kleiman, Daniel Cohen-Or, Yangyan Li
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding semantic similarity among images is the core of a wide range of computer vision applications. An important step towards this goal is to collect and learn human perceptions. Interestingly, the semantic context of images is often ambiguous as images can be perceived with emphasis on different aspects, which may be contradictory to each other.   In this paper, we present a method for learning the semantic similarity among images, inferring their latent aspects and embedding them into multi-spaces corresponding to their semantic aspects.   We consider the multi-embedding problem as an optimization function that evaluates the embedded distances with respect to the qualitative clustering queries. The key idea of our approach is to collect and embed qualitative measures that share the same aspects in bundles. To ensure similarity aspect sharing among multiple measures, image classification queries are presented to, and solved by users. The collected image clusters are then converted into bundles of tuples, which are fed into our bundle optimization algorithm that jointly infers the aspect similarity and multi-aspect embedding. Extensive experimental results show that our approach significantly outperforms state-of-the-art multi-embedding approaches on various datasets, and scales well for large multi-aspect similarity measures.



### Sentiment Recognition in Egocentric Photostreams
- **Arxiv ID**: http://arxiv.org/abs/1703.09933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09933v1)
- **Published**: 2017-03-29 08:38:32+00:00
- **Updated**: 2017-03-29 08:38:32+00:00
- **Authors**: Estefania Talavera, Nicola Strisciuglio, Nicolai Petkov, Petia Radeva
- **Comment**: None
- **Journal**: None
- **Summary**: Lifelogging is a process of collecting rich source of information about daily life of people. In this paper, we introduce the problem of sentiment analysis in egocentric events focusing on the moments that compose the images recalling positive, neutral or negative feelings to the observer. We propose a method for the classification of the sentiments in egocentric pictures based on global and semantic image features extracted by Convolutional Neural Networks. We carried out experiments on an egocentric dataset, which we organized in 3 classes on the basis of the sentiment that is recalled to the user (positive, negative or neutral).



### Image Restoration using Autoencoding Priors
- **Arxiv ID**: http://arxiv.org/abs/1703.09964v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1703.09964v1)
- **Published**: 2017-03-29 10:51:49+00:00
- **Updated**: 2017-03-29 10:51:49+00:00
- **Authors**: Siavash Arjomand Bigdeli, Matthias Zwicker
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to leverage denoising autoencoder networks as priors to address image restoration problems. We build on the key observation that the output of an optimal denoising autoencoder is a local mean of the true data density, and the autoencoder error (the difference between the output and input of the trained autoencoder) is a mean shift vector. We use the magnitude of this mean shift vector, that is, the distance to the local mean, as the negative log likelihood of our natural image prior. For image restoration, we maximize the likelihood using gradient descent by backpropagating the autoencoder error. A key advantage of our approach is that we do not need to train separate networks for different image restoration tasks, such as non-blind deconvolution with different kernels, or super-resolution at different magnification factors. We demonstrate state of the art results for non-blind deconvolution and super-resolution using the same autoencoding prior.



### A Geometric Framework for Stochastic Shape Analysis
- **Arxiv ID**: http://arxiv.org/abs/1703.09971v3
- **DOI**: 10.1007/s10208-018-9394-z
- **Categories**: **cs.CV**, math.DS, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1703.09971v3)
- **Published**: 2017-03-29 11:08:00+00:00
- **Updated**: 2018-10-20 19:12:05+00:00
- **Authors**: Alexis Arnaudon, Darryl D. Holm, Stefan Sommer
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a stochastic model of diffeomorphisms, whose action on a variety of data types descends to stochastic evolution of shapes, images and landmarks. The stochasticity is introduced in the vector field which transports the data in the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework for shape analysis and image registration. The stochasticity thereby models errors or uncertainties of the flow in following the prescribed deformation velocity. The approach is illustrated in the example of finite dimensional landmark manifolds, whose stochastic evolution is studied both via the Fokker-Planck equation and by numerical simulations. We derive two approaches for inferring parameters of the stochastic model from landmark configurations observed at discrete time points. The first of the two approaches matches moments of the Fokker-Planck equation to sample moments of the data, while the second approach employs an Expectation-Maximisation based algorithm using a Monte Carlo bridge sampling scheme to optimise the data likelihood. We derive and numerically test the ability of the two approaches to infer the spatial correlation length of the underlying noise.



### Iterative Object and Part Transfer for Fine-Grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.09983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09983v1)
- **Published**: 2017-03-29 11:50:34+00:00
- **Updated**: 2017-03-29 11:50:34+00:00
- **Authors**: Zhiqiang Shen, Yu-Gang Jiang, Dequan Wang, Xiangyang Xue
- **Comment**: To appear in ICME 2017 as an oral paper
- **Journal**: None
- **Summary**: The aim of fine-grained recognition is to identify sub-ordinate categories in images like different species of birds. Existing works have confirmed that, in order to capture the subtle differences across the categories, automatic localization of objects and parts is critical. Most approaches for object and part localization relied on the bottom-up pipeline, where thousands of region proposals are generated and then filtered by pre-trained object/part models. This is computationally expensive and not scalable once the number of objects/parts becomes large. In this paper, we propose a nonparametric data-driven method for object and part localization. Given an unlabeled test image, our approach transfers annotations from a few similar images retrieved in the training set. In particular, we propose an iterative transfer strategy that gradually refine the predicted bounding boxes. Based on the located objects and parts, deep convolutional features are extracted for recognition. We evaluate our approach on the widely-used CUB200-2011 dataset and a new and large dataset called Birdsnap. On both datasets, we achieve better results than many state-of-the-art approaches, including a few using oracle (manually annotated) bounding boxes in the test images.



### Object categorization in finer levels requires higher spatial frequencies, and therefore takes longer
- **Arxiv ID**: http://arxiv.org/abs/1703.09990v1
- **DOI**: 10.3389/fpsyg.2017.01261
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.09990v1)
- **Published**: 2017-03-29 12:03:21+00:00
- **Updated**: 2017-03-29 12:03:21+00:00
- **Authors**: Matin N. Ashtiani, Saeed Reza Kheradpisheh, Timothée Masquelier, Mohammad Ganjtabesh
- **Comment**: None
- **Journal**: Frontiers in Psychology 2017
- **Summary**: The human visual system contains a hierarchical sequence of modules that take part in visual perception at different levels of abstraction, i.e., superordinate, basic, and subordinate levels. One important question is to identify the "entry" level at which the visual representation is commenced in the process of object recognition. For a long time, it was believed that the basic level had advantage over two others; a claim that has been challenged recently. Here we used a series of psychophysics experiments, based on a rapid presentation paradigm, as well as two computational models, with bandpass filtered images to study the processing order of the categorization levels. In these experiments, we investigated the type of visual information required for categorizing objects in each level by varying the spatial frequency bands of the input image. The results of our psychophysics experiments and computational models are consistent. They indicate that the different spatial frequency information had different effects on object categorization in each level. In the absence of high frequency information, subordinate and basic level categorization are performed inaccurately, while superordinate level is performed well. This means that, low frequency information is sufficient for superordinate level, but not for the basic and subordinate levels. These finer levels require high frequency information, which appears to take longer to be processed, leading to longer reaction times. Finally, to avoid the ceiling effect, we evaluated the robustness of the results by adding different amounts of noise to the input images and repeating the experiments. As expected, the categorization accuracy decreased and the reaction time increased significantly, but the trends were the same.This shows that our results are not due to a ceiling effect.



### Flow-Guided Feature Aggregation for Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1703.10025v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10025v2)
- **Published**: 2017-03-29 13:21:28+00:00
- **Updated**: 2017-08-18 12:30:38+00:00
- **Authors**: Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, Yichen Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Extending state-of-the-art object detectors from image to video is challenging. The accuracy of detection suffers from degenerated object appearances in videos, e.g., motion blur, video defocus, rare poses, etc. Existing work attempts to exploit temporal information on box level, but such methods are not trained end-to-end. We present flow-guided feature aggregation, an accurate and end-to-end learning framework for video object detection. It leverages temporal coherence on feature level instead. It improves the per-frame features by aggregation of nearby features along the motion paths, and thus improves the video recognition accuracy. Our method significantly improves upon strong single-frame baselines in ImageNet VID, especially for more challenging fast moving objects. Our framework is principled, and on par with the best engineered systems winning the ImageNet VID challenges 2016, without additional bells-and-whistles. The proposed method, together with Deep Feature Flow, powered the winning entry of ImageNet VID challenges 2017. The code is available at https://github.com/msracver/Flow-Guided-Feature-Aggregation.



### Pose-conditioned Spatio-Temporal Attention for Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.10106v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10106v2)
- **Published**: 2017-03-29 15:48:30+00:00
- **Updated**: 2017-08-07 02:04:08+00:00
- **Authors**: Fabien Baradel, Christian Wolf, Julien Mille
- **Comment**: 10 pages, project page:
  https://fabienbaradel.github.io/pose_rgb_attention_human_action
- **Journal**: None
- **Summary**: We address human action recognition from multi-modal video data involving articulated pose and RGB frames and propose a two-stream approach. The pose stream is processed with a convolutional model taking as input a 3D tensor holding data from a sub-sequence. A specific joint ordering, which respects the topology of the human body, ensures that different convolutional layers correspond to meaningful levels of abstraction. The raw RGB stream is handled by a spatio-temporal soft-attention mechanism conditioned on features from the pose network. An LSTM network receives input from a set of image locations at each instant. A trainable glimpse sensor extracts features on a set of predefined locations specified by the pose stream, namely the 4 hands of the two people involved in the activity. Appearance features give important cues on hand motion and on objects held in each hand. We show that it is of high interest to shift the attention to different hands at different time steps depending on the activity itself. Finally a temporal attention mechanism learns how to fuse LSTM features over time. We evaluate the method on 3 datasets. State-of-the-art results are achieved on the largest dataset for human activity recognition, namely NTU-RGB+D, as well as on the SBU Kinect Interaction dataset. Performance close to state-of-the-art is achieved on the smaller MSR Daily Activity 3D dataset.



### Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.10114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10114v1)
- **Published**: 2017-03-29 16:12:12+00:00
- **Updated**: 2017-03-29 16:12:12+00:00
- **Authors**: Nick Johnston, Damien Vincent, David Minnen, Michele Covell, Saurabh Singh, Troy Chinen, Sung Jin Hwang, Joel Shor, George Toderici
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method for lossy image compression based on recurrent, convolutional neural networks that outperforms BPG (4:2:0 ), WebP, JPEG2000, and JPEG as measured by MS-SSIM. We introduce three improvements over previous research that lead to this state-of-the-art result. First, we show that training with a pixel-wise loss weighted by SSIM increases reconstruction quality according to several metrics. Second, we modify the recurrent architecture to improve spatial diffusion, which allows the network to more effectively capture and propagate image information through the network's hidden state. Finally, in addition to lossless entropy coding, we use a spatially adaptive bit allocation algorithm to more efficiently use the limited number of bits to encode visually complex image regions. We evaluate our method on the Kodak and Tecnick image sets and compare against standard codecs as well recently published methods based on deep neural networks.



### Google Map Aided Visual Navigation for UAVs in GPS-denied Environment
- **Arxiv ID**: http://arxiv.org/abs/1703.10125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10125v1)
- **Published**: 2017-03-29 16:34:25+00:00
- **Updated**: 2017-03-29 16:34:25+00:00
- **Authors**: Mo Shan, Fei Wang, Feng Lin, Zhi Gao, Ya Z. Tang, Ben M. Chen
- **Comment**: Published in ROBIO 2015, Zhuhai, China. Fixed a typo
- **Journal**: None
- **Summary**: We propose a framework for Google Map aided UAV navigation in GPS-denied environment. Geo-referenced navigation provides drift-free localization and does not require loop closures. The UAV position is initialized via correlation, which is simple and efficient. We then use optical flow to predict its position in subsequent frames. During pose tracking, we obtain inter-frame translation either by motion field or homography decomposition, and we use HOG features for registration on Google Map. We employ particle filter to conduct a coarse to fine search to localize the UAV. Offline test using aerial images collected by our quadrotor platform shows promising results as our approach eliminates the drift in dead-reckoning, and the small localization error indicates the superiority of our approach as a supplement to GPS.



### Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1703.10131v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10131v2)
- **Published**: 2017-03-29 16:52:14+00:00
- **Updated**: 2017-09-15 10:33:33+00:00
- **Authors**: Matan Sela, Elad Richardson, Ron Kimmel
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: It has been recently shown that neural networks can recover the geometric structure of a face from a single given image. A common denominator of most existing face geometry reconstruction methods is the restriction of the solution space to some low-dimensional subspace. While such a model significantly simplifies the reconstruction problem, it is inherently limited in its expressiveness. As an alternative, we propose an Image-to-Image translation network that jointly maps the input image to a depth image and a facial correspondence map. This explicit pixel-based mapping can then be utilized to provide high quality reconstructions of diverse faces under extreme expressions, using a purely geometric refinement process. In the spirit of recent approaches, the network is trained only with synthetic data, and is then evaluated on in-the-wild facial images. Both qualitative and quantitative analyses demonstrate the accuracy and the robustness of our approach.



### CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training
- **Arxiv ID**: http://arxiv.org/abs/1703.10155v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10155v2)
- **Published**: 2017-03-29 17:49:48+00:00
- **Updated**: 2017-10-12 15:19:40+00:00
- **Authors**: Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, Gang Hua
- **Comment**: to appear in ICCV 2017
- **Journal**: None
- **Summary**: We present variational generative adversarial networks, a general learning framework that combines a variational auto-encoder with a generative adversarial network, for synthesizing images in fine-grained categories, such as faces of a specific person or objects in a category. Our approach models an image as a composition of label and latent attributes in a probabilistic model. By varying the fine-grained category label fed into the resulting generative model, we can generate images in a specific category with randomly drawn values on a latent attribute vector. Our approach has two novel aspects. First, we adopt a cross entropy loss for the discriminative and classifier network, but a mean discrepancy objective for the generative network. This kind of asymmetric loss function makes the GAN training more stable. Second, we adopt an encoder network to learn the relationship between the latent space and the real image space, and use pairwise feature matching to keep the structure of generated images. We experiment with natural images of faces, flowers, and birds, and demonstrate that the proposed models are capable of generating realistic and diverse samples with fine-grained category labels. We further show that our models can be applied to other tasks, such as image inpainting, super-resolution, and data augmentation for training better face recognition models.



### Detecting Human Interventions on the Landscape: KAZE Features, Poisson Point Processes, and a Construction Dataset
- **Arxiv ID**: http://arxiv.org/abs/1703.10196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10196v1)
- **Published**: 2017-03-29 18:56:32+00:00
- **Updated**: 2017-03-29 18:56:32+00:00
- **Authors**: Edward Boyda, Colin McCormick, Dan Hammer
- **Comment**: None
- **Journal**: None
- **Summary**: We present an algorithm capable of identifying a wide variety of human-induced change on the surface of the planet by analyzing matches between local features in time-sequenced remote sensing imagery. We evaluate feature sets, match protocols, and the statistical modeling of feature matches. With application of KAZE features, k-nearest-neighbor descriptor matching, and geometric proximity and bi-directional match consistency checks, average match rates increase more than two-fold over the previous standard. In testing our platform, we developed a small, labeled benchmark dataset expressing large-scale residential, industrial, and civic construction, along with null instances, in California between the years 2010 and 2012. On the benchmark set, our algorithm makes precise, accurate change proposals on two-thirds of scenes. Further, the detection threshold can be tuned so that all or almost all proposed detections are true positives.



### Learning High Dynamic Range from Outdoor Panoramas
- **Arxiv ID**: http://arxiv.org/abs/1703.10200v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10200v4)
- **Published**: 2017-03-29 19:10:27+00:00
- **Updated**: 2017-11-07 15:13:06+00:00
- **Authors**: Jinsong Zhang, Jean-François Lalonde
- **Comment**: 8 pages + 2 pages of citations, 10 figures. Accepted as an oral paper
  at ICCV 2017
- **Journal**: None
- **Summary**: Outdoor lighting has extremely high dynamic range. This makes the process of capturing outdoor environment maps notoriously challenging since special equipment must be used. In this work, we propose an alternative approach. We first capture lighting with a regular, LDR omnidirectional camera, and aim to recover the HDR after the fact via a novel, learning-based inverse tonemapping method. We propose a deep autoencoder framework which regresses linear, high dynamic range data from non-linear, saturated, low dynamic range panoramas. We validate our method through a wide set of experiments on synthetic data, as well as on a novel dataset of real photographs with ground truth. Our approach finds applications in a variety of settings, ranging from outdoor light capture to image matching.



### SeGAN: Segmenting and Generating the Invisible
- **Arxiv ID**: http://arxiv.org/abs/1703.10239v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10239v3)
- **Published**: 2017-03-29 20:34:20+00:00
- **Updated**: 2018-05-07 20:16:33+00:00
- **Authors**: Kiana Ehsani, Roozbeh Mottaghi, Ali Farhadi
- **Comment**: Accepted to CVPR18 as spotlight
- **Journal**: None
- **Summary**: Objects often occlude each other in scenes; Inferring their appearance beyond their visible parts plays an important role in scene understanding, depth estimation, object interaction and manipulation. In this paper, we study the challenging problem of completing the appearance of occluded objects. Doing so requires knowing which pixels to paint (segmenting the invisible parts of objects) and what color to paint them (generating the invisible parts). Our proposed novel solution, SeGAN, jointly optimizes for both segmentation and generation of the invisible parts of objects. Our experimental results show that: (a) SeGAN can learn to generate the appearance of the occluded parts of objects; (b) SeGAN outperforms state-of-the-art segmentation baselines for the invisible parts of objects; (c) trained on synthetic photo realistic images, SeGAN can reliably segment natural images; (d) by reasoning about occluder occludee relations, our method can infer depth layering.



