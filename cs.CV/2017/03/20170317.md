# Arxiv Papers in cs.CV on 2017-03-17
### Towards Closing the Energy Gap Between HOG and CNN Features for Embedded Vision
- **Arxiv ID**: http://arxiv.org/abs/1703.05853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05853v1)
- **Published**: 2017-03-17 00:17:50+00:00
- **Updated**: 2017-03-17 00:17:50+00:00
- **Authors**: Amr Suleiman, Yu-Hsin Chen, Joel Emer, Vivienne Sze
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision enables a wide range of applications in robotics/drones, self-driving cars, smart Internet of Things, and portable/wearable electronics. For many of these applications, local embedded processing is preferred due to privacy and/or latency concerns. Accordingly, energy-efficient embedded vision hardware delivering real-time and robust performance is crucial. While deep learning is gaining popularity in several computer vision algorithms, a significant energy consumption difference exists compared to traditional hand-crafted approaches. In this paper, we provide an in-depth analysis of the computation, energy and accuracy trade-offs between learned features such as deep Convolutional Neural Networks (CNN) and hand-crafted features such as Histogram of Oriented Gradients (HOG). This analysis is supported by measurements from two chips that implement these algorithms. Our goal is to understand the source of the energy discrepancy between the two approaches and to provide insight about the potential areas where CNNs can be improved and eventually approach the energy-efficiency of HOG while maintaining its outstanding performance accuracy.



### Understanding Traffic Density from Large-Scale Web Camera Data
- **Arxiv ID**: http://arxiv.org/abs/1703.05868v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05868v3)
- **Published**: 2017-03-17 02:14:27+00:00
- **Updated**: 2017-06-30 20:10:48+00:00
- **Authors**: Shanghang Zhang, Guanhang Wu, João P. Costeira, José M. F. Moura
- **Comment**: Accepted by CVPR 2017. Preprint version was uploaded on
  http://welcome.isr.tecnico.ulisboa.pt/publications/understanding-traffic-density-from-large-scale-web-camera-data/
- **Journal**: None
- **Summary**: Understanding traffic density from large-scale web camera (webcam) videos is a challenging problem because such videos have low spatial and temporal resolution, high occlusion and large perspective. To deeply understand traffic density, we explore both deep learning based and optimization based methods. To avoid individual vehicle detection and tracking, both methods map the image into vehicle density map, one based on rank constrained regression and the other one based on fully convolution networks (FCN). The regression based method learns different weights for different blocks in the image to increase freedom degrees of weights and embed perspective information. The FCN based method jointly estimates vehicle density map and vehicle count with a residual learning framework to perform end-to-end dense prediction, allowing arbitrary image resolution, and adapting to different vehicle scales and perspectives. We analyze and compare both methods, and get insights from optimization based method to improve deep model. Since existing datasets do not cover all the challenges in our work, we collected and labelled a large-scale traffic video dataset, containing 60 million frames from 212 webcams. Both methods are extensively evaluated and compared on different counting tasks and datasets. FCN based method significantly reduces the mean absolute error from 10.99 to 5.31 on the public dataset TRANCOS compared with the state-of-the-art baseline.



### DropRegion Training of Inception Font Network for High-Performance Chinese Font Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.05870v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05870v2)
- **Published**: 2017-03-17 02:24:26+00:00
- **Updated**: 2017-03-27 04:51:40+00:00
- **Authors**: Shuangping Huangm Zhuoyao Zhong, Lianwen Jin, Shuye Zhang, Haobin Wang
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: Chinese font recognition (CFR) has gained significant attention in recent years. However, due to the sparsity of labeled font samples and the structural complexity of Chinese characters, CFR is still a challenging task. In this paper, a DropRegion method is proposed to generate a large number of stochastic variant font samples whose local regions are selectively disrupted and an inception font network (IFN) with two additional convolutional neural network (CNN) structure elements, i.e., a cascaded cross-channel parametric pooling (CCCP) and global average pooling, is designed. Because the distribution of strokes in a font image is non-stationary, an elastic meshing technique that adaptively constructs a set of local regions with equalized information is developed. Thus, DropRegion is seamlessly embedded in the IFN, which enables end-to-end training; the proposed DropRegion-IFN can be used for high performance CFR. Experimental results have confirmed the effectiveness of our new approach for CFR.



### Need for Speed: A Benchmark for Higher Frame Rate Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1703.05884v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05884v2)
- **Published**: 2017-03-17 04:18:25+00:00
- **Updated**: 2017-03-21 22:35:09+00:00
- **Authors**: Hamed Kiani Galoogahi, Ashton Fagg, Chen Huang, Deva Ramanan, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose the first higher frame rate video dataset (called Need for Speed - NfS) and benchmark for visual object tracking. The dataset consists of 100 videos (380K frames) captured with now commonly available higher frame rate (240 FPS) cameras from real world scenarios. All frames are annotated with axis aligned bounding boxes and all sequences are manually labelled with nine visual attributes - such as occlusion, fast motion, background clutter, etc. Our benchmark provides an extensive evaluation of many recent and state-of-the-art trackers on higher frame rate sequences. We ranked each of these trackers according to their tracking accuracy and real-time performance. One of our surprising conclusions is that at higher frame rates, simple trackers such as correlation filters outperform complex methods based on deep networks. This suggests that for practical applications (such as in robotics or embedded vision), one needs to carefully tradeoff bandwidth constraints associated with higher frame rate acquisition, computational costs of real-time analysis, and the required application accuracy. Our dataset and benchmark allows for the first time (to our knowledge) systematic exploration of such issues, and will be made available to allow for further research in this space.



### Learning Robust Visual-Semantic Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1703.05908v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.05908v2)
- **Published**: 2017-03-17 06:59:51+00:00
- **Updated**: 2017-03-20 00:28:07+00:00
- **Authors**: Yao-Hung Hubert Tsai, Liang-Kang Huang, Ruslan Salakhutdinov
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and few-shot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.



### Computer Aided Detection of Anemia-like Pallor
- **Arxiv ID**: http://arxiv.org/abs/1703.05913v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05913v1)
- **Published**: 2017-03-17 07:35:26+00:00
- **Updated**: 2017-03-17 07:35:26+00:00
- **Authors**: Sohini Roychowdhury, Donny Sun, Matthew Bihis, Johnny Ren, Paul Hage, Humairat H. Rahman
- **Comment**: 4 pages,2 figures, 2 tables
- **Journal**: None
- **Summary**: Paleness or pallor is a manifestation of blood loss or low hemoglobin concentrations in the human blood that can be caused by pathologies such as anemia. This work presents the first automated screening system that utilizes pallor site images, segments, and extracts color and intensity-based features for multi-class classification of patients with high pallor due to anemia-like pathologies, normal patients and patients with other abnormalities. This work analyzes the pallor sites of conjunctiva and tongue for anemia screening purposes. First, for the eye pallor site images, the sclera and conjunctiva regions are automatically segmented for regions of interest. Similarly, for the tongue pallor site images, the inner and outer tongue regions are segmented. Then, color-plane based feature extraction is performed followed by machine learning algorithms for feature reduction and image level classification for anemia. In this work, a suite of classification algorithms image-level classifications for normal (class 0), pallor (class 1) and other abnormalities (class 2). The proposed method achieves 86% accuracy, 85% precision and 67% recall in eye pallor site images and 98.2% accuracy and precision with 100% recall in tongue pallor site images for classification of images with pallor. The proposed pallor screening system can be further fine-tuned to detect the severity of anemia-like pathologies using controlled set of local images that can then be used for future benchmarking purposes.



### Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery
- **Arxiv ID**: http://arxiv.org/abs/1703.05921v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.05921v1)
- **Published**: 2017-03-17 08:27:05+00:00
- **Updated**: 2017-03-17 08:27:05+00:00
- **Authors**: Thomas Schlegl, Philipp Seeböck, Sebastian M. Waldstein, Ursula Schmidt-Erfurth, Georg Langs
- **Comment**: To be published in the proceedings of the international conference on
  Information Processing in Medical Imaging (IPMI), 2017
- **Journal**: None
- **Summary**: Obtaining models that capture imaging markers relevant for disease progression and treatment monitoring is challenging. Models are typically based on large amounts of data with annotated examples of known markers aiming at automating detection. High annotation effort and the limitation to a vocabulary of known markers limit the power of such approaches. Here, we perform unsupervised learning to identify anomalies in imaging data as candidates for markers. We propose AnoGAN, a deep convolutional generative adversarial network to learn a manifold of normal anatomical variability, accompanying a novel anomaly scoring scheme based on the mapping from image space to a latent space. Applied to new data, the model labels anomalies, and scores image patches indicating their fit into the learned distribution. Results on optical coherence tomography images of the retina demonstrate that the approach correctly identifies anomalous images, such as images containing retinal fluid or hyperreflective foci.



### Comparison of Different Methods for Tissue Segmentation in Histopathological Whole-Slide Images
- **Arxiv ID**: http://arxiv.org/abs/1703.05990v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.05990v2)
- **Published**: 2017-03-17 12:32:25+00:00
- **Updated**: 2017-04-03 17:46:32+00:00
- **Authors**: Péter Bándi, Rob van de Loo, Milad Intezar, Daan Geijs, Francesco Ciompi, Bram van Ginneken, Jeroen van der Laak, Geert Litjens
- **Comment**: Accepted for poster presentation at the IEEE International Symposium
  on Biomedical Imaging (ISBI) 2017
- **Journal**: None
- **Summary**: Tissue segmentation is an important pre-requisite for efficient and accurate diagnostics in digital pathology. However, it is well known that whole-slide scanners can fail in detecting all tissue regions, for example due to the tissue type, or due to weak staining because their tissue detection algorithms are not robust enough. In this paper, we introduce two different convolutional neural network architectures for whole slide image segmentation to accurately identify the tissue sections. We also compare the algorithms to a published traditional method. We collected 54 whole slide images with differing stains and tissue types from three laboratories to validate our algorithms. We show that while the two methods do not differ significantly they outperform their traditional counterpart (Jaccard index of 0.937 and 0.929 vs. 0.870, p < 0.01).



### Semi-Supervised Deep Learning for Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.06000v2
- **DOI**: 10.1007/978-3-319-66179-7_36
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06000v2)
- **Published**: 2017-03-17 13:14:36+00:00
- **Updated**: 2017-07-25 12:02:55+00:00
- **Authors**: Christoph Baur, Shadi Albarqouni, Nassir Navab
- **Comment**: 9 pages, 6 figures
- **Journal**: Medical Image Computing and Computer Assisted Intervention (MICCAI
  2017)
- **Summary**: Deep learning usually requires large amounts of labeled training data, but annotating data is costly and tedious. The framework of semi-supervised learning provides the means to use both labeled data and arbitrary amounts of unlabeled data for training. Recently, semi-supervised deep learning has been intensively studied for standard CNN architectures. However, Fully Convolutional Networks (FCNs) set the state-of-the-art for many image segmentation tasks. To the best of our knowledge, there is no existing semi-supervised learning method for such FCNs yet. We lift the concept of auxiliary manifold embedding for semi-supervised learning to FCNs with the help of Random Feature Embedding. In our experiments on the challenging task of MS Lesion Segmentation, we leverage the proposed framework for the purpose of domain adaptation and report substantial improvements over the baseline model.



### Color Orchestra: Ordering Color Palettes for Interpolation and Prediction
- **Arxiv ID**: http://arxiv.org/abs/1703.06003v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1703.06003v1)
- **Published**: 2017-03-17 13:25:49+00:00
- **Updated**: 2017-03-17 13:25:49+00:00
- **Authors**: Huy Q. Phan, Hongbo Fu, Antoni B. Chan
- **Comment**: IEEE Transactions on Visualization and Computer Graphics
- **Journal**: None
- **Summary**: Color theme or color palette can deeply influence the quality and the feeling of a photograph or a graphical design. Although color palettes may come from different sources such as online crowd-sourcing, photographs and graphical designs, in this paper, we consider color palettes extracted from fine art collections, which we believe to be an abundant source of stylistic and unique color themes. We aim to capture color styles embedded in these collections by means of statistical models and to build practical applications upon these models. As artists often use their personal color themes in their paintings, making these palettes appear frequently in the dataset, we employed density estimation to capture the characteristics of palette data. Via density estimation, we carried out various predictions and interpolations on palettes, which led to promising applications such as photo-style exploration, real-time color suggestion, and enriched photo recolorization. It was, however, challenging to apply density estimation to palette data as palettes often come as unordered sets of colors, which make it difficult to use conventional metrics on them. To this end, we developed a divide-and-conquer sorting algorithm to rearrange the colors in the palettes in a coherent order, which allows meaningful interpolation between color palettes. To confirm the performance of our model, we also conducted quantitative experiments on datasets of digitized paintings collected from the Internet and received favorable results.



### Smartphone Based Colorimetric Detection via Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.10217v1
- **DOI**: 10.1039/C7AN00741H
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10217v1)
- **Published**: 2017-03-17 13:38:13+00:00
- **Updated**: 2017-03-17 13:38:13+00:00
- **Authors**: Ali Y. Mutlu, Volkan Kılıç, Gizem K. Özdemir, Abdullah Bayram, Nesrin Horzum, Mehmet E. Solmaz
- **Comment**: None
- **Journal**: None
- **Summary**: We report the application of machine learning to smartphone based colorimetric detection of pH values. The strip images were used as the training set for Least Squares-Support Vector Machine (LS-SVM) classifier algorithms that were able to successfully classify the distinct pH values. The difference in the obtained image formats was found not to significantly affect the performance of the proposed machine learning approach. Moreover, the influence of the illumination conditions on the perceived color of pH strips was investigated and further experiments were carried out to study effect of color change on the learning model. Test results on JPEG, RAW and RAW-corrected image formats captured in different lighting conditions lead to perfect classification accuracy, sensitivity and specificity, which proves that the colorimetric detection using machine learning based systems is able to adapt to various experimental conditions and is a great candidate for smartphone based sensing in paper-based colorimetric assays.



### Towards Diverse and Natural Image Descriptions via a Conditional GAN
- **Arxiv ID**: http://arxiv.org/abs/1703.06029v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06029v3)
- **Published**: 2017-03-17 14:33:41+00:00
- **Updated**: 2017-08-11 05:02:36+00:00
- **Authors**: Bo Dai, Sanja Fidler, Raquel Urtasun, Dahua Lin
- **Comment**: accepted in ICCV2017 as an Oral paper
- **Journal**: None
- **Summary**: Despite the substantial progress in recent years, the image captioning techniques are still far from being perfect.Sentences produced by existing methods, e.g. those based on RNNs, are often overly rigid and lacking in variability. This issue is related to a learning principle widely used in practice, that is, to maximize the likelihood of training samples. This principle encourages high resemblance to the "ground-truth" captions while suppressing other reasonable descriptions. Conventional evaluation metrics, e.g. BLEU and METEOR, also favor such restrictive methods. In this paper, we explore an alternative approach, with the aim to improve the naturalness and diversity -- two essential properties of human expression. Specifically, we propose a new framework based on Conditional Generative Adversarial Networks (CGAN), which jointly learns a generator to produce descriptions conditioned on images and an evaluator to assess how well a description fits the visual content. It is noteworthy that training a sequence generator is nontrivial. We overcome the difficulty by Policy Gradient, a strategy stemming from Reinforcement Learning, which allows the generator to receive early feedback along the way. We tested our method on two large datasets, where it performed competitively against real people in our user study and outperformed other methods on various tasks.



### PSF field learning based on Optimal Transport Distances
- **Arxiv ID**: http://arxiv.org/abs/1703.06066v1
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.IM, 49M99
- **Links**: [PDF](http://arxiv.org/pdf/1703.06066v1)
- **Published**: 2017-03-17 16:09:40+00:00
- **Updated**: 2017-03-17 16:09:40+00:00
- **Authors**: F. M. Ngolè Mboula, J. -L. Starck
- **Comment**: None
- **Journal**: None
- **Summary**: Context: in astronomy, observing large fractions of the sky within a reasonable amount of time implies using large field-of-view (fov) optical instruments that typically have a spatially varying Point Spread Function (PSF). Depending on the scientific goals, galaxies images need to be corrected for the PSF whereas no direct measurement of the PSF is available. Aims: given a set of PSFs observed at random locations, we want to estimate the PSFs at galaxies locations for shapes measurements correction. Contributions: we propose an interpolation framework based on Sliced Optimal Transport. A non-linear dimension reduction is first performed based on local pairwise approximated Wasserstein distances. A low dimensional representation of the unknown PSFs is then estimated, which in turn is used to derive representations of those PSFs in the Wasserstein metric. Finally, the interpolated PSFs are calculated as approximated Wasserstein barycenters. Results: the proposed method was tested on simulated monochromatic PSFs of the Euclid space mission telescope (to be launched in 2020). It achieves a remarkable accuracy in terms of pixels values and shape compared to standard methods such as Inverse Distance Weighting or Radial Basis Function based interpolation methods.



### Hyperspectral Unmixing with Endmember Variability using Semi-supervised Partial Membership Latent Dirichlet Allocation
- **Arxiv ID**: http://arxiv.org/abs/1703.06151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06151v1)
- **Published**: 2017-03-17 18:13:59+00:00
- **Updated**: 2017-03-17 18:13:59+00:00
- **Authors**: Sheng Zou, Hao Sun, Alina Zare
- **Comment**: None
- **Journal**: None
- **Summary**: A semi-supervised Partial Membership Latent Dirichlet Allocation approach is developed for hyperspectral unmixing and endmember estimation while accounting for spectral variability and spatial information. Partial Membership Latent Dirichlet Allocation is an effective approach for spectral unmixing while representing spectral variability and leveraging spatial information. In this work, we extend Partial Membership Latent Dirichlet Allocation to incorporate any available (imprecise) label information to help guide unmixing. Experimental results on two hyperspectral datasets show that the proposed semi-supervised PM-LDA can yield improved hyperspectral unmixing and endmember estimation results.



### TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals
- **Arxiv ID**: http://arxiv.org/abs/1703.06189v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06189v2)
- **Published**: 2017-03-17 20:24:32+00:00
- **Updated**: 2017-08-04 19:21:31+00:00
- **Authors**: Jiyang Gao, Zhenheng Yang, Chen Sun, Kan Chen, Ram Nevatia
- **Comment**: ICCV 2017 camera ready
- **Journal**: None
- **Summary**: Temporal Action Proposal (TAP) generation is an important problem, as fast and accurate extraction of semantically important (e.g. human actions) segments from untrimmed videos is an important step for large-scale video analysis. We propose a novel Temporal Unit Regression Network (TURN) model. There are two salient aspects of TURN: (1) TURN jointly predicts action proposals and refines the temporal boundaries by temporal coordinate regression; (2) Fast computation is enabled by unit feature reuse: a long untrimmed video is decomposed into video units, which are reused as basic building blocks of temporal proposals. TURN outperforms the state-of-the-art methods under average recall (AR) by a large margin on THUMOS-14 and ActivityNet datasets, and runs at over 880 frames per second (FPS) on a TITAN X GPU. We further apply TURN as a proposal generation stage for existing temporal action localization pipelines, it outperforms state-of-the-art performance on THUMOS-14 and ActivityNet.



### Deformable Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.06211v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06211v3)
- **Published**: 2017-03-17 21:58:20+00:00
- **Updated**: 2017-06-05 10:08:50+00:00
- **Authors**: Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules. In this work, we introduce two new modules to enhance the transformation modeling capacity of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the effectiveness of our approach on sophisticated vision tasks of object detection and semantic segmentation. The code would be released.



### Deciding How to Decide: Dynamic Routing in Artificial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.06217v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1703.06217v2)
- **Published**: 2017-03-17 23:52:14+00:00
- **Updated**: 2017-09-12 22:14:36+00:00
- **Authors**: Mason McGill, Pietro Perona
- **Comment**: ICML 2017. Code at https://github.com/MasonMcGill/multipath-nn Video
  abstract at https://youtu.be/NHQsDaycwyQ
- **Journal**: None
- **Summary**: We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths. Though some approaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images. Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable statically-routed networks.



