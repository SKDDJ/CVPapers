# Arxiv Papers in cs.CV on 2017-03-05
### Deep-Learning for Classification of Colorectal Polyps on Whole-Slide Images
- **Arxiv ID**: http://arxiv.org/abs/1703.01550v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01550v2)
- **Published**: 2017-03-05 03:17:35+00:00
- **Updated**: 2017-04-12 22:26:56+00:00
- **Authors**: Bruno Korbar, Andrea M. Olofson, Allen P. Miraflor, Katherine M. Nicka, Matthew A. Suriawinata, Lorenzo Torresani, Arief A. Suriawinata, Saeed Hassanpour
- **Comment**: None
- **Journal**: None
- **Summary**: Histopathological characterization of colorectal polyps is an important principle for determining the risk of colorectal cancer and future rates of surveillance for patients. This characterization is time-intensive, requires years of specialized training, and suffers from significant inter-observer and intra-observer variability. In this work, we built an automatic image-understanding method that can accurately classify different types of colorectal polyps in whole-slide histology images to help pathologists with histopathological characterization and diagnosis of colorectal polyps. The proposed image-understanding method is based on deep-learning techniques, which rely on numerous levels of abstraction for data representation and have shown state-of-the-art results for various image analysis tasks. Our image-understanding method covers all five polyp types (hyperplastic polyp, sessile serrated polyp, traditional serrated adenoma, tubular adenoma, and tubulovillous/villous adenoma) that are included in the US multi-society task force guidelines for colorectal cancer risk assessment and surveillance, and encompasses the most common occurrences of colorectal polyps. Our evaluation on 239 independent test samples shows our proposed method can identify the types of colorectal polyps in whole-slide images with a high efficacy (accuracy: 93.0%, precision: 89.7%, recall: 88.3%, F1 score: 88.8%). The presented method in this paper can reduce the cognitive burden on pathologists and improve their accuracy and efficiency in histopathological characterization of colorectal polyps, and in subsequent risk assessment and follow-up recommendations.



### LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1703.01560v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.01560v3)
- **Published**: 2017-03-05 05:17:56+00:00
- **Updated**: 2017-08-02 03:51:57+00:00
- **Authors**: Jianwei Yang, Anitha Kannan, Dhruv Batra, Devi Parikh
- **Comment**: 21 pages, 22 figures, published as a conference paper at ICLR 2017,
  code available on GitHub
- **Journal**: None
- **Summary**: We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.



### Perceiving and Reasoning About Liquids Using Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.01564v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.01564v2)
- **Published**: 2017-03-05 07:10:50+00:00
- **Updated**: 2017-09-23 19:40:49+00:00
- **Authors**: Conor Schenck, Dieter Fox
- **Comment**: In The International Journal of Robotics Research (to appear)
- **Journal**: None
- **Summary**: Liquids are an important part of many common manipulation tasks in human environments. If we wish to have robots that can accomplish these types of tasks, they must be able to interact with liquids in an intelligent manner. In this paper, we investigate ways for robots to perceive and reason about liquids. That is, a robot asks the questions What in the visual data stream is liquid? and How can I use that to infer all the potential places where liquid might be? We collected two datasets to evaluate these questions, one using a realistic liquid simulator and another on our robot. We used fully convolutional neural networks to learn to detect and track liquids across pouring sequences. Our results show that these networks are able to perceive and reason about liquids, and that integrating temporal information is important to performing such tasks well.



### Face Alignment with Cascaded Semi-Parametric Deep Greedy Neural Forests
- **Arxiv ID**: http://arxiv.org/abs/1703.01597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01597v1)
- **Published**: 2017-03-05 13:47:46+00:00
- **Updated**: 2017-03-05 13:47:46+00:00
- **Authors**: Arnaud Dapogny, Kévin Bailly, Séverine Dubuisson
- **Comment**: 10 pages, 1 page appendix, 5 figures
- **Journal**: None
- **Summary**: Face alignment is an active topic in computer vision, consisting in aligning a shape model on the face. To this end, most modern approaches refine the shape in a cascaded manner, starting from an initial guess. Those shape updates can either be applied in the feature point space (\textit{i.e.} explicit updates) or in a low-dimensional, parametric space. In this paper, we propose a semi-parametric cascade that first aligns a parametric shape, then captures more fine-grained deformations of an explicit shape. For the purpose of learning shape updates at each cascade stage, we introduce a deep greedy neural forest (GNF) model, which is an improved version of deep neural forest (NF). GNF appears as an ideal regressor for face alignment, as it combines differentiability, high expressivity and fast evaluation runtime. The proposed framework is very fast and achieves high accuracies on multiple challenging benchmarks, including small, medium and large pose experiments.



### L2GSCI: Local to Global Seam Cutting and Integrating for Accurate Face Contour Extraction
- **Arxiv ID**: http://arxiv.org/abs/1703.01605v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01605v1)
- **Published**: 2017-03-05 15:06:28+00:00
- **Updated**: 2017-03-05 15:06:28+00:00
- **Authors**: Yongwei Nie, Xu Cao, Chengjiang Long, Ping Li, Guiqing Li
- **Comment**: None
- **Journal**: None
- **Summary**: Current face alignment algorithms can robustly find a set of landmarks along face contour. However, the landmarks are sparse and lack curve details, especially in chin and cheek areas where a lot of concave-convex bending information exists. In this paper, we propose a local to global seam cutting and integrating algorithm (L2GSCI) to extract continuous and accurate face contour. Our method works in three steps with the help of a rough initial curve. First, we sample small and overlapped squares along the initial curve. Second, the seam cutting part of L2GSCI extracts a local seam in each square region. Finally, the seam integrating part of L2GSCI connects all the redundant seams together to form a continuous and complete face curve. Overall, the proposed method is much more straightforward than existing face alignment algorithms, but can achieve pixel-level continuous face curves rather than discrete and sparse landmarks. Moreover, experiments on two face benchmark datasets (i.e., LFPW and HELEN) show that our method can precisely reveal concave-convex bending details of face contours, which has significantly improved the performance when compared with the state-ofthe- art face alignment approaches.



### Automatic Classification of Cancerous Tissue in Laserendomicroscopy Images of the Oral Cavity using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.01622v2
- **DOI**: 10.1038/s41598-017-12320-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01622v2)
- **Published**: 2017-03-05 16:48:01+00:00
- **Updated**: 2017-03-10 10:58:24+00:00
- **Authors**: Marc Aubreville, Christian Knipfer, Nicolai Oetter, Christian Jaremenko, Erik Rodner, Joachim Denzler, Christopher Bohr, Helmut Neumann, Florian Stelzle, Andreas Maier
- **Comment**: 12 pages, 5 figures
- **Journal**: Scientific Reports 7, Article number: 11979 (2017)
- **Summary**: Oral Squamous Cell Carcinoma (OSCC) is a common type of cancer of the oral epithelium. Despite their high impact on mortality, sufficient screening methods for early diagnosis of OSCC often lack accuracy and thus OSCCs are mostly diagnosed at a late stage. Early detection and accurate outline estimation of OSCCs would lead to a better curative outcome and an reduction in recurrence rates after surgical treatment.   Confocal Laser Endomicroscopy (CLE) records sub-surface micro-anatomical images for in vivo cell structure analysis. Recent CLE studies showed great prospects for a reliable, real-time ultrastructural imaging of OSCC in situ.   We present and evaluate a novel automatic approach for a highly accurate OSCC diagnosis using deep learning technologies on CLE images. The method is compared against textural feature-based machine learning approaches that represent the current state of the art.   For this work, CLE image sequences (7894 images) from patients diagnosed with OSCC were obtained from 4 specific locations in the oral cavity, including the OSCC lesion. The present approach is found to outperform the state of the art in CLE image recognition with an area under the curve (AUC) of 0.96 and a mean accuracy of 88.3% (sensitivity 86.6%, specificity 90%).



### Reasoning About Liquids via Closed-Loop Simulation
- **Arxiv ID**: http://arxiv.org/abs/1703.01656v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.01656v2)
- **Published**: 2017-03-05 19:53:37+00:00
- **Updated**: 2017-06-09 22:51:08+00:00
- **Authors**: Connor Schenck, Dieter Fox
- **Comment**: Robotics: Science & Systems (RSS), July 12-16, 2017. Cambridge, MA,
  USA
- **Journal**: None
- **Summary**: Simulators are powerful tools for reasoning about a robot's interactions with its environment. However, when simulations diverge from reality, that reasoning becomes less useful. In this paper, we show how to close the loop between liquid simulation and real-time perception. We use observations of liquids to correct errors when tracking the liquid's state in a simulator. Our results show that closed-loop simulation is an effective way to prevent large divergence between the simulated and real liquid states. As a direct consequence of this, our method can enable reasoning about liquids that would otherwise be infeasible due to large divergences, such as reasoning about occluded liquid.



### SegICP: Integrated Deep Semantic Segmentation and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1703.01661v2
- **DOI**: 10.1109/IROS.2017.8206470
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.01661v2)
- **Published**: 2017-03-05 20:42:44+00:00
- **Updated**: 2017-09-05 19:59:39+00:00
- **Authors**: Jay M. Wong, Vincent Kee, Tiffany Le, Syler Wagner, Gian-Luca Mariottini, Abraham Schneider, Lei Hamilton, Rahul Chipalkatty, Mitchell Hebert, David M. S. Johnson, Jimmy Wu, Bolei Zhou, Antonio Torralba
- **Comment**: IROS camera-ready
- **Journal**: None
- **Summary**: Recent robotic manipulation competitions have highlighted that sophisticated robots still struggle to achieve fast and reliable perception of task-relevant objects in complex, realistic scenarios. To improve these systems' perceptive speed and robustness, we present SegICP, a novel integrated solution to object recognition and pose estimation. SegICP couples convolutional neural networks and multi-hypothesis point cloud registration to achieve both robust pixel-wise semantic segmentation as well as accurate and real-time 6-DOF pose estimation for relevant objects. Our architecture achieves 1cm position error and <5^\circ$ angle error in real time without an initial seed. We evaluate and benchmark SegICP against an annotated dataset generated by motion capture.



### Diversified Texture Synthesis with Feed-forward Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.01664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01664v1)
- **Published**: 2017-03-05 21:09:19+00:00
- **Updated**: 2017-03-05 21:09:19+00:00
- **Authors**: Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang
- **Comment**: accepted by CVPR2017
- **Journal**: None
- **Summary**: Recent progresses on deep discriminative and generative modeling have shown promising results on texture synthesis. However, existing feed-forward based methods trade off generality for efficiency, which suffer from many issues, such as shortage of generality (i.e., build one network per texture), lack of diversity (i.e., always produce visually identical output) and suboptimality (i.e., generate less satisfying visual effects). In this work, we focus on solving these issues for improved texture synthesis. We propose a deep generative feed-forward network which enables efficient synthesis of multiple textures within one single network and meaningful interpolation between them. Meanwhile, a suite of important techniques are introduced to achieve better convergence and diversity. With extensive experiments, we demonstrate the effectiveness of the proposed model and techniques for synthesizing a large number of textures and show its applications with the stylization.



