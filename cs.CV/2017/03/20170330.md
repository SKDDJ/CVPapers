# Arxiv Papers in cs.CV on 2017-03-30
### Semantic Instance Segmentation via Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.10277v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10277v1)
- **Published**: 2017-03-30 00:39:21+00:00
- **Updated**: 2017-03-30 00:39:21+00:00
- **Authors**: Alireza Fathi, Zbigniew Wojna, Vivek Rathod, Peng Wang, Hyun Oh Song, Sergio Guadarrama, Kevin P. Murphy
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new method for semantic instance segmentation, by first computing how likely two pixels are to belong to the same object, and then by grouping similar pixels together. Our similarity metric is based on a deep, fully convolutional embedding model. Our grouping method is based on selecting all points that are sufficiently similar to a set of "seed points", chosen from a deep, fully convolutional scoring model. We show competitive results on the Pascal VOC instance segmentation benchmark.



### DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling
- **Arxiv ID**: http://arxiv.org/abs/1703.10295v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10295v3)
- **Published**: 2017-03-30 02:50:54+00:00
- **Updated**: 2017-07-21 02:46:05+00:00
- **Authors**: Lachlan Tychsen-Smith, Lars Petersson
- **Comment**: 8 pages, ICCV2017 (poster)
- **Journal**: None
- **Summary**: We define the object detection from imagery problem as estimating a very large but extremely sparse bounding box dependent probability distribution. Subsequently we identify a sparse distribution estimation scheme, Directed Sparse Sampling, and employ it in a single end-to-end CNN based detection model. This methodology extends and formalizes previous state-of-the-art detection models with an additional emphasis on high evaluation rates and reduced manual engineering. We introduce two novelties, a corner based region-of-interest estimator and a deconvolution based CNN model. The resulting model is scene adaptive, does not require manually defined reference bounding boxes and produces highly competitive results on MSCOCO, Pascal VOC 2007 and Pascal VOC 2012 with real-time evaluation rates. Further analysis suggests our model performs particularly well when finegrained object localization is desirable. We argue that this advantage stems from the significantly larger set of available regions-of-interest relative to other methods. Source-code is available from: https://github.com/lachlants/denet



### Planecell: Representing the 3D Space with Planes
- **Arxiv ID**: http://arxiv.org/abs/1703.10304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10304v1)
- **Published**: 2017-03-30 03:58:05+00:00
- **Updated**: 2017-03-30 03:58:05+00:00
- **Authors**: Lei Fan, Ziyu Pan, Long Chen, Kai Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstruction based on the stereo camera has received considerable attention recently, but two particular challenges still remain. The first concerns the need to aggregate similar pixels in an effective approach, and the second is to maintain as much of the available information as possible while ensuring sufficient accuracy. To overcome these issues, we propose a new 3D representation method, namely, planecell, that extracts planarity from the depth-assisted image segmentation and then projects these depth planes into the 3D world. An energy function formulated from Conditional Random Field that generalizes the planar relationships is maximized to merge coplanar segments. We evaluate our method with a variety of reconstruction baselines on both KITTI and Middlebury datasets, and the results indicate the superiorities compared to other 3D space representation methods in accuracy, memory requirements and further applications.



### Dynamic Computational Time for Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/1703.10332v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10332v3)
- **Published**: 2017-03-30 06:55:02+00:00
- **Updated**: 2017-09-07 00:59:49+00:00
- **Authors**: Zhichao Li, Yi Yang, Xiao Liu, Feng Zhou, Shilei Wen, Wei Xu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a dynamic computational time model to accelerate the average processing time for recurrent visual attention (RAM). Rather than attention with a fixed number of steps for each input image, the model learns to decide when to stop on the fly. To achieve this, we add an additional continue/stop action per time step to RAM and use reinforcement learning to learn both the optimal attention policy and stopping policy. The modification is simple but could dramatically save the average computational time while keeping the same recognition performance as RAM. Experimental results on CUB-200-2011 and Stanford Cars dataset demonstrate the dynamic computational model can work effectively for fine-grained image recognition.The source code of this paper can be obtained from https://github.com/baidu-research/DT-RAM



### Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1703.10476v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1703.10476v2)
- **Published**: 2017-03-30 13:54:51+00:00
- **Updated**: 2017-11-06 15:43:47+00:00
- **Authors**: Rakshith Shetty, Marcus Rohrbach, Lisa Anne Hendricks, Mario Fritz, Bernt Schiele
- **Comment**: 16 pages, Published in ICCV 2017
- **Journal**: None
- **Summary**: While strong progress has been made in image captioning over the last years, machine and human captions are still quite distinct. A closer look reveals that this is due to the deficiencies in the generated word distribution, vocabulary size, and strong bias in the generators towards frequent captions. Furthermore, humans -- rightfully so -- generate multiple, diverse captions, due to the inherent ambiguity in the captioning task which is not considered in today's systems.   To address these challenges, we change the training objective of the caption generator from reproducing groundtruth captions to generating a set of captions that is indistinguishable from human generated captions. Instead of handcrafting such a learning target, we employ adversarial training in combination with an approximate Gumbel sampler to implicitly match the generated distribution to the human one. While our method achieves comparable performance to the state-of-the-art in terms of the correctness of the captions, we generate a set of diverse captions, that are significantly less biased and match the word statistics better in several aspects.



### A deep learning classification scheme based on augmented-enhanced features to segment organs at risk on the optic region in brain cancer patients
- **Arxiv ID**: http://arxiv.org/abs/1703.10480v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10480v2)
- **Published**: 2017-03-30 14:09:53+00:00
- **Updated**: 2017-04-05 12:34:07+00:00
- **Authors**: Jose Dolz, Nicolas Reyns, Nacim Betrouni, Dris Kharroubi, Mathilde Quidet, Laurent Massoptier, Maximilien Vermandel
- **Comment**: Submitted to the Journal of Physics in Biology and Medicine
- **Journal**: None
- **Summary**: Radiation therapy has emerged as one of the preferred techniques to treat brain cancer patients. During treatment, a very high dose of radiation is delivered to a very narrow area. Prescribed radiation therapy for brain cancer requires precisely defining the target treatment area, as well as delineating vital brain structures which must be spared from radiotoxicity. Nevertheless, delineation task is usually still manually performed, which is inefficient and operator-dependent. Several attempts of automatizing this process have reported. however, marginal results when analyzing organs in the optic region. In this work we present a deep learning classification scheme based on augmented-enhanced features to automatically segment organs at risk (OARs) in the optic region -optic nerves, optic chiasm, pituitary gland and pituitary stalk-. Fifteen MR images with various types of brain tumors were retrospectively collected to undergo manual and automatic segmentation. Mean Dice Similarity coefficients around 0.80 were reported. Incorporation of proposed features yielded to improvements on the segmentation. Compared with support vector machines, our method achieved better performance with less variation on the results, as well as a considerably reduction on the classification time. Performance of the proposed approach was also evaluated with respect to manual contours. In this case, results obtained from the automatic contours mostly lie on the variability of the observers, showing no significant differences with respect to them. These results suggest therefore that the proposed system is more accurate than other presented approaches, up to date, to segment these structures. The speed, reproducibility, and robustness of the process make the proposed deep learning-based classification system a valuable tool for assisting in the delineation task of small OARs in brain cancer.



### A Paradigm Shift: Detecting Human Rights Violations Through Web Images
- **Arxiv ID**: http://arxiv.org/abs/1703.10501v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1703.10501v1)
- **Published**: 2017-03-30 14:53:55+00:00
- **Updated**: 2017-03-30 14:53:55+00:00
- **Authors**: Grigorios Kalliatakis, Shoaib Ehsan, Klaus D. McDonald-Maier
- **Comment**: Position paper, 8 pages, 3 figures
- **Journal**: None
- **Summary**: The growing presence of devices carrying digital cameras, such as mobile phones and tablets, combined with ever improving internet networks have enabled ordinary citizens, victims of human rights abuse, and participants in armed conflicts, protests, and disaster situations to capture and share via social media networks images and videos of specific events. This paper discusses the potential of images in human rights context including the opportunities and challenges they present. This study demonstrates that real-world images have the capacity to contribute complementary data to operational human rights monitoring efforts when combined with novel computer vision approaches. The analysis is concluded by arguing that if images are to be used effectively to detect and identify human rights violations by rights advocates, greater attention to gathering task-specific visual concepts from large-scale web images is required.



### Efficient optimization for Hierarchically-structured Interacting Segments (HINTS)
- **Arxiv ID**: http://arxiv.org/abs/1703.10530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10530v1)
- **Published**: 2017-03-30 15:32:29+00:00
- **Updated**: 2017-03-30 15:32:29+00:00
- **Authors**: Hossam Isack, Olga Veksler, Ipek Oguz, Milan Sonka, Yuri Boykov
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an effective optimization algorithm for a general hierarchical segmentation model with geometric interactions between segments. Any given tree can specify a partial order over object labels defining a hierarchy. It is well-established that segment interactions, such as inclusion/exclusion and margin constraints, make the model significantly more discriminant. However, existing optimization methods do not allow full use of such models. Generic -expansion results in weak local minima, while common binary multi-layered formulations lead to non-submodularity, complex high-order potentials, or polar domain unwrapping and shape biases. In practice, applying these methods to arbitrary trees does not work except for simple cases. Our main contribution is an optimization method for the Hierarchically-structured Interacting Segments (HINTS) model with arbitrary trees. Our Path-Moves algorithm is based on multi-label MRF formulation and can be seen as a combination of well-known a-expansion and Ishikawa techniques. We show state-of-the-art biomedical segmentation for many diverse examples of complex trees.



### Learning Convolutional Networks for Content-weighted Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1703.10553v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10553v2)
- **Published**: 2017-03-30 16:21:20+00:00
- **Updated**: 2017-09-19 11:23:26+00:00
- **Authors**: Mu Li, Wangmeng Zuo, Shuhang Gu, Debin Zhao, David Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Lossy image compression is generally formulated as a joint rate-distortion optimization to learn encoder, quantizer, and decoder. However, the quantizer is non-differentiable, and discrete entropy estimation usually is required for rate control. These make it very challenging to develop a convolutional network (CNN)-based image compression system. In this paper, motivated by that the local information content is spatially variant in an image, we suggest that the bit rate of the different parts of the image should be adapted to local content. And the content aware bit rate is allocated under the guidance of a content-weighted importance map. Thus, the sum of the importance map can serve as a continuous alternative of discrete entropy estimation to control compression rate. And binarizer is adopted to quantize the output of encoder due to the binarization scheme is also directly defined by the importance map. Furthermore, a proxy function is introduced for binary operation in backward propagation to make it differentiable. Therefore, the encoder, decoder, binarizer and importance map can be jointly optimized in an end-to-end manner by using a subset of the ImageNet database. In low bit rate image compression, experiments show that our system significantly outperforms JPEG and JPEG 2000 by structural similarity (SSIM) index, and can produce the much better visual result with sharp edges, rich textures, and fewer artifacts.



### Bootstrapping Labelled Dataset Construction for Cow Tracking and Behavior Analysis
- **Arxiv ID**: http://arxiv.org/abs/1703.10571v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.10571v1)
- **Published**: 2017-03-30 17:09:39+00:00
- **Updated**: 2017-03-30 17:09:39+00:00
- **Authors**: Aram Ter-Sarkisov, Robert Ross, John Kelleher
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a new approach to the long-term tracking of an object in a challenging environment. The object is a cow and the environment is an enclosure in a cowshed. Some of the key challenges in this domain are a cluttered background, low contrast and high similarity between moving objects which greatly reduces the efficiency of most existing approaches, including those based on background subtraction. Our approach is split into object localization, instance segmentation, learning and tracking stages. Our solution is compared to a range of semi-supervised object tracking algorithms and we show that the performance is strong and well suited to subsequent analysis. We present our solution as a first step towards broader tracking and behavior monitoring for cows in precision agriculture with the ultimate objective of early detection of lameness.



### MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1703.10580v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10580v2)
- **Published**: 2017-03-30 17:29:42+00:00
- **Updated**: 2017-12-07 20:38:13+00:00
- **Authors**: Ayush Tewari, Michael Zollhöfer, Hyeongwoo Kim, Pablo Garrido, Florian Bernard, Patrick Pérez, Christian Theobalt
- **Comment**: International Conference on Computer Vision (ICCV) 2017 (Oral), 13
  pages
- **Journal**: None
- **Summary**: In this work we propose a novel model-based deep convolutional autoencoder that addresses the highly challenging problem of reconstructing a 3D human face from a single in-the-wild color image. To this end, we combine a convolutional encoder network with an expert-designed generative model that serves as decoder. The core innovation is our new differentiable parametric decoder that encapsulates image formation analytically based on a generative model. Our decoder takes as input a code vector with exactly defined semantic meaning that encodes detailed face pose, shape, expression, skin reflectance and scene illumination. Due to this new way of combining CNN-based with model-based face reconstruction, the CNN-based encoder learns to extract semantically meaningful parameters from a single monocular input image. For the first time, a CNN encoder and an expert-designed generative model can be trained end-to-end in an unsupervised manner, which renders training on very large (unlabeled) real world data feasible. The obtained reconstructions compare favorably to current state-of-the-art approaches in terms of quality and richness of representation.



### Geometric Affordances from a Single Example via the Interaction Tensor
- **Arxiv ID**: http://arxiv.org/abs/1703.10584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10584v1)
- **Published**: 2017-03-30 17:32:02+00:00
- **Updated**: 2017-03-30 17:32:02+00:00
- **Authors**: Eduardo Ruiz, Walterio Mayol-Cuevas
- **Comment**: 10 pages, 12 figures
- **Journal**: None
- **Summary**: This paper develops and evaluates a new tensor field representation to express the geometric affordance of one object over another. We expand the well known bisector surface representation to one that is weight-driven and that retains the provenance of surface points with directional vectors. We also incorporate the notion of affordance keypoints which allow for faster decisions at a point of query and with a compact and straightforward descriptor. Using a single interaction example, we are able to generalize to previously-unseen scenarios; both synthetic and also real scenes captured with RGBD sensors. We show how our interaction tensor allows for significantly better performance over alternative formulations. Evaluations also include crowdsourcing comparisons that confirm the validity of our affordance proposals, which agree on average 84% of the time with human judgments, and which is 20-40% better than the baseline methods.



### Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.10593v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10593v7)
- **Published**: 2017-03-30 17:44:17+00:00
- **Updated**: 2020-08-24 16:51:03+00:00
- **Authors**: Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros
- **Comment**: An extended version of our ICCV 2017 paper, v7 fixed the typos and
  updated the implementation details. Code and data:
  https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix
- **Journal**: None
- **Summary**: Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.



### Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention
- **Arxiv ID**: http://arxiv.org/abs/1703.10631v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.10631v1)
- **Published**: 2017-03-30 18:37:49+00:00
- **Updated**: 2017-03-30 18:37:49+00:00
- **Authors**: Jinkyu Kim, John Canny
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable - they should provide easy-to-interpret rationales for their behavior - so that passengers, insurance companies, law enforcement, developers etc., can understand what triggered a particular behavior. Here we explore the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network's output (steering control). Our approach is two-stage. In the first stage, we use a visual attention model to train a convolution network end-to-end from images to steering angle. The attention model highlights image regions that potentially influence the network's output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network's behavior. We demonstrate the effectiveness of our model on three datasets totaling 16 hours of driving. We first show that training with attention does not degrade the performance of the end-to-end network. Then we show that the network causally cues on a variety of features that are used by humans while driving.



### Relevance Subject Machine: A Novel Person Re-identification Framework
- **Arxiv ID**: http://arxiv.org/abs/1703.10645v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10645v1)
- **Published**: 2017-03-30 19:21:55+00:00
- **Updated**: 2017-03-30 19:21:55+00:00
- **Authors**: Igor Fedorov, Ritwik Giri, Bhaskar D. Rao, Truong Q. Nguyen
- **Comment**: Submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: We propose a novel method called the Relevance Subject Machine (RSM) to solve the person re-identification (re-id) problem. RSM falls under the category of Bayesian sparse recovery algorithms and uses the sparse representation of the input video under a pre-defined dictionary to identify the subject in the video. Our approach focuses on the multi-shot re-id problem, which is the prevalent problem in many video analytics applications. RSM captures the essence of the multi-shot re-id problem by constraining the support of the sparse codes for each input video frame to be the same. Our proposed approach is also robust enough to deal with time varying outliers and occlusions by introducing a sparse, non-stationary noise term in the model error. We provide a novel Variational Bayesian based inference procedure along with an intuitive interpretation of the proposed update rules. We evaluate our approach over several commonly used re-id datasets and show superior performance over current state-of-the-art algorithms. Specifically, for ILIDS-VID, a recent large scale re-id dataset, RSM shows significant improvement over all published approaches, achieving an 11.5% (absolute) improvement in rank 1 accuracy over the closest competing algorithm considered.



### Towards a Visual Privacy Advisor: Understanding and Predicting Privacy Risks in Images
- **Arxiv ID**: http://arxiv.org/abs/1703.10660v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.CY, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1703.10660v2)
- **Published**: 2017-03-30 20:18:08+00:00
- **Updated**: 2017-08-07 08:35:35+00:00
- **Authors**: Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz
- **Comment**: ICCV 2017. Project page: https://tribhuvanesh.github.io/vpa
- **Journal**: None
- **Summary**: With an increasing number of users sharing information online, privacy implications entailing such actions are a major concern. For explicit content, such as user profile or GPS data, devices (e.g. mobile phones) as well as web services (e.g. Facebook) offer to set privacy settings in order to enforce the users' privacy preferences. We propose the first approach that extends this concept to image content in the spirit of a Visual Privacy Advisor. First, we categorize personal information in images into 68 image attributes and collect a dataset, which allows us to train models that predict such information directly from images. Second, we run a user study to understand the privacy preferences of different users w.r.t. such attributes. Third, we propose models that predict user specific privacy score from images in order to enforce the users' privacy preferences. Our model is trained to predict the user specific privacy risk and even outperforms the judgment of the users, who often fail to follow their own privacy preferences on image data.



### Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/1703.10664v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10664v3)
- **Published**: 2017-03-30 20:28:31+00:00
- **Updated**: 2017-08-02 04:52:02+00:00
- **Authors**: Rui Hou, Chen Chen, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been demonstrated to achieve excellent results for image classification and object detection. However, the impact of deep learning on video analysis (e.g. action detection and recognition) has been limited due to complexity of video data and lack of annotations. Previous convolutional neural networks (CNN) based video action detection approaches usually consist of two major steps: frame-level action proposal detection and association of proposals across frames. Also, these methods employ two-stream CNN framework to handle spatial and temporal feature separately. In this paper, we propose an end-to-end deep network called Tube Convolutional Neural Network (T-CNN) for action detection in videos. The proposed architecture is a unified network that is able to recognize and localize action based on 3D convolution features. A video is first divided into equal length clips and for each clip a set of tube proposals are generated next based on 3D Convolutional Network (ConvNet) features. Finally, the tube proposals of different clips are linked together employing network flow and spatio-temporal action detection is performed using these linked video proposals. Extensive experiments on several video datasets demonstrate the superior performance of T-CNN for classifying and localizing actions in both trimmed and untrimmed videos compared to state-of-the-arts.



### TS-LSTM and Temporal-Inception: Exploiting Spatiotemporal Dynamics for Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.10667v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10667v1)
- **Published**: 2017-03-30 20:45:00+00:00
- **Updated**: 2017-03-30 20:45:00+00:00
- **Authors**: Chih-Yao Ma, Min-Hung Chen, Zsolt Kira, Ghassan AlRegib
- **Comment**: 16 pages, 11 figures
- **Journal**: None
- **Summary**: Recent two-stream deep Convolutional Neural Networks (ConvNets) have made significant progress in recognizing human actions in videos. Despite their success, methods extending the basic two-stream ConvNet have not systematically explored possible network architectures to further exploit spatiotemporal dynamics within video sequences. Further, such networks often use different baseline two-stream networks. Therefore, the differences and the distinguishing factors between various methods using Recurrent Neural Networks (RNN) or convolutional networks on temporally-constructed feature vectors (Temporal-ConvNet) are unclear. In this work, we first demonstrate a strong baseline two-stream ConvNet using ResNet-101. We use this baseline to thoroughly examine the use of both RNNs and Temporal-ConvNets for extracting spatiotemporal information. Building upon our experimental results, we then propose and investigate two different networks to further integrate spatiotemporal information: 1) temporal segment RNN and 2) Inception-style Temporal-ConvNet. We demonstrate that using both RNNs (using LSTMs) and Temporal-ConvNets on spatiotemporal feature matrices are able to exploit spatiotemporal dynamics to improve the overall performance. However, each of these methods require proper care to achieve state-of-the-art performance; for example, LSTMs require pre-segmented data or else they cannot fully exploit temporal information. Our analysis identifies specific limitations for each method that could form the basis of future work. Our experimental results on UCF101 and HMDB51 datasets achieve state-of-the-art performances, 94.1% and 69.0%, respectively, without requiring extensive temporal augmentation.



### Concurrent Segmentation and Localization for Tracking of Surgical Instruments
- **Arxiv ID**: http://arxiv.org/abs/1703.10701v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10701v2)
- **Published**: 2017-03-30 22:37:38+00:00
- **Updated**: 2017-08-01 13:05:21+00:00
- **Authors**: Iro Laina, Nicola Rieke, Christian Rupprecht, Josué Page Vizcaíno, Abouzar Eslami, Federico Tombari, Nassir Navab
- **Comment**: I. Laina and N. Rieke contributed equally to this work. Accepted to
  MICCAI 2017
- **Journal**: None
- **Summary**: Real-time instrument tracking is a crucial requirement for various computer-assisted interventions. In order to overcome problems such as specular reflections and motion blur, we propose a novel method that takes advantage of the interdependency between localization and segmentation of the surgical tool. In particular, we reformulate the 2D instrument pose estimation as heatmap regression and thereby enable a concurrent, robust and near real-time regression of both tasks via deep learning. As demonstrated by our experimental results, this modeling leads to a significantly improved performance than directly regressing the tool position and allows our method to outperform the state of the art on a Retinal Microsurgery benchmark and the MICCAI EndoVis Challenge 2015.



### Deep 3D Face Identification
- **Arxiv ID**: http://arxiv.org/abs/1703.10714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10714v1)
- **Published**: 2017-03-30 23:49:23+00:00
- **Updated**: 2017-03-30 23:49:23+00:00
- **Authors**: Donghyun Kim, Matthias Hernandez, Jongmoo Choi, Gerard Medioni
- **Comment**: 9 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: We propose a novel 3D face recognition algorithm using a deep convolutional neural network (DCNN) and a 3D augmentation technique. The performance of 2D face recognition algorithms has significantly increased by leveraging the representational power of deep neural networks and the use of large-scale labeled training data. As opposed to 2D face recognition, training discriminative deep features for 3D face recognition is very difficult due to the lack of large-scale 3D face datasets. In this paper, we show that transfer learning from a CNN trained on 2D face images can effectively work for 3D face recognition by fine-tuning the CNN with a relatively small number of 3D facial scans. We also propose a 3D face augmentation technique which synthesizes a number of different facial expressions from a single 3D face scan. Our proposed method shows excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC datasets, without using hand-crafted features. The 3D identification using our deep features also scales well for large databases.



