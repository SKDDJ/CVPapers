# Arxiv Papers in cs.CV on 2017-03-18
### Recurrent Models for Situation Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.06233v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06233v2)
- **Published**: 2017-03-18 02:00:22+00:00
- **Updated**: 2017-08-04 17:03:56+00:00
- **Authors**: Arun Mallya, Svetlana Lazebnik
- **Comment**: To appear at ICCV 2017
- **Journal**: None
- **Summary**: This work proposes Recurrent Neural Network (RNN) models to predict structured 'image situations' -- actions and noun entities fulfilling semantic roles related to the action. In contrast to prior work relying on Conditional Random Fields (CRFs), we use a specialized action prediction network followed by an RNN for noun prediction. Our system obtains state-of-the-art accuracy on the challenging recent imSitu dataset, beating CRF-based models, including ones trained with additional data. Further, we show that specialized features learned from situation prediction can be transferred to the task of image captioning to more accurately describe human-object interactions.



### RoomNet: End-to-End Room Layout Estimation
- **Arxiv ID**: http://arxiv.org/abs/1703.06241v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06241v2)
- **Published**: 2017-03-18 03:35:56+00:00
- **Updated**: 2017-08-07 19:58:46+00:00
- **Authors**: Chen-Yu Lee, Vijay Badrinarayanan, Tomasz Malisiewicz, Andrew Rabinovich
- **Comment**: accepted at ICCV 2017
- **Journal**: None
- **Summary**: This paper focuses on the task of room layout estimation from a monocular RGB image. Prior works break the problem into two sub-tasks: semantic segmentation of floor, walls, ceiling to produce layout hypotheses, followed by an iterative optimization step to rank these hypotheses. In contrast, we adopt a more direct formulation of this problem as one of estimating an ordered set of room layout keypoints. The room layout and the corresponding segmentation is completely specified given the locations of these ordered keypoints. We predict the locations of the room layout keypoints using RoomNet, an end-to-end trainable encoder-decoder network. On the challenging benchmark datasets Hedau and LSUN, we achieve state-of-the-art performance along with 200x to 600x speedup compared to the most recent work. Additionally, we present optional extensions to the RoomNet architecture such as including recurrent computations and memory units to refine the keypoint locations under the same parametric capacity.



### Towards Context-aware Interaction Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.06246v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06246v3)
- **Published**: 2017-03-18 03:59:21+00:00
- **Updated**: 2017-04-30 23:55:42+00:00
- **Authors**: Bohan Zhuang, Lingqiao Liu, Chunhua Shen, Ian Reid
- **Comment**: Fixed typos
- **Journal**: None
- **Summary**: Recognizing how objects interact with each other is a crucial task in visual recognition. If we define the context of the interaction to be the objects involved, then most current methods can be categorized as either: (i) training a single classifier on the combination of the interaction and its context; or (ii) aiming to recognize the interaction independently of its explicit context. Both methods suffer limitations: the former scales poorly with the number of combinations and fails to generalize to unseen combinations, while the latter often leads to poor interaction recognition performance due to the difficulty of designing a context-independent interaction classifier. To mitigate those drawbacks, this paper proposes an alternative, context-aware interaction recognition framework. The key to our method is to explicitly construct an interaction classifier which combines the context, and the interaction. The context is encoded via word2vec into a semantic space, and is used to derive a classification result for the interaction.   The proposed method still builds one classifier for one interaction (as per type (ii) above), but the classifier built is adaptive to context via weights which are context dependent. The benefit of using the semantic space is that it naturally leads to zero-shot generalizations in which semantically similar contexts (subjectobject pairs) can be recognized as suitable contexts for an interaction, even if they were not observed in the training set.



### A Fast HOG Descriptor Using Lookup Table and Integral Image
- **Arxiv ID**: http://arxiv.org/abs/1703.06256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06256v1)
- **Published**: 2017-03-18 04:58:32+00:00
- **Updated**: 2017-03-18 04:58:32+00:00
- **Authors**: Chunde Huang, Jiaxiang Huang
- **Comment**: None
- **Journal**: None
- **Summary**: The histogram of oriented gradients (HOG) is a widely used feature descriptor in computer vision for the purpose of object detection. In the paper, a modified HOG descriptor is described, it uses a lookup table and the method of integral image to speed up the detection performance by a factor of 5~10. By exploiting the special hardware features of a given platform(e.g. a digital signal processor), further improvement can be made to the HOG descriptor in order to have real-time object detection and tracking.



### Single image super-resolution using self-optimizing mask via fractional-order gradient interpolation and reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1703.06260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06260v1)
- **Published**: 2017-03-18 06:57:12+00:00
- **Updated**: 2017-03-18 06:57:12+00:00
- **Authors**: Qi Yang, Yanzhu Zhang, Tiebiao Zhao, YangQuan Chen
- **Comment**: 24 pages, 13 figures, it is to appear in ISA Transactions
- **Journal**: None
- **Summary**: Image super-resolution using self-optimizing mask via fractional-order gradient interpolation and reconstruction aims to recover detailed information from low-resolution images and reconstruct them into high-resolution images. Due to the limited amount of data and information retrieved from low-resolution images, it is difficult to restore clear, artifact-free images, while still preserving enough structure of the image such as the texture. This paper presents a new single image super-resolution method which is based on adaptive fractional-order gradient interpolation and reconstruction. The interpolated image gradient via optimal fractional-order gradient is first constructed according to the image similarity and afterwards the minimum energy function is employed to reconstruct the final high-resolution image. Fractional-order gradient based interpolation methods provide an additional degree of freedom which helps optimize the implementation quality due to the fact that an extra free parameter $\alpha$-order is being used. The proposed method is able to produce a rich texture detail while still being able to maintain structural similarity even under large zoom conditions. Experimental results show that the proposed method performs better than current single image super-resolution techniques.



### Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters
- **Arxiv ID**: http://arxiv.org/abs/1703.06283v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1703.06283v2)
- **Published**: 2017-03-18 10:52:53+00:00
- **Updated**: 2017-04-10 14:59:25+00:00
- **Authors**: Shiyu Huang, Deva Ramanan
- **Comment**: To appear in CVPR 2017
- **Journal**: None
- **Summary**: As autonomous vehicles become an every-day reality, high-accuracy pedestrian detection is of paramount practical importance. Pedestrian detection is a highly researched topic with mature methods, but most datasets focus on common scenes of people engaged in typical walking poses on sidewalks. But performance is most crucial for dangerous scenarios, such as children playing in the street or people using bicycles/skateboards in unexpected ways. Such "in-the-tail" data is notoriously hard to observe, making both training and testing difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precarious Pedestrian dataset. Even given a dedicated collection effort, it is relatively small by contemporary standards (around 1000 images). To allow for large-scale data-driven learning, we explore the use of synthetic data generated by a game engine. A significant challenge is selected the right "priors" or parameters for synthesis: we would like realistic data with poses and object configurations that mimic true Precarious Pedestrians. Inspired by Generative Adversarial Networks (GANs), we generate a massive amount of synthetic data and train a discriminative classifier to select a realistic subset, which we deem the Adversarial Imposters. We demonstrate that this simple pipeline allows one to synthesize realistic training data by making use of rendering/animation engines within a GAN framework. Interestingly, we also demonstrate that such data can be used to rank algorithms, suggesting that Adversarial Imposters can also be used for "in-the-tail" validation at test-time, a notoriously difficult challenge for real-world deployment.



### PatternNet: Visual Pattern Mining with Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1703.06339v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06339v2)
- **Published**: 2017-03-18 19:21:04+00:00
- **Updated**: 2018-06-13 22:42:07+00:00
- **Authors**: Hongzhi Li, Joseph G. Ellis, Lei Zhang, Shih-Fu Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual patterns represent the discernible regularity in the visual world. They capture the essential nature of visual objects or scenes. Understanding and modeling visual patterns is a fundamental problem in visual recognition that has wide ranging applications. In this paper, we study the problem of visual pattern mining and propose a novel deep neural network architecture called PatternNet for discovering these patterns that are both discriminative and representative. The proposed PatternNet leverages the filters in the last convolution layer of a convolutional neural network to find locally consistent visual patches, and by combining these filters we can effectively discover unique visual patterns. In addition, PatternNet can discover visual patterns efficiently without performing expensive image patch sampling, and this advantage provides an order of magnitude speedup compared to most other approaches. We evaluate the proposed PatternNet subjectively by showing randomly selected visual patterns which are discovered by our method and quantitatively by performing image classification with the identified visual patterns and comparing our performance with the current state-of-the-art. We also directly evaluate the quality of the discovered visual patterns by leveraging the identified patterns as proposed objects in an image and compare with other relevant methods. Our proposed network and procedure, PatterNet, is able to outperform competing methods for the tasks described.



