# Arxiv Papers in cs.CV on 2017-03-06
### 4-DoF Tracking for Robot Fine Manipulation Tasks
- **Arxiv ID**: http://arxiv.org/abs/1703.01698v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01698v2)
- **Published**: 2017-03-06 00:59:46+00:00
- **Updated**: 2017-04-04 01:33:14+00:00
- **Authors**: Mennatullah Siam, Abhineet Singh, Camilo Perez, Martin Jagersand
- **Comment**: accepted in CRV 2017
- **Journal**: None
- **Summary**: This paper presents two visual trackers from the different paradigms of learning and registration based tracking and evaluates their application in image based visual servoing. They can track object motion with four degrees of freedom (DoF) which, as we will show here, is sufficient for many fine manipulation tasks. One of these trackers is a newly developed learning based tracker that relies on learning discriminative correlation filters while the other is a refinement of a recent 8 DoF RANSAC based tracker adapted with a new appearance model for tracking 4 DoF motion.   Both trackers are shown to provide superior performance to several state of the art trackers on an existing dataset for manipulation tasks. Further, a new dataset with challenging sequences for fine manipulation tasks captured from robot mounted eye-in-hand (EIH) cameras is also presented. These sequences have a variety of challenges encountered during real tasks including jittery camera movement, motion blur, drastic scale changes and partial occlusions. Quantitative and qualitative results on these sequences are used to show that these two trackers are robust to failures while providing high precision that makes them suitable for such fine manipulation tasks.



### Viewpoint Selection for Photographing Architectures
- **Arxiv ID**: http://arxiv.org/abs/1703.01702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01702v1)
- **Published**: 2017-03-06 01:44:35+00:00
- **Updated**: 2017-03-06 01:44:35+00:00
- **Authors**: Jingwu He, Linbo Wang, Wenzhe Zhou, Hongjie Zhang, Xiufen Cui, Yanwen Guo
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies the problem of how to choose good viewpoints for taking photographs of architectures. We achieve this by learning from professional photographs of world famous landmarks that are available on the Internet. Unlike previous efforts devoted to photo quality assessment which mainly rely on 2D image features, we show in this paper combining 2D image features extracted from images with 3D geometric features computed on the 3D models can result in more reliable evaluation of viewpoint quality. Specifically, we collect a set of photographs for each of 15 world famous architectures as well as their 3D models from the Internet. Viewpoint recovery for images is carried out through an image-model registration process, after which a newly proposed viewpoint clustering strategy is exploited to validate users' viewpoint preferences when photographing landmarks. Finally, we extract a number of 2D and 3D features for each image based on multiple visual and geometric cues and perform viewpoint recommendation by learning from both 2D and 3D features using a specifically designed SVM-2K multi-view learner, achieving superior performance over using solely 2D or 3D features. We show the effectiveness of the proposed approach through extensive experiments. The experiments also demonstrate that our system can be used to recommend viewpoints for rendering textured 3D models of buildings for the use of architectural design, in addition to viewpoint evaluation of photographs and recommendation of viewpoints for photographing architectures in practice.



### Cats and Captions vs. Creators and the Clock: Comparing Multimodal Content to Context in Predicting Relative Popularity
- **Arxiv ID**: http://arxiv.org/abs/1703.01725v1
- **DOI**: 10.1145/3038912.3052684
- **Categories**: **cs.SI**, cs.CL, cs.CV, physics.soc-ph
- **Links**: [PDF](http://arxiv.org/pdf/1703.01725v1)
- **Published**: 2017-03-06 04:56:19+00:00
- **Updated**: 2017-03-06 04:56:19+00:00
- **Authors**: Jack Hessel, Lillian Lee, David Mimno
- **Comment**: 10 pages, data and models available at
  http://www.cs.cornell.edu/~jhessel/cats/cats.html, Proceedings of WWW 2017
- **Journal**: None
- **Summary**: The content of today's social media is becoming more and more rich, increasingly mixing text, images, videos, and audio. It is an intriguing research question to model the interplay between these different modes in attracting user attention and engagement. But in order to pursue this study of multimodal content, we must also account for context: timing effects, community preferences, and social factors (e.g., which authors are already popular) also affect the amount of feedback and reaction that social-media posts receive. In this work, we separate out the influence of these non-content factors in several ways. First, we focus on ranking pairs of submissions posted to the same community in quick succession, e.g., within 30 seconds, this framing encourages models to focus on time-agnostic and community-specific content features. Within that setting, we determine the relative performance of author vs. content features. We find that victory usually belongs to "cats and captions," as visual and textual features together tend to outperform identity-based features. Moreover, our experiments show that when considered in isolation, simple unigram text features and deep neural network visual features yield the highest accuracy individually, and that the combination of the two modalities generally leads to the best accuracies overall.



### Building a Regular Decision Boundary with Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.01775v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.01775v1)
- **Published**: 2017-03-06 09:21:35+00:00
- **Updated**: 2017-03-06 09:21:35+00:00
- **Authors**: Edouard Oyallon
- **Comment**: CVPR 2017, 8 pages
- **Journal**: None
- **Summary**: In this work, we build a generic architecture of Convolutional Neural Networks to discover empirical properties of neural networks. Our first contribution is to introduce a state-of-the-art framework that depends upon few hyper parameters and to study the network when we vary them. It has no max pooling, no biases, only 13 layers, is purely convolutional and yields up to 95.4% and 79.6% accuracy respectively on CIFAR10 and CIFAR100. We show that the nonlinearity of a deep network does not need to be continuous, non expansive or point-wise, to achieve good performance. We show that increasing the width of our network permits being competitive with very deep networks. Our second contribution is an analysis of the contraction and separation properties of this network. Indeed, a 1-nearest neighbor classifier applied on deep features progressively improves with depth, which indicates that the representation is progressively more regular. Besides, we defined and analyzed local support vectors that separate classes locally. All our experiments are reproducible and code is available online, based on TensorFlow.



### All the people around me: face discovery in egocentric photo-streams
- **Arxiv ID**: http://arxiv.org/abs/1703.01790v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01790v2)
- **Published**: 2017-03-06 09:50:39+00:00
- **Updated**: 2017-05-12 11:29:38+00:00
- **Authors**: Maedeh Aghaei, Mariella Dimiccoli, Petia Radeva
- **Comment**: 5 pages, 3 figures, accepted in IEEE International Conference on
  Image Processing (ICIP 2017)
- **Journal**: None
- **Summary**: Given an unconstrained stream of images captured by a wearable photo-camera (2fpm), we propose an unsupervised bottom-up approach for automatic clustering appearing faces into the individual identities present in these data. The problem is challenging since images are acquired under real world conditions; hence the visible appearance of the people in the images undergoes intensive variations. Our proposed pipeline consists of first arranging the photo-stream into events, later, localizing the appearance of multiple people in them, and finally, grouping various appearances of the same person across different events. Experimental results performed on a dataset acquired by wearing a photo-camera during one month, demonstrate the effectiveness of the proposed approach for the considered purpose.



### All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation
- **Arxiv ID**: http://arxiv.org/abs/1703.01827v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1703.01827v3)
- **Published**: 2017-03-06 11:54:43+00:00
- **Updated**: 2017-04-10 02:12:29+00:00
- **Authors**: Di Xie, Jiang Xiong, Shiliang Pu
- **Comment**: Updating experiments; CVPR2017
- **Journal**: None
- **Summary**: Deep neural network is difficult to train and this predicament becomes worse as the depth increases. The essence of this problem exists in the magnitude of backpropagated errors that will result in gradient vanishing or exploding phenomenon. We show that a variant of regularizer which utilizes orthonormality among different filter banks can alleviate this problem. Moreover, we design a backward error modulation mechanism based on the quasi-isometry assumption between two consecutive parametric layers. Equipped with these two ingredients, we propose several novel optimization solutions that can be utilized for training a specific-structured (repetitively triple modules of Conv-BNReLU) extremely deep convolutional neural network (CNN) WITHOUT any shortcuts/ identity mappings from scratch. Experiments show that our proposed solutions can achieve distinct improvements for a 44-layer and a 110-layer plain networks on both the CIFAR-10 and ImageNet datasets. Moreover, we can successfully train plain CNNs to match the performance of the residual counterparts.   Besides, we propose new principles for designing network structure from the insights evoked by orthonormality. Combined with residual structure, we achieve comparative performance on the ImageNet dataset.



### Evaluating Graph Signal Processing for Neuroimaging Through Classification and Dimensionality Reduction
- **Arxiv ID**: http://arxiv.org/abs/1703.01842v3
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.01842v3)
- **Published**: 2017-03-06 12:45:39+00:00
- **Updated**: 2017-08-28 12:41:28+00:00
- **Authors**: Mathilde Ménoret, Nicolas Farrugia, Bastien Pasdeloup, Vincent Gripon
- **Comment**: 5 pages, GlobalSIP 2017
- **Journal**: None
- **Summary**: Graph Signal Processing (GSP) is a promising framework to analyze multi-dimensional neuroimaging datasets, while taking into account both the spatial and functional dependencies between brain signals. In the present work, we apply dimensionality reduction techniques based on graph representations of the brain to decode brain activity from real and simulated fMRI datasets. We introduce seven graphs obtained from a) geometric structure and/or b) functional connectivity between brain areas at rest, and compare them when performing dimension reduction for classification. We show that mixed graphs using both a) and b) offer the best performance. We also show that graph sampling methods perform better than classical dimension reduction including Principal Component Analysis (PCA) and Independent Component Analysis (ICA).



### Deep Head Pose Estimation from Depth Data for In-car Automotive Applications
- **Arxiv ID**: http://arxiv.org/abs/1703.01883v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01883v1)
- **Published**: 2017-03-06 14:11:55+00:00
- **Updated**: 2017-03-06 14:11:55+00:00
- **Authors**: Marco Venturelli, Guido Borghi, Roberto Vezzani, Rita Cucchiara
- **Comment**: 2nd International Workshop on Understanding Human Activities through
  3D Sensors (ICPR 2016)
- **Journal**: None
- **Summary**: Recently, deep learning approaches have achieved promising results in various fields of computer vision. In this paper, we tackle the problem of head pose estimation through a Convolutional Neural Network (CNN). Differently from other proposals in the literature, the described system is able to work directly and based only on raw depth data. Moreover, the head pose estimation is solved as a regression problem and does not rely on visual facial features like facial landmarks. We tested our system on a well known public dataset, Biwi Kinect Head Pose, showing that our approach achieves state-of-art results and is able to meet real time performance requirements.



### Direct White Matter Bundle Segmentation using Stacked U-Nets
- **Arxiv ID**: http://arxiv.org/abs/1703.02036v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1703.02036v1)
- **Published**: 2017-03-06 14:21:49+00:00
- **Updated**: 2017-03-06 14:21:49+00:00
- **Authors**: Jakob Wasserthal, Peter F. Neher, Fabian Isensee, Klaus H. Maier-Hein
- **Comment**: None
- **Journal**: None
- **Summary**: The state-of-the-art method for automatically segmenting white matter bundles in diffusion-weighted MRI is tractography in conjunction with streamline cluster selection. This process involves long chains of processing steps which are not only computationally expensive but also complex to setup and tedious with respect to quality control. Direct bundle segmentation methods treat the task as a traditional image segmentation problem. While they so far did not deliver competitive results, they can potentially mitigate many of the mentioned issues. We present a novel supervised approach for direct tract segmentation that shows major performance gains. It builds upon a stacked U-Net architecture which is trained on manual bundle segmentations from Human Connectome Project subjects. We evaluate our approach \textit{in vivo} as well as \textit{in silico} using the ISMRM 2015 Tractography Challenge phantom dataset. We achieve human segmentation performance and a major performance gain over previous pipelines. We show how the learned spatial priors efficiently guide the segmentation even at lower image qualities with little quality loss.



### High-Resolution Multispectral Dataset for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1703.01918v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1703.01918v1)
- **Published**: 2017-03-06 15:16:56+00:00
- **Updated**: 2017-03-06 15:16:56+00:00
- **Authors**: Ronald Kemker, Carl Salvaggio, Christopher Kanan
- **Comment**: 9 pages, 8 Figures
- **Journal**: None
- **Summary**: Unmanned aircraft have decreased the cost required to collect remote sensing imagery, which has enabled researchers to collect high-spatial resolution data from multiple sensor modalities more frequently and easily. The increase in data will push the need for semantic segmentation frameworks that are able to classify non-RGB imagery, but this type of algorithmic development requires an increase in publicly available benchmark datasets with class labels. In this paper, we introduce a high-resolution multispectral dataset with image labels. This new benchmark dataset has been pre-split into training/testing folds in order to standardize evaluation and continue to push state-of-the-art classification frameworks for non-RGB imagery.



### Cellulyzer - Automated analysis and interactive visualization/simulation of select cellular processes
- **Arxiv ID**: http://arxiv.org/abs/1703.02611v1
- **DOI**: None
- **Categories**: **physics.bio-ph**, cs.CV, physics.data-an, q-bio.SC
- **Links**: [PDF](http://arxiv.org/pdf/1703.02611v1)
- **Published**: 2017-03-06 16:29:09+00:00
- **Updated**: 2017-03-06 16:29:09+00:00
- **Authors**: Aliakbar Jafarpour, Holger Lorenz
- **Comment**: None
- **Journal**: None
- **Summary**: Here we report on a set of programs developed at the ZMBH Bio-Imaging Facility for tracking real-life images of cellular processes. These programs perform 1) automated tracking; 2) quantitative and comparative track analyses of different images in different groups; 3) different interactive visualization schemes; and 4) interactive realistic simulation of different cellular processes for validation and optimal problem-specific adjustment of image acquisition parameters (tradeoff between speed, resolution, and quality with feedback from the very final results). The collection of programs is primarily developed for the common bio-image analysis software ImageJ (as a single Java Plugin). Some programs are also available in other languages (C++ and Javascript) and may be run simply with a web-browser; even on a low-end Tablet or Smartphone. The programs are available at https://github.com/nurlicht/CellulyzerDemo



### Mesh-to-raster based non-rigid registration of multi-modal images
- **Arxiv ID**: http://arxiv.org/abs/1703.01972v2
- **DOI**: 10.1117/1.JMI.4.4.044002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01972v2)
- **Published**: 2017-03-06 16:58:03+00:00
- **Updated**: 2017-10-31 15:35:05+00:00
- **Authors**: Rosalia Tatano, Benjamin Berkels, Thomas M. Deserno
- **Comment**: None
- **Journal**: Journal of Medical Imaging 4(4), 044002 (27 October 2017)
- **Summary**: Region of interest (ROI) alignment in medical images plays a crucial role in diagnostics, procedure planning, treatment, and follow-up. Frequently, a model is represented as triangulated mesh while the patient data is provided from CAT scanners as pixel or voxel data. Previously, we presented a 2D method for curve-to-pixel registration. This paper contributes (i) a general mesh-to-raster (M2R) framework to register ROIs in multi-modal images; (ii) a 3D surface-to-voxel application, and (iii) a comprehensive quantitative evaluation in 2D using ground truth provided by the simultaneous truth and performance level estimation (STAPLE) method. The registration is formulated as a minimization problem where the objective consists of a data term, which involves the signed distance function of the ROI from the reference image, and a higher order elastic regularizer for the deformation. The evaluation is based on quantitative light-induced fluoroscopy (QLF) and digital photography (DP) of decalcified teeth. STAPLE is computed on 150 image pairs from 32 subjects, each showing one corresponding tooth in both modalities. The ROI in each image is manually marked by three experts (900 curves in total). In the QLF-DP setting, our approach significantly outperforms the mutual information-based registration algorithm implemented with the Insight Segmentation and Registration Toolkit (ITK) and Elastix.



### Incorporating the Knowledge of Dermatologists to Convolutional Neural Networks for the Diagnosis of Skin Lesions
- **Arxiv ID**: http://arxiv.org/abs/1703.01976v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01976v3)
- **Published**: 2017-03-06 17:02:19+00:00
- **Updated**: 2017-06-02 11:19:21+00:00
- **Authors**: Iván González Díaz
- **Comment**: None
- **Journal**: None
- **Summary**: This report describes our submission to the ISIC 2017 Challenge in Skin Lesion Analysis Towards Melanoma Detection. We have participated in the Part 3: Lesion Classification with a system for automatic diagnosis of nevus, melanoma and seborrheic keratosis. Our approach aims to incorporate the expert knowledge of dermatologists into the well known framework of Convolutional Neural Networks (CNN), which have shown impressive performance in many visual recognition tasks. In particular, we have designed several networks providing lesion area identification, lesion segmentation into structural patterns and final diagnosis of clinical cases. Furthermore, novel blocks for CNNs have been designed to integrate this information with the diagnosis processing pipeline.



### Activation Maximization Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1703.02000v9
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.02000v9)
- **Published**: 2017-03-06 17:42:55+00:00
- **Updated**: 2018-11-16 07:18:19+00:00
- **Authors**: Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang, Yong Yu, Jun Wang
- **Comment**: Accepted as a conference paper on ICLR 2018
- **Journal**: None
- **Summary**: Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide a more accurate estimation of the sample quality. Our proposed model also outperforms the baseline methods in the new metric.



### Learning across scales - A multiscale method for Convolution Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.02009v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.02009v2)
- **Published**: 2017-03-06 18:15:40+00:00
- **Updated**: 2017-06-22 16:39:14+00:00
- **Authors**: Eldad Haber, Lars Ruthotto, Elliot Holtham, Seong-Hwan Jun
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we establish the relation between optimal control and training deep Convolution Neural Networks (CNNs). We show that the forward propagation in CNNs can be interpreted as a time-dependent nonlinear differential equation and learning as controlling the parameters of the differential equation such that the network approximates the data-label relation for given training data. Using this continuous interpretation we derive two new methods to scale CNNs with respect to two different dimensions. The first class of multiscale methods connects low-resolution and high-resolution data through prolongation and restriction of CNN parameters. We demonstrate that this enables classifying high-resolution images using CNNs trained with low-resolution images and vice versa and warm-starting the learning process. The second class of multiscale methods connects shallow and deep networks and leads to new training strategies that gradually increase the depths of the CNN while re-using parameters for initializations.



### Fast Back-Projection for Non-Line of Sight Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1703.02016v2
- **DOI**: 10.1364/OE.25.011574
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1703.02016v2)
- **Published**: 2017-03-06 18:33:01+00:00
- **Updated**: 2017-06-20 11:02:49+00:00
- **Authors**: Victor Arellano, Diego Gutierrez, Adrian Jarabo
- **Comment**: None
- **Journal**: Opt. Express 25, 11574-11583 (2017)
- **Summary**: Recent works have demonstrated non-line of sight (NLOS) reconstruction by using the time-resolved signal frommultiply scattered light. These works combine ultrafast imaging systems with computation, which back-projects the recorded space-time signal to build a probabilistic map of the hidden geometry. Unfortunately, this computation is slow, becoming a bottleneck as the imaging technology improves. In this work, we propose a new back-projection technique for NLOS reconstruction, which is up to a thousand times faster than previous work, with almost no quality loss. We base on the observation that the hidden geometry probability map can be built as the intersection of the three-bounce space-time manifolds defined by the light illuminating the hidden geometry and the visible point receiving the scattered light from such hidden geometry. This allows us to pose the reconstruction of the hidden geometry as the voxelization of these space-time manifolds, which has lower theoretic complexity and is easily implementable in the GPU. We demonstrate the efficiency and quality of our technique compared against previous methods in both captured and synthetic data



### Combining Self-Supervised Learning and Imitation for Vision-Based Rope Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1703.02018v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1703.02018v1)
- **Published**: 2017-03-06 18:40:29+00:00
- **Updated**: 2017-03-06 18:40:29+00:00
- **Authors**: Ashvin Nair, Dian Chen, Pulkit Agrawal, Phillip Isola, Pieter Abbeel, Jitendra Malik, Sergey Levine
- **Comment**: 8 pages, accepted to International Conference on Robotics and
  Automation (ICRA) 2017
- **Journal**: None
- **Summary**: Manipulation of deformable objects, such as ropes and cloth, is an important but challenging problem in robotics. We present a learning-based system where a robot takes as input a sequence of images of a human manipulating a rope from an initial to goal configuration, and outputs a sequence of actions that can reproduce the human demonstration, using only monocular images as input. To perform this task, the robot learns a pixel-level inverse dynamics model of rope manipulation directly from images in a self-supervised manner, using about 60K interactions with the rope collected autonomously by the robot. The human demonstration provides a high-level plan of what to do and the low-level inverse model is used to execute the plan. We show that by combining the high and low-level plans, the robot can successfully manipulate a rope into a variety of target shapes using only a sequence of human-provided images for direction.



### Auto-context Convolutional Neural Network (Auto-Net) for Brain Extraction in Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/1703.02083v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02083v2)
- **Published**: 2017-03-06 19:50:20+00:00
- **Updated**: 2017-06-19 20:31:43+00:00
- **Authors**: Seyed Sadegh Mohseni Salehi, Deniz Erdogmus, Ali Gholipour
- **Comment**: This manuscripts has been submitted to TMI
- **Journal**: None
- **Summary**: Brain extraction or whole brain segmentation is an important first step in many of the neuroimage analysis pipelines. The accuracy and robustness of brain extraction, therefore, is crucial for the accuracy of the entire brain analysis process. With the aim of designing a learning-based, geometry-independent and registration-free brain extraction tool in this study, we present a technique based on an auto-context convolutional neural network (CNN), in which intrinsic local and global image features are learned through 2D patches of different window sizes. In this architecture three parallel 2D convolutional pathways for three different directions (axial, coronal, and sagittal) implicitly learn 3D image information without the need for computationally expensive 3D convolutions. Posterior probability maps generated by the network are used iteratively as context information along with the original image patches to learn the local shape and connectedness of the brain, to extract it from non-brain tissue.   The brain extraction results we have obtained from our algorithm are superior to the recently reported results in the literature on two publicly available benchmark datasets, namely LPBA40 and OASIS, in which we obtained Dice overlap coefficients of 97.42% and 95.40%, respectively. Furthermore, we evaluated the performance of our algorithm in the challenging problem of extracting arbitrarily-oriented fetal brains in reconstructed fetal brain magnetic resonance imaging (MRI) datasets. In this application our algorithm performed much better than the other methods (Dice coefficient: 95.98%), where the other methods performed poorly due to the non-standard orientation and geometry of the fetal brain in MRI. Our CNN-based method can provide accurate, geometry-independent brain extraction in challenging applications.



### Randomized Iterative Reconstruction for Sparse View X-ray Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/1703.04393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04393v1)
- **Published**: 2017-03-06 20:24:13+00:00
- **Updated**: 2017-03-06 20:24:13+00:00
- **Authors**: D. Trinca, Y. Zhong
- **Comment**: 23 pages
- **Journal**: None
- **Summary**: With the availability of more powerful computers, iterative reconstruction algorithms are the subject of an ongoing work in the design of more efficient reconstruction algorithms for X-ray computed tomography. In this work, we show how two analytical reconstruction algorithms can be improved by correcting the corresponding reconstructions using a randomized iterative reconstruction algorithm. The combined analytical reconstruction followed by randomized iterative reconstruction can also be viewed as a reconstruction algorithm which, in the experiments we have conducted, uses up to $35\%$ less projection angles as compared to the analytical reconstruction algorithms and produces the same results in terms of quality of reconstruction, without increasing the execution time significantly.



### An optimal hierarchical clustering approach to segmentation of mobile LiDAR point clouds
- **Arxiv ID**: http://arxiv.org/abs/1703.02150v2
- **DOI**: 10.1109/TITS.2019.2912455
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02150v2)
- **Published**: 2017-03-06 23:48:16+00:00
- **Updated**: 2017-11-09 21:12:45+00:00
- **Authors**: Sheng Xu, Ruisheng Wang, Han Zheng
- **Comment**: Submitted to IEEE Transaction on Geoscience and Remote Sense
- **Journal**: None
- **Summary**: This paper proposes a hierarchical clustering approach for the segmentation of mobile LiDAR point clouds. We perform the hierarchical clustering on unorganized point clouds based on a proximity matrix. The dissimilarity measure in the proximity matrix is calculated by the Euclidean distances between clusters and the difference of normal vectors at given points. The main contribution of this paper is that we succeed to optimize the combination of clusters in the hierarchical clustering. The combination is obtained by achieving the matching of a bipartite graph, and optimized by solving the minimum-cost perfect matching. Results show that the proposed optimal hierarchical clustering (OHC) succeeds to achieve the segmentation of multiple individual objects automatically and outperforms the state-of-the-art LiDAR point cloud segmentation approaches.



