# Arxiv Papers in cs.CV on 2017-03-27
### Transductive Zero-Shot Learning with a Self-training dictionary approach
- **Arxiv ID**: http://arxiv.org/abs/1703.08893v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08893v1)
- **Published**: 2017-03-27 01:36:38+00:00
- **Updated**: 2017-03-27 01:36:38+00:00
- **Authors**: Yunlong Yu, Zhong Ji, Xi Li, Jichang Guo, Zhongfei Zhang, Haibin Ling, Fei Wu
- **Comment**: None
- **Journal**: None
- **Summary**: As an important and challenging problem in computer vision, zero-shot learning (ZSL) aims at automatically recognizing the instances from unseen object classes without training data. To address this problem, ZSL is usually carried out in the following two aspects: 1) capturing the domain distribution connections between seen classes data and unseen classes data; and 2) modeling the semantic interactions between the image feature space and the label embedding space. Motivated by these observations, we propose a bidirectional mapping based semantic relationship modeling scheme that seeks for crossmodal knowledge transfer by simultaneously projecting the image features and label embeddings into a common latent space. Namely, we have a bidirectional connection relationship that takes place from the image feature space to the latent space as well as from the label embedding space to the latent space. To deal with the domain shift problem, we further present a transductive learning approach that formulates the class prediction problem in an iterative refining process, where the object classification capacity is progressively reinforced through bootstrapping-based model updating over highly reliable instances. Experimental results on three benchmark datasets (AwA, CUB and SUN) demonstrate the effectiveness of the proposed approach against the state-of-the-art approaches.



### Transductive Zero-Shot Learning with Adaptive Structural Embedding
- **Arxiv ID**: http://arxiv.org/abs/1703.08897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08897v1)
- **Published**: 2017-03-27 01:44:41+00:00
- **Updated**: 2017-03-27 01:44:41+00:00
- **Authors**: Yunlong Yu, Zhong Ji, Jichang Guo, Yanwei Pang
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) endows the computer vision system with the inferential capability to recognize instances of a new category that has never seen before. Two fundamental challenges in it are visual-semantic embedding and domain adaptation in cross-modality learning and unseen class prediction steps, respectively. To address both challenges, this paper presents two corresponding methods named Adaptive STructural Embedding (ASTE) and Self-PAsed Selective Strategy (SPASS), respectively. Specifically, ASTE formulates the visualsemantic interactions in a latent structural SVM framework to adaptively adjust the slack variables to embody the different reliableness among training instances. In this way, the reliable instances are imposed with small punishments, wheras the less reliable instances are imposed with more severe punishments. Thus, it ensures a more discriminative embedding. On the other hand, SPASS offers a framework to alleviate the domain shift problem in ZSL, which exploits the unseen data in an easy to hard fashion. Particularly, SPASS borrows the idea from selfpaced learning by iteratively selecting the unseen instances from reliable to less reliable to gradually adapt the knowledge from the seen domain to the unseen domain. Subsequently, by combining SPASS and ASTE, we present a self-paced Transductive ASTE (TASTE) method to progressively reinforce the classification capacity. Extensive experiments on three benchmark datasets (i.e., AwA, CUB, and aPY) demonstrate the superiorities of ASTE and TASTE. Furthermore, we also propose a fast training (FT) strategy to improve the efficiency of most of existing ZSL methods. The FT strategy is surprisingly simple and general enough, which can speed up the training time of most existing methods by 4~300 times while holding the previous performance.



### Exploiting Color Name Space for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1703.08912v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1703.08912v2)
- **Published**: 2017-03-27 03:08:58+00:00
- **Updated**: 2019-12-08 18:56:36+00:00
- **Authors**: Jing Lou, Huan Wang, Longtao Chen, Fenglei Xu, Qingyuan Xia, Wei Zhu, Mingwu Ren
- **Comment**: http://www.loujing.com/cns-sod/
- **Journal**: None
- **Summary**: In this paper, we will investigate the contribution of color names for the task of salient object detection. An input image is first converted to color name space, which is consisted of 11 probabilistic channels. By exploiting a surroundedness cue, we obtain a saliency map through a linear combination of a set of sequential attention maps. To overcome the limitation of only using the surroundedness cue, two global cues with respect to color names are invoked to guide the computation of a weighted saliency map. Finally, we integrate the above two saliency maps into a unified framework to generate the final result. In addition, an improved post-processing procedure is introduced to effectively suppress image backgrounds while uniformly highlight salient objects. Experimental results show that the proposed model produces more accurate saliency maps and performs well against twenty-one saliency models in terms of three evaluation metrics on three public data sets.



### A Visual Measure of Changes to Weighted Self-Organizing Map Patterns
- **Arxiv ID**: http://arxiv.org/abs/1703.08917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08917v1)
- **Published**: 2017-03-27 03:46:58+00:00
- **Updated**: 2017-03-27 03:46:58+00:00
- **Authors**: Younjin Chung, Joachim Gudmundsson, Masahiro Takatsuka
- **Comment**: 8 pages, 3 figures, conference, llncs style
- **Journal**: None
- **Summary**: Estimating output changes by input changes is the main task in causal analysis. In previous work, input and output Self-Organizing Maps (SOMs) were associated for causal analysis of multivariate and nonlinear data. Based on the association, a weight distribution of the output conditional on a given input was obtained over the output map space. Such a weighted SOM pattern of the output changes when the input changes. In order to analyze the change, it is important to measure the difference of the patterns. Many methods have been proposed for the dissimilarity measure of patterns. However, it remains a major challenge when attempting to measure how the patterns change. In this paper, we propose a visualization approach that simplifies the comparison of the difference in terms of the pattern property. Using this approach, the change can be analyzed by integrating colors and star glyph shapes representing the property dissimilarity. Ecological data is used to demonstrate the usefulness of our approach and the experimental results show that our approach provides the change information effectively.



### MIHash: Online Hashing with Mutual Information
- **Arxiv ID**: http://arxiv.org/abs/1703.08919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08919v2)
- **Published**: 2017-03-27 03:50:51+00:00
- **Updated**: 2017-07-29 23:09:59+00:00
- **Authors**: Fatih Cakir, Kun He, Sarah Adel Bargal, Stan Sclaroff
- **Comment**: International Conference on Computer Vision (ICCV), 2017
- **Journal**: None
- **Summary**: Learning-based hashing methods are widely used for nearest neighbor retrieval, and recently, online hashing methods have demonstrated good performance-complexity trade-offs by learning hash functions from streaming data. In this paper, we first address a key challenge for online hashing: the binary codes for indexed data must be recomputed to keep pace with updates to the hash functions. We propose an efficient quality measure for hash functions, based on an information-theoretic quantity, mutual information, and use it successfully as a criterion to eliminate unnecessary hash table updates. Next, we also show how to optimize the mutual information objective using stochastic gradient descent. We thus develop a novel hashing method, MIHash, that can be used in both online and batch settings. Experiments on image retrieval benchmarks (including a 2.5M image dataset) confirm the effectiveness of our formulation, both in reducing hash table recomputations and in learning high-quality hash functions.



### Scaling the Scattering Transform: Deep Hybrid Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.08961v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.08961v2)
- **Published**: 2017-03-27 07:49:43+00:00
- **Updated**: 2017-04-04 06:13:22+00:00
- **Authors**: Edouard Oyallon, Eugene Belilovsky, Sergey Zagoruyko
- **Comment**: None
- **Journal**: None
- **Summary**: We use the scattering network as a generic and fixed ini-tialization of the first layers of a supervised hybrid deep network. We show that early layers do not necessarily need to be learned, providing the best results to-date with pre-defined representations while being competitive with Deep CNNs. Using a shallow cascade of 1 x 1 convolutions, which encodes scattering coefficients that correspond to spatial windows of very small sizes, permits to obtain AlexNet accuracy on the imagenet ILSVRC2012. We demonstrate that this local encoding explicitly learns invariance w.r.t. rotations. Combining scattering networks with a modern ResNet, we achieve a single-crop top 5 error of 11.4% on imagenet ILSVRC2012, comparable to the Resnet-18 architecture, while utilizing only 10 layers. We also find that hybrid architectures can yield excellent performance in the small sample regime, exceeding their end-to-end counterparts, through their ability to incorporate geometrical priors. We demonstrate this on subsets of the CIFAR-10 dataset and on the STL-10 dataset.



### Mastering Sketching: Adversarial Augmentation for Structured Prediction
- **Arxiv ID**: http://arxiv.org/abs/1703.08966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08966v1)
- **Published**: 2017-03-27 08:23:47+00:00
- **Updated**: 2017-03-27 08:23:47+00:00
- **Authors**: Edgar Simo-Serra, Satoshi Iizuka, Hiroshi Ishikawa
- **Comment**: 12 pages, 14 figures
- **Journal**: None
- **Summary**: We present an integral framework for training sketch simplification networks that convert challenging rough sketches into clean line drawings. Our approach augments a simplification network with a discriminator network, training both networks jointly so that the discriminator network discerns whether a line drawing is a real training data or the output of the simplification network, which in turn tries to fool it. This approach has two major advantages. First, because the discriminator network learns the structure in line drawings, it encourages the output sketches of the simplification network to be more similar in appearance to the training sketches. Second, we can also train the simplification network with additional unsupervised data, using the discriminator network as a substitute teacher. Thus, by adding only rough sketches without simplified line drawings, or only line drawings without the original rough sketches, we can improve the quality of the sketch simplification. We show how our framework can be used to train models that significantly outperform the state of the art in the sketch simplification task, despite using the same architecture for inference. We additionally present an approach to optimize for a single image, which improves accuracy at the cost of additional computation time. Finally, we show that, using the same framework, it is possible to train the network to perform the inverse problem, i.e., convert simple line sketches into pencil drawings, which is not possible using the standard mean squared error loss. We validate our framework with two user tests, where our approach is preferred to the state of the art in sketch simplification 92.3% of the time and obtains 1.2 more points on a scale of 1 to 5.



### LIDAR-based Driving Path Generation Using Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.08987v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08987v2)
- **Published**: 2017-03-27 09:51:55+00:00
- **Updated**: 2017-04-03 13:00:02+00:00
- **Authors**: Luca Caltagirone, Mauro Bellone, Lennart Svensson, Mattias Wahde
- **Comment**: Changed title, formerly "Simultaneous Perception and Path Generation
  Using Fully Convolutional Neural Networks"
- **Journal**: None
- **Summary**: In this work, a novel learning-based approach has been developed to generate driving paths by integrating LIDAR point clouds, GPS-IMU information, and Google driving directions. The system is based on a fully convolutional neural network that jointly learns to carry out perception and path generation from real-world driving sequences and that is trained using automatically generated training examples. Several combinations of input data were tested in order to assess the performance gain provided by specific information modalities. The fully convolutional neural network trained using all the available sensors together with driving directions achieved the best MaxF score of 88.13% when considering a region of interest of 60x60 meters. By considering a smaller region of interest, the agreement between predicted paths and ground-truth increased to 92.60%. The positive results obtained in this work indicate that the proposed system may help fill the gap between low-level scene parsing and behavior-reflex approaches by generating outputs that are close to vehicle control and at the same time human-interpretable.



### Trespassing the Boundaries: Labeling Temporal Bounds for Object Interactions in Egocentric Video
- **Arxiv ID**: http://arxiv.org/abs/1703.09026v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09026v2)
- **Published**: 2017-03-27 12:14:07+00:00
- **Updated**: 2017-07-26 14:13:23+00:00
- **Authors**: Davide Moltisanti, Michael Wray, Walterio Mayol-Cuevas, Dima Damen
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: Manual annotations of temporal bounds for object interactions (i.e. start and end times) are typical training input to recognition, localization and detection algorithms. For three publicly available egocentric datasets, we uncover inconsistencies in ground truth temporal bounds within and across annotators and datasets. We systematically assess the robustness of state-of-the-art approaches to changes in labeled temporal bounds, for object interaction recognition. As boundaries are trespassed, a drop of up to 10% is observed for both Improved Dense Trajectories and Two-Stream Convolutional Neural Network.   We demonstrate that such disagreement stems from a limited understanding of the distinct phases of an action, and propose annotating based on the Rubicon Boundaries, inspired by a similarly named cognitive model, for consistent temporal bounds of object interactions. Evaluated on a public dataset, we report a 4% increase in overall accuracy, and an increase in accuracy for 55% of classes when Rubicon Boundaries are used for temporal annotations.



### Efficient Processing of Deep Neural Networks: A Tutorial and Survey
- **Arxiv ID**: http://arxiv.org/abs/1703.09039v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09039v2)
- **Published**: 2017-03-27 12:42:13+00:00
- **Updated**: 2017-08-13 14:47:12+00:00
- **Authors**: Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, Joel Emer
- **Comment**: Based on tutorial on DNN Hardware at eyeriss.mit.edu/tutorial.html
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems.   This article aims to provide a comprehensive tutorial and survey about the recent advances towards the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic co-designs, being proposed in academia and industry.   The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the trade-offs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.



### Active Convolution: Learning the Shape of Convolution for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1703.09076v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09076v1)
- **Published**: 2017-03-27 13:44:26+00:00
- **Updated**: 2017-03-27 13:44:26+00:00
- **Authors**: Yunho Jeon, Junmo Kim
- **Comment**: Accepted to appear in CVPR 2017
- **Journal**: None
- **Summary**: In recent years, deep learning has achieved great success in many computer vision applications. Convolutional neural networks (CNNs) have lately emerged as a major approach to image classification. Most research on CNNs thus far has focused on developing architectures such as the Inception and residual networks. The convolution layer is the core of the CNN, but few studies have addressed the convolution unit itself. In this paper, we introduce a convolution unit called the active convolution unit (ACU). A new convolution has no fixed shape, because of which we can define any form of convolution. Its shape can be learned through backpropagation during training. Our proposed unit has a few advantages. First, the ACU is a generalization of convolution; it can define not only all conventional convolutions, but also convolutions with fractional pixel coordinates. We can freely change the shape of the convolution, which provides greater freedom to form CNN structures. Second, the shape of the convolution is learned while training and there is no need to tune it by hand. Third, the ACU can learn better than a conventional unit, where we obtained the improvement simply by changing the conventional convolution to an ACU. We tested our proposed method on plain and residual networks, and the results showed significant improvement using our method on various datasets and architectures in comparison with the baseline.



### Where to put the Image in an Image Caption Generator
- **Arxiv ID**: http://arxiv.org/abs/1703.09137v2
- **DOI**: 10.1017/S1351324918000098
- **Categories**: **cs.NE**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.09137v2)
- **Published**: 2017-03-27 15:13:49+00:00
- **Updated**: 2018-03-14 08:56:53+00:00
- **Authors**: Marc Tanti, Albert Gatt, Kenneth P. Camilleri
- **Comment**: Accepted in JNLE Special Issue: Language for Images (24.3) (expanded
  with content that was removed from journal paper in order to reduce number of
  pages), 28 pages, 5 figures, 6 tables
- **Journal**: None
- **Summary**: When a recurrent neural network language model is used for caption generation, the image information can be fed to the neural network either by directly incorporating it in the RNN -- conditioning the language model by `injecting' image features -- or in a layer following the RNN -- conditioning the language model by `merging' image features. While both options are attested in the literature, there is as yet no systematic comparison between the two. In this paper we empirically show that it is not especially detrimental to performance whether one architecture is used or another. The merge architecture does have practical advantages, as conditioning by merging allows the RNN's hidden state vector to shrink in size by up to four times. Our results suggest that the visual and linguistic modalities for caption generation need not be jointly encoded by the RNN as that yields large, memory-intensive models with few tangible advantages in performance; rather, the multimodal integration should be delayed to a subsequent stage.



### Multi-Path Region-Based Convolutional Neural Network for Accurate Detection of Unconstrained "Hard Faces"
- **Arxiv ID**: http://arxiv.org/abs/1703.09145v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09145v1)
- **Published**: 2017-03-27 15:31:00+00:00
- **Updated**: 2017-03-27 15:31:00+00:00
- **Authors**: Yuguang Liu, Martin D. Levine
- **Comment**: 11 pages, 7 figures, to be presented at CRV 2017
- **Journal**: None
- **Summary**: Large-scale variations still pose a challenge in unconstrained face detection. To the best of our knowledge, no current face detection algorithm can detect a face as large as 800 x 800 pixels while simultaneously detecting another one as small as 8 x 8 pixels within a single image with equally high accuracy. We propose a two-stage cascaded face detection framework, Multi-Path Region-based Convolutional Neural Network (MP-RCNN), that seamlessly combines a deep neural network with a classic learning strategy, to tackle this challenge. The first stage is a Multi-Path Region Proposal Network (MP-RPN) that proposes faces at three different scales. It simultaneously utilizes three parallel outputs of the convolutional feature maps to predict multi-scale candidate face regions. The "atrous" convolution trick (convolution with up-sampled filters) and a newly proposed sampling layer for "hard" examples are embedded in MP-RPN to further boost its performance. The second stage is a Boosted Forests classifier, which utilizes deep facial features pooled from inside the candidate face regions as well as deep contextual features pooled from a larger region surrounding the candidate face regions. This step is included to further remove hard negative samples. Experiments show that this approach achieves state-of-the-art face detection performance on the WIDER FACE dataset "hard" partition, outperforming the former best result by 9.6% for the Average Precision.



### Reweighted Infrared Patch-Tensor Model With Both Non-Local and Local Priors for Single-Frame Small Target Detection
- **Arxiv ID**: http://arxiv.org/abs/1703.09157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09157v1)
- **Published**: 2017-03-27 15:57:27+00:00
- **Updated**: 2017-03-27 15:57:27+00:00
- **Authors**: Yimian Dai, Yiquan Wu
- **Comment**: Submitted to IEEE Journal of Selected Topics in Applied Earth
  Observations and Remote Sensing, 16 pages, 16 figures
- **Journal**: None
- **Summary**: Many state-of-the-art methods have been proposed for infrared small target detection. They work well on the images with homogeneous backgrounds and high-contrast targets. However, when facing highly heterogeneous backgrounds, they would not perform very well, mainly due to: 1) the existence of strong edges and other interfering components, 2) not utilizing the priors fully. Inspired by this, we propose a novel method to exploit both local and non-local priors simultaneously. Firstly, we employ a new infrared patch-tensor (IPT) model to represent the image and preserve its spatial correlations. Exploiting the target sparse prior and background non-local self-correlation prior, the target-background separation is modeled as a robust low-rank tensor recovery problem. Moreover, with the help of the structure tensor and reweighted idea, we design an entry-wise local-structure-adaptive and sparsity enhancing weight to replace the globally constant weighting parameter. The decomposition could be achieved via the element-wise reweighted higher-order robust principal component analysis with an additional convergence condition according to the practical situation of target detection. Extensive experiments demonstrate that our model outperforms the other state-of-the-arts, in particular for the images with very dim targets and heavy clutters.



### A Dynamic Programming Solution to Bounded Dejittering Problems
- **Arxiv ID**: http://arxiv.org/abs/1703.09161v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.09161v1)
- **Published**: 2017-03-27 16:09:20+00:00
- **Updated**: 2017-03-27 16:09:20+00:00
- **Authors**: Lukas F. Lang
- **Comment**: The final publication is available at link.springer.com
- **Journal**: None
- **Summary**: We propose a dynamic programming solution to image dejittering problems with bounded displacements and obtain efficient algorithms for the removal of line jitter, line pixel jitter, and pixel jitter.



### A Study on the Extraction and Analysis of a Large Set of Eye Movement Features during Reading
- **Arxiv ID**: http://arxiv.org/abs/1703.09167v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1703.09167v1)
- **Published**: 2017-03-27 16:21:26+00:00
- **Updated**: 2017-03-27 16:21:26+00:00
- **Authors**: Ioannis Rigas, Lee Friedman, Oleg Komogortsev
- **Comment**: 38 pages, 10 figures
- **Journal**: None
- **Summary**: This work presents a study on the extraction and analysis of a set of 101 categories of eye movement features from three types of eye movement events: fixations, saccades, and post-saccadic oscillations. The eye movements were recorded during a reading task. For the categories of features with multiple instances in a recording we extract corresponding feature subtypes by calculating descriptive statistics on the distributions of these instances. A unified framework of detailed descriptions and mathematical formulas are provided for the extraction of the feature set. The analysis of feature values is performed using a large database of eye movement recordings from a normative population of 298 subjects. We demonstrate the central tendency and overall variability of feature values over the experimental population, and more importantly, we quantify the test-retest reliability (repeatability) of each separate feature. The described methods and analysis can provide valuable tools in fields exploring the eye movements, such as in behavioral studies, attention and cognition research, medical research, biometric recognition, and human-computer interaction.



### Transfer learning for music classification and regression tasks
- **Arxiv ID**: http://arxiv.org/abs/1703.09179v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1703.09179v4)
- **Published**: 2017-03-27 16:48:03+00:00
- **Updated**: 2017-09-13 16:20:26+00:00
- **Authors**: Keunwoo Choi, György Fazekas, Mark Sandler, Kyunghyun Cho
- **Comment**: 18th International Society of Music Information Retrieval (ISMIR)
  Conference, Suzhou, China, 2017
- **Journal**: None
- **Summary**: In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pre-trained convnet feature, a concatenated feature vector using the activations of feature maps of multiple layers in a trained convolutional network. We show how this convnet feature can serve as general-purpose music representation. In the experiments, a convnet is trained for music tagging and then transferred to other music-related classification and regression tasks. The convnet feature outperforms the baseline MFCC feature in all the considered tasks and several previous approaches that are aggregating MFCCs as well as low- and high-level music features.



### Introduction To The Monogenic Signal
- **Arxiv ID**: http://arxiv.org/abs/1703.09199v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09199v1)
- **Published**: 2017-03-27 17:36:33+00:00
- **Updated**: 2017-03-27 17:36:33+00:00
- **Authors**: Christopher P. Bridge
- **Comment**: None
- **Journal**: None
- **Summary**: The monogenic signal is an image analysis methodology that was introduced by Felsberg and Sommer in 2001 and has been employed for a variety of purposes in image processing and computer vision research. In particular, it has been found to be useful in the analysis of ultrasound imagery in several research scenarios mostly in work done within the BioMedIA lab at Oxford. However, the literature on the monogenic signal can be difficult to penetrate due to the lack of a single resource to explain the various principles from basics. The purpose of this document is therefore to introduce the principles, purpose, applications, and limitations of the methodology. It assumes some background knowledge from the fields of image and signal processing, in particular a good knowledge of Fourier transforms as applied to signals and images. We will not attempt to provide a thorough math- ematical description or derivation of the monogenic signal, but rather focus on developing an intuition for understanding and using the methodology and refer the reader elsewhere for a more mathematical treatment.



### The Deep Poincaré Map: A Novel Approach for Left Ventricle Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1703.09200v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09200v2)
- **Published**: 2017-03-27 17:37:33+00:00
- **Updated**: 2018-10-30 11:10:09+00:00
- **Authors**: Yuanhan Mo, Fangde Liu, Douglas McIlwraith, Guang Yang, Jingqing Zhang, Taigang He, Yike Guo
- **Comment**: MICCAI 2018 Spotlight
- **Journal**: None
- **Summary**: Precise segmentation of the left ventricle (LV) within cardiac MRI images is a prerequisite for the quantitative measurement of heart function. However, this task is challenging due to the limited availability of labeled data and motion artifacts from cardiac imaging. In this work, we present an iterative segmentation algorithm for LV delineation. By coupling deep learning with a novel dynamic-based labeling scheme, we present a new methodology where a policy model is learned to guide an agent to travel over the the image, tracing out a boundary of the ROI -- using the magnitude difference of the Poincar\'e map as a stopping criterion. Our method is evaluated on two datasets, namely the Sunnybrook Cardiac Dataset (SCD) and data from the STACOM 2011 LV segmentation challenge. Our method outperforms the previous research over many metrics. In order to demonstrate the transferability of our method we present encouraging results over the STACOM 2011 data, when using a model trained on the SCD dataset.



### StyleBank: An Explicit Representation for Neural Image Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1703.09210v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09210v2)
- **Published**: 2017-03-27 17:52:18+00:00
- **Updated**: 2017-03-28 09:10:10+00:00
- **Authors**: Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, Gang Hua
- **Comment**: Accepted by CVPR 2017, corrected typos
- **Journal**: None
- **Summary**: We propose StyleBank, which is composed of multiple convolution filter banks and each filter bank explicitly represents one style, for neural image style transfer. To transfer an image to a specific style, the corresponding filter bank is operated on top of the intermediate feature embedding produced by a single auto-encoder. The StyleBank and the auto-encoder are jointly learnt, where the learning is conducted in such a way that the auto-encoder does not encode any style information thanks to the flexibility introduced by the explicit filter bank representation. It also enables us to conduct incremental learning to add a new image style by learning a new filter bank while holding the auto-encoder fixed. The explicit style representation along with the flexible network design enables us to fuse styles at not only the image level, but also the region level. Our method is the first style transfer network that links back to traditional texton mapping methods, and hence provides new understanding on neural style transfer. Our method is easy to train, runs in real-time, and produces results that qualitatively better or at least comparable to existing methods.



### Coherent Online Video Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1703.09211v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09211v2)
- **Published**: 2017-03-27 17:52:55+00:00
- **Updated**: 2017-03-28 09:04:15+00:00
- **Authors**: Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, Gang Hua
- **Comment**: Corrected typos
- **Journal**: None
- **Summary**: Training a feed-forward network for fast neural style transfer of images is proven to be successful. However, the naive extension to process video frame by frame is prone to producing flickering results. We propose the first end-to-end network for online video style transfer, which generates temporally coherent stylized video sequences in near real-time. Two key ideas include an efficient network by incorporating short-term coherence, and propagating short-term coherence to long-term, which ensures the consistency over larger period of time. Our network can incorporate different image stylization networks. We show that the proposed method clearly outperforms the per-frame baseline both qualitatively and quantitatively. Moreover, it can achieve visually comparable coherence to optimization-based video style transfer, but is three orders of magnitudes faster in runtime.



### Discriminative Transfer Learning for General Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1703.09245v1
- **DOI**: 10.1109/TIP.2018.2831925
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09245v1)
- **Published**: 2017-03-27 18:09:07+00:00
- **Updated**: 2017-03-27 18:09:07+00:00
- **Authors**: Lei Xiao, Felix Heide, Wolfgang Heidrich, Bernhard Schölkopf, Michael Hirsch
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, several discriminative learning approaches have been proposed for effective image restoration, achieving convincing trade-off between image quality and computational efficiency. However, these methods require separate training for each restoration task (e.g., denoising, deblurring, demosaicing) and problem condition (e.g., noise level of input images). This makes it time-consuming and difficult to encompass all tasks and conditions during training. In this paper, we propose a discriminative transfer learning method that incorporates formal proximal optimization and discriminative learning for general image restoration. The method requires a single-pass training and allows for reuse across various problems and conditions while achieving an efficiency comparable to previous discriminative approaches. Furthermore, after being trained, our model can be easily transferred to new likelihood terms to solve untrained tasks, or be combined with existing priors to further improve image restoration quality.



### Femoral ROIs and Entropy for Texture-based Detection of Osteoarthritis from High-Resolution Knee Radiographs
- **Arxiv ID**: http://arxiv.org/abs/1703.09296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09296v1)
- **Published**: 2017-03-27 20:20:50+00:00
- **Updated**: 2017-03-27 20:20:50+00:00
- **Authors**: Jiří Hladůvka, Bui Thi Mai Phuong, Richard Ljuhar, Davul Ljuhar, Ana M Rodrigues, Jaime C Branco, Helena Canhão
- **Comment**: None
- **Journal**: None
- **Summary**: The relationship between knee osteoarthritis progression and changes in tibial bone structure has long been recognized and various texture descriptors have been proposed to detect early osteoarthritis (OA) from radiographs. This work aims to investigate (1) femoral textures as an OA indicator and (2) the potential of entropy as a computationally efficient alternative to established texture descriptors.   We design a robust semi-automatically placed layout for regions of interest (ROI), compute the Hurst coefficient and the entropy in each ROI, and employ statistical and machine learning methods to evaluate feature combinations.   Based on 153 high-resolution radiographs, our results identify medial femur as an effective univariate descriptor, with significance comparable to medial tibia. Entropy is shown to contribute to classification performance. A linear five-feature classifier combining femur, entropic and standard texture descriptors, achieves AUC of 0.85, outperforming the state-of-the-art by roughly 0.1.



### Graph Regularized Tensor Sparse Coding for Image Representation
- **Arxiv ID**: http://arxiv.org/abs/1703.09342v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09342v1)
- **Published**: 2017-03-27 23:34:03+00:00
- **Updated**: 2017-03-27 23:34:03+00:00
- **Authors**: Fei Jiang, Xiao-Yang Liu, Hongtao Lu, Ruimin Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse coding (SC) is an unsupervised learning scheme that has received an increasing amount of interests in recent years. However, conventional SC vectorizes the input images, which destructs the intrinsic spatial structures of the images. In this paper, we propose a novel graph regularized tensor sparse coding (GTSC) for image representation. GTSC preserves the local proximity of elementary structures in the image by adopting the newly proposed tubal-tensor representation. Simultaneously, it considers the intrinsic geometric properties by imposing graph regularization that has been successfully applied to uncover the geometric distribution for the image data. Moreover, the returned sparse representations by GTSC have better physical explanations as the key operation (i.e., circular convolution) in the tubal-tensor model preserves the shifting invariance property. Experimental results on image clustering demonstrate the effectiveness of the proposed scheme.



