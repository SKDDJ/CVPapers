# Arxiv Papers in cs.CV on 2017-03-19
### Weakly-supervised DCNN for RGB-D Object Recognition in Real-World Applications Which Lack Large-scale Annotated Training Data
- **Arxiv ID**: http://arxiv.org/abs/1703.06370v1
- **DOI**: 10.1109/JSEN.2018.2888815
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06370v1)
- **Published**: 2017-03-19 00:29:35+00:00
- **Updated**: 2017-03-19 00:29:35+00:00
- **Authors**: Li Sun, Cheng Zhao, Rustam Stolkin
- **Comment**: 8 pages, 5 figures, submitted to conference
- **Journal**: None
- **Summary**: This paper addresses the problem of RGBD object recognition in real-world applications, where large amounts of annotated training data are typically unavailable. To overcome this problem, we propose a novel, weakly-supervised learning architecture (DCNN-GPC) which combines parametric models (a pair of Deep Convolutional Neural Networks (DCNN) for RGB and D modalities) with non-parametric models (Gaussian Process Classification). Our system is initially trained using a small amount of labeled data, and then automatically prop- agates labels to large-scale unlabeled data. We first run 3D- based objectness detection on RGBD videos to acquire many unlabeled object proposals, and then employ DCNN-GPC to label them. As a result, our multi-modal DCNN can be trained end-to-end using only a small amount of human annotation. Finally, our 3D-based objectness detection and multi-modal DCNN are integrated into a real-time detection and recognition pipeline. In our approach, bounding-box annotations are not required and boundary-aware detection is achieved. We also propose a novel way to pretrain a DCNN for the depth modality, by training on virtual depth images projected from CAD models. We pretrain our multi-modal DCNN on public 3D datasets, achieving performance comparable to state-of-the-art methods on Washington RGBS Dataset. We then finetune the network by further training on a small amount of annotated data from our novel dataset of industrial objects (nuclear waste simulants). Our weakly supervised approach has demonstrated to be highly effective in solving a novel RGBD object recognition application which lacks of human annotations.



### Recent Advances in Features Extraction and Description Algorithms: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/1703.06376v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1703.06376v1)
- **Published**: 2017-03-19 01:00:27+00:00
- **Updated**: 2017-03-19 01:00:27+00:00
- **Authors**: Ehab Salahat, Murad Qasaimeh
- **Comment**: Annual IEEE Industrial Electronics Societys 18th International Conf.
  on Industrial Technology (ICIT), 22-25 March, 2017
- **Journal**: None
- **Summary**: Computer vision is one of the most active research fields in information technology today. Giving machines and robots the ability to see and comprehend the surrounding world at the speed of sight creates endless potential applications and opportunities. Feature detection and description algorithms can be indeed considered as the retina of the eyes of such machines and robots. However, these algorithms are typically computationally intensive, which prevents them from achieving the speed of sight real-time performance. In addition, they differ in their capabilities and some may favor and work better given a specific type of input compared to others. As such, it is essential to compactly report their pros and cons as well as their performances and recent advances. This paper is dedicated to provide a comprehensive overview on the state-of-the-art and recent advances in feature detection and description algorithms. Specifically, it starts by overviewing fundamental concepts. It then compares, reports and discusses their performance and capabilities. The Maximally Stable Extremal Regions algorithm and the Scale Invariant Feature Transform algorithms, being two of the best of their type, are selected to report their recent algorithmic derivatives.



### Direct Monocular Odometry Using Points and Lines
- **Arxiv ID**: http://arxiv.org/abs/1703.06380v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1703.06380v1)
- **Published**: 2017-03-19 01:59:53+00:00
- **Updated**: 2017-03-19 01:59:53+00:00
- **Authors**: Shichao Yang, Sebastian Scherer
- **Comment**: ICRA 2017
- **Journal**: None
- **Summary**: Most visual odometry algorithm for a monocular camera focuses on points, either by feature matching, or direct alignment of pixel intensity, while ignoring a common but important geometry entity: edges. In this paper, we propose an odometry algorithm that combines points and edges to benefit from the advantages of both direct and feature based methods. It works better in texture-less environments and is also more robust to lighting changes and fast motion by increasing the convergence basin. We maintain a depth map for the keyframe then in the tracking part, the camera pose is recovered by minimizing both the photometric error and geometric error to the matched edge in a probabilistic framework. In the mapping part, edge is used to speed up and increase stereo matching accuracy. On various public datasets, our algorithm achieves better or comparable performance than state-of-the-art monocular odometry methods. In some challenging texture-less environments, our algorithm reduces the state estimation error over 50%.



### Zero-Shot Learning by Generating Pseudo Feature Representations
- **Arxiv ID**: http://arxiv.org/abs/1703.06389v1
- **DOI**: 10.1016/j.patcog.2018.03.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06389v1)
- **Published**: 2017-03-19 04:14:27+00:00
- **Updated**: 2017-03-19 04:14:27+00:00
- **Authors**: Jiang Lu, Jin Li, Ziang Yan, Changshui Zhang
- **Comment**: 9 pages
- **Journal**: Pattern Recognition, Volume 80, August 2018, Pages 129-142
- **Summary**: Zero-shot learning (ZSL) is a challenging task aiming at recognizing novel classes without any training instances. In this paper we present a simple but high-performance ZSL approach by generating pseudo feature representations (GPFR). Given the dataset of seen classes and side information of unseen classes (e.g. attributes), we synthesize feature-level pseudo representations for novel concepts, which allows us access to the formulation of unseen class predictor. Firstly we design a Joint Attribute Feature Extractor (JAFE) to acquire understandings about attributes, then construct a cognitive repository of attributes filtered by confidence margins, and finally generate pseudo feature representations using a probability based sampling strategy to facilitate subsequent training process of class predictor. We demonstrate the effectiveness in ZSL settings and the extensibility in supervised recognition scenario of our method on a synthetic colored MNIST dataset (C-MNIST). For several popular ZSL benchmark datasets, our approach also shows compelling results on zero-shot recognition task, especially leading to tremendous improvement to state-of-the-art mAP on zero-shot retrieval task.



### Multilevel Context Representation for Improving Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.06408v1
- **DOI**: 10.1109/ICDAR.2017.322
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06408v1)
- **Published**: 2017-03-19 09:52:28+00:00
- **Updated**: 2017-03-19 09:52:28+00:00
- **Authors**: Andreas KÃ¶lsch, Muhammad Zeshan Afzal, Marcus Liwicki
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose the combined usage of low- and high-level blocks of convolutional neural networks (CNNs) for improving object recognition. While recent research focused on either propagating the context from all layers, e.g. ResNet, (including the very low-level layers) or having multiple loss layers (e.g. GoogLeNet), the importance of the features close to the higher layers is ignored. This paper postulates that the use of context closer to the high-level layers provides the scale and translation invariance and works better than using the top layer only. In particular, we extend AlexNet and GoogLeNet by additional connections in the top $n$ layers. In order to demonstrate the effectiveness of the proposed approach, we evaluated it on the standard ImageNet task. The relative reduction of the classification error is around 1-2% without affecting the computational cost. Furthermore, we show that this approach is orthogonal to typical test data augmentation techniques, as recently introduced by Szegedy et al. (leading to a runtime reduction of 144 during test time).



### TAC-GAN - Text Conditioned Auxiliary Classifier Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1703.06412v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06412v2)
- **Published**: 2017-03-19 10:07:58+00:00
- **Updated**: 2017-03-26 11:29:21+00:00
- **Authors**: Ayushman Dash, John Cristian Borges Gamboa, Sheraz Ahmed, Marcus Liwicki, Muhammad Zeshan Afzal
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present the Text Conditioned Auxiliary Classifier Generative Adversarial Network, (TAC-GAN) a text to image Generative Adversarial Network (GAN) for synthesizing images from their text descriptions. Former approaches have tried to condition the generative process on the textual data; but allying it to the usage of class information, known to diversify the generated samples and improve their structural coherence, has not been explored. We trained the presented TAC-GAN model on the Oxford-102 dataset of flowers, and evaluated the discriminability of the generated images with Inception-Score, as well as their diversity using the Multi-Scale Structural Similarity Index (MS-SSIM). Our approach outperforms the state-of-the-art models, i.e., its inception score is 3.45, corresponding to a relative increase of 7.8% compared to the recently introduced StackGan. A comparison of the mean MS-SSIM scores of the training and generated samples per class shows that our approach is able to generate highly diverse images with an average MS-SSIM of 0.14 over all generated classes.



### A Fully-Automated Pipeline for Detection and Segmentation of Liver Lesions and Pathological Lymph Nodes
- **Arxiv ID**: http://arxiv.org/abs/1703.06418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06418v1)
- **Published**: 2017-03-19 10:43:28+00:00
- **Updated**: 2017-03-19 10:43:28+00:00
- **Authors**: Assaf Hoogi, John W. Lambert, Yefeng Zheng, Dorin Comaniciu, Daniel L. Rubin
- **Comment**: Workshop on Machine Learning in Healthcare, Neural Information
  Processing Systems (NIPS). Barcelona, Spain, 2016
- **Journal**: None
- **Summary**: We propose a fully-automated method for accurate and robust detection and segmentation of potentially cancerous lesions found in the liver and in lymph nodes. The process is performed in three steps, including organ detection, lesion detection and lesion segmentation. Our method applies machine learning techniques such as marginal space learning and convolutional neural networks, as well as active contour models. The method proves to be robust in its handling of extremely high lesion diversity. We tested our method on volumetric computed tomography (CT) images, including 42 volumes containing liver lesions and 86 volumes containing 595 pathological lymph nodes. Preliminary results under 10-fold cross validation show that for both the liver lesions and the lymph nodes, a total detection sensitivity of 0.53 and average Dice score of $0.71 \pm 0.15$ for segmentation were obtained.



### Algorithms for Semantic Segmentation of Multispectral Remote Sensing Imagery using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.06452v3
- **DOI**: 10.1016/j.isprsjprs.2018.04.014
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1703.06452v3)
- **Published**: 2017-03-19 15:21:32+00:00
- **Updated**: 2018-05-01 20:59:31+00:00
- **Authors**: Ronald Kemker, Carl Salvaggio, Christopher Kanan
- **Comment**: 45 pages
- **Journal**: Published in ISPRS Journal of Photogrammetry and Remote Sensing
  2018
- **Summary**: Deep convolutional neural networks (DCNNs) have been used to achieve state-of-the-art performance on many computer vision tasks (e.g., object recognition, object detection, semantic segmentation) thanks to a large repository of annotated image data. Large labeled datasets for other sensor modalities, e.g., multispectral imagery (MSI), are not available due to the large cost and manpower required. In this paper, we adapt state-of-the-art DCNN frameworks in computer vision for semantic segmentation for MSI imagery. To overcome label scarcity for MSI data, we substitute real MSI for generated synthetic MSI in order to initialize a DCNN framework. We evaluate our network initialization scheme on the new RIT-18 dataset that we present in this paper. This dataset contains very-high resolution MSI collected by an unmanned aircraft system. The models initialized with synthetic imagery were less prone to over-fitting and provide a state-of-the-art baseline for future work.



### VQABQ: Visual Question Answering by Basic Questions
- **Arxiv ID**: http://arxiv.org/abs/1703.06492v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1703.06492v2)
- **Published**: 2017-03-19 19:14:55+00:00
- **Updated**: 2017-08-28 22:40:19+00:00
- **Authors**: Jia-Hong Huang, Modar Alfadly, Bernard Ghanem
- **Comment**: Accepted by CVPR 2017 VQA Challenge Workshop. (Tables updated)
- **Journal**: None
- **Summary**: Taking an image and question as the input of our method, it can output the text-based answer of the query question about the given image, so called Visual Question Answering (VQA). There are two main modules in our algorithm. Given a natural language question about an image, the first module takes the question as input and then outputs the basic questions of the main given question. The second module takes the main question, image and these basic questions as input and then outputs the text-based answer of the main question. We formulate the basic questions generation problem as a LASSO optimization problem, and also propose a criterion about how to exploit these basic questions to help answer main question. Our method is evaluated on the challenging VQA dataset and yields state-of-the-art accuracy, 60.34% in open-ended task.



### Detecting Oriented Text in Natural Images by Linking Segments
- **Arxiv ID**: http://arxiv.org/abs/1703.06520v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06520v3)
- **Published**: 2017-03-19 21:43:41+00:00
- **Updated**: 2017-04-13 17:40:43+00:00
- **Authors**: Baoguang Shi, Xiang Bai, Serge Belongie
- **Comment**: To Appear in CVPR 2017
- **Journal**: None
- **Summary**: Most state-of-the-art text detection methods are specific to horizontal Latin text and are not fast enough for real-time applications. We introduce Segment Linking (SegLink), an oriented text detection method. The main idea is to decompose text into two locally detectable elements, namely segments and links. A segment is an oriented box covering a part of a word or text line; A link connects two adjacent segments, indicating that they belong to the same word or text line. Both elements are detected densely at multiple scales by an end-to-end trained, fully-convolutional neural network. Final detections are produced by combining segments connected by links. Compared with previous methods, SegLink improves along the dimensions of accuracy, speed, and ease of training. It achieves an f-measure of 75.0% on the standard ICDAR 2015 Incidental (Challenge 4) benchmark, outperforming the previous best by a large margin. It runs at over 20 FPS on 512x512 images. Moreover, without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese.



### Vision-based Real-Time Aerial Object Localization and Tracking for UAV Sensing System
- **Arxiv ID**: http://arxiv.org/abs/1703.06527v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06527v1)
- **Published**: 2017-03-19 22:19:20+00:00
- **Updated**: 2017-03-19 22:19:20+00:00
- **Authors**: Yuanwei Wu, Yao Sui, Guanghui Wang
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: The paper focuses on the problem of vision-based obstacle detection and tracking for unmanned aerial vehicle navigation. A real-time object localization and tracking strategy from monocular image sequences is developed by effectively integrating the object detection and tracking into a dynamic Kalman model. At the detection stage, the object of interest is automatically detected and localized from a saliency map computed via the image background connectivity cue at each frame; at the tracking stage, a Kalman filter is employed to provide a coarse prediction of the object state, which is further refined via a local detector incorporating the saliency map and the temporal information between two consecutive frames. Compared to existing methods, the proposed approach does not require any manual initialization for tracking, runs much faster than the state-of-the-art trackers of its kind, and achieves competitive tracking performance on a large number of image sequences. Extensive experiments demonstrate the effectiveness and superior performance of the proposed approach.



