# Arxiv Papers in cs.CV on 2017-03-13
### GUN: Gradual Upsampling Network for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1703.04244v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04244v2)
- **Published**: 2017-03-13 04:34:12+00:00
- **Updated**: 2018-07-04 01:49:33+00:00
- **Authors**: Yang Zhao, Guoqing Li, Wenjun Xie, Wei Jia, Hai Min, Xiaoping Liu
- **Comment**: 11 pages, 9 figures
- **Journal**: None
- **Summary**: In this paper, an efficient super-resolution (SR) method based on deep convolutional neural network (CNN) is proposed, namely Gradual Upsampling Network (GUN). Recent CNN based SR methods often preliminarily magnify the low resolution (LR) input to high resolution (HR) and then reconstruct the HR input, or directly reconstruct the LR input and then recover the HR result at the last layer. The proposed GUN utilizes a gradual process instead of these two commonly used frameworks. The GUN consists of an input layer, multiple upsampling and convolutional layers, and an output layer. By means of the gradual process, the proposed network can simplify the direct SR problem to multistep easier upsampling tasks with very small magnification factor in each step. Furthermore, a gradual training strategy is presented for the GUN. In the proposed training process, an initial network can be easily trained with edge-like samples, and then the weights are gradually tuned with more complex samples. The GUN can recover fine and vivid results, and is easy to be trained. The experimental results on several image sets demonstrate the effectiveness of the proposed network.



### Poisson multi-Bernoulli mixture filter: direct derivation and implementation
- **Arxiv ID**: http://arxiv.org/abs/1703.04264v4
- **DOI**: 10.1109/TAES.2018.2805153
- **Categories**: **cs.CV**, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1703.04264v4)
- **Published**: 2017-03-13 06:08:51+00:00
- **Updated**: 2018-09-13 14:35:36+00:00
- **Authors**: Ángel F. García-Fernández, Jason L. Williams, Karl Granström, Lennart Svensson
- **Comment**: None
- **Journal**: IEEE Transactions on Aerospace and Electronic Systems, vol. 54,
  no. 4, pp. 1883-1901, Aug. 2018
- **Summary**: We provide a derivation of the Poisson multi-Bernoulli mixture (PMBM) filter for multi-target tracking with the standard point target measurements without using probability generating functionals or functional derivatives. We also establish the connection with the \delta-generalised labelled multi-Bernoulli (\delta-GLMB) filter, showing that a \delta-GLMB density represents a multi-Bernoulli mixture with labelled targets so it can be seen as a special case of PMBM. In addition, we propose an implementation for linear/Gaussian dynamic and measurement models and how to efficiently obtain typical estimators in the literature from the PMBM. The PMBM filter is shown to outperform other filters in the literature in a challenging scenario.



### Automatic Skin Lesion Segmentation using Semi-supervised Learning Technique
- **Arxiv ID**: http://arxiv.org/abs/1703.04301v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04301v1)
- **Published**: 2017-03-13 09:30:34+00:00
- **Updated**: 2017-03-13 09:30:34+00:00
- **Authors**: S. M. Jaisakthi, Aravindan Chandrabose, P. Mirunalini
- **Comment**: 4 pages with 1 figure
- **Journal**: None
- **Summary**: Skin cancer is the most common of all cancers and each year million cases of skin cancer are treated. Treating and curing skin cancer is easy, if it is diagnosed and treated at an early stage. In this work we propose an automatic technique for skin lesion segmentation in dermoscopic images which helps in classifying the skin cancer types. The proposed method comprises of two major phases (1) preprocessing and (2) segmentation using semi-supervised learning algorithm. In the preprocessing phase noise are removed using filtering technique and in the segmentation phase skin lesions are segmented based on clustering technique. K-means clustering algorithm is used to cluster the preprocessed images and skin lesions are filtered from these clusters based on the color feature. Color of the skin lesions are learned from the training images using histograms calculations in RGB color space. The training images were downloaded from the ISIC 2017 challenge website and the experimental results were evaluated using validation and test sets.



### End-to-End Learning of Geometry and Context for Deep Stereo Regression
- **Arxiv ID**: http://arxiv.org/abs/1703.04309v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1703.04309v1)
- **Published**: 2017-03-13 10:00:52+00:00
- **Updated**: 2017-03-13 10:00:52+00:00
- **Authors**: Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, Adam Bry
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel deep learning architecture for regressing disparity from a rectified pair of stereo images. We leverage knowledge of the problem's geometry to form a cost volume using deep feature representations. We learn to incorporate contextual information using 3-D convolutions over this volume. Disparity values are regressed from the cost volume using a proposed differentiable soft argmin operation, which allows us to train our method end-to-end to sub-pixel accuracy without any additional post-processing or regularization. We evaluate our method on the Scene Flow and KITTI datasets and on KITTI we set a new state-of-the-art benchmark, while being significantly faster than competing approaches.



### A Pitfall of Unsupervised Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/1703.04332v4
- **DOI**: None
- **Categories**: **cs.CV**, I.2.6, I.5.2, I.7.5
- **Links**: [PDF](http://arxiv.org/pdf/1703.04332v4)
- **Published**: 2017-03-13 11:19:00+00:00
- **Updated**: 2017-12-17 20:22:25+00:00
- **Authors**: Michele Alberti, Mathias Seuret, Rolf Ingold, Marcus Liwicki
- **Comment**: Conference on Neural Information Processing Systems, Deep Learning:
  Bridging Theory and Practice, December 2017
- **Journal**: None
- **Summary**: The point of this paper is to question typical assumptions in deep learning and suggest alternatives. A particular contribution is to prove that even if a Stacked Convolutional Auto-Encoder is good at reconstructing pictures, it is not necessarily good at discriminating their classes. When using Auto-Encoders, intuitively one assumes that features which are good for reconstruction will also lead to high classification accuracy. Indeed, it became research practice and is a suggested strategy by introductory books. However, we prove that this is not always the case. We thoroughly investigate the quality of features produced by Stacked Convolutional Auto-Encoders when trained to reconstruct their input. In particular, we analyze the relation between the reconstruction and classification capabilities of the network, if we were to use the same features for both tasks. Experimental results suggest that in fact, there is no correlation between the reconstruction score and the quality of features for a classification task. This means, more formally, that the sub-dimension representation space learned from the Stacked Convolutional Auto-Encoder (while being trained for input reconstruction) is not necessarily better separable than the initial input space. Furthermore, we show that the reconstruction error is not a good metric to assess the quality of features, because it is biased by the decoder quality. We do not question the usefulness of pre-training, but we conclude that aiming for the lowest reconstruction error is not necessarily a good idea if afterwards one performs a classification task.



### A Localisation-Segmentation Approach for Multi-label Annotation of Lumbar Vertebrae using Deep Nets
- **Arxiv ID**: http://arxiv.org/abs/1703.04347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04347v1)
- **Published**: 2017-03-13 11:55:16+00:00
- **Updated**: 2017-03-13 11:55:16+00:00
- **Authors**: Anjany Sekuboyina, Alexander Valentinitsch, Jan S. Kirschke, Bjoern H. Menze
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-class segmentation of vertebrae is a non-trivial task mainly due to the high correlation in the appearance of adjacent vertebrae. Hence, such a task calls for the consideration of both global and local context. Based on this motivation, we propose a two-staged approach that, given a computed tomography dataset of the spine, segments the five lumbar vertebrae and simultaneously labels them. The first stage employs a multi-layered perceptron performing non-linear regression for locating the lumbar region using the global context. The second stage, comprised of a fully-convolutional deep network, exploits the local context in the localised lumbar region to segment and label the lumbar vertebrae in one go. Aided with practical data augmentation for training, our approach is highly generalisable, capable of successfully segmenting both healthy and abnormal vertebrae (fractured and scoliotic spines). We consistently achieve an average Dice coefficient of over 90 percent on a publicly available dataset of the xVertSeg segmentation challenge of MICCAI 2016. This is particularly noteworthy because the xVertSeg dataset is beset with severe deformities in the form of vertebral fractures and scoliosis.



### Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs
- **Arxiv ID**: http://arxiv.org/abs/1703.04363v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.04363v2)
- **Published**: 2017-03-13 12:49:20+00:00
- **Updated**: 2017-08-08 08:10:34+00:00
- **Authors**: Michael Gygli, Mohammad Norouzi, Anelia Angelova
- **Comment**: Published at ICML 2017
- **Journal**: None
- **Summary**: We approach structured output prediction by optimizing a deep value network (DVN) to precisely estimate the task loss on different output configurations for a given input. Once the model is trained, we perform inference by gradient descent on the continuous relaxations of the output variables to find outputs with promising scores from the value network. When applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar estimating the intersection over union between the input and ground truth masks. For multi-label classification, the DVN's objective is to correctly predict the F1 score for any potential label configuration. The DVN framework achieves the state-of-the-art results on multi-label prediction and image segmentation benchmarks.



### Deep Learning for Skin Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/1703.04364v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04364v1)
- **Published**: 2017-03-13 12:56:00+00:00
- **Updated**: 2017-03-13 12:56:00+00:00
- **Authors**: P. Mirunalini, Aravindan Chandrabose, Vignesh Gokul, S. M. Jaisakthi
- **Comment**: 3 pages with 3 figures
- **Journal**: None
- **Summary**: Melanoma, a malignant form of skin cancer is very threatening to life. Diagnosis of melanoma at an earlier stage is highly needed as it has a very high cure rate. Benign and malignant forms of skin cancer can be detected by analyzing the lesions present on the surface of the skin using dermoscopic images. In this work, an automated skin lesion detection system has been developed which learns the representation of the image using Google's pretrained CNN model known as Inception-v3 \cite{cnn}. After obtaining the representation vector for our input dermoscopic images we have trained two layer feed forward neural network to classify the images as malignant or benign. The system also classifies the images based on the cause of the cancer either due to melanocytic or non-melanocytic cells using a different neural network. These classification tasks are part of the challenge organized by International Skin Imaging Collaboration (ISIC) 2017. Our system learns to classify the images based on the model built using the training images given in the challenge and the experimental results were evaluated using validation and test sets. Our system has achieved an overall accuracy of 65.8\% for the validation set.



### Zero-Shot Learning -- The Good, the Bad and the Ugly
- **Arxiv ID**: http://arxiv.org/abs/1703.04394v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04394v2)
- **Published**: 2017-03-13 13:53:19+00:00
- **Updated**: 2020-09-23 14:55:12+00:00
- **Authors**: Yongqin Xian, Bernt Schiele, Zeynep Akata
- **Comment**: Accepted as a full paper in IEEE Computer Vision and Pattern
  Recognition (CVPR) 2017. We introduce Proposed Split Version 2.0 (Please
  download it from the project page)
- **Journal**: None
- **Summary**: Due to the importance of zero-shot learning, the number of proposed approaches has increased steadily recently. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss limitations of the current status of the area which can be taken as a basis for advancing it.



### Users prefer Guetzli JPEG over same-sized libjpeg
- **Arxiv ID**: http://arxiv.org/abs/1703.04416v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1703.04416v1)
- **Published**: 2017-03-13 14:33:47+00:00
- **Updated**: 2017-03-13 14:33:47+00:00
- **Authors**: Jyrki Alakuijala, Robert Obryk, Zoltan Szabadka, Jan Wassenberg
- **Comment**: None
- **Journal**: None
- **Summary**: We report on pairwise comparisons by human raters of JPEG images from libjpeg and our new Guetzli encoder. Although both files are size-matched, 75% of ratings are in favor of Guetzli. This implies the Butteraugli psychovisual image similarity metric which guides Guetzli is reasonably close to human perception at high quality levels. We provide access to the raw ratings and source images for further analysis and study.



### Improving LBP and its variants using anisotropic diffusion
- **Arxiv ID**: http://arxiv.org/abs/1703.04418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04418v1)
- **Published**: 2017-03-13 14:36:21+00:00
- **Updated**: 2017-03-13 14:36:21+00:00
- **Authors**: Mariane B. Neiva, Patrick Guidotti, Odemir M. Bruno
- **Comment**: 14 pages, 10 figures
- **Journal**: None
- **Summary**: The main purpose of this paper is to propose a new preprocessing step in order to improve local feature descriptors and texture classification. Preprocessing is implemented by using transformations which help highlight salient features that play a significant role in texture recognition. We evaluate and compare four different competing methods: three different anisotropic diffusion methods including the classical anisotropic Perona-Malik diffusion and two subsequent regularizations of it and the application of a Gaussian kernel, which is the classical multiscale approach in texture analysis. The combination of the transformed images and the original ones are analyzed. The results show that the use of the preprocessing step does lead to improved texture recognition.



### Guetzli: Perceptually Guided JPEG Encoder
- **Arxiv ID**: http://arxiv.org/abs/1703.04421v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1703.04421v1)
- **Published**: 2017-03-13 14:40:08+00:00
- **Updated**: 2017-03-13 14:40:08+00:00
- **Authors**: Jyrki Alakuijala, Robert Obryk, Ostap Stoliarchuk, Zoltan Szabadka, Lode Vandevenne, Jan Wassenberg
- **Comment**: None
- **Journal**: None
- **Summary**: Guetzli is a new JPEG encoder that aims to produce visually indistinguishable images at a lower bit-rate than other common JPEG encoders. It optimizes both the JPEG global quantization tables and the DCT coefficient values in each JPEG block using a closed-loop optimizer. Guetzli uses Butteraugli, our perceptual distance metric, as the source of feedback in its optimization process. We reach a 29-45% reduction in data size for a given perceptual distance, according to Butteraugli, in comparison to other compressors we tried. Guetzli's computation is currently extremely slow, which limits its applicability to compressing static content and serving as a proof- of-concept that we can achieve significant reductions in size by combining advanced psychovisual models with lossy compression techniques.



### A Lagrangian Gauss-Newton-Krylov Solver for Mass- and Intensity-Preserving Diffeomorphic Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1703.04446v2
- **DOI**: 10.1137/17M1114132
- **Categories**: **cs.CV**, cs.CE, 68U10, 49J20, 35Q93, 65M32, 76D55, 65K10
- **Links**: [PDF](http://arxiv.org/pdf/1703.04446v2)
- **Published**: 2017-03-13 15:29:32+00:00
- **Updated**: 2017-07-12 11:04:29+00:00
- **Authors**: Andreas Mang, Lars Ruthotto
- **Comment**: code available at:
  https://github.com/C4IR/FAIR.m/tree/master/add-ons/LagLDDMM
- **Journal**: SIAM J. Sci. Comput., 39(5), B860-B885, 2017
- **Summary**: We present an efficient solver for diffeomorphic image registration problems in the framework of Large Deformations Diffeomorphic Metric Mappings (LDDMM). We use an optimal control formulation, in which the velocity field of a hyperbolic PDE needs to be found such that the distance between the final state of the system (the transformed/transported template image) and the observation (the reference image) is minimized. Our solver supports both stationary and non-stationary (i.e., transient or time-dependent) velocity fields. As transformation models, we consider both the transport equation (assuming intensities are preserved during the deformation) and the continuity equation (assuming mass-preservation).   We consider the reduced form of the optimal control problem and solve the resulting unconstrained optimization problem using a discretize-then-optimize approach. A key contribution is the elimination of the PDE constraint using a Lagrangian hyperbolic PDE solver. Lagrangian methods rely on the concept of characteristic curves that we approximate here using a fourth-order Runge-Kutta method. We also present an efficient algorithm for computing the derivatives of final state of the system with respect to the velocity field. This allows us to use fast Gauss-Newton based methods. We present quickly converging iterative linear solvers using spectral preconditioners that render the overall optimization efficient and scalable. Our method is embedded into the image registration framework FAIR and, thus, supports the most commonly used similarity measures and regularization functionals. We demonstrate the potential of our new approach using several synthetic and real world test problems with up to 14.7 million degrees of freedom.



### Detailed, accurate, human shape estimation from clothed 3D scan sequences
- **Arxiv ID**: http://arxiv.org/abs/1703.04454v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04454v2)
- **Published**: 2017-03-13 15:41:36+00:00
- **Updated**: 2017-04-19 12:26:27+00:00
- **Authors**: Chao Zhang, Sergi Pujades, Michael Black, Gerard Pons-Moll
- **Comment**: CVPR 2017, camera ready
- **Journal**: None
- **Summary**: We address the problem of estimating human pose and body shape from 3D scans over time. Reliable estimation of 3D body shape is necessary for many applications including virtual try-on, health monitoring, and avatar creation for virtual reality. Scanning bodies in minimal clothing, however, presents a practical barrier to these applications. We address this problem by estimating body shape under clothing from a sequence of 3D scans. Previous methods that have exploited body models produce smooth shapes lacking personalized details. We contribute a new approach to recover a personalized shape of the person. The estimated shape deviates from a parametric model to fit the 3D scans. We demonstrate the method using high quality 4D data as well as sequences of visual hulls extracted from multi-view images. We also make available BUFF, a new 4D dataset that enables quantitative evaluation (http://buff.is.tue.mpg.de). Our method outperforms the state of the art in both pose estimation and shape estimation, qualitatively and quantitatively.



