# Arxiv Papers in cs.CV on 2017-03-01
### Remote Sensing Image Scene Classification: Benchmark and State of the Art
- **Arxiv ID**: http://arxiv.org/abs/1703.00121v1
- **DOI**: 10.1109/JPROC.2017.2675998
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00121v1)
- **Published**: 2017-03-01 03:38:13+00:00
- **Updated**: 2017-03-01 03:38:13+00:00
- **Authors**: Gong Cheng, Junwei Han, Xiaoqiang Lu
- **Comment**: This manuscript is the accepted version for Proceedings of the IEEE
- **Journal**: Proceedings of the IEEE, 105 (10): 1865-1883, 2017
- **Summary**: Remote sensing image scene classification plays an important role in a wide range of applications and hence has been receiving remarkable attention. During the past years, significant efforts have been made to develop various datasets or present a variety of approaches for scene classification from remote sensing images. However, a systematic review of the literature concerning datasets and methods for scene classification is still lacking. In addition, almost all existing datasets have a number of limitations, including the small scale of scene classes and the image numbers, the lack of image variations and diversity, and the saturation of accuracy. These limitations severely limit the development of new approaches especially deep learning-based methods. This paper first provides a comprehensive review of the recent progress. Then, we propose a large-scale dataset, termed "NWPU-RESISC45", which is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This dataset contains 31,500 images, covering 45 scene classes with 700 images in each class. The proposed NWPU-RESISC45 (i) is large-scale on the scene classes and the total image number, (ii) holds big variations in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion, and (iii) has high within-class diversity and between-class similarity. The creation of this dataset will enable the community to develop and evaluate various data-driven algorithms. Finally, several representative methods are evaluated using the proposed dataset and the results are reported as a useful baseline for future research.



### RGB-D Salient Object Detection Based on Discriminative Cross-modal Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.00122v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00122v2)
- **Published**: 2017-03-01 03:38:53+00:00
- **Updated**: 2017-05-31 03:51:50+00:00
- **Authors**: Hao Chen, Y. F. Li, Dan Su
- **Comment**: This paper has been rejected by CVPR2017, we plan to withdraw this
  manuscript for further revision
- **Journal**: None
- **Summary**: In this work, we propose to utilize Convolutional Neural Networks to boost the performance of depth-induced salient object detection by capturing the high-level representative features for depth modality. We formulate the depth-induced saliency detection as a CNN-based cross-modal transfer problem to bridge the gap between the "data-hungry" nature of CNNs and the unavailability of sufficient labeled training data in depth modality. In the proposed approach, we leverage the auxiliary data from the source modality effectively by training the RGB saliency detection network to obtain the task-specific pre-understanding layers for the target modality. Meanwhile, we exploit the depth-specific information by pre-training a modality classification network that encourages modal-specific representations during the optimizing course. Thus, it could make the feature representations of the RGB and depth modalities as discriminative as possible. These two modules are pre-trained independently and then stitched to initialize and optimize the eventual depth-induced saliency detection model. Experiments demonstrate the effectiveness of the proposed novel pre-training strategy as well as the significant and consistent improvements of the proposed approach over other state-of-the-art methods.



### Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank
- **Arxiv ID**: http://arxiv.org/abs/1703.00144v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.00144v4)
- **Published**: 2017-03-01 05:38:16+00:00
- **Updated**: 2017-09-22 01:53:39+00:00
- **Authors**: Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, Victor Pan, Bo Yuan
- **Comment**: 13 pages, 1 figure
- **Journal**: None
- **Summary**: Recently low displacement rank (LDR) matrices, or so-called structured matrices, have been proposed to compress large-scale neural networks. Empirical results have shown that neural networks with weight matrices of LDR matrices, referred as LDR neural networks, can achieve significant reduction in space and computational complexity while retaining high accuracy. We formally study LDR matrices in deep learning. First, we prove the universal approximation property of LDR neural networks with a mild condition on the displacement operators. We then show that the error bounds of LDR neural networks are as efficient as general neural networks with both single-layer and multiple-layer structure. Finally, we propose back-propagation based training algorithm for general LDR neural networks.



### Saliency Detection by Forward and Backward Cues in Deep-CNNs
- **Arxiv ID**: http://arxiv.org/abs/1703.00152v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00152v2)
- **Published**: 2017-03-01 06:56:37+00:00
- **Updated**: 2017-06-21 09:04:55+00:00
- **Authors**: Nevrez Imamoglu, Chi Zhang, Wataru Shimoda, Yuming Fang, Boxin Shi
- **Comment**: 5 pages,4 figures,and 1 table. the content of this work is accepted
  for ICIP 2017
- **Journal**: None
- **Summary**: As prior knowledge of objects or object features helps us make relations for similar objects on attentional tasks, pre-trained deep convolutional neural networks (CNNs) can be used to detect salient objects on images regardless of the object class is in the network knowledge or not. In this paper, we propose a top-down saliency model using CNN, a weakly supervised CNN model trained for 1000 object labelling task from RGB images. The model detects attentive regions based on their objectness scores predicted by selected features from CNNs. To estimate the salient objects effectively, we combine both forward and backward features, while demonstrating that partially-guided backpropagation will provide sufficient information for selecting the features from forward run of CNN model. Finally, these top-down cues are enhanced with a state-of-the-art bottom-up model as complementing the overall saliency. As the proposed model is an effective integration of forward and backward cues through objectness without any supervision or regression to ground truth data, it gives promising results compared to state-of-the-art models in two different datasets.



### Inertial Odometry on Handheld Smartphones
- **Arxiv ID**: http://arxiv.org/abs/1703.00154v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1703.00154v2)
- **Published**: 2017-03-01 07:00:01+00:00
- **Updated**: 2018-06-07 20:40:20+00:00
- **Authors**: Arno Solin, Santiago Cortes, Esa Rahtu, Juho Kannala
- **Comment**: Appearing in Proceedings of the International Conference on
  Information Fusion (FUSION 2018)
- **Journal**: None
- **Summary**: Building a complete inertial navigation system using the limited quality data provided by current smartphones has been regarded challenging, if not impossible. This paper shows that by careful crafting and accounting for the weak information in the sensor samples, smartphones are capable of pure inertial navigation. We present a probabilistic approach for orientation and use-case free inertial odometry, which is based on double-integrating rotated accelerations. The strength of the model is in learning additive and multiplicative IMU biases online. We are able to track the phone position, velocity, and pose in real-time and in a computationally lightweight fashion by solving the inference with an extended Kalman filter. The information fusion is completed with zero-velocity updates (if the phone remains stationary), altitude correction from barometric pressure readings (if available), and pseudo-updates constraining the momentary speed. We demonstrate our approach using an iPad and iPhone in several indoor dead-reckoning applications and in a measurement tool setup.



### Saliency Fusion in Eigenvector Space with Multi-Channel Pulse Coupled Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1703.00160v1
- **DOI**: 10.1007/978-3-642-41043-7_1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00160v1)
- **Published**: 2017-03-01 07:25:41+00:00
- **Updated**: 2017-03-01 07:25:41+00:00
- **Authors**: Nevrez Imamoglu, Zhixuan Wei, Huangjun Shi, Yuki Yoshida, Myagmarbayar Nergui, Jose Gonzalez, Dongyun Gu, Weidong Chen, Kenzo Nonami, Wenwei Yu
- **Comment**: 8 pages, 9 figures, 1 table. This submission includes detailed
  explanation of partial section (saliency detection) of the work "An Improved
  Saliency for RGB-D Visual Tracking and Control Strategies for a
  Bio-monitoring Mobile Robot", Evaluating AAL Systems Through Competitive
  Benchmarking, Communications in Computer and Information Science, vol. 386,
  pp.1-12, 2013
- **Journal**: Evaluating AAL Systems Through Competitive Benchmarking,
  Communications in Computer and Information Science, 2013
- **Summary**: Saliency computation has become a popular research field for many applications due to the useful information provided by saliency maps. For a saliency map, local relations around the salient regions in multi-channel perspective should be taken into consideration by aiming uniformity on the region of interest as an internal approach. And, irrelevant salient regions have to be avoided as much as possible. Most of the works achieve these criteria with external processing modules; however, these can be accomplished during the conspicuity map fusion process. Therefore, in this paper, a new model is proposed for saliency/conspicuity map fusion with two concepts: a) input image transformation relying on the principal component analysis (PCA), and b) saliency conspicuity map fusion with multi-channel pulsed coupled neural network (m-PCNN). Experimental results, which are evaluated by precision, recall, F-measure, and area under curve (AUC), support the reliability of the proposed method by enhancing the saliency computation.



### Optical Flow-based 3D Human Motion Estimation from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/1703.00177v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00177v2)
- **Published**: 2017-03-01 08:29:09+00:00
- **Updated**: 2017-03-21 12:24:53+00:00
- **Authors**: Thiemo Alldieck, Marc Kassubeck, Marcus Magnor
- **Comment**: None
- **Journal**: None
- **Summary**: We present a generative method to estimate 3D human motion and body shape from monocular video. Under the assumption that starting from an initial pose optical flow constrains subsequent human motion, we exploit flow to find temporally coherent human poses of a motion sequence. We estimate human motion by minimizing the difference between computed flow fields and the output of an artificial flow renderer. A single initialization step is required to estimate motion over multiple frames. Several regularization functions enhance robustness over time. Our test scenarios demonstrate that optical flow effectively regularizes the under-constrained problem of human shape and motion estimation from monocular video.



### Incorporating Intra-Class Variance to Fine-Grained Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.00196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00196v1)
- **Published**: 2017-03-01 09:41:02+00:00
- **Updated**: 2017-03-01 09:41:02+00:00
- **Authors**: Yan Bai, Feng Gao, Yihang Lou, Shiqi Wang, Tiejun Huang, Ling-Yu Duan
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Fine-grained visual recognition aims to capture discriminative characteristics amongst visually similar categories. The state-of-the-art research work has significantly improved the fine-grained recognition performance by deep metric learning using triplet network. However, the impact of intra-category variance on the performance of recognition and robust feature representation has not been well studied. In this paper, we propose to leverage intra-class variance in metric learning of triplet network to improve the performance of fine-grained recognition. Through partitioning training images within each category into a few groups, we form the triplet samples across different categories as well as different groups, which is called Group Sensitive TRiplet Sampling (GS-TRS). Accordingly, the triplet loss function is strengthened by incorporating intra-class variance with GS-TRS, which may contribute to the optimization objective of triplet network. Extensive experiments over benchmark datasets CompCar and VehicleID show that the proposed GS-TRS has significantly outperformed state-of-the-art approaches in both classification and retrieval tasks.



### Improving Object Detection with Region Similarity Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.00234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00234v1)
- **Published**: 2017-03-01 11:16:13+00:00
- **Updated**: 2017-03-01 11:16:13+00:00
- **Authors**: Feng Gao, Yihang Lou, Yan Bai, Shiqi Wang, Tiejun Huang, Ling-Yu Duan
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Object detection aims to identify instances of semantic objects of a certain class in images or videos. The success of state-of-the-art approaches is attributed to the significant progress of object proposal and convolutional neural networks (CNNs). Most promising detectors involve multi-task learning with an optimization objective of softmax loss and regression loss. The first is for multi-class categorization, while the latter is for improving localization accuracy. However, few of them attempt to further investigate the hardness of distinguishing different sorts of distracting background regions (i.e., negatives) from true object regions (i.e., positives). To improve the performance of classifying positive object regions vs. a variety of negative background regions, we propose to incorporate triplet embedding into learning objective. The triplet units are formed by assigning each negative region to a meaningful object class and establishing class- specific negatives, followed by triplets construction. Over the benchmark PASCAL VOC 2007, the proposed triplet em- bedding has improved the performance of well-known FastRCNN model with a mAP gain of 2.1%. In particular, the state-of-the-art approach OHEM can benefit from the triplet embedding and has achieved a mAP improvement of 1.2%.



### Human Eye Visual Hyperacuity: A New Paradigm for Sensing?
- **Arxiv ID**: http://arxiv.org/abs/1703.00249v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.3; I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/1703.00249v1)
- **Published**: 2017-03-01 11:48:32+00:00
- **Updated**: 2017-03-01 11:48:32+00:00
- **Authors**: Adur Lagunas, Oier Dominguez, Susana Martinez-Conde, Stephen L. Macknik, Carlos del-Rio
- **Comment**: 8 pages,9 figures
- **Journal**: None
- **Summary**: The human eye appears to be using a low number of sensors for image capturing. Furthermore, regarding the physical dimensions of cones-photoreceptors responsible for the sharp central vision-, we may realize that these sensors are of a relatively small size and area. Nonetheless, the eye is capable to obtain high resolution images due to visual hyperacuity and presents an impressive sensitivity and dynamic range when set against conventional digital cameras of similar characteristics. This article is based on the hypothesis that the human eye may be benefiting from diffraction to improve both image resolution and acquisition process. The developed method intends to explain and simulate using MATLAB software the visual hyperacuity: the introduction of a controlled diffraction pattern at an initial stage, enables the use of a reduced number of sensors for capturing the image and makes possible a subsequent processing to improve the final image resolution. The results have been compared with the outcome of an equivalent system but in absence of diffraction, achieving promising results. The main conclusion of this work is that diffraction could be helpful for capturing images or signals when a small number of sensors available, which is far from being a resolution-limiting factor.



### Group Sparsity Residual Constraint for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1703.00297v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00297v6)
- **Published**: 2017-03-01 13:52:40+00:00
- **Updated**: 2017-07-31 16:42:43+00:00
- **Authors**: Zhiyuan Zha, Xinggan Zhang, Qiong Wang, Lan Tang, Xin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Group-based sparse representation has shown great potential in image denoising. However, most existing methods only consider the nonlocal self-similarity (NSS) prior of noisy input image. That is, the similar patches are collected only from degraded input, which makes the quality of image denoising largely depend on the input itself. However, such methods often suffer from a common drawback that the denoising performance may degrade quickly with increasing noise levels. In this paper we propose a new prior model, called group sparsity residual constraint (GSRC). Unlike the conventional group-based sparse representation denoising methods, two kinds of prior, namely, the NSS priors of noisy and pre-filtered images, are used in GSRC. In particular, we integrate these two NSS priors through the mechanism of sparsity residual, and thus, the task of image denoising is converted to the problem of reducing the group sparsity residual. To this end, we first obtain a good estimation of the group sparse coefficients of the original image by pre-filtering, and then the group sparse coefficients of the noisy image are used to approximate this estimation. To improve the accuracy of the nonlocal similar patch selection, an adaptive patch search scheme is designed. Furthermore, to fuse these two NSS prior better, an effective iterative shrinkage algorithm is developed to solve the proposed GSRC model. Experimental results demonstrate that the proposed GSRC modeling outperforms many state-of-the-art denoising methods in terms of the objective and the perceptual metrics.



### Multi-stage Neural Networks with Single-sided Classifiers for False Positive Reduction and its Evaluation using Lung X-ray CT Images
- **Arxiv ID**: http://arxiv.org/abs/1703.00311v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00311v3)
- **Published**: 2017-03-01 14:24:42+00:00
- **Updated**: 2017-03-21 08:20:05+00:00
- **Authors**: Masaharu Sakamoto, Hiroki Nakano, Kun Zhao, Taro Sekiyama
- **Comment**: arXiv admin note: text overlap with arXiv:1611.07136
- **Journal**: None
- **Summary**: Lung nodule classification is a class imbalanced problem because nodules are found with much lower frequency than non-nodules. In the class imbalanced problem, conventional classifiers tend to be overwhelmed by the majority class and ignore the minority class. We therefore propose cascaded convolutional neural networks to cope with the class imbalanced problem. In the proposed approach, multi-stage convolutional neural networks that perform as single-sided classifiers filter out obvious non-nodules. Successively, a convolutional neural network trained with a balanced data set calculates nodule probabilities. The proposed method achieved the sensitivity of 92.4\% and 94.5% at 4 and 8 false positives per scan in Free Receiver Operating Characteristics (FROC) curve analysis, respectively.



### Perturb-and-MPM: Quantifying Segmentation Uncertainty in Dense Multi-Label CRFs
- **Arxiv ID**: http://arxiv.org/abs/1703.00312v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00312v2)
- **Published**: 2017-03-01 14:26:29+00:00
- **Updated**: 2017-03-02 09:04:33+00:00
- **Authors**: Raphael Meier, Urspeter Knecht, Alain Jungo, Roland Wiest, Mauricio Reyes
- **Comment**: Deactivated review mode (line spacing)
- **Journal**: None
- **Summary**: This paper proposes a novel approach for uncertainty quantification in dense Conditional Random Fields (CRFs). The presented approach, called Perturb-and-MPM, enables efficient, approximate sampling from dense multi-label CRFs via random perturbations. An analytic error analysis was performed which identified the main cause of approximation error as well as showed that the error is bounded. Spatial uncertainty maps can be derived from the Perturb-and-MPM model, which can be used to visualize uncertainty in image segmentation results. The method is validated on synthetic and clinical Magnetic Resonance Imaging data. The effectiveness of the approach is demonstrated on the challenging problem of segmenting the tumor core in glioblastoma. We found that areas of high uncertainty correspond well to wrongly segmented image regions. Furthermore, we demonstrate the potential use of uncertainty maps to refine imaging biomarkers in the case of extent of resection and residual tumor volume in brain tumor patients.



### Graph-based Isometry Invariant Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.00356v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.00356v1)
- **Published**: 2017-03-01 15:51:13+00:00
- **Updated**: 2017-03-01 15:51:13+00:00
- **Authors**: Renata Khasanova, Pascal Frossard
- **Comment**: None
- **Journal**: None
- **Summary**: Learning transformation invariant representations of visual data is an important problem in computer vision. Deep convolutional networks have demonstrated remarkable results for image and video classification tasks. However, they have achieved only limited success in the classification of images that undergo geometric transformations. In this work we present a novel Transformation Invariant Graph-based Network (TIGraNet), which learns graph-based features that are inherently invariant to isometric transformations such as rotation and translation of input images. In particular, images are represented as signals on graphs, which permits to replace classical convolution and pooling layers in deep networks with graph spectral convolution and dynamic graph pooling layers that together contribute to invariance to isometric transformations. Our experiments show high performance on rotated and translated images from the test set compared to classical architectures that are very sensitive to transformations in the data. The inherent invariance properties of our framework provide key advantages, such as increased resiliency to data variability and sustained performance with limited training sets.



### Lossy Image Compression with Compressive Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1703.00395v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.00395v1)
- **Published**: 2017-03-01 17:13:47+00:00
- **Updated**: 2017-03-01 17:13:47+00:00
- **Authors**: Lucas Theis, Wenzhe Shi, Andrew Cunningham, Ferenc Huszár
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.



### Augmented Reality for Depth Cues in Monocular Minimally Invasive Surgery
- **Arxiv ID**: http://arxiv.org/abs/1703.01243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.01243v1)
- **Published**: 2017-03-01 18:01:52+00:00
- **Updated**: 2017-03-01 18:01:52+00:00
- **Authors**: Long Chen, Wen Tang, Nigel W. John, Tao Ruan Wan, Jian Jun Zhang
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: One of the major challenges in Minimally Invasive Surgery (MIS) such as laparoscopy is the lack of depth perception. In recent years, laparoscopic scene tracking and surface reconstruction has been a focus of investigation to provide rich additional information to aid the surgical process and compensate for the depth perception issue. However, robust 3D surface reconstruction and augmented reality with depth perception on the reconstructed scene are yet to be reported. This paper presents our work in this area. First, we adopt a state-of-the-art visual simultaneous localization and mapping (SLAM) framework - ORB-SLAM - and extend the algorithm for use in MIS scenes for reliable endoscopic camera tracking and salient point mapping. We then develop a robust global 3D surface reconstruction frame- work based on the sparse point clouds extracted from the SLAM framework. Our approach is to combine an outlier removal filter within a Moving Least Squares smoothing algorithm and then employ Poisson surface reconstruction to obtain smooth surfaces from the unstructured sparse point cloud. Our proposed method has been quantitatively evaluated compared with ground-truth camera trajectories and the organ model surface we used to render the synthetic simulation videos. In vivo laparoscopic videos used in the tests have demonstrated the robustness and accuracy of our proposed framework on both camera tracking and surface reconstruction, illustrating the potential of our algorithm for depth augmentation and depth-corrected augmented reality in MIS with monocular endoscopes.



### Making 360$^{\circ}$ Video Watchable in 2D: Learning Videography for Click Free Viewing
- **Arxiv ID**: http://arxiv.org/abs/1703.00495v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00495v2)
- **Published**: 2017-03-01 20:58:19+00:00
- **Updated**: 2017-05-24 20:11:26+00:00
- **Authors**: Yu-Chuan Su, Kristen Grauman
- **Comment**: CVPR 2017 Spotlight
- **Journal**: None
- **Summary**: 360$^{\circ}$ video requires human viewers to actively control "where" to look while watching the video. Although it provides a more immersive experience of the visual content, it also introduces additional burden for viewers; awkward interfaces to navigate the video lead to suboptimal viewing experiences. Virtual cinematography is an appealing direction to remedy these problems, but conventional methods are limited to virtual environments or rely on hand-crafted heuristics. We propose a new algorithm for virtual cinematography that automatically controls a virtual camera within a 360$^{\circ}$ video. Compared to the state of the art, our algorithm allows more general camera control, avoids redundant outputs, and extracts its output videos substantially more efficiently. Experimental results on over 7 hours of real "in the wild" video show that our generalized camera control is crucial for viewing 360$^{\circ}$ video, while the proposed efficient algorithm is essential for making the generalized control computationally tractable.



### Learning Social Affordance Grammar from Videos: Transferring Human Interactions to Human-Robot Interactions
- **Arxiv ID**: http://arxiv.org/abs/1703.00503v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.00503v1)
- **Published**: 2017-03-01 21:05:10+00:00
- **Updated**: 2017-03-01 21:05:10+00:00
- **Authors**: Tianmin Shu, Xiaofeng Gao, Michael S. Ryoo, Song-Chun Zhu
- **Comment**: The 2017 IEEE International Conference on Robotics and Automation
  (ICRA)
- **Journal**: None
- **Summary**: In this paper, we present a general framework for learning social affordance grammar as a spatiotemporal AND-OR graph (ST-AOG) from RGB-D videos of human interactions, and transfer the grammar to humanoids to enable a real-time motion inference for human-robot interaction (HRI). Based on Gibbs sampling, our weakly supervised grammar learning can automatically construct a hierarchical representation of an interaction with long-term joint sub-tasks of both agents and short term atomic actions of individual agents. Based on a new RGB-D video dataset with rich instances of human interactions, our experiments of Baxter simulation, human evaluation, and real Baxter test demonstrate that the model learned from limited training data successfully generates human-like behaviors in unseen scenarios and outperforms both baselines.



### ISIC 2017 - Skin Lesion Analysis Towards Melanoma Detection
- **Arxiv ID**: http://arxiv.org/abs/1703.00523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00523v1)
- **Published**: 2017-03-01 21:41:58+00:00
- **Updated**: 2017-03-01 21:41:58+00:00
- **Authors**: Matt Berseth
- **Comment**: ISIC2017
- **Journal**: None
- **Summary**: Our system addresses Part 1, Lesion Segmentation and Part 3, Lesion Classification of the ISIC 2017 challenge. Both algorithms make use of deep convolutional networks to achieve the challenge objective.



### Skin cancer reorganization and classification with deep neural network
- **Arxiv ID**: http://arxiv.org/abs/1703.00534v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T20
- **Links**: [PDF](http://arxiv.org/pdf/1703.00534v1)
- **Published**: 2017-03-01 22:21:21+00:00
- **Updated**: 2017-03-01 22:21:21+00:00
- **Authors**: Hao Chang
- **Comment**: 5 pages, 2 figures. ISIC2017
- **Journal**: None
- **Summary**: As one kind of skin cancer, melanoma is very dangerous. Dermoscopy based early detection and recarbonization strategy is critical for melanoma therapy. However, well-trained dermatologists dominant the diagnostic accuracy. In order to solve this problem, many effort focus on developing automatic image analysis systems. Here we report a novel strategy based on deep learning technique, and achieve very high skin lesion segmentation and melanoma diagnosis accuracy: 1) we build a segmentation neural network (skin_segnn), which achieved very high lesion boundary detection accuracy; 2) We build another very deep neural network based on Google inception v3 network (skin_recnn) and its well-trained weight. The novel designed transfer learning based deep neural network skin_inceptions_v3_nn helps to achieve a high prediction accuracy.



### Label Refinement Network for Coarse-to-Fine Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1703.00551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00551v1)
- **Published**: 2017-03-01 23:42:30+00:00
- **Updated**: 2017-03-01 23:42:30+00:00
- **Authors**: Md Amirul Islam, Shujon Naha, Mrigank Rochan, Neil Bruce, Yang Wang
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: We consider the problem of semantic image segmentation using deep convolutional neural networks. We propose a novel network architecture called the label refinement network that predicts segmentation labels in a coarse-to-fine fashion at several resolutions. The segmentation labels at a coarse resolution are used together with convolutional features to obtain finer resolution segmentation labels. We define loss functions at several stages in the network to provide supervisions at different stages. Our experimental results on several standard datasets demonstrate that the proposed model provides an effective way of producing pixel-wise dense image labeling.



### Change Detection under Global Viewpoint Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1703.00552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00552v1)
- **Published**: 2017-03-01 23:51:03+00:00
- **Updated**: 2017-03-01 23:51:03+00:00
- **Authors**: Murase Tomoya, Tanaka Kanji
- **Comment**: 8 pages, 9 figures, technical report
- **Journal**: None
- **Summary**: This paper addresses the problem of change detection from a novel perspective of long-term map learning. We are particularly interested in designing an approach that can scale to large maps and that can function under global uncertainty in the viewpoint (i.e., GPS-denied situations). Our approach, which utilizes a compact bag-of-words (BoW) scene model, makes several contributions to the problem:   1) Two kinds of prior information are extracted from the view sequence map and used for change detection. Further, we propose a novel type of prior, called motion prior, to predict the relative motions of stationary objects and anomaly ego-motion detection. The proposed prior is also useful for distinguishing stationary from non-stationary objects.   2) A small set of good reference images (e.g., 10) are efficiently retrieved from the view sequence map by employing the recently developed Bag-of-Local-Convolutional-Features (BoLCF) scene model.   3) Change detection is reformulated as a scene retrieval over these reference images to find changed objects using a novel spatial Bag-of-Words (SBoW) scene model. Evaluations conducted of individual techniques and also their combinations on a challenging dataset of highly dynamic scenes in the publicly available Malaga dataset verify their efficacy.



### A Deep Cascade of Convolutional Neural Networks for MR Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1703.00555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00555v1)
- **Published**: 2017-03-01 23:54:12+00:00
- **Updated**: 2017-03-01 23:54:12+00:00
- **Authors**: Jo Schlemper, Jose Caballero, Joseph V. Hajnal, Anthony Price, Daniel Rueckert
- **Comment**: None
- **Journal**: None
- **Summary**: The acquisition of Magnetic Resonance Imaging (MRI) is inherently slow. Inspired by recent advances in deep learning, we propose a framework for reconstructing MR images from undersampled data using a deep cascade of convolutional neural networks to accelerate the data acquisition process. We show that for Cartesian undersampling of 2D cardiac MR images, the proposed method outperforms the state-of-the-art compressed sensing approaches, such as dictionary learning-based MRI (DLMRI) reconstruction, in terms of reconstruction error, perceptual quality and reconstruction speed for both 3-fold and 6-fold undersampling. Compared to DLMRI, the error produced by the method proposed is approximately twice as small, allowing to preserve anatomical structures more faithfully. Using our method, each image can be reconstructed in 23 ms, which is fast enough to enable real-time applications.



