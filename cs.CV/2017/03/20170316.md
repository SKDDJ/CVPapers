# Arxiv Papers in cs.CV on 2017-03-16
### Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/1703.05446v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.05446v2)
- **Published**: 2017-03-16 01:14:36+00:00
- **Updated**: 2017-07-28 01:41:39+00:00
- **Authors**: Ke Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen, Liang Lin
- **Comment**: Accepted to appear in CVPR 2017
- **Journal**: None
- **Summary**: Human parsing has recently attracted a lot of research interests due to its huge application potentials. However existing datasets have limited number of images and annotations, and lack the variety of human appearances and the coverage of challenging cases in unconstrained environment. In this paper, we introduce a new benchmark "Look into Person (LIP)" that makes a significant advance in terms of scalability, diversity and difficulty, a contribution that we feel is crucial for future developments in human-centric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from a wider range of viewpoints, occlusions and background complexity. Given these rich annotations we perform detailed analyses of the leading human parsing approaches, gaining insights into the success and failures of these methods. Furthermore, in contrast to the existing efforts on improving the feature discriminative capability, we solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need for specifically labeling human joints in model training). Our self-supervised learning framework can be injected into any advanced neural networks to help incorporate rich high-level knowledge regarding human joints from a global perspective and improve the parsing results. Extensive evaluations on our LIP and the public PASCAL-Person-Part dataset demonstrate the superiority of our method.



### Refining Image Categorization by Exploiting Web Images and General Corpus
- **Arxiv ID**: http://arxiv.org/abs/1703.05451v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.05451v1)
- **Published**: 2017-03-16 01:36:49+00:00
- **Updated**: 2017-03-16 01:36:49+00:00
- **Authors**: Yazhou Yao, Jian Zhang, Fumin Shen, Xiansheng Hua, Wankou Yang, Zhenmin Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Studies show that refining real-world categories into semantic subcategories contributes to better image modeling and classification. Previous image sub-categorization work relying on labeled images and WordNet's hierarchy is not only labor-intensive, but also restricted to classify images into NOUN subcategories. To tackle these problems, in this work, we exploit general corpus information to automatically select and subsequently classify web images into semantic rich (sub-)categories. The following two major challenges are well studied: 1) noise in the labels of subcategories derived from the general corpus; 2) noise in the labels of images retrieved from the web. Specifically, we first obtain the semantic refinement subcategories from the text perspective and remove the noise by the relevance-based approach. To suppress the search error induced noisy images, we then formulate image selection and classifier learning as a multi-class multi-instance learning problem and propose to solve the employed problem by the cutting-plane algorithm. The experiments show significant performance gains by using the generated data of our way on both image categorization and sub-categorization tasks. The proposed approach also consistently outperforms existing weakly supervised and web-supervised approaches.



### Ranking Based Locality Sensitive Hashing Enabled Cancelable Biometrics: Index-of-Max Hashing
- **Arxiv ID**: http://arxiv.org/abs/1703.05455v2
- **DOI**: 10.1109/TIFS.2017.2753172
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05455v2)
- **Published**: 2017-03-16 02:10:56+00:00
- **Updated**: 2017-09-17 16:52:24+00:00
- **Authors**: Zhe Jin, Yen-Lung Lai, Jung-Yeon Hwang, Soohyung Kim, Andrew Beng Jin Teoh
- **Comment**: 15 pages, 8 figures, 6 tables
- **Journal**: None
- **Summary**: In this paper, we propose a ranking based locality sensitive hashing inspired two-factor cancelable biometrics, dubbed "Index-of-Max" (IoM) hashing for biometric template protection. With externally generated random parameters, IoM hashing transforms a real-valued biometric feature vector into discrete index (max ranked) hashed code. We demonstrate two realizations from IoM hashing notion, namely Gaussian Random Projection based and Uniformly Random Permutation based hashing schemes. The discrete indices representation nature of IoM hashed codes enjoy serveral merits. Firstly, IoM hashing empowers strong concealment to the biometric information. This contributes to the solid ground of non-invertibility guarantee. Secondly, IoM hashing is insensitive to the features magnitude, hence is more robust against biometric features variation. Thirdly, the magnitude-independence trait of IoM hashing makes the hash codes being scale-invariant, which is critical for matching and feature alignment. The experimental results demonstrate favorable accuracy performance on benchmark FVC2002 and FVC2004 fingerprint databases. The analyses justify its resilience to the existing and newly introduced security and privacy attacks as well as satisfy the revocability and unlinkability criteria of cancelable biometrics.



### Using Human Brain Activity to Guide Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.05463v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05463v2)
- **Published**: 2017-03-16 02:56:54+00:00
- **Updated**: 2017-09-19 21:49:59+00:00
- **Authors**: Ruth Fong, Walter Scheirer, David Cox
- **Comment**: Supplemental material can be downloaded here:
  http://www.wjscheirer.com/misc/activity_weights/fong-et-al-supplementary.pdf
- **Journal**: None
- **Summary**: Machine learning is a field of computer science that builds algorithms that learn. In many cases, machine learning algorithms are used to recreate a human ability like adding a caption to a photo, driving a car, or playing a game. While the human brain has long served as a source of inspiration for machine learning, little effort has been made to directly use data collected from working brains as a guide for machine learning algorithms. Here we demonstrate a new paradigm of "neurally-weighted" machine learning, which takes fMRI measurements of human brain activity from subjects viewing images, and infuses these data into the training process of an object recognition learning algorithm to make it more consistent with the human brain. After training, these neurally-weighted classifiers are able to classify images without requiring any additional neural data. We show that our neural-weighting approach can lead to large performance gains when used with traditional machine vision features, as well as to significant improvements with already high-performing convolutional neural network features. The effectiveness of this approach points to a path forward for a new class of hybrid machine learning algorithms which take both inspiration and direct constraints from neuronal data.



### Global and Local Information Based Deep Network for Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1703.05467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05467v1)
- **Published**: 2017-03-16 03:23:48+00:00
- **Updated**: 2017-03-16 03:23:48+00:00
- **Authors**: Jin Qi, Miao Le, Chunming Li, Ping Zhou
- **Comment**: 4 pages, 3 figures. ISIC2017
- **Journal**: None
- **Summary**: With a large influx of dermoscopy images and a growing shortage of dermatologists, automatic dermoscopic image analysis plays an essential role in skin cancer diagnosis. In this paper, a new deep fully convolutional neural network (FCNN) is proposed to automatically segment melanoma out of skin images by end-to-end learning with only pixels and labels as inputs. Our proposed FCNN is capable of using both local and global information to segment melanoma by adopting skipping layers. The public benchmark database consisting of 150 validation images, 600 test images and 2000 training images in the melanoma detection challenge 2017 at International Symposium Biomedical Imaging 2017 is used to test the performance of our algorithm. All large size images (for example, $4000\times 6000$ pixels) are reduced to much smaller images with $384\times 384$ pixels (more than 10 times smaller). We got and submitted preliminary results to the challenge without any pre or post processing. The performance of our proposed method could be further improved by data augmentation and by avoiding image size reduction.



### Steganographic Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.05502v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CR, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1703.05502v2)
- **Published**: 2017-03-16 08:28:11+00:00
- **Updated**: 2019-10-07 19:56:14+00:00
- **Authors**: Denis Volkhonskiy, Ivan Nazarov, Evgeny Burnaev
- **Comment**: 15 pages, 10 figures, 5 tables, Workshop on Adversarial Training
  (NIPS 2016, Barcelona, Spain)
- **Journal**: None
- **Summary**: Steganography is collection of methods to hide secret information ("payload") within non-secret information "container"). Its counterpart, Steganalysis, is the practice of determining if a message contains a hidden payload, and recovering it if possible. Presence of hidden payloads is typically detected by a binary classifier. In the present study, we propose a new model for generating image-like containers based on Deep Convolutional Generative Adversarial Networks (DCGAN). This approach allows to generate more setganalysis-secure message embedding using standard steganography algorithms. Experiment results demonstrate that the new model successfully deceives the steganography analyzer, and for this reason, can be used in steganographic applications.



### Convolutional Neural Network on Three Orthogonal Planes for Dynamic Texture Classification
- **Arxiv ID**: http://arxiv.org/abs/1703.05530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05530v1)
- **Published**: 2017-03-16 09:30:07+00:00
- **Updated**: 2017-03-16 09:30:07+00:00
- **Authors**: Vincent Andrearczyk, Paul F. Whelan
- **Comment**: 19 pages, 10 figures
- **Journal**: None
- **Summary**: Dynamic Textures (DTs) are sequences of images of moving scenes that exhibit certain stationarity properties in time such as smoke, vegetation and fire. The analysis of DT is important for recognition, segmentation, synthesis or retrieval for a range of applications including surveillance, medical imaging and remote sensing. Deep learning methods have shown impressive results and are now the new state of the art for a wide range of computer vision tasks including image and video recognition and segmentation. In particular, Convolutional Neural Networks (CNNs) have recently proven to be well suited for texture analysis with a design similar to a filter bank approach. In this paper, we develop a new approach to DT analysis based on a CNN method applied on three orthogonal planes x y , xt and y t . We train CNNs on spatial frames and temporal slices extracted from the DT sequences and combine their outputs to obtain a competitive DT classifier. Our results on a wide range of commonly used DT classification benchmark datasets prove the robustness of our approach. Significant improvement of the state of the art is shown on the larger datasets.



### Combining Contrast Invariant L1 Data Fidelities with Nonlinear Spectral Image Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1703.05560v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, math.SP
- **Links**: [PDF](http://arxiv.org/pdf/1703.05560v1)
- **Published**: 2017-03-16 11:12:13+00:00
- **Updated**: 2017-03-16 11:12:13+00:00
- **Authors**: Leonie Zeune, Stephan A. van Gils, Leon W. M. M. Terstappen, Christoph Brune
- **Comment**: 13 pages, 7 figures, conference SSVM 2017
- **Journal**: None
- **Summary**: This paper focuses on multi-scale approaches for variational methods and corresponding gradient flows. Recently, for convex regularization functionals such as total variation, new theory and algorithms for nonlinear eigenvalue problems via nonlinear spectral decompositions have been developed. Those methods open new directions for advanced image filtering. However, for an effective use in image segmentation and shape decomposition, a clear interpretation of the spectral response regarding size and intensity scales is needed but lacking in current approaches. In this context, $L^1$ data fidelities are particularly helpful due to their interesting multi-scale properties such as contrast invariance. Hence, the novelty of this work is the combination of $L^1$-based multi-scale methods with nonlinear spectral decompositions. We compare $L^1$ with $L^2$ scale-space methods in view of spectral image representation and decomposition. We show that the contrast invariant multi-scale behavior of $L^1-TV$ promotes sparsity in the spectral response providing more informative decompositions. We provide a numerical method and analyze synthetic and biomedical images at which decomposition leads to improved segmentation.



### From visual words to a visual grammar: using language modelling for image classification
- **Arxiv ID**: http://arxiv.org/abs/1703.05571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05571v1)
- **Published**: 2017-03-16 11:46:57+00:00
- **Updated**: 2017-03-16 11:46:57+00:00
- **Authors**: Antonio Foncubierta-Rodríguez, Henning Müller, Adrien Depeursinge
- **Comment**: None
- **Journal**: None
- **Summary**: The Bag--of--Visual--Words (BoVW) is a visual description technique that aims at shortening the semantic gap by partitioning a low--level feature space into regions of the feature space that potentially correspond to visual concepts and by giving more value to this space. In this paper we present a conceptual analysis of three major properties of language grammar and how they can be adapted to the computer vision and image understanding domain based on the bag of visual words paradigm. Evaluation of the visual grammar shows that a positive impact on classification accuracy and/or descriptor size is obtained when the technique are applied when the proposed techniques are applied.



### Convolutional neural network architecture for geometric matching
- **Arxiv ID**: http://arxiv.org/abs/1703.05593v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.05593v2)
- **Published**: 2017-03-16 13:03:54+00:00
- **Updated**: 2017-04-13 22:32:43+00:00
- **Authors**: Ignacio Rocco, Relja Arandjelović, Josef Sivic
- **Comment**: In 2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR 2017)
- **Journal**: None
- **Summary**: We address the problem of determining correspondences between two images in agreement with a geometric model such as an affine or thin-plate spline transformation, and estimating its parameters. The contributions of this work are three-fold. First, we propose a convolutional neural network architecture for geometric matching. The architecture is based on three main components that mimic the standard steps of feature extraction, matching and simultaneous inlier detection and model parameter estimation, while being trainable end-to-end. Second, we demonstrate that the network parameters can be trained from synthetically generated imagery without the need for manual annotation and that our matching layer significantly increases generalization capabilities to never seen before images. Finally, we show that the same model can perform both instance-level and category-level matching giving state-of-the-art results on the challenging Proposal Flow dataset.



### Deep Sketch Hashing: Fast Free-hand Sketch-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1703.05605v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05605v1)
- **Published**: 2017-03-16 13:18:36+00:00
- **Updated**: 2017-03-16 13:18:36+00:00
- **Authors**: Li Liu, Fumin Shen, Yuming Shen, Xianglong Liu, Ling Shao
- **Comment**: This paper will appear as a spotlight paper in CVPR2017
- **Journal**: None
- **Summary**: Free-hand sketch-based image retrieval (SBIR) is a specific cross-view retrieval task, in which queries are abstract and ambiguous sketches while the retrieval database is formed with natural images. Work in this area mainly focuses on extracting representative and shared features for sketches and natural images. However, these can neither cope well with the geometric distortion between sketches and images nor be feasible for large-scale SBIR due to the heavy continuous-valued distance computation. In this paper, we speed up SBIR by introducing a novel binary coding method, named \textbf{Deep Sketch Hashing} (DSH), where a semi-heterogeneous deep architecture is proposed and incorporated into an end-to-end binary coding framework. Specifically, three convolutional neural networks are utilized to encode free-hand sketches, natural images and, especially, the auxiliary sketch-tokens which are adopted as bridges to mitigate the sketch-image geometric distortion. The learned DSH codes can effectively capture the cross-view similarities as well as the intrinsic semantic correlations between different categories. To the best of our knowledge, DSH is the first hashing work specifically designed for category-level SBIR with an end-to-end deep architecture. The proposed DSH is comprehensively evaluated on two large-scale datasets of TU-Berlin Extension and Sketchy, and the experiments consistently show DSH's superior SBIR accuracies over several state-of-the-art methods, while achieving significantly reduced retrieval time and memory footprint.



### Anisotropic-Scale Junction Detection and Matching for Indoor Images
- **Arxiv ID**: http://arxiv.org/abs/1703.05630v2
- **DOI**: 10.1109/TIP.2017.2754945
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05630v2)
- **Published**: 2017-03-16 14:10:44+00:00
- **Updated**: 2017-09-17 08:44:04+00:00
- **Authors**: Nan Xue, Gui-Song Xia, Xiang Bai, Liangpei Zhang, Weiming Shen
- **Comment**: This paper has been accepted for publication in the IEEE Transactions
  on Image Processing
- **Journal**: IEEE Transactions on Image Processing, Vol. 27, No.1, pp. 78-91,
  2018
- **Summary**: Junctions play an important role in the characterization of local geometric structures in images, the detection of which is a longstanding and challenging task. Existing junction detectors usually focus on identifying the junction locations and the orientations of the junction branches while ignoring their scales; however, these scales also contain rich geometric information. This paper presents a novel approach to junction detection and characterization that exploits the locally anisotropic geometries of a junction and estimates the scales of these geometries using an \emph{a contrario} model. The output junctions have anisotropic scales --- i.e., each branch of a junction is associated with an independent scale parameter --- and are thus termed anisotropic-scale junctions (ASJs). We then apply the newly detected ASJs for the matching of indoor images, in which there may be dramatic changes in viewpoint and the detected local visual features, e.g., key-points, are usually insufficiently distinctive. We propose to use the anisotropic geometries of our junctions to improve the matching precision for indoor images. Matching results obtained on sets of indoor images demonstrate that our approach achieves state-of-the-art performance in indoor image matching.



### Segmented and Directional Impact Detection for Parked Vehicles using Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1703.05680v1
- **DOI**: 10.1109/IWSSIP.2016.7502762
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05680v1)
- **Published**: 2017-03-16 15:51:56+00:00
- **Updated**: 2017-03-16 15:51:56+00:00
- **Authors**: Andre Ebert, Sebastian Feld, Florian Dorfmeister
- **Comment**: 4 Pages, 6 Figures, Accepted at the The 23rd International Conference
  on Systems, Signals and Image Processing (IWSSIP)
- **Journal**: None
- **Summary**: Mutual usage of vehicles as well as car sharing became more and more attractive during the last years. Especially in urban environments with limited parking possibilities and a higher risk for traffic jams, car rentals and sharing services may save time and money. But when renting a vehicle it could already be damaged (e.g., scratches or bumps inflicted by a previous user) without the damage being perceived by the service provider. In order to address such problems, we present an automated, motion-based system for impact detection, that facilitates a common smartphone as a sensor platform. The system is capable of detecting the impact segment and the point of time of an impact event on a vehicle's surface, as well as its direction of origin. With this additional specific knowledge, it may be possible to reconstruct the circumstances of an impact event, e.g., to prove possible innocence of a service's customer.



### SVDNet for Pedestrian Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1703.05693v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05693v4)
- **Published**: 2017-03-16 16:11:05+00:00
- **Updated**: 2017-08-06 05:37:09+00:00
- **Authors**: Yifan Sun, Liang Zheng, Weijian Deng, Shengjin Wang
- **Comment**: accepted as spotlight to ICCV 2017
- **Journal**: None
- **Summary**: This paper proposes the SVDNet for retrieval problems, with focus on the application of person re-identification (re-ID). We view each weight vector within a fully connected (FC) layer in a convolutional neuron network (CNN) as a projection basis. It is observed that the weight vectors are usually highly correlated. This problem leads to correlations among entries of the FC descriptor, and compromises the retrieval performance based on the Euclidean distance. To address the problem, this paper proposes to optimize the deep representation learning process with Singular Vector Decomposition (SVD). Specifically, with the restraint and relaxation iteration (RRI) training scheme, we are able to iteratively integrate the orthogonality constraint in CNN training, yielding the so-called SVDNet. We conduct experiments on the Market-1501, CUHK03, and Duke datasets, and show that RRI effectively reduces the correlation among the projection vectors, produces more discriminative FC descriptors, and significantly improves the re-ID accuracy. On the Market-1501 dataset, for instance, rank-1 accuracy is improved from 55.3% to 80.5% for CaffeNet, and from 73.8% to 82.3% for ResNet-50.



### Learning Robust Hash Codes for Multiple Instance Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1703.05724v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.05724v1)
- **Published**: 2017-03-16 17:07:26+00:00
- **Updated**: 2017-03-16 17:07:26+00:00
- **Authors**: Sailesh Conjeti, Magdalini Paschali, Amin Katouzian, Nassir Navab
- **Comment**: 10 pages, 7 figures, under review at MICCAI 2017
- **Journal**: None
- **Summary**: In this paper, for the first time, we introduce a multiple instance (MI) deep hashing technique for learning discriminative hash codes with weak bag-level supervision suited for large-scale retrieval. We learn such hash codes by aggregating deeply learnt hierarchical representations across bag members through a dedicated MI pool layer. For better trainability and retrieval quality, we propose a two-pronged approach that includes robust optimization and training with an auxiliary single instance hashing arm which is down-regulated gradually. We pose retrieval for tumor assessment as an MI problem because tumors often coexist with benign masses and could exhibit complementary signatures when scanned from different anatomical views. Experimental validations on benchmark mammography and histology datasets demonstrate improved retrieval performance over the state-of-the-art methods.



### Low-rank and Sparse NMF for Joint Endmembers' Number Estimation and Blind Unmixing of Hyperspectral Images
- **Arxiv ID**: http://arxiv.org/abs/1703.05785v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.05785v1)
- **Published**: 2017-03-16 18:25:21+00:00
- **Updated**: 2017-03-16 18:25:21+00:00
- **Authors**: Paris V. Giampouras, Athanasios A. Rontogiannis, Konstantinos D. Koutroumbas
- **Comment**: None
- **Journal**: None
- **Summary**: Estimation of the number of endmembers existing in a scene constitutes a critical task in the hyperspectral unmixing process. The accuracy of this estimate plays a crucial role in subsequent unsupervised unmixing steps i.e., the derivation of the spectral signatures of the endmembers (endmembers' extraction) and the estimation of the abundance fractions of the pixels. A common practice amply followed in literature is to treat endmembers' number estimation and unmixing, independently as two separate tasks, providing the outcome of the former as input to the latter. In this paper, we go beyond this computationally demanding strategy. More precisely, we set forth a multiple constrained optimization framework, which encapsulates endmembers' number estimation and unsupervised unmixing in a single task. This is attained by suitably formulating the problem via a low-rank and sparse nonnegative matrix factorization rationale, where low-rankness is promoted with the use of a sophisticated $\ell_2/\ell_1$ norm penalty term. An alternating proximal algorithm is then proposed for minimizing the emerging cost function. The results obtained by simulated and real data experiments verify the effectiveness of the proposed approach.



### Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning
- **Arxiv ID**: http://arxiv.org/abs/1703.05830v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.05830v5)
- **Published**: 2017-03-16 21:35:15+00:00
- **Updated**: 2017-11-15 19:29:24+00:00
- **Authors**: Mohammed Sadegh Norouzzadeh, Anh Nguyen, Margaret Kosmala, Ali Swanson, Meredith Palmer, Craig Packer, Jeff Clune
- **Comment**: None
- **Journal**: None
- **Summary**: Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would revolutionize our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could transform many fields of biology, ecology, and zoology into "big data" sciences. Motion sensor "camera traps" enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2-million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with over 93.8% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3% of the data while still performing at the same 96.6% accuracy as that of crowdsourced teams of human volunteers, saving more than 8.4 years (at 40 hours per week) of human labeling effort (i.e. over 17,000 hours) on this 3.2-million-image dataset. Those efficiency gains immediately highlight the importance of using deep neural networks to automate data extraction from camera-trap images. Our results suggest that this technology could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.



