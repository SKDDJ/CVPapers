# Arxiv Papers in cs.CV on 2017-03-28
### Ensembles of Deep LSTM Learners for Activity Recognition using Wearables
- **Arxiv ID**: http://arxiv.org/abs/1703.09370v1
- **DOI**: 10.1145/3090076
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.09370v1)
- **Published**: 2017-03-28 02:00:47+00:00
- **Updated**: 2017-03-28 02:00:47+00:00
- **Authors**: Yu Guan, Thomas Ploetz
- **Comment**: accepted for publication in ACM IMWUT (Ubicomp) 2017
- **Journal**: None
- **Summary**: Recently, deep learning (DL) methods have been introduced very successfully into human activity recognition (HAR) scenarios in ubiquitous and wearable computing. Especially the prospect of overcoming the need for manual feature design combined with superior classification capabilities render deep neural networks very attractive for real-life HAR application. Even though DL-based approaches now outperform the state-of-the-art in a number of recognitions tasks of the field, yet substantial challenges remain. Most prominently, issues with real-life datasets, typically including imbalanced datasets and problematic data quality, still limit the effectiveness of activity recognition using wearables. In this paper we tackle such challenges through Ensembles of deep Long Short Term Memory (LSTM) networks. We have developed modified training procedures for LSTM networks and combine sets of diverse LSTM learners into classifier collectives. We demonstrate, both formally and empirically, that Ensembles of deep LSTM learners outperform the individual LSTM networks. Through an extensive experimental evaluation on three standard benchmarks (Opportunity, PAMAP2, Skoda) we demonstrate the excellent recognition capabilities of our approach and its potential for real-life applications of human activity recognition.



### Robust Guided Image Filtering
- **Arxiv ID**: http://arxiv.org/abs/1703.09379v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09379v1)
- **Published**: 2017-03-28 02:59:15+00:00
- **Updated**: 2017-03-28 02:59:15+00:00
- **Authors**: Wei Liu, Xiaogang Chen, Chunhua Shen, Jingyi Yu, Qiang Wu, Jie Yang
- **Comment**: This paper is an extension of our previous work at arXiv:1512.08103
  and arXiv:1506.05187
- **Journal**: None
- **Summary**: The process of using one image to guide the filtering process of another one is called Guided Image Filtering (GIF). The main challenge of GIF is the structure inconsistency between the guidance image and the target image. Besides, noise in the target image is also a challenging issue especially when it is heavy. In this paper, we propose a general framework for Robust Guided Image Filtering (RGIF), which contains a data term and a smoothness term, to solve the two issues mentioned above. The data term makes our model simultaneously denoise the target image and perform GIF which is robust against the heavy noise. The smoothness term is able to make use of the property of both the guidance image and the target image which is robust against the structure inconsistency. While the resulting model is highly non-convex, it can be solved through the proposed Iteratively Re-weighted Least Squares (IRLS) in an efficient manner. For challenging applications such as guided depth map upsampling, we further develop a data-driven parameter optimization scheme to properly determine the parameter in our model. This optimization scheme can help to preserve small structures and sharp depth edges even for a large upsampling factor (8x for example). Moreover, the specially designed structure of the data term and the smoothness term makes our model perform well in edge-preserving smoothing for single-image tasks (i.e., the guidance image is the target image itself). This paper is an extension of our previous work [1], [2].



### Adversarial Transformation Networks: Learning to Generate Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1703.09387v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.09387v1)
- **Published**: 2017-03-28 03:24:33+00:00
- **Updated**: 2017-03-28 03:24:33+00:00
- **Authors**: Shumeet Baluja, Ian Fischer
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple different approaches of generating adversarial examples have been proposed to attack deep neural networks. These approaches involve either directly computing gradients with respect to the image pixels, or directly solving an optimization on the image pixels. In this work, we present a fundamentally new method for generating adversarial examples that is fast to execute and provides exceptional diversity of output. We efficiently train feed-forward neural networks in a self-supervised manner to generate adversarial examples against a target network or set of networks. We call such a network an Adversarial Transformation Network (ATN). ATNs are trained to generate adversarial examples that minimally modify the classifier's outputs given the original input, while constraining the new classification to match an adversarial target class. We present methods to train ATNs and analyze their effectiveness targeting a variety of MNIST classifiers as well as the latest state-of-the-art ImageNet classifier Inception ResNet v2.



### Mixture of Counting CNNs: Adaptive Integration of CNNs Specialized to Specific Appearance for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1703.09393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09393v1)
- **Published**: 2017-03-28 03:41:44+00:00
- **Updated**: 2017-03-28 03:41:44+00:00
- **Authors**: Shohei Kumagai, Kazuhiro Hotta, Takio Kurita
- **Comment**: 8pages, 8figures
- **Journal**: None
- **Summary**: This paper proposes a crowd counting method. Crowd counting is difficult because of large appearance changes of a target which caused by density and scale changes. Conventional crowd counting methods generally utilize one predictor (e,g., regression and multi-class classifier). However, such only one predictor can not count targets with large appearance changes well. In this paper, we propose to predict the number of targets using multiple CNNs specialized to a specific appearance, and those CNNs are adaptively selected according to the appearance of a test image. By integrating the selected CNNs, the proposed method has the robustness to large appearance changes. In experiments, we confirm that the proposed method can count crowd with lower counting error than a CNN and integration of CNNs with fixed weights. Moreover, we confirm that each predictor automatically specialized to a specific appearance.



### Evaluation of Classifiers for Image Segmentation: Applications for Eucalypt Forest Inventory
- **Arxiv ID**: http://arxiv.org/abs/1703.09436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09436v1)
- **Published**: 2017-03-28 07:43:42+00:00
- **Updated**: 2017-03-28 07:43:42+00:00
- **Authors**: Rodrigo M. Ferreira, Ricardo M. Marcacini
- **Comment**: in Portuguese
- **Journal**: None
- **Summary**: The task of counting eucalyptus trees from aerial images collected by unmanned aerial vehicles (UAVs) has been frequently explored by techniques of estimation of the basal area, i.e, by determining the expected number of trees based on sampling techniques. An alternative is the use of machine learning to identify patterns that represent a tree unit, and then search for the occurrence of these patterns throughout the image. This strategy depends on a supervised image segmentation step to define predefined interest regions. Thus, it is possible to automate the counting of eucalyptus trees in these images, thereby increasing the efficiency of the eucalyptus forest inventory management. In this paper, we evaluated 20 different classifiers for the image segmentation task. A real sample was used to analyze the counting trees task considering a practical environment. The results show that it possible to automate this task with 0.7% counting error, in particular, by using strategies based on a combination of classifiers. Moreover, we present some performance considerations about each classifier that can be useful as a basis for decision-making in future tasks.



### Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs
- **Arxiv ID**: http://arxiv.org/abs/1703.09438v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09438v3)
- **Published**: 2017-03-28 07:45:59+00:00
- **Updated**: 2017-08-07 18:07:33+00:00
- **Authors**: Maxim Tatarchenko, Alexey Dosovitskiy, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep convolutional decoder architecture that can generate volumetric 3D outputs in a compute- and memory-efficient manner by using an octree representation. The network learns to predict both the structure of the octree, and the occupancy values of individual cells. This makes it a particularly valuable technique for generating 3D shapes. In contrast to standard decoders acting on regular voxel grids, the architecture does not have cubic complexity. This allows representing much higher resolution outputs with a limited memory budget. We demonstrate this in several application domains, including 3D convolutional autoencoders, generation of objects and whole scenes from high-level representations, and shape from a single image.



### Learned Spectral Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1703.09470v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.09470v1)
- **Published**: 2017-03-28 09:17:38+00:00
- **Updated**: 2017-03-28 09:17:38+00:00
- **Authors**: Silvano Galliani, Charis Lanaras, Dimitrios Marmanis, Emmanuel Baltsavias, Konrad Schindler
- **Comment**: Submitted to ICCV 2017 (10 pages, 8 figures)
- **Journal**: None
- **Summary**: We describe a novel method for blind, single-image spectral super-resolution. While conventional super-resolution aims to increase the spatial resolution of an input image, our goal is to spectrally enhance the input, i.e., generate an image with the same spatial resolution, but a greatly increased number of narrow (hyper-spectral) wave-length bands. Just like the spatial statistics of natural images has rich structure, which one can exploit as prior to predict high-frequency content from a low resolution image, the same is also true in the spectral domain: the materials and lighting conditions of the observed world induce structure in the spectrum of wavelengths observed at a given pixel. Surprisingly, very little work exists that attempts to use this diagnosis and achieve blind spectral super-resolution from single images. We start from the conjecture that, just like in the spatial domain, we can learn the statistics of natural image spectra, and with its help generate finely resolved hyper-spectral images from RGB input. Technically, we follow the current best practice and implement a convolutional neural network (CNN), which is trained to carry out the end-to-end mapping from an entire RGB image to the corresponding hyperspectral image of equal size. We demonstrate spectral super-resolution both for conventional RGB images and for multi-spectral satellite data, outperforming the state-of-the-art.



### Adversarial Image Perturbation for Privacy Protection -- A Game Theory Perspective
- **Arxiv ID**: http://arxiv.org/abs/1703.09471v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.GT
- **Links**: [PDF](http://arxiv.org/pdf/1703.09471v2)
- **Published**: 2017-03-28 09:17:47+00:00
- **Updated**: 2017-07-26 10:01:43+00:00
- **Authors**: Seong Joon Oh, Mario Fritz, Bernt Schiele
- **Comment**: To appear at ICCV'17
- **Journal**: None
- **Summary**: Users like sharing personal photos with others through social media. At the same time, they might want to make automatic identification in such photos difficult or even impossible. Classic obfuscation methods such as blurring are not only unpleasant but also not as effective as one would expect. Recent studies on adversarial image perturbations (AIP) suggest that it is possible to confuse recognition systems effectively without unpleasant artifacts. However, in the presence of counter measures against AIPs, it is unclear how effective AIP would be in particular when the choice of counter measure is unknown. Game theory provides tools for studying the interaction between agents with uncertainties in the strategies. We introduce a general game theoretical framework for the user-recogniser dynamics, and present a case study that involves current state of the art AIP and person recognition techniques. We derive the optimal strategy for the user that assures an upper bound on the recognition rate independent of the recogniser's counter measure. Code is available at https://goo.gl/hgvbNK.



### Robust Depth-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1703.09474v1
- **DOI**: 10.1109/TIP.2017.2675201
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09474v1)
- **Published**: 2017-03-28 09:26:54+00:00
- **Updated**: 2017-03-28 09:26:54+00:00
- **Authors**: Ancong Wu, Wei-Shi Zheng, Jianhuang Lai
- **Comment**: IEEE Transactions on Image Processing Early Access
- **Journal**: None
- **Summary**: Person re-identification (re-id) aims to match people across non-overlapping camera views. So far the RGB-based appearance is widely used in most existing works. However, when people appeared in extreme illumination or changed clothes, the RGB appearance-based re-id methods tended to fail. To overcome this problem, we propose to exploit depth information to provide more invariant body shape and skeleton information regardless of illumination and color change. More specifically, we exploit depth voxel covariance descriptor and further propose a locally rotation invariant depth shape descriptor called Eigen-depth feature to describe pedestrian body shape. We prove that the distance between any two covariance matrices on the Riemannian manifold is equivalent to the Euclidean distance between the corresponding Eigen-depth features. Furthermore, we propose a kernelized implicit feature transfer scheme to estimate Eigen-depth feature implicitly from RGB image when depth information is not available. We find that combining the estimated depth features with RGB-based appearance features can sometimes help to better reduce visual ambiguities of appearance features caused by illumination and similar clothes. The effectiveness of our models was validated on publicly available depth pedestrian datasets as compared to related methods for person re-identification.



### Locality preserving projection on SPD matrix Lie group: algorithm and analysis
- **Arxiv ID**: http://arxiv.org/abs/1703.09499v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, F.2.2
- **Links**: [PDF](http://arxiv.org/pdf/1703.09499v2)
- **Published**: 2017-03-28 10:38:22+00:00
- **Updated**: 2017-11-16 02:47:32+00:00
- **Authors**: Yangyang Li, Ruqian Lu
- **Comment**: 15 pages, 3 tables
- **Journal**: None
- **Summary**: Symmetric positive definite (SPD) matrices used as feature descriptors in image recognition are usually high dimensional. Traditional manifold learning is only applicable for reducing the dimension of high-dimensional vector-form data. For high-dimensional SPD matrices, directly using manifold learning algorithms to reduce the dimension of matrix-form data is impossible. The SPD matrix must first be transformed into a long vector, and then the dimension of this vector must be reduced. However, this approach breaks the spatial structure of the SPD matrix space. To overcome this limitation, we propose a new dimension reduction algorithm on SPD matrix space to transform high-dimensional SPD matrices into low-dimensional SPD matrices. Our work is based on the fact that the set of all SPD matrices with the same size has a Lie group structure, and we aim to transform the manifold learning to the SPD matrix Lie group. We use the basic idea of the manifold learning algorithm called locality preserving projection (LPP) to construct the corresponding Laplacian matrix on the SPD matrix Lie group. Thus, we call our approach Lie-LPP to emphasize its Lie group character. We present a detailed algorithm analysis and show through experiments that Lie-LPP achieves effective results on human action recognition and human face recognition.



### L2-constrained Softmax Loss for Discriminative Face Verification
- **Arxiv ID**: http://arxiv.org/abs/1703.09507v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09507v3)
- **Published**: 2017-03-28 11:19:50+00:00
- **Updated**: 2017-06-07 18:58:18+00:00
- **Authors**: Rajeev Ranjan, Carlos D. Castillo, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the performance of face verification systems has significantly improved using deep convolutional neural networks (DCNNs). A typical pipeline for face verification includes training a deep network for subject classification with softmax loss, using the penultimate layer output as the feature descriptor, and generating a cosine similarity score given a pair of face images. The softmax loss function does not optimize the features to have higher similarity score for positive pairs and lower similarity score for negative pairs, which leads to a performance gap. In this paper, we add an L2-constraint to the feature descriptors which restricts them to lie on a hypersphere of a fixed radius. This module can be easily implemented using existing deep learning frameworks. We show that integrating this simple step in the training pipeline significantly boosts the performance of face verification. Specifically, we achieve state-of-the-art results on the challenging IJB-A dataset, achieving True Accept Rate of 0.909 at False Accept Rate 0.0001 on the face verification protocol. Additionally, we achieve state-of-the-art performance on LFW dataset with an accuracy of 99.78%, and competing performance on YTF dataset with accuracy of 96.08%.



### Objects as context for detecting their semantic parts
- **Arxiv ID**: http://arxiv.org/abs/1703.09529v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09529v3)
- **Published**: 2017-03-28 12:11:28+00:00
- **Updated**: 2018-03-27 13:40:57+00:00
- **Authors**: Abel Gonzalez-Garcia, Davide Modolo, Vittorio Ferrari
- **Comment**: None
- **Journal**: None
- **Summary**: We present a semantic part detection approach that effectively leverages object information.We use the object appearance and its class as indicators of what parts to expect. We also model the expected relative location of parts inside the objects based on their appearance. We achieve this with a new network module, called OffsetNet, that efficiently predicts a variable number of part locations within a given object. Our model incorporates all these cues to detect parts in the context of their objects. This leads to considerably higher performance for the challenging task of part detection compared to using part appearance alone (+5 mAP on the PASCAL-Part dataset). We also compare to other part detection methods on both PASCAL-Part and CUB200-2011 datasets.



### Important New Developments in Arabographic Optical Character Recognition (OCR)
- **Arxiv ID**: http://arxiv.org/abs/1703.09550v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DL
- **Links**: [PDF](http://arxiv.org/pdf/1703.09550v1)
- **Published**: 2017-03-28 12:42:58+00:00
- **Updated**: 2017-03-28 12:42:58+00:00
- **Authors**: Maxim Romanov, Matthew Thomas Miller, Sarah Bowen Savant, Benjamin Kiessling
- **Comment**: None
- **Journal**: None
- **Summary**: The OpenITI team has achieved Optical Character Recognition (OCR) accuracy rates for classical Arabic-script texts in the high nineties. These numbers are based on our tests of seven different Arabic-script texts of varying quality and typefaces, totaling over 7,000 lines. These accuracy rates not only represent a distinct improvement over the actual accuracy rates of the various proprietary OCR options for classical Arabic-script texts, but, equally important, they are produced using an open-source OCR software, thus enabling us to make this Arabic-script OCR technology freely available to the broader Islamic, Persian, and Arabic Studies communities.



### Lucid Data Dreaming for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1703.09554v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09554v5)
- **Published**: 2017-03-28 12:56:40+00:00
- **Updated**: 2019-03-13 19:55:04+00:00
- **Authors**: Anna Khoreva, Rodrigo Benenson, Eddy Ilg, Thomas Brox, Bernt Schiele
- **Comment**: Accepted in International Journal of Computer Vision (IJCV)
- **Journal**: None
- **Summary**: Convolutional networks reach top quality in pixel-level video object segmentation but require a large amount of training data (1k~100k) to deliver such results. We propose a new training strategy which achieves state-of-the-art results across three evaluation datasets while using 20x~1000x less annotated data than competing methods. Our approach is suitable for both single and multiple object segmentation. Instead of using large training sets hoping to generalize across domains, we generate in-domain training data using the provided annotation on the first frame of each video to synthesize ("lucid dream") plausible future video frames. In-domain per-video training data allows us to train high quality appearance- and motion-based models, as well as tune the post-processing stage. This approach allows to reach competitive results even when training from only a single annotated frame, without ImageNet pre-training. Our results indicate that using a larger training set is not automatically better, and that for the video object segmentation task a smaller training set that is closer to the target domain is more effective. This changes the mindset regarding how many training samples and general "objectness" knowledge are required for the video object segmentation task.



### Learning and Refining of Privileged Information-based RNNs for Action Recognition from Depth Sequences
- **Arxiv ID**: http://arxiv.org/abs/1703.09625v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09625v4)
- **Published**: 2017-03-28 15:08:37+00:00
- **Updated**: 2017-08-08 11:49:02+00:00
- **Authors**: Zhiyuan Shi, Tae-Kyun Kim
- **Comment**: conference cvpr 2017
- **Journal**: None
- **Summary**: Existing RNN-based approaches for action recognition from depth sequences require either skeleton joints or hand-crafted depth features as inputs. An end-to-end manner, mapping from raw depth maps to action classes, is non-trivial to design due to the fact that: 1) single channel map lacks texture thus weakens the discriminative power; 2) relatively small set of depth training data. To address these challenges, we propose to learn an RNN driven by privileged information (PI) in three-steps: An encoder is pre-trained to learn a joint embedding of depth appearance and PI (i.e. skeleton joints). The learned embedding layers are then tuned in the learning step, aiming to optimize the network by exploiting PI in a form of multi-task loss. However, exploiting PI as a secondary task provides little help to improve the performance of a primary task (i.e. classification) due to the gap between them. Finally, a bridging matrix is defined to connect two tasks by discovering latent PI in the refining step. Our PI-based classification loss maintains a consistency between latent PI and predicted distribution. The latent PI and network are iteratively estimated and updated in an expectation-maximization procedure. The proposed learning process provides greater discriminative power to model subtle depth difference, while helping avoid overfitting the scarcer training data. Our experiments show significant performance gains over state-of-the-art methods on three public benchmark datasets and our newly collected Blanket dataset.



### An Analysis of Visual Question Answering Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1703.09684v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1703.09684v2)
- **Published**: 2017-03-28 17:48:07+00:00
- **Updated**: 2017-09-13 18:56:45+00:00
- **Authors**: Kushal Kafle, Christopher Kanan
- **Comment**: To appear in ICCV 2017. Visit http://kushalkafle.com/projects/tdiuc
  to download the dataset
- **Journal**: None
- **Summary**: In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way algorithms are evaluated on them. As a result, evaluation scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods. In this paper, we analyze existing VQA algorithms using a new dataset. It contains over 1.6 million questions organized into 12 different categories. We also introduce questions that are meaningless for a given image to force a VQA system to reason about image content. We propose new evaluation schemes that compensate for over-represented question-types and make it easier to study the strengths and weaknesses of algorithms. We analyze the performance of both baseline and state-of-the-art VQA models, including multi-modal compact bilinear pooling (MCB), neural module networks, and recurrent answering units. Our experiments establish how attention helps certain categories more than others, determine which models work better than others, and explain how simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.



### Efficient Two-Dimensional Sparse Coding Using Tensor-Linear Combination
- **Arxiv ID**: http://arxiv.org/abs/1703.09690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09690v1)
- **Published**: 2017-03-28 17:54:39+00:00
- **Updated**: 2017-03-28 17:54:39+00:00
- **Authors**: Fei Jiang, Xiao-Yang Liu, Hongtao Lu, Ruimin Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse coding (SC) is an automatic feature extraction and selection technique that is widely used in unsupervised learning. However, conventional SC vectorizes the input images, which breaks apart the local proximity of pixels and destructs the elementary object structures of images. In this paper, we propose a novel two-dimensional sparse coding (2DSC) scheme that represents the input images as the tensor-linear combinations under a novel algebraic framework. 2DSC learns much more concise dictionaries because it uses the circular convolution operator, since the shifted versions of atoms learned by conventional SC are treated as the same ones. We apply 2DSC to natural images and demonstrate that 2DSC returns meaningful dictionaries for large patches. Moreover, for mutli-spectral images denoising, the proposed 2DSC reduces computational costs with competitive performance in comparison with the state-of-the-art algorithms.



### Semi and Weakly Supervised Semantic Segmentation Using Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1703.09695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09695v1)
- **Published**: 2017-03-28 17:57:21+00:00
- **Updated**: 2017-03-28 17:57:21+00:00
- **Authors**: Nasim Souly, Concetto Spampinato, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation has been a long standing challenging task in computer vision. It aims at assigning a label to each image pixel and needs significant number of pixellevel annotated data, which is often unavailable. To address this lack, in this paper, we leverage, on one hand, massive amount of available unlabeled or weakly labeled data, and on the other hand, non-real images created through Generative Adversarial Networks. In particular, we propose a semi-supervised framework ,based on Generative Adversarial Networks (GANs), which consists of a generator network to provide extra training examples to a multi-class classifier, acting as discriminator in the GAN framework, that assigns sample a label y from the K possible classes or marks it as a fake sample (extra class). The underlying idea is that adding large fake visual data forces real samples to be close in the feature space, enabling a bottom-up clustering process, which, in turn, improves multiclass pixel classification. To ensure higher quality of generated images for GANs with consequent improved pixel classification, we extend the above framework by adding weakly annotated data, i.e., we provide class level information to the generator. We tested our approaches on several challenging benchmarking visual datasets, i.e. PASCAL, SiftFLow, Stanford and CamVid, achieving competitive performance also compared to state-of-the-art semantic segmentation method



### An Epipolar Line from a Single Pixel
- **Arxiv ID**: http://arxiv.org/abs/1703.09725v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09725v3)
- **Published**: 2017-03-28 18:01:05+00:00
- **Updated**: 2018-12-15 17:48:59+00:00
- **Authors**: Tavi Halperin, Michael Werman
- **Comment**: WACV 2018
- **Journal**: None
- **Summary**: Computing the epipolar geometry from feature points between cameras with very different viewpoints is often error prone, as an object's appearance can vary greatly between images. For such cases, it has been shown that using motion extracted from video can achieve much better results than using a static image. This paper extends these earlier works based on the scene dynamics. In this paper we propose a new method to compute the epipolar geometry from a video stream, by exploiting the following observation: For a pixel p in Image A, all pixels corresponding to p in Image B are on the same epipolar line. Equivalently, the image of the line going through camera A's center and p is an epipolar line in B. Therefore, when cameras A and B are synchronized, the momentary images of two objects projecting to the same pixel, p, in camera A at times t1 and t2, lie on an epipolar line in camera B. Based on this observation we achieve fast and precise computation of epipolar lines. Calibrating cameras based on our method of finding epipolar lines is much faster and more robust than previous methods.



### Feature Analysis and Selection for Training an End-to-End Autonomous Vehicle Controller Using the Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1703.09744v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1703.09744v1)
- **Published**: 2017-03-28 18:52:38+00:00
- **Updated**: 2017-03-28 18:52:38+00:00
- **Authors**: Shun Yang, Wenshuo Wang, Chang Liu, Kevin Deng, J. Karl Hedrick
- **Comment**: 6 pages, 11 figures, 3 tables, accepted by 2017 IEEE Intelligent
  Vehicles Symposium
- **Journal**: None
- **Summary**: Deep learning-based approaches have been widely used for training controllers for autonomous vehicles due to their powerful ability to approximate nonlinear functions or policies. However, the training process usually requires large labeled data sets and takes a lot of time. In this paper, we analyze the influences of features on the performance of controllers trained using the convolutional neural networks (CNNs), which gives a guideline of feature selection to reduce computation cost. We collect a large set of data using The Open Racing Car Simulator (TORCS) and classify the image features into three categories (sky-related, roadside-related, and road-related features).We then design two experimental frameworks to investigate the importance of each single feature for training a CNN controller.The first framework uses the training data with all three features included to train a controller, which is then tested with data that has one feature removed to evaluate the feature's effects. The second framework is trained with the data that has one feature excluded, while all three features are included in the test data. Different driving scenarios are selected to test and analyze the trained controllers using the two experimental frameworks. The experiment results show that (1) the road-related features are indispensable for training the controller, (2) the roadside-related features are useful to improve the generalizability of the controller to scenarios with complicated roadside information, and (3) the sky-related features have limited contribution to train an end-to-end autonomous vehicle controller.



### Coordinating Filters for Faster Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.09746v3
- **DOI**: None
- **Categories**: **cs.CV**, I.2.6; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1703.09746v3)
- **Published**: 2017-03-28 18:55:05+00:00
- **Updated**: 2017-07-25 17:54:31+00:00
- **Authors**: Wei Wen, Cong Xu, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: Very large-scale Deep Neural Networks (DNNs) have achieved remarkable successes in a large variety of computer vision tasks. However, the high computation intensity of DNNs makes it challenging to deploy these models on resource-limited systems. Some studies used low-rank approaches that approximate the filters by low-rank basis to accelerate the testing. Those works directly decomposed the pre-trained DNNs by Low-Rank Approximations (LRA). How to train DNNs toward lower-rank space for more efficient DNNs, however, remains as an open area. To solve the issue, in this work, we propose Force Regularization, which uses attractive forces to enforce filters so as to coordinate more weight information into lower-rank space. We mathematically and empirically verify that after applying our technique, standard LRA methods can reconstruct filters using much lower basis and thus result in faster DNNs. The effectiveness of our approach is comprehensively evaluated in ResNets, AlexNet, and GoogLeNet. In AlexNet, for example, Force Regularization gains 2x speedup on modern GPU without accuracy loss and 4.05x speedup on CPU by paying small accuracy degradation. Moreover, Force Regularization better initializes the low-rank DNNs such that the fine-tuning can converge faster toward higher accuracy. The obtained lower-rank DNNs can be further sparsified, proving that Force Regularization can be integrated with state-of-the-art sparsity-based acceleration methods. Source code is available in https://github.com/wenwei202/caffe



### Deep 6-DOF Tracking
- **Arxiv ID**: http://arxiv.org/abs/1703.09771v2
- **DOI**: 10.1109/TVCG.2017.2734599
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09771v2)
- **Published**: 2017-03-28 19:55:19+00:00
- **Updated**: 2017-08-15 19:45:22+00:00
- **Authors**: Mathieu Garon, Jean-François Lalonde
- **Comment**: 9 pages, 9 figures, ISMAR 2017, TVCG special edition Website:
  http://vision.gel.ulaval.ca/~jflalonde/projects/deepTracking/index.html
- **Journal**: IEEE Transactions on Visualization and Computer Graphics 2017
- **Summary**: We present a temporal 6-DOF tracking method which leverages deep learning to achieve state-of-the-art performance on challenging datasets of real world capture. Our method is both more accurate and more robust to occlusions than the existing best performing approaches while maintaining real-time performance. To assess its efficacy, we evaluate our approach on several challenging RGBD sequences of real objects in a variety of conditions. Notably, we systematically evaluate robustness to occlusions through a series of sequences where the object to be tracked is increasingly occluded. Finally, our approach is purely data-driven and does not require any hand-designed features: robust tracking is automatically learned from data.



### Towards Automatic Learning of Procedures from Web Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/1703.09788v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.09788v3)
- **Published**: 2017-03-28 20:28:52+00:00
- **Updated**: 2017-11-21 20:37:43+00:00
- **Authors**: Luowei Zhou, Chenliang Xu, Jason J. Corso
- **Comment**: AAAI 2018 Camera-ready version. See http://youcook2.eecs.umich.edu
  for YouCook2 dataset
- **Journal**: None
- **Summary**: The potential for agents, whether embodied or software, to learn by observing other agents performing procedures involving objects and actions is rich. Current research on automatic procedure learning heavily relies on action labels or video subtitles, even during the evaluation phase, which makes them infeasible in real-world scenarios. This leads to our question: can the human-consensus structure of a procedure be learned from a large set of long, unconstrained videos (e.g., instructional videos from YouTube) with only visual evidence? To answer this question, we introduce the problem of procedure segmentation--to segment a video procedure into category-independent procedure segments. Given that no large-scale dataset is available for this problem, we collect a large-scale procedure segmentation dataset with procedure segments temporally localized and described; we use cooking videos and name the dataset YouCook2. We propose a segment-level recurrent network for generating procedure segments by modeling the dependencies across segments. The generated segments can be used as pre-processing for other tasks, such as dense video captioning and event parsing. We show in our experiments that the proposed model outperforms competitive baselines in procedure segmentation.



### Theory II: Landscape of the Empirical Risk in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.09833v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1703.09833v2)
- **Published**: 2017-03-28 22:47:04+00:00
- **Updated**: 2017-06-22 09:33:35+00:00
- **Authors**: Qianli Liao, Tomaso Poggio
- **Comment**: Merged figures to make the main text more compact. Moved some similar
  figures to the appendix
- **Journal**: None
- **Summary**: Previous theoretical work on deep learning and neural network optimization tend to focus on avoiding saddle points and local minima. However, the practical observation is that, at least in the case of the most successful Deep Convolutional Neural Networks (DCNNs), practitioners can always increase the network size to fit the training data (an extreme example would be [1]). The most successful DCNNs such as VGG and ResNets are best used with a degree of "overparametrization". In this work, we characterize with a mix of theory and experiments, the landscape of the empirical risk of overparametrized DCNNs. We first prove in the regression framework the existence of a large number of degenerate global minimizers with zero empirical error (modulo inconsistent equations). The argument that relies on the use of Bezout theorem is rigorous when the RELUs are replaced by a polynomial nonlinearity (which empirically works as well). As described in our Theory III [2] paper, the same minimizers are degenerate and thus very likely to be found by SGD that will furthermore select with higher probability the most robust zero-minimizer. We further experimentally explored and visualized the landscape of empirical risk of a DCNN on CIFAR-10 during the entire training process and especially the global minima. Finally, based on our theoretical and experimental results, we propose an intuitive model of the landscape of DCNN's empirical loss surface, which might not be as complicated as people commonly believe.



