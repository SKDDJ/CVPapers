# Arxiv Papers in cs.CV on 2017-03-23
### Single Image Super-resolution via a Lightweight Residual Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1703.08173v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08173v2)
- **Published**: 2017-03-23 01:51:14+00:00
- **Updated**: 2017-12-06 13:43:45+00:00
- **Authors**: Yudong Liang, Ze Yang, Kai Zhang, Yihui He, Jinjun Wang, Nanning Zheng
- **Comment**: Extentions of mmm 2017 paper
- **Journal**: None
- **Summary**: Recent years have witnessed great success of convolutional neural network (CNN) for various problems both in low and high level visions. Especially noteworthy is the residual network which was originally proposed to handle high-level vision problems and enjoys several merits. This paper aims to extend the merits of residual network, such as skip connection induced fast training, for a typical low-level vision problem, i.e., single image super-resolution. In general, the two main challenges of existing deep CNN for supper-resolution lie in the gradient exploding/vanishing problem and large numbers of parameters or computational cost as CNN goes deeper. Correspondingly, the skip connections or identity mapping shortcuts are utilized to avoid gradient exploding/vanishing problem. In addition, the skip connections have naturally centered the activation which led to better performance. To tackle with the second problem, a lightweight CNN architecture which has carefully designed width, depth and skip connections was proposed. In particular, a strategy of gradually varying the shape of network has been proposed for residual network. Different residual architectures for image super-resolution have also been compared. Experimental results have demonstrated that the proposed CNN model can not only achieve state-of-the-art PSNR and SSIM results for single image super-resolution but also produce visually pleasant results. This paper has extended the mmm 2017 oral conference paper with a considerable new analyses and more experiments especially from the perspective of centering activations and ensemble behaviors of residual network.



### Bidirectional-Convolutional LSTM Based Spectral-Spatial Feature Learning for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1703.07910v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07910v1)
- **Published**: 2017-03-23 02:50:32+00:00
- **Updated**: 2017-03-23 02:50:32+00:00
- **Authors**: Qingshan Liu, Feng Zhou, Renlong Hang, Xiaotong Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel deep learning framework named bidirectional-convolutional long short term memory (Bi-CLSTM) network to automatically learn the spectral-spatial feature from hyperspectral images (HSIs). In the network, the issue of spectral feature extraction is considered as a sequence learning problem, and a recurrent connection operator across the spectral domain is used to address it. Meanwhile, inspired from the widely used convolutional neural network (CNN), a convolution operator across the spatial domain is incorporated into the network to extract the spatial feature. Besides, to sufficiently capture the spectral information, a bidirectional recurrent connection is proposed. In the classification phase, the learned features are concatenated into a vector and fed to a softmax classifier via a fully-connected operator. To validate the effectiveness of the proposed Bi-CLSTM framework, we compare it with several state-of-the-art methods, including the CNN framework, on three widely used HSIs. The obtained results show that Bi-CLSTM can improve the classification performance as compared to other methods.



### Perspective: Energy Landscapes for Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.07915v1
- **DOI**: 10.1039/C7CP01108C
- **Categories**: **stat.ML**, cond-mat.dis-nn, cs.CV, cs.LG, hep-th
- **Links**: [PDF](http://arxiv.org/pdf/1703.07915v1)
- **Published**: 2017-03-23 03:17:14+00:00
- **Updated**: 2017-03-23 03:17:14+00:00
- **Authors**: Andrew J. Ballard, Ritankar Das, Stefano Martiniani, Dhagash Mehta, Levent Sagun, Jacob D. Stevenson, David J. Wales
- **Comment**: 41 pages, 25 figures. Accepted for publication in Physical Chemistry
  Chemical Physics, 2017
- **Journal**: None
- **Summary**: Machine learning techniques are being increasingly used as flexible non-linear fitting and prediction tools in the physical sciences. Fitting functions that exhibit multiple solutions as local minima can be analysed in terms of the corresponding machine learning landscape. Methods to explore and visualise molecular potential energy landscapes can be applied to these machine learning landscapes to gain new insight into the solution space involved in training and the nature of the corresponding predictions. In particular, we can define quantities analogous to molecular structure, thermodynamics, and kinetics, and relate these emergent properties to the structure of the underlying landscape. This Perspective aims to describe these analogies with examples from recent applications, and suggest avenues for new interdisciplinary research.



### Changing Fashion Cultures
- **Arxiv ID**: http://arxiv.org/abs/1703.07920v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1703.07920v1)
- **Published**: 2017-03-23 03:48:08+00:00
- **Updated**: 2017-03-23 03:48:08+00:00
- **Authors**: Kaori Abe, Teppei Suzuki, Shunya Ueta, Akio Nakamura, Yutaka Satoh, Hirokatsu Kataoka
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: The paper presents a novel concept that analyzes and visualizes worldwide fashion trends. Our goal is to reveal cutting-edge fashion trends without displaying an ordinary fashion style. To achieve the fashion-based analysis, we created a new fashion culture database (FCDB), which consists of 76 million geo-tagged images in 16 cosmopolitan cities. By grasping a fashion trend of mixed fashion styles,the paper also proposes an unsupervised fashion trend descriptor (FTD) using a fashion descriptor, a codeword vetor, and temporal analysis. To unveil fashion trends in the FCDB, the temporal analysis in FTD effectively emphasizes consecutive features between two different times. In experiments, we clearly show the analysis of fashion trends and fashion-based city similarity. As the result of large-scale data collection and an unsupervised analyzer, the proposed approach achieves world-level fashion visualization in a time series. The code, model, and FCDB will be publicly available after the construction of the project page.



### Self corrective Perturbations for Semantic Segmentation and Classification
- **Arxiv ID**: http://arxiv.org/abs/1703.07928v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.07928v2)
- **Published**: 2017-03-23 04:25:48+00:00
- **Updated**: 2017-08-03 15:42:06+00:00
- **Authors**: Swami Sankaranarayanan, Arpit Jain, Ser Nam Lim
- **Comment**: Accepted to ICCV 2017
- **Journal**: None
- **Summary**: Convolutional Neural Networks have been a subject of great importance over the past decade and great strides have been made in their utility for producing state of the art performance in many computer vision problems. However, the behavior of deep networks is yet to be fully understood and is still an active area of research. In this work, we present an intriguing behavior: pre-trained CNNs can be made to improve their predictions by structurally perturbing the input. We observe that these perturbations - referred as Guided Perturbations - enable a trained network to improve its prediction performance without any learning or change in network weights. We perform various ablative experiments to understand how these perturbations affect the local context and feature representations. Furthermore, we demonstrate that this idea can improve performance of several existing approaches on semantic segmentation and scene labeling tasks on the PASCAL VOC dataset and supervised classification tasks on MNIST and CIFAR10 datasets.



### Planar Object Tracking in the Wild: A Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1703.07938v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07938v2)
- **Published**: 2017-03-23 05:21:24+00:00
- **Updated**: 2018-05-22 06:54:43+00:00
- **Authors**: Pengpeng Liang, Yifan Wu, Hu Lu, Liming Wang, Chunyuan Liao, Haibin Ling
- **Comment**: Accepted by ICRA 2018
- **Journal**: None
- **Summary**: Planar object tracking is an actively studied problem in vision-based robotic applications. While several benchmarks have been constructed for evaluating state-of-the-art algorithms, there is a lack of video sequences captured in the wild rather than in constrained laboratory environment. In this paper, we present a carefully designed planar object tracking benchmark containing 210 videos of 30 planar objects sampled in the natural environment. In particular, for each object, we shoot seven videos involving various challenging factors, namely scale change, rotation, perspective distortion, motion blur, occlusion, out-of-view, and unconstrained. The ground truth is carefully annotated semi-manually to ensure the quality. Moreover, eleven state-of-the-art algorithms are evaluated on the benchmark using two evaluation metrics, with detailed analysis provided for the evaluation results. We expect the proposed benchmark to benefit future studies on planar object tracking.



### Recurrent Multimodal Interaction for Referring Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1703.07939v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07939v2)
- **Published**: 2017-03-23 05:22:22+00:00
- **Updated**: 2017-08-04 21:53:15+00:00
- **Authors**: Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Alan Yuille
- **Comment**: To appear in ICCV 2017. See http://www.cs.jhu.edu/~cxliu/ for code
  and supplementary material
- **Journal**: None
- **Summary**: In this paper we are interested in the problem of image segmentation given natural language descriptions, i.e. referring expressions. Existing works tackle this problem by first modeling images and sentences independently and then segment images by combining these two types of representations. We argue that learning word-to-image interaction is more native in the sense of jointly modeling two modalities for the image segmentation task, and we propose convolutional multimodal LSTM to encode the sequential interactions between individual words, visual information, and spatial information. We show that our proposed model outperforms the baseline model on benchmark datasets. In addition, we analyze the intermediate output of the proposed multimodal LSTM approach and empirically explain how this approach enforces a more effective word-to-image interaction.



### Robust SfM with Little Image Overlap
- **Arxiv ID**: http://arxiv.org/abs/1703.07957v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07957v2)
- **Published**: 2017-03-23 07:52:31+00:00
- **Updated**: 2017-03-28 09:57:56+00:00
- **Authors**: Yohann Salaun, Renaud Marlet, Pascal Monasse
- **Comment**: None
- **Journal**: None
- **Summary**: Usual Structure-from-Motion (SfM) techniques require at least trifocal overlaps to calibrate cameras and reconstruct a scene. We consider here scenarios of reduced image sets with little overlap, possibly as low as two images at most seeing the same part of the scene. We propose a new method, based on line coplanarity hypotheses, for estimating the relative scale of two independent bifocal calibrations sharing a camera, without the need of any trifocal information or Manhattan-world assumption. We use it to compute SfM in a chain of up-to-scale relative motions. For accuracy, we however also make use of trifocal information for line and/or point features, when present, relaxing usual trifocal constraints. For robustness to wrong assumptions and mismatches, we embed all constraints in a parameterless RANSAC-like approach. Experiments show that we can calibrate datasets that previously could not, and that this wider applicability does not come at the cost of inaccuracy.



### Image-based Localization using Hourglass Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.07971v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07971v3)
- **Published**: 2017-03-23 09:06:13+00:00
- **Updated**: 2017-08-24 06:10:26+00:00
- **Authors**: Iaroslav Melekhov, Juha Ylioinas, Juho Kannala, Esa Rahtu
- **Comment**: Camera-ready version for ICCVW 2017 (fixed glitches in abstract)
- **Journal**: None
- **Summary**: In this paper, we propose an encoder-decoder convolutional neural network (CNN) architecture for estimating camera pose (orientation and location) from a single RGB-image. The architecture has a hourglass shape consisting of a chain of convolution and up-convolution layers followed by a regression part. The up-convolution layers are introduced to preserve the fine-grained information of the input image. Following the common practice, we train our model in end-to-end manner utilizing transfer learning from large scale classification data. The experiments demonstrate the performance of the approach on data exhibiting different lighting conditions, reflections, and motion blur. The results indicate a clear improvement over the previous state-of-the-art even when compared to methods that utilize sequence of test frames instead of a single frame.



### Discriminatively Boosted Image Clustering with Fully Convolutional Auto-Encoders
- **Arxiv ID**: http://arxiv.org/abs/1703.07980v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.07980v1)
- **Published**: 2017-03-23 09:49:37+00:00
- **Updated**: 2017-03-23 09:49:37+00:00
- **Authors**: Fengfu Li, Hong Qiao, Bo Zhang, Xuanyang Xi
- **Comment**: 27 pages
- **Journal**: None
- **Summary**: Traditional image clustering methods take a two-step approach, feature learning and clustering, sequentially. However, recent research results demonstrated that combining the separated phases in a unified framework and training them jointly can achieve a better performance. In this paper, we first introduce fully convolutional auto-encoders for image feature learning and then propose a unified clustering framework to learn image representations and cluster centers jointly based on a fully convolutional auto-encoder and soft $k$-means scores. At initial stages of the learning procedure, the representations extracted from the auto-encoder may not be very discriminative for latter clustering. We address this issue by adopting a boosted discriminative distribution, where high score assignments are highlighted and low score ones are de-emphasized. With the gradually boosted discrimination, clustering assignment scores are discriminated and cluster purities are enlarged. Experiments on several vision benchmark datasets show that our methods can achieve a state-of-the-art performance.



### Weakly Supervised Object Localization Using Things and Stuff Transfer
- **Arxiv ID**: http://arxiv.org/abs/1703.08000v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08000v2)
- **Published**: 2017-03-23 11:01:27+00:00
- **Updated**: 2017-08-07 14:00:41+00:00
- **Authors**: Miaojing Shi, Holger Caesar, Vittorio Ferrari
- **Comment**: ICCV 2017 camera-ready including supplementary material
- **Journal**: None
- **Summary**: We propose to help weakly supervised object localization for classes where location annotations are not available, by transferring things and stuff knowledge from a source set with available annotations. The source and target classes might share similar appearance (e.g. bear fur is similar to cat fur) or appear against similar background (e.g. horse and sheep appear against grass). To exploit this, we acquire three types of knowledge from the source set: a segmentation model trained on both thing and stuff classes; similarity relations between target and source classes; and co-occurrence relations between thing and stuff classes in the source. The segmentation model is used to generate thing and stuff segmentation maps on a target image, while the class similarity and co-occurrence knowledge help refining them. We then incorporate these maps as new cues into a multiple instance learning framework (MIL), propagating the transferred knowledge from the pixel level to the object proposal level. In extensive experiments, we conduct our transfer from the PASCAL Context dataset (source) to the ILSVRC, COCO and PASCAL VOC 2007 datasets (targets). We evaluate our transfer across widely different thing classes, including some that are not similar in appearance, but appear against similar background. The results demonstrate significant improvement over standard MIL, and we outperform the state-of-the-art in the transfer setting.



### Nonlinear Spectral Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/1703.08001v1
- **DOI**: None
- **Categories**: **cs.CV**, math.NA, 35P30, 62H35, 65M70, 94A08, G.1.3; G.1.6; G.1.8; I.4.0; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/1703.08001v1)
- **Published**: 2017-03-23 11:02:42+00:00
- **Updated**: 2017-03-23 11:02:42+00:00
- **Authors**: Martin Benning, Michael Möller, Raz Z. Nossek, Martin Burger, Daniel Cremers, Guy Gilboa, Carola-Bibiane Schönlieb
- **Comment**: 13 pages, 9 figures, submitted to SSVM conference proceedings 2017
- **Journal**: None
- **Summary**: In this paper we demonstrate that the framework of nonlinear spectral decompositions based on total variation (TV) regularization is very well suited for image fusion as well as more general image manipulation tasks. The well-localized and edge-preserving spectral TV decomposition allows to select frequencies of a certain image to transfer particular features, such as wrinkles in a face, from one image to another. We illustrate the effectiveness of the proposed approach in several numerical experiments, including a comparison to the competing techniques of Poisson image editing, linear osmosis, wavelet fusion and Laplacian pyramid fusion. We conclude that the proposed spectral TV image decomposition framework is a valuable tool for semi- and fully-automatic image editing and fusion.



### Content-based similar document image retrieval using fusion of CNN features
- **Arxiv ID**: http://arxiv.org/abs/1703.08013v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.08013v3)
- **Published**: 2017-03-23 11:35:27+00:00
- **Updated**: 2017-09-01 00:34:52+00:00
- **Authors**: Mao Tan, Si-Ping Yuan, Yong-Xin Su
- **Comment**: None
- **Journal**: None
- **Summary**: Rapid increase of digitized document give birth to high demand of document image retrieval. While conventional document image retrieval approaches depend on complex OCR-based text recognition and text similarity detection, this paper proposes a new content-based approach, in which more attention is paid to features extraction and fusion. In the proposed approach, multiple features of document images are extracted by different CNN models. After that, the extracted CNN features are reduced and fused into weighted average feature. Finally, the document images are ranked based on feature similarity to a provided query image. Experimental procedure is performed on a group of document images that transformed from academic papers, which contain both English and Chinese document, the results show that the proposed approach has good ability to retrieve document images with similar text content, and the fusion of CNN features can effectively improve the retrieval accuracy.



### Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs
- **Arxiv ID**: http://arxiv.org/abs/1703.08014v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1703.08014v2)
- **Published**: 2017-03-23 11:35:41+00:00
- **Updated**: 2017-03-24 08:24:07+00:00
- **Authors**: Timo von Marcard, Bodo Rosenhahn, Michael J. Black, Gerard Pons-Moll
- **Comment**: 12 pages, Accepted at Eurographics 2017
- **Journal**: None
- **Summary**: We address the problem of making human motion capture in the wild more practical by using a small set of inertial sensors attached to the body. Since the problem is heavily under-constrained, previous methods either use a large number of sensors, which is intrusive, or they require additional video input. We take a different approach and constrain the problem by: (i) making use of a realistic statistical body model that includes anthropometric constraints and (ii) using a joint optimization framework to fit the model to orientation and acceleration measurements over multiple frames. The resulting tracker Sparse Inertial Poser (SIP) enables 3D human pose estimation using only 6 sensors (attached to the wrists, lower legs, back and head) and works for arbitrary human motions. Experiments on the recently released TNT15 dataset show that, using the same number of sensors, SIP achieves higher accuracy than the dataset baseline without using any video data. We further demonstrate the effectiveness of SIP on newly recorded challenging motions in outdoor scenarios such as climbing or jumping over a wall.



### Saliency-guided video classification via adaptively weighted learning
- **Arxiv ID**: http://arxiv.org/abs/1703.08025v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08025v2)
- **Published**: 2017-03-23 12:02:21+00:00
- **Updated**: 2017-03-26 02:43:13+00:00
- **Authors**: Yunzhen Zhao, Yuxin Peng
- **Comment**: 6 pages, 1 figure, accepted by ICME 2017
- **Journal**: None
- **Summary**: Video classification is productive in many practical applications, and the recent deep learning has greatly improved its accuracy. However, existing works often model video frames indiscriminately, but from the view of motion, video frames can be decomposed into salient and non-salient areas naturally. Salient and non-salient areas should be modeled with different networks, for the former present both appearance and motion information, and the latter present static background information. To address this problem, in this paper, video saliency is predicted by optical flow without supervision firstly. Then two streams of 3D CNN are trained individually for raw frames and optical flow on salient areas, and another 2D CNN is trained for raw frames on non-salient areas. For the reason that these three streams play different roles for each class, the weights of each stream are adaptively learned for each class. Experimental results show that saliency-guided modeling and adaptively weighted learning can reinforce each other, and we achieve the state-of-the-art results.



### Generative Adversarial Residual Pairwise Networks for One Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.08033v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1703.08033v1)
- **Published**: 2017-03-23 12:19:09+00:00
- **Updated**: 2017-03-23 12:19:09+00:00
- **Authors**: Akshay Mehrotra, Ambedkar Dukkipati
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks achieve unprecedented performance levels over many tasks and scale well with large quantities of data, but performance in the low-data regime and tasks like one shot learning still lags behind. While recent work suggests many hypotheses from better optimization to more complicated network structures, in this work we hypothesize that having a learnable and more expressive similarity objective is an essential missing component. Towards overcoming that, we propose a network design inspired by deep residual networks that allows the efficient computation of this more expressive pairwise similarity objective. Further, we argue that regularization is key in learning with small amounts of data, and propose an additional generator network based on the Generative Adversarial Networks where the discriminator is our residual pairwise network. This provides a strong regularizer by leveraging the generated data samples. The proposed model can generate plausible variations of exemplars over unseen classes and outperforms strong discriminative baselines for few shot classification tasks. Notably, our residual pairwise network design outperforms previous state-of-theart on the challenging mini-Imagenet dataset for one shot learning by getting over 55% accuracy for the 5-way classification task over unseen classes.



### Is Second-order Information Helpful for Large-scale Visual Recognition?
- **Arxiv ID**: http://arxiv.org/abs/1703.08050v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08050v3)
- **Published**: 2017-03-23 12:55:34+00:00
- **Updated**: 2018-04-01 22:24:31+00:00
- **Authors**: Peihua Li, Jiangtao Xie, Qilong Wang, Wangmeng Zuo
- **Comment**: accepted to ICCV 2017
- **Journal**: None
- **Summary**: By stacking layers of convolution and nonlinearity, convolutional networks (ConvNets) effectively learn from low-level to high-level features and discriminative representations. Since the end goal of large-scale recognition is to delineate complex boundaries of thousands of classes, adequate exploration of feature distributions is important for realizing full potentials of ConvNets. However, state-of-the-art works concentrate only on deeper or wider architecture design, while rarely exploring feature statistics higher than first-order. We take a step towards addressing this problem. Our method consists in covariance pooling, instead of the most commonly used first-order pooling, of high-level convolutional features. The main challenges involved are robust covariance estimation given a small sample of large-dimensional features and usage of the manifold structure of covariance matrices. To address these challenges, we present a Matrix Power Normalized Covariance (MPN-COV) method. We develop forward and backward propagation formulas regarding the nonlinear matrix functions such that MPN-COV can be trained end-to-end. In addition, we analyze both qualitatively and quantitatively its advantage over the well-known Log-Euclidean metric. On the ImageNet 2012 validation set, by combining MPN-COV we achieve over 4%, 3% and 2.5% gains for AlexNet, VGG-M and VGG-16, respectively; integration of MPN-COV into 50-layer ResNet outperforms ResNet-101 and is comparable to ResNet-152. The source code will be available on the project page: http://www.peihuali.org/MPN-COV



### A Bag-of-Words Equivalent Recurrent Neural Network for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.08089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08089v1)
- **Published**: 2017-03-23 14:46:46+00:00
- **Updated**: 2017-03-23 14:46:46+00:00
- **Authors**: Alexander Richard, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: The traditional bag-of-words approach has found a wide range of applications in computer vision. The standard pipeline consists of a generation of a visual vocabulary, a quantization of the features into histograms of visual words, and a classification step for which usually a support vector machine in combination with a non-linear kernel is used. Given large amounts of data, however, the model suffers from a lack of discriminative power. This applies particularly for action recognition, where the vast amount of video features needs to be subsampled for unsupervised visual vocabulary generation. Moreover, the kernel computation can be very expensive on large datasets. In this work, we propose a recurrent neural network that is equivalent to the traditional bag-of-words approach but enables for the application of discriminative training. The model further allows to incorporate the kernel computation into the neural network directly, solving the complexity issue and allowing to represent the complete classification system within a single network. We evaluate our method on four recent action recognition benchmarks and show that the conventional model as well as sparse coding methods are outperformed.



### Quality Resilient Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.08119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08119v1)
- **Published**: 2017-03-23 15:56:33+00:00
- **Updated**: 2017-03-23 15:56:33+00:00
- **Authors**: Samuel Dodge, Lina Karam
- **Comment**: None
- **Journal**: None
- **Summary**: We study deep neural networks for classification of images with quality distortions. We first show that networks fine-tuned on distorted data greatly outperform the original networks when tested on distorted data. However, fine-tuned networks perform poorly on quality distortions that they have not been trained for. We propose a mixture of experts ensemble method that is robust to different types of distortions. The "experts" in our model are trained on a particular type of distortion. The output of the model is a weighted sum of the expert models, where the weights are determined by a separate gating network. The gating network is trained to predict optimal weights for a particular distortion type and level. During testing, the network is blind to the distortion level and type, yet can still assign appropriate weights to the expert models. We additionally investigate weight sharing methods for the mixture model and show that improved performance can be achieved with a large reduction in the number of unique network parameters.



### Recurrent and Contextual Models for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1703.08120v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.08120v1)
- **Published**: 2017-03-23 15:57:23+00:00
- **Updated**: 2017-03-23 15:57:23+00:00
- **Authors**: Abhijit Sharang, Eric Lau
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a series of recurrent and contextual neural network models for multiple choice visual question answering on the Visual7W dataset. Motivated by divergent trends in model complexities in the literature, we explore the balance between model expressiveness and simplicity by studying incrementally more complex architectures. We start with LSTM-encoding of input questions and answers; build on this with context generation by LSTM-encodings of neural image and question representations and attention over images; and evaluate the diversity and predictive power of our models and the ensemble thereof. All models are evaluated against a simple baseline inspired by the current state-of-the-art, consisting of involving simple concatenation of bag-of-words and CNN representations for the text and images, respectively. Generally, we observe marked variation in image-reasoning performance between our models not obvious from their overall performance, as well as evidence of dataset bias. Our standalone models achieve accuracies up to $64.6\%$, while the ensemble of all models achieves the best accuracy of $66.67\%$, within $0.5\%$ of the current state-of-the-art for Visual7W.



### Weakly Supervised Action Learning with RNN based Fine-to-coarse Modeling
- **Arxiv ID**: http://arxiv.org/abs/1703.08132v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08132v3)
- **Published**: 2017-03-23 16:31:48+00:00
- **Updated**: 2017-10-09 10:10:07+00:00
- **Authors**: Alexander Richard, Hilde Kuehne, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach for weakly supervised learning of human actions. Given a set of videos and an ordered list of the occurring actions, the goal is to infer start and end frames of the related action classes within the video and to train the respective action classifiers without any need for hand labeled frame boundaries. To address this task, we propose a combination of a discriminative representation of subactions, modeled by a recurrent neural network, and a coarse probabilistic model to allow for a temporal alignment and inference over long sequences. While this system alone already generates good results, we show that the performance can be further improved by approximating the number of subactions to the characteristics of the different action classes. To this end, we adapt the number of subaction classes by iterating realignment and reestimation during training. The proposed system is evaluated on two benchmark datasets, the Breakfast and the Hollywood extended dataset, showing a competitive performance on various weak learning tasks such as temporal action segmentation and action alignment.



### Visually grounded learning of keyword prediction from untranscribed speech
- **Arxiv ID**: http://arxiv.org/abs/1703.08136v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.08136v2)
- **Published**: 2017-03-23 16:46:00+00:00
- **Updated**: 2017-05-25 20:49:15+00:00
- **Authors**: Herman Kamper, Shane Settle, Gregory Shakhnarovich, Karen Livescu
- **Comment**: 5 pages, 3 figures, 5 tables; small updates, added link to code;
  accepted to Interspeech 2017
- **Journal**: None
- **Summary**: During language acquisition, infants have the benefit of visual cues to ground spoken language. Robots similarly have access to audio and visual sensors. Recent work has shown that images and spoken captions can be mapped into a meaningful common space, allowing images to be retrieved using speech and vice versa. In this setting of images paired with untranscribed spoken captions, we consider whether computer vision systems can be used to obtain textual labels for the speech. Concretely, we use an image-to-words multi-label visual classifier to tag images with soft textual labels, and then train a neural network to map from the speech to these soft targets. We show that the resulting speech system is able to predict which words occur in an utterance---acting as a spoken bag-of-words classifier---without seeing any parallel speech and text. We find that the model often confuses semantically related words, e.g. "man" and "person", making it even more effective as a semantic keyword spotter.



### Effect of Super Resolution on High Dimensional Features for Unsupervised Face Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1704.01464v2
- **DOI**: 10.1109/AIPR.2017.8457967
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.01464v2)
- **Published**: 2017-03-23 19:58:27+00:00
- **Updated**: 2017-05-13 18:10:59+00:00
- **Authors**: Ahmed ElSayed, Ausif Mahmood, Tarek Sobh
- **Comment**: None
- **Journal**: None
- **Summary**: Majority of the face recognition algorithms use query faces captured from uncontrolled, in the wild, environment. Often caused by the cameras limited capabilities, it is common for these captured facial images to be blurred or low resolution. Super resolution algorithms are therefore crucial in improving the resolution of such images especially when the image size is small requiring enlargement. This paper aims to demonstrate the effect of one of the state-of-the-art algorithms in the field of image super resolution. To demonstrate the functionality of the algorithm, various before and after 3D face alignment cases are provided using the images from the Labeled Faces in the Wild (lfw). Resulting images are subject to testing on a closed set face recognition protocol using unsupervised algorithms with high dimension extracted features. The inclusion of super resolution algorithm resulted in significant improved recognition rate over recently reported results obtained from unsupervised algorithms.



### Semi-Automatic Segmentation and Ultrasonic Characterization of Solid Breast Lesions
- **Arxiv ID**: http://arxiv.org/abs/1703.08238v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.08238v1)
- **Published**: 2017-03-23 21:25:48+00:00
- **Updated**: 2017-03-23 21:25:48+00:00
- **Authors**: Mohammad Saad Billah, Tahmida Binte Mahmud
- **Comment**: None
- **Journal**: None
- **Summary**: Characterization of breast lesions is an essential prerequisite to detect breast cancer in an early stage. Automatic segmentation makes this categorization method robust by freeing it from subjectivity and human error. Both spectral and morphometric features are successfully used for differentiating between benign and malignant breast lesions. In this thesis, we used empirical mode decomposition method for semi-automatic segmentation. Sonographic features like ehcogenicity, heterogeneity, FNPA, margin definition, Hurst coefficient, compactness, roundness, aspect ratio, convexity, solidity, form factor were calculated to be used as our characterization parameters. All of these parameters did not give desired comparative results. But some of them namely echogenicity, heterogeneity, margin definition, aspect ratio and convexity gave good results and were used for characterization.



### On the Robustness of Convolutional Neural Networks to Internal Architecture and Weight Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1703.08245v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.08245v1)
- **Published**: 2017-03-23 22:25:05+00:00
- **Updated**: 2017-03-23 22:25:05+00:00
- **Authors**: Nicholas Cheney, Martin Schrimpf, Gabriel Kreiman
- **Comment**: under review at ICML 2017
- **Journal**: None
- **Summary**: Deep convolutional neural networks are generally regarded as robust function approximators. So far, this intuition is based on perturbations to external stimuli such as the images to be classified. Here we explore the robustness of convolutional neural networks to perturbations to the internal weights and architecture of the network itself. We show that convolutional networks are surprisingly robust to a number of internal perturbations in the higher convolutional layers but the bottom convolutional layers are much more fragile. For instance, Alexnet shows less than a 30% decrease in classification performance when randomly removing over 70% of weight connections in the top convolutional or dense layers but performance is almost at chance with the same perturbation in the first convolutional layer. Finally, we suggest further investigations which could continue to inform the robustness of convolutional networks to internal perturbations.



