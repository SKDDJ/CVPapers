# Arxiv Papers in cs.CV on 2017-03-07
### Distance Metric Learning using Graph Convolutional Networks: Application to Functional Brain Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.02161v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.02161v2)
- **Published**: 2017-03-07 00:49:27+00:00
- **Updated**: 2017-06-14 11:05:52+00:00
- **Authors**: Sofia Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Rajchl, Matthew Lee, Ben Glocker, Daniel Rueckert
- **Comment**: International Conference on Medical Image Computing and
  Computer-Assisted Interventions (MICCAI) 2017
- **Journal**: None
- **Summary**: Evaluating similarity between graphs is of major importance in several computer vision and pattern recognition problems, where graph representations are often used to model objects or interactions between elements. The choice of a distance or similarity metric is, however, not trivial and can be highly dependent on the application at hand. In this work, we propose a novel metric learning method to evaluate distance between graphs that leverages the power of convolutional neural networks, while exploiting concepts from spectral graph theory to allow these operations on irregular graphs. We demonstrate the potential of our method in the field of connectomics, where neuronal pathways or functional connections between brain regions are commonly modelled as graphs. In this problem, the definition of an appropriate graph similarity function is critical to unveil patterns of disruptions associated with certain brain disorders. Experimental results on the ABIDE dataset show that our method can learn a graph similarity metric tailored for a clinical application, improving the performance of a simple k-nn classifier by 11.9% compared to a traditional distance metric.



### Deep View Morphing
- **Arxiv ID**: http://arxiv.org/abs/1703.02168v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02168v1)
- **Published**: 2017-03-07 01:21:01+00:00
- **Updated**: 2017-03-07 01:21:01+00:00
- **Authors**: Dinghuang Ji, Junghyun Kwon, Max McFarland, Silvio Savarese
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: Recently, convolutional neural networks (CNN) have been successfully applied to view synthesis problems. However, such CNN-based methods can suffer from lack of texture details, shape distortions, or high computational complexity. In this paper, we propose a novel CNN architecture for view synthesis called "Deep View Morphing" that does not suffer from these issues. To synthesize a middle view of two input images, a rectification network first rectifies the two input images. An encoder-decoder network then generates dense correspondences between the rectified images and blending masks to predict the visibility of pixels of the rectified images in the middle view. A view morphing network finally synthesizes the middle view using the dense correspondences and blending masks. We experimentally show the proposed method significantly outperforms the state-of-the-art CNN-based view synthesis method.



### Sharing Residual Units Through Collective Tensor Factorization in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.02180v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02180v2)
- **Published**: 2017-03-07 02:20:57+00:00
- **Updated**: 2017-03-15 15:00:26+00:00
- **Authors**: Chen Yunpeng, Jin Xiaojie, Kang Bingyi, Feng Jiashi, Yan Shuicheng
- **Comment**: None
- **Journal**: None
- **Summary**: Residual units are wildly used for alleviating optimization difficulties when building deep neural networks. However, the performance gain does not well compensate the model size increase, indicating low parameter efficiency in these residual units. In this work, we first revisit the residual function in several variations of residual units and demonstrate that these residual functions can actually be explained with a unified framework based on generalized block term decomposition. Then, based on the new explanation, we propose a new architecture, Collective Residual Unit (CRU), which enhances the parameter efficiency of deep neural networks through collective tensor factorization. CRU enables knowledge sharing across different residual units using shared factors. Experimental results show that our proposed CRU Network demonstrates outstanding parameter efficiency, achieving comparable classification performance to ResNet-200 with the model size of ResNet-50. By building a deeper network using CRU, we can achieve state-of-the-art single model classification accuracy on ImageNet-1k and Places365-Standard benchmark datasets. (Code and trained models are available on GitHub)



### Using Deep Learning Method for Classification: A Proposed Algorithm for the ISIC 2017 Skin Lesion Classification Challenge
- **Arxiv ID**: http://arxiv.org/abs/1703.02182v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02182v2)
- **Published**: 2017-03-07 02:26:21+00:00
- **Updated**: 2017-03-10 08:17:47+00:00
- **Authors**: Wenhao Zhang, Liangcai Gao, Runtao Liu
- **Comment**: Skin Lesion Classification Challenge Competition, ISIC2017
- **Journal**: None
- **Summary**: Skin cancer, the most common human malignancy, is primarily diagnosed visually by physicians [1]. Classification with an automated method like CNN [2, 3] shows potential for challenging tasks [1]. By now, the deep convolutional neural networks are on par with human dermatologist [1]. This abstract is dedicated on developing a Deep Learning method for ISIC [5] 2017 Skin Lesion Detection Competition hosted at [6] to classify the dermatology pictures, which is aimed at improving the diagnostic accuracy rate and general level of the human health. The challenge falls into three sub-challenges, including Lesion Segmentation, Lesion Dermoscopic Feature Extraction and Lesion Classification. This project only participates in the Lesion Classification part. This algorithm is comprised of three steps: (1) original images preprocessing, (2) modelling the processed images using CNN [2, 3] in Caffe [4] framework, (3) predicting the test images and calculating the scores that represent the likelihood of corresponding classification. The models are built on the source images are using the Caffe [4] framework. The scores in prediction step are obtained by two different models from the source images.



### Indoor Localization Using Visible Light Via Fusion Of Multiple Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1703.02184v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.02184v2)
- **Published**: 2017-03-07 02:41:07+00:00
- **Updated**: 2017-12-20 06:44:32+00:00
- **Authors**: Xiansheng Guo, Sihua Shao, Nirwan Ansari, Abdallah Khreishah
- **Comment**: None
- **Journal**: None
- **Summary**: A multiple classifiers fusion localization technique using received signal strengths (RSSs) of visible light is proposed, in which the proposed system transmits different intensity modulated sinusoidal signals by LEDs and the signals received by a Photo Diode (PD) placed at various grid points. First, we obtain some {\emph{approximate}} received signal strengths (RSSs) fingerprints by capturing the peaks of power spectral density (PSD) of the received signals at each given grid point. Unlike the existing RSSs based algorithms, several representative machine learning approaches are adopted to train multiple classifiers based on these RSSs fingerprints. The multiple classifiers localization estimators outperform the classical RSS-based LED localization approaches in accuracy and robustness. To further improve the localization performance, two robust fusion localization algorithms, namely, grid independent least square (GI-LS) and grid dependent least square (GD-LS), are proposed to combine the outputs of these classifiers. We also use a singular value decomposition (SVD) based LS (LS-SVD) method to mitigate the numerical stability problem when the prediction matrix is singular. Experiments conducted on intensity modulated direct detection (IM/DD) systems have demonstrated the effectiveness of the proposed algorithms. The experimental results show that the probability of having mean square positioning error (MSPE) of less than 5cm achieved by GD-LS is improved by 93.03\% and 93.15\%, respectively, as compared to those by the RSS ratio (RSSR) and RSS matching methods with the FFT length of 2000.



### Indoor Localization by Fusing a Group of Fingerprints Based on Random Forests
- **Arxiv ID**: http://arxiv.org/abs/1703.02185v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.02185v1)
- **Published**: 2017-03-07 02:41:40+00:00
- **Updated**: 2017-03-07 02:41:40+00:00
- **Authors**: Xiansheng Guo, Nirwan Ansari, Huiyong Li
- **Comment**: arXiv admin note: text overlap with arXiv:1609.00661
- **Journal**: None
- **Summary**: Indoor localization based on SIngle Of Fingerprint (SIOF) is rather susceptible to the changing environment, multipath, and non-line-of-sight (NLOS) propagation. Building SIOF is also a very time-consuming process. Recently, we first proposed a GrOup Of Fingerprints (GOOF) to improve the localization accuracy and reduce the burden of building fingerprints. However, the main drawback is the timeliness. In this paper, we propose a novel localization framework by Fusing A Group Of fingerprinTs (FAGOT) based on random forests. In the offline phase, we first build a GOOF from different transformations of the received signals of multiple antennas. Then, we design multiple GOOF strong classifiers based on Random Forests (GOOF-RF) by training each fingerprint in the GOOF. In the online phase, we input the corresponding transformations of the real measurements into these strong classifiers to obtain multiple independent decisions. Finally, we propose a Sliding Window aIded Mode-based (SWIM) fusion algorithm to balance the localization accuracy and time. Our proposed approaches can work better in an unknown indoor scenario. The burden of building fingerprints can also be reduced drastically. We demonstrate the performance of our algorithms through simulations and real experimental data using two Universal Software Radio Peripheral (USRP) platforms.



### Removal of Salt and Pepper noise from Gray-Scale and Color Images: An Adaptive Approach
- **Arxiv ID**: http://arxiv.org/abs/1703.02217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02217v1)
- **Published**: 2017-03-07 05:24:26+00:00
- **Updated**: 2017-03-07 05:24:26+00:00
- **Authors**: Sujaya Kumar Sathua, Arabinda Dash, Aishwaryarani Behera
- **Comment**: 10 pages, 16 figures
- **Journal**: International Journal of Computer Science Trends and Technology
  (IJCST) V5(1): Page(117-126) Jan-Feb 2017. ISSN: 2347-8578.
  www.ijcstjournal.org.Published by Eighth Sense Research Group
- **Summary**: An efficient adaptive algorithm for the removal of Salt and Pepper noise from gray scale and color image is presented in this paper. In this proposed method first a 3X3 window is taken and the central pixel of the window is considered as the processing pixel. If the processing pixel is found as uncorrupted, then it is left unchanged. And if the processing pixel is found corrupted one, then the window size is increased according to the conditions given in the proposed algorithm. Finally the processing pixel or the central pixel is replaced by either the mean, median or trimmed value of the elements in the current window depending upon different conditions of the algorithm. The proposed algorithm efficiently removes noise at all densities with better Peak Signal to Noise Ratio (PSNR) and Image Enhancement Factor (IEF). The proposed algorithm is compared with different existing algorithms like MF, AMF, MDBUTMF, MDBPTGMF and AWMF.



### Shape DNA: Basic Generating Functions for Geometric Moment Invariants
- **Arxiv ID**: http://arxiv.org/abs/1703.02242v3
- **DOI**: None
- **Categories**: **cs.CV**, I.4.7.c; I.4.7; I.4; I.4.7.b; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1703.02242v3)
- **Published**: 2017-03-07 06:55:31+00:00
- **Updated**: 2017-05-24 19:14:45+00:00
- **Authors**: Erbo Li, Yazhou Huang, Dong Xu, Hua Li
- **Comment**: None
- **Journal**: None
- **Summary**: Geometric moment invariants (GMIs) have been widely used as basic tool in shape analysis and information retrieval. Their structure and characteristics determine efficiency and effectiveness. Two fundamental building blocks or generating functions (GFs) for invariants are discovered, which are dot product and vector product of point vectors in Euclidean space. The primitive invariants (PIs) can be derived by carefully selecting different products of GFs and calculating the corresponding multiple integrals, which translates polynomials of coordinates of point vectors into geometric moments. Then the invariants themselves are expressed in the form of product of moments. This procedure is just like DNA encoding proteins. All GMIs available in the literature can be decomposed into linear combinations of PIs. This paper shows that Hu's seven well known GMIs in computer vision have a more deep structure, which can be further divided into combination of simpler PIs. In practical uses, low order independent GMIs are of particular interest. In this paper, a set of PIs for similarity transformation and affine transformation in 2D are presented, which are simpler to use, and some of which are newly reported. The discovery of the two generating functions provides a new perspective of better understanding shapes in 2D and 3D Euclidean spaces, and the method proposed can be further extended to higher dimensional spaces and different manifolds, such as curves, surfaces and so on.



### SRN: Side-output Residual Network for Object Symmetry Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1703.02243v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02243v2)
- **Published**: 2017-03-07 07:09:40+00:00
- **Updated**: 2017-04-01 01:58:50+00:00
- **Authors**: Wei Ke, Jie Chen, Jianbin Jiao, Guoying Zhao, Qixiang Ye
- **Comment**: Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition, 2017
- **Journal**: None
- **Summary**: In this paper, we establish a baseline for object symmetry detection in complex backgrounds by presenting a new benchmark and an end-to-end deep learning approach, opening up a promising direction for symmetry detection in the wild. The new benchmark, named Sym-PASCAL, spans challenges including object diversity, multi-objects, part-invisibility, and various complex backgrounds that are far beyond those in existing datasets. The proposed symmetry detection approach, named Side-output Residual Network (SRN), leverages output Residual Units (RUs) to fit the errors between the object symmetry groundtruth and the outputs of RUs. By stacking RUs in a deep-to-shallow manner, SRN exploits the 'flow' of errors among multiple scales to ease the problems of fitting complex outputs with limited layers, suppressing the complex backgrounds, and effectively matching object symmetry of different scales. Experimental results validate both the benchmark and its challenging aspects related to realworld images, and the state-of-the-art performance of our symmetry detection approach. The benchmark and the code for SRN are publicly available at https://github.com/KevinKecc/SRN.



### X-ray Astronomical Point Sources Recognition Using Granular Binary-tree SVM
- **Arxiv ID**: http://arxiv.org/abs/1703.02271v1
- **DOI**: 10.1109/ICSP.2016.7877984
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02271v1)
- **Published**: 2017-03-07 08:31:23+00:00
- **Updated**: 2017-03-07 08:31:23+00:00
- **Authors**: Zhixian Ma, Weitian Li, Lei Wang, Haiguang Xu, Jie Zhu
- **Comment**: Accepted by ICSP2016
- **Journal**: None
- **Summary**: The study on point sources in astronomical images is of special importance, since most energetic celestial objects in the Universe exhibit a point-like appearance. An approach to recognize the point sources (PS) in the X-ray astronomical images using our newly designed granular binary-tree support vector machine (GBT-SVM) classifier is proposed. First, all potential point sources are located by peak detection on the image. The image and spectral features of these potential point sources are then extracted. Finally, a classifier to recognize the true point sources is build through the extracted features. Experiments and applications of our approach on real X-ray astronomical images are demonstrated. comparisons between our approach and other SVM-based classifiers are also carried out by evaluating the precision and recall rates, which prove that our approach is better and achieves a higher accuracy of around 89%.



### Triple Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1703.02291v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.02291v4)
- **Published**: 2017-03-07 09:26:56+00:00
- **Updated**: 2017-11-05 17:25:11+00:00
- **Authors**: Chongxuan Li, Kun Xu, Jun Zhu, Bo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players---a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.



### Large-scale image analysis using docker sandboxing
- **Arxiv ID**: http://arxiv.org/abs/1703.02898v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.IR, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1703.02898v1)
- **Published**: 2017-03-07 09:40:48+00:00
- **Updated**: 2017-03-07 09:40:48+00:00
- **Authors**: B Sengupta, E Vazquez, M Sasdelli, Y Qian, M Peniak, L Netherton, G Delfino
- **Comment**: None
- **Journal**: None
- **Summary**: With the advent of specialized hardware such as Graphics Processing Units (GPUs), large scale image localization, classification and retrieval have seen increased prevalence. Designing scalable software architecture that co-evolves with such specialized hardware is a challenge in the commercial setting. In this paper, we describe one such architecture (\textit{Cortexica}) that leverages scalability of GPUs and sandboxing offered by docker containers. This allows for the flexibility of mixing different computer architectures as well as computational algorithms with the security of a trusted environment. We illustrate the utility of this framework in a commercial setting i.e., searching for multiple products in an image by combining image localisation and retrieval.



### Deep Learning based Large Scale Visual Recommendation and Search for E-Commerce
- **Arxiv ID**: http://arxiv.org/abs/1703.02344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02344v1)
- **Published**: 2017-03-07 11:58:36+00:00
- **Updated**: 2017-03-07 11:58:36+00:00
- **Authors**: Devashish Shankar, Sujay Narumanchi, H A Ananya, Pramod Kompalli, Krishnendu Chaudhury
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a unified end-to-end approach to build a large scale Visual Search and Recommendation system for e-commerce. Previous works have targeted these problems in isolation. We believe a more effective and elegant solution could be obtained by tackling them together. We propose a unified Deep Convolutional Neural Network architecture, called VisNet, to learn embeddings to capture the notion of visual similarity, across several semantic granularities. We demonstrate the superiority of our approach for the task of image retrieval, by comparing against the state-of-the-art on the Exact Street2Shop dataset. We then share the design decisions and trade-offs made while deploying the model to power Visual Recommendations across a catalog of 50M products, supporting 2K queries a second at Flipkart, India's largest e-commerce company. The deployment of our solution has yielded a significant business impact, as measured by the conversion-rate.



### Qualitative Assessment of Recurrent Human Motion
- **Arxiv ID**: http://arxiv.org/abs/1703.02363v2
- **DOI**: 10.23919/EUSIPCO.2017.8081218
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.02363v2)
- **Published**: 2017-03-07 12:57:01+00:00
- **Updated**: 2017-11-22 14:17:14+00:00
- **Authors**: Andre Ebert, Michael Till Beck, Andy Mattausch, Lenz Belzner, Claudia Linnhoff Popien
- **Comment**: Published within the proceedings of the 25th European Signal
  Processing Conference (EUSIPCO) 2017, Kos Island, Greece, IEEE 6 Pages, 5
  Figures
- **Journal**: None
- **Summary**: Smartphone applications designed to track human motion in combination with wearable sensors, e.g., during physical exercising, raised huge attention recently. Commonly, they provide quantitative services, such as personalized training instructions or the counting of distances. But qualitative monitoring and assessment is still missing, e.g., to detect malpositions, to prevent injuries, or to optimize training success. We address this issue by presenting a concept for qualitative as well as generic assessment of recurrent human motion by processing multi-dimensional, continuous time series tracked with motion sensors. Therefore, our segmentation procedure extracts individual events of specific length and we propose expressive features to accomplish a qualitative motion assessment by supervised classification. We verified our approach within a comprehensive study encompassing 27 athletes undertaking different body weight exercises. We are able to recognize six different exercise types with a success rate of 100% and to assess them qualitatively with an average success rate of 99.3%.



### Learning from Noisy Labels with Distillation
- **Arxiv ID**: http://arxiv.org/abs/1703.02391v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.02391v2)
- **Published**: 2017-03-07 14:15:14+00:00
- **Updated**: 2017-04-07 07:21:56+00:00
- **Authors**: Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, Li-Jia Li
- **Comment**: None
- **Journal**: None
- **Summary**: The ability of learning from noisy labels is very useful in many visual recognition tasks, as a vast amount of data with noisy labels are relatively easy to obtain. Traditionally, the label noises have been treated as statistical outliers, and approaches such as importance re-weighting and bootstrap have been proposed to alleviate the problem. According to our observation, the real-world noisy labels exhibit multi-mode characteristics as the true labels, rather than behaving like independent random outliers. In this work, we propose a unified distillation framework to use side information, including a small clean dataset and label relations in knowledge graph, to "hedge the risk" of learning from noisy labels. Furthermore, unlike the traditional approaches evaluated based on simulated label noises, we propose a suite of new benchmark datasets, in Sports, Species and Artifacts domains, to evaluate the task of learning from noisy labels in the practical setting. The empirical study demonstrates the effectiveness of our proposed method in all the domains.



### PathTrack: Fast Trajectory Annotation with Path Supervision
- **Arxiv ID**: http://arxiv.org/abs/1703.02437v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1703.02437v2)
- **Published**: 2017-03-07 15:36:39+00:00
- **Updated**: 2017-03-22 07:08:34+00:00
- **Authors**: Santiago Manen, Michael Gygli, Dengxin Dai, Luc Van Gool
- **Comment**: 10 pages, ICCV submission
- **Journal**: None
- **Summary**: Progress in Multiple Object Tracking (MOT) has been historically limited by the size of the available datasets. We present an efficient framework to annotate trajectories and use it to produce a MOT dataset of unprecedented size. In our novel path supervision the annotator loosely follows the object with the cursor while watching the video, providing a path annotation for each object in the sequence. Our approach is able to turn such weak annotations into dense box trajectories. Our experiments on existing datasets prove that our framework produces more accurate annotations than the state of the art, in a fraction of the time. We further validate our approach by crowdsourcing the PathTrack dataset, with more than 15,000 person trajectories in 720 sequences. Tracking approaches can benefit training on such large-scale datasets, as did object recognition. We prove this by re-training an off-the-shelf person matching network, originally trained on the MOT15 dataset, almost halving the misclassification rate. Additionally, training on our data consistently improves tracking results, both on our dataset and on MOT15. On the latter, we improve the top-performing tracker (NOMT) dropping the number of IDSwitches by 18% and fragments by 5%.



### Object classification in images of Neoclassical furniture using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.02445v1
- **DOI**: 10.1007/978-3-319-46224-0_10
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02445v1)
- **Published**: 2017-03-07 15:48:55+00:00
- **Updated**: 2017-03-07 15:48:55+00:00
- **Authors**: Bernhard Bermeitinger, André Freitas, Simon Donig, Siegfried Handschuh
- **Comment**: None
- **Journal**: Computational History and Data-Driven Humanities. CHDDH 2016. IFIP
  Advances in Information and Communication Technology, vol 482. Springer, Cham
- **Summary**: This short paper outlines research results on object classification in images of Neoclassical furniture. The motivation was to provide an object recognition framework which is able to support the alignment of furniture images with a symbolic level model. A data-driven bottom-up research routine in the Neoclassica research framework is the main use-case. It strives to deliver tools for analyzing the spread of aesthetic forms which are considered as a cultural transfer process.



### Deep Learning for Automated Quality Assessment of Color Fundus Images in Diabetic Retinopathy Screening
- **Arxiv ID**: http://arxiv.org/abs/1703.02511v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02511v1)
- **Published**: 2017-03-07 18:28:50+00:00
- **Updated**: 2017-03-07 18:28:50+00:00
- **Authors**: Sajib Kumar Saha, Basura Fernando, Jorge Cuadros, Di Xiao, Yogesan Kanagasingam
- **Comment**: 23 pages, 9 figures
- **Journal**: None
- **Summary**: Purpose To develop a computer based method for the automated assessment of image quality in the context of diabetic retinopathy (DR) to guide the photographer.   Methods A deep learning framework was trained to grade the images automatically. A large representative set of 7000 color fundus images were used for the experiment which were obtained from the EyePACS that were made available by the California Healthcare Foundation. Three retinal image analysis experts were employed to categorize these images into Accept and Reject classes based on the precise definition of image quality in the context of DR. A deep learning framework was trained using 3428 images.   Results A total of 3572 images were used for the evaluation of the proposed method. The method shows an accuracy of 100% to successfully categorise Accept and Reject images.   Conclusion Image quality is an essential prerequisite for the grading of DR. In this paper we have proposed a deep learning based automated image quality assessment method in the context of DR. The method can be easily incorporated with the fundus image capturing system and thus can guide the photographer whether a recapture is necessary or not.



### Faster Coordinate Descent via Adaptive Importance Sampling
- **Arxiv ID**: http://arxiv.org/abs/1703.02518v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.CO, stat.ML, G.1.6
- **Links**: [PDF](http://arxiv.org/pdf/1703.02518v1)
- **Published**: 2017-03-07 18:36:55+00:00
- **Updated**: 2017-03-07 18:36:55+00:00
- **Authors**: Dmytro Perekrestenko, Volkan Cevher, Martin Jaggi
- **Comment**: appearing at AISTATS 2017
- **Journal**: None
- **Summary**: Coordinate descent methods employ random partial updates of decision variables in order to solve huge-scale convex optimization problems. In this work, we introduce new adaptive rules for the random selection of their updates. By adaptive, we mean that our selection rules are based on the dual residual or the primal-dual gap estimates and can change at each iteration. We theoretically characterize the performance of our selection rules and demonstrate improvements over the state-of-the-art, and extend our theory and algorithms to general convex objectives. Numerical evidence with hinge-loss support vector machines and Lasso confirm that the practice follows the theory.



### Unsupervised Visual-Linguistic Reference Resolution in Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/1703.02521v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02521v2)
- **Published**: 2017-03-07 18:40:27+00:00
- **Updated**: 2017-05-20 22:33:54+00:00
- **Authors**: De-An Huang, Joseph J. Lim, Li Fei-Fei, Juan Carlos Niebles
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: We propose an unsupervised method for reference resolution in instructional videos, where the goal is to temporally link an entity (e.g., "dressing") to the action (e.g., "mix yogurt") that produced it. The key challenge is the inevitable visual-linguistic ambiguities arising from the changes in both visual appearance and referring expression of an entity in the video. This challenge is amplified by the fact that we aim to resolve references with no supervision. We address these challenges by learning a joint visual-linguistic model, where linguistic cues can help resolve visual ambiguities and vice versa. We verify our approach by learning our model unsupervisedly using more than two thousand unstructured cooking videos from YouTube, and show that our visual-linguistic model can substantially improve upon state-of-the-art linguistic only model on reference resolution in instructional videos.



### NoScope: Optimizing Neural Network Queries over Video at Scale
- **Arxiv ID**: http://arxiv.org/abs/1703.02529v3
- **DOI**: None
- **Categories**: **cs.DB**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.02529v3)
- **Published**: 2017-03-07 18:54:28+00:00
- **Updated**: 2017-08-08 21:34:30+00:00
- **Authors**: Daniel Kang, John Emmons, Firas Abuzaid, Peter Bailis, Matei Zaharia
- **Comment**: PVLDB 2017
- **Journal**: None
- **Summary**: Recent advances in computer vision-in the form of deep neural networks-have made it possible to query increasing volumes of video data with high accuracy. However, neural network inference is computationally expensive at scale: applying a state-of-the-art object detector in real time (i.e., 30+ frames per second) to a single video requires a $4000 GPU. In response, we present NoScope, a system for querying videos that can reduce the cost of neural network video analysis by up to three orders of magnitude via inference-optimized model search. Given a target video, object to detect, and reference neural network, NoScope automatically searches for and trains a sequence, or cascade, of models that preserves the accuracy of the reference network but is specialized to the target video and are therefore far less computationally expensive. NoScope cascades two types of models: specialized models that forego the full generality of the reference model but faithfully mimic its behavior for the target video and object; and difference detectors that highlight temporal differences across frames. We show that the optimal cascade architecture differs across videos and objects, so NoScope uses an efficient cost-based optimizer to search across models and cascades. With this approach, NoScope achieves two to three order of magnitude speed-ups (265-15,500x real-time) on binary classification tasks over fixed-angle webcam and surveillance video while maintaining accuracy within 1-5% of state-of-the-art neural networks.



### Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1703.02563v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1703.02563v2)
- **Published**: 2017-03-07 19:28:45+00:00
- **Updated**: 2018-10-28 08:00:11+00:00
- **Authors**: Christian Bailer, Bertram Taetz, Didier Stricker
- **Comment**: Extended TPAMI version of publication (conference version:
  arXiv:1508.05151)
- **Journal**: None
- **Summary**: Modern large displacement optical flow algorithms usually use an initialization by either sparse descriptor matching techniques or dense approximate nearest neighbor fields. While the latter have the advantage of being dense, they have the major disadvantage of being very outlier-prone as they are not designed to find the optical flow, but the visually most similar correspondence. In this article we present a dense correspondence field approach that is much less outlier-prone and thus much better suited for optical flow estimation than approximate nearest neighbor fields. Our approach does not require explicit regularization, smoothing (like median filtering) or a new data term. Instead we solely rely on patch matching techniques and a novel multi-scale matching strategy. We also present enhancements for outlier filtering. We show that our approach is better suited for large displacement optical flow estimation than modern descriptor matching techniques. We do so by initializing EpicFlow with our approach instead of their originally used state-of-the-art descriptor matching technique. We significantly outperform the original EpicFlow on MPI-Sintel, KITTI 2012, KITTI 2015 and Middlebury. In this extended article of our former conference publication we further improve our approach in matching accuracy as well as runtime and present more experiments and insights.



### Texture Classification of MR Images of the Brain in ALS using CoHOG
- **Arxiv ID**: http://arxiv.org/abs/1703.02589v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02589v2)
- **Published**: 2017-03-07 20:50:14+00:00
- **Updated**: 2017-09-25 16:50:45+00:00
- **Authors**: G M Mashrur E Elahi, Sanjay Kalra, Yee-Hong Yang
- **Comment**: We found an error in the feature selection part (Sec. 3.3) of the
  proposed approach. In the feature selection part, by mistake, we have used
  both the training and testing data for feature selection. We are working on
  it. We will update the proposed approach as early as possible
- **Journal**: None
- **Summary**: Texture analysis is a well-known research topic in computer vision and image processing and has many applications. Gradient-based texture methods have become popular in classification problems. For the first time we extend a well-known gradient-based method, Co-occurrence Histograms of Oriented Gradients (CoHOG) to extract texture features from 2D Magnetic Resonance Images (MRI). Unlike the original CoHOG method, we use the whole image instead of sub-regions for feature calculation. Also, we use a larger neighborhood size. Gradient orientations of the image pixels are calculated using Sobel, Gaussian Derivative (GD) and Local Frequency Descriptor Gradient (LFDG) operators. The extracted feature vector size is very large and classification using a large number of similar features does not provide the best results. In our proposed method, for the first time to our best knowledge, only a minimum number of significant features are selected using area under the receiver operator characteristic (ROC) curve (AUC) thresholds with <= 0.01. In this paper, we apply the proposed method to classify Amyotrophic Lateral Sclerosis (ALS) patients from the controls. It is observed that selected texture features from downsampled images are significantly different between patients and controls. These features are used in a linear support vector machine (SVM) classifier to determine the classification accuracy. Optimal sensitivity and specificity are also calculated. Three different cohort datasets are used in the experiments. The performance of the proposed method using three gradient operators and two different neighborhood sizes is analyzed. Region based analysis is performed to demonstrate that significant changes between patients and controls are limited to the motor cortex.



