# Arxiv Papers in cs.CV on 2017-03-02
### Skin Lesion Analysis Towards Melanoma Detection Using Deep Learning Network
- **Arxiv ID**: http://arxiv.org/abs/1703.00577v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00577v2)
- **Published**: 2017-03-02 01:24:04+00:00
- **Updated**: 2017-11-22 12:13:09+00:00
- **Authors**: Yuexiang Li, Linlin Shen
- **Comment**: ISIC2017
- **Journal**: None
- **Summary**: Skin lesion is a severe disease in world-wide extent. Early detection of melanoma in dermoscopy images significantly increases the survival rate. However, the accurate recognition of melanoma is extremely challenging due to the following reasons, e.g. low contrast between lesions and skin, visual similarity between melanoma and non-melanoma lesions, etc. Hence, reliable automatic detection of skin tumors is very useful to increase the accuracy and efficiency of pathologists. International Skin Imaging Collaboration (ISIC) is a challenge focusing on the automatic analysis of skin lesion. In this paper, we proposed two deep learning methods to address all the three tasks announced in ISIC 2017, i.e. lesion segmentation (task 1), lesion dermoscopic feature extraction (task 2) and lesion classification (task 3). A deep learning framework consisting of two fully-convolutional residual networks (FCRN) is proposed to simultaneously produce the segmentation result and the coarse classification result. A lesion index calculation unit (LICU) is developed to refine the coarse classification results by calculating the distance heat-map. A straight-forward CNN is proposed for the dermoscopic feature extraction task. To our best knowledges, we are not aware of any previous work proposed for this task. The proposed deep learning frameworks were evaluated on the ISIC 2017 testing set. Experimental results show the promising accuracies of our frameworks, i.e. 0.718 for task 1, 0.833 for task 2 and 0.823 for task 3 were achieved.



### A novel image tag completion method based on convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1703.00586v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00586v2)
- **Published**: 2017-03-02 02:15:05+00:00
- **Updated**: 2017-06-03 12:21:31+00:00
- **Authors**: Yanyan Geng, Guohui Zhang, Weizhi Li, Yi Gu, Ru-Ze Liang, Gaoyuan Liang, Jingbin Wang, Yanbin Wu, Nitin Patil, Jing-Yan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In the problems of image retrieval and annotation, complete textual tag lists of images play critical roles. However, in real-world applications, the image tags are usually incomplete, thus it is important to learn the complete tags for images. In this paper, we study the problem of image tag complete and proposed a novel method for this problem based on a popular image representation method, convolutional neural network (CNN). The method estimates the complete tags from the convolutional filtering outputs of images based on a linear predictor. The CNN parameters, linear predictor, and the complete tags are learned jointly by our method. We build a minimization problem to encourage the consistency between the complete tags and the available incomplete tags, reduce the estimation error, and reduce the model complexity. An iterative algorithm is developed to solve the minimization problem. Experiments over benchmark image data sets show its effectiveness.



### TumorNet: Lung Nodule Characterization Using Multi-View Convolutional Neural Network with Gaussian Process
- **Arxiv ID**: http://arxiv.org/abs/1703.00645v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.00645v1)
- **Published**: 2017-03-02 07:26:37+00:00
- **Updated**: 2017-03-02 07:26:37+00:00
- **Authors**: Sarfaraz Hussein, Robert Gillies, Kunlin Cao, Qi Song, Ulas Bagci
- **Comment**: Accepted for publication in IEEE International Symposium on
  Biomedical Imaging (ISBI) 2017
- **Journal**: None
- **Summary**: Characterization of lung nodules as benign or malignant is one of the most important tasks in lung cancer diagnosis, staging and treatment planning. While the variation in the appearance of the nodules remains large, there is a need for a fast and robust computer aided system. In this work, we propose an end-to-end trainable multi-view deep Convolutional Neural Network (CNN) for nodule characterization. First, we use median intensity projection to obtain a 2D patch corresponding to each dimension. The three images are then concatenated to form a tensor, where the images serve as different channels of the input image. In order to increase the number of training samples, we perform data augmentation by scaling, rotating and adding noise to the input image. The trained network is used to extract features from the input image followed by a Gaussian Process (GP) regression to obtain the malignancy score. We also empirically establish the significance of different high level nodule attributes such as calcification, sphericity and others for malignancy determination. These attributes are found to be complementary to the deep multi-view CNN features and a significant improvement over other methods is obtained.



### Introduction to Nonnegative Matrix Factorization
- **Arxiv ID**: http://arxiv.org/abs/1703.00663v1
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV, cs.LG, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.00663v1)
- **Published**: 2017-03-02 08:23:04+00:00
- **Updated**: 2017-03-02 08:23:04+00:00
- **Authors**: Nicolas Gillis
- **Comment**: 18 pages, 4 figures
- **Journal**: SIAG/OPT Views and News 25 (1), pp. 7-16 (2017)
- **Summary**: In this paper, we introduce and provide a short overview of nonnegative matrix factorization (NMF). Several aspects of NMF are discussed, namely, the application in hyperspectral imaging, geometry and uniqueness of NMF solutions, complexity, algorithms, and its link with extended formulations of polyhedra. In order to put NMF into perspective, the more general problem class of constrained low-rank matrix approximation problems is first briefly introduced.



### Extrinsic Calibration of 3D Range Finder and Camera without Auxiliary Object or Human Intervention
- **Arxiv ID**: http://arxiv.org/abs/1703.04391v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1703.04391v1)
- **Published**: 2017-03-02 08:49:23+00:00
- **Updated**: 2017-03-02 08:49:23+00:00
- **Authors**: Qinghai Liao, Ming Liu, Lei Tai, Haoyang Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Fusion of heterogeneous extroceptive sensors is the most effient and effective way to representing the environment precisely, as it overcomes various defects of each homogeneous sensor. The rigid transformation (aka. extrinsic parameters) of heterogeneous sensory systems should be available before precisely fusing the multisensor information. Researchers have proposed several approaches to estimating the extrinsic parameters. These approaches require either auxiliary objects, like chessboards, or extra help from human to select correspondences. In this paper, we proposed a novel extrinsic calibration approach for the extrinsic calibration of range and image sensors. As far as we know, it is the first automatic approach with no requirement of auxiliary objects or any human interventions. First, we estimate the initial extrinsic parameters from the individual motion of the range finder and the camera. Then we extract lines in the image and point-cloud pairs, to refine the line feature associations by the initial extrinsic parameters. At the end, we discussed the degenerate case which may lead to the algorithm failure and validate our approach by simulation. The results indicate high-precision extrinsic calibration results against the ground-truth.



### BoxCars: Improving Fine-Grained Recognition of Vehicles using 3-D Bounding Boxes in Traffic Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1703.00686v3
- **DOI**: 10.1109/TITS.2018.2799228
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00686v3)
- **Published**: 2017-03-02 09:51:51+00:00
- **Updated**: 2018-03-09 12:01:57+00:00
- **Authors**: Jakub Sochor, Jakub Špaňhel, Adam Herout
- **Comment**: None
- **Journal**: IEEE Transactions on Intelligent Transportation Systems, 2018,
  ISSN: 1524-9050
- **Summary**: In this paper, we focus on fine-grained recognition of vehicles mainly in traffic surveillance applications. We propose an approach that is orthogonal to recent advancements in fine-grained recognition (automatic part discovery and bilinear pooling). In addition, in contrast to other methods focused on fine-grained recognition of vehicles, we do not limit ourselves to a frontal/rear viewpoint, but allow the vehicles to be seen from any viewpoint. Our approach is based on 3-D bounding boxes built around the vehicles. The bounding box can be automatically constructed from traffic surveillance data. For scenarios where it is not possible to use precise construction, we propose a method for an estimation of the 3-D bounding box. The 3-D bounding box is used to normalize the image viewpoint by "unpacking" the image into a plane. We also propose to randomly alter the color of the image and add a rectangle with random noise to a random position in the image during the training of convolutional neural networks (CNNs). We have collected a large fine-grained vehicle data set BoxCars116k, with 116k images of vehicles from various viewpoints taken by numerous surveillance cameras. We performed a number of experiments, which show that our proposed method significantly improves CNN classification accuracy (the accuracy is increased by up to 12% points and the error is reduced by up to 50% compared with CNNs without the proposed modifications). We also show that our method outperforms the state-of-the-art methods for fine-grained recognition.



### Wireless Interference Identification with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.00737v1
- **DOI**: 10.1109/indin.2017.8104767
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.00737v1)
- **Published**: 2017-03-02 11:52:47+00:00
- **Updated**: 2017-03-02 11:52:47+00:00
- **Authors**: Malte Schmidt, Dimitri Block, Uwe Meier
- **Comment**: None
- **Journal**: IEEE 15th International Conference on Industrial Informatics
  (INDIN)
- **Summary**: The steadily growing use of license-free frequency bands requires reliable coexistence management for deterministic medium utilization. For interference mitigation, proper wireless interference identification (WII) is essential. In this work we propose the first WII approach based upon deep convolutional neural networks (CNNs). The CNN naively learns its features through self-optimization during an extensive data-driven GPU-based training process. We propose a CNN example which is based upon sensing snapshots with a limited duration of 12.8 {\mu}s and an acquisition bandwidth of 10 MHz. The CNN differs between 15 classes. They represent packet transmissions of IEEE 802.11 b/g, IEEE 802.15.4 and IEEE 802.15.1 with overlapping frequency channels within the 2.4 GHz ISM band. We show that the CNN outperforms state-of-the-art WII approaches and has a classification accuracy greater than 95% for signal-to-noise ratio of at least -5 dB.



### Attentive Recurrent Comparators
- **Arxiv ID**: http://arxiv.org/abs/1703.00767v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.00767v3)
- **Published**: 2017-03-02 12:47:40+00:00
- **Updated**: 2017-06-30 07:37:56+00:00
- **Authors**: Pranav Shyam, Shubham Gupta, Ambedkar Dukkipati
- **Comment**: None
- **Journal**: None
- **Summary**: Rapid learning requires flexible representations to quickly adopt to new evidence. We develop a novel class of models called Attentive Recurrent Comparators (ARCs) that form representations of objects by cycling through them and making observations. Using the representations extracted by ARCs, we develop a way of approximating a \textit{dynamic representation space} and use it for one-shot learning. In the task of one-shot classification on the Omniglot dataset, we achieve the state of the art performance with an error rate of 1.5\%. This represents the first super-human result achieved for this task with a generic model that uses only pixel information.



### Robust Spatial Filtering with Graph Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.00792v3
- **DOI**: 10.1109/JSTSP.2017.2726981
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00792v3)
- **Published**: 2017-03-02 14:09:32+00:00
- **Updated**: 2017-07-14 19:57:57+00:00
- **Authors**: Felipe Petroski Such, Shagan Sah, Miguel Dominguez, Suhas Pillai, Chao Zhang, Andrew Michael, Nathan Cahill, Raymond Ptucha
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have recently led to incredible breakthroughs on a variety of pattern recognition problems. Banks of finite impulse response filters are learned on a hierarchy of layers, each contributing more abstract information than the previous layer. The simplicity and elegance of the convolutional filtering process makes them perfect for structured problems such as image, video, or voice, where vertices are homogeneous in the sense of number, location, and strength of neighbors. The vast majority of classification problems, for example in the pharmaceutical, homeland security, and financial domains are unstructured. As these problems are formulated into unstructured graphs, the heterogeneity of these problems, such as number of vertices, number of connections per vertex, and edge strength, cannot be tackled with standard convolutional techniques. We propose a novel neural learning framework that is capable of handling both homogeneous and heterogeneous data, while retaining the benefits of traditional CNN successes.   Recently, researchers have proposed variations of CNNs that can handle graph data. In an effort to create learnable filter banks of graphs, these methods either induce constraints on the data or require preprocessing. As opposed to spectral methods, our framework, which we term Graph-CNNs, defines filters as polynomials of functions of the graph adjacency matrix. Graph-CNNs can handle both heterogeneous and homogeneous graph data, including graphs having entirely different vertex or edge sets. We perform experiments to validate the applicability of Graph-CNNs to a variety of structured and unstructured classification problems and demonstrate state-of-the-art results on document and molecule classification problems.



### A Simple, Fast and Fully Automated Approach for Midline Shift Measurement on Brain Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/1703.00797v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.00797v1)
- **Published**: 2017-03-02 14:33:14+00:00
- **Updated**: 2017-03-02 14:33:14+00:00
- **Authors**: Huan-Chih Wang, Shih-Hao Ho, Furen Xiao, Jen-Hai Chou
- **Comment**: None
- **Journal**: None
- **Summary**: Brain CT has become a standard imaging tool for emergent evaluation of brain condition, and measurement of midline shift (MLS) is one of the most important features to address for brain CT assessment. We present a simple method to estimate MLS and propose a new alternative parameter to MLS: the ratio of MLS over the maximal width of intracranial region (MLS/ICWMAX). Three neurosurgeons and our automated system were asked to measure MLS and MLS/ICWMAX in the same sets of axial CT images obtained from 41 patients admitted to ICU under neurosurgical service. A weighted midline (WML) was plotted based on individual pixel intensities, with higher weighted given to the darker portions. The MLS could then be measured as the distance between the WML and ideal midline (IML) near the foramen of Monro. The average processing time to output an automatic MLS measurement was around 10 seconds. Our automated system achieved an overall accuracy of 90.24% when the CT images were calibrated automatically, and performed better when the calibrations of head rotation were done manually (accuracy: 92.68%). MLS/ICWMAX and MLS both gave results in same confusion matrices and produced similar ROC curve results. We demonstrated a simple, fast and accurate automated system of MLS measurement and introduced a new parameter (MLS/ICWMAX) as a good alternative to MLS in terms of estimating the degree of brain deformation, especially when non-DICOM images (e.g. JPEG) are more easily accessed.



### On the Reconstruction of Face Images from Deep Face Templates
- **Arxiv ID**: http://arxiv.org/abs/1703.00832v4
- **DOI**: 10.1109/TPAMI.2018.2827389
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00832v4)
- **Published**: 2017-03-02 15:29:07+00:00
- **Updated**: 2018-04-29 03:39:50+00:00
- **Authors**: Guangcan Mai, Kai Cao, Pong C. Yuen, Anil K. Jain
- **Comment**: To appear in TPAMI, IEEE Transactions on Pattern Analysis and Machine
  Intelligence, 2018
- **Journal**: None
- **Summary**: State-of-the-art face recognition systems are based on deep (convolutional) neural networks. Therefore, it is imperative to determine to what extent face templates derived from deep networks can be inverted to obtain the original face image. In this paper, we study the vulnerabilities of a state-of-the-art face recognition system based on template reconstruction attack. We propose a neighborly de-convolutional neural network (\textit{NbNet}) to reconstruct face images from their deep templates. In our experiments, we assumed that no knowledge about the target subject and the deep network are available. To train the \textit{NbNet} reconstruction models, we augmented two benchmark face datasets (VGG-Face and Multi-PIE) with a large collection of images synthesized using a face generator. The proposed reconstruction was evaluated using type-I (comparing the reconstructed images against the original face images used to generate the deep template) and type-II (comparing the reconstructed images against a different face image of the same subject) attacks. Given the images reconstructed from \textit{NbNets}, we show that for verification, we achieve TAR of 95.20\% (58.05\%) on LFW under type-I (type-II) attacks @ FAR of 0.1\%. Besides, 96.58\% (92.84\%) of the images reconstruction from templates of partition \textit{fa} (\textit{fb}) can be identified from partition \textit{fa} in color FERET. Our study demonstrates the need to secure deep templates in face recognition systems.



### Towards CNN Map Compression for camera relocalisation
- **Arxiv ID**: http://arxiv.org/abs/1703.00845v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00845v1)
- **Published**: 2017-03-02 16:12:29+00:00
- **Updated**: 2017-03-02 16:12:29+00:00
- **Authors**: Luis Contreras, Walterio Mayol-Cuevas
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a study on the use of Convolutional Neural Networks for camera relocalisation and its application to map compression. We follow state of the art visual relocalisation results and evaluate response to different data inputs -- namely, depth, grayscale, RGB, spatial position and combinations of these. We use a CNN map representation and introduce the notion of CNN map compression by using a smaller CNN architecture. We evaluate our proposal in a series of publicly available datasets. This formulation allows us to improve relocalisation accuracy by increasing the number of training trajectories while maintaining a constant-size CNN.



### Unsupervised Image-to-Image Translation Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.00848v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1703.00848v6)
- **Published**: 2017-03-02 16:29:30+00:00
- **Updated**: 2018-07-23 03:39:28+00:00
- **Authors**: Ming-Yu Liu, Thomas Breuel, Jan Kautz
- **Comment**: NIPS 2017, 11 pages, 6 figures
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit .



### Non-line-of-sight tracking of people at long range
- **Arxiv ID**: http://arxiv.org/abs/1703.02124v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/1703.02124v1)
- **Published**: 2017-03-02 16:57:04+00:00
- **Updated**: 2017-03-02 16:57:04+00:00
- **Authors**: Susan Chan, Ryan E. Warburton, Genevieve Gariepy, Jonathan Leach, Daniele Faccio
- **Comment**: None
- **Journal**: None
- **Summary**: A remote-sensing system that can determine the position of hidden objects has applications in many critical real-life scenarios, such as search and rescue missions and safe autonomous driving. Previous work has shown the ability to range and image objects hidden from the direct line of sight, employing advanced optical imaging technologies aimed at small objects at short range. In this work we demonstrate a long-range tracking system based on single laser illumination and single-pixel single-photon detection. This enables us to track one or more people hidden from view at a stand-off distance of over 50~m. These results pave the way towards next generation LiDAR systems that will reconstruct not only the direct-view scene but also the main elements hidden behind walls or corners.



### Araguaia Medical Vision Lab at ISIC 2017 Skin Lesion Classification Challenge
- **Arxiv ID**: http://arxiv.org/abs/1703.00856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.00856v1)
- **Published**: 2017-03-02 17:01:24+00:00
- **Updated**: 2017-03-02 17:01:24+00:00
- **Authors**: Rafael Teixeira Sousa, Larissa Vasconcellos de Moraes
- **Comment**: Abstract submitted as a requirement to ISIC2017 challenge
- **Journal**: None
- **Summary**: This paper describes the participation of Araguaia Medical Vision Lab at the International Skin Imaging Collaboration 2017 Skin Lesion Challenge. We describe the use of deep convolutional neural networks in attempt to classify images of Melanoma and Seborrheic Keratosis lesions. With use of finetuned GoogleNet and AlexNet we attained results of 0.950 and 0.846 AUC on Seborrheic Keratosis and Melanoma respectively.



### Binarized Convolutional Landmark Localizers for Human Pose Estimation and Face Alignment with Limited Resources
- **Arxiv ID**: http://arxiv.org/abs/1703.00862v2
- **DOI**: 10.1109/ICCV.2017.400
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.00862v2)
- **Published**: 2017-03-02 17:26:46+00:00
- **Updated**: 2017-08-07 15:35:04+00:00
- **Authors**: Adrian Bulat, Georgios Tzimiropoulos
- **Comment**: ICCV 2017 Oral
- **Journal**: None
- **Summary**: Our goal is to design architectures that retain the groundbreaking performance of CNNs for landmark localization and at the same time are lightweight, compact and suitable for applications with limited computational resources. To this end, we make the following contributions: (a) we are the first to study the effect of neural network binarization on localization tasks, namely human pose estimation and face alignment. We exhaustively evaluate various design choices, identify performance bottlenecks, and more importantly propose multiple orthogonal ways to boost performance. (b) Based on our analysis, we propose a novel hierarchical, parallel and multi-scale residual architecture that yields large performance improvement over the standard bottleneck block while having the same number of parameters, thus bridging the gap between the original network and its binarized counterpart. (c) We perform a large number of ablation studies that shed light on the properties and the performance of the proposed block. (d) We present results for experiments on the most challenging datasets for human pose estimation and face alignment, reporting in many cases state-of-the-art performance. Code can be downloaded from https://www.adrianbulat.com/binary-cnn-landmarks



### Using Synthetic Data to Train Neural Networks is Model-Based Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1703.00868v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T05, 68T10, I.2.6; I.7.5
- **Links**: [PDF](http://arxiv.org/pdf/1703.00868v1)
- **Published**: 2017-03-02 17:43:19+00:00
- **Updated**: 2017-03-02 17:43:19+00:00
- **Authors**: Tuan Anh Le, Atilim Gunes Baydin, Robert Zinkov, Frank Wood
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: We draw a formal connection between using synthetic training data to optimize neural network parameters and approximate, Bayesian, model-based reasoning. In particular, training a neural network using synthetic data can be viewed as learning a proposal distribution generator for approximate inference in the synthetic-data generative model. We demonstrate this connection in a recognition task where we develop a novel Captcha-breaking architecture and train it using synthetic data, demonstrating both state-of-the-art performance and a way of computing task-specific posterior uncertainty. Using a neural network trained this way, we also demonstrate successful breaking of real-world Captchas currently used by Facebook and Wikipedia. Reasoning from these empirical results and drawing connections with Bayesian modeling, we discuss the robustness of synthetic data results and suggest important considerations for ensuring good neural network generalization when training with synthetic data.



### Depth Estimation using Modified Cost Function for Occlusion Handling
- **Arxiv ID**: http://arxiv.org/abs/1703.00919v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.00919v2)
- **Published**: 2017-03-02 19:05:05+00:00
- **Updated**: 2017-11-10 12:15:20+00:00
- **Authors**: Krzysztof Wegner, Olgierd Stankiewicz, Marek Domanski
- **Comment**: None
- **Journal**: None
- **Summary**: The paper presents a novel approach to occlusion handling problem in depth estimation using three views. A solution based on modification of similarity cost function is proposed. During the depth estimation via optimization algorithms like Graph Cut similarity metric is constantly updated so that only non-occluded fragments in side views are considered. At each iteration of the algorithm non-occluded fragments are detected based on side view virtual depth maps synthesized from the best currently estimated depth map of the center view. Then similarity metric is updated for correspondence search only in non-occluded regions of the side views. The experimental results, conducted on well-known 3D video test sequences, have proved that the depth maps estimated with the proposed approach provide about 1.25 dB virtual view quality improvement in comparison to the virtual view synthesized based on depth maps generated by the state-of-the-art MPEG Depth Estimation Reference Software.



### Enhancing human color vision by breaking binocular redundancy
- **Arxiv ID**: http://arxiv.org/abs/1703.04392v3
- **DOI**: None
- **Categories**: **physics.bio-ph**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1703.04392v3)
- **Published**: 2017-03-02 22:48:49+00:00
- **Updated**: 2018-06-11 19:01:32+00:00
- **Authors**: Bradley S. Gundlach, Michel Frising, Alireza Shahsafi, Gregory Vershbow, Chenghao Wan, Jad Salman, Bas Rokers, Laurent Lessard, Mikhail A. Kats
- **Comment**: Main text and supplementary info
- **Journal**: None
- **Summary**: To see color, the human visual system combines the response of three types of cone cells in the retina--a compressive process that discards a significant amount of spectral information. Here, we present an approach to enhance human color vision by breaking its inherent binocular redundancy, providing different spectral content to each eye. We fabricated a set of optical filters that "splits" the response of the short-wavelength cone between the two eyes in individuals with typical trichromatic vision, simulating the presence of approximately four distinct cone types ("tetrachromacy"). Such an increase in the number of effective cone types can reduce the prevalence of metamers--pairs of distinct spectra that resolve to the same tristimulus values. This technique may result in an enhancement of spectral perception, with applications ranging from camouflage detection and anti-counterfeiting to new types of artwork and data visualization.



### A Restaurant Process Mixture Model for Connectivity Based Parcellation of the Cortex
- **Arxiv ID**: http://arxiv.org/abs/1703.00981v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CE, cs.CV, q-bio.QM, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1703.00981v1)
- **Published**: 2017-03-02 23:03:56+00:00
- **Updated**: 2017-03-02 23:03:56+00:00
- **Authors**: Daniel Moyer, Boris A Gutman, Neda Jahanshad, Paul M. Thompson
- **Comment**: In the Proceedings of Information Processing in Medical Imaging 2017
- **Journal**: None
- **Summary**: One of the primary objectives of human brain mapping is the division of the cortical surface into functionally distinct regions, i.e. parcellation. While it is generally agreed that at macro-scale different regions of the cortex have different functions, the exact number and configuration of these regions is not known. Methods for the discovery of these regions are thus important, particularly as the volume of available information grows. Towards this end, we present a parcellation method based on a Bayesian non-parametric mixture model of cortical connectivity.



### Belief Propagation in Conditional RBMs for Structured Prediction
- **Arxiv ID**: http://arxiv.org/abs/1703.00986v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.00986v1)
- **Published**: 2017-03-02 23:28:53+00:00
- **Updated**: 2017-03-02 23:28:53+00:00
- **Authors**: Wei Ping, Alexander Ihler
- **Comment**: Artificial Intelligence and Statistics (AISTATS) 2017
- **Journal**: None
- **Summary**: Restricted Boltzmann machines~(RBMs) and conditional RBMs~(CRBMs) are popular models for a wide range of applications. In previous work, learning on such models has been dominated by contrastive divergence~(CD) and its variants. Belief propagation~(BP) algorithms are believed to be slow for structured prediction on conditional RBMs~(e.g., Mnih et al. [2011]), and not as good as CD when applied in learning~(e.g., Larochelle et al. [2012]). In this work, we present a matrix-based implementation of belief propagation algorithms on CRBMs, which is easily scalable to tens of thousands of visible and hidden units. We demonstrate that, in both maximum likelihood and max-margin learning, training conditional RBMs with BP as the inference routine can provide significantly better results than current state-of-the-art CD methods on structured prediction problems. We also include practical guidelines on training CRBMs with BP, and some insights on the interaction of learning and inference algorithms for CRBMs.



### Estimating the resolution of real images
- **Arxiv ID**: http://arxiv.org/abs/1703.00992v1
- **DOI**: 10.1088/1742-6596/849/1/012042
- **Categories**: **physics.data-an**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1703.00992v1)
- **Published**: 2017-03-02 23:55:13+00:00
- **Updated**: 2017-03-02 23:55:13+00:00
- **Authors**: Ryuta Mizutani, Rino Saiga, Susumu Takekoshi, Chie Inomoto, Naoya Nakamura, Makoto Arai, Kenichi Oshima, Masanari Itokawa, Akihisa Takeuchi, Kentaro Uesugi, Yasuko Terada, Yoshio Suzuki
- **Comment**: 4 pages, 2 figures
- **Journal**: J. Phys. Conf. Ser. 849, 012042 (2017)
- **Summary**: Image resolvability is the primary concern in imaging. This paper reports an estimation of the full width at half maximum of the point spread function from a Fourier domain plot of real sample images by neither using test objects, nor defining a threshold criterion. We suggest that this method can be applied to any type of image, independently of the imaging modality.



