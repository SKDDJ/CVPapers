# Arxiv Papers in cs.CV on 2017-03-14
### Fully Convolutional Neural Networks to Detect Clinical Dermoscopic Features
- **Arxiv ID**: http://arxiv.org/abs/1703.04559v2
- **DOI**: 10.1109/JBHI.2018.2831680
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04559v2)
- **Published**: 2017-03-14 00:54:18+00:00
- **Updated**: 2019-03-27 03:04:08+00:00
- **Authors**: Jeremy Kawahara, Ghassan Hamarneh
- **Comment**: Accepted JBHI version
- **Journal**: IEEE Journal of Biomedical and Health Informatics, vol. 23, no. 2,
  pp. 578-585, 2019
- **Summary**: The presence of certain clinical dermoscopic features within a skin lesion may indicate melanoma, and automatically detecting these features may lead to more quantitative and reproducible diagnoses. We reformulate the task of classifying clinical dermoscopic features within superpixels as a segmentation problem, and propose a fully convolutional neural network to detect clinical dermoscopic features from dermoscopy skin lesion images. Our neural network architecture uses interpolated feature maps from several intermediate network layers, and addresses imbalanced labels by minimizing a negative multi-label Dice-F$_1$ score, where the score is computed across the mini-batch for each label. Our approach ranked first place in the 2017 ISIC-ISBI Part 2: Dermoscopic Feature Classification Task challenge over both the provided validation and test datasets, achieving a 0.895% area under the receiver operator characteristic curve score. We show how simple baseline models can outrank state-of-the-art approaches when using the official metrics of the challenge, and propose to use a fuzzy Jaccard Index that ignores the empty set (i.e., masks devoid of positive pixels) when ranking models. Our results suggest that (i) the classification of clinical dermoscopic features can be effectively approached as a segmentation problem, and (ii) the current metrics used to rank models may not well capture the efficacy of the model. We plan to make our trained model and code publicly available.



### Learning Background-Aware Correlation Filters for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1703.04590v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04590v2)
- **Published**: 2017-03-14 17:16:23+00:00
- **Updated**: 2017-03-21 22:48:46+00:00
- **Authors**: Hamed Kiani Galoogahi, Ashton Fagg, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: Correlation Filters (CFs) have recently demonstrated excellent performance in terms of rapidly tracking objects under challenging photometric and geometric variations. The strength of the approach comes from its ability to efficiently learn - "on the fly" - how the object is changing over time. A fundamental drawback to CFs, however, is that the background of the object is not be modelled over time which can result in suboptimal results. In this paper we propose a Background-Aware CF that can model how both the foreground and background of the object varies over time. Our approach, like conventional CFs, is extremely computationally efficient - and extensive experiments over multiple tracking benchmarks demonstrate the superior accuracy and real-time performance of our method compared to the state-of-the-art trackers including those based on a deep learning paradigm.



### Subspace Learning in The Presence of Sparse Structured Outliers and Noise
- **Arxiv ID**: http://arxiv.org/abs/1703.04611v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04611v4)
- **Published**: 2017-03-14 17:37:53+00:00
- **Updated**: 2017-07-12 12:53:47+00:00
- **Authors**: Shervin Minaee, Yao Wang
- **Comment**: IEEE International Symposium on Circuits and Systems, 2017
- **Journal**: None
- **Summary**: Subspace learning is an important problem, which has many applications in image and video processing. It can be used to find a low-dimensional representation of signals and images. But in many applications, the desired signal is heavily distorted by outliers and noise, which negatively affect the learned subspace. In this work, we present a novel algorithm for learning a subspace for signal representation, in the presence of structured outliers and noise. The proposed algorithm tries to jointly detect the outliers and learn the subspace for images. We present an alternating optimization algorithm for solving this problem, which iterates between learning the subspace and finding the outliers. This algorithm has been trained on a large number of image patches, and the learned subspace is used for image segmentation, and is shown to achieve better segmentation results than prior methods, including least absolute deviation fitting, k-means clustering based segmentation in DjVu, and shape primitive extraction and coding algorithm.



### Recasting Residual-based Local Descriptors as Convolutional Neural Networks: an Application to Image Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/1703.04615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04615v1)
- **Published**: 2017-03-14 17:42:13+00:00
- **Updated**: 2017-03-14 17:42:13+00:00
- **Authors**: Davide Cozzolino, Giovanni Poggi, Luisa Verdoliva
- **Comment**: None
- **Journal**: None
- **Summary**: Local descriptors based on the image noise residual have proven extremely effective for a number of forensic applications, like forgery detection and localization. Nonetheless, motivated by promising results in computer vision, the focus of the research community is now shifting on deep learning. In this paper we show that a class of residual-based descriptors can be actually regarded as a simple constrained convolutional neural network (CNN). Then, by relaxing the constraints, and fine-tuning the net on a relatively small training set, we obtain a significant performance improvement with respect to the conventional detector.



### A PatchMatch-based Dense-field Algorithm for Video Copy-Move Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/1703.04636v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04636v1)
- **Published**: 2017-03-14 18:08:49+00:00
- **Updated**: 2017-03-14 18:08:49+00:00
- **Authors**: Luca D'Amiano, Davide Cozzolino, Giovanni Poggi, Luisa Verdoliva
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new algorithm for the reliable detection and localization of video copy-move forgeries. Discovering well crafted video copy-moves may be very difficult, especially when some uniform background is copied to occlude foreground objects. To reliably detect both additive and occlusive copy-moves we use a dense-field approach, with invariant features that guarantee robustness to several post-processing operations. To limit complexity, a suitable video-oriented version of PatchMatch is used, with a multiresolution search strategy, and a focus on volumes of interest. Performance assessment relies on a new dataset, designed ad hoc, with realistic copy-moves and a wide variety of challenging situations. Experimental results show the proposed method to detect and localize video copy-moves with good accuracy even in adverse conditions.



### A Framework for Dynamic Image Sampling Based on Supervised Learning (SLADS)
- **Arxiv ID**: http://arxiv.org/abs/1703.04653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04653v1)
- **Published**: 2017-03-14 18:36:16+00:00
- **Updated**: 2017-03-14 18:36:16+00:00
- **Authors**: G. M. Dilshan P. Godaliyadda, Dong Hye Ye, Michael D. Uchic, Michael A. Groeber, Gregery T. Buzzard, Charles A. Bouman
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse sampling schemes have the potential to dramatically reduce image acquisition time while simultaneously reducing radiation damage to samples. However, for a sparse sampling scheme to be useful it is important that we are able to reconstruct the underlying object with sufficient clarity using the sparse measurements. In dynamic sampling, each new measurement location is selected based on information obtained from previous measurements. Therefore, dynamic sampling schemes have the potential to dramatically reduce the number of measurements needed for high fidelity reconstructions. However, most existing dynamic sampling methods for point-wise measurement acquisition tend to be computationally expensive and are therefore too slow for practical applications.   In this paper, we present a framework for dynamic sampling based on machine learning techniques, which we call a supervised learning approach for dynamic sampling (SLADS). In each step of SLADS, the objective is to find the pixel that maximizes the expected reduction in distortion (ERD) given previous measurements. SLADS is fast because we use a simple regression function to compute the ERD, and it is accurate because the regression function is trained using data sets that are representative of the specific application. In addition, we introduce a method to terminate dynamic sampling at a desired level of distortion, and we extended the SLADS methodology to sample groups of pixels at each step. Finally, we present results on computationally-generated synthetic data and experimentally-collected data to demonstrate a dramatic improvement over state-of-the-art static sampling methods.



### Geometry-Based Region Proposals for Real-Time Robot Detection of Tabletop Objects
- **Arxiv ID**: http://arxiv.org/abs/1703.04665v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.04665v1)
- **Published**: 2017-03-14 18:51:18+00:00
- **Updated**: 2017-03-14 18:51:18+00:00
- **Authors**: Alexander Broad, Brenna Argall
- **Comment**: Update based on work presented at RSS 2016 Deep Learning Workshop
- **Journal**: None
- **Summary**: We present a novel object detection pipeline for localization and recognition in three dimensional environments. Our approach makes use of an RGB-D sensor and combines state-of-the-art techniques from the robotics and computer vision communities to create a robust, real-time detection system. We focus specifically on solving the object detection problem for tabletop scenes, a common environment for assistive manipulators. Our detection pipeline locates objects in a point cloud representation of the scene. These clusters are subsequently used to compute a bounding box around each object in the RGB space. Each defined patch is then fed into a Convolutional Neural Network (CNN) for object recognition. We also demonstrate that our region proposal method can be used to develop novel datasets that are both large and diverse enough to train deep learning models, and easy enough to collect that end-users can develop their own datasets. Lastly, we validate the resulting system through an extensive analysis of the accuracy and run-time of the full pipeline.



### 6-DoF Object Pose from Semantic Keypoints
- **Arxiv ID**: http://arxiv.org/abs/1703.04670v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1703.04670v1)
- **Published**: 2017-03-14 18:58:46+00:00
- **Updated**: 2017-03-14 18:58:46+00:00
- **Authors**: Georgios Pavlakos, Xiaowei Zhou, Aaron Chan, Konstantinos G. Derpanis, Kostas Daniilidis
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA), 2017
- **Journal**: None
- **Summary**: This paper presents a novel approach to estimating the continuous six degree of freedom (6-DoF) pose (3D translation and rotation) of an object from a single RGB image. The approach combines semantic keypoints predicted by a convolutional network (convnet) with a deformable shape model. Unlike prior work, we are agnostic to whether the object is textured or textureless, as the convnet learns the optimal representation from the available training image data. Furthermore, the approach can be applied to instance- and class-based pose recovery. Empirically, we show that the proposed approach can accurately recover the 6-DoF object pose for both instance- and class-based scenarios with a cluttered background. For class-based object pose estimation, state-of-the-art accuracy is shown on the large-scale PASCAL3D+ dataset.



### A fully end-to-end deep learning approach for real-time simultaneous 3D reconstruction and material recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.04699v1
- **DOI**: 10.1109/ICAR.2017.8023499
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04699v1)
- **Published**: 2017-03-14 20:23:48+00:00
- **Updated**: 2017-03-14 20:23:48+00:00
- **Authors**: Cheng Zhao, Li Sun, Rustam Stolkin
- **Comment**: 8 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: This paper addresses the problem of simultaneous 3D reconstruction and material recognition and segmentation. Enabling robots to recognise different materials (concrete, metal etc.) in a scene is important for many tasks, e.g. robotic interventions in nuclear decommissioning. Previous work on 3D semantic reconstruction has predominantly focused on recognition of everyday domestic objects (tables, chairs etc.), whereas previous work on material recognition has largely been confined to single 2D images without any 3D reconstruction. Meanwhile, most 3D semantic reconstruction methods rely on computationally expensive post-processing, using Fully-Connected Conditional Random Fields (CRFs), to achieve consistent segmentations. In contrast, we propose a deep learning method which performs 3D reconstruction while simultaneously recognising different types of materials and labelling them at the pixel level. Unlike previous methods, we propose a fully end-to-end approach, which does not require hand-crafted features or CRF post-processing. Instead, we use only learned features, and the CRF segmentation constraints are incorporated inside the fully end-to-end learned system. We present the results of experiments, in which we trained our system to perform real-time 3D semantic reconstruction for 23 different materials in a real-world application. The run-time performance of the system can be boosted to around 10Hz, using a conventional GPU, which is enough to achieve real-time semantic reconstruction using a 30fps RGB-D camera. To the best of our knowledge, this work is the first real-time end-to-end system for simultaneous 3D reconstruction and material recognition.



### Tracking Gaze and Visual Focus of Attention of People Involved in Social Interaction
- **Arxiv ID**: http://arxiv.org/abs/1703.04727v2
- **DOI**: 10.1109/TPAMI.2017.2782819
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04727v2)
- **Published**: 2017-03-14 21:06:08+00:00
- **Updated**: 2017-11-21 16:05:15+00:00
- **Authors**: Benoît Massé, Silèye Ba, Radu Horaud
- **Comment**: 15 pages, 8 figures, 6 tables
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  40(11), 2711 - 2724, 2018
- **Summary**: The visual focus of attention (VFOA) has been recognized as a prominent conversational cue. We are interested in estimating and tracking the VFOAs associated with multi-party social interactions. We note that in this type of situations the participants either look at each other or at an object of interest; therefore their eyes are not always visible. Consequently both gaze and VFOA estimation cannot be based on eye detection and tracking. We propose a method that exploits the correlation between eye gaze and head movements. Both VFOA and gaze are modeled as latent variables in a Bayesian switching state-space model. The proposed formulation leads to a tractable learning procedure and to an efficient algorithm that simultaneously tracks gaze and visual focus. The method is tested and benchmarked using two publicly available datasets that contain typical multi-party human-robot and human-human interactions.



### Discriminate-and-Rectify Encoders: Learning from Image Transformation Sets
- **Arxiv ID**: http://arxiv.org/abs/1703.04775v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.04775v1)
- **Published**: 2017-03-14 22:21:48+00:00
- **Updated**: 2017-03-14 22:21:48+00:00
- **Authors**: Andrea Tacchetti, Stephen Voinea, Georgios Evangelopoulos
- **Comment**: None
- **Journal**: None
- **Summary**: The complexity of a learning task is increased by transformations in the input space that preserve class identity. Visual object recognition for example is affected by changes in viewpoint, scale, illumination or planar transformations. While drastically altering the visual appearance, these changes are orthogonal to recognition and should not be reflected in the representation or feature encoding used for learning. We introduce a framework for weakly supervised learning of image embeddings that are robust to transformations and selective to the class distribution, using sets of transforming examples (orbit sets), deep parametrizations and a novel orbit-based loss. The proposed loss combines a discriminative, contrastive part for orbits with a reconstruction error that learns to rectify orbit transformations. The learned embeddings are evaluated in distance metric-based tasks, such as one-shot classification under geometric transformations, as well as face verification and retrieval under more realistic visual variability. Our results suggest that orbit sets, suitably computed or observed, can be used for efficient, weakly-supervised learning of semantically relevant image embeddings.



### RECOD Titans at ISIC Challenge 2017
- **Arxiv ID**: http://arxiv.org/abs/1703.04819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04819v1)
- **Published**: 2017-03-14 23:11:04+00:00
- **Updated**: 2017-03-14 23:11:04+00:00
- **Authors**: Afonso Menegola, Julia Tavares, Michel Fornaciali, Lin Tzy Li, Sandra Avila, Eduardo Valle
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: This extended abstract describes the participation of RECOD Titans in parts 1 and 3 of the ISIC Challenge 2017 "Skin Lesion Analysis Towards Melanoma Detection" (ISBI 2017). Although our team has a long experience with melanoma classification, the ISIC Challenge 2017 was the very first time we worked on skin-lesion segmentation. For part 1 (segmentation), our final submission used four of our models: two trained with all 2000 samples, without a validation split, for 250 and for 500 epochs respectively; and other two trained and validated with two different 1600/400 splits, for 220 epochs. Those four models, individually, achieved between 0.780 and 0.783 official validation scores. Our final submission averaged the output of those four models achieved a score of 0.793. For part 3 (classification), the submitted test run as well as our last official validation run were the result from a meta-model that assembled seven base deep-learning models: three based on Inception-V4 trained on our largest dataset; three based on Inception trained on our smallest dataset; and one based on ResNet-101 trained on our smaller dataset. The results of those component models were stacked in a meta-learning layer based on an SVM trained on the validation set of our largest dataset.



### In Search of a Dataset for Handwritten Optical Music Recognition: Introducing MUSCIMA++
- **Arxiv ID**: http://arxiv.org/abs/1703.04824v1
- **DOI**: None
- **Categories**: **cs.CV**, I.7.5
- **Links**: [PDF](http://arxiv.org/pdf/1703.04824v1)
- **Published**: 2017-03-14 23:21:26+00:00
- **Updated**: 2017-03-14 23:21:26+00:00
- **Authors**: Jan Hajič jr., Pavel Pecina
- **Comment**: None
- **Journal**: None
- **Summary**: Optical Music Recognition (OMR) has long been without an adequate dataset and ground truth for evaluating OMR systems, which has been a major problem for establishing a state of the art in the field. Furthermore, machine learning methods require training data. We analyze how the OMR processing pipeline can be expressed in terms of gradually more complex ground truth, and based on this analysis, we design the MUSCIMA++ dataset of handwritten music notation that addresses musical symbol recognition and notation reconstruction. The MUSCIMA++ dataset version 0.9 consists of 140 pages of handwritten music, with 91255 manually annotated notation symbols and 82261 explicitly marked relationships between symbol pairs. The dataset allows training and evaluating models for symbol classification, symbol localization, and notation graph assembly, both in isolation and jointly. Open-source tools are provided for manipulating the dataset, visualizing the data and further annotation, and the dataset itself is made available under an open license.



### A Proximity-Aware Hierarchical Clustering of Faces
- **Arxiv ID**: http://arxiv.org/abs/1703.04835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.04835v1)
- **Published**: 2017-03-14 23:41:45+00:00
- **Updated**: 2017-03-14 23:41:45+00:00
- **Authors**: Wei-An Lin, Jun-Cheng Chen, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an unsupervised face clustering algorithm called "Proximity-Aware Hierarchical Clustering" (PAHC) that exploits the local structure of deep representations. In the proposed method, a similarity measure between deep features is computed by evaluating linear SVM margins. SVMs are trained using nearest neighbors of sample data, and thus do not require any external training data. Clusters are then formed by thresholding the similarity scores. We evaluate the clustering performance using three challenging unconstrained face datasets, including Celebrity in Frontal-Profile (CFP), IARPA JANUS Benchmark A (IJB-A), and JANUS Challenge Set 3 (JANUS CS3) datasets. Experimental results demonstrate that the proposed approach can achieve significant improvements over state-of-the-art methods. Moreover, we also show that the proposed clustering algorithm can be applied to curate a set of large-scale and noisy training dataset while maintaining sufficient amount of images and their variations due to nuisance factors. The face verification performance on JANUS CS3 improves significantly by finetuning a DCNN model with the curated MS-Celeb-1M dataset which contains over three million face images.



