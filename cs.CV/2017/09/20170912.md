# Arxiv Papers in cs.CV on 2017-09-12
### Anti-Makeup: Learning A Bi-Level Adversarial Network for Makeup-Invariant Face Verification
- **Arxiv ID**: http://arxiv.org/abs/1709.03654v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03654v2)
- **Published**: 2017-09-12 02:08:40+00:00
- **Updated**: 2017-11-22 13:04:41+00:00
- **Authors**: Yi Li, Lingxiao Song, Xiang Wu, Ran He, Tieniu Tan
- **Comment**: The paper is accepted by AAAI-18
- **Journal**: None
- **Summary**: Makeup is widely used to improve facial attractiveness and is well accepted by the public. However, different makeup styles will result in significant facial appearance changes. It remains a challenging problem to match makeup and non-makeup face images. This paper proposes a learning from generation approach for makeup-invariant face verification by introducing a bi-level adversarial network (BLAN). To alleviate the negative effects from makeup, we first generate non-makeup images from makeup ones, and then use the synthesized non-makeup images for further verification. Two adversarial networks in BLAN are integrated in an end-to-end deep network, with the one on pixel level for reconstructing appealing facial images and the other on feature level for preserving identity information. These two networks jointly reduce the sensing gap between makeup and non-makeup images. Moreover, we make the generator well constrained by incorporating multiple perceptual losses. Experimental results on three benchmark makeup face datasets demonstrate that our method achieves state-of-the-art verification accuracy across makeup status and can produce photo-realistic non-makeup face images.



### Learning Gating ConvNet for Two-Stream based Methods in Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1709.03655v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03655v2)
- **Published**: 2017-09-12 02:09:04+00:00
- **Updated**: 2017-09-13 09:13:31+00:00
- **Authors**: Jiagang Zhu, Wei Zou, Zheng Zhu
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: For the two-stream style methods in action recognition, fusing the two streams' predictions is always by the weighted averaging scheme. This fusion method with fixed weights lacks of pertinence to different action videos and always needs trial and error on the validation set. In order to enhance the adaptability of two-stream ConvNets and improve its performance, an end-to-end trainable gated fusion method, namely gating ConvNet, for the two-stream ConvNets is proposed in this paper based on the MoE (Mixture of Experts) theory. The gating ConvNet takes the combination of feature maps from the same layer of the spatial and the temporal nets as input and adopts ReLU (Rectified Linear Unit) as the gating output activation function. To reduce the over-fitting of gating ConvNet caused by the redundancy of parameters, a new multi-task learning method is designed, which jointly learns the gating fusion weights for the two streams and learns the gating ConvNet for action classification. With our gated fusion method and multi-task learning approach, a high accuracy of 94.5% is achieved on the dataset UCF101.



### Joint Adaptive Neighbours and Metric Learning for Multi-view Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1709.03656v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03656v1)
- **Published**: 2017-09-12 02:09:29+00:00
- **Updated**: 2017-09-12 02:09:29+00:00
- **Authors**: Nan Xu, Yanqing Guo, Jiujun Wang, Xiangyang Luo, Ran He
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Due to the existence of various views or representations in many real-world data, multi-view learning has drawn much attention recently. Multi-view spectral clustering methods based on similarity matrixes or graphs are pretty popular. Generally, these algorithms learn informative graphs by directly utilizing original data. However, in the real-world applications, original data often contain noises and outliers that lead to unreliable graphs. In addition, different views may have different contributions to data clustering. In this paper, a novel Multiview Subspace Clustering method unifying Adaptive neighbours and Metric learning (MSCAM), is proposed to address the above problems. In this method, we use the subspace representations of different views to adaptively learn a consensus similarity matrix, uncovering the subspace structure and avoiding noisy nature of original data. For all views, we also learn different Mahalanobis matrixes that parameterize the squared distances and consider the contributions of different views. Further, we constrain the graph constructed by the similarity matrix to have exact c (c is the number of clusters) connected components. An iterative algorithm is developed to solve this optimization problem. Moreover, experiments on a synthetic dataset and different real-world datasets demonstrate the effectiveness of MSCAM.



### Adversarial Discriminative Heterogeneous Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1709.03675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03675v1)
- **Published**: 2017-09-12 03:22:45+00:00
- **Updated**: 2017-09-12 03:22:45+00:00
- **Authors**: Lingxiao Song, Man Zhang, Xiang Wu, Ran He
- **Comment**: None
- **Journal**: None
- **Summary**: The gap between sensing patterns of different face modalities remains a challenging problem in heterogeneous face recognition (HFR). This paper proposes an adversarial discriminative feature learning framework to close the sensing gap via adversarial learning on both raw-pixel space and compact feature space. This framework integrates cross-spectral face hallucination and discriminative feature learning into an end-to-end adversarial network. In the pixel space, we make use of generative adversarial networks to perform cross-spectral face hallucination. An elaborate two-path model is introduced to alleviate the lack of paired images, which gives consideration to both global structures and local textures. In the feature space, an adversarial loss and a high-order variance discrepancy loss are employed to measure the global and local discrepancy between two heterogeneous distributions respectively. These two losses enhance domain-invariant feature learning and modality independent noise removing. Experimental results on three NIR-VIS databases show that our proposed approach outperforms state-of-the-art HFR methods, without requiring of complex network or large-scale training dataset.



### Joint Dictionaries for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1709.03688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03688v1)
- **Published**: 2017-09-12 04:41:33+00:00
- **Updated**: 2017-09-12 04:41:33+00:00
- **Authors**: Soheil Kolouri, Mohammad Rostami, Yuri Owechko, Kyungnam Kim
- **Comment**: None
- **Journal**: None
- **Summary**: A classic approach toward zero-shot learning (ZSL) is to map the input domain to a set of semantically meaningful attributes that could be used later on to classify unseen classes of data (e.g. visual data). In this paper, we propose to learn a visual feature dictionary that has semantically meaningful atoms. Such dictionary is learned via joint dictionary learning for the visual domain and the attribute domain, while enforcing the same sparse coding for both dictionaries. Our novel attribute aware formulation provides an algorithmic solution to the domain shift/hubness problem in ZSL. Upon learning the joint dictionaries, images from unseen classes can be mapped into the attribute space by finding the attribute aware joint sparse representation using solely the visual data. We demonstrate that our approach provides superior or comparable performance to that of the state of the art on benchmark datasets.



### A Benchmark for Sparse Coding: When Group Sparsity Meets Rank Minimization
- **Arxiv ID**: http://arxiv.org/abs/1709.03979v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03979v5)
- **Published**: 2017-09-12 05:34:19+00:00
- **Updated**: 2020-02-05 01:46:29+00:00
- **Authors**: Zhiyuan Zha, Xin Yuan, Bihan Wen, Jiantao Zhou, Jiachao Zhang, Ce Zhu
- **Comment**: arXiv admin note: text overlap with arXiv:1611.08983
- **Journal**: IEEE Transaction on Image Processing 2020
- **Summary**: Sparse coding has achieved a great success in various image processing tasks. However, a benchmark to measure the sparsity of image patch/group is missing since sparse coding is essentially an NP-hard problem. This work attempts to fill the gap from the perspective of rank minimization. More details please see the manuscript....



### Automatic Ground Truths: Projected Image Annotations for Omnidirectional Vision
- **Arxiv ID**: http://arxiv.org/abs/1709.03697v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03697v1)
- **Published**: 2017-09-12 05:38:42+00:00
- **Updated**: 2017-09-12 05:38:42+00:00
- **Authors**: Victor Stamatescu, Peter Barsznica, Manjung Kim, Kin K. Liu, Mark McKenzie, Will Meakin, Gwilyn Saunders, Sebastien C. Wong, Russell S. A. Brinkworth
- **Comment**: 2017 International Conference on Digital Image Computing: Techniques
  and Applications (DICTA), Page 1 - 8
- **Journal**: None
- **Summary**: We present a novel data set made up of omnidirectional video of multiple objects whose centroid positions are annotated automatically. Omnidirectional vision is an active field of research focused on the use of spherical imagery in video analysis and scene understanding, involving tasks such as object detection, tracking and recognition. Our goal is to provide a large and consistently annotated video data set that can be used to train and evaluate new algorithms for these tasks. Here we describe the experimental setup and software environment used to capture and map the 3D ground truth positions of multiple objects into the image. Furthermore, we estimate the expected systematic error on the mapped positions. In addition to final data products, we release publicly the software tools and raw data necessary to re-calibrate the camera and/or redo this mapping. The software also provides a simple framework for comparing the results of standard image annotation tools or visual tracking systems against our mapped ground truth annotations.



### Reversible Architectures for Arbitrarily Deep Residual Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.03698v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1709.03698v2)
- **Published**: 2017-09-12 05:41:13+00:00
- **Updated**: 2017-11-18 22:10:57+00:00
- **Authors**: Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, Elliot Holtham
- **Comment**: Accepted at AAAI 2018
- **Journal**: None
- **Summary**: Recently, deep residual networks have been successfully applied in many computer vision and natural language processing tasks, pushing the state-of-the-art performance with deeper and wider architectures. In this work, we interpret deep residual networks as ordinary differential equations (ODEs), which have long been studied in mathematics and physics with rich theoretical and empirical success. From this interpretation, we develop a theoretical framework on stability and reversibility of deep neural networks, and derive three reversible neural network architectures that can go arbitrarily deep in theory. The reversibility property allows a memory-efficient implementation, which does not need to store the activations for most hidden layers. Together with the stability of our architectures, this enables training deeper networks using only modest computational resources. We provide both theoretical analyses and empirical results. Experimental results demonstrate the efficacy of our architectures against several strong baselines on CIFAR-10, CIFAR-100 and STL-10 with superior or on-par state-of-the-art performance. Furthermore, we show our architectures yield superior results when trained using fewer training data.



### PQk-means: Billion-scale Clustering for Product-quantized Codes
- **Arxiv ID**: http://arxiv.org/abs/1709.03708v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1709.03708v1)
- **Published**: 2017-09-12 07:00:18+00:00
- **Updated**: 2017-09-12 07:00:18+00:00
- **Authors**: Yusuke Matsui, Keisuke Ogaki, Toshihiko Yamasaki, Kiyoharu Aizawa
- **Comment**: To appear in ACMMM 2017
- **Journal**: None
- **Summary**: Data clustering is a fundamental operation in data analysis. For handling large-scale data, the standard k-means clustering method is not only slow, but also memory-inefficient. We propose an efficient clustering method for billion-scale feature vectors, called PQk-means. By first compressing input vectors into short product-quantized (PQ) codes, PQk-means achieves fast and memory-efficient clustering, even for high-dimensional vectors. Similar to k-means, PQk-means repeats the assignment and update steps, both of which can be performed in the PQ-code domain. Experimental results show that even short-length (32 bit) PQ-codes can produce competitive results compared with k-means. This result is of practical importance for clustering in memory-restricted environments. Using the proposed PQk-means scheme, the clustering of one billion 128D SIFT features with K = 10^5 is achieved within 14 hours, using just 32 GB of memory consumption on a single computer.



### A low cost non-wearable gaze detection system based on infrared image processing
- **Arxiv ID**: http://arxiv.org/abs/1709.03717v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.03717v1)
- **Published**: 2017-09-12 07:46:05+00:00
- **Updated**: 2017-09-12 07:46:05+00:00
- **Authors**: Ehsan Arbabi, Mohammad Shabani, Ali Yarigholi
- **Comment**: None
- **Journal**: None
- **Summary**: Human eye gaze detection plays an important role in various fields, including human-computer interaction, virtual reality and cognitive science. Although different relatively accurate systems of eye tracking and gaze detection exist, they are usually either too expensive to be bought for low cost applications or too complex to be implemented easily. In this article, we propose a non-wearable system for eye tracking and gaze detection with low complexity and cost. The proposed system provides a medium accuracy which makes it suitable for general applications in which low cost and easy implementation is more important than achieving very precise gaze detection. The proposed method includes pupil and marker detection using infrared image processing, and gaze evaluation using an interpolation-based strategy. The interpolation-based strategy exploits the positions of the detected pupils and markers in a target captured image and also in some previously captured training images for estimating the position of a point that the user is gazing at. The proposed system has been evaluated by three users in two different lighting conditions. The experimental results show that the accuracy of this low cost system can be between 90% and 100% for finding major gazing directions.



### Cascaded Region-based Densely Connected Network for Event Detection: A Seismic Application
- **Arxiv ID**: http://arxiv.org/abs/1709.07943v2
- **DOI**: 10.1109/TGRS.2018.2852302
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.07943v2)
- **Published**: 2017-09-12 08:00:46+00:00
- **Updated**: 2017-11-29 04:15:26+00:00
- **Authors**: Yue Wu, Youzuo Lin, Zheng Zhou, David Chas Bolton, Ji Liu, Paul Johnson
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic event detection from time series signals has wide applications, such as abnormal event detection in video surveillance and event detection in geophysical data. Traditional detection methods detect events primarily by the use of similarity and correlation in data. Those methods can be inefficient and yield low accuracy. In recent years, because of the significantly increased computational power, machine learning techniques have revolutionized many science and engineering domains. In this study, we apply a deep-learning-based method to the detection of events from time series seismic signals. However, a direct adaptation of the similar ideas from 2D object detection to our problem faces two challenges. The first challenge is that the duration of earthquake event varies significantly; The other is that the proposals generated are temporally correlated. To address these challenges, we propose a novel cascaded region-based convolutional neural network to capture earthquake events in different sizes, while incorporating contextual information to enrich features for each individual proposal. To achieve a better generalization performance, we use densely connected blocks as the backbone of our network. Because of the fact that some positive events are not correctly annotated, we further formulate the detection problem as a learning-from-noise problem. To verify the performance of our detection methods, we employ our methods to seismic data generated from a bi-axial "earthquake machine" located at Rock Mechanics Laboratory, and we acquire labels with the help of experts. Through our numerical tests, we show that our novel detection techniques yield high accuracy. Therefore, our novel deep-learning-based detection methods can potentially be powerful tools for locating events from time series data in various applications.



### Construction of Latent Descriptor Space and Inference Model of Hand-Object Interactions
- **Arxiv ID**: http://arxiv.org/abs/1709.03739v1
- **DOI**: 10.1587/transinf.2016EDP7410
- **Categories**: **cs.CV**, I.2.10; I.5.1; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1709.03739v1)
- **Published**: 2017-09-12 08:34:23+00:00
- **Updated**: 2017-09-12 08:34:23+00:00
- **Authors**: Tadashi Matsuo, Nobutaka Shimada
- **Comment**: None
- **Journal**: IEICE Trans. on Info. and Sys., Vol.E100-D, No.6, pp.1350-1359,
  2017
- **Summary**: Appearance-based generic object recognition is a challenging problem because all possible appearances of objects cannot be registered, especially as new objects are produced every day. Function of objects, however, has a comparatively small number of prototypes. Therefore, function-based classification of new objects could be a valuable tool for generic object recognition. Object functions are closely related to hand-object interactions during handling of a functional object; i.e., how the hand approaches the object, which parts of the object and contact the hand, and the shape of the hand during interaction. Hand-object interactions are helpful for modeling object functions. However, it is difficult to assign discrete labels to interactions because an object shape and grasping hand-postures intrinsically have continuous variations. To describe these interactions, we propose the interaction descriptor space which is acquired from unlabeled appearances of human hand-object interactions. By using interaction descriptors, we can numerically describe the relation between an object's appearance and its possible interaction with the hand. The model infers the quantitative state of the interaction from the object image alone. It also identifies the parts of objects designed for hand interactions such as grips and handles. We demonstrate that the proposed method can unsupervisedly generate interaction descriptors that make clusters corresponding to interaction types. And also we demonstrate that the model can infer possible hand-object interactions.



### Deep Mean-Shift Priors for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1709.03749v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.03749v2)
- **Published**: 2017-09-12 09:11:24+00:00
- **Updated**: 2017-10-04 13:13:53+00:00
- **Authors**: Siavash Arjomand Bigdeli, Meiguang Jin, Paolo Favaro, Matthias Zwicker
- **Comment**: NIPS 2017
- **Journal**: None
- **Summary**: In this paper we introduce a natural image prior that directly represents a Gaussian-smoothed version of the natural image distribution. We include our prior in a formulation of image restoration as a Bayes estimator that also allows us to solve noise-blind image restoration problems. We show that the gradient of our prior corresponds to the mean-shift vector on the natural image distribution. In addition, we learn the mean-shift vector field using denoising autoencoders, and use it in a gradient descent approach to perform Bayes risk minimization. We demonstrate competitive results for noise-blind deblurring, super-resolution, and demosaicing.



### Transform Invariant Auto-encoder
- **Arxiv ID**: http://arxiv.org/abs/1709.03754v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.5.1; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1709.03754v1)
- **Published**: 2017-09-12 09:19:34+00:00
- **Updated**: 2017-09-12 09:19:34+00:00
- **Authors**: Tadashi Matsuo, Hiroya Fukuhara, Nobutaka Shimada
- **Comment**: 6 pages, 17 figures, to be published in IROS 2017
- **Journal**: None
- **Summary**: The auto-encoder method is a type of dimensionality reduction method. A mapping from a vector to a descriptor that represents essential information can be automatically generated from a set of vectors without any supervising information. However, an image and its spatially shifted version are encoded into different descriptors by an existing ordinary auto-encoder because each descriptor includes a spatial subpattern and its position. To generate a descriptor representing a spatial subpattern in an image, we need to normalize its spatial position in the images prior to training an ordinary auto-encoder; however, such a normalization is generally difficult for images without obvious standard positions. We propose a transform invariant auto-encoder and an inference model of transform parameters. By the proposed method, we can separate an input into a transform invariant descriptor and transform parameters. The proposed method can be applied to various auto-encoders without requiring any special modules or labeled training samples. By applying it to shift transforms, we can achieve a shift invariant auto-encoder that can extract a typical spatial subpattern independent of its relative position in a window. In addition, we can achieve a model that can infer shift parameters required to restore the input from the typical subpattern. As an example of the proposed method, we demonstrate that a descriptor generated by a shift invariant auto-encoder can represent a typical spatial subpattern. In addition, we demonstrate the imitation of a human hand by a robot hand as an example of a regression based on spatial subpatterns.



### Efficient Online Surface Correction for Real-time Large-Scale 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1709.03763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03763v1)
- **Published**: 2017-09-12 09:44:23+00:00
- **Updated**: 2017-09-12 09:44:23+00:00
- **Authors**: Robert Maier, Raphael Schaller, Daniel Cremers
- **Comment**: British Machine Vision Conference (BMVC), London, September 2017
- **Journal**: None
- **Summary**: State-of-the-art methods for large-scale 3D reconstruction from RGB-D sensors usually reduce drift in camera tracking by globally optimizing the estimated camera poses in real-time without simultaneously updating the reconstructed surface on pose changes. We propose an efficient on-the-fly surface correction method for globally consistent dense 3D reconstruction of large-scale scenes. Our approach uses a dense Visual RGB-D SLAM system that estimates the camera motion in real-time on a CPU and refines it in a global pose graph optimization. Consecutive RGB-D frames are locally fused into keyframes, which are incorporated into a sparse voxel hashed Signed Distance Field (SDF) on the GPU. On pose graph updates, the SDF volume is corrected on-the-fly using a novel keyframe re-integration strategy with reduced GPU-host streaming. We demonstrate in an extensive quantitative evaluation that our method is up to 93% more runtime efficient compared to the state-of-the-art and requires significantly less memory, with only negligible loss of surface quality. Overall, our system requires only a single GPU and allows for real-time surface correction of large environments.



### Sparse Representation Based Augmented Multinomial Logistic Extreme Learning Machine with Weighted Composite Features for Spectral Spatial Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1709.03792v2
- **DOI**: 10.1109/TGRS.2018.2828601
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03792v2)
- **Published**: 2017-09-12 11:39:51+00:00
- **Updated**: 2017-10-14 12:25:21+00:00
- **Authors**: Faxian Cao, Zhijing Yang, Jinchang Ren, Wing-Kuen Ling
- **Comment**: 16 pages, 6 figuers and 4 tables
- **Journal**: None
- **Summary**: Although extreme learning machine (ELM) has been successfully applied to a number of pattern recognition problems, it fails to pro-vide sufficient good results in hyperspectral image (HSI) classification due to two main drawbacks. The first is due to the random weights and bias of ELM, which may lead to ill-posed problems. The second is the lack of spatial information for classification. To tackle these two problems, in this paper, we propose a new framework for ELM based spectral-spatial classification of HSI, where probabilistic modelling with sparse representation and weighted composite features (WCF) are employed respectively to derive the op-timized output weights and extract spatial features. First, the ELM is represented as a concave logarithmic likelihood function under statistical modelling using the maximum a posteriori (MAP). Second, the sparse representation is applied to the Laplacian prior to effi-ciently determine a logarithmic posterior with a unique maximum in order to solve the ill-posed problem of ELM. The variable splitting and the augmented Lagrangian are subsequently used to further reduce the computation complexity of the proposed algorithm and it has been proven a more efficient method for speed improvement. Third, the spatial information is extracted using the weighted compo-site features (WCFs) to construct the spectral-spatial classification framework. In addition, the lower bound of the proposed method is derived by a rigorous mathematical proof. Experimental results on two publicly available HSI data sets demonstrate that the proposed methodology outperforms ELM and a number of state-of-the-art approaches.



### Can Deep Neural Networks Match the Related Objects?: A Survey on ImageNet-trained Classification Models
- **Arxiv ID**: http://arxiv.org/abs/1709.03806v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03806v1)
- **Published**: 2017-09-12 12:35:47+00:00
- **Updated**: 2017-09-12 12:35:47+00:00
- **Authors**: Han S. Lee, Heechul Jung, Alex A. Agarwal, Junmo Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have shown the state-of-the-art level of performances in wide range of complicated tasks. In recent years, the studies have been actively conducted to analyze the black box characteristics of DNNs and to grasp the learning behaviours, tendency, and limitations of DNNs. In this paper, we investigate the limitation of DNNs in image classification task and verify it with the method inspired by cognitive psychology. Through analyzing the failure cases of ImageNet classification task, we hypothesize that the DNNs do not sufficiently learn to associate related classes of objects. To verify how DNNs understand the relatedness between object classes, we conducted experiments on the image database provided in cognitive psychology. We applied the ImageNet-trained DNNs to the database consisting of pairs of related and unrelated object images to compare the feature similarities and determine whether the pairs match each other. In the experiments, we observed that the DNNs show limited performance in determining relatedness between object classes. In addition, the DNNs present somewhat improved performance in discovering relatedness based on similarity, but they perform weaker in discovering relatedness based on association. Through these experiments, a novel analysis of learning behaviour of DNNs is provided and the limitation which needs to be overcome is suggested.



### Emotion Recognition in the Wild using Deep Neural Networks and Bayesian Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1709.03820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03820v1)
- **Published**: 2017-09-12 13:09:22+00:00
- **Updated**: 2017-09-12 13:09:22+00:00
- **Authors**: Luca Surace, Massimiliano Patacchiola, Elena Battini Sönmez, William Spataro, Angelo Cangelosi
- **Comment**: accepted by the Fifth Emotion Recognition in the Wild (EmotiW)
  Challenge 2017
- **Journal**: None
- **Summary**: Group emotion recognition in the wild is a challenging problem, due to the unstructured environments in which everyday life pictures are taken. Some of the obstacles for an effective classification are occlusions, variable lighting conditions, and image quality. In this work we present a solution based on a novel combination of deep neural networks and Bayesian classifiers. The neural network works on a bottom-up approach, analyzing emotions expressed by isolated faces. The Bayesian classifier estimates a global emotion integrating top-down features obtained through a scene descriptor. In order to validate the system we tested the framework on the dataset released for the Emotion Recognition in the Wild Challenge 2017. Our method achieved an accuracy of 64.68% on the test set, significantly outperforming the 53.62% competition baseline.



### Une véritable approche $\ell_0$ pour l'apprentissage de dictionnaire
- **Arxiv ID**: http://arxiv.org/abs/1709.05937v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1709.05937v1)
- **Published**: 2017-09-12 13:21:47+00:00
- **Updated**: 2017-09-12 13:21:47+00:00
- **Authors**: Yuan Liu, Stéphane Canu, Paul Honeine, Su Ruan
- **Comment**: in French
- **Journal**: None
- **Summary**: Sparse representation learning has recently gained a great success in signal and image processing, thanks to recent advances in dictionary learning. To this end, the $\ell_0$-norm is often used to control the sparsity level. Nevertheless, optimization problems based on the $\ell_0$-norm are non-convex and NP-hard. For these reasons, relaxation techniques have been attracting much attention of researchers, by priorly targeting approximation solutions (e.g. $\ell_1$-norm, pursuit strategies). On the contrary, this paper considers the exact $\ell_0$-norm optimization problem and proves that it can be solved effectively, despite of its complexity. The proposed method reformulates the problem as a Mixed-Integer Quadratic Program (MIQP) and gets the global optimal solution by applying existing optimization software. Because the main difficulty of this approach is its computational time, two techniques are introduced that improve the computational speed. Finally, our method is applied to image denoising which shows its feasibility and relevance compared to the state-of-the-art.



### ExprGAN: Facial Expression Editing with Controllable Expression Intensity
- **Arxiv ID**: http://arxiv.org/abs/1709.03842v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03842v2)
- **Published**: 2017-09-12 13:51:54+00:00
- **Updated**: 2017-09-13 13:05:23+00:00
- **Authors**: Hui Ding, Kumar Sricharan, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expression editing is a challenging task as it needs a high-level semantic understanding of the input face image. In conventional methods, either paired training data is required or the synthetic face resolution is low. Moreover, only the categories of facial expression can be changed. To address these limitations, we propose an Expression Generative Adversarial Network (ExprGAN) for photo-realistic facial expression editing with controllable expression intensity. An expression controller module is specially designed to learn an expressive and compact expression code in addition to the encoder-decoder network. This novel architecture enables the expression intensity to be continuously adjusted from low to high. We further show that our ExprGAN can be applied for other tasks, such as expression transfer, image retrieval, and data augmentation for training improved face expression recognition models. To tackle the small size of the training database, an effective incremental learning scheme is proposed. Quantitative and qualitative evaluations on the widely used Oulu-CASIA dataset demonstrate the effectiveness of ExprGAN.



### A Deep Cascade Network for Unaligned Face Attribute Classification
- **Arxiv ID**: http://arxiv.org/abs/1709.03851v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03851v2)
- **Published**: 2017-09-12 14:05:14+00:00
- **Updated**: 2017-09-13 13:01:25+00:00
- **Authors**: Hui Ding, Hao Zhou, Shaohua Kevin Zhou, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Humans focus attention on different face regions when recognizing face attributes. Most existing face attribute classification methods use the whole image as input. Moreover, some of these methods rely on fiducial landmarks to provide defined face parts. In this paper, we propose a cascade network that simultaneously learns to localize face regions specific to attributes and performs attribute classification without alignment. First, a weakly-supervised face region localization network is designed to automatically detect regions (or parts) specific to attributes. Then multiple part-based networks and a whole-image-based network are separately constructed and combined together by the region switch layer and attribute relation layer for final attribute classification. A multi-net learning method and hint-based model compression is further proposed to get an effective localization model and a compact classification model, respectively. Our approach achieves significantly better performance than state-of-the-art methods on unaligned CelebA dataset, reducing the classification error by 30.9%.



### Kernel Cross-Correlator
- **Arxiv ID**: http://arxiv.org/abs/1709.05936v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05936v4)
- **Published**: 2017-09-12 15:09:29+00:00
- **Updated**: 2018-02-26 08:45:40+00:00
- **Authors**: Chen Wang, Le Zhang, Lihua Xie, Junsong Yuan
- **Comment**: The Thirty-Second AAAI Conference on Artificial Intelligence
  (AAAI-18)
- **Journal**: The Thirty-Second AAAI Conference on Artificial Intelligence
  (AAAI-2018)
- **Summary**: Cross-correlator plays a significant role in many visual perception tasks, such as object detection and tracking. Beyond the linear cross-correlator, this paper proposes a kernel cross-correlator (KCC) that breaks traditional limitations. First, by introducing the kernel trick, the KCC extends the linear cross-correlation to non-linear space, which is more robust to signal noises and distortions. Second, the connection to the existing works shows that KCC provides a unified solution for correlation filters. Third, KCC is applicable to any kernel function and is not limited to circulant structure on training data, thus it is able to predict affine transformations with customized properties. Last, by leveraging the fast Fourier transform (FFT), KCC eliminates direct calculation of kernel vectors, thus achieves better performance yet still with a reasonable computational cost. Comprehensive experiments on visual tracking and human activity recognition using wearable devices demonstrate its robustness, flexibility, and efficiency. The source codes of both experiments are released at https://github.com/wang-chen/KCC



### End-to-End Audiovisual Fusion with LSTMs
- **Arxiv ID**: http://arxiv.org/abs/1709.04343v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04343v1)
- **Published**: 2017-09-12 15:23:55+00:00
- **Updated**: 2017-09-12 15:23:55+00:00
- **Authors**: Stavros Petridis, Yujiang Wang, Zuwei Li, Maja Pantic
- **Comment**: Accepted to AVSP 2017. arXiv admin note: substantial text overlap
  with arXiv:1709.00443 and text overlap with arXiv:1701.05847
- **Journal**: None
- **Summary**: Several end-to-end deep learning approaches have been recently presented which simultaneously extract visual features from the input images and perform visual speech classification. However, research on jointly extracting audio and visual features and performing classification is very limited. In this work, we present an end-to-end audiovisual model based on Bidirectional Long Short-Term Memory (BLSTM) networks. To the best of our knowledge, this is the first audiovisual fusion model which simultaneously learns to extract features directly from the pixels and spectrograms and perform classification of speech and nonlinguistic vocalisations. The model consists of multiple identical streams, one for each modality, which extract features directly from mouth regions and spectrograms. The temporal dynamics in each stream/modality are modeled by a BLSTM and the fusion of multiple streams/modalities takes place via another BLSTM. An absolute improvement of 1.9% in the mean F1 of 4 nonlingusitic vocalisations over audio-only classification is reported on the AVIC database. At the same time, the proposed end-to-end audiovisual fusion system improves the state-of-the-art performance on the AVIC database leading to a 9.7% absolute increase in the mean F1 measure. We also perform audiovisual speech recognition experiments on the OuluVS2 database using different views of the mouth, frontal to profile. The proposed audiovisual system significantly outperforms the audio-only model for all views when the acoustic noise is high.



### Image Matching: An Application-oriented Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1709.03917v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03917v4)
- **Published**: 2017-09-12 15:41:34+00:00
- **Updated**: 2018-08-07 08:00:22+00:00
- **Authors**: JiaWang Bian, Le Zhang, Yun Liu, Wen-Yan Lin, Ming-Ming Cheng, Ian D. Reid
- **Comment**: We made a significant change and re-submitted as "MatchBench: An
  Evaluation of Feature Matchers"
- **Journal**: None
- **Summary**: Image matching approaches have been widely used in computer vision applications in which the image-level matching performance of matchers is critical. However, it has not been well investigated by previous works which place more emphases on evaluating local features. To this end, we present a uniform benchmark with novel evaluation metrics and a large-scale dataset for evaluating the overall performance of image matching methods. The proposed metrics are application-oriented as they emphasize application requirements for matchers. The dataset contains two portions for benchmarking video frame matching and unordered image matching separately, where each portion consists of real-world image sequences and each sequence has a specific attribute. Subsequently, we carry out a comprehensive performance evaluation of different state-of-the-art methods and conduct in-depth analyses regarding various aspects such as application requirements, matching types, and data diversity. Moreover, we shed light on how to choose appropriate approaches for different applications based on empirical results and analyses. Conclusions in this benchmark can be used as general guidelines to design practical matching systems and also advocate potential future research directions in this field.



### End-to-End United Video Dehazing and Detection
- **Arxiv ID**: http://arxiv.org/abs/1709.03919v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.03919v1)
- **Published**: 2017-09-12 15:43:28+00:00
- **Updated**: 2017-09-12 15:43:28+00:00
- **Authors**: Boyi Li, Xiulian Peng, Zhangyang Wang, Jizheng Xu, Dan Feng
- **Comment**: None
- **Journal**: None
- **Summary**: The recent development of CNN-based image dehazing has revealed the effectiveness of end-to-end modeling. However, extending the idea to end-to-end video dehazing has not been explored yet. In this paper, we propose an End-to-End Video Dehazing Network (EVD-Net), to exploit the temporal consistency between consecutive video frames. A thorough study has been conducted over a number of structure options, to identify the best temporal fusion strategy. Furthermore, we build an End-to-End United Video Dehazing and Detection Network(EVDD-Net), which concatenates and jointly trains EVD-Net with a video object detection model. The resulting augmented end-to-end pipeline has demonstrated much more stable and accurate detection results in hazy video.



### Unsupervised Deep Homography: A Fast and Robust Homography Estimation Model
- **Arxiv ID**: http://arxiv.org/abs/1709.03966v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03966v3)
- **Published**: 2017-09-12 17:30:58+00:00
- **Updated**: 2018-02-21 04:41:07+00:00
- **Authors**: Ty Nguyen, Steven W. Chen, Shreyas S. Shivakumar, Camillo J. Taylor, Vijay Kumar
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Homography estimation between multiple aerial images can provide relative pose estimation for collaborative autonomous exploration and monitoring. The usage on a robotic system requires a fast and robust homography estimation algorithm. In this study, we propose an unsupervised learning algorithm that trains a Deep Convolutional Neural Network to estimate planar homographies. We compare the proposed algorithm to traditional feature-based and direct methods, as well as a corresponding supervised learning algorithm. Our empirical results demonstrate that compared to traditional approaches, the unsupervised algorithm achieves faster inference speed, while maintaining comparable or better accuracy and robustness to illumination variation. In addition, on both a synthetic dataset and representative real-world aerial dataset, our unsupervised method has superior adaptability and performance compared to the supervised deep learning method.



### Multi-scale Forest Species Recognition Systems for Reduced Cost
- **Arxiv ID**: http://arxiv.org/abs/1709.04056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04056v1)
- **Published**: 2017-09-12 20:49:11+00:00
- **Updated**: 2017-09-12 20:49:11+00:00
- **Authors**: Paulo R. Cavalin, Marcelo N. Kapp, Luiz S. Oliveira
- **Comment**: None
- **Journal**: None
- **Summary**: This work focuses on cost reduction methods for forest species recognition systems. Current state-of-the-art shows that the accuracy of these systems have increased considerably in the past years, but the cost in time to perform the recognition of input samples has also increased proportionally. For this reason, in this work we focus on investigating methods for cost reduction locally (at either feature extraction or classification level individually) and globally (at both levels combined), and evaluate two main aspects: 1) the impact in cost reduction, given the proposed measures for it; and 2) the impact in recognition accuracy. The experimental evaluation conducted on two forest species datasets demonstrated that, with global cost reduction, the cost of the system can be reduced to less than 1/20 and recognition rates that are better than those of the original system can be achieved.



### MUFold-SS: Protein Secondary Structure Prediction Using Deep Inception-Inside-Inception Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.06165v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1709.06165v1)
- **Published**: 2017-09-12 20:59:27+00:00
- **Updated**: 2017-09-12 20:59:27+00:00
- **Authors**: Chao Fang, Yi Shang, Dong Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Motivation: Protein secondary structure prediction can provide important information for protein 3D structure prediction and protein functions. Deep learning, which has been successfully applied to various research fields such as image classification and voice recognition, provides a new opportunity to significantly improve the secondary structure prediction accuracy. Although several deep-learning methods have been developed for secondary structure prediction, there is room for improvement. MUFold-SS was developed to address these issues. Results: Here, a very deep neural network, the deep inception-inside-inception networks (Deep3I), is proposed for protein secondary structure prediction and a software tool was implemented using this network. This network takes two inputs: a protein sequence and a profile generated by PSI-BLAST. The output is the predicted eight states (Q8) or three states (Q3) of secondary structures. The proposed Deep3I not only achieves the state-of-the-art performance but also runs faster than other tools. Deep3I achieves Q3 82.8% and Q8 71.1% accuracies on the CB513 benchmark.



### Streamlined Deployment for Quantized Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.04060v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04060v2)
- **Published**: 2017-09-12 21:14:57+00:00
- **Updated**: 2018-05-30 14:20:28+00:00
- **Authors**: Yaman Umuroglu, Magnus Jahre
- **Comment**: Presented at the International Workshop on Highly Efficient Neural
  Networks Design (HENND) co-located with CASES'17
- **Journal**: None
- **Summary**: Running Deep Neural Network (DNN) models on devices with limited computational capability is a challenge due to large compute and memory requirements. Quantized Neural Networks (QNNs) have emerged as a potential solution to this problem, promising to offer most of the DNN accuracy benefits with much lower computational cost. However, harvesting these benefits on existing mobile CPUs is a challenge since operations on highly quantized datatypes are not natively supported in most instruction set architectures (ISAs). In this work, we first describe a streamlining flow to convert all QNN inference operations to integer ones. Afterwards, we provide techniques based on processing one bit position at a time (bit-serial) to show how QNNs can be efficiently deployed using common bitwise operations. We demonstrate the potential of QNNs on mobile CPUs with microbenchmarks and on a quantized AlexNet, which is 3.5x faster than an optimized 8-bit baseline. Our bit-serial matrix multiplication library is available on GitHub at https://git.io/vhshn



