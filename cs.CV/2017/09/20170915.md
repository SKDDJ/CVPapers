# Arxiv Papers in cs.CV on 2017-09-15
### ClickBAIT: Click-based Accelerated Incremental Training of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.05021v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, C.1.3
- **Links**: [PDF](http://arxiv.org/pdf/1709.05021v1)
- **Published**: 2017-09-15 00:58:38+00:00
- **Updated**: 2017-09-15 00:58:38+00:00
- **Authors**: Ervin Teng, João Diogo Falcão, Bob Iannucci
- **Comment**: 11 pages, 14 figures. Datasets available at
  http://clickbait.crossmobile.info
- **Journal**: None
- **Summary**: Today's general-purpose deep convolutional neural networks (CNN) for image classification and object detection are trained offline on large static datasets. Some applications, however, will require training in real-time on live video streams with a human-in-the-loop. We refer to this class of problem as Time-ordered Online Training (ToOT) - these problems will require a consideration of not only the quantity of incoming training data, but the human effort required to tag and use it. In this paper, we define training benefit as a metric to measure the effectiveness of a sequence in using each user interaction. We demonstrate and evaluate a system tailored to performing ToOT in the field, capable of training an image classifier on a live video stream through minimal input from a human operator. We show that by exploiting the time-ordered nature of the video stream through optical flow-based object tracking, we can increase the effectiveness of human actions by about 8 times.



### Adaptive compressed 3D imaging based on wavelet trees and Hadamard multiplexing with a single photon counting detector
- **Arxiv ID**: http://arxiv.org/abs/1709.05961v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05961v1)
- **Published**: 2017-09-15 02:23:36+00:00
- **Updated**: 2017-09-15 02:23:36+00:00
- **Authors**: Huidong Dai, Weiji He, Guohua Gu, Ling Ye, Tianyi Mao, Qian Chen
- **Comment**: 11 pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: Photon counting 3D imaging allows to obtain 3D images with single-photon sensitivity and sub-ns temporal resolution. However, it is challenging to scale to high spatial resolution. In this work, we demonstrate a photon counting 3D imaging technique with short-pulsed structured illumination and a single-pixel photon counting detector. The proposed multi-resolution photon counting 3D imaging technique acquires a high-resolution 3D image from a coarse image and edges at successfully finer resolution sampled by Hadamard multiplexing along the wavelet trees. The detected power is significantly increased thanks to the Hadamard multiplexing. Both the required measurements and the reconstruction time can be significantly reduced by performing wavelet-tree-based regions of edges predication and Hadamard demultiplexing, which makes the proposed technique suitable for scenes with high spatial resolution. The experimental results indicate that a 3D image at resolution up to 512*512 pixels can be acquired and retrieved with practical time as low as 17 seconds.



### Self-Guiding Multimodal LSTM - when we do not have a perfect training dataset for image captioning
- **Arxiv ID**: http://arxiv.org/abs/1709.05038v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.05038v1)
- **Published**: 2017-09-15 02:53:16+00:00
- **Updated**: 2017-09-15 02:53:16+00:00
- **Authors**: Yang Xian, Yingli Tian
- **Comment**: The paper is under consideration at Computer Vision and Image
  Understanding
- **Journal**: None
- **Summary**: In this paper, a self-guiding multimodal LSTM (sg-LSTM) image captioning model is proposed to handle uncontrolled imbalanced real-world image-sentence dataset. We collect FlickrNYC dataset from Flickr as our testbed with 306,165 images and the original text descriptions uploaded by the users are utilized as the ground truth for training. Descriptions in FlickrNYC dataset vary dramatically ranging from short term-descriptions to long paragraph-descriptions and can describe any visual aspects, or even refer to objects that are not depicted. To deal with the imbalanced and noisy situation and to fully explore the dataset itself, we propose a novel guiding textual feature extracted utilizing a multimodal LSTM (m-LSTM) model. Training of m-LSTM is based on the portion of data in which the image content and the corresponding descriptions are strongly bonded. Afterwards, during the training of sg-LSTM on the rest training data, this guiding information serves as additional input to the network along with the image representations and the ground-truth descriptions. By integrating these input components into a multimodal block, we aim to form a training scheme with the textual information tightly coupled with the image content. The experimental results demonstrate that the proposed sg-LSTM model outperforms the traditional state-of-the-art multimodal RNN captioning framework in successfully describing the key components of the input images.



### Feature-Fused SSD: Fast Detection for Small Objects
- **Arxiv ID**: http://arxiv.org/abs/1709.05054v3
- **DOI**: 10.1117/12.2304811
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05054v3)
- **Published**: 2017-09-15 04:21:05+00:00
- **Updated**: 2018-11-27 03:54:04+00:00
- **Authors**: Guimei Cao, Xuemei Xie, Wenzhe Yang, Quan Liao, Guangming Shi, Jinjian Wu
- **Comment**: Artificial Intelligence;8 pages,8 figures
- **Journal**: Ninth International Conference on Graphic and Image Processing
  (ICGIP 2017)
- **Summary**: Small objects detection is a challenging task in computer vision due to its limited resolution and information. In order to solve this problem, the majority of existing methods sacrifice speed for improvement in accuracy. In this paper, we aim to detect small objects at a fast speed, using the best object detector Single Shot Multibox Detector (SSD) with respect to accuracy-vs-speed trade-off as base architecture. We propose a multi-level feature fusion method for introducing contextual information in SSD, in order to improve the accuracy for small objects. In detailed fusion operation, we design two feature fusion modules, concatenation module and element-sum module, different in the way of adding contextual information. Experimental results show that these two fusion modules obtain higher mAP on PASCALVOC2007 than baseline SSD by 1.6 and 1.7 points respectively, especially with 2-3 points improvement on some smallobjects categories. The testing speed of them is 43 and 40 FPS respectively, superior to the state of the art Deconvolutional single shot detector (DSSD) by 29.4 and 26.4 FPS. Code is available at https://github.com/wnzhyee/Feature-Fused-SSD. Keywords: small object detection, feature fusion, real-time, single shot multi-box detector



### Learning Compact Geometric Features
- **Arxiv ID**: http://arxiv.org/abs/1709.05056v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.05056v1)
- **Published**: 2017-09-15 04:44:28+00:00
- **Updated**: 2017-09-15 04:44:28+00:00
- **Authors**: Marc Khoury, Qian-Yi Zhou, Vladlen Koltun
- **Comment**: International Conference on Computer Vision (ICCV), 2017
- **Journal**: None
- **Summary**: We present an approach to learning features that represent the local geometry around a point in an unstructured point cloud. Such features play a central role in geometric registration, which supports diverse applications in robotics and 3D vision. Current state-of-the-art local features for unstructured point clouds have been manually crafted and none combines the desirable properties of precision, compactness, and robustness. We show that features with these properties can be learned from data, by optimizing deep networks that map high-dimensional histograms into low-dimensional Euclidean spaces. The presented approach yields a family of features, parameterized by dimension, that are both more compact and more accurate than existing descriptors.



### Asian Stamps Identification and Classification System
- **Arxiv ID**: http://arxiv.org/abs/1709.05065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05065v1)
- **Published**: 2017-09-15 05:38:13+00:00
- **Updated**: 2017-09-15 05:38:13+00:00
- **Authors**: Behzad Mahaseni, Nabhan D. Salih
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of stamp recognition. The goal is to classify a given stamp to a certain country and also identify the year it is published. We propose a new approach for stamp recognition based on describing a given stamp image using color information and texture information. For color information we use color histogram for the entire image and for texture we use two features. SIFT which is based on local feature descriptors and HOG which is a dens texture descriptor. As a result on total we have three different types of features. Our initial evaluation shows that give these information we are able to classify the images with a reasonable accuracy.



### Joint Hierarchical Category Structure Learning and Large-Scale Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1709.05072v1
- **DOI**: 10.1109/TIP.2016.2615423
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05072v1)
- **Published**: 2017-09-15 06:49:12+00:00
- **Updated**: 2017-09-15 06:49:12+00:00
- **Authors**: Yanyun Qu, Li Lin, Fumin Shen, Chang Lu, Yang Wu, Yuan Xie, Dacheng Tao
- **Comment**: 16 pages, 14 figures
- **Journal**: IEEE Trans. on Image Processing,2017
- **Summary**: We investigate the scalable image classification problem with a large number of categories. Hierarchical visual data structures are helpful for improving the efficiency and performance of large-scale multi-class classification. We propose a novel image classification method based on learning hierarchical inter-class structures. Specifically, we first design a fast algorithm to compute the similarity metric between categories, based on which a visual tree is constructed by hierarchical spectral clustering. Using the learned visual tree, a test sample label is efficiently predicted by searching for the best path over the entire tree. The proposed method is extensively evaluated on the ILSVRC2010 and Caltech 256 benchmark datasets. Experimental results show that our method obtains significantly better category hierarchies than other state-of-the-art visual tree-based methods and, therefore, much more accurate classification.



### Robust Kernelized Multi-View Self-Representations for Clustering by Tensor Multi-Rank Minimization
- **Arxiv ID**: http://arxiv.org/abs/1709.05083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05083v1)
- **Published**: 2017-09-15 07:32:20+00:00
- **Updated**: 2017-09-15 07:32:20+00:00
- **Authors**: Yanyun Qu, Jinyan Liu, Yuan Xie, Wensheng Zhang
- **Comment**: 8 pages, 5 figures, AAAI2018 submitted
- **Journal**: None
- **Summary**: Most recently, tensor-SVD is implemented on multi-view self-representation clustering and has achieved the promising results in many real-world applications such as face clustering, scene clustering and generic object clustering. However, tensor-SVD based multi-view self-representation clustering is proposed originally to solve the clustering problem in the multiple linear subspaces, leading to unsatisfactory results when dealing with the case of non-linear subspaces. To handle data clustering from the non-linear subspaces, a kernelization method is designed by mapping the data from the original input space to a new feature space in which the transformed data can be clustered by a multiple linear clustering method. In this paper, we make an optimization model for the kernelized multi-view self-representation clustering problem. We also develop a new efficient algorithm based on the alternation direction method and infer a closed-form solution. Since all the subproblems can be solved exactly, the proposed optimization algorithm is guaranteed to obtain the optimal solution. In particular, the original tensor-based multi-view self-representation clustering problem is a special case of our approach and can be solved by our algorithm. Experimental results on several popular real-world clustering datasets demonstrate that our approach achieves the state-of-the-art performance.



### Viewpoint Invariant Action Recognition using RGB-D Videos
- **Arxiv ID**: http://arxiv.org/abs/1709.05087v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05087v2)
- **Published**: 2017-09-15 07:39:34+00:00
- **Updated**: 2018-01-15 03:32:59+00:00
- **Authors**: Jian Liu, Naveed Akhtar, Ajmal Mian
- **Comment**: None
- **Journal**: None
- **Summary**: In video-based action recognition, viewpoint variations often pose major challenges because the same actions can appear different from different views. We use the complementary RGB and Depth information from the RGB-D cameras to address this problem. The proposed technique capitalizes on the spatio-temporal information available in the two data streams to the extract action features that are largely insensitive to the viewpoint variations. We use the RGB data to compute dense trajectories that are translated to viewpoint insensitive deep features under a non-linear knowledge transfer model. Similarly, the Depth stream is used to extract CNN-based view invariant features on which Fourier Temporal Pyramid is computed to incorporate the temporal information. The heterogeneous features from the two streams are combined and used as a dictionary to predict the label of the test samples. To that end, we propose a sparse-dense collaborative representation classification scheme that strikes a balance between the discriminative abilities of the dense and the sparse representations of the samples over the extracted heterogeneous dictionary.



### Multi-Label Zero-Shot Human Action Recognition via Joint Latent Ranking Embedding
- **Arxiv ID**: http://arxiv.org/abs/1709.05107v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.05107v3)
- **Published**: 2017-09-15 08:44:02+00:00
- **Updated**: 2019-02-06 12:42:58+00:00
- **Authors**: Qian Wang, Ke Chen
- **Comment**: 27 pages, 10 figures and 7 tables. Technical report submitted to a
  journal. More experimental results/references were added and typos were
  corrected
- **Journal**: None
- **Summary**: Human action recognition refers to automatic recognizing human actions from a video clip. In reality, there often exist multiple human actions in a video stream. Such a video stream is often weakly-annotated with a set of relevant human action labels at a global level rather than assigning each label to a specific video episode corresponding to a single action, which leads to a multi-label learning problem. Furthermore, there are many meaningful human actions in reality but it would be extremely difficult to collect/annotate video clips regarding all of various human actions, which leads to a zero-shot learning scenario. To the best of our knowledge, there is no work that has addressed all the above issues together in human action recognition. In this paper, we formulate a real-world human action recognition task as a multi-label zero-shot learning problem and propose a framework to tackle this problem in a holistic way. Our framework holistically tackles the issue of unknown temporal boundaries between different actions for multi-label learning and exploits the side information regarding the semantic relationship between different human actions for knowledge transfer. Consequently, our framework leads to a joint latent ranking embedding for multi-label zero-shot human action recognition. A novel neural architecture of two component models and an alternate learning algorithm are proposed to carry out the joint latent ranking embedding learning. Thus, multi-label zero-shot recognition is done by measuring relatedness scores of action labels to a test video clip in the joint latent visual and semantic embedding spaces. We evaluate our framework with different settings, including a novel data split scheme designed especially for evaluating multi-label zero-shot learning, on two datasets: Breakfast and Charades. The experimental results demonstrate the effectiveness of our framework.



### Towards CNN map representation and compression for camera relocalisation
- **Arxiv ID**: http://arxiv.org/abs/1709.05972v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.05972v2)
- **Published**: 2017-09-15 11:02:22+00:00
- **Updated**: 2018-05-16 08:10:09+00:00
- **Authors**: Luis Contreras, Walterio Mayol-Cuevas
- **Comment**: Submitted to the 1st International Workshop on Deep Learning for
  Visual SLAM, at the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)
- **Journal**: None
- **Summary**: This paper presents a study on the use of Convolutional Neural Networks for camera relocalisation and its application to map compression. We follow state of the art visual relocalisation results and evaluate the response to different data inputs. We use a CNN map representation and introduce the notion of map compression under this paradigm by using smaller CNN architectures without sacrificing relocalisation performance. We evaluate this approach in a series of publicly available datasets over a number of CNN architectures with different sizes, both in complexity and number of layers. This formulation allows us to improve relocalisation accuracy by increasing the number of training trajectories while maintaining a constant-size CNN.



### Multi-scale Deep Learning Architectures for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1709.05165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05165v1)
- **Published**: 2017-09-15 11:53:59+00:00
- **Updated**: 2017-09-15 11:53:59+00:00
- **Authors**: Xuelin Qian, Yanwei Fu, Yu-Gang Jiang, Tao Xiang, Xiangyang Xue
- **Comment**: 9 pages, 3 figures, accepted by ICCV 2017
- **Journal**: None
- **Summary**: Person Re-identification (re-id) aims to match people across non-overlapping camera views in a public space. It is a challenging problem because many people captured in surveillance videos wear similar clothes. Consequently, the differences in their appearance are often subtle and only detectable at the right location and scales. Existing re-id models, particularly the recently proposed deep learning based ones match people at a single scale. In contrast, in this paper, a novel multi-scale deep learning model is proposed. Our model is able to learn deep discriminative feature representations at different scales and automatically determine the most suitable scales for matching. The importance of different spatial locations for extracting discriminative features is also learned explicitly. Experiments are carried out to demonstrate that the proposed model outperforms the state-of-the art on a number of benchmarks



### Unsupervised state representation learning with robotic priors: a robustness benchmark
- **Arxiv ID**: http://arxiv.org/abs/1709.05185v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.05185v1)
- **Published**: 2017-09-15 13:15:58+00:00
- **Updated**: 2017-09-15 13:15:58+00:00
- **Authors**: Timothée Lesort, Mathieu Seurin, Xinrui Li, Natalia Díaz-Rodríguez, David Filliat
- **Comment**: ICRA 2018 submission
- **Journal**: None
- **Summary**: Our understanding of the world depends highly on our capacity to produce intuitive and simplified representations which can be easily used to solve problems. We reproduce this simplification process using a neural network to build a low dimensional state representation of the world from images acquired by a robot. As in Jonschkowski et al. 2015, we learn in an unsupervised way using prior knowledge about the world as loss functions called robotic priors and extend this approach to high dimension richer images to learn a 3D representation of the hand position of a robot from RGB images. We propose a quantitative evaluation of the learned representation using nearest neighbors in the state space that allows to assess its quality and show both the potential and limitations of robotic priors in realistic environments. We augment image size, add distractors and domain randomization, all crucial components to achieve transfer learning to real robots. Finally, we also contribute a new prior to improve the robustness of the representation. The applications of such low dimensional state representation range from easing reinforcement learning (RL) and knowledge transfer across tasks, to facilitating learning from raw data with more efficient and compact high level representations. The results show that the robotic prior approach is able to extract high level representation as the 3D position of an arm and organize it into a compact and coherent space of states in a challenging dataset.



### Adversarial Occlusion-aware Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1709.05188v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05188v6)
- **Published**: 2017-09-15 13:22:22+00:00
- **Updated**: 2018-09-29 22:25:40+00:00
- **Authors**: Yujia Chen, Lingxiao Song, Ran He
- **Comment**: Accepted by ACPR2018
- **Journal**: None
- **Summary**: Occluded face detection is a challenging detection task due to the large appearance variations incurred by various real-world occlusions. This paper introduces an Adversarial Occlusion-aware Face Detector (AOFD) by simultaneously detecting occluded faces and segmenting occluded areas. Specifically, we employ an adversarial training strategy to generate occlusion-like face features that are difficult for a face detector to recognize. Occlusion mask is predicted simultaneously while detecting occluded faces and the occluded area is utilized as an auxiliary instead of being regarded as a hindrance. Moreover, the supervisory signals from the segmentation branch will reversely affect the features, aiding in detecting heavily-occluded faces accordingly. Consequently, AOFD is able to find the faces with few exposed facial landmarks with very high confidences and keeps high detection accuracy even for masked faces. Extensive experiments demonstrate that AOFD not only significantly outperforms state-of-the-art methods on the MAFA occluded face detection dataset, but also achieves competitive detection accuracy on benchmark dataset for general face detection such as FDDB.



### Commonsense Scene Semantics for Cognitive Robotics: Towards Grounding Embodied Visuo-Locomotive Interactions
- **Arxiv ID**: http://arxiv.org/abs/1709.05293v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.05293v1)
- **Published**: 2017-09-15 16:22:56+00:00
- **Updated**: 2017-09-15 16:22:56+00:00
- **Authors**: Jakob Suchan, Mehul Bhatt
- **Comment**: to appear in: ICCV 2017 Workshop - Vision in Practice on Autonomous
  Robots (ViPAR), International Conference on Computer Vision (ICCV), Venice,
  Italy
- **Journal**: None
- **Summary**: We present a commonsense, qualitative model for the semantic grounding of embodied visuo-spatial and locomotive interactions. The key contribution is an integrative methodology combining low-level visual processing with high-level, human-centred representations of space and motion rooted in artificial intelligence. We demonstrate practical applicability with examples involving object interactions, and indoor movement.



### Top-Down Saliency Detection Driven by Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/1709.05307v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05307v3)
- **Published**: 2017-09-15 16:58:57+00:00
- **Updated**: 2018-03-23 18:39:30+00:00
- **Authors**: Francesca Murabito, Concetto Spampinato, Simone Palazzo, Konstantin Pogorelov, Michael Riegler
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an approach for top-down saliency detection guided by visual classification tasks. We first learn how to compute visual saliency when a specific visual task has to be accomplished, as opposed to most state-of-the-art methods which assess saliency merely through bottom-up principles. Afterwards, we investigate if and to what extent visual saliency can support visual classification in nontrivial cases. To achieve this, we propose SalClassNet, a CNN framework consisting of two networks jointly trained: a) the first one computing top-down saliency maps from input images, and b) the second one exploiting the computed saliency maps for visual classification. To test our approach, we collected a dataset of eye-gaze maps, using a Tobii T60 eye tracker, by asking several subjects to look at images from the Stanford Dogs dataset, with the objective of distinguishing dog breeds. Performance analysis on our dataset and other saliency bench-marking datasets, such as POET, showed that SalClassNet out-performs state-of-the-art saliency detectors, such as SalNet and SALICON. Finally, we analyzed the performance of SalClassNet in a fine-grained recognition task and found out that it generalizes better than existing visual classifiers. The achieved results, thus, demonstrate that 1) conditioning saliency detectors with object classes reaches state-of-the-art performance, and 2) providing explicitly top-down saliency maps to visual classifiers enhances classification accuracy.



### Video Synopsis Generation Using Spatio-Temporal Groups
- **Arxiv ID**: http://arxiv.org/abs/1709.05311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05311v1)
- **Published**: 2017-09-15 17:05:58+00:00
- **Updated**: 2017-09-15 17:05:58+00:00
- **Authors**: A. Ahmed, D. P. Dogra, S. Kar, R. Patnaik, S. Lee, H. Choi, I. Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Millions of surveillance cameras operate at 24x7 generating huge amount of visual data for processing. However, retrieval of important activities from such a large data can be time consuming. Thus, researchers are working on finding solutions to present hours of visual data in a compressed, but meaningful way. Video synopsis is one of the ways to represent activities using relatively shorter duration clips. So far, two main approaches have been used by researchers to address this problem, namely synopsis by tracking moving objects and synopsis by clustering moving objects. Synopses outputs, mainly depend on tracking, segmenting, and shifting of moving objects temporally as well as spatially. In many situations, tracking fails, thus produces multiple trajectories of the same object. Due to this, the object may appear and disappear multiple times within the same synopsis output, which is misleading. This also leads to discontinuity and often can be confusing to the viewer of the synopsis. In this paper, we present a new approach for generating compressed video synopsis by grouping tracklets of moving objects. Grouping helps to generate a synopsis where chronologically related objects appear together with meaningful spatio-temporal relation. Our proposed method produces continuous, but a less confusing synopses when tested on publicly available dataset videos as well as in-house dataset videos.



### Cystoid macular edema segmentation of Optical Coherence Tomography images using fully convolutional neural networks and fully connected CRFs
- **Arxiv ID**: http://arxiv.org/abs/1709.05324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05324v1)
- **Published**: 2017-09-15 17:33:19+00:00
- **Updated**: 2017-09-15 17:33:19+00:00
- **Authors**: Fangliang Bai, Manuel J. Marques, Stuart J. Gibson
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a new method for cystoid macular edema (CME) segmentation in retinal Optical Coherence Tomography (OCT) images, using a fully convolutional neural network (FCN) and a fully connected conditional random fields (dense CRFs). As a first step, the framework trains the FCN model to extract features from retinal layers in OCT images, which exhibit CME, and then segments CME regions using the trained model. Thereafter, dense CRFs are used to refine the segmentation according to the edema appearance. We have trained and tested the framework with OCT images from 10 patients with diabetic macular edema (DME). Our experimental results show that fluid and concrete macular edema areas were segmented with good adherence to boundaries. A segmentation accuracy of $0.61\pm 0.21$ (Dice coefficient) was achieved, with respect to the ground truth, which compares favourably with the previous state-of-the-art that used a kernel regression based method ($0.51\pm 0.34$). Our approach is versatile and we believe it can be easily adapted to detect other macular defects.



### Embedding Deep Networks into Visual Explanations
- **Arxiv ID**: http://arxiv.org/abs/1709.05360v3
- **DOI**: 10.1016/j.artint.2020.103435
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1709.05360v3)
- **Published**: 2017-09-15 18:16:34+00:00
- **Updated**: 2020-12-11 09:26:19+00:00
- **Authors**: Zhongang Qi, Saeed Khorram, Fuxin Li
- **Comment**: None
- **Journal**: Artificial Intelligence (2020)
- **Summary**: In this paper, we propose a novel Explanation Neural Network (XNN) to explain the predictions made by a deep network. The XNN works by learning a nonlinear embedding of a high-dimensional activation vector of a deep network layer into a low-dimensional explanation space while retaining faithfulness i.e., the original deep learning predictions can be constructed from the few concepts extracted by our explanation network. We then visualize such concepts for human to learn about the high-level concepts that the deep network is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A pull-away term is applied to SRAE to make the bases of the explanation space more orthogonal to each other. A visualization system is then introduced for human understanding of the features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks. We conducted a human study, which shows that the proposed approach outperforms single saliency map baselines, and improves human performance on a difficult classification tasks. Also, several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement.



### General Phase Regularized Reconstruction using Phase Cycling
- **Arxiv ID**: http://arxiv.org/abs/1709.05374v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1709.05374v1)
- **Published**: 2017-09-15 19:17:13+00:00
- **Updated**: 2017-09-15 19:17:13+00:00
- **Authors**: Frank Ong, Joseph Cheng, Michael Lustig
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To develop a general phase regularized image reconstruction method, with applications to partial Fourier imaging, water-fat imaging and flow imaging.   Theory and Methods: The problem of enforcing phase constraints in reconstruction was studied under a regularized inverse problem framework. A general phase regularized reconstruction algorithm was proposed to enable various joint reconstruction of partial Fourier imaging, water-fat imaging and flow imaging, along with parallel imaging (PI) and compressed sensing (CS). Since phase regularized reconstruction is inherently non-convex and sensitive to phase wraps in the initial solution, a reconstruction technique, named phase cycling, was proposed to render the overall algorithm invariant to phase wraps. The proposed method was applied to retrospectively under-sampled in vivo datasets and compared with state of the art reconstruction methods.   Results: Phase cycling reconstructions showed reduction of artifacts compared to reconstructions with- out phase cycling and achieved similar performances as state of the art results in partial Fourier, water-fat and divergence-free regularized flow reconstruction. Joint reconstruction of partial Fourier + water-fat imaging + PI + CS, and partial Fourier + divergence-free regularized flow imaging + PI + CS were demonstrated.   Conclusion: The proposed phase cycling reconstruction provides an alternative way to perform phase regularized reconstruction, without the need to perform phase unwrapping. It is robust to the choice of initial solutions and encourages the joint reconstruction of phase imaging applications.



### Zero-Shot Learning to Manage a Large Number of Place-Specific Compressive Change Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1709.05397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05397v1)
- **Published**: 2017-09-15 20:43:40+00:00
- **Updated**: 2017-09-15 20:43:40+00:00
- **Authors**: Tanaka Kanji
- **Comment**: 8 pages, 11 figures, technical report
- **Journal**: None
- **Summary**: With recent progress in large-scale map maintenance and long-term map learning, the task of change detection on a large-scale map from a visual image captured by a mobile robot has become a problem of increasing criticality. Previous approaches for change detection are typically based on image differencing and require the memorization of a prohibitively large number of mapped images in the above context. In contrast, this study follows the recent, efficient paradigm of change-classifier-learning and specifically employs a collection of place-specific change classifiers. Our change-classifier-learning algorithm is based on zero-shot learning (ZSL) and represents a place-specific change classifier by its training examples mined from an external knowledge base (EKB). The proposed algorithm exhibits several advantages. First, we are required to memorize only training examples (rather than the classifier itself), which can be further compressed in the form of bag-of-words (BoW). Secondly, we can incorporate the most recent map into the classifiers by straightforwardly adding or deleting a few training examples that correspond to these classifiers. Thirdly, we can share the BoW vocabulary with other related task scenarios (e.g., BoW-based self-localization), wherein the vocabulary is generally designed as a rich, continuously growing, and domain-adaptive knowledge base. In our contribution, the proposed algorithm is applied and evaluated on a practical long-term cross-season change detection system that consists of a large number of place-specific object-level change classifiers.



### NIMA: Neural Image Assessment
- **Arxiv ID**: http://arxiv.org/abs/1709.05424v2
- **DOI**: 10.1109/TIP.2018.2831899
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05424v2)
- **Published**: 2017-09-15 22:04:49+00:00
- **Updated**: 2018-04-26 23:35:58+00:00
- **Authors**: Hossein Talebi, Peyman Milanfar
- **Comment**: IEEE Transactions on Image Processing 2018
- **Journal**: None
- **Summary**: Automatically learned quality assessment for images has recently become a hot topic due to its usefulness in a wide variety of applications such as evaluating image capture pipelines, storage techniques and sharing media. Despite the subjective nature of this problem, most existing methods only predict the mean opinion score provided by datasets such as AVA [1] and TID2013 [2]. Our approach differs from others in that we predict the distribution of human opinion scores using a convolutional neural network. Our architecture also has the advantage of being significantly simpler than other methods with comparable performance. Our proposed approach relies on the success (and retraining) of proven, state-of-the-art deep object recognition networks. Our resulting network can be used to not only score images reliably and with high correlation to human perception, but also to assist with adaptation and optimization of photo editing/enhancement algorithms in a photographic pipeline. All this is done without need for a "golden" reference image, consequently allowing for single-image, semantic- and perceptually-aware, no-reference quality assessment.



