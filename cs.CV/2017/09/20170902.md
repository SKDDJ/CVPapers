# Arxiv Papers in cs.CV on 2017-09-02
### Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.00513v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.00513v2)
- **Published**: 2017-09-02 01:03:08+00:00
- **Updated**: 2018-04-16 18:42:13+00:00
- **Authors**: Zheng Xu, Yen-Chang Hsu, Jiawei Huang
- **Comment**: Shorter version will appear at ICLR workshop 2018
- **Journal**: None
- **Summary**: There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information learned from a large and accurate teacher network. We propose to use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The proposed method is particularly effective for relatively small student networks. Moreover, experimental results show the effect of network size when the modern networks are used as student. We empirically study the trade-off between inference time and classification accuracy, and provide suggestions on choosing a proper student network.



### Gaussian Filter in CRF Based Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1709.00516v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T04
- **Links**: [PDF](http://arxiv.org/pdf/1709.00516v1)
- **Published**: 2017-09-02 01:38:34+00:00
- **Updated**: 2017-09-02 01:38:34+00:00
- **Authors**: Yichi Gu, Qisheng Wu, Jing Li, Kai Cheng
- **Comment**: 11 pages, 9 figures, 2 tables
- **Journal**: None
- **Summary**: Artificial intelligence is making great changes in academy and industry with the fast development of deep learning, which is a branch of machine learning and statistical learning. Fully convolutional network [1] is the standard model for semantic segmentation. Conditional random fields coded as CNN [2] or RNN [3] and connected with FCN has been successfully applied in object detection [4]. In this paper, we introduce a multi-resolution neural network for FCN and apply Gaussian filter to the extended CRF kernel neighborhood and the label image to reduce the oscillating effect of CRF neural network segmentation, thus achieve higher precision and faster training speed.



### Facial 3D Model Registration Under Occlusions With SensiblePoints-based Reinforced Hypothesis Refinement
- **Arxiv ID**: http://arxiv.org/abs/1709.00531v1
- **DOI**: 10.1109/BTAS.2017.8272734
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00531v1)
- **Published**: 2017-09-02 04:00:29+00:00
- **Updated**: 2017-09-02 04:00:29+00:00
- **Authors**: Yuhang Wu, Ioannis A. Kakadiaris
- **Comment**: Accepted in International Joint Conference on Biometrics (IJCB) 2017
- **Journal**: 2017 IEEE International Joint Conference on Biometrics
- **Summary**: Registering a 3D facial model to a 2D image under occlusion is difficult. First, not all of the detected facial landmarks are accurate under occlusions. Second, the number of reliable landmarks may not be enough to constrain the problem. We propose a method to synthesize additional points (SensiblePoints) to create pose hypotheses. The visual clues extracted from the fiducial points, non-fiducial points, and facial contour are jointly employed to verify the hypotheses. We define a reward function to measure whether the projected dense 3D model is well-aligned with the confidence maps generated by two fully convolutional networks, and use the function to train recurrent policy networks to move the SensiblePoints. The same reward function is employed in testing to select the best hypothesis from a candidate pool of hypotheses. Experimentation demonstrates that the proposed approach is very promising in solving the facial model registration problem under occlusion.



### Learning Dense Facial Correspondences in Unconstrained Images
- **Arxiv ID**: http://arxiv.org/abs/1709.00536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00536v1)
- **Published**: 2017-09-02 06:02:26+00:00
- **Updated**: 2017-09-02 06:02:26+00:00
- **Authors**: Ronald Yu, Shunsuke Saito, Haoxiang Li, Duygu Ceylan, Hao Li
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: We present a minimalistic but effective neural network that computes dense facial correspondences in highly unconstrained RGB images. Our network learns a per-pixel flow and a matchability mask between 2D input photographs of a person and the projection of a textured 3D face model. To train such a network, we generate a massive dataset of synthetic faces with dense labels using renderings of a morphable face model with variations in pose, expressions, lighting, and occlusions. We found that a training refinement using real photographs is required to drastically improve the ability to handle real images. When combined with a facial detection and 3D face fitting step, we show that our approach outperforms the state-of-the-art face alignment methods in terms of accuracy and speed. By directly estimating dense correspondences, we do not rely on the full visibility of sparse facial landmarks and are not limited to the model space of regression-based approaches. We also assess our method on video frames and demonstrate successful per-frame processing under extreme pose variations, occlusions, and lighting conditions. Compared to existing 3D facial tracking techniques, our fitting does not rely on previous frames or frontal facial initialization and is robust to imperfect face detections.



### Deep Galaxy: Classification of Galaxies based on Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.02245v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02245v1)
- **Published**: 2017-09-02 11:14:43+00:00
- **Updated**: 2017-09-02 11:14:43+00:00
- **Authors**: Nour Eldeen M. Khalifa, Mohamed Hamed N. Taha, Aboul Ella Hassanien, I. M. Selim
- **Comment**: 4 pages, 6 figures, 2 tables, Conference
- **Journal**: None
- **Summary**: In this paper, a deep convolutional neural network architecture for galaxies classification is presented. The galaxy can be classified based on its features into main three categories Elliptical, Spiral, and Irregular. The proposed deep galaxies architecture consists of 8 layers, one main convolutional layer for features extraction with 96 filters, followed by two principles fully connected layers for classification. It is trained over 1356 images and achieved 97.272% in testing accuracy. A comparative result is made and the testing accuracy was compared with other related works. The proposed architecture outperformed other related works in terms of testing accuracy.



### XFlow: Cross-modal Deep Neural Networks for Audiovisual Classification
- **Arxiv ID**: http://arxiv.org/abs/1709.00572v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.00572v2)
- **Published**: 2017-09-02 12:43:59+00:00
- **Updated**: 2019-04-12 21:43:42+00:00
- **Authors**: Cătălina Cangea, Petar Veličković, Pietro Liò
- **Comment**: Accepted at the IEEE ICDL-EPIROB 2017 Workshop on Computational
  Models for Crossmodal Learning (CMCML), 4 pages, 6 figures
- **Journal**: None
- **Summary**: In recent years, there have been numerous developments towards solving multimodal tasks, aiming to learn a stronger representation than through a single modality. Certain aspects of the data can be particularly useful in this case - for example, correlations in the space or time domain across modalities - but should be wisely exploited in order to benefit from their full predictive potential. We propose two deep learning architectures with multimodal cross-connections that allow for dataflow between several feature extractors (XFlow). Our models derive more interpretable features and achieve better performances than models which do not exchange representations, usefully exploiting correlations between audio and visual data, which have a different dimensionality and are nontrivially exchangeable. Our work improves on existing multimodal deep learning algorithms in two essential ways: (1) it presents a novel method for performing cross-modality (before features are learned from individual modalities) and (2) extends the previously proposed cross-connections which only transfer information between streams that process compatible data. Illustrating some of the representations learned by the connections, we analyse their contribution to the increase in discrimination ability and reveal their compatibility with a lip-reading network intermediate representation. We provide the research community with Digits, a new dataset consisting of three data types extracted from videos of people saying the digits 0-9. Results show that both cross-modal architectures outperform their baselines (by up to 11.5%) when evaluated on the AVletters, CUAVE and Digits datasets, achieving state-of-the-art results.



### Deep Learning-Guided Image Reconstruction from Incomplete Data
- **Arxiv ID**: http://arxiv.org/abs/1709.00584v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.00584v1)
- **Published**: 2017-09-02 14:15:24+00:00
- **Updated**: 2017-09-02 14:15:24+00:00
- **Authors**: Brendan Kelly, Thomas P. Matthews, Mark A. Anastasio
- **Comment**: None
- **Journal**: None
- **Summary**: An approach to incorporate deep learning within an iterative image reconstruction framework to reconstruct images from severely incomplete measurement data is presented. Specifically, we utilize a convolutional neural network (CNN) as a quasi-projection operator within a least squares minimization procedure. The CNN is trained to encode high level information about the class of images being imaged; this information is utilized to mitigate artifacts in intermediate images produced by use of an iterative method. The structure of the method was inspired by the proximal gradient descent method, where the proximal operator is replaced by a deep CNN and the gradient descent step is generalized by use of a linear reconstruction operator. It is demonstrated that this approach improves image quality for several cases of limited-view image reconstruction and that using a CNN in an iterative method increases performance compared to conventional image reconstruction approaches. We test our method on several limited-view image reconstruction problems. Qualitative and quantitative results demonstrate state-of-the-art performance.



### A Survey of Efficient Regression of General-Activity Human Poses from Depth Images
- **Arxiv ID**: http://arxiv.org/abs/1709.02246v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02246v1)
- **Published**: 2017-09-02 14:32:08+00:00
- **Updated**: 2017-09-02 14:32:08+00:00
- **Authors**: Wenye He
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a comprehensive review on regression-based method for human pose estimation. The problem of human pose estimation has been intensively studied and enabled many application from entertainment to training. Traditional methods often rely on color image only which cannot completely ambiguity of joint 3D position, especially in the complex context. With the popularity of depth sensors, the precision of 3D estimation has significant improvement. In this paper, we give a detailed analysis of state-of-the-art on human pose estimation, including depth image based and RGB-D based approaches. The experimental results demonstrate their advantages and limitation for different scenarios.



### Fast Image Processing with Fully-Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.00643v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.00643v1)
- **Published**: 2017-09-02 22:38:13+00:00
- **Updated**: 2017-09-02 22:38:13+00:00
- **Authors**: Qifeng Chen, Jia Xu, Vladlen Koltun
- **Comment**: Published at the International Conference on Computer Vision (ICCV
  2017)
- **Journal**: None
- **Summary**: We present an approach to accelerating a wide variety of image processing operators. Our approach uses a fully-convolutional network that is trained on input-output pairs that demonstrate the operator's action. After training, the original operator need not be run at all. The trained network operates at full resolution and runs in constant time. We investigate the effect of network architecture on approximation accuracy, runtime, and memory footprint, and identify a specific architecture that balances these considerations. We evaluate the presented approach on ten advanced image processing operators, including multiple variational models, multiscale tone and detail manipulation, photographic style transfer, nonlocal dehazing, and nonphotorealistic stylization. All operators are approximated by the same model. Experiments demonstrate that the presented approach is significantly more accurate than prior approximation schemes. It increases approximation accuracy as measured by PSNR across the evaluated operators by 8.5 dB on the MIT-Adobe dataset (from 27.5 to 36 dB) and reduces DSSIM by a multiplicative factor of 3 compared to the most accurate prior approximation scheme, while being the fastest. We show that our models generalize across datasets and across resolutions, and investigate a number of extensions of the presented approach. The results are shown in the supplementary video at https://youtu.be/eQyfHgLx8Dc



