# Arxiv Papers in cs.CV on 2017-09-04
### Machine learning methods for histopathological image analysis
- **Arxiv ID**: http://arxiv.org/abs/1709.00786v2
- **DOI**: 10.1016/j.csbj.2018.01.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00786v2)
- **Published**: 2017-09-04 02:13:15+00:00
- **Updated**: 2017-12-03 01:35:37+00:00
- **Authors**: Daisuke Komura, Shumpei Ishikawa
- **Comment**: 23 pages, 4 figures
- **Journal**: None
- **Summary**: Abundant accumulation of digital histopathological images has led to the increased demand for their analysis, such as computer-aided diagnosis using machine learning techniques. However, digital pathological images and related tasks have some issues to be considered. In this mini-review, we introduce the application of digital pathological image analysis using machine learning algorithms, address some problems specific to such analysis, and propose possible solutions.



### Non-rigid image registration using fully convolutional networks with deep self-supervision
- **Arxiv ID**: http://arxiv.org/abs/1709.00799v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00799v1)
- **Published**: 2017-09-04 03:22:20+00:00
- **Updated**: 2017-09-04 03:22:20+00:00
- **Authors**: Hongming Li, Yong Fan
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel non-rigid image registration algorithm that is built upon fully convolutional networks (FCNs) to optimize and learn spatial transformations between pairs of images to be registered. Different from most existing deep learning based image registration methods that learn spatial transformations from training data with known corresponding spatial transformations, our method directly estimates spatial transformations between pairs of images by maximizing an image-wise similarity metric between fixed and deformed moving images, similar to conventional image registration algorithms. At the same time, our method also learns FCNs for encoding the spatial transformations at the same spatial resolution of images to be registered, rather than learning coarse-grained spatial transformation information. The image registration is implemented in a multi-resolution image registration framework to jointly optimize and learn spatial transformations and FCNs at different resolutions with deep self-supervision through typical feedforward and backpropagation computation. Since our method simultaneously optimizes and learns spatial transformations for the image registration, our method can be directly used to register a pair of images, and the registration of a set of images is also a training procedure for FCNs so that the trained FCNs can be directly adopted to register new images by feedforward computation of the learned FCNs without any optimization. The proposed method has been evaluated for registering 3D structural brain magnetic resonance (MR) images and obtained better performance than state-of-the-art image registration algorithms.



### Hyperspectral Light Field Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1709.00835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00835v1)
- **Published**: 2017-09-04 06:36:55+00:00
- **Updated**: 2017-09-04 06:36:55+00:00
- **Authors**: Kang Zhu, Yujia Xue, Qiang Fu, Sing Bing Kang, Xilin Chen, Jingyi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we describe how scene depth can be extracted using a hyperspectral light field capture (H-LF) system. Our H-LF system consists of a 5 x 6 array of cameras, with each camera sampling a different narrow band in the visible spectrum. There are two parts to extracting scene depth. The first part is our novel cross-spectral pairwise matching technique, which involves a new spectral-invariant feature descriptor and its companion matching metric we call bidirectional weighted normalized cross correlation (BWNCC). The second part, namely, H-LF stereo matching, uses a combination of spectral-dependent correspondence and defocus cues that rely on BWNCC. These two new cost terms are integrated into a Markov Random Field (MRF) for disparity estimation. Experiments on synthetic and real H-LF data show that our approach can produce high-quality disparity maps. We also show that these results can be used to produce the complete plenoptic cube in addition to synthesizing all-focus and defocused color images under different sensor spectral responses.



### Extrinsic Parameter Calibration for Line Scanning Cameras on Ground Vehicles with Navigation Systems Using a Calibration Pattern
- **Arxiv ID**: http://arxiv.org/abs/1709.00846v3
- **DOI**: 10.3390/s17112491
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.00846v3)
- **Published**: 2017-09-04 07:46:43+00:00
- **Updated**: 2018-02-12 06:26:47+00:00
- **Authors**: Alexander Wendel, James Underwood
- **Comment**: Published in MDPI Sensors, 30 October 2017
- **Journal**: Sensors 2017, 17, 2491
- **Summary**: Line scanning cameras, which capture only a single line of pixels, have been increasingly used in ground based mobile or robotic platforms. In applications where it is advantageous to directly georeference the camera data to world coordinates, an accurate estimate of the camera's 6D pose is required. This paper focuses on the common case where a mobile platform is equipped with a rigidly mounted line scanning camera, whose pose is unknown, and a navigation system providing vehicle body pose estimates. We propose a novel method that estimates the camera's pose relative to the navigation system. The approach involves imaging and manually labelling a calibration pattern with distinctly identifiable points, triangulating these points from camera and navigation system data and reprojecting them in order to compute a likelihood, which is maximised to estimate the 6D camera pose. Additionally, a Markov Chain Monte Carlo (MCMC) algorithm is used to estimate the uncertainty of the offset. Tested on two different platforms, the method was able to estimate the pose to within 0.06 m / 1.05$^{\circ}$ and 0.18 m / 2.39$^{\circ}$. We also propose several approaches to displaying and interpreting the 6D results in a human readable way.



### Dataset Augmentation with Synthetic Images Improves Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1709.00849v3
- **DOI**: 10.1007/978-981-13-0020-2_31
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00849v3)
- **Published**: 2017-09-04 07:58:59+00:00
- **Updated**: 2018-06-26 11:19:09+00:00
- **Authors**: Manik Goyal, Param Rajpura, Hristo Bojinov, Ravi Hegde
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: Although Deep Convolutional Neural Networks trained with strong pixel-level annotations have significantly pushed the performance in semantic segmentation, annotation efforts required for the creation of training data remains a roadblock for further improvements. We show that augmentation of the weakly annotated training dataset with synthetic images minimizes both the annotation efforts and also the cost of capturing images with sufficient variety. Evaluation on the PASCAL 2012 validation dataset shows an increase in mean IOU from 52.80% to 55.47% by adding just 100 synthetic images per object class. Our approach is thus a promising solution to the problems of annotation and dataset collection.



### Medical Image Analysis using Convolutional Neural Networks: A Review
- **Arxiv ID**: http://arxiv.org/abs/1709.02250v2
- **DOI**: 10.1007/s10916-018-1088-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02250v2)
- **Published**: 2017-09-04 08:37:28+00:00
- **Updated**: 2019-05-21 05:57:58+00:00
- **Authors**: Syed Muhammad Anwar, Muhammad Majid, Adnan Qayyum, Muhammad Awais, Majdi Alnowami, Muhammad Khurram Khan
- **Comment**: None
- **Journal**: Journal of Medical Systems (2018)
- **Summary**: The science of solving clinical problems by analyzing images generated in clinical practice is known as medical image analysis. The aim is to extract information in an effective and efficient manner for improved clinical diagnosis. The recent advances in the field of biomedical engineering has made medical image analysis one of the top research and development area. One of the reason for this advancement is the application of machine learning techniques for the analysis of medical images. Deep learning is successfully used as a tool for machine learning, where a neural network is capable of automatically learning features. This is in contrast to those methods where traditionally hand crafted features are used. The selection and calculation of these features is a challenging task. Among deep learning techniques, deep convolutional networks are actively used for the purpose of medical image analysis. This include application areas such as segmentation, abnormality detection, disease classification, computer aided diagnosis and retrieval. In this study, a comprehensive review of the current state-of-the-art in medical image analysis using deep convolutional networks is presented. The challenges and potential of these techniques are also highlighted.



### CSSTag: Optical Nanoscale Radar and Particle Tracking for In-Body and Microfluidic Systems with Vibrating Graphene and Resonance Energy Transfer
- **Arxiv ID**: http://arxiv.org/abs/1709.00907v1
- **DOI**: 10.1109/TNB.2017.2785226
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.00907v1)
- **Published**: 2017-09-04 11:59:58+00:00
- **Updated**: 2017-09-04 11:59:58+00:00
- **Authors**: Burhan Gulbahar, Gorkem Memisoglu
- **Comment**: 13 double column pages, 9 figures, 1 table
- **Journal**: IEEE Transactions on NanoBioscience, vol. 16, no. 8, pp. 905--916,
  2017
- **Summary**: Single particle tracking systems monitor cellular processes with great accuracy in nano-biological systems. The emissions of the fluorescent molecules are detected with cameras or photodetectors. However, state-of-the-art imaging systems have challenges in the detection capability, collection and analysis of imaging data, penetration depth and complicated set-ups. In this article, a \textit{signaling based nanoscale acousto-optic radar and microfluidic particle tracking system} is proposed based on the theoretical design providing nanoscale optical modulator with vibrating F{\"{o}}rster resonance energy transfer (VFRET) and vibrating CdSe/ZnS quantum dots (QDs) on graphene resonators. The modulator structure combines the significant advantages of graphene membranes having wideband resonance frequencies with QDs having broad absorption spectrum and tunable properties. The solution denoted by chirp spread spectrum (CSS) Tag (\textit{CSSTag}) utilizes classical radar target tracking approaches in nanoscale environments based on the capability to generate CSS sequences to identify different bio-particles. Numerical and Monte-Carlo simulations are realized showing the significant performance for multiple particle tracking (MPT) with a modulator of $10 \, \mu$m $\times$ $10 \, \mu$m $\times$ $10 \, \mu$m dimension and several picograms of weight, signal to noise ratio (SNR) in the range $-7$ dB to $10$ dB and high speed tracking capability for microfluidic and in-body environments.



### Multi-modal Conditional Attention Fusion for Dimensional Emotion Prediction
- **Arxiv ID**: http://arxiv.org/abs/1709.02251v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1709.02251v1)
- **Published**: 2017-09-04 12:05:47+00:00
- **Updated**: 2017-09-04 12:05:47+00:00
- **Authors**: Shizhe Chen, Qin Jin
- **Comment**: Appeared at ACM Multimedia 2016
- **Journal**: None
- **Summary**: Continuous dimensional emotion prediction is a challenging task where the fusion of various modalities usually achieves state-of-the-art performance such as early fusion or late fusion. In this paper, we propose a novel multi-modal fusion strategy named conditional attention fusion, which can dynamically pay attention to different modalities at each time step. Long-short term memory recurrent neural networks (LSTM-RNN) is applied as the basic uni-modality model to capture long time dependencies. The weights assigned to different modalities are automatically decided by the current input features and recent history information rather than being fixed at any kinds of situation. Our experimental results on a benchmark dataset AVEC2015 show the effectiveness of our method which outperforms several common fusion strategies for valence prediction.



### Self-Supervised Learning for Stereo Matching with Self-Improving Ability
- **Arxiv ID**: http://arxiv.org/abs/1709.00930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00930v1)
- **Published**: 2017-09-04 12:56:18+00:00
- **Updated**: 2017-09-04 12:56:18+00:00
- **Authors**: Yiran Zhong, Yuchao Dai, Hongdong Li
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: Exiting deep-learning based dense stereo matching methods often rely on ground-truth disparity maps as the training signals, which are however not always available in many situations. In this paper, we design a simple convolutional neural network architecture that is able to learn to compute dense disparity maps directly from the stereo inputs. Training is performed in an end-to-end fashion without the need of ground-truth disparity maps. The idea is to use image warping error (instead of disparity-map residuals) as the loss function to drive the learning process, aiming to find a depth-map that minimizes the warping error. While this is a simple concept well-known in stereo matching, to make it work in a deep-learning framework, many non-trivial challenges must be overcome, and in this work we provide effective solutions. Our network is self-adaptive to different unseen imageries as well as to different camera settings. Experiments on KITTI and Middlebury stereo benchmark datasets show that our method outperforms many state-of-the-art stereo matching methods with a margin, and at the same time significantly faster.



### ARIGAN: Synthetic Arabidopsis Plants using Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1709.00938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00938v1)
- **Published**: 2017-09-04 13:12:58+00:00
- **Updated**: 2017-09-04 13:12:58+00:00
- **Authors**: Mario Valerio Giuffrida, Hanno Scharr, Sotirios A Tsaftaris
- **Comment**: 8 pages, 6 figures, 1 table, ICCV CVPPP Workshop 2017
- **Journal**: None
- **Summary**: In recent years, there has been an increasing interest in image-based plant phenotyping, applying state-of-the-art machine learning approaches to tackle challenging problems, such as leaf segmentation (a multi-instance problem) and counting. Most of these algorithms need labelled data to learn a model for the task at hand. Despite the recent release of a few plant phenotyping datasets, large annotated plant image datasets for the purpose of training deep learning algorithms are lacking. One common approach to alleviate the lack of training data is dataset augmentation. Herein, we propose an alternative solution to dataset augmentation for plant phenotyping, creating artificial images of plants using generative neural networks. We propose the Arabidopsis Rosette Image Generator (through) Adversarial Network: a deep convolutional network that is able to generate synthetic rosette-shaped plants, inspired by DCGAN (a recent adversarial network model using convolutional layers). Specifically, we trained the network using A1, A2, and A4 of the CVPPP 2017 LCC dataset, containing Arabidopsis Thaliana plants. We show that our model is able to generate realistic 128x128 colour images of plants. We train our network conditioning on leaf count, such that it is possible to generate plants with a given number of leaves suitable, among others, for training regression based models. We propose a new Ax dataset of artificial plants images, obtained by our ARIGAN. We evaluate this new dataset using a state-of-the-art leaf counting algorithm, showing that the testing error is reduced when Ax is used as part of the training data.



### A Reproducible Study on Remote Heart Rate Measurement
- **Arxiv ID**: http://arxiv.org/abs/1709.00962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00962v1)
- **Published**: 2017-09-04 13:55:27+00:00
- **Updated**: 2017-09-04 13:55:27+00:00
- **Authors**: Guillaume Heusch, André Anjos, Sébastien Marcel
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies the problem of reproducible research in remote photoplethysmography (rPPG). Most of the work published in this domain is assessed on privately-owned databases, making it difficult to evaluate proposed algorithms in a standard and principled manner. As a consequence, we present a new, publicly available database containing a relatively large number of subjects recorded under two different lighting conditions. Also, three state-of-the-art rPPG algorithms from the literature were selected, implemented and released as open source free software. After a thorough, unbiased experimental evaluation in various settings, it is shown that none of the selected algorithms is precise enough to be used in a real-world scenario.



### Feasibility of Corneal Imaging for Handheld Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/1709.00965v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.00965v1)
- **Published**: 2017-09-04 14:00:59+00:00
- **Updated**: 2017-09-04 14:00:59+00:00
- **Authors**: Daniel Schneider, Jens Grubert
- **Comment**: None
- **Journal**: None
- **Summary**: Smartphones are a popular device class for mobile Augmented Reality but suffer from a limited input space. Around-device interaction techniques aim at extending this input space using various sensing modalities. In this paper we present our work towards extending the input area of mobile devices using front-facing device-centered cameras that capture reflections in the cornea. As current generation mobile devices lack high resolution front-facing cameras, we study the feasibility of around-device interaction using corneal reflective imaging based on a high resolution camera. We present a workflow, a technical prototype and a feasibility evaluation.



### Towards Around-Device Interaction using Corneal Imaging
- **Arxiv ID**: http://arxiv.org/abs/1709.00966v1
- **DOI**: 10.1145/3132272.3134127
- **Categories**: **cs.HC**, cs.CV, H.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1709.00966v1)
- **Published**: 2017-09-04 14:01:45+00:00
- **Updated**: 2017-09-04 14:01:45+00:00
- **Authors**: Daniel Schneider, Jens Grubert
- **Comment**: None
- **Journal**: None
- **Summary**: Around-device interaction techniques aim at extending the input space using various sensing modalities on mobile and wearable devices. In this paper, we present our work towards extending the input area of mobile devices using front-facing device-centered cameras that capture reflections in the human eye. As current generation mobile devices lack high resolution front-facing cameras we study the feasibility of around-device interaction using corneal reflective imaging based on a high resolution camera. We present a workflow, a technical prototype and an evaluation, including a migration path from high resolution to low resolution imagers. Our study indicates, that under optimal conditions a spatial sensing resolution of 5 cm in the vicinity of a mobile phone is possible.



### A Geometric Approach to Harmonic Color Palette Design
- **Arxiv ID**: http://arxiv.org/abs/1709.02252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02252v1)
- **Published**: 2017-09-04 15:18:29+00:00
- **Updated**: 2017-09-04 15:18:29+00:00
- **Authors**: Carlos Lara-Alvarez, Tania Reyes
- **Comment**: 7 pages, 8 figures
- **Journal**: None
- **Summary**: We address the problem of finding harmonic colors, this problem has many applications, from fashion to industrial design. In order to solve this problem we consider that colors follow normal distributions in tone (chroma and lightness) and hue. The proposed approach relies in the CIE standard for representing colors and evaluate proximity. Other approaches to this problem use a set of rules. Experimental results show that lines with specific parameters angles of inclination, and distance from the reference point are preferred over others, and that uncertain line patterns outperform non-linear patterns.



### Domain-adaptive deep network compression
- **Arxiv ID**: http://arxiv.org/abs/1709.01041v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01041v2)
- **Published**: 2017-09-04 16:52:46+00:00
- **Updated**: 2017-09-06 10:26:38+00:00
- **Authors**: Marc Masana, Joost van de Weijer, Luis Herranz, Andrew D. Bagdanov, Jose M Alvarez
- **Comment**: Accepted at ICCV 2017
- **Journal**: None
- **Summary**: Deep Neural Networks trained on large datasets can be easily transferred to new domains with far fewer labeled examples by a process called fine-tuning. This has the advantage that representations learned in the large source domain can be exploited on smaller target domains. However, networks designed to be optimal for the source task are often prohibitively large for the target task. In this work we address the compression of networks after domain transfer.   We focus on compression algorithms based on low-rank matrix decomposition. Existing methods base compression solely on learned network weights and ignore the statistics of network activations. We show that domain transfer leads to large shifts in network activations and that it is desirable to take this into account when compressing. We demonstrate that considering activation statistics when compressing weights leads to a rank-constrained regression problem with a closed-form solution. Because our method takes into account the target domain, it can more optimally remove the redundancy in the weights. Experiments show that our Domain Adaptive Low Rank (DALR) method significantly outperforms existing low-rank compression techniques. With our approach, the fc6 layer of VGG19 can be compressed more than 4x more than using truncated SVD alone -- with only a minor or no loss in accuracy. When applied to domain-transferred networks it allows for compression down to only 5-20% of the original number of parameters with only a minor drop in performance.



### A Nonparametric Model for Multimodal Collaborative Activities Summarization
- **Arxiv ID**: http://arxiv.org/abs/1709.01077v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01077v1)
- **Published**: 2017-09-04 17:14:47+00:00
- **Updated**: 2017-09-04 17:14:47+00:00
- **Authors**: Guy Rosman, John W. Fisher III, Daniela Rus
- **Comment**: None
- **Journal**: None
- **Summary**: Ego-centric data streams provide a unique opportunity to reason about joint behavior by pooling data across individuals. This is especially evident in urban environments teeming with human activities, but which suffer from incomplete and noisy data. Collaborative human activities exhibit common spatial, temporal, and visual characteristics facilitating inference across individuals from multiple sensory modalities as we explore in this paper from the perspective of meetings. We propose a new Bayesian nonparametric model that enables us to efficiently pool video and GPS data towards collaborative activities analysis from multiple individuals. We demonstrate the utility of this model for inference tasks such as activity detection, classification, and summarization. We further demonstrate how spatio-temporal structure embedded in our model enables better understanding of partial and noisy observations such as localization and face detections based on social interactions. We show results on both synthetic experiments and a new dataset of egocentric video and noisy GPS data from multiple individuals.



### To Learn or Not to Learn Features for Deformable Registration?
- **Arxiv ID**: http://arxiv.org/abs/1709.01057v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01057v3)
- **Published**: 2017-09-04 17:54:15+00:00
- **Updated**: 2018-07-06 03:13:48+00:00
- **Authors**: Aabhas Majumdar, Raghav Mehta, Jayanthi Sivaswamy
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: Feature-based registration has been popular with a variety of features ranging from voxel intensity to Self-Similarity Context (SSC). In this paper, we examine the question on how features learnt using various Deep Learning (DL) frameworks can be used for deformable registration and whether this feature learning is necessary or not. We investigate the use of features learned by different DL methods in the current state-of-the-art discrete registration framework and analyze its performance on 2 publicly available datasets. We draw insights into the type of DL framework useful for feature learning and the impact, if any, of the complexity of different DL models and brain parcellation methods on the performance of discrete registration. Our results indicate that the registration performance with DL features and SSC are comparable and stable across datasets whereas this does not hold for low level features.



### WESPE: Weakly Supervised Photo Enhancer for Digital Cameras
- **Arxiv ID**: http://arxiv.org/abs/1709.01118v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01118v2)
- **Published**: 2017-09-04 18:59:06+00:00
- **Updated**: 2018-03-03 18:51:55+00:00
- **Authors**: Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Low-end and compact mobile cameras demonstrate limited photo quality mainly due to space, hardware and budget constraints. In this work, we propose a deep learning solution that translates photos taken by cameras with limited capabilities into DSLR-quality photos automatically. We tackle this problem by introducing a weakly supervised photo enhancer (WESPE) - a novel image-to-image Generative Adversarial Network-based architecture. The proposed model is trained by under weak supervision: unlike previous works, there is no need for strong supervision in the form of a large annotated dataset of aligned original/enhanced photo pairs. The sole requirement is two distinct datasets: one from the source camera, and one composed of arbitrary high-quality images that can be generally crawled from the Internet - the visual content they exhibit may be unrelated. Hence, our solution is repeatable for any camera: collecting the data and training can be achieved in a couple of hours. In this work, we emphasize on extensive evaluation of obtained results. Besides standard objective metrics and subjective user study, we train a virtual rater in the form of a separate CNN that mimics human raters on Flickr data and use this network to get reference scores for both original and enhanced photos. Our experiments on the DPED, KITTI and Cityscapes datasets as well as pictures from several generations of smartphones demonstrate that WESPE produces comparable or improved qualitative results with state-of-the-art strongly supervised methods.



### WRPN: Wide Reduced-Precision Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.01134v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1709.01134v1)
- **Published**: 2017-09-04 19:56:48+00:00
- **Updated**: 2017-09-04 19:56:48+00:00
- **Authors**: Asit Mishra, Eriko Nurvitadhi, Jeffrey J Cook, Debbie Marr
- **Comment**: None
- **Journal**: None
- **Summary**: For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks. Activation maps, however, occupy a large memory footprint during both the training and inference step when using mini-batches of inputs. One way to reduce this large memory footprint is to reduce the precision of activations. However, past works have shown that reducing the precision of activations hurts model accuracy. We study schemes to train networks from scratch using reduced-precision activations without hurting accuracy. We reduce the precision of activation maps (along with model parameters) and increase the number of filter maps in a layer, and find that this scheme matches or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly improve the execution efficiency (e.g. reduce dynamic memory footprint, memory bandwidth and computational energy) and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN - wide reduced-precision networks. We report results and show that WRPN scheme is better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.



### A Multilayer-Based Framework for Online Background Subtraction with Freely Moving Cameras
- **Arxiv ID**: http://arxiv.org/abs/1709.01140v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01140v1)
- **Published**: 2017-09-04 20:06:51+00:00
- **Updated**: 2017-09-04 20:06:51+00:00
- **Authors**: Yizhe Zhu, Ahmed Elgammal
- **Comment**: Accepted by ICCV'17
- **Journal**: None
- **Summary**: The exponentially increasing use of moving platforms for video capture introduces the urgent need to develop the general background subtraction algorithms with the capability to deal with the moving background. In this paper, we propose a multilayer-based framework for online background subtraction for videos captured by moving cameras. Unlike the previous treatments of the problem, the proposed method is not restricted to binary segmentation of background and foreground, but formulates it as a multi-label segmentation problem by modeling multiple foreground objects in different layers when they appear simultaneously in the scene. We assign an independent processing layer to each foreground object, as well as the background, where both motion and appearance models are estimated, and a probability map is inferred using a Bayesian filtering framework. Finally, Multi-label Graph-cut on Markov Random Field is employed to perform pixel-wise labeling. Extensive evaluation results show that the proposed method outperforms state-of-the-art methods on challenging video sequences.



### Link the head to the "beak": Zero Shot Learning from Noisy Text Description at Part Precision
- **Arxiv ID**: http://arxiv.org/abs/1709.01148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01148v1)
- **Published**: 2017-09-04 20:36:14+00:00
- **Updated**: 2017-09-04 20:36:14+00:00
- **Authors**: Mohamed Elhoseiny, Yizhe Zhu, Han Zhang, Ahmed Elgammal
- **Comment**: Accepted by CVPR'17
- **Journal**: None
- **Summary**: In this paper, we study learning visual classifiers from unstructured text descriptions at part precision with no training images. We propose a learning framework that is able to connect text terms to its relevant parts and suppress connections to non-visual text terms without any part-text annotations. For instance, this learning process enables terms like "beak" to be sparsely linked to the visual representation of parts like head, while reduces the effect of non-visual terms like "migrate" on classifier prediction. Images are encoded by a part-based CNN that detect bird parts and learn part-specific representation. Part-based visual classifiers are predicted from text descriptions of unseen visual classifiers to facilitate classification without training images (also known as zero-shot recognition). We performed our experiments on CUBirds 2011 dataset and improves the state-of-the-art text-based zero-shot recognition results from 34.7\% to 43.6\%. We also created large scale benchmarks on North American Bird Images augmented with text descriptions, where we also show that our approach outperforms existing methods. Our code, data, and models are publically available.



### Is human face processing a feature- or pattern-based task? Evidence using a unified computational method driven by eye movements
- **Arxiv ID**: http://arxiv.org/abs/1709.01182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01182v1)
- **Published**: 2017-09-04 22:29:56+00:00
- **Updated**: 2017-09-04 22:29:56+00:00
- **Authors**: Carlos E. Thomaz, Vagner Amaral, Gilson A. Giraldi, Duncan F. Gillies, Daniel Rueckert
- **Comment**: None
- **Journal**: None
- **Summary**: Research on human face processing using eye movements has provided evidence that we recognize face images successfully focusing our visual attention on a few inner facial regions, mainly on the eyes, nose and mouth. To understand how we accomplish this process of coding high-dimensional faces so efficiently, this paper proposes and implements a multivariate extraction method that combines face images variance with human spatial attention maps modeled as feature- and pattern-based information sources. It is based on a unified multidimensional representation of the well-known face-space concept. The spatial attention maps are summary statistics of the eye-tracking fixations of a number of participants and trials to frontal and well-framed face images during separate gender and facial expression recognition tasks. Our experimental results carried out on publicly available face databases have indicated that we might emulate the human extraction system as a pattern-based computational method rather than a feature-based one to properly explain the proficiency of the human system in recognizing visual face information.



