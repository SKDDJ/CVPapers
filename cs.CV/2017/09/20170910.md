# Arxiv Papers in cs.CV on 2017-09-10
### Convolutional Neural Networks: Ensemble Modeling, Fine-Tuning and Unsupervised Semantic Localization for Intraoperative CLE Images
- **Arxiv ID**: http://arxiv.org/abs/1709.03028v2
- **DOI**: 10.1016/j.jvcir.2018.04.004
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1709.03028v2)
- **Published**: 2017-09-10 02:33:35+00:00
- **Updated**: 2017-10-23 18:07:26+00:00
- **Authors**: Mohammadhassan Izadyyazdanabadi, Evgenii Belykh, Michael Mooney, Nikolay Martirosyan, Jennifer Eschbacher, Peter Nakaji, Mark C. Preul, Yezhou Yang
- **Comment**: The related work was updated
- **Journal**: None
- **Summary**: Confocal laser endomicroscopy (CLE) is an advanced optical fluorescence technology undergoing assessment for applications in brain tumor surgery. Despite its promising potential, interpreting the unfamiliar gray tone images of fluorescent stains can be difficult. Many of the CLE images can be distorted by motion, extremely low or high fluorescence signal, or obscured by red blood cell accumulation, and these can be interpreted as nondiagnostic. However, just one neat CLE image might suffice for intraoperative diagnosis of the tumor. While manual examination of thousands of nondiagnostic images during surgery would be impractical, this creates an opportunity for a model to select diagnostic images for the pathologists or surgeon's review. In this study, we sought to develop a deep learning model to automatically detect the diagnostic images using a manually annotated dataset, and we employed a patient-based nested cross-validation approach to explore generalizability of the model. We explored various training regimes: deep training, shallow fine-tuning, and deep fine-tuning. Further, we investigated the effect of ensemble modeling by combining the top-5 single models crafted in the development phase. We localized histological features from diagnostic CLE images by visualization of shallow and deep neural activations. Our inter-rater experiment results confirmed that our ensemble of deeply fine-tuned models achieved higher agreement with the ground truth than the other observers. With the speed and precision of the proposed method (110 images/second; 85% on the gold standard test subset), it has potential to be integrated into the operative workflow in the brain tumor surgery.



### Robust Sparse Coding via Self-Paced Learning
- **Arxiv ID**: http://arxiv.org/abs/1709.03030v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.03030v1)
- **Published**: 2017-09-10 03:15:48+00:00
- **Updated**: 2017-09-10 03:15:48+00:00
- **Authors**: Xiaodong Feng, Zhiwei Tang, Sen Wu
- **Comment**: submitted to AAAI2018
- **Journal**: None
- **Summary**: Sparse coding (SC) is attracting more and more attention due to its comprehensive theoretical studies and its excellent performance in many signal processing applications. However, most existing sparse coding algorithms are nonconvex and are thus prone to becoming stuck into bad local minima, especially when there are outliers and noisy data. To enhance the learning robustness, in this paper, we propose a unified framework named Self-Paced Sparse Coding (SPSC), which gradually include matrix elements into SC learning from easy to complex. We also generalize the self-paced learning schema into different levels of dynamic selection on samples, features and elements respectively. Experimental results on real-world data demonstrate the efficacy of the proposed algorithms.



### A Product Shape Congruity Measure via Entropy in Shape Scale Space
- **Arxiv ID**: http://arxiv.org/abs/1709.03086v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03086v1)
- **Published**: 2017-09-10 11:51:08+00:00
- **Updated**: 2017-09-10 11:51:08+00:00
- **Authors**: Asli Genctav, Sibel Tari
- **Comment**: Proceedings of EUSIPCO 2017 Satellite Workshops, Corresponding
  Workshop: Creative Design and Advanced Manufacturing: An emerging application
  area for Signals and Systems
- **Journal**: None
- **Summary**: Product shape is one of the factors that trigger preference decisions of customers. Congruity of shape elements and deformation of shape from the prototype are two factors that are found to influence aesthetic response, hence preference. We propose a measure to indirectly quantify congruity of different parts of the shape and the degree to which the parts deviate from a sphere, i.e. our choice of the prototype, without explicitly defining parts and their relations. The basic signals and systems concept that we use is the entropy. Our measure attains its lowest value for a volume enclosed by a sphere. On one hand, deformations from the prototype cause an increase in the measure. On the other hand, as deformations create congruent parts, our measure decreases due to the attained harmony. Our preliminary experimental results are consistent with our expectations.



### Institutionally Distributed Deep Learning Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.05929v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1709.05929v1)
- **Published**: 2017-09-10 15:36:17+00:00
- **Updated**: 2017-09-10 15:36:17+00:00
- **Authors**: Ken Chang, Niranjan Balachandar, Carson K Lam, Darvin Yi, James M Brown, Andrew Beers, Bruce R Rosen, Daniel L Rubin, Jayashree Kalpathy-Cramer
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has become a promising approach for automated medical diagnoses. When medical data samples are limited, collaboration among multiple institutions is necessary to achieve high algorithm performance. However, sharing patient data often has limitations due to technical, legal, or ethical concerns. In such cases, sharing a deep learning model is a more attractive alternative. The best method of performing such a task is unclear, however. In this study, we simulate the dissemination of learning deep learning network models across four institutions using various heuristics and compare the results with a deep learning model trained on centrally hosted patient data. The heuristics investigated include ensembling single institution models, single weight transfer, and cyclical weight transfer. We evaluated these approaches for image classification in three independent image collections (retinal fundus photos, mammography, and ImageNet). We find that cyclical weight transfer resulted in a performance (testing accuracy = 77.3%) that was closest to that of centrally hosted patient data (testing accuracy = 78.7%). We also found that there is an improvement in the performance of cyclical weight transfer heuristic with high frequency of weight transfer.



### A Detail Based Method for Linear Full Reference Image Quality Prediction
- **Arxiv ID**: http://arxiv.org/abs/1709.03124v3
- **DOI**: 10.1109/TIP.2017.2757139
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03124v3)
- **Published**: 2017-09-10 16:28:24+00:00
- **Updated**: 2017-11-05 18:32:08+00:00
- **Authors**: Elio D. Di Claudio, Giovanni Jacovitti
- **Comment**: 15 pages, 9 figures. Copyright notice: The paper has been accepted
  for publication on the IEEE Trans. on Image Processing on 19/09/2017 and the
  copyright has been transferred to the IEEE
- **Journal**: IEEE Transactions on Image Processing, vol. 27, no. 1, pp.
  179-193, Jan. 2018
- **Summary**: In this paper, a novel Full Reference method is proposed for image quality assessment, using the combination of two separate metrics to measure the perceptually distinct impact of detail losses and of spurious details. To this purpose, the gradient of the impaired image is locally decomposed as a predicted version of the original gradient, plus a gradient residual. It is assumed that the detail attenuation identifies the detail loss, whereas the gradient residuals describe the spurious details. It turns out that the perceptual impact of detail losses is roughly linear with the loss of the positional Fisher information, while the perceptual impact of the spurious details is roughly proportional to a logarithmic measure of the signal to residual ratio. The affine combination of these two metrics forms a new index strongly correlated with the empirical Differential Mean Opinion Score (DMOS) for a significant class of image impairments, as verified for three independent popular databases. The method allowed alignment and merging of DMOS data coming from these different databases to a common DMOS scale by affine transformations. Unexpectedly, the DMOS scale setting is possible by the analysis of a single image affected by additive noise.



### Robust Emotion Recognition from Low Quality and Low Bit Rate Video: A Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1709.03126v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.03126v1)
- **Published**: 2017-09-10 16:31:56+00:00
- **Updated**: 2017-09-10 16:31:56+00:00
- **Authors**: Bowen Cheng, Zhangyang Wang, Zhaobin Zhang, Zhu Li, Ding Liu, Jianchao Yang, Shuai Huang, Thomas S. Huang
- **Comment**: Accepted by the Seventh International Conference on Affective
  Computing and Intelligent Interaction (ACII2017)
- **Journal**: None
- **Summary**: Emotion recognition from facial expressions is tremendously useful, especially when coupled with smart devices and wireless multimedia applications. However, the inadequate network bandwidth often limits the spatial resolution of the transmitted video, which will heavily degrade the recognition reliability. We develop a novel framework to achieve robust emotion recognition from low bit rate video. While video frames are downsampled at the encoder side, the decoder is embedded with a deep network model for joint super-resolution (SR) and recognition. Notably, we propose a novel max-mix training strategy, leading to a single "One-for-All" model that is remarkably robust to a vast range of downsampling factors. That makes our framework well adapted for the varied bandwidths in real transmission scenarios, without hampering scalability or efficiency. The proposed framework is evaluated on the AVEC 2016 benchmark, and demonstrates significantly improved stand-alone recognition performance, as well as rate-distortion (R-D) performance, than either directly recognizing from LR frames, or separating SR and recognition.



### DPC-Net: Deep Pose Correction for Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/1709.03128v4
- **DOI**: 10.1109/LRA.2017.2778765
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03128v4)
- **Published**: 2017-09-10 16:35:55+00:00
- **Updated**: 2022-07-05 04:35:12+00:00
- **Authors**: Valentin Peretroukhin, Jonathan Kelly
- **Comment**: In IEEE Robotics and Automation Letters (RA-L) and presented at the
  IEEE International Conference on Robotics and Automation (ICRA'18), Brisbane,
  Australia, May 21-25, 2018
- **Journal**: IEEE Robotics and Automation Letters (RA-L), Vol. 3, No. 3, pp.
  2424-2431, Jul. 2018
- **Summary**: We present a novel method to fuse the power of deep networks with the computational efficiency of geometric and probabilistic localization algorithms. In contrast to other methods that completely replace a classical visual estimator with a deep network, we propose an approach that uses a convolutional neural network to learn difficult-to-model corrections to the estimator from ground-truth training data. To this end, we derive a novel loss function for learning SE(3) corrections based on a matrix Lie groups approach, with a natural formulation for balancing translation and rotation errors. We use this loss to train a Deep Pose Correction network (DPC-Net) that predicts corrections for a particular estimator, sensor and environment. Using the KITTI odometry dataset, we demonstrate significant improvements to the accuracy of a computationally-efficient sparse stereo visual odometry pipeline, that render it as accurate as a modern computationally-intensive dense estimator. Further, we show how DPC-Net can be used to mitigate the effect of poorly calibrated lens distortion parameters.



### Fully Convolutional Neural Networks for Dynamic Object Detection in Grid Maps (Masters Thesis)
- **Arxiv ID**: http://arxiv.org/abs/1709.03138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03138v1)
- **Published**: 2017-09-10 17:06:23+00:00
- **Updated**: 2017-09-10 17:06:23+00:00
- **Authors**: Florian Piewak
- **Comment**: This is the masters thesis of Florian Piewak. A shorter version of
  this thesis was accepted at IV 2017
- **Journal**: None
- **Summary**: One of the most important parts of environment perception is the detection of obstacles in the surrounding of the vehicle. To achieve that, several sensors like radars, LiDARs and cameras are installed in autonomous vehicles. The produced sensor data is fused to a general representation of the surrounding. In this thesis the dynamic occupancy grid map approach of Nuss et al. is used while three goals are achieved. First, the approach of Nuss et al. to distinguish between moving and non-moving obstacles is improved by using Fully Convolutional Neural Networks to create a class prediction for each grid cell. For this purpose, the network is initialized with public pre-trained network models and the training is executed with a semi-automatic generated dataset. The second goal is to provide orientation information for each detected moving obstacle. This could improve tracking algorithms, which are based on the dynamic occupancy grid map. The orientation extraction based on the Convolutional Neural Network shows a better performance in comparison to an orientation extraction directly over the velocity information of the dynamic occupancy grid map. A general problem of developing machine learning approaches like Neural Networks is the number of labeled data, which can always be increased. For this reason, the last goal is to evaluate a semi-supervised learning algorithm, to generate automatically more labeled data. The result of this evaluation shows that the automated labeled data does not improve the performance of the Convolutional Neural Network. All in all, the best results are combined to compare the detection against the approach of Nuss et al. [36] and a relative improvement of 34.8% is reached.



### Fully Convolutional Neural Networks for Dynamic Object Detection in Grid Maps
- **Arxiv ID**: http://arxiv.org/abs/1709.03139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03139v1)
- **Published**: 2017-09-10 17:06:48+00:00
- **Updated**: 2017-09-10 17:06:48+00:00
- **Authors**: Florian Piewak, Timo Rehfeld, Michael Weber, J. Marius Zöllner
- **Comment**: This is a shorter version of the masters thesis of Florian Piewak and
  it was accapted at IV 2017
- **Journal**: None
- **Summary**: Grid maps are widely used in robotics to represent obstacles in the environment and differentiating dynamic objects from static infrastructure is essential for many practical applications. In this work, we present a methods that uses a deep convolutional neural network (CNN) to infer whether grid cells are covering a moving object or not. Compared to tracking approaches, that use e.g. a particle filter to estimate grid cell velocities and then make a decision for individual grid cells based on this estimate, our approach uses the entire grid map as input image for a CNN that inspects a larger area around each cell and thus takes the structural appearance in the grid map into account to make a decision. Compared to our reference method, our concept yields a performance increase from 83.9% to 97.2%. A runtime optimized version of our approach yields similar improvements with an execution time of just 10 milliseconds.



### An Iterative Regression Approach for Face Pose Estimation from RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1709.03170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03170v1)
- **Published**: 2017-09-10 20:34:24+00:00
- **Updated**: 2017-09-10 20:34:24+00:00
- **Authors**: Wenye He
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a iterative optimization method, explicit shape regression, for face pose detection and localization. The regression function is learnt to find out the entire facial shape and minimize the alignment errors. A cascaded learning framework is employed to enhance shape constraint during detection. A combination of a two-level boosted regression, shape indexed features and a correlation-based feature selection method is used to improve the performance. In this paper, we have explain the advantage of ESR for deformable object like face pose estimation and reveal its generic applications of the method. In the experiment, we compare the results with different work and demonstrate the accuracy and robustness in different scenarios.



### Deep multi-frame face super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1709.03196v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03196v2)
- **Published**: 2017-09-10 23:16:26+00:00
- **Updated**: 2017-10-15 19:49:34+00:00
- **Authors**: E. Ustinova, V. Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: Face verification and recognition problems have seen rapid progress in recent years, however recognition from small size images remains a challenging task that is inherently intertwined with the task of face super-resolution. Tackling this problem using multiple frames is an attractive idea, yet requires solving the alignment problem that is also challenging for low-resolution faces. Here we present a holistic system for multi-frame recognition, alignment, and superresolution of faces. Our neural network architecture restores the central frame of each input sequence additionally taking into account a number of adjacent frames and making use of sub-pixel movements. We present our results using the popular dataset for video face recognition (YouTube Faces). We show a notable improvement of identification score compared to several baselines including the one based on single-image super-resolution.



