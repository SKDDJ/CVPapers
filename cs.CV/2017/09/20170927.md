# Arxiv Papers in cs.CV on 2017-09-27
### Dynamic Label Graph Matching for Unsupervised Video Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1709.09297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09297v1)
- **Published**: 2017-09-27 01:07:10+00:00
- **Updated**: 2017-09-27 01:07:10+00:00
- **Authors**: Mang Ye, Andy J Ma, Liang Zheng, Jiawei Li, P C Yuen
- **Comment**: Accepted by ICCV 2017. Revised our IDE results on MARS dataset under
  standard evaluation protocol
- **Journal**: None
- **Summary**: Label estimation is an important component in an unsupervised person re-identification (re-ID) system. This paper focuses on cross-camera label estimation, which can be subsequently used in feature learning to learn robust re-ID models. Specifically, we propose to construct a graph for samples in each camera, and then graph matching scheme is introduced for cross-camera labeling association. While labels directly output from existing graph matching methods may be noisy and inaccurate due to significant cross-camera variations, this paper proposes a dynamic graph matching (DGM) method. DGM iteratively updates the image graph and the label estimation process by learning a better feature space with intermediate estimated labels. DGM is advantageous in two aspects: 1) the accuracy of estimated labels is improved significantly with the iterations; 2) DGM is robust to noisy initial training data. Extensive experiments conducted on three benchmarks including the large-scale MARS dataset show that DGM yields competitive performance to fully supervised baselines, and outperforms competing unsupervised learning methods.



### Effective Image Retrieval via Multilinear Multi-index Fusion
- **Arxiv ID**: http://arxiv.org/abs/1709.09304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09304v1)
- **Published**: 2017-09-27 01:59:17+00:00
- **Updated**: 2017-09-27 01:59:17+00:00
- **Authors**: Zhizhong Zhang, Yuan Xie, Wensheng Zhang, Qi Tian
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Multi-index fusion has demonstrated impressive performances in retrieval task by integrating different visual representations in a unified framework. However, previous works mainly consider propagating similarities via neighbor structure, ignoring the high order information among different visual representations. In this paper, we propose a new multi-index fusion scheme for image retrieval. By formulating this procedure as a multilinear based optimization problem, the complementary information hidden in different indexes can be explored more thoroughly. Specially, we first build our multiple indexes from various visual representations. Then a so-called index-specific functional matrix, which aims to propagate similarities, is introduced for updating the original index. The functional matrices are then optimized in a unified tensor space to achieve a refinement, such that the relevant images can be pushed more closer. The optimization problem can be efficiently solved by the augmented Lagrangian method with theoretical convergence guarantee. Unlike the traditional multi-index fusion scheme, our approach embeds the multi-index subspace structure into the new indexes with sparse constraint, thus it has little additional memory consumption in online query stage. Experimental evaluation on three benchmark datasets reveals that the proposed approach achieves the state-of-the-art performance, i.e., N-score 3.94 on UKBench, mAP 94.1\% on Holiday and 62.39\% on Market-1501.



### Pseudo-labels for Supervised Learning on Dynamic Vision Sensor Data, Applied to Object Detection under Ego-motion
- **Arxiv ID**: http://arxiv.org/abs/1709.09323v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09323v3)
- **Published**: 2017-09-27 03:45:27+00:00
- **Updated**: 2018-03-14 12:13:18+00:00
- **Authors**: Nicholas F. Y. Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, dynamic vision sensors (DVS), also known as event-based cameras or neuromorphic sensors, have seen increased use due to various advantages over conventional frame-based cameras. Using principles inspired by the retina, its high temporal resolution overcomes motion blurring, its high dynamic range overcomes extreme illumination conditions and its low power consumption makes it ideal for embedded systems on platforms such as drones and self-driving cars. However, event-based data sets are scarce and labels are even rarer for tasks such as object detection. We transferred discriminative knowledge from a state-of-the-art frame-based convolutional neural network (CNN) to the event-based modality via intermediate pseudo-labels, which are used as targets for supervised learning. We show, for the first time, event-based car detection under ego-motion in a real environment at 100 frames per second with a test average precision of 40.3% relative to our annotated ground truth. The event-based car detector handles motion blur and poor illumination conditions despite not explicitly trained to do so, and even complements frame-based CNN detectors, suggesting that it has learnt generalized visual representations.



### Augmented Robust PCA For Foreground-Background Separation on Noisy, Moving Camera Video
- **Arxiv ID**: http://arxiv.org/abs/1709.09328v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.09328v1)
- **Published**: 2017-09-27 04:17:43+00:00
- **Updated**: 2017-09-27 04:17:43+00:00
- **Authors**: Chen Gao, Brian E. Moore, Raj Rao Nadakuditi
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents a novel approach for robust PCA with total variation regularization for foreground-background separation and denoising on noisy, moving camera video. Our proposed algorithm registers the raw (possibly corrupted) frames of a video and then jointly processes the registered frames to produce a decomposition of the scene into a low-rank background component that captures the static components of the scene, a smooth foreground component that captures the dynamic components of the scene, and a sparse component that can isolate corruptions and other non-idealities. Unlike existing methods, our proposed algorithm produces a panoramic low-rank component that spans the entire field of view, automatically stitching together corrupted data from partially overlapping scenes. The low-rank portion of our robust PCA model is based on a recently discovered optimal low-rank matrix estimator (OptShrink) that requires no parameter tuning. We demonstrate the performance of our algorithm on both static and moving camera videos corrupted by noise and outliers.



### A Read-Write Memory Network for Movie Story Understanding
- **Arxiv ID**: http://arxiv.org/abs/1709.09345v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1709.09345v4)
- **Published**: 2017-09-27 06:02:57+00:00
- **Updated**: 2018-03-16 13:43:15+00:00
- **Authors**: Seil Na, Sangho Lee, Jisung Kim, Gunhee Kim
- **Comment**: accepted paper at ICCV 2017
- **Journal**: None
- **Summary**: We propose a novel memory network model named Read-Write Memory Network (RWMN) to perform question and answering tasks for large-scale, multimodal movie story understanding. The key focus of our RWMN model is to design the read network and the write network that consist of multiple convolutional layers, which enable memory read and write operations to have high capacity and flexibility. While existing memory-augmented network models treat each memory slot as an independent block, our use of multi-layered CNNs allows the model to read and write sequential memory cells as chunks, which is more reasonable to represent a sequential story because adjacent memory blocks often have strong correlations. For evaluation, we apply our model to all the six tasks of the MovieQA benchmark, and achieve the best accuracies on several tasks, especially on the visual QA task. Our model shows a potential to better understand not only the content in the story, but also more abstract information, such as relationships between characters and the reasons for their actions.



### Signature Verification Approach using Fusion of Hybrid Texture Features
- **Arxiv ID**: http://arxiv.org/abs/1709.09348v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09348v2)
- **Published**: 2017-09-27 06:13:04+00:00
- **Updated**: 2019-05-03 19:40:07+00:00
- **Authors**: Ankan Kumar Bhunia, Alireza Alaei, Partha Pratim Roy
- **Comment**: Neural Computing and Application
- **Journal**: None
- **Summary**: In this paper, a writer-dependent signature verification method is proposed. Two different types of texture features, namely Wavelet and Local Quantized Patterns (LQP) features, are employed to extract two kinds of transform and statistical based information from signature images. For each writer two separate one-class support vector machines (SVMs) corresponding to each set of LQP and Wavelet features are trained to obtain two different authenticity scores for a given signature. Finally, a score level classifier fusion method is used to integrate the scores obtained from the two one-class SVMs to achieve the verification score. In the proposed method only genuine signatures are used to train the one-class SVMs. The proposed signature verification method has been tested using four different publicly available datasets and the results demonstrate the generality of the proposed method. The proposed system outperforms other existing systems in the literature.



### Generative Adversarial Networks with Inverse Transformation Unit
- **Arxiv ID**: http://arxiv.org/abs/1709.09354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09354v1)
- **Published**: 2017-09-27 06:38:30+00:00
- **Updated**: 2017-09-27 06:38:30+00:00
- **Authors**: Zhifeng Kong, Shuo Ding
- **Comment**: 11 pages, 9 figures
- **Journal**: None
- **Summary**: In this paper we introduce a new structure to Generative Adversarial Networks by adding an inverse transformation unit behind the generator. We present two theorems to claim the convergence of the model, and two conjectures to nonideal situations when the transformation is not bijection. A general survey on models with different transformations was done on the MNIST dataset and the Fashion-MNIST dataset, which shows the transformation does not necessarily need to be bijection. Also, with certain transformations that blurs an image, our model successfully learned to sharpen the images and recover blurred images, which was additionally verified by our measurement of sharpness.



### Globally-Optimal Inlier Set Maximisation for Simultaneous Camera Pose and Feature Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1709.09384v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.09384v1)
- **Published**: 2017-09-27 08:36:07+00:00
- **Updated**: 2017-09-27 08:36:07+00:00
- **Authors**: Dylan Campbell, Lars Petersson, Laurent Kneip, Hongdong Li
- **Comment**: Manuscript in press 2017 IEEE International Conference on Computer
  Vision
- **Journal**: None
- **Summary**: Estimating the 6-DoF pose of a camera from a single image relative to a pre-computed 3D point-set is an important task for many computer vision applications. Perspective-n-Point (PnP) solvers are routinely used for camera pose estimation, provided that a good quality set of 2D-3D feature correspondences are known beforehand. However, finding optimal correspondences between 2D key-points and a 3D point-set is non-trivial, especially when only geometric (position) information is known. Existing approaches to the simultaneous pose and correspondence problem use local optimisation, and are therefore unlikely to find the optimal solution without a good pose initialisation, or introduce restrictive assumptions. Since a large proportion of outliers are common for this problem, we instead propose a globally-optimal inlier set cardinality maximisation approach which jointly estimates optimal camera pose and optimal correspondences. Our approach employs branch-and-bound to search the 6D space of camera poses, guaranteeing global optimality without requiring a pose prior. The geometry of SE(3) is used to find novel upper and lower bounds for the number of inliers and local optimisation is integrated to accelerate convergence. The evaluation empirically supports the optimality proof and shows that the method performs much more robustly than existing approaches, including on a large-scale outdoor data-set.



### Human Detection for Night Surveillance using Adaptive Background Subtracted Image
- **Arxiv ID**: http://arxiv.org/abs/1709.09389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09389v1)
- **Published**: 2017-09-27 08:46:01+00:00
- **Updated**: 2017-09-27 08:46:01+00:00
- **Authors**: Yash Khandhediya, Karishma Sav, Vandit Gajjar
- **Comment**: 5 Pages, 7 Figures, 1 Table, National Conference on Advances in
  Computer Science Engineering & Technology
- **Journal**: None
- **Summary**: Surveillance based on Computer Vision has become a major necessity in current era. Most of the surveillance systems operate on visible light imaging, but performance based on visible light imaging is limited due to some factors like variation in light intensity during the daytime. The matter of concern lies in the need for processing images in low light, such as in the need of nighttime surveillance. In this paper, we have proposed a novel approach for human detection using FLIR(Forward Looking Infrared) camera. As the principle involves sensing based on thermal radiation in the Near IR Region, it is possible to detect Humans from an image captured using a FLIR camera even in low light. The proposed method for human detection involves processing of Thermal images by using HOG (Histogram of Oriented Gradients) feature extraction technique along with some enhancements. The principle of the proposed technique lies in an adaptive background subtraction algorithm, which works in association with the HOG technique. By means of this method, we are able to reduce execution time, precision and some other parameters, which result in improvement of overall accuracy of the human detection system.



### Light field super resolution through controlled micro-shifts of light field sensor
- **Arxiv ID**: http://arxiv.org/abs/1709.09422v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09422v2)
- **Published**: 2017-09-27 09:54:08+00:00
- **Updated**: 2018-06-10 19:40:10+00:00
- **Authors**: M. Umair Mukati, Bahadir K. Gunturk
- **Comment**: None
- **Journal**: None
- **Summary**: Light field cameras enable new capabilities, such as post-capture refocusing and aperture control, through capturing directional and spatial distribution of light rays in space. Micro-lens array based light field camera design is often preferred due to its light transmission efficiency, cost-effectiveness and compactness. One drawback of the micro-lens array based light field cameras is low spatial resolution due to the fact that a single sensor is shared to capture both spatial and angular information. To address the low spatial resolution issue, we present a light field imaging approach, where multiple light fields are captured and fused to improve the spatial resolution. For each capture, the light field sensor is shifted by a pre-determined fraction of a micro-lens size using an XY translation stage for optimal performance.



### Leveraging Weakly Annotated Data for Fashion Image Retrieval and Label Prediction
- **Arxiv ID**: http://arxiv.org/abs/1709.09426v1
- **DOI**: 10.1109/ICCVW.2017.266
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09426v1)
- **Published**: 2017-09-27 10:02:10+00:00
- **Updated**: 2017-09-27 10:02:10+00:00
- **Authors**: Charles Corbière, Hedi Ben-Younes, Alexandre Ramé, Charles Ollion
- **Comment**: None
- **Journal**: 2017 IEEE International Conference on Computer Vision Workshop
  (ICCVW)
- **Summary**: In this paper, we present a method to learn a visual representation adapted for e-commerce products. Based on weakly supervised learning, our model learns from noisy datasets crawled on e-commerce website catalogs and does not require any manual labeling. We show that our representation can be used for downward classification tasks over clothing categories with different levels of granularity. We also demonstrate that the learnt representation is suitable for image retrieval. We achieve nearly state-of-art results on the DeepFashion In-Shop Clothes Retrieval and Categories Attributes Prediction tasks, without using the provided training set.



### FoodNet: Recognizing Foods Using Ensemble of Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.09429v1
- **DOI**: 10.1109/LSP.2017.2758862
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09429v1)
- **Published**: 2017-09-27 10:13:31+00:00
- **Updated**: 2017-09-27 10:13:31+00:00
- **Authors**: Paritosh Pandey, Akella Deepthi, Bappaditya Mandal, N. B. Puhan
- **Comment**: 5 pages, 3 figures, 3 tables, IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: In this work we propose a methodology for an automatic food classification system which recognizes the contents of the meal from the images of the food. We developed a multi-layered deep convolutional neural network (CNN) architecture that takes advantages of the features from other deep networks and improves the efficiency. Numerous classical handcrafted features and approaches are explored, among which CNNs are chosen as the best performing features. Networks are trained and fine-tuned using preprocessed images and the filter outputs are fused to achieve higher accuracy. Experimental results on the largest real-world food recognition database ETH Food-101 and newly contributed Indian food image database demonstrate the effectiveness of the proposed methodology as compared to many other benchmark deep learned CNN frameworks.



### Fast Convolutional Sparse Coding in the Dual Domain
- **Arxiv ID**: http://arxiv.org/abs/1709.09479v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09479v2)
- **Published**: 2017-09-27 12:55:00+00:00
- **Updated**: 2018-04-08 12:03:33+00:00
- **Authors**: Lama Affara, Bernard Ghanem, Peter Wonka
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional sparse coding (CSC) is an important building block of many computer vision applications ranging from image and video compression to deep learning. We present two contributions to the state of the art in CSC. First, we significantly speed up the computation by proposing a new optimization framework that tackles the problem in the dual domain. Second, we extend the original formulation to higher dimensions in order to process a wider range of inputs, such as RGB images and videos. Our results show up to 20 times speedup compared to current state-of-the-art CSC solvers.



### Hierarchical Scene Parsing by Weakly Supervised Learning with Image Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1709.09490v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09490v2)
- **Published**: 2017-09-27 13:17:41+00:00
- **Updated**: 2018-01-27 03:25:40+00:00
- **Authors**: Ruimao Zhang, Liang Lin, Guangrun Wang, Meng Wang, Wangmeng Zuo
- **Comment**: Accepted by Transactions on Pattern Analysis and Machine Intelligence
  (T-PAMI) 2018
- **Journal**: None
- **Summary**: This paper investigates a fundamental problem of scene understanding: how to parse a scene image into a structured configuration (i.e., a semantic object hierarchy with object interaction relations). We propose a deep architecture consisting of two networks: i) a convolutional neural network (CNN) extracting the image representation for pixel-wise object labeling and ii) a recursive neural network (RsNN) discovering the hierarchical object structure and the inter-object relations. Rather than relying on elaborative annotations (e.g., manually labeled semantic maps and relations), we train our deep model in a weakly-supervised learning manner by leveraging the descriptive sentences of the training images. Specifically, we decompose each sentence into a semantic tree consisting of nouns and verb phrases, and apply these tree structures to discover the configurations of the training images. Once these scene configurations are determined, then the parameters of both the CNN and RsNN are updated accordingly by back propagation. The entire model training is accomplished through an Expectation-Maximization method. Extensive experiments show that our model is capable of producing meaningful scene configurations and achieving more favorable scene labeling results on two benchmarks (i.e., PASCAL VOC 2012 and SYSU-Scenes) compared with other state-of-the-art weakly-supervised deep learning methods. In particular, SYSU-Scenes contains more than 5000 scene images with their semantic sentence descriptions, which is created by us for advancing research on scene parsing.



### Modeling the Resource Requirements of Convolutional Neural Networks on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1709.09503v1
- **DOI**: 10.1145/3123266.3123389
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09503v1)
- **Published**: 2017-09-27 13:36:12+00:00
- **Updated**: 2017-09-27 13:36:12+00:00
- **Authors**: Zongqing Lu, Swati Rallapalli, Kevin Chan, Thomas La Porta
- **Comment**: None
- **Journal**: ACM Multimedia 2017
- **Summary**: Convolutional Neural Networks (CNNs) have revolutionized the research in computer vision, due to their ability to capture complex patterns, resulting in high inference accuracies. However, the increasingly complex nature of these neural networks means that they are particularly suited for server computers with powerful GPUs. We envision that deep learning applications will be eventually and widely deployed on mobile devices, e.g., smartphones, self-driving cars, and drones. Therefore, in this paper, we aim to understand the resource requirements (time, memory) of CNNs on mobile devices. First, by deploying several popular CNNs on mobile CPUs and GPUs, we measure and analyze the performance and resource usage for every layer of the CNNs. Our findings point out the potential ways of optimizing the performance on mobile devices. Second, we model the resource requirements of the different CNN computations. Finally, based on the measurement, pro ling, and modeling, we build and evaluate our modeling tool, Augur, which takes a CNN configuration (descriptor) as the input and estimates the compute time and resource usage of the CNN, to give insights about whether and how e ciently a CNN can be run on a given mobile platform. In doing so Augur tackles several challenges: (i) how to overcome pro ling and measurement overhead; (ii) how to capture the variance in different mobile platforms with different processors, memory, and cache sizes; and (iii) how to account for the variance in the number, type and size of layers of the different CNN configurations.



### ANSAC: Adaptive Non-minimal Sample and Consensus
- **Arxiv ID**: http://arxiv.org/abs/1709.09559v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09559v1)
- **Published**: 2017-09-27 14:47:03+00:00
- **Updated**: 2017-09-27 14:47:03+00:00
- **Authors**: Victor Fragoso, Chris Sweeney, Pradeep Sen, Matthew Turk
- **Comment**: None
- **Journal**: None
- **Summary**: While RANSAC-based methods are robust to incorrect image correspondences (outliers), their hypothesis generators are not robust to correct image correspondences (inliers) with positional error (noise). This slows down their convergence because hypotheses drawn from a minimal set of noisy inliers can deviate significantly from the optimal model. This work addresses this problem by introducing ANSAC, a RANSAC-based estimator that accounts for noise by adaptively using more than the minimal number of correspondences required to generate a hypothesis. ANSAC estimates the inlier ratio (the fraction of correct correspondences) of several ranked subsets of candidate correspondences and generates hypotheses from them. Its hypothesis-generation mechanism prioritizes the use of subsets with high inlier ratio to generate high-quality hypotheses. ANSAC uses an early termination criterion that keeps track of the inlier ratio history and terminates when it has not changed significantly for a period of time. The experiments show that ANSAC finds good homography and fundamental matrix estimates in a few iterations, consistently outperforming state-of-the-art methods.



### Connectivity Learning in Multi-Branch Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.09582v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.09582v2)
- **Published**: 2017-09-27 15:34:21+00:00
- **Updated**: 2017-12-07 16:57:45+00:00
- **Authors**: Karim Ahmed, Lorenzo Torresani
- **Comment**: None
- **Journal**: None
- **Summary**: While much of the work in the design of convolutional networks over the last five years has revolved around the empirical investigation of the importance of depth, filter sizes, and number of feature channels, recent studies have shown that branching, i.e., splitting the computation along parallel but distinct threads and then aggregating their outputs, represents a new promising dimension for significant improvements in performance. To combat the complexity of design choices in multi-branch architectures, prior work has adopted simple strategies, such as a fixed branching factor, the same input being fed to all parallel branches, and an additive combination of the outputs produced by all branches at aggregation points.   In this work we remove these predefined choices and propose an algorithm to learn the connections between branches in the network. Instead of being chosen a priori by the human designer, the multi-branch connectivity is learned simultaneously with the weights of the network by optimizing a single loss function defined with respect to the end task. We demonstrate our approach on the problem of multi-class image classification using three different datasets where it yields consistently higher accuracy compared to the state-of-the-art "ResNeXt" multi-branch network given the same learning capacity.



### Exposure: A White-Box Photo Post-Processing Framework
- **Arxiv ID**: http://arxiv.org/abs/1709.09602v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.09602v2)
- **Published**: 2017-09-27 16:15:58+00:00
- **Updated**: 2018-02-06 16:47:58+00:00
- **Authors**: Yuanming Hu, Hao He, Chenxi Xu, Baoyuan Wang, Stephen Lin
- **Comment**: ACM Transaction on Graphics (Accepted with minor revisions)
- **Journal**: None
- **Summary**: Retouching can significantly elevate the visual appeal of photos, but many casual photographers lack the expertise to do this well. To address this problem, previous works have proposed automatic retouching systems based on supervised learning from paired training images acquired before and after manual editing. As it is difficult for users to acquire paired images that reflect their retouching preferences, we present in this paper a deep learning approach that is instead trained on unpaired data, namely a set of photographs that exhibits a retouching style the user likes, which is much easier to collect. Our system is formulated using deep convolutional neural networks that learn to apply different retouching operations on an input image. Network training with respect to various types of edits is enabled by modeling these retouching operations in a unified manner as resolution-independent differentiable filters. To apply the filters in a proper sequence and with suitable parameters, we employ a deep reinforcement learning approach that learns to make decisions on what action to take next, given the current state of the image. In contrast to many deep learning systems, ours provides users with an understandable solution in the form of conventional retouching edits, rather than just a "black-box" result. Through quantitative comparisons and user studies, we show that this technique generates retouching results consistent with the provided photo set.



### Neural Multi-Atlas Label Fusion: Application to Cardiac MR Images
- **Arxiv ID**: http://arxiv.org/abs/1709.09641v2
- **DOI**: 10.1016/j.media.2018.07.009
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09641v2)
- **Published**: 2017-09-27 17:25:17+00:00
- **Updated**: 2018-08-01 18:34:11+00:00
- **Authors**: Heran Yang, Jian Sun, Huibin Li, Lisheng Wang, Zongben Xu
- **Comment**: Medical Image Analysis
- **Journal**: Heran Yang, et al. Neural Multi-Atlas Label Fusion: Application to
  Cardiac MR Images, Medical Image Analysis, 2018, ISSN 1361-8415
- **Summary**: Multi-atlas segmentation approach is one of the most widely-used image segmentation techniques in biomedical applications. There are two major challenges in this category of methods, i.e., atlas selection and label fusion. In this paper, we propose a novel multi-atlas segmentation method that formulates multi-atlas segmentation in a deep learning framework for better solving these challenges. The proposed method, dubbed deep fusion net (DFN), is a deep architecture that integrates a feature extraction subnet and a non-local patch-based label fusion (NL-PLF) subnet in a single network. The network parameters are learned by end-to-end training for automatically learning deep features that enable optimal performance in a NL-PLF framework. The learned deep features are further utilized in defining a similarity measure for atlas selection. By evaluating on two public cardiac MR datasets of SATA-13 and LV-09 for left ventricle segmentation, our approach achieved 0.833 in averaged Dice metric (ADM) on SATA-13 dataset and 0.95 in ADM for epicardium segmentation on LV-09 dataset, comparing favorably with the other automatic left ventricle segmentation methods. We also tested our approach on Cardiac Atlas Project (CAP) testing set of MICCAI 2013 SATA Segmentation Challenge, and our method achieved 0.815 in ADM, ranking highest at the time of writing.



### Exact Camera Location Recovery by Least Unsquared Deviations
- **Arxiv ID**: http://arxiv.org/abs/1709.09683v4
- **DOI**: 10.1137/17M115061X
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1709.09683v4)
- **Published**: 2017-09-27 18:11:40+00:00
- **Updated**: 2018-09-09 17:38:04+00:00
- **Authors**: Gilad Lerman, Yunpeng Shi, Teng Zhang
- **Comment**: None
- **Journal**: SIAM Journal on Imaging Sciences, 11 (2018), no. 4, 2692-2721
- **Summary**: We establish exact recovery for the Least Unsquared Deviations (LUD) algorithm of Ozyesil and Singer. More precisely, we show that for sufficiently many cameras with given corrupted pairwise directions, where both camera locations and pairwise directions are generated by a special probabilistic model, the LUD algorithm exactly recovers the camera locations with high probability. A similar exact recovery guarantee was established for the ShapeFit algorithm by Hand, Lee and Voroninski, but with typically less corruption.



### Learning Autoencoded Radon Projections
- **Arxiv ID**: http://arxiv.org/abs/1710.01247v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01247v1)
- **Published**: 2017-09-27 22:35:59+00:00
- **Updated**: 2017-09-27 22:35:59+00:00
- **Authors**: Aditya Sriram, Shivam Kalra, H. R. Tizhoosh, Shahryar Rahnamayan
- **Comment**: To appear in proceedings of The IEEE Symposium Series on
  Computational Intelligence (IEEE SSCI 2017), Honolulu, Hawaii, USA, Nov. 27
  -- Dec 1, 2017
- **Journal**: None
- **Summary**: Autoencoders have been recently used for encoding medical images. In this study, we design and validate a new framework for retrieving medical images by classifying Radon projections, compressed in the deepest layer of an autoencoder. As the autoencoder reduces the dimensionality, a multilayer perceptron (MLP) can be employed to classify the images. The integration of MLP promotes a rather shallow learning architecture which makes the training faster. We conducted a comparative study to examine the capabilities of autoencoders for different inputs such as raw images, Histogram of Oriented Gradients (HOG) and normalized Radon projections. Our framework is benchmarked on IRMA dataset containing $14,410$ x-ray images distributed across $57$ different classes. Experiments show an IRMA error of $313$ (equivalent to $\approx 82\%$ accuracy) outperforming state-of-the-art works on retrieval from IRMA dataset using autoencoders.



### Skin Lesion Segmentation: U-Nets versus Clustering
- **Arxiv ID**: http://arxiv.org/abs/1710.01248v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01248v1)
- **Published**: 2017-09-27 22:46:29+00:00
- **Updated**: 2017-09-27 22:46:29+00:00
- **Authors**: Bill S. Lin, Kevin Michael, Shivam Kalra, H. R. Tizhoosh
- **Comment**: To appear in proceedings of The IEEE Symposium Series on
  Computational Intelligence (IEEE SSCI 2017), Honolulu, Hawaii, USA, Nov. 27
  -- Dec 1, 2017
- **Journal**: None
- **Summary**: Many automatic skin lesion diagnosis systems use segmentation as a preprocessing step to diagnose skin conditions because skin lesion shape, border irregularity, and size can influence the likelihood of malignancy. This paper presents, examines and compares two different approaches to skin lesion segmentation. The first approach uses U-Nets and introduces a histogram equalization based preprocessing step. The second approach is a C-Means clustering based approach that is much simpler to implement and faster to execute. The Jaccard Index between the algorithm output and hand segmented images by dermatologists is used to evaluate the proposed algorithms. While many recently proposed deep neural networks to segment skin lesions require a significant amount of computational power for training (i.e., computer with GPUs), the main objective of this paper is to present methods that can be used with only a CPU. This severely limits, for example, the number of training instances that can be presented to the U-Net. Comparing the two proposed algorithms, U-Nets achieved a significantly higher Jaccard Index compared to the clustering approach. Moreover, using the histogram equalization for preprocessing step significantly improved the U-Net segmentation results.



### Combining Real-Valued and Binary Gabor-Radon Features for Classification and Search in Medical Imaging Archives
- **Arxiv ID**: http://arxiv.org/abs/1709.09754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09754v1)
- **Published**: 2017-09-27 22:53:13+00:00
- **Updated**: 2017-09-27 22:53:13+00:00
- **Authors**: Hamed Erfankhah, Mehran Yazdi, H. R. Tizhoosh
- **Comment**: To appear in proceedings of The IEEE Symposium Series on
  Computational Intelligence (IEEE SSCI 2017), Honolulu, Hawaii, USA, Nov. 27
  -- Dec 1, 2017
- **Journal**: None
- **Summary**: Content-based image retrieval (CBIR) of medical images in large datasets to identify similar images when a query image is given can be very useful in improving the diagnostic decision of the clinical experts and as well in educational scenarios. In this paper, we used two stage classification and retrieval approach to retrieve similar images. First, the Gabor filters are applied to Radon-transformed images to extract features and to train a multi-class SVM. Then based on the classification results and using an extracted Gabor barcode, similar images are retrieved. The proposed method was tested on IRMA dataset which contains more than 14,000 images. Experimental results show the efficiency of our approach in retrieving similar images compared to other Gabor-Radon-oriented methods.



### A Comparative Study of CNN, BoVW and LBP for Classification of Histopathological Images
- **Arxiv ID**: http://arxiv.org/abs/1710.01249v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01249v1)
- **Published**: 2017-09-27 23:02:52+00:00
- **Updated**: 2017-09-27 23:02:52+00:00
- **Authors**: Meghana Dinesh Kumar, Morteza Babaie, Shujin Zhu, Shivam Kalra, H. R. Tizhoosh
- **Comment**: To appear in proceedings of The IEEE Symposium Series on
  Computational Intelligence (IEEE SSCI 2017), Honolulu, Hawaii, USA, Nov. 27
  -- Dec 1, 2017
- **Journal**: None
- **Summary**: Despite the progress made in the field of medical imaging, it remains a large area of open research, especially due to the variety of imaging modalities and disease-specific characteristics. This paper is a comparative study describing the potential of using local binary patterns (LBP), deep features and the bag-of-visual words (BoVW) scheme for the classification of histopathological images. We introduce a new dataset, \emph{KIMIA Path960}, that contains 960 histopathology images belonging to 20 different classes (different tissue types). We make this dataset publicly available. The small size of the dataset and its inter- and intra-class variability makes it ideal for initial investigations when comparing image descriptors for search and classification in complex medical imaging cases like histopathology. We investigate deep features, LBP histograms and BoVW to classify the images via leave-one-out validation. The accuracy of image classification obtained using LBP was 90.62\% while the highest accuracy using deep features reached 94.72\%. The dictionary approach (BoVW) achieved 96.50\%. Deep solutions may be able to deliver higher accuracies but they need extensive training with a large number of (balanced) image datasets.



