# Arxiv Papers in cs.CV on 2017-09-26
### Image similarity using Deep CNN and Curriculum Learning
- **Arxiv ID**: http://arxiv.org/abs/1709.08761v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08761v2)
- **Published**: 2017-09-26 00:25:40+00:00
- **Updated**: 2018-07-13 15:34:34+00:00
- **Authors**: Srikar Appalaraju, Vineet Chaoji
- **Comment**: 9 pages, 6 figures, GHCI 17 conference
- **Journal**: None
- **Summary**: Image similarity involves fetching similar looking images given a reference image. Our solution called SimNet, is a deep siamese network which is trained on pairs of positive and negative images using a novel online pair mining strategy inspired by Curriculum learning. We also created a multi-scale CNN, where the final image embedding is a joint representation of top as well as lower layer embedding's. We go on to show that this multi-scale siamese network is better at capturing fine grained image similarities than traditional CNN's.



### Dynamic Reconfiguration of Mission Parameters in Underwater Human-Robot Collaboration
- **Arxiv ID**: http://arxiv.org/abs/1709.08772v8
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.08772v8)
- **Published**: 2017-09-26 01:04:46+00:00
- **Updated**: 2018-02-20 23:04:45+00:00
- **Authors**: Md Jahidul Islam, Marc Ho, Junaed Sattar
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a real-time programming and parameter reconfiguration method for autonomous underwater robots in human-robot collaborative tasks. Using a set of intuitive and meaningful hand gestures, we develop a syntactically simple framework that is computationally more efficient than a complex, grammar-based approach. In the proposed framework, a convolutional neural network is trained to provide accurate hand gesture recognition; subsequently, a finite-state machine-based deterministic model performs efficient gesture-to-instruction mapping, and further improves robustness of the interaction scheme. The key aspect of this framework is that it can be easily adopted by divers for communicating simple instructions to underwater robots without using artificial tags such as fiducial markers, or requiring them to memorize a potentially complex set of language rules. Extensive experiments are performed both on field-trial data and through simulation, which demonstrate the robustness, efficiency, and portability of this framework in a number of different scenarios. Finally, a user interaction study is presented that illustrates the gain in usability of our proposed interaction framework compared to the existing methods for underwater domains.



### Converting Your Thoughts to Texts: Enabling Brain Typing via Deep Feature Learning of EEG Signals
- **Arxiv ID**: http://arxiv.org/abs/1709.08820v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.08820v1)
- **Published**: 2017-09-26 04:20:34+00:00
- **Updated**: 2017-09-26 04:20:34+00:00
- **Authors**: Xiang Zhang, Lina Yao, Quan Z. Sheng, Salil S. Kanhere, Tao Gu, Dalin Zhang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: An electroencephalography (EEG) based Brain Computer Interface (BCI) enables people to communicate with the outside world by interpreting the EEG signals of their brains to interact with devices such as wheelchairs and intelligent robots. More specifically, motor imagery EEG (MI-EEG), which reflects a subjects active intent, is attracting increasing attention for a variety of BCI applications. Accurate classification of MI-EEG signals while essential for effective operation of BCI systems, is challenging due to the significant noise inherent in the signals and the lack of informative correlation between the signals and brain activities. In this paper, we propose a novel deep neural network based learning framework that affords perceptive insights into the relationship between the MI-EEG data and brain activities. We design a joint convolutional recurrent neural network that simultaneously learns robust high-level feature presentations through low-dimensional dense embeddings from raw MI-EEG signals. We also employ an Autoencoder layer to eliminate various artifacts such as background activities. The proposed approach has been evaluated extensively on a large- scale public MI-EEG dataset and a limited but easy-to-deploy dataset collected in our lab. The results show that our approach outperforms a series of baselines and the competitive state-of-the- art methods, yielding a classification accuracy of 95.53%. The applicability of our proposed approach is further demonstrated with a practical BCI system for typing.



### Towards End-to-End Car License Plates Detection and Recognition with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.08828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08828v1)
- **Published**: 2017-09-26 04:50:23+00:00
- **Updated**: 2017-09-26 04:50:23+00:00
- **Authors**: Hui Li, Peng Wang, Chunhua Shen
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: In this work, we tackle the problem of car license plate detection and recognition in natural scene images. We propose a unified deep neural network which can localize license plates and recognize the letters simultaneously in a single forward pass. The whole network can be trained end-to-end. In contrast to existing approaches which take license plate detection and recognition as two separate tasks and settle them step by step, our method jointly solves these two tasks by a single network. It not only avoids intermediate error accumulation, but also accelerates the processing speed. For performance evaluation, three datasets including images captured from various scenes under different conditions are tested. Extensive experiments show the effectiveness and efficiency of our proposed approach.



### UAV and Service Robot Coordination for Indoor Object Search Tasks
- **Arxiv ID**: http://arxiv.org/abs/1709.08831v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.08831v1)
- **Published**: 2017-09-26 05:04:37+00:00
- **Updated**: 2017-09-26 05:04:37+00:00
- **Authors**: Sandeep Konam, Stephanie Rosenthal, Manuela Veloso
- **Comment**: IJCAI-2016 Workshop on Autonomous Mobile Service Robots
- **Journal**: None
- **Summary**: Our CoBot robots have successfully performed a variety of service tasks in our multi-building environment including accompanying people to meetings and delivering objects to offices due to its navigation and localization capabilities. However, they lack the capability to visually search over desks and other confined locations for an object of interest. Conversely, an inexpensive GPS-denied quadcopter platform such as the Parrot ARDrone 2.0 could perform this object search task if it had access to reasonable localization. In this paper, we propose the concept of coordination between CoBot and the Parrot ARDrone 2.0 to perform service-based object search tasks, in which CoBot localizes and navigates to the general search areas carrying the ARDrone and the ARDrone searches locally for objects. We propose a vision-based moving target navigation algorithm that enables the ARDrone to localize with respect to CoBot, search for objects, and return to the CoBot for future searches. We demonstrate our algorithm in indoor environments on several search trajectories.



### An In-field Automatic Wheat Disease Diagnosis System
- **Arxiv ID**: http://arxiv.org/abs/1710.08299v1
- **DOI**: 10.1016/j.compag.2017.09.012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.08299v1)
- **Published**: 2017-09-26 05:20:12+00:00
- **Updated**: 2017-09-26 05:20:12+00:00
- **Authors**: Jiang Lu, Jie Hu, Guannan Zhao, Fenghua Mei, Changshui Zhang
- **Comment**: 15 pages
- **Journal**: Computers and Electronics in Agriculture, 142PA (2017): 369-379
- **Summary**: Crop diseases are responsible for the major production reduction and economic losses in agricultural industry world- wide. Monitoring for health status of crops is critical to control the spread of diseases and implement effective management. This paper presents an in-field automatic wheat disease diagnosis system based on a weakly super- vised deep learning framework, i.e. deep multiple instance learning, which achieves an integration of identification for wheat diseases and localization for disease areas with only image-level annotation for training images in wild conditions. Furthermore, a new in-field image dataset for wheat disease, Wheat Disease Database 2017 (WDD2017), is collected to verify the effectiveness of our system. Under two different architectures, i.e. VGG-FCN-VD16 and VGG-FCN-S, our system achieves the mean recognition accuracies of 97.95% and 95.12% respectively over 5-fold cross-validation on WDD2017, exceeding the results of 93.27% and 73.00% by two conventional CNN frameworks, i.e. VGG-CNN-VD16 and VGG-CNN-S. Experimental results demonstrate that the proposed system outperforms conventional CNN architectures on recognition accuracy under the same amount of parameters, meanwhile main- taining accurate localization for corresponding disease areas. Moreover, the proposed system has been packed into a real-time mobile app to provide support for agricultural disease diagnosis.



### Learning to Inpaint for Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1709.08855v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08855v4)
- **Published**: 2017-09-26 06:30:55+00:00
- **Updated**: 2017-11-10 06:58:40+00:00
- **Authors**: Mohammad Haris Baig, Vladlen Koltun, Lorenzo Torresani
- **Comment**: Published in Advances in Neural Information Processing Systems (NIPS
  2017)
- **Journal**: None
- **Summary**: We study the design of deep architectures for lossy image compression. We present two architectural recipes in the context of multi-stage progressive encoders and empirically demonstrate their importance on compression performance. Specifically, we show that: (a) predicting the original image data from residuals in a multi-stage progressive architecture facilitates learning and leads to improved performance at approximating the original content and (b) learning to inpaint (from neighboring image pixels) before performing compression reduces the amount of information that must be stored to achieve a high-quality approximation. Incorporating these design choices in a baseline progressive encoder yields an average reduction of over $60\%$ in file size with similar quality compared to the original residual encoder.



### Robust non-local means filter for ultrasound image denoising
- **Arxiv ID**: http://arxiv.org/abs/1710.01245v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01245v1)
- **Published**: 2017-09-26 07:36:10+00:00
- **Updated**: 2017-09-26 07:36:10+00:00
- **Authors**: Hamid Reza Shahdoosti
- **Comment**: 6 pages, 2 figures, conference
- **Journal**: None
- **Summary**: This paper introduces a new approach to non-local means image denoising. Instead of using all pixels located in the search window for estimating the value of a pixel, we identify the highly corrupted pixels and assign less weight to these pixels. This method is called robust non-local means. Numerical and subjective evaluations using ultrasound images show good performances of the proposed denoising method in recovering the shape of edges and important detailed components, in comparison to traditional ultrasound image denoising methods



### Learning Energy-Based Models as Generative ConvNets via Multi-grid Modeling and Sampling
- **Arxiv ID**: http://arxiv.org/abs/1709.08868v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.08868v3)
- **Published**: 2017-09-26 07:48:52+00:00
- **Updated**: 2020-10-14 22:43:45+00:00
- **Authors**: Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, Ying Nian Wu
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: This paper proposes a multi-grid method for learning energy-based generative ConvNet models of images. For each grid, we learn an energy-based probabilistic model where the energy function is defined by a bottom-up convolutional neural network (ConvNet or CNN). Learning such a model requires generating synthesized examples from the model. Within each iteration of our learning algorithm, for each observed training image, we generate synthesized images at multiple grids by initializing the finite-step MCMC sampling from a minimal 1 x 1 version of the training image. The synthesized image at each subsequent grid is obtained by a finite-step MCMC initialized from the synthesized image generated at the previous coarser grid. After obtaining the synthesized examples, the parameters of the models at multiple grids are updated separately and simultaneously based on the differences between synthesized and observed examples. We show that this multi-grid method can learn realistic energy-based generative ConvNet models, and it outperforms the original contrastive divergence (CD) and persistent CD.



### Learning to Label Affordances from Simulated and Real Data
- **Arxiv ID**: http://arxiv.org/abs/1709.08872v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.08872v1)
- **Published**: 2017-09-26 07:58:19+00:00
- **Updated**: 2017-09-26 07:58:19+00:00
- **Authors**: Timo Lüddecke, Florentin Wörgötter
- **Comment**: None
- **Journal**: None
- **Summary**: An autonomous robot should be able to evaluate the affordances that are offered by a given situation. Here we address this problem by designing a system that can densely predict affordances given only a single 2D RGB image. This is achieved with a convolutional neural network (ResNet), which we combine with refinement modules recently proposed for addressing semantic image segmentation. We define a novel cost function, which is able to handle (potentially multiple) affordances of objects and their parts in a pixel-wise manner even in the case of incomplete data. We perform qualitative as well as quantitative evaluations with simulated and real data assessing 15 different affordances. In general, we find that affordances, which are well-enough represented in the training data, are correctly recognized with a substantial fraction of correctly assigned pixels. Furthermore, we show that our model outperforms several baselines. Hence, this method can give clear action guidelines for a robot.



### UBSegNet: Unified Biometric Region of Interest Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/1709.08924v1
- **DOI**: 10.1109/ACPR.2017.148
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08924v1)
- **Published**: 2017-09-26 10:14:35+00:00
- **Updated**: 2017-09-26 10:14:35+00:00
- **Authors**: Ranjeet Ranjan Jha, Daksh Thapar, Shreyas Malakarjun Patil, Aditya Nigam
- **Comment**: 4th Asian Conference on Pattern Recognition (ACPR 2017)
- **Journal**: None
- **Summary**: Digital human identity management, can now be seen as a social necessity, as it is essentially required in almost every public sector such as, financial inclusions, security, banking, social networking e.t.c. Hence, in today's rampantly emerging world with so many adversarial entities, relying on a single biometric trait is being too optimistic. In this paper, we have proposed a novel end-to-end, Unified Biometric ROI Segmentation Network (UBSegNet), for extracting region of interest from five different biometric traits viz. face, iris, palm, knuckle and 4-slap fingerprint. The architecture of the proposed UBSegNet consists of two stages: (i) Trait classification and (ii) Trait localization. For these stages, we have used a state of the art region based convolutional neural network (RCNN), comprising of three major parts namely convolutional layers, region proposal network (RPN) along with classification and regression heads. The model has been evaluated over various huge publicly available biometric databases. To the best of our knowledge this is the first unified architecture proposed, segmenting multiple biometric traits. It has been tested over around 5000 * 5 = 25,000 images (5000 images per trait) and produces very good results. Our work on unified biometric segmentation, opens up the vast opportunities in the field of multiple biometric traits based authentication systems.



### Multi-layer Visualization for Medical Mixed Reality
- **Arxiv ID**: http://arxiv.org/abs/1709.08962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08962v1)
- **Published**: 2017-09-26 12:13:01+00:00
- **Updated**: 2017-09-26 12:13:01+00:00
- **Authors**: Séverine Habert, Ma Meng, Pascal Fallavollita, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Medical Mixed Reality helps surgeons to contextualize intraoperative data with video of the surgical scene. Nonetheless, the surgical scene and anatomical target are often occluded by surgical instruments and surgeon hands. In this paper and to our knowledge, we propose a multi-layer visualization in Medical Mixed Reality solution which subtly improves a surgeon's visualization by making transparent the occluding objects. As an example scenario, we use an augmented reality C-arm fluoroscope device. A video image is created using a volumetric-based image synthesization technique and stereo-RGBD cameras mounted on the C-arm. From this synthesized view, the background which is occluded by the surgical instruments and surgeon hands is recovered by modifying the volumetric-based image synthesization technique. The occluding objects can, therefore, become transparent over the surgical scene. Experimentation with different augmented reality scenarios yield results demonstrating that the background of the surgical scenes can be recovered with accuracy between 45%-99%. In conclusion, we presented a solution that a Mixed Reality solution for medicine, providing transparency to objects occluding the surgical scene. This work is also the first application of volumetric field for Diminished Reality/ Mixed Reality.



### Automated sub-cortical brain structure segmentation combining spatial and deep convolutional features
- **Arxiv ID**: http://arxiv.org/abs/1709.09075v1
- **DOI**: 10.1016/j.media.2018.06.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09075v1)
- **Published**: 2017-09-26 15:02:16+00:00
- **Updated**: 2017-09-26 15:02:16+00:00
- **Authors**: Kaisar Kushibar, Sergi Valverde, Sandra Gonzalez-Villa, Jose Bernal, Mariano Cabezas, Arnau Oliver, Xavier Llado
- **Comment**: None
- **Journal**: None
- **Summary**: Sub-cortical brain structure segmentation in Magnetic Resonance Images (MRI) has attracted the interest of the research community for a long time because morphological changes in these structures are related to different neurodegenerative disorders. However, manual segmentation of these structures can be tedious and prone to variability, highlighting the need for robust automated segmentation methods. In this paper, we present a novel convolutional neural network based approach for accurate segmentation of the sub-cortical brain structures that combines both convolutional and prior spatial features for improving the segmentation accuracy. In order to increase the accuracy of the automated segmentation, we propose to train the network using a restricted sample selection to force the network to learn the most difficult parts of the structures. We evaluate the accuracy of the proposed method on the public MICCAI 2012 challenge and IBSR 18 datasets, comparing it with different available state-of-the-art methods and other recently proposed deep learning approaches. On the MICCAI 2012 dataset, our method shows an excellent performance comparable to the best challenge participant strategy, while performing significantly better than state-of-the-art techniques such as FreeSurfer and FIRST. On the IBSR 18 dataset, our method also exhibits a significant increase in the performance with respect to not only FreeSurfer and FIRST, but also comparable or better results than other recent deep learning approaches. Moreover, our experiments show that both the addition of the spatial priors and the restricted sampling strategy have a significant effect on the accuracy of the proposed method. In order to encourage the reproducibility and the use of the proposed method, a public version of our approach is available to download for the neuroimaging community.



### Multi-Person Brain Activity Recognition via Comprehensive EEG Signal Analysis
- **Arxiv ID**: http://arxiv.org/abs/1709.09077v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.09077v1)
- **Published**: 2017-09-26 15:03:13+00:00
- **Updated**: 2017-09-26 15:03:13+00:00
- **Authors**: Xiang Zhang, Lina Yao, Dalin Zhang, Xianzhi Wang, Quan Z. Sheng, Tao Gu
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: An electroencephalography (EEG) based brain activity recognition is a fundamental field of study for a number of significant applications such as intention prediction, appliance control, and neurological disease diagnosis in smart home and smart healthcare domains. Existing techniques mostly focus on binary brain activity recognition for a single person, which limits their deployment in wider and complex practical scenarios. Therefore, multi-person and multi-class brain activity recognition has obtained popularity recently. Another challenge faced by brain activity recognition is the low recognition accuracy due to the massive noises and the low signal-to-noise ratio in EEG signals. Moreover, the feature engineering in EEG processing is time-consuming and highly re- lies on the expert experience. In this paper, we attempt to solve the above challenges by proposing an approach which has better EEG interpretation ability via raw Electroencephalography (EEG) signal analysis for multi-person and multi-class brain activity recognition. Specifically, we analyze inter-class and inter-person EEG signal characteristics, based on which to capture the discrepancy of inter-class EEG data. Then, we adopt an Autoencoder layer to automatically refine the raw EEG signals by eliminating various artifacts. We evaluate our approach on both a public and a local EEG datasets and conduct extensive experiments to explore the effect of several factors (such as normalization methods, training data size, and Autoencoder hidden neuron size) on the recognition results. The experimental results show that our approach achieves a high accuracy comparing to competitive state-of-the-art methods, indicating its potential in promoting future research on multi-person EEG recognition.



### Scale Adaptive Clustering of Multiple Structures
- **Arxiv ID**: http://arxiv.org/abs/1709.09550v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09550v1)
- **Published**: 2017-09-26 15:15:52+00:00
- **Updated**: 2017-09-26 15:15:52+00:00
- **Authors**: Xiang Yang, Peter Meer
- **Comment**: 14 pages, 17 figures. arXiv admin note: substantial text overlap with
  arXiv:1609.06371
- **Journal**: None
- **Summary**: We propose the segmentation of noisy datasets into Multiple Inlier Structures with a new Robust Estimator (MISRE). The scale of each individual structure is estimated adaptively from the input data and refined by mean shift, without tuning any parameter in the process, or manually specifying thresholds for different estimation problems. Once all the data points were classified into separate structures, these structures are sorted by their densities with the strongest inlier structures coming out first. Several 2D and 3D synthetic and real examples are presented to illustrate the efficiency, robustness and the limitations of the MISRE algorithm.



### Region-Based Image Retrieval Revisited
- **Arxiv ID**: http://arxiv.org/abs/1709.09106v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.09106v1)
- **Published**: 2017-09-26 16:09:48+00:00
- **Updated**: 2017-09-26 16:09:48+00:00
- **Authors**: Ryota Hinami, Yusuke Matsui, Shin'ichi Satoh
- **Comment**: To appear in ACM Multimedia 2017 (Oral)
- **Journal**: None
- **Summary**: Region-based image retrieval (RBIR) technique is revisited. In early attempts at RBIR in the late 90s, researchers found many ways to specify region-based queries and spatial relationships; however, the way to characterize the regions, such as by using color histograms, were very poor at that time. Here, we revisit RBIR by incorporating semantic specification of objects and intuitive specification of spatial relationships. Our contributions are the following. First, to support multiple aspects of semantic object specification (category, instance, and attribute), we propose a multitask CNN feature that allows us to use deep learning technique and to jointly handle multi-aspect object specification. Second, to help users specify spatial relationships among objects in an intuitive way, we propose recommendation techniques of spatial relationships. In particular, by mining the search results, a system can recommend feasible spatial relationships among the objects. The system also can recommend likely spatial relationships by assigned object category names based on language prior. Moreover, object-level inverted indexing supports very fast shortlist generation, and re-ranking based on spatial constraints provides users with instant RBIR experiences.



### Tensor Product Generation Networks for Deep NLP Modeling
- **Arxiv ID**: http://arxiv.org/abs/1709.09118v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1709.09118v5)
- **Published**: 2017-09-26 16:32:20+00:00
- **Updated**: 2017-12-16 12:01:09+00:00
- **Authors**: Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, Dapeng Wu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new approach to the design of deep networks for natural language processing (NLP), based on the general technique of Tensor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks. A network architecture --- the Tensor Product Generation Network (TPGN) --- is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation.



### Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1709.09121v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09121v1)
- **Published**: 2017-09-26 16:38:03+00:00
- **Updated**: 2017-09-26 16:38:03+00:00
- **Authors**: Ryota Hinami, Tao Mei, Shin'ichi Satoh
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: This paper addresses the problem of joint detection and recounting of abnormal events in videos. Recounting of abnormal events, i.e., explaining why they are judged to be abnormal, is an unexplored but critical task in video surveillance, because it helps human observers quickly judge if they are false alarms or not. To describe the events in the human-understandable form for event recounting, learning generic knowledge about visual concepts (e.g., object and action) is crucial. Although convolutional neural networks (CNNs) have achieved promising results in learning such concepts, it remains an open question as to how to effectively use CNNs for abnormal event detection, mainly due to the environment-dependent nature of the anomaly detection. In this paper, we tackle this problem by integrating a generic CNN model and environment-dependent anomaly detectors. Our approach first learns CNN with multiple visual tasks to exploit semantic information that is useful for detecting and recounting abnormal events. By appropriately plugging the model into anomaly detectors, we can detect and recount abnormal events while taking advantage of the discriminative power of CNNs. Our approach outperforms the state-of-the-art on Avenue and UCSD Ped2 benchmarks for abnormal event detection and also produces promising results of abnormal event recounting.



### Understanding Infographics through Textual and Visual Tag Prediction
- **Arxiv ID**: http://arxiv.org/abs/1709.09215v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09215v1)
- **Published**: 2017-09-26 18:45:28+00:00
- **Updated**: 2017-09-26 18:45:28+00:00
- **Authors**: Zoya Bylinskii, Sami Alsheikh, Spandan Madan, Adria Recasens, Kimberli Zhong, Hanspeter Pfister, Fredo Durand, Aude Oliva
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the problem of visual hashtag discovery for infographics: extracting visual elements from an infographic that are diagnostic of its topic. Given an infographic as input, our computational approach automatically outputs textual and visual elements predicted to be representative of the infographic content. Concretely, from a curated dataset of 29K large infographic images sampled across 26 categories and 391 tags, we present an automated two step approach. First, we extract the text from an infographic and use it to predict text tags indicative of the infographic content. And second, we use these predicted text tags as a supervisory signal to localize the most diagnostic visual elements from within the infographic i.e. visual hashtags. We report performances on a categorization and multi-label tag prediction problem and compare our proposed visual hashtags to human annotations.



### A feasibility study for predicting optimal radiation therapy dose distributions of prostate cancer patients from patient anatomy using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1709.09233v4
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.09233v4)
- **Published**: 2017-09-26 19:43:29+00:00
- **Updated**: 2018-11-29 21:34:27+00:00
- **Authors**: Dan Nguyen, Troy Long, Xun Jia, Weiguo Lu, Xuejun Gu, Zohaib Iqbal, Steve Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: With the advancement of treatment modalities in radiation therapy for cancer patients, outcomes have improved, but at the cost of increased treatment plan complexity and planning time. The accurate prediction of dose distributions would alleviate this issue by guiding clinical plan optimization to save time and maintain high quality plans. We have modified a convolutional deep network model, U-net (originally designed for segmentation purposes), for predicting dose from patient image contours of the planning target volume (PTV) and organs at risk (OAR). We show that, as an example, we are able to accurately predict the dose of intensity-modulated radiation therapy (IMRT) for prostate cancer patients, where the average Dice similarity coefficient is 0.91 when comparing the predicted vs. true isodose volumes between 0% and 100% of the prescription dose. The average value of the absolute differences in [max, mean] dose is found to be under 5% of the prescription dose, specifically for each structure is [1.80%, 1.03%](PTV), [1.94%, 4.22%](Bladder), [1.80%, 0.48%](Body), [3.87%, 1.79%](L Femoral Head), [5.07%, 2.55%](R Femoral Head), and [1.26%, 1.62%](Rectum) of the prescription dose. We thus managed to map a desired radiation dose distribution from a patient's PTV and OAR contours. As an additional advantage, relatively little data was used in the techniques and models described in this paper.



### Fast Shadow Detection from a Single Image Using a Patched Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1709.09283v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09283v2)
- **Published**: 2017-09-26 23:20:42+00:00
- **Updated**: 2018-03-16 18:35:25+00:00
- **Authors**: Sepideh Hosseinzadeh, Moein Shakeri, Hong Zhang
- **Comment**: 6 pages, 5 figures, Submitted to IROS 2018
- **Journal**: None
- **Summary**: In recent years, various shadow detection methods from a single image have been proposed and used in vision systems; however, most of them are not appropriate for the robotic applications due to the expensive time complexity. This paper introduces a fast shadow detection method using a deep learning framework, with a time cost that is appropriate for robotic applications. In our solution, we first obtain a shadow prior map with the help of multi-class support vector machine using statistical features. Then, we use a semantic- aware patch-level Convolutional Neural Network that efficiently trains on shadow examples by combining the original image and the shadow prior map. Experiments on benchmark datasets demonstrate the proposed method significantly decreases the time complexity of shadow detection, by one or two orders of magnitude compared with state-of-the-art methods, without losing accuracy.



