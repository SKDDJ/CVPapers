# Arxiv Papers in cs.CV on 2017-09-22
### Virtual Blood Vessels in Complex Background using Stereo X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/1709.07551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07551v1)
- **Published**: 2017-09-22 00:43:55+00:00
- **Updated**: 2017-09-22 00:43:55+00:00
- **Authors**: Qiuyu Chen, Ryoma Bise, Lin Gu, Yinqiang Zheng, Imari Sato, Jenq-Neng Hwang, Nobuaki Imanishi, Sadakazu Aiso
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a fully automatic system to reconstruct and visualize 3D blood vessels in Augmented Reality (AR) system from stereo X-ray images with bones and body fat. Currently, typical 3D imaging technologies are expensive and carrying the risk of irradiation exposure. To reduce the potential harm, we only need to take two X-ray images before visualizing the vessels. Our system can effectively reconstruct and visualize vessels in following steps. We first conduct initial segmentation using Markov Random Field and then refine segmentation in an entropy based post-process. We parse the segmented vessels by extracting their centerlines and generating trees. We propose a coarse-to-fine scheme for stereo matching, including initial matching using affine transform and dense matching using Hungarian algorithm guided by Gaussian regression. Finally, we render and visualize the reconstructed model in a HoloLens based AR system, which can essentially change the way of visualizing medical data. We have evaluated its performance by using synthetic and real stereo X-ray images, and achieved satisfactory quantitative and qualitative results.



### Novel Evaluation Metrics for Seam Carving based Image Retargeting
- **Arxiv ID**: http://arxiv.org/abs/1709.07565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07565v1)
- **Published**: 2017-09-22 01:36:11+00:00
- **Updated**: 2017-09-22 01:36:11+00:00
- **Authors**: Tam V. Nguyen, Guangyu Gao
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Image retargeting effectively resizes images by preserving the recognizability of important image regions. Most of retargeting methods rely on good importance maps as a cue to retain or remove certain regions in the input image. In addition, the traditional evaluation exhaustively depends on user ratings. There is a legitimate need for a methodological approach for evaluating retargeted results. Therefore, in this paper, we conduct a study and analysis on the prominent method in image retargeting, Seam Carving. First, we introduce two novel evaluation metrics which can be considered as the proxy of user ratings. Second, we exploit salient object dataset as a benchmark for this task. We then investigate different types of importance maps for this particular problem. The experiments show that humans in general agree with the evaluation metrics on the retargeted results and some importance map methods are consistently more favorable than others.



### Smart Mirror: Intelligent Makeup Recommendation and Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1709.07566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07566v1)
- **Published**: 2017-09-22 01:36:23+00:00
- **Updated**: 2017-09-22 01:36:23+00:00
- **Authors**: Tam V. Nguyen, Luoqi Liu
- **Comment**: accepted to ACM MM 2017
- **Journal**: None
- **Summary**: The female facial image beautification usually requires professional editing softwares, which are relatively difficult for common users. In this demo, we introduce a practical system for automatic and personalized facial makeup recommendation and synthesis. First, a model describing the relations among facial features, facial attributes and makeup attributes is learned as the makeup recommendation model for suggesting the most suitable makeup attributes. Then the recommended makeup attributes are seamlessly synthesized onto the input facial image.



### Hierarchical Detail Enhancing Mesh-Based Shape Generation with 3D Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1709.07581v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.07581v1)
- **Published**: 2017-09-22 03:17:34+00:00
- **Updated**: 2017-09-22 03:17:34+00:00
- **Authors**: Chiyu "Max" Jiang, Philip Marcus
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic mesh-based shape generation is of great interest across a wide range of disciplines, from industrial design to gaming, computer graphics and various other forms of digital art. While most traditional methods focus on primitive based model generation, advances in deep learning made it possible to learn 3-dimensional geometric shape representations in an end-to-end manner. However, most current deep learning based frameworks focus on the representation and generation of voxel and point-cloud based shapes, making it not directly applicable to design and graphics communities. This study addresses the needs for automatic generation of mesh-based geometries, and propose a novel framework that utilizes signed distance function representation that generates detail preserving three-dimensional surface mesh by a deep learning based approach.



### Happy Travelers Take Big Pictures: A Psychological Study with Machine Learning and Big Data
- **Arxiv ID**: http://arxiv.org/abs/1709.07584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07584v1)
- **Published**: 2017-09-22 03:25:34+00:00
- **Updated**: 2017-09-22 03:25:34+00:00
- **Authors**: Xuefeng Liang, Lixin Fan, Yuen Peng Loh, Yang Liu, Song Tong
- **Comment**: None
- **Journal**: None
- **Summary**: In psychology, theory-driven researches are usually conducted with extensive laboratory experiments, yet rarely tested or disproved with big data. In this paper, we make use of 418K travel photos with traveler ratings to test the influential "broaden-and-build" theory, that suggests positive emotions broaden one's visual attention. The core hypothesis examined in this study is that positive emotion is associated with a wider attention, hence highly-rated sites would trigger wide-angle photographs. By analyzing travel photos, we find a strong correlation between a preference for wide-angle photos and the high rating of tourist sites on TripAdvisor. We are able to carry out this analysis through the use of deep learning algorithms to classify the photos into wide and narrow angles, and present this study as an exemplar of how big data and deep learning can be used to test laboratory findings in the wild.



### Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.07592v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07592v3)
- **Published**: 2017-09-22 05:03:39+00:00
- **Updated**: 2018-03-30 05:29:11+00:00
- **Authors**: Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, Jiebo Luo
- **Comment**: To appear in Proceedings of CVPR 2018
- **Journal**: None
- **Summary**: Taking a photo outside, can we predict the immediate future, e.g., how would the cloud move in the sky? We address this problem by presenting a generative adversarial network (GAN) based two-stage approach to generating realistic time-lapse videos of high resolution. Given the first frame, our model learns to generate long-term future frames. The first stage generates videos of realistic contents for each frame. The second stage refines the generated video from the first stage by enforcing it to be closer to real videos with regard to motion dynamics. To further encourage vivid motion in the final generated video, Gram matrix is employed to model the motion more precisely. We build a large scale time-lapse dataset, and test our approach on this new dataset. Using our model, we are able to generate realistic videos of up to $128\times 128$ resolution for 32 frames. Quantitative and qualitative experiment results have demonstrated the superiority of our model over the state-of-the-art models.



### Demography-based Facial Retouching Detection using Subclass Supervised Sparse Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1709.07598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07598v1)
- **Published**: 2017-09-22 05:13:48+00:00
- **Updated**: 2017-09-22 05:13:48+00:00
- **Authors**: Aparna Bharati, Mayank Vatsa, Richa Singh, Kevin W. Bowyer, Xin Tong
- **Comment**: Accepted in International Joint Conference on Biometrics, 2017
- **Journal**: None
- **Summary**: Digital retouching of face images is becoming more widespread due to the introduction of software packages that automate the task. Several researchers have introduced algorithms to detect whether a face image is original or retouched. However, previous work on this topic has not considered whether or how accuracy of retouching detection varies with the demography of face images. In this paper, we introduce a new Multi-Demographic Retouched Faces (MDRF) dataset, which contains images belonging to two genders, male and female, and three ethnicities, Indian, Chinese, and Caucasian. Further, retouched images are created using two different retouching software packages. The second major contribution of this research is a novel semi-supervised autoencoder incorporating "subclass" information to improve classification. The proposed approach outperforms existing state-of-the-art detection algorithms for the task of generalized retouching detection. Experiments conducted with multiple combinations of ethnicities show that accuracy of retouching detection can vary greatly based on the demographics of the training and testing images.



### High-Resolution Shape Completion Using Deep Neural Networks for Global Structure and Local Geometry Inference
- **Arxiv ID**: http://arxiv.org/abs/1709.07599v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1709.07599v1)
- **Published**: 2017-09-22 05:16:17+00:00
- **Updated**: 2017-09-22 05:16:17+00:00
- **Authors**: Xiaoguang Han, Zhen Li, Haibin Huang, Evangelos Kalogerakis, Yizhou Yu
- **Comment**: 8 pages paper, 11 pages supplementary material, ICCV spotlight paper
- **Journal**: None
- **Summary**: We propose a data-driven method for recovering miss-ing parts of 3D shapes. Our method is based on a new deep learning architecture consisting of two sub-networks: a global structure inference network and a local geometry refinement network. The global structure inference network incorporates a long short-term memorized context fusion module (LSTM-CF) that infers the global structure of the shape based on multi-view depth information provided as part of the input. It also includes a 3D fully convolutional (3DFCN) module that further enriches the global structure representation according to volumetric information in the input. Under the guidance of the global structure network, the local geometry refinement network takes as input lo-cal 3D patches around missing regions, and progressively produces a high-resolution, complete surface through a volumetric encoder-decoder architecture. Our method jointly trains the global structure inference and local geometry refinement networks in an end-to-end manner. We perform qualitative and quantitative evaluations on six object categories, demonstrating that our method outperforms existing state-of-the-art work on shape completion.



### EraseReLU: A Simple Way to Ease the Training of Deep Convolution Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.07634v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07634v2)
- **Published**: 2017-09-22 08:31:45+00:00
- **Updated**: 2017-11-17 08:43:42+00:00
- **Authors**: Xuanyi Dong, Guoliang Kang, Kun Zhan, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: For most state-of-the-art architectures, Rectified Linear Unit (ReLU) becomes a standard component accompanied with each layer. Although ReLU can ease the network training to an extent, the character of blocking negative values may suppress the propagation of useful information and leads to the difficulty of optimizing very deep Convolutional Neural Networks (CNNs). Moreover, stacking layers with nonlinear activations is hard to approximate the intrinsic linear transformations between feature representations.   In this paper, we investigate the effect of erasing ReLUs of certain layers and apply it to various representative architectures following deterministic rules. It can ease the optimization and improve the generalization performance for very deep CNN models. We find two key factors being essential to the performance improvement: 1) the location where ReLU should be erased inside the basic module; 2) the proportion of basic modules to erase ReLU; We show that erasing the last ReLU layer of all basic modules in a network usually yields improved performance. In experiments, our approach successfully improves the performance of various representative architectures, and we report the improved results on SVHN, CIFAR-10/100, and ImageNet. Moreover, we achieve competitive single-model performance on CIFAR-100 with 16.53% error rate compared to state-of-the-art.



### SwGridNet: A Deep Convolutional Neural Network based on Grid Topology for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1709.07646v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07646v3)
- **Published**: 2017-09-22 09:24:08+00:00
- **Updated**: 2017-10-03 04:43:34+00:00
- **Authors**: Atsushi Takeda
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) achieve remarkable performance on image classification tasks. Recent studies, however, have demonstrated that generalization abilities are more important than the depth of neural networks for improving performance on image classification tasks. Herein, a new neural network called SwGridNet is proposed. A SwGridNet includes many convolutional processing units which connect mutually as a grid network where many processing paths exist between input and output. A SwGridNet has high generalization capability because the multipath architecture has the same effect of ensemble learning. As described in this paper, details of the SwGridNet network architecture are presented. Experimentally obtained results presented in this paper show that SwGridNets respectively achieve test error rates of 2.95% and 15.67% in a CIFAR-10 and CIFAR-100 classification tasks. The results indicate that the SwGridNet performance approximates that of state-of-the-art deep CNNs.



### Semantic Segmentation from Limited Training Data
- **Arxiv ID**: http://arxiv.org/abs/1709.07665v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.07665v1)
- **Published**: 2017-09-22 09:55:18+00:00
- **Updated**: 2017-09-22 09:55:18+00:00
- **Authors**: A. Milan, T. Pham, K. Vijay, D. Morrison, A. W. Tow, L. Liu, J. Erskine, R. Grinover, A. Gurman, T. Hunn, N. Kelly-Boxall, D. Lee, M. McTaggart, G. Rallos, A. Razjigaev, T. Rowntree, T. Shen, R. Smith, S. Wade-McCue, Z. Zhuang, C. Lehnert, G. Lin, I. Reid, P. Corke, J. Leitner
- **Comment**: None
- **Journal**: None
- **Summary**: We present our approach for robotic perception in cluttered scenes that led to winning the recent Amazon Robotics Challenge (ARC) 2017. Next to small objects with shiny and transparent surfaces, the biggest challenge of the 2017 competition was the introduction of unseen categories. In contrast to traditional approaches which require large collections of annotated data and many hours of training, the task here was to obtain a robust perception pipeline with only few minutes of data acquisition and training time. To that end, we present two strategies that we explored. One is a deep metric learning approach that works in three separate steps: semantic-agnostic boundary detection, patch classification and pixel-wise voting. The other is a fully-supervised semantic segmentation approach with efficient dataset collection. We conduct an extensive analysis of the two methods on our ARC 2017 dataset. Interestingly, only few examples of each class are sufficient to fine-tune even very deep convolutional neural networks for this specific task.



### STAR: Spatio-Temporal Altimeter Waveform Retracking using Sparse Representation and Conditional Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1709.07681v1
- **DOI**: 10.1016/j.rse.2017.07.024
- **Categories**: **physics.ao-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.07681v1)
- **Published**: 2017-09-22 10:32:35+00:00
- **Updated**: 2017-09-22 10:32:35+00:00
- **Authors**: Ribana Roscher, Bernd Uebbing, Jürgen Kusche
- **Comment**: None
- **Journal**: Remote Sensing of Environment, Vol. 201, pages 148-164, 2017
- **Summary**: Satellite radar altimetry is one of the most powerful techniques for measuring sea surface height variations, with applications ranging from operational oceanography to climate research. Over open oceans, altimeter return waveforms generally correspond to the Brown model, and by inversion, estimated shape parameters provide mean surface height and wind speed. However, in coastal areas or over inland waters, the waveform shape is often distorted by land influence, resulting in peaks or fast decaying trailing edges. As a result, derived sea surface heights are then less accurate and waveforms need to be reprocessed by sophisticated algorithms. To this end, this work suggests a novel Spatio-Temporal Altimetry Retracking (STAR) technique. We show that STAR enables the derivation of sea surface heights over the open ocean as well as over coastal regions of at least the same quality as compared to existing retracking methods, but for a larger number of cycles and thus retaining more useful data. Novel elements of our method are (a) integrating information from spatially and temporally neighboring waveforms through a conditional random field approach, (b) sub-waveform detection, where relevant sub-waveforms are separated from corrupted or non-relevant parts through a sparse representation approach, and (c) identifying the final best set of sea surfaces heights from multiple likely heights using Dijkstra's algorithm. We apply STAR to data from the Jason-1, Jason-2 and Envisat missions for study sites in the Gulf of Trieste, Italy and in the coastal region of the Ganges-Brahmaputra-Meghna estuary, Bangladesh. We compare to several established and recent retracking methods, as well as to tide gauge data. Our experiments suggest that the obtained sea surface heights are significantly less affected by outliers when compared to results obtained by other approaches.



### Real-time 3D Shape Instantiation from Single Fluoroscopy Projection for Fenestrated Stent Graft Deployment
- **Arxiv ID**: http://arxiv.org/abs/1709.07689v2
- **DOI**: 10.1109/LRA.2018.2798286
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.07689v2)
- **Published**: 2017-09-22 11:00:11+00:00
- **Updated**: 2018-01-08 21:06:40+00:00
- **Authors**: Xiao-Yun Zhou, Jianyu Lin, Celia Riga, Guang-Zhong Yang, Su-Lin Lee
- **Comment**: 7 pages, 10 figures
- **Journal**: IEEE Robotics and Automation Letters 2018
- **Summary**: Robot-assisted deployment of fenestrated stent grafts in Fenestrated Endovascular Aortic Repair (FEVAR) requires accurate geometrical alignment. Currently, this process is guided by 2D fluoroscopy, which is uninformative and error prone. In this paper, a real-time framework is proposed to instantiate the 3D shape of a fenestrated stent graft based on only a single low-dose 2D fluoroscopic image. Firstly, the fenestrated stent graft was placed with markers. Secondly, the 3D pose of each stent segment was instantiated by the RPnP (Robust Perspective-n-Point) method. Thirdly, the 3D shape of the whole stent graft was instantiated via graft gap interpolation. Focal-Unet was proposed to segment the markers from 2D fluoroscopic images to achieve semi-automatic marker detection. The proposed framework was validated on five patient-specific 3D printed phantoms of aortic aneurysms and three stent grafts with new marker placements, showing an average distance error of 1-3mm and an average angle error of 4 degree.



### Can We Boost the Power of the Viola-Jones Face Detector Using Pre-processing? An Empirical Study
- **Arxiv ID**: http://arxiv.org/abs/1709.07720v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07720v3)
- **Published**: 2017-09-22 12:44:01+00:00
- **Updated**: 2017-12-11 04:50:52+00:00
- **Authors**: Mahmoud Afifi, Marwa Nasser, Mostafa Korashy, Katherine Rohde, Aly Abdelrahim
- **Comment**: 14 pages, 10 figures, 8 tables
- **Journal**: None
- **Summary**: The Viola-Jones face detection algorithm was (and still is) a quite popular face detector. In spite of the numerous face detection techniques that have been recently presented, there are many research works that are still based on the Viola-Jones algorithm because of its simplicity. In this paper, we study the influence of a set of blind pre-processing methods on the face detection rate using the Viola-Jones algorithm. We focus on two aspects of improvement, specifically badly illuminated faces and blurred faces. Many methods for lighting invariant and deblurring are used in order to improve the detection accuracy. We want to avoid using blind pre-processing methods that may obstruct the face detector. To that end, we perform two sets of experiments. The first set is performed to avoid any blind pre-processing method that may hurt the face detector. The second set is performed to study the effect of the selected pre-processing methods on images that suffer from hard conditions. We present two manners of applying the pre-processing method to the image prior to being used by the Viola-Jones face detector. Four different datasets are used to draw a coherent conclusion about the potential improvement caused by using prior enhanced images. The results demonstrate that some of the pre-processing methods may hurt the accuracy of Viola-Jones face detection algorithm. However, other pre-processing methods have an evident positive impact on the accuracy of the face detector. Overall, we recommend three simple and fast blind photometric normalization methods as a pre-processing step in order to improve the accuracy of the pre-trained Viola-Jones face detector.



### Single-pixel imaging with Morlet wavelet correlated random patterns
- **Arxiv ID**: http://arxiv.org/abs/1709.07739v2
- **DOI**: 10.1038/s41598-017-18968-6
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.07739v2)
- **Published**: 2017-09-22 13:32:49+00:00
- **Updated**: 2017-10-05 07:56:22+00:00
- **Authors**: Krzysztof M. Czajkowski, Anna Pastuszczak, Rafał Kotyński
- **Comment**: None
- **Journal**: Sci. Rep. 8, 466 (2018)
- **Summary**: Single-pixel imaging is an indirect imaging technique which utilizes simplified optical hardware and advanced computational methods. It offers novel solutions for hyper-spectral imaging, polarimetric imaging, three-dimensional imaging, holographic imaging, optical encryption and imaging through scattering media. The main limitations for its use come from relatively high measurement and reconstruction times. In this paper we propose to reduce the required signal acquisition time by using a novel sampling scheme based on a random selection of Morlet wavelets convolved with white noise. While such functions exhibit random properties, they are locally determined by Morlet wavelet parameters. The proposed method is equivalent to random sampling of the properly selected part of the feature space, which maps the measured images accurately both in the spatial and spatial frequency domains. We compare both numerically and experimentally the image quality obtained with our sampling protocol against widely-used sampling with Walsh-Hadamard or noiselet functions. The results show considerable improvement over the former methods, enabling single-pixel imaging at low compression rates on the order of a few percent.



### Tropical Land Use Land Cover Mapping in Pará (Brazil) using Discriminative Markov Random Fields and Multi-temporal TerraSAR-X Data
- **Arxiv ID**: http://arxiv.org/abs/1709.07794v1
- **DOI**: 10.1016/j.jag.2017.07.019
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07794v1)
- **Published**: 2017-09-22 14:53:56+00:00
- **Updated**: 2017-09-22 14:53:56+00:00
- **Authors**: Ron Hagensieker, Ribana Roscher, Johannes Rosentreter, Benjamin Jakimow, Björn Waske
- **Comment**: None
- **Journal**: International Journal of Applied Earth Observation and
  Geoinformation, Volume 63, December 2017, Pages 244-256
- **Summary**: Remote sensing satellite data offer the unique possibility to map land use land cover transformations by providing spatially explicit information. However, detection of short-term processes and land use patterns of high spatial-temporal variability is a challenging task. We present a novel framework using multi-temporal TerraSAR-X data and machine learning techniques, namely Discriminative Markov Random Fields with spatio-temporal priors, and Import Vector Machines, in order to advance the mapping of land cover characterized by short-term changes. Our study region covers a current deforestation frontier in the Brazilian state Par\'{a} with land cover dominated by primary forests, different types of pasture land and secondary vegetation, and land use dominated by short-term processes such as slash-and-burn activities. The data set comprises multi-temporal TerraSAR-X imagery acquired over the course of the 2014 dry season, as well as optical data (RapidEye, Landsat) for reference. Results show that land use land cover is reliably mapped, resulting in spatially adjusted overall accuracies of up to $79\%$ in a five class setting, yet limitations for the differentiation of different pasture types remain. The proposed method is applicable on multi-temporal data sets, and constitutes a feasible approach to map land use land cover in regions that are affected by high-frequent temporal changes.



### Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping
- **Arxiv ID**: http://arxiv.org/abs/1709.07857v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.07857v2)
- **Published**: 2017-09-22 17:23:12+00:00
- **Updated**: 2017-09-25 21:35:45+00:00
- **Authors**: Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, Sergey Levine, Vincent Vanhoucke
- **Comment**: 9 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times, using only randomly generated simulated objects. We also show that by using only unlabeled real-world data and our GraspGAN methodology, we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples.



### FiLM: Visual Reasoning with a General Conditioning Layer
- **Arxiv ID**: http://arxiv.org/abs/1709.07871v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1709.07871v2)
- **Published**: 2017-09-22 17:54:12+00:00
- **Updated**: 2017-12-18 21:25:53+00:00
- **Authors**: Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, Aaron Courville
- **Comment**: AAAI 2018. Code available at http://github.com/ethanjperez/film .
  Extends arXiv:1707.03017
- **Journal**: None
- **Summary**: We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.



### On Encoding Temporal Evolution for Real-time Action Prediction
- **Arxiv ID**: http://arxiv.org/abs/1709.07894v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07894v3)
- **Published**: 2017-09-22 18:05:50+00:00
- **Updated**: 2018-02-08 00:07:15+00:00
- **Authors**: Fahimeh Rezazadegan, Sareh Shirazi, Mahsa Baktashmotlagh, Larry S. Davis
- **Comment**: Submitted Version
- **Journal**: None
- **Summary**: Anticipating future actions is a key component of intelligence, specifically when it applies to real-time systems, such as robots or autonomous cars. While recent works have addressed prediction of raw RGB pixel values, we focus on anticipating the motion evolution in future video frames. To this end, we construct dynamic images (DIs) by summarising moving pixels through a sequence of future frames. We train a convolutional LSTMs to predict the next DIs based on an unsupervised learning process, and then recognise the activity associated with the predicted DI. We demonstrate the effectiveness of our approach on 3 benchmark action datasets showing that despite running on videos with complex activities, our approach is able to anticipate the next human action with high accuracy and obtain better results than the state-of-the-art methods.



### Context Embedding Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.01691v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.01691v3)
- **Published**: 2017-09-22 18:46:40+00:00
- **Updated**: 2018-03-29 16:32:35+00:00
- **Authors**: Kun Ho Kim, Oisin Mac Aodha, Pietro Perona
- **Comment**: CVPR 2018 spotlight
- **Journal**: None
- **Summary**: Low dimensional embeddings that capture the main variations of interest in collections of data are important for many applications. One way to construct these embeddings is to acquire estimates of similarity from the crowd. However, similarity is a multi-dimensional concept that varies from individual to individual. Existing models for learning embeddings from the crowd typically make simplifying assumptions such as all individuals estimate similarity using the same criteria, the list of criteria is known in advance, or that the crowd workers are not influenced by the data that they see. To overcome these limitations we introduce Context Embedding Networks (CENs). In addition to learning interpretable embeddings from images, CENs also model worker biases for different attributes along with the visual context i.e. the visual attributes highlighted by a set of images. Experiments on two noisy crowd annotated datasets show that modeling both worker bias and visual context results in more interpretable embeddings compared to existing approaches.



### Modeling Image Virality with Pairwise Spatial Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.07914v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1709.07914v1)
- **Published**: 2017-09-22 19:17:15+00:00
- **Updated**: 2017-09-22 19:17:15+00:00
- **Authors**: Abhimanyu Dubey, Sumeet Agarwal
- **Comment**: 9 pages, Accepted as a full paper at the ACM Multimedia Conference
  (MM) 2017
- **Journal**: None
- **Summary**: The study of virality and information diffusion online is a topic gaining traction rapidly in the computational social sciences. Computer vision and social network analysis research have also focused on understanding the impact of content and information diffusion in making content viral, with prior approaches not performing significantly well as other traditional classification tasks. In this paper, we present a novel pairwise reformulation of the virality prediction problem as an attribute prediction task and develop a novel algorithm to model image virality on online media using a pairwise neural network. Our model provides significant insights into the features that are responsible for promoting virality and surpasses the existing state-of-the-art by a 12% average improvement in prediction. We also investigate the effect of external category supervision on relative attribute prediction and observe an increase in prediction accuracy for the same across several attribute learning datasets.



### MR Acquisition-Invariant Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1709.07944v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1709.07944v2)
- **Published**: 2017-09-22 20:52:14+00:00
- **Updated**: 2018-04-19 22:31:50+00:00
- **Authors**: Wouter M. Kouw, Marco Loog, Lambertus W. Bartels, Adriënne M. Mendrik
- **Comment**: 36 pages, 2 appendices, 12 figures, 3 tables
- **Journal**: None
- **Summary**: Voxelwise classification approaches are popular and effective methods for tissue quantification in brain magnetic resonance imaging (MRI) scans. However, generalization of these approaches is hampered by large differences between sets of MRI scans such as differences in field strength, vendor or acquisition protocols. Due to this acquisition related variation, classifiers trained on data from a specific scanner fail or under-perform when applied to data that was acquired differently. In order to address this lack of generalization, we propose a Siamese neural network (MRAI-net) to learn a representation that minimizes the between-scanner variation, while maintaining the contrast between brain tissues necessary for brain tissue quantification. The proposed MRAI-net was evaluated on both simulated and real MRI data. After learning the MR acquisition invariant representation, any supervised classification model that uses feature vectors can be applied. In this paper, we provide a proof of principle, which shows that a linear classifier applied on the MRAI representation is able to outperform supervised convolutional neural network classifiers for tissue classification when little target training data is available.



### Elliptification of Rectangular Imagery
- **Arxiv ID**: http://arxiv.org/abs/1709.07875v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.07875v4)
- **Published**: 2017-09-22 21:56:11+00:00
- **Updated**: 2019-11-12 04:01:22+00:00
- **Authors**: Chamberlain Fong
- **Comment**: JMM2019 SIGMAA-ARTS; corrected typo on detJ surfaces; added
  description on bijective spherize filter
- **Journal**: None
- **Summary**: We present and discuss different algorithms for converting rectangular imagery into elliptical regions. We mainly focus on methods that use mathematical mappings with explicit and invertible equations. The key idea is to start with invertible mappings between the square and the circular disc then extend it to handle rectangles and ellipses. This extension can be done by simply removing the eccentricity and reintroducing it back after using a chosen square-to-disc mapping.



