# Arxiv Papers in cs.CV on 2017-09-17
### The Cafe Wall Illusion: Local and Global Perception from multiple scale to multiscale
- **Arxiv ID**: http://arxiv.org/abs/1710.01215v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01215v1)
- **Published**: 2017-09-17 10:12:06+00:00
- **Updated**: 2017-09-17 10:12:06+00:00
- **Authors**: Nasim Nematzadeh, David M. W. Powers
- **Comment**: Under revision by Applied Computational Intelligence and Soft
  Computing
- **Journal**: None
- **Summary**: Geometrical illusions are a subclass of optical illusions in which the geometrical characteristics of patterns such as orientations and angles are distorted and misperceived as the result of low- to high-level retinal/cortical processing. Modelling the detection of tilt in these illusions and their strengths as they are perceived is a challenging task computationally and leads to development of techniques that match with human performance. In this study, we present a predictive and quantitative approach for modeling foveal and peripheral vision in the induced tilt in Caf\'e Wall illusion in which parallel mortar lines between shifted rows of black and white tiles appear to converge and diverge. A bioderived filtering model for the responses of retinal/cortical simple cells to the stimulus using Difference of Gaussians is utilized with an analytic processing pipeline introduced in our previous studies to quantify the angle of tilt in the model. Here we have considered visual characteristics of foveal and peripheral vision in the perceived tilt in the pattern to predict different degrees of tilt in different areas of the fovea and periphery as the eye saccades to different parts of the image. The tilt analysis results from several sampling sizes and aspect ratios, modelling variant foveal views are used from our previous investigations on the local tilt, and we specifically investigate in this work, different configurations of the whole pattern modelling variant Gestalt views across multiple scales in order to provide confidence intervals around the predicted tilts. The foveal sample sets are verified and quantified using two different sampling methods. We present here a precise and quantified comparison contrasting local tilt detection in the foveal sets with a global average across all of the Caf\'e Wall configurations tested in this work.



### Automatic Tool Landmark Detection for Stereo Vision in Robot-Assisted Retinal Surgery
- **Arxiv ID**: http://arxiv.org/abs/1709.05665v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.05665v2)
- **Published**: 2017-09-17 14:00:26+00:00
- **Updated**: 2017-11-20 17:59:08+00:00
- **Authors**: Thomas Probst, Kevis-Kokitsi Maninis, Ajad Chhatkuli, Mouloud Ourak, Emmanuel Vander Poorten, Luc Van Gool
- **Comment**: Accepted in Robotics and Automation Letters (RA-L). Project page:
  http://www.vision.ee.ethz.ch/~kmaninis/keypoints2stereo/index.html
- **Journal**: None
- **Summary**: Computer vision and robotics are being increasingly applied in medical interventions. Especially in interventions where extreme precision is required they could make a difference. One such application is robot-assisted retinal microsurgery. In recent works, such interventions are conducted under a stereo-microscope, and with a robot-controlled surgical tool. The complementarity of computer vision and robotics has however not yet been fully exploited. In order to improve the robot control we are interested in 3D reconstruction of the anatomy and in automatic tool localization using a stereo microscope. In this paper, we solve this problem for the first time using a single pipeline, starting from uncalibrated cameras to reach metric 3D reconstruction and registration, in retinal microsurgery. The key ingredients of our method are: (a) surgical tool landmark detection, and (b) 3D reconstruction with the stereo microscope, using the detected landmarks. To address the former, we propose a novel deep learning method that detects and recognizes keypoints in high definition images at higher than real-time speed. We use the detected 2D keypoints along with their corresponding 3D coordinates obtained from the robot sensors to calibrate the stereo microscope using an affine projection model. We design an online 3D reconstruction pipeline that makes use of smoothness constraints and performs robot-to-camera registration. The entire pipeline is extensively validated on open-sky porcine eye sequences. Quantitative and qualitative results are presented for all steps.



### An Improved Fatigue Detection System Based on Behavioral Characteristics of Driver
- **Arxiv ID**: http://arxiv.org/abs/1709.05669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05669v1)
- **Published**: 2017-09-17 14:28:21+00:00
- **Updated**: 2017-09-17 14:28:21+00:00
- **Authors**: Rajat Gupta, Kanishk Aman, Nalin Shiva, Yadvendra Singh
- **Comment**: 4 pages, 2 figures, edited version of published paper in IEEE ICITE
  2017
- **Journal**: 2017 2nd IEEE International Conference on Intelligent
  Transportation Engineering, pp 227-230
- **Summary**: In recent years, road accidents have increased significantly. One of the major reasons for these accidents, as reported is driver fatigue. Due to continuous and longtime driving, the driver gets exhausted and drowsy which may lead to an accident. Therefore, there is a need for a system to measure the fatigue level of driver and alert him when he/she feels drowsy to avoid accidents. Thus, we propose a system which comprises of a camera installed on the car dashboard. The camera detect the driver's face and observe the alteration in its facial features and uses these features to observe the fatigue level. Facial features include eyes and mouth. Principle Component Analysis is thus implemented to reduce the features while minimizing the amount of information lost. The parameters thus obtained are processed through Support Vector Classifier for classifying the fatigue level. After that classifier output is sent to the alert unit.



### Neural Affine Grayscale Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1709.05672v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.05672v1)
- **Published**: 2017-09-17 14:44:07+00:00
- **Updated**: 2017-09-17 14:44:07+00:00
- **Authors**: Sungmin Cha, Taesup Moon
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new grayscale image denoiser, dubbed as Neural Affine Image Denoiser (Neural AIDE), which utilizes neural network in a novel way. Unlike other neural network based image denoising methods, which typically apply simple supervised learning to learn a mapping from a noisy patch to a clean patch, we formulate to train a neural network to learn an \emph{affine} mapping that gets applied to a noisy pixel, based on its context. Our formulation enables both supervised training of the network from the labeled training dataset and adaptive fine-tuning of the network parameters using the given noisy image subject to denoising. The key tool for devising Neural AIDE is to devise an estimated loss function of the MSE of the affine mapping, solely based on the noisy data. As a result, our algorithm can outperform most of the recent state-of-the-art methods in the standard benchmark datasets. Moreover, our fine-tuning method can nicely overcome one of the drawbacks of the patch-level supervised learning methods in image denoising; namely, a supervised trained model with a mismatched noise variance can be mostly corrected as long as we have the matched noise variance during the fine-tuning step.



### Organizing Multimedia Data in Video Surveillance Systems Based on Face Verification with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.05675v1
- **DOI**: 10.1007/978-3-319-73013-4_20
- **Categories**: **cs.CV**, 68T10, 68T45, I.4.8; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1709.05675v1)
- **Published**: 2017-09-17 14:57:55+00:00
- **Updated**: 2017-09-17 14:57:55+00:00
- **Authors**: Anastasiia D. Sokolova, Angelina S. Kharchevnikova, Andrey V. Savchenko
- **Comment**: 8 pages; 1 figure, accepted for publication at AIST17
- **Journal**: Proceedings of the International Conference on Analysis of Images,
  Social Networks and Texts (AIST), 2018, pp. 223-230
- **Summary**: In this paper we propose the two-stage approach of organizing information in video surveillance systems. At first, the faces are detected in each frame and a video stream is split into sequences of frames with face region of one person. Secondly, these sequences (tracks) that contain identical faces are grouped using face verification algorithms and hierarchical agglomerative clustering. Gender and age are estimated for each cluster (person) in order to facilitate the usage of the organized video collection. The particular attention is focused on the aggregation of features extracted from each frame with the deep convolutional neural networks. The experimental results of the proposed approach using YTF and IJB-A datasets demonstrated that the most accurate and fast solution is achieved for matching of normalized average of feature vectors of all frames in a track.



### Group Affect Prediction Using Multimodal Distributions
- **Arxiv ID**: http://arxiv.org/abs/1710.01216v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01216v2)
- **Published**: 2017-09-17 19:04:39+00:00
- **Updated**: 2018-03-12 16:14:36+00:00
- **Authors**: Saqib Shamsi, Bhanu Pratap Singh Rawat, Manya Wadhwa
- **Comment**: This research paper has been accepted at Workshop on Computer Vision
  for Active and Assisted Living, WACV 2018
- **Journal**: None
- **Summary**: We describe our approach towards building an efficient predictive model to detect emotions for a group of people in an image. We have proposed that training a Convolutional Neural Network (CNN) model on the emotion heatmaps extracted from the image, outperforms a CNN model trained entirely on the raw images. The comparison of the models have been done on a recently published dataset of Emotion Recognition in the Wild (EmotiW) challenge, 2017. The proposed method achieved validation accuracy of 55.23% which is 2.44% above the baseline accuracy, provided by the EmotiW organizers.



