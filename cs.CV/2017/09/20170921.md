# Arxiv Papers in cs.CV on 2017-09-21
### SceneCut: Joint Geometric and Object Segmentation for Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/1709.07158v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.07158v2)
- **Published**: 2017-09-21 05:08:35+00:00
- **Updated**: 2018-05-24 06:44:56+00:00
- **Authors**: Trung Pham, Thanh-Toan Do, Niko Sünderhauf, Ian Reid
- **Comment**: Published in ICRA 2018
- **Journal**: ICRA 2018
- **Summary**: This paper presents SceneCut, a novel approach to jointly discover previously unseen objects and non-object surfaces using a single RGB-D image. SceneCut's joint reasoning over scene semantics and geometry allows a robot to detect and segment object instances in complex scenes where modern deep learning-based methods either fail to separate object instances, or fail to detect objects that were not seen during training. SceneCut automatically decomposes a scene into meaningful regions which either represent objects or scene surfaces. The decomposition is qualified by an unified energy function over objectness and geometric fitting. We show how this energy function can be optimized efficiently by utilizing hierarchical segmentation trees. Moreover, we leverage a pre-trained convolutional oriented boundary network to predict accurate boundaries from images, which are used to construct high-quality region hierarchies. We evaluate SceneCut on several different indoor environments, and the results show that SceneCut significantly outperforms all the existing methods.



### Drought Stress Classification using 3D Plant Models
- **Arxiv ID**: http://arxiv.org/abs/1709.09496v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09496v2)
- **Published**: 2017-09-21 05:20:13+00:00
- **Updated**: 2017-10-18 06:09:47+00:00
- **Authors**: Siddharth Srivastava, Swati Bhugra, Brejesh Lall, Santanu Chaudhury
- **Comment**: Appears in Workshop on Computer Vision Problems in Plant Phenotyping
  (CVPPP), International Conference on Computer Vision (ICCV) 2017
- **Journal**: None
- **Summary**: Quantification of physiological changes in plants can capture different drought mechanisms and assist in selection of tolerant varieties in a high throughput manner. In this context, an accurate 3D model of plant canopy provides a reliable representation for drought stress characterization in contrast to using 2D images. In this paper, we propose a novel end-to-end pipeline including 3D reconstruction, segmentation and feature extraction, leveraging deep neural networks at various stages, for drought stress study. To overcome the high degree of self-similarities and self-occlusions in plant canopy, prior knowledge of leaf shape based on features from deep siamese network are used to construct an accurate 3D model using structure from motion on wheat plants. The drought stress is characterized with a deep network based feature aggregation. We compare the proposed methodology on several descriptors, and show that the network outperforms conventional methods.



### Semi-Automated Nasal PAP Mask Sizing using Facial Photographs
- **Arxiv ID**: http://arxiv.org/abs/1709.07166v1
- **DOI**: 10.1109/EMBC.2017.8037049
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07166v1)
- **Published**: 2017-09-21 06:08:58+00:00
- **Updated**: 2017-09-21 06:08:58+00:00
- **Authors**: Benjamin Johnston, Alistair McEwan, Philip de Chazal
- **Comment**: 4 pages, 3 figures, 4 tables, IEEE Engineering Medicine and Biology
  Conference 2017
- **Journal**: None
- **Summary**: We present a semi-automated system for sizing nasal Positive Airway Pressure (PAP) masks based upon a neural network model that was trained with facial photographs of both PAP mask users and non-users. It demonstrated an accuracy of 72% in correctly sizing a mask and 96% accuracy sizing to within 1 mask size group. The semi-automated system performed comparably to sizing from manual measurements taken from the same images which produced 89% and 100% accuracy respectively.



### Visual Question Generation as Dual Task of Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1709.07192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07192v1)
- **Published**: 2017-09-21 08:04:48+00:00
- **Updated**: 2017-09-21 08:04:48+00:00
- **Authors**: Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Recently visual question answering (VQA) and visual question generation (VQG) are two trending topics in the computer vision, which have been explored separately. In this work, we propose an end-to-end unified framework, the Invertible Question Answering Network (iQAN), to leverage the complementary relations between questions and answers in images by jointly training the model on VQA and VQG tasks. Corresponding parameter sharing scheme and regular terms are proposed as constraints to explicitly leverage Q,A's dependencies to guide the training process. After training, iQAN can take either question or answer as input, then output the counterpart. Evaluated on the large-scale visual question answering datasets CLEVR and VQA2, our iQAN improves the VQA accuracy over the baselines. We also show the dual learning framework of iQAN can be generalized to other VQA architectures and consistently improve the results over both the VQA and VQG tasks.



### Temporal Multimodal Fusion for Video Emotion Classification in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1709.07200v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1709.07200v1)
- **Published**: 2017-09-21 08:14:40+00:00
- **Updated**: 2017-09-21 08:14:40+00:00
- **Authors**: Valentin Vielzeuf, Stéphane Pateux, Frédéric Jurie
- **Comment**: None
- **Journal**: ACM - ICMI 2017, Nov 2017, Glasgow, United Kingdom
- **Summary**: This paper addresses the question of emotion classification. The task consists in predicting emotion labels (taken among a set of possible labels) best describing the emotions contained in short video clips. Building on a standard framework -- lying in describing videos by audio and visual features used by a supervised classifier to infer the labels -- this paper investigates several novel directions. First of all, improved face descriptors based on 2D and 3D Convo-lutional Neural Networks are proposed. Second, the paper explores several fusion methods, temporal and multimodal, including a novel hierarchical method combining features and scores. In addition, we carefully reviewed the different stages of the pipeline and designed a CNN architecture adapted to the task; this is important as the size of the training set is small compared to the difficulty of the problem, making generalization difficult. The so-obtained model ranked 4th at the 2017 Emotion in the Wild challenge with the accuracy of 58.8 %.



### A First Derivative Potts Model for Segmentation and Denoising Using ILP
- **Arxiv ID**: http://arxiv.org/abs/1709.07212v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07212v2)
- **Published**: 2017-09-21 08:42:00+00:00
- **Updated**: 2018-01-10 15:52:59+00:00
- **Authors**: Ruobing Shen, Gerhard Reinelt, Stéphane Canu
- **Comment**: 6 pages, 2 figures. To appear at Proceedings of International
  Conference on Operations Research 2017, Berlin
- **Journal**: None
- **Summary**: Unsupervised image segmentation and denoising are two fundamental tasks in image processing. Usually, graph based models such as multicut are used for segmentation and variational models are employed for denoising. Our approach addresses both problems at the same time. We propose a novel ILP formulation of the first derivative Potts model with the $\ell_1$ data term, where binary variables are introduced to deal with the $\ell_0$ norm of the regularization term. The ILP is then solved by a standard off-the-shelf MIP solver. Numerical experiments are compared with the multicut problem.



### Human Pose Estimation using Global and Local Normalization
- **Arxiv ID**: http://arxiv.org/abs/1709.07220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07220v1)
- **Published**: 2017-09-21 09:13:31+00:00
- **Updated**: 2017-09-21 09:13:31+00:00
- **Authors**: Ke Sun, Cuiling Lan, Junliang Xing, Wenjun Zeng, Dong Liu, Jingdong Wang
- **Comment**: ICCV2017
- **Journal**: None
- **Summary**: In this paper, we address the problem of estimating the positions of human joints, i.e., articulated pose estimation. Recent state-of-the-art solutions model two key issues, joint detection and spatial configuration refinement, together using convolutional neural networks. Our work mainly focuses on spatial configuration refinement by reducing variations of human poses statistically, which is motivated by the observation that the scattered distribution of the relative locations of joints e.g., the left wrist is distributed nearly uniformly in a circular area around the left shoulder) makes the learning of convolutional spatial models hard. We present a two-stage normalization scheme, human body normalization and limb normalization, to make the distribution of the relative joint locations compact, resulting in easier learning of convolutional spatial models and more accurate pose estimation. In addition, our empirical results show that incorporating multi-scale supervision and multi-scale fusion into the joint detection network is beneficial. Experiment results demonstrate that our method consistently outperforms state-of-the-art methods on the benchmarks.



### Convolutional neural networks that teach microscopes how to image
- **Arxiv ID**: http://arxiv.org/abs/1709.07223v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1709.07223v1)
- **Published**: 2017-09-21 09:17:47+00:00
- **Updated**: 2017-09-21 09:17:47+00:00
- **Authors**: Roarke Horstmeyer, Richard Y. Chen, Barbara Kappes, Benjamin Judkewitz
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Deep learning algorithms offer a powerful means to automatically analyze the content of medical images. However, many biological samples of interest are primarily transparent to visible light and contain features that are difficult to resolve with a standard optical microscope. Here, we use a convolutional neural network (CNN) not only to classify images, but also to optimize the physical layout of the imaging device itself. We increase the classification accuracy of a microscope's recorded images by merging an optical model of image formation into the pipeline of a CNN. The resulting network simultaneously determines an ideal illumination arrangement to highlight important sample features during image acquisition, along with a set of convolutional weights to classify the detected images post-capture. We demonstrate our joint optimization technique with an experimental microscope configuration that automatically identifies malaria-infected cells with 5-10% higher accuracy than standard and alternative microscope lighting designs.



### Neural network identification of people hidden from view with a single-pixel, single-photon detector
- **Arxiv ID**: http://arxiv.org/abs/1709.07244v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1709.07244v1)
- **Published**: 2017-09-21 10:12:06+00:00
- **Updated**: 2017-09-21 10:12:06+00:00
- **Authors**: Piergiorgio Caramazza, Alessandro Boccolini, Daniel Buschek, Matthias Hullin, Catherine Higham, Robert Henderson, Roderick Murray-Smith, Daniele Faccio
- **Comment**: None
- **Journal**: None
- **Summary**: Light scattered from multiple surfaces can be used to retrieve information of hidden environments. However, full three-dimensional retrieval of an object hidden from view by a wall has only been achieved with scanning systems and requires intensive computational processing of the retrieved data. Here we use a non-scanning, single-photon single-pixel detector in combination with an artificial neural network: this allows us to locate the position and to also simultaneously provide the actual identity of a hidden person, chosen from a database of people (N=3). Artificial neural networks applied to specific computational imaging problems can therefore enable novel imaging capabilities with hugely simplified hardware and processing times



### Yet Another ADNI Machine Learning Paper? Paving The Way Towards Fully-reproducible Research on Classification of Alzheimer's Disease
- **Arxiv ID**: http://arxiv.org/abs/1709.07267v1
- **DOI**: 10.1007/978-3-319-67389-9_7
- **Categories**: **stat.ML**, cs.CV, q-bio.NC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1709.07267v1)
- **Published**: 2017-09-21 11:37:01+00:00
- **Updated**: 2017-09-21 11:37:01+00:00
- **Authors**: Jorge Samper-González, Ninon Burgos, Sabrina Fontanella, Hugo Bertin, Marie-Odile Habert, Stanley Durrleman, Theodoros Evgeniou, Olivier Colliot
- **Comment**: None
- **Journal**: Proc. Machine Learning in Medical Imaging MLMI 2017, MICCAI
  Worskhop, Lecture Notes in Computer Science, volume 10541, pp 53-60, Springer
- **Summary**: In recent years, the number of papers on Alzheimer's disease classification has increased dramatically, generating interesting methodological ideas on the use machine learning and feature extraction methods. However, practical impact is much more limited and, eventually, one could not tell which of these approaches are the most efficient. While over 90\% of these works make use of ADNI an objective comparison between approaches is impossible due to variations in the subjects included, image pre-processing, performance metrics and cross-validation procedures. In this paper, we propose a framework for reproducible classification experiments using multimodal MRI and PET data from ADNI. The core components are: 1) code to automatically convert the full ADNI database into BIDS format; 2) a modular architecture based on Nipype in order to easily plug-in different classification and feature extraction tools; 3) feature extraction pipelines for MRI and PET data; 4) baseline classification approaches for unimodal and multimodal features. This provides a flexible framework for benchmarking different feature extraction and classification tools in a reproducible manner. We demonstrate its use on all (1519) baseline T1 MR images and all (1102) baseline FDG PET images from ADNI 1, GO and 2 with SPM-based feature extraction pipelines and three different classification techniques (linear SVM, anatomically regularized SVM and multiple kernel learning SVM). The highest accuracies achieved were: 91% for AD vs CN, 83% for MCIc vs CN, 75% for MCIc vs MCInc, 94% for AD-A$\beta$+ vs CN-A$\beta$- and 72% for MCIc-A$\beta$+ vs MCInc-A$\beta$+. The code is publicly available at https://gitlab.icm-institute.org/aramislab/AD-ML (depends on the Clinica software platform, publicly available at http://www.clinica.run).



### Playing for Benchmarks
- **Arxiv ID**: http://arxiv.org/abs/1709.07322v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1709.07322v1)
- **Published**: 2017-09-21 13:44:47+00:00
- **Updated**: 2017-09-21 13:44:47+00:00
- **Authors**: Stephan R. Richter, Zeeshan Hayder, Vladlen Koltun
- **Comment**: Published at the International Conference on Computer Vision (ICCV
  2017)
- **Journal**: None
- **Summary**: We present a benchmark suite for visual perception. The benchmark is based on more than 250K high-resolution video frames, all annotated with ground-truth data for both low-level and high-level vision tasks, including optical flow, semantic instance segmentation, object detection and tracking, object-level 3D scene layout, and visual odometry. Ground-truth data for all tasks is available for every frame. The data was collected while driving, riding, and walking a total of 184 kilometers in diverse ambient conditions in a realistic virtual world. To create the benchmark, we have developed a new approach to collecting ground-truth data from simulated worlds without access to their source code or content. We conduct statistical analyses that show that the composition of the scenes in the benchmark closely matches the composition of corresponding physical environments. The realism of the collected data is further validated via perceptual experiments. We analyze the performance of state-of-the-art methods for multiple tasks, providing reference baselines and highlighting challenges for future research. The supplementary video can be viewed at https://youtu.be/T9OybWv923Y



### AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection
- **Arxiv ID**: http://arxiv.org/abs/1709.07326v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.07326v3)
- **Published**: 2017-09-21 13:48:49+00:00
- **Updated**: 2018-03-04 14:04:29+00:00
- **Authors**: Thanh-Toan Do, Anh Nguyen, Ian Reid
- **Comment**: In ICRA 2018
- **Journal**: None
- **Summary**: We propose AffordanceNet, a new deep learning approach to simultaneously detect multiple objects and their affordances from RGB images. Our AffordanceNet has two branches: an object detection branch to localize and classify the object, and an affordance detection branch to assign each pixel in the object to its most probable affordance label. The proposed framework employs three key components for effectively handling the multiclass problem in the affordance mask: a sequence of deconvolutional layers, a robust resizing strategy, and a multi-task loss function. The experimental results on the public datasets show that our AffordanceNet outperforms recent state-of-the-art methods by a fair margin, while its end-to-end architecture allows the inference at the speed of 150ms per image. This makes our AffordanceNet well suitable for real-time robotic applications. Furthermore, we demonstrate the effectiveness of AffordanceNet in different testing environments and in real robotic applications. The source code is available at https://github.com/nqanh/affordance-net



### H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes
- **Arxiv ID**: http://arxiv.org/abs/1709.07330v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07330v3)
- **Published**: 2017-09-21 13:58:21+00:00
- **Updated**: 2018-07-03 13:00:48+00:00
- **Authors**: Xiaomeng Li, Hao Chen, Xiaojuan Qi, Qi Dou, Chi-Wing Fu, Pheng Ann Heng
- **Comment**: Accept for publication at IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Liver cancer is one of the leading causes of cancer death. To assist doctors in hepatocellular carcinoma diagnosis and treatment planning, an accurate and automatic liver and tumor segmentation method is highly demanded in the clinical practice. Recently, fully convolutional neural networks (FCNs), including 2D and 3D FCNs, serve as the back-bone in many volumetric image segmentation. However, 2D convolutions can not fully leverage the spatial information along the third dimension while 3D convolutions suffer from high computational cost and GPU memory consumption. To address these issues, we propose a novel hybrid densely connected UNet (H-DenseUNet), which consists of a 2D DenseUNet for efficiently extracting intra-slice features and a 3D counterpart for hierarchically aggregating volumetric contexts under the spirit of the auto-context algorithm for liver and tumor segmentation. We formulate the learning process of H-DenseUNet in an end-to-end manner, where the intra-slice representations and inter-slice features can be jointly optimized through a hybrid feature fusion (HFF) layer. We extensively evaluated our method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge and 3DIRCADb Dataset. Our method outperformed other state-of-the-arts on the segmentation results of tumors and achieved very competitive performance for liver segmentation even with a single model.



### Efficient Column Generation for Cell Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1709.07337v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1709.07337v1)
- **Published**: 2017-09-21 14:15:23+00:00
- **Updated**: 2017-09-21 14:15:23+00:00
- **Authors**: Chong Zhang, Shaofei Wang, Miguel A. Gonzalez-Ballester, Julian Yarkony
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of instance segmentation in biological images with crowded and compact cells. We formulate this task as an integer program where variables correspond to cells and constraints enforce that cells do not overlap. To solve this integer program, we propose a column generation formulation where the pricing program is solved via exact optimization of very small scale integer programs. Column generation is tightened using odd set inequalities which fit elegantly into pricing problem optimization. Our column generation approach achieves fast stable anytime inference for our instance segmentation problems. We demonstrate on three distinct light microscopy datasets, with several hundred cells each, that our proposed algorithm rapidly achieves or exceeds state of the art accuracy.



### Class-Splitting Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.07359v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.07359v2)
- **Published**: 2017-09-21 14:55:54+00:00
- **Updated**: 2018-05-17 14:07:35+00:00
- **Authors**: Guillermo L. Grinblat, Lucas C. Uzal, Pablo M. Granitto
- **Comment**: Under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) produce systematically better quality samples when class label information is provided., i.e. in the conditional GAN setup. This is still observed for the recently proposed Wasserstein GAN formulation which stabilized adversarial training and allows considering high capacity network architectures such as ResNet. In this work we show how to boost conditional GAN by augmenting available class labels. The new classes come from clustering in the representation space learned by the same GAN model. The proposed strategy is also feasible when no class information is available, i.e. in the unsupervised setup. Our generated samples reach state-of-the-art Inception scores for CIFAR-10 and STL-10 datasets in both supervised and unsupervised setup.



### Multi-label Pixelwise Classification for Reconstruction of Large-scale Urban Areas
- **Arxiv ID**: http://arxiv.org/abs/1709.07368v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07368v2)
- **Published**: 2017-09-21 15:13:09+00:00
- **Updated**: 2018-01-23 21:32:13+00:00
- **Authors**: Yuanlie He, Sudhir Mudur, Charalambos Poullis
- **Comment**: None
- **Journal**: None
- **Summary**: Object classification is one of the many holy grails in computer vision and as such has resulted in a very large number of algorithms being proposed already. Specifically in recent years there has been considerable progress in this area primarily due to the increased efficiency and accessibility of deep learning techniques. In fact, for single-label object classification [i.e. only one object present in the image] the state-of-the-art techniques employ deep neural networks and are reporting very close to human-like performance. There are specialized applications in which single-label object-level classification will not suffice; for example in cases where the image contains multiple intertwined objects of different labels.   In this paper, we address the complex problem of multi-label pixelwise classification. We present our distinct solution based on a convolutional neural network (CNN) for performing multi-label pixelwise classification and its application to large-scale urban reconstruction. A supervised learning approach is followed for training a 13-layer CNN using both LiDAR and satellite images. An empirical study has been conducted to determine the hyperparameters which result in the optimal performance of the CNN. Scale invariance is introduced by training the network on five different scales of the input and labeled data. This results in six pixelwise classifications for each different scale. An SVM is then trained to map the six pixelwise classifications into a single-label. Lastly, we refine boundary pixel labels using graph-cuts for maximum a-posteriori (MAP) estimation with Markov Random Field (MRF) priors. The resulting pixelwise classification is then used to accurately extract and reconstruct the buildings in large-scale urban areas. The proposed approach has been extensively tested and the results are reported.



### Urban Land Cover Classification with Missing Data Modalities Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.07383v2
- **DOI**: 10.1109/JSTARS.2018.2834961
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07383v2)
- **Published**: 2017-09-21 15:46:27+00:00
- **Updated**: 2018-05-08 20:44:16+00:00
- **Authors**: Michael Kampffmeyer, Arnt-Børre Salberg, Robert Jenssen
- **Comment**: None
- **Journal**: IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing 2018
- **Summary**: Automatic urban land cover classification is a fundamental problem in remote sensing, e.g. for environmental monitoring. The problem is highly challenging, as classes generally have high inter-class and low intra-class variance. Techniques to improve urban land cover classification performance in remote sensing include fusion of data from different sensors with different data modalities. However, such techniques require all modalities to be available to the classifier in the decision-making process, i.e. at test time, as well as in training. If a data modality is missing at test time, current state-of-the-art approaches have in general no procedure available for exploiting information from these modalities. This represents a waste of potentially useful information. We propose as a remedy a convolutional neural network (CNN) architecture for urban land cover classification which is able to embed all available training modalities in a so-called hallucination network. The network will in effect replace missing data modalities in the test phase, enabling fusion capabilities even when data modalities are missing in testing. We demonstrate the method using two datasets consisting of optical and digital surface model (DSM) images. We simulate missing modalities by assuming that DSM images are missing during testing. Our method outperforms both standard CNNs trained only on optical images as well as an ensemble of two standard CNNs. We further evaluate the potential of our method to handle situations where only some DSM images are missing during testing. Overall, we show that we can clearly exploit training time information of the missing modality during testing.



### Learned Features are better for Ethnicity Classification
- **Arxiv ID**: http://arxiv.org/abs/1709.07429v2
- **DOI**: 10.1515/cait-2017-0036
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07429v2)
- **Published**: 2017-09-21 17:45:41+00:00
- **Updated**: 2017-10-31 11:39:22+00:00
- **Authors**: Inzamam Anwar, Naeem Ul Islam
- **Comment**: 15 pages, 8 figures, 2 tables, code and framework available on
  request
- **Journal**: Cybernetics and Information Technologies, vol. 17, no. 3, pp.
  152-164, Sep., 2017
- **Summary**: Ethnicity is a key demographic attribute of human beings and it plays a vital role in automatic facial recognition and have extensive real world applications such as Human Computer Interaction (HCI); demographic based classification; biometric based recognition; security and defense to name a few. In this paper we present a novel approach for extracting ethnicity from the facial images. The proposed method makes use of a pre trained Convolutional Neural Network (CNN) to extract the features and then Support Vector Machine (SVM) with linear kernel is used as a classifier. This technique uses translational invariant hierarchical features learned by the network, in contrast to previous works, which use hand crafted features such as Local Binary Pattern (LBP); Gabor etc. Thorough experiments are presented on ten different facial databases which strongly suggest that our approach is robust to different expressions and illuminations conditions. Here we consider ethnicity classification as a three class problem including Asian, African-American and Caucasian. Average classification accuracy over all databases is 98.28%, 99.66% and 99.05% for Asian, African-American and Caucasian respectively.



### Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1709.07492v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.07492v2)
- **Published**: 2017-09-21 18:50:04+00:00
- **Updated**: 2018-02-26 04:16:14+00:00
- **Authors**: Fangchang Ma, Sertac Karaman
- **Comment**: accepted to ICRA 2018. 8 pages, 8 figures, 3 tables. Video at
  https://www.youtube.com/watch?v=vNIIT_M7x7Y. Code at
  https://github.com/fangchangma/sparse-to-dense
- **Journal**: None
- **Summary**: We consider the problem of dense depth prediction from a sparse set of depth measurements and a single RGB image. Since depth estimation from monocular images alone is inherently ambiguous and unreliable, to attain a higher level of robustness and accuracy, we introduce additional sparse depth samples, which are either acquired with a low-resolution depth sensor or computed via visual Simultaneous Localization and Mapping (SLAM) algorithms. We propose the use of a single deep regression network to learn directly from the RGB-D raw data, and explore the impact of number of depth samples on prediction accuracy. Our experiments show that, compared to using only RGB images, the addition of 100 spatially random depth samples reduces the prediction root-mean-square error by 50% on the NYU-Depth-v2 indoor dataset. It also boosts the percentage of reliable prediction from 59% to 92% on the KITTI dataset. We demonstrate two applications of the proposed algorithm: a plug-in module in SLAM to convert sparse maps to dense maps, and super-resolution for LiDARs. Software and video demonstration are publicly available.



### A Multimodal, Full-Surround Vehicular Testbed for Naturalistic Studies and Benchmarking: Design, Calibration and Deployment
- **Arxiv ID**: http://arxiv.org/abs/1709.07502v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07502v4)
- **Published**: 2017-09-21 19:40:06+00:00
- **Updated**: 2019-01-30 23:15:14+00:00
- **Authors**: Akshay Rangesh, Kevan Yuen, Ravi Kumar Satzoda, Rakesh Nattoji Rajaram, Pujitha Gunaratne, Mohan M. Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: Recent progress in autonomous and semi-autonomous driving has been made possible in part through an assortment of sensors that provide the intelligent agent with an enhanced perception of its surroundings. It has been clear for quite some while now that for intelligent vehicles to function effectively in all situations and conditions, a fusion of different sensor technologies is essential. Consequently, the availability of synchronized multi-sensory data streams are necessary to promote the development of fusion based algorithms for low, mid and high level semantic tasks. In this paper, we provide a comprehensive description of LISA-A: our heavily sensorized, full-surround testbed capable of providing high quality data from a slew of synchronized and calibrated sensors such as cameras, LIDARs, radars, and the IMU/GPS. The vehicle has recorded over 100 hours of real world data for a very diverse set of weather, traffic and daylight conditions. All captured data is accurately calibrated and synchronized using timestamps, and stored safely in high performance servers mounted inside the vehicle itself. Details on the testbed instrumentation, sensor layout, sensor outputs, calibration and synchronization are described in this paper.



