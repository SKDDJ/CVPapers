# Arxiv Papers in cs.CV on 2017-09-25
### Fine-grained Discriminative Localization via Saliency-guided Faster R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1709.08295v1
- **DOI**: 10.1145/3123266.3123319
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08295v1)
- **Published**: 2017-09-25 02:43:49+00:00
- **Updated**: 2017-09-25 02:43:49+00:00
- **Authors**: Xiangteng He, Yuxin Peng, Junjie Zhao
- **Comment**: 9 pages, to appear in ACM MM 2017
- **Journal**: None
- **Summary**: Discriminative localization is essential for fine-grained image classification task, which devotes to recognizing hundreds of subcategories in the same basic-level category. Reflecting on discriminative regions of objects, key differences among different subcategories are subtle and local. Existing methods generally adopt a two-stage learning framework: The first stage is to localize the discriminative regions of objects, and the second is to encode the discriminative features for training classifiers. However, these methods generally have two limitations: (1) Separation of the two-stage learning is time-consuming. (2) Dependence on object and parts annotations for discriminative localization learning leads to heavily labor-consuming labeling. It is highly challenging to address these two important limitations simultaneously. Existing methods only focus on one of them. Therefore, this paper proposes the discriminative localization approach via saliency-guided Faster R-CNN to address the above two limitations at the same time, and our main novelties and advantages are: (1) End-to-end network based on Faster R-CNN is designed to simultaneously localize discriminative regions and encode discriminative features, which accelerates classification speed. (2) Saliency-guided localization learning is proposed to localize the discriminative region automatically, avoiding labor-consuming labeling. Both are jointly employed to simultaneously accelerate classification speed and eliminate dependence on object and parts annotations. Comparing with the state-of-the-art methods on the widely-used CUB-200-2011 dataset, our approach achieves both the best classification accuracy and efficiency.



### Pose-driven Deep Convolutional Model for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1709.08325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08325v1)
- **Published**: 2017-09-25 05:36:39+00:00
- **Updated**: 2017-09-25 05:36:39+00:00
- **Authors**: Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Feature extraction and matching are two crucial components in person Re-Identification (ReID). The large pose deformations and the complex view variations exhibited by the captured person images significantly increase the difficulty of learning and matching of the features from person images. To overcome these difficulties, in this work we propose a Pose-driven Deep Convolutional (PDC) model to learn improved feature extraction and matching models from end to end. Our deep architecture explicitly leverages the human part cues to alleviate the pose variations and learn robust feature representations from both the global image and different local parts. To match the features from global human body and local body parts, a pose driven feature weighting sub-network is further designed to learn adaptive feature fusions. Extensive experimental analyses and results on three popular datasets demonstrate significant performance improvements of our model over all published state-of-the-art methods.



### Realizing Half-Diminished Reality from Video Stream of Manipulating Objects
- **Arxiv ID**: http://arxiv.org/abs/1709.08340v1
- **DOI**: 10.1109/ICAICTA.2016.7803120
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08340v1)
- **Published**: 2017-09-25 06:40:03+00:00
- **Updated**: 2017-09-25 06:40:03+00:00
- **Authors**: Hayato Okumoto, Mitsuo Yoshida, Kyoji Umemura
- **Comment**: The 2016 International Conference On Advanced Informatics: Concepts,
  Theory And Application (ICAICTA2016)
- **Journal**: None
- **Summary**: When we watch a video, in which human hands manipulate objects, these hands may obscure some parts of those objects. We are willing to make clear how the objects are manipulated by making the image of hands semi-transparent, and showing the complete images of the hands and the object. By carefully choosing a Half-Diminished Reality method, this paper proposes a method that can process the video in real time and verifies that the proposed method works well.



### An Evolutionary Computing Enriched RS Attack Resilient Medical Image Steganography Model for Telemedicine Applications
- **Arxiv ID**: http://arxiv.org/abs/1709.08362v2
- **DOI**: 10.1007/s11045-018-0575-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08362v2)
- **Published**: 2017-09-25 08:05:39+00:00
- **Updated**: 2018-04-30 09:17:18+00:00
- **Authors**: Romany F. Mansour, Elsaid MD. Abdelrahim
- **Comment**: 14 page / 3 figures / 6 tables, Multidimensional Systems and Signal
  Processing 2018
- **Journal**: None
- **Summary**: The recent advancement in computing technologies and resulting vision based applications have gives rise to a novel practice called telemedicine that requires patient diagnosis images or allied information to recommend or even perform diagnosis practices being located remotely. However, to ensure accurate and optimal telemedicine there is the requirement of seamless or flawless biomedical information about patient. On the contrary, medical data transmitted over insecure channel often remains prone to get manipulated or corrupted by attackers. The existing cryptosystems alone are not sufficient to deal with these issues and hence in this paper a highly robust reversible image steganography model has been developed for secret information hiding. Unlike traditional wavelet transform techniques, we incorporated Discrete Ripplet Transformation (DRT) technique for message embedding in the medical cover images. In addition, to assure seamless communication over insecure channel, a dual cryptosystem model containing proposed steganography scheme and RSA cryptosystem has been developed. One of the key novelties of the proposed research work is the use of adaptive genetic algorithm (AGA) for optimal pixel adjustment process (OPAP) that enriches data hiding capacity as well as imperceptibility features. The performance assessment reveals that the proposed steganography model outperforms other wavelet transformation based approaches in terms of high PSNR, embedding capacity, imperceptibility etc.



### Multi-level Chaotic Maps for 3D Textured Model Encryption
- **Arxiv ID**: http://arxiv.org/abs/1709.08364v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1709.08364v2)
- **Published**: 2017-09-25 08:20:17+00:00
- **Updated**: 2020-11-05 12:15:58+00:00
- **Authors**: Xin Jin, Shuyun Zhu, Le Wu, Geng Zhao, Xiaodong Li, Quan Zhou, Huimin Lu
- **Comment**: ROSENET 2018 and SCIS paper
- **Journal**: None
- **Summary**: With rapid progress of Virtual Reality and Augmented Reality technologies, 3D contents are the next widespread media in many applications. Thus, the protection of 3D models is primarily important. Encryption of 3D models is essential to maintain confidentiality. Previous work on encryption of 3D surface model often consider the point clouds, the meshes and the textures individually. In this work, a multi-level chaotic maps models for 3D textured encryption was presented by observing the different contributions for recognizing cipher 3D models between vertices (point cloud), polygons and textures. For vertices which make main contribution for recognizing, we use high level 3D Lu chaotic map to encrypt them. For polygons and textures which make relatively smaller contributions for recognizing, we use 2D Arnold's cat map and 1D Logistic map to encrypt them, respectively. The experimental results show that our method can get similar performance with the other method use the same high level chaotic map for point cloud, polygons and textures, while we use less time. Besides, our method can resist more method of attacks such as statistic attack, brute-force attack, correlation attack.



### Deep Sparse Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1709.08374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08374v1)
- **Published**: 2017-09-25 08:43:04+00:00
- **Updated**: 2017-09-25 08:43:04+00:00
- **Authors**: Xi Peng, Jiashi Feng, Shijie Xiao, Jiwen Lu, Zhang Yi, Shuicheng Yan
- **Comment**: The initial version is completed at the beginning of 2015
- **Journal**: None
- **Summary**: In this paper, we present a deep extension of Sparse Subspace Clustering, termed Deep Sparse Subspace Clustering (DSSC). Regularized by the unit sphere distribution assumption for the learned deep features, DSSC can infer a new data affinity matrix by simultaneously satisfying the sparsity principle of SSC and the nonlinearity given by neural networks. One of the appealing advantages brought by DSSC is: when original real-world data do not meet the class-specific linear subspace distribution assumption, DSSC can employ neural networks to make the assumption valid with its hierarchical nonlinear transformations. To the best of our knowledge, this is among the first deep learning based subspace clustering methods. Extensive experiments are conducted on four real-world datasets to show the proposed DSSC is significantly superior to 12 existing methods for subspace clustering.



### Variational Reflectance Estimation from Multi-view Images
- **Arxiv ID**: http://arxiv.org/abs/1709.08378v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08378v2)
- **Published**: 2017-09-25 08:53:03+00:00
- **Updated**: 2018-01-23 08:36:13+00:00
- **Authors**: Jean Mélou, Yvain Quéau, Jean-Denis Durou, Fabien Castan, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of reflectance estimation from a set of multi-view images, assuming known geometry. The approach we put forward turns the input images into reflectance maps, through a robust variational method. The variational model comprises an image-driven fidelity term and a term which enforces consistency of the reflectance estimates with respect to each view. If illumination is fixed across the views, then reflectance estimation remains under-constrained: a regularization term, which ensures piecewise-smoothness of the reflectance, is thus used. Reflectance is parameterized in the image domain, rather than on the surface, which makes the numerical solution much easier, by resorting to an alternating majorization-minimization approach. Experiments on both synthetic and real datasets are carried out to validate the proposed strategy.



### Multi-view Registration Based on Weighted Low Rank and Sparse Matrix Decomposition of Motions
- **Arxiv ID**: http://arxiv.org/abs/1709.08393v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08393v2)
- **Published**: 2017-09-25 09:30:02+00:00
- **Updated**: 2018-04-05 01:22:46+00:00
- **Authors**: Congcong Jin, Jihua Zhu, Yaochen Li, Shanmin Pang, Lei Chen, Jun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the low rank and sparse (LRS) matrix decomposition has been introduced as an effective mean to solve the multi-view registration. It views each available relative motion as a block element to reconstruct one matrix so as to approximate the low rank matrix, where global motions can be recovered for multi-view registration. However, this approach is sensitive to the sparsity of the reconstructed matrix and it treats all block elements equally in spite of their varied reliability. Therefore, this paper proposes an effective approach for multi-view registration by the weighted LRS decomposition. Based on the anti-symmetry property of relative motions, it firstly proposes a completion strategy to reduce the sparsity of the reconstructed matrix. The reduced sparsity of reconstructed matrix can improve the robustness of LRS decomposition. Then, it proposes the weighted LRS decomposition, where each block element is assigned with one estimated weight to denote its reliability. By introducing the weight, more accurate registration results can be recovered from the estimated low rank matrix with good efficiency. Experimental results tested on public data sets illustrate the superiority of the proposed approach over the state-of-the-art approaches on robustness, accuracy, and efficiency.



### Morphable Face Models - An Open Framework
- **Arxiv ID**: http://arxiv.org/abs/1709.08398v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08398v2)
- **Published**: 2017-09-25 09:36:19+00:00
- **Updated**: 2017-09-26 07:18:15+00:00
- **Authors**: Thomas Gerig, Andreas Morel-Forster, Clemens Blumer, Bernhard Egger, Marcel Lüthi, Sandro Schönborn, Thomas Vetter
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel open-source pipeline for face registration based on Gaussian processes as well as an application to face image analysis. Non-rigid registration of faces is significant for many applications in computer vision, such as the construction of 3D Morphable face models (3DMMs). Gaussian Process Morphable Models (GPMMs) unify a variety of non-rigid deformation models with B-splines and PCA models as examples. GPMM separate problem specific requirements from the registration algorithm by incorporating domain-specific adaptions as a prior model. The novelties of this paper are the following: (i) We present a strategy and modeling technique for face registration that considers symmetry, multi-scale and spatially-varying details. The registration is applied to neutral faces and facial expressions. (ii) We release an open-source software framework for registration and model-building, demonstrated on the publicly available BU3D-FE database. The released pipeline also contains an implementation of an Analysis-by-Synthesis model adaption of 2D face images, tested on the Multi-PIE and LFW database. This enables the community to reproduce, evaluate and compare the individual steps of registration to model-building and 3D/2D model fitting. (iii) Along with the framework release, we publish a new version of the Basel Face Model (BFM-2017) with an improved age distribution and an additional facial expression model.



### Summarization of User-Generated Sports Video by Using Deep Action Recognition Features
- **Arxiv ID**: http://arxiv.org/abs/1709.08421v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1709.08421v2)
- **Published**: 2017-09-25 10:57:30+00:00
- **Updated**: 2018-04-13 09:28:40+00:00
- **Authors**: Antonio Tejero-de-Pablos, Yuta Nakashima, Tomokazu Sato, Naokazu Yokoya, Marko Linna, Esa Rahtu
- **Comment**: 12 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: Automatically generating a summary of sports video poses the challenge of detecting interesting moments, or highlights, of a game. Traditional sports video summarization methods leverage editing conventions of broadcast sports video that facilitate the extraction of high-level semantics. However, user-generated videos are not edited, and thus traditional methods are not suitable to generate a summary. In order to solve this problem, this work proposes a novel video summarization method that uses players' actions as a cue to determine the highlights of the original video. A deep neural network-based approach is used to extract two types of action-related features and to classify video segments into interesting or uninteresting parts. The proposed method can be applied to any sports in which games consist of a succession of actions. Especially, this work considers the case of Kendo (Japanese fencing) as an example of a sport to evaluate the proposed method. The method is trained using Kendo videos with ground truth labels that indicate the video highlights. The labels are provided by annotators possessing different experience with respect to Kendo to demonstrate how the proposed method adapts to different needs. The performance of the proposed method is compared with several combinations of different features, and the results show that it outperforms previous summarization methods.



### DeepVO: Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.08429v1
- **DOI**: 10.1109/ICRA.2017.7989236
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.08429v1)
- **Published**: 2017-09-25 11:29:00+00:00
- **Updated**: 2017-09-25 11:29:00+00:00
- **Authors**: Sen Wang, Ronald Clark, Hongkai Wen, Niki Trigoni
- **Comment**: Published in 2017 IEEE International Conference on Robotics and
  Automation (ICRA 2017). Official version at
  http://ieeexplore.ieee.org/document/7989236/ . Project website and videos at
  http://senwang.gitlab.io/DeepVO/
- **Journal**: None
- **Summary**: This paper studies monocular visual odometry (VO) problem. Most of existing VO algorithms are developed under a standard pipeline including feature extraction, feature matching, motion estimation, local optimisation, etc. Although some of them have demonstrated superior performance, they usually need to be carefully designed and specifically fine-tuned to work well in different environments. Some prior knowledge is also required to recover an absolute scale for monocular VO. This paper presents a novel end-to-end framework for monocular VO by using deep Recurrent Convolutional Neural Networks (RCNNs). Since it is trained and deployed in an end-to-end manner, it infers poses directly from a sequence of raw RGB images (videos) without adopting any module in the conventional VO pipeline. Based on the RCNNs, it not only automatically learns effective feature representation for the VO problem through Convolutional Neural Networks, but also implicitly models sequential dynamics and relations using deep Recurrent Neural Networks. Extensive experiments on the KITTI VO dataset show competitive performance to state-of-the-art methods, verifying that the end-to-end Deep Learning technique can be a viable complement to the traditional VO systems.



### Statistical learning of spatiotemporal patterns from longitudinal manifold-valued networks
- **Arxiv ID**: http://arxiv.org/abs/1709.08491v1
- **DOI**: 10.1007/978-3-319-66182-7_52
- **Categories**: **stat.ML**, cs.CV, q-bio.NC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1709.08491v1)
- **Published**: 2017-09-25 13:57:08+00:00
- **Updated**: 2017-09-25 13:57:08+00:00
- **Authors**: Igor Koval, Jean-Baptiste Schiratti, Alexandre Routier, Michael Bacci, Olivier Colliot, Stéphanie Allassonnière, Stanley Durrleman
- **Comment**: None
- **Journal**: Proc. Medical Image Computing and Computer-Assisted Intervention,
  MICCAI 2017, Lecture Notes in Computer Science, volume 10433, pp 451-459,
  Springer
- **Summary**: We introduce a mixed-effects model to learn spatiotempo-ral patterns on a network by considering longitudinal measures distributed on a fixed graph. The data come from repeated observations of subjects at different time points which take the form of measurement maps distributed on a graph such as an image or a mesh. The model learns a typical group-average trajectory characterizing the propagation of measurement changes across the graph nodes. The subject-specific trajectories are defined via spatial and temporal transformations of the group-average scenario, thus estimating the variability of spatiotemporal patterns within the group. To estimate population and individual model parameters, we adapted a stochastic version of the Expectation-Maximization algorithm, the MCMC-SAEM. The model is used to describe the propagation of cortical atrophy during the course of Alzheimer's Disease. Model parameters show the variability of this average pattern of atrophy in terms of trajectories across brain regions, age at disease onset and pace of propagation. We show that the personalization of this model yields accurate prediction of maps of cortical thickness in patients.



### LADAR-Based Mover Detection from Moving Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1709.08515v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.08515v1)
- **Published**: 2017-09-25 14:30:33+00:00
- **Updated**: 2017-09-25 14:30:33+00:00
- **Authors**: Daniel D. Morris, Brian Colonna, Paul Haley
- **Comment**: None
- **Journal**: Proceedings of the 25th Army Science Conference, Nov 27-30, 2006
- **Summary**: Detecting moving vehicles and people is crucial for safe operation of UGVs but is challenging in cluttered, real world environments. We propose a registration technique that enables objects to be robustly matched and tracked, and hence movers to be detected even in high clutter. Range data are acquired using a 2D scanning Ladar from a moving platform. These are automatically clustered into objects and modeled using a surface density function. A Bhattacharya similarity is optimized to register subsequent views of each object enabling good discrimination and tracking, and hence mover detection.



### Generative learning for deep networks
- **Arxiv ID**: http://arxiv.org/abs/1709.08524v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1709.08524v1)
- **Published**: 2017-09-25 14:43:53+00:00
- **Updated**: 2017-09-25 14:43:53+00:00
- **Authors**: Boris Flach, Alexander Shekhovtsov, Ondrej Fikar
- **Comment**: submitted to AAAI
- **Journal**: None
- **Summary**: Learning, taking into account full distribution of the data, referred to as generative, is not feasible with deep neural networks (DNNs) because they model only the conditional distribution of the outputs given the inputs. Current solutions are either based on joint probability models facing difficult estimation problems or learn two separate networks, mapping inputs to outputs (recognition) and vice-versa (generation). We propose an intermediate approach. First, we show that forward computation in DNNs with logistic sigmoid activations corresponds to a simplified approximate Bayesian inference in a directed probabilistic multi-layer model. This connection allows to interpret DNN as a probabilistic model of the output and all hidden units given the input. Second, we propose that in order for the recognition and generation networks to be more consistent with the joint model of the data, weights of the recognition and generator network should be related by transposition. We demonstrate in a tentative experiment that such a coupled pair can be learned generatively, modelling the full distribution of the data, and has enough capacity to perform well in both recognition and generation.



### Multi-view pose estimation with mixtures-of-parts and adaptive viewpoint selection
- **Arxiv ID**: http://arxiv.org/abs/1709.08527v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08527v1)
- **Published**: 2017-09-25 14:45:23+00:00
- **Updated**: 2017-09-25 14:45:23+00:00
- **Authors**: Emre Dogan, Gonen Eren, Christian Wolf, Eric Lombardi, Atilla Baskurt
- **Comment**: 8 pages, 7 figures, 4 tables. Second revision to the paper, as
  submitted to IET Computer Vision on September 24th 2017
- **Journal**: None
- **Summary**: We propose a new method for human pose estimation which leverages information from multiple views to impose a strong prior on articulated pose. The novelty of the method concerns the types of coherence modelled. Consistency is maximised over the different views through different terms modelling classical geometric information (coherence of the resulting poses) as well as appearance information which is modelled as latent variables in the global energy function. Moreover, adequacy of each view is assessed and their contributions are adjusted accordingly. Experiments on the HumanEva and UMPM datasets show that the proposed method significantly decreases the estimation error compared to single-view results.



### Attribute Recognition by Joint Recurrent Learning of Context and Correlation
- **Arxiv ID**: http://arxiv.org/abs/1709.08553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08553v1)
- **Published**: 2017-09-25 15:28:47+00:00
- **Updated**: 2017-09-25 15:28:47+00:00
- **Authors**: Jingya Wang, Xiatian Zhu, Shaogang Gong, Wei Li
- **Comment**: Accepted by ICCV 2017
- **Journal**: None
- **Summary**: Recognising semantic pedestrian attributes in surveillance images is a challenging task for computer vision, particularly when the imaging quality is poor with complex background clutter and uncontrolled viewing conditions, and the number of labelled training data is small. In this work, we formulate a Joint Recurrent Learning (JRL) model for exploring attribute context and correlation in order to improve attribute recognition given small sized training data with poor quality images. The JRL model learns jointly pedestrian attribute correlations in a pedestrian image and in particular their sequential ordering dependencies (latent high-order correlation) in an end-to-end encoder/decoder recurrent network. We demonstrate the performance advantage and robustness of the JRL model over a wide range of state-of-the-art deep models for pedestrian attribute recognition, multi-label image classification, and multi-person image annotation on two largest pedestrian attribute benchmarks PETA and RAP.



### Dense scale selection over space, time and space-time
- **Arxiv ID**: http://arxiv.org/abs/1709.08603v5
- **DOI**: 10.1137/17M114892X
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08603v5)
- **Published**: 2017-09-25 17:14:17+00:00
- **Updated**: 2018-08-03 05:06:46+00:00
- **Authors**: Tony Lindeberg
- **Comment**: 43 pages, 14 figures, 3 tables
- **Journal**: SIAM Journal on Imaging Sciences, 11(1): 407-441, 2018
- **Summary**: Scale selection methods based on local extrema over scale of scale-normalized derivatives have been primarily developed to be applied sparsely --- at image points where the magnitude of a scale-normalized differential expression additionally assumes local extrema over the domain where the data are defined. This paper presents a methodology for performing dense scale selection, so that hypotheses about local characteristic scales in images, temporal signals and video can be computed at every image point and every time moment.   A critical problem when designing mechanisms for dense scale selection is that the scale at which scale-normalized differential entities assume local extrema over scale can be strongly dependent on the local order of the locally dominant differential structure. To address this problem, we propose a methodology where local extrema over scale are detected of a quasi quadrature measure involving scale-space derivatives up to order two and propose two independent mechanisms to reduce the phase dependency of the local scale estimates by: (i) introducing a second layer of post-smoothing prior to the detection of local extrema over scale and (ii) performing local phase compensation based on a model of the phase dependency of the local scale estimates depending on the relative strengths between first- vs. second-order differential structure.   This general methodology is applied over three types of domains: (i) spatial images, (ii) temporal signals and (iii) spatio-temporal video. Experiments show that the proposed methodology leads to intuitively reasonable results with local scale estimates that reflect variations in the characteristic scales of locally dominant structures over space and time.



### Muon Trigger for Mobile Phones
- **Arxiv ID**: http://arxiv.org/abs/1709.08605v1
- **DOI**: 10.1088/1742-6596/898/3/032048
- **Categories**: **cs.CV**, astro-ph.IM, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/1709.08605v1)
- **Published**: 2017-09-25 17:15:09+00:00
- **Updated**: 2017-09-25 17:15:09+00:00
- **Authors**: Maxim Borisyak, Michail Usvyatsov, Michael Mulhearn, Chase Shimmin, Andrey Ustyuzhanin
- **Comment**: None
- **Journal**: None
- **Summary**: The CRAYFIS experiment proposes to use privately owned mobile phones as a ground detector array for Ultra High Energy Cosmic Rays. Upon interacting with Earth's atmosphere, these events produce extensive particle showers which can be detected by cameras on mobile phones. A typical shower contains minimally-ionizing particles such as muons. As these particles interact with CMOS image sensors, they may leave tracks of faintly-activated pixels that are sometimes hard to distinguish from random detector noise. Triggers that rely on the presence of very bright pixels within an image frame are not efficient in this case.   We present a trigger algorithm based on Convolutional Neural Networks which selects images containing such tracks and are evaluated in a lazy manner: the response of each successive layer is computed only if activation of the current layer satisfies a continuation criterion. Usage of neural networks increases the sensitivity considerably comparable with image thresholding, while the lazy evaluation allows for execution of the trigger under the limited computational power of mobile phones.



### Numerical optimization for Artificial Retina Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1709.08610v2
- **DOI**: 10.1088/1742-6596/898/3/032046
- **Categories**: **cs.CV**, hep-ex, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1709.08610v2)
- **Published**: 2017-09-25 17:33:11+00:00
- **Updated**: 2017-10-01 23:42:24+00:00
- **Authors**: Maxim Borisyak, Andrey Ustyuzhanin, Denis Derkach, Mikhail Belous
- **Comment**: None
- **Journal**: None
- **Summary**: High-energy physics experiments rely on reconstruction of the trajectories of particles produced at the interaction point. This is a challenging task, especially in the high track multiplicity environment generated by p-p collisions at the LHC energies. A typical event includes hundreds of signal examples (interesting decays) and a significant amount of noise (uninteresting examples).   This work describes a modification of the Artificial Retina algorithm for fast track finding: numerical optimization methods were adopted for fast local track search. This approach allows for considerable reduction of the total computational time per event. Test results on simplified simulated model of LHCb VELO (VErtex LOcator) detector are presented. Also this approach is well-suited for implementation of paralleled computations as GPGPU which look very attractive in the context of upcoming detector upgrades.



### Fast Vehicle Detection in Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/1709.08666v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08666v1)
- **Published**: 2017-09-25 18:41:01+00:00
- **Updated**: 2017-09-25 18:41:01+00:00
- **Authors**: Jennifer Carlet, Bernard Abayowa
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, several real-time or near real-time object detectors have been developed. However these object detectors are typically designed for first-person view images where the subject is large in the image and do not directly apply well to detecting vehicles in aerial imagery. Though some detectors have been developed for aerial imagery, these are either slow or do not handle multi-scale imagery very well. Here the popular YOLOv2 detector is modified to vastly improve it's performance on aerial data. The modified detector is compared to Faster RCNN on several aerial imagery datasets. The proposed detector gives near state of the art performance at more than 4x the speed.



### Multimodal Image Super-resolution via Joint Sparse Representations induced by Coupled Dictionaries
- **Arxiv ID**: http://arxiv.org/abs/1709.08680v2
- **DOI**: 10.1109/TCI.2019.2916502
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08680v2)
- **Published**: 2017-09-25 19:04:08+00:00
- **Updated**: 2018-03-08 19:41:41+00:00
- **Authors**: Pingfan Song, Xin Deng, João F. C. Mota, Nikos Deligiannis, Pier Luigi Dragotti, Miguel R. D. Rodrigues
- **Comment**: 13 pages, 8 figures, 9 tables
- **Journal**: None
- **Summary**: Real-world data processing problems often involve various image modalities associated with a certain scene, including RGB images, infrared images or multi-spectral images. The fact that different image modalities often share certain attributes, such as certain edges, textures and other structure primitives, represents an opportunity to enhance various image processing tasks. This paper proposes a new approach to construct a high-resolution (HR) version of a low-resolution (LR) image given another HR image modality as reference, based on joint sparse representations induced by coupled dictionaries. Our approach, which captures the similarities and disparities between different image modalities in a learned sparse feature domain in \emph{lieu} of the original image domain, consists of two phases. The coupled dictionary learning phase is used to learn a set of dictionaries that couple different image modalities in the sparse feature domain given a set of training data. In turn, the coupled super-resolution phase leverages such coupled dictionaries to construct a HR version of the LR target image given another related image modality. One of the merits of our sparsity-driven approach relates to the fact that it overcomes drawbacks such as the texture copying artifacts commonly resulting from inconsistency between the guidance and target images. Experiments on real multimodal images demonstrate that incorporating appropriate guidance information via joint sparse representation induced by coupled dictionary learning brings notable benefits in the super-resolution task with respect to the state-of-the-art. Of particular relevance, the proposed approach also demonstrates better robustness than competing deep-learning-based methods in the presence of noise.



### Camera-Aware Multi-Resolution Analysis (CAMRA) for Raw Sensor Data Compression
- **Arxiv ID**: http://arxiv.org/abs/1709.08739v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.08739v1)
- **Published**: 2017-09-25 22:50:28+00:00
- **Updated**: 2017-09-25 22:50:28+00:00
- **Authors**: Y. Lee, K. Hirakawa, T. Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel lossless and lossy compression scheme for color filter array~(CFA) sampled images based on the wavelet transform of them. Our analysis suggests that the wavelet coefficients of HL and LH subbands are highly correlated. Hence, we decorrelate Mallat wavelet packet decomposition to further sparsify the coefficients. In addition, we develop a camera processing pipeline for compressing CFA sampled images aimed at maximizing the quality of the color images constructed from the compressed CFA sampled images. We validated our theoretical analysis and the performance of the proposed compression scheme using images of natural scenes captured in a raw format. The experimental results verify that our proposed method improves coding efficiency relative to the standard and the state-of-the-art compression schemes CFA sampled images.



