# Arxiv Papers in cs.CV on 2017-09-07
### Towards high-throughput 3D insect capture for species discovery and diagnostics
- **Arxiv ID**: http://arxiv.org/abs/1709.02033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02033v1)
- **Published**: 2017-09-07 00:31:03+00:00
- **Updated**: 2017-09-07 00:31:03+00:00
- **Authors**: Chuong Nguyen, Matt Adcock, Stuart Anderson, David Lovell, Nicole Fisher, John La Salle
- **Comment**: 2 pages, 1 figure, for BigDig workshop at 2017 eScience conference
- **Journal**: None
- **Summary**: Digitisation of natural history collections not only preserves precious information about biological diversity, it also enables us to share, analyse, annotate and compare specimens to gain new insights. High-resolution, full-colour 3D capture of biological specimens yields color and geometry information complementary to other techniques (e.g., 2D capture, electron scanning and micro computed tomography). However 3D colour capture of small specimens is slow for reasons including specimen handling, the narrow depth of field of high magnification optics, and the large number of images required to resolve complex shapes of specimens. In this paper, we outline techniques to accelerate 3D image capture, including using a desktop robotic arm to automate the insect handling process; using a calibrated pan-tilt rig to avoid attaching calibration targets to specimens; using light field cameras to capture images at an extended depth of field in one shot; and using 3D Web and mixed reality tools to facilitate the annotation, distribution and visualisation of 3D digital models.



### Capturing natural-colour 3D models of insects for species discovery
- **Arxiv ID**: http://arxiv.org/abs/1709.02039v1
- **DOI**: 10.1371/journal.pone.0094346
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02039v1)
- **Published**: 2017-09-07 01:23:01+00:00
- **Updated**: 2017-09-07 01:23:01+00:00
- **Authors**: Chuong V. Nguyen, David R. Lovell, Matt Adcock, John La Salle
- **Comment**: 24 pages, 17 figures, PLOS ONE journal
- **Journal**: published 2014
- **Summary**: Collections of biological specimens are fundamental to scientific understanding and characterization of natural diversity. This paper presents a system for liberating useful information from physical collections by bringing specimens into the digital domain so they can be more readily shared, analyzed, annotated and compared. It focuses on insects and is strongly motivated by the desire to accelerate and augment current practices in insect taxonomy which predominantly use text, 2D diagrams and images to describe and characterize species. While these traditional kinds of descriptions are informative and useful, they cannot cover insect specimens "from all angles" and precious specimens are still exchanged between researchers and collections for this reason. Furthermore, insects can be complex in structure and pose many challenges to computer vision systems. We present a new prototype for a practical, cost-effective system of off-the-shelf components to acquire natural-colour 3D models of insects from around 3mm to 30mm in length. Colour images are captured from different angles and focal depths using a digital single lens reflex (DSLR) camera rig and two-axis turntable. These 2D images are processed into 3D reconstructions using software based on a visual hull algorithm. The resulting models are compact (around 10 megabytes), afford excellent optical resolution, and can be readily embedded into documents and web pages, as well as viewed on mobile devices. The system is portable, safe, relatively affordable, and complements the sort of volumetric data that can be acquired by computed tomography. This system provides a new way to augment the description and documentation of insect species holotypes, reducing the need to handle or ship specimens. It opens up new opportunities to collect data for research, education, art, entertainment, biodiversity assessment and biosecurity control.



### The Mating Rituals of Deep Neural Networks: Learning Compact Feature Representations through Sexual Evolutionary Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1709.02043v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.02043v1)
- **Published**: 2017-09-07 01:43:19+00:00
- **Updated**: 2017-09-07 01:43:19+00:00
- **Authors**: Audrey Chung, Mohammad Javad Shafiee, Paul Fieguth, Alexander Wong
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Evolutionary deep intelligence was recently proposed as a method for achieving highly efficient deep neural network architectures over successive generations. Drawing inspiration from nature, we propose the incorporation of sexual evolutionary synthesis. Rather than the current asexual synthesis of networks, we aim to produce more compact feature representations by synthesizing more diverse and generalizable offspring networks in subsequent generations via the combination of two parent networks. Experimental results were obtained using the MNIST and CIFAR-10 datasets, and showed improved architectural efficiency and comparable testing accuracy relative to the baseline asexual evolutionary neural networks. In particular, the network synthesized via sexual evolutionary synthesis for MNIST had approximately double the architectural efficiency (cluster efficiency of 34.29X and synaptic efficiency of 258.37X) in comparison to the network synthesized via asexual evolutionary synthesis, with both networks achieving a testing accuracy of ~97%.



### Focusing Attention: Towards Accurate Text Recognition in Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1709.02054v3
- **DOI**: 10.1109/ICCV.2017.543
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02054v3)
- **Published**: 2017-09-07 02:58:01+00:00
- **Updated**: 2017-10-17 04:54:55+00:00
- **Authors**: Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang Pu, Shuigeng Zhou
- **Comment**: Revise the description of IC15 datasets (1811 samples)
- **Journal**: None
- **Summary**: Scene text recognition has been a hot research topic in computer vision due to its various applications. The state of the art is the attention-based encoder-decoder framework that learns the mapping between input images and output sequences in a purely data-driven way. However, we observe that existing attention-based methods perform poorly on complicated and/or low-quality images. One major reason is that existing methods cannot get accurate alignments between feature areas and targets for such images. We call this phenomenon "attention drift". To tackle this problem, in this paper we propose the FAN (the abbreviation of Focusing Attention Network) method that employs a focusing attention mechanism to automatically draw back the drifted attention. FAN consists of two major components: an attention network (AN) that is responsible for recognizing character targets as in the existing methods, and a focusing network (FN) that is responsible for adjusting attention by evaluating whether AN pays attention properly on the target areas in the images. Furthermore, different from the existing methods, we adopt a ResNet-based network to enrich deep representations of scene text images. Extensive experiments on various benchmarks, including the IIIT5k, SVT and ICDAR datasets, show that the FAN method substantially outperforms the existing methods.



### Deep Embedding Convolutional Neural Network for Synthesizing CT Image from T1-Weighted MR Image
- **Arxiv ID**: http://arxiv.org/abs/1709.02073v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02073v2)
- **Published**: 2017-09-07 05:20:26+00:00
- **Updated**: 2017-11-09 04:06:13+00:00
- **Authors**: Lei Xiang, Qian Wang, Xiyao Jin, Dong Nie, Yu Qiao, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, more and more attention is drawn to the field of medical image synthesis across modalities. Among them, the synthesis of computed tomography (CT) image from T1-weighted magnetic resonance (MR) image is of great importance, although the mapping between them is highly complex due to large gaps of appearances of the two modalities. In this work, we aim to tackle this MR-to-CT synthesis by a novel deep embedding convolutional neural network (DECNN). Specifically, we generate the feature maps from MR images, and then transform these feature maps forward through convolutional layers in the network. We can further compute a tentative CT synthesis from the midway of the flow of feature maps, and then embed this tentative CT synthesis back to the feature maps. This embedding operation results in better feature maps, which are further transformed forward in DECNN. After repeat-ing this embedding procedure for several times in the network, we can eventually synthesize a final CT image in the end of the DECNN. We have validated our proposed method on both brain and prostate datasets, by also compar-ing with the state-of-the-art methods. Experimental results suggest that our DECNN (with repeated embedding op-erations) demonstrates its superior performances, in terms of both the perceptive quality of the synthesized CT image and the run-time cost for synthesizing a CT image.



### An unsupervised long short-term memory neural network for event detection in cell videos
- **Arxiv ID**: http://arxiv.org/abs/1709.02081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02081v1)
- **Published**: 2017-09-07 05:58:23+00:00
- **Updated**: 2017-09-07 05:58:23+00:00
- **Authors**: Ha Tran Hong Phan, Ashnil Kumar, David Feng, Michael Fulham, Jinman Kim
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an automatic unsupervised cell event detection and classification method, which expands convolutional Long Short-Term Memory (LSTM) neural networks, for cellular events in cell video sequences. Cells in images that are captured from various biomedical applications usually have different shapes and motility, which pose difficulties for the automated event detection in cell videos. Current methods to detect cellular events are based on supervised machine learning and rely on tedious manual annotation from investigators with specific expertise. So that our LSTM network could be trained in an unsupervised manner, we designed it with a branched structure where one branch learns the frequent, regular appearance and movements of objects and the second learns the stochastic events, which occur rarely and without warning in a cell video sequence. We tested our network on a publicly available dataset of densely packed stem cell phase-contrast microscopy images undergoing cell division. This dataset is considered to be more challenging that a dataset with sparse cells. We compared our method to several published supervised methods evaluated on the same dataset and to a supervised LSTM method with a similar design and configuration to our unsupervised method. We used an F1-score, which is a balanced measure for both precision and recall. Our results show that our unsupervised method has a higher or similar F1-score when compared to two fully supervised methods that are based on Hidden Conditional Random Fields (HCRF), and has comparable accuracy with the current best supervised HCRF-based method. Our method was generalizable as after being trained on one video it could be applied to videos where the cells were in different conditions. The accuracy of our unsupervised method approached that of its supervised counterpart.



### Integrating Specialized Classifiers Based on Continuous Time Markov Chain
- **Arxiv ID**: http://arxiv.org/abs/1709.02123v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.02123v1)
- **Published**: 2017-09-07 07:56:30+00:00
- **Updated**: 2017-09-07 07:56:30+00:00
- **Authors**: Zhizhong Li, Dahua Lin
- **Comment**: Published at IJCAI-17, typo fixed
- **Journal**: None
- **Summary**: Specialized classifiers, namely those dedicated to a subset of classes, are often adopted in real-world recognition systems. However, integrating such classifiers is nontrivial. Existing methods, e.g. weighted average, usually implicitly assume that all constituents of an ensemble cover the same set of classes. Such methods can produce misleading predictions when used to combine specialized classifiers. This work explores a novel approach. Instead of combining predictions from individual classifiers directly, it first decomposes the predictions into sets of pairwise preferences, treating them as transition channels between classes, and thereon constructs a continuous-time Markov chain, and use the equilibrium distribution of this chain as the final prediction. This way allows us to form a coherent picture over all specialized predictions. On large public datasets, the proposed method obtains considerable improvement compared to mainstream ensemble methods, especially when the classifier coverage is highly unbalanced.



### Rotational Subgroup Voting and Pose Clustering for Robust 3D Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1709.02142v1
- **DOI**: 10.1109/iccv.2017.443
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02142v1)
- **Published**: 2017-09-07 08:54:20+00:00
- **Updated**: 2017-09-07 08:54:20+00:00
- **Authors**: Anders Glent Buch, Lilita Kiforenko, Dirk Kraft
- **Comment**: Accepted for International Conference on Computer Vision (ICCV), 2017
- **Journal**: 2017 IEEE International Conference on Computer Vision (ICCV)
- **Summary**: It is possible to associate a highly constrained subset of relative 6 DoF poses between two 3D shapes, as long as the local surface orientation, the normal vector, is available at every surface point. Local shape features can be used to find putative point correspondences between the models due to their ability to handle noisy and incomplete data. However, this correspondence set is usually contaminated by outliers in practical scenarios, which has led to many past contributions based on robust detectors such as the Hough transform or RANSAC. The key insight of our work is that a single correspondence between oriented points on the two models is constrained to cast votes in a 1 DoF rotational subgroup of the full group of poses, SE(3). Kernel density estimation allows combining the set of votes efficiently to determine a full 6 DoF candidate pose between the models. This modal pose with the highest density is stable under challenging conditions, such as noise, clutter, and occlusions, and provides the output estimate of our method.   We first analyze the robustness of our method in relation to noise and show that it handles high outlier rates much better than RANSAC for the task of 6 DoF pose estimation. We then apply our method to four state of the art data sets for 3D object recognition that contain occluded and cluttered scenes. Our method achieves perfect recall on two LIDAR data sets and outperforms competing methods on two RGB-D data sets, thus setting a new standard for general 3D object recognition using point cloud data.



### Improving Sonar Image Patch Matching via Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1709.02150v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.02150v1)
- **Published**: 2017-09-07 09:25:58+00:00
- **Updated**: 2017-09-07 09:25:58+00:00
- **Authors**: Matias Valdenegro-Toro
- **Comment**: Author version
- **Journal**: Proceedings of the European Conference on Mobile Robotics 2017
- **Summary**: Matching sonar images with high accuracy has been a problem for a long time, as sonar images are inherently hard to model due to reflections, noise and viewpoint dependence. Autonomous Underwater Vehicles require good sonar image matching capabilities for tasks such as tracking, simultaneous localization and mapping (SLAM) and some cases of object detection/recognition. We propose the use of Convolutional Neural Networks (CNN) to learn a matching function that can be trained from labeled sonar data, after pre-processing to generate matching and non-matching pairs. In a dataset of 39K training pairs, we obtain 0.91 Area under the ROC Curve (AUC) for a CNN that outputs a binary classification matching decision, and 0.89 AUC for another CNN that outputs a matching score. In comparison, classical keypoint matching methods like SIFT, SURF, ORB and AKAZE obtain AUC 0.61 to 0.68. Alternative learning methods obtain similar results, with a Random Forest Classifier obtaining AUC 0.79, and a Support Vector Machine resulting in AUC 0.66.



### Real-time convolutional networks for sonar image classification in low-power embedded systems
- **Arxiv ID**: http://arxiv.org/abs/1709.02153v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.02153v1)
- **Published**: 2017-09-07 09:36:06+00:00
- **Updated**: 2017-09-07 09:36:06+00:00
- **Authors**: Matias Valdenegro-Toro
- **Comment**: Author version
- **Journal**: ESANN 2017 proceedings
- **Summary**: Deep Neural Networks have impressive classification performance, but this comes at the expense of significant computational resources at inference time. Autonomous Underwater Vehicles use low-power embedded systems for sonar image perception, and cannot execute large neural networks in real-time. We propose the use of max-pooling aggressively, and we demonstrate it with a Fire-based module and a new Tiny module that includes max-pooling in each module. By stacking them we build networks that achieve the same accuracy as bigger ones, while reducing the number of parameters and considerably increasing computational performance. Our networks can classify a 96x96 sonar image with 98.8 - 99.7 accuracy on only 41 to 61 milliseconds on a Raspberry Pi 2, which corresponds to speedups of 28.6 - 19.7.



### FingerNet: An Unified Deep Network for Fingerprint Minutiae Extraction
- **Arxiv ID**: http://arxiv.org/abs/1709.02228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02228v1)
- **Published**: 2017-09-07 13:31:38+00:00
- **Updated**: 2017-09-07 13:31:38+00:00
- **Authors**: Yao Tang, Fei Gao, Jufu Feng, Yuhang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Minutiae extraction is of critical importance in automated fingerprint recognition. Previous works on rolled/slap fingerprints failed on latent fingerprints due to noisy ridge patterns and complex background noises. In this paper, we propose a new way to design deep convolutional network combining domain knowledge and the representation ability of deep learning. In terms of orientation estimation, segmentation, enhancement and minutiae extraction, several typical traditional methods performed well on rolled/slap fingerprints are transformed into convolutional manners and integrated as an unified plain network. We demonstrate that this pipeline is equivalent to a shallow network with fixed weights. The network is then expanded to enhance its representation ability and the weights are released to learn complex background variance from data, while preserving end-to-end differentiability. Experimental results on NIST SD27 latent database and FVC 2004 slap database demonstrate that the proposed algorithm outperforms the state-of-the-art minutiae extraction algorithms. Code is made publicly available at: https://github.com/felixTY/FingerNet.



### Monocular Navigation in Large Scale Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/1709.02285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02285v1)
- **Published**: 2017-09-07 14:46:45+00:00
- **Updated**: 2017-09-07 14:46:45+00:00
- **Authors**: Darius Burschka
- **Comment**: 2017 British Machine Vision Conference, London (BMVC 2017)
- **Journal**: None
- **Summary**: We present a processing technique for a robust reconstruction of motion properties for single points in large scale, dynamic environments. We assume that the acquisition camera is moving and that there are other independently moving agents in a large environment, like road scenarios. The separation of direction and magnitude of the reconstructed motion allows for robust reconstruction of the dynamic state of the objects in situations, where conventional binocular systems fail due to a small signal (disparity) from the images due to a constant detection error, and where structure from motion approaches fail due to unobserved motion of other agents between the camera frames.   We present the mathematical framework and the sensitivity analysis for the resulting system.



### PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume
- **Arxiv ID**: http://arxiv.org/abs/1709.02371v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02371v3)
- **Published**: 2017-09-07 17:47:59+00:00
- **Updated**: 2018-06-25 20:34:58+00:00
- **Authors**: Deqing Sun, Xiaodong Yang, Ming-Yu Liu, Jan Kautz
- **Comment**: CVPR 2018 camera ready version (with github link to Caffe and PyTorch
  code)
- **Journal**: None
- **Summary**: We present a compact but effective CNN model for optical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the cur- rent optical flow estimate to warp the CNN features of the second image. It then uses the warped features and features of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024x436) images. Our models are available on https://github.com/NVlabs/PWC-Net.



### Adaptive PCA for Time-Varying Data
- **Arxiv ID**: http://arxiv.org/abs/1709.02373v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.02373v2)
- **Published**: 2017-09-07 17:49:47+00:00
- **Updated**: 2017-09-12 15:55:44+00:00
- **Authors**: Salaheddin Alakkari, John Dingliana
- **Comment**: Typos fixed, uncited references removed
- **Journal**: None
- **Summary**: In this paper, we present an online adaptive PCA algorithm that is able to compute the full dimensional eigenspace per new time-step of sequential data. The algorithm is based on a one-step update rule that considers all second order correlations between previous samples and the new time-step. Our algorithm has O(n) complexity per new time-step in its deterministic mode and O(1) complexity per new time-step in its stochastic mode. We test our algorithm on a number of time-varying datasets of different physical phenomena. Explained variance curves indicate that our technique provides an excellent approximation to the original eigenspace computed using standard PCA in batch mode. In addition, our experiments show that the stochastic mode, despite its much lower computational complexity, converges to the same eigenspace computed using the deterministic mode.



### End-to-end Face Detection and Cast Grouping in Movies Using Erdős-Rényi Clustering
- **Arxiv ID**: http://arxiv.org/abs/1709.02458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02458v1)
- **Published**: 2017-09-07 21:22:26+00:00
- **Updated**: 2017-09-07 21:22:26+00:00
- **Authors**: SouYoung Jin, Hang Su, Chris Stauffer, Erik Learned-Miller
- **Comment**: to appear in ICCV 2017 (spotlight)
- **Journal**: None
- **Summary**: We present an end-to-end system for detecting and clustering faces by identity in full-length movies. Unlike works that start with a predefined set of detected faces, we consider the end-to-end problem of detection and clustering together. We make three separate contributions. First, we combine a state-of-the-art face detector with a generic tracker to extract high quality face tracklets. We then introduce a novel clustering method, motivated by the classic graph theory results of Erd\H{o}s and R\'enyi. It is based on the observations that large clusters can be fully connected by joining just a small fraction of their point pairs, while just a single connection between two different people can lead to poor clustering results. This suggests clustering using a verification system with very few false positives but perhaps moderate recall. We introduce a novel verification method, rank-1 counts verification, that has this property, and use it in a link-based clustering scheme. Finally, we define a novel end-to-end detection and clustering evaluation metric allowing us to assess the accuracy of the entire end-to-end system. We present state-of-the-art results on multiple video data sets and also on standard face databases.



### Local Neighborhood Intensity Pattern: A new texture feature descriptor for image retrieval
- **Arxiv ID**: http://arxiv.org/abs/1709.02463v3
- **DOI**: 10.1016/j.eswa.2018.06.044
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02463v3)
- **Published**: 2017-09-07 21:56:32+00:00
- **Updated**: 2018-07-02 05:49:52+00:00
- **Authors**: Prithaj Banerjee, Ayan Kumar Bhunia, Avirup Bhattacharyya, Partha Pratim Roy, Subrahmanyam Murala
- **Comment**: Expert Systems with Applications(Elsevier)
- **Journal**: None
- **Summary**: In this paper, a new texture descriptor based on the local neighborhood intensity difference is proposed for content based image retrieval (CBIR). For computation of texture features like Local Binary Pattern (LBP), the center pixel in a 3*3 window of an image is compared with all the remaining neighbors, one pixel at a time to generate a binary bit pattern. It ignores the effect of the adjacent neighbors of a particular pixel for its binary encoding and also for texture description. The proposed method is based on the concept that neighbors of a particular pixel hold a significant amount of texture information that can be considered for efficient texture representation for CBIR. Taking this into account, we develop a new texture descriptor, named as Local Neighborhood Intensity Pattern (LNIP) which considers the relative intensity difference between a particular pixel and the center pixel by considering its adjacent neighbors and generate a sign and a magnitude pattern. Since sign and magnitude patterns hold complementary information to each other, these two patterns are concatenated into a single feature descriptor to generate a more concrete and useful feature descriptor. The proposed descriptor has been tested for image retrieval on four databases, including three texture image databases - Brodatz texture image database, MIT VisTex database and Salzburg texture database and one face database AT&T face database. The precision and recall values observed on these databases are compared with some state-of-art local patterns. The proposed method showed a significant improvement over many other existing methods.



### Fine-grained Recognition in the Wild: A Multi-Task Domain Adaptation Approach
- **Arxiv ID**: http://arxiv.org/abs/1709.02476v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02476v1)
- **Published**: 2017-09-07 22:31:45+00:00
- **Updated**: 2017-09-07 22:31:45+00:00
- **Authors**: Timnit Gebru, Judy Hoffman, Li Fei-Fei
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: While fine-grained object recognition is an important problem in computer vision, current models are unlikely to accurately classify objects in the wild. These fully supervised models need additional annotated images to classify objects in every new scenario, a task that is infeasible. However, sources such as e-commerce websites and field guides provide annotated images for many classes. In this work, we study fine-grained domain adaptation as a step towards overcoming the dataset shift between easily acquired annotated images and the real world. Adaptation has not been studied in the fine-grained setting where annotations such as attributes could be used to increase performance. Our work uses an attribute based multi-task adaptation loss to increase accuracy from a baseline of 4.1% to 19.1% in the semi-supervised adaptation case. Prior do- main adaptation works have been benchmarked on small datasets such as [46] with a total of 795 images for some domains, or simplistic datasets such as [41] consisting of digits. We perform experiments on a subset of a new challenging fine-grained dataset consisting of 1,095,021 images of 2, 657 car categories drawn from e-commerce web- sites and Google Street View.



### Fine-Grained Car Detection for Visual Census Estimation
- **Arxiv ID**: http://arxiv.org/abs/1709.02480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02480v1)
- **Published**: 2017-09-07 22:56:21+00:00
- **Updated**: 2017-09-07 22:56:21+00:00
- **Authors**: Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Li Fei-Fei
- **Comment**: AAAI 2016
- **Journal**: None
- **Summary**: Targeted socioeconomic policies require an accurate understanding of a country's demographic makeup. To that end, the United States spends more than 1 billion dollars a year gathering census data such as race, gender, education, occupation and unemployment rates. Compared to the traditional method of collecting surveys across many years which is costly and labor intensive, data-driven, machine learning driven approaches are cheaper and faster--with the potential ability to detect trends in close to real time. In this work, we leverage the ubiquity of Google Street View images and develop a computer vision pipeline to predict income, per capita carbon emission, crime rates and other city attributes from a single source of publicly available visual data. We first detect cars in 50 million images across 200 of the largest US cities and train a model to predict demographic attributes using the detected cars. To facilitate our work, we have collected the largest and most challenging fine-grained dataset reported to date consisting of over 2600 classes of cars comprised of images from Google Street View and other web sources, classified by car experts to account for even the most subtle of visual differences. We use this data to construct the largest scale fine-grained detection system reported to date. Our prediction results correlate well with ground truth income data (r=0.82), Massachusetts department of vehicle registration, and sources investigating crime rates, income segregation, per capita carbon emission, and other market research. Finally, we learn interesting relationships between cars and neighborhoods allowing us to perform the first large scale sociological analysis of cities using computer vision techniques.



### Scalable Annotation of Fine-Grained Categories Without Experts
- **Arxiv ID**: http://arxiv.org/abs/1709.02482v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.02482v1)
- **Published**: 2017-09-07 23:08:26+00:00
- **Updated**: 2017-09-07 23:08:26+00:00
- **Authors**: Timnit Gebru, Jonathan Krause, Jia Deng, Li Fei-Fei
- **Comment**: CHI 2017
- **Journal**: None
- **Summary**: We present a crowdsourcing workflow to collect image annotations for visually similar synthetic categories without requiring experts. In animals, there is a direct link between taxonomy and visual similarity: e.g. a collie (type of dog) looks more similar to other collies (e.g. smooth collie) than a greyhound (another type of dog). However, in synthetic categories such as cars, objects with similar taxonomy can have very different appearance: e.g. a 2011 Ford F-150 Supercrew-HD looks the same as a 2011 Ford F-150 Supercrew-LL but very different from a 2011 Ford F-150 Supercrew-SVT. We introduce a graph based crowdsourcing algorithm to automatically group visually indistinguishable objects together. Using our workflow, we label 712,430 images by ~1,000 Amazon Mechanical Turk workers; resulting in the largest fine-grained visual dataset reported to date with 2,657 categories of cars annotated at 1/20th the cost of hiring experts.



