# Arxiv Papers in cs.CV on 2017-09-20
### Local Directional Relation Pattern for Unconstrained and Robust Face Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1709.09518v2
- **DOI**: 10.1007/s11042-019-07908-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09518v2)
- **Published**: 2017-09-20 04:36:56+00:00
- **Updated**: 2019-07-03 09:01:27+00:00
- **Authors**: Shiv Ram Dubey
- **Comment**: Multimedia Tools and Applications
- **Journal**: None
- **Summary**: Face recognition is still a very demanding area of research. This problem becomes more challenging in unconstrained environment and in the presence of several variations like pose, illumination, expression, etc. Local descriptors are widely used for this task. The most of the existing local descriptors consider only few immediate local neighbors and not able to utilize the wider local information to make the descriptor more discriminative. The wider local information based descriptors mainly suffer due to the increased dimensionality. In this paper, this problem is solved by encoding the relationship among directional neighbors in an efficient manner. The relationship between the center pixel and the encoded directional neighbors is utilized further to form the proposed local directional relation pattern (LDRP). The descriptor is inherently uniform illumination invariant. The multi-scale mechanism is also adapted to further boost the discriminative ability of the descriptor. The proposed descriptor is evaluated under the image retrieval framework over face databases. Very challenging databases like PaSC, LFW, PubFig, ESSEX, FERET, AT&T, and FaceScrub are used to test the discriminative ability and robustness of LDRP descriptor. Results are also compared with the recent state-of-the-art face descriptors such as LBP, LTP, LDP, LDN, LVP, DCP, LDGP and LGHP. Very promising performance is observed using the proposed descriptor over very appealing face databases as compared to the existing face descriptors. The proposed LDRP descriptor also outperforms the pre-trained ImageNet CNN models over large-scale FaceScrub face dataset. Moreover, it also outperforms the deep learning based DLib face descriptor in many scenarios.



### SegFlow: Joint Learning for Video Object Segmentation and Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1709.06750v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06750v1)
- **Published**: 2017-09-20 07:38:20+00:00
- **Updated**: 2017-09-20 07:38:20+00:00
- **Authors**: Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, Ming-Hsuan Yang
- **Comment**: Accepted in ICCV'17. Code is available at
  https://sites.google.com/site/yihsuantsai/research/iccv17-segflow
- **Journal**: None
- **Summary**: This paper proposes an end-to-end trainable network, SegFlow, for simultaneously predicting pixel-wise object segmentation and optical flow in videos. The proposed SegFlow has two branches where useful information of object segmentation and optical flow is propagated bidirectionally in a unified framework. The segmentation branch is based on a fully convolutional network, which has been proved effective in image segmentation task, and the optical flow branch takes advantage of the FlowNet model. The unified framework is trained iteratively offline to learn a generic notion, and fine-tuned online for specific objects. Extensive experiments on both the video object segmentation and optical flow datasets demonstrate that introducing optical flow improves the performance of segmentation and vice versa, against the state-of-the-art algorithms.



### Transfer learning from synthetic to real images using variational autoencoders for robotic applications
- **Arxiv ID**: http://arxiv.org/abs/1709.06762v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.06762v1)
- **Published**: 2017-09-20 08:18:07+00:00
- **Updated**: 2017-09-20 08:18:07+00:00
- **Authors**: Tadanobu Inoue, Subhajit Chaudhury, Giovanni De Magistris, Sakyasingha Dasgupta
- **Comment**: video: https://youtu.be/Wd-1WU8emkw
- **Journal**: None
- **Summary**: Robotic learning in simulation environments provides a faster, more scalable, and safer training methodology than learning directly with physical robots. Also, synthesizing images in a simulation environment for collecting large-scale image data is easy, whereas capturing camera images in the real world is time consuming and expensive. However, learning from only synthetic images may not achieve the desired performance in real environments due to the gap between synthetic and real images. We thus propose a method that transfers learned capability of detecting object position from a simulation environment to the real world. Our method enables us to use only a very limited dataset of real images while leveraging a large dataset of synthetic images using multiple variational autoencoders. It detects object positions 6 to 7 times more precisely than the baseline of directly learning from the dataset of the real images. Object position estimation under varying environmental conditions forms one of the underlying requirement for standard robotic manipulation tasks. We show that the proposed method performs robustly in different lighting conditions or with other distractor objects present for this requirement. Using this detected object position, we transfer pick-and-place or reaching tasks learned in a simulation environment to an actual physical robot without re-training.



### Real-time Semantic Segmentation of Crop and Weed for Precision Agriculture Robots Leveraging Background Knowledge in CNNs
- **Arxiv ID**: http://arxiv.org/abs/1709.06764v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.06764v2)
- **Published**: 2017-09-20 08:24:18+00:00
- **Updated**: 2018-03-02 15:46:48+00:00
- **Authors**: Andres Milioto, Philipp Lottes, Cyrill Stachniss
- **Comment**: Accepted for publication at IEEE International Conference on Robotics
  and Automation 2018 (ICRA 2018)
- **Journal**: None
- **Summary**: Precision farming robots, which target to reduce the amount of herbicides that need to be brought out in the fields, must have the ability to identify crops and weeds in real time to trigger weeding actions. In this paper, we address the problem of CNN-based semantic segmentation of crop fields separating sugar beet plants, weeds, and background solely based on RGB data. We propose a CNN that exploits existing vegetation indexes and provides a classification in real time. Furthermore, it can be effectively re-trained to so far unseen fields with a comparably small amount of training data. We implemented and thoroughly evaluated our system on a real agricultural robot operating in different fields in Germany and Switzerland. The results show that our system generalizes well, can operate at around 20Hz, and is suitable for online operation in the fields.



### Latent Embeddings for Collective Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1709.06770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06770v1)
- **Published**: 2017-09-20 08:46:53+00:00
- **Updated**: 2017-09-20 08:46:53+00:00
- **Authors**: Yongyi Tang, Peizhen Zhang, Jian-Fang Hu, Wei-Shi Zheng
- **Comment**: 6pages, accepted by IEEE-AVSS2017
- **Journal**: None
- **Summary**: Rather than simply recognizing the action of a person individually, collective activity recognition aims to find out what a group of people is acting in a collective scene. Previ- ous state-of-the-art methods using hand-crafted potentials in conventional graphical model which can only define a limited range of relations. Thus, the complex structural de- pendencies among individuals involved in a collective sce- nario cannot be fully modeled. In this paper, we overcome these limitations by embedding latent variables into feature space and learning the feature mapping functions in a deep learning framework. The embeddings of latent variables build a global relation containing person-group interac- tions and richer contextual information by jointly modeling broader range of individuals. Besides, we assemble atten- tion mechanism during embedding for achieving more com- pact representations. We evaluate our method on three col- lective activity datasets, where we contribute a much larger dataset in this work. The proposed model has achieved clearly better performance as compared to the state-of-the- art methods in our experiments.



### Updating the silent speech challenge benchmark with deep learning
- **Arxiv ID**: http://arxiv.org/abs/1709.06818v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1709.06818v1)
- **Published**: 2017-09-20 11:28:40+00:00
- **Updated**: 2017-09-20 11:28:40+00:00
- **Authors**: Yan Ji, Licheng Liu, Hongcui Wang, Zhilei Liu, Zhibin Niu, Bruce Denby
- **Comment**: 25 pages, 6 pages
- **Journal**: None
- **Summary**: The 2010 Silent Speech Challenge benchmark is updated with new results obtained in a Deep Learning strategy, using the same input features and decoding strategy as in the original article. A Word Error Rate of 6.4% is obtained, compared to the published value of 17.4%. Additional results comparing new auto-encoder-based features with the original features at reduced dimensionality, as well as decoding scenarios on two different language models, are also presented. The Silent Speech Challenge archive has been updated to contain both the original and the new auto-encoder features, in addition to the original raw data.



### UnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1709.06841v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06841v2)
- **Published**: 2017-09-20 12:54:26+00:00
- **Updated**: 2018-02-21 14:44:30+00:00
- **Authors**: Ruihao Li, Sen Wang, Zhiqiang Long, Dongbing Gu
- **Comment**: 6 pages, 6 figures, Accepted by ICRA18. Video:
  (https://www.youtube.com/watch?v=5RdjO93wJqo) Website:
  (http://senwang.gitlab.io/UnDeepVO/)
- **Journal**: None
- **Summary**: We propose a novel monocular visual odometry (VO) system called UnDeepVO in this paper. UnDeepVO is able to estimate the 6-DoF pose of a monocular camera and the depth of its view by using deep neural networks. There are two salient features of the proposed UnDeepVO: one is the unsupervised deep learning scheme, and the other is the absolute scale recovery. Specifically, we train UnDeepVO by using stereo image pairs to recover the scale but test it by using consecutive monocular images. Thus, UnDeepVO is a monocular system. The loss function defined for training the networks is based on spatial and temporal dense information. A system overview is shown in Fig. 1. The experiments on KITTI dataset show our UnDeepVO achieves good performance in terms of pose accuracy.



### Learning quadrangulated patches for 3D shape parameterization and completion
- **Arxiv ID**: http://arxiv.org/abs/1709.06868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06868v1)
- **Published**: 2017-09-20 13:56:34+00:00
- **Updated**: 2017-09-20 13:56:34+00:00
- **Authors**: Kripasindhu Sarkar, Kiran Varanasi, Didier Stricker
- **Comment**: To be presented at International Conference on 3D Vision 2017, 2017
- **Journal**: None
- **Summary**: We propose a novel 3D shape parameterization by surface patches, that are oriented by 3D mesh quadrangulation of the shape. By encoding 3D surface detail on local patches, we learn a patch dictionary that identifies principal surface features of the shape. Unlike previous methods, we are able to encode surface patches of variable size as determined by the user. We propose novel methods for dictionary learning and patch reconstruction based on the query of a noisy input patch with holes. We evaluate the patch dictionary towards various applications in 3D shape inpainting, denoising and compression. Our method is able to predict missing vertices and inpaint moderately sized holes. We demonstrate a complete pipeline for reconstructing the 3D mesh from the patch encoding. We validate our shape parameterization and reconstruction methods on both synthetic shapes and real world scans. We show that our patch dictionary performs successful shape completion of complicated surface textures.



### Open Source Dataset and Deep Learning Models for Online Digit Gesture Recognition on Touchscreens
- **Arxiv ID**: http://arxiv.org/abs/1709.06871v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1709.06871v1)
- **Published**: 2017-09-20 14:02:55+00:00
- **Updated**: 2017-09-20 14:02:55+00:00
- **Authors**: Philip J. Corr, Guenole C. Silvestre, Chris J. Bleakley
- **Comment**: Irish Machine Vision and Image Processing Conference (IMVIP) 2017,
  Maynooth, Ireland, 30 August-1 September 2017
- **Journal**: None
- **Summary**: This paper presents an evaluation of deep neural networks for recognition of digits entered by users on a smartphone touchscreen. A new large dataset of Arabic numerals was collected for training and evaluation of the network. The dataset consists of spatial and temporal touch data recorded for 80 digits entered by 260 users. Two neural network models were investigated. The first model was a 2D convolutional neural (ConvNet) network applied to bitmaps of the glpyhs created by interpolation of the sensed screen touches and its topology is similar to that of previously published models for offline handwriting recognition from scanned images. The second model used a 1D ConvNet architecture but was applied to the sequence of polar vectors connecting the touch points. The models were found to provide accuracies of 98.50% and 95.86%, respectively. The second model was much simpler, providing a reduction in the number of parameters from 1,663,370 to 287,690. The dataset has been made available to the community as an open source resource.



### Multi-camera Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1709.07065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.07065v1)
- **Published**: 2017-09-20 20:08:16+00:00
- **Updated**: 2017-09-20 20:08:16+00:00
- **Authors**: Wenqian Liu, Octavia Camps, Mario Sznaier
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a pipeline for multi-target visual tracking under multi-camera system. For multi-camera system tracking problem, efficient data association across cameras, and at the same time, across frames becomes more important than single-camera system tracking. However, most of the multi-camera tracking algorithms emphasis on single camera across frame data association. Thus in our work, we model our tracking problem as a global graph, and adopt Generalized Maximum Multi Clique optimization problem as our core algorithm to take both across frame and across camera data correlation into account all together. Furthermore, in order to compute good similarity scores as the input of our graph model, we extract both appearance and dynamic motion similarities. For appearance feature, Local Maximal Occurrence Representation(LOMO) feature extraction algorithm for ReID is conducted. When it comes to capturing the dynamic information, we build Hankel matrix for each tracklet of target and apply rank estimation with Iterative Hankel Total Least Squares(IHTLS) algorithm to it. We evaluate our tracker on the challenging Terrace Sequences from EPFL CVLAB as well as recently published Duke MTMC dataset.



### Estimated Depth Map Helps Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1709.07077v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.07077v1)
- **Published**: 2017-09-20 20:39:47+00:00
- **Updated**: 2017-09-20 20:39:47+00:00
- **Authors**: Yihui He
- **Comment**: None
- **Journal**: None
- **Summary**: We consider image classification with estimated depth. This problem falls into the domain of transfer learning, since we are using a model trained on a set of depth images to generate depth maps (additional features) for use in another classification problem using another disjoint set of images. It's challenging as no direct depth information is provided. Though depth estimation has been well studied, none have attempted to aid image classification with estimated depth. Therefore, we present a way of transferring domain knowledge on depth estimation to a separate image classification task over a disjoint set of train, and test data. We build a RGBD dataset based on RGB dataset and do image classification on it. Then evaluation the performance of neural networks on the RGBD dataset compared to the RGB dataset. From our experiments, the benefit is significant with shallow and deep networks. It improves ResNet-20 by 0.55% and ResNet-56 by 0.53%. Our code and dataset are available publicly.



