# Arxiv Papers in cs.CV on 2017-09-16
### Scene-centric Joint Parsing of Cross-view Videos
- **Arxiv ID**: http://arxiv.org/abs/1709.05436v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1709.05436v3)
- **Published**: 2017-09-16 00:21:29+00:00
- **Updated**: 2018-02-05 05:59:11+00:00
- **Authors**: Hang Qi, Yuanlu Xu, Tao Yuan, Tianfu Wu, Song-Chun Zhu
- **Comment**: Accepted by AAAI 2018
- **Journal**: None
- **Summary**: Cross-view video understanding is an important yet under-explored area in computer vision. In this paper, we introduce a joint parsing framework that integrates view-centric proposals into scene-centric parse graphs that represent a coherent scene-centric understanding of cross-view scenes. Our key observations are that overlapping fields of views embed rich appearance and geometry correlations and that knowledge fragments corresponding to individual vision tasks are governed by consistency constraints available in commonsense knowledge. The proposed joint parsing framework represents such correlations and constraints explicitly and generates semantic scene-centric parse graphs. Quantitative experiments show that scene-centric predictions in the parse graph outperform view-centric predictions.



### A Causal And-Or Graph Model for Visibility Fluent Reasoning in Tracking Interacting Objects
- **Arxiv ID**: http://arxiv.org/abs/1709.05437v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1709.05437v2)
- **Published**: 2017-09-16 00:21:44+00:00
- **Updated**: 2018-03-29 02:05:45+00:00
- **Authors**: Yuanlu Xu, Lei Qin, Xiaobai Liu, Jianwen Xie, Song-Chun Zhu
- **Comment**: accepted by CVPR 2018
- **Journal**: None
- **Summary**: Tracking humans that are interacting with the other subjects or environment remains unsolved in visual tracking, because the visibility of the human of interests in videos is unknown and might vary over time. In particular, it is still difficult for state-of-the-art human trackers to recover complete human trajectories in crowded scenes with frequent human interactions. In this work, we consider the visibility status of a subject as a fluent variable, whose change is mostly attributed to the subject's interaction with the surrounding, e.g., crossing behind another object, entering a building, or getting into a vehicle, etc. We introduce a Causal And-Or Graph (C-AOG) to represent the causal-effect relations between an object's visibility fluent and its activities, and develop a probabilistic graph model to jointly reason the visibility fluent change (e.g., from visible to invisible) and track humans in videos. We formulate this joint task as an iterative search of a feasible causal graph structure that enables fast search algorithm, e.g., dynamic programming method. We apply the proposed method on challenging video sequences to evaluate its capabilities of estimating visibility fluent changes of subjects and tracking subjects of interests over time. Results with comparisons demonstrate that our method outperforms the alternative trackers and can recover complete trajectories of humans in complicated scenarios with frequent human interactions.



### To Go or Not To Go? A Near Unsupervised Learning Approach For Robot Navigation
- **Arxiv ID**: http://arxiv.org/abs/1709.05439v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.05439v1)
- **Published**: 2017-09-16 00:39:19+00:00
- **Updated**: 2017-09-16 00:39:19+00:00
- **Authors**: Noriaki Hirose, Amir Sadeghian, Patrick Goebel, Silvio Savarese
- **Comment**: Noriaki Hirose and Amir Sadeghian contributed equally
- **Journal**: None
- **Summary**: It is important for robots to be able to decide whether they can go through a space or not, as they navigate through a dynamic environment. This capability can help them avoid injury or serious damage, e.g., as a result of running into people and obstacles, getting stuck, or falling off an edge. To this end, we propose an unsupervised and a near-unsupervised method based on Generative Adversarial Networks (GAN) to classify scenarios as traversable or not based on visual data. Our method is inspired by the recent success of data-driven approaches on computer vision problems and anomaly detection, and reduces the need for vast amounts of negative examples at training time. Collecting negative data indicating that a robot should not go through a space is typically hard and dangerous because of collisions, whereas collecting positive data can be automated and done safely based on the robot's own traveling experience. We verify the generality and effectiveness of the proposed approach on a test dataset collected in a previously unseen environment with a mobile robot. Furthermore, we show that our method can be used to build costmaps (we call as "GoNoGo" costmaps) for robot path planning using visual data only.



### A LBP Based Correspondence Identification Scheme for Multi-view Sensing Network
- **Arxiv ID**: http://arxiv.org/abs/1709.06509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06509v1)
- **Published**: 2017-09-16 00:49:44+00:00
- **Updated**: 2017-09-16 00:49:44+00:00
- **Authors**: Raghavendra Kandukuri
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we describes a correspondence identification method between two-views of regular RGB camera that can be run in real-time. The basic idea is first applying normalized cross correlation to retrieve a sparse set of matching pairs from image pair. Then loopy belief propagation scheme is applied to the the set of possible candidates to densely identify correspondences from different views. The experiment results demonstrate superb accuracy and precision that outperform the state-of-the-art in the computer vision field. Meanwhile, the implementation is simple enough that can be optimized for real-time performance. We have given the detailed comparison of existing approaches and show that this method can enable various practical applications from 3D reconstruction to image search.



### Face Retrieval using Frequency Decoded Local Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1709.06508v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06508v2)
- **Published**: 2017-09-16 06:43:59+00:00
- **Updated**: 2018-12-04 02:47:28+00:00
- **Authors**: Shiv Ram Dubey
- **Comment**: Accepted in Multimedia Tools and Applications, Springer
- **Journal**: None
- **Summary**: The local descriptors have been the backbone of most of the computer vision problems. Most of the existing local descriptors are generated over the raw input images. In order to increase the discriminative power of the local descriptors, some researchers converted the raw image into multiple images with the help of some high and low pass frequency filters, then the local descriptors are computed over each filtered image and finally concatenated into a single descriptor. By doing so, these approaches do not utilize the inter frequency relationship which causes the less improvement in the discriminative power of the descriptor that could be achieved. In this paper, this problem is solved by utilizing the decoder concept of multi-channel decoded local binary pattern over the multi-frequency patterns. A frequency decoded local binary pattern (FDLBP) is proposed with two decoders. Each decoder works with one low frequency pattern and two high frequency patterns. Finally, the descriptors from both decoders are concatenated to form the single descriptor. The face retrieval experiments are conducted over four benchmarks and challenging databases such as PaSC, LFW, PubFig, and ESSEX. The experimental results confirm the superiority of the FDLBP descriptor as compared to the state-of-the-art descriptors such as LBP, SOBEL_LBP, BoF_LBP, SVD_S_LBP, mdLBP, etc.



### Long-Term Ensemble Learning of Visual Place Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1709.05470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05470v1)
- **Published**: 2017-09-16 06:52:20+00:00
- **Updated**: 2017-09-16 06:52:20+00:00
- **Authors**: Xiaoxiao Fei, Kanji Tanaka, Yichu Fang, Akitaka Takayama
- **Comment**: 8 pages, 9 figures, technical report
- **Journal**: None
- **Summary**: This paper addresses the problem of cross-season visual place classification (VPC) from a novel perspective of long-term map learning. Our goal is to enable transfer learning efficiently from one season to the next, at a small constant cost, and without wasting the robot's available long-term-memory by memorizing very large amounts of training data. To realize a good tradeoff between generalization and specialization abilities, we employ an ensemble of convolutional neural network (DCN) classifiers and consider the task of scheduling (when and which classifiers to retrain), given a previous season's DCN classifiers as the sole prior knowledge. We present a unified framework for retraining scheduling and discuss practical implementation strategies. Furthermore, we address the task of partitioning a robot's workspace into places to define place classes in an unsupervised manner, rather than using uniform partitioning, so as to maximize VPC performance. Experiments using the publicly available NCLT dataset revealed that retraining scheduling of a DCN classifier ensemble is crucial and performance is significantly increased by using planned scheduling.



### The Multiscale Bowler-Hat Transform for Blood Vessel Enhancement in Retinal Images
- **Arxiv ID**: http://arxiv.org/abs/1709.05495v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05495v3)
- **Published**: 2017-09-16 11:00:33+00:00
- **Updated**: 2018-03-28 12:51:10+00:00
- **Authors**: Çiğdem Sazak, Carl J. Nelson, Boguslaw Obara
- **Comment**: None
- **Journal**: None
- **Summary**: Enhancement, followed by segmentation, quantification and modelling, of blood vessels in retinal images plays an essential role in computer-aid retinopathy diagnosis. In this paper, we introduce a new vessel enhancement method which is the bowler-hat transform based on mathematical morphology. The proposed method combines different structuring elements to detect innate features of vessel-like structures. We evaluate the proposed method qualitatively and quantitatively, and compare it with the existing, state-of-the-art methods using both synthetic and real datasets. Our results show that the proposed method achieves high-quality vessel-like structure enhancement in both synthetic examples and in clinically relevant retinal images, and is shown to be able to detect fine vessels while remaining robust at junctions.



### DeepLung: 3D Deep Convolutional Nets for Automated Pulmonary Nodule Detection and Classification
- **Arxiv ID**: http://arxiv.org/abs/1709.05538v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1709.05538v1)
- **Published**: 2017-09-16 16:18:22+00:00
- **Updated**: 2017-09-16 16:18:22+00:00
- **Authors**: Wentao Zhu, Chaochun Liu, Wei Fan, Xiaohui Xie
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a fully automated lung CT cancer diagnosis system, DeepLung. DeepLung contains two parts, nodule detection and classification. Considering the 3D nature of lung CT data, two 3D networks are designed for the nodule detection and classification respectively. Specifically, a 3D Faster R-CNN is designed for nodule detection with a U-net-like encoder-decoder structure to effectively learn nodule features. For nodule classification, gradient boosting machine (GBM) with 3D dual path network (DPN) features is proposed. The nodule classification subnetwork is validated on a public dataset from LIDC-IDRI, on which it achieves better performance than state-of-the-art approaches, and surpasses the average performance of four experienced doctors. For the DeepLung system, candidate nodules are detected first by the nodule detection subnetwork, and nodule diagnosis is conducted by the classification subnetwork. Extensive experimental results demonstrate the DeepLung is comparable to the experienced doctors both for the nodule-level and patient-level diagnosis on the LIDC-IDRI dataset.



