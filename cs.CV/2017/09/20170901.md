# Arxiv Papers in cs.CV on 2017-09-01
### Single Shot Text Detector with Regional Attention
- **Arxiv ID**: http://arxiv.org/abs/1709.00138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00138v1)
- **Published**: 2017-09-01 02:42:57+00:00
- **Updated**: 2017-09-01 02:42:57+00:00
- **Authors**: Pan He, Weilin Huang, Tong He, Qile Zhu, Yu Qiao, Xiaolin Li
- **Comment**: To appear in IEEE International Conference on Computer Vision (ICCV),
  2017
- **Journal**: None
- **Summary**: We present a novel single-shot text detector that directly outputs word-level bounding boxes in a natural image. We propose an attention mechanism which roughly identifies text regions via an automatically learned attentional map. This substantially suppresses background interference in the convolutional features, which is the key to producing accurate inference of words, particularly at extremely small sizes. This results in a single model that essentially works in a coarse-to-fine manner. It departs from recent FCN- based text detectors which cascade multiple FCN models to achieve an accurate prediction. Furthermore, we develop a hierarchical inception module which efficiently aggregates multi-scale inception features. This enhances local details, and also encodes strong context information, allow- ing the detector to work reliably on multi-scale and multi- orientation text with single-scale images. Our text detector achieves an F-measure of 77% on the ICDAR 2015 bench- mark, advancing the state-of-the-art results in [18, 28]. Demo is available at: http://sstd.whuang.org/.



### Context Based Visual Content Verification
- **Arxiv ID**: http://arxiv.org/abs/1709.00141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00141v1)
- **Published**: 2017-09-01 02:52:46+00:00
- **Updated**: 2017-09-01 02:52:46+00:00
- **Authors**: Martin Lukac, Aigerim Bazarbayeva, Michitaka Kameyama
- **Comment**: 6 pages, 6 Figures, Published in Proceedings of the Information and
  Digital Technology Conference, 2017
- **Journal**: None
- **Summary**: In this paper the intermediary visual content verification method based on multi-level co-occurrences is studied. The co-occurrence statistics are in general used to determine relational properties between objects based on information collected from data. As such these measures are heavily subject to relative number of occurrences and give only limited amount of accuracy when predicting objects in real world. In order to improve the accuracy of this method in the verification task, we include the context information such as location, type of environment etc. In order to train our model we provide new annotated dataset the Advanced Attribute VOC (AAVOC) that contains additional properties of the image. We show that the usage of context greatly improve the accuracy of verification with up to 16% improvement.



### Towards a Dedicated Computer Vision Tool set for Crowd Simulation Models
- **Arxiv ID**: http://arxiv.org/abs/1709.02243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02243v1)
- **Published**: 2017-09-01 04:02:36+00:00
- **Updated**: 2017-09-01 04:02:36+00:00
- **Authors**: Sultan Daud Khan, Muhammad Saqib, Michael Blumenstein
- **Comment**: None
- **Journal**: None
- **Summary**: As the population of world is increasing, and even more concentrated in urban areas, ensuring public safety is becoming a taunting job for security personnel and crowd managers. Mass events like sports, festivals, concerts, political gatherings attract thousand of people in a constraint environment,therefore adequate safety measures should be adopted. Despite safety measures, crowd disasters still occur frequently. Understanding underlying dynamics and behavior of crowd is becoming areas of interest for most of computer scientists. In recent years, researchers developed several models for understanding crowd dynamics. These models should be properly calibrated and validated by means of data acquired in the field. In this paper, we developed a computer vision tool set that can be helpful not only in initializing the crowd simulation models but can also validate the simulation results. The main features of proposed tool set are: (1) Crowd flow segmentation and crowd counting, (2) Identifying source/sink location for understanding crowd behavior, (3) Group detection and tracking in crowds.



### Reasoning with shapes: profiting cognitive susceptibilities to infer linear mapping transformations between shapes
- **Arxiv ID**: http://arxiv.org/abs/1709.00158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00158v1)
- **Published**: 2017-09-01 05:08:19+00:00
- **Updated**: 2017-09-01 05:08:19+00:00
- **Authors**: Vahid Jalili
- **Comment**: None
- **Journal**: None
- **Summary**: Visual information plays an indispensable role in our daily interactions with environment. Such information is manipulated for a wide range of purposes spanning from basic object and material perception to complex gesture interpretations. There have been novel studies in cognitive science for in-depth understanding of visual information manipulation, which lead to answer questions such as: how we infer 2D/3D motion from a sequence of 2D images? how we understand a motion from a single image frame? how we see forest avoiding trees?   Leveraging on congruence, linear mapping transformation determination between a set of shapes facilitate motion perception. Present study methodizes recent discoveries of human cognitive ability for scene understanding. The proposed method processes images hierarchically, that is an iterative analysis of scene abstractions using a rapidly converging heuristic iterative method. The method hierarchically abstracts images; the abstractions are represented in polar coordinate system, and any two consecutive abstractions have incremental level of details. The method then creates a graph of approximated linear mapping transformations based on circular shift permutations of hierarchical abstractions. The graph is then traversed in best-first fashion to find best linear mapping transformation. The accuracy of the proposed method is assessed using normal, noisy, and deformed images. Additionally, the present study deduces (i) the possibility of determining optimal mapping linear transformations in logarithmic iterations with respect to the precision of results, and (ii) computational cost is independent from the resolution of input shapes.



### Effective Use of Dilated Convolutions for Segmenting Small Object Instances in Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/1709.00179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00179v1)
- **Published**: 2017-09-01 07:20:01+00:00
- **Updated**: 2017-09-01 07:20:01+00:00
- **Authors**: Ryuhei Hamaguchi, Aito Fujita, Keisuke Nemoto, Tomoyuki Imaizumi, Shuhei Hikosaka
- **Comment**: None
- **Journal**: None
- **Summary**: Thanks to recent advances in CNNs, solid improvements have been made in semantic segmentation of high resolution remote sensing imagery. However, most of the previous works have not fully taken into account the specific difficulties that exist in remote sensing tasks. One of such difficulties is that objects are small and crowded in remote sensing imagery. To tackle with this challenging task we have proposed a novel architecture called local feature extraction (LFE) module attached on top of dilated front-end module. The LFE module is based on our findings that aggressively increasing dilation factors fails to aggregate local features due to sparsity of the kernel, and detrimental to small objects. The proposed LFE module solves this problem by aggregating local features with decreasing dilation factor. We tested our network on three remote sensing datasets and acquired remarkably good results for all datasets especially for small objects.



### Weighted Low-rank Tensor Recovery for Hyperspectral Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1709.00192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00192v1)
- **Published**: 2017-09-01 07:58:34+00:00
- **Updated**: 2017-09-01 07:58:34+00:00
- **Authors**: Yi Chang, Luxin Yan, Houzhang Fang, Sheng Zhong, Zhijun Zhang
- **Comment**: 22 pages, 22 figures
- **Journal**: None
- **Summary**: Hyperspectral imaging, providing abundant spatial and spectral information simultaneously, has attracted a lot of interest in recent years. Unfortunately, due to the hardware limitations, the hyperspectral image (HSI) is vulnerable to various degradations, such noises (random noise, HSI denoising), blurs (Gaussian and uniform blur, HSI deblurring), and down-sampled (both spectral and spatial downsample, HSI super-resolution). Previous HSI restoration methods are designed for one specific task only. Besides, most of them start from the 1-D vector or 2-D matrix models and cannot fully exploit the structurally spectral-spatial correlation in 3-D HSI. To overcome these limitations, in this work, we propose a unified low-rank tensor recovery model for comprehensive HSI restoration tasks, in which non-local similarity between spectral-spatial cubic and spectral correlation are simultaneously captured by 3-order tensors. Further, to improve the capability and flexibility, we formulate it as a weighted low-rank tensor recovery (WLRTR) model by treating the singular values differently, and study its analytical solution. We also consider the exclusive stripe noise in HSI as the gross error by extending WLRTR to robust principal component analysis (WLRTR-RPCA). Extensive experiments demonstrate the proposed WLRTR models consistently outperform state-of-the-arts in typical low level vision HSI tasks, including denoising, destriping, deblurring and super-resolution.



### DeepUNet: A Deep Fully Convolutional Network for Pixel-level Sea-Land Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1709.00201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00201v1)
- **Published**: 2017-09-01 08:54:12+00:00
- **Updated**: 2017-09-01 08:54:12+00:00
- **Authors**: Ruirui Li, Wenjie Liu, Lei Yang, Shihao Sun, Wei Hu, Fan Zhang, Wei Li
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is a fundamental research in remote sensing image processing. Because of the complex maritime environment, the sea-land segmentation is a challenging task. Although the neural network has achieved excellent performance in semantic segmentation in the last years, there are a few of works using CNN for sea-land segmentation and the results could be further improved. This paper proposes a novel deep convolution neural network named DeepUNet. Like the U-Net, its structure has a contracting path and an expansive path to get high resolution output. But differently, the DeepUNet uses DownBlocks instead of convolution layers in the contracting path and uses UpBlock in the expansive path. The two novel blocks bring two new connections that are U-connection and Plus connection. They are promoted to get more precise segmentation results. To verify our network architecture, we made a new challenging sea-land dataset and compare the DeepUNet on it with the SegNet and the U-Net. Experimental results show that DeepUNet achieved good performance compared with other architectures, especially in high-resolution remote sensing imagery.



### Too Far to See? Not Really! --- Pedestrian Detection with Scale-aware Localization Policy
- **Arxiv ID**: http://arxiv.org/abs/1709.00235v1
- **DOI**: 10.1109/TIP.2018.2818018
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00235v1)
- **Published**: 2017-09-01 10:32:09+00:00
- **Updated**: 2017-09-01 10:32:09+00:00
- **Authors**: Xiaowei Zhang, Li Cheng, Bo Li, Hai-Miao Hu
- **Comment**: None
- **Journal**: None
- **Summary**: A major bottleneck of pedestrian detection lies on the sharp performance deterioration in the presence of small-size pedestrians that are relatively far from the camera. Motivated by the observation that pedestrians of disparate spatial scales exhibit distinct visual appearances, we propose in this paper an active pedestrian detector that explicitly operates over multiple-layer neuronal representations of the input still image. More specifically, convolutional neural nets such as ResNet and faster R-CNNs are exploited to provide a rich and discriminative hierarchy of feature representations as well as initial pedestrian proposals. Here each pedestrian observation of distinct size could be best characterized in terms of the ResNet feature representation at a certain layer of the hierarchy; Meanwhile, initial pedestrian proposals are attained by faster R-CNNs techniques, i.e. region proposal network and follow-up region of interesting pooling layer employed right after the specific ResNet convolutional layer of interest, to produce joint predictions on the bounding-box proposals' locations and categories (i.e. pedestrian or not). This is engaged as input to our active detector where for each initial pedestrian proposal, a sequence of coordinate transformation actions is carried out to determine its proper x-y 2D location and layer of feature representation, or eventually terminated as being background. Empirically our approach is demonstrated to produce overall lower detection errors on widely-used benchmarks, and it works particularly well with far-scale pedestrians. For example, compared with 60.51% log-average miss rate of the state-of-the-art MS-CNN for far-scale pedestrians (those below 80 pixels in bounding-box height) of the Caltech benchmark, the miss rate of our approach is 41.85%, with a notable reduction of 18.68%.



### Adversarial Networks for Spatial Context-Aware Spectral Image Reconstruction from RGB
- **Arxiv ID**: http://arxiv.org/abs/1709.00265v2
- **DOI**: 10.1109/ICCVW.2017.64
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00265v2)
- **Published**: 2017-09-01 12:00:51+00:00
- **Updated**: 2018-03-14 14:48:14+00:00
- **Authors**: Aitor Alvarez-Gila, Joost van de Weijer, Estibaliz Garrote
- **Comment**: Accepted at IEEE ICCVW 2017 - "Physics Based Vision meets Deep
  Learning" Workshop (PBDL 2017). Added train-test splits and updated results
- **Journal**: None
- **Summary**: Hyperspectral signal reconstruction aims at recovering the original spectral input that produced a certain trichromatic (RGB) response from a capturing device or observer. Given the heavily underconstrained, non-linear nature of the problem, traditional techniques leverage different statistical properties of the spectral signal in order to build informative priors from real world object reflectances for constructing such RGB to spectral signal mapping. However, most of them treat each sample independently, and thus do not benefit from the contextual information that the spatial dimensions can provide. We pose hyperspectral natural image reconstruction as an image to image mapping learning problem, and apply a conditional generative adversarial framework to help capture spatial semantics. This is the first time Convolutional Neural Networks -and, particularly, Generative Adversarial Networks- are used to solve this task. Quantitative evaluation shows a Root Mean Squared Error (RMSE) drop of 33.2% and a Relative RMSE drop of 54.0% on the ICVL natural hyperspectral image dataset.



### Telepath: Understanding Users from a Human Vision Perspective in Large-Scale Recommender Systems
- **Arxiv ID**: http://arxiv.org/abs/1709.00300v2
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.00300v2)
- **Published**: 2017-09-01 13:29:36+00:00
- **Updated**: 2017-09-04 16:06:57+00:00
- **Authors**: Yu Wang, Jixing Xu, Aohan Wu, Mantian Li, Yang He, Jinghe Hu, Weipeng P. Yan
- **Comment**: 8 pages, 11 figures, 1 table
- **Journal**: None
- **Summary**: Designing an e-commerce recommender system that serves hundreds of millions of active users is a daunting challenge. From a human vision perspective, there're two key factors that affect users' behaviors: items' attractiveness and their matching degree with users' interests. This paper proposes Telepath, a vision-based bionic recommender system model, which understands users from such perspective. Telepath is a combination of a convolutional neural network (CNN), a recurrent neural network (RNN) and deep neural networks (DNNs). Its CNN subnetwork simulates the human vision system to extract key visual signals of items' attractiveness and generate corresponding activations. Its RNN and DNN subnetworks simulate cerebral cortex to understand users' interest based on the activations generated from browsed items. In practice, the Telepath model has been launched to JD's recommender system and advertising system. For one of the major item recommendation blocks on the JD app, click-through rate (CTR), gross merchandise value (GMV) and orders have increased 1.59%, 8.16% and 8.71% respectively. For several major ads publishers of JD demand-side platform, CTR, GMV and return on investment have increased 6.58%, 61.72% and 65.57% respectively by the first launch, and further increased 2.95%, 41.75% and 41.37% respectively by the second launch.



### A Comprehensive Survey of Deep Learning in Remote Sensing: Theories, Tools and Challenges for the Community
- **Arxiv ID**: http://arxiv.org/abs/1709.00308v2
- **DOI**: 10.1117/1.JRS.11.042609
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00308v2)
- **Published**: 2017-09-01 13:40:35+00:00
- **Updated**: 2017-09-24 06:11:30+00:00
- **Authors**: John E. Ball, Derek T. Anderson, Chee Seng Chan
- **Comment**: 64 pages, 411 references. To appear in Journal of Applied Remote
  Sensing
- **Journal**: J. Appl. Remote Sens. 11(4) (2017) 042609
- **Summary**: In recent years, deep learning (DL), a re-branding of neural networks (NNs), has risen to the top in numerous areas, namely computer vision (CV), speech recognition, natural language processing, etc. Whereas remote sensing (RS) possesses a number of unique challenges, primarily related to sensors and applications, inevitably RS draws from many of the same theories as CV; e.g., statistics, fusion, and machine learning, to name a few. This means that the RS community should be aware of, if not at the leading edge of, of advancements like DL. Herein, we provide the most comprehensive survey of state-of-the-art RS DL research. We also review recent new developments in the DL field that can be used in DL for RS. Namely, we focus on theories, tools and challenges for the RS community. Specifically, we focus on unsolved challenges and opportunities as it relates to (i) inadequate data sets, (ii) human-understandable solutions for modelling physical phenomena, (iii) Big Data, (iv) non-traditional heterogeneous data sources, (v) DL architectures and learning algorithms for spectral, spatial and temporal data, (vi) transfer learning, (vii) an improved theoretical understanding of DL systems, (viii) high barriers to entry, and (ix) training and optimizing the DL.



### Automatic Brain Tumor Segmentation using Cascaded Anisotropic Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.00382v2
- **DOI**: 10.1007/978-3-319-75238-9_16
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00382v2)
- **Published**: 2017-09-01 16:11:34+00:00
- **Updated**: 2017-12-15 09:15:00+00:00
- **Authors**: Guotai Wang, Wenqi Li, Sebastien Ourselin, Tom Vercauteren
- **Comment**: 12 pages, 5 figures. MICCAI Brats Challenge 2017
- **Journal**: None
- **Summary**: A cascade of fully convolutional neural networks is proposed to segment multi-modal Magnetic Resonance (MR) images with brain tumor into background and three hierarchical regions: whole tumor, tumor core and enhancing tumor core. The cascade is designed to decompose the multi-class segmentation problem into a sequence of three binary segmentation problems according to the subregion hierarchy. The whole tumor is segmented in the first step and the bounding box of the result is used for the tumor core segmentation in the second step. The enhancing tumor core is then segmented based on the bounding box of the tumor core segmentation result. Our networks consist of multiple layers of anisotropic and dilated convolution filters, and they are combined with multi-view fusion to reduce false positives. Residual connections and multi-scale predictions are employed in these networks to boost the segmentation performance. Experiments with BraTS 2017 validation set show that the proposed method achieved average Dice scores of 0.7859, 0.9050, 0.8378 for enhancing tumor core, whole tumor and tumor core, respectively. The corresponding values for BraTS 2017 testing set were 0.7831, 0.8739, and 0.7748, respectively.



### End-to-End Multi-View Lipreading
- **Arxiv ID**: http://arxiv.org/abs/1709.00443v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00443v1)
- **Published**: 2017-09-01 18:51:53+00:00
- **Updated**: 2017-09-01 18:51:53+00:00
- **Authors**: Stavros Petridis, Yujiang Wang, Zuwei Li, Maja Pantic
- **Comment**: Accepted to BMVC 2017
- **Journal**: None
- **Summary**: Non-frontal lip views contain useful information which can be used to enhance the performance of frontal view lipreading. However, the vast majority of recent lipreading works, including the deep learning approaches which significantly outperform traditional approaches, have focused on frontal mouth images. As a consequence, research on joint learning of visual features and speech classification from multiple views is limited. In this work, we present an end-to-end multi-view lipreading system based on Bidirectional Long-Short Memory (BLSTM) networks. To the best of our knowledge, this is the first model which simultaneously learns to extract features directly from the pixels and performs visual speech classification from multiple views and also achieves state-of-the-art performance. The model consists of multiple identical streams, one for each view, which extract features directly from different poses of mouth images. The temporal dynamics in each stream/view are modelled by a BLSTM and the fusion of multiple streams/views takes place via another BLSTM. An absolute average improvement of 3% and 3.8% over the frontal view performance is reported on the OuluVS2 database when the best two (frontal and profile) and three views (frontal, profile, 45) are combined, respectively. The best three-view model results in a 10.5% absolute improvement over the current multi-view state-of-the-art performance on OuluVS2, without using external databases for training, achieving a maximum classification accuracy of 96.9%.



### Iteratively Linearized Reweighted Alternating Direction Method of Multipliers for a Class of Nonconvex Problems
- **Arxiv ID**: http://arxiv.org/abs/1709.00483v5
- **DOI**: 10.1109/TSP.2018.2868269
- **Categories**: **cs.NA**, cs.CV, math.NA, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1709.00483v5)
- **Published**: 2017-09-01 21:20:30+00:00
- **Updated**: 2018-03-22 23:26:33+00:00
- **Authors**: Tao Sun, Hao Jiang, Lizhi Cheng, Wei Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider solving a class of nonconvex and nonsmooth problems frequently appearing in signal processing and machine learning research. The traditional alternating direction method of multipliers encounters troubles in both mathematics and computations in solving the nonconvex and nonsmooth subproblem. In view of this, we propose a reweighted alternating direction method of multipliers. In this algorithm, all subproblems are convex and easy to solve. We also provide several guarantees for the convergence and prove that the algorithm globally converges to a critical point of an auxiliary function with the help of the Kurdyka-{\L}ojasiewicz property. Several numerical results are presented to demonstrate the efficiency of the proposed algorithm.



### ShapeCodes: Self-Supervised Feature Learning by Lifting Views to Viewgrids
- **Arxiv ID**: http://arxiv.org/abs/1709.00505v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00505v4)
- **Published**: 2017-09-01 23:15:28+00:00
- **Updated**: 2018-07-31 03:02:06+00:00
- **Authors**: Dinesh Jayaraman, Ruohan Gao, Kristen Grauman
- **Comment**: To appear at ECCV 2018
- **Journal**: None
- **Summary**: We introduce an unsupervised feature learning approach that embeds 3D shape information into a single-view image representation. The main idea is a self-supervised training objective that, given only a single 2D image, requires all unseen views of the object to be predictable from learned features. We implement this idea as an encoder-decoder convolutional neural network. The network maps an input image of an unknown category and unknown viewpoint to a latent space, from which a deconvolutional decoder can best "lift" the image to its complete viewgrid showing the object from all viewing angles. Our class-agnostic training procedure encourages the representation to capture fundamental shape primitives and semantic regularities in a data-driven manner---without manual semantic labels. Our results on two widely-used shape datasets show 1) our approach successfully learns to perform "mental rotation" even for objects unseen during training, and 2) the learned latent space is a powerful representation for object recognition, outperforming several existing unsupervised feature learning methods.



### Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks
- **Arxiv ID**: http://arxiv.org/abs/1709.00507v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00507v2)
- **Published**: 2017-09-01 23:34:29+00:00
- **Updated**: 2017-12-21 17:11:10+00:00
- **Authors**: Dinesh Jayaraman, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: It is common to implicitly assume access to intelligently captured inputs (e.g., photos from a human photographer), yet autonomously capturing good observations is itself a major challenge. We address the problem of learning to look around: if a visual agent has the ability to voluntarily acquire new views to observe its environment, how can it learn efficient exploratory behaviors to acquire informative observations? We propose a reinforcement learning solution, where the agent is rewarded for actions that reduce its uncertainty about the unobserved portions of its environment. Based on this principle, we develop a recurrent neural network-based approach to perform active completion of panoramic natural scenes and 3D object shapes. Crucially, the learned policies are not tied to any recognition task nor to the particular semantic content seen during training. As a result, 1) the learned "look around" behavior is relevant even for new tasks in unseen environments, and 2) training data acquisition involves no manual labeling. Through tests in diverse settings, we demonstrate that our approach learns useful generic policies that transfer to new unseen tasks and environments. Completion episodes are shown at https://goo.gl/BgWX3W.



### A hierarchical loss and its problems when classifying non-hierarchically
- **Arxiv ID**: http://arxiv.org/abs/1709.01062v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1709.01062v2)
- **Published**: 2017-09-01 23:46:59+00:00
- **Updated**: 2019-12-09 20:38:32+00:00
- **Authors**: Cinna Wu, Mark Tygert, Yann LeCun
- **Comment**: 19 pages, 4 figures, 7 tables
- **Journal**: None
- **Summary**: Failing to distinguish between a sheepdog and a skyscraper should be worse and penalized more than failing to distinguish between a sheepdog and a poodle; after all, sheepdogs and poodles are both breeds of dogs. However, existing metrics of failure (so-called "loss" or "win") used in textual or visual classification/recognition via neural networks seldom leverage a-priori information, such as a sheepdog being more similar to a poodle than to a skyscraper. We define a metric that, inter alia, can penalize failure to distinguish between a sheepdog and a skyscraper more than failure to distinguish between a sheepdog and a poodle. Unlike previously employed possibilities, this metric is based on an ultrametric tree associated with any given tree organization into a semantically meaningful hierarchy of a classifier's classes. An ultrametric tree is a tree with a so-called ultrametric distance metric such that all leaves are at the same distance from the root. Unfortunately, extensive numerical experiments indicate that the standard practice of training neural networks via stochastic gradient descent with random starting points often drives down the hierarchical loss nearly as much when minimizing the standard cross-entropy loss as when trying to minimize the hierarchical loss directly. Thus, this hierarchical loss is unreliable as an objective for plain, randomly started stochastic gradient descent to minimize; the main value of the hierarchical loss may be merely as a meaningful metric of success of a classifier.



