# Arxiv Papers in cs.CV on 2017-09-29
### Light Cascaded Convolutional Neural Networks for Accurate Player Detection
- **Arxiv ID**: http://arxiv.org/abs/1709.10230v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.10230v1)
- **Published**: 2017-09-29 03:52:00+00:00
- **Updated**: 2017-09-29 03:52:00+00:00
- **Authors**: Keyu Lu, Jianhui Chen, James J. Little, Hangen He
- **Comment**: Published in proceedings of BMVC 2017
- **Journal**: None
- **Summary**: Vision based player detection is important in sports applications. Accuracy, efficiency, and low memory consumption are desirable for real-time tasks such as intelligent broadcasting and automatic event classification. In this paper, we present a cascaded convolutional neural network (CNN) that satisfies all three of these requirements. Our method first trains a binary (player/non-player) classification network from labeled image patches. Then, our method efficiently applies the network to a whole image in testing. We conducted experiments on basketball and soccer games. Experimental results demonstrate that our method can accurately detect players under challenging conditions such as varying illumination, highly dynamic camera movements and motion blur. Comparing with conventional CNNs, our approach achieves state-of-the-art accuracy on both games with 1000x fewer parameters (i.e., it is light}.



### Deep Competitive Pathway Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.10282v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.10282v1)
- **Published**: 2017-09-29 08:34:05+00:00
- **Updated**: 2017-09-29 08:34:05+00:00
- **Authors**: Jia-Ren Chang, Yong-Sheng Chen
- **Comment**: To appear in ACML17
- **Journal**: None
- **Summary**: In the design of deep neural architectures, recent studies have demonstrated the benefits of grouping subnetworks into a larger network. For examples, the Inception architecture integrates multi-scale subnetworks and the residual network can be regarded that a residual unit combines a residual subnetwork with an identity shortcut. In this work, we embrace this observation and propose the Competitive Pathway Network (CoPaNet). The CoPaNet comprises a stack of competitive pathway units and each unit contains multiple parallel residual-type subnetworks followed by a max operation for feature competition. This mechanism enhances the model capability by learning a variety of features in subnetworks. The proposed strategy explicitly shows that the features propagate through pathways in various routing patterns, which is referred to as pathway encoding of category information. Moreover, the cross-block shortcut can be added to the CoPaNet to encourage feature reuse. We evaluated the proposed CoPaNet on four object recognition benchmarks: CIFAR-10, CIFAR-100, SVHN, and ImageNet. CoPaNet obtained the state-of-the-art or comparable results using similar amounts of parameters. The code of CoPaNet is available at: https://github.com/JiaRenChang/CoPaNet.



### IQ of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.01692v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.01692v1)
- **Published**: 2017-09-29 11:48:58+00:00
- **Updated**: 2017-09-29 11:48:58+00:00
- **Authors**: Dokhyam Hoshen, Michael Werman
- **Comment**: None
- **Journal**: None
- **Summary**: IQ tests are an accepted method for assessing human intelligence. The tests consist of several parts that must be solved under a time constraint. Of all the tested abilities, pattern recognition has been found to have the highest correlation with general intelligence. This is primarily because pattern recognition is the ability to find order in a noisy environment, a necessary skill for intelligent agents. In this paper, we propose a convolutional neural network (CNN) model for solving geometric pattern recognition problems. The CNN receives as input multiple ordered input images and outputs the next image according to the pattern. Our CNN is able to solve problems involving rotation, reflection, color, size and shape patterns and score within the top 5% of human performance.



### A Variational Approach to Shape-from-shading Under Natural Illumination
- **Arxiv ID**: http://arxiv.org/abs/1709.10354v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.10354v2)
- **Published**: 2017-09-29 11:58:13+00:00
- **Updated**: 2017-12-03 19:10:56+00:00
- **Authors**: Yvain Quéau, Jean Mélou, Fabien Castan, Daniel Cremers, Jean-Denis Durou
- **Comment**: Presented at EMMCVPR 2017 conference
- **Journal**: None
- **Summary**: A numerical solution to shape-from-shading under natural illumination is presented. It builds upon an augmented Lagrangian approach for solving a generic PDE-based shape-from-shading model which handles directional or spherical harmonic lighting, orthographic or perspective projection, and greylevel or multi-channel images. Real-world applications to shading-aware depth map denoising, refinement and completion are presented.



### A Study of Cross-domain Generative Models applied to Cartoon Series
- **Arxiv ID**: http://arxiv.org/abs/1710.00755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00755v1)
- **Published**: 2017-09-29 13:59:48+00:00
- **Updated**: 2017-09-29 13:59:48+00:00
- **Authors**: Eman T. Hassan, David J. Crandall
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate Generative Adversarial Networks (GANs) to model one particular kind of image: frames from TV cartoons. Cartoons are particularly interesting because their visual appearance emphasizes the important semantic information about a scene while abstracting out the less important details, but each cartoon series has a distinctive artistic style that performs this abstraction in different ways. We consider a dataset consisting of images from two popular television cartoon series, Family Guy and The Simpsons. We examine the ability of GANs to generate images from each of these two domains, when trained independently as well as on both domains jointly. We find that generative models may be capable of finding semantic-level correspondences between these two image domains despite the unsupervised setting, even when the training data does not give labeled alignments between them.



### On the Capacity of Face Representation
- **Arxiv ID**: http://arxiv.org/abs/1709.10433v3
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1709.10433v3)
- **Published**: 2017-09-29 14:47:13+00:00
- **Updated**: 2019-04-11 19:45:51+00:00
- **Authors**: Sixue Gong, Vishnu Naresh Boddeti, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we address the following question, given a face representation, how many identities can it resolve? In other words, what is the capacity of the face representation? A scientific basis for estimating the capacity of a given face representation will not only benefit the evaluation and comparison of different representation methods, but will also establish an upper bound on the scalability of an automatic face recognition system. We cast the face capacity problem in terms of packing bounds on a low-dimensional manifold embedded within a deep representation space. By explicitly accounting for the manifold structure of the representation as well two different sources of representational noise: epistemic (model) uncertainty and aleatoric (data) variability, our approach is able to estimate the capacity of a given face representation. To demonstrate the efficacy of our approach, we estimate the capacity of two deep neural network based face representations, namely 128-dimensional FaceNet and 512-dimensional SphereFace. Numerical experiments on unconstrained faces (IJB-C) provides a capacity upper bound of $2.7\times10^4$ for FaceNet and $8.4\times10^4$ for SphereFace representation at a false acceptance rate (FAR) of 1%. As expected, capacity reduces drastically at lower FARs. The capacity at FAR of 0.1% and 0.001% is $2.2\times10^3$ and $1.6\times10^{1}$, respectively for FaceNet and $3.6\times10^3$ and $6.0\times10^0$, respectively for SphereFace.



### Optimisation of photometric stereo methods by non-convex variational minimisation
- **Arxiv ID**: http://arxiv.org/abs/1709.10437v1
- **DOI**: None
- **Categories**: **cs.CV**, 65K10
- **Links**: [PDF](http://arxiv.org/pdf/1709.10437v1)
- **Published**: 2017-09-29 14:50:55+00:00
- **Updated**: 2017-09-29 14:50:55+00:00
- **Authors**: Georg Radow, Laurent Hoeltgen, Yvain Quéau, Michael Breuß
- **Comment**: 18 pages, 18 Figures
- **Journal**: None
- **Summary**: Estimating shape and appearance of a three dimensional object from a given set of images is a classic research topic that is still actively pursued. Among the various techniques available, PS is distinguished by the assumption that the underlying input images are taken from the same point of view but under different lighting conditions. The most common techniques provide the shape information in terms of surface normals. In this work, we instead propose to minimise a much more natural objective function, namely the reprojection error in terms of depth. Minimising the resulting non-trivial variational model for PS allows to recover the depth of the photographed scene directly. As a solving strategy, we follow an approach based on a recently published optimisation scheme for non-convex and non-smooth cost functions.   The main contributions of our paper are of theoretical nature. A technical novelty in our framework is the usage of matrix differential calculus. We supplement our approach by a detailed convergence analysis of the resulting optimisation algorithm and discuss possibilities to ease the computational complexity. At hand of an experimental evaluation we discuss important properties of the method. Overall, our strategy achieves more accurate results than competing approaches. The experiments also highlights some practical aspects of the underlying optimisation algorithm that may be of interest in a more general context.



### Improving image generative models with human interactions
- **Arxiv ID**: http://arxiv.org/abs/1709.10459v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1709.10459v1)
- **Published**: 2017-09-29 15:38:42+00:00
- **Updated**: 2017-09-29 15:38:42+00:00
- **Authors**: Andrew Kyle Lampinen, David So, Douglas Eck, Fred Bertsch
- **Comment**: None
- **Journal**: None
- **Summary**: GANs provide a framework for training generative models which mimic a data distribution. However, in many cases we wish to train these generative models to optimize some auxiliary objective function within the data it generates, such as making more aesthetically pleasing images. In some cases, these objective functions are difficult to evaluate, e.g. they may require human interaction. Here, we develop a system for efficiently improving a GAN to target an objective involving human interaction, specifically generating images that increase rates of positive user interactions. To improve the generative model, we build a model of human behavior in the targeted domain from a relatively small set of interactions, and then use this behavioral model as an auxiliary loss function to improve the generative model. We show that this system is successful at improving positive interaction rates, at least on simulated data, and characterize some of the factors that affect its performance.



### Discovery and recognition of motion primitives in human activities
- **Arxiv ID**: http://arxiv.org/abs/1709.10494v7
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.10494v7)
- **Published**: 2017-09-29 16:59:06+00:00
- **Updated**: 2019-02-04 13:17:58+00:00
- **Authors**: Marta Sanzari, Valsamis Ntouskos, Fiora Pirri
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel framework for the automatic discovery and recognition of motion primitives in videos of human activities. Given the 3D pose of a human in a video, human motion primitives are discovered by optimizing the `motion flux', a quantity which captures the motion variation of a group of skeletal joints. A normalization of the primitives is proposed in order to make them invariant with respect to a subject anatomical variations and data sampling rate. The discovered primitives are unknown and unlabeled and are unsupervisedly collected into classes via a hierarchical non-parametric Bayes mixture model. Once classes are determined and labeled they are further analyzed for establishing models for recognizing discovered primitives. Each primitive model is defined by a set of learned parameters.   Given new video data and given the estimated pose of the subject appearing on the video, the motion is segmented into primitives, which are recognized with a probability given according to the parameters of the learned models.   Using our framework we build a publicly available dataset of human motion primitives, using sequences taken from well-known motion capture datasets. We expect that our framework, by providing an objective way for discovering and categorizing human motion, will be a useful tool in numerous research fields including video analysis, human inspired motion generation, learning by demonstration, intuitive human-robot interaction, and human behavior analysis.



### Vision-based deep execution monitoring
- **Arxiv ID**: http://arxiv.org/abs/1709.10507v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.10507v1)
- **Published**: 2017-09-29 17:33:39+00:00
- **Updated**: 2017-09-29 17:33:39+00:00
- **Authors**: Francesco Puja, Simone Grazioso, Antonio Tammaro, Valsmis Ntouskos, Marta Sanzari, Fiora Pirri
- **Comment**: None
- **Journal**: None
- **Summary**: Execution monitor of high-level robot actions can be effectively improved by visual monitoring the state of the world in terms of preconditions and postconditions that hold before and after the execution of an action. Furthermore a policy for searching where to look at, either for verifying the relations that specify the pre and postconditions or to refocus in case of a failure, can tremendously improve the robot execution in an uncharted environment. It is now possible to strongly rely on visual perception in order to make the assumption that the environment is observable, by the amazing results of deep learning. In this work we present visual execution monitoring for a robot executing tasks in an uncharted Lab environment. The execution monitor interacts with the environment via a visual stream that uses two DCNN for recognizing the objects the robot has to deal with and manipulate, and a non-parametric Bayes estimation to discover the relations out of the DCNN features. To recover from lack of focus and failures due to missed objects we resort to visual search policies via deep reinforcement learning.



### Unsupervised Domain Adaptation with Copula Models
- **Arxiv ID**: http://arxiv.org/abs/1710.00018v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.00018v1)
- **Published**: 2017-09-29 18:14:55+00:00
- **Updated**: 2017-09-29 18:14:55+00:00
- **Authors**: Cuong D. Tran, Ognjen Rudovic, Vladimir Pavlovic
- **Comment**: IEEE International Workshop On Machine Learning for Signal Processing
  2017
- **Journal**: None
- **Summary**: We study the task of unsupervised domain adaptation, where no labeled data from the target domain is provided during training time. To deal with the potential discrepancy between the source and target distributions, both in features and labels, we exploit a copula-based regression framework. The benefits of this approach are two-fold: (a) it allows us to model a broader range of conditional predictive densities beyond the common exponential family, (b) we show how to leverage Sklar's theorem, the essence of the copula formulation relating the joint density to the copula dependency functions, to find effective feature mappings that mitigate the domain mismatch. By transforming the data to a copula domain, we show on a number of benchmark datasets (including human emotion estimation), and using different regression models for prediction, that we can achieve a more robust and accurate estimation of target labels, compared to recently proposed feature transformation (adaptation) methods.



### A Gaussian mixture model representation of endmember variability in hyperspectral unmixing
- **Arxiv ID**: http://arxiv.org/abs/1710.00075v2
- **DOI**: 10.1109/TIP.2018.2795744
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00075v2)
- **Published**: 2017-09-29 20:10:00+00:00
- **Updated**: 2018-01-15 21:01:34+00:00
- **Authors**: Yuan Zhou, Anand Rangarajan, Paul D. Gader
- **Comment**: Accepted by IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Hyperspectral unmixing while considering endmember variability is usually performed by the normal compositional model (NCM), where the endmembers for each pixel are assumed to be sampled from unimodal Gaussian distributions. However, in real applications, the distribution of a material is often not Gaussian. In this paper, we use Gaussian mixture models (GMM) to represent the endmember variability. We show, given the GMM starting premise, that the distribution of the mixed pixel (under the linear mixing model) is also a GMM (and this is shown from two perspectives). The first perspective originates from the random variable transformation and gives a conditional density function of the pixels given the abundances and GMM parameters. With proper smoothness and sparsity prior constraints on the abundances, the conditional density function leads to a standard maximum a posteriori (MAP) problem which can be solved using generalized expectation maximization. The second perspective originates from marginalizing over the endmembers in the GMM, which provides us with a foundation to solve for the endmembers at each pixel. Hence, our model can not only estimate the abundances and distribution parameters, but also the distinct endmember set for each pixel. We tested the proposed GMM on several synthetic and real datasets, and showed its potential by comparing it to current popular methods.



