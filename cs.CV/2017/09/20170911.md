# Arxiv Papers in cs.CV on 2017-09-11
### 3D Densely Convolutional Networks for Volumetric Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1709.03199v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03199v2)
- **Published**: 2017-09-11 00:02:48+00:00
- **Updated**: 2017-09-13 18:35:02+00:00
- **Authors**: Toan Duc Bui, Jitae Shin, Taesup Moon
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: In the isointense stage, the accurate volumetric image segmentation is a challenging task due to the low contrast between tissues. In this paper, we propose a novel very deep network architecture based on a densely convolutional network for volumetric brain segmentation. The proposed network architecture provides a dense connection between layers that aims to improve the information flow in the network. By concatenating features map of fine and coarse dense blocks, it allows capturing multi-scale contextual information. Experimental results demonstrate significant advantages of the proposed method over existing methods, in terms of both segmentation accuracy and parameter efficiency in MICCAI grand challenge on 6-month infant brain MRI segmentation.



### Recurrent neural networks based Indic word-wise script identification using character-wise training
- **Arxiv ID**: http://arxiv.org/abs/1709.03209v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03209v2)
- **Published**: 2017-09-11 01:35:22+00:00
- **Updated**: 2018-12-27 15:01:36+00:00
- **Authors**: Rohun Tripathi, Aman Gill, Riccha Tripati
- **Comment**: Version accepted at ICPRS 2017
- **Journal**: None
- **Summary**: This paper presents a novel methodology of Indic handwritten script recognition using Recurrent Neural Networks and addresses the problem of script recognition in poor data scenarios, such as when only character level online data is available. It is based on the hypothesis that curves of online character data comprise sufficient information for prediction at the word level. Online character data is used to train RNNs using BLSTM architecture which are then used to make predictions of online word level data. These prediction results on the test set are at par with prediction results of models trained with online word data, while the training of the character level model is much less data intensive and takes much less time. Performance for binary-script models and then 5 Indic script models are reported, along with comparison with HMM models.The system is extended for offline data prediction. Raw offline data lacks the temporal information available in online data and required for prediction using models trained with online data. To overcome this, stroke recovery is implemented and the strokes are utilized for predicting using the online character level models. The performance on character and word level offline data is reported.



### Fused Text Segmentation Networks for Multi-oriented Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1709.03272v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03272v4)
- **Published**: 2017-09-11 07:25:37+00:00
- **Updated**: 2018-05-07 06:05:52+00:00
- **Authors**: Yuchen Dai, Zheng Huang, Yuting Gao, Youxuan Xu, Kai Chen, Jie Guo, Weidong Qiu
- **Comment**: Accepted by ICPR2018
- **Journal**: None
- **Summary**: In this paper, we introduce a novel end-end framework for multi-oriented scene text detection from an instance-aware semantic segmentation perspective. We present Fused Text Segmentation Networks, which combine multi-level features during the feature extracting as text instance may rely on finer feature expression compared to general objects. It detects and segments the text instance jointly and simultaneously, leveraging merits from both semantic segmentation task and region proposal based object detection task. Not involving any extra pipelines, our approach surpasses the current state of the art on multi-oriented scene text detection benchmarks: ICDAR2015 Incidental Scene Text and MSRA-TD500 reaching Hmean 84.1% and 82.0% respectively. Morever, we report a baseline on total-text containing curved text which suggests effectiveness of the proposed approach.



### weedNet: Dense Semantic Weed Classification Using Multispectral Images and MAV for Smart Farming
- **Arxiv ID**: http://arxiv.org/abs/1709.03329v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.03329v1)
- **Published**: 2017-09-11 10:59:01+00:00
- **Updated**: 2017-09-11 10:59:01+00:00
- **Authors**: Inkyu Sa, Zetao Chen, Marija Popovic, Raghav Khanna, Frank Liebisch, Juan Nieto, Roland Siegwart
- **Comment**: None
- **Journal**: None
- **Summary**: Selective weed treatment is a critical step in autonomous crop management as related to crop health and yield. However, a key challenge is reliable, and accurate weed detection to minimize damage to surrounding plants. In this paper, we present an approach for dense semantic weed classification with multispectral images collected by a micro aerial vehicle (MAV). We use the recently developed encoder-decoder cascaded Convolutional Neural Network (CNN), Segnet, that infers dense semantic classes while allowing any number of input image channels and class balancing with our sugar beet and weed datasets. To obtain training datasets, we established an experimental field with varying herbicide levels resulting in field plots containing only either crop or weed, enabling us to use the Normalized Difference Vegetation Index (NDVI) as a distinguishable feature for automatic ground truth generation. We train 6 models with different numbers of input channels and condition (fine-tune) it to achieve about 0.8 F1-score and 0.78 Area Under the Curve (AUC) classification metrics. For model deployment, an embedded GPU system (Jetson TX2) is tested for MAV integration. Dataset used in this paper is released to support the community and future work.



### Stack-Captioning: Coarse-to-Fine Learning for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1709.03376v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03376v3)
- **Published**: 2017-09-11 13:44:30+00:00
- **Updated**: 2018-03-14 09:31:38+00:00
- **Authors**: Jiuxiang Gu, Jianfei Cai, Gang Wang, Tsuhan Chen
- **Comment**: AAAI-2018, Oral Presentation
- **Journal**: None
- **Summary**: The existing image captioning approaches typically train a one-stage sentence decoder, which is difficult to generate rich fine-grained descriptions. On the other hand, multi-stage image caption model is hard to train due to the vanishing gradient problem. In this paper, we propose a coarse-to-fine multi-stage prediction framework for image captioning, composed of multiple decoders each of which operates on the output of the previous stage, producing increasingly refined image descriptions. Our proposed learning approach addresses the difficulty of vanishing gradients during training by providing a learning objective function that enforces intermediate supervisions. Particularly, we optimize our model with a reinforcement learning approach which utilizes the output of each intermediate decoder's test-time inference algorithm as well as the output of its preceding decoder to normalize the rewards, which simultaneously solves the well-known exposure bias problem and the loss-evaluation mismatch problem. We extensively evaluate the proposed approach on MSCOCO and show that our approach can achieve the state-of-the-art performance.



### Automated Identification of Trampoline Skills Using Computer Vision Extracted Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1709.03399v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03399v1)
- **Published**: 2017-09-11 14:23:51+00:00
- **Updated**: 2017-09-11 14:23:51+00:00
- **Authors**: Paul W. Connolly, Guenole C. Silvestre, Chris J. Bleakley
- **Comment**: 8 pages, Irish Machine Vision and Image Processing Conference (IMVIP)
  2017
- **Journal**: None
- **Summary**: A novel method to identify trampoline skills using a single video camera is proposed herein. Conventional computer vision techniques are used for identification, estimation, and tracking of the gymnast's body in a video recording of the routine. For each frame, an open source convolutional neural network is used to estimate the pose of the athlete's body. Body orientation and joint angle estimates are extracted from these pose estimates. The trajectories of these angle estimates over time are compared with those of labelled reference skills. A nearest neighbour classifier utilising a mean squared error distance metric is used to identify the skill performed. A dataset containing 714 skill examples with 20 distinct skills performed by adult male and female gymnasts was recorded and used for evaluation of the system. The system was found to achieve a skill identification accuracy of 80.7% for the dataset.



### Deep Shape Matching
- **Arxiv ID**: http://arxiv.org/abs/1709.03409v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03409v2)
- **Published**: 2017-09-11 14:33:42+00:00
- **Updated**: 2018-07-25 20:11:12+00:00
- **Authors**: Filip Radenović, Giorgos Tolias, Ondřej Chum
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: We cast shape matching as metric learning with convolutional networks. We break the end-to-end process of image representation into two parts. Firstly, well established efficient methods are chosen to turn the images into edge maps. Secondly, the network is trained with edge maps of landmark images, which are automatically obtained by a structure-from-motion pipeline. The learned representation is evaluated on a range of different tasks, providing improvements on challenging cases of domain generalization, generic sketch-based image retrieval or its fine-grained counterpart. In contrast to other methods that learn a different model per task, object category, or domain, we use the same network throughout all our experiments, achieving state-of-the-art results in multiple benchmarks.



### One-Shot Learning for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1709.03410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03410v1)
- **Published**: 2017-09-11 14:34:58+00:00
- **Updated**: 2017-09-11 14:34:58+00:00
- **Authors**: Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, Byron Boots
- **Comment**: To appear in the proceedings of the British Machine Vision Conference
  (BMVC) 2017. The code is available at https://github.com/lzzcd001/OSLSM
- **Journal**: None
- **Summary**: Low-shot learning methods for image classification support learning from sparse data. We extend these techniques to support dense semantic image segmentation. Specifically, we train a network that, given a small set of annotated images, produces parameters for a Fully Convolutional Network (FCN). We use this FCN to perform dense pixel-level prediction on a test image for the new semantic class. Our architecture shows a 25% relative meanIoU improvement compared to the best baseline methods for one-shot segmentation on unseen classes in the PASCAL VOC 2012 dataset and is at least 3 times faster.



### Why Do Deep Neural Networks Still Not Recognize These Images?: A Qualitative Analysis on Failure Cases of ImageNet Classification
- **Arxiv ID**: http://arxiv.org/abs/1709.03439v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03439v1)
- **Published**: 2017-09-11 15:35:05+00:00
- **Updated**: 2017-09-11 15:35:05+00:00
- **Authors**: Han S. Lee, Alex A. Agarwal, Junmo Kim
- **Comment**: Poster presented at CVPR 2017 Scene Understanding Workshop
- **Journal**: None
- **Summary**: In a recent decade, ImageNet has become the most notable and powerful benchmark database in computer vision and machine learning community. As ImageNet has emerged as a representative benchmark for evaluating the performance of novel deep learning models, its evaluation tends to include only quantitative measures such as error rate, rather than qualitative analysis. Thus, there are few studies that analyze the failure cases of deep learning models in ImageNet, though there are numerous works analyzing the networks themselves and visualizing them. In this abstract, we qualitatively analyze the failure cases of ImageNet classification results from recent deep learning model, and categorize these cases according to the certain image patterns. Through this failure analysis, we believe that it can be discovered what the final challenges are in ImageNet database, which the current deep learning model is still vulnerable to.



### UI-Net: Interactive Artificial Neural Networks for Iterative Image Segmentation Based on a User Model
- **Arxiv ID**: http://arxiv.org/abs/1709.03450v1
- **DOI**: 10.2312/vcbm.20171248
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, 68T05, 68T45, I.2.6; I.4.6; I.5.5
- **Links**: [PDF](http://arxiv.org/pdf/1709.03450v1)
- **Published**: 2017-09-11 15:50:24+00:00
- **Updated**: 2017-09-11 15:50:24+00:00
- **Authors**: Mario Amrehn, Sven Gaube, Mathias Unberath, Frank Schebesch, Tim Horz, Maddalena Strumia, Stefan Steidl, Markus Kowarschik, Andreas Maier
- **Comment**: This work is submitted to the 2017 Eurographics Workshop on Visual
  Computing for Biology and Medicine
- **Journal**: Eurographics Workshop on Visual Computing for Biology and Medicine
  (2017) 143-147
- **Summary**: For complex segmentation tasks, fully automatic systems are inherently limited in their achievable accuracy for extracting relevant objects. Especially in cases where only few data sets need to be processed for a highly accurate result, semi-automatic segmentation techniques exhibit a clear benefit for the user. One area of application is medical image processing during an intervention for a single patient. We propose a learning-based cooperative segmentation approach which includes the computing entity as well as the user into the task. Our system builds upon a state-of-the-art fully convolutional artificial neural network (FCN) as well as an active user model for training. During the segmentation process, a user of the trained system can iteratively add additional hints in form of pictorial scribbles as seed points into the FCN system to achieve an interactive and precise segmentation result. The segmentation quality of interactive FCNs is evaluated. Iterative FCN approaches can yield superior results compared to networks without the user input channel component, due to a consistent improvement in segmentation quality after each interaction.



### CLAD: A Complex and Long Activities Dataset with Rich Crowdsourced Annotations
- **Arxiv ID**: http://arxiv.org/abs/1709.03456v2
- **DOI**: 10.5518/249
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1709.03456v2)
- **Published**: 2017-09-11 16:01:17+00:00
- **Updated**: 2017-09-21 16:52:04+00:00
- **Authors**: Jawad Tayyub, Majd Hawasly, David C. Hogg, Anthony G. Cohn
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel activity dataset which exhibits real-life and diverse scenarios of complex, temporally-extended human activities and actions. The dataset presents a set of videos of actors performing everyday activities in a natural and unscripted manner. The dataset was recorded using a static Kinect 2 sensor which is commonly used on many robotic platforms. The dataset comprises of RGB-D images, point cloud data, automatically generated skeleton tracks in addition to crowdsourced annotations. Furthermore, we also describe the methodology used to acquire annotations through crowdsourcing. Finally some activity recognition benchmarks are presented using current state-of-the-art techniques. We believe that this dataset is particularly suitable as a testbed for activity recognition research but it can also be applicable for other common tasks in robotics/computer vision research such as object detection and human skeleton tracking.



### Deep Generative Filter for Motion Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1709.03481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03481v1)
- **Published**: 2017-09-11 17:18:26+00:00
- **Updated**: 2017-09-11 17:18:26+00:00
- **Authors**: Sainandan Ramakrishnan, Shubham Pachori. Aalok Gangopadhyay, Shanmuganathan Raman
- **Comment**: None
- **Journal**: None
- **Summary**: Removing blur caused by camera shake in images has always been a challenging problem in computer vision literature due to its ill-posed nature. Motion blur caused due to the relative motion between the camera and the object in 3D space induces a spatially varying blurring effect over the entire image. In this paper, we propose a novel deep filter based on Generative Adversarial Network (GAN) architecture integrated with global skip connection and dense architecture in order to tackle this problem. Our model, while bypassing the process of blur kernel estimation, significantly reduces the test time which is necessary for practical applications. The experiments on the benchmark datasets prove the effectiveness of the proposed method which outperforms the state-of-the-art blind deblurring algorithms both quantitatively and qualitatively.



### NiftyNet: a deep-learning platform for medical imaging
- **Arxiv ID**: http://arxiv.org/abs/1709.03485v2
- **DOI**: 10.1016/j.cmpb.2018.01.025
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1709.03485v2)
- **Published**: 2017-09-11 17:42:10+00:00
- **Updated**: 2017-10-16 13:46:31+00:00
- **Authors**: Eli Gibson, Wenqi Li, Carole Sudre, Lucas Fidon, Dzhoshkun I. Shakir, Guotai Wang, Zach Eaton-Rosen, Robert Gray, Tom Doel, Yipeng Hu, Tom Whyntie, Parashkev Nachev, Marc Modat, Dean C. Barratt, Sébastien Ourselin, M. Jorge Cardoso, Tom Vercauteren
- **Comment**: Wenqi Li and Eli Gibson contributed equally to this work. M. Jorge
  Cardoso and Tom Vercauteren contributed equally to this work. 26 pages, 6
  figures; Update includes additional applications, updated author list and
  formatting for journal submission
- **Journal**: None
- **Summary**: Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. Established deep-learning platforms are flexible but do not provide specific functionality for medical image analysis and adapting them for this application requires substantial implementation effort. Thus, there has been substantial duplication of effort and incompatible infrastructure developed across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon.   NiftyNet provides a modular deep-learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications. Components of the NiftyNet pipeline including data loading, data augmentation, network architectures, loss functions and evaluation metrics are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted intervention. NiftyNet is built on TensorFlow and supports TensorBoard visualization of 2D and 3D images and computational graphs by default.   We present 3 illustrative medical image analysis applications built using NiftyNet: (1) segmentation of multiple abdominal organs from computed tomography; (2) image regression to predict computed tomography attenuation maps from brain magnetic resonance images; and (3) generation of simulated ultrasound images for specified anatomical poses.   NiftyNet enables researchers to rapidly develop and distribute deep learning solutions for segmentation, regression, image generation and representation learning applications, or extend the platform to new applications.



### Recovering Homography from Camera Captured Documents using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.03524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03524v1)
- **Published**: 2017-09-11 18:08:58+00:00
- **Updated**: 2017-09-11 18:08:58+00:00
- **Authors**: Syed Ammar Abbas, Sibt ul Hussain
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Removing perspective distortion from hand held camera captured document images is one of the primitive tasks in document analysis, but unfortunately, no such method exists that can reliably remove the perspective distortion from document images automatically. In this paper, we propose a convolutional neural network based method for recovering homography from hand-held camera captured documents.   Our proposed method works independent of document's underlying content and is trained end-to-end in a fully automatic way. Specifically, this paper makes following three contributions: Firstly, we introduce a large scale synthetic dataset for recovering homography from documents images captured under different geometric and photometric transformations; secondly, we show that a generic convolutional neural network based architecture can be successfully used for regressing the corners positions of documents captured under wild settings; thirdly, we show that L1 loss can be reliably used for corners regression. Our proposed method gives state-of-the-art performance on the tested datasets, and has potential to become an integral part of document analysis pipeline.



### Exploring Geometric Property Thresholds For Filtering Non-Text Regions In A Connected Component Based Text Detection Application
- **Arxiv ID**: http://arxiv.org/abs/1709.03548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03548v1)
- **Published**: 2017-09-11 19:05:53+00:00
- **Updated**: 2017-09-11 19:05:53+00:00
- **Authors**: Teresa Nicole Brooks
- **Comment**: None
- **Journal**: None
- **Summary**: Automated text detection is a difficult computer vision task. In order to accurately detect and identity text in an image or video, two major problems must be addressed. The primary problem is implementing a robust and reliable method for distinguishing text vs non-text regions in images and videos. Part of the difficulty stems from the almost unlimited combinations of fonts, lighting conditions, distortions, and other variations that can be found in images and videos. This paper explores key properties of two popular and proven methods for implementing text detection; maximum stable external regions (MSER) and stroke width variation.



### Extracting Traffic Primitives Directly from Naturalistically Logged Data for Self-Driving Applications
- **Arxiv ID**: http://arxiv.org/abs/1709.03553v3
- **DOI**: 10.1109/LRA.2018.2794604
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03553v3)
- **Published**: 2017-09-11 19:14:42+00:00
- **Updated**: 2018-05-26 03:05:45+00:00
- **Authors**: Wenshuo Wang, Ding Zhao
- **Comment**: 7 pages, 8 figures, 2 tables, ICRA 2018
- **Journal**: None
- **Summary**: Developing an automated vehicle, that can handle complicated driving scenarios and appropriately interact with other road users, requires the ability to semantically learn and understand driving environment, oftentimes, based on analyzing massive amounts of naturalistic driving data. An important paradigm that allows automated vehicles to both learn from human drivers and gain insights is understanding the principal compositions of the entire traffic, termed as traffic primitives. However, the exploding data growth presents a great challenge in extracting primitives from high-dimensional time-series traffic data with various types of road users engaged. Therefore, automatically extracting primitives is becoming one of the cost-efficient ways to help autonomous vehicles understand and predict the complex traffic scenarios. In addition, the extracted primitives from raw data should 1) be appropriate for automated driving applications and also 2) be easily used to generate new traffic scenarios. However, existing literature does not provide a method to automatically learn these primitives from large-scale traffic data. The contribution of this paper has two manifolds. The first one is that we proposed a new framework to generate new traffic scenarios from a handful of limited traffic data. The second one is that we introduce a nonparametric Bayesian learning method -- a sticky hierarchical Dirichlet process hidden Markov model -- to automatically extract primitives from multidimensional traffic data without prior knowledge of the primitive settings. The developed method is then validated using one day of naturalistic driving data. Experiment results show that the nonparametric Bayesian learning method is able to extract primitives from traffic scenarios where both the binary and continuous events coexist.



### Real-Time Multiple Object Tracking - A Study on the Importance of Speed
- **Arxiv ID**: http://arxiv.org/abs/1709.03572v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03572v2)
- **Published**: 2017-09-11 20:16:22+00:00
- **Updated**: 2017-10-02 16:26:37+00:00
- **Authors**: Samuel Murray
- **Comment**: Master Thesis
- **Journal**: None
- **Summary**: In this project, we implement a multiple object tracker, following the tracking-by-detection paradigm, as an extension of an existing method. It works by modelling the movement of objects by solving the filtering problem, and associating detections with predicted new locations in new frames using the Hungarian algorithm. Three different similarity measures are used, which use the location and shape of the bounding boxes. Compared to other trackers on the MOTChallenge leaderboard, our method, referred to as C++SORT, is the fastest non-anonymous submission, while also achieving decent score on other metrics. By running our model on the Okutama-Action dataset, sampled at different frame-rates, we show that the performance is greatly reduced when running the model - including detecting objects - in real-time. In most metrics, the score is reduced by 50%, but in certain cases as much as 90%. We argue that this indicates that other, slower methods could not be used for tracking in real-time, but that more research is required specifically on this.



### Art of singular vectors and universal adversarial perturbations
- **Arxiv ID**: http://arxiv.org/abs/1709.03582v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.03582v2)
- **Published**: 2017-09-11 20:22:37+00:00
- **Updated**: 2017-11-20 03:09:07+00:00
- **Authors**: Valentin Khrulkov, Ivan Oseledets
- **Comment**: Submitted to CVPR 2018
- **Journal**: None
- **Summary**: Vulnerability of Deep Neural Networks (DNNs) to adversarial attacks has been attracting a lot of attention in recent studies. It has been shown that for many state of the art DNNs performing image classification there exist universal adversarial perturbations --- image-agnostic perturbations mere addition of which to natural images with high probability leads to their misclassification. In this work we propose a new algorithm for constructing such universal perturbations. Our approach is based on computing the so-called $(p, q)$-singular vectors of the Jacobian matrices of hidden layers of a network. Resulting perturbations present interesting visual patterns, and by using only 64 images we were able to construct universal perturbations with more than 60 \% fooling rate on the dataset consisting of 50000 images. We also investigate a correlation between the maximal singular value of the Jacobian matrix and the fooling rate of the corresponding singular vector, and show that the constructed perturbations generalize across networks.



### On the definition of Shape Parts: a Dominant Sets Approach
- **Arxiv ID**: http://arxiv.org/abs/1709.03588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03588v1)
- **Published**: 2017-09-11 20:40:49+00:00
- **Updated**: 2017-09-11 20:40:49+00:00
- **Authors**: Foteini Fotopoulou, George Economou
- **Comment**: None
- **Journal**: None
- **Summary**: In the present paper a novel graph-based approach to the shape decomposition problem is addressed. The shape is appropriately transformed into a visibility graph enriched with local neighborhood information. A two-step diffusion process is then applied to the visibility graph that efficiently enhances the information provided, thus leading to a more robust and meaningful graph construction. Inspired by the notion of a clique as a strict cluster definition, the dominant sets algorithm is invoked, slightly modified to comport with the specific problem of defining shape parts. The cluster cohesiveness and a node participation vector are two important outputs of the proposed graph partitioning method. Opposed to most of the existing techniques, the final number of the clusters is determined automatically, by estimating the cluster cohesiveness on a random network generation process. Experimental results on several shape databases show the effectiveness of our framework for graph based shape decomposition.



### Holistic, Instance-Level Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/1709.03612v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.03612v1)
- **Published**: 2017-09-11 22:06:34+00:00
- **Updated**: 2017-09-11 22:06:34+00:00
- **Authors**: Qizhu Li, Anurag Arnab, Philip H. S. Torr
- **Comment**: Poster at BMVC 2017
- **Journal**: None
- **Summary**: Object parsing -- the task of decomposing an object into its semantic parts -- has traditionally been formulated as a category-level segmentation problem. Consequently, when there are multiple objects in an image, current methods cannot count the number of objects in the scene, nor can they determine which part belongs to which object. We address this problem by segmenting the parts of objects at an instance-level, such that each pixel in the image is assigned a part label, as well as the identity of the object it belongs to. Moreover, we show how this approach benefits us in obtaining segmentations at coarser granularities as well. Our proposed network is trained end-to-end given detections, and begins with a category-level segmentation module. Thereafter, a differentiable Conditional Random Field, defined over a variable number of instances for every input image, reasons about the identity of each part by associating it with a human detection. In contrast to other approaches, our method can handle the varying number of people in each image and our holistic network produces state-of-the-art results in instance-level part and human segmentation, together with competitive results in category-level part segmentation, all achieved by a single forward-pass through our neural network.



