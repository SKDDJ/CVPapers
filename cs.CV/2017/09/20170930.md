# Arxiv Papers in cs.CV on 2017-09-30
### 3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous Mobile Robot Deployment Data
- **Arxiv ID**: http://arxiv.org/abs/1710.00126v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1710.00126v1)
- **Published**: 2017-09-30 00:40:54+00:00
- **Updated**: 2017-09-30 00:40:54+00:00
- **Authors**: Li Sun, Zhi Yan, Sergi Molina Mellado, Marc Hanheide, Tom Duckett
- **Comment**: 7 pages, conference
- **Journal**: None
- **Summary**: This paper presents a novel 3DOF pedestrian trajectory prediction approach for autonomous mobile service robots. While most previously reported methods are based on learning of 2D positions in monocular camera images, our approach uses range-finder sensors to learn and predict 3DOF pose trajectories (i.e. 2D position plus 1D rotation within the world coordinate system). Our approach, T-Pose-LSTM (Temporal 3DOF-Pose Long-Short-Term Memory), is trained using long-term data from real-world robot deployments and aims to learn context-dependent (environment- and time-specific) human activities. Our approach incorporates long-term temporal information (i.e. date and time) with short-term pose observations as input. A sequence-to-sequence LSTM encoder-decoder is trained, which encodes observations into LSTM and then decodes as predictions. For deployment, it can perform on-the-fly prediction in real-time. Instead of using manually annotated data, we rely on a robust human detection, tracking and SLAM system, providing us with examples in a global coordinate system. We validate the approach using more than 15K pedestrian trajectories recorded in a care home environment over a period of three months. The experiment shows that the proposed T-Pose-LSTM model advances the state-of-the-art 2D-based method for human trajectory prediction in long-term mobile robot deployments.



### Dense RGB-D semantic mapping with Pixel-Voxel neural network
- **Arxiv ID**: http://arxiv.org/abs/1710.00132v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00132v3)
- **Published**: 2017-09-30 01:10:53+00:00
- **Updated**: 2017-10-04 21:21:05+00:00
- **Authors**: Cheng Zhao, Li Sun, Pulak Purkait, Rustam Stolkin
- **Comment**: 8 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: For intelligent robotics applications, extending 3D mapping to 3D semantic mapping enables robots to, not only localize themselves with respect to the scene's geometrical features but also simultaneously understand the higher level meaning of the scene contexts. Most previous methods focus on geometric 3D reconstruction and scene understanding independently notwithstanding the fact that joint estimation can boost the accuracy of the semantic mapping. In this paper, a dense RGB-D semantic mapping system with a Pixel-Voxel network is proposed, which can perform dense 3D mapping while simultaneously recognizing and semantically labelling each point in the 3D map. The proposed Pixel-Voxel network obtains global context information by using PixelNet to exploit the RGB image and meanwhile, preserves accurate local shape information by using VoxelNet to exploit the corresponding 3D point cloud. Unlike the existing architecture that fuses score maps from different models with equal weights, we proposed a Softmax weighted fusion stack that adaptively learns the varying contributions of PixelNet and VoxelNet, and fuses the score maps of the two models according to their respective confidence levels. The proposed Pixel-Voxel network achieves the state-of-the-art semantic segmentation performance on the SUN RGB-D benchmark dataset. The runtime of the proposed system can be boosted to 11-12Hz, enabling near to real-time performance using an i7 8-cores PC with Titan X GPU.



### PCANet-II: When PCANet Meets the Second Order Pooling
- **Arxiv ID**: http://arxiv.org/abs/1710.00166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00166v1)
- **Published**: 2017-09-30 09:11:38+00:00
- **Updated**: 2017-09-30 09:11:38+00:00
- **Authors**: Lei Tian, Xiaopeng Hong, Guoying Zhao, Chunxiao Fan, Yue Ming, Matti Pietikäinen
- **Comment**: None
- **Journal**: None
- **Summary**: PCANet, as one noticeable shallow network, employs the histogram representation for feature pooling. However, there are three main problems about this kind of pooling method. First, the histogram-based pooling method binarizes the feature maps and leads to inevitable discriminative information loss. Second, it is difficult to effectively combine other visual cues into a compact representation, because the simple concatenation of various visual cues leads to feature representation inefficiency. Third, the dimensionality of histogram-based output grows exponentially with the number of feature maps used. In order to overcome these problems, we propose a novel shallow network model, named as PCANet-II. Compared with the histogram-based output, the second order pooling not only provides more discriminative information by preserving both the magnitude and sign of convolutional responses, but also dramatically reduces the size of output features. Thus we combine the second order statistical pooling method with the shallow network, i.e., PCANet. Moreover, it is easy to combine other discriminative and robust cues by using the second order pooling. So we introduce the binary feature difference encoding scheme into our PCANet-II to further improve robustness. Experiments demonstrate the effectiveness and robustness of our proposed PCANet-II method.



### Deep learning for source camera identification on mobile devices
- **Arxiv ID**: http://arxiv.org/abs/1710.01257v2
- **DOI**: 10.1016/j.patrec.2018.01.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01257v2)
- **Published**: 2017-09-30 11:34:10+00:00
- **Updated**: 2017-10-13 17:03:52+00:00
- **Authors**: David Freire-Obregón, Fabio Narducci, Silvio Barra, Modesto Castrillón-Santana
- **Comment**: 15 pages single column, 9 figures
- **Journal**: None
- **Summary**: In the present paper, we propose a source camera identification method for mobile devices based on deep learning. Recently, convolutional neural networks (CNNs) have shown a remarkable performance on several tasks such as image recognition, video analysis or natural language processing. A CNN consists on a set of layers where each layer is composed by a set of high pass filters which are applied all over the input image. This convolution process provides the unique ability to extract features automatically from data and to learn from those features. Our proposal describes a CNN architecture which is able to infer the noise pattern of mobile camera sensors (also known as camera fingerprint) with the aim at detecting and identifying not only the mobile device used to capture an image (with a 98\% of accuracy), but also from which embedded camera the image was captured. More specifically, we provide an extensive analysis on the proposed architecture considering different configurations. The experiment has been carried out using the images captured from different mobile devices cameras (MICHE-I Dataset was used) and the obtained results have proved the robustness of the proposed method.



### Variational Grid Setting Network
- **Arxiv ID**: http://arxiv.org/abs/1710.01255v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01255v3)
- **Published**: 2017-09-30 11:50:02+00:00
- **Updated**: 2017-10-26 15:36:53+00:00
- **Authors**: Yu-Neng Chuang, Zi-Yu Huang, Yen-Lung Tsai
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: We propose a new neural network architecture for automatic generation of missing characters in a Chinese font set. We call the neural network architecture the Variational Grid Setting Network which is based on the variational autoencoder (VAE) with some tweaks. The neural network model is able to generate missing characters relatively large in size ($256 \times 256$ pixels). Moreover, we show that one can use very few samples for training data set, and get a satisfied result.



### Unsupervised Segmentation of Action Segments in Egocentric Videos using Gaze
- **Arxiv ID**: http://arxiv.org/abs/1710.00187v2
- **DOI**: 10.1109/ICSIPA.2017.8120635
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00187v2)
- **Published**: 2017-09-30 12:19:41+00:00
- **Updated**: 2021-06-23 10:46:10+00:00
- **Authors**: I. Hipiny, H. Ujir, J. L. Minoi, S. F. Samson Juan, M. A. Khairuddin, M. S. Sunar
- **Comment**: Published in 2017 IEEE International Conference On Signal and Image
  Processing Applications
- **Journal**: None
- **Summary**: Unsupervised segmentation of action segments in egocentric videos is a desirable feature in tasks such as activity recognition and content-based video retrieval. Reducing the search space into a finite set of action segments facilitates a faster and less noisy matching. However, there exist a substantial gap in machine understanding of natural temporal cuts during a continuous human activity. This work reports on a novel gaze-based approach for segmenting action segments in videos captured using an egocentric camera. Gaze is used to locate the region-of-interest inside a frame. By tracking two simple motion-based parameters inside successive regions-of-interest, we discover a finite set of temporal cuts. We present several results using combinations (of the two parameters) on a dataset, i.e., BRISGAZE-ACTIONS. The dataset contains egocentric videos depicting several daily-living activities. The quality of the temporal cuts is further improved by implementing two entropy measures.



### Unsupervised Classification of Intrusive Igneous Rock Thin Section Images using Edge Detection and Colour Analysis
- **Arxiv ID**: http://arxiv.org/abs/1710.00189v2
- **DOI**: 10.1109/ICSIPA.2017.8120669
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00189v2)
- **Published**: 2017-09-30 12:22:21+00:00
- **Updated**: 2021-06-23 10:39:18+00:00
- **Authors**: S. Joseph, H. Ujir, I. Hipiny
- **Comment**: Published in 2017 IEEE International Conference On Signal and Image
  Processing Applications
- **Journal**: None
- **Summary**: Classification of rocks is one of the fundamental tasks in a geological study. The process requires a human expert to examine sampled thin section images under a microscope. In this study, we propose a method that uses microscope automation, digital image acquisition, edge detection and colour analysis (histogram). We collected 60 digital images from 20 standard thin sections using a digital camera mounted on a conventional microscope. Each image is partitioned into a finite number of cells that form a grid structure. Edge and colour profile of pixels inside each cell determine its classification. The individual cells then determine the thin section image classification via a majority voting scheme. Our method yielded successful results as high as 90% to 100% precision.



### Where computer vision can aid physics: dynamic cloud motion forecasting from satellite images
- **Arxiv ID**: http://arxiv.org/abs/1710.00194v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, physics.flu-dyn, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/1710.00194v1)
- **Published**: 2017-09-30 12:55:13+00:00
- **Updated**: 2017-09-30 12:55:13+00:00
- **Authors**: Sergiy Zhuk, Tigran Tchrakian, Albert Akhriev, Siyuan Lu, Hendrik Hamann
- **Comment**: published in the proceedings of 2017 IEEE 56th Conference on Decision
  and Control (CDC)
- **Journal**: None
- **Summary**: This paper describes a new algorithm for solar energy forecasting from a sequence of Cloud Optical Depth (COD) images. The algorithm is based on the following simple observation: the dynamics of clouds represented by COD images resembles the motion (transport) of a density in a fluid flow. This suggests that, to forecast the motion of COD images, it is sufficient to forecast the flow. The latter, in turn, can be accomplished by fitting a parametric model of the fluid flow to the COD images observed in the past. Namely, the learning phase of the algorithm is composed of the following steps: (i) given a sequence of COD images, the snapshots of the optical flow are estimated from two consecutive COD images; (ii) these snapshots are then assimilated into a Navier-Stokes Equation (NSE), i.e. an initial velocity field for NSE is selected so that the corresponding NSE' solution is as close as possible to the optical flow snapshots. The prediction phase consists of utilizing a linear transport equation, which describes the propagation of COD images in the fluid flow predicted by NSE, to estimate the future motion of the COD images. The algorithm has been tested on COD images provided by two geostationary operational environmental satellites from NOAA serving the west-hemisphere.



### Robust Photometric Stereo Using Learned Image and Gradient Dictionaries
- **Arxiv ID**: http://arxiv.org/abs/1710.00002v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.00002v1)
- **Published**: 2017-09-30 17:22:55+00:00
- **Updated**: 2017-09-30 17:22:55+00:00
- **Authors**: Andrew J. Wagenmaker, Brian E. Moore, Raj Rao Nadakuditi
- **Comment**: ICIP 2017
- **Journal**: None
- **Summary**: Photometric stereo is a method for estimating the normal vectors of an object from images of the object under varying lighting conditions. Motivated by several recent works that extend photometric stereo to more general objects and lighting conditions, we study a new robust approach to photometric stereo that utilizes dictionary learning. Specifically, we propose and analyze two approaches to adaptive dictionary regularization for the photometric stereo problem. First, we propose an image preprocessing step that utilizes an adaptive dictionary learning model to remove noise and other non-idealities from the image dataset before estimating the normal vectors. We also propose an alternative model where we directly apply the adaptive dictionary regularization to the normal vectors themselves during estimation. We study the practical performance of both methods through extensive simulations, which demonstrate the state-of-the-art performance of both methods in the presence of noise.



### Robust Surface Reconstruction from Gradients via Adaptive Dictionary Regularization
- **Arxiv ID**: http://arxiv.org/abs/1710.00230v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.00230v1)
- **Published**: 2017-09-30 17:23:10+00:00
- **Updated**: 2017-09-30 17:23:10+00:00
- **Authors**: Andrew J. Wagenmaker, Brian E. Moore, Raj Rao Nadakuditi
- **Comment**: ICIP 2017
- **Journal**: None
- **Summary**: This paper introduces a novel approach to robust surface reconstruction from photometric stereo normal vector maps that is particularly well-suited for reconstructing surfaces from noisy gradients. Specifically, we propose an adaptive dictionary learning based approach that attempts to simultaneously integrate the gradient fields while sparsely representing the spatial patches of the reconstructed surface in an adaptive dictionary domain. We show that our formulation learns the underlying structure of the surface, effectively acting as an adaptive regularizer that enforces a smoothness constraint on the reconstructed surface. Our method is general and may be coupled with many existing approaches in the literature to improve the integrity of the reconstructed surfaces. We demonstrate the performance of our method on synthetic data as well as real photometric stereo data and evaluate its robustness to noise.



### Fast Fine-grained Image Classification via Weakly Supervised Discriminative Localization
- **Arxiv ID**: http://arxiv.org/abs/1710.01168v1
- **DOI**: 10.1109/TCSVT.2018.2834480
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01168v1)
- **Published**: 2017-09-30 17:24:21+00:00
- **Updated**: 2017-09-30 17:24:21+00:00
- **Authors**: Xiangteng He, Yuxin Peng, Junjie Zhao
- **Comment**: 13pages, submitted to IEEE Transactions on Circuits and Systems for
  Video Technology. arXiv admin note: text overlap with arXiv:1709.08295
- **Journal**: None
- **Summary**: Fine-grained image classification is to recognize hundreds of subcategories in each basic-level category. Existing methods employ discriminative localization to find the key distinctions among subcategories. However, they generally have two limitations: (1) Discriminative localization relies on region proposal methods to hypothesize the locations of discriminative regions, which are time-consuming. (2) The training of discriminative localization depends on object or part annotations, which are heavily labor-consuming. It is highly challenging to address the two key limitations simultaneously, and existing methods only focus on one of them. Therefore, we propose a weakly supervised discriminative localization approach (WSDL) for fast fine-grained image classification to address the two limitations at the same time, and its main advantages are: (1) n-pathway end-to-end discriminative localization network is designed to improve classification speed, which simultaneously localizes multiple different discriminative regions for one image to boost classification accuracy, and shares full-image convolutional features generated by region proposal network to accelerate the process of generating region proposals as well as reduce the computation of convolutional operation. (2) Multi-level attention guided localization learning is proposed to localize discriminative regions with different focuses automatically, without using object and part annotations, avoiding the labor consumption. Different level attentions focus on different characteristics of the image, which are complementary and boost the classification accuracy. Both are jointly employed to simultaneously improve classification speed and eliminate dependence on object and part annotations. Compared with state-of-the-art methods on 2 widely-used fine-grained image classification datasets, our WSDL approach achieves the best performance.



### DeepWheat: Estimating Phenotypic Traits from Crop Images with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1710.00241v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00241v2)
- **Published**: 2017-09-30 18:31:53+00:00
- **Updated**: 2018-01-26 19:14:00+00:00
- **Authors**: Shubhra Aich, Anique Josuttes, Ilya Ovsyannikov, Keegan Strueby, Imran Ahmed, Hema Sudhakar Duddu, Curtis Pozniak, Steve Shirtliffe, Ian Stavness
- **Comment**: WACV 2018 (Code repository:
  https://github.com/p2irc/deepwheat_WACV-2018)
- **Journal**: None
- **Summary**: In this paper, we investigate estimating emergence and biomass traits from color images and elevation maps of wheat field plots. We employ a state-of-the-art deconvolutional network for segmentation and convolutional architectures, with residual and Inception-like layers, to estimate traits via high dimensional nonlinear regression. Evaluation was performed on two different species of wheat, grown in field plots for an experimental plant breeding study. Our framework achieves satisfactory performance with mean and standard deviation of absolute difference of 1.05 and 1.40 counts for emergence and 1.45 and 2.05 for biomass estimation. Our results for counting wheat plants from field images are better than the accuracy reported for the similar, but arguably less difficult, task of counting leaves from indoor images of rosette plants. Our results for biomass estimation, even with a very small dataset, improve upon all previously proposed approaches in the literature.



### Gaussian Three-Dimensional kernel SVM for Edge Detection Applications
- **Arxiv ID**: http://arxiv.org/abs/1710.01260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01260v1)
- **Published**: 2017-09-30 18:50:10+00:00
- **Updated**: 2017-09-30 18:50:10+00:00
- **Authors**: Safar Irandoust-Pakchin, Aydin Ayanzadeh, Siamak Beikzadeh
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: This paper presents a novel and uniform algorithm for edge detection based on SVM (support vector machine) with Three-dimensional Gaussian radial basis function with kernel. Because of disadvantages in traditional edge detection such as inaccurate edge location, rough edge and careless on detect soft edge. The experimental results indicate how the SVM can detect edge in efficient way. The performance of the proposed algorithm is compared with existing methods, including Sobel and canny detectors. The results show that this method is better than classical algorithm such as canny and Sobel detector.



### Fine-grained Event Learning of Human-Object Interaction with LSTM-CRF
- **Arxiv ID**: http://arxiv.org/abs/1710.00262v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1710.00262v1)
- **Published**: 2017-09-30 21:04:25+00:00
- **Updated**: 2017-09-30 21:04:25+00:00
- **Authors**: Tuan Do, James Pustejovsky
- **Comment**: European Symposium on Artificial Neural Networks, Computational
  Intelligence and Machine Learning (ESANN 2017)
- **Journal**: None
- **Summary**: Event learning is one of the most important problems in AI. However, notwithstanding significant research efforts, it is still a very complex task, especially when the events involve the interaction of humans or agents with other objects, as it requires modeling human kinematics and object movements. This study proposes a methodology for learning complex human-object interaction (HOI) events, involving the recording, annotation and classification of event interactions. For annotation, we allow multiple interpretations of a motion capture by slicing over its temporal span, for classification, we use Long-Short Term Memory (LSTM) sequential models with Conditional Randon Field (CRF) for constraints of outputs. Using a setup involving captures of human-object interaction as three dimensional inputs, we argue that this approach could be used for event types involving complex spatio-temporal dynamics.



