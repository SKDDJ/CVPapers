# Arxiv Papers in cs.CV on 2017-09-28
### Improving Dermoscopic Image Segmentation with Enhanced Convolutional-Deconvolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.09780v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09780v1)
- **Published**: 2017-09-28 02:01:11+00:00
- **Updated**: 2017-09-28 02:01:11+00:00
- **Authors**: Yading Yuan, Yeh-Chi Lo
- **Comment**: submitted to IEEE Journal of Biomedical and Health Informatics
  special issue on "Skin Lesion Analysis for Melanoma Detection"
- **Journal**: None
- **Summary**: Automatic skin lesion segmentation on dermoscopic images is an essential step in computer-aided diagnosis of melanoma. However, this task is challenging due to significant variations of lesion appearances across different patients. This challenge is further exacerbated when dealing with a large amount of image data. In this paper, we extended our previous work by developing a deeper network architecture with smaller kernels to enhance its discriminant capacity. In addition, we explicitly included color information from multiple color spaces to facilitate network training and thus to further improve the segmentation performance. We extensively evaluated our method on the ISBI 2017 skin lesion segmentation challenge. By training with the 2000 challenge training images, our method achieved an average Jaccard Index (JA) of 0.765 on the 600 challenge testing images, which ranked itself in the first place in the challenge



### Photorealistic Style Transfer with Screened Poisson Equation
- **Arxiv ID**: http://arxiv.org/abs/1709.09828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09828v1)
- **Published**: 2017-09-28 07:28:35+00:00
- **Updated**: 2017-09-28 07:28:35+00:00
- **Authors**: Roey Mechrez, Eli Shechtman, Lihi Zelnik-Manor
- **Comment**: presented in BMVC 2017
- **Journal**: None
- **Summary**: Recent work has shown impressive success in transferring painterly style to images. These approaches, however, fall short of photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. In this paper we propose an approach that takes as input a stylized image and makes it more photorealistic. It relies on the Screened Poisson Equation, maintaining the fidelity of the stylized image while constraining the gradients to those of the original input image. Our method is fast, simple, fully automatic and shows positive progress in making a stylized image photorealistic. Our results exhibit finer details and are less prone to artifacts than the state-of-the-art.



### Soft Correspondences in Multimodal Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/1709.09843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09843v1)
- **Published**: 2017-09-28 08:08:11+00:00
- **Updated**: 2017-09-28 08:08:11+00:00
- **Authors**: Sarah Taghavi Namin, Mohammad Najafi, Mathieu Salzmann, Lars Petersson
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Exploiting multiple modalities for semantic scene parsing has been shown to improve accuracy over the singlemodality scenario. However multimodal datasets often suffer from problems such as data misalignment and label inconsistencies, where the existing methods assume that corresponding regions in two modalities must have identical labels. We propose to address this issue, by formulating multimodal semantic labeling as inference in a CRF and introducing latent nodes to explicitly model inconsistencies between two modalities. These latent nodes allow us not only to leverage information from both domains to improve their labeling, but also to cut the edges between inconsistent regions. We propose to learn intradomain and inter-domain potential functions from training data to avoid hand-tuning of the model parameters. We evaluate our approach on two publicly available datasets containing 2D and 3D data. Thanks to our latent nodes and our learning strategy, our method outperforms the state-of-the-art in both cases. Moreover, in order to highlight the benefits of the geometric information and the potential of our method in simultaneous 2D/3D semantic and geometric inference, we performed simultaneous inference of semantic and geometric classes both in 2D and 3D that led to satisfactory improvements of the labeling results in both datasets.



### Distance-based Confidence Score for Neural Network Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1709.09844v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1709.09844v1)
- **Published**: 2017-09-28 08:09:47+00:00
- **Updated**: 2017-09-28 08:09:47+00:00
- **Authors**: Amit Mandelbaum, Daphna Weinshall
- **Comment**: None
- **Journal**: None
- **Summary**: The reliable measurement of confidence in classifiers' predictions is very important for many applications and is, therefore, an important part of classifier design. Yet, although deep learning has received tremendous attention in recent years, not much progress has been made in quantifying the prediction confidence of neural network classifiers. Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with prohibitive computational costs. In this paper we propose a simple, scalable method to achieve a reliable confidence score, based on the data embedding derived from the penultimate layer of the network. We investigate two ways to achieve desirable embeddings, by using either a distance-based loss or Adversarial Training. We then test the benefits of our method when used for classification error prediction, weighting an ensemble of classifiers, and novelty detection. In all tasks we show significant improvement over traditional, commonly used confidence scores.



### Recognition of Documents in Braille
- **Arxiv ID**: http://arxiv.org/abs/1709.09875v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09875v1)
- **Published**: 2017-09-28 09:51:33+00:00
- **Updated**: 2017-09-28 09:51:33+00:00
- **Authors**: Jomy John
- **Comment**: None
- **Journal**: None
- **Summary**: Visually impaired people are integral part of the society and it has been a must to provide them with means and system through which they may communicate with the world. In this work, I would like to address how computers can be made useful to read the scripts in Braille. The importance of this work is to reduce communication gap between visually impaired people and the society. Braille remains the most popular tactile reading code even in this century. There are numerous amount of literature locked up in Braille. Braille recognition not only reduces time in reading or extracting information from Braille document but also helps people engaged in special education for correcting papers and other school related works. The availability of such a system will enhance communication and collaboration possibilities with visually impaired people. Existing works supports only documents in white either bright or dull in colour. Hardly any work could be traced on hand printed ordinary documents in Braille.



### Are we done with object recognition? The iCub robot's perspective
- **Arxiv ID**: http://arxiv.org/abs/1709.09882v2
- **DOI**: 10.1016/j.robot.2018.11.001
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, I.2.9; I.2.10; I.2.11; I.4.7; I.4.8; I.4.9; I.4.10; I.5.1; I.5.2;
  I.5.4; I.5.5
- **Links**: [PDF](http://arxiv.org/pdf/1709.09882v2)
- **Published**: 2017-09-28 10:16:52+00:00
- **Updated**: 2019-01-03 14:16:09+00:00
- **Authors**: Giulia Pasquale, Carlo Ciliberto, Francesca Odone, Lorenzo Rosasco, Lorenzo Natale
- **Comment**: 21 pages + supplementary material
- **Journal**: Robotics and Autonomous Systems, Volume 112, February 2019, Pages
  260-281
- **Summary**: We report on an extensive study of the benefits and limitations of current deep learning approaches to object recognition in robot vision scenarios, introducing a novel dataset used for our investigation. To avoid the biases in currently available datasets, we consider a natural human-robot interaction setting to design a data-acquisition protocol for visual object recognition on the iCub humanoid robot. Analyzing the performance of off-the-shelf models trained off-line on large-scale image retrieval datasets, we show the necessity for knowledge transfer. We evaluate different ways in which this last step can be done, and identify the major bottlenecks affecting robotic scenarios. By studying both object categorization and identification problems, we highlight key differences between object recognition in robotics applications and in image retrieval tasks, for which the considered deep learning approaches have been originally designed. In a nutshell, our results confirm the remarkable improvements yield by deep learning in this setting, while pointing to specific open challenges that need be addressed for seamless deployment in robotics.



### Efficient Convolutional Neural Network For Audio Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1709.09888v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1709.09888v1)
- **Published**: 2017-09-28 10:54:01+00:00
- **Updated**: 2017-09-28 10:54:01+00:00
- **Authors**: Matthias Meyer, Lukas Cavigelli, Lothar Thiele
- **Comment**: None
- **Journal**: None
- **Summary**: Wireless distributed systems as used in sensor networks, Internet-of-Things and cyber-physical systems, impose high requirements on resource efficiency. Advanced preprocessing and classification of data at the network edge can help to decrease the communication demand and to reduce the amount of data to be processed centrally. In the area of distributed acoustic sensing, the combination of algorithms with a high classification rate and resource-constraint embedded systems is essential. Unfortunately, algorithms for acoustic event detection have a high memory and computational demand and are not suited for execution at the network edge. This paper addresses these aspects by applying structural optimizations to a convolutional neural network for audio event detection to reduce the memory requirement by a factor of more than 500 and the computational effort by a factor of 2.1 while performing 9.2% better.



### B-CNN: Branch Convolutional Neural Network for Hierarchical Classification
- **Arxiv ID**: http://arxiv.org/abs/1709.09890v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09890v2)
- **Published**: 2017-09-28 11:02:43+00:00
- **Updated**: 2017-10-05 08:14:57+00:00
- **Authors**: Xinqi Zhu, Michael Bain
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) image classifiers are traditionally designed to have sequential convolutional layers with a single output layer. This is based on the assumption that all target classes should be treated equally and exclusively. However, some classes can be more difficult to distinguish than others, and classes may be organized in a hierarchy of categories. At the same time, a CNN is designed to learn internal representations that abstract from the input data based on its hierarchical layered structure. So it is natural to ask if an inverse of this idea can be applied to learn a model that can predict over a classification hierarchy using multiple output layers in decreasing order of class abstraction. In this paper, we introduce a variant of the traditional CNN model named the Branch Convolutional Neural Network (B-CNN). A B-CNN model outputs multiple predictions ordered from coarse to fine along the concatenated convolutional layers corresponding to the hierarchical structure of the target classes, which can be regarded as a form of prior knowledge on the output. To learn with B-CNNs a novel training strategy, named the Branch Training strategy (BT-strategy), is introduced which balances the strictness of the prior with the freedom to adjust parameters on the output layers to minimize the loss. In this way we show that CNN based models can be forced to learn successively coarse to fine concepts in the internal layers at the output stage, and that hierarchical prior knowledge can be adopted to boost CNN models' classification performance. Our models are evaluated to show that the B-CNN extensions improve over the corresponding baseline CNN on the benchmark datasets MNIST, CIFAR-10 and CIFAR-100.



### Improving Efficiency in Convolutional Neural Network with Multilinear Filters
- **Arxiv ID**: http://arxiv.org/abs/1709.09902v3
- **DOI**: 10.1016/j.neunet.2018.05.017
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1709.09902v3)
- **Published**: 2017-09-28 11:55:13+00:00
- **Updated**: 2017-10-23 15:42:11+00:00
- **Authors**: Dat Thanh Tran, Alexandros Iosifidis, Moncef Gabbouj
- **Comment**: 10 pages, 3 figures
- **Journal**: Neural Networks vol. 105, pp. 328-339, 2018
- **Summary**: The excellent performance of deep neural networks has enabled us to solve several automatization problems, opening an era of autonomous devices. However, current deep net architectures are heavy with millions of parameters and require billions of floating point operations. Several works have been developed to compress a pre-trained deep network to reduce memory footprint and, possibly, computation. Instead of compressing a pre-trained network, in this work, we propose a generic neural network layer structure employing multilinear projection as the primary feature extractor. The proposed architecture requires several times less memory as compared to the traditional Convolutional Neural Networks (CNN), while inherits the similar design principles of a CNN. In addition, the proposed architecture is equipped with two computation schemes that enable computation reduction or scalability. Experimental results show the effectiveness of our compact projection that outperforms traditional CNN, while requiring far fewer parameters.



### X-View: Graph-Based Semantic Multi-View Localization
- **Arxiv ID**: http://arxiv.org/abs/1709.09905v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.09905v3)
- **Published**: 2017-09-28 11:58:58+00:00
- **Updated**: 2018-02-16 11:26:34+00:00
- **Authors**: Abel Gawel, Carlo Del Don, Roland Siegwart, Juan Nieto, Cesar Cadena
- **Comment**: None
- **Journal**: None
- **Summary**: Global registration of multi-view robot data is a challenging task. Appearance-based global localization approaches often fail under drastic view-point changes, as representations have limited view-point invariance. This work is based on the idea that human-made environments contain rich semantics which can be used to disambiguate global localization. Here, we present X-View, a Multi-View Semantic Global Localization system. X-View leverages semantic graph descriptor matching for global localization, enabling localization under drastically different view-points. While the approach is general in terms of the semantic input data, we present and evaluate an implementation on visual data. We demonstrate the system in experiments on the publicly available SYNTHIA dataset, on a realistic urban dataset recorded with a simulator, and on real-world StreetView data. Our findings show that X-View is able to globally localize aerial-to-ground, and ground-to-ground robot data of drastically different view-points. Our approach achieves an accuracy of up to 85 % on global localizations in the multi-view case, while the benchmarked baseline appearance-based methods reach up to 75 %.



### HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis
- **Arxiv ID**: http://arxiv.org/abs/1709.09930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09930v1)
- **Published**: 2017-09-28 13:06:55+00:00
- **Updated**: 2017-09-28 13:06:55+00:00
- **Authors**: Xihui Liu, Haiyu Zhao, Maoqing Tian, Lu Sheng, Jing Shao, Shuai Yi, Junjie Yan, Xiaogang Wang
- **Comment**: Accepted by ICCV 2017
- **Journal**: None
- **Summary**: Pedestrian analysis plays a vital role in intelligent video surveillance and is a key component for security-centric computer vision systems. Despite that the convolutional neural networks are remarkable in learning discriminative features from images, the learning of comprehensive features of pedestrians for fine-grained tasks remains an open problem. In this study, we propose a new attention-based deep neural network, named as HydraPlus-Net (HP-net), that multi-directionally feeds the multi-level attention maps to different feature layers. The attentive deep features learned from the proposed HP-net bring unique advantages: (1) the model is capable of capturing multiple attentions from low-level to semantic-level, and (2) it explores the multi-scale selectiveness of attentive features to enrich the final feature representations for a pedestrian image. We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e. pedestrian attribute recognition and person re-identification. Intensive experimental results have been provided to prove that the HP-net outperforms the state-of-the-art methods on various datasets.



### Recognition of feature curves on 3D shapes using an algebraic approach to Hough transforms
- **Arxiv ID**: http://arxiv.org/abs/1709.10177v1
- **DOI**: 10.1016/j.patcog.2017.08.008
- **Categories**: **cs.CV**, 8U05, 65D18
- **Links**: [PDF](http://arxiv.org/pdf/1709.10177v1)
- **Published**: 2017-09-28 21:36:05+00:00
- **Updated**: 2017-09-28 21:36:05+00:00
- **Authors**: Maria-Laura Torrente, Silvia Biasotti, Bianca Falcidieno
- **Comment**: None
- **Journal**: Pattern Recognition, Volume 73, Pages 1-288 (January 2018)
- **Summary**: Feature curves are largely adopted to highlight shape features, such as sharp lines, or to divide surfaces into meaningful segments, like convex or concave regions. Extracting these curves is not sufficient to convey prominent and meaningful information about a shape. We have first to separate the curves belonging to features from those caused by noise and then to select the lines, which describe non-trivial portions of a surface. The automatic detection of such features is crucial for the identification and/or annotation of relevant parts of a given shape. To do this, the Hough transform (HT) is a feature extraction technique widely used in image analysis, computer vision and digital image processing, while, for 3D shapes, the extraction of salient feature curves is still an open problem.   Thanks to algebraic geometry concepts, the HT technique has been recently extended to include a vast class of algebraic curves, thus proving to be a competitive tool for yielding an explicit representation of the diverse feature lines equations. In the paper, for the first time we apply this novel extension of the HT technique to the realm of 3D shapes in order to identify and localize semantic features like patterns, decorations or anatomical details on 3D objects (both complete and fragments), even in the case of features partially damaged or incomplete. The method recognizes various features, possibly compound, and it selects the most suitable feature profiles among families of algebraic curves.



### Possibilistic Fuzzy Local Information C-Means for Sonar Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1709.10180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.10180v1)
- **Published**: 2017-09-28 21:47:50+00:00
- **Updated**: 2017-09-28 21:47:50+00:00
- **Authors**: Alina Zare, Nicholas Young, Daniel Suen, Thomas Nabelek, Aquila Galusha, James Keller
- **Comment**: 8 pages, 11 figures, to appear in the 2017 IEEE Symposium Series on
  Computational Intelligence (SSCI) Proceedings
- **Journal**: None
- **Summary**: Side-look synthetic aperture sonar (SAS) can produce very high quality images of the sea-floor. When viewing this imagery, a human observer can often easily identify various sea-floor textures such as sand ripple, hard-packed sand, sea grass and rock. In this paper, we present the Possibilistic Fuzzy Local Information C-Means (PFLICM) approach to segment SAS imagery into sea-floor regions that exhibit these various natural textures. The proposed PFLICM method incorporates fuzzy and possibilistic clustering methods and leverages (local) spatial information to perform soft segmentation. Results are shown on several SAS scenes and compared to alternative segmentation approaches.



### Unified Deep Supervised Domain Adaptation and Generalization
- **Arxiv ID**: http://arxiv.org/abs/1709.10190v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.10190v1)
- **Published**: 2017-09-28 22:35:36+00:00
- **Updated**: 2017-09-28 22:35:36+00:00
- **Authors**: Saeid Motiian, Marco Piccirilli, Donald A. Adjeroh, Gianfranco Doretto
- **Comment**: International Conference on Computer Vision ICCV 2017
- **Journal**: None
- **Summary**: This work provides a unified framework for addressing the problem of visual supervised domain adaptation and generalization with deep models. The main idea is to exploit the Siamese architecture to learn an embedding subspace that is discriminative, and where mapped visual domains are semantically aligned and yet maximally separated. The supervised setting becomes attractive especially when only few target data samples need to be labeled. In this scenario, alignment and separation of semantic probability distributions is difficult because of the lack of data. We found that by reverting to point-wise surrogates of distribution distances and similarities provides an effective solution. In addition, the approach has a high speed of adaptation, which requires an extremely low number of labeled target training samples, even one per category can be effective. The approach is extended to domain generalization. For both applications the experiments show very promising results.



### Fast Barcode Retrieval for Consensus Contouring
- **Arxiv ID**: http://arxiv.org/abs/1709.10197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.10197v1)
- **Published**: 2017-09-28 23:28:49+00:00
- **Updated**: 2017-09-28 23:28:49+00:00
- **Authors**: H. R. Tizhoosh, G. J. Czarnota
- **Comment**: Images used in this paper are available to the public:
  http://kimia.uwaterloo.ca/
- **Journal**: None
- **Summary**: Marking tumors and organs is a challenging task suffering from both inter- and intra-observer variability. The literature quantifies observer variability by generating consensus among multiple experts when they mark the same image. Automatically building consensus contours to establish quality assurance for image segmentation is presently absent in the clinical practice. As the \emph{big data} becomes more and more available, techniques to access a large number of existing segments of multiple experts becomes possible. Fast algorithms are, hence, required to facilitate the search for similar cases. The present work puts forward a potential framework that tested with small datasets (both synthetic and real images) displays the reliability of finding similar images. In this paper, the idea of content-based barcodes is used to retrieve similar cases in order to build consensus contours in medical image segmentation. This approach may be regarded as an extension of the conventional atlas-based segmentation that generally works with rather small atlases due to required computational expenses. The fast segment-retrieval process via barcodes makes it possible to create and use large atlases, something that directly contributes to the quality of the consensus building. Because the accuracy of experts' contours must be measured, we first used 500 synthetic prostate images with their gold markers and delineations by 20 simulated users. The fast barcode-guided computed consensus delivered an average error of $8\%\!\pm\!5\%$ compared against the gold standard segments. Furthermore, we used magnetic resonance images of prostates from 15 patients delineated by 5 oncologists and selected the best delineations to serve as the gold-standard segments. The proposed barcode atlas achieved a Jaccard overlap of $87\%\!\pm\!9\%$ with the contours of the gold-standard segments.



