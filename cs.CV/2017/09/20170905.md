# Arxiv Papers in cs.CV on 2017-09-05
### Multi-View Spectral Clustering via Structured Low-Rank Matrix Factorization
- **Arxiv ID**: http://arxiv.org/abs/1709.01212v3
- **DOI**: 10.1109/TNNLS.2017.2777489
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01212v3)
- **Published**: 2017-09-05 01:54:43+00:00
- **Updated**: 2017-12-07 00:33:05+00:00
- **Authors**: Yang Wang, Lin Wu
- **Comment**: Accepted to appear at IEEE Trans on Neural Networks and Learning
  Systems
- **Journal**: None
- **Summary**: Multi-view data clustering attracts more attention than their single view counterparts due to the fact that leveraging multiple independent and complementary information from multi-view feature spaces outperforms the single one. Multi-view Spectral Clustering aims at yielding the data partition agreement over their local manifold structures by seeking eigenvalue-eigenvector decompositions. However, as we observed, such classical paradigm still suffers from (1) overlooking the flexible local manifold structure, caused by (2) enforcing the low-rank data correlation agreement among all views; worse still, (3) LRR is not intuitively flexible to capture the latent data clustering structures. In this paper, we present the structured LRR by factorizing into the latent low-dimensional data-cluster representations, which characterize the data clustering structure for each view. Upon such representation, (b) the laplacian regularizer is imposed to be capable of preserving the flexible local manifold structure for each view. (c) We present an iterative multi-view agreement strategy by minimizing the divergence objective among all factorized latent data-cluster representations during each iteration of optimization process, where such latent representation from each view serves to regulate those from other views, such intuitive process iteratively coordinates all views to be agreeable. (d) We remark that such data-cluster representation can flexibly encode the data clustering structure from any view with adaptive input cluster number. To this end, (e) a novel non-convex objective function is proposed via the efficient alternating minimization strategy. The complexity analysis are also presented. The extensive experiments conducted against the real-world multi-view datasets demonstrate the superiority over state-of-the-arts.



### ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching
- **Arxiv ID**: http://arxiv.org/abs/1709.01215v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1709.01215v2)
- **Published**: 2017-09-05 02:18:06+00:00
- **Updated**: 2017-11-05 03:58:52+00:00
- **Authors**: Chunyuan Li, Hao Liu, Changyou Chen, Yunchen Pu, Liqun Chen, Ricardo Henao, Lawrence Carin
- **Comment**: NIPS 2017 (22 pages); short version (9 pages):
  http://people.duke.edu/~cl319/doc/papers/nips_2017_alice.pdf
- **Journal**: None
- **Summary**: We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.



### Multi-Modal Multi-Scale Deep Learning for Large-Scale Image Annotation
- **Arxiv ID**: http://arxiv.org/abs/1709.01220v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01220v2)
- **Published**: 2017-09-05 02:50:45+00:00
- **Updated**: 2018-10-19 01:35:38+00:00
- **Authors**: Yulei Niu, Zhiwu Lu, Ji-Rong Wen, Tao Xiang, Shih-Fu Chang
- **Comment**: Submited to IEEE TIP
- **Journal**: None
- **Summary**: Image annotation aims to annotate a given image with a variable number of class labels corresponding to diverse visual concepts. In this paper, we address two main issues in large-scale image annotation: 1) how to learn a rich feature representation suitable for predicting a diverse set of visual concepts ranging from object, scene to abstract concept; 2) how to annotate an image with the optimal number of class labels. To address the first issue, we propose a novel multi-scale deep model for extracting rich and discriminative features capable of representing a wide range of visual concepts. Specifically, a novel two-branch deep neural network architecture is proposed which comprises a very deep main network branch and a companion feature fusion network branch designed for fusing the multi-scale features computed from the main branch. The deep model is also made multi-modal by taking noisy user-provided tags as model input to complement the image input. For tackling the second issue, we introduce a label quantity prediction auxiliary task to the main label prediction task to explicitly estimate the optimal label number for a given image. Extensive experiments are carried out on two large-scale image annotation benchmark datasets and the results show that our method significantly outperforms the state-of-the-art.



### Newton-type Methods for Inference in Higher-Order Markov Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1709.01237v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1709.01237v1)
- **Published**: 2017-09-05 04:55:47+00:00
- **Updated**: 2017-09-05 04:55:47+00:00
- **Authors**: Hariprasad Kannan, Nikos Komodakis, Nikos Paragios
- **Comment**: 10 pages, 3 figures, 3 tables, CVPR 2017
- **Journal**: Poster at IEEE International Conference on Computer Vision and
  Pattern Recognition 2017
- **Summary**: Linear programming relaxations are central to {\sc map} inference in discrete Markov Random Fields. The ability to properly solve the Lagrangian dual is a critical component of such methods. In this paper, we study the benefit of using Newton-type methods to solve the Lagrangian dual of a smooth version of the problem. We investigate their ability to achieve superior convergence behavior and to better handle the ill-conditioned nature of the formulation, as compared to first order methods. We show that it is indeed possible to efficiently apply a trust region Newton method for a broad range of {\sc map} inference problems. In this paper we propose a provably convergent and efficient framework that includes (i) excellent compromise between computational complexity and precision concerning the Hessian matrix construction, (ii) a damping strategy that aids efficient optimization, (iii) a truncation strategy coupled with a generic pre-conditioner for Conjugate Gradients, (iv) efficient sum-product computation for sparse clique potentials. Results for higher-order Markov Random Fields demonstrate the potential of this approach.



### Linear vs Nonlinear Extreme Learning Machine for Spectral-Spatial Classification of Hyperspectral Image
- **Arxiv ID**: http://arxiv.org/abs/1709.02253v2
- **DOI**: 10.3390/s17112603
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.02253v2)
- **Published**: 2017-09-05 06:53:02+00:00
- **Updated**: 2017-10-12 02:16:40+00:00
- **Authors**: Faxian Cao, Zhijing Yang, Jinchang Ren, Mengying Jiang, Wing-Kuen Ling
- **Comment**: 13 pages,8 figures,3 tables,article
- **Journal**: Sensors,17,2017,2603
- **Summary**: As a new machine learning approach, extreme learning machine (ELM) has received wide attentions due to its good performances. However, when directly applied to the hyperspectral image (HSI) classification, the recognition rate is too low. This is because ELM does not use the spatial information which is very important for HSI classification. In view of this, this paper proposes a new framework for spectral-spatial classification of HSI by combining ELM with loopy belief propagation (LBP). The original ELM is linear, and the nonlinear ELMs (or Kernel ELMs) are the improvement of linear ELM (LELM). However, based on lots of experiments and analysis, we found out that the LELM is a better choice than nonlinear ELM for spectral-spatial classification of HSI. Furthermore, we exploit the marginal probability distribution that uses the whole information in the HSI and learn such distribution using the LBP. The proposed method not only maintain the fast speed of ELM, but also greatly improves the accuracy of classification. The experimental results in the well-known HSI data sets, Indian Pines and Pavia University, demonstrate the good performances of the proposed method.



### SketchParse : Towards Rich Descriptions for Poorly Drawn Sketches using Multi-Task Hierarchical Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.01295v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1709.01295v1)
- **Published**: 2017-09-05 09:10:59+00:00
- **Updated**: 2017-09-05 09:10:59+00:00
- **Authors**: Ravi Kiran Sarvadevabhatla, Isht Dwivedi, Abhijat Biswas, Sahil Manocha, R. Venkatesh Babu
- **Comment**: A shorter version of this submission was accepted at ACM Multimedia
  (ACMMM) 2017. Code, annotated datasets and pre-trained models available at
  https://github.com/val-iisc/sketch-parse
- **Journal**: None
- **Summary**: The ability to semantically interpret hand-drawn line sketches, although very challenging, can pave way for novel applications in multimedia. We propose SketchParse, the first deep-network architecture for fully automatic parsing of freehand object sketches. SketchParse is configured as a two-level fully convolutional network. The first level contains shared layers common to all object categories. The second level contains a number of expert sub-networks. Each expert specializes in parsing sketches from object categories which contain structurally similar parts. Effectively, the two-level configuration enables our architecture to scale up efficiently as additional categories are added. We introduce a router layer which (i) relays sketch features from shared layers to the correct expert (ii) eliminates the need to manually specify object category during inference. To bypass laborious part-level annotation, we sketchify photos from semantic object-part image datasets and use them for training. Our architecture also incorporates object pose prediction as a novel auxiliary task which boosts overall performance while providing supplementary information regarding the sketch. We demonstrate SketchParse's abilities (i) on two challenging large-scale sketch datasets (ii) in parsing unseen, semantically related object categories (iii) in improving fine-grained sketch-based image retrieval. As a novel application, we also outline how SketchParse's output can be used to generate caption-style descriptions for hand-drawn sketches.



### Cross-Media Similarity Evaluation for Web Image Retrieval in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1709.01305v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01305v2)
- **Published**: 2017-09-05 09:38:32+00:00
- **Updated**: 2018-01-06 06:09:12+00:00
- **Authors**: Jianfeng Dong, Xirong Li, Duanqing Xu
- **Comment**: 14 pages, 10 figures, accepted by IEEE Transactions on Multimedia
  2018
- **Journal**: None
- **Summary**: In order to retrieve unlabeled images by textual queries, cross-media similarity computation is a key ingredient. Although novel methods are continuously introduced, little has been done to evaluate these methods together with large-scale query log analysis. Consequently, how far have these methods brought us in answering real-user queries is unclear. Given baseline methods that compute cross-media similarity using relatively simple text/image matching, how much progress have advanced models made is also unclear. This paper takes a pragmatic approach to answering the two questions. Queries are automatically categorized according to the proposed query visualness measure, and later connected to the evaluation of multiple cross-media similarity models on three test sets. Such a connection reveals that the success of the state-of-the-art is mainly attributed to their good performance on visual-oriented queries, while these queries account for only a small part of real-user queries. To quantify the current progress, we propose a simple text2image method, representing a novel test query by a set of images selected from large-scale query log. Consequently, computing cross-media similarity between the test query and a given image boils down to comparing the visual similarity between the given image and the selected images. Image retrieval experiments on the challenging Clickture dataset show that the proposed text2image compares favorably to recent deep learning based alternatives.



### Learning Non-Metric Visual Similarity for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1709.01353v2
- **DOI**: 10.1016/j.imavis.2019.01.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01353v2)
- **Published**: 2017-09-05 12:39:30+00:00
- **Updated**: 2019-04-10 02:06:00+00:00
- **Authors**: Noa Garcia, George Vogiatzis
- **Comment**: Image and Vision Computing (2019)
- **Journal**: None
- **Summary**: Measuring visual similarity between two or more instances within a data distribution is a fundamental task in image retrieval. Theoretically, non-metric distances are able to generate a more complex and accurate similarity model than metric distances, provided that the non-linear data distribution is precisely captured by the system. In this work, we explore neural networks models for learning a non-metric similarity function for instance search. We argue that non-metric similarity functions based on neural networks can build a better model of human visual perception than standard metric distances. As our proposed similarity function is differentiable, we explore a real end-to-end trainable approach for image retrieval, i.e. we learn the weights from the input image pixels to the final similarity score. Experimental evaluation shows that non-metric similarity networks are able to learn visual similarities between images and improve performance on top of state-of-the-art image representations, boosting results in standard image retrieval datasets with respect standard metric distances.



### Visualizing and Improving Scattering Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.01355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01355v1)
- **Published**: 2017-09-05 12:41:05+00:00
- **Updated**: 2017-09-05 12:41:05+00:00
- **Authors**: Fergal Cotter, Nick Kingsbury
- **Comment**: To Appear in the 27th IEEE International Workshop on Machine Learning
  For Signal Processing (MLSP) 2017. 6 pages, 3 figures
- **Journal**: None
- **Summary**: Scattering Transforms (or ScatterNets) introduced by Mallat are a promising start into creating a well-defined feature extractor to use for pattern recognition and image classification tasks. They are of particular interest due to their architectural similarity to Convolutional Neural Networks (CNNs), while requiring no parameter learning and still performing very well (particularly in constrained classification tasks).   In this paper we visualize what the deeper layers of a ScatterNet are sensitive to using a 'DeScatterNet'. We show that the higher orders of ScatterNets are sensitive to complex, edge-like patterns (checker-boards and rippled edges). These complex patterns may be useful for texture classification, but are quite dissimilar from the patterns visualized in second and third layers of Convolutional Neural Networks (CNNs) - the current state of the art Image Classifiers. We propose that this may be the source of the current gaps in performance between ScatterNets and CNNs (83% vs 93% on CIFAR-10 for ScatterNet+SVM vs ResNet). We then use these visualization tools to propose possible enhancements to the ScatterNet design, which show they have the power to extract features more closely resembling CNNs, while still being well-defined and having the invariance properties fundamental to ScatterNets.



### Photometric stereo for strong specular highlights
- **Arxiv ID**: http://arxiv.org/abs/1709.01357v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1709.01357v1)
- **Published**: 2017-09-05 12:44:32+00:00
- **Updated**: 2017-09-05 12:44:32+00:00
- **Authors**: Maryam Khanian, Ali Sharifi Boroujerdi, Michael Breuß
- **Comment**: None
- **Journal**: None
- **Summary**: Photometric stereo (PS) is a fundamental technique in computer vision known to produce 3-D shape with high accuracy. The setting of PS is defined by using several input images of a static scene taken from one and the same camera position but under varying illumination. The vast majority of studies in this 3-D reconstruction method assume orthographic projection for the camera model. In addition, they mainly consider the Lambertian reflectance model as the way that light scatters at surfaces. So, providing reliable PS results from real world objects still remains a challenging task. We address 3-D reconstruction by PS using a more realistic set of assumptions combining for the first time the complete Blinn-Phong reflectance model and perspective projection. To this end, we will compare two different methods of incorporating the perspective projection into our model. Experiments are performed on both synthetic and real world images. Note that our real-world experiments do not benefit from laboratory conditions. The results show the high potential of our method even for complex real world applications such as medical endoscopy images which may include high amounts of specular highlights.



### Predicting Visual Features from Text for Image and Video Caption Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1709.01362v3
- **DOI**: 10.1109/TMM.2018.2832602
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01362v3)
- **Published**: 2017-09-05 12:55:44+00:00
- **Updated**: 2018-07-14 12:01:20+00:00
- **Authors**: Jianfeng Dong, Xirong Li, Cees G. M. Snoek
- **Comment**: Accepted by Transaction on Multimedia. Code is available at
  https://github.com/danieljf24/w2vv
- **Journal**: None
- **Summary**: This paper strives to find amidst a set of sentences the one best describing the content of a given image or video. Different from existing works, which rely on a joint subspace for their image and video caption retrieval, we propose to do so in a visual space exclusively. Apart from this conceptual novelty, we contribute \emph{Word2VisualVec}, a deep neural network architecture that learns to predict a visual feature representation from textual input. Example captions are encoded into a textual embedding based on multi-scale sentence vectorization and further transferred into a deep visual feature of choice via a simple multi-layer perceptron. We further generalize Word2VisualVec for video caption retrieval, by predicting from text both 3-D convolutional neural network features as well as a visual-audio representation. Experiments on Flickr8k, Flickr30k, the Microsoft Video Description dataset and the very recent NIST TrecVid challenge for video caption retrieval detail Word2VisualVec's properties, its benefit over textual embeddings, the potential for multimodal query composition and its state-of-the-art results.



### Multi-label Class-imbalanced Action Recognition in Hockey Videos via 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.01421v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.01421v2)
- **Published**: 2017-09-05 14:44:20+00:00
- **Updated**: 2018-05-03 15:23:28+00:00
- **Authors**: Konstantin Sozykin, Stanislav Protasov, Adil Khan, Rasheed Hussain, Jooyoung Lee
- **Comment**: Accepted to IEEE/ACIS SNPD 2018, 6 pages, 3 figures
- **Journal**: None
- **Summary**: Automatic analysis of the video is one of most complex problems in the fields of computer vision and machine learning. A significant part of this research deals with (human) activity recognition (HAR) since humans, and the activities that they perform, generate most of the video semantics. Video-based HAR has applications in various domains, but one of the most important and challenging is HAR in sports videos. Some of the major issues include high inter- and intra-class variations, large class imbalance, the presence of both group actions and single player actions, and recognizing simultaneous actions, i.e., the multi-label learning problem. Keeping in mind these challenges and the recent success of CNNs in solving various computer vision problems, in this work, we implement a 3D CNN based multi-label deep HAR system for multi-label class-imbalanced action recognition in hockey videos. We test our system for two different scenarios: an ensemble of $k$ binary networks vs. a single $k$-output network, on a publicly available dataset. We also compare our results with the system that was originally designed for the chosen dataset. Experimental results show that the proposed approach performs better than the existing solution.



### Towards social pattern characterization in egocentric photo-streams
- **Arxiv ID**: http://arxiv.org/abs/1709.01424v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01424v3)
- **Published**: 2017-09-05 14:50:00+00:00
- **Updated**: 2018-01-09 11:14:53+00:00
- **Authors**: Maedeh Aghaei, Mariella Dimiccoli, Cristian Canton Ferrer, Petia Radeva
- **Comment**: 42 pages, 14 figures. Submitted to Elsevier, Computer Vision and
  Image Understanding (Under Review)
- **Journal**: None
- **Summary**: Following the increasingly popular trend of social interaction analysis in egocentric vision, this manuscript presents a comprehensive study for automatic social pattern characterization of a wearable photo-camera user, by relying on the visual analysis of egocentric photo-streams. The proposed framework consists of three major steps. The first step is to detect social interactions of the user where the impact of several social signals on the task is explored. The detected social events are inspected in the second step for categorization into different social meetings. These two steps act at event-level where each potential social event is modeled as a multi-dimensional time-series, whose dimensions correspond to a set of relevant features for each task, and LSTM is employed to classify the time-series. The last step of the framework is to characterize social patterns, which is essentially to infer the diversity and frequency of the social relations of the user through discovery of recurrences of the same people across the whole set of social events of the user. Experimental evaluation over a dataset acquired by 9 users demonstrates promising results on the task of social pattern characterization from egocentric photo-streams.



### Dense Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1709.01442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01442v1)
- **Published**: 2017-09-05 15:14:32+00:00
- **Updated**: 2017-09-05 15:14:32+00:00
- **Authors**: Yaojie Liu, Amin Jourabloo, William Ren, Xiaoming Liu
- **Comment**: To appear in ICCV 2017 Workshop
- **Journal**: None
- **Summary**: Face alignment is a classic problem in the computer vision field. Previous works mostly focus on sparse alignment with a limited number of facial landmark points, i.e., facial landmark detection. In this paper, for the first time, we aim at providing a very dense 3D alignment for large-pose face images. To achieve this, we train a CNN to estimate the 3D face shape, which not only aligns limited facial landmarks but also fits face contours and SIFT feature points. Moreover, we also address the bottleneck of training CNN with multiple datasets, due to different landmark markups on different datasets, such as 5, 34, 68. Experimental results show our method not only provides high-quality, dense 3D face fitting but also outperforms the state-of-the-art facial landmark detection methods on the challenging datasets. Our model can run at real time during testing.



### The Devil is in the Tails: Fine-grained Classification in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1709.01450v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01450v1)
- **Published**: 2017-09-05 15:26:47+00:00
- **Updated**: 2017-09-05 15:26:47+00:00
- **Authors**: Grant Van Horn, Pietro Perona
- **Comment**: None
- **Journal**: None
- **Summary**: The world is long-tailed. What does this mean for computer vision and visual recognition? The main two implications are (1) the number of categories we need to consider in applications can be very large, and (2) the number of training examples for most categories can be very small. Current visual recognition algorithms have achieved excellent classification accuracy. However, they require many training examples to reach peak performance, which suggests that long-tailed distributions will not be dealt with well. We analyze this question in the context of eBird, a large fine-grained classification dataset, and a state-of-the-art deep network classification algorithm. We find that (a) peak classification performance on well-represented categories is excellent, (b) given enough data, classification performance suffers only minimally from an increase in the number of classes, (c) classification performance decays precipitously as the number of training examples decreases, (d) surprisingly, transfer learning is virtually absent in current methods. Our findings suggest that our community should come to grips with the question of long tails.



### 6D Object Pose Estimation with Depth Images: A Seamless Approach for Robotic Interaction and Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/1709.01459v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01459v1)
- **Published**: 2017-09-05 15:38:26+00:00
- **Updated**: 2017-09-05 15:38:26+00:00
- **Authors**: David Joseph Tan, Nassir Navab, Federico Tombari
- **Comment**: None
- **Journal**: None
- **Summary**: To determine the 3D orientation and 3D location of objects in the surroundings of a camera mounted on a robot or mobile device, we developed two powerful algorithms in object detection and temporal tracking that are combined seamlessly for robotic perception and interaction as well as Augmented Reality (AR). A separate evaluation of, respectively, the object detection and the temporal tracker demonstrates the important stride in research as well as the impact on industrial robotic applications and AR. When evaluated on a standard dataset, the detector produced the highest f1-score with a large margin while the tracker generated the best accuracy at a very low latency of approximately 2 ms per frame with one CPU core: both algorithms outperforming the state of the art. When combined, we achieve a powerful framework that is robust to handle multiple instances of the same object under occlusion and clutter while attaining real-time performance. Aiming at stepping beyond the simple scenarios used by current systems, often constrained by having a single object in absence of clutter, averting to touch the object to prevent close-range partial occlusion, selecting brightly colored objects to easily segment them individually or assuming that the object has simple geometric structure, we demonstrate the capacity to handle challenging cases under clutter, partial occlusion and varying lighting conditions with objects of different shapes and sizes.



### Subspace Segmentation by Successive Approximations: A Method for Low-Rank and High-Rank Data with Missing Entries
- **Arxiv ID**: http://arxiv.org/abs/1709.01467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01467v1)
- **Published**: 2017-09-05 15:58:30+00:00
- **Updated**: 2017-09-05 15:58:30+00:00
- **Authors**: João Carvalho, Manuel Marques, João P. Costeira
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method to reconstruct and cluster incomplete high-dimensional data lying in a union of low-dimensional subspaces. Exploring the sparse representation model, we jointly estimate the missing data while imposing the intrinsic subspace structure. Since we have a non-convex problem, we propose an iterative method to reconstruct the data and provide a sparse similarity affinity matrix. This method is robust to initialization and achieves greater reconstruction accuracy than current methods, which dramatically improves clustering performance. Extensive experiments with synthetic and real data show that our approach leads to significant improvements in the reconstruction and segmentation, outperforming current state of the art for both low and high-rank data.



### Leveraging multiple datasets for deep leaf counting
- **Arxiv ID**: http://arxiv.org/abs/1709.01472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01472v1)
- **Published**: 2017-09-05 16:09:18+00:00
- **Updated**: 2017-09-05 16:09:18+00:00
- **Authors**: Andrei Dobrescu, Mario Valerio Giuffrida, Sotirios A Tsaftaris
- **Comment**: 8 pages, 3 figures, 3 tables
- **Journal**: CVPPP workshop 2017, ICCV
- **Summary**: The number of leaves a plant has is one of the key traits (phenotypes) describing its development and growth. Here, we propose an automated, deep learning based approach for counting leaves in model rosette plants. While state-of-the-art results on leaf counting with deep learning methods have recently been reported, they obtain the count as a result of leaf segmentation and thus require per-leaf (instance) segmentation to train the models (a rather strong annotation). Instead, our method treats leaf counting as a direct regression problem and thus only requires as annotation the total leaf count per plant. We argue that combining different datasets when training a deep neural network is beneficial and improves the results of the proposed approach. We evaluate our method on the CVPPP 2017 Leaf Counting Challenge dataset, which contains images of Arabidopsis and tobacco plants. Experimental results show that the proposed method significantly outperforms the winner of the previous CVPPP challenge, improving the results by a minimum of ~50% on each of the test datasets, and can achieve this performance without knowing the experimental origin of the data (i.e. in the wild setting of the challenge). We also compare the counting accuracy of our model with that of per leaf segmentation algorithms, achieving a 20% decrease in mean absolute difference in count (|DiC|).



### Fine-tuning deep CNN models on specific MS COCO categories
- **Arxiv ID**: http://arxiv.org/abs/1709.01476v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.01476v1)
- **Published**: 2017-09-05 16:22:28+00:00
- **Updated**: 2017-09-05 16:22:28+00:00
- **Authors**: Daniel Sonntag, Michael Barz, Jan Zacharias, Sven Stauden, Vahid Rahmani, Áron Fóthi, András Lőrincz
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-tuning of a deep convolutional neural network (CNN) is often desired. This paper provides an overview of our publicly available py-faster-rcnn-ft software library that can be used to fine-tune the VGG_CNN_M_1024 model on custom subsets of the Microsoft Common Objects in Context (MS COCO) dataset. For example, we improved the procedure so that the user does not have to look for suitable image files in the dataset by hand which can then be used in the demo program. Our implementation randomly selects images that contain at least one object of the categories on which the model is fine-tuned.



### SeDAR - Semantic Detection and Ranging: Humans can localise without LiDAR, can robots?
- **Arxiv ID**: http://arxiv.org/abs/1709.01500v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.01500v2)
- **Published**: 2017-09-05 17:35:07+00:00
- **Updated**: 2018-05-02 09:02:29+00:00
- **Authors**: Oscar Mendez, Simon Hadfield, Nicolas Pugeault, Richard Bowden
- **Comment**: None
- **Journal**: None
- **Summary**: How does a person work out their location using a floorplan? It is probably safe to say that we do not explicitly measure depths to every visible surface and try to match them against different pose estimates in the floorplan. And yet, this is exactly how most robotic scan-matching algorithms operate. Similarly, we do not extrude the 2D geometry present in the floorplan into 3D and try to align it to the real-world. And yet, this is how most vision-based approaches localise.   Humans do the exact opposite. Instead of depth, we use high level semantic cues. Instead of extruding the floorplan up into the third dimension, we collapse the 3D world into a 2D representation. Evidence of this is that many of the floorplans we use in everyday life are not accurate, opting instead for high levels of discriminative landmarks.   In this work, we use this insight to present a global localisation approach that relies solely on the semantic labels present in the floorplan and extracted from RGB images. While our approach is able to use range measurements if available, we demonstrate that they are unnecessary as we can achieve results comparable to state-of-the-art without them.



### Squeeze-and-Excitation Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.01507v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01507v4)
- **Published**: 2017-09-05 17:42:13+00:00
- **Updated**: 2019-05-16 05:32:17+00:00
- **Authors**: Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu
- **Comment**: journal version of the CVPR 2018 paper, accepted by TPAMI
- **Journal**: None
- **Summary**: The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at https://github.com/hujie-frank/SENet.



### Opening the Black Box of Financial AI with CLEAR-Trade: A CLass-Enhanced Attentive Response Approach for Explaining and Visualizing Deep Learning-Driven Stock Market Prediction
- **Arxiv ID**: http://arxiv.org/abs/1709.01574v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1709.01574v1)
- **Published**: 2017-09-05 19:56:36+00:00
- **Updated**: 2017-09-05 19:56:36+00:00
- **Authors**: Devinder Kumar, Graham W Taylor, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been shown to outperform traditional machine learning algorithms across a wide range of problem domains. However, current deep learning algorithms have been criticized as uninterpretable "black-boxes" which cannot explain their decision making processes. This is a major shortcoming that prevents the widespread application of deep learning to domains with regulatory processes such as finance. As such, industries such as finance have to rely on traditional models like decision trees that are much more interpretable but less effective than deep learning for complex problems. In this paper, we propose CLEAR-Trade, a novel financial AI visualization framework for deep learning-driven stock market prediction that mitigates the interpretability issue of deep learning methods. In particular, CLEAR-Trade provides a effective way to visualize and explain decisions made by deep stock market prediction models. We show the efficacy of CLEAR-Trade in enhancing the interpretability of stock market prediction by conducting experiments based on S&P 500 stock index prediction. The results demonstrate that CLEAR-Trade can provide significant insight into the decision-making process of deep learning-driven financial models, particularly for regulatory processes, thus improving their potential uptake in the financial industry.



### Improving Landmark Localization with Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1709.01591v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01591v7)
- **Published**: 2017-09-05 20:52:23+00:00
- **Updated**: 2018-10-28 15:05:52+00:00
- **Authors**: Sina Honari, Pavlo Molchanov, Stephen Tyree, Pascal Vincent, Christopher Pal, Jan Kautz
- **Comment**: Published as a conference paper in CVPR 2018
- **Journal**: None
- **Summary**: We present two techniques to improve landmark localization in images from partially annotated datasets. Our primary goal is to leverage the common situation where precise landmark locations are only provided for a small data subset, but where class labels for classification or regression tasks related to the landmarks are more abundantly available. First, we propose the framework of sequential multitasking and explore it here through an architecture for landmark localization where training with class labels acts as an auxiliary signal to guide the landmark localization on unlabeled data. A key aspect of our approach is that errors can be backpropagated through a complete landmark localization model. Second, we propose and explore an unsupervised learning technique for landmark localization based on having a model predict equivariant landmarks with respect to transformations applied to the image. We show that these techniques, improve landmark prediction considerably and can learn effective detectors even when only a small fraction of the dataset has landmark labels. We present results on two toy datasets and four real datasets, with hands and faces, and report new state-of-the-art on two datasets in the wild, e.g. with only 5\% of labeled images we outperform previous state-of-the-art trained on the AFLW dataset.



### Intraoperative Organ Motion Models with an Ensemble of Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.02255v1
- **DOI**: 10.1007/978-3-319-66185-8_42
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.02255v1)
- **Published**: 2017-09-05 21:14:39+00:00
- **Updated**: 2017-09-05 21:14:39+00:00
- **Authors**: Yipeng Hu, Eli Gibson, Tom Vercauteren, Hashim U. Ahmed, Mark Emberton, Caroline M. Moore, J. Alison Noble, Dean C. Barratt
- **Comment**: Accepted to MICCAI 2017
- **Journal**: None
- **Summary**: In this paper, we describe how a patient-specific, ultrasound-probe-induced prostate motion model can be directly generated from a single preoperative MR image. Our motion model allows for sampling from the conditional distribution of dense displacement fields, is encoded by a generative neural network conditioned on a medical image, and accepts random noise as additional input. The generative network is trained by a minimax optimisation with a second discriminative neural network, tasked to distinguish generated samples from training motion data. In this work, we propose that 1) jointly optimising a third conditioning neural network that pre-processes the input image, can effectively extract patient-specific features for conditioning; and 2) combining multiple generative models trained separately with heuristically pre-disjointed training data sets can adequately mitigate the problem of mode collapse. Trained with diagnostic T2-weighted MR images from 143 real patients and 73,216 3D dense displacement fields from finite element simulations of intraoperative prostate motion due to transrectal ultrasound probe pressure, the proposed models produced physically-plausible patient-specific motion of prostate glands. The ability to capture biomechanically simulated motion was evaluated using two errors representing generalisability and specificity of the model. The median values, calculated from a 10-fold cross-validation, were 2.8+/-0.3 mm and 1.7+/-0.1 mm, respectively. We conclude that the introduced approach demonstrates the feasibility of applying state-of-the-art machine learning algorithms to generate organ motion models from patient images, and shows significant promise for future research.



### Deep Ordinal Ranking for Multi-Category Diagnosis of Alzheimer's Disease using Hippocampal MRI data
- **Arxiv ID**: http://arxiv.org/abs/1709.01599v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01599v2)
- **Published**: 2017-09-05 21:29:39+00:00
- **Updated**: 2017-09-13 15:26:38+00:00
- **Authors**: Hongming Li, Mohamad Habes, Yong Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Increasing effort in brain image analysis has been dedicated to early diagnosis of Alzheimer's disease (AD) based on neuroimaging data. Most existing studies have been focusing on binary classification problems, e.g., distinguishing AD patients from normal control (NC) elderly or mild cognitive impairment (MCI) individuals from NC elderly. However, identifying individuals with AD and MCI, especially MCI individuals who will convert to AD (progressive MCI, pMCI), in a single setting, is needed to achieve the goal of early diagnosis of AD. In this paper, we propose a deep ordinal ranking model for distinguishing NC, stable MCI (sMCI), pMCI, and AD at an individual subject level, taking into account the inherent ordinal severity of brain degeneration caused by normal aging, MCI, and AD, rather than formulating the classification as a multi-category classification problem. The proposed deep ordinal ranking model focuses on the hippocampal morphology of individuals and learns informative and discriminative features automatically. Experiment results based on a large cohort of individuals from the Alzheimer's Disease Neuroimaging Initiative (ADNI) indicate that the proposed method can achieve better performance than traditional multi-category classification techniques using shape and radiomics features from structural magnetic resonance imaging (MRI) data.



### Dynamic Multiscale Tree Learning Using Ensemble Strong Classifiers for Multi-label Segmentation of Medical Images with Lesions
- **Arxiv ID**: http://arxiv.org/abs/1709.01602v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01602v1)
- **Published**: 2017-09-05 21:41:58+00:00
- **Updated**: 2017-09-05 21:41:58+00:00
- **Authors**: Samya Amiri, Mohamed Ali Mahjoub, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a dynamic multiscale tree (DMT) architecture that learns how to leverage the strengths of different existing classifiers for supervised multi-label image segmentation. Unlike previous works that simply aggregate or cascade classifiers for addressing image segmentation and labeling tasks, we propose to embed strong classifiers into a tree structure that allows bi-directional flow of information between its classifier nodes to gradually improve their performances. Our DMT is a generic classification model that inherently embeds different cascades of classifiers while enhancing learning transfer between them to boost up their classification accuracies. Specifically, each node in our DMT can nest a Structured Random Forest (SRF) classifier or a Bayesian Network (BN) classifier. The proposed SRF-BN DMT architecture has several appealing properties. First, while SRF operates at a patch-level (regular image region), BN operates at the super-pixel level (irregular image region), thereby enabling the DMT to integrate multi-level image knowledge in the learning process. Second, although BN is powerful in modeling dependencies between image elements (superpixels, edges) and their features, the learning of its structure and parameters is challenging. On the other hand, SRF may fail to accurately detect very irregular object boundaries. The proposed DMT robustly overcomes these limitations for both classifiers through the ascending and descending flow of contextual information between each parent node and its children nodes. Third, we train DMT using different scales, where we progressively decrease the patch and superpixel sizes as we go deeper along the tree edges nearing its leaf nodes. Last, DMT demonstrates its outperformance in comparison to several state-of-the-art segmentation methods for multi-labeling of brain images with gliomas.



### PageNet: Page Boundary Extraction in Historical Handwritten Documents
- **Arxiv ID**: http://arxiv.org/abs/1709.01618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01618v1)
- **Published**: 2017-09-05 22:54:49+00:00
- **Updated**: 2017-09-05 22:54:49+00:00
- **Authors**: Chris Tensmeyer, Brian Davis, Curtis Wigington, Iain Lee, Bill Barrett
- **Comment**: HIP 2017 (in submission)
- **Journal**: None
- **Summary**: When digitizing a document into an image, it is common to include a surrounding border region to visually indicate that the entire document is present in the image. However, this border should be removed prior to automated processing. In this work, we present a deep learning based system, PageNet, which identifies the main page region in an image in order to segment content from both textual and non-textual border noise. In PageNet, a Fully Convolutional Network obtains a pixel-wise segmentation which is post-processed into the output quadrilateral region. We evaluate PageNet on 4 collections of historical handwritten documents and obtain over 94% mean intersection over union on all datasets and approach human performance on 2 of these collections. Additionally, we show that PageNet can segment documents that are overlayed on top of other documents.



### Exploring and Exploiting Diversity for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1709.01625v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01625v1)
- **Published**: 2017-09-05 23:30:11+00:00
- **Updated**: 2017-09-05 23:30:11+00:00
- **Authors**: Payman Yadollahpour
- **Comment**: PhD Thesis. For overall document size considerations the results in
  this appendix section have been moved to
  http://ttic.uchicago.edu/~pyadolla/papers/thesis.pdf
- **Journal**: None
- **Summary**: Semantic image segmentation is an important computer vision task that is difficult because it consists of both recognition and segmentation. The task is often cast as a structured output problem on an exponentially large output-space, which is typically modeled by a discrete probabilistic model. The best segmentation is found by inferring the Maximum a-Posteriori (MAP) solution over the output distribution defined by the model. Due to limitations in optimization, the model cannot be arbitrarily complex. This leads to a trade-off: devise a more accurate model that incorporates rich high-order interactions between image elements at the cost of inaccurate and possibly intractable optimization OR leverage a tractable model which produces less accurate MAP solutions but may contain high quality solutions as other modes of its output distribution.   This thesis investigates the latter and presents a two stage approach to semantic segmentation. In the first stage a tractable segmentation model outputs a set of high probability segmentations from the underlying distribution that are not just minor perturbations of each other. Critically the output of this stage is a diverse set of plausible solutions and not just a single one. In the second stage, a discriminatively trained re-ranking model selects the best segmentation from this set. The re-ranking stage can use much more complex features than what could be tractably used in the segmentation model, allowing a better exploration of the solution space than simply returning the MAP solution. The formulation is agnostic to the underlying segmentation model (e.g. CRF, CNN, etc.) and optimization algorithm, which makes it applicable to a wide range of models and inference methods. Evaluation of the approach on a number of semantic image segmentation benchmark datasets highlight its superiority over inferring the MAP solution.



### Using Cross-Model EgoSupervision to Learn Cooperative Basketball Intention
- **Arxiv ID**: http://arxiv.org/abs/1709.01630v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01630v1)
- **Published**: 2017-09-05 23:49:51+00:00
- **Updated**: 2017-09-05 23:49:51+00:00
- **Authors**: Gedas Bertasius, Jianbo Shi
- **Comment**: None
- **Journal**: None
- **Summary**: We present a first-person method for cooperative basketball intention prediction: we predict with whom the camera wearer will cooperate in the near future from unlabeled first-person images. This is a challenging task that requires inferring the camera wearer's visual attention, and decoding the social cues of other players. Our key observation is that a first-person view provides strong cues to infer the camera wearer's momentary visual attention, and his/her intentions. We exploit this observation by proposing a new cross-model EgoSupervision learning scheme that allows us to predict with whom the camera wearer will cooperate in the near future, without using manually labeled intention labels. Our cross-model EgoSupervision operates by transforming the outputs of a pretrained pose-estimation network, into pseudo ground truth labels, which are then used as a supervisory signal to train a new network for a cooperative intention task. We evaluate our method, and show that it achieves similar or even better accuracy than the fully supervised methods do.



