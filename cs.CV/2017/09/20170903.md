# Arxiv Papers in cs.CV on 2017-09-03
### Simulated Annealing for JPEG Quantization
- **Arxiv ID**: http://arxiv.org/abs/1709.00649v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1709.00649v1)
- **Published**: 2017-09-03 01:10:18+00:00
- **Updated**: 2017-09-03 01:10:18+00:00
- **Authors**: Max Hopkins, Michael Mitzenmacher, Sebastian Wagner-Carena
- **Comment**: Appendix not included in arXiv version due to size restrictions. For
  full paper go to:
  http://www.eecs.harvard.edu/~michaelm/SimAnneal/PAPER/simulated-annealing-jpeg.pdf
- **Journal**: None
- **Summary**: JPEG is one of the most widely used image formats, but in some ways remains surprisingly unoptimized, perhaps because some natural optimizations would go outside the standard that defines JPEG. We show how to improve JPEG compression in a standard-compliant, backward-compatible manner, by finding improved default quantization tables. We describe a simulated annealing technique that has allowed us to find several quantization tables that perform better than the industry standard, in terms of both compressed size and image fidelity. Specifically, we derive tables that reduce the FSIM error by over 10% while improving compression by over 20% at quality level 95 in our tests; we also provide similar results for other quality levels. While we acknowledge our approach can in some images lead to visible artifacts under large magnification, we believe use of these quantization tables, or additional tables that could be found using our methodology, would significantly reduce JPEG file sizes with improved overall image quality.



### Detection of Moving Object in Dynamic Background Using Gaussian Max-Pooling and Segmentation Constrained RPCA
- **Arxiv ID**: http://arxiv.org/abs/1709.00657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00657v1)
- **Published**: 2017-09-03 03:38:58+00:00
- **Updated**: 2017-09-03 03:38:58+00:00
- **Authors**: Yang Li, Guangcan Liu, Shengyong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Due to its efficiency and stability, Robust Principal Component Analysis (RPCA) has been emerging as a promising tool for moving object detection. Unfortunately, existing RPCA based methods assume static or quasi-static background, and thereby they may have trouble in coping with the background scenes that exhibit a persistent dynamic behavior. In this work, we shall introduce two techniques to fill in the gap. First, instead of using the raw pixel-value as features that are brittle in the presence of dynamic background, we devise a so-called Gaussian max-pooling operator to estimate a "stable-value" for each pixel. Those stable-values are robust to various background changes and can therefore distinguish effectively the foreground objects from the background. Then, to obtain more accurate results, we further propose a Segmentation Constrained RPCA (SC-RPCA) model, which incorporates the temporal and spatial continuity in images into RPCA. The inference process of SC-RPCA is a group sparsity constrained nuclear norm minimization problem, which is convex and easy to solve. Experimental results on seven videos from the CDCNET 2014 database show the superior performance of the proposed method.



### A Generative Model For Zero Shot Learning Using Conditional Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1709.00663v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00663v2)
- **Published**: 2017-09-03 04:17:27+00:00
- **Updated**: 2018-01-27 13:30:42+00:00
- **Authors**: Ashish Mishra, M Shiva Krishna Reddy, Anurag Mittal, Hema A Murthy
- **Comment**: None
- **Journal**: None
- **Summary**: Zero shot learning in Image Classification refers to the setting where images from some novel classes are absent in the training data but other information such as natural language descriptions or attribute vectors of the classes are available. This setting is important in the real world since one may not be able to obtain images of all the possible classes at training. While previous approaches have tried to model the relationship between the class attribute space and the image space via some kind of a transfer function in order to model the image space correspondingly to an unseen class, we take a different approach and try to generate the samples from the given attributes, using a conditional variational autoencoder, and use the generated samples for classification of the unseen classes. By extensive testing on four benchmark datasets, we show that our model outperforms the state of the art, particularly in the more realistic generalized setting, where the training classes can also appear at the test time along with the novel classes.



### Unsupervised feature learning with discriminative encoder
- **Arxiv ID**: http://arxiv.org/abs/1709.00672v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00672v1)
- **Published**: 2017-09-03 06:40:35+00:00
- **Updated**: 2017-09-03 06:40:35+00:00
- **Authors**: Gaurav Pandey, Ambedkar Dukkipati
- **Comment**: 10 pages, 4 figures, International Conference on Data Mining, 2017
- **Journal**: None
- **Summary**: In recent years, deep discriminative models have achieved extraordinary performance on supervised learning tasks, significantly outperforming their generative counterparts. However, their success relies on the presence of a large amount of labeled data. How can one use the same discriminative models for learning useful features in the absence of labels? We address this question in this paper, by jointly modeling the distribution of data and latent features in a manner that explicitly assigns zero probability to unobserved data. Rather than maximizing the marginal probability of observed data, we maximize the joint probability of the data and the latent features using a two step EM-like procedure. To prevent the model from overfitting to our initial selection of latent features, we use adversarial regularization. Depending on the task, we allow the latent features to be one-hot or real-valued vectors and define a suitable prior on the features. For instance, one-hot features correspond to class labels and are directly used for the unsupervised and semi-supervised classification task, whereas real-valued feature vectors are fed as input to simple classifiers for auxiliary supervised discrimination tasks. The proposed model, which we dub discriminative encoder (or DisCoder), is flexible in the type of latent features that it can capture. The proposed model achieves state-of-the-art performance on several challenging tasks.



### Complete End-To-End Low Cost Solution To a 3D Scanning System with Integrated Turntable
- **Arxiv ID**: http://arxiv.org/abs/1709.02247v1
- **DOI**: 10.5121/ijcsit.2017.9404
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02247v1)
- **Published**: 2017-09-03 13:40:23+00:00
- **Updated**: 2017-09-03 13:40:23+00:00
- **Authors**: Saed Khawaldeh, Tajwar Abrar Aleef, Usama Pervaiz, Vu Hoang Minh, Yeman Brhane Hagos
- **Comment**: 22 pages
- **Journal**: International Journal of Computer Science & Information Technology
  (IJCSIT), 9, 39-55 (2017)
- **Summary**: 3D reconstruction is a technique used in computer vision which has a wide range of applications in areas like object recognition, city modelling, virtual reality, physical simulations, video games and special effects. Previously, to perform a 3D reconstruction, specialized hardwares were required. Such systems were often very expensive and was only available for industrial or research purpose. With the rise of the availability of high-quality low cost 3D sensors, it is now possible to design inexpensive complete 3D scanning systems. The objective of this work was to design an acquisition and processing system that can perform 3D scanning and reconstruction of objects seamlessly. In addition, the goal of this work also included making the 3D scanning process fully automated by building and integrating a turntable alongside the software. This means the user can perform a full 3D scan only by a press of a few buttons from our dedicated graphical user interface. Three main steps were followed to go from acquisition of point clouds to the finished reconstructed 3D model. First, our system acquires point cloud data of a person/object using inexpensive camera sensor. Second, align and convert the acquired point cloud data into a watertight mesh of good quality. Third, export the reconstructed model to a 3D printer to obtain a proper 3D print of the model.



### Blind Stereo Image Quality Assessment Inspired by Brain Sensory-Motor Fusion
- **Arxiv ID**: http://arxiv.org/abs/1709.00725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00725v1)
- **Published**: 2017-09-03 14:50:48+00:00
- **Updated**: 2017-09-03 14:50:48+00:00
- **Authors**: Maryam Karimi, Najmeh Soltanian, Shadrokh Samavi, Nader Karimi, S. M. Reza Soroushmehr, Kayvan Najarian
- **Comment**: 11 pages, 13 figures, 3 tables
- **Journal**: None
- **Summary**: The use of 3D and stereo imaging is rapidly increasing. Compression, transmission, and processing could degrade the quality of stereo images. Quality assessment of such images is different than their 2D counterparts. Metrics that represent 3D perception by human visual system (HVS) are expected to assess stereoscopic quality more accurately. In this paper, inspired by brain sensory/motor fusion process, two stereo images are fused together. Then from every fused image two synthesized images are extracted. Effects of different distortions on statistical distributions of the synthesized images are shown. Based on the observed statistical changes, features are extracted from these synthesized images. These features can reveal type and severity of distortions. Then, a stacked neural network model is proposed, which learns the extracted features and accurately evaluates the quality of stereo images. This model is tested on 3D images of popular databases. Experimental results show the superiority of this method over state of the art stereo image quality assessment approaches



### Human Detection and Tracking for Video Surveillance A Cognitive Science Approach
- **Arxiv ID**: http://arxiv.org/abs/1709.00726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00726v1)
- **Published**: 2017-09-03 14:52:54+00:00
- **Updated**: 2017-09-03 14:52:54+00:00
- **Authors**: Vandit Gajjar, Ayesha Gurnani, Yash Khandhediya
- **Comment**: ICCV 2017 Venice, Italy Pages 5 Figures 5
- **Journal**: None
- **Summary**: With crimes on the rise all around the world, video surveillance is becoming more important day by day. Due to the lack of human resources to monitor this increasing number of cameras manually new computer vision algorithms to perform lower and higher level tasks are being developed. We have developed a new method incorporating the most acclaimed Histograms of Oriented Gradients the theory of Visual Saliency and the saliency prediction model Deep Multi Level Network to detect human beings in video sequences. Furthermore we implemented the k Means algorithm to cluster the HOG feature vectors of the positively detected windows and determined the path followed by a person in the video. We achieved a detection precision of 83.11% and a recall of 41.27%. We obtained these results 76.866 times faster than classification on normal images.



### Hand Gesture Real Time Paint Tool - Box
- **Arxiv ID**: http://arxiv.org/abs/1709.00727v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00727v3)
- **Published**: 2017-09-03 14:53:05+00:00
- **Updated**: 2018-03-18 05:10:35+00:00
- **Authors**: Vandit Gajjar, Viraj Mavani, Ayesha Gurnani
- **Comment**: This paper needs a proper writing and experiments need to be
  implemented, thus we are withdrawing this submission
- **Journal**: None
- **Summary**: With current development universally in computing, now a days user interaction approaches with mouse, keyboard, touch-pens etc. are not sufficient. Directly using of hands or hand gestures as an input device is a method to attract people with providing the applications, through Machine Learning and Computer Vision. Human-computer interaction application in which you can simply draw different shapes, fill the colors, moving the folder from one place to another place and rotating your image with rotating your hand gesture all this will be without touching your device only. In this paper Machine Learning based hand gestures recognition is presented, with the use of Computer Vision different types of gesture applications have been created.



### Sushi Dish - Object detection and classification from real images
- **Arxiv ID**: http://arxiv.org/abs/1709.00751v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00751v2)
- **Published**: 2017-09-03 18:02:01+00:00
- **Updated**: 2017-10-15 16:18:10+00:00
- **Authors**: Yeongjin Oh, Seunghyun Son, Gyumin Sim
- **Comment**: 6 pages, 13 figures
- **Journal**: None
- **Summary**: In conveyor belt sushi restaurants, billing is a burdened job because one has to manually count the number of dishes and identify the color of them to calculate the price. In a busy situation, there can be a mistake that customers are overcharged or under-charged. To deal with this problem, we developed a method that automatically identifies the color of dishes and calculate the total price using real images. Our method consists of ellipse fitting and convol-utional neural network. It achieves ellipse detection precision 85% and recall 96% and classification accuracy 92%.



### Compressed Sensing MRI Reconstruction using a Generative Adversarial Network with a Cyclic Loss
- **Arxiv ID**: http://arxiv.org/abs/1709.00753v2
- **DOI**: 10.1109/TMI.2018.2820120
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00753v2)
- **Published**: 2017-09-03 18:08:14+00:00
- **Updated**: 2018-03-15 15:39:43+00:00
- **Authors**: Tran Minh Quan, Thanh Nguyen-Duc, Won-Ki Jeong
- **Comment**: submitted to IEEE Transactions on Medical Imaging
- **Journal**: IEEE Trans. Med. Imaging 37(6): 1488-1497 (2018)
- **Summary**: Compressed Sensing MRI (CS-MRI) has provided theoretical foundations upon which the time-consuming MRI acquisition process can be accelerated. However, it primarily relies on iterative numerical solvers which still hinders their adaptation in time-critical applications. In addition, recent advances in deep neural networks have shown their potential in computer vision and image processing, but their adaptation to MRI reconstruction is still in an early stage. In this paper, we propose a novel deep learning-based generative adversarial model, RefineGAN, for fast and accurate CS-MRI reconstruction. The proposed model is a variant of fully-residual convolutional autoencoder and generative adversarial networks (GANs), specifically designed for CS-MRI formulation; it employs deeper generator and discriminator networks with cyclic data consistency loss for faithful interpolation in the given under-sampled k-space data. In addition, our solution leverages a chained network to further enhance the reconstruction quality. RefineGAN is fast and accurate -- the reconstruction process is extremely rapid, as low as tens of milliseconds for reconstruction of a 256x256 image, because it is one-way deployment on a feed-forward network, and the image quality is superior even for extremely low sampling rate (as low as 10%) due to the data-driven nature of the method. We demonstrate that RefineGAN outperforms the state-of-the-art CS-MRI methods by a large margin in terms of both running time and image quality via evaluation using several open-source MRI databases.



### Uncertainty-Aware Learning from Demonstration using Mixture Density Networks with Sampling-Free Variance Modeling
- **Arxiv ID**: http://arxiv.org/abs/1709.02249v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.02249v2)
- **Published**: 2017-09-03 18:57:54+00:00
- **Updated**: 2017-09-08 06:11:36+00:00
- **Authors**: Sungjoon Choi, Kyungjae Lee, Sungbin Lim, Songhwai Oh
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an uncertainty-aware learning from demonstration method by presenting a novel uncertainty estimation method utilizing a mixture density network appropriate for modeling complex and noisy human behaviors. The proposed uncertainty acquisition can be done with a single forward path without Monte Carlo sampling and is suitable for real-time robotics applications. The properties of the proposed uncertainty measure are analyzed through three different synthetic examples, absence of data, heavy measurement noise, and composition of functions scenarios. We show that each case can be distinguished using the proposed uncertainty measure and presented an uncertainty-aware learn- ing from demonstration method of an autonomous driving using this property. The proposed uncertainty-aware learning from demonstration method outperforms other compared methods in terms of safety using a complex real-world driving dataset.



### Lensless-camera based machine learning for image classification
- **Arxiv ID**: http://arxiv.org/abs/1709.00408v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1709.00408v1)
- **Published**: 2017-09-03 23:42:29+00:00
- **Updated**: 2017-09-03 23:42:29+00:00
- **Authors**: Ganghun Kim, Stefan Kapetanovic, Rachael Palmer, Rajesh Menon
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning (ML) has been widely applied to image classification. Here, we extend this application to data generated by a camera comprised of only a standard CMOS image sensor with no lens. We first created a database of lensless images of handwritten digits. Then, we trained a ML algorithm on this dataset. Finally, we demonstrated that the trained ML algorithm is able to classify the digits with accuracy as high as 99% for 2 digits. Our approach clearly demonstrates the potential for non-human cameras in machine-based decision-making scenarios.



