# Arxiv Papers in cs.CV on 2017-05-29
### Data Driven Coded Aperture Design for Depth Recovery
- **Arxiv ID**: http://arxiv.org/abs/1705.10021v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10021v2)
- **Published**: 2017-05-29 02:25:39+00:00
- **Updated**: 2017-06-01 04:36:42+00:00
- **Authors**: Prasan A Shedligeri, Sreyas Mohan, Kaushik Mitra
- **Comment**: 5 pages, 4 figures. Accepted at IEEE ICIP 2017, Beijing, China
- **Journal**: None
- **Summary**: Inserting a patterned occluder at the aperture of a camera lens has been shown to improve the recovery of depth map and all-focus image compared to a fully open aperture. However, design of the aperture pattern plays a very critical role. Previous approaches for designing aperture codes make simple assumptions on image distributions to obtain metrics for evaluating aperture codes. However, real images may not follow those assumptions and hence the designed code may not be optimal for them. To address this drawback we propose a data driven approach for learning the optimal aperture pattern to recover depth map from a single coded image. We propose a two stage architecture where, in the first stage we simulate coded aperture images from a training dataset of all-focus images and depth maps and in the second stage we recover the depth map using a deep neural network. We demonstrate that our learned aperture code performs better than previously designed codes even on code design metrics proposed by previous approaches.



### Ensemble of Part Detectors for Simultaneous Classification and Localization
- **Arxiv ID**: http://arxiv.org/abs/1705.10034v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10034v1)
- **Published**: 2017-05-29 04:04:08+00:00
- **Updated**: 2017-05-29 04:04:08+00:00
- **Authors**: Xiaopeng Zhang, Hongkai Xiong, Weiyao Lin, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Part-based representation has been proven to be effective for a variety of visual applications. However, automatic discovery of discriminative parts without object/part-level annotations is challenging. This paper proposes a discriminative mid-level representation paradigm based on the responses of a collection of part detectors, which only requires the image-level labels. Towards this goal, we first develop a detector-based spectral clustering method to mine the representative and discriminative mid-level patterns for detector initialization. The advantage of the proposed pattern mining technology is that the distance metric based on detectors only focuses on discriminative details, and a set of such grouped detectors offer an effective way for consistent pattern mining. Relying on the discovered patterns, we further formulate the detector learning process as a confidence-loss sparse Multiple Instance Learning (cls-MIL) task, which considers the diversity of the positive samples, while avoid drifting away the well localized ones by assigning a confidence value to each positive sample. The responses of the learned detectors can form an effective mid-level image representation for both image classification and object localization. Experiments conducted on benchmark datasets demonstrate the superiority of our method over existing approaches.



### Towards Metamerism via Foveated Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1705.10041v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1705.10041v3)
- **Published**: 2017-05-29 05:38:20+00:00
- **Updated**: 2018-12-28 22:37:48+00:00
- **Authors**: Arturo Deza, Aditya Jonnalagadda, Miguel Eckstein
- **Comment**: Published at ICLR 2019
- **Journal**: None
- **Summary**: The problem of $\textit{visual metamerism}$ is defined as finding a family of perceptually indistinguishable, yet physically different images. In this paper, we propose our NeuroFovea metamer model, a foveated generative model that is based on a mixture of peripheral representations and style transfer forward-pass algorithms. Our gradient-descent free model is parametrized by a foveated VGG19 encoder-decoder which allows us to encode images in high dimensional space and interpolate between the content and texture information with adaptive instance normalization anywhere in the visual field. Our contributions include: 1) A framework for computing metamers that resembles a noisy communication system via a foveated feed-forward encoder-decoder network -- We observe that metamerism arises as a byproduct of noisy perturbations that partially lie in the perceptual null space; 2) A perceptual optimization scheme as a solution to the hyperparametric nature of our metamer model that requires tuning of the image-texture tradeoff coefficients everywhere in the visual field which are a consequence of internal noise; 3) An ABX psychophysical evaluation of our metamers where we also find that the rate of growth of the receptive fields in our model match V1 for reference metamers and V2 between synthesized samples. Our model also renders metamers at roughly a second, presenting a $\times1000$ speed-up compared to the previous work, which allows for tractable data-driven metamer experiments.



### On the Power Spectral Density Applied to the Analysis of Old Canvases
- **Arxiv ID**: http://arxiv.org/abs/1705.10060v1
- **DOI**: None
- **Categories**: **cs.CV**, math.SP
- **Links**: [PDF](http://arxiv.org/pdf/1705.10060v1)
- **Published**: 2017-05-29 07:49:32+00:00
- **Updated**: 2017-05-29 07:49:32+00:00
- **Authors**: Francisco J. Simois, Juan J. Murillo-Fuentes
- **Comment**: None
- **Journal**: None
- **Summary**: A routine task for art historians is painting diagnostics, such as dating or attribution. Signal processing of the X-ray image of a canvas provides useful information about its fabric. However, previous methods may fail when very old and deteriorated artworks or simply canvases of small size are studied. We present a new framework to analyze and further characterize the paintings from their radiographs. First, we start from a general analysis of lattices and provide new unifying results about the theoretical spectra of weaves. Then, we use these results to infer the main structure of the fabric, like the type of weave and the thread densities. We propose a practical estimation of these theoretical results from paintings with the averaged power spectral density (PSD), which provides a more robust tool. Furthermore, we found that the PSD provides a fingerprint that characterizes the whole canvas. We search and discuss some distinctive features we may find in that fingerprint. We apply these results to several masterpieces of the 17th and 18th centuries from the Museo Nacional del Prado to show that this approach yields accurate results in thread counting and is very useful for paintings comparison, even in situations where previous methods fail.



### Beyond Counting: Comparisons of Density Maps for Crowd Analysis Tasks - Counting, Detection, and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1705.10118v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10118v2)
- **Published**: 2017-05-29 11:11:07+00:00
- **Updated**: 2018-06-13 14:01:09+00:00
- **Authors**: Di Kang, Zheng Ma, Antoni B. Chan
- **Comment**: accepted to IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT)
- **Journal**: None
- **Summary**: For crowded scenes, the accuracy of object-based computer vision methods declines when the images are low-resolution and objects have severe occlusions. Taking counting methods for example, almost all the recent state-of-the-art counting methods bypass explicit detection and adopt regression-based methods to directly count the objects of interest. Among regression-based methods, density map estimation, where the number of objects inside a subregion is the integral of the density map over that subregion, is especially promising because it preserves spatial information, which makes it useful for both counting and localization (detection and tracking). With the power of deep convolutional neural networks (CNNs) the counting performance has improved steadily. The goal of this paper is to evaluate density maps generated by density estimation methods on a variety of crowd analysis tasks, including counting, detection, and tracking. Most existing CNN methods produce density maps with resolution that is smaller than the original images, due to the downsample strides in the convolution/pooling operations. To produce an original-resolution density map, we also evaluate a classical CNN that uses a sliding window regressor to predict the density for every pixel in the image. We also consider a fully convolutional (FCNN) adaptation, with skip connections from lower convolutional layers to compensate for loss in spatial information during upsampling. In our experiments, we found that the lower-resolution density maps sometimes have better counting performance. In contrast, the original-resolution density maps improved localization tasks, such as detection and tracking, compared to bilinear upsampling the lower-resolution density maps. Finally, we also propose several metrics for measuring the quality of a density map, and relate them to experiment results on counting and localization.



### Pose-Aware Person Recognition
- **Arxiv ID**: http://arxiv.org/abs/1705.10120v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10120v1)
- **Published**: 2017-05-29 11:11:47+00:00
- **Updated**: 2017-05-29 11:11:47+00:00
- **Authors**: Vijay Kumar, Anoop Namboodiri, Manohar Paluri, C V Jawahar
- **Comment**: To appear in CVPR 2017
- **Journal**: None
- **Summary**: Person recognition methods that use multiple body regions have shown significant improvements over traditional face-based recognition. One of the primary challenges in full-body person recognition is the extreme variation in pose and view point. In this work, (i) we present an approach that tackles pose variations utilizing multiple models that are trained on specific poses, and combined using pose-aware weights during testing. (ii) For learning a person representation, we propose a network that jointly optimizes a single loss over multiple body regions. (iii) Finally, we introduce new benchmarks to evaluate person recognition in diverse scenarios and show significant improvements over previously proposed approaches on all the benchmarks including the photo album setting of PIPA.



### Towards Visual Ego-motion Learning in Robots
- **Arxiv ID**: http://arxiv.org/abs/1705.10279v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.10279v1)
- **Published**: 2017-05-29 16:25:50+00:00
- **Updated**: 2017-05-29 16:25:50+00:00
- **Authors**: Sudeep Pillai, John J. Leonard
- **Comment**: Conference paper; Submitted to IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS) 2017, Vancouver CA; 8 pages, 8 figures,
  2 tables
- **Journal**: None
- **Summary**: Many model-based Visual Odometry (VO) algorithms have been proposed in the past decade, often restricted to the type of camera optics, or the underlying motion manifold observed. We envision robots to be able to learn and perform these tasks, in a minimally supervised setting, as they gain more experience. To this end, we propose a fully trainable solution to visual ego-motion estimation for varied camera optics. We propose a visual ego-motion learning architecture that maps observed optical flow vectors to an ego-motion density estimate via a Mixture Density Network (MDN). By modeling the architecture as a Conditional Variational Autoencoder (C-VAE), our model is able to provide introspective reasoning and prediction for ego-motion induced scene-flow. Additionally, our proposed model is especially amenable to bootstrapped ego-motion learning in robots where the supervision in ego-motion estimation for a particular camera sensor can be obtained from standard navigation-based sensor fusion strategies (GPS/INS and wheel-odometry fusion). Through experiments, we show the utility of our proposed approach in enabling the concept of self-supervised learning for visual ego-motion estimation in autonomous robots.



### Feature Incay for Representation Regularization
- **Arxiv ID**: http://arxiv.org/abs/1705.10284v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10284v1)
- **Published**: 2017-05-29 16:33:54+00:00
- **Updated**: 2017-05-29 16:33:54+00:00
- **Authors**: Yuhui Yuan, Kuiyuan Yang, Chao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Softmax loss is widely used in deep neural networks for multi-class classification, where each class is represented by a weight vector, a sample is represented as a feature vector, and the feature vector has the largest projection on the weight vector of the correct category when the model correctly classifies a sample. To ensure generalization, weight decay that shrinks the weight norm is often used as regularizer. Different from traditional learning algorithms where features are fixed and only weights are tunable, features are also tunable as representation learning in deep learning. Thus, we propose feature incay to also regularize representation learning, which favors feature vectors with large norm when the samples can be correctly classified. With the feature incay, feature vectors are further pushed away from the origin along the direction of their corresponding weight vectors, which achieves better inter-class separability. In addition, the proposed feature incay encourages intra-class compactness along the directions of weight vectors by increasing the small feature norm faster than the large ones. Empirical results on MNIST, CIFAR10 and CIFAR100 demonstrate feature incay can improve the generalization ability.



### Emergent Communication in a Multi-Modal, Multi-Step Referential Game
- **Arxiv ID**: http://arxiv.org/abs/1705.10369v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.IT, cs.MA, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1705.10369v4)
- **Published**: 2017-05-29 19:25:49+00:00
- **Updated**: 2018-04-16 19:22:22+00:00
- **Authors**: Katrina Evtimova, Andrew Drozdov, Douwe Kiela, Kyunghyun Cho
- **Comment**: Published as a conference paper at ICLR 2018. 12 pages
- **Journal**: None
- **Summary**: Inspired by previous work on emergent communication in referential games, we propose a novel multi-modal, multi-step referential game, where the sender and receiver have access to distinct modalities of an object, and their information exchange is bidirectional and of arbitrary duration. The multi-modal multi-step setting allows agents to develop an internal communication significantly closer to natural language, in that they share a single set of messages, and that the length of the conversation may vary according to the difficulty of the task. We examine these properties empirically using a dataset consisting of images and textual descriptions of mammals, where the agents are tasked with identifying the correct object. Our experiments indicate that a robust and efficient communication protocol emerges, where gradual information exchange informs better predictions and higher communication bandwidth improves generalization.



### PCM-TV-TFV: A Novel Two Stage Framework for Image Reconstruction from Fourier Data
- **Arxiv ID**: http://arxiv.org/abs/1705.10784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10784v1)
- **Published**: 2017-05-29 22:24:34+00:00
- **Updated**: 2017-05-29 22:24:34+00:00
- **Authors**: Weihong Guo, Guohui Song, Yue Zhang
- **Comment**: 22 pages, 11 figures
- **Journal**: None
- **Summary**: We propose in this paper a novel two-stage Projection Correction Modeling (PCM) framework for image reconstruction from (non-uniform) Fourier measurements. PCM consists of a projection stage (P-stage) motivated by the multi-scale Galerkin method and a correction stage (C-stage) with an edge guided regularity fusing together the advantages of total variation (TV) and total fractional variation (TFV). The P-stage allows for continuous modeling of the underlying image of interest. The given measurements are projected onto a space in which the image is well represented. We then enhance the reconstruction result at the C-stage that minimizes an energy functional consisting of a fidelity in the transformed domain and a novel edge guided regularity. We further develop efficient proximal algorithms to solve the corresponding optimization problem. Various numerical results in both 1D signals and 2D images have also been presented to demonstrate the superior performance of the proposed two-stage method to other classical one-stage methods.



### Learning to Generate Chairs with Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1705.10413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10413v1)
- **Published**: 2017-05-29 23:15:50+00:00
- **Updated**: 2017-05-29 23:15:50+00:00
- **Authors**: Evgeny Zamyatin, Andrey Filchenkov
- **Comment**: Submitted to NIPS 2017
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) has gained tremendous popularity lately due to an ability to reinforce quality of its predictive model with generated objects and the quality of the generative model with and supervised feedback. GANs allow to synthesize images with a high degree of realism. However, the learning process of such models is a very complicated optimization problem and certain limitation for such models were found. It affects the choice of certain layers and nonlinearities when designing architectures. In particular, it does not allow to train convolutional GAN models with fully-connected hidden layers. In our work, we propose a modification of the previously described set of rules, as well as new approaches to designing architectures that will allow us to train more powerful GAN models. We show the effectiveness of our methods on the problem of synthesizing projections of 3D objects with the possibility of interpolation by class and view point.



