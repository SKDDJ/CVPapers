# Arxiv Papers in cs.CV on 2017-05-25
### Extraction and Classification of Diving Clips from Continuous Video Footage
- **Arxiv ID**: http://arxiv.org/abs/1705.09003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09003v1)
- **Published**: 2017-05-25 00:08:40+00:00
- **Updated**: 2017-05-25 00:08:40+00:00
- **Authors**: Aiden Nibali, Zhen He, Stuart Morgan, Daniel Greenwood
- **Comment**: To appear at CVsports 2017
- **Journal**: None
- **Summary**: Due to recent advances in technology, the recording and analysis of video data has become an increasingly common component of athlete training programmes. Today it is incredibly easy and affordable to set up a fixed camera and record athletes in a wide range of sports, such as diving, gymnastics, golf, tennis, etc. However, the manual analysis of the obtained footage is a time-consuming task which involves isolating actions of interest and categorizing them using domain-specific knowledge. In order to automate this kind of task, three challenging sub-problems are often encountered: 1) temporally cropping events/actions of interest from continuous video; 2) tracking the object of interest; and 3) classifying the events/actions of interest.   Most previous work has focused on solving just one of the above sub-problems in isolation. In contrast, this paper provides a complete solution to the overall action monitoring task in the context of a challenging real-world exemplar. Specifically, we address the problem of diving classification. This is a challenging problem since the person (diver) of interest typically occupies fewer than 1% of the pixels in each frame. The model is required to learn the temporal boundaries of a dive, even though other divers and bystanders may be in view. Finally, the model must be sensitive to subtle changes in body pose over a large number of frames to determine the classification code. We provide effective solutions to each of the sub-problems which combine to provide a highly functional solution to the task as a whole. The techniques proposed can be easily generalized to video footage recorded from other sports.



### Weakly Supervised Semantic Segmentation Based on Web Image Co-segmentation
- **Arxiv ID**: http://arxiv.org/abs/1705.09052v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09052v3)
- **Published**: 2017-05-25 05:35:41+00:00
- **Updated**: 2017-08-06 04:09:27+00:00
- **Authors**: Tong Shen, Guosheng Lin, Lingqiao Liu, Chunhua Shen, Ian Reid
- **Comment**: BMVC
- **Journal**: None
- **Summary**: Training a Fully Convolutional Network (FCN) for semantic segmentation requires a large number of masks with pixel level labelling, which involves a large amount of human labour and time for annotation. In contrast, web images and their image-level labels are much easier and cheaper to obtain. In this work, we propose a novel method for weakly supervised semantic segmentation with only image-level labels. The method utilizes the internet to retrieve a large number of images and uses a large scale co-segmentation framework to generate masks for the retrieved images. We first retrieve images from search engines, e.g. Flickr and Google, using semantic class names as queries, e.g. class names in the dataset PASCAL VOC 2012. We then use high quality masks produced by co-segmentation on the retrieved images as well as the target dataset images with image level labels to train segmentation networks. We obtain an IoU score of 56.9 on test set of PASCAL VOC 2012, which reaches the state-of-the-art performance.



### SLAM based Quasi Dense Reconstruction For Minimally Invasive Surgery Scenes
- **Arxiv ID**: http://arxiv.org/abs/1705.09107v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09107v1)
- **Published**: 2017-05-25 09:44:34+00:00
- **Updated**: 2017-05-25 09:44:34+00:00
- **Authors**: Nader Mahmoud, Alexandre Hostettler, Toby Collins, Luc Soler, Christophe Doignon, J. M. M. Montiel
- **Comment**: ICRA 2017 workshop C4 Surgical Robots: Compliant, Continuum,
  Cognitive, and Collaborative
- **Journal**: None
- **Summary**: Recovering surgical scene structure in laparoscope surgery is crucial step for surgical guidance and augmented reality applications. In this paper, a quasi dense reconstruction algorithm of surgical scene is proposed. This is based on a state-of-the-art SLAM system, and is exploiting the initial exploration phase that is typically performed by the surgeon at the beginning of the surgery. We show how to convert the sparse SLAM map to a quasi dense scene reconstruction, using pairs of keyframe images and correlation-based featureless patch matching. We have validated the approach with a live porcine experiment using Computed Tomography as ground truth, yielding a Root Mean Squared Error of 4.9mm.



### First-spike based visual categorization using reward-modulated STDP
- **Arxiv ID**: http://arxiv.org/abs/1705.09132v3
- **DOI**: 10.1109/TNNLS.2018.2826721
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.09132v3)
- **Published**: 2017-05-25 11:38:16+00:00
- **Updated**: 2018-07-10 12:20:52+00:00
- **Authors**: Milad Mozafari, Saeed Reza Kheradpisheh, Timoth√©e Masquelier, Abbas Nowzari-Dalini, Mohammad Ganjtabesh
- **Comment**: supplementary materials are added, Caltech face/motorbike
  demonstration figure is updated, some parts of the main manuscript are moved
  to the supplementary materials, additional network analysis and performance
  comparison with deep nets are added
- **Journal**: Mozafari, Milad, et al. "First-Spike-Based Visual Categorization
  Using Reward-Modulated STDP". IEEE Transactions on Neural Networks and
  Learning Systems (2018). DOI: https://doi.org/10.1109/TNNLS.2018.2826721
- **Summary**: Reinforcement learning (RL) has recently regained popularity, with major achievements such as beating the European game of Go champion. Here, for the first time, we show that RL can be used efficiently to train a spiking neural network (SNN) to perform object recognition in natural images without using an external classifier. We used a feedforward convolutional SNN and a temporal coding scheme where the most strongly activated neurons fire first, while less activated ones fire later, or not at all. In the highest layers, each neuron was assigned to an object category, and it was assumed that the stimulus category was the category of the first neuron to fire. If this assumption was correct, the neuron was rewarded, i.e. spike-timing-dependent plasticity (STDP) was applied, which reinforced the neuron's selectivity. Otherwise, anti-STDP was applied, which encouraged the neuron to learn something else. As demonstrated on various image datasets (Caltech, ETH-80, and NORB), this reward modulated STDP (R-STDP) approach extracted particularly discriminative visual features, whereas classic unsupervised STDP extracts any feature that consistently repeats. As a result, R-STDP outperformed STDP on these datasets. Furthermore, R-STDP is suitable for online learning, and can adapt to drastic changes such as label permutations. Finally, it is worth mentioning that both feature extraction and classification were done with spikes, using at most one spike per neuron. Thus the network is hardware friendly and energy efficient.



### Deep image representations using caption generators
- **Arxiv ID**: http://arxiv.org/abs/1705.09142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09142v1)
- **Published**: 2017-05-25 12:13:27+00:00
- **Updated**: 2017-05-25 12:13:27+00:00
- **Authors**: Konda Reddy Mopuri, Vishal B. Athreya, R. Venkatesh Babu
- **Comment**: ICME 2017
- **Journal**: None
- **Summary**: Deep learning exploits large volumes of labeled data to learn powerful models. When the target dataset is small, it is a common practice to perform transfer learning using pre-trained models to learn new task specific representations. However, pre-trained CNNs for image recognition are provided with limited information about the image during training, which is label alone. Tasks such as scene retrieval suffer from features learned from this weak supervision and require stronger supervision to better understand the contents of the image. In this paper, we exploit the features learned from caption generating models to learn novel task specific image representations. In particular, we consider the state-of-the art captioning system Show and Tell~\cite{SnT-pami-2016} and the dense region description model DenseCap~\cite{densecap-cvpr-2016}. We demonstrate that, owing to richer supervision provided during the process of training, the features learned by the captioning system perform better than those of CNNs. Further, we train a siamese network with a modified pair-wise loss to fuse the features learned by~\cite{SnT-pami-2016} and~\cite{densecap-cvpr-2016} and learn image representations suitable for retrieval. Experiments show that the proposed fusion exploits the complementary nature of the individual features and yields state-of-the art retrieval results on benchmark datasets.



### Jeffrey's prior sampling of deep sigmoidal networks
- **Arxiv ID**: http://arxiv.org/abs/1705.10589v1
- **DOI**: None
- **Categories**: **cond-mat.dis-nn**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.10589v1)
- **Published**: 2017-05-25 13:58:37+00:00
- **Updated**: 2017-05-25 13:58:37+00:00
- **Authors**: Lorien X. Hayden, Alexander A. Alemi, Paul H. Ginsparg, James P. Sethna
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks have been shown to have a remarkable ability to uncover low dimensional structure in data: the space of possible reconstructed images form a reduced model manifold in image space. We explore this idea directly by analyzing the manifold learned by Deep Belief Networks and Stacked Denoising Autoencoders using Monte Carlo sampling. The model manifold forms an only slightly elongated hyperball with actual reconstructed data appearing predominantly on the boundaries of the manifold. In connection with the results we present, we discuss problems of sampling high-dimensional manifolds as well as recent work [M. Transtrum, G. Hart, and P. Qiu, Submitted (2014)] discussing the relation between high dimensional geometry and model reduction.



### Classification of Quantitative Light-Induced Fluorescence Images Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1705.09193v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.09193v1)
- **Published**: 2017-05-25 14:21:40+00:00
- **Updated**: 2017-05-25 14:21:40+00:00
- **Authors**: Sultan Imangaliyev, Monique H. van der Veen, Catherine M. C. Volgenant, Bruno G. Loos, Bart J. F. Keijser, Wim Crielaard, Evgeni Levin
- **Comment**: Full version of ICANN 2017 submission
- **Journal**: None
- **Summary**: Images are an important data source for diagnosis and treatment of oral diseases. The manual classification of images may lead to misdiagnosis or mistreatment due to subjective errors. In this paper an image classification model based on Convolutional Neural Network is applied to Quantitative Light-induced Fluorescence images. The deep neural network outperforms other state of the art shallow classification models in predicting labels derived from three different dental plaque assessment scores. The model directly benefits from multi-channel representation of the images resulting in improved performance when, besides the Red colour channel, additional Green and Blue colour channels are used.



### Who Will Share My Image? Predicting the Content Diffusion Path in Online Social Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.09275v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1705.09275v4)
- **Published**: 2017-05-25 17:46:52+00:00
- **Updated**: 2017-11-29 18:40:13+00:00
- **Authors**: Wenjian Hu, Krishna Kumar Singh, Fanyi Xiao, Jinyoung Han, Chen-Nee Chuah, Yong Jae Lee
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Content popularity prediction has been extensively studied due to its importance and interest for both users and hosts of social media sites like Facebook, Instagram, Twitter, and Pinterest. However, existing work mainly focuses on modeling popularity using a single metric such as the total number of likes or shares. In this work, we propose Diffusion-LSTM, a memory-based deep recurrent network that learns to recursively predict the entire diffusion path of an image through a social network. By combining user social features and image features, and encoding the diffusion path taken thus far with an explicit memory cell, our model predicts the diffusion path of an image more accurately compared to alternate baselines that either encode only image or social features, or lack memory. By mapping individual users to user prototypes, our model can generalize to new users not seen during training. Finally, we demonstrate our model's capability of generating diffusion trees, and show that the generated trees closely resemble ground-truth trees.



### GXNOR-Net: Training deep neural networks with ternary weights and activations without full-precision memory under a unified discretization framework
- **Arxiv ID**: http://arxiv.org/abs/1705.09283v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.09283v5)
- **Published**: 2017-05-25 17:59:41+00:00
- **Updated**: 2018-05-02 17:30:40+00:00
- **Authors**: Lei Deng, Peng Jiao, Jing Pei, Zhenzhi Wu, Guoqi Li
- **Comment**: 11 pages, 13 figures
- **Journal**: Neural Networks(Volume 100,April 2018)
- **Summary**: There is a pressing need to build an architecture that could subsume these networks under a unified framework that achieves both higher performance and less overhead. To this end, two fundamental issues are yet to be addressed. The first one is how to implement the back propagation when neuronal activations are discrete. The second one is how to remove the full-precision hidden weights in the training phase to break the bottlenecks of memory/computation consumption. To address the first issue, we present a multi-step neuronal activation discretization method and a derivative approximation technique that enable the implementing the back propagation algorithm on discrete DNNs. While for the second issue, we propose a discrete state transition (DST) methodology to constrain the weights in a discrete space without saving the hidden weights. Through this way, we build a unified framework that subsumes the binary or ternary networks as its special cases, and under which a heuristic algorithm is provided at the website https://github.com/AcrossV/Gated-XNOR. More particularly, we find that when both the weights and activations become ternary values, the DNNs can be reduced to sparse binary networks, termed as gated XNOR networks (GXNOR-Nets) since only the event of non-zero weight and non-zero activation enables the control gate to start the XNOR logic operations in the original binary networks. This promises the event-driven hardware design for efficient mobile intelligence. We achieve advanced performance compared with state-of-the-art algorithms. Furthermore, the computational sparsity and the number of states in the discrete space can be flexibly modified to make it suitable for various hardware platforms.



### Direct Multitype Cardiac Indices Estimation via Joint Representation and Regression Learning
- **Arxiv ID**: http://arxiv.org/abs/1705.09307v1
- **DOI**: 10.1109/TMI.2017.2709251
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09307v1)
- **Published**: 2017-05-25 18:01:41+00:00
- **Updated**: 2017-05-25 18:01:41+00:00
- **Authors**: Wufeng Xue, Ali Islam, Mousumi Bhaduri, Shuo Li
- **Comment**: accepted by IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Cardiac indices estimation is of great importance during identification and diagnosis of cardiac disease in clinical routine. However, estimation of multitype cardiac indices with consistently reliable and high accuracy is still a great challenge due to the high variability of cardiac structures and complexity of temporal dynamics in cardiac MR sequences. While efforts have been devoted into cardiac volumes estimation through feature engineering followed by a independent regression model, these methods suffer from the vulnerable feature representation and incompatible regression model. In this paper, we propose a semi-automated method for multitype cardiac indices estimation. After manual labelling of two landmarks for ROI cropping, an integrated deep neural network Indices-Net is designed to jointly learn the representation and regression models. It comprises two tightly-coupled networks: a deep convolution autoencoder (DCAE) for cardiac image representation, and a multiple output convolution neural network (CNN) for indices regression. Joint learning of the two networks effectively enhances the expressiveness of image representation with respect to cardiac indices, and the compatibility between image representation and indices regression, thus leading to accurate and reliable estimations for all the cardiac indices.   When applied with five-fold cross validation on MR images of 145 subjects, Indices-Net achieves consistently low estimation error for LV wall thicknesses (1.44$\pm$0.71mm) and areas of cavity and myocardium (204$\pm$133mm$^2$). It outperforms, with significant error reductions, segmentation method (55.1% and 17.4%) and two-phase direct volume-only methods (12.7% and 14.6%) for wall thicknesses and areas, respectively. These advantages endow the proposed method a great potential in clinical cardiac function assessment.



### Plan3D: Viewpoint and Trajectory Optimization for Aerial Multi-View Stereo Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1705.09314v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09314v2)
- **Published**: 2017-05-25 18:13:43+00:00
- **Updated**: 2018-09-18 17:43:04+00:00
- **Authors**: Benjamin Hepp, Matthias Nie√üner, Otmar Hilliges
- **Comment**: 31 pages, 12 figures, 9 tables
- **Journal**: None
- **Summary**: We introduce a new method that efficiently computes a set of viewpoints and trajectories for high-quality 3D reconstructions in outdoor environments. Our goal is to automatically explore an unknown area, and obtain a complete 3D scan of a region of interest (e.g., a large building). Images from a commodity RGB camera, mounted on an autonomously navigated quadcopter, are fed into a multi-view stereo reconstruction pipeline that produces high-quality results but is computationally expensive. In this setting, the scanning result is constrained by the restricted flight time of quadcopters. To this end, we introduce a novel optimization strategy that respects these constraints by maximizing the information gain from sparsely-sampled view points while limiting the total travel distance of the quadcopter. At the core of our method lies a hierarchical volumetric representation that allows the algorithm to distinguish between unknown, free, and occupied space. Furthermore, our information gain based formulation leverages this representation to handle occlusions in an efficient manner. In addition to the surface geometry, we utilize the free-space information to avoid obstacles and determine collision-free flight paths. Our tool can be used to specify the region of interest and to plan trajectories. We demonstrate our method by obtaining a number of compelling 3D reconstructions, and provide a thorough quantitative evaluation showing improvement over previous state-of-the-art and regular patterns.



### Rejection-Cascade of Gaussians: Real-time adaptive background subtraction framework
- **Arxiv ID**: http://arxiv.org/abs/1705.09339v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.09339v2)
- **Published**: 2017-05-25 19:50:45+00:00
- **Updated**: 2019-11-16 16:51:04+00:00
- **Authors**: B Ravi Kiran, Arindam Das, Senthil Yogamani
- **Comment**: Accepted for National Conference on Computer Vision, Pattern
  Recognition, Image Processing and Graphics (NCVPRIPG 2019)
- **Journal**: None
- **Summary**: Background-Foreground classification is a well-studied problem in computer vision. Due to the pixel-wise nature of modeling and processing in the algorithm, it is usually difficult to satisfy real-time constraints. There is a trade-off between the speed (because of model complexity) and accuracy. Inspired by the rejection cascade of Viola-Jones classifier, we decompose the Gaussian Mixture Model (GMM) into an adaptive cascade of Gaussians(CoG). We achieve a good improvement in speed without compromising the accuracy with respect to the baseline GMM model. We demonstrate a speed-up factor of 4-5x and 17 percent average improvement in accuracy over Wallflowers surveillance datasets. The CoG is then demonstrated to over the latent space representation of images of a convolutional variational autoencoder(VAE). We provide initial results over CDW-2014 dataset, which could speed up background subtraction for deep architectures.



### Pose Guided Person Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1705.09368v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09368v6)
- **Published**: 2017-05-25 21:29:07+00:00
- **Updated**: 2018-01-28 09:25:08+00:00
- **Authors**: Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuytelaars, Luc Van Gool
- **Comment**: Xu Jia and Qianru Sun contribute equally. Accepted in Proceedings of
  31st Conference on Neural Information Processing Systems (NIPS 2017)
- **Journal**: None
- **Summary**: This paper proposes the novel Pose Guided Person Generation Network (PG$^2$) that allows to synthesize person images in arbitrary poses, based on an image of that person and a novel pose. Our generation framework PG$^2$ utilizes the pose information explicitly and consists of two key stages: pose integration and image refinement. In the first stage the condition image and the target pose are fed into a U-Net-like network to generate an initial but coarse image of the person with the target pose. The second stage then refines the initial and blurry result by training a U-Net-like generator in an adversarial way. Extensive experimental results on both 128$\times$64 re-identification images and 256$\times$256 fashion photos show that our model generates high-quality person images with convincing details.



### Unsupervised Feature Learning for Writer Identification and Writer Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1705.09369v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09369v3)
- **Published**: 2017-05-25 21:30:40+00:00
- **Updated**: 2017-08-18 09:04:49+00:00
- **Authors**: Vincent Christlein, Martin Gropp, Stefan Fiel, Andreas Maier
- **Comment**: ICDAR2017 camera ready (fixed p@2 values, missing table references)
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNN) have shown great success in supervised classification tasks such as character classification or dating. Deep learning methods typically need a lot of annotated training data, which is not available in many scenarios. In these cases, traditional methods are often better than or equivalent to deep learning methods. In this paper, we propose a simple, yet effective, way to learn CNN activation features in an unsupervised manner. Therefore, we train a deep residual network using surrogate classes. The surrogate classes are created by clustering the training dataset, where each cluster index represents one surrogate class. The activations from the penultimate CNN layer serve as features for subsequent classification tasks. We evaluate the feature representations on two publicly available datasets. The focus lies on the ICDAR17 competition dataset on historical document writer identification (Historical-WI). We show that the activation features trained without supervision are superior to descriptors of state-of-the-art writer identification methods. Additionally, we achieve comparable results in the case of handwriting classification using the ICFHR16 competition dataset on historical Latin script types (CLaMM16).



