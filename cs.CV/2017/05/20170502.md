# Arxiv Papers in cs.CV on 2017-05-02
### A Strategy for an Uncompromising Incremental Learner
- **Arxiv ID**: http://arxiv.org/abs/1705.00744v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.00744v2)
- **Published**: 2017-05-02 00:17:54+00:00
- **Updated**: 2017-07-17 07:30:18+00:00
- **Authors**: Ragav Venkatesan, Hemanth Venkateswara, Sethuraman Panchanathan, Baoxin Li
- **Comment**: Under review at IEEE Transactions of Neural Networks and Learning
  Systems
- **Journal**: None
- **Summary**: Multi-class supervised learning systems require the knowledge of the entire range of labels they predict. Often when learnt incrementally, they suffer from catastrophic forgetting. To avoid this, generous leeways have to be made to the philosophy of incremental learning that either forces a part of the machine to not learn, or to retrain the machine again with a selection of the historic data. While these hacks work to various degrees, they do not adhere to the spirit of incremental learning. In this article, we redefine incremental learning with stringent conditions that do not allow for any undesirable relaxations and assumptions. We design a strategy involving generative models and the distillation of dark knowledge as a means of hallucinating data along with appropriate targets from past distributions. We call this technique, phantom sampling.We show that phantom sampling helps avoid catastrophic forgetting during incremental learning. Using an implementation based on deep neural networks, we demonstrate that phantom sampling dramatically avoids catastrophic forgetting. We apply these strategies to competitive multi-class incremental learning of deep neural networks. Using various benchmark datasets and through our strategy, we demonstrate that strict incremental learning could be achieved. We further put our strategy to test on challenging cases, including cross-domain increments and incrementing on a novel label space. We also propose a trivial extension to unbounded-continual learning and identify potential for future development.



### Dense-Captioning Events in Videos
- **Arxiv ID**: http://arxiv.org/abs/1705.00754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00754v1)
- **Published**: 2017-05-02 01:21:58+00:00
- **Updated**: 2017-05-02 01:21:58+00:00
- **Authors**: Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, Juan Carlos Niebles
- **Comment**: 16 pages, 16 figures
- **Journal**: None
- **Summary**: Most natural videos contain numerous events. For example, in a video of a "man playing a piano", the video might also contain "another man dancing" or "a crowd clapping". We introduce the task of dense-captioning events, which involves both detecting and describing events in a video. We propose a new model that is able to identify all events in a single pass of the video while simultaneously describing the detected events with natural language. Our model introduces a variant of an existing proposal module that is designed to capture both short as well as long events that span minutes. To capture the dependencies between the events in a video, our model introduces a new captioning module that uses contextual information from past and future events to jointly describe all events. We also introduce ActivityNet Captions, a large-scale benchmark for dense-captioning events. ActivityNet Captions contains 20k videos amounting to 849 video hours with 100k total descriptions, each with it's unique start and end time. Finally, we report performances of our model for dense-captioning events, video retrieval and localization.



### Lesion detection and Grading of Diabetic Retinopathy via Two-stages Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.00771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00771v1)
- **Published**: 2017-05-02 02:44:39+00:00
- **Updated**: 2017-05-02 02:44:39+00:00
- **Authors**: Yehui Yang, Tao Li, Wensi Li, Haishan Wu, Wei Fan, Wensheng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an automatic diabetic retinopathy (DR) analysis algorithm based on two-stages deep convolutional neural networks (DCNN). Compared to existing DCNN-based DR detection methods, the proposed algorithm have the following advantages: (1) Our method can point out the location and type of lesions in the fundus images, as well as giving the severity grades of DR. Moreover, since retina lesions and DR severity appear with different scales in fundus images, the integration of both local and global networks learn more complete and specific features for DR analysis. (2) By introducing imbalanced weighting map, more attentions will be given to lesion patches for DR grading, which significantly improve the performance of the proposed algorithm. In this study, we label 12,206 lesion patches and re-annotate the DR grades of 23,595 fundus images from Kaggle competition dataset. Under the guidance of clinical ophthalmologists, the experimental results show that our local lesion detection net achieve comparable performance with trained human observers, and the proposed imbalanced weighted scheme also be proved to significantly improve the capability of our DCNN-based DR grading algorithm.



### Offline Handwritten Recognition of Malayalam District Name - A Holistic Approach
- **Arxiv ID**: http://arxiv.org/abs/1705.00794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00794v1)
- **Published**: 2017-05-02 04:51:47+00:00
- **Updated**: 2017-05-02 04:51:47+00:00
- **Authors**: Jino P J, Kannan Balakrishnan
- **Comment**: 8 pages, IJET 2017
- **Journal**: None
- **Summary**: Various machine learning methods for writer independent recognition of Malayalam handwritten district names are discussed in this paper. Data collected from 56 different writers are used for the experiments. The proposed work can be used for the recognition of district in the address written in Malayalam. Different methods for Dimensionality reduction are discussed. Features consider for the recognition are Histogram of Oriented Gradient descriptor, Number of Black Pixels in the upper half and lower half, length of image. Classifiers used in this work are Neural Network, SVM and RandomForest.



### Statistical learning of rational wavelet transform for natural images
- **Arxiv ID**: http://arxiv.org/abs/1705.00821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00821v1)
- **Published**: 2017-05-02 06:51:20+00:00
- **Updated**: 2017-05-02 06:51:20+00:00
- **Authors**: Naushad Ansari, Anubha Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Motivated with the concept of transform learning and the utility of rational wavelet transform in audio and speech processing, this paper proposes Rational Wavelet Transform Learning in Statistical sense (RWLS) for natural images. The proposed RWLS design is carried out via lifting framework and is shown to have a closed form solution. The efficacy of the learned transform is demonstrated in the application of compressed sensing (CS) based reconstruction. The learned RWLS is observed to perform better than the existing standard dyadic wavelet transforms.



### STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset
- **Arxiv ID**: http://arxiv.org/abs/1705.00823v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.00823v1)
- **Published**: 2017-05-02 07:07:55+00:00
- **Updated**: 2017-05-02 07:07:55+00:00
- **Authors**: Yuya Yoshikawa, Yutaro Shigeto, Akikazu Takeuchi
- **Comment**: Accepted as ACL2017 short paper. 5 pages
- **Journal**: None
- **Summary**: In recent years, automatic generation of image descriptions (captions), that is, image captioning, has attracted a great deal of attention. In this paper, we particularly consider generating Japanese captions for images. Since most available caption datasets have been constructed for English language, there are few datasets for Japanese. To tackle this problem, we construct a large-scale Japanese image caption dataset based on images from MS-COCO, which is called STAIR Captions. STAIR Captions consists of 820,310 Japanese captions for 164,062 images. In the experiment, we show that a neural network trained using STAIR Captions can generate more natural and better Japanese captions, compared to those generated using English-Japanese machine translation after generating English captions.



### Investigation of Different Skeleton Features for CNN-based 3D Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1705.00835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00835v1)
- **Published**: 2017-05-02 07:42:35+00:00
- **Updated**: 2017-05-02 07:42:35+00:00
- **Authors**: Zewei Ding, Pichao Wang, Philip O. Ogunbona, Wanqing Li
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning techniques are being used in skeleton based action recognition tasks and outstanding performance has been reported. Compared with RNN based methods which tend to overemphasize temporal information, CNN-based approaches can jointly capture spatio-temporal information from texture color images encoded from skeleton sequences. There are several skeleton-based features that have proven effective in RNN-based and handcrafted-feature-based methods. However, it remains unknown whether they are suitable for CNN-based approaches. This paper proposes to encode five spatial skeleton features into images with different encoding methods. In addition, the performance implication of different joints used for feature extraction is studied. The proposed method achieved state-of-the-art performance on NTU RGB+D dataset for 3D human action analysis. An accuracy of 75.32\% was achieved in Large Scale 3D Human Activity Analysis Challenge in Depth Videos.



### Transfer Learning by Ranking for Weakly Supervised Object Annotation
- **Arxiv ID**: http://arxiv.org/abs/1705.00873v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00873v1)
- **Published**: 2017-05-02 09:23:27+00:00
- **Updated**: 2017-05-02 09:23:27+00:00
- **Authors**: Zhiyuan Shi, Parthipan Siva, Tao Xiang
- **Comment**: BMVC 2012
- **Journal**: None
- **Summary**: Most existing approaches to training object detectors rely on fully supervised learning, which requires the tedious manual annotation of object location in a training set. Recently there has been an increasing interest in developing weakly supervised approach to detector training where the object location is not manually annotated but automatically determined based on binary (weak) labels indicating if a training image contains the object. This is a challenging problem because each image can contain many candidate object locations which partially overlaps the object of interest. Existing approaches focus on how to best utilise the binary labels for object location annotation. In this paper we propose to solve this problem from a very different perspective by casting it as a transfer learning problem. Specifically, we formulate a novel transfer learning based on learning to rank, which effectively transfers a model for automatic annotation of object location from an auxiliary dataset to a target dataset with completely unrelated object categories. We show that our approach outperforms existing state-of-the-art weakly supervised approach to annotating objects in the challenging VOC dataset.



### Show, Adapt and Tell: Adversarial Training of Cross-domain Image Captioner
- **Arxiv ID**: http://arxiv.org/abs/1705.00930v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.00930v2)
- **Published**: 2017-05-02 12:06:54+00:00
- **Updated**: 2017-08-14 15:54:32+00:00
- **Authors**: Tseng-Hung Chen, Yuan-Hong Liao, Ching-Yao Chuang, Wan-Ting Hsu, Jianlong Fu, Min Sun
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: Impressive image captioning results are achieved in domains with plenty of training image and sentence pairs (e.g., MSCOCO). However, transferring to a target domain with significant domain shifts but no paired training data (referred to as cross-domain image captioning) remains largely unexplored. We propose a novel adversarial training procedure to leverage unpaired data in the target domain. Two critic networks are introduced to guide the captioner, namely domain critic and multi-modal critic. The domain critic assesses whether the generated sentences are indistinguishable from sentences in the target domain. The multi-modal critic assesses whether an image and its generated sentence are a valid pair. During training, the critics and captioner act as adversaries -- captioner aims to generate indistinguishable sentences, whereas critics aim at distinguishing them. The assessment improves the captioner through policy gradient updates. During inference, we further propose a novel critic-based planning method to select high-quality sentences without additional supervision (e.g., tags). To evaluate, we use MSCOCO as the source domain and four other datasets (CUB-200-2011, Oxford-102, TGIF, and Flickr30k) as the target domains. Our method consistently performs well on all datasets. In particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after adaptation. Utilizing critics during inference further gives another 4.5% boost.



### Error Corrective Boosting for Learning Fully Convolutional Networks with Limited Data
- **Arxiv ID**: http://arxiv.org/abs/1705.00938v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00938v2)
- **Published**: 2017-05-02 12:28:24+00:00
- **Updated**: 2017-07-02 07:58:41+00:00
- **Authors**: Abhijit Guha Roy, Sailesh Conjeti, Debdoot Sheet, Amin Katouzian, Nassir Navab, Christian Wachinger
- **Comment**: Accepted at MICCAI 2017
- **Journal**: None
- **Summary**: Training deep fully convolutional neural networks (F-CNNs) for semantic image segmentation requires access to abundant labeled data. While large datasets of unlabeled image data are available in medical applications, access to manually labeled data is very limited. We propose to automatically create auxiliary labels on initially unlabeled data with existing tools and to use them for pre-training. For the subsequent fine-tuning of the network with manually labeled data, we introduce error corrective boosting (ECB), which emphasizes parameter updates on classes with lower accuracy. Furthermore, we introduce SkipDeconv-Net (SD-Net), a new F-CNN architecture for brain segmentation that combines skip connections with the unpooling strategy for upsampling. The SD-Net addresses challenges of severe class imbalance and errors along boundaries. With application to whole-brain MRI T1 scan segmentation, we generate auxiliary labels on a large dataset with FreeSurfer and fine-tune on two datasets with manual annotations. Our results show that the inclusion of auxiliary labels and ECB yields significant improvements. SD-Net segments a 3D scan in 7 secs in comparison to 30 hours for the closest multi-atlas segmentation method, while reaching similar performance. It also outperforms the latest state-of-the-art F-CNN models.



### Scalable Surface Reconstruction from Point Clouds with Extreme Scale and Density Diversity
- **Arxiv ID**: http://arxiv.org/abs/1705.00949v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1705.00949v1)
- **Published**: 2017-05-02 13:13:47+00:00
- **Updated**: 2017-05-02 13:13:47+00:00
- **Authors**: Christian Mostegel, Rudolf Prettenthaler, Friedrich Fraundorfer, Horst Bischof
- **Comment**: This paper was accepted to the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR), 2017. The copyright was transfered to IEEE
  (ieee.org). The official version of the paper will be made available on IEEE
  Xplore (R) (ieeexplore.ieee.org). This version of the paper also contains the
  supplementary material, which will not appear IEEE Xplore (R)
- **Journal**: None
- **Summary**: In this paper we present a scalable approach for robustly computing a 3D surface mesh from multi-scale multi-view stereo point clouds that can handle extreme jumps of point density (in our experiments three orders of magnitude). The backbone of our approach is a combination of octree data partitioning, local Delaunay tetrahedralization and graph cut optimization. Graph cut optimization is used twice, once to extract surface hypotheses from local Delaunay tetrahedralizations and once to merge overlapping surface hypotheses even when the local tetrahedralizations do not share the same topology.This formulation allows us to obtain a constant memory consumption per sub-problem while at the same time retaining the density independent interpolation properties of the Delaunay-based optimization. On multiple public datasets, we demonstrate that our approach is highly competitive with the state-of-the-art in terms of accuracy, completeness and outlier resilience. Further, we demonstrate the multi-scale potential of our approach by processing a newly recorded dataset with 2 billion points and a point density variation of more than four orders of magnitude - requiring less than 9GB of RAM per process.



### Active Image-based Modeling with a Toy Drone
- **Arxiv ID**: http://arxiv.org/abs/1705.01010v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01010v3)
- **Published**: 2017-05-02 15:06:36+00:00
- **Updated**: 2018-03-07 09:52:14+00:00
- **Authors**: Rui Huang, Danping Zou, Richard Vaughan, Ping Tan
- **Comment**: To be published on International Conference on Robotics and
  Automation 2018, Brisbane, Australia. Project Page:
  https://huangrui815.github.io/active-image-based-modeling/ The author's
  personal page: http://www.sfu.ca/~rha55/
- **Journal**: None
- **Summary**: Image-based modeling techniques can now generate photo-realistic 3D models from images. But it is up to users to provide high quality images with good coverage and view overlap, which makes the data capturing process tedious and time consuming. We seek to automate data capturing for image-based modeling. The core of our system is an iterative linear method to solve the multi-view stereo (MVS) problem quickly and plan the Next-Best-View (NBV) effectively. Our fast MVS algorithm enables online model reconstruction and quality assessment to determine the NBVs on the fly. We test our system with a toy unmanned aerial vehicle (UAV) in simulated, indoor and outdoor experiments. Results show that our system improves the efficiency of data acquisition and ensures the completeness of the final model.



### Visual Attribute Transfer through Deep Image Analogy
- **Arxiv ID**: http://arxiv.org/abs/1705.01088v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01088v2)
- **Published**: 2017-05-02 17:44:01+00:00
- **Updated**: 2017-06-06 15:16:19+00:00
- **Authors**: Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, Sing Bing Kang
- **Comment**: Accepted by SIGGRAPH 2017
- **Journal**: None
- **Summary**: We propose a new technique for visual attribute transfer across images that may have very different appearance but have perceptually similar semantic structure. By visual attribute transfer, we mean transfer of visual information (such as color, tone, texture, and style) from one image to another. For example, one image could be that of a painting or a sketch while the other is a photo of a real scene, and both depict the same type of scene.   Our technique finds semantically-meaningful dense correspondences between two input images. To accomplish this, it adapts the notion of "image analogy" with features extracted from a Deep Convolutional Neutral Network for matching; we call our technique Deep Image Analogy. A coarse-to-fine strategy is used to compute the nearest-neighbor field for generating the results. We validate the effectiveness of our proposed method in a variety of cases, including style/texture transfer, color/style swap, sketch/painting to photo, and time lapse.



### Recovery of structure of looped jointed objects from multiframes
- **Arxiv ID**: http://arxiv.org/abs/1705.01148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01148v1)
- **Published**: 2017-05-02 19:21:51+00:00
- **Updated**: 2017-05-02 19:21:51+00:00
- **Authors**: Mieczysław Kłopotek
- **Comment**: None
- **Journal**: a preliminary version for Machine Graphics and Vision, Vol. 3 No.
  4, pp. 645-656, 1995
- **Summary**: A method to recover structural parameters of looped jointed objects from multiframes is being developed. Each rigid part of the jointed body needs only to be traced at two (that is at junction) points.   This method has been linearized for 4-part loops, with recovery from at least 19 frames.



### Out-of-focus: Learning Depth from Image Bokeh for Robotic Perception
- **Arxiv ID**: http://arxiv.org/abs/1705.01152v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1705.01152v1)
- **Published**: 2017-05-02 19:30:51+00:00
- **Updated**: 2017-05-02 19:30:51+00:00
- **Authors**: Eric Cristofalo, Zijian Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this project, we propose a novel approach for estimating depth from RGB images. Traditionally, most work uses a single RGB image to estimate depth, which is inherently difficult and generally results in poor performance, even with thousands of data examples. In this work, we alternatively use multiple RGB images that were captured while changing the focus of the camera's lens. This method leverages the natural depth information correlated to the different patterns of clarity/blur in the sequence of focal images, which helps distinguish objects at different depths. Since no such data set exists for learning this mapping, we collect our own data set using customized hardware. We then use a convolutional neural network for learning the depth from the stacked focal images. Comparative studies were conducted on both a standard RGBD data set and our own data set (learning from both single and multiple images), and results verified that stacked focal images yield better depth estimation than using just single RGB image.



### Shading Annotations in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1705.01156v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1705.01156v1)
- **Published**: 2017-05-02 19:54:31+00:00
- **Updated**: 2017-05-02 19:54:31+00:00
- **Authors**: Balazs Kovacs, Sean Bell, Noah Snavely, Kavita Bala
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Understanding shading effects in images is critical for a variety of vision and graphics problems, including intrinsic image decomposition, shadow removal, image relighting, and inverse rendering. As is the case with other vision tasks, machine learning is a promising approach to understanding shading - but there is little ground truth shading data available for real-world images. We introduce Shading Annotations in the Wild (SAW), a new large-scale, public dataset of shading annotations in indoor scenes, comprised of multiple forms of shading judgments obtained via crowdsourcing, along with shading annotations automatically generated from RGB-D imagery. We use this data to train a convolutional neural network to predict per-pixel shading information in an image. We demonstrate the value of our data and network in an application to intrinsic images, where we can reduce decomposition artifacts produced by existing algorithms. Our database is available at http://opensurfaces.cs.cornell.edu/saw/.



### Cascaded Boundary Regression for Temporal Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1705.01180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01180v1)
- **Published**: 2017-05-02 21:45:21+00:00
- **Updated**: 2017-05-02 21:45:21+00:00
- **Authors**: Jiyang Gao, Zhenheng Yang, Ram Nevatia
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal action detection in long videos is an important problem. State-of-the-art methods address this problem by applying action classifiers on sliding windows. Although sliding windows may contain an identifiable portion of the actions, they may not necessarily cover the entire action instance, which would lead to inferior performance. We adapt a two-stage temporal action detection pipeline with Cascaded Boundary Regression (CBR) model. Class-agnostic proposals and specific actions are detected respectively in the first and the second stage. CBR uses temporal coordinate regression to refine the temporal boundaries of the sliding windows. The salient aspect of the refinement process is that, inside each stage, the temporal boundaries are adjusted in a cascaded way by feeding the refined windows back to the system for further boundary refinement. We test CBR on THUMOS-14 and TVSeries, and achieve state-of-the-art performance on both datasets. The performance gain is especially remarkable under high IoU thresholds, e.g. map@tIoU=0.5 on THUMOS-14 is improved from 19.0% to 31.0%.



