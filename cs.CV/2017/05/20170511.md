# Arxiv Papers in cs.CV on 2017-05-11
### Distribution of degrees of freedom over structure and motion of rigid bodies
- **Arxiv ID**: http://arxiv.org/abs/1705.03986v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03986v1)
- **Published**: 2017-05-11 01:53:18+00:00
- **Updated**: 2017-05-11 01:53:18+00:00
- **Authors**: Mieczysław A. Kłopotek
- **Comment**: 20 pages, 7 figures
- **Journal**: Machine Graphics and Vision, 1995, Vol. 4, No 1-2 (preliminary
  version)
- **Summary**: This paper is concerned with recovery of motion and structure parameters from multiframes under orthogonal projection when only points are traced. The main question is how many points and/or how many frames are necessary for the task. It is demonstrated that 3 frames and 3 points are the absolute minimum. Closed-form solution is presented. Furthermore, it is shown that the task may be linearized if either four points or four frames are available. It is demonstrated that no increase in the number of points may lead to recovery of structure and motion parameters from two frames only. It is shown that instead the increase in the number of points may support the task of tracing the points from frame to frame.



### SCNet: Learning Semantic Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1705.04043v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04043v3)
- **Published**: 2017-05-11 07:30:36+00:00
- **Updated**: 2017-08-17 13:25:25+00:00
- **Authors**: Kai Han, Rafael S. Rezende, Bumsub Ham, Kwan-Yee K. Wong, Minsu Cho, Cordelia Schmid, Jean Ponce
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: This paper addresses the problem of establishing semantic correspondences between images depicting different instances of the same object or scene category. Previous approaches focus on either combining a spatial regularizer with hand-crafted features, or learning a correspondence model for appearance only. We propose instead a convolutional neural network architecture, called SCNet, for learning a geometrically plausible model for semantic correspondence. SCNet uses region proposals as matching primitives, and explicitly incorporates geometric consistency in its loss function. It is trained on image pairs obtained from the PASCAL VOC 2007 keypoint dataset, and a comparative evaluation on several standard benchmarks demonstrates that the proposed approach substantially outperforms both recent deep learning architectures and previous methods based on hand-crafted features.



### Neural Style Transfer: A Review
- **Arxiv ID**: http://arxiv.org/abs/1705.04058v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.04058v7)
- **Published**: 2017-05-11 08:08:44+00:00
- **Updated**: 2018-10-30 09:48:05+00:00
- **Authors**: Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu, Mingli Song
- **Comment**: Project page: https://github.com/ycjing/Neural-Style-Transfer-Papers
- **Journal**: None
- **Summary**: The seminal work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNNs) in creating artistic imagery by separating and recombining image content and style. This process of using CNNs to render a content image in different styles is referred to as Neural Style Transfer (NST). Since then, NST has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention and a variety of approaches are proposed to either improve or extend the original NST algorithm. In this paper, we aim to provide a comprehensive overview of the current progress towards NST. We first propose a taxonomy of current algorithms in the field of NST. Then, we present several evaluation methods and compare different NST algorithms both qualitatively and quantitatively. The review concludes with a discussion of various applications of NST and open problems for future research. A list of papers discussed in this review, corresponding codes, pre-trained models and more comparison results are publicly available at https://github.com/ycjing/Neural-Style-Transfer-Papers.



### An Efficient Approach for Object Detection and Tracking of Objects in a Video with Variable Background
- **Arxiv ID**: http://arxiv.org/abs/1706.02672v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02672v1)
- **Published**: 2017-05-11 08:23:35+00:00
- **Updated**: 2017-05-11 08:23:35+00:00
- **Authors**: Kumar S. Ray, Soma Chakraborty
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel approach to create an automated visual surveillance system which is very efficient in detecting and tracking moving objects in a video captured by moving camera without any apriori information about the captured scene. Separating foreground from the background is challenging job in videos captured by moving camera as both foreground and background information change in every consecutive frames of the image sequence; thus a pseudo-motion is perceptive in background. In the proposed algorithm, the pseudo-motion in background is estimated and compensated using phase correlation of consecutive frames based on the principle of Fourier shift theorem. Then a method is proposed to model an acting background from recent history of commonality of the current frame and the foreground is detected by the differences between the background model and the current frame. Further exploiting the recent history of dissimilarities of the current frame, actual moving objects are detected in the foreground. Next, a two-stepped morphological operation is proposed to refine the object region for an optimum object size. Each object is attributed by its centroid, dimension and three highest peaks of its gray value histogram. Finally, each object is tracked using Kalman filter based on its attributes. The major advantage of this algorithm over most of the existing object detection and tracking algorithms is that, it does not require initialization of object position in the first frame or training on sample data to perform. Performance of the algorithm is tested on benchmark videos containing variable background and very satisfiable results is achieved. The performance of the algorithm is also comparable with some of the state-of-the-art algorithms for object detection and tracking.



### Automatic Extrinsic Calibration for Lidar-Stereo Vehicle Sensor Setups
- **Arxiv ID**: http://arxiv.org/abs/1705.04085v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, 68T45, I.4.8; I.2.9; I.4.1
- **Links**: [PDF](http://arxiv.org/pdf/1705.04085v3)
- **Published**: 2017-05-11 09:27:59+00:00
- **Updated**: 2017-07-27 13:54:46+00:00
- **Authors**: Carlos Guindel, Jorge Beltrán, David Martín, Fernando García
- **Comment**: Accepted to IEEE International Conference on Intelligent
  Transportation Systems 2017 (ITSC)
- **Journal**: None
- **Summary**: Sensor setups consisting of a combination of 3D range scanner lasers and stereo vision systems are becoming a popular choice for on-board perception systems in vehicles; however, the combined use of both sources of information implies a tedious calibration process. We present a method for extrinsic calibration of lidar-stereo camera pairs without user intervention. Our calibration approach is aimed to cope with the constraints commonly found in automotive setups, such as low-resolution and specific sensor poses. To demonstrate the performance of our method, we also introduce a novel approach for the quantitative assessment of the calibration results, based on a simulation environment. Tests using real devices have been conducted as well, proving the usability of the system and the improvement over the existing approaches. Code is available at http://wiki.ros.org/velo2cam_calibration



### A Generative Model of People in Clothing
- **Arxiv ID**: http://arxiv.org/abs/1705.04098v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04098v3)
- **Published**: 2017-05-11 10:07:09+00:00
- **Updated**: 2017-07-31 16:09:17+00:00
- **Authors**: Christoph Lassner, Gerard Pons-Moll, Peter V. Gehler
- **Comment**: None
- **Journal**: None
- **Summary**: We present the first image-based generative model of people in clothing for the full body. We sidestep the commonly used complex graphics rendering pipeline and the need for high-quality 3D scans of dressed people. Instead, we learn generative models from a large image database. The main challenge is to cope with the high variance in human pose, shape and appearance. For this reason, pure image-based approaches have not been considered so far. We show that this challenge can be overcome by splitting the generating process in two parts. First, we learn to generate a semantic segmentation of the body and clothing. Second, we learn a conditional model on the resulting segments that creates realistic images. The full model is differentiable and can be conditioned on pose, shape or color. The result are samples of people in different clothing items and styles. The proposed model can generate entirely new people with realistic clothing. In several experiments we present encouraging results that suggest an entirely data-driven approach to people generation is possible.



### Obstacle Avoidance Using Stereo Camera
- **Arxiv ID**: http://arxiv.org/abs/1705.04114v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04114v2)
- **Published**: 2017-05-11 11:34:27+00:00
- **Updated**: 2017-05-16 16:46:57+00:00
- **Authors**: Akkas Uddin Haque, Ashkan Nejadpak
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a novel method for obstacle avoidance using the stereo camera. The conventional obstacle avoidance methods and their limitations are discussed. A new algorithm is developed for the real-time obstacle avoidance which responds faster to unexpected obstacles. In this approach the depth map is divided into optimized number of regions and the minimum depth at each section is assigned as the depth of that region. A fuzzy controller is designed to create the drive commands for the robot/quadcopter. The system was tested on multiple paths with different obstacles and the results demonstrated the high accuracy of the developed system.



### Incremental Learning Through Deep Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1705.04228v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.04228v2)
- **Published**: 2017-05-11 15:04:10+00:00
- **Updated**: 2018-02-13 19:41:40+00:00
- **Authors**: Amir Rosenfeld, John K. Tsotsos
- **Comment**: Extended version
- **Journal**: None
- **Summary**: Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added domain, typically as many as the original network. We propose a method called \emph{Deep Adaptation Networks} (DAN) that constrains newly learned filters to be linear combinations of existing ones. DANs precisely preserve performance on the original domain, require a fraction (typically 13\%, dependent on network architecture) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance. When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3\% of the original with negligible or no loss in accuracy. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior.



### Probabilistic Image Colorization
- **Arxiv ID**: http://arxiv.org/abs/1705.04258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04258v1)
- **Published**: 2017-05-11 16:09:16+00:00
- **Updated**: 2017-05-11 16:09:16+00:00
- **Authors**: Amelie Royer, Alexander Kolesnikov, Christoph H. Lampert
- **Comment**: None
- **Journal**: None
- **Summary**: We develop a probabilistic technique for colorizing grayscale natural images. In light of the intrinsic uncertainty of this task, the proposed probabilistic framework has numerous desirable properties. In particular, our model is able to produce multiple plausible and vivid colorizations for a given grayscale image and is one of the first colorization models to provide a proper stochastic sampling scheme. Moreover, our training procedure is supported by a rigorous theoretical framework that does not require any ad hoc heuristics and allows for efficient modeling and learning of the joint pixel color distribution. We demonstrate strong quantitative and qualitative experimental results on the CIFAR-10 dataset and the challenging ILSVRC 2012 dataset.



### A Cascaded Convolutional Neural Network for X-ray Low-dose CT Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1705.04267v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML, I.2.6; I.4.3; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1705.04267v2)
- **Published**: 2017-05-11 16:32:55+00:00
- **Updated**: 2017-08-28 13:48:08+00:00
- **Authors**: Dufan Wu, Kyungsang Kim, Georges El Fakhri, Quanzheng Li
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: Image denoising techniques are essential to reducing noise levels and enhancing diagnosis reliability in low-dose computed tomography (CT). Machine learning based denoising methods have shown great potential in removing the complex and spatial-variant noises in CT images. However, some residue artifacts would appear in the denoised image due to complexity of noises. A cascaded training network was proposed in this work, where the trained CNN was applied on the training dataset to initiate new trainings and remove artifacts induced by denoising. A cascades of convolutional neural networks (CNN) were built iteratively to achieve better performance with simple CNN structures. Experiments were carried out on 2016 Low-dose CT Grand Challenge datasets to evaluate the method's performance.



### Challenges in Monocular Visual Odometry: Photometric Calibration, Motion Bias and Rolling Shutter Effect
- **Arxiv ID**: http://arxiv.org/abs/1705.04300v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04300v4)
- **Published**: 2017-05-11 17:36:43+00:00
- **Updated**: 2018-06-07 11:46:59+00:00
- **Authors**: Nan Yang, Rui Wang, Xiang Gao, Daniel Cremers
- **Comment**: Accepted by IEEE Robotics and Automation Letters (RA-L), 2018. The
  first two authors contributed equally to this paper
- **Journal**: None
- **Summary**: Monocular visual odometry (VO) and simultaneous localization and mapping (SLAM) have seen tremendous improvements in accuracy, robustness and efficiency, and have gained increasing popularity over recent years. Nevertheless, not so many discussions have been carried out to reveal the influences of three very influential yet easily overlooked aspects: photometric calibration, motion bias and rolling shutter effect. In this work, we evaluate these three aspects quantitatively on the state of the art of direct, feature-based and semi-direct methods, providing the community with useful practical knowledge both for better applying existing methods and developing new algorithms of VO and SLAM. Conclusions (some of which are counter-intuitive) are drawn with both technical and empirical analyses to all of our experiments. Possible improvements on existing methods are directed or proposed, such as a sub-pixel accuracy refinement of ORB-SLAM which boosts its performance.



### A Feature Embedding Strategy for High-level CNN representations from Multiple ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1705.04301v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04301v1)
- **Published**: 2017-05-11 17:38:12+00:00
- **Updated**: 2017-05-11 17:38:12+00:00
- **Authors**: Thangarajah Akilan, Q. M. Jonathan Wu, Wei Jiang
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Following the rapidly growing digital image usage, automatic image categorization has become preeminent research area. It has broaden and adopted many algorithms from time to time, whereby multi-feature (generally, hand-engineered features) based image characterization comes handy to improve accuracy. Recently, in machine learning, pre-trained deep convolutional neural networks (DCNNs or ConvNets) have been that the features extracted through such DCNN can improve classification accuracy. Thence, in this paper, we further investigate a feature embedding strategy to exploit cues from multiple DCNNs. We derive a generalized feature space by embedding three different DCNN bottleneck features with weights respect to their Softmax cross-entropy loss. Test outcomes on six different object classification data-sets and an action classification data-set show that regardless of variation in image statistics and tasks the proposed multi-DCNN bottleneck feature fusion is well suited to image classification tasks and an effective complement of DCNN. The comparisons to existing fusion-based image classification approaches prove that the proposed method surmounts the state-of-the-art methods and produces competitive results with fully trained DCNNs as well.



### Imagination improves Multimodal Translation
- **Arxiv ID**: http://arxiv.org/abs/1705.04350v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.04350v2)
- **Published**: 2017-05-11 18:49:17+00:00
- **Updated**: 2017-07-07 09:18:55+00:00
- **Authors**: Desmond Elliott, Ákos Kádár
- **Comment**: Clarified main contributions, minor correction to Equation 8,
  additional comparisons in Table 2, added more related work
- **Journal**: None
- **Summary**: We decompose multimodal translation into two sub-tasks: learning to translate and learning visually grounded representations. In a multitask learning framework, translations are learned in an attention-based encoder-decoder, and grounded representations are learned through image representation prediction. Our approach improves translation performance compared to the state of the art on the Multi30K dataset. Furthermore, it is equally effective if we train the image prediction task on the external MS COCO dataset, and we find improvements if we train the translation model on the external News Commentary parallel text.



### Reconfiguring the Imaging Pipeline for Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1705.04352v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04352v3)
- **Published**: 2017-05-11 18:57:01+00:00
- **Updated**: 2017-08-01 18:04:40+00:00
- **Authors**: Mark Buckler, Suren Jayasuriya, Adrian Sampson
- **Comment**: None
- **Journal**: None
- **Summary**: Advancements in deep learning have ignited an explosion of research on efficient hardware for embedded computer vision. Hardware vision acceleration, however, does not address the cost of capturing and processing the image data that feeds these algorithms. We examine the role of the image signal processing (ISP) pipeline in computer vision to identify opportunities to reduce computation and save energy. The key insight is that imaging pipelines should be designed to be configurable: to switch between a traditional photography mode and a low-power vision mode that produces lower-quality image data suitable only for computer vision. We use eight computer vision algorithms and a reversible pipeline simulation tool to study the imaging system's impact on vision performance. For both CNN-based and classical vision algorithms, we observe that only two ISP stages, demosaicing and gamma compression, are critical for task performance. We propose a new image sensor design that can compensate for skipping these stages. The sensor design features an adjustable resolution and tunable analog-to-digital converters (ADCs). Our proposed imaging system's vision mode disables the ISP entirely and configures the sensor to produce subsampled, lower-precision image data. This vision mode can save ~75% of the average energy of a baseline photography mode while having only a small impact on vision task accuracy.



### Object-Level Context Modeling For Scene Classification with Context-CNN
- **Arxiv ID**: http://arxiv.org/abs/1705.04358v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04358v2)
- **Published**: 2017-05-11 19:25:37+00:00
- **Updated**: 2017-06-02 05:29:20+00:00
- **Authors**: Syed Ashar Javed, Anil Kumar Nelakanti
- **Comment**: Scene Understanding workshop (SUNw), CVPR 2017
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have been used extensively for computer vision tasks and produce rich feature representation for objects or parts of an image. But reasoning about scenes requires integration between the low-level feature representations and the high-level semantic information. We propose a deep network architecture which models the semantic context of scenes by capturing object-level information. We use Long Short Term Memory(LSTM) units in conjunction with object proposals to incorporate object-object relationship and object-scene relationship in an end-to-end trainable manner. We evaluate our model on the LSUN dataset and achieve results comparable to the state-of-art. We further show visualization of the learned features and analyze the model with experiments to verify our model's ability to model context.



### Recent Advances in Transfer Learning for Cross-Dataset Visual Recognition: A Problem-Oriented Perspective
- **Arxiv ID**: http://arxiv.org/abs/1705.04396v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04396v3)
- **Published**: 2017-05-11 23:24:37+00:00
- **Updated**: 2019-05-20 03:14:21+00:00
- **Authors**: Jing Zhang, Wanqing Li, Philip Ogunbona, Dong Xu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper takes a problem-oriented perspective and presents a comprehensive review of transfer learning methods, both shallow and deep, for cross-dataset visual recognition. Specifically, it categorises the cross-dataset recognition into seventeen problems based on a set of carefully chosen data and label attributes. Such a problem-oriented taxonomy has allowed us to examine how different transfer learning approaches tackle each problem and how well each problem has been researched to date. The comprehensive problem-oriented review of the advances in transfer learning with respect to the problem has not only revealed the challenges in transfer learning for visual recognition, but also the problems (e.g. eight of the seventeen problems) that have been scarcely studied. This survey not only presents an up-to-date technical review for researchers, but also a systematic approach and a reference for a machine learning practitioner to categorise a real problem and to look up for a possible solution accordingly.



### Negative Results in Computer Vision: A Perspective
- **Arxiv ID**: http://arxiv.org/abs/1705.04402v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04402v3)
- **Published**: 2017-05-11 23:39:18+00:00
- **Updated**: 2017-06-06 23:23:28+00:00
- **Authors**: Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: A negative result is when the outcome of an experiment or a model is not what is expected or when a hypothesis does not hold. Despite being often overlooked in the scientific community, negative results are results and they carry value. While this topic has been extensively discussed in other fields such as social sciences and biosciences, less attention has been paid to it in the computer vision community. The unique characteristics of computer vision, particularly its experimental aspect, call for a special treatment of this matter. In this paper, I will address what makes negative results important, how they should be disseminated and incentivized, and what lessons can be learned from cognitive vision research in this regard. Further, I will discuss issues such as computer vision and human vision interaction, experimental design and statistical hypothesis testing, explanatory versus predictive modeling, performance evaluation, model comparison, as well as computer vision research culture.



