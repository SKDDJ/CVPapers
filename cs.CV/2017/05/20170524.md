# Arxiv Papers in cs.CV on 2017-05-24
### Sequence Summarization Using Order-constrained Kernelized Feature Subspaces
- **Arxiv ID**: http://arxiv.org/abs/1705.08583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08583v1)
- **Published**: 2017-05-24 02:11:04+00:00
- **Updated**: 2017-05-24 02:11:04+00:00
- **Authors**: Anoop Cherian, Suvrit Sra, Richard Hartley
- **Comment**: None
- **Journal**: None
- **Summary**: Representations that can compactly and effectively capture temporal evolution of semantic content are important to machine learning algorithms that operate on multi-variate time-series data. We investigate such representations motivated by the task of human action recognition. Here each data instance is encoded by a multivariate feature (such as via a deep CNN) where action dynamics are characterized by their variations in time. As these features are often non-linear, we propose a novel pooling method, kernelized rank pooling, that represents a given sequence compactly as the pre-image of the parameters of a hyperplane in an RKHS, projections of data onto which captures their temporal order. We develop this idea further and show that such a pooling scheme can be cast as an order-constrained kernelized PCA objective; we then propose to use the parameters of a kernelized low-rank feature subspace as the representation of the sequences. We cast our formulation as an optimization problem on generalized Grassmann manifolds and then solve it efficiently using Riemannian optimization techniques. We present experiments on several action recognition datasets using diverse feature modalities and demonstrate state-of-the-art results.



### Generative Model with Coordinate Metric Learning for Object Recognition Based on 3D Models
- **Arxiv ID**: http://arxiv.org/abs/1705.08590v2
- **DOI**: 10.1109/TIP.2018.2858553
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08590v2)
- **Published**: 2017-05-24 03:22:18+00:00
- **Updated**: 2018-04-24 13:14:41+00:00
- **Authors**: Yida Wang, Weihong Deng
- **Comment**: 14 pages
- **Journal**: in IEEE Transactions on Image Processing, vol. 27, no. 12, pp.
  5813-5826, Dec. 2018
- **Summary**: Given large amount of real photos for training, Convolutional neural network shows excellent performance on object recognition tasks. However, the process of collecting data is so tedious and the background are also limited which makes it hard to establish a perfect database. In this paper, our generative model trained with synthetic images rendered from 3D models reduces the workload of data collection and limitation of conditions. Our structure is composed of two sub-networks: semantic foreground object reconstruction network based on Bayesian inference and classification network based on multi-triplet cost function for avoiding over-fitting problem on monotone surface and fully utilizing pose information by establishing sphere-like distribution of descriptors in each category which is helpful for recognition on regular photos according to poses, lighting condition, background and category information of rendered images. Firstly, our conjugate structure called generative model with metric learning utilizing additional foreground object channels generated from Bayesian rendering as the joint of two sub-networks. Multi-triplet cost function based on poses for object recognition are used for metric learning which makes it possible training a category classifier purely based on synthetic data. Secondly, we design a coordinate training strategy with the help of adaptive noises acting as corruption on input images to help both sub-networks benefit from each other and avoid inharmonious parameter tuning due to different convergence speed of two sub-networks. Our structure achieves the state of the art accuracy of over 50\% on ShapeNet database with data migration obstacle from synthetic images to real photos. This pipeline makes it applicable to do recognition on real images only based on 3D models.



### Deep Learning Improves Template Matching by Normalized Cross Correlation
- **Arxiv ID**: http://arxiv.org/abs/1705.08593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08593v1)
- **Published**: 2017-05-24 03:24:25+00:00
- **Updated**: 2017-05-24 03:24:25+00:00
- **Authors**: Davit Buniatyan, Thomas Macrina, Dodam Ih, Jonathan Zung, H. Sebastian Seung
- **Comment**: None
- **Journal**: None
- **Summary**: Template matching by normalized cross correlation (NCC) is widely used for finding image correspondences. We improve the robustness of this algorithm by preprocessing images with "siamese" convolutional networks trained to maximize the contrast between NCC values of true and false matches. The improvement is quantified using patches of brain images from serial section electron microscopy. Relative to a parameter-tuned bandpass filter, siamese convolutional networks significantly reduce false matches. Furthermore, all false matches can be eliminated by removing a tiny fraction of all matches based on NCC values. The improved accuracy of our method could be essential for connectomics, because emerging petascale datasets may require billions of template matches to assemble 2D images of serial sections into a 3D image stack. Our method is also expected to generalize to many other computer vision applications that use NCC template matching to find image correspondences.



### Robust Data Geometric Structure Aligned Close yet Discriminative Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1705.08620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08620v1)
- **Published**: 2017-05-24 06:01:18+00:00
- **Updated**: 2017-05-24 06:01:18+00:00
- **Authors**: Lingkun Luo, Xiaofang Wang, Shiqiang Hu, Liming Chen
- **Comment**: 12 pages, 1 figure
- **Journal**: None
- **Summary**: Domain adaptation (DA) is transfer learning which aims to leverage labeled data in a related source domain to achieve informed knowledge transfer and help the classification of unlabeled data in a target domain. In this paper, we propose a novel DA method, namely Robust Data Geometric Structure Aligned, Close yet Discriminative Domain Adaptation (RSA-CDDA), which brings closer, in a latent joint subspace, both source and target data distributions, and aligns inherent hidden source and target data geometric structures while performing discriminative DA in repulsing both interclass source and target data. The proposed method performs domain adaptation between source and target in solving a unified model, which incorporates data distribution constraints, in particular via a nonparametric distance, i.e., Maximum Mean Discrepancy (MMD), as well as constraints on inherent hidden data geometric structure segmentation and alignment between source and target, through low rank and sparse representation. RSA-CDDA achieves the search of a joint subspace in solving the proposed unified model through iterative optimization, alternating Rayleigh quotient algorithm and inexact augmented Lagrange multiplier algorithm. Extensive experiments carried out on standard DA benchmarks, i.e., 16 cross-domain image classification tasks, verify the effectiveness of the proposed method, which consistently outperforms the state-of-the-art methods.



### Deep Rotation Equivariant Network
- **Arxiv ID**: http://arxiv.org/abs/1705.08623v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08623v2)
- **Published**: 2017-05-24 06:22:09+00:00
- **Updated**: 2018-02-28 07:13:48+00:00
- **Authors**: Junying Li, Zichen Yang, Haifeng Liu, Deng Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, learning equivariant representations has attracted considerable research attention. Dieleman et al. introduce four operations which can be inserted into convolutional neural network to learn deep representations equivariant to rotation. However, feature maps should be copied and rotated four times in each layer in their approach, which causes much running time and memory overhead. In order to address this problem, we propose Deep Rotation Equivariant Network consisting of cycle layers, isotonic layers and decycle layers. Our proposed layers apply rotation transformation on filters rather than feature maps, achieving a speed up of more than 2 times with even less memory overhead. We evaluate DRENs on Rotated MNIST and CIFAR-10 datasets and demonstrate that it can improve the performance of state-of-the-art architectures.



### VANETs Meet Autonomous Vehicles: A Multimodal 3D Environment Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1705.08624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08624v1)
- **Published**: 2017-05-24 06:24:21+00:00
- **Updated**: 2017-05-24 06:24:21+00:00
- **Authors**: Yassine Maalej, Sameh Sorour, Ahmed Abdel-Rahim, Mohsen Guizani
- **Comment**: 7 pages, 12 figures
- **Journal**: None
- **Summary**: In this paper, we design a multimodal framework for object detection, recognition and mapping based on the fusion of stereo camera frames, point cloud Velodyne Lidar scans, and Vehicle-to-Vehicle (V2V) Basic Safety Messages (BSMs) exchanged using Dedicated Short Range Communication (DSRC). We merge the key features of rich texture descriptions of objects from 2D images, depth and distance between objects provided by 3D point cloud and awareness of hidden vehicles from BSMs' 3D information. We present a joint pixel to point cloud and pixel to V2V correspondences of objects in frames from the Kitti Vision Benchmark Suite by using a semi-supervised manifold alignment approach to achieve camera-Lidar and camera-V2V mapping of their recognized objects that have the same underlying manifold.



### Self-supervised learning of visual features through embedding images into text topic spaces
- **Arxiv ID**: http://arxiv.org/abs/1705.08631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08631v1)
- **Published**: 2017-05-24 06:59:30+00:00
- **Updated**: 2017-05-24 06:59:30+00:00
- **Authors**: Lluis Gomez, Yash Patel, Marçal Rusiñol, Dimosthenis Karatzas, C. V. Jawahar
- **Comment**: Accepted CVPR 2017 paper
- **Journal**: None
- **Summary**: End-to-end training from scratch of current deep architectures for new computer vision problems would require Imagenet-scale datasets, and this is not always possible. In this paper we present a method that is able to take advantage of freely available multi-modal content to train computer vision algorithms without human supervision. We put forward the idea of performing self-supervised learning of visual features by mining a large scale corpus of multi-modal (text and image) documents. We show that discriminative visual features can be learnt efficiently by training a CNN to predict the semantic context in which a particular image is more probable to appear as an illustration. For this we leverage the hidden semantic structures discovered in the text corpus with a well-known topic modeling technique. Our experiments demonstrate state of the art performance in image classification, object detection, and multi-modal retrieval compared to recent self-supervised or natural-supervised approaches.



### Object Tracking based on Quantum Particle Swarm Optimization
- **Arxiv ID**: http://arxiv.org/abs/1707.05228v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1707.05228v1)
- **Published**: 2017-05-24 10:29:45+00:00
- **Updated**: 2017-05-24 10:29:45+00:00
- **Authors**: Rajesh Misra, Kumar S. Ray
- **Comment**: None
- **Journal**: None
- **Summary**: In Computer Vision domain, moving Object Tracking considered as one of the toughest problem.As there so many factors associated like illumination of light, noise, occlusion, sudden start and stop of moving object, shading which makes tracking even harder problem not only for dynamic background but also for static background.In this paper we present a new object tracking algorithm based on Dominant points on tracked object using Quantum particle swarm optimization (QPSO) which is a new different version of PSO based on Quantum theory. The novelty in our approach is that it can be successfully applicable in variable background as well as static background and application of quantum PSO makes the algorithm runs lot faster where other basic PSO algorithm failed to do so due to heavy computation.In our approach firstly dominants points of tracked objects detected, then a group of particles form a swarm are initialized randomly over the image search space and then start searching the curvature connected between two consecutive dominant points until they satisfy fitness criteria. Obviously it is a Multi-Swarm approach as there are multiple dominant points, as they moves, the curvature moves and the curvature movement is tracked by the swarm throughout the video and eventually when the swarm reaches optimal solution , a bounding box drawn based on particles final position.Experimental results demonstrate this proposed QPSO based method work efficiently and effectively in visual object tracking in both dynamic and static environments and run time shows that it runs closely 90% faster than basic PSO.in our approach we also apply parallelism using MatLab Parfor command to show how very less number of iteration and swarm size will enable us to successfully track object.



### Continual Learning with Deep Generative Replay
- **Arxiv ID**: http://arxiv.org/abs/1705.08690v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.08690v3)
- **Published**: 2017-05-24 10:37:38+00:00
- **Updated**: 2017-12-12 02:14:21+00:00
- **Authors**: Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim
- **Comment**: NIPS 2017
- **Journal**: None
- **Summary**: Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model ("solver"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.



### Stochastic Sequential Neural Networks with Structured Inference
- **Arxiv ID**: http://arxiv.org/abs/1705.08695v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.08695v1)
- **Published**: 2017-05-24 10:52:19+00:00
- **Updated**: 2017-05-24 10:52:19+00:00
- **Authors**: Hao Liu, Haoli Bai, Lirong He, Zenglin Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised structure learning in high-dimensional time series data has attracted a lot of research interests. For example, segmenting and labelling high dimensional time series can be helpful in behavior understanding and medical diagnosis. Recent advances in generative sequential modeling have suggested to combine recurrent neural networks with state space models (e.g., Hidden Markov Models). This combination can model not only the long term dependency in sequential data, but also the uncertainty included in the hidden states. Inheriting these advantages of stochastic neural sequential models, we propose a structured and stochastic sequential neural network, which models both the long-term dependencies via recurrent neural networks and the uncertainty in the segmentation and labels via discrete random variables. For accurate and efficient inference, we present a bi-directional inference network by reparamterizing the categorical segmentation and labels with the recent proposed Gumbel-Softmax approximation and resort to the Stochastic Gradient Variational Bayes. We evaluate the proposed model in a number of tasks, including speech modeling, automatic segmentation and labeling in behavior understanding, and sequential multi-objects recognition. Experimental results have demonstrated that our proposed model can achieve significant improvement over the state-of-the-art methods.



### Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence Models for Fill-in-the-Blank Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1705.08759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08759v1)
- **Published**: 2017-05-24 13:42:47+00:00
- **Updated**: 2017-05-24 13:42:47+00:00
- **Authors**: Qing Sun, Stefan Lee, Dhruv Batra
- **Comment**: None
- **Journal**: None
- **Summary**: We develop the first approximate inference algorithm for 1-Best (and M-Best) decoding in bidirectional neural sequence models by extending Beam Search (BS) to reason about both forward and backward time dependencies. Beam Search (BS) is a widely used approximate inference algorithm for decoding sequences from unidirectional neural sequence models. Interestingly, approximate inference in bidirectional models remains an open problem, despite their significant advantage in modeling information from both the past and future. To enable the use of bidirectional models, we present Bidirectional Beam Search (BiBS), an efficient algorithm for approximate bidirectional inference.To evaluate our method and as an interesting problem in its own right, we introduce a novel Fill-in-the-Blank Image Captioning task which requires reasoning about both past and future sentence structure to reconstruct sensible image descriptions. We use this task as well as the Visual Madlibs dataset to demonstrate the effectiveness of our approach, consistently outperforming all baseline methods.



### Adaptive Detrending to Accelerate Convolutional Gated Recurrent Unit Training for Contextual Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/1705.08764v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08764v1)
- **Published**: 2017-05-24 13:52:23+00:00
- **Updated**: 2017-05-24 13:52:23+00:00
- **Authors**: Minju Jung, Haanvid Lee, Jun Tani
- **Comment**: None
- **Journal**: None
- **Summary**: Based on the progress of image recognition, video recognition has been extensively studied recently. However, most of the existing methods are focused on short-term but not long-term video recognition, called contextual video recognition. To address contextual video recognition, we use convolutional recurrent neural networks (ConvRNNs) having a rich spatio-temporal information processing capability, but ConvRNNs requires extensive computation that slows down training. In this paper, inspired by the normalization and detrending methods, we propose adaptive detrending (AD) for temporal normalization in order to accelerate the training of ConvRNNs, especially for convolutional gated recurrent unit (ConvGRU). AD removes internal covariate shift within a sequence of each neuron in recurrent neural networks (RNNs) by subtracting a trend. In the experiments for contextual recognition on ConvGRU, the results show that (1) ConvGRU clearly outperforms the feed-forward neural networks, (2) AD consistently offers a significant training acceleration and generalization improvement, and (3) AD is further improved by collaborating with the existing normalization methods.



### The Lovász-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks
- **Arxiv ID**: http://arxiv.org/abs/1705.08790v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08790v2)
- **Published**: 2017-05-24 14:33:41+00:00
- **Updated**: 2018-04-09 09:25:36+00:00
- **Authors**: Maxim Berman, Amal Rannen Triki, Matthew B. Blaschko
- **Comment**: Accepted as a conference paper at CVPR 2018
- **Journal**: None
- **Summary**: The Jaccard index, also referred to as the intersection-over-union score, is commonly employed in the evaluation of image segmentation results given its perceptual qualities, scale invariance - which lends appropriate relevance to small objects, and appropriate counting of false negatives, in comparison to per-pixel losses. We present a method for direct optimization of the mean intersection-over-union loss in neural networks, in the context of semantic image segmentation, based on the convex Lov\'asz extension of submodular losses. The loss is shown to perform better with respect to the Jaccard index measure than the traditionally used cross-entropy loss. We show quantitative and qualitative differences between optimizing the Jaccard index per image versus optimizing the Jaccard index taken over an entire dataset. We evaluate the impact of our method in a semantic segmentation pipeline and show substantially improved intersection-over-union segmentation scores on the Pascal VOC and Cityscapes datasets using state-of-the-art deep learning segmentation architectures.



### From source to target and back: symmetric bi-directional adaptive GAN
- **Arxiv ID**: http://arxiv.org/abs/1705.08824v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08824v2)
- **Published**: 2017-05-24 15:43:20+00:00
- **Updated**: 2017-11-29 10:14:52+00:00
- **Authors**: Paolo Russo, Fabio Maria Carlucci, Tatiana Tommasi, Barbara Caputo
- **Comment**: None
- **Journal**: None
- **Summary**: The effectiveness of generative adversarial approaches in producing images according to a specific style or visual domain has recently opened new directions to solve the unsupervised domain adaptation problem. It has been shown that source labeled images can be modified to mimic target samples making it possible to train directly a classifier in the target domain, despite the original lack of annotated data. Inverse mappings from the target to the source domain have also been evaluated but only passing through adapted feature spaces, thus without new image generation. In this paper we propose to better exploit the potential of generative adversarial networks for adaptation by introducing a novel symmetric mapping among domains. We jointly optimize bi-directional image transformations combining them with target self-labeling. Moreover we define a new class consistency loss that aligns the generators in the two directions imposing to conserve the class identity of an image passing through both domain mappings. A detailed qualitative and quantitative analysis of the reconstructed images confirm the power of our approach. By integrating the two domain specific classifiers obtained with our bi-directional network we exceed previous state-of-the-art unsupervised adaptation results on four different benchmark datasets.



### How a General-Purpose Commonsense Ontology can Improve Performance of Learning-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1705.08844v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1705.08844v1)
- **Published**: 2017-05-24 16:22:53+00:00
- **Updated**: 2017-05-24 16:22:53+00:00
- **Authors**: Rodrigo Toro Icarte, Jorge A. Baier, Cristian Ruz, Alvaro Soto
- **Comment**: Accepted in IJCAI-17
- **Journal**: None
- **Summary**: The knowledge representation community has built general-purpose ontologies which contain large amounts of commonsense knowledge over relevant aspects of the world, including useful visual information, e.g.: "a ball is used by a football player", "a tennis player is located at a tennis court". Current state-of-the-art approaches for visual recognition do not exploit these rule-based knowledge sources. Instead, they learn recognition models directly from training examples. In this paper, we study how general-purpose ontologies---specifically, MIT's ConceptNet ontology---can improve the performance of state-of-the-art vision systems. As a testbed, we tackle the problem of sentence-based image retrieval. Our retrieval approach incorporates knowledge from ConceptNet on top of a large pool of object detectors derived from a deep learning technique. In our experiments, we show that ConceptNet can improve performance on a common benchmark dataset. Key to our performance is the use of the ESPGAME dataset to select visually relevant relations from ConceptNet. Consequently, a main conclusion of this work is that general-purpose commonsense ontologies improve performance on visual reasoning tasks when properly filtered to select meaningful visual relations.



### Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference
- **Arxiv ID**: http://arxiv.org/abs/1705.08850v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.08850v2)
- **Published**: 2017-05-24 16:35:37+00:00
- **Updated**: 2017-12-05 18:34:17+00:00
- **Authors**: Abhishek Kumar, Prasanna Sattigeri, P. Thomas Fletcher
- **Comment**: NIPS 2017 accepted version, including appendix
- **Journal**: None
- **Summary**: Semi-supervised learning methods using Generative Adversarial Networks (GANs) have shown promising empirical success recently. Most of these methods use a shared discriminator/classifier which discriminates real examples from fake while also predicting the class label. Motivated by the ability of the GANs generator to capture the data manifold well, we propose to estimate the tangent space to the data manifold using GANs and employ it to inject invariances into the classifier. In the process, we propose enhancements over existing methods for learning the inverse mapping (i.e., the encoder) which greatly improves in terms of semantic similarity of the reconstructed sample with the input sample. We observe considerable empirical gains in semi-supervised learning over baselines, particularly in the cases when the number of labeled examples is low. We also provide insights into how fake examples influence the semi-supervised learning procedure.



### Dense Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.08881v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.08881v2)
- **Published**: 2017-05-24 17:50:32+00:00
- **Updated**: 2017-06-08 00:10:04+00:00
- **Authors**: Jun Li, Yongjun Chen, Lei Cai, Ian Davidson, Shuiwang Ji
- **Comment**: None
- **Journal**: None
- **Summary**: The key idea of current deep learning methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel-wise predictions. These methods are limited in the sense that the patches are determined by network architecture instead of learned from data. In this work, we propose the dense transformer networks, which can learn the shapes and sizes of patches from data. The dense transformer networks employ an encoder-decoder architecture, and a pair of dense transformer modules are inserted into each of the encoder and decoder paths. The novelty of this work is that we provide technical solutions for learning the shapes and sizes of patches from data and efficiently restoring the spatial correspondence required for dense prediction. The proposed dense transformer modules are differentiable, thus the entire network can be trained. We apply the proposed networks on natural and biological image segmentation tasks and show superior performance is achieved in comparison to baseline methods.



### Unsupervised Learning Layers for Video Analysis
- **Arxiv ID**: http://arxiv.org/abs/1705.08918v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.08918v1)
- **Published**: 2017-05-24 18:22:41+00:00
- **Updated**: 2017-05-24 18:22:41+00:00
- **Authors**: Liang Zhao, Yang Wang, Yi Yang, Wei Xu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents two unsupervised learning layers (UL layers) for label-free video analysis: one for fully connected layers, and the other for convolutional ones. The proposed UL layers can play two roles: they can be the cost function layer for providing global training signal; meanwhile they can be added to any regular neural network layers for providing local training signals and combined with the training signals backpropagated from upper layers for extracting both slow and fast changing features at layers of different depths. Therefore, the UL layers can be used in either pure unsupervised or semi-supervised settings. Both a closed-form solution and an online learning algorithm for two UL layers are provided. Experiments with unlabeled synthetic and real-world videos demonstrated that the neural networks equipped with UL layers and trained with the proposed online learning algorithm can extract shape and motion information from video sequences of moving objects. The experiments demonstrated the potential applications of UL layers and online learning algorithm to head orientation estimation and moving object localization.



### Attention-based Natural Language Person Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1705.08923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08923v1)
- **Published**: 2017-05-24 18:36:58+00:00
- **Updated**: 2017-05-24 18:36:58+00:00
- **Authors**: Tao Zhou, Muhao Chen, Jie Yu, Demetri Terzopoulos
- **Comment**: CVPR 2017 Workshop (vision meets cognition)
- **Journal**: None
- **Summary**: Following the recent progress in image classification and captioning using deep learning, we develop a novel natural language person retrieval system based on an attention mechanism. More specifically, given the description of a person, the goal is to localize the person in an image. To this end, we first construct a benchmark dataset for natural language person retrieval. To do so, we generate bounding boxes for persons in a public image dataset from the segmentation masks, which are then annotated with descriptions and attributes using the Amazon Mechanical Turk. We then adopt a region proposal network in Faster R-CNN as a candidate region generator. The cropped images based on the region proposals as well as the whole images with attention weights are fed into Convolutional Neural Networks for visual feature extraction, while the natural language expression and attributes are input to Bidirectional Long Short- Term Memory (BLSTM) models for text feature extraction. The visual and text features are integrated to score region proposals, and the one with the highest score is retrieved as the output of our system. The experimental results show significant improvement over the state-of-the-art method for generic object retrieval and this line of research promises to benefit search in surveillance video footage.



### Optical Mapping Near-eye Three-dimensional Display with Correct Focus Cues
- **Arxiv ID**: http://arxiv.org/abs/1707.03685v1
- **DOI**: 10.1364/OL.42.002475
- **Categories**: **cs.CV**, physics.med-ph, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1707.03685v1)
- **Published**: 2017-05-24 19:13:44+00:00
- **Updated**: 2017-05-24 19:13:44+00:00
- **Authors**: Wei Cui, Liang Gao
- **Comment**: 5 pages, 6 figures, 2 tables, short article for Optics Letters
- **Journal**: None
- **Summary**: We present an optical mapping near-eye (OMNI) three-dimensional display method for wearable devices. By dividing a display screen into different sub-panels and optically mapping them to various depths, we create a multiplane volumetric image with correct focus cues for depth perception. The resultant system can drive the eye's accommodation to the distance that is consistent with binocular stereopsis, thereby alleviating the vergence-accommodation conflict, the primary cause for eye fatigue and discomfort. Compared with the previous methods, the OMNI display offers prominent advantages in adaptability, image dynamic range, and refresh rate.



### Visual Servoing from Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.08940v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.08940v2)
- **Published**: 2017-05-24 19:39:25+00:00
- **Updated**: 2017-06-07 09:26:34+00:00
- **Authors**: Quentin Bateux, Eric Marchand, Jürgen Leitner, Francois Chaumette, Peter Corke
- **Comment**: fixed authors list
- **Journal**: None
- **Summary**: We present a deep neural network-based method to perform high-precision, robust and real-time 6 DOF visual servoing. The paper describes how to create a dataset simulating various perturbations (occlusions and lighting conditions) from a single real-world image of the scene. A convolutional neural network is fine-tuned using this dataset to estimate the relative pose between two images of the same scene. The output of the network is then employed in a visual servoing control scheme. The method converges robustly even in difficult real-world settings with strong lighting variations and occlusions.A positioning error of less than one millimeter is obtained in experiments with a 6 DOF robot.



### GridNet with automatic shape prior registration for automatic MRI cardiac segmentation
- **Arxiv ID**: http://arxiv.org/abs/1705.08943v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08943v2)
- **Published**: 2017-05-24 19:44:45+00:00
- **Updated**: 2017-09-12 19:48:38+00:00
- **Authors**: Clement Zotti, Zhiming Luo, Alain Lalande, Olivier Humbert, Pierre-Marc Jodoin
- **Comment**: 8 pages, 1 tables, 2 figures
- **Journal**: None
- **Summary**: In this paper, we propose a fully automatic MRI cardiac segmentation method based on a novel deep convolutional neural network (CNN) designed for the 2017 ACDC MICCAI challenge. The novelty of our network comes with its embedded shape prior and its loss function tailored to the cardiac anatomy. Our model includes a cardiac centerof-mass regression module which allows for an automatic shape prior registration. Also, since our method processes raw MR images without any manual preprocessing and/or image cropping, our CNN learns both high-level features (useful to distinguish the heart from other organs with a similar shape) and low-level features (useful to get accurate segmentation results). Those features are learned with a multi-resolution conv-deconv "grid" architecture which can be seen as an extension of the U-Net. Experimental results reveal that our method can segment the left and right ventricles as well as the myocardium from a 3D MRI cardiac volume in 0.4 second with an average Dice coefficient of 0.90 and an average Hausdorff distance of 10.4 mm.



### Cultural Diffusion and Trends in Facebook Photographs
- **Arxiv ID**: http://arxiv.org/abs/1705.08974v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1705.08974v1)
- **Published**: 2017-05-24 21:49:29+00:00
- **Updated**: 2017-05-24 21:49:29+00:00
- **Authors**: Quanzeng You, Darío García-García, Mahohar Paluri, Jiebo Luo, Jungseock Joo
- **Comment**: 10 pages, To appear in ICWSM 2017 (Full Paper)
- **Journal**: None
- **Summary**: Online social media is a social vehicle in which people share various moments of their lives with their friends, such as playing sports, cooking dinner or just taking a selfie for fun, via visual means, that is, photographs. Our study takes a closer look at the popular visual concepts illustrating various cultural lifestyles from aggregated, de-identified photographs. We perform analysis both at macroscopic and microscopic levels, to gain novel insights about global and local visual trends as well as the dynamics of interpersonal cultural exchange and diffusion among Facebook friends. We processed images by automatically classifying the visual content by a convolutional neural network (CNN). Through various statistical tests, we find that socially tied individuals more likely post images showing similar cultural lifestyles. To further identify the main cause of the observed social correlation, we use the Shuffle test and the Preference-based Matched Estimation (PME) test to distinguish the effects of influence and homophily. The results indicate that the visual content of each user's photographs are temporally, although not necessarily causally, correlated with the photographs of their friends, which may suggest the effect of influence. Our paper demonstrates that Facebook photographs exhibit diverse cultural lifestyles and preferences and that the social interaction mediated through the visual channel in social media can be an effective mechanism for cultural diffusion.



### Plug-and-Play Unplugged: Optimization Free Reconstruction using Consensus Equilibrium
- **Arxiv ID**: http://arxiv.org/abs/1705.08983v3
- **DOI**: None
- **Categories**: **cs.CV**, math.OC, 94A08, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1705.08983v3)
- **Published**: 2017-05-24 22:27:00+00:00
- **Updated**: 2018-06-13 12:36:07+00:00
- **Authors**: Gregery T. Buzzard, Stanley H. Chan, Suhas Sreehari, Charles A. Bouman
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: Regularized inversion methods for image reconstruction are used widely due to their tractability and ability to combine complex physical sensor models with useful regularity criteria. Such methods motivated the recently developed Plug-and-Play prior method, which provides a framework to use advanced denoising algorithms as regularizers in inversion. However, the need to formulate regularized inversion as the solution to an optimization problem limits the possible regularity conditions and physical sensor models.   In this paper, we introduce Consensus Equilibrium (CE), which generalizes regularized inversion to include a much wider variety of both forward components and prior components without the need for either to be expressed with a cost function. CE is based on the solution of a set of equilibrium equations that balance data fit and regularity. In this framework, the problem of MAP estimation in regularized inversion is replaced by the problem of solving these equilibrium equations, which can be approached in multiple ways.   The key contribution of CE is to provide a novel framework for fusing multiple heterogeneous models of physical sensors or models learned from data. We describe the derivation of the CE equations and prove that the solution of the CE equations generalizes the standard MAP estimate under appropriate circumstances.   We also discuss algorithms for solving the CE equations, including ADMM with a novel form of preconditioning and Newton's method. We give examples to illustrate consensus equilibrium and the convergence properties of these algorithms and demonstrate this method on some toy problems and on a denoising example in which we use an array of convolutional neural network denoisers, none of which is tuned to match the noise level in a noisy image but which in consensus can achieve a better result than any of them individually.



