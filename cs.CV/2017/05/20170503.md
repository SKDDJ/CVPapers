# Arxiv Papers in cs.CV on 2017-05-03
### Marine Animal Classification with Correntropy Loss Based Multi-view Learning
- **Arxiv ID**: http://arxiv.org/abs/1705.01217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01217v1)
- **Published**: 2017-05-03 01:26:24+00:00
- **Updated**: 2017-05-03 01:26:24+00:00
- **Authors**: Zheng Cao, Shujian Yu, Bing Ouyang, Fraser Dalgleish, Anni Vuorenkoski, Gabriel Alsenas, Jose Principe
- **Comment**: None
- **Journal**: None
- **Summary**: To analyze marine animals behavior, seasonal distribution and abundance, digital imagery can be acquired by visual or Lidar camera. Depending on the quantity and properties of acquired imagery, the animals are characterized as either features (shape, color, texture, etc.), or dissimilarity matrices derived from different shape analysis methods (shape context, internal distance shape context, etc.). For both cases, multi-view learning is critical in integrating more than one set of feature or dissimilarity matrix for higher classification accuracy. This paper adopts correntropy loss as cost function in multi-view learning, which has favorable statistical properties for rejecting noise. For the case of features, the correntropy loss-based multi-view learning and its entrywise variation are developed based on the multi-view intact space learning algorithm. For the case of dissimilarity matrices, the robust Euclidean embedding algorithm is extended to its multi-view form with the correntropy loss function. Results from simulated data and real-world marine animal imagery show that the proposed algorithms can effectively enhance classification rate, as well as suppress noise under different noise conditions.



### Unsupervised Part-based Weighting Aggregation of Deep Convolutional Features for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1705.01247v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01247v3)
- **Published**: 2017-05-03 03:54:02+00:00
- **Updated**: 2017-11-29 08:53:36+00:00
- **Authors**: Jian Xu, Cunzhao Shi, Chengzuo Qi, Chunheng Wang, Baihua Xiao
- **Comment**: 8 pages, 4 figures. Accepted by AAAI2018
- **Journal**: None
- **Summary**: In this paper, we propose a simple but effective semantic part-based weighting aggregation (PWA) for image retrieval. The proposed PWA utilizes the discriminative filters of deep convolutional layers as part detectors. Moreover, we propose the effective unsupervised strategy to select some part detectors to generate the "probabilistic proposals", which highlight certain discriminative parts of objects and suppress the noise of background. The final global PWA representation could then be acquired by aggregating the regional representations weighted by the selected "probabilistic proposals" corresponding to various semantic content. We conduct comprehensive experiments on four standard datasets and show that our unsupervised PWA outperforms the state-of-the-art unsupervised and supervised aggregation methods. Code is available at https://github.com/XJhaoren/PWA.



### The Forgettable-Watcher Model for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1705.01253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1705.01253v1)
- **Published**: 2017-05-03 04:46:33+00:00
- **Updated**: 2017-05-03 04:46:33+00:00
- **Authors**: Hongyang Xue, Zhou Zhao, Deng Cai
- **Comment**: None
- **Journal**: None
- **Summary**: A number of visual question answering approaches have been proposed recently, aiming at understanding the visual scenes by answering the natural language questions. While the image question answering has drawn significant attention, video question answering is largely unexplored.   Video-QA is different from Image-QA since the information and the events are scattered among multiple frames. In order to better utilize the temporal structure of the videos and the phrasal structures of the answers, we propose two mechanisms: the re-watching and the re-reading mechanisms and combine them into the forgettable-watcher model. Then we propose a TGIF-QA dataset for video question answering with the help of automatic question generation. Finally, we evaluate the models on our dataset. The experimental results show the effectiveness of our proposed models.



### Super-Resolution of Wavelet-Encoded Images
- **Arxiv ID**: http://arxiv.org/abs/1705.01258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01258v1)
- **Published**: 2017-05-03 05:42:14+00:00
- **Updated**: 2017-05-03 05:42:14+00:00
- **Authors**: Vildan Atalay Aydin, Hassan Foroosh
- **Comment**: None
- **Journal**: None
- **Summary**: Multiview super-resolution image reconstruction (SRIR) is often cast as a resampling problem by merging non-redundant data from multiple low-resolution (LR) images on a finer high-resolution (HR) grid, while inverting the effect of the camera point spread function (PSF). One main problem with multiview methods is that resampling from nonuniform samples (provided by LR images) and the inversion of the PSF are highly nonlinear and ill-posed problems. Non-linearity and ill-posedness are typically overcome by linearization and regularization, often through an iterative optimization process, which essentially trade off the very same information (i.e. high frequency) that we want to recover. We propose a novel point of view for multiview SRIR: Unlike existing multiview methods that reconstruct the entire spectrum of the HR image from the multiple given LR images, we derive explicit expressions that show how the high-frequency spectra of the unknown HR image are related to the spectra of the LR images. Therefore, by taking any of the LR images as the reference to represent the low-frequency spectra of the HR image, one can reconstruct the super-resolution image by focusing only on the reconstruction of the high-frequency spectra. This is very much like single-image methods, which extrapolate the spectrum of one image, except that we rely on information provided by all other views, rather than by prior constraints as in single-image methods (which may not be an accurate source of information). This is made possible by deriving and applying explicit closed-form expressions that define how the local high frequency information that we aim to recover for the reference high resolution image is related to the local low frequency information in the sequence of views. Results and comparisons with recently published state-of-the-art methods show the superiority of the proposed solution.



### Learning to segment with image-level supervision
- **Arxiv ID**: http://arxiv.org/abs/1705.01262v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01262v2)
- **Published**: 2017-05-03 06:02:32+00:00
- **Updated**: 2019-02-04 17:56:16+00:00
- **Authors**: Gaurav Pandey, Ambedkar Dukkipati
- **Comment**: Published in WACV 2019
- **Journal**: None
- **Summary**: Deep convolutional networks have achieved the state-of-the-art for semantic image segmentation tasks. However, training these networks requires access to densely labeled images, which are known to be very expensive to obtain. On the other hand, the web provides an almost unlimited source of images annotated at the image level. How can one utilize this much larger weakly annotated set for tasks that require dense labeling? Prior work often relied on localization cues, such as saliency maps, objectness priors, bounding boxes etc., to address this challenging problem. In this paper, we propose a model that generates auxiliary labels for each image, while simultaneously forcing the output of the CNN to satisfy the mean-field constraints imposed by a conditional random field. We show that one can enforce the CRF constraints by forcing the distribution at each pixel to be close to the distribution of its neighbors. This is in stark contrast with methods that compute a recursive expansion of the mean-field distribution using a recurrent architecture and train the resultant distribution. Instead, the proposed model adds an extra loss term to the output of the CNN, and hence, is faster than recursive implementations. We achieve the state-of-the-art for weakly supervised semantic image segmentation on VOC 2012 dataset, assuming no manually labeled pixel level information is available. Furthermore, the incorporation of conditional random fields in CNN incurs little extra time during training.



### Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation
- **Arxiv ID**: http://arxiv.org/abs/1705.01314v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01314v4)
- **Published**: 2017-05-03 09:04:21+00:00
- **Updated**: 2018-05-01 09:00:52+00:00
- **Authors**: Yen-Cheng Liu, Yu-Ying Yeh, Tzu-Chien Fu, Sheng-De Wang, Wei-Chen Chiu, Yu-Chiang Frank Wang
- **Comment**: CVPR 2018 Spotlight
- **Journal**: None
- **Summary**: While representation learning aims to derive interpretable features for describing visual data, representation disentanglement further results in such features so that particular image attributes can be identified and manipulated. However, one cannot easily address this task without observing ground truth annotation for the training data. To address this problem, we propose a novel deep learning model of Cross-Domain Representation Disentangler (CDRD). By observing fully annotated source-domain data and unlabeled target-domain data of interest, our model bridges the information across data domains and transfers the attribute information accordingly. Thus, cross-domain joint feature disentanglement and adaptation can be jointly performed. In the experiments, we provide qualitative results to verify our disentanglement capability. Moreover, we further confirm that our model can be applied for solving classification tasks of unsupervised domain adaptation, and performs favorably against state-of-the-art image disentanglement and translation methods.



### Optical Flow in Mostly Rigid Scenes
- **Arxiv ID**: http://arxiv.org/abs/1705.01352v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01352v1)
- **Published**: 2017-05-03 10:48:21+00:00
- **Updated**: 2017-05-03 10:48:21+00:00
- **Authors**: Jonas Wulff, Laura Sevilla-Lara, Michael J. Black
- **Comment**: 15 pages, 10 figures; accepted for publication at CVPR 2017
- **Journal**: None
- **Summary**: The optical flow of natural scenes is a combination of the motion of the observer and the independent motion of objects. Existing algorithms typically focus on either recovering motion and structure under the assumption of a purely static world or optical flow for general unconstrained scenes. We combine these approaches in an optical flow algorithm that estimates an explicit segmentation of moving objects from appearance and physical constraints. In static regions we take advantage of strong constraints to jointly estimate the camera motion and the 3D structure of the scene over multiple frames. This allows us to also regularize the structure instead of the motion. Our formulation uses a Plane+Parallax framework, which works even under small baselines, and reduces the motion estimation to a one-dimensional search problem, resulting in more accurate estimation. In moving regions the flow is treated as unconstrained, and computed with an existing optical flow method. The resulting Mostly-Rigid Flow (MR-Flow) method achieves state-of-the-art results on both the MPI-Sintel and KITTI-2015 benchmarks.



### FOIL it! Find One mismatch between Image and Language caption
- **Arxiv ID**: http://arxiv.org/abs/1705.01359v1
- **DOI**: 10.18653/v1/P17-1024
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1705.01359v1)
- **Published**: 2017-05-03 11:07:13+00:00
- **Updated**: 2017-05-03 11:07:13+00:00
- **Authors**: Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurelie Herbelot, Moin Nabi, Enver Sangineto, Raffaella Bernardi
- **Comment**: To appear at ACL 2017
- **Journal**: None
- **Summary**: In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MSCOCO dataset, FOIL-COCO, which associates images with both correct and "foil" captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake ("foil word"). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image.



### Rotation Averaging and Strong Duality
- **Arxiv ID**: http://arxiv.org/abs/1705.01362v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01362v2)
- **Published**: 2017-05-03 11:17:18+00:00
- **Updated**: 2017-11-29 00:49:18+00:00
- **Authors**: Anders Eriksson, Carl Olsson, Fredrik Kahl, Tat-Jun Chin
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we explore the role of duality principles within the problem of rotation averaging, a fundamental task in a wide range of computer vision applications. In its conventional form, rotation averaging is stated as a minimization over multiple rotation constraints. As these constraints are non-convex, this problem is generally considered challenging to solve globally. We show how to circumvent this difficulty through the use of Lagrangian duality. While such an approach is well-known it is normally not guaranteed to provide a tight relaxation. Based on spectral graph theory, we analytically prove that in many cases there is no duality gap unless the noise levels are severe. This allows us to obtain certifiably global solutions to a class of important non-convex problems in polynomial time.   We also propose an efficient, scalable algorithm that out-performs general purpose numerical solvers and is able to handle the large problem instances commonly occurring in structure from motion settings. The potential of this proposed method is demonstrated on a number of different problems, consisting of both synthetic and real-world data.



### Weakly-supervised Visual Grounding of Phrases with Linguistic Structures
- **Arxiv ID**: http://arxiv.org/abs/1705.01371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01371v1)
- **Published**: 2017-05-03 11:53:33+00:00
- **Updated**: 2017-05-03 11:53:33+00:00
- **Authors**: Fanyi Xiao, Leonid Sigal, Yong Jae Lee
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: We propose a weakly-supervised approach that takes image-sentence pairs as input and learns to visually ground (i.e., localize) arbitrary linguistic phrases, in the form of spatial attention masks. Specifically, the model is trained with images and their associated image-level captions, without any explicit region-to-phrase correspondence annotations. To this end, we introduce an end-to-end model which learns visual groundings of phrases with two types of carefully designed loss functions. In addition to the standard discriminative loss, which enforces that attended image regions and phrases are consistently encoded, we propose a novel structural loss which makes use of the parse tree structures induced by the sentences. In particular, we ensure complementarity among the attention masks that correspond to sibling noun phrases, and compositionality of attention masks among the children and parent phrases, as defined by the sentence parse tree. We validate the effectiveness of our approach on the Microsoft COCO and Visual Genome datasets.



### Learning to Estimate 3D Hand Pose from Single RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1705.01389v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01389v3)
- **Published**: 2017-05-03 12:50:18+00:00
- **Updated**: 2017-10-15 15:52:51+00:00
- **Authors**: Christian Zimmermann, Thomas Brox
- **Comment**: Accepted to ICCV 2017. Code and dataset is released:
  https://lmb.informatik.uni-freiburg.de/projects/hand3d/
- **Journal**: None
- **Summary**: Low-cost consumer depth cameras and deep learning have enabled reasonable 3D hand pose estimation from single depth images. In this paper, we present an approach that estimates 3D hand pose from regular RGB images. This task has far more ambiguities due to the missing depth information. To this end, we propose a deep network that learns a network-implicit 3D articulation prior. Together with detected keypoints in the images, this network yields good estimates of the 3D pose. We introduce a large scale 3D hand pose dataset based on synthetic hand models for training the involved networks. Experiments on a variety of test sets, including one on sign language recognition, demonstrate the feasibility of 3D hand pose estimation on single color images.



### Gabor Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.01450v4
- **DOI**: 10.1109/TIP.2018.2835143
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01450v4)
- **Published**: 2017-05-03 14:37:55+00:00
- **Updated**: 2023-03-29 03:27:18+00:00
- **Authors**: Shangzhen Luan, Baochang Zhang, Chen Chen, Xianbin Cao, Jungong Han, Jianzhuang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Steerable properties dominate the design of traditional filters, e.g., Gabor filters, and endow features the capability of dealing with spatial transformations. However, such excellent properties have not been well explored in the popular deep convolutional neural networks (DCNNs). In this paper, we propose a new deep model, termed Gabor Convolutional Networks (GCNs or Gabor CNNs), which incorporates Gabor filters into DCNNs to enhance the resistance of deep learned features to the orientation and scale changes. By only manipulating the basic element of DCNNs based on Gabor filters, i.e., the convolution operator, GCNs can be easily implemented and are compatible with any popular deep learning architecture. Experimental results demonstrate the super capability of our algorithm in recognizing objects, where the scale and rotation changes occur frequently. The proposed GCNs have much fewer learnable network parameters, and thus is easier to train with an end-to-end pipeline.



### Toward Open-Set Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1705.01567v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01567v2)
- **Published**: 2017-05-03 18:10:09+00:00
- **Updated**: 2017-05-19 00:24:43+00:00
- **Authors**: Manuel Günther, Steve Cruz, Ethan M. Rudd, Terrance E. Boult
- **Comment**: Accepted for Publication in CVPR 2017 Biometrics Workshop
- **Journal**: None
- **Summary**: Much research has been conducted on both face identification and face verification, with greater focus on the latter. Research on face identification has mostly focused on using closed-set protocols, which assume that all probe images used in evaluation contain identities of subjects that are enrolled in the gallery. Real systems, however, where only a fraction of probe sample identities are enrolled in the gallery, cannot make this closed-set assumption. Instead, they must assume an open set of probe samples and be able to reject/ignore those that correspond to unknown identities. In this paper, we address the widespread misconception that thresholding verification-like scores is a good way to solve the open-set face identification problem, by formulating an open-set face identification protocol and evaluating different strategies for assessing similarity. Our open-set identification protocol is based on the canonical labeled faces in the wild (LFW) dataset. Additionally to the known identities, we introduce the concepts of known unknowns (known, but uninteresting persons) and unknown unknowns (people never seen before) to the biometric community. We compare three algorithms for assessing similarity in a deep feature space under an open-set protocol: thresholded verification-like scores, linear discriminant analysis (LDA) scores, and an extreme value machine (EVM) probabilities. Our findings suggest that thresholding EVM probabilities, which are open-set by design, outperforms thresholding verification-like scores.



### VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera
- **Arxiv ID**: http://arxiv.org/abs/1705.01583v1
- **DOI**: 10.1145/3072959.3073596
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1705.01583v1)
- **Published**: 2017-05-03 19:13:23+00:00
- **Updated**: 2017-05-03 19:13:23+00:00
- **Authors**: Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko, Helge Rhodin, Mohammad Shafiei, Hans-Peter Seidel, Weipeng Xu, Dan Casas, Christian Theobalt
- **Comment**: Accepted to SIGGRAPH 2017
- **Journal**: None
- **Summary**: We present the first real-time method to capture the full global 3D skeletal pose of a human in a stable, temporally consistent manner using a single RGB camera. Our method combines a new convolutional neural network (CNN) based pose regressor with kinematic skeleton fitting. Our novel fully-convolutional pose formulation regresses 2D and 3D joint positions jointly in real time and does not require tightly cropped input frames. A real-time kinematic skeleton fitting method uses the CNN output to yield temporally stable 3D global pose reconstructions on the basis of a coherent kinematic skeleton. This makes our approach the first monocular RGB method usable in real-time applications such as 3D character control---thus far, the only monocular methods for such applications employed specialized RGB-D cameras. Our method's accuracy is quantitatively on par with the best offline 3D monocular RGB pose estimation methods. Our results are qualitatively comparable to, and sometimes better than, results from monocular RGB-D approaches, such as the Kinect. However, we show that our approach is more broadly applicable than RGB-D solutions, i.e. it works for outdoor scenes, community videos, and low quality commodity RGB cameras.



