# Arxiv Papers in cs.CV on 2017-05-28
### Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1705.09882v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09882v2)
- **Published**: 2017-05-28 01:22:38+00:00
- **Updated**: 2018-12-27 01:06:54+00:00
- **Authors**: Nikolaos Karianakis, Zicheng Liu, Yinpeng Chen, Stefano Soatto
- **Comment**: 19 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: We address the problem of person re-identification from commodity depth sensors. One challenge for depth-based recognition is data scarcity. Our first contribution addresses this problem by introducing split-rate RGB-to-Depth transfer, which leverages large RGB datasets more effectively than popular fine-tuning approaches. Our transfer scheme is based on the observation that the model parameters at the bottom layers of a deep convolutional neural network can be directly shared between RGB and depth data while the remaining layers need to be fine-tuned rapidly. Our second contribution enhances re-identification for video by implementing temporal attention as a Bernoulli-Sigmoid unit acting upon frame-level features. Since this unit is stochastic, the temporal attention parameters are trained using reinforcement learning. Extensive experiments validate the accuracy of our method in person re-identification from depth sequences. Finally, in a scenario where subjects wear unseen clothes, we show large performance gains compared to a state-of-the-art model which relies on RGB data.



### Vocabulary-informed Extreme Value Learning
- **Arxiv ID**: http://arxiv.org/abs/1705.09887v2
- **DOI**: None
- **Categories**: **cs.CV**, math.ST, stat.ML, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1705.09887v2)
- **Published**: 2017-05-28 02:13:06+00:00
- **Updated**: 2018-01-26 23:27:27+00:00
- **Authors**: Yanwei Fu, HanZe Dong, Yu-feng Ma, Zhengjun Zhang, Xiangyang Xue
- **Comment**: we significantly change the content of this paper which makes it
  another paper. In order not to misleading, we decided to withdraw it
- **Journal**: None
- **Summary**: The novel unseen classes can be formulated as the extreme values of known classes. This inspired the recent works on open-set recognition \cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no way of naming the novel unseen classes. To solve this problem, we propose the Extreme Value Learning (EVL) formulation to learn the mapping from visual feature to semantic space. To model the margin and coverage distributions of each class, the Vocabulary-informed Learning (ViL) is adopted by using vast open vocabulary in the semantic space. Essentially, by incorporating the EVL and ViL, we for the first time propose a novel semantic embedding paradigm -- Vocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual features into semantic space in a probabilistic way. The learned embedding can be directly used to solve supervised learning, zero-shot and open set recognition simultaneously. Experiments on two benchmark datasets demonstrate the effectiveness of proposed frameworks.



### Cross-modal Subspace Learning for Fine-grained Sketch-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1705.09888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09888v1)
- **Published**: 2017-05-28 03:45:26+00:00
- **Updated**: 2017-05-28 03:45:26+00:00
- **Authors**: Peng Xu, Qiyue Yin, Yongye Huang, Yi-Zhe Song, Zhanyu Ma, Liang Wang, Tao Xiang, W. Bastiaan Kleijn, Jun Guo
- **Comment**: Accepted by Neurocomputing
- **Journal**: None
- **Summary**: Sketch-based image retrieval (SBIR) is challenging due to the inherent domain-gap between sketch and photo. Compared with pixel-perfect depictions of photos, sketches are iconic renderings of the real world with highly abstract. Therefore, matching sketch and photo directly using low-level visual clues are unsufficient, since a common low-level subspace that traverses semantically across the two modalities is non-trivial to establish. Most existing SBIR studies do not directly tackle this cross-modal problem. This naturally motivates us to explore the effectiveness of cross-modal retrieval methods in SBIR, which have been applied in the image-text matching successfully. In this paper, we introduce and compare a series of state-of-the-art cross-modal subspace learning methods and benchmark them on two recently released fine-grained SBIR datasets. Through thorough examination of the experimental results, we have demonstrated that the subspace learning can effectively model the sketch-photo domain-gap. In addition we draw a few key insights to drive future research.



### Care about you: towards large-scale human-centric visual relationship detection
- **Arxiv ID**: http://arxiv.org/abs/1705.09892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09892v1)
- **Published**: 2017-05-28 05:53:38+00:00
- **Updated**: 2017-05-28 05:53:38+00:00
- **Authors**: Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: Visual relationship detection aims to capture interactions between pairs of objects in images. Relationships between objects and humans represent a particularly important subset of this problem, with implications for challenges such as understanding human behaviour, and identifying affordances, amongst others. In addressing this problem we first construct a large-scale human-centric visual relationship detection dataset (HCVRD), which provides many more types of relationship annotation (nearly 10K categories) than the previous released datasets.   This large label space better reflects the reality of human-object interactions, but gives rise to a long-tail distribution problem, which in turn demands a zero-shot approach to labels appearing only in the test set. This is the first time this issue has been addressed. We propose a webly-supervised approach to these problems and demonstrate that the proposed model provides a strong baseline on our HCVRD dataset.



### Continuous Video to Simple Signals for Swimming Stroke Detection with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.09894v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09894v1)
- **Published**: 2017-05-28 06:14:06+00:00
- **Updated**: 2017-05-28 06:14:06+00:00
- **Authors**: Brandon Victor, Zhen He, Stuart Morgan, Dino Miniutti
- **Comment**: None
- **Journal**: None
- **Summary**: In many sports, it is useful to analyse video of an athlete in competition for training purposes. In swimming, stroke rate is a common metric used by coaches; requiring a laborious labelling of each individual stroke. We show that using a Convolutional Neural Network (CNN) we can automatically detect discrete events in continuous video (in this case, swimming strokes). We create a CNN that learns a mapping from a window of frames to a point on a smooth 1D target signal, with peaks denoting the location of a stroke, evaluated as a sliding window. To our knowledge this process of training and utilizing a CNN has not been investigated before; either in sports or fundamental computer vision research. Most research has been focused on action recognition and using it to classify many clips in continuous video for action localisation.   In this paper we demonstrate our process works well on the task of detecting swimming strokes in the wild. However, without modifying the model architecture or training method, the process is also shown to work equally well on detecting tennis strokes, implying that this is a general process.   The outputs of our system are surprisingly smooth signals that predict an arbitrary event at least as accurately as humans (manually evaluated from a sample of negative results). A number of different architectures are evaluated, pertaining to slightly different problem formulations and signal targets.



### Multiband NFC for High-Throughput Wireless Computer Vision Sensor Network
- **Arxiv ID**: http://arxiv.org/abs/1707.03720v1
- **DOI**: None
- **Categories**: **cs.NI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.03720v1)
- **Published**: 2017-05-28 06:43:29+00:00
- **Updated**: 2017-05-28 06:43:29+00:00
- **Authors**: F. Li, J. Du
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput.



### Multi-channel Weighted Nuclear Norm Minimization for Real Color Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1705.09912v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09912v2)
- **Published**: 2017-05-28 08:51:39+00:00
- **Updated**: 2018-12-18 17:34:47+00:00
- **Authors**: Jun Xu, Lei Zhang, David Zhang, Xiangchu Feng
- **Comment**: 9 pages, ICCV 2017
- **Journal**: None
- **Summary**: Most of the existing denoising algorithms are developed for grayscale images, while it is not a trivial work to extend them for color image denoising because the noise statistics in R, G, B channels can be very different for real noisy images. In this paper, we propose a multi-channel (MC) optimization model for real color image denoising under the weighted nuclear norm minimization (WNNM) framework. We concatenate the RGB patches to make use of the channel redundancy, and introduce a weight matrix to balance the data fidelity of the three channels in consideration of their different noise statistics. The proposed MC-WNNM model does not have an analytical solution. We reformulate it into a linear equality-constrained problem and solve it with the alternating direction method of multipliers. Each alternative updating step has closed-form solution and the convergence can be guaranteed. Extensive experiments on both synthetic and real noisy image datasets demonstrate the superiority of the proposed MC-WNNM over state-of-the-art denoising methods.



### Dilated Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.09914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09914v1)
- **Published**: 2017-05-28 09:01:44+00:00
- **Updated**: 2017-05-28 09:01:44+00:00
- **Authors**: Fisher Yu, Vladlen Koltun, Thomas Funkhouser
- **Comment**: Published at the Conference on Computer Vision and Pattern
  Recognition (CVPR 2017)
- **Journal**: None
- **Summary**: Convolutional networks for image classification progressively reduce resolution until the image is represented by tiny feature maps in which the spatial structure of the scene is no longer discernible. Such loss of spatial acuity can limit image classification accuracy and complicate the transfer of the model to downstream applications that require detailed scene understanding. These problems can be alleviated by dilation, which increases the resolution of output feature maps without reducing the receptive field of individual neurons. We show that dilated residual networks (DRNs) outperform their non-dilated counterparts in image classification without increasing the model's depth or complexity. We then study gridding artifacts introduced by dilation, develop an approach to removing these artifacts (`degridding'), and show that this further increases the performance of DRNs. In addition, we show that the accuracy advantage of DRNs is further magnified in downstream applications such as object localization and semantic segmentation.



### L1-norm Error Function Robustness and Outlier Regularization
- **Arxiv ID**: http://arxiv.org/abs/1705.09954v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09954v1)
- **Published**: 2017-05-28 15:53:25+00:00
- **Updated**: 2017-05-28 15:53:25+00:00
- **Authors**: Chris Ding, Bo Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: In many real-world applications, data come with corruptions, large errors or outliers. One popular approach is to use L1-norm function. However, the robustness of L1-norm function is not well understood so far. In this paper, we present a new outlier regularization framework to understand and analyze the robustness of L1-norm function. There are two main features for the proposed outlier regularization. (1) A key property of outlier regularization is that how far an outlier lies away from its theoretically predicted value does not affect the final regularization and analysis results. (2) Another important feature of outlier regularization is that it has an equivalent continuous representation that closely relates to L1 function. This provides a new way to understand and analyze the robustness of L1 function. We apply our outlier regularization framework to PCA and propose an outlier regularized PCA (ORPCA) model. Comparing to the trace-normbased robust PCA, ORPCA has several benefits: (1) It does not suffer singular value suppression. (2) It can retain small high rank components which help retain fine details of data. (3) ORPCA can be computed more efficiently.



### Attribute-Guided Face Generation Using Conditional CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/1705.09966v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.09966v2)
- **Published**: 2017-05-28 17:37:23+00:00
- **Updated**: 2018-11-14 13:35:49+00:00
- **Authors**: Yongyi Lu, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: We are interested in attribute-guided face generation: given a low-res face input image, an attribute vector that can be extracted from a high-res image (attribute image), our new method generates a high-res face image for the low-res input that satisfies the given attributes. To address this problem, we condition the CycleGAN and propose conditional CycleGAN, which is designed to 1) handle unpaired training data because the training low/high-res and high-res attribute images may not necessarily align with each other, and to 2) allow easy control of the appearance of the generated face via the input attributes. We demonstrate impressive results on the attribute-guided conditional CycleGAN, which can synthesize realistic face images with appearance easily controlled by user-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using the attribute image as identity to produce the corresponding conditional vector and by incorporating a face verification network, the attribute-guided network becomes the identity-guided conditional CycleGAN which produces impressive and interesting results on identity transfer. We demonstrate three applications on identity-guided conditional CycleGAN: identity-preserving face superresolution, face swapping, and frontal face generation, which consistently show the advantage of our new method.



### LAP: a Linearize and Project Method for Solving Inverse Problems with Coupled Variables
- **Arxiv ID**: http://arxiv.org/abs/1705.09992v3
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, math.OC, 65F10, 65F22, 65M32
- **Links**: [PDF](http://arxiv.org/pdf/1705.09992v3)
- **Published**: 2017-05-28 21:11:53+00:00
- **Updated**: 2018-06-14 14:35:48+00:00
- **Authors**: James Herring, James Nagy, Lars Ruthotto
- **Comment**: 21 pages, 6 figures, 3 tables
- **Journal**: STSIP 17.2 (2018) pp.127-151
- **Summary**: Many inverse problems involve two or more sets of variables that represent different physical quantities but are tightly coupled with each other. For example, image super-resolution requires joint estimation of the image and motion parameters from noisy measurements. Exploiting this structure is key for efficiently solving these large-scale optimization problems, which are often ill-conditioned.   In this paper, we present a new method called Linearize And Project (LAP) that offers a flexible framework for solving inverse problems with coupled variables. LAP is most promising for cases when the subproblem corresponding to one of the variables is considerably easier to solve than the other. LAP is based on a Gauss-Newton method, and thus after linearizing the residual, it eliminates one block of variables through projection. Due to the linearization, this block can be chosen freely. Further, LAP supports direct, iterative, and hybrid regularization as well as constraints. Therefore LAP is attractive, e.g., for ill-posed imaging problems. These traits differentiate LAP from common alternatives for this type of problem such as variable projection (VarPro) and block coordinate descent (BCD). Our numerical experiments compare the performance of LAP to BCD and VarPro using three coupled problems whose forward operators are linear with respect to one block and nonlinear for the other set of variables.



### Robust Online Matrix Factorization for Dynamic Background Subtraction
- **Arxiv ID**: http://arxiv.org/abs/1705.10000v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10000v1)
- **Published**: 2017-05-28 23:09:29+00:00
- **Updated**: 2017-05-28 23:09:29+00:00
- **Authors**: Hongwei Yong, Deyu Meng, Wangmeng Zuo, Lei Zhang
- **Comment**: 14 pages, 13 figures
- **Journal**: None
- **Summary**: We propose an effective online background subtraction method, which can be robustly applied to practical videos that have variations in both foreground and background. Different from previous methods which often model the foreground as Gaussian or Laplacian distributions, we model the foreground for each frame with a specific mixture of Gaussians (MoG) distribution, which is updated online frame by frame. Particularly, our MoG model in each frame is regularized by the learned foreground/background knowledge in previous frames. This makes our online MoG model highly robust, stable and adaptive to practical foreground and background variations. The proposed model can be formulated as a concise probabilistic MAP model, which can be readily solved by EM algorithm. We further embed an affine transformation operator into the proposed model, which can be automatically adjusted to fit a wide range of video background transformations and make the method more robust to camera movements. With using the sub-sampling technique, the proposed method can be accelerated to execute more than 250 frames per second on average, meeting the requirement of real-time background subtraction for practical video processing tasks. The superiority of the proposed method is substantiated by extensive experiments implemented on synthetic and real videos, as compared with state-of-the-art online and offline background subtraction methods.



