# Arxiv Papers in cs.CV on 2017-05-04
### Generative Convolutional Networks for Latent Fingerprint Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1705.01707v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.01707v1)
- **Published**: 2017-05-04 05:29:23+00:00
- **Updated**: 2017-05-04 05:29:23+00:00
- **Authors**: Jan Svoboda, Federico Monti, Michael M. Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: Performance of fingerprint recognition depends heavily on the extraction of minutiae points. Enhancement of the fingerprint ridge pattern is thus an essential pre-processing step that noticeably reduces false positive and negative detection rates. A particularly challenging setting is when the fingerprint images are corrupted or partially missing. In this work, we apply generative convolutional networks to denoise visible minutiae and predict the missing parts of the ridge pattern. The proposed enhancement approach is tested as a pre-processing step in combination with several standard feature extraction methods such as MINDTCT, followed by biometric comparison using MCC and BOZORTH3. We evaluate our method on several publicly available latent fingerprint datasets captured using different sensors.



### Image-based immersed boundary model of the aortic root
- **Arxiv ID**: http://arxiv.org/abs/1705.04279v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CE, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.04279v1)
- **Published**: 2017-05-04 06:34:16+00:00
- **Updated**: 2017-05-04 06:34:16+00:00
- **Authors**: Ali Hasan, Ebrahim M. Kolahdouz, Andinet Enquobahrie, Thomas G. Caranasos, John P. Vavalle, Boyce E. Griffith
- **Comment**: None
- **Journal**: None
- **Summary**: Each year, approximately 300,000 heart valve repair or replacement procedures are performed worldwide, including approximately 70,000 aortic valve replacement surgeries in the United States alone. This paper describes progress in constructing anatomically and physiologically realistic immersed boundary (IB) models of the dynamics of the aortic root and ascending aorta. This work builds on earlier IB models of fluid-structure interaction (FSI) in the aortic root, which previously achieved realistic hemodynamics over multiple cardiac cycles, but which also were limited to simplified aortic geometries and idealized descriptions of the biomechanics of the aortic valve cusps. By contrast, the model described herein uses an anatomical geometry reconstructed from patient-specific computed tomography angiography (CTA) data, and employs a description of the elasticity of the aortic valve leaflets based on a fiber-reinforced constitutive model fit to experimental tensile test data. Numerical tests show that the model is able to resolve the leaflet biomechanics in diastole and early systole at practical grid spacings. The model is also used to examine differences in the mechanics and fluid dynamics yielded by fresh valve leaflets and glutaraldehyde-fixed leaflets similar to those used in bioprosthetic heart valves. Although there are large differences in the leaflet deformations during diastole, the differences in the open configurations of the valve models are relatively small, and nearly identical hemodynamics are obtained in all cases considered.



### Attributes2Classname: A discriminative model for attribute-based unsupervised zero-shot learning
- **Arxiv ID**: http://arxiv.org/abs/1705.01734v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01734v2)
- **Published**: 2017-05-04 08:28:44+00:00
- **Updated**: 2017-08-05 21:51:45+00:00
- **Authors**: Berkan Demirel, Ramazan Gokberk Cinbis, Nazli Ikizler-Cinbis
- **Comment**: To appear at IEEE Int. Conference on Computer Vision (ICCV) 2017
- **Journal**: None
- **Summary**: We propose a novel approach for unsupervised zero-shot learning (ZSL) of classes based on their names. Most existing unsupervised ZSL methods aim to learn a model for directly comparing image features and class names. However, this proves to be a difficult task due to dominance of non-visual semantics in underlying vector-space embeddings of class names. To address this issue, we discriminatively learn a word representation such that the similarities between class and combination of attribute names fall in line with the visual similarity. Contrary to the traditional zero-shot learning approaches that are built upon attribute presence, our approach bypasses the laborious attribute-class relation annotations for unseen classes. In addition, our proposed approach renders text-only training possible, hence, the training can be augmented without the need to collect additional image data. The experimental results show that our method yields state-of-the-art results for unsupervised ZSL in three benchmark datasets.



### Deep 360 Pilot: Learning a Deep Agent for Piloting through 360Â° Sports Video
- **Arxiv ID**: http://arxiv.org/abs/1705.01759v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1705.01759v1)
- **Published**: 2017-05-04 09:26:58+00:00
- **Updated**: 2017-05-04 09:26:58+00:00
- **Authors**: Hou-Ning Hu, Yen-Chen Lin, Ming-Yu Liu, Hsien-Tzu Cheng, Yung-Ju Chang, Min Sun
- **Comment**: 13 pages, 8 figures, To appear in CVPR 2017 as an Oral paper. The
  first two authors contributed equally to this work.
  https://aliensunmin.github.io/project/360video/
- **Journal**: None
- **Summary**: Watching a 360{\deg} sports video requires a viewer to continuously select a viewing angle, either through a sequence of mouse clicks or head movements. To relieve the viewer from this "360 piloting" task, we propose "deep 360 pilot" -- a deep learning-based agent for piloting through 360{\deg} sports videos automatically. At each frame, the agent observes a panoramic image and has the knowledge of previously selected viewing angles. The task of the agent is to shift the current viewing angle (i.e. action) to the next preferred one (i.e., goal). We propose to directly learn an online policy of the agent from data. We use the policy gradient technique to jointly train our pipeline: by minimizing (1) a regression loss measuring the distance between the selected and ground truth viewing angles, (2) a smoothness loss encouraging smooth transition in viewing angle, and (3) maximizing an expected reward of focusing on a foreground object. To evaluate our method, we build a new 360-Sports video dataset consisting of five sports domains. We train domain-specific agents and achieve the best performance on viewing angle selection accuracy and transition smoothness compared to [51] and other baselines.



### Am I Done? Predicting Action Progress in Videos
- **Arxiv ID**: http://arxiv.org/abs/1705.01781v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01781v4)
- **Published**: 2017-05-04 10:28:21+00:00
- **Updated**: 2020-03-10 01:43:40+00:00
- **Authors**: Federico Becattini, Tiberio Uricchio, Lorenzo Seidenari, Lamberto Ballan, Alberto Del Bimbo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we deal with the problem of predicting action progress in videos. We argue that this is an extremely important task since it can be valuable for a wide range of interaction applications. To this end we introduce a novel approach, named ProgressNet, capable of predicting when an action takes place in a video, where it is located within the frames, and how far it has progressed during its execution. To provide a general definition of action progress, we ground our work in the linguistics literature, borrowing terms and concepts to understand which actions can be the subject of progress estimation. As a result, we define a categorization of actions and their phases. Motivated by the recent success obtained from the interaction of Convolutional and Recurrent Neural Networks, our model is based on a combination of the Faster R-CNN framework, to make frame-wise predictions, and LSTM networks, to estimate action progress through time. After introducing two evaluation protocols for the task at hand, we demonstrate the capability of our model to effectively predict action progress on the UCF-101 and J-HMDB datasets.



### From Zero-shot Learning to Conventional Supervised Classification: Unseen Visual Data Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1705.01782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01782v1)
- **Published**: 2017-05-04 10:28:37+00:00
- **Updated**: 2017-05-04 10:28:37+00:00
- **Authors**: Yang Long, Li Liu, Ling Shao, Fumin Shen, Guiguang Ding, Jungong Han
- **Comment**: None
- **Journal**: None
- **Summary**: Robust object recognition systems usually rely on powerful feature extraction mechanisms from a large number of real images. However, in many realistic applications, collecting sufficient images for ever-growing new classes is unattainable. In this paper, we propose a new Zero-shot learning (ZSL) framework that can synthesise visual features for unseen classes without acquiring real images. Using the proposed Unseen Visual Data Synthesis (UVDS) algorithm, semantic attributes are effectively utilised as an intermediate clue to synthesise unseen visual features at the training stage. Hereafter, ZSL recognition is converted into the conventional supervised problem, i.e. the synthesised visual features can be straightforwardly fed to typical classifiers such as SVM. On four benchmark datasets, we demonstrate the benefit of using synthesised unseen data. Extensive experimental results suggest that our proposed approach significantly improve the state-of-the-art results.



### Pixel Normalization from Numeric Data as Input to Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.01809v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1705.01809v1)
- **Published**: 2017-05-04 12:20:56+00:00
- **Updated**: 2017-05-04 12:20:56+00:00
- **Authors**: Parth Sane, Ravindra Agrawal
- **Comment**: IEEE WiSPNET 2017 conference in Chennai
- **Journal**: None
- **Summary**: Text to image transformation for input to neural networks requires intermediate steps. This paper attempts to present a new approach to pixel normalization so as to convert textual data into image, suitable as input for neural networks. This method can be further improved by its Graphics Processing Unit (GPU) implementation to provide significant speedup in computational time.



### Fast k-means based on KNN Graph
- **Arxiv ID**: http://arxiv.org/abs/1705.01813v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.01813v1)
- **Published**: 2017-05-04 12:27:28+00:00
- **Updated**: 2017-05-04 12:27:28+00:00
- **Authors**: Cheng-Hao Deng, Wan-Lei Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: In the era of big data, k-means clustering has been widely adopted as a basic processing tool in various contexts. However, its computational cost could be prohibitively high as the data size and the cluster number are large. It is well known that the processing bottleneck of k-means lies in the operation of seeking closest centroid in each iteration. In this paper, a novel solution towards the scalability issue of k-means is presented. In the proposal, k-means is supported by an approximate k-nearest neighbors graph. In the k-means iteration, each data sample is only compared to clusters that its nearest neighbors reside. Since the number of nearest neighbors we consider is much less than k, the processing cost in this step becomes minor and irrelevant to k. The processing bottleneck is therefore overcome. The most interesting thing is that k-nearest neighbor graph is constructed by iteratively calling the fast $k$-means itself. Comparing with existing fast k-means variants, the proposed algorithm achieves hundreds to thousands times speed-up while maintaining high clustering quality. As it is tested on 10 million 512-dimensional data, it takes only 5.2 hours to produce 1 million clusters. In contrast, to fulfill the same scale of clustering, it would take 3 years for traditional k-means.



### A Deep Learning Perspective on the Origin of Facial Expressions
- **Arxiv ID**: http://arxiv.org/abs/1705.01842v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01842v2)
- **Published**: 2017-05-04 13:59:07+00:00
- **Updated**: 2017-05-10 13:05:00+00:00
- **Authors**: Ran Breuer, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expressions play a significant role in human communication and behavior. Psychologists have long studied the relationship between facial expressions and emotions. Paul Ekman et al., devised the Facial Action Coding System (FACS) to taxonomize human facial expressions and model their behavior. The ability to recognize facial expressions automatically, enables novel applications in fields like human-computer interaction, social gaming, and psychological research. There has been a tremendously active research in this field, with several recent papers utilizing convolutional neural networks (CNN) for feature extraction and inference. In this paper, we employ CNN understanding methods to study the relation between the features these computational networks are using, the FACS and Action Units (AU). We verify our findings on the Extended Cohn-Kanade (CK+), NovaEmotions and FER2013 datasets. We apply these models to various tasks and tests using transfer learning, including cross-dataset validation and cross-task performance. Finally, we exploit the nature of the FER based CNN models for the detection of micro-expressions and achieve state-of-the-art accuracy using a simple long-short-term-memory (LSTM) recurrent neural network (RNN).



### Action Tubelet Detector for Spatio-Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1705.01861v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01861v3)
- **Published**: 2017-05-04 14:41:56+00:00
- **Updated**: 2017-08-21 13:54:34+00:00
- **Authors**: Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari, Cordelia Schmid
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Current state-of-the-art approaches for spatio-temporal action localization rely on detections at the frame level that are then linked or tracked across time. In this paper, we leverage the temporal continuity of videos instead of operating at the frame level. We propose the ACtion Tubelet detector (ACT-detector) that takes as input a sequence of frames and outputs tubelets, i.e., sequences of bounding boxes with associated scores. The same way state-of-the-art object detectors rely on anchor boxes, our ACT-detector is based on anchor cuboids. We build upon the SSD framework. Convolutional features are extracted for each frame, while scores and regressions are based on the temporal stacking of these features, thus exploiting information from a sequence. Our experimental results show that leveraging sequences of frames significantly improves detection performance over using individual frames. The gain of our tubelet detector can be explained by both more accurate scores and more precise localization. Our ACT-detector outperforms the state-of-the-art methods for frame-mAP and video-mAP on the J-HMDB and UCF-101 datasets, in particular at high overlap thresholds.



### Derivate-based Component-Trees for Multi-Channel Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1705.01906v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01906v2)
- **Published**: 2017-05-04 16:51:33+00:00
- **Updated**: 2018-04-19 07:41:32+00:00
- **Authors**: Tobias BÃ¶ttger, Dominik Gutermuth
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: We introduce the concept of derivate-based component-trees for images with an arbitrary number of channels. The approach is a natural extension of the classical component-tree devoted to gray-scale images. The similar structure enables the translation of many gray-level image processing techniques based on the component-tree to hyperspectral and color images. As an example application, we present an image segmentation approach that extracts Maximally Stable Homogeneous Regions (MSHR). The approach very similar to MSER but can be applied to images with an arbitrary number of channels. As opposed to MSER, our approach implicitly segments regions with are both lighter and darker than their background for gray-scale images and can be used in OCR applications where MSER will fail. We introduce a local flooding-based immersion for the derivate-based component-tree construction which is linear in the number of pixels. In the experiments, we show that the runtime scales favorably with an increasing number of channels and may improve algorithms which build on MSER.



### Auto-painter: Cartoon Image Generation from Sketch by Using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.01908v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9; I.4.8; I.3.3
- **Links**: [PDF](http://arxiv.org/pdf/1705.01908v2)
- **Published**: 2017-05-04 17:04:28+00:00
- **Updated**: 2017-05-07 03:40:05+00:00
- **Authors**: Yifan Liu, Zengchang Qin, Zhenbo Luo, Hua Wang
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Recently, realistic image generation using deep neural networks has become a hot topic in machine learning and computer vision. Images can be generated at the pixel level by learning from a large collection of images. Learning to generate colorful cartoon images from black-and-white sketches is not only an interesting research problem, but also a potential application in digital entertainment. In this paper, we investigate the sketch-to-image synthesis problem by using conditional generative adversarial networks (cGAN). We propose the auto-painter model which can automatically generate compatible colors for a sketch. The new model is not only capable of painting hand-draw sketch with proper colors, but also allowing users to indicate preferred colors. Experimental results on two sketch datasets show that the auto-painter performs better that existing image-to-image methods.



### Recurrent Soft Attention Model for Common Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1705.01921v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.01921v2)
- **Published**: 2017-05-04 17:27:42+00:00
- **Updated**: 2017-05-29 07:02:52+00:00
- **Authors**: Liliang Ren
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: We propose the Recurrent Soft Attention Model, which integrates the visual attention from the original image to a LSTM memory cell through a down-sample network. The model recurrently transmits visual attention to the memory cells for glimpse mask generation, which is a more natural way for attention integration and exploitation in general object detection and recognition problem. We test our model under the metric of the top-1 accuracy on the CIFAR-10 dataset. The experiment shows that our down-sample network and feedback mechanism plays an effective role among the whole network structure.



### Streaming Algorithm for Euler Characteristic Curves of Multidimensional Images
- **Arxiv ID**: http://arxiv.org/abs/1705.02045v3
- **DOI**: 10.1007/978-3-319-64689-3_32
- **Categories**: **cs.CV**, math.AT
- **Links**: [PDF](http://arxiv.org/pdf/1705.02045v3)
- **Published**: 2017-05-04 23:14:45+00:00
- **Updated**: 2018-10-17 14:36:57+00:00
- **Authors**: Teresa Heiss, Hubert Wagner
- **Comment**: None
- **Journal**: In: Computer Analysis of Images and Patterns. CAIP 2017. Lecture
  Notes in Computer Science, vol 10424. Springer International Publishing,
  Cham, 2017, pp. 397-409
- **Summary**: We present an efficient algorithm to compute Euler characteristic curves of gray scale images of arbitrary dimension. In various applications the Euler characteristic curve is used as a descriptor of an image.   Our algorithm is the first streaming algorithm for Euler characteristic curves. The usage of streaming removes the necessity to store the entire image in RAM. Experiments show that our implementation handles terabyte scale images on commodity hardware. Due to lock-free parallelism, it scales well with the number of processor cores. Our software---CHUNKYEuler---is available as open source on Bitbucket.   Additionally, we put the concept of the Euler characteristic curve in the wider context of computational topology. In particular, we explain the connection with persistence diagrams.



