# Arxiv Papers in cs.CV on 2017-05-18
### Fashion Forward: Forecasting Visual Style in Fashion
- **Arxiv ID**: http://arxiv.org/abs/1705.06394v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06394v3)
- **Published**: 2017-05-18 02:26:52+00:00
- **Updated**: 2020-08-09 03:06:15+00:00
- **Authors**: Ziad Al-Halah, Rainer Stiefelhagen, Kristen Grauman
- **Comment**: ICCV 2017. Project page:
  https://cvhci.anthropomatik.kit.edu/~zalhalah/prj_fashion_forecast.html
- **Journal**: None
- **Summary**: What is the future of fashion? Tackling this question from a data-driven vision perspective, we propose to forecast visual style trends before they occur. We introduce the first approach to predict the future popularity of styles discovered from fashion images in an unsupervised manner. Using these styles as a basis, we train a forecasting model to represent their trends over time. The resulting model can hypothesize new mixtures of styles that will become popular in the future, discover style dynamics (trendy vs. classic), and name the key visual attributes that will dominate tomorrow's fashion. We demonstrate our idea applied to three datasets encapsulating 80,000 fashion products sold across six years on Amazon. Results indicate that fashion forecasting benefits greatly from visual analysis, much more than textual or meta-data cues surrounding products.



### Probabilistic Combination of Noisy Points and Planes for RGB-D Odometry
- **Arxiv ID**: http://arxiv.org/abs/1705.06516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06516v1)
- **Published**: 2017-05-18 10:53:51+00:00
- **Updated**: 2017-05-18 10:53:51+00:00
- **Authors**: Pedro F. Proença, Yang Gao
- **Comment**: Accepted to TAROS 2017
- **Journal**: None
- **Summary**: This work proposes a visual odometry method that combines points and plane primitives, extracted from a noisy depth camera. Depth measurement uncertainty is modelled and propagated through the extraction of geometric primitives to the frame-to-frame motion estimation, where pose is optimized by weighting the residuals of 3D point and planes matches, according to their uncertainties. Results on an RGB-D dataset show that the combination of points and planes, through the proposed method, is able to perform well in poorly textured environments, where point-based odometry is bound to fail.



### A fully dense and globally consistent 3D map reconstruction approach for GI tract to enhance therapeutic relevance of the endoscopic capsule robot
- **Arxiv ID**: http://arxiv.org/abs/1705.06524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06524v1)
- **Published**: 2017-05-18 11:12:32+00:00
- **Updated**: 2017-05-18 11:12:32+00:00
- **Authors**: Mehmet Turan, Yusuf Yigit Pilavci, Redhwan Jamiruddin, Helder Araujo, Ender Konukoglu, Metin Sitti
- **Comment**: None
- **Journal**: None
- **Summary**: In the gastrointestinal (GI) tract endoscopy field, ingestible wireless capsule endoscopy is emerging as a novel, minimally invasive diagnostic technology for inspection of the GI tract and diagnosis of a wide range of diseases and pathologies. Since the development of this technology, medical device companies and many research groups have made substantial progress in converting passive capsule endoscopes to robotic active capsule endoscopes with most of the functionality of current active flexible endoscopes. However, robotic capsule endoscopy still has some challenges. In particular, the use of such devices to generate a precise three-dimensional (3D) mapping of the entire inner organ remains an unsolved problem. Such global 3D maps of inner organs would help doctors to detect the location and size of diseased areas more accurately and intuitively, thus permitting more reliable diagnoses. To our knowledge, this paper presents the first complete pipeline for a complete 3D visual map reconstruction of the stomach. The proposed pipeline is modular and includes a preprocessing module, an image registration module, and a final shape-from-shading-based 3D reconstruction module; the 3D map is primarily generated by a combination of image stitching and shape-from-shading techniques, and is updated in a frame-by-frame iterative fashion via capsule motion inside the stomach. A comprehensive quantitative analysis of the proposed 3D reconstruction method is performed using an esophagus gastro duodenoscopy simulator, three different endoscopic cameras, and a 3D optical scanner.



### Agent-Centric Risk Assessment: Accident Anticipation and Risky Region Localization
- **Arxiv ID**: http://arxiv.org/abs/1705.06560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06560v1)
- **Published**: 2017-05-18 12:56:20+00:00
- **Updated**: 2017-05-18 12:56:20+00:00
- **Authors**: Kuo-Hao Zeng, Shih-Han Chou, Fu-Hsiang Chan, Juan Carlos Niebles, Min Sun
- **Comment**: None
- **Journal**: None
- **Summary**: For survival, a living agent must have the ability to assess risk (1) by temporally anticipating accidents before they occur, and (2) by spatially localizing risky regions in the environment to move away from threats. In this paper, we take an agent-centric approach to study the accident anticipation and risky region localization tasks. We propose a novel soft-attention Recurrent Neural Network (RNN) which explicitly models both spatial and appearance-wise non-linear interaction between the agent triggering the event and another agent or static-region involved. In order to test our proposed method, we introduce the Epic Fail (EF) dataset consisting of 3000 viral videos capturing various accidents. In the experiments, we evaluate the risk assessment accuracy both in the temporal domain (accident anticipation) and spatial domain (risky region localization) on our EF dataset and the Street Accident (SA) dataset. Our method consistently outperforms other baselines on both datasets.



### Learning Texture Manifolds with the Periodic Spatial GAN
- **Arxiv ID**: http://arxiv.org/abs/1705.06566v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.06566v2)
- **Published**: 2017-05-18 13:09:45+00:00
- **Updated**: 2017-09-08 21:26:03+00:00
- **Authors**: Urs Bergmann, Nikolay Jetchev, Roland Vollgraf
- **Comment**: Proceedings of the 34th International Conference on Machine Learning,
  Sydney, Australia, 2017. JMLR: W&CP. Copyright 2017 by the author(s)
- **Journal**: None
- **Summary**: This paper introduces a novel approach to texture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the structure of the input noise distribution by constructing tensors with different types of dimensions. We call this technique Periodic Spatial GAN (PSGAN). The PSGAN has several novel abilities which surpass the current state of the art in texture synthesis. First, we can learn multiple textures from datasets of one or more complex large images. Second, we show that the image generation with PSGANs has properties of a texture manifold: we can smoothly interpolate between samples in the structured noise space and generate novel samples, which lie perceptually between the textures of the original dataset. In addition, we can also accurately learn periodical textures. We make multiple experiments which show that PSGANs can flexibly handle diverse texture and image data sources. Our method is highly scalable and it can generate output images of arbitrary large size.



### Accelerating Discrete Wavelet Transforms on GPUs
- **Arxiv ID**: http://arxiv.org/abs/1705.08266v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1705.08266v1)
- **Published**: 2017-05-18 14:42:19+00:00
- **Updated**: 2017-05-18 14:42:19+00:00
- **Authors**: David Barina, Michal Kula, Michal Matysek, Pavel Zemcik
- **Comment**: preprint submitted to ICIP 2017. arXiv admin note: substantial text
  overlap with arXiv:1704.08657
- **Journal**: None
- **Summary**: The two-dimensional discrete wavelet transform has a huge number of applications in image-processing techniques. Until now, several papers compared the performance of such transform on graphics processing units (GPUs). However, all of them only dealt with lifting and convolution computation schemes. In this paper, we show that corresponding horizontal and vertical lifting parts of the lifting scheme can be merged into non-separable lifting units, which halves the number of steps. We also discuss an optimization strategy leading to a reduction in the number of arithmetic operations. The schemes were assessed using the OpenCL and pixel shaders. The proposed non-separable lifting scheme outperforms the existing schemes in many cases, irrespective of its higher complexity.



### MUTAN: Multimodal Tucker Fusion for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1705.06676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06676v1)
- **Published**: 2017-05-18 16:23:22+00:00
- **Updated**: 2017-05-18 16:23:22+00:00
- **Authors**: Hedi Ben-younes, Rémi Cadene, Matthieu Cord, Nicolas Thome
- **Comment**: None
- **Journal**: None
- **Summary**: Bilinear models provide an appealing framework for mixing and merging information in Visual Question Answering (VQA) tasks. They help to learn high level associations between question meaning and visual concepts in the image, but they suffer from huge dimensionality issues. We introduce MUTAN, a multimodal tensor-based Tucker decomposition to efficiently parametrize bilinear interactions between visual and textual representations. Additionally to the Tucker framework, we design a low-rank matrix-based decomposition to explicitly constrain the interaction rank. With MUTAN, we control the complexity of the merging scheme while keeping nice interpretable fusion relations. We show how our MUTAN model generalizes some of the latest VQA architectures, providing state-of-the-art results.



### Target-Quality Image Compression with Recurrent, Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.06687v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06687v1)
- **Published**: 2017-05-18 16:44:31+00:00
- **Updated**: 2017-05-18 16:44:31+00:00
- **Authors**: Michele Covell, Nick Johnston, David Minnen, Sung Jin Hwang, Joel Shor, Saurabh Singh, Damien Vincent, George Toderici
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a stop-code tolerant (SCT) approach to training recurrent convolutional neural networks for lossy image compression. Our methods introduce a multi-pass training method to combine the training goals of high-quality reconstructions in areas around stop-code masking as well as in highly-detailed areas. These methods lead to lower true bitrates for a given recursion count, both pre- and post-entropy coding, even using unstructured LZ77 code compression. The pre-LZ77 gains are achieved by trimming stop codes. The post-LZ77 gains are due to the highly unequal distributions of 0/1 codes from the SCT architectures. With these code compressions, the SCT architecture maintains or exceeds the image quality at all compression rates compared to JPEG and to RNN auto-encoders across the Kodak dataset. In addition, the SCT coding results in lower variance in image quality across the extent of the image, a characteristic that has been shown to be important in human ratings of image quality



### Learning Spatiotemporal Features for Infrared Action Recognition with 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.06709v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1705.06709v1)
- **Published**: 2017-05-18 17:26:34+00:00
- **Updated**: 2017-05-18 17:26:34+00:00
- **Authors**: Zhuolin Jiang, Viktor Rozgic, Sancar Adali
- **Comment**: None
- **Journal**: None
- **Summary**: Infrared (IR) imaging has the potential to enable more robust action recognition systems compared to visible spectrum cameras due to lower sensitivity to lighting conditions and appearance variability. While the action recognition task on videos collected from visible spectrum imaging has received much attention, action recognition in IR videos is significantly less explored. Our objective is to exploit imaging data in this modality for the action recognition task. In this work, we propose a novel two-stream 3D convolutional neural network (CNN) architecture by introducing the discriminative code layer and the corresponding discriminative code loss function. The proposed network processes IR image and the IR-based optical flow field sequences. We pretrain the 3D CNN model on the visible spectrum Sports-1M action dataset and finetune it on the Infrared Action Recognition (InfAR) dataset. To our best knowledge, this is the first application of the 3D CNN to action recognition in the IR domain. We conduct an elaborate analysis of different fusion schemes (weighted average, single and double-layer neural nets) applied to different 3D CNN outputs. Experimental results demonstrate that our approach can achieve state-of-the-art average precision (AP) performances on the InfAR dataset: (1) the proposed two-stream 3D CNN achieves the best reported 77.5% AP, and (2) our 3D CNN model applied to the optical flow fields achieves the best reported single stream 75.42% AP.



### Model-based Catheter Segmentation in MRI-images
- **Arxiv ID**: http://arxiv.org/abs/1705.06712v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06712v2)
- **Published**: 2017-05-18 17:28:53+00:00
- **Updated**: 2020-12-10 07:55:13+00:00
- **Authors**: Andre Mastmeyer, Guillaume Pernelle, Lauren Barber, Steve Pieper, Dirk Fortmeier, Sandy Wells, Heinz Handels, Tina Kapur
- **Comment**: MICCAI 2015
- **Journal**: None
- **Summary**: Accurate and reliable segmentation of catheters in MR-guided interventions remains a challenge, and a step of critical importance in clinical workflows. In this work, under reasonable assumptions, mechanical model based heuristics guide the segmentation process allows correct catheter identification rates greater than 98% (error 2.88 mm), and reduction in outliers to one-fourth compared to the state of the art. Given distal tips, searching towards the proximal ends of the catheters is guided by mechanical models that are estimated on a per-catheter basis. Their bending characteristics are used to constrain the image feature based candidate points. The final catheter trajectories are hybrid sequences of individual points, each derived from model and image features. We evaluate the method on a database of 10 patient MRI scans including 101 manually segmented catheters. The mean errors were 1.40 mm and the median errors were 1.05 mm. The number of outliers deviating more than 2 mm from the gold standard is 7, and the number of outliers deviating more than 3 mm from the gold standard is just 2.



### A General Model for Robust Tensor Factorization with Unknown Noise
- **Arxiv ID**: http://arxiv.org/abs/1705.06755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06755v1)
- **Published**: 2017-05-18 18:04:15+00:00
- **Updated**: 2017-05-18 18:04:15+00:00
- **Authors**: Xi'ai Chen, Zhi Han, Yao Wang, Qian Zhao, Deyu Meng, Lin Lin, Yandong Tang
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: Because of the limitations of matrix factorization, such as losing spatial structure information, the concept of low-rank tensor factorization (LRTF) has been applied for the recovery of a low dimensional subspace from high dimensional visual data. The low-rank tensor recovery is generally achieved by minimizing the loss function between the observed data and the factorization representation. The loss function is designed in various forms under different noise distribution assumptions, like $L_1$ norm for Laplacian distribution and $L_2$ norm for Gaussian distribution. However, they often fail to tackle the real data which are corrupted by the noise with unknown distribution. In this paper, we propose a generalized weighted low-rank tensor factorization method (GWLRTF) integrated with the idea of noise modelling. This procedure treats the target data as high-order tensor directly and models the noise by a Mixture of Gaussians, which is called MoG GWLRTF. The parameters in the model are estimated under the EM framework and through a new developed algorithm of weighted low-rank tensor factorization. We provide two versions of the algorithm with different tensor factorization operations, i.e., CP factorization and Tucker factorization. Extensive experiments indicate the respective advantages of this two versions in different applications and also demonstrate the effectiveness of MoG GWLRTF compared with other competing methods.



### Building effective deep neural network architectures one feature at a time
- **Arxiv ID**: http://arxiv.org/abs/1705.06778v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1705.06778v2)
- **Published**: 2017-05-18 19:40:37+00:00
- **Updated**: 2017-10-19 21:59:52+00:00
- **Authors**: Martin Mundt, Tobias Weis, Kishore Konda, Visvanathan Ramesh
- **Comment**: None
- **Journal**: None
- **Summary**: Successful training of convolutional neural networks is often associated with sufficiently deep architectures composed of high amounts of features. These networks typically rely on a variety of regularization and pruning techniques to converge to less redundant states. We introduce a novel bottom-up approach to expand representations in fixed-depth architectures. These architectures start from just a single feature per layer and greedily increase width of individual layers to attain effective representational capacities needed for a specific task. While network growth can rely on a family of metrics, we propose a computationally efficient version based on feature time evolution and demonstrate its potency in determining feature importance and a networks' effective capacity. We demonstrate how automatically expanded architectures converge to similar topologies that benefit from lesser amount of parameters or improved accuracy and exhibit systematic correspondence in representational complexity with the specified task. In contrast to conventional design patterns with a typical monotonic increase in the amount of features with increased depth, we observe that CNNs perform better when there is more learnable parameters in intermediate, with falloffs to earlier and later layers.



### Pixel Deconvolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.06820v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.06820v4)
- **Published**: 2017-05-18 22:31:26+00:00
- **Updated**: 2017-11-27 02:53:39+00:00
- **Authors**: Hongyang Gao, Hao Yuan, Zhengyang Wang, Shuiwang Ji
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Deconvolutional layers have been widely used in a variety of deep models for up-sampling, including encoder-decoder networks for semantic segmentation and deep generative models for unsupervised learning. One of the key limitations of deconvolutional operations is that they result in the so-called checkerboard problem. This is caused by the fact that no direct relationship exists among adjacent pixels on the output feature map. To address this problem, we propose the pixel deconvolutional layer (PixelDCL) to establish direct relationships among adjacent pixels on the up-sampled feature map. Our method is based on a fresh interpretation of the regular deconvolution operation. The resulting PixelDCL can be used to replace any deconvolutional layer in a plug-and-play manner without compromising the fully trainable capabilities of original models. The proposed PixelDCL may result in slight decrease in efficiency, but this can be overcome by an implementation trick. Experimental results on semantic segmentation demonstrate that PixelDCL can consider spatial features such as edges and shapes and yields more accurate segmentation outputs than deconvolutional layers. When used in image generation tasks, our PixelDCL can largely overcome the checkerboard problem suffered by regular deconvolution operations.



### Spatial Variational Auto-Encoding via Matrix-Variate Normal Distributions
- **Arxiv ID**: http://arxiv.org/abs/1705.06821v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.06821v2)
- **Published**: 2017-05-18 22:32:57+00:00
- **Updated**: 2019-01-22 17:48:22+00:00
- **Authors**: Zhengyang Wang, Hao Yuan, Shuiwang Ji
- **Comment**: Accepted by SDM2019. Code is publicly available at
  https://github.com/divelab/svae
- **Journal**: None
- **Summary**: The key idea of variational auto-encoders (VAEs) resembles that of traditional auto-encoder models in which spatial information is supposed to be explicitly encoded in the latent space. However, the latent variables in VAEs are vectors, which can be interpreted as multiple feature maps of size 1x1. Such representations can only convey spatial information implicitly when coupled with powerful decoders. In this work, we propose spatial VAEs that use feature maps of larger size as latent variables to explicitly capture spatial information. This is achieved by allowing the latent variables to be sampled from matrix-variate normal (MVN) distributions whose parameters are computed from the encoder network. To increase dependencies among locations on latent feature maps and reduce the number of parameters, we further propose spatial VAEs via low-rank MVN distributions. Experimental results show that the proposed spatial VAEs outperform original VAEs in capturing rich structural and spatial information.



### Exploring the structure of a real-time, arbitrary neural artistic stylization network
- **Arxiv ID**: http://arxiv.org/abs/1705.06830v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06830v2)
- **Published**: 2017-05-18 23:25:48+00:00
- **Updated**: 2017-08-24 19:06:03+00:00
- **Authors**: Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent Dumoulin, Jonathon Shlens
- **Comment**: Accepted as an oral presentation at British Machine Vision Conference
  (BMVC) 2017
- **Journal**: None
- **Summary**: In this paper, we present a method which combines the flexibility of the neural algorithm of artistic style with the speed of fast style transfer networks to allow real-time stylization using any content/style image pair. We build upon recent work leveraging conditional instance normalization for multi-style transfer networks by learning to predict the conditional instance normalization parameters directly from a style image. The model is successfully trained on a corpus of roughly 80,000 paintings and is able to generalize to paintings previously unobserved. We demonstrate that the learned embedding space is smooth and contains a rich structure and organizes semantic information associated with paintings in an entirely unsupervised manner.



