# Arxiv Papers in cs.CV on 2017-05-30
### Discriminatively Learned Hierarchical Rank Pooling Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.10420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10420v1)
- **Published**: 2017-05-30 00:43:16+00:00
- **Updated**: 2017-05-30 00:43:16+00:00
- **Authors**: Basura Fernando, Stephen Gould
- **Comment**: International Journal of Computer Vision
- **Journal**: None
- **Summary**: In this work, we present novel temporal encoding methods for action and activity classification by extending the unsupervised rank pooling temporal encoding method in two ways. First, we present "discriminative rank pooling" in which the shared weights of our video representation and the parameters of the action classifiers are estimated jointly for a given training dataset of labelled vector sequences using a bilevel optimization formulation of the learning problem. When the frame level features vectors are obtained from a convolutional neural network (CNN), we rank pool the network activations and jointly estimate all parameters of the model, including CNN filters and fully-connected weights, in an end-to-end manner which we coined as "end-to-end trainable rank pooled CNN". Importantly, this model can make use of any existing convolutional neural network architecture (e.g., AlexNet or VGG) without modification or introduction of additional parameters. Then, we extend rank pooling to a high capacity video representation, called "hierarchical rank pooling". Hierarchical rank pooling consists of a network of rank pooling functions, which encode temporal semantics over arbitrary long video clips based on rich frame level features. By stacking non-linear feature functions and temporal sub-sequence encoders one on top of the other, we build a high capacity encoding network of the dynamic behaviour of the video. The resulting video representation is a fixed-length feature vector describing the entire video clip that can be used as input to standard machine learning classifiers. We demonstrate our approach on the task of action and activity recognition. Obtained results are comparable to state-of-the-art methods on three important activity recognition benchmarks with classification performance of 76.7% mAP on Hollywood2, 69.4% on HMDB51, and 93.6% on UCF101.



### Unsupervised Person Re-identification: Clustering and Fine-tuning
- **Arxiv ID**: http://arxiv.org/abs/1705.10444v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10444v2)
- **Published**: 2017-05-30 03:14:49+00:00
- **Updated**: 2017-06-29 00:58:47+00:00
- **Authors**: Hehe Fan, Liang Zheng, Yi Yang
- **Comment**: Add more results, parameter analysis and comparisons
- **Journal**: None
- **Summary**: The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this paper, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between 1) pedestrian clustering and 2) fine-tuning of the convolutional neural network (CNN) to improve the original model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning when the model is weak, CNN is fine-tuned on a small amount of reliable examples which locate near to cluster centroids in the feature space. As the model becomes stronger in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy.



### Robust Tracking Using Region Proposal Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.10447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10447v1)
- **Published**: 2017-05-30 03:32:07+00:00
- **Updated**: 2017-05-30 03:32:07+00:00
- **Authors**: Jimmy Ren, Zhiyang Yu, Jianbo Liu, Rui Zhang, Wenxiu Sun, Jiahao Pang, Xiaohao Chen, Qiong Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in visual tracking showed that deep Convolutional Neural Networks (CNN) trained for image classification can be strong feature extractors for discriminative trackers. However, due to the drastic difference between image classification and tracking, extra treatments such as model ensemble and feature engineering must be carried out to bridge the two domains. Such procedures are either time consuming or hard to generalize well across datasets. In this paper we discovered that the internal structure of Region Proposal Network (RPN)'s top layer feature can be utilized for robust visual tracking. We showed that such property has to be unleashed by a novel loss function which simultaneously considers classification accuracy and bounding box quality. Without ensemble and any extra treatment on feature maps, our proposed method achieved state-of-the-art results on several large scale benchmarks including OTB50, OTB100 and VOT2016. We will make our code publicly available.



### RSI-CB: A Large Scale Remote Sensing Image Classification Benchmark via Crowdsource Data
- **Arxiv ID**: http://arxiv.org/abs/1705.10450v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10450v3)
- **Published**: 2017-05-30 03:53:03+00:00
- **Updated**: 2020-01-10 11:31:17+00:00
- **Authors**: Haifeng Li, Xin Dou, Chao Tao, Zhixiang Hou, Jie Chen, Jian Peng, Min Deng, Ling Zhao
- **Comment**: 41 pages, 19 figures, 7 tables
- **Journal**: None
- **Summary**: In recent years, deep convolutional neural network (DCNN) has seen a breakthrough progress in natural image recognition because of three points: universal approximation ability via DCNN, large-scale database (such as ImageNet), and supercomputing ability powered by GPU. The remote sensing field is still lacking a large-scale benchmark compared to ImageNet and Place2. In this paper, we propose a remote sensing image classification benchmark (RSI-CB) based on massive, scalable, and diverse crowdsource data. Using crowdsource data, such as Open Street Map (OSM) data, ground objects in remote sensing images can be annotated effectively by points of interest, vector data from OSM, or other crowdsource data. The annotated images can be used in remote sensing image classification tasks. Based on this method, we construct a worldwide large-scale benchmark for remote sensing image classification. This benchmark has two sub-datasets with 256 by 256 and 128 by 128 sizes because different DCNNs require different image sizes. The former contains 6 categories with 35 subclasses of more than 24,000 images. The latter contains 6 categories with 45 subclasses of more than 36,000 images. This classification system of ground objects is defined according to the national standard of land-use classification in China and is inspired by the hierarchy mechanism of ImageNet. Finally, we conduct many experiments to compare RSI-CB with the SAT-4, SAT-6, and UC-Merced datasets on handcrafted features, such as scale-invariant feature transform, color histogram, local binary patterns, and GIST, and classical DCNN models, such as AlexNet, VGGNet, GoogLeNet, and ResNet.



### Decorrelation of Neutral Vector Variables: Theory and Applications
- **Arxiv ID**: http://arxiv.org/abs/1705.10524v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.10524v1)
- **Published**: 2017-05-30 09:53:11+00:00
- **Updated**: 2017-05-30 09:53:11+00:00
- **Authors**: Zhanyu Ma, Jing-Hao Xue, Arne Leijon, Zheng-Hua Tan, Zhen Yang, Jun Guo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose novel strategies for neutral vector variable decorrelation. Two fundamental invertible transformations, namely serial nonlinear transformation and parallel nonlinear transformation, are proposed to carry out the decorrelation. For a neutral vector variable, which is not multivariate Gaussian distributed, the conventional principal component analysis (PCA) cannot yield mutually independent scalar variables. With the two proposed transformations, a highly negatively correlated neutral vector can be transformed to a set of mutually independent scalar variables with the same degrees of freedom. We also evaluate the decorrelation performances for the vectors generated from a single Dirichlet distribution and a mixture of Dirichlet distributions. The mutual independence is verified with the distance correlation measurement. The advantages of the proposed decorrelation strategies are intensively studied and demonstrated with synthesized data and practical application evaluations.



### Parcellation of Visual Cortex on high-resolution histological Brain Sections using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.10545v1
- **DOI**: 10.1109/ISBI.2017.7950666
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10545v1)
- **Published**: 2017-05-30 11:05:00+00:00
- **Updated**: 2017-05-30 11:05:00+00:00
- **Authors**: Hannah Spitzer, Katrin Amunts, Stefan Harmeling, Timo Dickscheid
- **Comment**: Accepted for oral presentation at International Symposium of
  Biomedical Imaging (ISBI) 2017
- **Journal**: None
- **Summary**: Microscopic analysis of histological sections is considered the "gold standard" to verify structural parcellations in the human brain. Its high resolution allows the study of laminar and columnar patterns of cell distributions, which build an important basis for the simulation of cortical areas and networks. However, such cytoarchitectonic mapping is a semiautomatic, time consuming process that does not scale with high throughput imaging. We present an automatic approach for parcellating histological sections at 2um resolution. It is based on a convolutional neural network that combines topological information from probabilistic atlases with the texture features learned from high-resolution cell-body stained images. The model is applied to visual areas and trained on a sparse set of partial annotations. We show how predictions are transferable to new brains and spatially consistent across sections.



### Saliency Revisited: Analysis of Mouse Movements versus Fixations
- **Arxiv ID**: http://arxiv.org/abs/1705.10546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10546v1)
- **Published**: 2017-05-30 11:07:43+00:00
- **Updated**: 2017-05-30 11:07:43+00:00
- **Authors**: Hamed R. Tavakoli, Fawad Ahmed, Ali Borji, Jorma Laaksonen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper revisits visual saliency prediction by evaluating the recent advancements in this field such as crowd-sourced mouse tracking-based databases and contextual annotations. We pursue a critical and quantitative approach towards some of the new challenges including the quality of mouse tracking versus eye tracking for model training and evaluation. We extend quantitative evaluation of models in order to incorporate contextual information by proposing an evaluation methodology that allows accounting for contextual factors such as text, faces, and object attributes. The proposed contextual evaluation scheme facilitates detailed analysis of models and helps identify their pros and cons. Through several experiments, we find that (1) mouse tracking data has lower inter-participant visual congruency and higher dispersion, compared to the eye tracking data, (2) mouse tracking data does not totally agree with eye tracking in general and in terms of different contextual regions in specific, and (3) mouse tracking data leads to acceptable results in training current existing models, and (4) mouse tracking data is less reliable for model selection and evaluation. The contextual evaluation also reveals that, among the studied models, there is no single model that performs best on all the tested annotations.



### Interpreting and Extending The Guided Filter Via Cyclic Coordinate Descent
- **Arxiv ID**: http://arxiv.org/abs/1705.10552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10552v1)
- **Published**: 2017-05-30 11:29:23+00:00
- **Updated**: 2017-05-30 11:29:23+00:00
- **Authors**: Longquan Dai
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we will disclose that the Guided Filter (GF) can be interpreted as the Cyclic Coordinate Descent (CCD) solver of a Least Square (LS) objective function. This discovery implies a possible way to extend GF because we can alter the objective function of GF and define new filters as the first pass iteration of the CCD solver of modified objective functions. Moreover, referring to the iterative minimizing procedure of CCD, we can derive new rolling filtering schemes. Hence, under the guidance of this discovery, we not only propose new GF-like filters adapting to the specific requirements of applications but also offer thoroughly explanations for two rolling filtering schemes of GF as well as the way to extend them. Experiments show that our new filters and extensions produce state-of-the-art results.



### End-to-end Active Object Tracking via Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1705.10561v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10561v3)
- **Published**: 2017-05-30 11:44:50+00:00
- **Updated**: 2018-06-01 16:14:24+00:00
- **Authors**: Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong Zhang, Yizhou Wang
- **Comment**: To appear in ICML2018
- **Journal**: None
- **Summary**: We study active object tracking, where a tracker takes as input the visual observation (i.e., frame sequence) and produces the camera control signal (e.g., move forward, turn left, etc.). Conventional methods tackle the tracking and the camera control separately, which is challenging to tune jointly. It also incurs many human efforts for labeling and many expensive trial-and-errors in realworld. To address these issues, we propose, in this paper, an end-to-end solution via deep reinforcement learning, where a ConvNet-LSTM function approximator is adopted for the direct frame-toaction prediction. We further propose an environment augmentation technique and a customized reward function, which are crucial for a successful training. The tracker trained in simulators (ViZDoom, Unreal Engine) shows good generalization in the case of unseen object moving path, unseen object appearance, unseen background, and distracting object. It can restore tracking when occasionally losing the target. With the experiments over the VOT dataset, we also find that the tracking ability, obtained solely from simulators, can potentially transfer to real-world scenarios.



### Multi-Focus Image Fusion Using Sparse Representation and Coupled Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1705.10574v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.10574v3)
- **Published**: 2017-05-30 12:20:26+00:00
- **Updated**: 2019-05-03 08:05:44+00:00
- **Authors**: Farshad G. Veshki, Sergiy A. Vorobyov
- **Comment**: 25 pages, 15 figures, 2 table
- **Journal**: None
- **Summary**: We address the multi-focus image fusion problem, where multiple images captured with different focal settings are to be fused into an all-in-focus image of higher quality. Algorithms for this problem necessarily admit the source image characteristics along with focused and blurred features. However, most sparsity-based approaches use a single dictionary in focused feature space to describe multi-focus images, and ignore the representations in blurred feature space. We propose a multi-focus image fusion approach based on sparse representation using a coupled dictionary. It exploits the observations that the patches from a given training set can be sparsely represented by a couple of overcomplete dictionaries related to the focused and blurred categories of images and that a sparse approximation based on such coupled dictionary leads to a more flexible and therefore better fusion strategy than the one based on just selecting the sparsest representation in the original image estimate. In addition, to improve the fusion performance, we employ a coupled dictionary learning approach that enforces pairwise correlation between atoms of dictionaries learned to represent the focused and blurred feature spaces. We also discuss the advantages of the fusion approach based on coupled dictionary learning, and present efficient algorithms for fusion based on coupled dictionary learning. Extensive experimental comparisons with state-of-the-art multi-focus image fusion algorithms validate the effectiveness of the proposed approach.



### Nighttime sky/cloud image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1705.10583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10583v1)
- **Published**: 2017-05-30 12:39:51+00:00
- **Updated**: 2017-05-30 12:39:51+00:00
- **Authors**: Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler
- **Comment**: Accepted in Proc. IEEE International Conference on Image Processing
  (ICIP), 2017
- **Journal**: None
- **Summary**: Imaging the atmosphere using ground-based sky cameras is a popular approach to study various atmospheric phenomena. However, it usually focuses on the daytime. Nighttime sky/cloud images are darker and noisier, and thus harder to analyze. An accurate segmentation of sky/cloud images is already challenging because of the clouds' non-rigid structure and size, and the lower and less stable illumination of the night sky increases the difficulty. Nonetheless, nighttime cloud imaging is essential in certain applications, such as continuous weather analysis and satellite communication.   In this paper, we propose a superpixel-based method to segment nighttime sky/cloud images. We also release the first nighttime sky/cloud image segmentation database to the research community. The experimental results show the efficacy of our proposed algorithm for nighttime images.



### Discovering Visual Concept Structure with Sparse and Incomplete Tags
- **Arxiv ID**: http://arxiv.org/abs/1705.10659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10659v1)
- **Published**: 2017-05-30 14:12:43+00:00
- **Updated**: 2017-05-30 14:12:43+00:00
- **Authors**: Jingya Wang, Xiatian Zhu, Shaogang Gong
- **Comment**: Artificial Intelligence journal 2017
- **Journal**: None
- **Summary**: Discovering automatically the semantic structure of tagged visual data (e.g. web videos and images) is important for visual data analysis and interpretation, enabling the machine intelligence for effectively processing the fast-growing amount of multi-media data. However, this is non-trivial due to the need for jointly learning underlying correlations between heterogeneous visual and tag data. The task is made more challenging by inherently sparse and incomplete tags. In this work, we develop a method for modelling the inherent visual data concept structures based on a novel Hierarchical-Multi-Label Random Forest model capable of correlating structured visual and tag information so as to more accurately interpret the visual semantics, e.g. disclosing meaningful visual groups with similar high-level concepts, and recovering missing tags for individual visual data samples. Specifically, our model exploits hierarchically structured tags of different semantic abstractness and multiple tag statistical correlations in addition to modelling visual and tag interactions. As a result, our model is able to discover more accurate semantic correlation between textual tags and visual features, and finally providing favourable visual semantics interpretation even with highly sparse and incomplete tags. We demonstrate the advantages of our proposed approach in two fundamental applications, visual data clustering and missing tag completion, on benchmarking video (i.e. TRECVID MED 2011) and image (i.e. NUS-WIDE) datasets.



### Deep Learning is Robust to Massive Label Noise
- **Arxiv ID**: http://arxiv.org/abs/1705.10694v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1705.10694v3)
- **Published**: 2017-05-30 15:10:51+00:00
- **Updated**: 2018-02-26 16:51:57+00:00
- **Authors**: David Rolnick, Andreas Veit, Serge Belongie, Nir Shavit
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks trained on large supervised datasets have led to impressive results in image classification and other tasks. However, well-annotated datasets can be time-consuming and expensive to collect, lending increased interest to larger but noisy datasets that are more easily obtained. In this paper, we show that deep neural networks are capable of generalizing from training data for which true labels are massively outnumbered by incorrect labels. We demonstrate remarkably high test performance after training on corrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain test accuracy above 90 percent even after each clean training example has been diluted with 100 randomly-labeled examples. Such behavior holds across multiple patterns of label noise, even when erroneous labels are biased towards confusing classes. We show that training in this regime requires a significant but manageable increase in dataset size that is related to the factor by which correct labels have been diluted. Finally, we provide an analysis of our results that shows how increasing noise decreases the effective batch size.



### ResnetCrowd: A Residual Deep Learning Architecture for Crowd Counting, Violent Behaviour Detection and Crowd Density Level Classification
- **Arxiv ID**: http://arxiv.org/abs/1705.10698v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10698v1)
- **Published**: 2017-05-30 15:18:41+00:00
- **Updated**: 2017-05-30 15:18:41+00:00
- **Authors**: Mark Marsden, Kevin McGuinness, Suzanne Little, Noel E. O'Connor
- **Comment**: 7 Pages, AVSS 2017
- **Journal**: None
- **Summary**: In this paper we propose ResnetCrowd, a deep residual architecture for simultaneous crowd counting, violent behaviour detection and crowd density level classification. To train and evaluate the proposed multi-objective technique, a new 100 image dataset referred to as Multi Task Crowd is constructed. This new dataset is the first computer vision dataset fully annotated for crowd counting, violent behaviour detection and density level classification. Our experiments show that a multi-task approach boosts individual task performance for all tasks and most notably for violent behaviour detection which receives a 9\% boost in ROC curve AUC (Area under the curve). The trained ResnetCrowd model is also evaluated on several additional benchmarks highlighting the superior generalisation of crowd analysis models trained for multiple objectives.



### Multi-View Task-Driven Recognition in Visual Sensor Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.10715v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10715v2)
- **Published**: 2017-05-30 16:10:36+00:00
- **Updated**: 2017-05-31 22:03:10+00:00
- **Authors**: Ali Taalimi, Alireza Rahimpour, Liu Liu, Hairong Qi
- **Comment**: 5 pages, Accepted in International Conference of Image Processing,
  2017
- **Journal**: None
- **Summary**: Nowadays, distributed smart cameras are deployed for a wide set of tasks in several application scenarios, ranging from object recognition, image retrieval, and forensic applications. Due to limited bandwidth in distributed systems, efficient coding of local visual features has in fact been an active topic of research. In this paper, we propose a novel approach to obtain a compact representation of high-dimensional visual data using sensor fusion techniques. We convert the problem of visual analysis in resource-limited scenarios to a multi-view representation learning, and we show that the key to finding properly compressed representation is to exploit the position of cameras with respect to each other as a norm-based regularization in the particular signal representation of sparse coding. Learning the representation of each camera is viewed as an individual task and a multi-task learning with joint sparsity for all nodes is employed. The proposed representation learning scheme is referred to as the multi-view task-driven learning for visual sensor network (MT-VSN). We demonstrate that MT-VSN outperforms state-of-the-art in various surveillance recognition tasks.



### Addressing Ambiguity in Multi-target Tracking by Hierarchical Strategy
- **Arxiv ID**: http://arxiv.org/abs/1705.10716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10716v1)
- **Published**: 2017-05-30 16:11:34+00:00
- **Updated**: 2017-05-30 16:11:34+00:00
- **Authors**: Ali Taalimi, Liu Liu, Hairong Qi
- **Comment**: 5 pages, Accepted in International Conference of Image Processing,
  2017
- **Journal**: None
- **Summary**: This paper presents a novel hierarchical approach for the simultaneous tracking of multiple targets in a video. We use a network flow approach to link detections in low-level and tracklets in high-level. At each step of the hierarchy, the confidence of candidates is measured by using a new scoring system, ConfRank, that considers the quality and the quantity of its neighborhood. The output of the first stage is a collection of safe tracklets and unlinked high-confidence detections. For each individual detection, we determine if it belongs to an existing or is a new tracklet. We show the effect of our framework to recover missed detections and reduce switch identity. The proposed tracker is referred to as TVOD for multi-target tracking using the visual tracker and generic object detector. We achieve competitive results with lower identity switches on several datasets comparing to state-of-the-art.



### Deep manifold-to-manifold transforming network for action recognition
- **Arxiv ID**: http://arxiv.org/abs/1705.10732v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10732v3)
- **Published**: 2017-05-30 16:38:44+00:00
- **Updated**: 2017-09-30 03:09:53+00:00
- **Authors**: Tong Zhang, Wenming Zheng, Zhen Cui, Chaolong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Symmetric positive definite (SPD) matrices (e.g., covariances, graph Laplacians, etc.) are widely used to model the relationship of spatial or temporal domain. Nevertheless, SPD matrices are theoretically embedded on Riemannian manifolds. In this paper, we propose an end-to-end deep manifold-to-manifold transforming network (DMT-Net) which can make SPD matrices flow from one Riemannian manifold to another more discriminative one. To learn discriminative SPD features characterizing both spatial and temporal dependencies, we specifically develop three novel layers on manifolds: (i) the local SPD convolutional layer, (ii) the non-linear SPD activation layer, and (iii) the Riemannian-preserved recursive layer. The SPD property is preserved through all layers without any requirement of singular value decomposition (SVD), which is often used in the existing methods with expensive computation cost. Furthermore, a diagonalizing SPD layer is designed to efficiently calculate the final metric for the classification task. To evaluate our proposed method, we conduct extensive experiments on the task of action recognition, where input signals are popularly modeled as SPD matrices. The experimental results demonstrate that our DMT-Net is much more competitive over state-of-the-art.



### Efficient Decentralized Visual Place Recognition From Full-Image Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1705.10739v1
- **DOI**: 10.1109/MRS.2017.8250934
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.10739v1)
- **Published**: 2017-05-30 16:43:57+00:00
- **Updated**: 2017-05-30 16:43:57+00:00
- **Authors**: Titus Cieslewski, Davide Scaramuzza
- **Comment**: 3 pages, 4 figures. This is a self-published paper that accompanies
  our original work [1] as well as the ICRA 2017 Workshop on Multi-robot
  Perception-Driven Control and Planning [2]
- **Journal**: 2017 International Symposium on Multi-Robot and Multi-Agent
  Systems (MRS) 78-82
- **Summary**: In this paper, we discuss the adaptation of our decentralized place recognition method described in [1] to full image descriptors. As we had shown, the key to making a scalable decentralized visual place recognition lies in exploting deterministic key assignment in a distributed key-value map. Through this, it is possible to reduce bandwidth by up to a factor of n, the robot count, by casting visual place recognition to a key-value lookup problem. In [1], we exploited this for the bag-of-words method [3], [4]. Our method of casting bag-of-words, however, results in a complex decentralized system, which has inherently worse recall than its centralized counterpart. In this paper, we instead start from the recent full-image description method NetVLAD [5]. As we show, casting this to a key-value lookup problem can be achieved with k-means clustering, and results in a much simpler system than [1]. The resulting system still has some flaws, albeit of a completely different nature: it suffers when the environment seen during deployment lies in a different distribution in feature space than the environment seen during training.



### Computation-Performance Optimization of Convolutional Neural Networks with Redundant Kernel Removal
- **Arxiv ID**: http://arxiv.org/abs/1705.10748v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10748v3)
- **Published**: 2017-05-30 16:59:46+00:00
- **Updated**: 2018-04-10 16:34:56+00:00
- **Authors**: Chih-Ting Liu, Yi-Heng Wu, Yu-Sheng Lin, Shao-Yi Chien
- **Comment**: This paper was accepted by 2018 The International Symposium on
  Circuits and Systems (ISCAS)
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNNs) are widely employed in modern computer vision algorithms, where the input image is convolved iteratively by many kernels to extract the knowledge behind it. However, with the depth of convolutional layers getting deeper and deeper in recent years, the enormous computational complexity makes it difficult to be deployed on embedded systems with limited hardware resources. In this paper, we propose two computation-performance optimization methods to reduce the redundant convolution kernels of a CNN with performance and architecture constraints, and apply it to a network for super resolution (SR). Using PSNR drop compared to the original network as the performance criterion, our method can get the optimal PSNR under a certain computation budget constraint. On the other hand, our method is also capable of minimizing the computation required under a given PSNR drop.



### Generative Models of Visually Grounded Imagination
- **Arxiv ID**: http://arxiv.org/abs/1705.10762v8
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.10762v8)
- **Published**: 2017-05-30 17:32:26+00:00
- **Updated**: 2018-11-09 08:16:22+00:00
- **Authors**: Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, Kevin Murphy
- **Comment**: International Conference on Learning Representations (ICLR), 2018
- **Journal**: None
- **Summary**: It is easy for people to imagine what a man with pink hair looks like, even if they have never seen such a person before. We call the ability to create images of novel semantic concepts visually grounded imagination. In this paper, we show how we can modify variational auto-encoders to perform this task. Our method uses a novel training objective, and a novel product-of-experts inference network, which can handle partially specified (abstract) concepts in a principled and efficient way. We also propose a set of easy-to-compute evaluation metrics that capture our intuitive notions of what it means to have good visual imagination, namely correctness, coverage, and compositionality (the 3 C's). Finally, we perform a detailed comparison of our method with two existing joint image-attribute VAE methods (the JMVAE method of Suzuki et.al. and the BiVCCA method of Wang et.al.) by applying them to two datasets: the MNIST-with-attributes dataset (which we introduce here), and the CelebA dataset.



### Reflection Invariant and Symmetry Detection
- **Arxiv ID**: http://arxiv.org/abs/1705.10768v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10768v2)
- **Published**: 2017-05-30 17:45:02+00:00
- **Updated**: 2017-05-31 19:03:27+00:00
- **Authors**: Erbo Li, Hua Li
- **Comment**: None
- **Journal**: None
- **Summary**: Symmetry detection and discrimination are of fundamental meaning in science, technology, and engineering. This paper introduces reflection invariants and defines the directional moment to detect symmetry for shape analysis and object recognition. And it demonstrates that detection of reflection symmetry can be done in a simple way by solving a trigonometric system derived from the directional moment, and discrimination of reflection symmetry can be achieved by application of the reflection invariants in 2D and 3D. Rotation symmetry can also be determined based on that.The experiments in 2D and 3D, including the regular triangle, the square, and the five Platonic objects, show that all the reflection lines or planes can be deterministically found using directional moments up to order six. This result can be used to simplify the efforts of symmetry detection in research areas, such as protein structure, model retrieval, inverse engineering, and machine vision etc.



### Accelerating Neural Architecture Search using Performance Prediction
- **Arxiv ID**: http://arxiv.org/abs/1705.10823v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1705.10823v2)
- **Published**: 2017-05-30 19:00:53+00:00
- **Updated**: 2017-11-08 16:09:35+00:00
- **Authors**: Bowen Baker, Otkrist Gupta, Ramesh Raskar, Nikhil Naik
- **Comment**: Submitted to International Conference on Learning Representations,
  (2018)
- **Journal**: None
- **Summary**: Methods for neural network hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of model configurations. In this paper, we show that standard frequentist regression models can predict the final performance of partially trained model configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We empirically show that our performance prediction models are much more effective than prominent Bayesian counterparts, are simpler to implement, and are faster to train. Our models can predict final performance in both visual classification and language modeling domains, are effective for predicting performance of drastically varying model architectures, and can even generalize between model classes. Using these prediction models, we also propose an early stopping method for hyperparameter optimization and meta-modeling, which obtains a speedup of a factor up to 6x in both hyperparameter optimization and meta-modeling. Finally, we empirically show that our early stopping method can be seamlessly incorporated into both reinforcement learning-based architecture selection algorithms and bandit based search methods. Through extensive experimentation, we empirically show our performance prediction models and early stopping algorithm are state-of-the-art in terms of prediction accuracy and speedup achieved while still identifying the optimal model configurations.



### Generic Tubelet Proposals for Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1705.10861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10861v1)
- **Published**: 2017-05-30 20:38:31+00:00
- **Updated**: 2017-05-30 20:38:31+00:00
- **Authors**: Jiawei He, Mostafa S. Ibrahim, Zhiwei Deng, Greg Mori
- **Comment**: None
- **Journal**: None
- **Summary**: We develop a novel framework for action localization in videos. We propose the Tube Proposal Network (TPN), which can generate generic, class-independent, video-level tubelet proposals in videos. The generated tubelet proposals can be utilized in various video analysis tasks, including recognizing and localizing actions in videos. In particular, we integrate these generic tubelet proposals into a unified temporal deep network for action classification. Compared with other methods, our generic tubelet proposal method is accurate, general, and is fully differentiable under a smoothL1 loss function. We demonstrate the performance of our algorithm on the standard UCF-Sports, J-HMDB21, and UCF-101 datasets. Our class-independent TPN outperforms other tubelet generation methods, and our unified temporal deep network achieves state-of-the-art localization results on all three datasets.



### Working hard to know your neighbor's margins: Local descriptor learning loss
- **Arxiv ID**: http://arxiv.org/abs/1705.10872v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10872v4)
- **Published**: 2017-05-30 21:23:19+00:00
- **Updated**: 2018-01-12 14:40:46+00:00
- **Authors**: Anastasiya Mishchuk, Dmytro Mishkin, Filip Radenovic, Jiri Matas
- **Comment**: Post-NIPS-2017 update. Better hyperparameters and better results on
  HPatches + Brown dataset, + couple of references
- **Journal**: None
- **Summary**: We introduce a novel loss for learning local feature descriptors which is inspired by the Lowe's matching criterion for SIFT. We show that the proposed loss that maximizes the distance between the closest positive and closest negative patch in the batch is better than complex regularization methods; it works well for both shallow and deep convolution network architectures. Applying the novel loss to the L2Net CNN architecture results in a compact descriptor -- it has the same dimensionality as SIFT (128) that shows state-of-art performance in wide baseline stereo, patch verification and instance retrieval benchmarks. It is fast, computing a descriptor takes about 1 millisecond on a low-end GPU.



### Morphological Error Detection in 3D Segmentations
- **Arxiv ID**: http://arxiv.org/abs/1705.10882v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.10882v1)
- **Published**: 2017-05-30 22:25:44+00:00
- **Updated**: 2017-05-30 22:25:44+00:00
- **Authors**: David Rolnick, Yaron Meirovitch, Toufiq Parag, Hanspeter Pfister, Viren Jain, Jeff W. Lichtman, Edward S. Boyden, Nir Shavit
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Deep learning algorithms for connectomics rely upon localized classification, rather than overall morphology. This leads to a high incidence of erroneously merged objects. Humans, by contrast, can easily detect such errors by acquiring intuition for the correct morphology of objects. Biological neurons have complicated and variable shapes, which are challenging to learn, and merge errors take a multitude of different forms. We present an algorithm, MergeNet, that shows 3D ConvNets can, in fact, detect merge errors from high-level neuronal morphology. MergeNet follows unsupervised training and operates across datasets. We demonstrate the performance of MergeNet both on a variety of connectomics data and on a dataset created from merged MNIST images.



### Efficient, sparse representation of manifold distance matrices for classical scaling
- **Arxiv ID**: http://arxiv.org/abs/1705.10887v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NA, 65D05, 68T99, 65F50, 68T45, I.2.10; G.1; I.4
- **Links**: [PDF](http://arxiv.org/pdf/1705.10887v2)
- **Published**: 2017-05-30 23:18:18+00:00
- **Updated**: 2018-03-29 17:35:03+00:00
- **Authors**: Javier S. Turek, Alexander Huth
- **Comment**: Conference CVPR 2018
- **Journal**: None
- **Summary**: Geodesic distance matrices can reveal shape properties that are largely invariant to non-rigid deformations, and thus are often used to analyze and represent 3-D shapes. However, these matrices grow quadratically with the number of points. Thus for large point sets it is common to use a low-rank approximation to the distance matrix, which fits in memory and can be efficiently analyzed using methods such as multidimensional scaling (MDS). In this paper we present a novel sparse method for efficiently representing geodesic distance matrices using biharmonic interpolation. This method exploits knowledge of the data manifold to learn a sparse interpolation operator that approximates distances using a subset of points. We show that our method is 2x faster and uses 20x less memory than current leading methods for solving MDS on large point sets, with similar quality. This enables analyses of large point sets that were previously infeasible.



