# Arxiv Papers in cs.CV on 2017-05-12
### Convolutional Sparse Representations with Gradient Penalties
- **Arxiv ID**: http://arxiv.org/abs/1705.04407v2
- **DOI**: 10.1109/ICASSP.2018.8462151
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1705.04407v2)
- **Published**: 2017-05-12 00:33:05+00:00
- **Updated**: 2018-02-16 03:47:27+00:00
- **Authors**: Brendt Wohlberg
- **Comment**: None
- **Journal**: None
- **Summary**: While convolutional sparse representations enjoy a number of useful properties, they have received limited attention for image reconstruction problems. The present paper compares the performance of block-based and convolutional sparse representations in the removal of Gaussian white noise. While the usual formulation of the convolutional sparse coding problem is slightly inferior to the block-based representations in this problem, the performance of the convolutional form can be boosted beyond that of the block-based form by the inclusion of suitable penalties on the gradients of the coefficient maps.



### View-Invariant Template Matching Using Homography Constraints
- **Arxiv ID**: http://arxiv.org/abs/1705.04433v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04433v1)
- **Published**: 2017-05-12 03:40:02+00:00
- **Updated**: 2017-05-12 03:40:02+00:00
- **Authors**: Sina Lotfian, Hassan Foroosh
- **Comment**: None
- **Journal**: None
- **Summary**: Change in viewpoint is one of the major factors for variation in object appearance across different images. Thus, view-invariant object recognition is a challenging and important image understanding task. In this paper, we propose a method that can match objects in images taken under different viewpoints. Unlike most methods in the literature, no restriction on camera orientations or internal camera parameters are imposed and no prior knowledge of 3D structure of the object is required. We prove that when two cameras take pictures of the same object from two different viewing angels, the relationship between every quadruple of points reduces to the special case of homography with two equal eigenvalues. Based on this property, we formulate the problem as an error function that indicates how likely two sets of 2D points are projections of the same set of 3D points under two different cameras. Comprehensive set of experiments were conducted to prove the robustness of the method to noise, and evaluate its performance on real-world applications, such as face and object recognition.



### Adaptive Feature Representation for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1705.04442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04442v1)
- **Published**: 2017-05-12 05:04:41+00:00
- **Updated**: 2017-05-12 05:04:41+00:00
- **Authors**: Yuqi Han, Chenwei Deng, Zengshuo Zhang, Jiatong Li, Baojun Zhao
- **Comment**: 4 pages, ICIP 2017
- **Journal**: None
- **Summary**: Robust feature representation plays significant role in visual tracking. However, it remains a challenging issue, since many factors may affect the experimental performance. The existing method which combine different features by setting them equally with the fixed weight could hardly solve the issues, due to the different statistical properties of different features across various of scenarios and attributes. In this paper, by exploiting the internal relationship among these features, we develop a robust method to construct a more stable feature representation. More specifically, we utilize a co-training paradigm to formulate the intrinsic complementary information of multi-feature template into the efficient correlation filter framework. We test our approach on challenging se- quences with illumination variation, scale variation, deformation etc. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods favorably.



### Using Satellite Imagery for Good: Detecting Communities in Desert and Mapping Vaccination Activities
- **Arxiv ID**: http://arxiv.org/abs/1705.04451v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04451v1)
- **Published**: 2017-05-12 07:23:06+00:00
- **Updated**: 2017-05-12 07:23:06+00:00
- **Authors**: Anza Shakeel, Mohsen Ali
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have outperformed existing object recognition and detection algorithms. On the other hand satellite imagery captures scenes that are diverse. This paper describes a deep learning approach that analyzes a geo referenced satellite image and efficiently detects built structures in it. A Fully Convolution Network (FCN) is trained on low resolution Google earth satellite imagery in order to achieve end result. The detected built communities are then correlated with the vaccination activity that has furnished some useful statistics.



### Learning to Refine Object Contours with a Top-Down Fully Convolutional Encoder-Decoder Network
- **Arxiv ID**: http://arxiv.org/abs/1705.04456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04456v1)
- **Published**: 2017-05-12 07:52:27+00:00
- **Updated**: 2017-05-12 07:52:27+00:00
- **Authors**: Yahui Liu, Jian Yao, Li Li, Xiaohu Lu, Jing Han
- **Comment**: 12 pages, 13 figures
- **Journal**: None
- **Summary**: We develop a novel deep contour detection algorithm with a top-down fully convolutional encoder-decoder network. Our proposed method, named TD-CEDN, solves two important issues in this low-level vision problem: (1) learning multi-scale and multi-level features; and (2) applying an effective top-down refined approach in the networks. TD-CEDN performs the pixel-wise prediction by means of leveraging features at all layers of the net. Unlike skip connections and previous encoder-decoder methods, we first learn a coarse feature map after the encoder stage in a feedforward pass, and then refine this feature map in a top-down strategy during the decoder stage utilizing features at successively lower layers. Therefore, the deconvolutional process is conducted stepwise, which is guided by Deeply-Supervision Net providing the integrated direct supervision. The above proposed technologies lead to a more precise and clearer prediction. Our proposed algorithm achieved the state-of-the-art on the BSDS500 dataset (ODS F-score of 0.788), the PASCAL VOC2012 dataset (ODS F-score of 0.588), and and the NYU Depth dataset (ODS F-score of 0.735).



### TraX: The visual Tracking eXchange Protocol and Library
- **Arxiv ID**: http://arxiv.org/abs/1705.04469v1
- **DOI**: 10.1016/j.neucom.2017.02.036
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04469v1)
- **Published**: 2017-05-12 08:33:20+00:00
- **Updated**: 2017-05-12 08:33:20+00:00
- **Authors**: Luka ÄŒehovin
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we address the problem of developing on-line visual tracking algorithms. We present a specialized communication protocol that serves as a bridge between a tracker implementation and utilizing application. It decouples development of algorithms and application, encouraging re-usability. The primary use case is algorithm evaluation where the protocol facilitates more complex evaluation scenarios that are used nowadays thus pushing forward the field of visual tracking. We present a reference implementation of the protocol that makes it easy to use in several popular programming languages and discuss where the protocol is already used and some usage scenarios that we envision for the future.



### External Prior Guided Internal Prior Learning for Real-World Noisy Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1705.04505v2
- **DOI**: 10.1109/TIP.2018.2811546
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04505v2)
- **Published**: 2017-05-12 10:49:33+00:00
- **Updated**: 2018-10-15 08:53:12+00:00
- **Authors**: Jun Xu, Lei Zhang, David Zhang
- **Comment**: 14 pages, 13figures, IEEE Trans. Image Processing 27(6): 2996-3010
  (2018)
- **Journal**: None
- **Summary**: Most of existing image denoising methods learn image priors from either external data or the noisy image itself to remove noise. However, priors learned from external data may not be adaptive to the image to be denoised, while priors learned from the given noisy image may not be accurate due to the interference of corrupted noise. Meanwhile, the noise in real-world noisy images is very complex, which is hard to be described by simple distributions such as Gaussian distribution, making real-world noisy image denoising a very challenging problem. We propose to exploit the information in both external data and the given noisy image, and develop an external prior guided internal prior learning method for real-world noisy image denoising. We first learn external priors from an independent set of clean natural images. With the aid of learned external priors, we then learn internal priors from the given noisy image to refine the prior model. The external and internal priors are formulated as a set of orthogonal dictionaries to efficiently reconstruct the desired image. Extensive experiments are performed on several real-world noisy image datasets. The proposed method demonstrates highly competitive denoising performance, outperforming state-of-the-art denoising methods including those designed for real-world noisy images.



### Spatial-Temporal Recurrent Neural Network for Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1705.04515v1
- **DOI**: 10.1109/TCYB.2017.2788081
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04515v1)
- **Published**: 2017-05-12 11:23:07+00:00
- **Updated**: 2017-05-12 11:23:07+00:00
- **Authors**: Tong Zhang, Wenming Zheng, Zhen Cui, Yuan Zong, Yang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Emotion analysis is a crucial problem to endow artifact machines with real intelligence in many large potential applications. As external appearances of human emotions, electroencephalogram (EEG) signals and video face signals are widely used to track and analyze human's affective information. According to their common characteristics of spatial-temporal volumes, in this paper we propose a novel deep learning framework named spatial-temporal recurrent neural network (STRNN) to unify the learning of two different signal sources into a spatial-temporal dependency model. In STRNN, to capture those spatially cooccurrent variations of human emotions, a multi-directional recurrent neural network (RNN) layer is employed to capture longrange contextual cues by traversing the spatial region of each time slice from multiple angles. Then a bi-directional temporal RNN layer is further used to learn discriminative temporal dependencies from the sequences concatenating spatial features of each time slice produced from the spatial RNN layer. To further select those salient regions of emotion representation, we impose sparse projection onto those hidden states of spatial and temporal domains, which actually also increases the model discriminant ability because of this global consideration. Consequently, such a two-layer RNN model builds spatial dependencies as well as temporal dependencies of the input signals. Experimental results on the public emotion datasets of EEG and facial expression demonstrate the proposed STRNN method is more competitive over those state-of-the-art methods.



### Detection of irregular QRS complexes using Hermite Transform and Support Vector Machine
- **Arxiv ID**: http://arxiv.org/abs/1705.04519v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04519v1)
- **Published**: 2017-05-12 11:40:02+00:00
- **Updated**: 2017-05-12 11:40:02+00:00
- **Authors**: Zoja Vulaj, Milos Brajovic, Andjela Draganic, Irena Orovic
- **Comment**: submitted to 59th International Symposium ELMAR-2017, Zadar, Croatia
- **Journal**: None
- **Summary**: Computer based recognition and detection of abnormalities in ECG signals is proposed. For this purpose, the Support Vector Machines (SVM) are combined with the advantages of Hermite transform representation. SVM represent a special type of classification techniques commonly used in medical applications. Automatic classification of ECG could make the work of cardiologic departments faster and more efficient. It would also reduce the number of false diagnosis and, as a result, save lives. The working principle of the SVM is based on translating the data into a high dimensional feature space and separating it using a linear classificator. In order to provide an optimal representation for SVM application, the Hermite transform domain is used. This domain is proved to be suitable because of the similarity of the QRS complex with Hermite basis functions. The maximal signal information is obtained using a small set of features that are used for detection of irregular QRS complexes. The aim of the paper is to show that these features can be employed for automatic ECG signal analysis.



### Self-Committee Approach for Image Restoration Problems using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1705.04528v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04528v2)
- **Published**: 2017-05-12 12:10:52+00:00
- **Updated**: 2017-06-12 01:16:05+00:00
- **Authors**: Byeongyong Ahn, Nam Ik Cho
- **Comment**: 4 pages, 5 figures
- **Journal**: None
- **Summary**: There have been many discriminative learning methods using convolutional neural networks (CNN) for several image restoration problems, which learn the mapping function from a degraded input to the clean output. In this letter, we propose a self-committee method that can find enhanced restoration results from the multiple trial of a trained CNN with different but related inputs. Specifically, it is noted that the CNN sometimes finds different mapping functions when the input is transformed by a reversible transform and thus produces different but related outputs with the original. Hence averaging the outputs for several different transformed inputs can enhance the results as evidenced by the network committee methods. Unlike the conventional committee approaches that require several networks, the proposed method needs only a single network. Experimental results show that adding an additional transform as a committee always brings additional gain on image denoising and single image supre-resolution problems.



### Towards a Principled Integration of Multi-Camera Re-Identification and Tracking through Optimal Bayes Filters
- **Arxiv ID**: http://arxiv.org/abs/1705.04608v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04608v2)
- **Published**: 2017-05-12 14:50:15+00:00
- **Updated**: 2017-05-16 10:07:56+00:00
- **Authors**: Lucas Beyer, Stefan Breuers, Vitaly Kurin, Bastian Leibe
- **Comment**: First two authors have equal contribution. This is initial work into
  a new direction, not a benchmark-beating method. v2 only adds
  acknowledgements and fixes a typo in e-mail
- **Journal**: None
- **Summary**: With the rise of end-to-end learning through deep learning, person detectors and re-identification (ReID) models have recently become very strong. Multi-camera multi-target (MCMT) tracking has not fully gone through this transformation yet. We intend to take another step in this direction by presenting a theoretically principled way of integrating ReID with tracking formulated as an optimal Bayes filter. This conveniently side-steps the need for data-association and opens up a direct path from full images to the core of the tracker. While the results are still sub-par, we believe that this new, tight integration opens many interesting research opportunities and leads the way towards full end-to-end tracking from raw pixels.



### Single Image Action Recognition by Predicting Space-Time Saliency
- **Arxiv ID**: http://arxiv.org/abs/1705.04641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04641v1)
- **Published**: 2017-05-12 16:03:33+00:00
- **Updated**: 2017-05-12 16:03:33+00:00
- **Authors**: Marjaneh Safaei, Hassan Foroosh
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach based on deep Convolutional Neural Networks (CNN) to recognize human actions in still images by predicting the future motion, and detecting the shape and location of the salient parts of the image. We make the following major contributions to this important area of research: (i) We use the predicted future motion in the static image (Walker et al., 2015) as a means of compensating for the missing temporal information, while using the saliency map to represent the the spatial information in the form of location and shape of what is predicted as significant. (ii) We cast action classification in static images as a domain adaptation problem by transfer learning. We first map the input static image to a new domain that we refer to as the Predicted Optical Flow-Saliency Map domain (POF-SM), and then fine-tune the layers of a deep CNN model trained on classifying the ImageNet dataset to perform action classification in the POF-SM domain. (iii) We tested our method on the popular Willow dataset. But unlike existing methods, we also tested on a more realistic and challenging dataset of over 2M still images that we collected and labeled by taking random frames from the UCF-101 video dataset. We call our dataset the UCF Still Image dataset or UCFSI-101 in short. Our results outperform the state of the art.



### Deep Learning Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1705.04709v1
- **DOI**: 10.1364/OPTICA.4.001437
- **Categories**: **cs.LG**, cs.CV, physics.optics, 68T01, 68T05, 68U10, 62M45, 78M32, 92C50, 92C55, 94A08, I.2; I.2.1; I.2.6; I.2.10; I.3; I.3.3; I.4.3; I.4.4; I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1705.04709v1)
- **Published**: 2017-05-12 18:22:54+00:00
- **Updated**: 2017-05-12 18:22:54+00:00
- **Authors**: Yair Rivenson, Zoltan Gorocs, Harun Gunaydin, Yibo Zhang, Hongda Wang, Aydogan Ozcan
- **Comment**: None
- **Journal**: Optica, Vol. 4, Issue 11, pp. 1437-1443 (2017)
- **Summary**: We demonstrate that a deep neural network can significantly improve optical microscopy, enhancing its spatial resolution over a large field-of-view and depth-of-field. After its training, the only input to this network is an image acquired using a regular optical microscope, without any changes to its design. We blindly tested this deep learning approach using various tissue samples that are imaged with low-resolution and wide-field systems, where the network rapidly outputs an image with remarkably better resolution, matching the performance of higher numerical aperture lenses, also significantly surpassing their limited field-of-view and depth-of-field. These results are transformative for various fields that use microscopy tools, including e.g., life sciences, where optical microscopy is considered as one of the most widely used and deployed techniques. Beyond such applications, our presented approach is broadly applicable to other imaging modalities, also spanning different parts of the electromagnetic spectrum, and can be used to design computational imagers that get better and better as they continue to image specimen and establish new transformations among different modes of imaging.



### Person Re-Identification by Deep Joint Learning of Multi-Loss Classification
- **Arxiv ID**: http://arxiv.org/abs/1705.04724v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1705.04724v2)
- **Published**: 2017-05-12 19:18:07+00:00
- **Updated**: 2017-05-23 01:04:56+00:00
- **Authors**: Wei Li, Xiatian Zhu, Shaogang Gong
- **Comment**: Accepted by IJCAI 2017
- **Journal**: None
- **Summary**: Existing person re-identification (re-id) methods rely mostly on either localised or global feature representation alone. This ignores their joint benefit and mutual complementary effects. In this work, we show the advantages of jointly learning local and global features in a Convolutional Neural Network (CNN) by aiming to discover correlated local and global features in different context. Specifically, we formulate a method for joint learning of local and global feature selection losses designed to optimise person re-id when using only generic matching metrics such as the L2 distance. We design a novel CNN architecture for Jointly Learning Multi-Loss (JLML) of local and global discriminative feature optimisation subject concurrently to the same re-id labelled information. Extensive comparative evaluations demonstrate the advantages of this new JLML model for person re-id over a wide range of state-of-the-art re-id methods on five benchmarks (VIPeR, GRID, CUHK01, CUHK03, Market-1501).



### Gabor Filter Assisted Energy Efficient Fast Learning Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.04748v1
- **DOI**: 10.1109/ISLPED.2017.8009202
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.04748v1)
- **Published**: 2017-05-12 20:50:51+00:00
- **Updated**: 2017-05-12 20:50:51+00:00
- **Authors**: Syed Shakib Sarwar, Priyadarshini Panda, Kaushik Roy
- **Comment**: Accepted in ISLPED 2017
- **Journal**: EEE/ACM International Symposium on Low Power Electronics and
  Design (ISLPED), Taipei, 2017, pp. 1-6
- **Summary**: Convolutional Neural Networks (CNN) are being increasingly used in computer vision for a wide range of classification and recognition problems. However, training these large networks demands high computational time and energy requirements; hence, their energy-efficient implementation is of great interest. In this work, we reduce the training complexity of CNNs by replacing certain weight kernels of a CNN with Gabor filters. The convolutional layers use the Gabor filters as fixed weight kernels, which extracts intrinsic features, with regular trainable weight kernels. This combination creates a balanced system that gives better training performance in terms of energy and time, compared to the standalone CNN (without any Gabor kernels), in exchange for tolerable accuracy degradation. We show that the accuracy degradation can be mitigated by partially training the Gabor kernels, for a small fraction of the total training cycles. We evaluated the proposed approach on 4 benchmark applications. Simple tasks like face detection and character recognition (MNIST and TiCH), were implemented using LeNet architecture. While a more complex task of object recognition (CIFAR10) was implemented on a state of the art deep CNN (Network in Network) architecture. The proposed approach yields 1.31-1.53x improvement in training energy in comparison to conventional CNN implementation. We also obtain improvement up to 1.4x in training time, up to 2.23x in storage requirements, and up to 2.2x in memory access energy. The accuracy degradation suffered by the approximate implementations is within 0-3% of the baseline.



