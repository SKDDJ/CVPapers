# Arxiv Papers in cs.CV on 2017-05-22
### Classification and Retrieval of Digital Pathology Scans: A New Dataset
- **Arxiv ID**: http://arxiv.org/abs/1705.07522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07522v1)
- **Published**: 2017-05-22 00:00:18+00:00
- **Updated**: 2017-05-22 00:00:18+00:00
- **Authors**: Morteza Babaie, Shivam Kalra, Aditya Sriram, Christopher Mitcheltree, Shujin Zhu, Amin Khatami, Shahryar Rahnamayan, H. R. Tizhoosh
- **Comment**: Accepted for presentation at Workshop for Computer Vision for
  Microscopy Image Analysis (CVMI 2017) @ CVPR 2017, Honolulu, Hawaii
- **Journal**: None
- **Summary**: In this paper, we introduce a new dataset, \textbf{Kimia Path24}, for image classification and retrieval in digital pathology. We use the whole scan images of 24 different tissue textures to generate 1,325 test patches of size 1000$\times$1000 (0.5mm$\times$0.5mm). Training data can be generated according to preferences of algorithm designer and can range from approximately 27,000 to over 50,000 patches if the preset parameters are adopted. We propose a compound patch-and-scan accuracy measurement that makes achieving high accuracies quite challenging. In addition, we set the benchmarking line by applying LBP, dictionary approach and convolutional neural nets (CNNs) and report their results. The highest accuracy was 41.80\% for CNN.



### Building Emotional Machines: Recognizing Image Emotions through Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.07543v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1705.07543v2)
- **Published**: 2017-05-22 02:56:23+00:00
- **Updated**: 2017-07-03 07:56:49+00:00
- **Authors**: Hye-Rin Kim, Yeong-Seok Kim, Seon Joo Kim, In-Kwon Lee
- **Comment**: 11 pages, 15 figures
- **Journal**: None
- **Summary**: An image is a very effective tool for conveying emotions. Many researchers have investigated in computing the image emotions by using various features extracted from images. In this paper, we focus on two high level features, the object and the background, and assume that the semantic information of images is a good cue for predicting emotion. An object is one of the most important elements that define an image, and we find out through experiments that there is a high correlation between the object and the emotion in images. Even with the same object, there may be slight difference in emotion due to different backgrounds, and we use the semantic information of the background to improve the prediction performance. By combining the different levels of features, we build an emotion based feed forward deep neural network which produces the emotion values of a given image. The output emotion values in our framework are continuous values in the 2-dimensional space (Valence and Arousal), which are more effective than using a few number of emotion categories in describing emotions. Experiments confirm the effectiveness of our network in predicting the emotion of images.



### Boosting the accuracy of multi-spectral image pan-sharpening by learning a deep residual network
- **Arxiv ID**: http://arxiv.org/abs/1705.07556v2
- **DOI**: 10.1109/LGRS.2017.2736020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07556v2)
- **Published**: 2017-05-22 05:13:15+00:00
- **Updated**: 2017-05-23 12:16:42+00:00
- **Authors**: Yancong Wei, Qiangqiang Yuan, Huanfeng Shen, Liangpei Zhang
- **Comment**: 5 pages,5 figures, 1 table
- **Journal**: None
- **Summary**: In the field of fusing multi-spectral and panchromatic images (Pan-sharpening), the impressive effectiveness of deep neural networks has been recently employed to overcome the drawbacks of traditional linear models and boost the fusing accuracy. However, to the best of our knowledge, existing research works are mainly based on simple and flat networks with relatively shallow architecture, which severely limited their performances. In this paper, the concept of residual learning has been introduced to form a very deep convolutional neural network to make a full use of the high non-linearity of deep learning models. By both quantitative and visual assessments on a large number of high quality multi-spectral images from various sources, it has been supported that our proposed model is superior to all mainstream algorithms included in the comparison, and achieved the highest spatial-spectral unified accuracy.



### Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon
- **Arxiv ID**: http://arxiv.org/abs/1705.07565v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.07565v2)
- **Published**: 2017-05-22 05:54:37+00:00
- **Updated**: 2017-11-09 23:50:55+00:00
- **Authors**: Xin Dong, Shangyu Chen, Sinno Jialin Pan
- **Comment**: None
- **Journal**: None
- **Summary**: How to develop slim and accurate deep neural networks has become crucial for real- world applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. Therefore, there is a guarantee that one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods.



### An Invariant Model of the Significance of Different Body Parts in Recognizing Different Actions
- **Arxiv ID**: http://arxiv.org/abs/1705.08293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08293v1)
- **Published**: 2017-05-22 06:03:07+00:00
- **Updated**: 2017-05-22 06:03:07+00:00
- **Authors**: Yuping Shen, Hassan Foroosh
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1705.04641,
  arXiv:1705.05741, arXiv:1705.04433
- **Journal**: None
- **Summary**: In this paper, we show that different body parts do not play equally important roles in recognizing a human action in video data. We investigate to what extent a body part plays a role in recognition of different actions and hence propose a generic method of assigning weights to different body points. The approach is inspired by the strong evidence in the applied perception community that humans perform recognition in a foveated manner, that is they recognize events or objects by only focusing on visually significant aspects. An important contribution of our method is that the computation of the weights assigned to body parts is invariant to viewing directions and camera parameters in the input data. We have performed extensive experiments to validate the proposed approach and demonstrate its significance. In particular, results show that considerable improvement in performance is gained by taking into account the relative importance of different body parts as defined by our approach.



### Learning Robust Object Recognition Using Composed Scenes from Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1705.07594v1
- **DOI**: 10.1109/CRV.2017.42
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07594v1)
- **Published**: 2017-05-22 07:50:24+00:00
- **Updated**: 2017-05-22 07:50:24+00:00
- **Authors**: Hao Wang, Xingyu Lin, Yimeng Zhang, Tai Sing Lee
- **Comment**: Accepted by 14th Conference on Computer and Robot Vision
- **Journal**: None
- **Summary**: Recurrent feedback connections in the mammalian visual system have been hypothesized to play a role in synthesizing input in the theoretical framework of analysis by synthesis. The comparison of internally synthesized representation with that of the input provides a validation mechanism during perceptual inference and learning. Inspired by these ideas, we proposed that the synthesis machinery can compose new, unobserved images by imagination to train the network itself so as to increase the robustness of the system in novel scenarios. As a proof of concept, we investigated whether images composed by imagination could help an object recognition system to deal with occlusion, which is challenging for the current state-of-the-art deep convolutional neural networks. We fine-tuned a network on images containing objects in various occlusion scenarios, that are imagined or self-generated through a deep generator network. Trained on imagined occluded scenarios under the object persistence constraint, our network discovered more subtle and localized image features that were neglected by the original network for object classification, obtaining better separability of different object classes in the feature space. This leads to significant improvement of object recognition under occlusion for our network relative to the original network trained only on un-occluded images. In addition to providing practical benefits in object recognition under occlusion, this work demonstrates the use of self-generated composition of visual scenes through the synthesis loop, combined with the object persistence constraint, can provide opportunities for neural networks to discover new relevant patterns in the data, and become more flexible in dealing with novel situations.



### View-Invariant Recognition of Action Style Self-Dissimilarity
- **Arxiv ID**: http://arxiv.org/abs/1705.07609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07609v1)
- **Published**: 2017-05-22 08:38:19+00:00
- **Updated**: 2017-05-22 08:38:19+00:00
- **Authors**: Yuping Shen, Hassan Foroosh
- **Comment**: None
- **Journal**: None
- **Summary**: Self-similarity was recently introduced as a measure of inter-class congruence for classification of actions. Herein, we investigate the dual problem of intra-class dissimilarity for classification of action styles. We introduce self-dissimilarity matrices that discriminate between same actions performed by different subjects regardless of viewing direction and camera parameters. We investigate two frameworks using these invariant style dissimilarity measures based on Principal Component Analysis (PCA) and Fisher Discriminant Analysis (FDA). Extensive experiments performed on IXMAS dataset indicate remarkably good discriminant characteristics for the proposed invariant measures for gender recognition from video data.



### Computer vision-based food calorie estimation: dataset, method, and experiment
- **Arxiv ID**: http://arxiv.org/abs/1705.07632v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07632v3)
- **Published**: 2017-05-22 09:47:29+00:00
- **Updated**: 2017-05-24 07:48:37+00:00
- **Authors**: Yanchao Liang, Jianhua Li
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Computer vision has been introduced to estimate calories from food images. But current food image data sets don't contain volume and mass records of foods, which leads to an incomplete calorie estimation. In this paper, we present a novel food image data set with volume and mass records of foods, and a deep learning method for food detection, to make a complete calorie estimation. Our data set includes 2978 images, and every image contains corresponding each food's annotation, volume and mass records, as well as a certain calibration reference. To estimate calorie of food in the proposed data set, a deep learning method using Faster R-CNN first is put forward to detect the food. And the experiment results show our method is effective to estimate calories and our data set contains adequate information for calorie estimation. Our data set is the first released food image data set which can be used to evaluate computer vision-based calorie estimation methods.



### Dynamics Based 3D Skeletal Hand Tracking
- **Arxiv ID**: http://arxiv.org/abs/1705.07640v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, I.3.7
- **Links**: [PDF](http://arxiv.org/pdf/1705.07640v1)
- **Published**: 2017-05-22 10:01:39+00:00
- **Updated**: 2017-05-22 10:01:39+00:00
- **Authors**: Stan Melax, Leonid Keselman, Sterling Orsten
- **Comment**: Published in Graphics Interface 2013
- **Journal**: None
- **Summary**: Tracking the full skeletal pose of the hands and fingers is a challenging problem that has a plethora of applications for user interaction. Existing techniques either require wearable hardware, add restrictions to user pose, or require significant computation resources. This research explores a new approach to tracking hands, or any articulated model, by using an augmented rigid body simulation. This allows us to phrase 3D object tracking as a linear complementarity problem with a well-defined solution. Based on a depth sensor's samples, the system generates constraints that limit motion orthogonal to the rigid body model's surface. These constraints, along with prior motion, collision/contact constraints, and joint mechanics, are resolved with a projected Gauss-Seidel solver. Due to camera noise properties and attachment errors, the numerous surface constraints are impulse capped to avoid overpowering mechanical constraints. To improve tracking accuracy, multiple simulations are spawned at each frame and fed a variety of heuristics, constraints and poses. A 3D error metric selects the best-fit simulation, helping the system handle challenging hand motions. Such an approach enables real-time, robust, and accurate 3D skeletal tracking of a user's hand on a variety of depth cameras, while only utilizing a single x86 CPU core for processing.



### Semantic Softmax Loss for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1705.07692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07692v1)
- **Published**: 2017-05-22 12:26:04+00:00
- **Updated**: 2017-05-22 12:26:04+00:00
- **Authors**: Zhong Ji, Yunxin Sun, Yulong Yu, Jichang Guo, Yanwei Pang
- **Comment**: None
- **Journal**: None
- **Summary**: A typical pipeline for Zero-Shot Learning (ZSL) is to integrate the visual features and the class semantic descriptors into a multimodal framework with a linear or bilinear model. However, the visual features and the class semantic descriptors locate in different structural spaces, a linear or bilinear model can not capture the semantic interactions between different modalities well. In this letter, we propose a nonlinear approach to impose ZSL as a multi-class classification problem via a Semantic Softmax Loss by embedding the class semantic descriptors into the softmax layer of multi-class classification network. To narrow the structural differences between the visual features and semantic descriptors, we further use an L2 normalization constraint to the differences between the visual features and visual prototypes reconstructed with the semantic descriptors. The results on three benchmark datasets, i.e., AwA, CUB and SUN demonstrate the proposed approach can boost the performances steadily and achieve the state-of-the-art performance for both zero-shot classification and zero-shot retrieval.



### Anatomically Constrained Neural Networks (ACNN): Application to Cardiac Image Enhancement and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1705.08302v4
- **DOI**: 10.1109/TMI.2017.2743464
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08302v4)
- **Published**: 2017-05-22 12:32:25+00:00
- **Updated**: 2017-12-05 23:04:00+00:00
- **Authors**: Ozan Oktay, Enzo Ferrante, Konstantinos Kamnitsas, Mattias Heinrich, Wenjia Bai, Jose Caballero, Stuart Cook, Antonio de Marvao, Timothy Dawes, Declan O'Regan, Bernhard Kainz, Ben Glocker, Daniel Rueckert
- **Comment**: Published in IEEE Transactions on Medical Imaging (Aug 2017)
- **Journal**: None
- **Summary**: Incorporation of prior knowledge about organ shape and location is key to improve performance of image analysis approaches. In particular, priors can be useful in cases where images are corrupted and contain artefacts due to limitations in image acquisition. The highly constrained nature of anatomical objects can be well captured with learning based techniques. However, in most recent and promising techniques such as CNN based segmentation it is not obvious how to incorporate such prior knowledge. State-of-the-art methods operate as pixel-wise classifiers where the training objectives do not incorporate the structure and inter-dependencies of the output. To overcome this limitation, we propose a generic training strategy that incorporates anatomical prior knowledge into CNNs through a new regularisation model, which is trained end-to-end. The new framework encourages models to follow the global anatomical properties of the underlying anatomy (e.g. shape, label structure) via learned non-linear representations of the shape. We show that the proposed approach can be easily adapted to different analysis tasks (e.g. image enhancement, segmentation) and improve the prediction accuracy of the state-of-the-art models. The applicability of our approach is shown on multi-modal cardiac datasets and public benchmarks. Additionally, we demonstrate how the learned deep models of 3D shapes can be interpreted and used as biomarkers for classification of cardiac pathologies.



### Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset
- **Arxiv ID**: http://arxiv.org/abs/1705.07750v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.07750v3)
- **Published**: 2017-05-22 13:57:53+00:00
- **Updated**: 2018-02-12 17:10:11+00:00
- **Authors**: Joao Carreira, Andrew Zisserman
- **Comment**: Removed references to mini-kinetics dataset that was never made
  publicly available and repeated all experiments on the full Kinetics dataset
- **Journal**: None
- **Summary**: The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics.   We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101.



### Learning to Associate Words and Images Using a Large-scale Graph
- **Arxiv ID**: http://arxiv.org/abs/1705.07768v1
- **DOI**: 10.1109/CRV.2017.52
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07768v1)
- **Published**: 2017-05-22 14:24:08+00:00
- **Updated**: 2017-05-22 14:24:08+00:00
- **Authors**: Heqing Ya, Haonan Sun, Jeffrey Helt, Tai Sing Lee
- **Comment**: 8 pages, 7 figures, 14th Conference on Computer and Robot Vision 2017
- **Journal**: None
- **Summary**: We develop an approach for unsupervised learning of associations between co-occurring perceptual events using a large graph. We applied this approach to successfully solve the image captcha of China's railroad system. The approach is based on the principle of suspicious coincidence. In this particular problem, a user is presented with a deformed picture of a Chinese phrase and eight low-resolution images. They must quickly select the relevant images in order to purchase their train tickets. This problem presents several challenges: (1) the teaching labels for both the Chinese phrases and the images were not available for supervised learning, (2) no pre-trained deep convolutional neural networks are available for recognizing these Chinese phrases or the presented images, and (3) each captcha must be solved within a few seconds. We collected 2.6 million captchas, with 2.6 million deformed Chinese phrases and over 21 million images. From these data, we constructed an association graph, composed of over 6 million vertices, and linked these vertices based on co-occurrence information and feature similarity between pairs of images. We then trained a deep convolutional neural network to learn a projection of the Chinese phrases onto a 230-dimensional latent space. Using label propagation, we computed the likelihood of each of the eight images conditioned on the latent space projection of the deformed phrase for each captcha. The resulting system solved captchas with 77% accuracy in 2 seconds on average. Our work, in answering this practical challenge, illustrates the power of this class of unsupervised association learning techniques, which may be related to the brain's general strategy for associating language stimuli with visual objects on the principle of suspicious coincidence.



### Convolutional Networks with MuxOut Layers as Multi-rate Systems for Image Upscaling
- **Arxiv ID**: http://arxiv.org/abs/1705.07772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07772v1)
- **Published**: 2017-05-22 14:37:57+00:00
- **Updated**: 2017-05-22 14:37:57+00:00
- **Authors**: Pablo Navarrete Michelini, Hanwen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We interpret convolutional networks as adaptive filters and combine them with so-called MuxOut layers to efficiently upscale low resolution images. We formalize this interpretation by deriving a linear and space-variant structure of a convolutional network when its activations are fixed. We introduce general purpose algorithms to analyze a network and show its overall filter effect for each given location. We use this analysis to evaluate two types of image upscalers: deterministic upscalers that target the recovery of details from original content; and second, a new generation of upscalers that can sample the distribution of upscale aliases (images that share the same downscale version) that look like real content.



### Robust Localized Multi-view Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1705.07777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07777v1)
- **Published**: 2017-05-22 14:44:18+00:00
- **Updated**: 2017-05-22 14:44:18+00:00
- **Authors**: Yanbo Fan, Jian Liang, Ran He, Bao-Gang Hu, Siwei Lyu
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: In multi-view clustering, different views may have different confidence levels when learning a consensus representation. Existing methods usually address this by assigning distinctive weights to different views. However, due to noisy nature of real-world applications, the confidence levels of samples in the same view may also vary. Thus considering a unified weight for a view may lead to suboptimal solutions. In this paper, we propose a novel localized multi-view subspace clustering model that considers the confidence levels of both views and samples. By assigning weight to each sample under each view properly, we can obtain a robust consensus representation via fusing the noiseless structures among views and samples. We further develop a regularizer on weight parameters based on the convex conjugacy theory, and samples weights are determined in an adaptive manner. An efficient iterative algorithm is developed with a convergence guarantee. Experimental results on four benchmarks demonstrate the correctness and effectiveness of the proposed model.



### Optimal Multi-Object Segmentation with Novel Gradient Vector Flow Based Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/1705.10311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10311v1)
- **Published**: 2017-05-22 15:33:39+00:00
- **Updated**: 2017-05-22 15:33:39+00:00
- **Authors**: Junjie Bai, Abhay Shah, Xiaodong Wu
- **Comment**: Paper in review
- **Journal**: None
- **Summary**: Shape priors have been widely utilized in medical image segmentation to improve segmentation accuracy and robustness. A major way to encode such a prior shape model is to use a mesh representation, which is prone to causing self-intersection or mesh folding. Those problems require complex and expensive algorithms to mitigate. In this paper, we propose a novel shape prior directly embedded in the voxel grid space, based on gradient vector flows of a pre-segmentation. The flexible and powerful prior shape representation is ready to be extended to simultaneously segmenting multiple interacting objects with minimum separation distance constraint. The problem is formulated as a Markov random field problem whose exact solution can be efficiently computed with a single minimum s-t cut in an appropriately constructed graph. The proposed algorithm is validated on two multi-object segmentation applications: the brain tissue segmentation in MRI images, and the bladder/prostate segmentation in CT images. Both sets of experiments show superior or competitive performance of the proposed method to other state-of-the-art methods.



### TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1705.07818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07818v1)
- **Published**: 2017-05-22 15:55:08+00:00
- **Updated**: 2017-05-22 15:55:08+00:00
- **Authors**: Li Ding, Chenliang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years. It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing. In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage. Our model is simple but extremely effective in terms of video sequence labeling. The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.



### Regularizing deep networks using efficient layerwise adversarial training
- **Arxiv ID**: http://arxiv.org/abs/1705.07819v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.07819v2)
- **Published**: 2017-05-22 15:55:42+00:00
- **Updated**: 2018-05-29 02:27:51+00:00
- **Authors**: Swami Sankaranarayanan, Arpit Jain, Rama Chellappa, Ser Nam Lim
- **Comment**: Published at the Thirty-Second AAAI Conference on Artificial
  Intelligence (AAAI-18). Official link:
  https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16634
- **Journal**: None
- **Summary**: Adversarial training has been shown to regularize deep neural networks in addition to increasing their robustness to adversarial examples. However, its impact on very deep state of the art networks has not been fully investigated. In this paper, we present an efficient approach to perform adversarial training by perturbing intermediate layer activations and study the use of such perturbations as a regularizer during training. We use these perturbations to train very deep models such as ResNets and show improvement in performance both on adversarial and original test data. Our experiments highlight the benefits of perturbing intermediate layer activations compared to perturbing only the inputs. The results on CIFAR-10 and CIFAR-100 datasets show the merits of the proposed adversarial training approach. Additional results on WideResNets show that our approach provides significant improvement in classification accuracy for a given base model, outperforming dropout and other base models of larger size.



### Stabilizing GAN Training with Multiple Random Projections
- **Arxiv ID**: http://arxiv.org/abs/1705.07831v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.07831v2)
- **Published**: 2017-05-22 16:23:26+00:00
- **Updated**: 2018-06-23 00:53:46+00:00
- **Authors**: Behnam Neyshabur, Srinadh Bhojanapalli, Ayan Chakrabarti
- **Comment**: None
- **Journal**: None
- **Summary**: Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space. The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training. In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training. Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.



### DepthCut: Improved Depth Edge Estimation Using Multiple Unreliable Channels
- **Arxiv ID**: http://arxiv.org/abs/1705.07844v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07844v2)
- **Published**: 2017-05-22 16:48:15+00:00
- **Updated**: 2017-05-26 14:21:54+00:00
- **Authors**: Paul Guerrero, Holger Winnemöller, Wilmot Li, Niloy J. Mitra
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: In the context of scene understanding, a variety of methods exists to estimate different information channels from mono or stereo images, including disparity, depth, and normals. Although several advances have been reported in the recent years for these tasks, the estimated information is often imprecise particularly near depth discontinuities or creases. Studies have however shown that precisely such depth edges carry critical cues for the perception of shape, and play important roles in tasks like depth-based segmentation or foreground selection. Unfortunately, the currently extracted channels often carry conflicting signals, making it difficult for subsequent applications to effectively use them. In this paper, we focus on the problem of obtaining high-precision depth edges (i.e., depth contours and creases) by jointly analyzing such unreliable information channels. We propose DepthCut, a data-driven fusion of the channels using a convolutional neural network trained on a large dataset with known depth. The resulting depth edges can be used for segmentation, decomposing a scene into depth layers with relatively flat depth, or improving the accuracy of the depth estimate near depth edges by constraining its gradients to agree with these edges. Quantitatively, we compare against 15 variants of baselines and demonstrate that our depth edges result in an improved segmentation performance and an improved depth estimate near depth edges compared to data-agnostic channel fusion. Qualitatively, we demonstrate that the depth edges result in superior segmentation and depth orderings.



### Facial Expression Recognition Using Enhanced Deep 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.07871v1
- **DOI**: 10.1109/CVPRW.2017.282
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07871v1)
- **Published**: 2017-05-22 17:31:46+00:00
- **Updated**: 2017-05-22 17:31:46+00:00
- **Authors**: Behzad Hasani, Mohammad H. Mahoor
- **Comment**: To appear in 2017 IEEE Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)
- **Journal**: 2017 IEEE Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW)
- **Summary**: Deep Neural Networks (DNNs) have shown to outperform traditional methods in various visual recognition tasks including Facial Expression Recognition (FER). In spite of efforts made to improve the accuracy of FER systems using DNN, existing methods still are not generalizable enough in practical applications. This paper proposes a 3D Convolutional Neural Network method for FER in videos. This new network architecture consists of 3D Inception-ResNet layers followed by an LSTM unit that together extracts the spatial relations within facial images as well as the temporal relations between different frames in the video. Facial landmark points are also used as inputs to our network which emphasize on the importance of facial components rather than the facial regions that may not contribute significantly to generating facial expressions. Our proposed method is evaluated using four publicly available databases in subject-independent and cross-database tasks and outperforms state-of-the-art methods.



### Facial Affect Estimation in the Wild Using Deep Residual and Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.07884v1
- **DOI**: 10.1109/CVPRW.2017.245
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07884v1)
- **Published**: 2017-05-22 17:59:10+00:00
- **Updated**: 2017-05-22 17:59:10+00:00
- **Authors**: Behzad Hasani, Mohammad H. Mahoor
- **Comment**: To appear in 2017 IEEE Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)
- **Journal**: 2017 IEEE Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW)
- **Summary**: Automated affective computing in the wild is a challenging task in the field of computer vision. This paper presents three neural network-based methods proposed for the task of facial affect estimation submitted to the First Affect-in-the-Wild challenge. These methods are based on Inception-ResNet modules redesigned specifically for the task of facial affect estimation. These methods are: Shallow Inception-ResNet, Deep Inception-ResNet, and Inception-ResNet with LSTMs. These networks extract facial features in different scales and simultaneously estimate both the valence and arousal in each frame. Root Mean Square Error (RMSE) rates of 0.4 and 0.3 are achieved for the valence and arousal respectively with corresponding Concordance Correlation Coefficient (CCC) rates of 0.04 and 0.29 using Deep Inception-ResNet method.



### Semantically Decomposing the Latent Spaces of Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.07904v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.07904v3)
- **Published**: 2017-05-22 18:00:02+00:00
- **Updated**: 2018-02-22 19:36:33+00:00
- **Authors**: Chris Donahue, Zachary C. Lipton, Akshay Balsubramani, Julian McAuley
- **Comment**: Published as a conference paper at ICLR 2018
- **Journal**: None
- **Summary**: We propose a new algorithm for training generative adversarial networks that jointly learns latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). By fixing the identity portion of the latent codes, we can generate diverse images of the same subject, and by fixing the observation portion, we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce pairs that are photorealistic, distinct, and appear to depict the same individual. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to facilitate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm's ability to generate convincing, identity-matched photographs.



### pix2code: Generating Code from a Graphical User Interface Screenshot
- **Arxiv ID**: http://arxiv.org/abs/1705.07962v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, cs.NE, 68T45, I.2.1; I.2.10; I.2.2; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1705.07962v2)
- **Published**: 2017-05-22 19:32:20+00:00
- **Updated**: 2017-09-19 11:27:47+00:00
- **Authors**: Tony Beltramelli
- **Comment**: None
- **Journal**: None
- **Summary**: Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).



### Universal 3D Wearable Fingerprint Targets: Advancing Fingerprint Reader Evaluations
- **Arxiv ID**: http://arxiv.org/abs/1705.07972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07972v1)
- **Published**: 2017-05-22 19:51:59+00:00
- **Updated**: 2017-05-22 19:51:59+00:00
- **Authors**: Joshua J. Engelsma, Sunpreet S. Arora, Anil K. Jain, Nicholas G. Paulter Jr
- **Comment**: 14 pages, 14 figures
- **Journal**: None
- **Summary**: We present the design and manufacturing of high fidelity universal 3D fingerprint targets, which can be imaged on a variety of fingerprint sensing technologies, namely capacitive, contact-optical, and contactless-optical. Universal 3D fingerprint targets enable, for the first time, not only a repeatable and controlled evaluation of fingerprint readers, but also the ability to conduct fingerprint reader interoperability studies. Fingerprint reader interoperability refers to how robust fingerprint recognition systems are to variations in the images acquired by different types of fingerprint readers. To build universal 3D fingerprint targets, we adopt a molding and casting framework consisting of (i) digital mapping of fingerprint images to a negative mold, (ii) CAD modeling a scaffolding system to hold the negative mold, (iii) fabricating the mold and scaffolding system with a high resolution 3D printer, (iv) producing or mixing a material with similar electrical, optical, and mechanical properties to that of the human finger, and (v) fabricating a 3D fingerprint target using controlled casting. Our experiments conducted with PIV and Appendix F certified optical (contact and contactless) and capacitive fingerprint readers demonstrate the usefulness of universal 3D fingerprint targets for controlled and repeatable fingerprint reader evaluations and also fingerprint reader interoperability studies.



### GP-Unet: Lesion Detection from Weak Labels with a 3D Regression Network
- **Arxiv ID**: http://arxiv.org/abs/1705.07999v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.07999v2)
- **Published**: 2017-05-22 20:55:47+00:00
- **Updated**: 2017-10-30 09:46:11+00:00
- **Authors**: Florian Dubost, Gerda Bortsova, Hieab Adams, Arfan Ikram, Wiro Niessen, Meike Vernooij, Marleen De Bruijne
- **Comment**: Article published in MICCAI 2017. We corrected a few errors from the
  first version: padding, loss, typos and update of the DOI number
- **Journal**: None
- **Summary**: We propose a novel convolutional neural network for lesion detection from weak labels. Only a single, global label per image - the lesion count - is needed for training. We train a regression network with a fully convolutional architecture combined with a global pooling layer to aggregate the 3D output into a scalar indicating the lesion count. When testing on unseen images, we first run the network to estimate the number of lesions. Then we remove the global pooling layer to compute localization maps of the size of the input image. We evaluate the proposed network on the detection of enlarged perivascular spaces in the basal ganglia in MRI. Our method achieves a sensitivity of 62% with on average 1.5 false positives per image. Compared with four other approaches based on intensity thresholding, saliency and class maps, our method has a 20% higher sensitivity.



### Pairwise Confusion for Fine-Grained Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/1705.08016v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08016v3)
- **Published**: 2017-05-22 21:44:38+00:00
- **Updated**: 2018-07-25 18:35:21+00:00
- **Authors**: Abhimanyu Dubey, Otkrist Gupta, Pei Guo, Ramesh Raskar, Ryan Farrell, Nikhil Naik
- **Comment**: Camera-Ready version for ECCV 2018
- **Journal**: None
- **Summary**: Fine-Grained Visual Classification (FGVC) datasets contain small sample sizes, along with significant intra-class variation and inter-class similarity. While prior work has addressed intra-class variation using localization and segmentation techniques, inter-class similarity may also affect feature learning and reduce classification performance. In this work, we address this problem using a novel optimization procedure for the end-to-end neural network training on FGVC tasks. Our procedure, called Pairwise Confusion (PC) reduces overfitting by intentionally {introducing confusion} in the activations. With PC regularization, we obtain state-of-the-art performance on six of the most widely-used FGVC datasets and demonstrate improved localization ability. {PC} is easy to implement, does not need excessive hyperparameter tuning during training, and does not add significant overhead during test time.



### Unrolled Optimization with Deep Priors
- **Arxiv ID**: http://arxiv.org/abs/1705.08041v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08041v2)
- **Published**: 2017-05-22 23:24:29+00:00
- **Updated**: 2018-12-18 22:03:24+00:00
- **Authors**: Steven Diamond, Vincent Sitzmann, Felix Heide, Gordon Wetzstein
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: A broad class of problems at the core of computational imaging, sensing, and low-level computer vision reduces to the inverse problem of extracting latent images that follow a prior distribution, from measurements taken under a known physical image formation model. Traditionally, hand-crafted priors along with iterative optimization methods have been used to solve such problems. In this paper we present unrolled optimization with deep priors, a principled framework for infusing knowledge of the image formation into deep networks that solve inverse problems in imaging, inspired by classical iterative methods. We show that instances of the framework outperform the state-of-the-art by a substantial margin for a wide variety of imaging problems, such as denoising, deblurring, and compressed sensing magnetic resonance imaging (MRI). Moreover, we conduct experiments that explain how the framework is best used and why it outperforms previous methods.



### Learning multiple visual domains with residual adapters
- **Arxiv ID**: http://arxiv.org/abs/1705.08045v5
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.08045v5)
- **Published**: 2017-05-22 23:59:23+00:00
- **Updated**: 2017-11-27 17:35:38+00:00
- **Authors**: Sylvestre-Alvise Rebuffi, Hakan Bilen, Andrea Vedaldi
- **Comment**: None
- **Journal**: None
- **Summary**: There is a growing interest in learning data representations that work well for many different types of problems and data. In this paper, we look in particular at the task of learning a single visual representation that can be successfully utilized in the analysis of very different types of images, from dog breeds to stop signs and digits. Inspired by recent work on learning networks that predict the parameters of another, we develop a tunable deep network architecture that, by means of adapter residual modules, can be steered on the fly to diverse visual domains. Our method achieves a high degree of parameter sharing while maintaining or even improving the accuracy of domain-specific representations. We also introduce the Visual Decathlon Challenge, a benchmark that evaluates the ability of representations to capture simultaneously ten very different visual domains and measures their ability to recognize well uniformly.



