# Arxiv Papers in cs.CV on 2017-05-23
### Multiple Images Recovery Using a Single Affine Transformation
- **Arxiv ID**: http://arxiv.org/abs/1705.08066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08066v1)
- **Published**: 2017-05-23 03:14:50+00:00
- **Updated**: 2017-05-23 03:14:50+00:00
- **Authors**: Bo Jiang, Chris Ding, Bin Luo
- **Comment**: None
- **Journal**: None
- **Summary**: In many real-world applications, image data often come with noises, corruptions or large errors. One approach to deal with noise image data is to use data recovery techniques which aim to recover the true uncorrupted signals from the observed noise images. In this paper, we first introduce a novel corruption recovery transformation (CRT) model which aims to recover multiple (or a collection of) corrupted images using a single affine transformation. Then, we show that the introduced CRT can be efficiently constructed through learning from training data. Once CRT is learned, we can recover the true signals from the new incoming/test corrupted images explicitly. As an application, we apply our CRT to image recognition task. Experimental results on six image datasets demonstrate that the proposed CRT model is effective in recovering noise image data and thus leads to better recognition results.



### Patchnet: Interpretable Neural Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1705.08078v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08078v4)
- **Published**: 2017-05-23 05:18:14+00:00
- **Updated**: 2018-11-29 23:44:21+00:00
- **Authors**: Adityanarayanan Radhakrishnan, Charles Durham, Ali Soylemezoglu, Caroline Uhler
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216
- **Journal**: None
- **Summary**: Understanding how a complex machine learning model makes a classification decision is essential for its acceptance in sensitive areas such as health care. Towards this end, we present PatchNet, a method that provides the features indicative of each class in an image using a tradeoff between restricting global image context and classification error. We mathematically analyze this tradeoff, demonstrate Patchnet's ability to construct sharp visual heatmap representations of the learned features, and quantitatively compare these features with features selected by domain experts by applying PatchNet to the classification of benign/malignant skin lesions from the ISBI-ISIC 2017 melanoma classification challenge.



### Visual Semantic Planning using Deep Successor Representations
- **Arxiv ID**: http://arxiv.org/abs/1705.08080v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1705.08080v2)
- **Published**: 2017-05-23 05:22:47+00:00
- **Updated**: 2017-08-15 21:13:49+00:00
- **Authors**: Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, Ali Farhadi
- **Comment**: ICCV 2017 camera ready
- **Journal**: None
- **Summary**: A crucial capability of real-world intelligent agents is their ability to plan a sequence of actions to achieve their goals in the visual world. In this work, we address the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state. Doing so entails knowledge about objects and their affordances, as well as actions and their preconditions and effects. We propose learning these through interacting with a visual and dynamic environment. Our proposed solution involves bootstrapping reinforcement learning with imitation learning. To ensure cross task generalization, we develop a deep predictive model based on successor representations. Our experimental results show near optimal results across a wide range of tasks in the challenging THOR environment.



### Universal Style Transfer via Feature Transforms
- **Arxiv ID**: http://arxiv.org/abs/1705.08086v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08086v2)
- **Published**: 2017-05-23 06:10:58+00:00
- **Updated**: 2017-11-17 18:30:43+00:00
- **Authors**: Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang
- **Comment**: Accepted by NIPS 2017
- **Journal**: None
- **Summary**: Universal style transfer aims to transfer arbitrary visual styles to content images. Existing feed-forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. In this paper, we present a simple yet effective method that tackles these limitations without training on any pre-defined styles. The key ingredient of our method is a pair of feature transforms, whitening and coloring, that are embedded to an image reconstruction network. The whitening and coloring transforms reflect a direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high-quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures via simple feature coloring.



### Towards seamless multi-view scene analysis from satellite to street-level
- **Arxiv ID**: http://arxiv.org/abs/1705.08101v1
- **DOI**: 10.1109/JPROC.2017.2684300
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08101v1)
- **Published**: 2017-05-23 07:19:52+00:00
- **Updated**: 2017-05-23 07:19:52+00:00
- **Authors**: Sébastien Lefèvre, Devis Tuia, Jan Dirk Wegner, Timothée Produit, Ahmed Samy Nassar
- **Comment**: None
- **Journal**: Proceedings of the IEEE, 105, pp. 1884-1899, 2017
- **Summary**: In this paper, we discuss and review how combined multi-view imagery from satellite to street-level can benefit scene analysis. Numerous works exist that merge information from remote sensing and images acquired from the ground for tasks like land cover mapping, object detection, or scene understanding. What makes the combination of overhead and street-level images challenging, is the strongly varying viewpoint, different scale, illumination, sensor modality and time of acquisition. Direct (dense) matching of images on a per-pixel basis is thus often impossible, and one has to resort to alternative strategies that will be discussed in this paper. We review recent works that attempt to combine images taken from the ground and overhead views for purposes like scene registration, reconstruction, or classification. Three methods that represent the wide range of potential methods and applications (change detection, image orientation, and tree cataloging) are described in detail. We show that cross-fertilization between remote sensing, computer vision and machine learning is very valuable to make the best of geographic data available from Earth Observation sensors and ground imagery. Despite its challenges, we believe that integrating these complementary data sources will lead to major breakthroughs in Big GeoData.



### Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1705.08106v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08106v2)
- **Published**: 2017-05-23 07:36:51+00:00
- **Updated**: 2017-06-07 11:23:40+00:00
- **Authors**: Hong Liu, Juanhui Tu, Mengyuan Liu
- **Comment**: 5 pages, 6 figures, 3 tabels
- **Journal**: None
- **Summary**: It remains a challenge to efficiently extract spatialtemporal information from skeleton sequences for 3D human action recognition. Although most recent action recognition methods are based on Recurrent Neural Networks which present outstanding performance, one of the shortcomings of these methods is the tendency to overemphasize the temporal information. Since 3D convolutional neural network(3D CNN) is a powerful tool to simultaneously learn features from both spatial and temporal dimensions through capturing the correlations between three dimensional signals, this paper proposes a novel two-stream model using 3D CNN. To our best knowledge, this is the first application of 3D CNN in skeleton-based action recognition. Our method consists of three stages. First, skeleton joints are mapped into a 3D coordinate space and then encoding the spatial and temporal information, respectively. Second, 3D CNN models are seperately adopted to extract deep features from two streams. Third, to enhance the ability of deep features to capture global relationships, we extend every stream into multitemporal version. Extensive experiments on the SmartHome dataset and the large-scale NTU RGB-D dataset demonstrate that our method outperforms most of RNN-based methods, which verify the complementary property between spatial and temporal information and the robustness to noise.



### A Multi-Armed Bandit to Smartly Select a Training Set from Big Medical Data
- **Arxiv ID**: http://arxiv.org/abs/1705.08111v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08111v2)
- **Published**: 2017-05-23 07:51:54+00:00
- **Updated**: 2017-05-29 12:50:19+00:00
- **Authors**: Benjamín Gutiérrez, Loïc Peter, Tassilo Klein, Christian Wachinger
- **Comment**: MICCAI 2017 Proceedings
- **Journal**: None
- **Summary**: With the availability of big medical image data, the selection of an adequate training set is becoming more important to address the heterogeneity of different datasets. Simply including all the data does not only incur high processing costs but can even harm the prediction. We formulate the smart and efficient selection of a training dataset from big medical image data as a multi-armed bandit problem, solved by Thompson sampling. Our method assumes that image features are not available at the time of the selection of the samples, and therefore relies only on meta information associated with the images. Our strategy simultaneously exploits data sources with high chances of yielding useful samples and explores new data regions. For our evaluation, we focus on the application of estimating the age from a brain MRI. Our results on 7,250 subjects from 10 datasets show that our approach leads to higher accuracy while only requiring a fraction of the training data.



### Look, Listen and Learn
- **Arxiv ID**: http://arxiv.org/abs/1705.08168v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.08168v2)
- **Published**: 2017-05-23 10:37:54+00:00
- **Updated**: 2017-08-01 12:04:50+00:00
- **Authors**: Relja Arandjelović, Andrew Zisserman
- **Comment**: Appears in: IEEE International Conference on Computer Vision (ICCV)
  2017
- **Journal**: None
- **Summary**: We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself -- the correspondence between the visual and the audio streams, and we introduce a novel "Audio-Visual Correspondence" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art self-supervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.



### Correlation Alignment by Riemannian Metric for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1705.08180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08180v1)
- **Published**: 2017-05-23 11:08:48+00:00
- **Updated**: 2017-05-23 11:08:48+00:00
- **Authors**: Pietro Morerio, Vittorio Murino
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation techniques address the problem of reducing the sensitivity of machine learning methods to the so-called domain shift, namely the difference between source (training) and target (test) data distributions. In particular, unsupervised domain adaptation assumes no labels are available in the target domain. To this end, aligning second order statistics (covariances) of target and source domains have proven to be an effective approach ti fill the gap between the domains. However, covariance matrices do not form a subspace of the Euclidean space, but live in a Riemannian manifold with non-positive curvature, making the usual Euclidean metric suboptimal to measure distances. In this paper, we extend the idea of training a neural network with a constraint on the covariances of the hidden layer features, by rigorously accounting for the curved structure of the manifold of symmetric positive definite matrices. The resulting loss function exploits a theoretically sound geodesic distance on such manifold. Results show indeed the suboptimal nature of the Euclidean distance. This makes us able to perform better than previous approaches on the standard Office dataset, a benchmark for domain adaptation techniques.



### Unmasking the abnormal events in video
- **Arxiv ID**: http://arxiv.org/abs/1705.08182v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08182v3)
- **Published**: 2017-05-23 11:14:17+00:00
- **Updated**: 2017-07-25 14:10:46+00:00
- **Authors**: Radu Tudor Ionescu, Sorina Smeureanu, Bogdan Alexe, Marius Popescu
- **Comment**: Accepted at the 2017 International Conference on Computer Vision
  (ICCV 2017)
- **Journal**: None
- **Summary**: We propose a novel framework for abnormal event detection in video that requires no training sequences. Our framework is based on unmasking, a technique previously used for authorship verification in text documents, which we adapt to our task. We iteratively train a binary classifier to distinguish between two consecutive video sequences while removing at each step the most discriminant features. Higher training accuracy rates of the intermediately obtained classifiers represent abnormal events. To the best of our knowledge, this is the first work to apply unmasking for a computer vision task. We compare our method with several state-of-the-art supervised and unsupervised methods on four benchmark data sets. The empirical results indicate that our abnormal event detection framework can achieve state-of-the-art results, while running in real-time at 20 frames per second.



### Salient Object Detection with Semantic Priors
- **Arxiv ID**: http://arxiv.org/abs/1705.08207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08207v1)
- **Published**: 2017-05-23 12:24:09+00:00
- **Updated**: 2017-05-23 12:24:09+00:00
- **Authors**: Tam V. Nguyen, Luoqi Liu
- **Comment**: accepted to IJCAI 2017
- **Journal**: None
- **Summary**: Salient object detection has increasingly become a popular topic in cognitive and computational sciences, including computer vision and artificial intelligence research. In this paper, we propose integrating \textit{semantic priors} into the salient object detection process. Our algorithm consists of three basic steps. Firstly, the explicit saliency map is obtained based on the semantic segmentation refined by the explicit saliency priors learned from the data. Next, the implicit saliency map is computed based on a trained model which maps the implicit saliency priors embedded into regional features with the saliency values. Finally, the explicit semantic map and the implicit map are adaptively fused to form a pixel-accurate saliency map which uniformly covers the objects of interest. We further evaluate the proposed framework on two challenging datasets, namely, ECSSD and HKUIS. The extensive experimental results demonstrate that our method outperforms other state-of-the-art methods.



### Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.08214v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1705.08214v1)
- **Published**: 2017-05-23 12:39:51+00:00
- **Updated**: 2017-05-23 12:39:51+00:00
- **Authors**: Michael Gygli
- **Comment**: None
- **Journal**: None
- **Summary**: Shot boundary detection (SBD) is an important component of many video analysis tasks, such as action recognition, video indexing, summarization and editing. Previous work typically used a combination of low-level features like color histograms, in conjunction with simple models such as SVMs. Instead, we propose to learn shot detection end-to-end, from pixels to final shot boundaries. For training such a model, we rely on our insight that all shot boundaries are generated. Thus, we create a dataset with one million frames and automatically generated transitions such as cuts, dissolves and fades. In order to efficiently analyze hours of videos, we propose a Convolutional Neural Network (CNN) which is fully convolutional in time, thus allowing to use a large temporal context without the need to repeatedly processing frames. With this architecture our method obtains state-of-the-art results while running at an unprecedented speed of more than 120x real-time.



### How hard can it be? Estimating the difficulty of visual search in an image
- **Arxiv ID**: http://arxiv.org/abs/1705.08280v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08280v1)
- **Published**: 2017-05-23 14:03:20+00:00
- **Updated**: 2017-05-23 14:03:20+00:00
- **Authors**: Radu Tudor Ionescu, Bogdan Alexe, Marius Leordeanu, Marius Popescu, Dim P. Papadopoulos, Vittorio Ferrari
- **Comment**: Published at CVPR 2016
- **Journal**: In Proceedings of CVPR, pp. 2157-2166, 2016
- **Summary**: We address the problem of estimating image difficulty defined as the human response time for solving a visual search task. We collect human annotations of image difficulty for the PASCAL VOC 2012 data set through a crowd-sourcing platform. We then analyze what human interpretable image properties can have an impact on visual search difficulty, and how accurate are those properties for predicting difficulty. Next, we build a regression model based on deep features learned with state of the art convolutional neural networks and show better results for predicting the ground-truth visual search difficulty scores produced by human annotators. Our model is able to correctly rank about 75% image pairs according to their difficulty score. We also show that our difficulty predictor generalizes well to new classes not seen during training. Finally, we demonstrate that our predicted difficulty scores are useful for weakly supervised object localization (8% improvement) and semi-supervised object classification (1% improvement).



### Fusion of Head and Full-Body Detectors for Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1705.08314v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08314v4)
- **Published**: 2017-05-23 14:29:53+00:00
- **Updated**: 2018-04-24 09:24:49+00:00
- **Authors**: Roberto Henschel, Laura Leal-Taixé, Daniel Cremers, Bodo Rosenhahn
- **Comment**: 10 pages, 4 figures; Winner of the MOT17 challenge; CVPRW 2018
- **Journal**: None
- **Summary**: In order to track all persons in a scene, the tracking-by-detection paradigm has proven to be a very effective approach. Yet, relying solely on a single detector is also a major limitation, as useful image information might be ignored. Consequently, this work demonstrates how to fuse two detectors into a tracking system. To obtain the trajectories, we propose to formulate tracking as a weighted graph labeling problem, resulting in a binary quadratic program. As such problems are NP-hard, the solution can only be approximated. Based on the Frank-Wolfe algorithm, we present a new solver that is crucial to handle such difficult problems. Evaluation on pedestrian tracking is provided for multiple scenarios, showing superior results over single detector tracking and standard QP-solvers. Finally, our tracker ranks 2nd on the MOT16 benchmark and 1st on the new MOT17 benchmark, outperforming over 90 trackers.



### Her2 Challenge Contest: A Detailed Assessment of Automated Her2 Scoring Algorithms in Whole Slide Images of Breast Cancer Tissues
- **Arxiv ID**: http://arxiv.org/abs/1705.08369v3
- **DOI**: 10.1111/his.13333
- **Categories**: **cs.CV**, cs.AI, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1705.08369v3)
- **Published**: 2017-05-23 15:36:04+00:00
- **Updated**: 2017-07-24 21:39:53+00:00
- **Authors**: Talha Qaiser, Abhik Mukherjee, Chaitanya Reddy Pb, Sai Dileep Munugoti, Vamsi Tallam, Tomi Pitkäaho, Taina Lehtimäki, Thomas Naughton, Matt Berseth, Aníbal Pedraza, Ramakrishnan Mukundan, Matthew Smith, Abhir Bhalerao, Erik Rodner, Marcel Simon, Joachim Denzler, Chao-Hui Huang, Gloria Bueno, David Snead, Ian Ellis, Mohammad Ilyas, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Evaluating expression of the Human epidermal growth factor receptor 2 (Her2) by visual examination of immunohistochemistry (IHC) on invasive breast cancer (BCa) is a key part of the diagnostic assessment of BCa due to its recognised importance as a predictive and prognostic marker in clinical practice. However, visual scoring of Her2 is subjective and consequently prone to inter-observer variability. Given the prognostic and therapeutic implications of Her2 scoring, a more objective method is required. In this paper, we report on a recent automated Her2 scoring contest, held in conjunction with the annual PathSoc meeting held in Nottingham in June 2016, aimed at systematically comparing and advancing the state-of-the-art Artificial Intelligence (AI) based automated methods for Her2 scoring. The contest dataset comprised of digitised whole slide images (WSI) of sections from 86 cases of invasive breast carcinoma stained with both Haematoxylin & Eosin (H&E) and IHC for Her2. The contesting algorithms automatically predicted scores of the IHC slides for an unseen subset of the dataset and the predicted scores were compared with the 'ground truth' (a consensus score from at least two experts). We also report on a simple Man vs Machine contest for the scoring of Her2 and show that the automated methods could beat the pathology experts on this contest dataset. This paper presents a benchmark for comparing the performance of automated algorithms for scoring of Her2. It also demonstrates the enormous potential of automated algorithms in assisting the pathologist with objective IHC scoring.



### Classification of Aerial Photogrammetric 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1705.08374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08374v1)
- **Published**: 2017-05-23 15:44:40+00:00
- **Updated**: 2017-05-23 15:44:40+00:00
- **Authors**: Carlos Becker, Nicolai Häni, Elena Rosinskaya, Emmanuel d'Angelo, Christoph Strecha
- **Comment**: ISPRS 2017
- **Journal**: None
- **Summary**: We present a powerful method to extract per-point semantic class labels from aerialphotogrammetry data. Labeling this kind of data is important for tasks such as environmental modelling, object classification and scene understanding. Unlike previous point cloud classification methods that rely exclusively on geometric features, we show that incorporating color information yields a significant increase in accuracy in detecting semantic classes. We test our classification method on three real-world photogrammetry datasets that were generated with Pix4Dmapper Pro, and with varying point densities. We show that off-the-shelf machine learning techniques coupled with our new features allow us to train highly accurate classifiers that generalize well to unseen data, processing point clouds containing 10 million points in less than 3 minutes on a desktop computer.



### Better Text Understanding Through Image-To-Text Transfer
- **Arxiv ID**: http://arxiv.org/abs/1705.08386v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.08386v2)
- **Published**: 2017-05-23 16:06:32+00:00
- **Updated**: 2017-05-26 08:08:20+00:00
- **Authors**: Karol Kurach, Sylvain Gelly, Michal Jastrzebski, Philip Haeusser, Olivier Teytaud, Damien Vincent, Olivier Bousquet
- **Comment**: None
- **Journal**: None
- **Summary**: Generic text embeddings are successfully used in a variety of tasks. However, they are often learnt by capturing the co-occurrence structure from pure text corpora, resulting in limitations of their ability to generalize. In this paper, we explore models that incorporate visual information into the text representation. Based on comprehensive ablation studies, we propose a conceptually simple, yet well performing architecture. It outperforms previous multimodal approaches on a set of well established benchmarks. We also improve the state-of-the-art results for image-related text datasets, using orders of magnitude less data.



### AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions
- **Arxiv ID**: http://arxiv.org/abs/1705.08421v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08421v4)
- **Published**: 2017-05-23 17:11:46+00:00
- **Updated**: 2018-04-30 17:45:11+00:00
- **Authors**: Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, Jitendra Malik
- **Comment**: To appear in CVPR 2018. Check dataset page
  https://research.google.com/ava/ for details
- **Journal**: None
- **Summary**: This paper introduces a video dataset of spatio-temporally localized Atomic Visual Actions (AVA). The AVA dataset densely annotates 80 atomic visual actions in 430 15-minute video clips, where actions are localized in space and time, resulting in 1.58M action labels with multiple labels per person occurring frequently. The key characteristics of our dataset are: (1) the definition of atomic visual actions, rather than composite actions; (2) precise spatio-temporal annotations with possibly multiple annotations for each person; (3) exhaustive annotation of these atomic actions over 15-minute video clips; (4) people temporally linked across consecutive segments; and (5) using movies to gather a varied set of action representations. This departs from existing datasets for spatio-temporal action recognition, which typically provide sparse annotations for composite actions in short video clips. We will release the dataset publicly.   AVA, with its realistic scene and action complexity, exposes the intrinsic difficulty of action recognition. To benchmark this, we present a novel approach for action localization that builds upon the current state-of-the-art methods, and demonstrates better performance on JHMDB and UCF101-24 categories. While setting a new state of the art on existing datasets, the overall results on AVA are low at 15.6% mAP, underscoring the need for developing new approaches for video understanding.



### Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1705.08475v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.08475v2)
- **Published**: 2017-05-23 18:48:20+00:00
- **Updated**: 2017-11-05 20:58:09+00:00
- **Authors**: Matthias Hein, Maksym Andriushchenko
- **Comment**: final version accepted at NIPS 2017, fixed bug in implementation of
  Cross-Lipschitz regularization and lower bound computation, now results are
  better
- **Journal**: None
- **Summary**: Recent work has shown that state-of-the-art classifiers are quite brittle, in the sense that a small adversarial change of an originally with high confidence correctly classified input leads to a wrong classification again with high confidence. This raises concerns that such classifiers are vulnerable to attacks and calls into question their usage in safety-critical systems. We show in this paper for the first time formal guarantees on the robustness of a classifier by giving instance-specific lower bounds on the norm of the input manipulation required to change the classifier decision. Based on this analysis we propose the Cross-Lipschitz regularization functional. We show that using this form of regularization in kernel methods resp. neural networks improves the robustness of the classifier without any loss in prediction performance.



### Input Fast-Forwarding for Better Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1705.08479v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08479v1)
- **Published**: 2017-05-23 18:57:08+00:00
- **Updated**: 2017-05-23 18:57:08+00:00
- **Authors**: Ahmed Ibrahim, A. Lynn Abbott, Mohamed E. Hussein
- **Comment**: Accepted in the 14th International Conference on Image Analysis and
  Recognition (ICIAR) 2017, Montreal, Canada
- **Journal**: None
- **Summary**: This paper introduces a new architectural framework, known as input fast-forwarding, that can enhance the performance of deep networks. The main idea is to incorporate a parallel path that sends representations of input values forward to deeper network layers. This scheme is substantially different from "deep supervision" in which the loss layer is re-introduced to earlier layers. The parallel path provided by fast-forwarding enhances the training process in two ways. First, it enables the individual layers to combine higher-level information (from the standard processing path) with lower-level information (from the fast-forward path). Second, this new architecture reduces the problem of vanishing gradients substantially because the fast-forwarding path provides a shorter route for gradient backpropagation. In order to evaluate the utility of the proposed technique, a Fast-Forward Network (FFNet), with 20 convolutional layers along with parallel fast-forward paths, has been created and tested. The paper presents empirical results that demonstrate improved learning capacity of FFNet due to fast-forwarding, as compared to GoogLeNet (with deep supervision) and CaffeNet, which are 4x and 18x larger in size, respectively. All of the source code and deep learning models described in this paper will be made available to the entire research community



### Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification
- **Arxiv ID**: http://arxiv.org/abs/1705.08550v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1705.08550v1)
- **Published**: 2017-05-23 22:16:20+00:00
- **Updated**: 2017-05-23 22:16:20+00:00
- **Authors**: Wentao Zhu, Qi Lou, Yeeleng Scott Vang, Xiaohui Xie
- **Comment**: MICCAI 2017 Camera Ready
- **Journal**: None
- **Summary**: Mammogram classification is directly related to computer-aided diagnosis of breast cancer. Traditional methods rely on regions of interest (ROIs) which require great efforts to annotate. Inspired by the success of using deep convolutional features for natural image analysis and multi-instance learning (MIL) for labeling a set of instances/patches, we propose end-to-end trained deep multi-instance networks for mass classification based on whole mammogram without the aforementioned ROIs. We explore three different schemes to construct deep multi-instance networks for whole mammogram classification. Experimental results on the INbreast dataset demonstrate the robustness of proposed networks compared to previous work using segmentation and detection annotations.



### Hashing as Tie-Aware Learning to Rank
- **Arxiv ID**: http://arxiv.org/abs/1705.08562v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.08562v4)
- **Published**: 2017-05-23 23:42:46+00:00
- **Updated**: 2018-10-09 20:37:00+00:00
- **Authors**: Kun He, Fatih Cakir, Sarah Adel Bargal, Stan Sclaroff
- **Comment**: 15 pages, 3 figures. IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2018
- **Journal**: None
- **Summary**: Hashing, or learning binary embeddings of data, is frequently used in nearest neighbor retrieval. In this paper, we develop learning to rank formulations for hashing, aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings, and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive their continuous relaxations, and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.



