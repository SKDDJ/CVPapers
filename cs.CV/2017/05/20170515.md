# Arxiv Papers in cs.CV on 2017-05-15
### AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1705.05065v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1705.05065v2)
- **Published**: 2017-05-15 04:06:22+00:00
- **Updated**: 2017-07-18 05:30:28+00:00
- **Authors**: Shital Shah, Debadeepta Dey, Chris Lovett, Ashish Kapoor
- **Comment**: Accepted for Field and Service Robotics conference 2017 (FSR 2017)
- **Journal**: None
- **Summary**: Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.



### Single Image Super-Resolution Using Multi-Scale Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1705.05084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05084v1)
- **Published**: 2017-05-15 06:38:04+00:00
- **Updated**: 2017-05-15 06:38:04+00:00
- **Authors**: Xiaoyi Jia, Xiangmin Xu, Bolun Cai, Kailing Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Methods based on convolutional neural network (CNN) have demonstrated tremendous improvements on single image super-resolution. However, the previous methods mainly restore images from one single area in the low resolution (LR) input, which limits the flexibility of models to infer various scales of details for high resolution (HR) output. Moreover, most of them train a specific model for each up-scale factor. In this paper, we propose a multi-scale super resolution (MSSR) network. Our network consists of multi-scale paths to make the HR inference, which can learn to synthesize features from different scales. This property helps reconstruct various kinds of regions in HR images. In addition, only one single model is needed for multiple up-scale factors, which is more efficient without loss of restoration quality. Experiments on four public datasets demonstrate that the proposed method achieved state-of-the-art performance with fast speed.



### Learning Semantics for Image Annotation
- **Arxiv ID**: http://arxiv.org/abs/1705.05102v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05102v1)
- **Published**: 2017-05-15 07:45:10+00:00
- **Updated**: 2017-05-15 07:45:10+00:00
- **Authors**: Amara Tariq, Hassan Foroosh
- **Comment**: None
- **Journal**: None
- **Summary**: Image search and retrieval engines rely heavily on textual annotation in order to match word queries to a set of candidate images. A system that can automatically annotate images with meaningful text can be highly beneficial for such engines. Currently, the approaches to develop such systems try to establish relationships between keywords and visual features of images. In this paper, We make three main contributions to this area: (i) We transform this problem from the low-level keyword space to the high-level semantics space that we refer to as the "{\em image theme}", (ii) Instead of treating each possible keyword independently, we use latent Dirichlet allocation to learn image themes from the associated texts in a training phase. Images are then annotated with image themes rather than keywords, using a modified continuous relevance model, which takes into account the spatial coherence and the visual continuity among images of common theme. (iii) To achieve more coherent annotations among images of common theme, we have integrated ConceptNet in learning the semantics of images, and hence augment image descriptions beyond annotations provided by humans. Images are thus further annotated by a few most significant words of the prominent image theme. Our extensive experiments show that a coherent theme-based image annotation using high-level semantics results in improved precision and recall as compared with equivalent classical keyword annotation systems.



### Kernel Truncated Regression Representation for Robust Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1705.05108v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1705.05108v3)
- **Published**: 2017-05-15 08:16:34+00:00
- **Updated**: 2020-03-27 09:16:24+00:00
- **Authors**: Liangli Zhen, Dezhong Peng, Wei Wang, Xin Yao
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Subspace clustering aims to group data points into multiple clusters of which each corresponds to one subspace. Most existing subspace clustering approaches assume that input data lie on linear subspaces. In practice, however, this assumption usually does not hold. To achieve nonlinear subspace clustering, we propose a novel method, called kernel truncated regression representation. Our method consists of the following four steps: 1) projecting the input data into a hidden space, where each data point can be linearly represented by other data points; 2) calculating the linear representation coefficients of the data representations in the hidden space; 3) truncating the trivial coefficients to achieve robustness and block-diagonality; and 4) executing the graph cutting operation on the coefficient matrix by solving a graph Laplacian problem. Our method has the advantages of a closed-form solution and the capacity of clustering data points that lie on nonlinear subspaces. The first advantage makes our method efficient in handling large-scale datasets, and the second one enables the proposed method to conquer the nonlinear subspace clustering challenge. Extensive experiments on six benchmarks demonstrate the effectiveness and the efficiency of the proposed method in comparison with current state-of-the-art approaches.



### Tuning Modular Networks with Weighted Losses for Hand-Eye Coordination
- **Arxiv ID**: http://arxiv.org/abs/1705.05116v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1705.05116v1)
- **Published**: 2017-05-15 08:57:27+00:00
- **Updated**: 2017-05-15 08:57:27+00:00
- **Authors**: Fangyi Zhang, JÃ¼rgen Leitner, Michael Milford, Peter I. Corke
- **Comment**: 2 pages, to appear in the Deep Learning for Robotic Vision (DLRV)
  Workshop in CVPR 2017
- **Journal**: None
- **Summary**: This paper introduces an end-to-end fine-tuning method to improve hand-eye coordination in modular deep visuo-motor policies (modular networks) where each module is trained independently. Benefiting from weighted losses, the fine-tuning method significantly improves the performance of the policies for a robotic planar reaching task.



### A Perceptually Weighted Rank Correlation Indicator for Objective Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1705.05126v1
- **DOI**: 10.1109/TIP.2018.2799331
- **Categories**: **cs.CV**, I.4.0; I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/1705.05126v1)
- **Published**: 2017-05-15 09:24:05+00:00
- **Updated**: 2017-05-15 09:24:05+00:00
- **Authors**: Qingbo Wu, Hongliang Li, Fanman Meng, King N. Ngan
- **Comment**: This paper has been submitted to IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: In the field of objective image quality assessment (IQA), the Spearman's $\rho$ and Kendall's $\tau$ are two most popular rank correlation indicators, which straightforwardly assign uniform weight to all quality levels and assume each pair of images are sortable. They are successful for measuring the average accuracy of an IQA metric in ranking multiple processed images. However, two important perceptual properties are ignored by them as well. Firstly, the sorting accuracy (SA) of high quality images are usually more important than the poor quality ones in many real world applications, where only the top-ranked images would be pushed to the users. Secondly, due to the subjective uncertainty in making judgement, two perceptually similar images are usually hardly sortable, whose ranks do not contribute to the evaluation of an IQA metric. To more accurately compare different IQA algorithms, we explore a perceptually weighted rank correlation indicator in this paper, which rewards the capability of correctly ranking high quality images, and suppresses the attention towards insensitive rank mistakes. More specifically, we focus on activating `valid' pairwise comparison towards image quality, whose difference exceeds a given sensory threshold (ST). Meanwhile, each image pair is assigned an unique weight, which is determined by both the quality level and rank deviation. By modifying the perception threshold, we can illustrate the sorting accuracy with a more sophisticated SA-ST curve, rather than a single rank correlation coefficient. The proposed indicator offers a new insight for interpreting visual perception behaviors. Furthermore, the applicability of our indicator is validated in recommending robust IQA metrics for both the degraded and enhanced image data.



### Design of a Very Compact CNN Classifier for Online Handwritten Chinese Character Recognition Using DropWeight and Global Pooling
- **Arxiv ID**: http://arxiv.org/abs/1705.05207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05207v1)
- **Published**: 2017-05-15 13:18:38+00:00
- **Updated**: 2017-05-15 13:18:38+00:00
- **Authors**: Xuefeng Xiao, Yafeng Yang, Tasweer Ahmad, Lianwen Jin, Tianhai Chang
- **Comment**: 5 pages, 2 figures, 2 tables
- **Journal**: None
- **Summary**: Currently, owing to the ubiquity of mobile devices, online handwritten Chinese character recognition (HCCR) has become one of the suitable choice for feeding input to cell phones and tablet devices. Over the past few years, larger and deeper convolutional neural networks (CNNs) have extensively been employed for improving character recognition performance. However, its substantial storage requirement is a significant obstacle in deploying such networks into portable electronic devices. To circumvent this problem, we propose a novel technique called DropWeight for pruning redundant connections in the CNN architecture. It is revealed that the proposed method not only treats streamlined architectures such as AlexNet and VGGNet well but also exhibits remarkable performance for deep residual network and inception network. We also demonstrate that global pooling is a better choice for building very compact online HCCR systems. Experiments were performed on the ICDAR-2013 online HCCR competition dataset using our proposed network, and it is found that the proposed approach requires only 0.57 MB for storage, whereas state-of-the-art CNN-based methods require up to 135 MB; meanwhile the performance is decreased only by 0.91%.



### Distributed Algorithms for Feature Extraction Off-loading in Multi-Camera Visual Sensor Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.08252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.08252v1)
- **Published**: 2017-05-15 14:16:43+00:00
- **Updated**: 2017-05-15 14:16:43+00:00
- **Authors**: Emil Eriksson, GyÃ¶rgy DÃ¡n, Viktoria Fodor
- **Comment**: 12 pages, 7 figures, submitted to Transactions on Circuits and
  Systems for Video Technology
- **Journal**: None
- **Summary**: Real-time visual analysis tasks, like tracking and recognition, require swift execution of computationally intensive algorithms. Visual sensor networks can be enabled to perform such tasks by augmenting the sensor network with processing nodes and distributing the computational burden in a way that the cameras contend for the processing nodes while trying to minimize their task completion times. In this paper, we formulate the problem of minimizing the completion time of all camera sensors as an optimization problem. We propose algorithms for fully distributed optimization, analyze the existence of equilibrium allocations, evaluate the effect of the network topology and of the video characteristics, and the benefits of central coordination. Our results demonstrate that with sufficient information available, distributed optimization can provide low completion times, moreover predictable and stable performance can be achieved with additional, sparse central coordination.



### View-invariant Gait Recognition through Genetic Template Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1705.05273v3
- **DOI**: 10.1109/LSP.2017.2715179
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05273v3)
- **Published**: 2017-05-15 14:44:44+00:00
- **Updated**: 2017-07-03 08:46:36+00:00
- **Authors**: Ebenezer Isaac, Susan Elias, Srinivasan Rajagopalan, K. S. Easwarakumar
- **Comment**: Published in IEEE Signal Processing Letters. Received April 24, 2017,
  revised June 6, 2017, accepted June 10, 2017, published June 14, 2017
- **Journal**: IEEE Signal Processing Letters, vol. 24, no. 8, pp. 1188-1192,
  2017
- **Summary**: Template-based model-free approach provides by far the most successful solution to the gait recognition problem in literature. Recent work discusses how isolating the head and leg portion of the template increase the performance of a gait recognition system making it robust against covariates like clothing and carrying conditions. However, most involve a manual definition of the boundaries. The method we propose, the genetic template segmentation (GTS), employs the genetic algorithm to automate the boundary selection process. This method was tested on the GEI, GEnI and AEI templates. GEI seems to exhibit the best result when segmented with our approach. Experimental results depict that our approach significantly outperforms the existing implementations of view-invariant gait recognition.



### Back to RGB: 3D tracking of hands and hand-object interactions based on short-baseline stereo
- **Arxiv ID**: http://arxiv.org/abs/1705.05301v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1705.05301v1)
- **Published**: 2017-05-15 15:38:56+00:00
- **Updated**: 2017-05-15 15:38:56+00:00
- **Authors**: Paschalis Panteleris, Antonis Argyros
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel solution to the problem of 3D tracking of the articulated motion of human hand(s), possibly in interaction with other objects. The vast majority of contemporary relevant work capitalizes on depth information provided by RGBD cameras. In this work, we show that accurate and efficient 3D hand tracking is possible, even for the case of RGB stereo. A straightforward approach for solving the problem based on such input would be to first recover depth and then apply a state of the art depth-based 3D hand tracking method. Unfortunately, this does not work well in practice because the stereo-based, dense 3D reconstruction of hands is far less accurate than the one obtained by RGBD cameras. Our approach bypasses 3D reconstruction and follows a completely different route: 3D hand tracking is formulated as an optimization problem whose solution is the hand configuration that maximizes the color consistency between the two views of the hand. We demonstrate the applicability of our method for real time tracking of a single hand, of a hand manipulating an object and of two interacting hands. The method has been evaluated quantitatively on standard datasets and in comparison to relevant, state of the art RGBD-based approaches. The obtained results demonstrate that the proposed stereo-based method performs equally well to its RGBD-based competitors, and in some cases, it even outperforms them.



### Curiosity-driven Exploration by Self-supervised Prediction
- **Arxiv ID**: http://arxiv.org/abs/1705.05363v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.05363v1)
- **Published**: 2017-05-15 17:56:22+00:00
- **Updated**: 2017-05-15 17:56:22+00:00
- **Authors**: Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell
- **Comment**: In ICML 2017. Website at https://pathak22.github.io/noreward-rl/
- **Journal**: None
- **Summary**: In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/



### A Deep Learning Based 6 Degree-of-Freedom Localization Method for Endoscopic Capsule Robots
- **Arxiv ID**: http://arxiv.org/abs/1705.05435v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05435v1)
- **Published**: 2017-05-15 20:33:37+00:00
- **Updated**: 2017-05-15 20:33:37+00:00
- **Authors**: Mehmet Turan, Yasin Almalioglu, Ender Konukoglu, Metin Sitti
- **Comment**: None
- **Journal**: None
- **Summary**: We present a robust deep learning based 6 degrees-of-freedom (DoF) localization system for endoscopic capsule robots. Our system mainly focuses on localization of endoscopic capsule robots inside the GI tract using only visual information captured by a mono camera integrated to the robot. The proposed system is a 23-layer deep convolutional neural network (CNN) that is capable to estimate the pose of the robot in real time using a standard CPU. The dataset for the evaluation of the system was recorded inside a surgical human stomach model with realistic surface texture, softness, and surface liquid properties so that the pre-trained CNN architecture can be transferred confidently into a real endoscopic scenario. An average error of 7:1% and 3:4% for translation and rotation has been obtained, respectively. The results accomplished from the experiments demonstrate that a CNN pre-trained with raw 2D endoscopic images performs accurately inside the GI tract and is robust to various challenges posed by reflection distortions, lens imperfections, vignetting, noise, motion blur, low resolution, and lack of unique landmarks to track.



### A Non-Rigid Map Fusion-Based RGB-Depth SLAM Method for Endoscopic Capsule Robots
- **Arxiv ID**: http://arxiv.org/abs/1705.05444v1
- **DOI**: 10.1007/s41315-017-0036-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05444v1)
- **Published**: 2017-05-15 20:42:29+00:00
- **Updated**: 2017-05-15 20:42:29+00:00
- **Authors**: Mehmet Turan, Yasin Almalioglu, Helder Araujo, Ender Konukoglu, Metin Sitti
- **Comment**: None
- **Journal**: None
- **Summary**: In the gastrointestinal (GI) tract endoscopy field, ingestible wireless capsule endoscopy is considered as a minimally invasive novel diagnostic technology to inspect the entire GI tract and to diagnose various diseases and pathologies. Since the development of this technology, medical device companies and many groups have made significant progress to turn such passive capsule endoscopes into robotic active capsule endoscopes to achieve almost all functions of current active flexible endoscopes. However, the use of robotic capsule endoscopy still has some challenges. One such challenge is the precise localization of such active devices in 3D world, which is essential for a precise three-dimensional (3D) mapping of the inner organ. A reliable 3D map of the explored inner organ could assist the doctors to make more intuitive and correct diagnosis. In this paper, we propose to our knowledge for the first time in literature a visual simultaneous localization and mapping (SLAM) method specifically developed for endoscopic capsule robots. The proposed RGB-Depth SLAM method is capable of capturing comprehensive dense globally consistent surfel-based maps of the inner organs explored by an endoscopic capsule robot in real time. This is achieved by using dense frame-to-model camera tracking and windowed surfelbased fusion coupled with frequent model refinement through non-rigid surface deformations.



### Handwritten Urdu Character Recognition using 1-Dimensional BLSTM Classifier
- **Arxiv ID**: http://arxiv.org/abs/1705.05455v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05455v1)
- **Published**: 2017-05-15 21:13:08+00:00
- **Updated**: 2017-05-15 21:13:08+00:00
- **Authors**: Saad Bin Ahmed, Saeeda Naz, Salahuddin Swati, Muhammad Imran Razzak
- **Comment**: 10 pages, Accepted in NCA for publication
- **Journal**: None
- **Summary**: The recognition of cursive script is regarded as a subtle task in optical character recognition due to its varied representation. Every cursive script has different nature and associated challenges. As Urdu is one of cursive language that is derived from Arabic script, thats why it nearly shares the same challenges and difficulties even more harder. We can categorized Urdu and Arabic language on basis of its script they use. Urdu is mostly written in Nastaliq style whereas, Arabic follows Naskh style of writing. This paper presents new and comprehensive Urdu handwritten offline database name Urdu-Nastaliq Handwritten Dataset (UNHD). Currently, there is no standard and comprehensive Urdu handwritten dataset available publicly for researchers. The acquired dataset covers commonly used ligatures that were written by 500 writers with their natural handwriting on A4 size paper. We performed experiments using recurrent neural networks and reported a significant accuracy for handwritten Urdu character recognition.



### WordFence: Text Detection in Natural Images with Border Awareness
- **Arxiv ID**: http://arxiv.org/abs/1705.05483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05483v1)
- **Published**: 2017-05-15 23:42:59+00:00
- **Updated**: 2017-05-15 23:42:59+00:00
- **Authors**: Andrei Polzounov, Artsiom Ablavatski, Sergio Escalera, Shijian Lu, Jianfei Cai
- **Comment**: 5 pages, 2 figures, ICIP 2017
- **Journal**: None
- **Summary**: In recent years, text recognition has achieved remarkable success in recognizing scanned document text. However, word recognition in natural images is still an open problem, which generally requires time consuming post-processing steps. We present a novel architecture for individual word detection in scene images based on semantic segmentation. Our contributions are twofold: the concept of WordFence, which detects border areas surrounding each individual word and a novel pixelwise weighted softmax loss function which penalizes background and emphasizes small text regions. WordFence ensures that each word is detected individually, and the new loss function provides a strong training signal to both text and word border localization. The proposed technique avoids intensive post-processing, producing an end-to-end word detection system. We achieve superior localization recall on common benchmark datasets - 92% recall on ICDAR11 and ICDAR13 and 63% recall on SVT. Furthermore, our end-to-end word recognition system achieves state-of-the-art 86% F-Score on ICDAR13.



