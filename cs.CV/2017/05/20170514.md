# Arxiv Papers in cs.CV on 2017-05-14
### Volumetric Super-Resolution of Multispectral Data
- **Arxiv ID**: http://arxiv.org/abs/1705.05745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05745v1)
- **Published**: 2017-05-14 03:53:16+00:00
- **Updated**: 2017-05-14 03:53:16+00:00
- **Authors**: Vildan Atalay Aydin, Hassan Foroosh
- **Comment**: arXiv admin note: text overlap with arXiv:1705.01258
- **Journal**: None
- **Summary**: Most multispectral remote sensors (e.g. QuickBird, IKONOS, and Landsat 7 ETM+) provide low-spatial high-spectral resolution multispectral (MS) or high-spatial low-spectral resolution panchromatic (PAN) images, separately. In order to reconstruct a high-spatial/high-spectral resolution multispectral image volume, either the information in MS and PAN images are fused (i.e. pansharpening) or super-resolution reconstruction (SRR) is used with only MS images captured on different dates. Existing methods do not utilize temporal information of MS and high spatial resolution of PAN images together to improve the resolution. In this paper, we propose a multiframe SRR algorithm using pansharpened MS images, taking advantage of both temporal and spatial information available in multispectral imagery, in order to exceed spatial resolution of given PAN images. We first apply pansharpening to a set of multispectral images and their corresponding PAN images captured on different dates. Then, we use the pansharpened multispectral images as input to the proposed wavelet-based multiframe SRR method to yield full volumetric SRR. The proposed SRR method is obtained by deriving the subband relations between multitemporal MS volumes. We demonstrate the results on Landsat 7 ETM+ images comparing our method to conventional techniques.



### Spatial-Temporal Union of Subspaces for Multi-body Non-rigid Structure-from-Motion
- **Arxiv ID**: http://arxiv.org/abs/1705.04916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04916v1)
- **Published**: 2017-05-14 05:59:51+00:00
- **Updated**: 2017-05-14 05:59:51+00:00
- **Authors**: Suryansh Kumar, Yuchao Dai, Hongdong Li
- **Comment**: Author version of this paper has been accepted by Pattern Recognition
  Journal in the special issue on Articulated Motion and Deformable Objects.
  This work was originally submitted to ACCV 16 conference on 27th May 2016 for
  review
- **Journal**: None
- **Summary**: Non-rigid structure-from-motion (NRSfM) has so far been mostly studied for recovering 3D structure of a single non-rigid/deforming object. To handle the real world challenging multiple deforming objects scenarios, existing methods either pre-segment different objects in the scene or treat multiple non-rigid objects as a whole to obtain the 3D non-rigid reconstruction. However, these methods fail to exploit the inherent structure in the problem as the solution of segmentation and the solution of reconstruction could not benefit each other. In this paper, we propose a unified framework to jointly segment and reconstruct multiple non-rigid objects. To compactly represent complex multi-body non-rigid scenes, we propose to exploit the structure of the scenes along both temporal direction and spatial direction, thus achieving a spatio-temporal representation. Specifically, we represent the 3D non-rigid deformations as lying in a union of subspaces along the temporal direction and represent the 3D trajectories as lying in the union of subspaces along the spatial direction. This spatio-temporal representation not only provides competitive 3D reconstruction but also outputs robust segmentation of multiple non-rigid objects. The resultant optimization problem is solved efficiently using the Alternating Direction Method of Multipliers (ADMM). Extensive experimental results on both synthetic and real multi-body NRSfM datasets demonstrate the superior performance of our proposed framework compared with the state-of-the-art methods.



### Discovery and visualization of structural biomarkers from MRI using transport-based morphometry
- **Arxiv ID**: http://arxiv.org/abs/1705.04919v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04919v1)
- **Published**: 2017-05-14 06:29:34+00:00
- **Updated**: 2017-05-14 06:29:34+00:00
- **Authors**: Shinjini Kundu, Soheil Kolouri, Kirk I Erickson, Arthur F Kramer, Edward McAuley, Gustavo K Rohde
- **Comment**: None
- **Journal**: None
- **Summary**: Disease in the brain is often associated with subtle, spatially diffuse, or complex tissue changes that may lie beneath the level of gross visual inspection, even on magnetic resonance imaging (MRI). Unfortunately, current computer-assisted approaches that examine pre-specified features, whether anatomically-defined (i.e. thalamic volume, cortical thickness) or based on pixelwise comparison (i.e. deformation-based methods), are prone to missing a vast array of physical changes that are not well-encapsulated by these metrics. In this paper, we have developed a technique for automated pattern analysis that can fully determine the relationship between brain structure and observable phenotype without requiring any a priori features. Our technique, called transport-based morphometry (TBM), is an image transformation that maps brain images losslessly to a domain where they become much more separable. The new approach is validated on structural brain images of healthy older adult subjects where even linear models for discrimination, regression, and blind source separation enable TBM to independently discover the characteristic changes of aging and highlight potential mechanisms by which aerobic fitness may mediate brain health later in life. TBM is a generative approach that can provide visualization of physically meaningful shifts in tissue distribution through inverse transformation. The proposed framework is a powerful technique that can potentially elucidate genotype-structural-behavioral associations in myriad diseases.



### Gland Segmentation in Histopathology Images Using Random Forest Guided Boundary Construction
- **Arxiv ID**: http://arxiv.org/abs/1705.04924v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04924v3)
- **Published**: 2017-05-14 07:01:08+00:00
- **Updated**: 2017-08-15 17:57:52+00:00
- **Authors**: Rohith AP, Salman S. Khan, Kumar Anubhav, Angshuman Paul
- **Comment**: 7 pages, 8 figures
- **Journal**: None
- **Summary**: Grading of cancer is important to know the extent of its spread. Prior to grading, segmentation of glandular structures is important. Manual segmentation is a time consuming process and is subject to observer bias. Hence, an automated process is required to segment the gland structures. These glands show a large variation in shape size and texture. This makes the task challenging as the glands cannot be segmented using mere morphological operations and conventional segmentation mechanisms. In this project we propose a method which detects the boundary epithelial cells of glands and then a novel approach is used to construct the complete gland boundary. The region enclosed within the boundary can then be obtained to get the segmented gland regions.



### A Closed-Form Model for Image-Based Distant Lighting
- **Arxiv ID**: http://arxiv.org/abs/1705.04927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04927v1)
- **Published**: 2017-05-14 08:39:26+00:00
- **Updated**: 2017-05-14 08:39:26+00:00
- **Authors**: Mais Alnasser, Hassan Foroosh
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a new mathematical foundation for image-based lighting. Using a simple manipulation of the local coordinate system, we derive a closed-form solution to the light integral equation under distant environment illumination. We derive our solution for different BRDF's such as lambertian and Phong-like. The method is free of noise, and provides the possibility of using the full spectrum of frequencies captured by images taken from the environment. This allows for the color of the rendered object to be toned according to the color of the light in the environment. Experimental results also show that one can gain an order of magnitude or higher in rendering time compared to Monte Carlo quadrature methods and spherical harmonics.



### GeneGAN: Learning Object Transfiguration and Attribute Subspace from Unpaired Data
- **Arxiv ID**: http://arxiv.org/abs/1705.04932v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1705.04932v1)
- **Published**: 2017-05-14 08:59:36+00:00
- **Updated**: 2017-05-14 08:59:36+00:00
- **Authors**: Shuchang Zhou, Taihong Xiao, Yi Yang, Dieqiao Feng, Qinyao He, Weiran He
- **Comment**: Github: https://github.com/Prinsphield/GeneGAN
- **Journal**: None
- **Summary**: Object Transfiguration replaces an object in an image with another object from a second image. For example it can perform tasks like "putting exactly those eyeglasses from image A on the nose of the person in image B". Usage of exemplar images allows more precise specification of desired modifications and improves the diversity of conditional image generation. However, previous methods that rely on feature space operations, require paired data and/or appearance models for training or disentangling objects from background. In this work, we propose a model that can learn object transfiguration from two unpaired sets of images: one set containing images that "have" that kind of object, and the other set being the opposite, with the mild constraint that the objects be located approximately at the same place. For example, the training data can be one set of reference face images that have eyeglasses, and another set of images that have not, both of which spatially aligned by face landmarks. Despite the weak 0/1 labels, our model can learn an "eyeglasses" subspace that contain multiple representatives of different types of glasses. Consequently, we can perform fine-grained control of generated images, like swapping the glasses in two images by swapping the projected components in the "eyeglasses" subspace, to create novel images of people wearing eyeglasses.   Overall, our deterministic generative model learns disentangled attribute subspaces from weakly labeled data by adversarial training. Experiments on CelebA and Multi-PIE datasets validate the effectiveness of the proposed model on real world data, in generating images with specified eyeglasses, smiling, hair styles, and lighting conditions etc. The code is available online.



### Machine learning methods for multimedia information retrieval
- **Arxiv ID**: http://arxiv.org/abs/1705.04964v1
- **DOI**: 10.15476/ELTE.2016.086
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04964v1)
- **Published**: 2017-05-14 14:10:22+00:00
- **Updated**: 2017-05-14 14:10:22+00:00
- **Authors**: Bálint Zoltán Daróczy
- **Comment**: doctoral thesis, 2016
- **Journal**: None
- **Summary**: In this thesis we examined several multimodal feature extraction and learning methods for retrieval and classification purposes. We reread briefly some theoretical results of learning in Section 2 and reviewed several generative and discriminative models in Section 3 while we described the similarity kernel in Section 4. We examined different aspects of the multimodal image retrieval and classification in Section 5 and suggested methods for identifying quality assessments of Web documents in Section 6. In our last problem we proposed similarity kernel for time-series based classification. The experiments were carried over publicly available datasets and source codes for the most essential parts are either open source or released. Since the used similarity graphs (Section 4.2) are greatly constrained for computational purposes, we would like to continue work with more complex, evolving and capable graphs and apply for different problems such as capturing the rapid change in the distribution (e.g. session based recommendation) or complex graphs of the literature work. The similarity kernel with the proper metrics reaches and in many cases improves over the state-of-the-art. Hence we may conclude generative models based on instance similarities with multiple modes is a generally applicable model for classification and regression tasks ranging over various domains, including but not limited to the ones presented in this thesis. More generally, the Fisher kernel is not only unique in many ways but one of the most powerful kernel functions. Therefore we may exploit the Fisher kernel in the future over widely used generative models, such as Boltzmann Machines [Hinton et al., 1984], a particular subset, the Restricted Boltzmann Machines and Deep Belief Networks [Hinton et al., 2006]), Latent Dirichlet Allocation [Blei et al., 2003] or Hidden Markov Models [Baum and Petrie, 1966] to name a few.



### A Correspondence Relaxation Approach for 3D Shape Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1705.05016v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1705.05016v1)
- **Published**: 2017-05-14 19:02:03+00:00
- **Updated**: 2017-05-14 19:02:03+00:00
- **Authors**: Yong Khoo
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new method for 3D shape reconstruction based on two existing methods. A 3D reconstruction from a single photograph is introduced by both papers: the first one uses a photograph and a set of existing 3D model to generate the 3D object in the photograph, while the second one uses a photograph and a selected similar model to create the 3D object in the photograph. According to their difference, we propose a relaxation based method for more accurate correspondence establishment and shape recovery. The experiment demonstrates promising results compared to the state-of-the-art work on 3D shape estimation.



