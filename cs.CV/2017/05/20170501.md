# Arxiv Papers in cs.CV on 2017-05-01
### Level Playing Field for Million Scale Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1705.00393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00393v1)
- **Published**: 2017-05-01 01:04:53+00:00
- **Updated**: 2017-05-01 01:04:53+00:00
- **Authors**: Aaron Nech, Ira Kemelmacher-Shlizerman
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition has the perception of a solved problem, however when tested at the million-scale exhibits dramatic variation in accuracies across the different algorithms. Are the algorithms very different? Is access to good/big training data their secret weapon? Where should face recognition improve? To address those questions, we created a benchmark, MF2, that requires all algorithms to be trained on same data, and tested at the million scale. MF2 is a public large-scale set with 672K identities and 4.7M photos created with the goal to level playing field for large scale face recognition. We contrast our results with findings from the other two large-scale benchmarks MegaFace Challenge and MS-Celebs-1M where groups were allowed to train on any private/public/big/small set. Some key discoveries: 1) algorithms, trained on MF2, were able to achieve state of the art and comparable results to algorithms trained on massive private sets, 2) some outperformed themselves once trained on MF2, 3) invariance to aging suffers from low accuracies as in MegaFace, identifying the need for larger age variations possibly within identities or adjustment of algorithms in future testings.



### Sub-Pixel Registration of Wavelet-Encoded Images
- **Arxiv ID**: http://arxiv.org/abs/1705.00430v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00430v1)
- **Published**: 2017-05-01 07:27:04+00:00
- **Updated**: 2017-05-01 07:27:04+00:00
- **Authors**: Vildan Atalay Aydin, Hassan Foroosh
- **Comment**: None
- **Journal**: None
- **Summary**: Sub-pixel registration is a crucial step for applications such as super-resolution in remote sensing, motion compensation in magnetic resonance imaging, and non-destructive testing in manufacturing, to name a few. Recently, these technologies have been trending towards wavelet encoded imaging and sparse/compressive sensing. The former plays a crucial role in reducing imaging artifacts, while the latter significantly increases the acquisition speed. In view of these new emerging needs for applications of wavelet encoded imaging, we propose a sub-pixel registration method that can achieve direct wavelet domain registration from a sparse set of coefficients. We make the following contributions: (i) We devise a method of decoupling scale, rotation, and translation parameters in the Haar wavelet domain, (ii) We derive explicit mathematical expressions that define in-band sub-pixel registration in terms of wavelet coefficients, (iii) Using the derived expressions, we propose an approach to achieve in-band subpixel registration, avoiding back and forth transformations. (iv) Our solution remains highly accurate even when a sparse set of coefficients are used, which is due to localization of signals in a sparse set of wavelet coefficients. We demonstrate the accuracy of our method, and show that it outperforms the state-of-the-art on simulated and real data, even when the data is sparse.



### A Statistical Model for Simultaneous Template Estimation, Bias Correction, and Registration of 3D Brain Images
- **Arxiv ID**: http://arxiv.org/abs/1705.00432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00432v1)
- **Published**: 2017-05-01 07:31:45+00:00
- **Updated**: 2017-05-01 07:31:45+00:00
- **Authors**: Akshay Pai, Stefan Sommer, Lars Lau Raket, Line Kühnel, Sune Darkner, Lauge Sørensen, Mads Nielsen
- **Comment**: None
- **Journal**: None
- **Summary**: Template estimation plays a crucial role in computational anatomy since it provides reference frames for performing statistical analysis of the underlying anatomical population variability. While building models for template estimation, variability in sites and image acquisition protocols need to be accounted for. To account for such variability, we propose a generative template estimation model that makes simultaneous inference of both bias fields in individual images, deformations for image registration, and variance hyperparameters. In contrast, existing maximum a posterori based methods need to rely on either bias-invariant similarity measures or robust image normalization. Results on synthetic and real brain MRI images demonstrate the capability of the model to capture heterogeneity in intensities and provide a reliable template estimation from registration.



### Detecting Drivable Area for Self-driving Cars: An Unsupervised Approach
- **Arxiv ID**: http://arxiv.org/abs/1705.00451v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00451v1)
- **Published**: 2017-05-01 09:14:29+00:00
- **Updated**: 2017-05-01 09:14:29+00:00
- **Authors**: Ziyi Liu, Siyu Yu, Xiao Wang, Nanning Zheng
- **Comment**: 6 pages, 5 figures, conference
- **Journal**: None
- **Summary**: It has been well recognized that detecting drivable area is central to self-driving cars. Most of existing methods attempt to locate road surface by using lane line, thereby restricting to drivable area on which have a clear lane mark. This paper proposes an unsupervised approach for detecting drivable area utilizing both image data from a monocular camera and point cloud data from a 3D-LIDAR scanner. Our approach locates initial drivable areas based on a "direction ray map" obtained by image-LIDAR data fusion. Besides, a fusion of the feature level is also applied for more robust performance. Once the initial drivable areas are described by different features, the feature fusion problem is formulated as a Markov network and a belief propagation algorithm is developed to perform the model inference. Our approach is unsupervised and avoids common hypothesis, yet gets state-of-the-art results on ROAD-KITTI benchmark. Experiments show that our unsupervised approach is efficient and robust for detecting drivable area for self-driving cars.



### Shearlet-based compressed sensing for fast 3D cardiac MR imaging using iterative reweighting
- **Arxiv ID**: http://arxiv.org/abs/1705.00463v1
- **DOI**: 10.1088/1361-6560/aaea04
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00463v1)
- **Published**: 2017-05-01 10:43:17+00:00
- **Updated**: 2017-05-01 10:43:17+00:00
- **Authors**: Jackie Ma, Maximilian März, Stephanie Funk, Jeanette Schulz-Menger, Gitta Kutyniok, Tobias Schaeffter, Christoph Kolbitsch
- **Comment**: None
- **Journal**: None
- **Summary**: High-resolution three-dimensional (3D) cardiovascular magnetic resonance (CMR) is a valuable medical imaging technique, but its widespread application in clinical practice is hampered by long acquisition times. Here we present a novel compressed sensing (CS) reconstruction approach using shearlets as a sparsifying transform allowing for fast 3D CMR (3DShearCS). Shearlets are mathematically optimal for a simplified model of natural images and have been proven to be more efficient than classical systems such as wavelets. Data is acquired with a 3D Radial Phase Encoding (RPE) trajectory and an iterative reweighting scheme is used during image reconstruction to ensure fast convergence and high image quality. In our in-vivo cardiac MRI experiments we show that the proposed method 3DShearCS has lower relative errors and higher structural similarity compared to the other reconstruction techniques especially for high undersampling factors, i.e. short scan times. In this paper, we further show that 3DShearCS provides improved depiction of cardiac anatomy (measured by assessing the sharpness of coronary arteries) and two clinical experts qualitatively analyzed the image quality.



### Speech-Based Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1705.00464v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.00464v2)
- **Published**: 2017-05-01 10:43:28+00:00
- **Updated**: 2017-09-16 03:43:20+00:00
- **Authors**: Ted Zhang, Dengxin Dai, Tinne Tuytelaars, Marie-Francine Moens, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces speech-based visual question answering (VQA), the task of generating an answer given an image and a spoken question. Two methods are studied: an end-to-end, deep neural network that directly uses audio waveforms as input versus a pipelined approach that performs ASR (Automatic Speech Recognition) on the question, followed by text-based visual question answering. Furthermore, we investigate the robustness of both methods by injecting various levels of noise into the spoken question and find both methods to be tolerate noise at similar levels.



### Generalized orderless pooling performs implicit salient matching
- **Arxiv ID**: http://arxiv.org/abs/1705.00487v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00487v3)
- **Published**: 2017-05-01 12:00:49+00:00
- **Updated**: 2017-07-20 13:59:36+00:00
- **Authors**: Marcel Simon, Yang Gao, Trevor Darrell, Joachim Denzler, Erik Rodner
- **Comment**: Published at International Conference on Computer Vision (ICCV) 2017
- **Journal**: None
- **Summary**: Most recent CNN architectures use average pooling as a final feature encoding step. In the field of fine-grained recognition, however, recent global representations like bilinear pooling offer improved performance. In this paper, we generalize average and bilinear pooling to "alpha-pooling", allowing for learning the pooling strategy during training. In addition, we present a novel way to visualize decisions made by these approaches. We identify parts of training images having the highest influence on the prediction of a given test image. It allows for justifying decisions to users and also for analyzing the influence of semantic parts. For example, we can show that the higher capacity VGG16 model focuses much more on the bird's head than, e.g., the lower-capacity VGG-M model when recognizing fine-grained bird categories. Both contributions allow us to analyze the difference when moving between average and bilinear pooling. In addition, experiments show that our generalized approach can outperform both across a variety of standard datasets.



### Regularized Residual Quantization: a multi-layer sparse dictionary learning approach
- **Arxiv ID**: http://arxiv.org/abs/1705.00522v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.00522v1)
- **Published**: 2017-05-01 13:59:04+00:00
- **Updated**: 2017-05-01 13:59:04+00:00
- **Authors**: Sohrab Ferdowsi, Slava Voloshynovskiy, Dimche Kostadinov
- **Comment**: To be presented at SPARS 2017, Lisbon, Portugal
- **Journal**: None
- **Summary**: The Residual Quantization (RQ) framework is revisited where the quantization distortion is being successively reduced in multi-layers. Inspired by the reverse-water-filling paradigm in rate-distortion theory, an efficient regularization on the variances of the codewords is introduced which allows to extend the RQ for very large numbers of layers and also for high dimensional data, without getting over-trained. The proposed Regularized Residual Quantization (RRQ) results in multi-layer dictionaries which are additionally sparse, thanks to the soft-thresholding nature of the regularization when applied to variance-decaying data which can arise from de-correlating transformations applied to correlated data. Furthermore, we also propose a general-purpose pre-processing for natural images which makes them suitable for such quantization. The RRQ framework is first tested on synthetic variance-decaying data to show its efficiency in quantization of high-dimensional data. Next, we use the RRQ in super-resolution of a database of facial images where it is shown that low-resolution facial images from the test set quantized with codebooks trained on high-resolution images from the training set show relevant high-frequency content when reconstructed with those codebooks.



### Query-adaptive Video Summarization via Quality-aware Relevance Estimation
- **Arxiv ID**: http://arxiv.org/abs/1705.00581v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1705.00581v2)
- **Published**: 2017-05-01 16:28:18+00:00
- **Updated**: 2017-09-28 13:18:56+00:00
- **Authors**: Arun Balajee Vasudevan, Michael Gygli, Anna Volokitin, Luc Van Gool
- **Comment**: ACM Multimedia 2017
- **Journal**: None
- **Summary**: Although the problem of automatic video summarization has recently received a lot of attention, the problem of creating a video summary that also highlights elements relevant to a search query has been less studied. We address this problem by posing query-relevant summarization as a video frame subset selection problem, which lets us optimise for summaries which are simultaneously diverse, representative of the entire video, and relevant to a text query. We quantify relevance by measuring the distance between frames and queries in a common textual-visual semantic embedding space induced by a neural network. In addition, we extend the model to capture query-independent properties, such as frame quality. We compare our method against previous state of the art on textual-visual embeddings for thumbnail selection and show that our model outperforms them on relevance prediction. Furthermore, we introduce a new dataset, annotated with diversity and query-specific relevance labels. On this dataset, we train and test our complete model for video summarization and show that it outperforms standard baselines such as Maximal Marginal Relevance.



### The Promise of Premise: Harnessing Question Premises in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1705.00601v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1705.00601v2)
- **Published**: 2017-05-01 17:41:37+00:00
- **Updated**: 2017-08-17 18:12:18+00:00
- **Authors**: Aroma Mahendru, Viraj Prabhu, Akrit Mohapatra, Dhruv Batra, Stefan Lee
- **Comment**: Published at EMNLP 2017
- **Journal**: None
- **Summary**: In this paper, we make a simple observation that questions about images often contain premises - objects and relationships implied by the question - and that reasoning about premises can help Visual Question Answering (VQA) models respond more intelligently to irrelevant or previously unseen questions. When presented with a question that is irrelevant to an image, state-of-the-art VQA models will still answer purely based on learned language biases, resulting in non-sensical or even misleading answers. We note that a visual question is irrelevant to an image if at least one of its premises is false (i.e. not depicted in the image). We leverage this observation to construct a dataset for Question Relevance Prediction and Explanation (QRPE) by searching for false premises. We train novel question relevance detection models and show that models that reason about premises consistently outperform models that do not. We also find that forcing standard VQA models to reason about premises during training can lead to improvements on tasks requiring compositional reasoning.



### Spotting the Difference: Context Retrieval and Analysis for Improved Forgery Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/1705.00604v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1705.00604v1)
- **Published**: 2017-05-01 17:43:49+00:00
- **Updated**: 2017-05-01 17:43:49+00:00
- **Authors**: Joel Brogan, Paolo Bestagini, Aparna Bharati, Allan Pinto, Daniel Moreira, Kevin Bowyer, Patrick Flynn, Anderson Rocha, Walter Scheirer
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: As image tampering becomes ever more sophisticated and commonplace, the need for image forensics algorithms that can accurately and quickly detect forgeries grows. In this paper, we revisit the ideas of image querying and retrieval to provide clues to better localize forgeries. We propose a method to perform large-scale image forensics on the order of one million images using the help of an image search algorithm and database to gather contextual clues as to where tampering may have taken place. In this vein, we introduce five new strongly invariant image comparison methods and test their effectiveness under heavy noise, rotation, and color space changes. Lastly, we show the effectiveness of these methods compared to passive image forensics using Nimble [https://www.nist.gov/itl/iad/mig/nimble-challenge], a new, state-of-the-art dataset from the National Institute of Standards and Technology (NIST).



### Mind the Class Weight Bias: Weighted Maximum Mean Discrepancy for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1705.00609v1
- **DOI**: None
- **Categories**: **cs.CV**, 68-45
- **Links**: [PDF](http://arxiv.org/pdf/1705.00609v1)
- **Published**: 2017-05-01 17:54:53+00:00
- **Updated**: 2017-05-01 17:54:53+00:00
- **Authors**: Hongliang Yan, Yukang Ding, Peihua Li, Qilong Wang, Yong Xu, Wangmeng Zuo
- **Comment**: 10 pages, 5 figures, accepted by CVPR17
- **Journal**: None
- **Summary**: In domain adaptation, maximum mean discrepancy (MMD) has been widely adopted as a discrepancy metric between the distributions of source and target domains. However, existing MMD-based domain adaptation methods generally ignore the changes of class prior distributions, i.e., class weight bias across domains. This remains an open problem but ubiquitous for domain adaptation, which can be caused by changes in sample selection criteria and application scenarios. We show that MMD cannot account for class weight bias and results in degraded domain adaptation performance. To address this issue, a weighted MMD model is proposed in this paper. Specifically, we introduce class-specific auxiliary weights into the original MMD for exploiting the class prior probability on source and target domains, whose challenge lies in the fact that the class label in target domain is unavailable. To account for it, our proposed weighted MMD model is defined by introducing an auxiliary weight for each class in the source domain, and a classification EM algorithm is suggested by alternating between assigning the pseudo-labels, estimating auxiliary weights and updating model parameters. Extensive experiments demonstrate the superiority of our weighted MMD over conventional MMD for domain adaptation.



### Bayesian Image Quality Transfer with CNNs: Exploring Uncertainty in dMRI Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1705.00664v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00664v2)
- **Published**: 2017-05-01 18:56:22+00:00
- **Updated**: 2017-05-30 09:37:57+00:00
- **Authors**: Ryutaro Tanno, Daniel E. Worrall, Aurobrata Ghosh, Enrico Kaden, Stamatios N. Sotiropoulos, Antonio Criminisi, Daniel C. Alexander
- **Comment**: Accepted paper at MICCAI 2017
- **Journal**: None
- **Summary**: In this work, we investigate the value of uncertainty modeling in 3D super-resolution with convolutional neural networks (CNNs). Deep learning has shown success in a plethora of medical image transformation problems, such as super-resolution (SR) and image synthesis. However, the highly ill-posed nature of such problems results in inevitable ambiguity in the learning of networks. We propose to account for intrinsic uncertainty through a per-patch heteroscedastic noise model and for parameter uncertainty through approximate Bayesian inference in the form of variational dropout. We show that the combined benefits of both lead to the state-of-the-art performance SR of diffusion MR brain images in terms of errors compared to ground truth. We further show that the reduced error scores produce tangible benefits in downstream tractography. In addition, the probabilistic nature of the methods naturally confers a mechanism to quantify uncertainty over the super-resolved output. We demonstrate through experiments on both healthy and pathological brains the potential utility of such an uncertainty measure in the risk assessment of the super-resolved images for subsequent clinical use.



### Twin Learning for Similarity and Clustering: A Unified Kernel Approach
- **Arxiv ID**: http://arxiv.org/abs/1705.00678v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.00678v2)
- **Published**: 2017-05-01 19:33:27+00:00
- **Updated**: 2017-05-03 00:29:13+00:00
- **Authors**: Zhao Kang, Chong Peng, Qiang Cheng
- **Comment**: Published in AAAI 2017
- **Journal**: None
- **Summary**: Many similarity-based clustering methods work in two separate steps including similarity matrix computation and subsequent spectral clustering. However, similarity measurement is challenging because it is usually impacted by many factors, e.g., the choice of similarity metric, neighborhood size, scale of data, noise and outliers. Thus the learned similarity matrix is often not suitable, let alone optimal, for the subsequent clustering. In addition, nonlinear similarity often exists in many real world data which, however, has not been effectively considered by most existing methods. To tackle these two challenges, we propose a model to simultaneously learn cluster indicator matrix and similarity information in kernel spaces in a principled way. We show theoretical relationships to kernel k-means, k-means, and spectral clustering methods. Then, to address the practical issue of how to select the most suitable kernel for a particular clustering task, we further extend our model with a multiple kernel learning ability. With this joint model, we can automatically accomplish three subtasks of finding the best cluster indicator matrix, the most accurate similarity relations and the optimal combination of multiple kernels. By leveraging the interactions between these three subtasks in a joint framework, each subtask can be iteratively boosted by using the results of the others towards an overall optimal solution. Extensive experiments are performed to demonstrate the effectiveness of our method.



### Submodular Trajectory Optimization for Aerial 3D Scanning
- **Arxiv ID**: http://arxiv.org/abs/1705.00703v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00703v3)
- **Published**: 2017-05-01 20:32:43+00:00
- **Updated**: 2017-08-04 05:22:24+00:00
- **Authors**: Mike Roberts, Debadeepta Dey, Anh Truong, Sudipta Sinha, Shital Shah, Ashish Kapoor, Pat Hanrahan, Neel Joshi
- **Comment**: Accepted for publication at the International Conference on Computer
  Vision (ICCV) 2017; Supplementary video:
  http://www.youtube.com/watch?v=89fFmfVZSO8
- **Journal**: None
- **Summary**: Drones equipped with cameras are emerging as a powerful tool for large-scale aerial 3D scanning, but existing automatic flight planners do not exploit all available information about the scene, and can therefore produce inaccurate and incomplete 3D models. We present an automatic method to generate drone trajectories, such that the imagery acquired during the flight will later produce a high-fidelity 3D model. Our method uses a coarse estimate of the scene geometry to plan camera trajectories that: (1) cover the scene as thoroughly as possible; (2) encourage observations of scene geometry from a diverse set of viewing angles; (3) avoid obstacles; and (4) respect a user-specified flight time budget. Our method relies on a mathematical model of scene coverage that exhibits an intuitive diminishing returns property known as submodularity. We leverage this property extensively to design a trajectory planning algorithm that reasons globally about the non-additive coverage reward obtained across a trajectory, jointly with the cost of traveling between views. We evaluate our method by using it to scan three large outdoor scenes, and we perform a quantitative evaluation using a photorealistic video game simulator.



### Hyperspectral Image Classification with Markov Random Fields and a Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1705.00727v2
- **DOI**: 10.1109/TIP.2018.2799324
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.00727v2)
- **Published**: 2017-05-01 22:15:11+00:00
- **Updated**: 2017-11-12 15:48:49+00:00
- **Authors**: Xiangyong Cao, Feng Zhou, Lin Xu, Deyu Meng, Zongben Xu, John Paisley
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new supervised classification algorithm for remotely sensed hyperspectral image (HSI) which integrates spectral and spatial information in a unified Bayesian framework. First, we formulate the HSI classification problem from a Bayesian perspective. Then, we adopt a convolutional neural network (CNN) to learn the posterior class distributions using a patch-wise training strategy to better use the spatial information. Next, spatial information is further considered by placing a spatial smoothness prior on the labels. Finally, we iteratively update the CNN parameters using stochastic gradient decent (SGD) and update the class labels of all pixel vectors using an alpha-expansion min-cut-based algorithm. Compared with other state-of-the-art methods, the proposed classification method achieves better performance on one synthetic dataset and two benchmark HSI datasets in a number of experimental settings.



