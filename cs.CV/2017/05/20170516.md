# Arxiv Papers in cs.CV on 2017-05-16
### Joint Geometrical and Statistical Alignment for Visual Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1705.05498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05498v1)
- **Published**: 2017-05-16 01:35:58+00:00
- **Updated**: 2017-05-16 01:35:58+00:00
- **Authors**: Jing Zhang, Wanqing Li, Philip Ogunbona
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel unsupervised domain adaptation method for cross-domain visual recognition. We propose a unified framework that reduces the shift between domains both statistically and geometrically, referred to as Joint Geometrical and Statistical Alignment (JGSA). Specifically, we learn two coupled projections that project the source domain and target domain data into low dimensional subspaces where the geometrical shift and distribution shift are reduced simultaneously. The objective function can be solved efficiently in a closed form. Extensive experiments have verified that the proposed method significantly outperforms several state-of-the-art domain adaptation methods on a synthetic dataset and three different real world cross-domain visual recognition tasks.



### Automated Body Structure Extraction from Arbitrary 3D Mesh
- **Arxiv ID**: http://arxiv.org/abs/1705.05508v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.05508v1)
- **Published**: 2017-05-16 02:58:44+00:00
- **Updated**: 2017-05-16 02:58:44+00:00
- **Authors**: Yong Khoo, Sang Chung
- **Comment**: None
- **Journal**: Imaging and Graphics, 2017
- **Summary**: This paper presents an automated method for 3D character skeleton extraction that can be applied for generic 3D shapes. Our work is motivated by the skeleton-based prior work on automatic rigging focused on skeleton extraction and can automatically aligns the extracted structure to fit the 3D shape of the given 3D mesh. The body mesh can be subsequently skinned based on the extracted skeleton and thus enables rigging process. In the experiment, we apply public dataset to drive the estimated skeleton from different body shapes, as well as the real data obtained from 3D scanning systems. Satisfactory results are obtained compared to the existing approaches.



### Cooperative Learning with Visual Attributes
- **Arxiv ID**: http://arxiv.org/abs/1705.05512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05512v1)
- **Published**: 2017-05-16 03:04:55+00:00
- **Updated**: 2017-05-16 03:04:55+00:00
- **Authors**: Tanmay Batra, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: Learning paradigms involving varying levels of supervision have received a lot of interest within the computer vision and machine learning communities. The supervisory information is typically considered to come from a human supervisor -- a "teacher" figure. In this paper, we consider an alternate source of supervision -- a "peer" -- i.e. a different machine. We introduce cooperative learning, where two agents trying to learn the same visual concepts, but in potentially different environments using different sources of data (sensors), communicate their current knowledge of these concepts to each other. Given the distinct sources of data in both agents, the mode of communication between the two agents is not obvious. We propose the use of visual attributes -- semantic mid-level visual properties such as furry, wooden, etc.-- as the mode of communication between the agents. Our experiments in three domains -- objects, scenes, and animals -- demonstrate that our proposed cooperative learning approach improves the performance of both agents as compared to their performance if they were to learn in isolation. Our approach is particularly applicable in scenarios where privacy, security and/or bandwidth constraints restrict the amount and type of information the two agents can exchange.



### Intel RealSense Stereoscopic Depth Cameras
- **Arxiv ID**: http://arxiv.org/abs/1705.05548v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1705.05548v2)
- **Published**: 2017-05-16 06:36:11+00:00
- **Updated**: 2017-10-29 05:31:14+00:00
- **Authors**: Leonid Keselman, John Iselin Woodfill, Anders Grunnet-Jepsen, Achintya Bhowmik
- **Comment**: Accepted to CCD 2017, a CVPR 2017 Workshop
- **Journal**: None
- **Summary**: We present a comprehensive overview of the stereoscopic Intel RealSense RGBD imaging systems. We discuss these systems' mode-of-operation, functional behavior and include models of their expected performance, shortcomings, and limitations. We provide information about the systems' optical characteristics, their correlation algorithms, and how these properties can affect different applications, including 3D reconstruction and gesture recognition. Our discussion covers the Intel RealSense R200 and the Intel RealSense D400 (formally RS400).



### IAN: The Individual Aggregation Network for Person Search
- **Arxiv ID**: http://arxiv.org/abs/1705.05552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05552v1)
- **Published**: 2017-05-16 06:55:02+00:00
- **Updated**: 2017-05-16 06:55:02+00:00
- **Authors**: Jimin Xiao, Yanchun Xie, Tammam Tillo, Kaizhu Huang, Yunchao Wei, Jiashi Feng
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Person search in real-world scenarios is a new challenging computer version task with many meaningful applications. The challenge of this task mainly comes from: (1) unavailable bounding boxes for pedestrians and the model needs to search for the person over the whole gallery images; (2) huge variance of visual appearance of a particular person owing to varying poses, lighting conditions, and occlusions. To address these two critical issues in modern person search applications, we propose a novel Individual Aggregation Network (IAN) that can accurately localize persons by learning to minimize intra-person feature variations. IAN is built upon the state-of-the-art object detection framework, i.e., faster R-CNN, so that high-quality region proposals for pedestrians can be produced in an online manner. In addition, to relieve the negative effect caused by varying visual appearances of the same individual, IAN introduces a novel center loss that can increase the intra-class compactness of feature representations. The engaged center loss encourages persons with the same identity to have similar feature characteristics. Extensive experimental results on two benchmarks, i.e., CUHK-SYSU and PRW, well demonstrate the superiority of the proposed model. In particular, IAN achieves 77.23% mAP and 80.45% top-1 accuracy on CUHK-SYSU, which outperform the state-of-the-art by 1.7% and 1.85%, respectively.



### Research on Bi-mode Biometrics Based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1705.05619v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05619v1)
- **Published**: 2017-05-16 09:55:05+00:00
- **Updated**: 2017-05-16 09:55:05+00:00
- **Authors**: Hao Jiang
- **Comment**: in Chinese
- **Journal**: None
- **Summary**: In view of the fact that biological characteristics have excellent independent distinguishing characteristics,biometric identification technology involves almost all the relevant areas of human distinction. Fingerprints, iris, face, voice-print and other biological features have been widely used in the public security departments to detect detection, mobile equipment unlock, target tracking and other fields. With the use of electronic devices more and more widely and the frequency is getting higher and higher. Only the Biometrics identification technology with excellent recognition rate can guarantee the long-term development of these fields.



### Picasso: A Modular Framework for Visualizing the Learning Process of Neural Network Image Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1705.05627v3
- **DOI**: 10.5334/jors.178
- **Categories**: **cs.CV**, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/1705.05627v3)
- **Published**: 2017-05-16 10:06:19+00:00
- **Updated**: 2017-09-11 12:35:18+00:00
- **Authors**: Ryan Henderson, Rasmus Rothe
- **Comment**: 9 pages, submission to the Journal of Open Research Software,
  github.com/merantix/picasso
- **Journal**: Journal of Open Research Software. 5(1), p.22 (2017)
- **Summary**: Picasso is a free open-source (Eclipse Public License) web application written in Python for rendering standard visualizations useful for analyzing convolutional neural networks. Picasso ships with occlusion maps and saliency maps, two visualizations which help reveal issues that evaluation metrics like loss and accuracy might hide: for example, learning a proxy classification task. Picasso works with the Tensorflow deep learning framework, and Keras (when the model can be loaded into the Tensorflow backend). Picasso can be used with minimal configuration by deep learning researchers and engineers alike across various neural network architectures. Adding new visualizations is simple: the user can specify their visualization code and HTML template separately from the application code.



### WebVision Challenge: Visual Learning and Understanding With Web Data
- **Arxiv ID**: http://arxiv.org/abs/1705.05640v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05640v1)
- **Published**: 2017-05-16 10:59:23+00:00
- **Updated**: 2017-05-16 10:59:23+00:00
- **Authors**: Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, Jesse Berent, Abhinav Gupta, Rahul Sukthankar, Luc Van Gool
- **Comment**: project page: http://www.vision.ee.ethz.ch/webvision/
- **Journal**: None
- **Summary**: We present the 2017 WebVision Challenge, a public image recognition challenge designed for deep learning based on web images without instance-level human annotation. Following the spirit of previous vision challenges, such as ILSVRC, Places2 and PASCAL VOC, which have played critical roles in the development of computer vision by contributing to the community with large scale annotated data for model designing and standardized benchmarking, we contribute with this challenge a large scale web images dataset, and a public competition with a workshop co-located with CVPR 2017. The WebVision dataset contains more than $2.4$ million web images crawled from the Internet by using queries generated from the $1,000$ semantic concepts of the benchmark ILSVRC 2012 dataset. Meta information is also included. A validation set and test set containing human annotated images are also provided to facilitate algorithmic development. The 2017 WebVision challenge consists of two tracks, the image classification task on WebVision test set, and the transfer learning task on PASCAL VOC 2012 dataset. In this paper, we describe the details of data collection and annotation, highlight the characteristics of the dataset, and introduce the evaluation metrics.



### Learning Image Relations with Contrast Association Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.05665v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.05665v2)
- **Published**: 2017-05-16 12:09:44+00:00
- **Updated**: 2019-03-11 21:44:39+00:00
- **Authors**: Yao Lu, Zhirong Yang, Juho Kannala, Samuel Kaski
- **Comment**: None
- **Journal**: None
- **Summary**: Inferring the relations between two images is an important class of tasks in computer vision. Examples of such tasks include computing optical flow and stereo disparity. We treat the relation inference tasks as a machine learning problem and tackle it with neural networks. A key to the problem is learning a representation of relations. We propose a new neural network module, contrast association unit (CAU), which explicitly models the relations between two sets of input variables. Due to the non-negativity of the weights in CAU, we adopt a multiplicative update algorithm for learning these weights. Experiments show that neural networks with CAUs are more effective in learning five fundamental image transformations than conventional neural networks.



### Active Control of Camera Parameters for Object Detection Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1705.05685v1
- **DOI**: None
- **Categories**: **cs.CV**, 65D19, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1705.05685v1)
- **Published**: 2017-05-16 12:47:47+00:00
- **Updated**: 2017-05-16 12:47:47+00:00
- **Authors**: Yulong Wu, John Tsotsos
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Camera parameters not only play an important role in determining the visual quality of perceived images, but also affect the performance of vision algorithms, for a vision-guided robot. By quantitatively evaluating four object detection algorithms, with respect to varying ambient illumination, shutter speed and voltage gain, it is observed that the performance of the algorithms is highly dependent on these variables. From this observation, a novel active control of camera parameters method is proposed, to make robot vision more robust under different light conditions. Experimental results demonstrate the effectiveness of our proposed approach, which improves the performance of object detection algorithms, compared with the conventional auto-exposure algorithm.



### Learning Features for Offline Handwritten Signature Verification using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.05787v1
- **DOI**: 10.1016/j.patcog.2017.05.012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05787v1)
- **Published**: 2017-05-16 16:08:09+00:00
- **Updated**: 2017-05-16 16:08:09+00:00
- **Authors**: Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira
- **Comment**: None
- **Journal**: None
- **Summary**: Verifying the identity of a person using handwritten signatures is challenging in the presence of skilled forgeries, where a forger has access to a person's signature and deliberately attempt to imitate it. In offline (static) signature verification, the dynamic information of the signature writing process is lost, and it is difficult to design good feature extractors that can distinguish genuine signatures and skilled forgeries. This reflects in a relatively poor performance, with verification errors around 7% in the best systems in the literature. To address both the difficulty of obtaining good features, as well as improve system performance, we propose learning the representations from signature images, in a Writer-Independent format, using Convolutional Neural Networks. In particular, we propose a novel formulation of the problem that includes knowledge of skilled forgeries from a subset of users in the feature learning process, that aims to capture visual cues that distinguish genuine signatures and forgeries regardless of the user. Extensive experiments were conducted on four datasets: GPDS, MCYT, CEDAR and Brazilian PUC-PR datasets. On GPDS-160, we obtained a large improvement in state-of-the-art performance, achieving 1.72% Equal Error Rate, compared to 6.97% in the literature. We also verified that the features generalize beyond the GPDS dataset, surpassing the state-of-the-art performance in the other datasets, without requiring the representation to be fine-tuned to each particular dataset.



### The Incremental Multiresolution Matrix Factorization Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1705.05804v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.05804v1)
- **Published**: 2017-05-16 17:10:05+00:00
- **Updated**: 2017-05-16 17:10:05+00:00
- **Authors**: Vamsi K. Ithapu, Risi Kondor, Sterling C. Johnson, Vikas Singh
- **Comment**: Computer Vision and Pattern Recognition (CVPR) 2017, 10 pages
- **Journal**: None
- **Summary**: Multiresolution analysis and matrix factorization are foundational tools in computer vision. In this work, we study the interface between these two distinct topics and obtain techniques to uncover hierarchical block structure in symmetric matrices -- an important aspect in the success of many vision problems. Our new algorithm, the incremental multiresolution matrix factorization, uncovers such structure one feature at a time, and hence scales well to large matrices. We describe how this multiscale analysis goes much farther than what a direct global factorization of the data can identify. We evaluate the efficacy of the resulting factorizations for relative leveraging within regression tasks using medical imaging data. We also use the factorization on representations learned by popular deep networks, providing evidence of their ability to infer semantic relationships even when they are not explicitly trained to do so. We show that this algorithm can be used as an exploratory tool to improve the network architecture, and within numerous other settings in vision.



### Real-Time Adaptive Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1705.05823v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.05823v1)
- **Published**: 2017-05-16 17:51:07+00:00
- **Updated**: 2017-05-16 17:51:07+00:00
- **Authors**: Oren Rippel, Lubomir Bourdev
- **Comment**: Published at ICML 2017
- **Journal**: None
- **Summary**: We present a machine learning-based approach to lossy image compression which outperforms all existing codecs, while running in real-time.   Our algorithm typically produces files 2.5 times smaller than JPEG and JPEG 2000, 2 times smaller than WebP, and 1.7 times smaller than BPG on datasets of generic images across all quality levels. At the same time, our codec is designed to be lightweight and deployable: for example, it can encode or decode the Kodak dataset in around 10ms per image on GPU.   Our architecture is an autoencoder featuring pyramidal analysis, an adaptive coding module, and regularization of the expected codelength. We also supplement our approach with adversarial training specialized towards use in a compression setting: this enables us to produce visually pleasing reconstructions for very low bitrates.



### Static Gesture Recognition using Leap Motion
- **Arxiv ID**: http://arxiv.org/abs/1705.05884v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1705.05884v1)
- **Published**: 2017-05-16 19:38:20+00:00
- **Updated**: 2017-05-16 19:38:20+00:00
- **Authors**: Babak Toghiani-Rizi, Christofer Lind, Maria Svensson, Marcus Windmark
- **Comment**: Results based on a study conducted during the course Intelligent
  Interactive Systems at Uppsala University, spring 2016
- **Journal**: None
- **Summary**: In this report, an automated bartender system was developed for making orders in a bar using hand gestures. The gesture recognition of the system was developed using Machine Learning techniques, where the model was trained to classify gestures using collected data. The final model used in the system reached an average accuracy of 95%. The system raised ethical concerns both in terms of user interaction and having such a system in a real world scenario, but it could initially work as a complement to a real bartender.



### What's In A Patch, I: Tensors, Differential Geometry and Statistical Shading Analysis
- **Arxiv ID**: http://arxiv.org/abs/1705.05885v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05885v1)
- **Published**: 2017-05-16 19:39:52+00:00
- **Updated**: 2017-05-16 19:39:52+00:00
- **Authors**: Daniel Niels Holtmann-Rice, Benjamin S. Kunsberg, Steven W. Zucker
- **Comment**: None
- **Journal**: None
- **Summary**: We develop a linear algebraic framework for the shape-from-shading problem, because tensors arise when scalar (e.g. image) and vector (e.g. surface normal) fields are differentiated multiple times. The work is in two parts. In this first part we investigate when image derivatives exhibit invariance to changing illumination by calculating the statistics of image derivatives under general distributions on the light source. We computationally validate the hypothesis that image orientations (derivatives) provide increased invariance to illumination by showing (for a Lambertian model) that a shape-from-shading algorithm matching gradients instead of intensities provides more accurate reconstructions when illumination is incorrectly estimated under a flatness prior.



### Tensors, Differential Geometry and Statistical Shading Analysis
- **Arxiv ID**: http://arxiv.org/abs/1705.05902v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05902v2)
- **Published**: 2017-05-16 20:25:21+00:00
- **Updated**: 2018-07-27 15:23:02+00:00
- **Authors**: Daniel Niels Holtmann-Rice, Benjamin S. Kunsberg, Steven W. Zucker
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1705.05885
- **Journal**: None
- **Summary**: We develop a linear algebraic framework for the shape-from-shading problem, because tensors arise when scalar (e.g. image) and vector (e.g. surface normal) fields are differentiated multiple times. Using this framework, we first investigate when image derivatives exhibit invariance to changing illumination by calculating the statistics of image derivatives under general distributions on the light source. Second, we apply that framework to develop Taylor-like expansions, and build a boot-strapping algorithm to find the polynomial surface solutions (under any light source) consistent with a given patch to arbitrary order. A generic constraint on the light source restricts these solutions to a 2-D subspace, plus an unknown rotation matrix. It is this unknown matrix that encapsulates the ambiguity in the problem. Finally, we use the framework to computationally validate the hypothesis that image orientations (derivatives) provide increased invariance to illumination by showing (for a Lambertian model) that a shape-from-shading algorithm matching gradients instead of intensities provides more accurate reconstructions when illumination is incorrectly estimated under a flatness prior.



### LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems
- **Arxiv ID**: http://arxiv.org/abs/1705.05922v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05922v1)
- **Published**: 2017-05-16 21:05:49+00:00
- **Updated**: 2017-05-16 21:05:49+00:00
- **Authors**: Subarna Tripathi, Gokce Dane, Byeongkeun Kang, Vasudev Bhaskaran, Truong Nguyen
- **Comment**: Embedded Vision Workshop in CVPR
- **Journal**: None
- **Summary**: Deep convolutional Neural Networks (CNN) are the state-of-the-art performers for object detection task. It is well known that object detection requires more computation and memory than image classification. Thus the consolidation of a CNN-based object detection for an embedded system is more challenging. In this work, we propose LCDet, a fully-convolutional neural network for generic object detection that aims to work in embedded systems. We design and develop an end-to-end TensorFlow(TF)-based model. Additionally, we employ 8-bit quantization on the learned weights. We use face detection as a use case. Our TF-Slim based network can predict different faces of different shapes and sizes in a single forward pass. Our experimental results show that the proposed method achieves comparative accuracy comparing with state-of-the-art CNN-based face detection methods, while reducing the model size by 3x and memory-BW by ~4x comparing with one of the best real-time CNN-based object detector such as YOLO. TF 8-bit quantized model provides additional 4x memory reduction while keeping the accuracy as good as the floating point model. The proposed model thus becomes amenable for embedded implementations.



