# Arxiv Papers in cs.CV on 2017-05-10
### Discovery Radiomics via Evolutionary Deep Radiomic Sequencer Discovery for Pathologically-Proven Lung Cancer Detection
- **Arxiv ID**: http://arxiv.org/abs/1705.03572v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.03572v2)
- **Published**: 2017-05-10 00:20:23+00:00
- **Updated**: 2017-10-20 00:01:27+00:00
- **Authors**: Mohammad Javad Shafiee, Audrey G. Chung, Farzad Khalvati, Masoom A. Haider, Alexander Wong
- **Comment**: 26 pages
- **Journal**: None
- **Summary**: While lung cancer is the second most diagnosed form of cancer in men and women, a sufficiently early diagnosis can be pivotal in patient survival rates. Imaging-based, or radiomics-driven, detection methods have been developed to aid diagnosticians, but largely rely on hand-crafted features which may not fully encapsulate the differences between cancerous and healthy tissue. Recently, the concept of discovery radiomics was introduced, where custom abstract features are discovered from readily available imaging data. We propose a novel evolutionary deep radiomic sequencer discovery approach based on evolutionary deep intelligence. Motivated by patient privacy concerns and the idea of operational artificial intelligence, the evolutionary deep radiomic sequencer discovery approach organically evolves increasingly more efficient deep radiomic sequencers that produce significantly more compact yet similarly descriptive radiomic sequences over multiple generations. As a result, this framework improves operational efficiency and enables diagnosis to be run locally at the radiologist's computer while maintaining detection accuracy. We evaluated the evolved deep radiomic sequencer (EDRS) discovered via the proposed evolutionary deep radiomic sequencer discovery framework against state-of-the-art radiomics-driven and discovery radiomics methods using clinical lung CT data with pathologically-proven diagnostic data from the LIDC-IDRI dataset. The evolved deep radiomic sequencer shows improved sensitivity (93.42%), specificity (82.39%), and diagnostic accuracy (88.78%) relative to previous radiomics approaches.



### Collaborative Descriptors: Convolutional Maps for Preprocessing
- **Arxiv ID**: http://arxiv.org/abs/1705.03595v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.03595v1)
- **Published**: 2017-05-10 03:04:48+00:00
- **Updated**: 2017-05-10 03:04:48+00:00
- **Authors**: Hirokatsu Kataoka, Kaori Abe, Akio Nakamura, Yutaka Satoh
- **Comment**: CVPR 2017 Workshop Submission
- **Journal**: None
- **Summary**: The paper presents a novel concept for collaborative descriptors between deeply learned and hand-crafted features. To achieve this concept, we apply convolutional maps for pre-processing, namely the convovlutional maps are used as input of hand-crafted features. We recorded an increase in the performance rate of +17.06 % (multi-class object recognition) and +24.71 % (car detection) from grayscale input to convolutional maps. Although the framework is straight-forward, the concept should be inherited for an improved representation.



### Phase recovery and holographic image reconstruction using deep learning in neural networks
- **Arxiv ID**: http://arxiv.org/abs/1705.04286v1
- **DOI**: 10.1038/lsa.2017.141
- **Categories**: **cs.CV**, cs.IR, cs.LG, physics.app-ph, physics.optics, 68T01, 68T05, 68U10, 62M45, 78M32, 92C55, 94A08, I.2; I.2.1; I.2.6; I.2.10; I.4.5; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1705.04286v1)
- **Published**: 2017-05-10 03:26:30+00:00
- **Updated**: 2017-05-10 03:26:30+00:00
- **Authors**: Yair Rivenson, Yibo Zhang, Harun Gunaydin, Da Teng, Aydogan Ozcan
- **Comment**: None
- **Journal**: Light: Science and Applications, 7, e17141 (2018)
- **Summary**: Phase recovery from intensity-only measurements forms the heart of coherent imaging techniques and holography. Here we demonstrate that a neural network can learn to perform phase recovery and holographic image reconstruction after appropriate training. This deep learning-based approach provides an entirely new framework to conduct holographic imaging by rapidly eliminating twin-image and self-interference related spatial artifacts. Compared to existing approaches, this neural network based method is significantly faster to compute, and reconstructs improved phase and amplitude images of the objects using only one hologram, i.e., requires less number of measurements in addition to being computationally faster. We validated this method by reconstructing phase and amplitude images of various samples, including blood and Pap smears, and tissue sections. These results are broadly applicable to any phase recovery problem, and highlight that through machine learning challenging problems in imaging science can be overcome, providing new avenues to design powerful computational imaging systems.



### Learning RGB-D Salient Object Detection using background enclosure, depth contrast, and top-down features
- **Arxiv ID**: http://arxiv.org/abs/1705.03607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03607v1)
- **Published**: 2017-05-10 05:12:45+00:00
- **Updated**: 2017-05-10 05:12:45+00:00
- **Authors**: Riku Shigematsu, David Feng, Shaodi You, Nick Barnes
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep Convolutional Neural Networks (CNN) have demonstrated strong performance on RGB salient object detection. Although, depth information can help improve detection results, the exploration of CNNs for RGB-D salient object detection remains limited. Here we propose a novel deep CNN architecture for RGB-D salient object detection that exploits high-level, mid-level, and low level features. Further, we present novel depth features that capture the ideas of background enclosure and depth contrast that are suitable for a learned approach. We show improved results compared to state-of-the-art RGB-D salient object detection methods. We also show that the low-level and mid-level depth features both contribute to improvements in the results. Especially, F-Score of our method is 0.848 on RGBD1000 dataset, which is 10.7% better than the second place.



### Inferring and Executing Programs for Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1705.03633v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.03633v1)
- **Published**: 2017-05-10 07:08:23+00:00
- **Updated**: 2017-05-10 07:08:23+00:00
- **Authors**: Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick
- **Comment**: None
- **Journal**: None
- **Summary**: Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings.



### 4d isip: 4d implicit surface interest point detection
- **Arxiv ID**: http://arxiv.org/abs/1705.03634v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03634v2)
- **Published**: 2017-05-10 07:12:31+00:00
- **Updated**: 2017-06-23 07:18:54+00:00
- **Authors**: Shirui Li, Alper Yilmaz, Changlin Xiao, Hua Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new method to detect 4D spatiotemporal interest points though an implicit surface, we refer to as the 4D-ISIP. We use a 3D volume which has a truncated signed distance function(TSDF) for every voxel to represent our 3D object model. The TSDF represents the distance between the spatial points and object surface points which is an implicit surface representation. Our novelty is to detect the points where the local neighborhood has significant variations along both spatial and temporal directions. We established a system to acquire 3D human motion dataset using only one Kinect. Experimental results show that our method can detect 4D-ISIP for different human actions.



### Context-aware stacked convolutional neural networks for classification of breast carcinomas in whole-slide histopathology images
- **Arxiv ID**: http://arxiv.org/abs/1705.03678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03678v1)
- **Published**: 2017-05-10 10:05:06+00:00
- **Updated**: 2017-05-10 10:05:06+00:00
- **Authors**: Babak Ehteshami Bejnordi, Guido Zuidhof, Maschenka Balkenhol, Meyke Hermsen, Peter Bult, Bram van Ginneken, Nico Karssemeijer, Geert Litjens, Jeroen van der Laak
- **Comment**: None
- **Journal**: None
- **Summary**: Automated classification of histopathological whole-slide images (WSI) of breast tissue requires analysis at very high resolutions with a large contextual area. In this paper, we present context-aware stacked convolutional neural networks (CNN) for classification of breast WSIs into normal/benign, ductal carcinoma in situ (DCIS), and invasive ductal carcinoma (IDC). We first train a CNN using high pixel resolution patches to capture cellular level information. The feature responses generated by this model are then fed as input to a second CNN, stacked on top of the first. Training of this stacked architecture with large input patches enables learning of fine-grained (cellular) details and global interdependence of tissue structures. Our system is trained and evaluated on a dataset containing 221 WSIs of H&E stained breast tissue specimens. The system achieves an AUC of 0.962 for the binary classification of non-malignant and malignant slides and obtains a three class accuracy of 81.3% for classification of WSIs into normal/benign, DCIS, and IDC, demonstrating its potentials for routine diagnostics.



### Efficient and Scalable View Generation from a Single Image using Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.03737v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03737v3)
- **Published**: 2017-05-10 13:02:24+00:00
- **Updated**: 2019-09-04 23:30:05+00:00
- **Authors**: Sung-Ho Bae, Mohamed Elgharib, Mohamed Hefeeda, Wojciech Matusik
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Single-image-based view generation (SIVG) is important for producing 3D stereoscopic content. Here, handling different spatial resolutions as input and optimizing both reconstruction accuracy and processing speed is desirable. Latest approaches are based on convolutional neural network (CNN), and they generate promising results. However, their use of fully connected layers as well as pre-trained VGG forces a compromise between reconstruction accuracy and processing speed. In addition, this approach is limited to the use of a specific spatial resolution. To remedy these problems, we propose exploiting fully convolutional networks (FCN) for SIVG. We present two FCN architectures for SIVG. The first one is based on combination of an FCN and a view-rendering network called DeepView$_{ren}$. The second one consists of decoupled networks for luminance and chrominance signals, denoted by DeepView$_{dec}$. To train our solutions we present a large dataset of 2M stereoscopic images. Results show that both of our architectures improve accuracy and speed over the state of the art. DeepView$_{ren}$ generates competitive accuracy to the state of the art, however, with the fastest processing speed of all. That is x5 times faster speed and x24 times lower memory consumption compared to the state of the art. DeepView$_{dec}$ has much higher accuracy, but with x2.5 times faster speed and x12 times lower memory consumption. We evaluated our approach with both objective and subjective studies.



### Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.03820v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03820v3)
- **Published**: 2017-05-10 15:29:52+00:00
- **Updated**: 2017-06-03 22:47:47+00:00
- **Authors**: Hao Dong, Guang Yang, Fangde Liu, Yuanhan Mo, Yike Guo
- **Comment**: Medical Image Understanding and Analysis (MIUA) 2017
- **Journal**: None
- **Summary**: A major challenge in brain tumor treatment planning and quantitative evaluation is determination of the tumor extent. The noninvasive magnetic resonance imaging (MRI) technique has emerged as a front-line diagnostic tool for brain tumors without ionizing radiation. Manual segmentation of brain tumor extent from 3D MRI volumes is a very time-consuming task and the performance is highly relied on operator's experience. In this context, a reliable fully automatic segmentation method for the brain tumor segmentation is necessary for an efficient measurement of the tumor extent. In this study, we propose a fully automatic method for brain tumor segmentation, which is developed using U-Net based deep convolutional networks. Our method was evaluated on Multimodal Brain Tumor Image Segmentation (BRATS 2015) datasets, which contain 220 high-grade brain tumor and 54 low-grade tumor cases. Cross-validation has shown that our method can obtain promising segmentation efficiently.



### Predicting the Driver's Focus of Attention: the DR(eye)VE Project
- **Arxiv ID**: http://arxiv.org/abs/1705.03854v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03854v3)
- **Published**: 2017-05-10 17:00:14+00:00
- **Updated**: 2018-06-06 08:54:53+00:00
- **Authors**: Andrea Palazzi, Davide Abati, Simone Calderara, Francesco Solera, Rita Cucchiara
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: In this work we aim to predict the driver's focus of attention. The goal is to estimate what a person would pay attention to while driving, and which part of the scene around the vehicle is more critical for the task. To this end we propose a new computer vision model based on a multi-branch deep architecture that integrates three sources of information: raw video, motion and scene semantics. We also introduce DR(eye)VE, the largest dataset of driving scenes for which eye-tracking annotations are available. This dataset features more than 500,000 registered frames, matching ego-centric views (from glasses worn by drivers) and car-centric views (from roof-mounted camera), further enriched by other sensors measurements. Results highlight that several attention patterns are shared across drivers and can be reproduced to some extent. The indication of which elements in the scene are likely to capture the driver's attention may benefit several applications in the context of human-vehicle interaction and driver attention analysis.



### Survey of Visual Question Answering: Datasets and Techniques
- **Arxiv ID**: http://arxiv.org/abs/1705.03865v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.03865v2)
- **Published**: 2017-05-10 17:30:17+00:00
- **Updated**: 2017-05-11 06:46:52+00:00
- **Authors**: Akshay Kumar Gupta
- **Comment**: 10 pages, 3 figures, 3 tables Added references, corrected typos, made
  references less wordy
- **Journal**: None
- **Summary**: Visual question answering (or VQA) is a new and exciting problem that combines natural language processing and computer vision techniques. We present a survey of the various datasets and models that have been used to tackle this task. The first part of the survey details the various datasets for VQA and compares them along some common factors. The second part of this survey details the different approaches for VQA, classified into four types: non-deep learning models, deep learning models without attention, deep learning models with attention, and other models which do not fit into the first three. Finally, we compare the performances of these approaches and provide some directions for future work.



### An Improved Video Analysis using Context based Extension of LSH
- **Arxiv ID**: http://arxiv.org/abs/1705.03933v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03933v2)
- **Published**: 2017-05-10 19:42:51+00:00
- **Updated**: 2018-05-29 13:49:03+00:00
- **Authors**: Angana Chakraborty, Sanghamitra Bandyopadhyay
- **Comment**: The work us regarding a conceptual phenomenon which is found no
  longer valid in Computer Vision (we are not continuing this work). Rather we
  want to apply it to Bioinformatics (different domain ). Therefore, it will
  not be a replacement of this submission. As, a significant portion of this
  submission is not valid and fullproof, we want to withdraw this submission
- **Journal**: None
- **Summary**: Locality Sensitive Hashing (LSH) based algorithms have already shown their promise in finding approximate nearest neighbors in high dimen- sional data space. However, there are certain scenarios, as in sequential data, where the proximity of a pair of points cannot be captured without considering their surroundings or context. In videos, as for example, a particular frame is meaningful only when it is seen in the context of its preceding and following frames. LSH has no mechanism to handle the con- texts of the data points. In this article, a novel scheme of Context based Locality Sensitive Hashing (conLSH) has been introduced, in which points are hashed together not only based on their closeness, but also because of similar context. The contribution made in this article is three fold. First, conLSH is integrated with a recently proposed fast optimal sequence alignment algorithm (FOGSAA) using a layered approach. The resultant method is applied to video retrieval for extracting similar sequences. The pro- posed algorithm yields more than 80% accuracy on an average in different datasets. It has been found to save 36.3% of the total time, consumed by the exhaustive search. conLSH reduces the search space to approximately 42% of the entire dataset, when compared with an exhaustive search by the aforementioned FOGSAA, Bag of Words method and the standard LSH implementations. Secondly, the effectiveness of conLSH is demon- strated in action recognition of the video clips, which yields an average gain of 12.83% in terms of classification accuracy over the state of the art methods using STIP descriptors. The last but of great significance is that this article provides a way of automatically annotating long and composite real life videos. The source code of conLSH is made available at http://www.isical.ac.in/~bioinfo_miu/conLSH/conLSH.html



### Learning 3D Object Categories by Looking Around Them
- **Arxiv ID**: http://arxiv.org/abs/1705.03951v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03951v3)
- **Published**: 2017-05-10 21:01:39+00:00
- **Updated**: 2021-12-02 14:49:48+00:00
- **Authors**: David Novotny, Diane Larlus, Andrea Vedaldi
- **Comment**: Proceedings of the International Conference on Computer Vision, 2017
- **Journal**: None
- **Summary**: Traditional approaches for learning 3D object categories use either synthetic data or manual supervision. In this paper, we propose a method which does not require manual annotations and is instead cued by observing objects from a moving vantage point. Our system builds on two innovations: a Siamese viewpoint factorization network that robustly aligns different videos together without explicitly comparing 3D shapes; and a 3D shape completion network that can extract the full shape of an object from partial observations. We also demonstrate the benefits of configuring networks to perform probabilistic predictions as well as of geometry-aware data augmentation schemes. We obtain state-of-the-art results on publicly-available benchmarks.



