# Arxiv Papers in cs.CV on 2017-05-31
### Weakly supervised 3D Reconstruction with Adversarial Constraint
- **Arxiv ID**: http://arxiv.org/abs/1705.10904v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10904v2)
- **Published**: 2017-05-31 01:00:34+00:00
- **Updated**: 2017-10-04 05:45:38+00:00
- **Authors**: JunYoung Gwak, Christopher B. Choy, Animesh Garg, Manmohan Chandraker, Silvio Savarese
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised 3D reconstruction has witnessed a significant progress through the use of deep neural networks. However, this increase in performance requires large scale annotations of 2D/3D data. In this paper, we explore inexpensive 2D supervision as an alternative for expensive 3D CAD annotation. Specifically, we use foreground masks as weak supervision through a raytrace pooling layer that enables perspective projection and backpropagation. Additionally, since the 3D reconstruction from masks is an ill posed problem, we propose to constrain the 3D reconstruction to the manifold of unlabeled realistic 3D shapes that match mask observations. We demonstrate that learning a log-barrier solution to this constrained optimization problem resembles the GAN objective, enabling the use of existing tools for training GANs. We evaluate and analyze the manifold constrained reconstruction on various datasets for single and multi-view reconstruction of both synthetic and real images.



### Unsupervised Learning of Disentangled Representations from Video
- **Arxiv ID**: http://arxiv.org/abs/1705.10915v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.10915v1)
- **Published**: 2017-05-31 02:12:19+00:00
- **Updated**: 2017-05-31 02:12:19+00:00
- **Authors**: Emily Denton, Vighnesh Birodkar
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new model DrNET that learns disentangled image representations from video. Our approach leverages the temporal coherence of video and a novel adversarial loss to learn a representation that factorizes each frame into a stationary part and a temporally varying component. The disentangled representation can be used for a range of tasks. For example, applying a standard LSTM to the time-vary components enables prediction of future frames. We evaluate our approach on a range of synthetic and real videos, demonstrating the ability to coherently generate hundreds of steps into the future.



### Naturally Combined Shape-Color Moment Invariants under Affine Transformations
- **Arxiv ID**: http://arxiv.org/abs/1705.10928v2
- **DOI**: 10.1016/j.cviu.2017.07.003
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10928v2)
- **Published**: 2017-05-31 03:04:35+00:00
- **Updated**: 2017-07-27 02:44:57+00:00
- **Authors**: Ming Gong, You Hao, Hanlin Mo, Hua Li
- **Comment**: None
- **Journal**: None
- **Summary**: We proposed a kind of naturally combined shape-color affine moment invariants (SCAMI), which consider both shape and color affine transformations simultaneously in one single system. In the real scene, color and shape deformations always exist in images simultaneously. Simple shape invariants or color invariants can not be qualified for this situation. The conventional method is just to make a simple linear combination of the two factors. Meanwhile, the manual selection of weights is a complex issue. Our construction method is based on the multiple integration framework. The integral kernel is assigned as the continued product of the shape and color invariant cores. It is the first time to directly derive an invariant to dual affine transformations of shape and color. The manual selection of weights is no longer necessary, and both the shape and color transformations are extended to affine transformation group. With the various of invariant cores, a set of lower-order invariants are constructed and the completeness and independence are discussed detailedly. A set of SCAMIs, which called SCAMI24, are recommended, and the effectiveness and robustness have been evaluated on both synthetic and real datasets.



### Micro Fourier Transform Profilometry ($Î¼$FTP): 3D shape measurement at 10,000 frames per second
- **Arxiv ID**: http://arxiv.org/abs/1705.10930v1
- **DOI**: 10.1016/j.optlaseng.2017.10.013
- **Categories**: **physics.ins-det**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1705.10930v1)
- **Published**: 2017-05-31 03:09:11+00:00
- **Updated**: 2017-05-31 03:09:11+00:00
- **Authors**: Chao Zuo, Tianyang Tao, Shijie Feng, Lei Huang, Anand Asundi, Qian Chen
- **Comment**: This manuscript was originally submitted on 30th January 17
- **Journal**: None
- **Summary**: Recent advances in imaging sensors and digital light projection technology have facilitated a rapid progress in 3D optical sensing, enabling 3D surfaces of complex-shaped objects to be captured with improved resolution and accuracy. However, due to the large number of projection patterns required for phase recovery and disambiguation, the maximum fame rates of current 3D shape measurement techniques are still limited to the range of hundreds of frames per second (fps). Here, we demonstrate a new 3D dynamic imaging technique, Micro Fourier Transform Profilometry ($\mu$FTP), which can capture 3D surfaces of transient events at up to 10,000 fps based on our newly developed high-speed fringe projection system. Compared with existing techniques, $\mu$FTP has the prominent advantage of recovering an accurate, unambiguous, and dense 3D point cloud with only two projected patterns. Furthermore, the phase information is encoded within a single high-frequency fringe image, thereby allowing motion-artifact-free reconstruction of transient events with temporal resolution of 50 microseconds. To show $\mu$FTP's broad utility, we use it to reconstruct 3D videos of 4 transient scenes: vibrating cantilevers, rotating fan blades, bullet fired from a toy gun, and balloon's explosion triggered by a flying dart, which were previously difficult or even unable to be captured with conventional approaches.



### Bridge Simulation and Metric Estimation on Landmark Manifolds
- **Arxiv ID**: http://arxiv.org/abs/1705.10943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10943v1)
- **Published**: 2017-05-31 05:08:08+00:00
- **Updated**: 2017-05-31 05:08:08+00:00
- **Authors**: Stefan Sommer, Alexis Arnaudon, Line Kuhnel, Sarang Joshi
- **Comment**: None
- **Journal**: None
- **Summary**: We present an inference algorithm and connected Monte Carlo based estimation procedures for metric estimation from landmark configurations distributed according to the transition distribution of a Riemannian Brownian motion arising from the Large Deformation Diffeomorphic Metric Mapping (LDDMM) metric. The distribution possesses properties similar to the regular Euclidean normal distribution but its transition density is governed by a high-dimensional PDE with no closed-form solution in the nonlinear case. We show how the density can be numerically approximated by Monte Carlo sampling of conditioned Brownian bridges, and we use this to estimate parameters of the LDDMM kernel and thus the metric structure by maximum likelihood.



### Class Specific Feature Selection for Interval Valued Data Through Interval K-Means Clustering
- **Arxiv ID**: http://arxiv.org/abs/1705.10986v1
- **DOI**: 10.1007/978-981-10-4859-3
- **Categories**: **cs.CV**, 68T10, I.5.2; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/1705.10986v1)
- **Published**: 2017-05-31 08:43:58+00:00
- **Updated**: 2017-05-31 08:43:58+00:00
- **Authors**: D. S. Guru, N. Vinay Kumar
- **Comment**: 12 Pages, 3 figures, 7 tables
- **Journal**: RTIP2R 2016, CCIS 709, pp. 228 TO 239, 2017
- **Summary**: In this paper, a novel feature selection approach for supervised interval valued features is proposed. The proposed approach takes care of selecting the class specific features through interval K-Means clustering. The kernel of K-Means clustering algorithm is modified to adapt interval valued data. During training, a set of samples corresponding to a class is fed into the interval K-Means clustering algorithm, which clusters features into K distinct clusters. Hence, there are K number of features corresponding to each class. Subsequently, corresponding to each class, the cluster representatives are chosen. This procedure is repeated for all the samples of remaining classes. During testing the feature indices correspond to each class are used for validating the given dataset through classification using suitable symbolic classifiers. For experimentation, four standard supervised interval datasets are used. The results show the superiority of the proposed model when compared with the other existing state-of-the-art feature selection methods.



### Deep Supervised Discrete Hashing
- **Arxiv ID**: http://arxiv.org/abs/1705.10999v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.10999v2)
- **Published**: 2017-05-31 09:16:38+00:00
- **Updated**: 2017-11-27 14:24:08+00:00
- **Authors**: Qi Li, Zhenan Sun, Ran He, Tieniu Tan
- **Comment**: Accepted by NIPS 2017
- **Journal**: None
- **Summary**: With the rapid growth of image and video data on the web, hashing has been extensively studied for image or video search in recent years. Benefit from recent advances in deep learning, deep hashing methods have achieved promising results for image retrieval. However, there are some limitations of previous deep hashing methods (e.g., the semantic information is not fully exploited). In this paper, we develop a deep supervised discrete hashing algorithm based on the assumption that the learned binary codes should be ideal for classification. Both the pairwise label information and the classification information are used to learn the hash codes within one stream framework. We constrain the outputs of the last layer to be binary codes directly, which is rarely investigated in deep hashing algorithm. Because of the discrete nature of hash codes, an alternating minimization method is used to optimize the objective function. Experimental results have shown that our method outperforms current state-of-the-art methods on benchmark datasets.



### Brain Tumor Detection and Classification with Feed Forward Back-Prop Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1706.06411v1
- **DOI**: 10.5120/ijca2016910738
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.06411v1)
- **Published**: 2017-05-31 12:04:40+00:00
- **Updated**: 2017-05-31 12:04:40+00:00
- **Authors**: Neha Rani, Sharda Vashisth
- **Comment**: None
- **Journal**: International Journal of Computer Applications (0975 -- 8887),
  Volume 146, No.12, July 2016
- **Summary**: Brain is an organ that controls activities of all the parts of the body. Recognition of automated brain tumor in Magnetic resonance imaging (MRI) is a difficult task due to complexity of size and location variability. This automatic method detects all the type of cancer present in the body. Previous methods for tumor are time consuming and less accurate. In the present work, statistical analysis morphological and thresholding techniques are used to process the images obtained by MRI. Feed-forward back-prop neural network is used to classify the performance of tumors part of the image. This method results high accuracy and less iterations detection which further reduces the consumption time.



### Neuron Segmentation Using Deep Complete Bipartite Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.11053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.11053v1)
- **Published**: 2017-05-31 12:15:59+00:00
- **Updated**: 2017-05-31 12:15:59+00:00
- **Authors**: Jianxu Chen, Sreya Banerjee, Abhinav Grama, Walter J. Scheirer, Danny Z. Chen
- **Comment**: miccai 2017
- **Journal**: None
- **Summary**: In this paper, we consider the problem of automatically segmenting neuronal cells in dual-color confocal microscopy images. This problem is a key task in various quantitative analysis applications in neuroscience, such as tracing cell genesis in Danio rerio (zebrafish) brains. Deep learning, especially using fully convolutional networks (FCN), has profoundly changed segmentation research in biomedical imaging. We face two major challenges in this problem. First, neuronal cells may form dense clusters, making it difficult to correctly identify all individual cells (even to human experts). Consequently, segmentation results of the known FCN-type models are not accurate enough. Second, pixel-wise ground truth is difficult to obtain. Only a limited amount of approximate instance-wise annotation can be collected, which makes the training of FCN models quite cumbersome. We propose a new FCN-type deep learning model, called deep complete bipartite networks (CB-Net), and a new scheme for leveraging approximate instance-wise annotation to train our pixel-wise prediction model. Evaluated using seven real datasets, our proposed new CB-Net model outperforms the state-of-the-art FCN models and produces neuron segmentation results of remarkable quality



### EvaluationNet: Can Human Skill be Evaluated by Deep Networks?
- **Arxiv ID**: http://arxiv.org/abs/1705.11077v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.11077v1)
- **Published**: 2017-05-31 13:28:01+00:00
- **Updated**: 2017-05-31 13:28:01+00:00
- **Authors**: Seong Tae Kim, Yong Man Ro
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: With the recent substantial growth of media such as YouTube, a considerable number of instructional videos covering a wide variety of tasks are available online. Therefore, online instructional videos have become a rich resource for humans to learn everyday skills. In order to improve the effectiveness of the learning with instructional video, observation and evaluation of the activity are required. However, it is difficult to observe and evaluate every activity steps by expert. In this study, a novel deep learning framework which targets human activity evaluation for learning from instructional video has been proposed. In order to deal with the inherent variability of activities, we propose to model activity as a structured process. First, action units are encoded from dense trajectories with LSTM network. The variable-length action unit features are then evaluated by a Siamese LSTM network. By the comparative experiments on public dataset, the effectiveness of the proposed method has been demonstrated.



### Representation Learning by Rotating Your Faces
- **Arxiv ID**: http://arxiv.org/abs/1705.11136v2
- **DOI**: 10.1109/TPAMI.2018.2868350
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.11136v2)
- **Published**: 2017-05-31 15:18:37+00:00
- **Updated**: 2018-09-11 22:35:23+00:00
- **Authors**: Luan Tran, Xi Yin, Xiaoming Liu
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI)
- **Journal**: None
- **Summary**: The large pose discrepancy between two face images is one of the fundamental challenges in automatic face recognition. Conventional approaches to pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes a Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator enables DR-GAN to learn a representation that is both generative and discriminative, which can be used for face image synthesis and pose-invariant face recognition. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified identity representation along with an arbitrary number of synthetic face images. Extensive quantitative and qualitative evaluation on a number of controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art in both learning representations and rotating large-pose face images.



### Adversarial Inverse Graphics Networks: Learning 2D-to-3D Lifting and Image-to-Image Translation from Unpaired Supervision
- **Arxiv ID**: http://arxiv.org/abs/1705.11166v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.11166v3)
- **Published**: 2017-05-31 16:30:07+00:00
- **Updated**: 2017-09-02 01:10:17+00:00
- **Authors**: Hsiao-Yu Fish Tung, Adam W. Harley, William Seto, Katerina Fragkiadaki
- **Comment**: None
- **Journal**: The IEEE International Conference on Computer Vision (ICCV), 2017,
  pp. 4354-4362
- **Summary**: Researchers have developed excellent feed-forward models that learn to map images to desired outputs, such as to the images' latent factors, or to other images, using supervised learning. Learning such mappings from unlabelled data, or improving upon supervised models by exploiting unlabelled data, remains elusive. We argue that there are two important parts to learning without annotations: (i) matching the predictions to the input observations, and (ii) matching the predictions to known priors. We propose Adversarial Inverse Graphics networks (AIGNs): weakly supervised neural network models that combine feedback from rendering their predictions, with distribution matching between their predictions and a collection of ground-truth factors. We apply AIGNs to 3D human pose estimation and 3D structure and egomotion estimation, and outperform models supervised by only paired annotations. We further apply AIGNs to facial image transformation using super-resolution and inpainting renderers, while deliberately adding biases in the ground-truth datasets. Our model seamlessly incorporates such biases, rendering input faces towards young, old, feminine, masculine or Tom Cruise-like equivalents (depending on the chosen bias), or adding lip and nose augmentations while inpainting concealed lips and noses.



### Long-term Correlation Tracking using Multi-layer Hybrid Features in Sparse and Dense Environments
- **Arxiv ID**: http://arxiv.org/abs/1705.11175v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.11175v6)
- **Published**: 2017-05-31 16:44:45+00:00
- **Updated**: 2019-02-03 21:19:22+00:00
- **Authors**: Nathanael L. Baisa, Deepayan Bhowmik, Andrew Wallace
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking a target of interest in both sparse and crowded environments is a challenging problem, not yet successfully addressed in the literature. In this paper, we propose a new long-term visual tracking algorithm, learning discriminative correlation filters and using an online classifier, to track a target of interest in both sparse and crowded video sequences. First, we learn a translation correlation filter using a multi-layer hybrid of convolutional neural networks (CNN) and traditional hand-crafted features. We combine advantages of both the lower convolutional layer which retains more spatial details for precise localization and the higher convolutional layer which encodes semantic information for handling appearance variations, and then integrate these with histogram of oriented gradients (HOG) and color-naming traditional features. Second, we include a re-detection module for overcoming tracking failures due to long-term occlusions by training an incremental (online) SVM on the most confident frames using hand-engineered features. This re-detection module is activated only when the correlation response of the object is below some pre-defined threshold. This generates high score detection proposals which are temporally filtered using a Gaussian mixture probability hypothesis density (GM-PHD) filter to find the detection proposal with the maximum weight as the target state estimate by removing the other detection proposals as clutter. Finally, we learn a scale correlation filter for estimating the scale of a target by constructing a target pyramid around the estimated or re-detected position using the HOG features. We carry out extensive experiments on both sparse and dense data sets which show that our method significantly outperforms state-of-the-art methods.



### U-Phylogeny: Undirected Provenance Graph Construction in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1705.11187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.11187v1)
- **Published**: 2017-05-31 17:33:31+00:00
- **Updated**: 2017-05-31 17:33:31+00:00
- **Authors**: Aparna Bharati, Daniel Moreira, Allan Pinto, Joel Brogan, Kevin Bowyer, Patrick Flynn, Walter Scheirer, Anderson Rocha
- **Comment**: 5 pages, Accepted in International Conference on Image Processing,
  2017
- **Journal**: None
- **Summary**: Deriving relationships between images and tracing back their history of modifications are at the core of Multimedia Phylogeny solutions, which aim to combat misinformation through doctored visual media. Nonetheless, most recent image phylogeny solutions cannot properly address cases of forged composite images with multiple donors, an area known as multiple parenting phylogeny (MPP). This paper presents a preliminary undirected graph construction solution for MPP, without any strict assumptions. The algorithm is underpinned by robust image representative keypoints and different geometric consistency checks among matching regions in both images to provide regions of interest for direct comparison. The paper introduces a novel technique to geometrically filter the most promising matches as well as to aid in the shared region localization task. The strength of the approach is corroborated by experiments with real-world cases, with and without image distractors (unrelated cases).



### Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols
- **Arxiv ID**: http://arxiv.org/abs/1705.11192v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/1705.11192v2)
- **Published**: 2017-05-31 17:47:55+00:00
- **Updated**: 2017-11-04 15:04:51+00:00
- **Authors**: Serhii Havrylov, Ivan Titov
- **Comment**: The paper was accepted at NIPS 2017. The extended abstract was
  presented at ICLR 2017 workshop track
- **Journal**: None
- **Summary**: Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.



### Development of a N-type GM-PHD Filter for Multiple Target, Multiple Type Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1706.00672v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00672v5)
- **Published**: 2017-05-31 18:03:33+00:00
- **Updated**: 2019-02-03 22:44:54+00:00
- **Authors**: Nathanael L. Baisa, Andrew Wallace
- **Comment**: arXiv admin note: text overlap with arXiv:1705.04757
- **Journal**: None
- **Summary**: We propose a new framework that extends the standard Probability Hypothesis Density (PHD) filter for multiple targets having $N\geq2$ different types based on Random Finite Set theory, taking into account not only background clutter, but also confusions among detections of different target types, which are in general different in character from background clutter. Under Gaussianity and linearity assumptions, our framework extends the existing Gaussian mixture (GM) implementation of the standard PHD filter to create a N-type GM-PHD filter. The methodology is applied to real video sequences by integrating object detectors' information into this filter for two scenarios. For both cases, Munkres's variant of the Hungarian assignment algorithm is used to associate tracked target identities between frames. This approach is evaluated and compared to both raw detection and independent GM-PHD filters using the Optimal Sub-pattern Assignment metric and discrimination rate. This shows the improved performance of our strategy on real video sequences.



### Deep Generative Adversarial Networks for Compressed Sensing Automates MRI
- **Arxiv ID**: http://arxiv.org/abs/1706.00051v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.00051v1)
- **Published**: 2017-05-31 19:12:14+00:00
- **Updated**: 2017-05-31 19:12:14+00:00
- **Authors**: Morteza Mardani, Enhao Gong, Joseph Y. Cheng, Shreyas Vasanawala, Greg Zaharchuk, Marcus Alley, Neil Thakur, Song Han, William Dally, John M. Pauly, Lei Xing
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic resonance image (MRI) reconstruction is a severely ill-posed linear inverse task demanding time and resource intensive computations that can substantially trade off {\it accuracy} for {\it speed} in real-time imaging. In addition, state-of-the-art compressed sensing (CS) analytics are not cognizant of the image {\it diagnostic quality}. To cope with these challenges we put forth a novel CS framework that permeates benefits from generative adversarial networks (GAN) to train a (low-dimensional) manifold of diagnostic-quality MR images from historical patients. Leveraging a mixture of least-squares (LS) GANs and pixel-wise $\ell_1$ cost, a deep residual network with skip connections is trained as the generator that learns to remove the {\it aliasing} artifacts by projecting onto the manifold. LSGAN learns the texture details, while $\ell_1$ controls the high-frequency noise. A multilayer convolutional neural network is then jointly trained based on diagnostic quality images to discriminate the projection quality. The test phase performs feed-forward propagation over the generator network that demands a very low computational overhead. Extensive evaluations are performed on a large contrast-enhanced MR dataset of pediatric patients. In particular, images rated based on expert radiologists corroborate that GANCS retrieves high contrast images with detailed texture relative to conventional CS, and pixel-wise schemes. In addition, it offers reconstruction under a few milliseconds, two orders of magnitude faster than state-of-the-art CS-MRI schemes.



### Putting a Face to the Voice: Fusing Audio and Visual Signals Across a Video to Determine Speakers
- **Arxiv ID**: http://arxiv.org/abs/1706.00079v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1706.00079v1)
- **Published**: 2017-05-31 20:35:26+00:00
- **Updated**: 2017-05-31 20:35:26+00:00
- **Authors**: Ken Hoover, Sourish Chaudhuri, Caroline Pantofaru, Malcolm Slaney, Ian Sturdy
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a system that associates faces with voices in a video by fusing information from the audio and visual signals. The thesis underlying our work is that an extremely simple approach to generating (weak) speech clusters can be combined with visual signals to effectively associate faces and voices by aggregating statistics across a video. This approach does not need any training data specific to this task and leverages the natural coherence of information in the audio and visual streams. It is particularly applicable to tracking speakers in videos on the web where a priori information about the environment (e.g., number of speakers, spatial signals for beamforming) is not available. We performed experiments on a real-world dataset using this analysis framework to determine the speaker in a video. Given a ground truth labeling determined by human rater consensus, our approach had ~71% accuracy.



### Megapixel Size Image Creation using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1706.00082v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1706.00082v1)
- **Published**: 2017-05-31 20:43:19+00:00
- **Updated**: 2017-05-31 20:43:19+00:00
- **Authors**: Marco Marchesi
- **Comment**: 3 pages, 4 figures
- **Journal**: None
- **Summary**: Since its appearance, Generative Adversarial Networks (GANs) have received a lot of interest in the AI community. In image generation several projects showed how GANs are able to generate photorealistic images but the results so far did not look adequate for the quality standard of visual media production industry. We present an optimized image generation process based on a Deep Convolutional Generative Adversarial Networks (DCGANs), in order to create photorealistic high-resolution images (up to 1024x1024 pixels). Furthermore, the system was fed with a limited dataset of images, less than two thousand images. All these results give more clue about future exploitation of GANs in Computer Graphics and Visual Effects.



### Blood capillaries and vessels segmentation in optical coherence tomography angiogram using fuzzy C-means and Curvelet transform
- **Arxiv ID**: http://arxiv.org/abs/1706.00083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00083v1)
- **Published**: 2017-05-31 20:44:55+00:00
- **Updated**: 2017-05-31 20:44:55+00:00
- **Authors**: Fariborz Taherkhani
- **Comment**: arXiv admin note: This paper has been removed from arXiv as the
  submitter did not have ownership of the data presented in this work
- **Journal**: None
- **Summary**: This paper has been removed from arXiv as the submitter did not have ownership of the data presented in this work.



### The in-town monitoring system for ambulance dispatch centre
- **Arxiv ID**: http://arxiv.org/abs/1706.03699v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.03699v1)
- **Published**: 2017-05-31 22:21:06+00:00
- **Updated**: 2017-05-31 22:21:06+00:00
- **Authors**: Bartlomiej Placzek, Jolnta Golosz
- **Comment**: 6 pages, 6 figures
- **Journal**: Journal of Medical Informatics & Technologies, vol. 5, MI101--106,
  2003
- **Summary**: The paper presents the vehicles integrated monitoring system giving priorities for emergency vehicles. The described system exploits the data gathered by: geographical positioning systems and geographical information systems. The digital maps and roadside cameras provide the dispatchers with aims for in town ambulances traffic management. The method of vehicles positioning in the city network and algorithms for ambulances recognition by image processing techniques have been discussed in the paper. These priorities are needed for an efficient life-saving actions that require the real-time controlling strategies.



### Superhuman Accuracy on the SNEMI3D Connectomics Challenge
- **Arxiv ID**: http://arxiv.org/abs/1706.00120v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00120v1)
- **Published**: 2017-05-31 23:19:37+00:00
- **Updated**: 2017-05-31 23:19:37+00:00
- **Authors**: Kisuk Lee, Jonathan Zung, Peter Li, Viren Jain, H. Sebastian Seung
- **Comment**: None
- **Journal**: None
- **Summary**: For the past decade, convolutional networks have been used for 3D reconstruction of neurons from electron microscopic (EM) brain images. Recent years have seen great improvements in accuracy, as evidenced by submissions to the SNEMI3D benchmark challenge. Here we report the first submission to surpass the estimate of human accuracy provided by the SNEMI3D leaderboard. A variant of 3D U-Net is trained on a primary task of predicting affinities between nearest neighbor voxels, and an auxiliary task of predicting long-range affinities. The training data is augmented by simulated image defects. The nearest neighbor affinities are used to create an oversegmentation, and then supervoxels are greedily agglomerated based on mean affinity. The resulting SNEMI3D score exceeds the estimate of human accuracy by a large margin. While one should be cautious about extrapolating from the SNEMI3D benchmark to real-world accuracy of large-scale neural circuit reconstruction, our result inspires optimism that the goal of full automation may be realizable in the future.



