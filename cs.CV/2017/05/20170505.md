# Arxiv Papers in cs.CV on 2017-05-05
### Motion Prediction Under Multimodality with Conditional Stochastic Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.02082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02082v1)
- **Published**: 2017-05-05 04:19:40+00:00
- **Updated**: 2017-05-05 04:19:40+00:00
- **Authors**: Katerina Fragkiadaki, Jonathan Huang, Alex Alemi, Sudheendra Vijayanarasimhan, Susanna Ricco, Rahul Sukthankar
- **Comment**: None
- **Journal**: None
- **Summary**: Given a visual history, multiple future outcomes for a video scene are equally probable, in other words, the distribution of future outcomes has multiple modes. Multimodality is notoriously hard to handle by standard regressors or classifiers: the former regress to the mean and the latter discretize a continuous high dimensional output space. In this work, we present stochastic neural network architectures that handle such multimodality through stochasticity: future trajectories of objects, body joints or frames are represented as deep, non-linear transformations of random (as opposed to deterministic) variables. Such random variables are sampled from simple Gaussian distributions whose means and variances are parametrized by the output of convolutional encoders over the visual history. We introduce novel convolutional architectures for predicting future body joint trajectories that outperform fully connected alternatives \cite{DBLP:journals/corr/WalkerDGH16}. We introduce stochastic spatial transformers through optical flow warping for predicting future frames, which outperform their deterministic equivalents \cite{DBLP:journals/corr/PatrauceanHC15}. Training stochastic networks involves an intractable marginalization over stochastic variables. We compare various training schemes that handle such marginalization through a) straightforward sampling from the prior, b) conditional variational autoencoders \cite{NIPS2015_5775,DBLP:journals/corr/WalkerDGH16}, and, c) a proposed K-best-sample loss that penalizes the best prediction under a fixed "prediction budget". We show experimental results on object trajectory prediction, human body joint trajectory prediction and video prediction under varying future uncertainty, validating quantitatively and qualitatively our architectural choices and training schemes.



### GRASS: Generative Recursive Autoencoders for Shape Structures
- **Arxiv ID**: http://arxiv.org/abs/1705.02090v2
- **DOI**: 10.1145/3072959.3073613
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.02090v2)
- **Published**: 2017-05-05 05:45:10+00:00
- **Updated**: 2017-05-13 04:49:23+00:00
- **Authors**: Jun Li, Kai Xu, Siddhartha Chaudhuri, Ersin Yumer, Hao Zhang, Leonidas Guibas
- **Comment**: Corresponding author: Kai Xu (kevin.kai.xu@gmail.com)
- **Journal**: ACM Transactions on Graphics (SIGGRAPH 2017) 36, 4, Article 52
- **Summary**: We introduce a novel neural network architecture for encoding and synthesis of 3D shapes, particularly their structures. Our key insight is that 3D shapes are effectively characterized by their hierarchical organization of parts, which reflects fundamental intra-shape relationships such as adjacency and symmetry. We develop a recursive neural net (RvNN) based autoencoder to map a flat, unlabeled, arbitrary part layout to a compact code. The code effectively captures hierarchical structures of man-made 3D objects of varying structural complexities despite being fixed-dimensional: an associated decoder maps a code back to a full hierarchy. The learned bidirectional mapping is further tuned using an adversarial setup to yield a generative model of plausible structures, from which novel structures can be sampled. Finally, our structure synthesis framework is augmented by a second trained module that produces fine-grained part geometry, conditioned on global and local structural context, leading to a full generative pipeline for 3D shapes. We demonstrate that without supervision, our network learns meaningful structural hierarchies adhering to perceptual grouping principles, produces compact codes which enable applications such as shape classification and partial matching, and supports shape synthesis and interpolation with significant variations in topology and geometry.



### Learning to see people like people
- **Arxiv ID**: http://arxiv.org/abs/1705.04282v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.04282v1)
- **Published**: 2017-05-05 05:47:15+00:00
- **Updated**: 2017-05-05 05:47:15+00:00
- **Authors**: Amanda Song, Linjie Li, Chad Atalla, Garrison Cottrell
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Humans make complex inferences on faces, ranging from objective properties (gender, ethnicity, expression, age, identity, etc) to subjective judgments (facial attractiveness, trustworthiness, sociability, friendliness, etc). While the objective aspects of face perception have been extensively studied, relatively fewer computational models have been developed for the social impressions of faces. Bridging this gap, we develop a method to predict human impressions of faces in 40 subjective social dimensions, using deep representations from state-of-the-art neural networks. We find that model performance grows as the human consensus on a face trait increases, and that model predictions outperform human groups in correlation with human averages. This illustrates the learnability of subjective social perception of faces, especially when there is high human consensus. Our system can be used to decide which photographs from a personal collection will make the best impression. The results are significant for the field of social robotics, demonstrating that robots can learn the subjective judgments defining the underlying fabric of human interaction.



### Characterizing and Improving Stability in Neural Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1705.02092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02092v1)
- **Published**: 2017-05-05 05:54:05+00:00
- **Updated**: 2017-05-05 05:54:05+00:00
- **Authors**: Agrim Gupta, Justin Johnson, Alexandre Alahi, Li Fei-Fei
- **Comment**: None
- **Journal**: None
- **Summary**: Recent progress in style transfer on images has focused on improving the quality of stylized images and speed of methods. However, real-time methods are highly unstable resulting in visible flickering when applied to videos. In this work we characterize the instability of these methods by examining the solution set of the style transfer objective. We show that the trace of the Gram matrix representing style is inversely related to the stability of the method. Then, we present a recurrent convolutional network for real-time video style transfer which incorporates a temporal consistency loss and overcomes the instability of prior methods. Our networks can be applied at any resolution, do not re- quire optical flow at test time, and produce high quality, temporally consistent stylized videos in real-time.



### TALL: Temporal Activity Localization via Language Query
- **Arxiv ID**: http://arxiv.org/abs/1705.02101v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02101v2)
- **Published**: 2017-05-05 06:54:41+00:00
- **Updated**: 2017-08-03 21:31:07+00:00
- **Authors**: Jiyang Gao, Chen Sun, Zhenheng Yang, Ram Nevatia
- **Comment**: ICCV 2017 camera ready (with supplemental material)
- **Journal**: None
- **Summary**: This paper focuses on temporal localization of actions in untrimmed videos. Existing methods typically train classifiers for a pre-defined list of actions and apply them in a sliding window fashion. However, activities in the wild consist of a wide combination of actors, actions and objects; it is difficult to design a proper activity list that meets users' needs. We propose to localize activities by natural language queries. Temporal Activity Localization via Language (TALL) is challenging as it requires: (1) suitable design of text and video representations to allow cross-modal matching of actions and language queries; (2) ability to locate actions accurately given features from sliding windows of limited granularity. We propose a novel Cross-modal Temporal Regression Localizer (CTRL) to jointly model text query and video clips, output alignment scores and action boundary regression results for candidate clips. For evaluation, we adopt TaCoS dataset, and build a new dataset for this task on top of Charades by adding sentence temporal annotations, called Charades-STA. We also build complex sentence queries in Charades-STA for test. Experimental results show that CTRL outperforms previous methods significantly on both datasets.



### Phase Congruency Parameter Optimization for Enhanced Detection of Image Features for both Natural and Medical Applications
- **Arxiv ID**: http://arxiv.org/abs/1705.02102v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.02102v1)
- **Published**: 2017-05-05 06:58:03+00:00
- **Updated**: 2017-05-05 06:58:03+00:00
- **Authors**: Seyed Mohammad Mahdi Alavi, Yunyan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Following the presentation and proof of the hypothesis that image features are particularly perceived at points where the Fourier components are maximally in phase, the concept of phase congruency (PC) is introduced. Subsequently, a two-dimensional multi-scale phase congruency (2D-MSPC) is developed, which has been an important tool for detecting and evaluation of image features. However, the 2D-MSPC requires many parameters to be appropriately tuned for optimal image features detection. In this paper, we defined a criterion for parameter optimization of the 2D-MSPC, which is a function of its maximum and minimum moments. We formulated the problem in various optimal and suboptimal frameworks, and discussed the conditions and features of the suboptimal solutions. The effectiveness of the proposed method was verified through several examples, ranging from natural objects to medical images from patients with a neurological disease, multiple sclerosis.



### Bridging between Computer and Robot Vision through Data Augmentation: a Case Study on Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1705.02139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02139v1)
- **Published**: 2017-05-05 09:05:11+00:00
- **Updated**: 2017-05-05 09:05:11+00:00
- **Authors**: Antonio D'Innocente, Fabio Maria Carlucci, Mirco Colosi, Barbara Caputo
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the impressive progress brought by deep network in visual object recognition, robot vision is still far from being a solved problem. The most successful convolutional architectures are developed starting from ImageNet, a large scale collection of images of object categories downloaded from the Web. This kind of images is very different from the situated and embodied visual experience of robots deployed in unconstrained settings. To reduce the gap between these two visual experiences, this paper proposes a simple yet effective data augmentation layer that zooms on the object of interest and simulates the object detection outcome of a robot vision system. The layer, that can be used with any convolutional deep architecture, brings to an increase in object recognition performance of up to 7\%, in experiments performed over three different benchmark databases. Upon acceptance of the paper, our robot data augmentation layer will be made publicly available.



### Part-based Deep Hashing for Large-scale Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1705.02145v1
- **DOI**: 10.1109/TIP.2017.2695101
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02145v1)
- **Published**: 2017-05-05 09:24:13+00:00
- **Updated**: 2017-05-05 09:24:13+00:00
- **Authors**: Fuqing Zhu, Xiangwei Kong, Liang Zheng, Haiyan Fu, Qi Tian
- **Comment**: 12 pages, 4 figures. IEEE Transactions on Image Processing, 2017
- **Journal**: None
- **Summary**: Large-scale is a trend in person re-identification (re-id). It is important that real-time search be performed in a large gallery. While previous methods mostly focus on discriminative learning, this paper makes the attempt in integrating deep learning and hashing into one framework to evaluate the efficiency and accuracy for large-scale person re-id. We integrate spatial information for discriminative visual representation by partitioning the pedestrian image into horizontal parts. Specifically, Part-based Deep Hashing (PDH) is proposed, in which batches of triplet samples are employed as the input of the deep hashing architecture. Each triplet sample contains two pedestrian images (or parts) with the same identity and one pedestrian image (or part) of the different identity. A triplet loss function is employed with a constraint that the Hamming distance of pedestrian images (or parts) with the same identity is smaller than ones with the different identity. In the experiment, we show that the proposed Part-based Deep Hashing method yields very competitive re-id accuracy on the large-scale Market-1501 and Market-1501+500K datasets.



### Unified Embedding and Metric Learning for Zero-Exemplar Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1705.02148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02148v1)
- **Published**: 2017-05-05 09:45:58+00:00
- **Updated**: 2017-05-05 09:45:58+00:00
- **Authors**: Noureldien Hussein, Efstratios Gavves, Arnold W. M. Smeulders
- **Comment**: IEEE CVPR 2017
- **Journal**: None
- **Summary**: Event detection in unconstrained videos is conceived as a content-based video retrieval with two modalities: textual and visual. Given a text describing a novel event, the goal is to rank related videos accordingly. This task is zero-exemplar, no video examples are given to the novel event.   Related works train a bank of concept detectors on external data sources. These detectors predict confidence scores for test videos, which are ranked and retrieved accordingly. In contrast, we learn a joint space in which the visual and textual representations are embedded. The space casts a novel event as a probability of pre-defined events. Also, it learns to measure the distance between an event and its related videos.   Our model is trained end-to-end on publicly available EventNet. When applied to TRECVID Multimedia Event Detection dataset, it outperforms the state-of-the-art by a considerable margin.



### Unsupervised learning of object landmarks by factorized spatial embeddings
- **Arxiv ID**: http://arxiv.org/abs/1705.02193v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.02193v2)
- **Published**: 2017-05-05 12:45:35+00:00
- **Updated**: 2017-08-06 08:54:26+00:00
- **Authors**: James Thewlis, Hakan Bilen, Andrea Vedaldi
- **Comment**: To be published in ICCV 2017
- **Journal**: None
- **Summary**: Learning automatically the structure of object categories remains an important open problem in computer vision. In this paper, we propose a novel unsupervised approach that can discover and learn landmarks in object categories, thus characterizing their structure. Our approach is based on factorizing image deformations, as induced by a viewpoint change or an object deformation, by learning a deep neural network that detects landmarks consistently with such visual effects. Furthermore, we show that the learned landmarks establish meaningful correspondences between different object instances in a category without having to impose this requirement explicitly. We assess the method qualitatively on a variety of object types, natural and man-made. We also show that our unsupervised landmarks are highly predictive of manually-annotated landmarks in face benchmark datasets, and can be used to regress these with a high degree of accuracy.



### SEAGLE: Sparsity-Driven Image Reconstruction under Multiple Scattering
- **Arxiv ID**: http://arxiv.org/abs/1705.04281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04281v1)
- **Published**: 2017-05-05 13:05:26+00:00
- **Updated**: 2017-05-05 13:05:26+00:00
- **Authors**: Hsiou-Yuan Liu, Dehong Liu, Hassan Mansour, Petros T. Boufounos, Laura Waller, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple scattering of an electromagnetic wave as it passes through an object is a fundamental problem that limits the performance of current imaging systems. In this paper, we describe a new technique-called Series Expansion with Accelerated Gradient Descent on Lippmann-Schwinger Equation (SEAGLE)-for robust imaging under multiple scattering based on a combination of a new nonlinear forward model and a total variation (TV) regularizer. The proposed forward model can account for multiple scattering, which makes it advantageous in applications where linear models are inaccurate. Specifically, it corresponds to a series expansion of the scattered wave with an accelerated-gradient method. This expansion guarantees the convergence even for strongly scattering objects. One of our key insights is that it is possible to obtain an explicit formula for computing the gradient of our nonlinear forward model with respect to the unknown object, thus enabling fast image reconstruction with the state-of-the-art fast iterative shrinkage/thresholding algorithm (FISTA). The proposed method is validated on both simulated and experimentally measured data.



### S-OHEM: Stratified Online Hard Example Mining for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1705.02233v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02233v4)
- **Published**: 2017-05-05 14:13:17+00:00
- **Updated**: 2017-08-15 08:41:43+00:00
- **Authors**: Minne Li, Zhaoning Zhang, Hao Yu, Xinyuan Chen, Dongsheng Li
- **Comment**: 9 pages, 3 figures, accepted by CCCV 2017
- **Journal**: None
- **Summary**: One of the major challenges in object detection is to propose detectors with highly accurate localization of objects. The online sampling of high-loss region proposals (hard examples) uses the multitask loss with equal weight settings across all loss types (e.g, classification and localization, rigid and non-rigid categories) and ignores the influence of different loss distributions throughout the training process, which we find essential to the training efficacy. In this paper, we present the Stratified Online Hard Example Mining (S-OHEM) algorithm for training higher efficiency and accuracy detectors. S-OHEM exploits OHEM with stratified sampling, a widely-adopted sampling technique, to choose the training examples according to this influence during hard example mining, and thus enhance the performance of object detectors. We show through systematic experiments that S-OHEM yields an average precision (AP) improvement of 0.5% on rigid categories of PASCAL VOC 2007 for both the IoU threshold of 0.6 and 0.7. For KITTI 2012, both results of the same metric are 1.6%. Regarding the mean average precision (mAP), a relative increase of 0.3% and 0.5% (1% and 0.5%) is observed for VOC07 (KITTI12) using the same set of IoU threshold. Also, S-OHEM is easy to integrate with existing region-based detectors and is capable of acting with post-recognition level regressors.



### Detecting Adversarial Samples Using Density Ratio Estimates
- **Arxiv ID**: http://arxiv.org/abs/1705.02224v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.02224v4)
- **Published**: 2017-05-05 15:28:59+00:00
- **Updated**: 2017-11-20 16:17:18+00:00
- **Authors**: Lovedeep Gondara
- **Comment**: Updated
- **Journal**: None
- **Summary**: Machine learning models, especially based on deep architectures are used in everyday applications ranging from self driving cars to medical diagnostics. It has been shown that such models are dangerously susceptible to adversarial samples, indistinguishable from real samples to human eye, adversarial samples lead to incorrect classifications with high confidence. Impact of adversarial samples is far-reaching and their efficient detection remains an open problem. We propose to use direct density ratio estimation as an efficient model agnostic measure to detect adversarial samples. Our proposed method works equally well with single and multi-channel samples, and with different adversarial sample generation methods. We also propose a method to use density ratio estimates for generating adversarial samples with an added constraint of preserving density ratio.



### ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases
- **Arxiv ID**: http://arxiv.org/abs/1705.02315v5
- **DOI**: 10.1109/CVPR.2017.369
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1705.02315v5)
- **Published**: 2017-05-05 17:31:12+00:00
- **Updated**: 2017-12-14 19:35:31+00:00
- **Authors**: Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, Ronald M. Summers
- **Comment**: CVPR 2017 spotlight;V1: CVPR submission+supplementary; V2: Statistics
  and benchmark results on published ChestX-ray14 dataset are updated in
  Appendix B V3: Minor correction V4: new data download link upated:
  https://nihcc.app.box.com/v/ChestXray-NIHCC V5: Update benchmark results on
  the published data split in the appendix
- **Journal**: IEEE CVPR 2017, pp. 2097-2106 (2017)
- **Summary**: The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems.   In this paper, we present a new chest X-ray database, namely "ChestX-ray8", which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based "reading chest X-rays" (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems. Data download link: https://nihcc.app.box.com/v/ChestXray-NIHCC



### Face Detection, Bounding Box Aggregation and Pose Estimation for Robust Facial Landmark Localisation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1705.02402v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02402v2)
- **Published**: 2017-05-05 21:28:27+00:00
- **Updated**: 2017-06-01 10:28:51+00:00
- **Authors**: Zhen-Hua Feng, Josef Kittler, Muhammad Awais, Patrik Huber, Xiao-Jun Wu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a framework for robust face detection and landmark localisation of faces in the wild, which has been evaluated as part of `the 2nd Facial Landmark Localisation Competition'. The framework has four stages: face detection, bounding box aggregation, pose estimation and landmark localisation. To achieve a high detection rate, we use two publicly available CNN-based face detectors and two proprietary detectors. We aggregate the detected face bounding boxes of each input image to reduce false positives and improve face detection accuracy. A cascaded shape regressor, trained using faces with a variety of pose variations, is then employed for pose estimation and image pre-processing. Last, we train the final cascaded shape regressor for fine-grained landmark localisation, using a large number of training samples with limited pose variations. The experimental results obtained on the 300W and Menpo benchmarks demonstrate the superiority of our framework over state-of-the-art methods.



### DeepCorrect: Correcting DNN models against Image Distortions
- **Arxiv ID**: http://arxiv.org/abs/1705.02406v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02406v5)
- **Published**: 2017-05-05 21:52:49+00:00
- **Updated**: 2019-04-09 01:22:33+00:00
- **Authors**: Tejas Borkar, Lina Karam
- **Comment**: Accepted to IEEE Transactions on Image Processing, April 2019. For
  associated code, see https://github.com/tsborkar/DeepCorrect
- **Journal**: None
- **Summary**: In recent years, the widespread use of deep neural networks (DNNs) has facilitated great improvements in performance for computer vision tasks like image classification and object recognition. In most realistic computer vision applications, an input image undergoes some form of image distortion such as blur and additive noise during image acquisition or transmission. Deep networks trained on pristine images perform poorly when tested on such distortions. In this paper, we evaluate the effect of image distortions like Gaussian blur and additive noise on the activations of pre-trained convolutional filters. We propose a metric to identify the most noise susceptible convolutional filters and rank them in order of the highest gain in classification accuracy upon correction. In our proposed approach called DeepCorrect, we apply small stacks of convolutional layers with residual connections, at the output of these ranked filters and train them to correct the worst distortion affected filter activations, whilst leaving the rest of the pre-trained filter outputs in the network unchanged. Performance results show that applying DeepCorrect models for common vision tasks like image classification (ImageNet), object recognition (Caltech-101, Caltech-256) and scene classification (SUN-397), significantly improves the robustness of DNNs against distorted images and outperforms other alternative approaches..



### Knowledge-Guided Deep Fractal Neural Networks for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1705.02407v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02407v2)
- **Published**: 2017-05-05 22:06:55+00:00
- **Updated**: 2017-08-08 21:05:15+00:00
- **Authors**: Guanghan Ning, Zhi Zhang, Zhihai He
- **Comment**: 13 pages, 12 figures. arXiv admin note: text overlap with
  arXiv:1609.01743, arXiv:1702.07432, arXiv:1602.00134 by other authors
- **Journal**: None
- **Summary**: Human pose estimation using deep neural networks aims to map input images with large variations into multiple body keypoints which must satisfy a set of geometric constraints and inter-dependency imposed by the human body model. This is a very challenging nonlinear manifold learning process in a very high dimensional feature space. We believe that the deep neural network, which is inherently an algebraic computation system, is not the most effecient way to capture highly sophisticated human knowledge, for example those highly coupled geometric characteristics and interdependence between keypoints in human poses. In this work, we propose to explore how external knowledge can be effectively represented and injected into the deep neural networks to guide its training process using learned projections that impose proper prior. Specifically, we use the stacked hourglass design and inception-resnet module to construct a fractal network to regress human pose images into heatmaps with no explicit graphical modeling. We encode external knowledge with visual features which are able to characterize the constraints of human body models and evaluate the fitness of intermediate network output. We then inject these external features into the neural network using a projection matrix learned using an auxiliary cost function. The effectiveness of the proposed inception-resnet module and the benefit in guided learning with knowledge projection is evaluated on two widely used benchmarks. Our approach achieves state-of-the-art performance on both datasets.



### Residual Squeeze VGG16
- **Arxiv ID**: http://arxiv.org/abs/1705.03004v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03004v1)
- **Published**: 2017-05-05 23:46:26+00:00
- **Updated**: 2017-05-05 23:46:26+00:00
- **Authors**: Hussam Qassim, David Feinzimer, Abhishek Verma
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has given way to a new era of machine learning, apart from computer vision. Convolutional neural networks have been implemented in image classification, segmentation and object detection. Despite recent advancements, we are still in the very early stages and have yet to settle on best practices for network architecture in terms of deep design, small in size and a short training time. In this work, we propose a very deep neural network comprised of 16 Convolutional layers compressed with the Fire Module adapted from the SQUEEZENET model. We also call for the addition of residual connections to help suppress degradation. This model can be implemented on almost every neural network model with fully incorporated residual learning. This proposed model Residual-Squeeze-VGG16 (ResSquVGG16) trained on the large-scale MIT Places365-Standard scene dataset. In our tests, the model performed with accuracy similar to the pre-trained VGG16 model in Top-1 and Top-5 validation accuracy while also enjoying a 23.86% reduction in training time and an 88.4% reduction in size. In our tests, this model was trained from scratch.



