# Arxiv Papers in cs.CV on 2017-05-09
### CHAM: action recognition using convolutional hierarchical attention model
- **Arxiv ID**: http://arxiv.org/abs/1705.03146v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03146v2)
- **Published**: 2017-05-09 02:27:37+00:00
- **Updated**: 2017-05-19 06:11:26+00:00
- **Authors**: Shiyang Yan, Jeremy S. Smith, Wenjin Lu, Bailing Zhang
- **Comment**: accepted by ICIP2017
- **Journal**: None
- **Summary**: Recently, the soft attention mechanism, which was originally proposed in language processing, has been applied in computer vision tasks like image captioning. This paper presents improvements to the soft attention model by combining a convolutional LSTM with a hierarchical system architecture to recognize action categories in videos. We call this model the Convolutional Hierarchical Attention Model (CHAM). The model applies a convolutional operation inside the LSTM cell and an attention map generation process to recognize actions. The hierarchical architecture of this model is able to explicitly reason on multi-granularities of action categories. The proposed architecture achieved improved results on three publicly available datasets: the UCF sports dataset, the Olympic sports dataset and the HMDB51 dataset.



### Deep Spatio-temporal Manifold Network for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1705.03148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03148v1)
- **Published**: 2017-05-09 02:37:30+00:00
- **Updated**: 2017-05-09 02:37:30+00:00
- **Authors**: Ce Li, Chen Chen, Baochang Zhang, Qixiang Ye, Jungong Han, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Visual data such as videos are often sampled from complex manifold. We propose leveraging the manifold structure to constrain the deep action feature learning, thereby minimizing the intra-class variations in the feature space and alleviating the over-fitting problem. Considering that manifold can be transferred, layer by layer, from the data domain to the deep features, the manifold priori is posed from the top layer into the back propagation learning procedure of convolutional neural network (CNN). The resulting algorithm --Spatio-Temporal Manifold Network-- is solved with the efficient Alternating Direction Method of Multipliers and Backward Propagation (ADMM-BP). We theoretically show that STMN recasts the problem as projection over the manifold via an embedding method. The proposed approach is evaluated on two benchmark datasets, showing significant improvements to the baselines.



### Contour Detection from Deep Patch-level Boundary Prediction
- **Arxiv ID**: http://arxiv.org/abs/1705.03159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03159v1)
- **Published**: 2017-05-09 03:12:02+00:00
- **Updated**: 2017-05-09 03:12:02+00:00
- **Authors**: Teck Wee Chua, Li Shen
- **Comment**: IEEE International Conference on Signal and Image Processing 2017
- **Journal**: None
- **Summary**: In this paper, we present a novel approach for contour detection with Convolutional Neural Networks. A multi-scale CNN learning framework is designed to automatically learn the most relevant features for contour patch detection. Our method uses patch-level measurements to create contour maps with overlapping patches. We show the proposed CNN is able to to detect large-scale contours in an image efficienly. We further propose a guided filtering method to refine the contour maps produced from large-scale contours. Experimental results on the major contour benchmark databases demonstrate the effectiveness of the proposed technique. We show our method can achieve good detection of both fine-scale and large-scale contours.



### Efficient Structure from Motion for Oblique UAV Images Based on Maximal Spanning Tree Expansions
- **Arxiv ID**: http://arxiv.org/abs/1705.03212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03212v1)
- **Published**: 2017-05-09 07:22:23+00:00
- **Updated**: 2017-05-09 07:22:23+00:00
- **Authors**: San Jiang, Wanshou Jiang
- **Comment**: 33 pages, 66 figures
- **Journal**: None
- **Summary**: The primary contribution of this paper is an efficient Structure from Motion (SfM) solution for oblique unmanned aerial vehicle (UAV) images. First, an algorithm, considering spatial relationship constrains between image footprints, is designed for match pair selection with assistant of UAV flight control data and oblique camera mounting angles. Second, a topological connection network (TCN), represented by an undirected weighted graph, is constructed from initial match pairs, which encodes overlap area and intersection angle into edge weights. Then, an algorithm, termed MST-Expansion, is proposed to extract the match graph from the TCN where the TCN is firstly simplified by a maximum spanning tree (MST). By further analysis of local structure in the MST, expansion operations are performed on the nodes of the MST for match graph enhancement, which is achieved by introducing critical connections in two expansion directions. Finally, guided by the match graph, an efficient SfM solution is proposed, and its validation is verified through comprehensive analysis and comparison using three UAV datasets captured with different oblique multi-camera systems. Experiment results demonstrate that the efficiency of image matching is improved with a speedup ratio ranging from 19 to 35, and competitive orientation accuracy is achieved from both relative bundle adjustment (BA) without GCPs (Ground Control Points) and absolute BA with GCPs. At the same time, images in the three datasets are successfully oriented. For orientation of oblique UAV images, the proposed method can be a more efficient solution.



### Convolutional Dictionary Learning via Local Processing
- **Arxiv ID**: http://arxiv.org/abs/1705.03239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03239v1)
- **Published**: 2017-05-09 09:07:59+00:00
- **Updated**: 2017-05-09 09:07:59+00:00
- **Authors**: Vardan Papyan, Yaniv Romano, Jeremias Sulam, Michael Elad
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Sparse Coding (CSC) is an increasingly popular model in the signal and image processing communities, tackling some of the limitations of traditional patch-based sparse representations. Although several works have addressed the dictionary learning problem under this model, these relied on an ADMM formulation in the Fourier domain, losing the sense of locality and the relation to the traditional patch-based sparse pursuit. A recent work suggested a novel theoretical analysis of this global model, providing guarantees that rely on a localized sparsity measure. Herein, we extend this local-global relation by showing how one can efficiently solve the convolutional sparse pursuit problem and train the filters involved, while operating locally on image patches. Our approach provides an intuitive algorithm that can leverage standard techniques from the sparse representations field. The proposed method is fast to train, simple to implement, and flexible enough that it can be easily deployed in a variety of applications. We demonstrate the proposed training scheme for image inpainting and image separation, while achieving state-of-the-art results.



### Diving Performance Assessment by means of Video Processing
- **Arxiv ID**: http://arxiv.org/abs/1705.03255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03255v1)
- **Published**: 2017-05-09 10:05:07+00:00
- **Updated**: 2017-05-09 10:05:07+00:00
- **Authors**: Stefano Frassinelli, Alessandro Niccolai, Riccardo E. Zich
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1705.02854
- **Journal**: None
- **Summary**: The aim of this paper is to present a procedure for video analysis applied in an innovative way to diving performance assessment. Sport performance analysis is a trend that is growing exponentially for all level athletes. The technique here shown is based on two important requirements: flexibility and low cost. These two requirements lead to many problems in the video processing that have been faced and solved in this paper.



### Large-scale, Fast and Accurate Shot Boundary Detection through Spatio-temporal Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.03281v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03281v2)
- **Published**: 2017-05-09 11:37:25+00:00
- **Updated**: 2017-07-27 07:10:33+00:00
- **Authors**: Ahmed Hassanien, Mohamed Elgharib, Ahmed Selim, Sung-Ho Bae, Mohamed Hefeeda, Wojciech Matusik
- **Comment**: None
- **Journal**: None
- **Summary**: Shot boundary detection (SBD) is an important pre-processing step for video manipulation. Here, each segment of frames is classified as either sharp, gradual or no transition. Current SBD techniques analyze hand-crafted features and attempt to optimize both detection accuracy and processing speed. However, the heavy computations of optical flow prevents this. To achieve this aim, we present an SBD technique based on spatio-temporal Convolutional Neural Networks (CNN). Since current datasets are not large enough to train an accurate SBD CNN, we present a new dataset containing more than 3.5 million frames of sharp and gradual transitions. The transitions are generated synthetically using image compositing models. Our dataset contain additional 70,000 frames of important hard-negative no transitions. We perform the largest evaluation to date for one SBD algorithm, on real and synthetic data, containing more than 4.85 million frames. In comparison to the state of the art, we outperform dissolve gradual detection, generate competitive performance for sharp detections and produce significant improvement in wipes. In addition, we are up to 11 times faster than the state of the art.



### READ-BAD: A New Dataset and Evaluation Scheme for Baseline Detection in Archival Documents
- **Arxiv ID**: http://arxiv.org/abs/1705.03311v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03311v2)
- **Published**: 2017-05-09 13:19:39+00:00
- **Updated**: 2017-12-11 08:15:20+00:00
- **Authors**: Tobias Grüning, Roger Labahn, Markus Diem, Florian Kleber, Stefan Fiel
- **Comment**: Submitted to DAS2018
- **Journal**: None
- **Summary**: Text line detection is crucial for any application associated with Automatic Text Recognition or Keyword Spotting. Modern algorithms perform good on well-established datasets since they either comprise clean data or simple/homogeneous page layouts. We have collected and annotated 2036 archival document images from different locations and time periods. The dataset contains varying page layouts and degradations that challenge text line segmentation methods. Well established text line segmentation evaluation schemes such as the Detection Rate or Recognition Accuracy demand for binarized data that is annotated on a pixel level. Producing ground truth by these means is laborious and not needed to determine a method's quality. In this paper we propose a new evaluation scheme that is based on baselines. The proposed scheme has no need for binarization and it can handle skewed as well as rotated text lines. The ICDAR 2017 Competition on Baseline Detection and the ICDAR 2017 Competition on Layout Analysis for Challenging Medieval Manuscripts used this evaluation scheme. Finally, we present results achieved by a recently published text line detection algorithm.



### Deep Person Re-Identification with Improved Embedding and Efficient Training
- **Arxiv ID**: http://arxiv.org/abs/1705.03332v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03332v3)
- **Published**: 2017-05-09 13:47:59+00:00
- **Updated**: 2017-07-08 11:05:17+00:00
- **Authors**: Haibo Jin, Xiaobo Wang, Shengcai Liao, Stan Z. Li
- **Comment**: Accepted by IJCB 2017
- **Journal**: None
- **Summary**: Person re-identification task has been greatly boosted by deep convolutional neural networks (CNNs) in recent years. The core of which is to enlarge the inter-class distinction as well as reduce the intra-class variance. However, to achieve this, existing deep models prefer to adopt image pairs or triplets to form verification loss, which is inefficient and unstable since the number of training pairs or triplets grows rapidly as the number of training data grows. Moreover, their performance is limited since they ignore the fact that different dimension of embedding may play different importance. In this paper, we propose to employ identification loss with center loss to train a deep model for person re-identification. The training process is efficient since it does not require image pairs or triplets for training while the inter-class distinction and intra-class variance are well handled. To boost the performance, a new feature reweighting (FRW) layer is designed to explicitly emphasize the importance of each embedding dimension, thus leading to an improved embedding. Experiments on several benchmark datasets have shown the superiority of our method over the state-of-the-art alternatives on both accuracy and speed.



### Model Complexity-Accuracy Trade-off for a Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1705.03338v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1705.03338v1)
- **Published**: 2017-05-09 14:04:45+00:00
- **Updated**: 2017-05-09 14:04:45+00:00
- **Authors**: Atul Dhingra
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks(CNN) has had a great success in the recent past, because of the advent of faster GPUs and memory access. CNNs are really powerful as they learn the features from data in layers such that they exhibit the structure of the V-1 features of the human brain. A huge bottleneck, in this case, is that CNNs are very large and have a very high memory footprint, and hence they cannot be employed on devices with limited storage such as mobile phone, IoT etc. In this work, we study the model complexity versus accuracy trade-off on MNSIT dataset, and give a concrete framework for handling such a problem, given the worst case accuracy that a system can tolerate. In our work, we reduce the model complexity by 236 times, and memory footprint by 19.5 times compared to the base model while achieving worst case accuracy threshold.



### Adaptive Regularization of Some Inverse Problems in Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1705.03350v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03350v1)
- **Published**: 2017-05-09 14:27:58+00:00
- **Updated**: 2017-05-09 14:27:58+00:00
- **Authors**: Byung-Woo Hong, Ja-Keoung Koo, Martin Burger, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We present an adaptive regularization scheme for optimizing composite energy functionals arising in image analysis problems. The scheme automatically trades off data fidelity and regularization depending on the current data fit during the iterative optimization, so that regularization is strongest initially, and wanes as data fidelity improves, with the weight of the regularizer being minimized at convergence. We also introduce the use of a Huber loss function in both data fidelity and regularization terms, and present an efficient convex optimization algorithm based on the alternating direction method of multipliers (ADMM) using the equivalent relation between the Huber function and the proximal operator of the one-norm. We illustrate and validate our adaptive Huber-Huber model on synthetic and real images in segmentation, motion estimation, and denoising problems.



### Skin lesion detection based on an ensemble of deep convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1705.03360v1
- **DOI**: 10.1016/j.jbi.2018.08.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03360v1)
- **Published**: 2017-05-09 14:43:52+00:00
- **Updated**: 2017-05-09 14:43:52+00:00
- **Authors**: Balazs Harangi
- **Comment**: None
- **Journal**: None
- **Summary**: Skin cancer is a major public health problem, with over 5 million newly diagnosed cases in the United States each year. Melanoma is the deadliest form of skin cancer, responsible for over 9,000 deaths each year. In this paper, we propose an ensemble of deep convolutional neural networks to classify dermoscopy images into three classes. To achieve the highest classification accuracy, we fuse the outputs of the softmax layers of four different neural architectures. For aggregation, we consider the individual accuracies of the networks weighted by the confidence values provided by their final softmax layers. This fusion-based approach outperformed all the individual neural networks regarding classification accuracy.



### Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation
- **Arxiv ID**: http://arxiv.org/abs/1705.03372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03372v1)
- **Published**: 2017-05-09 15:00:07+00:00
- **Updated**: 2017-05-09 15:00:07+00:00
- **Authors**: Zhiyuan Shi, Timothy M. Hospedales, Tao Xiang
- **Comment**: iccv 2013
- **Journal**: None
- **Summary**: We address the problem of localisation of objects as bounding boxes in images with weak labels. This weakly supervised object localisation problem has been tackled in the past using discriminative models where each object class is localised independently from other classes. We propose a novel framework based on Bayesian joint topic modelling. Our framework has three distinctive advantages over previous works: (1) All object classes and image backgrounds are modelled jointly together in a single generative model so that "explaining away" inference can resolve ambiguity and lead to better learning and localisation. (2) The Bayesian formulation of the model enables easy integration of prior knowledge about object appearance to compensate for limited supervision. (3) Our model can be learned with a mixture of weakly labelled and unlabelled data, allowing the large volume of unlabelled images on the Internet to be exploited for learning. Extensive experiments on the challenging VOC dataset demonstrate that our approach outperforms the state-of-the-art competitors.



### Cell Tracking via Proposal Generation and Selection
- **Arxiv ID**: http://arxiv.org/abs/1705.03386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03386v1)
- **Published**: 2017-05-09 15:30:39+00:00
- **Updated**: 2017-05-09 15:30:39+00:00
- **Authors**: Saad Ullah Akram, Juho Kannala, Lauri Eklund, Janne Heikkilä
- **Comment**: None
- **Journal**: None
- **Summary**: Microscopy imaging plays a vital role in understanding many biological processes in development and disease. The recent advances in automation of microscopes and development of methods and markers for live cell imaging has led to rapid growth in the amount of image data being captured. To efficiently and reliably extract useful insights from these captured sequences, automated cell tracking is essential. This is a challenging problem due to large variation in the appearance and shapes of cells depending on many factors including imaging methodology, biological characteristics of cells, cell matrix composition, labeling methodology, etc. Often cell tracking methods require a sequence-specific segmentation method and manual tuning of many tracking parameters, which limits their applicability to sequences other than those they are designed for. In this paper, we propose 1) a deep learning based cell proposal method, which proposes candidates for cells along with their scores, and 2) a cell tracking method, which links proposals in adjacent frames in a graphical model using edges representing different cellular events and poses joint cell detection and tracking as the selection of a subset of cell and edge proposals. Our method is completely automated and given enough training data can be applied to a wide variety of microscopy sequences. We evaluate our method on multiple fluorescence and phase contrast microscopy sequences containing cells of various shapes and appearances from ISBI cell tracking challenge, and show that our method outperforms existing cell tracking methods.   Code is available at: https://github.com/SaadUllahAkram/CellTracker



### Learning Deep Networks from Noisy Labels with Dropout Regularization
- **Arxiv ID**: http://arxiv.org/abs/1705.03419v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.03419v1)
- **Published**: 2017-05-09 16:42:32+00:00
- **Updated**: 2017-05-09 16:42:32+00:00
- **Authors**: Ishan Jindal, Matthew Nokleby, Xuewen Chen
- **Comment**: Published at 2016 IEEE 16th International Conference on Data Mining
- **Journal**: None
- **Summary**: Large datasets often have unreliable labels-such as those obtained from Amazon's Mechanical Turk or social media platforms-and classifiers trained on mislabeled datasets often exhibit poor performance. We present a simple, effective technique for accounting for label noise when training deep neural networks. We augment a standard deep network with a softmax layer that models the label noise statistics. Then, we train the deep network and noise model jointly via end-to-end stochastic gradient descent on the (perhaps mislabeled) dataset. The augmented model is overdetermined, so in order to encourage the learning of a non-trivial noise model, we apply dropout regularization to the weights of the noise model during training. Numerical experiments on noisy versions of the CIFAR-10 and MNIST datasets show that the proposed dropout technique outperforms state-of-the-art methods.



### Deep Projective 3D Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1705.03428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03428v1)
- **Published**: 2017-05-09 16:59:41+00:00
- **Updated**: 2017-05-09 16:59:41+00:00
- **Authors**: Felix Järemo Lawin, Martin Danelljan, Patrik Tosteberg, Goutam Bhat, Fahad Shahbaz Khan, Michael Felsberg
- **Comment**: Submitted to CAIP 2017
- **Journal**: None
- **Summary**: Semantic segmentation of 3D point clouds is a challenging problem with numerous real-world applications. While deep learning has revolutionized the field of image semantic segmentation, its impact on point cloud data has been limited so far. Recent attempts, based on 3D deep learning approaches (3D-CNNs), have achieved below-expected results. Such methods require voxelizations of the underlying point cloud data, leading to decreased spatial resolution and increased memory consumption. Additionally, 3D-CNNs greatly suffer from the limited availability of annotated datasets.   In this paper, we propose an alternative framework that avoids the limitations of 3D-CNNs. Instead of directly solving the problem in 3D, we first project the point cloud onto a set of synthetic 2D-images. These images are then used as input to a 2D-CNN, designed for semantic segmentation. Finally, the obtained prediction scores are re-projected to the point cloud to obtain the segmentation results. We further investigate the impact of multiple modalities, such as color, depth and surface normals, in a multi-stream network architecture. Experiments are performed on the recent Semantic3D dataset. Our approach sets a new state-of-the-art by achieving a relative gain of 7.9 %, compared to the previous best approach.



### Signal reconstruction via operator guiding
- **Arxiv ID**: http://arxiv.org/abs/1705.03493v1
- **DOI**: 10.1109/SAMPTA.2017.8024424
- **Categories**: **cs.CV**, cs.GR, cs.IT, cs.NA, math.IT, math.NA, 65F30, I.4.3; G.1.3
- **Links**: [PDF](http://arxiv.org/pdf/1705.03493v1)
- **Published**: 2017-05-09 19:06:16+00:00
- **Updated**: 2017-05-09 19:06:16+00:00
- **Authors**: Andrew Knyazev, Alexander Malyshev
- **Comment**: 5 pages, 8 figures. To appear in Proceedings of SampTA 2017: Sampling
  Theory and Applications, 12th International Conference, July 3-7, 2017,
  Tallinn, Estonia
- **Journal**: IEEE Xplore: 2017 International Conference on Sampling Theory and
  Applications (SampTA), Tallin, Estonia, 2017, pp. 630-634
- **Summary**: Signal reconstruction from a sample using an orthogonal projector onto a guiding subspace is theoretically well justified, but may be difficult to practically implement. We propose more general guiding operators, which increase signal components in the guiding subspace relative to those in a complementary subspace, e.g., iterative low-pass edge-preserving filters for super-resolution of images. Two examples of super-resolution illustrate our technology: a no-flash RGB photo guided using a high resolution flash RGB photo, and a depth image guided using a high resolution RGB photo.



### Multi-Scale Spatially Weighted Local Histograms in O(1)
- **Arxiv ID**: http://arxiv.org/abs/1705.03524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03524v1)
- **Published**: 2017-05-09 20:08:11+00:00
- **Updated**: 2017-05-09 20:08:11+00:00
- **Authors**: Mahdieh Poostchi, Ali Shafiekhani, Kannappan Palaniappan, Guna Seetharaman
- **Comment**: 5 pages, 7 figures
- **Journal**: None
- **Summary**: Weighting pixel contribution considering its location is a key feature in many fundamental image processing tasks including filtering, object modeling and distance matching. Several techniques have been proposed that incorporate Spatial information to increase the accuracy and boost the performance of detection, tracking and recognition systems at the cost of speed. But, it is still not clear how to efficiently ex- tract weighted local histograms in constant time using integral histogram. This paper presents a novel algorithm to compute accurately multi-scale Spatially weighted local histograms in constant time using Weighted Integral Histogram (SWIH) for fast search. We applied our spatially weighted integral histogram approach for fast tracking and obtained more accurate and robust target localization result in comparison with using plain histogram.



### CORe50: a New Dataset and Benchmark for Continuous Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1705.03550v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1705.03550v1)
- **Published**: 2017-05-09 21:32:19+00:00
- **Updated**: 2017-05-09 21:32:19+00:00
- **Authors**: Vincenzo Lomonaco, Davide Maltoni
- **Comment**: None
- **Journal**: None
- **Summary**: Continuous/Lifelong learning of high-dimensional data streams is a challenging research problem. In fact, fully retraining models each time new data become available is infeasible, due to computational and storage issues, while na\"ive incremental strategies have been shown to suffer from catastrophic forgetting. In the context of real-world object recognition applications (e.g., robotic vision), where continuous learning is crucial, very few datasets and benchmarks are available to evaluate and compare emerging techniques. In this work we propose a new dataset and benchmark CORe50, specifically designed for continuous object recognition, and introduce baseline approaches for different continuous learning scenarios.



