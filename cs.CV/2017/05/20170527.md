# Arxiv Papers in cs.CV on 2017-05-27
### CASENet: Deep Category-Aware Semantic Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/1705.09759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09759v1)
- **Published**: 2017-05-27 03:35:36+00:00
- **Updated**: 2017-05-27 03:35:36+00:00
- **Authors**: Zhiding Yu, Chen Feng, Ming-Yu Liu, Srikumar Ramalingam
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: Boundary and edge cues are highly beneficial in improving a wide variety of vision tasks such as semantic segmentation, object recognition, stereo, and object proposal generation. Recently, the problem of edge detection has been revisited and significant progress has been made with deep learning. While classical edge detection is a challenging binary problem in itself, the category-aware semantic edge detection by nature is an even more challenging multi-label problem. We model the problem such that each edge pixel can be associated with more than one class as they appear in contours or junctions belonging to two or more semantic classes. To this end, we propose a novel end-to-end deep semantic edge learning architecture based on ResNet and a new skip-layer architecture where category-wise edge activations at the top convolution layer share and are fused with the same set of bottom layer features. We then propose a multi-label loss function to supervise the fused activations. We show that our proposed architecture benefits this problem with better performance, and we outperform the current state-of-the-art semantic edge detection methods by a large margin on standard data sets such as SBD and Cityscapes.



### MAT: A Multi-strength Adversarial Training Method to Mitigate Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1705.09764v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.09764v2)
- **Published**: 2017-05-27 04:29:04+00:00
- **Updated**: 2018-05-11 18:29:36+00:00
- **Authors**: Chang Song, Hsin-Pai Cheng, Huanrui Yang, Sicheng Li, Chunpeng Wu, Qing Wu, Hai Li, Yiran Chen
- **Comment**: 6 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Some recent works revealed that deep neural networks (DNNs) are vulnerable to so-called adversarial attacks where input examples are intentionally perturbed to fool DNNs. In this work, we revisit the DNN training process that includes adversarial examples into the training dataset so as to improve DNN's resilience to adversarial attacks, namely, adversarial training. Our experiments show that different adversarial strengths, i.e., perturbation levels of adversarial examples, have different working zones to resist the attack. Based on the observation, we propose a multi-strength adversarial training method (MAT) that combines the adversarial training examples with different adversarial strengths to defend adversarial attacks. Two training structures - mixed MAT and parallel MAT - are developed to facilitate the tradeoffs between training time and memory occupation. Our results show that MAT can substantially minimize the accuracy degradation of deep learning systems to adversarial attacks on MNIST, CIFAR-10, CIFAR-100, and SVHN.



### Deep Matching and Validation Network -- An End-to-End Solution to Constrained Image Splicing Localization and Detection
- **Arxiv ID**: http://arxiv.org/abs/1705.09765v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1705.09765v1)
- **Published**: 2017-05-27 05:33:25+00:00
- **Updated**: 2017-05-27 05:33:25+00:00
- **Authors**: Yue Wu, Wael AbdAlmageed, Prem Natarajan
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: Image splicing is a very common image manipulation technique that is sometimes used for malicious purposes. A splicing detec- tion and localization algorithm usually takes an input image and produces a binary decision indicating whether the input image has been manipulated, and also a segmentation mask that corre- sponds to the spliced region. Most existing splicing detection and localization pipelines suffer from two main shortcomings: 1) they use handcrafted features that are not robust against subsequent processing (e.g., compression), and 2) each stage of the pipeline is usually optimized independently. In this paper we extend the formulation of the underlying splicing problem to consider two input images, a query image and a potential donor image. Here the task is to estimate the probability that the donor image has been used to splice the query image, and obtain the splicing masks for both the query and donor images. We introduce a novel deep convolutional neural network architecture, called Deep Matching and Validation Network (DMVN), which simultaneously localizes and detects image splicing. The proposed approach does not depend on handcrafted features and uses raw input images to create deep learned representations. Furthermore, the DMVN is end-to-end op- timized to produce the probability estimates and the segmentation masks. Our extensive experiments demonstrate that this approach outperforms state-of-the-art splicing detection methods by a large margin in terms of both AUC score and speed.



### Deep Metric Learning and Image Classification with Nearest Neighbour Gaussian Kernels
- **Arxiv ID**: http://arxiv.org/abs/1705.09780v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09780v3)
- **Published**: 2017-05-27 07:34:48+00:00
- **Updated**: 2018-07-02 06:18:57+00:00
- **Authors**: Benjamin J. Meyer, Ben Harwood, Tom Drummond
- **Comment**: Accepted in the International Conference on Image Processing (ICIP)
  2018. Formerly titled Nearest Neighbour Radial Basis Function Solvers for
  Deep Neural Networks
- **Journal**: None
- **Summary**: We present a Gaussian kernel loss function and training algorithm for convolutional neural networks that can be directly applied to both distance metric learning and image classification problems. Our method treats all training features from a deep neural network as Gaussian kernel centres and computes loss by summing the influence of a feature's nearby centres in the feature embedding space. Our approach is made scalable by treating it as an approximate nearest neighbour search problem. We show how to make end-to-end learning feasible, resulting in a well formed embedding space, in which semantically related instances are likely to be located near one another, regardless of whether or not the network was trained on those classes. Our approach outperforms state-of-the-art deep metric learning approaches on embedding learning challenges, as well as conventional softmax classification on several datasets.



### LiDAR-Camera Calibration using 3D-3D Point correspondences
- **Arxiv ID**: http://arxiv.org/abs/1705.09785v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1705.09785v1)
- **Published**: 2017-05-27 07:57:50+00:00
- **Updated**: 2017-05-27 07:57:50+00:00
- **Authors**: Ankit Dhall, Kunal Chelani, Vishnu Radhakrishnan, K. M. Krishna
- **Comment**: None
- **Journal**: None
- **Summary**: With the advent of autonomous vehicles, LiDAR and cameras have become an indispensable combination of sensors. They both provide rich and complementary data which can be used by various algorithms and machine learning to sense and make vital inferences about the surroundings. We propose a novel pipeline and experimental setup to find accurate rigid-body transformation for extrinsically calibrating a LiDAR and a camera. The pipeling uses 3D-3D point correspondences in LiDAR and camera frame and gives a closed form solution. We further show the accuracy of the estimate by fusing point clouds from two stereo cameras which align perfectly with the rotation and translation estimated by our method, confirming the accuracy of our method's estimates both mathematically and visually. Taking our idea of extrinsic LiDAR-camera calibration forward, we demonstrate how two cameras with no overlapping field-of-view can also be calibrated extrinsically using 3D point correspondences. The code has been made available as open-source software in the form of a ROS package, more information about which can be sought here: https://github.com/ankitdhall/lidar_camera_calibration .



### PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured State Representations
- **Arxiv ID**: http://arxiv.org/abs/1705.09805v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.09805v3)
- **Published**: 2017-05-27 11:17:49+00:00
- **Updated**: 2017-07-24 12:15:40+00:00
- **Authors**: Rico Jonschkowski, Roland Hafner, Jonathan Scholz, Martin Riedmiller
- **Comment**: Accepted at Robotics: Science and Systems (RSS 2017) Workshop -- New
  Frontiers for Deep Learning in Robotics
  http://juxi.net/workshop/deep-learning-rss-2017/
- **Journal**: None
- **Summary**: We propose position-velocity encoders (PVEs) which learn---without supervision---to encode images to positions and velocities of task-relevant objects. PVEs encode a single image into a low-dimensional position state and compute the velocity state from finite differences in position. In contrast to autoencoders, position-velocity encoders are not trained by image reconstruction, but by making the position-velocity representation consistent with priors about interacting with the physical world. We applied PVEs to several simulated control tasks from pixels and achieved promising preliminary results.



### Global hard thresholding algorithms for joint sparse image representation and denoising
- **Arxiv ID**: http://arxiv.org/abs/1705.09816v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.09816v1)
- **Published**: 2017-05-27 12:40:24+00:00
- **Updated**: 2017-05-27 12:40:24+00:00
- **Authors**: Reza Borhani, Jeremy Watt, Aggelos Katsaggelos
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse coding of images is traditionally done by cutting them into small patches and representing each patch individually over some dictionary given a pre-determined number of nonzero coefficients to use for each patch. In lack of a way to effectively distribute a total number (or global budget) of nonzero coefficients across all patches, current sparse recovery algorithms distribute the global budget equally across all patches despite the wide range of differences in structural complexity among them. In this work we propose a new framework for joint sparse representation and recovery of all image patches simultaneously. We also present two novel global hard thresholding algorithms, based on the notion of variable splitting, for solving the joint sparse model. Experimentation using both synthetic and real data shows effectiveness of the proposed framework for sparse image representation and denoising tasks. Additionally, time complexity analysis of the proposed algorithms indicate high scalability of both algorithms, making them favorable to use on large megapixel images.



### Abnormality Detection and Localization in Chest X-Rays using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.09850v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09850v3)
- **Published**: 2017-05-27 17:59:57+00:00
- **Updated**: 2017-09-27 09:49:09+00:00
- **Authors**: Mohammad Tariqul Islam, Md Abdul Aowal, Ahmed Tahseen Minhaz, Khalid Ashraf
- **Comment**: 14 figures, 16 pages, 8 tables
- **Journal**: None
- **Summary**: Chest X-Rays (CXRs) are widely used for diagnosing abnormalities in the heart and lung area. Automatically detecting these abnormalities with high accuracy could greatly enhance real world diagnosis processes. Lack of standard publicly available dataset and benchmark studies, however, makes it difficult to compare various detection methods. In order to overcome these difficulties, we have used a publicly available Indiana CXR, JSRT and Shenzhen dataset and studied the performance of known deep convolutional network (DCN) architectures on different abnormalities. We find that the same DCN architecture doesn't perform well across all abnormalities. Shallow features or earlier layers consistently provide higher detection accuracy compared to deep features. We have also found ensemble models to improve classification significantly compared to single model. Combining these insight, we report the highest accuracy on chest X-Ray abnormality detection on these datasets. We find that for cardiomegaly detection, the deep learning method improves the accuracy by a staggering 17 percentage point compared to rule based methods. We applied the techniques to the problem of tuberculosis detection on a different dataset and achieved the highest accuracy. Our localization experiments using these trained classifiers show that for spatially spread out abnormalities like cardiomegaly and pulmonary edema, the network can localize the abnormalities successfully most of the time. One remarkable result of the cardiomegaly localization is that the heart and its surrounding region is most responsible for cardiomegaly detection, in contrast to the rule based models where the ratio of heart and lung area is used as the measure. We believe that through deep learning based classification and localization, we will discover many more interesting features in medical image diagnosis that are not considered traditionally.



### Probabilistic Global Scale Estimation for MonoSLAM Based on Generic Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1705.09860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.09860v1)
- **Published**: 2017-05-27 20:14:31+00:00
- **Updated**: 2017-05-27 20:14:31+00:00
- **Authors**: Edgar Sucar, Jean-Bernard Hayet
- **Comment**: Int. Workshop on Visual Odometry, CVPR, (July 2017)
- **Journal**: None
- **Summary**: This paper proposes a novel method to estimate the global scale of a 3D reconstructed model within a Kalman filtering-based monocular SLAM algorithm. Our Bayesian framework integrates height priors over the detected objects belonging to a set of broad predefined classes, based on recent advances in fast generic object detection. Each observation is produced on single frames, so that we do not need a data association process along video frames. This is because we associate the height priors with the image region sizes at image places where map features projections fall within the object detection regions. We present very promising results of this approach obtained on several experiments with different object classes.



### BMXNet: An Open-Source Binary Neural Network Implementation Based on MXNet
- **Arxiv ID**: http://arxiv.org/abs/1705.09864v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1705.09864v1)
- **Published**: 2017-05-27 20:52:10+00:00
- **Updated**: 2017-05-27 20:52:10+00:00
- **Authors**: Haojin Yang, Martin Fritzsche, Christian Bartz, Christoph Meinel
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Binary Neural Networks (BNNs) can drastically reduce memory size and accesses by applying bit-wise operations instead of standard arithmetic operations. Therefore it could significantly improve the efficiency and lower the energy consumption at runtime, which enables the application of state-of-the-art deep learning models on low power devices. BMXNet is an open-source BNN library based on MXNet, which supports both XNOR-Networks and Quantized Neural Networks. The developed BNN layers can be seamlessly applied with other standard library components and work in both GPU and CPU mode. BMXNet is maintained and developed by the multimedia research group at Hasso Plattner Institute and released under Apache license. Extensive experiments validate the efficiency and effectiveness of our implementation. The BMXNet library, several sample projects, and a collection of pre-trained binary deep models are available for download at https://github.com/hpi-xnor



