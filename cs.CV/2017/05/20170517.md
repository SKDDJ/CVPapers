# Arxiv Papers in cs.CV on 2017-05-17
### Learning a Hierarchical Latent-Variable Model of 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/1705.05994v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05994v4)
- **Published**: 2017-05-17 03:04:34+00:00
- **Updated**: 2018-08-04 07:17:28+00:00
- **Authors**: Shikun Liu, C. Lee Giles, Alexander G. Ororbia II
- **Comment**: Accepted as oral presentation at International Conference on 3D
  Vision (3DV), 2018
- **Journal**: None
- **Summary**: We propose the Variational Shape Learner (VSL), a generative model that learns the underlying structure of voxelized 3D shapes in an unsupervised fashion. Through the use of skip-connections, our model can successfully learn and infer a latent, hierarchical representation of objects. Furthermore, realistic 3D objects can be easily generated by sampling the VSL's latent probabilistic manifold. We show that our generative model can be trained end-to-end from 2D images to perform single image 3D model retrieval. Experiments show, both quantitatively and qualitatively, the improved generalization of our proposed model over a range of tasks, performing better or comparable to various state-of-the-art alternatives.



### Localized LRR on Grassmann Manifolds: An Extrinsic View
- **Arxiv ID**: http://arxiv.org/abs/1705.06599v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06599v1)
- **Published**: 2017-05-17 03:04:43+00:00
- **Updated**: 2017-05-17 03:04:43+00:00
- **Authors**: Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Baocai Yin
- **Comment**: IEEE Transactions on Circuits and Systems for Video Technology with
  Minor Revisions. arXiv admin note: text overlap with arXiv:1504.01807
- **Journal**: None
- **Summary**: Subspace data representation has recently become a common practice in many computer vision tasks. It demands generalizing classical machine learning algorithms for subspace data. Low-Rank Representation (LRR) is one of the most successful models for clustering vectorial data according to their subspace structures. This paper explores the possibility of extending LRR for subspace data on Grassmann manifolds. Rather than directly embedding the Grassmann manifolds into the symmetric matrix space, an extrinsic view is taken to build the LRR self-representation in the local area of the tangent space at each Grassmannian point, resulting in a localized LRR method on Grassmann manifolds. A novel algorithm for solving the proposed model is investigated and implemented. The performance of the new clustering algorithm is assessed through experiments on several real-world datasets including MNIST handwritten digits, ballet video clips, SKIG action clips, DynTex++ dataset and highway traffic video clips. The experimental results show the new method outperforms a number of state-of-the-art clustering methods



### Automatic Vertebra Labeling in Large-Scale 3D CT using Deep Image-to-Image Network with Message Passing and Sparsity Regularization
- **Arxiv ID**: http://arxiv.org/abs/1705.05998v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.05998v1)
- **Published**: 2017-05-17 03:56:14+00:00
- **Updated**: 2017-05-17 03:56:14+00:00
- **Authors**: Dong Yang, Tao Xiong, Daguang Xu, Qiangui Huang, David Liu, S. Kevin Zhou, Zhoubing Xu, JinHyeong Park, Mingqing Chen, Trac D. Tran, Sang Peter Chin, Dimitris Metaxas, Dorin Comaniciu
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic localization and labeling of vertebra in 3D medical images plays an important role in many clinical tasks, including pathological diagnosis, surgical planning and postoperative assessment. However, the unusual conditions of pathological cases, such as the abnormal spine curvature, bright visual imaging artifacts caused by metal implants, and the limited field of view, increase the difficulties of accurate localization. In this paper, we propose an automatic and fast algorithm to localize and label the vertebra centroids in 3D CT volumes. First, we deploy a deep image-to-image network (DI2IN) to initialize vertebra locations, employing the convolutional encoder-decoder architecture together with multi-level feature concatenation and deep supervision. Next, the centroid probability maps from DI2IN are iteratively evolved with the message passing schemes based on the mutual relation of vertebra centroids. Finally, the localization results are refined with sparsity regularization. The proposed method is evaluated on a public dataset of 302 spine CT volumes with various pathologies. Our method outperforms other state-of-the-art methods in terms of localization accuracy. The run time is around 3 seconds on average per case. To further boost the performance, we retrain the DI2IN on additional 1000+ 3D CT volumes from different patients. To the best of our knowledge, this is the first time more than 1000 3D CT volumes with expert annotation are adopted in experiments for the anatomic landmark detection tasks. Our experimental results show that training with such a large dataset significantly improves the performance and the overall identification rate, for the first time by our knowledge, reaches 90 %.



### One Shot Joint Colocalization and Cosegmentation
- **Arxiv ID**: http://arxiv.org/abs/1705.06000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.06000v1)
- **Published**: 2017-05-17 04:18:19+00:00
- **Updated**: 2017-05-17 04:18:19+00:00
- **Authors**: Abhishek Sharma
- **Comment**: 8 pages, Under Review
- **Journal**: None
- **Summary**: This paper presents a novel framework in which image cosegmentation and colocalization are cast into a single optimization problem that integrates information from low level appearance cues with that of high level localization cues in a very weakly supervised manner. In contrast to multi-task learning paradigm that learns similar tasks using a shared representation, the proposed framework leverages two representations at different levels and simultaneously discriminates between foreground and background at the bounding box and superpixel level using discriminative clustering. We show empirically that constraining the two problems at different scales enables the transfer of semantic localization cues to improve cosegmentation output whereas local appearance based segmentation cues help colocalization. The unified framework outperforms strong baseline approaches, of learning the two problems separately, by a large margin on four benchmark datasets. Furthermore, it obtains competitive results compared to the state of the art for cosegmentation on two benchmark datasets and second best result for colocalization on Pascal VOC 2007.



### PaMM: Pose-aware Multi-shot Matching for Improving Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1705.06011v1
- **DOI**: 10.1109/TIP.2018.2815840
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06011v1)
- **Published**: 2017-05-17 05:16:42+00:00
- **Updated**: 2017-05-17 05:16:42+00:00
- **Authors**: Yeong-Jun Cho, Kuk-Jin Yoon
- **Comment**: 12 pages, 12 figures, 4 tables
- **Journal**: None
- **Summary**: Person re-identification is the problem of recognizing people across different images or videos with non-overlapping views. Although there has been much progress in person re-identification over the last decade, it remains a challenging task because appearances of people can seem extremely different across diverse camera viewpoints and person poses. In this paper, we propose a novel framework for person re-identification by analyzing camera viewpoints and person poses in a so-called Pose-aware Multi-shot Matching (PaMM), which robustly estimates people's poses and efficiently conducts multi-shot matching based on pose information. Experimental results using public person re-identification datasets show that the proposed methods outperform state-of-the-art methods and are promising for person re-identification from diverse viewpoints and pose variances.



### Joint Learning from Earth Observation and OpenStreetMap Data to Get Faster Better Semantic Maps
- **Arxiv ID**: http://arxiv.org/abs/1705.06057v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1705.06057v1)
- **Published**: 2017-05-17 09:07:08+00:00
- **Updated**: 2017-05-17 09:07:08+00:00
- **Authors**: Nicolas Audebert, Bertrand Le Saux, Sébastien Lefèvre
- **Comment**: None
- **Journal**: EARTHVISION 2017 IEEE/ISPRS CVPR Workshop. Large Scale Computer
  Vision for Remote Sensing Imagery, Jul 2017, Honolulu, United States. 2017
- **Summary**: In this work, we investigate the use of OpenStreetMap data for semantic labeling of Earth Observation images. Deep neural networks have been used in the past for remote sensing data classification from various sensors, including multispectral, hyperspectral, SAR and LiDAR data. While OpenStreetMap has already been used as ground truth data for training such networks, this abundant data source remains rarely exploited as an input information layer. In this paper, we study different use cases and deep network architectures to leverage OpenStreetMap data for semantic labeling of aerial and satellite images. Especially , we look into fusion based architectures and coarse-to-fine segmentation to include the OpenStreetMap layer into multispectral-based deep fully convolutional networks. We illustrate how these methods can be successfully used on two public datasets: ISPRS Potsdam and DFC2017. We show that OpenStreetMap data can efficiently be integrated into the vision-based deep learning models and that it significantly improves both the accuracy performance and the convergence speed of the networks.



### Self-Supervised Siamese Learning on Stereo Image Pairs for Depth Estimation in Robotic Surgery
- **Arxiv ID**: http://arxiv.org/abs/1705.08260v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1705.08260v1)
- **Published**: 2017-05-17 11:10:49+00:00
- **Updated**: 2017-05-17 11:10:49+00:00
- **Authors**: Menglong Ye, Edward Johns, Ankur Handa, Lin Zhang, Philip Pratt, Guang-Zhong Yang
- **Comment**: A two-page short report to be presented at the Hamlyn Symposium on
  Medical Robotics 2017. An extension of this work is on progress
- **Journal**: None
- **Summary**: Robotic surgery has become a powerful tool for performing minimally invasive procedures, providing advantages in dexterity, precision, and 3D vision, over traditional surgery. One popular robotic system is the da Vinci surgical platform, which allows preoperative information to be incorporated into live procedures using Augmented Reality (AR). Scene depth estimation is a prerequisite for AR, as accurate registration requires 3D correspondences between preoperative and intraoperative organ models. In the past decade, there has been much progress on depth estimation for surgical scenes, such as using monocular or binocular laparoscopes [1,2]. More recently, advances in deep learning have enabled depth estimation via Convolutional Neural Networks (CNNs) [3], but training requires a large image dataset with ground truth depths. Inspired by [4], we propose a deep learning framework for surgical scene depth estimation using self-supervision for scalable data acquisition. Our framework consists of an autoencoder for depth prediction, and a differentiable spatial transformer for training the autoencoder on stereo image pairs without ground truth depths. Validation was conducted on stereo videos collected in robotic partial nephrectomy.



### Robust Registration of Gaussian Mixtures for Colour Transfer
- **Arxiv ID**: http://arxiv.org/abs/1705.06091v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4; I.4.3; I.5.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1705.06091v1)
- **Published**: 2017-05-17 11:11:14+00:00
- **Updated**: 2017-05-17 11:11:14+00:00
- **Authors**: Mairéad Grogan, Rozenn Dahyot
- **Comment**: None
- **Journal**: None
- **Summary**: We present a flexible approach to colour transfer inspired by techniques recently proposed for shape registration. Colour distributions of the palette and target images are modelled with Gaussian Mixture Models (GMMs) that are robustly registered to infer a non linear parametric transfer function. We show experimentally that our approach compares well to current techniques both quantitatively and qualitatively. Moreover, our technique is computationally the fastest and can take efficient advantage of parallel processing architectures for recolouring images and videos. Our transfer function is parametric and hence can be stored in memory for later usage and also combined with other computed transfer functions to create interesting visual effects. Overall this paper provides a fast user friendly approach to recolouring of image and video materials.



### Magnetic-Visual Sensor Fusion based Medical SLAM for Endoscopic Capsule Robot
- **Arxiv ID**: http://arxiv.org/abs/1705.06196v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06196v2)
- **Published**: 2017-05-17 15:01:37+00:00
- **Updated**: 2017-11-06 03:57:54+00:00
- **Authors**: Mehmet Turan, Yasin Almalioglu, Hunter Gilbert, Helder Araujo, Ender Konukoglu, Metin Sitti
- **Comment**: None
- **Journal**: None
- **Summary**: A reliable, real-time simultaneous localization and mapping (SLAM) method is crucial for the navigation of actively controlled capsule endoscopy robots. These robots are an emerging, minimally invasive diagnostic and therapeutic technology for use in the gastrointestinal (GI) tract. In this study, we propose a dense, non-rigidly deformable, and real-time map fusion approach for actively controlled endoscopic capsule robot applications. The method combines magnetic and vision based localization, and makes use of frame-to-model fusion and model-to-model loop closure. The performance of the method is demonstrated using an ex-vivo porcine stomach model. Across four trajectories of varying speed and complexity, and across three cameras, the root mean square localization errors range from 0.42 to 1.92 cm, and the root mean square surface reconstruction errors range from 1.23 to 2.39 cm.



### A deep level set method for image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1705.06260v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06260v2)
- **Published**: 2017-05-17 17:01:16+00:00
- **Updated**: 2017-10-24 16:14:14+00:00
- **Authors**: Min Tang, Sepehr Valipour, Zichen Vincent Zhang, Dana Cobzas, MartinJagersand
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel image segmentation approachthat integrates fully convolutional networks (FCNs) with a level setmodel. Compared with a FCN, the integrated method can incorporatesmoothing and prior information to achieve an accurate segmentation.Furthermore, different than using the level set model as a post-processingtool, we integrate it into the training phase to fine-tune the FCN. Thisallows the use of unlabeled data during training in a semi-supervisedsetting. Using two types of medical imaging data (liver CT and left ven-tricle MRI data), we show that the integrated method achieves goodperformance even when little training data is available, outperformingthe FCN or the level set model alone.



### Deep Diagnostics: Applying Convolutional Neural Networks for Vessels Defects Detection
- **Arxiv ID**: http://arxiv.org/abs/1705.06264v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06264v2)
- **Published**: 2017-05-17 17:17:07+00:00
- **Updated**: 2017-06-06 16:56:23+00:00
- **Authors**: Stanislav Filippov, Arsenii Moiseev, Andronenko Andrey
- **Comment**: Complaint to the article due to low research quality
- **Journal**: None
- **Summary**: Coronary angiography is considered to be a safe tool for the evaluation of coronary artery disease and perform in approximately 12 million patients each year worldwide. [1] In most cases, angiograms are manually analyzed by a cardiologist. Actually, there are no clinical practice algorithms which could improve and automate this work. Neural networks show high efficiency in tasks of image analysis and they can be used for the analysis of angiograms and facilitate diagnostics. We have developed an algorithm based on Convolutional Neural Network and Neural Network U-Net [2] for vessels segmentation and defects detection such as stenosis. For our research we used anonymized angiography data obtained from one of the city's hospitals and augmented them to improve learning efficiency. U-Net usage provided high quality segmentation and the combination of our algorithm with an ensemble of classifiers shows a good accuracy in the task of ischemia evaluation on test data. Subsequently, this approach can be served as a basis for the creation of an analytical system that could speed up the diagnosis of cardiovascular diseases and greatly facilitate the work of a specialist.



### Bayer Demosaicking Using Optimized Mean Curvature over RGB channels
- **Arxiv ID**: http://arxiv.org/abs/1705.06300v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06300v1)
- **Published**: 2017-05-17 18:17:17+00:00
- **Updated**: 2017-05-17 18:17:17+00:00
- **Authors**: Rui Chen, Huizhu Jia, Xiange Wen, Xiaodong Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Color artifacts of demosaicked images are often found at contours due to interpolation across edges and cross-channel aliasing. To tackle this problem, we propose a novel demosaicking method to reliably reconstruct color channels of a Bayer image based on two different optimized mean-curvature (MC) models. The missing pixel values in green (G) channel are first estimated by minimizing a variational MC model. The curvatures of restored G-image surface are approximated as a linear MC model which guides the initial reconstruction of red (R) and blue (B) channels. Then a refinement process is performed to interpolate accurate full-resolution R and B images. Experiments on benchmark images have testified to the superiority of the proposed method in terms of both the objective and subjective quality.



### CardiacNET: Segmentation of Left Atrium and Proximal Pulmonary Veins from MRI Using Multi-View CNN
- **Arxiv ID**: http://arxiv.org/abs/1705.06333v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.06333v2)
- **Published**: 2017-05-17 20:18:32+00:00
- **Updated**: 2017-05-19 15:57:13+00:00
- **Authors**: Aliasghar Mortazi, Rashed Karim, Kawal Rhode, Jeremy Burt, Ulas Bagci
- **Comment**: The paper is accepted by MICCAI 2017 for publication
- **Journal**: None
- **Summary**: Anatomical and biophysical modeling of left atrium (LA) and proximal pulmonary veins (PPVs) is important for clinical management of several cardiac diseases. Magnetic resonance imaging (MRI) allows qualitative assessment of LA and PPVs through visualization. However, there is a strong need for an advanced image segmentation method to be applied to cardiac MRI for quantitative analysis of LA and PPVs. In this study, we address this unmet clinical need by exploring a new deep learning-based segmentation strategy for quantification of LA and PPVs with high accuracy and heightened efficiency. Our approach is based on a multi-view convolutional neural network (CNN) with an adaptive fusion strategy and a new loss function that allows fast and more accurate convergence of the backpropagation based optimization. After training our network from scratch by using more than 60K 2D MRI images (slices), we have evaluated our segmentation strategy to the STACOM 2013 cardiac segmentation challenge benchmark. Qualitative and quantitative evaluations, obtained from the segmentation challenge, indicate that the proposed method achieved the state-of-the-art sensitivity (90%), specificity (99%), precision (94%), and efficiency levels (10 seconds in GPU, and 7.5 minutes in CPU).



### Optimizing and Visualizing Deep Learning for Benign/Malignant Classification in Breast Tumors
- **Arxiv ID**: http://arxiv.org/abs/1705.06362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06362v1)
- **Published**: 2017-05-17 22:35:28+00:00
- **Updated**: 2017-05-17 22:35:28+00:00
- **Authors**: Darvin Yi, Rebecca Lynn Sawyer, David Cohn III, Jared Dunnmon, Carson Lam, Xuerong Xiao, Daniel Rubin
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer has the highest incidence and second highest mortality rate for women in the US. Our study aims to utilize deep learning for benign/malignant classification of mammogram tumors using a subset of cases from the Digital Database of Screening Mammography (DDSM). Though it was a small dataset from the view of Deep Learning (about 1000 patients), we show that currently state of the art architectures of deep learning can find a robust signal, even when trained from scratch. Using convolutional neural networks (CNNs), we are able to achieve an accuracy of 85% and an ROC AUC of 0.91, while leading hand-crafted feature based methods are only able to achieve an accuracy of 71%. We investigate an amalgamation of architectures to show that our best result is reached with an ensemble of the lightweight GoogLe Nets tasked with interpreting both the coronal caudal view and the mediolateral oblique view, simply averaging the probability scores of both views to make the final prediction. In addition, we have created a novel method to visualize what features the neural network detects for the benign/malignant classification, and have correlated those features with well known radiological features, such as spiculation. Our algorithm significantly improves existing classification methods for mammography lesions and identifies features that correlate with established clinical markers.



### Re3 : Real-Time Recurrent Regression Networks for Visual Tracking of Generic Objects
- **Arxiv ID**: http://arxiv.org/abs/1705.06368v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.06368v3)
- **Published**: 2017-05-17 23:20:28+00:00
- **Updated**: 2018-02-26 19:21:12+00:00
- **Authors**: Daniel Gordon, Ali Farhadi, Dieter Fox
- **Comment**: Presented at ICRA 2018
- **Journal**: IEEE Robotics and Automation Letters 2018
- **Summary**: Robust object tracking requires knowledge and understanding of the object being tracked: its appearance, its motion, and how it changes over time. A tracker must be able to modify its underlying model and adapt to new observations. We present Re3, a real-time deep object tracker capable of incorporating temporal information into its model. Rather than focusing on a limited set of objects or training a model at test-time to track a specific instance, we pretrain our generic tracker on a large variety of objects and efficiently update on the fly; Re3 simultaneously tracks and updates the appearance model with a single forward pass. This lightweight model is capable of tracking objects at 150 FPS, while attaining competitive results on challenging benchmarks. We also show that our method handles temporary occlusion better than other comparable trackers using experiments that directly measure performance on sequences with occlusion.



