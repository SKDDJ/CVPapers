# Arxiv Papers in cs.CV on 2017-05-07
### Deep Visual Attention Prediction
- **Arxiv ID**: http://arxiv.org/abs/1705.02544v3
- **DOI**: 10.1109/TIP.2017.2787612
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02544v3)
- **Published**: 2017-05-07 01:53:17+00:00
- **Updated**: 2018-03-22 22:59:02+00:00
- **Authors**: Wenguan Wang, Jianbing Shen
- **Comment**: W. Wang and J. Shen. Deep visual attention prediction. IEEE TIP,
  27(5):2368-2378,2018. Code and results can be found in
  https://github.com/wenguanwang/deepattention
- **Journal**: IEEE Transactions on Image Processing, Vol. 27, No. 5, pp
  2368-2378, 2018
- **Summary**: In this work, we aim to predict human eye fixation with view-free scenes based on an end-to-end deep learning architecture. Although Convolutional Neural Networks (CNNs) have made substantial improvement on human attention prediction, it is still needed to improve CNN based attention models by efficiently leveraging multi-scale features. Our visual attention network is proposed to capture hierarchical saliency information from deep, coarse layers with global saliency information to shallow, fine layers with local saliency response. Our model is based on a skip-layer network structure, which predicts human attention from multiple convolutional layers with various reception fields. Final saliency prediction is achieved via the cooperation of those global and local predictions. Our model is learned in a deep supervision manner, where supervision is directly fed into multi-level layers, instead of previous approaches of providing supervision only at the output layer and propagating this supervision back to earlier layers. Our model thus incorporates multi-level saliency predictions within a single network, which significantly decreases the redundancy of previous approaches of learning multiple network streams with different input scales. Extensive experimental analysis on various challenging benchmark datasets demonstrate our method yields state-of-the-art performance with competitive inference time.



### Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Images using Weakly-Supervised Joint Convolutional Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1705.02596v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02596v1)
- **Published**: 2017-05-07 10:55:33+00:00
- **Updated**: 2017-05-07 10:55:33+00:00
- **Authors**: Yawen Huang, Ling Shao, Alejandro F. Frangi
- **Comment**: 10 pages, 6 figures. Accepted by CVPR 2017
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) offers high-resolution \emph{in vivo} imaging and rich functional and anatomical multimodality tissue contrast. In practice, however, there are challenges associated with considerations of scanning costs, patient comfort, and scanning time that constrain how much data can be acquired in clinical or research studies. In this paper, we explore the possibility of generating high-resolution and multimodal images from low-resolution single-modality imagery. We propose the weakly-supervised joint convolutional sparse coding to simultaneously solve the problems of super-resolution (SR) and cross-modality image synthesis. The learning process requires only a few registered multimodal image pairs as the training set. Additionally, the quality of the joint dictionary learning can be improved using a larger set of unpaired images. To combine unpaired data from different image resolutions/modalities, a hetero-domain image alignment term is proposed. Local image neighborhoods are naturally preserved by operating on the whole image domain (as opposed to image patches) and using joint convolutional sparse coding. The paired images are enhanced in the joint learning process with unpaired data and an additional maximum mean discrepancy term, which minimizes the dissimilarity between their feature distributions. Experiments show that the proposed method outperforms state-of-the-art techniques on both SR reconstruction and simultaneous SR and cross-modality synthesis.



### TrajectoryNet: An Embedded GPS Trajectory Representation for Point-based Classification Using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.02636v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2.6; H.2.8; I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/1705.02636v2)
- **Published**: 2017-05-07 15:40:08+00:00
- **Updated**: 2017-08-30 15:06:43+00:00
- **Authors**: Xiang Jiang, Erico N de Souza, Ahmad Pesaranghader, Baifan Hu, Daniel L. Silver, Stan Matwin
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding and discovering knowledge from GPS (Global Positioning System) traces of human activities is an essential topic in mobility-based urban computing. We propose TrajectoryNet-a neural network architecture for point-based trajectory classification to infer real world human transportation modes from GPS traces. To overcome the challenge of capturing the underlying latent factors in the low-dimensional and heterogeneous feature space imposed by GPS data, we develop a novel representation that embeds the original feature space into another space that can be understood as a form of basis expansion. We also enrich the feature space via segment-based information and use Maxout activations to improve the predictive power of Recurrent Neural Networks (RNNs). We achieve over 98% classification accuracy when detecting four types of transportation modes, outperforming existing models without additional sensory data or location-based prior knowledge.



### Towards Applying the OPRA Theory to Shape Similarity
- **Arxiv ID**: http://arxiv.org/abs/1705.02653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02653v1)
- **Published**: 2017-05-07 16:37:27+00:00
- **Updated**: 2017-05-07 16:37:27+00:00
- **Authors**: Christopher H. Dorr, Reinhard Moratz
- **Comment**: None
- **Journal**: None
- **Summary**: The motivation for using qualitative shape descriptions is as follows: qualitative shape descriptions can implicitly act as a schema for measuring the similarity of shapes, which has the potential to be cognitively adequate. Then, shapes which are similar to each other would also be similar for a pattern recognition algorithm. There is substantial work in pattern recognition and computer vision dealing with shape similarity. Here with our approach to qualitative shape descriptions and shape similarity, the focus is on achieving a representation using only simple predicates that a human could even apply without computer support.



### Large scale digital prostate pathology image analysis combining feature extraction and deep neural network
- **Arxiv ID**: http://arxiv.org/abs/1705.02678v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02678v2)
- **Published**: 2017-05-07 18:45:04+00:00
- **Updated**: 2017-05-10 19:30:23+00:00
- **Authors**: Naiyun Zhou, Andrey Fedorov, Fiona Fennessy, Ron Kikinis, Yi Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Histopathological assessments, including surgical resection and core needle biopsy, are the standard procedures in the diagnosis of the prostate cancer. Current interpretation of the histopathology images includes the determination of the tumor area, Gleason grading, and identification of certain prognosis-critical features. Such a process is not only tedious, but also prune to intra/inter-observe variabilities. Recently, FDA cleared the marketing of the first whole slide imaging system for digital pathology. This opens a new era for the computer aided prostate image analysis and feature extraction based on the digital histopathology images. In this work, we present an analysis pipeline that includes localization of the cancer region, grading, area ratio of different Gleason grades, and cytological/architectural feature extraction. The proposed algorithm combines the human engineered feature extraction as well as those learned by the deep neural network. Moreover, the entire pipeline is implemented to directly operate on the whole slide images produced by the digital scanners and is therefore potentially easy to translate into clinical practices. The algorithm is tested on 368 whole slide images from the TCGA data set and achieves an overall accuracy of 75% in differentiating Gleason 3+4 with 4+3 slides.



### Handwritten Bangla Digit Recognition Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1705.02680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02680v1)
- **Published**: 2017-05-07 18:49:27+00:00
- **Updated**: 2017-05-07 18:49:27+00:00
- **Authors**: Md Zahangir Alom, Paheding Sidike, Tarek M. Taha, Vijayan K. Asari
- **Comment**: 12 pages, 10 figures, 3 tables
- **Journal**: None
- **Summary**: In spite of the advances in pattern recognition technology, Handwritten Bangla Character Recognition (HBCR) (such as alpha-numeric and special characters) remains largely unsolved due to the presence of many perplexing characters and excessive cursive in Bangla handwriting. Even the best existing recognizers do not lead to satisfactory performance for practical applications. To improve the performance of Handwritten Bangla Digit Recognition (HBDR), we herein present a new approach based on deep neural networks which have recently shown excellent performance in many pattern recognition and machine learning applications, but has not been throughly attempted for HBDR. We introduce Bangla digit recognition techniques based on Deep Belief Network (DBN), Convolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and Gaussian filters, and CNN with dropout and Gabor filters. These networks have the advantage of extracting and using feature information, improving the recognition of two dimensional shapes with a high degree of invariance to translation, scaling and other pattern distortions. We systematically evaluated the performance of our method on publicly available Bangla numeral image database named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition rate using the proposed method: CNN with Gabor features and dropout, which outperforms the state-of-the-art algorithms for HDBR.



### AirDraw: Leveraging Smart Watch Motion Sensors for Mobile Human Computer Interactions
- **Arxiv ID**: http://arxiv.org/abs/1705.02689v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.02689v1)
- **Published**: 2017-05-07 19:58:54+00:00
- **Updated**: 2017-05-07 19:58:54+00:00
- **Authors**: Seyed A Sajjadi, Danial Moazen, Ani Nahapetian
- **Comment**: 6 pages, AirDraw, Leveraging Smart Watch Motion Sensors for Mobile
  Human Computer Interactions : IEEE, CCNC 2016
- **Journal**: None
- **Summary**: Wearable computing is one of the fastest growing technologies today. Smart watches are poised to take over at least of half the wearable devices market in the near future. Smart watch screen size, however, is a limiting factor for growth, as it restricts practical text input. On the other hand, wearable devices have some features, such as consistent user interaction and hands-free, heads-up operations, which pave the way for gesture recognition methods of text entry. This paper proposes a new text input method for smart watches, which utilizes motion sensor data and machine learning approaches to detect letters written in the air by a user. This method is less computationally intensive and less expensive when compared to computer vision approaches. It is also not affected by lighting factors, which limit computer vision solutions. The AirDraw system prototype developed to test this approach is presented. Additionally, experimental results close to 71% accuracy are presented.



### Multimodal Affect Analysis for Product Feedback Assessment
- **Arxiv ID**: http://arxiv.org/abs/1705.02694v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.02694v1)
- **Published**: 2017-05-07 20:39:35+00:00
- **Updated**: 2017-05-07 20:39:35+00:00
- **Authors**: Amol S Patwardhan, Gerald M Knapp
- **Comment**: 10 pages, ISERC 2013, IIE Annual Conference. Proceedings. Institute
  of Industrial Engineers
- **Journal**: None
- **Summary**: Consumers often react expressively to products such as food samples, perfume, jewelry, sunglasses, and clothing accessories. This research discusses a multimodal affect recognition system developed to classify whether a consumer likes or dislikes a product tested at a counter or kiosk, by analyzing the consumer's facial expression, body posture, hand gestures, and voice after testing the product. A depth-capable camera and microphone system - Kinect for Windows - is utilized. An emotion identification engine has been developed to analyze the images and voice to determine affective state of the customer. The image is segmented using skin color and adaptive threshold. Face, body and hands are detected using the Haar cascade classifier. Canny edges are identified and the lip, body and hand contours are extracted using spatial filtering. Edge count and orientation around the mouth, cheeks, eyes, shoulders, fingers and the location of the edges are used as features. Classification is done by an emotion template mapping algorithm and training a classifier using support vector machines. The real-time performance, accuracy and feasibility for multimodal affect recognition in feedback assessment are evaluated.



