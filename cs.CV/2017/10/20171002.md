# Arxiv Papers in cs.CV on 2017-10-02
### Learning event representation: As sparse as possible, but not sparser
- **Arxiv ID**: http://arxiv.org/abs/1710.00448v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1710.00448v1)
- **Published**: 2017-10-02 01:18:16+00:00
- **Updated**: 2017-10-02 01:18:16+00:00
- **Authors**: Tuan Do, James Pustejovsky
- **Comment**: Qualitative reasoning Workshop 2017
- **Journal**: None
- **Summary**: Selecting an optimal event representation is essential for event classification in real world contexts. In this paper, we investigate the application of qualitative spatial reasoning (QSR) frameworks for classification of human-object interaction in three dimensional space, in comparison with the use of quantitative feature extraction approaches for the same purpose. In particular, we modify QSRLib, a library that allows computation of Qualitative Spatial Relations and Calculi, and employ it for feature extraction, before inputting features into our neural network models. Using an experimental setup involving motion captures of human-object interaction as three dimensional inputs, we observe that the use of qualitative spatial features significantly improves the performance of our machine learning algorithm against our baseline, while quantitative features of similar kinds fail to deliver similar improvement. We also observe that sequential representations of QSR features yield the best classification performance. A result of our learning method is a simple approach to the qualitative representation of 3D activities as compositions of 2D actions that can be visualized and learned using 2-dimensional QSR.



### Margin Sample Mining Loss: A Deep Learning Based Method for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1710.00478v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00478v3)
- **Published**: 2017-10-02 04:27:07+00:00
- **Updated**: 2017-10-07 03:21:22+00:00
- **Authors**: Qiqi Xiao, Hao Luo, Chi Zhang
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Person re-identification (ReID) is an important task in computer vision. Recently, deep learning with a metric learning loss has become a common framework for ReID. In this paper, we also propose a new metric learning loss with hard sample mining called margin smaple mining loss (MSML) which can achieve better accuracy compared with other metric learning losses, such as triplet loss. In experi- ments, our proposed methods outperforms most of the state-of-the-art algorithms on Market1501, MARS, CUHK03 and CUHK-SYSU.



### SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Planning and Control
- **Arxiv ID**: http://arxiv.org/abs/1710.00489v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.NE, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1710.00489v1)
- **Published**: 2017-10-02 05:18:12+00:00
- **Updated**: 2017-10-02 05:18:12+00:00
- **Authors**: Arunkumar Byravan, Felix Leeb, Franziska Meier, Dieter Fox
- **Comment**: 8 pages, Initial submission to IEEE International Conference on
  Robotics and Automation (ICRA) 2018
- **Journal**: None
- **Summary**: In this work, we present an approach to deep visuomotor control using structured deep dynamics models. Our deep dynamics model, a variant of SE3-Nets, learns a low-dimensional pose embedding for visuomotor control via an encoder-decoder structure. Unlike prior work, our dynamics model is structured: given an input scene, our network explicitly learns to segment salient parts and predict their pose-embedding along with their motion modeled as a change in the pose space due to the applied actions. We train our model using a pair of point clouds separated by an action and show that given supervision only in the form of point-wise data associations between the frames our network is able to learn a meaningful segmentation of the scene along with consistent poses. We further show that our model can be used for closed-loop control directly in the learned low-dimensional pose space, where the actions are computed by minimizing error in the pose space using gradient-based methods, similar to traditional model-based control. We present results on controlling a Baxter robot from raw depth data in simulation and in the real world and compare against two baseline deep networks. Our method runs in real-time, achieves good prediction of scene dynamics and outperforms the baseline methods on multiple control runs. Video results can be found at: https://rse-lab.cs.washington.edu/se3-structured-deep-ctrl/



### Depth estimation using structured light flow -- analysis of projected pattern flow on an object's surface --
- **Arxiv ID**: http://arxiv.org/abs/1710.00513v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00513v1)
- **Published**: 2017-10-02 07:27:22+00:00
- **Updated**: 2017-10-02 07:27:22+00:00
- **Authors**: Ryo Furukawa, Ryusuke Sagawa, Hiroshi Kawasaki
- **Comment**: 9 pages, Published at the International Conference on Computer Vision
  (ICCV 2017)
- **Journal**: None
- **Summary**: Shape reconstruction techniques using structured light have been widely researched and developed due to their robustness, high precision, and density. Because the techniques are based on decoding a pattern to find correspondences, it implicitly requires that the projected patterns be clearly captured by an image sensor, i.e., to avoid defocus and motion blur of the projected pattern. Although intensive researches have been conducted for solving defocus blur, few researches for motion blur and only solution is to capture with extremely fast shutter speed. In this paper, unlike the previous approaches, we actively utilize motion blur, which we refer to as a light flow, to estimate depth. Analysis reveals that minimum two light flows, which are retrieved from two projected patterns on the object, are required for depth estimation. To retrieve two light flows at the same time, two sets of parallel line patterns are illuminated from two video projectors and the size of motion blur of each line is precisely measured. By analyzing the light flows, i.e. lengths of the blurs, scene depth information is estimated. In the experiments, 3D shapes of fast moving objects, which are inevitably captured with motion blur, are successfully reconstructed by our technique.



### Temporal shape super-resolution by intra-frame motion encoding using high-fps structured light
- **Arxiv ID**: http://arxiv.org/abs/1710.00517v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00517v1)
- **Published**: 2017-10-02 07:52:04+00:00
- **Updated**: 2017-10-02 07:52:04+00:00
- **Authors**: Yuki Shiba, Satoshi Ono, Ryo Furukawa, Shinsaku Hiura, Hiroshi Kawasaki
- **Comment**: 9 pages, Published at the International Conference on Computer Vision
  (ICCV 2017)
- **Journal**: None
- **Summary**: One of the solutions of depth imaging of moving scene is to project a static pattern on the object and use just a single image for reconstruction. However, if the motion of the object is too fast with respect to the exposure time of the image sensor, patterns on the captured image are blurred and reconstruction fails. In this paper, we impose multiple projection patterns into each single captured image to realize temporal super resolution of the depth image sequences. With our method, multiple patterns are projected onto the object with higher fps than possible with a camera. In this case, the observed pattern varies depending on the depth and motion of the object, so we can extract temporal information of the scene from each single image. The decoding process is realized using a learning-based approach where no geometric calibration is needed. Experiments confirm the effectiveness of our method where sequential shapes are reconstructed from a single image. Both quantitative evaluations and comparisons with recent techniques were also conducted.



### Indirect Match Highlights Detection with Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.00568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00568v1)
- **Published**: 2017-10-02 10:14:41+00:00
- **Updated**: 2017-10-02 10:14:41+00:00
- **Authors**: Marco Godi, Paolo Rota, Francesco Setti
- **Comment**: "Social Signal Processing and Beyond" workshop, in conjunction with
  ICIAP 2017
- **Journal**: None
- **Summary**: Highlights in a sport video are usually referred as actions that stimulate excitement or attract attention of the audience. A big effort is spent in designing techniques which find automatically highlights, in order to automatize the otherwise manual editing process. Most of the state-of-the-art approaches try to solve the problem by training a classifier using the information extracted on the tv-like framing of players playing on the game pitch, learning to detect game actions which are labeled by human observers according to their perception of highlight. Obviously, this is a long and expensive work. In this paper, we reverse the paradigm: instead of looking at the gameplay, inferring what could be exciting for the audience, we directly analyze the audience behavior, which we assume is triggered by events happening during the game. We apply deep 3D Convolutional Neural Network (3D-CNN) to extract visual features from cropped video recordings of the supporters that are attending the event. Outputs of the crops belonging to the same frame are then accumulated to produce a value indicating the Highlight Likelihood (HL) which is then used to discriminate between positive (i.e. when a highlight occurs) and negative samples (i.e. standard play or time-outs). Experimental results on a public dataset of ice-hockey matches demonstrate the effectiveness of our method and promote further research in this new exciting direction.



### Out-of-focus Blur: Image De-blurring
- **Arxiv ID**: http://arxiv.org/abs/1710.00620v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1710.00620v2)
- **Published**: 2017-10-02 13:08:12+00:00
- **Updated**: 2017-11-02 03:11:32+00:00
- **Authors**: Yuzhen Lu
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Image de-blurring is important in many cases of imaging a real scene or object by a camera. This project focuses on de-blurring an image distorted by an out-of-focus blur through a simulation study. A pseudo-inverse filter is first explored but it fails because of severe noise amplification. Then Tikhonov regularization methods are employed, which produce greatly improved results compared to the pseudo-inverse filter. In Tikhonov regularization, the choice of the regularization parameter plays a critical rule in obtaining a high-quality image, and the regularized solutions possess a semi-convergence property. The best result, with the relative restoration error of 8.49%, is achieved when the prescribed discrepancy principle is used to decide an optimal value. Furthermore, an iterative method, Conjugated Gradient, is employed for image de-blurring, which is fast in computation and leads to an even better result with the relative restoration error of 8.22%. The number of iteration in CG acts as a regularization parameter, and the iterates have a semi-convergence property as well.



### Adaptive Smoothing in fMRI Data Processing Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.00629v1
- **DOI**: 10.1109/PRNI.2017.7981499
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.00629v1)
- **Published**: 2017-10-02 13:29:27+00:00
- **Updated**: 2017-10-02 13:29:27+00:00
- **Authors**: Albert Vilamala, Kristoffer Hougaard Madsen, Lars Kai Hansen
- **Comment**: 4 pages, 3 figures, 1 table, IEEE 2017 International Workshop on
  Pattern Recognition in Neuroimaging (PRNI)
- **Journal**: None
- **Summary**: Functional Magnetic Resonance Imaging (fMRI) relies on multi-step data processing pipelines to accurately determine brain activity; among them, the crucial step of spatial smoothing. These pipelines are commonly suboptimal, given the local optimisation strategy they use, treating each step in isolation. With the advent of new tools for deep learning, recent work has proposed to turn these pipelines into end-to-end learning networks. This change of paradigm offers new avenues to improvement as it allows for a global optimisation. The current work aims at benefitting from this paradigm shift by defining a smoothing step as a layer in these networks able to adaptively modulate the degree of smoothing required by each brain volume to better accomplish a given data analysis task. The viability is evaluated on real fMRI data where subjects did alternate between left and right finger tapping tasks.



### Deep Convolutional Neural Networks for Interpretable Analysis of EEG Sleep Stage Scoring
- **Arxiv ID**: http://arxiv.org/abs/1710.00633v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.00633v1)
- **Published**: 2017-10-02 13:36:29+00:00
- **Updated**: 2017-10-02 13:36:29+00:00
- **Authors**: Albert Vilamala, Kristoffer H. Madsen, Lars K. Hansen
- **Comment**: 8 pages, 1 figure, 2 tables, IEEE 2017 International Workshop on
  Machine Learning for Signal Processing
- **Journal**: None
- **Summary**: Sleep studies are important for diagnosing sleep disorders such as insomnia, narcolepsy or sleep apnea. They rely on manual scoring of sleep stages from raw polisomnography signals, which is a tedious visual task requiring the workload of highly trained professionals. Consequently, research efforts to purse for an automatic stage scoring based on machine learning techniques have been carried out over the last years. In this work, we resort to multitaper spectral analysis to create visually interpretable images of sleep patterns from EEG signals as inputs to a deep convolutional network trained to solve visual recognition tasks. As a working example of transfer learning, a system able to accurately classify sleep stages in new unseen patients is presented. Evaluations in a widely-used publicly available dataset favourably compare to state-of-the-art results, while providing a framework for visual interpretation of outcomes.



### Restoration of Pansharpened Images by Conditional Filtering in the PCA Domain
- **Arxiv ID**: http://arxiv.org/abs/1710.00672v3
- **DOI**: 10.1109/LGRS.2018.2873654
- **Categories**: **cs.CV**, math.FA
- **Links**: [PDF](http://arxiv.org/pdf/1710.00672v3)
- **Published**: 2017-10-02 14:13:58+00:00
- **Updated**: 2018-08-25 12:05:02+00:00
- **Authors**: Joan Duran, Antoni Buades
- **Comment**: None
- **Journal**: IEEE Geoscience and Remote Sensing Letters, vol. 16(3), pp.
  442-446, 2019
- **Summary**: Pansharpening techniques aim at fusing low-resolution multispectral (MS) images and high-resolution panchromatic (PAN) images to produce high-resolution MS images. Despite significant progress in the field, spectral and spatial distortions might still compromise the quality of the results. We introduce a restoration strategy to mitigate artifacts of fused products. After applying the Principal Component Analysis (PCA) transform to a pansharpened image, the chromatic components are filtered conditionally to the geometry of PAN. The structural component is then replaced by the locally histogram-matched PAN for spatial enhancement. Experimental results illustrate the efficiency of the proposed restoration chain.



### Spinal cord gray matter segmentation using deep dilated convolutions
- **Arxiv ID**: http://arxiv.org/abs/1710.01269v1
- **DOI**: 10.1038/s41598-018-24304-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01269v1)
- **Published**: 2017-10-02 16:25:14+00:00
- **Updated**: 2017-10-02 16:25:14+00:00
- **Authors**: Christian S. Perone, Evan Calabrese, Julien Cohen-Adad
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: Gray matter (GM) tissue changes have been associated with a wide range of neurological disorders and was also recently found relevant as a biomarker for disability in amyotrophic lateral sclerosis. The ability to automatically segment the GM is, therefore, an important task for modern studies of the spinal cord. In this work, we devise a modern, simple and end-to-end fully automated human spinal cord gray matter segmentation method using Deep Learning, that works both on in vivo and ex vivo MRI acquisitions. We evaluate our method against six independently developed methods on a GM segmentation challenge and report state-of-the-art results in 8 out of 10 different evaluation metrics as well as major network parameter reduction when compared to the traditional medical imaging architectures such as U-Nets.



### Progressive Color Transfer with Dense Semantic Correspondences
- **Arxiv ID**: http://arxiv.org/abs/1710.00756v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00756v2)
- **Published**: 2017-10-02 16:25:41+00:00
- **Updated**: 2018-12-12 18:47:07+00:00
- **Authors**: Mingming He, Jing Liao, Dongdong Chen, Lu Yuan, Pedro V. Sander
- **Comment**: Accepted by TOG
- **Journal**: None
- **Summary**: We propose a new algorithm for color transfer between images that have perceptually similar semantic structures. We aim to achieve a more accurate color transfer that leverages semantically-meaningful dense correspondence between images. To accomplish this, our algorithm uses neural representations for matching. Additionally, the color transfer should be spatially variant and globally coherent. Therefore, our algorithm optimizes a local linear model for color transfer satisfying both local and global constraints. Our proposed approach jointly optimizes matching and color transfer, adopting a coarse-to-fine strategy. The proposed method can be successfully extended from one-to-one to one-to-many color transfer. The latter further addresses the problem of mismatching elements of the input image. We validate our proposed method by testing it on a large variety of image content.



### Detecting Adversarial Attacks on Neural Network Policies with Visual Foresight
- **Arxiv ID**: http://arxiv.org/abs/1710.00814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1710.00814v1)
- **Published**: 2017-10-02 17:56:26+00:00
- **Updated**: 2017-10-02 17:56:26+00:00
- **Authors**: Yen-Chen Lin, Ming-Yu Liu, Min Sun, Jia-Bin Huang
- **Comment**: Project page: http://yclin.me/RL_attack_detection/ Code:
  https://github.com/yenchenlin/rl-attack-detection
- **Journal**: None
- **Summary**: Deep reinforcement learning has shown promising results in learning control policies for complex sequential decision-making tasks. However, these neural network-based policies are known to be vulnerable to adversarial examples. This vulnerability poses a potentially serious threat to safety-critical systems such as autonomous vehicles. In this paper, we propose a defense mechanism to defend reinforcement learning agents from adversarial attacks by leveraging an action-conditioned frame prediction module. Our core idea is that the adversarial examples targeting at a neural network-based policy are not effective for the frame prediction model. By comparing the action distribution produced by a policy from processing the current observed frame to the action distribution produced by the same policy from processing the predicted frame from the action-conditioned frame prediction module, we can detect the presence of adversarial examples. Beyond detecting the presence of adversarial examples, our method allows the agent to continue performing the task using the predicted frame when the agent is under attack. We evaluate the performance of our algorithm using five games in Atari 2600. Our results demonstrate that the proposed defense mechanism achieves favorable performance against baseline algorithms in detecting adversarial examples and in earning rewards when the agents are under attack.



### Rethinking Feature Discrimination and Polymerization for Large-scale Recognition
- **Arxiv ID**: http://arxiv.org/abs/1710.00870v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00870v2)
- **Published**: 2017-10-02 19:11:58+00:00
- **Updated**: 2017-10-29 12:37:40+00:00
- **Authors**: Yu Liu, Hongyang Li, Xiaogang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Feature matters. How to train a deep network to acquire discriminative features across categories and polymerized features within classes has always been at the core of many computer vision tasks, specially for large-scale recognition systems where test identities are unseen during training and the number of classes could be at million scale. In this paper, we address this problem based on the simple intuition that the cosine distance of features in high-dimensional space should be close enough within one class and far away across categories. To this end, we proposed the congenerous cosine (COCO) algorithm to simultaneously optimize the cosine similarity among data. It inherits the softmax property to make inter-class features discriminative as well as shares the idea of class centroid in metric learning. Unlike previous work where the center is a temporal, statistical variable within one mini-batch during training, the formulated centroid is responsible for clustering inner-class features to enforce them polymerized around the network truncus. COCO is bundled with discriminative training and learned end-to-end with stable convergence. Experiments on five benchmarks have been extensively conducted to verify the effectiveness of our approach on both small-scale classification task and large-scale human recognition problem.



### Classification of Time-Series Images Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.00886v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00886v2)
- **Published**: 2017-10-02 19:59:24+00:00
- **Updated**: 2017-10-07 13:39:44+00:00
- **Authors**: Nima Hatami, Yann Gavet, Johan Debayle
- **Comment**: The 10th International Conference on Machine Vision (ICMV 2017)
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) has achieved a great success in image recognition task by automatically learning a hierarchical feature representation from raw data. While the majority of Time-Series Classification (TSC) literature is focused on 1D signals, this paper uses Recurrence Plots (RP) to transform time-series into 2D texture images and then take advantage of the deep CNN classifier. Image representation of time-series introduces different feature types that are not available for 1D signals, and therefore TSC can be treated as texture image recognition task. CNN model also allows learning different levels of representations together with a classifier, jointly and automatically. Therefore, using RP and CNN in a unified framework is expected to boost the recognition rate of TSC. Experimental results on the UCR time-series classification archive demonstrate competitive accuracy of the proposed approach, compared not only to the existing deep architectures, but also to the state-of-the art TSC algorithms.



### PIRVS: An Advanced Visual-Inertial SLAM System with Flexible Sensor Fusion and Hardware Co-Design
- **Arxiv ID**: http://arxiv.org/abs/1710.00893v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.00893v1)
- **Published**: 2017-10-02 20:17:54+00:00
- **Updated**: 2017-10-02 20:17:54+00:00
- **Authors**: Zhe Zhang, Shaoshan Liu, Grace Tsai, Hongbing Hu, Chen-Chi Chu, Feng Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present the PerceptIn Robotics Vision System (PIRVS) system, a visual-inertial computing hardware with embedded simultaneous localization and mapping (SLAM) algorithm. The PIRVS hardware is equipped with a multi-core processor, a global-shutter stereo camera, and an IMU with precise hardware synchronization. The PIRVS software features a novel and flexible sensor fusion approach to not only tightly integrate visual measurements with inertial measurements and also to loosely couple with additional sensor modalities. It runs in real-time on both PC and the PIRVS hardware. We perform a thorough evaluation of the proposed system using multiple public visual-inertial datasets. Experimental results demonstrate that our system reaches comparable accuracy of state-of-the-art visual-inertial algorithms on PC, while being more efficient on the PIRVS hardware.



### End-to-end Learning for 3D Facial Animation from Raw Waveforms of Speech
- **Arxiv ID**: http://arxiv.org/abs/1710.00920v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00920v2)
- **Published**: 2017-10-02 21:44:32+00:00
- **Updated**: 2017-12-07 21:13:47+00:00
- **Authors**: Hai X. Pham, Yuting Wang, Vladimir Pavlovic
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep learning framework for real-time speech-driven 3D facial animation from just raw waveforms. Our deep neural network directly maps an input sequence of speech audio to a series of micro facial action unit activations and head rotations to drive a 3D blendshape face model. In particular, our deep model is able to learn the latent representations of time-varying contextual information and affective states within the speech. Hence, our model not only activates appropriate facial action units at inference to depict different utterance generating actions, in the form of lip movements, but also, without any assumption, automatically estimates emotional intensity of the speaker and reproduces her ever-changing affective states by adjusting strength of facial unit activations. For example, in a happy speech, the mouth opens wider than normal, while other facial units are relaxed; or in a surprised state, both eyebrows raise higher. Experiments on a diverse audiovisual corpus of different actors across a wide range of emotional states show interesting and promising results of our approach. Being speaker-independent, our generalized model is readily applicable to various tasks in human-machine interaction and animation.



### Fine-Grained Head Pose Estimation Without Keypoints
- **Arxiv ID**: http://arxiv.org/abs/1710.00925v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00925v5)
- **Published**: 2017-10-02 22:01:20+00:00
- **Updated**: 2018-04-13 18:10:29+00:00
- **Authors**: Nataniel Ruiz, Eunji Chong, James M. Rehg
- **Comment**: Accepted to Computer Vision and Pattern Recognition Workshops
  (CVPRW), 2018 IEEE Conference on. IEEE, 2018
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR) Workshops, 2018, pp. 2074-2083
- **Summary**: Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the target face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detection performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to determine pose by training a multi-loss convolutional neural network on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from image intensities through joint binned pose classification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose methods. We open-source our training and testing code as well as release our pre-trained models.



### Interpretable Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.00935v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00935v4)
- **Published**: 2017-10-02 23:00:42+00:00
- **Updated**: 2018-02-14 10:25:43+00:00
- **Authors**: Quanshi Zhang, Ying Nian Wu, Song-Chun Zhu
- **Comment**: In this version, we release the website of the code. Compared to the
  previous version, we have corrected all values of location instability in
  Table 3--6 by dividing the values by sqrt(2), i.e., a=a/sqrt(2). Such
  revisions do NOT decrease the significance of the superior performance of our
  method, because we make the same correction to location-instability values of
  all baselines
- **Journal**: None
- **Summary**: This paper proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs. In an interpretable CNN, each filter in a high conv-layer represents a certain object part. We do not need any annotations of object parts or textures to supervise the learning process. Instead, the interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. Our method can be applied to different types of CNNs with different structures. The clear knowledge representation in an interpretable CNN can help people understand the logics inside a CNN, i.e., based on which patterns the CNN makes the decision. Experiments showed that filters in an interpretable CNN were more semantically meaningful than those in traditional CNNs.



