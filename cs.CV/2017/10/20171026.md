# Arxiv Papers in cs.CV on 2017-10-26
### Knowledge Projection for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.09505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09505v1)
- **Published**: 2017-10-26 01:30:00+00:00
- **Updated**: 2017-10-26 01:30:00+00:00
- **Authors**: Zhi Zhang, Guanghan Ning, Zhihai He
- **Comment**: None
- **Journal**: None
- **Summary**: While deeper and wider neural networks are actively pushing the performance limits of various computer vision and machine learning tasks, they often require large sets of labeled data for effective training and suffer from extremely high computational complexity. In this paper, we will develop a new framework for training deep neural networks on datasets with limited labeled samples using cross-network knowledge projection which is able to improve the network performance while reducing the overall computational complexity significantly. Specifically, a large pre-trained teacher network is used to observe samples from the training data. A projection matrix is learned to project this teacher-level knowledge and its visual representations from an intermediate layer of the teacher network to an intermediate layer of a thinner and faster student network to guide and regulate its training process. Both the intermediate layers from the teacher network and the injection layers from the student network are adaptively selected during training by evaluating a joint loss function in an iterative manner. This knowledge projection framework allows us to use crucial knowledge learned by large networks to guide the training of thinner student networks, avoiding over-fitting, achieving better network performance, and significantly reducing the complexity. Extensive experimental results on benchmark datasets have demonstrated that our proposed knowledge projection approach outperforms existing methods, improving accuracy by up to 4% while reducing network complexity by 4 to 10 times, which is very attractive for practical applications of deep neural networks.



### A Generative Model for Volume Rendering
- **Arxiv ID**: http://arxiv.org/abs/1710.09545v2
- **DOI**: 10.1109/TVCG.2018.2816059
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.09545v2)
- **Published**: 2017-10-26 05:20:05+00:00
- **Updated**: 2019-07-16 22:50:24+00:00
- **Authors**: Matthew Berger, Jixian Li, Joshua A. Levine
- **Comment**: None
- **Journal**: IEEE Trans. Vis. Comput. Graph. 25(4) (2019) 1636-1650
- **Summary**: We present a technique to synthesize and analyze volume-rendered images using generative models. We use the Generative Adversarial Network (GAN) framework to compute a model from a large collection of volume renderings, conditioned on (1) viewpoint and (2) transfer functions for opacity and color. Our approach facilitates tasks for volume analysis that are challenging to achieve using existing rendering techniques such as ray casting or texture-based methods. We show how to guide the user in transfer function editing by quantifying expected change in the output image. Additionally, the generative model transforms transfer functions into a view-invariant latent space specifically designed to synthesize volume-rendered images. We use this space directly for rendering, enabling the user to explore the space of volume-rendered images. As our model is independent of the choice of volume rendering process, we show how to analyze volume-rendered images produced by direct and global illumination lighting, for a variety of volume datasets.



### Artifact reduction for separable non-local means
- **Arxiv ID**: http://arxiv.org/abs/1710.09552v1
- **DOI**: 10.1117/1.JEI.26.6.063012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09552v1)
- **Published**: 2017-10-26 06:05:06+00:00
- **Updated**: 2017-10-26 06:05:06+00:00
- **Authors**: Sanjay Ghosh, Kunal N. Chaudhury
- **Comment**: To appear in Journal of Electronic Imaging
- **Journal**: None
- **Summary**: It was recently demonstrated [J. Electron. Imaging, 25(2), 2016] that one can perform fast non-local means (NLM) denoising of one-dimensional signals using a method called lifting. The cost of lifting is independent of the patch length, which dramatically reduces the run-time for large patches. Unfortunately, it is difficult to directly extend lifting for non-local means denoising of images. To bypass this, the authors proposed a separable approximation in which the image rows and columns are filtered using lifting. The overall algorithm is significantly faster than NLM, and the results are comparable in terms of PSNR. However, the separable processing often produces vertical and horizontal stripes in the image. This problem was previously addressed by using a bilateral filter-based post-smoothing, which was effective in removing some of the stripes. In this letter, we demonstrate that stripes can be mitigated in the first place simply by involving the neighboring rows (or columns) in the filtering. In other words, we use a two-dimensional search (similar to NLM), while still using one-dimensional patches (as in the previous proposal). The novelty is in the observation that one can use lifting for performing two-dimensional searches. The proposed approach produces artifact-free images, whose quality and PSNR are comparable to NLM, while being significantly faster.



### Improved Workflow for Unsupervised Multiphase Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1710.09671v1
- **DOI**: 10.1016/j.cageo.2018.05.013
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09671v1)
- **Published**: 2017-10-26 12:58:09+00:00
- **Updated**: 2017-10-26 12:58:09+00:00
- **Authors**: Brendan A. West, Taylor S. Hodgdon, Matthew D. Parno, Arnold J. Song
- **Comment**: None
- **Journal**: None
- **Summary**: Quantitative image analysis often depends on accurate classification of pixels through a segmentation process. However, imaging artifacts such as the partial volume effect and sensor noise complicate the classification process. These effects increase the pixel intensity variance of each constituent class, causing intensities from one class to overlap with another. This increased variance makes threshold based segmentation methods insufficient due to ambiguous overlap regions in the pixel intensity distributions. The class ambiguity becomes even more complex for systems with more than two constituents, such as unsaturated moist granular media. In this paper, we propose an image processing workflow that improves segmentation accuracy for multiphase systems. First, the ambiguous transition regions between classes are identified and removed, which allows for global thresholding of single-class regions. Then the transition regions are classified using a distance function, and finally both segmentations are combined into one classified image. This workflow includes three methodologies for identifying transition pixels and we demonstrate on a variety of synthetic images that these approaches are able to accurately separate the ambiguous transition pixels from the single-class regions. For situations with typical amounts of image noise, misclassification errors and area differences calculated between each class of the synthetic images and the resultant segmented images range from 0.69-1.48% and 0.01-0.74%, respectively, showing the segmentation accuracy of this approach. We demonstrate that we are able to accurately segment x-ray microtomography images of moist granular media using these computationally efficient methodologies.



### Class Correlation affects Single Object Localization using Pre-trained ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1710.09685v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09685v2)
- **Published**: 2017-10-26 13:25:51+00:00
- **Updated**: 2017-10-27 05:11:01+00:00
- **Authors**: Pokkalla Harsha Vardhan, Kunal Sekhri, Dipan K. Pal, Marios Savvides
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of object localization has become one of the mainstream problems of vision. Most of the algorithms proposed involve the design for the model to be specifically for localizing objects. In this paper, we explore whether a pre-trained canonical ConvNet (without fine-tuning) trained purely for object classification on one dataset with global image level labels can be used to localize objects in images containing a single instance on a separate dataset while generalizing to novel classes. We propose a simple algorithm involving cropping and blackening out regions in the image space called Explicit Image Space based Search (EISS) for locating the most responsive regions in an image in the context of object localization. EISS brings to light the interesting phenomenon of a ConvNets responding more to features within objects as opposed to object level descriptors, as the classes in the training data get more correlated (visually/semantically similar).



### Deep Spatial Regression Model for Image Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1710.09757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09757v1)
- **Published**: 2017-10-26 15:28:41+00:00
- **Updated**: 2017-10-26 15:28:41+00:00
- **Authors**: Haiyan Yao, Kang Han, Wanggen Wan, Li Hou
- **Comment**: 15pages
- **Journal**: None
- **Summary**: Computer vision techniques have been used to produce accurate and generic crowd count estimators in recent years. Due to severe occlusions, appearance variations, perspective distortions and illumination conditions, crowd counting is a very challenging task. To this end, we propose a deep spatial regression model(DSRM) for counting the number of individuals present in a still image with arbitrary perspective and arbitrary resolution. Our proposed model is based on Convolutional Neural Network (CNN) and long short term memory (LSTM). First, we put the images into a pretrained CNN to extract a set of high-level features. Then the features in adjacent regions are used to regress the local counts with a LSTM structure which takes the spatial information into consideration. The final global count is obtained by a sum of the local patches. We apply our framework on several challenging crowd counting datasets, and the experiment results illustrate that our method on the crowd counting and density estimation problem outperforms state-of-the-art methods in terms of reliability and effectiveness.



### How to Fool Radiologists with Generative Adversarial Networks? A Visual Turing Test for Lung Cancer Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1710.09762v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1710.09762v2)
- **Published**: 2017-10-26 15:38:50+00:00
- **Updated**: 2018-01-09 04:31:54+00:00
- **Authors**: Maria J. M. Chuquicusma, Sarfaraz Hussein, Jeremy Burt, Ulas Bagci
- **Comment**: Accepted for publication in IEEE International Symposium on
  Biomedical Imaging (ISBI) 2018
- **Journal**: None
- **Summary**: Discriminating lung nodules as malignant or benign is still an underlying challenge. To address this challenge, radiologists need computer aided diagnosis (CAD) systems which can assist in learning discriminative imaging features corresponding to malignant and benign nodules. However, learning highly discriminative imaging features is an open problem. In this paper, our aim is to learn the most discriminative features pertaining to lung nodules by using an adversarial learning methodology. Specifically, we propose to use unsupervised learning with Deep Convolutional-Generative Adversarial Networks (DC-GANs) to generate lung nodule samples realistically. We hypothesize that imaging features of lung nodules will be discriminative if it is hard to differentiate them (fake) from real (true) nodules. To test this hypothesis, we present Visual Turing tests to two radiologists in order to evaluate the quality of the generated (fake) nodules. Extensive comparisons are performed in discerning real, generated, benign, and malignant nodules. This experimental set up allows us to validate the overall quality of the generated nodules, which can then be used to (1) improve diagnostic decisions by mining highly discriminative imaging features, (2) train radiologists for educational purposes, and (3) generate realistic samples to train deep networks with big data.



### Deep Multi-Modal Classification of Intraductal Papillary Mucinous Neoplasms (IPMN) with Canonical Correlation Analysis
- **Arxiv ID**: http://arxiv.org/abs/1710.09779v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.QM, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1710.09779v3)
- **Published**: 2017-10-26 16:01:31+00:00
- **Updated**: 2018-04-27 16:47:53+00:00
- **Authors**: Sarfaraz Hussein, Pujan Kandel, Juan E. Corral, Candice W. Bolan, Michael B. Wallace, Ulas Bagci
- **Comment**: Accepted for publication in IEEE International Symposium on
  Biomedical Imaging (ISBI) 2018
- **Journal**: None
- **Summary**: Pancreatic cancer has the poorest prognosis among all cancer types. Intraductal Papillary Mucinous Neoplasms (IPMNs) are radiographically identifiable precursors to pancreatic cancer; hence, early detection and precise risk assessment of IPMN are vital. In this work, we propose a Convolutional Neural Network (CNN) based computer aided diagnosis (CAD) system to perform IPMN diagnosis and risk assessment by utilizing multi-modal MRI. In our proposed approach, we use minimum and maximum intensity projections to ease the annotation variations among different slices and type of MRIs. Then, we present a CNN to obtain deep feature representation corresponding to each MRI modality (T1-weighted and T2-weighted). At the final step, we employ canonical correlation analysis (CCA) to perform a fusion operation at the feature level, leading to discriminative canonical correlation features. Extracted features are used for classification. Our results indicate significant improvements over other potential approaches to solve this important problem. The proposed approach doesn't require explicit sample balancing in cases of imbalance between positive and negative examples. To the best of our knowledge, our study is the first to automatically diagnose IPMN using multi-modal MRI.



### Lip2AudSpec: Speech reconstruction from silent lip movements video
- **Arxiv ID**: http://arxiv.org/abs/1710.09798v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1710.09798v1)
- **Published**: 2017-10-26 16:39:05+00:00
- **Updated**: 2017-10-26 16:39:05+00:00
- **Authors**: Hassan Akbari, Himani Arora, Liangliang Cao, Nima Mesgarani
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we propose a deep neural network for reconstructing intelligible speech from silent lip movement videos. We use auditory spectrogram as spectral representation of speech and its corresponding sound generation method resulting in a more natural sounding reconstructed speech. Our proposed network consists of an autoencoder to extract bottleneck features from the auditory spectrogram which is then used as target to our main lip reading network comprising of CNN, LSTM and fully connected layers. Our experiments show that the autoencoder is able to reconstruct the original auditory spectrogram with a 98% correlation and also improves the quality of reconstructed speech from the main lip reading network. Our model, trained jointly on different speakers is able to extract individual speaker characteristics and gives promising results of reconstructing intelligible speech with superior word recognition accuracy.



### Spiking Optical Flow for Event-based Sensors Using IBM's TrueNorth Neurosynaptic System
- **Arxiv ID**: http://arxiv.org/abs/1710.09820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09820v1)
- **Published**: 2017-10-26 17:33:34+00:00
- **Updated**: 2017-10-26 17:33:34+00:00
- **Authors**: Germain Haessig, Andrew Cassidy, Rodrigo Alvarez, Ryad Benosman, Garrick Orchard
- **Comment**: 11 pages, 11 figures without biography figures
- **Journal**: None
- **Summary**: This paper describes a fully spike-based neural network for optical flow estimation from Dynamic Vision Sensor data. A low power embedded implementation of the method which combines the Asynchronous Time-based Image Sensor with IBM's TrueNorth Neurosynaptic System is presented. The sensor generates spikes with sub-millisecond resolution in response to scene illumination changes. These spike are processed by a spiking neural network running on TrueNorth with a 1 millisecond resolution to accurately determine the order and time difference of spikes from neighboring pixels, and therefore infer the velocity. The spiking neural network is a variant of the Barlow Levick method for optical flow estimation. The system is evaluated on two recordings for which ground truth motion is available, and achieves an Average Endpoint Error of 11% at an estimated power budget of under 80mW for the sensor and computation.



### Dynamic Routing Between Capsules
- **Arxiv ID**: http://arxiv.org/abs/1710.09829v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09829v2)
- **Published**: 2017-10-26 17:49:04+00:00
- **Updated**: 2017-11-07 19:26:38+00:00
- **Authors**: Sara Sabour, Nicholas Frosst, Geoffrey E Hinton
- **Comment**: None
- **Journal**: None
- **Summary**: A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.



### Kernel k-Groups via Hartigan's Method
- **Arxiv ID**: http://arxiv.org/abs/1710.09859v4
- **DOI**: 10.1109/TPAMI.2020.2998120
- **Categories**: **stat.ML**, cs.CV, cs.DS, cs.LG, math.ST, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1710.09859v4)
- **Published**: 2017-10-26 18:38:28+00:00
- **Updated**: 2020-06-11 19:57:09+00:00
- **Authors**: Guilherme França, Maria L. Rizzo, Joshua T. Vogelstein
- **Comment**: several improvements; connections with community detection and
  stochastic block model. Matches published version
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2020
- **Summary**: Energy statistics was proposed by Sz\' ekely in the 80's inspired by Newton's gravitational potential in classical mechanics and it provides a model-free hypothesis test for equality of distributions. In its original form, energy statistics was formulated in Euclidean spaces. More recently, it was generalized to metric spaces of negative type. In this paper, we consider a formulation for the clustering problem using a weighted version of energy statistics in spaces of negative type. We show that this approach leads to a quadratically constrained quadratic program in the associated kernel space, establishing connections with graph partitioning problems and kernel methods in machine learning. To find local solutions of such an optimization problem, we propose kernel k-groups, which is an extension of Hartigan's method to kernel spaces. Kernel k-groups is cheaper than spectral clustering and has the same computational cost as kernel k-means (which is based on Lloyd's heuristic) but our numerical results show an improved performance, especially in higher dimensions. Moreover, we verify the efficiency of kernel k-groups in community detection in sparse stochastic block models which has fascinating applications in several areas of science.



### How far did we get in face spoofing detection?
- **Arxiv ID**: http://arxiv.org/abs/1710.09868v2
- **DOI**: 10.1016/j.engappai.2018.04.013
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09868v2)
- **Published**: 2017-10-26 18:49:01+00:00
- **Updated**: 2018-09-29 18:55:18+00:00
- **Authors**: Luiz Souza, Mauricio Pamplona, Luciano Oliveira, João Papa
- **Comment**: None
- **Journal**: Engineering Applications of Artificial Intelligence, vol 72, pp.
  368-381, 2018
- **Summary**: The growing use of control access systems based on face recognition shed light over the need for even more accurate systems to detect face spoofing attacks. In this paper, an extensive analysis on face spoofing detection works published in the last decade is presented. The analyzed works are categorized by their fundamental parts, i.e., descriptors and classifiers. This structured survey also brings the temporal evolution of the face spoofing detection field, as well as a comparative analysis of the works considering the most important public data sets in the field. The methodology followed in this work is particularly relevant to observe trends in the existing approaches, to discuss still opened issues, and to propose new perspectives for the future of face spoofing detection.



### Phase Transitions in Image Denoising via Sparsely Coding Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.09875v1
- **DOI**: None
- **Categories**: **cs.NE**, cond-mat.stat-mech, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.09875v1)
- **Published**: 2017-10-26 19:09:54+00:00
- **Updated**: 2017-10-26 19:09:54+00:00
- **Authors**: Jacob Carroll, Nils Carlson, Garrett T. Kenyon
- **Comment**: 4 pages, 3 figures, submitted to NIPS 2017 workshop: Advances in
  Modeling and Learning Interactions from Complex Data
- **Journal**: None
- **Summary**: Neural networks are analogous in many ways to spin glasses, systems which are known for their rich set of dynamics and equally complex phase diagrams. We apply well-known techniques in the study of spin glasses to a convolutional sparsely encoding neural network and observe power law finite-size scaling behavior in the sparsity and reconstruction error as the network denoises 32$\times$32 RGB CIFAR-10 images. This finite-size scaling indicates the presence of a continuous phase transition at a critical value of this sparsity. By using the power law scaling relations inherent to finite-size scaling, we can determine the optimal value of sparsity for any network size by tuning the system to the critical point and operate the system at the minimum denoising error.



### Image Compression: Sparse Coding vs. Bottleneck Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1710.09926v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09926v2)
- **Published**: 2017-10-26 21:49:30+00:00
- **Updated**: 2018-01-23 20:38:26+00:00
- **Authors**: Yijing Watkins, Mohammad Sayeh, Oleksandr Iaroshenko, Garrett Kenyon
- **Comment**: None
- **Journal**: None
- **Summary**: Bottleneck autoencoders have been actively researched as a solution to image compression tasks. However, we observed that bottleneck autoencoders produce subjectively low quality reconstructed images. In this work, we explore the ability of sparse coding to improve reconstructed image quality for the same degree of compression. We observe that sparse image compression produces visually superior reconstructed images and yields higher values of pixel-wise measures of reconstruction quality (PSNR and SSIM) compared to bottleneck autoencoders. % In addition, we find that using alternative metrics that correlate better with human perception, such as feature perceptual loss and the classification accuracy, sparse image compression scores up to 18.06\% and 2.7\% higher, respectively, compared to bottleneck autoencoders. Although computationally much more intensive, we find that sparse coding is otherwise superior to bottleneck autoencoders for the same degree of compression.



### SEGMENT3D: A Web-based Application for Collaborative Segmentation of 3D images used in the Shoot Apical Meristem
- **Arxiv ID**: http://arxiv.org/abs/1710.09933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09933v1)
- **Published**: 2017-10-26 22:40:28+00:00
- **Updated**: 2017-10-26 22:40:28+00:00
- **Authors**: Thiago V. Spina, Johannes Stegmaier, Alexandre X. Falcão, Elliot Meyerowitz, Alexandre Cunha
- **Comment**: None
- **Journal**: None
- **Summary**: The quantitative analysis of 3D confocal microscopy images of the shoot apical meristem helps understanding the growth process of some plants. Cell segmentation in these images is crucial for computational plant analysis and many automated methods have been proposed. However, variations in signal intensity across the image mitigate the effectiveness of those approaches with no easy way for user correction. We propose a web-based collaborative 3D image segmentation application, SEGMENT3D, to leverage automatic segmentation results. The image is divided into 3D tiles that can be either segmented interactively from scratch or corrected from a pre-existing segmentation. Individual segmentation results per tile are then automatically merged via consensus analysis and then stitched to complete the segmentation for the entire image stack. SEGMENT3D is a comprehensive application that can be applied to other 3D imaging modalities and general objects. It also provides an easy way to create supervised data to advance segmentation using machine learning models.



### Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1710.09934v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, 68T01, I.2.1; I.4.6; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1710.09934v1)
- **Published**: 2017-10-26 22:45:28+00:00
- **Updated**: 2017-10-26 22:45:28+00:00
- **Authors**: William M. Severa, Jerilyn A. Timlin, Suraj Kholwadwala, Conrad D. James, James B. Aimone
- **Comment**: None
- **Journal**: None
- **Summary**: The high dimensionality of hyperspectral imaging forces unique challenges in scope, size and processing requirements. Motivated by the potential for an in-the-field cell sorting detector, we examine a $\textit{Synechocystis sp.}$ PCC 6803 dataset wherein cells are grown alternatively in nitrogen rich or deplete cultures. We use deep learning techniques to both successfully classify cells and generate a mask segmenting the cells/condition from the background. Further, we use the classification accuracy to guide a data-driven, iterative feature selection method, allowing the design neural networks requiring 90% fewer input features with little accuracy degradation.



