# Arxiv Papers in cs.CV on 2017-10-03
### VIDOSAT: High-dimensional Sparsifying Transform Learning for Online Video Denoising
- **Arxiv ID**: http://arxiv.org/abs/1710.00947v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00947v1)
- **Published**: 2017-10-03 00:47:00+00:00
- **Updated**: 2017-10-03 00:47:00+00:00
- **Authors**: Bihan Wen, Saiprasad Ravishankar, Yoram Bresler
- **Comment**: None
- **Journal**: None
- **Summary**: Techniques exploiting the sparsity of images in a transform domain have been effective for various applications in image and video processing. Transform learning methods involve cheap computations and have been demonstrated to perform well in applications such as image denoising and medical image reconstruction. Recently, we proposed methods for online learning of sparsifying transforms from streaming signals, which enjoy good convergence guarantees, and involve lower computational costs than online synthesis dictionary learning. In this work, we apply online transform learning to video denoising. We present a novel framework for online video denoising based on high-dimensional sparsifying transform learning for spatio-temporal patches. The patches are constructed either from corresponding 2D patches in successive frames or using an online block matching technique. The proposed online video denoising requires little memory, and offers efficient processing. Numerical experiments compare the performance to the proposed video denoising scheme but fixing the transform to be 3D DCT, as well as prior schemes such as dictionary learning-based schemes, and the state-of-the-art VBM3D and VBM4D on several video data sets, demonstrating the promising performance of the proposed methods.



### GP-GAN: Gender Preserving GAN for Synthesizing Faces from Landmarks
- **Arxiv ID**: http://arxiv.org/abs/1710.00962v2
- **DOI**: 10.1109/ICPR.2018.8545081
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00962v2)
- **Published**: 2017-10-03 02:22:21+00:00
- **Updated**: 2018-04-25 05:21:35+00:00
- **Authors**: Xing Di, Vishwanath A. Sindagi, Vishal M. Patel
- **Comment**: 6 pages, 5 figures, this paper is accepted as 2018 24th International
  Conference on Pattern Recognition (ICPR2018)
- **Journal**: None
- **Summary**: Facial landmarks constitute the most compressed representation of faces and are known to preserve information such as pose, gender and facial structure present in the faces. Several works exist that attempt to perform high-level face-related analysis tasks based on landmarks. In contrast, in this work, an attempt is made to tackle the inverse problem of synthesizing faces from their respective landmarks. The primary aim of this work is to demonstrate that information preserved by landmarks (gender in particular) can be further accentuated by leveraging generative models to synthesize corresponding faces. Though the problem is particularly challenging due to its ill-posed nature, we believe that successful synthesis will enable several applications such as boosting performance of high-level face related tasks using landmark points and performing dataset augmentation. To this end, a novel face-synthesis method known as Gender Preserving Generative Adversarial Network (GP-GAN) that is guided by adversarial loss, perceptual loss and a gender preserving loss is presented. Further, we propose a novel generator sub-network UDeNet for GP-GAN that leverages advantages of U-Net and DenseNet architectures. Extensive experiments and comparison with recent methods are performed to verify the effectiveness of the proposed method.



### A concatenating framework of shortcut convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1710.00974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00974v1)
- **Published**: 2017-10-03 03:56:33+00:00
- **Updated**: 2017-10-03 03:56:33+00:00
- **Authors**: Yujian Li, Ting Zhang, Zhaoying Liu, Haihe Hu
- **Comment**: 17 pages, 5 figures, 15 tables
- **Journal**: None
- **Summary**: It is well accepted that convolutional neural networks play an important role in learning excellent features for image classification and recognition. However, in tradition they only allow adjacent layers connected, limiting integration of multi-scale information. To further improve their performance, we present a concatenating framework of shortcut convolutional neural networks. This framework can concatenate multi-scale features by shortcut connections to the fully-connected layer that is directly fed to the output layer. We do a large number of experiments to investigate performance of the shortcut convolutional neural networks on many benchmark visual datasets for different tasks. The datasets include AR, FERET, FaceScrub, CelebA for gender classification, CUReT for texture classification, MNIST for digit recognition, and CIFAR-10 for object recognition. Experimental results show that the shortcut convolutional neural networks can achieve better results than the traditional ones on these tasks, with more stability in different settings of pooling schemes, activation functions, optimizations, initializations, kernel numbers and kernel sizes.



### Facial Key Points Detection using Deep Convolutional Neural Network - NaimishNet
- **Arxiv ID**: http://arxiv.org/abs/1710.00977v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.00977v1)
- **Published**: 2017-10-03 04:23:08+00:00
- **Updated**: 2017-10-03 04:23:08+00:00
- **Authors**: Naimish Agarwal, Artus Krohn-Grimberghe, Ranjana Vyas
- **Comment**: 7 pages, 21 figures, 3 tables
- **Journal**: None
- **Summary**: Facial Key Points (FKPs) Detection is an important and challenging problem in the fields of computer vision and machine learning. It involves predicting the co-ordinates of the FKPs, e.g. nose tip, center of eyes, etc, for a given face. In this paper, we propose a LeNet adapted Deep CNN model - NaimishNet, to operate on facial key points data and compare our model's performance against existing state of the art approaches.



### Joint Person Re-identification and Camera Network Topology Inference in Multiple Cameras
- **Arxiv ID**: http://arxiv.org/abs/1710.00983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.00983v1)
- **Published**: 2017-10-03 05:07:11+00:00
- **Updated**: 2017-10-03 05:07:11+00:00
- **Authors**: Yeong-Jun Cho, Su-A Kim, Jae-Han Park, Kyuewang Lee, Kuk-Jin Yoon
- **Comment**: 14 pages, 14 figures, 6 tables
- **Journal**: None
- **Summary**: Person re-identification is the task of recognizing or identifying a person across multiple views in multi-camera networks. Although there has been much progress in person re-identification, person re-identification in large-scale multi-camera networks still remains a challenging task because of the large spatio-temporal uncertainty and high complexity due to a large number of cameras and people. To handle these difficulties, additional information such as camera network topology should be provided, which is also difficult to automatically estimate, unfortunately. In this study, we propose a unified framework which jointly solves both person re-identification and camera network topology inference problems with minimal prior knowledge about the environments. The proposed framework takes general multi-camera network environments into account and can be applied to online person re-identification in large-scale multi-camera networks. In addition, to effectively show the superiority of the proposed framework, we provide a new person re-identification dataset with full annotations, named SLP, captured in the multi-camera network consisting of nine non-overlapping cameras. Experimental results using our person re-identification and public datasets show that the proposed methods are promising for both person re-identification and camera topology inference tasks.



### Learning Affinity via Spatial Propagation Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.01020v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1710.01020v1)
- **Published**: 2017-10-03 08:00:15+00:00
- **Updated**: 2017-10-03 08:00:15+00:00
- **Authors**: Sifei Liu, Shalini De Mello, Jinwei Gu, Guangyu Zhong, Ming-Hsuan Yang, Jan Kautz
- **Comment**: A long version of NIPS 2017
- **Journal**: None
- **Summary**: In this paper, we propose spatial propagation networks for learning the affinity matrix for vision tasks. We show that by constructing a row/column linear propagation model, the spatially varying transformation matrix exactly constitutes an affinity matrix that models dense, global pairwise relationships of an image. Specifically, we develop a three-way connection for the linear propagation model, which (a) formulates a sparse transformation matrix, where all elements can be the output from a deep CNN, but (b) results in a dense affinity matrix that effectively models any task-specific pairwise similarity matrix. Instead of designing the similarity kernels according to image features of two points, we can directly output all the similarities in a purely data-driven manner. The spatial propagation network is a generic framework that can be applied to many affinity-related tasks, including but not limited to image matting, segmentation and colorization, to name a few. Essentially, the model can learn semantically-aware affinity values for high-level vision tasks due to the powerful learning capability of the deep neural network classifier. We validate the framework on the task of refinement for image segmentation boundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic segmentation tasks show that the spatial propagation network provides a general, effective and efficient solution for generating high-quality segmentation results.



### Simulating Structure-from-Motion
- **Arxiv ID**: http://arxiv.org/abs/1710.01052v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1710.01052v1)
- **Published**: 2017-10-03 09:34:38+00:00
- **Updated**: 2017-10-03 09:34:38+00:00
- **Authors**: Martin Hahner, Orestis Varesis, Panagiotis Bountouris
- **Comment**: This paper was written as part of a group project in the "3D Vision"
  course at ETH Z\"urich in Spring semester 2017
- **Journal**: None
- **Summary**: The implementation of a Structure-from-Motion (SfM) pipeline from a synthetically generated scene as well as the investigation of the faithfulness of diverse reconstructions is the subject of this project. A series of different SfM reconstructions are implemented and their camera pose estimations are being contrasted with their respective ground truth locations. Finally, injection of ground truth location data into the rendered images in order to reduce the estimation error of the camera poses is studied as well.



### Resolution limits on visual speech recognition
- **Arxiv ID**: http://arxiv.org/abs/1710.01073v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1710.01073v1)
- **Published**: 2017-10-03 11:07:06+00:00
- **Updated**: 2017-10-03 11:07:06+00:00
- **Authors**: Helen L. Bear, Richard Harvey, Barry-John Theobald, Yuxuan Lan
- **Comment**: None
- **Journal**: Helen L. Bear, Richard Harvey, Barry-John Theobald, Yuxuan Lan.
  Resolution limits on visual speech recognition. International Conference on
  Image Processing (ICIP). 2014. p1371-1375
- **Summary**: Visual-only speech recognition is dependent upon a number of factors that can be difficult to control, such as: lighting; identity; motion; emotion and expression. But some factors, such as video resolution are controllable, so it is surprising that there is not yet a systematic study of the effect of resolution on lip-reading. Here we use a new data set, the Rosetta Raven data, to train and test recognizers so we can measure the affect of video resolution on recognition accuracy. We conclude that, contrary to common practice, resolution need not be that great for automatic lip-reading. However it is highly unlikely that automatic lip-reading can work reliably when the distance between the bottom of the lower lip and the top of the upper lip is less than four pixels at rest.



### Optimal DNN Primitive Selection with Partitioned Boolean Quadratic Programming
- **Arxiv ID**: http://arxiv.org/abs/1710.01079v2
- **DOI**: None
- **Categories**: **cs.PF**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.01079v2)
- **Published**: 2017-10-03 11:25:24+00:00
- **Updated**: 2018-11-02 10:56:03+00:00
- **Authors**: Andrew Anderson, David Gregg
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) require very large amounts of computation both for training and for inference when deployed in the field. Many different algorithms have been proposed to implement the most computationally expensive layers of DNNs. Further, each of these algorithms has a large number of variants, which offer different trade-offs of parallelism, data locality, memory footprint, and execution time. In addition, specific algorithms operate much more efficiently on specialized data layouts and formats.   We state the problem of optimal primitive selection in the presence of data format transformations, and show that it is NP-hard by demonstrating an embedding in the Partitioned Boolean Quadratic Assignment problem (PBQP).   We propose an analytic solution via a PBQP solver, and evaluate our approach experimentally by optimizing several popular DNNs using a library of more than 70 DNN primitives, on an embedded platform and a general purpose platform. We show experimentally that significant gains are possible versus the state of the art vendor libraries by using a principled analytic solution to the problem of layout selection in the presence of data format transformations.



### Some observations on computer lip-reading: moving from the dream to the reality
- **Arxiv ID**: http://arxiv.org/abs/1710.01084v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1710.01084v1)
- **Published**: 2017-10-03 11:33:50+00:00
- **Updated**: 2017-10-03 11:33:50+00:00
- **Authors**: Helen L. Bear, Gari Owen, Richard Harvey, Barry-John Theobald
- **Comment**: None
- **Journal**: Helen L. Bear, Gari Owen, Richard Harvey, and Barry-John Theobald.
  Some observations on computer lip-reading: moving from the dream to the
  reality. International Society for Optics and Photonics- Security and
  defence. 2014. p92530G--92530G
- **Summary**: In the quest for greater computer lip-reading performance there are a number of tacit assumptions which are either present in the datasets (high resolution for example) or in the methods (recognition of spoken visual units called visemes for example). Here we review these and other assumptions and show the surprising result that computer lip-reading is not heavily constrained by video resolution, pose, lighting and other practical factors. However, the working assumption that visemes, which are the visual equivalent of phonemes, are the best unit for recognition does need further examination. We conclude that visemes, which were defined over a century ago, are unlikely to be optimal for a modern computer lip-reading system.



### Which phoneme-to-viseme maps best improve visual-only computer lip-reading?
- **Arxiv ID**: http://arxiv.org/abs/1710.01093v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1710.01093v1)
- **Published**: 2017-10-03 11:44:40+00:00
- **Updated**: 2017-10-03 11:44:40+00:00
- **Authors**: Helen L. Bear, Richard W. Harvey, Barry-John Theobald, Yuxuan Lan
- **Comment**: None
- **Journal**: Helen L. Bear, Richard W. Harvey, Barry-John Theobald, and Yuxuan
  Lan. Which phoneme-to-viseme maps best improve visual-only computer
  lip-reading? Advances in Visual Computing 2014. p230-239
- **Summary**: A critical assumption of all current visual speech recognition systems is that there are visual speech units called visemes which can be mapped to units of acoustic speech, the phonemes. Despite there being a number of published maps it is infrequent to see the effectiveness of these tested, particularly on visual-only lip-reading (many works use audio-visual speech). Here we examine 120 mappings and consider if any are stable across talkers. We show a method for devising maps based on phoneme confusions from an automated lip-reading system, and we present new mappings that show improvements for individual talkers.



### Isotropic and Steerable Wavelets in N Dimensions. A multiresolution analysis framework for ITK
- **Arxiv ID**: http://arxiv.org/abs/1710.01103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01103v1)
- **Published**: 2017-10-03 12:11:10+00:00
- **Updated**: 2017-10-03 12:11:10+00:00
- **Authors**: Pablo Hernandez-Cerdan
- **Comment**: Manuscript submitted to InsightJournal (ITK) in 2017:
  http://hdl.handle.net/10380/3558
- **Journal**: None
- **Summary**: This document describes the implementation of the external module ITKIsotropicWavelets, a multiresolution (MRA) analysis framework using isotropic and steerable wavelets in the frequency domain. This framework provides the backbone for state of the art filters for denoising, feature detection or phase analysis in N-dimensions. It focus on reusability, and highly decoupled modules for easy extension and implementation of new filters, and it contains a filter for multiresolution phase analysis,   The backbone of the multi-scale analysis is provided by an isotropic band-limited wavelet pyramid, and the detection of directional features is provided by coupling the pyramid with a generalized Riesz transform. The generalized Riesz transform of order N behaves like a smoothed version of the Nth order derivatives of the signal. Also, it is steerable: its components impulse responses can be rotated to any spatial orientation, reducing computation time when detecting directional features.



### Detection of Inferior Myocardial Infarction using Shallow Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.01115v4
- **DOI**: 10.1109/R10-HTC.2017.8289058
- **Categories**: **cs.CV**, 68T10, 68T10, I.5.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1710.01115v4)
- **Published**: 2017-10-03 12:56:49+00:00
- **Updated**: 2018-08-18 04:59:13+00:00
- **Authors**: Tahsin Reasat, Celia Shahnaz
- **Comment**: None
- **Journal**: None
- **Summary**: Myocardial Infarction is one of the leading causes of death worldwide. This paper presents a Convolutional Neural Network (CNN) architecture which takes raw Electrocardiography (ECG) signal from lead II, III and AVF and differentiates between inferior myocardial infarction (IMI) and healthy signals. The performance of the model is evaluated on IMI and healthy signals obtained from Physikalisch-Technische Bundesanstalt (PTB) database. A subject-oriented approach is taken to comprehend the generalization capability of the model and compared with the current state of the art. In a subject-oriented approach, the network is tested on one patient and trained on rest of the patients. Our model achieved a superior metrics scores (accuracy= 84.54%, sensitivity= 85.33% and specificity= 84.09%) when compared to the benchmark. We also analyzed the discriminating strength of the features extracted by the convolutional layers by means of geometric separability index and euclidean distance and compared it with the benchmark model.



### Speaker-independent machine lip-reading with speaker-dependent viseme classifiers
- **Arxiv ID**: http://arxiv.org/abs/1710.01122v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1710.01122v1)
- **Published**: 2017-10-03 13:02:41+00:00
- **Updated**: 2017-10-03 13:02:41+00:00
- **Authors**: Helen L. Bear, Stephen J. Cox, Richard W. Harvey
- **Comment**: None
- **Journal**: Helen L. Bear, Stephen J. Cox, Richard W. Harvey,
  Speaker-independent machine lip-reading with speaker-dependent viseme
  classifiers. Audio-Visual Speech Processing (AVSP) 2015, p190-195
- **Summary**: In machine lip-reading, which is identification of speech from visual-only information, there is evidence to show that visual speech is highly dependent upon the speaker [1]. Here, we use a phoneme-clustering method to form new phoneme-to-viseme maps for both individual and multiple speakers. We use these maps to examine how similarly speakers talk visually. We conclude that broadly speaking, speakers have the same repertoire of mouth gestures, where they differ is in the use of the gestures.



### Finding phonemes: improving machine lip-reading
- **Arxiv ID**: http://arxiv.org/abs/1710.01142v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1710.01142v1)
- **Published**: 2017-10-03 13:32:40+00:00
- **Updated**: 2017-10-03 13:32:40+00:00
- **Authors**: Helen L. Bear, Richard W. Harvey, Yuxuan Lan
- **Comment**: None
- **Journal**: Helen L. Bear, Richard W. Harvey, Yuxuan Lan. Finding phonemes:
  improving machine lip-reading. Audio-Visual Speech Processing (AVSP), 2015
  p115-120
- **Summary**: In machine lip-reading there is continued debate and research around the correct classes to be used for recognition. In this paper we use a structured approach for devising speaker-dependent viseme classes, which enables the creation of a set of phoneme-to-viseme maps where each has a different quantity of visemes ranging from two to 45. Viseme classes are based upon the mapping of articulated phonemes, which have been confused during phoneme recognition, into viseme groups. Using these maps, with the LiLIR dataset, we show the effect of changing the viseme map size in speaker-dependent machine lip-reading, measured by word recognition correctness and so demonstrate that word recognition with phoneme classifiers is not just possible, but often better than word recognition with viseme classifiers. Furthermore, there are intermediate units between visemes and phonemes which are better still.



### Decoding visemes: improving machine lipreading
- **Arxiv ID**: http://arxiv.org/abs/1710.01169v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1710.01169v1)
- **Published**: 2017-10-03 14:01:32+00:00
- **Updated**: 2017-10-03 14:01:32+00:00
- **Authors**: Helen L. Bear, Richard Harvey
- **Comment**: None
- **Journal**: Helen L Bear and Richard Harvey. Decoding visemes: improving
  machine lipreading. IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP), 2016. p2009-2013
- **Summary**: To undertake machine lip-reading, we try to recognise speech from a visual signal. Current work often uses viseme classification supported by language models with varying degrees of success. A few recent works suggest phoneme classification, in the right circumstances, can outperform viseme classification. In this work we present a novel two-pass method of training phoneme classifiers which uses previously trained visemes in the first pass. With our new training algorithm, we show classification performance which significantly improves on previous lip-reading results.



### Person Re-Identification with Vision and Language
- **Arxiv ID**: http://arxiv.org/abs/1710.01202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01202v1)
- **Published**: 2017-10-03 15:05:31+00:00
- **Updated**: 2017-10-03 15:05:31+00:00
- **Authors**: Fei Yan, Krystian Mikolajczyk, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a new approach to person re-identification using images and natural language descriptions. We propose a joint vision and language model based on CCA and CNN architectures to match across the two modalities as well as to enrich visual examples for which there are no language descriptions. We also introduce new annotations in the form of natural language descriptions for two standard Re-ID benchmarks, namely CUHK03 and VIPeR. We perform experiments on these two datasets with techniques based on CNN, hand-crafted features as well as LSTM for analysing visual and natural description data. We investigate and demonstrate the advantages of using natural language descriptions compared to attributes as well as CNN compared to LSTM in the context of Re-ID. We show that the joint use of language and vision can significantly improve the state-of-the-art performance on standard Re-ID benchmarks.



### A Survey on Optical Character Recognition System
- **Arxiv ID**: http://arxiv.org/abs/1710.05703v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/1710.05703v1)
- **Published**: 2017-10-03 15:08:49+00:00
- **Updated**: 2017-10-03 15:08:49+00:00
- **Authors**: Noman Islam, Zeeshan Islam, Nazia Noor
- **Comment**: None
- **Journal**: None
- **Summary**: Optical Character Recognition (OCR) has been a topic of interest for many years. It is defined as the process of digitizing a document image into its constituent characters. Despite decades of intense research, developing OCR with capabilities comparable to that of human still remains an open challenge. Due to this challenging nature, researchers from industry and academic circles have directed their attentions towards Optical Character Recognition. Over the last few years, the number of academic laboratories and companies involved in research on Character Recognition has increased dramatically. This research aims at summarizing the research so far done in the field of OCR. It provides an overview of different aspects of OCR and discusses corresponding proposals aimed at resolving issues of OCR.



### Decoding visemes: improving machine lipreading
- **Arxiv ID**: http://arxiv.org/abs/1710.01288v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1710.01288v1)
- **Published**: 2017-10-03 17:29:24+00:00
- **Updated**: 2017-10-03 17:29:24+00:00
- **Authors**: Helen L Bear
- **Comment**: PhD thesis. Computer Vision and Pattern Recognition (CVPR), Women in
  Computer Vision (WiCV) Workshop 2017
- **Journal**: Helen L Bear. Decoding visemes: improving lipreading (PhD thesis).
  University of East Anglia. July 2016
- **Summary**: Machine lipreading (MLR) is speech recognition from visual cues and a niche research problem in speech processing & computer vision. Current challenges fall into two groups: the content of the video, such as rate of speech or; the parameters of the video recording e.g, video resolution. We show that HD video is not needed to successfully lipread with a computer. The term "viseme" is used in machine lipreading to represent a visual cue or gesture which corresponds to a subgroup of phonemes where the phonemes are visually indistinguishable. A phoneme is the smallest sound one can utter, because there are more phonemes per viseme, maps between units show a many-to-one relationship. Many maps have been presented, we compare these and our results show Lee's is best. We propose a new method of speaker-dependent phoneme-to-viseme maps and compare these to Lee's. Our results show the sensitivity of phoneme clustering and we use our new knowledge to augment a conventional MLR system. It has been observed in MLR, that classifiers need training on test subjects to achieve accuracy. Thus machine lipreading is highly speaker-dependent. Conversely speaker independence is robust classification of non-training speakers. We investigate the dependence of phoneme-to-viseme maps between speakers and show there is not a high variability of visemes, but there is high variability in trajectory between visemes of individual speakers with the same ground truth. This implies a dependency upon the number of visemes within each set for each individual. We show that prior phoneme-to-viseme maps rarely have enough visemes and the optimal size, which varies by speaker, ranges from 11-35. Finally we decode from visemes back to phonemes and into words. Our novel approach uses the optimum range visemes within hierarchical training of phoneme classifiers and demonstrates a significant increase in classification accuracy.



### Visual speech recognition: aligning terminologies for better understanding
- **Arxiv ID**: http://arxiv.org/abs/1710.01292v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1710.01292v1)
- **Published**: 2017-10-03 17:45:32+00:00
- **Updated**: 2017-10-03 17:45:32+00:00
- **Authors**: Helen L Bear, Sarah Taylor
- **Comment**: None
- **Journal**: Helen L Bear and Sarah Taylor. Visual speech recognition: aligning
  terminologies for better understanding. British Machine Vision Conference
  (BMVC) Deep learning for machine lip reading workshop. 2017
- **Summary**: We are at an exciting time for machine lipreading. Traditional research stemmed from the adaptation of audio recognition systems. But now, the computer vision community is also participating. This joining of two previously disparate areas with different perspectives on computer lipreading is creating opportunities for collaborations, but in doing so the literature is experiencing challenges in knowledge sharing due to multiple uses of terms and phrases and the range of methods for scoring results.   In particular we highlight three areas with the intention to improve communication between those researching lipreading; the effects of interchanging between speech reading and lipreading; speaker dependence across train, validation, and test splits; and the use of accuracy, correctness, errors, and varying units (phonemes, visemes, words, and sentences) to measure system performance. We make recommendations as to how we can be more consistent.



### Visual gesture variability between talkers in continuous visual speech
- **Arxiv ID**: http://arxiv.org/abs/1710.01297v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1710.01297v1)
- **Published**: 2017-10-03 17:59:43+00:00
- **Updated**: 2017-10-03 17:59:43+00:00
- **Authors**: Helen L Bear
- **Comment**: None
- **Journal**: Helen L Bear. Visual gesture variability between talkers in
  continuous visual speech. British Machine Vision Conference (BMVC) Deep
  learning for machine lip reading workshop. 2017
- **Summary**: Recent adoption of deep learning methods to the field of machine lipreading research gives us two options to pursue to improve system performance. Either, we develop end-to-end systems holistically or, we experiment to further our understanding of the visual speech signal. The latter option is more difficult but this knowledge would enable researchers to both improve systems and apply the new knowledge to other domains such as speech therapy. One challenge in lipreading systems is the correct labeling of the classifiers. These labels map an estimated function between visemes on the lips and the phonemes uttered. Here we ask if such maps are speaker-dependent? Prior work investigated isolated word recognition from speaker-dependent (SD) visemes, we extend this to continuous speech. Benchmarked against SD results, and the isolated words performance, we test with RMAV dataset speakers and observe that with continuous speech, the trajectory between visemes has a greater negative effect on the speaker differentiation.



### Robotic Pick-and-Place of Novel Objects in Clutter with Multi-Affordance Grasping and Cross-Domain Image Matching
- **Arxiv ID**: http://arxiv.org/abs/1710.01330v5
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.01330v5)
- **Published**: 2017-10-03 18:16:09+00:00
- **Updated**: 2020-05-30 19:54:31+00:00
- **Authors**: Andy Zeng, Shuran Song, Kuan-Ting Yu, Elliott Donlon, Francois R. Hogan, Maria Bauza, Daolin Ma, Orion Taylor, Melody Liu, Eudald Romo, Nima Fazeli, Ferran Alet, Nikhil Chavan Dafle, Rachel Holladay, Isabella Morona, Prem Qu Nair, Druck Green, Ian Taylor, Weber Liu, Thomas Funkhouser, Alberto Rodriguez
- **Comment**: Project webpage: http://arc.cs.princeton.edu Summary video:
  https://youtu.be/6fG7zwGfIkI
- **Journal**: None
- **Summary**: This paper presents a robotic pick-and-place system that is capable of grasping and recognizing both known and novel objects in cluttered environments. The key new feature of the system is that it handles a wide range of object categories without needing any task-specific training data for novel objects. To achieve this, it first uses a category-agnostic affordance prediction algorithm to select and execute among four different grasping primitive behaviors. It then recognizes picked objects with a cross-domain image classification framework that matches observed images to product images. Since product images are readily available for a wide range of objects (e.g., from the web), the system works out-of-the-box for novel objects without requiring any additional training data. Exhaustive experimental results demonstrate that our multi-affordance grasping achieves high success rates for a wide variety of objects in clutter, and our recognition algorithm achieves high accuracy for both known and novel grasped objects. The approach was part of the MIT-Princeton Team system that took 1st place in the stowing task at the 2017 Amazon Robotics Challenge. All code, datasets, and pre-trained models are available online at http://arc.cs.princeton.edu



### Understanding the visual speech signal
- **Arxiv ID**: http://arxiv.org/abs/1710.01351v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1710.01351v1)
- **Published**: 2017-10-03 19:10:46+00:00
- **Updated**: 2017-10-03 19:10:46+00:00
- **Authors**: Helen L Bear
- **Comment**: Computer Vision and Pattern Recognition (CVPR) Women in Computer
  Vision (WiCV) workshop. 2017
- **Journal**: None
- **Summary**: For machines to lipread, or understand speech from lip movement, they decode lip-motions (known as visemes) into the spoken sounds. We investigate the visual speech channel to further our understanding of visemes. This has applications beyond machine lipreading; speech therapists, animators, and psychologists can benefit from this work. We explain the influence of speaker individuality, and demonstrate how one can use visemes to boost lipreading.



### BodyDigitizer: An Open Source Photogrammetry-based 3D Body Scanner
- **Arxiv ID**: http://arxiv.org/abs/1710.01370v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1710.01370v2)
- **Published**: 2017-10-03 20:10:10+00:00
- **Updated**: 2017-10-28 19:28:06+00:00
- **Authors**: Travis Gesslein, Daniel Scherer, Jens Grubert
- **Comment**: changed template, minor modifications for camera ready version
- **Journal**: None
- **Summary**: With the rising popularity of Augmented and Virtual Reality, there is a need for representing humans as virtual avatars in various application domains ranging from remote telepresence, games to medical applications. Besides explicitly modelling 3D avatars, sensing approaches that create person-specific avatars are becoming popular. However, affordable solutions typically suffer from a low visual quality and professional solution are often too expensive to be deployed in nonprofit projects.   We present an open-source project, BodyDigitizer, which aims at providing both build instructions and configuration software for a high-resolution photogrammetry-based 3D body scanner. Our system encompasses up to 96 Rasperry PI cameras, active LED lighting, a sturdy frame construction and open-source configuration software. %We demonstrate the applicability of the body scanner in a nonprofit Mixed Reality health project. The detailed build instruction and software are available at http://www.bodydigitizer.org.



### A Fully Convolutional Network for Semantic Labeling of 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1710.01408v1
- **DOI**: 10.1016/j.isprsjprs.2018.03.018
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.01408v1)
- **Published**: 2017-10-03 22:35:25+00:00
- **Updated**: 2017-10-03 22:35:25+00:00
- **Authors**: Mohammed Yousefhussien, David J. Kelbe, Emmett J. Ientilucci, Carl Salvaggio
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, 2018
- **Summary**: When classifying point clouds, a large amount of time is devoted to the process of engineering a reliable set of features which are then passed to a classifier of choice. Generally, such features - usually derived from the 3D-covariance matrix - are computed using the surrounding neighborhood of points. While these features capture local information, the process is usually time-consuming, and requires the application at multiple scales combined with contextual methods in order to adequately describe the diversity of objects within a scene. In this paper we present a 1D-fully convolutional network that consumes terrain-normalized points directly with the corresponding spectral data,if available, to generate point-wise labeling while implicitly learning contextual features in an end-to-end fashion. Our method uses only the 3D-coordinates and three corresponding spectral features for each point. Spectral features may either be extracted from 2D-georeferenced images, as shown here for Light Detection and Ranging (LiDAR) point clouds, or extracted directly for passive-derived point clouds,i.e. from muliple-view imagery. We train our network by splitting the data into square regions, and use a pooling layer that respects the permutation-invariance of the input points. Evaluated using the ISPRS 3D Semantic Labeling Contest, our method scored second place with an overall accuracy of 81.6%. We ranked third place with a mean F1-score of 63.32%, surpassing the F1-score of the method with highest accuracy by 1.69%. In addition to labeling 3D-point clouds, we also show that our method can be easily extended to 2D-semantic segmentation tasks, with promising initial results.



### Smoothness-based Edge Detection using Low-SNR Camera for Robot Navigation
- **Arxiv ID**: http://arxiv.org/abs/1710.01416v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.01416v1)
- **Published**: 2017-10-03 22:48:41+00:00
- **Updated**: 2017-10-03 22:48:41+00:00
- **Authors**: Vu Hoang Minh, Tajwar Abrar Aleef, Usama Pervaiz, Yeman Brhane Hagos, Saed Khawaldeh
- **Comment**: None
- **Journal**: None
- **Summary**: In the emerging advancement in the branch of autonomous robotics, the ability of a robot to efficiently localize and construct maps of its surrounding is crucial. This paper deals with utilizing thermal-infrared cameras, as opposed to conventional cameras as the primary sensor to capture images of the robot's surroundings. For localization, the images need to be further processed before feeding them to a navigational system. The main motivation of this paper was to develop an edge detection methodology capable of utilizing the low-SNR poor output from such a thermal camera and effectively detect smooth edges of the surrounding environment. The enhanced edge detector proposed in this paper takes the raw image from the thermal sensor, denoises the images, applies Canny edge detection followed by CSS method. The edges are ranked to remove any noise and only edges of the highest rank are kept. Then, the broken edges are linked by computing edge metrics and a smooth edge of the surrounding is displayed in a binary image. Several comparisons are also made in the paper between the proposed technique and the existing techniques.



