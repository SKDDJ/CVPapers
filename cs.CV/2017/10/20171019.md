# Arxiv Papers in cs.CV on 2017-10-19
### Computational ghost imaging using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1710.08343v1
- **DOI**: 10.1016/j.optcom.2017.12.041
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1710.08343v1)
- **Published**: 2017-10-19 01:54:52+00:00
- **Updated**: 2017-10-19 01:54:52+00:00
- **Authors**: Tomoyoshi Shimobaba, Yutaka Endo, Takashi Nishitsuji, Takayuki Takahashi, Yuki Nagahama, Satoki Hasegawa, Marie Sano, Ryuji Hirayama, Takashi Kakue, Atsushi Shiraki, Tomoyoshi Ito
- **Comment**: None
- **Journal**: None
- **Summary**: Computational ghost imaging (CGI) is a single-pixel imaging technique that exploits the correlation between known random patterns and the measured intensity of light transmitted (or reflected) by an object. Although CGI can obtain two- or three- dimensional images with a single or a few bucket detectors, the quality of the reconstructed images is reduced by noise due to the reconstruction of images from random patterns. In this study, we improve the quality of CGI images using deep learning. A deep neural network is used to automatically learn the features of noise-contaminated CGI images. After training, the network is able to predict low-noise images from new noise-contaminated CGI images.



### Improved Search in Hamming Space using Deep Multi-Index Hashing
- **Arxiv ID**: http://arxiv.org/abs/1710.06993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06993v1)
- **Published**: 2017-10-19 02:51:12+00:00
- **Updated**: 2017-10-19 02:51:12+00:00
- **Authors**: Hanjiang Lai, Yan Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. There has been considerable research on generating efficient image representation via the deep-network-based hashing methods. However, the issue of efficient searching in the deep representation space remains largely unsolved. To this end, we propose a simple yet efficient deep-network-based multi-index hashing method for simultaneously learning the powerful image representation and the efficient searching. To achieve these two goals, we introduce the multi-index hashing (MIH) mechanism into the proposed deep architecture, which divides the binary codes into multiple substrings. Due to the non-uniformly distributed codes will result in inefficiency searching, we add the two balanced constraints at feature-level and instance-level, respectively. Extensive evaluations on several benchmark image retrieval datasets show that the learned balanced binary codes bring dramatic speedups and achieve comparable performance over the existing baselines.



### Generative Adversarial Networks: An Overview
- **Arxiv ID**: http://arxiv.org/abs/1710.07035v1
- **DOI**: 10.1109/MSP.2017.2765202
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07035v1)
- **Published**: 2017-10-19 08:29:50+00:00
- **Updated**: 2017-10-19 08:29:50+00:00
- **Authors**: Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, Anil A Bharath
- **Comment**: Accepted in the IEEE Signal Processing Magazine Special Issue on Deep
  Learning for Visual Understanding
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this through deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image super-resolution and classification. The aim of this review paper is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.



### Emerging from Water: Underwater Image Color Correction Based on Weakly Supervised Color Transfer
- **Arxiv ID**: http://arxiv.org/abs/1710.07084v3
- **DOI**: 10.1109/LSP.2018.2792050
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07084v3)
- **Published**: 2017-10-19 11:09:19+00:00
- **Updated**: 2018-01-03 10:19:32+00:00
- **Authors**: Chongyi Li, Jichang Guo, Chunle Guo
- **Comment**: Submitted to IEEE Signal Processing Letters
- **Journal**: IEEE Signal Processing Letters 2018
- **Summary**: Underwater vision suffers from severe effects due to selective attenuation and scattering when light propagates through water. Such degradation not only affects the quality of underwater images but limits the ability of vision tasks. Different from existing methods which either ignore the wavelength dependency of the attenuation or assume a specific spectral profile, we tackle color distortion problem of underwater image from a new view. In this letter, we propose a weakly supervised color transfer method to correct color distortion, which relaxes the need of paired underwater images for training and allows for the underwater images unknown where were taken. Inspired by Cycle-Consistent Adversarial Networks, we design a multi-term loss function including adversarial loss, cycle consistency loss, and SSIM (Structural Similarity Index Measure) loss, which allows the content and structure of the corrected result the same as the input, but the color as if the image was taken without the water. Experiments on underwater images captured under diverse scenes show that our method produces visually pleasing results, even outperforms the art-of-the-state methods. Besides, our method can improve the performance of vision tasks.



### Deep Self-taught Learning for Remote Sensing Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1710.07096v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07096v2)
- **Published**: 2017-10-19 11:32:53+00:00
- **Updated**: 2017-12-19 20:55:12+00:00
- **Authors**: Anika Bettge, Ribana Roscher, Susanne Wenzel
- **Comment**: This is a corrected version of the final paper published in the
  proceedings
- **Journal**: Proceedings of the 2017 conference on Big Data from Space
- **Summary**: This paper addresses the land cover classification task for remote sensing images by deep self-taught learning. Our self-taught learning approach learns suitable feature representations of the input data using sparse representation and undercomplete dictionary learning. We propose a deep learning framework which extracts representations in multiple layers and use the output of the deepest layer as input to a classification algorithm. We evaluate our approach using a multispectral Landsat 5 TM image of a study area in the North of Novo Progresso (South America) and the Zurich Summer Data Set provided by the University of Zurich. Experiments indicate that features learned by a deep self-taught learning framework can be used for classification and improve the results compared to classification results using the original feature representation.



### Sea Level Anomaly Prediction using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.07099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07099v1)
- **Published**: 2017-10-19 11:43:34+00:00
- **Updated**: 2017-10-19 11:43:34+00:00
- **Authors**: Anne Braakmann-Folgmann, Ribana Roscher, Susanne Wenzel, Bernd Uebbing, JÃ¼rgen Kusche
- **Comment**: None
- **Journal**: Proceedings of the 2017 conference on Big Data from Space
- **Summary**: Sea level change, one of the most dire impacts of anthropogenic global warming, will affect a large amount of the world's population. However, sea level change is not uniform in time and space, and the skill of conventional prediction methods is limited due to the ocean's internal variabi-lity on timescales from weeks to decades. Here we study the potential of neural network methods which have been used successfully in other applications, but rarely been applied for this task. We develop a combination of a convolutional neural network (CNN) and a recurrent neural network (RNN) to ana-lyse both the spatial and the temporal evolution of sea level and to suggest an independent, accurate method to predict interannual sea level anomalies (SLA). We test our method for the northern and equatorial Pacific Ocean, using gridded altimeter-derived SLA data. We show that the used network designs outperform a simple regression and that adding a CNN improves the skill significantly. The predictions are stable over several years.



### Nonlinear Supervised Dimensionality Reduction via Smooth Regular Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1710.07120v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07120v2)
- **Published**: 2017-10-19 12:48:52+00:00
- **Updated**: 2018-05-28 15:33:18+00:00
- **Authors**: Cem Ornek, Elif Vural
- **Comment**: None
- **Journal**: None
- **Summary**: The recovery of the intrinsic geometric structures of data collections is an important problem in data analysis. Supervised extensions of several manifold learning approaches have been proposed in the recent years. Meanwhile, existing methods primarily focus on the embedding of the training data, and the generalization of the embedding to initially unseen test data is rather ignored. In this work, we build on recent theoretical results on the generalization performance of supervised manifold learning algorithms. Motivated by these performance bounds, we propose a supervised manifold learning method that computes a nonlinear embedding while constructing a smooth and regular interpolation function that extends the embedding to the whole data space in order to achieve satisfactory generalization. The embedding and the interpolator are jointly learnt such that the Lipschitz regularity of the interpolator is imposed while ensuring the separation between different classes. Experimental results on several image data sets show that the proposed method outperforms traditional classifiers and the supervised dimensionality reduction algorithms in comparison in terms of classification accuracy in most settings.



### Visual Speech Recognition Using PCA Networks and LSTMs in a Tandem GMM-HMM System
- **Arxiv ID**: http://arxiv.org/abs/1710.07161v1
- **DOI**: 10.1007/978-3-319-54427-4_20
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07161v1)
- **Published**: 2017-10-19 14:41:25+00:00
- **Updated**: 2017-10-19 14:41:25+00:00
- **Authors**: Marina Zimmermann, Mostafa Mehdipour Ghazi, HazÄ±m Kemal Ekenel, Jean-Philippe Thiran
- **Comment**: None
- **Journal**: ACCV 2016 Workshops. ACCV 2016. Lecture Notes in Computer Science,
  vol 10117. Springer, Cham
- **Summary**: Automatic visual speech recognition is an interesting problem in pattern recognition especially when audio data is noisy or not readily available. It is also a very challenging task mainly because of the lower amount of information in the visual articulations compared to the audible utterance. In this work, principle component analysis is applied to the image patches - extracted from the video data - to learn the weights of a two-stage convolutional network. Block histograms are then extracted as the unsupervised learning features. These features are employed to learn a recurrent neural network with a set of long short-term memory cells to obtain spatiotemporal features. Finally, the obtained features are used in a tandem GMM-HMM system for speech recognition. Our results show that the proposed method has outperformed the baseline techniques applied to the OuluVS2 audiovisual database for phrase recognition with the frontal view cross-validation and testing sentence correctness reaching 79% and 73%, respectively, as compared to the baseline of 74% on cross-validation.



### Combining Multiple Views for Visual Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/1710.07168v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07168v2)
- **Published**: 2017-10-19 14:52:34+00:00
- **Updated**: 2018-06-28 11:49:58+00:00
- **Authors**: Marina Zimmermann, Mostafa Mehdipour Ghazi, HazÄ±m Kemal Ekenel, Jean-Philippe Thiran
- **Comment**: None
- **Journal**: Proceedings of the 14th International Conference on
  Auditory-Visual Speech Processing (AVSP2017)
- **Summary**: Visual speech recognition is a challenging research problem with a particular practical application of aiding audio speech recognition in noisy scenarios. Multiple camera setups can be beneficial for the visual speech recognition systems in terms of improved performance and robustness. In this paper, we explore this aspect and provide a comprehensive study on combining multiple views for visual speech recognition. The thorough analysis covers fusion of all possible view angle combinations both at feature level and decision level. The employed visual speech recognition system in this study extracts features through a PCA-based convolutional neural network, followed by an LSTM network. Finally, these features are processed in a tandem system, being fed into a GMM-HMM scheme. The decision fusion acts after this point by combining the Viterbi path log-likelihoods. The results show that the complementary information contained in recordings from different view angles improves the results significantly. For example, the sentence correctness on the test set is increased from 76% for the highest performing single view ($30^\circ$) to up to 83% when combining this view with the frontal and $60^\circ$ view angles.



### Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description
- **Arxiv ID**: http://arxiv.org/abs/1710.07177v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.07177v1)
- **Published**: 2017-10-19 15:20:14+00:00
- **Updated**: 2017-10-19 15:20:14+00:00
- **Authors**: Desmond Elliott, Stella Frank, LoÃ¯c Barrault, Fethi Bougares, Lucia Specia
- **Comment**: None
- **Journal**: Proceedings of the Second Conference on Machine Translation, 2017,
  pp. 215--233
- **Summary**: We present the results from the second shared task on multimodal machine translation and multilingual image description. Nine teams submitted 19 systems to two tasks. The multimodal translation task, in which the source sentence is supplemented by an image, was extended with a new language (French) and two new test sets. The multilingual image description task was changed such that at test time, only the image is given. Compared to last year, multimodal systems improved, but text-only systems remain competitive.



### Block DCT filtering using vector processing
- **Arxiv ID**: http://arxiv.org/abs/1710.07193v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07193v1)
- **Published**: 2017-10-19 15:38:33+00:00
- **Updated**: 2017-10-19 15:38:33+00:00
- **Authors**: Mostafa Amin-Naji, Ali Aghagolzadeh
- **Comment**: 2016 1st International Conference on New Research Achievements in
  Electrical and Computer Engineering (ICNRAECE), pp. 722-727, 2016
- **Journal**: None
- **Summary**: Filtering is an important issue in signals and images processing. Many images and videos are compressed using discrete cosine transform (DCT). For reducing the computation complexity, we are interested in filtering block and images directly in DCT domain. This article proposed an efficient and yet very simple filtering method directly in DCT domain for any symmetric, asymmetric, separable, inseparable and one or two dimensional filter. The proposed method is achieved by mathematical relations using vector processing for image filtering which it is equivalent to the spatial domain zero padding filtering. Also to avoid the zero padding artifacts around the edge of the block, we prepare preliminary matrices in DCT domain by implementation elements of selected mask which satisfies border replication for a block in the spatial domain. To evaluate the performance of the proposed algorithm, we compared the spatial domain filtering results with the results of the proposed method in DCT domain. The experiments show that the results of our proposed method in DCT are exactly the same as the spatial domain filtering.



### Dress like a Star: Retrieving Fashion Products from Videos
- **Arxiv ID**: http://arxiv.org/abs/1710.07198v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07198v1)
- **Published**: 2017-10-19 15:45:32+00:00
- **Updated**: 2017-10-19 15:45:32+00:00
- **Authors**: Noa Garcia, George Vogiatzis
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes a system for retrieving clothing and fashion products from video content. Although films and television are the perfect showcase for fashion brands to promote their products, spectators are not always aware of where to buy the latest trends they see on screen. Here, a framework for breaking the gap between fashion products shown on videos and users is presented. By relating clothing items and video frames in an indexed database and performing frame retrieval with temporal aggregation and fast indexing techniques, we can find fashion products from videos in a simple and non-intrusive way. Experiments in a large-scale dataset conducted here show that, by using the proposed framework, memory requirements can be reduced by 42.5X with respect to linear search, whereas accuracy is maintained at around 90%.



### FigureQA: An Annotated Figure Dataset for Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1710.07300v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07300v2)
- **Published**: 2017-10-19 18:01:38+00:00
- **Updated**: 2018-02-22 22:50:42+00:00
- **Authors**: Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, Yoshua Bengio
- **Comment**: workshop paper at ICLR 2018
- **Journal**: None
- **Summary**: We introduce FigureQA, a visual reasoning corpus of over one million question-answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts. We formulate our reasoning task by generating questions from 15 templates; questions concern various relationships between plot elements and examine characteristics like the maximum, the minimum, area-under-the-curve, smoothness, and intersection. To resolve, such questions often require reference to multiple plot elements and synthesis of information distributed spatially throughout a figure. To facilitate the training of machine learning systems, the corpus also includes side data that can be used to formulate auxiliary objectives. In particular, we provide the numerical data used to generate each figure as well as bounding-box annotations for all plot elements. We study the proposed visual reasoning task by training several models, including the recently proposed Relation Network as a strong baseline. Preliminary results indicate that the task poses a significant machine learning challenge. We envision FigureQA as a first step towards developing models that can intuitively recognize patterns from visual representations of data.



### Interpretable Transformations with Encoder-Decoder Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.07307v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07307v1)
- **Published**: 2017-10-19 18:28:15+00:00
- **Updated**: 2017-10-19 18:28:15+00:00
- **Authors**: Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, Gabriel J. Brostow
- **Comment**: Accepted at ICCV 2017
- **Journal**: None
- **Summary**: Deep feature spaces have the capacity to encode complex transformations of their input data. However, understanding the relative feature-space relationship between two transformed encoded images is difficult. For instance, what is the relative feature space relationship between two rotated images? What is decoded when we interpolate in feature space? Ideally, we want to disentangle confounding factors, such as pose, appearance, and illumination, from object identity. Disentangling these is difficult because they interact in very nonlinear ways. We propose a simple method to construct a deep feature space, with explicitly disentangled representations of several known transformations. A person or algorithm can then manipulate the disentangled representation, for example, to re-render an image with explicit control over parameterized degrees of freedom. The feature space is constructed using a transforming encoder-decoder network with a custom feature transform layer, acting on the hidden representations. We demonstrate the advantages of explicit disentangling on a variety of datasets and transformations, and as an aid for traditional tasks, such as classification.



### StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.10916v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.10916v3)
- **Published**: 2017-10-19 18:45:59+00:00
- **Updated**: 2018-06-28 00:49:19+00:00
- **Authors**: Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas
- **Comment**: In IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI),
  2018. (16 pages, 15 figures.)
- **Journal**: None
- **Summary**: Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) aiming at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of the object based on given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and discriminators in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.



### Be Your Own Prada: Fashion Synthesis with Structural Coherence
- **Arxiv ID**: http://arxiv.org/abs/1710.07346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07346v1)
- **Published**: 2017-10-19 20:46:26+00:00
- **Updated**: 2017-10-19 20:46:26+00:00
- **Authors**: Shizhan Zhu, Sanja Fidler, Raquel Urtasun, Dahua Lin, Chen Change Loy
- **Comment**: This is the updated version of our original paper appeared in ICCV
  2017 proceedings
- **Journal**: None
- **Summary**: We present a novel and effective approach for generating new clothing on a wearer through generative adversarial learning. Given an input image of a person and a sentence describing a different outfit, our model "redresses" the person as desired, while at the same time keeping the wearer and her/his pose unchanged. Generating new outfits with precise regions conforming to a language description while retaining wearer's body structure is a new challenging task. Existing generative adversarial networks are not ideal in ensuring global coherence of structure given both the input photograph and language description as conditions. We address this challenge by decomposing the complex generative process into two conditional stages. In the first stage, we generate a plausible semantic segmentation map that obeys the wearer's pose as a latent spatial arrangement. An effective spatial constraint is formulated to guide the generation of this semantic segmentation map. In the second stage, a generative model with a newly proposed compositional mapping layer is used to render the final image with precise regions and textures conditioned on this map. We extended the DeepFashion dataset [8] by collecting sentence descriptions for 79K images. We demonstrate the effectiveness of our approach through both quantitative and qualitative evaluations. A user study is also conducted. The codes and the data are available at http://mmlab.ie.cuhk. edu.hk/projects/FashionGAN/.



### Learning to Recognize Actions from Limited Training Examples Using a Recurrent Spiking Neural Model
- **Arxiv ID**: http://arxiv.org/abs/1710.07354v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.07354v1)
- **Published**: 2017-10-19 21:07:02+00:00
- **Updated**: 2017-10-19 21:07:02+00:00
- **Authors**: Priyadarshini Panda, Narayan Srinivasa
- **Comment**: 13 figures (includes supplementary information)
- **Journal**: None
- **Summary**: A fundamental challenge in machine learning today is to build a model that can learn from few examples. Here, we describe a reservoir based spiking neural model for learning to recognize actions with a limited number of labeled videos. First, we propose a novel encoding, inspired by how microsaccades influence visual perception, to extract spike information from raw video data while preserving the temporal correlation across different frames. Using this encoding, we show that the reservoir generalizes its rich dynamical activity toward signature action/movements enabling it to learn from few training examples. We evaluate our approach on the UCF-101 dataset. Our experiments demonstrate that our proposed reservoir achieves 81.3%/87% Top-1/Top-5 accuracy, respectively, on the 101-class data while requiring just 8 video examples per class for training. Our results establish a new benchmark for action recognition from limited video examples for spiking neural models while yielding competetive accuracy with respect to state-of-the-art non-spiking neural models.



### Historical Document Image Segmentation with LDA-Initialized Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.07363v1
- **DOI**: 10.1145/3151509.3151519
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07363v1)
- **Published**: 2017-10-19 22:43:47+00:00
- **Updated**: 2017-10-19 22:43:47+00:00
- **Authors**: Michele Alberti, Mathias Seuret, Vinaychandran Pondenkandath, Rolf Ingold, Marcus Liwicki
- **Comment**: 5 pages
- **Journal**: ICDAR-HIP 2017
- **Summary**: In this paper, we present a novel approach to perform deep neural networks layer-wise weight initialization using Linear Discriminant Analysis (LDA). Typically, the weights of a deep neural network are initialized with: random values, greedy layer-wise pre-training (usually as Deep Belief Network or as auto-encoder) or by re-using the layers from another network (transfer learning). Hence, many training epochs are needed before meaningful weights are learned, or a rather similar dataset is required for seeding a fine-tuning of transfer learning. In this paper, we describe how to turn an LDA into either a neural layer or a classification layer. We analyze the initialization technique on historical documents. First, we show that an LDA-based initialization is quick and leads to a very stable initialization. Furthermore, for the task of layout analysis at pixel level, we investigate the effectiveness of LDA-based initialization and show that it outperforms state-of-the-art random weight initialization methods.



### SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1710.07368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07368v1)
- **Published**: 2017-10-19 23:03:33+00:00
- **Updated**: 2017-10-19 23:03:33+00:00
- **Authors**: Bichen Wu, Alvin Wan, Xiangyu Yue, Kurt Keutzer
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address semantic segmentation of road-objects from 3D LiDAR point clouds. In particular, we wish to detect and categorize instances of interest, such as cars, pedestrians and cyclists. We formulate this problem as a point- wise classification problem, and propose an end-to-end pipeline called SqueezeSeg based on convolutional neural networks (CNN): the CNN takes a transformed LiDAR point cloud as input and directly outputs a point-wise label map, which is then refined by a conditional random field (CRF) implemented as a recurrent layer. Instance-level labels are then obtained by conventional clustering algorithms. Our CNN model is trained on LiDAR point clouds from the KITTI dataset, and our point-wise segmentation labels are derived from 3D bounding boxes from KITTI. To obtain extra training data, we built a LiDAR simulator into Grand Theft Auto V (GTA-V), a popular video game, to synthesize large amounts of realistic training data. Our experiments show that SqueezeSeg achieves high accuracy with astonishingly fast and stable runtime (8.7 ms per frame), highly desirable for autonomous driving applications. Furthermore, additionally training on synthesized data boosts validation accuracy on real-world data. Our source code and synthesized data will be open-sourced.



