# Arxiv Papers in cs.CV on 2017-10-28
### Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/1711.00111v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00111v2)
- **Published**: 2017-10-28 03:51:16+00:00
- **Updated**: 2018-03-15 16:48:22+00:00
- **Authors**: Ludovic Trottier, Philippe Giguère, Brahim Chaib-draa
- **Comment**: Under review at the 15th European Conference on Computer Vision
  (ECCV) (2018)
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have become the most successful approach in many vision-related domains. However, they are limited to domains where data is abundant. Recent works have looked at multi-task learning (MTL) to mitigate data scarcity by leveraging domain-specific information from related tasks. In this paper, we present a novel soft-parameter sharing mechanism for CNNs in a MTL setting, which we refer to as Deep Collaboration. We propose taking into account the notion that task relevance depends on depth by using lateral transformation blocs with skip connections. This allows extracting task-specific features at various depth without sacrificing features relevant to all tasks. We show that CNNs connected with our Deep Collaboration obtain better accuracy on facial landmark detection with related tasks. We finally verify that our approach effectively allows knowledge sharing by showing depth-specific influence of tasks that we know are related.



### Dual Skipping Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.10386v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1710.10386v3)
- **Published**: 2017-10-28 04:18:11+00:00
- **Updated**: 2018-05-27 12:31:43+00:00
- **Authors**: Changmao Cheng, Yanwei Fu, Yu-Gang Jiang, Wei Liu, Wenlian Lu, Jianfeng Feng, Xiangyang Xue
- **Comment**: CVPR 2018 (poster); fix typo
- **Journal**: None
- **Summary**: Inspired by the recent neuroscience studies on the left-right asymmetry of the human brain in processing low and high spatial frequency information, this paper introduces a dual skipping network which carries out coarse-to-fine object categorization. Such a network has two branches to simultaneously deal with both coarse and fine-grained classification tasks. Specifically, we propose a layer-skipping mechanism that learns a gating network to predict which layers to skip in the testing stage. This layer-skipping mechanism endows the network with good flexibility and capability in practice. Evaluations are conducted on several widely used coarse-to-fine object categorization benchmarks, and promising results are achieved by our proposed network model.



### Label Embedding Network: Learning Label Representation for Soft Training of Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.10393v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.10393v1)
- **Published**: 2017-10-28 05:42:19+00:00
- **Updated**: 2017-10-28 05:42:19+00:00
- **Authors**: Xu Sun, Bingzhen Wei, Xuancheng Ren, Shuming Ma
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method, called Label Embedding Network, which can learn label representation (label embedding) during the training process of deep networks. With the proposed method, the label embedding is adaptively and automatically learned through back propagation. The original one-hot represented loss function is converted into a new loss function with soft distributions, such that the originally unrelated labels have continuous interactions with each other during the training process. As a result, the trained model can achieve substantially higher accuracy and with faster convergence speed. Experimental results based on competitive tasks demonstrate the effectiveness of the proposed method, and the learned label embedding is reasonable and interpretable. The proposed method achieves comparable or even better results than the state-of-the-art systems. The source code is available at \url{https://github.com/lancopku/LabelEmb}.



### Total-Text: A Comprehensive Dataset for Scene Text Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1710.10400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.10400v1)
- **Published**: 2017-10-28 06:39:43+00:00
- **Updated**: 2017-10-28 06:39:43+00:00
- **Authors**: Chee Kheng Chng, Chee Seng Chan
- **Comment**: Accepted as Oral presentation in ICDAR2017 (Extended version, 13
  pages 17 figures). We introduce a new scene text dataset namely as
  Total-Text, which is more comprehensive than the existing scene text datasets
  as it consists of 1555 natural images with more than 3 different text
  orientations, one of a kind
- **Journal**: None
- **Summary**: Text in curve orientation, despite being one of the common text orientations in real world environment, has close to zero existence in well received scene text datasets such as ICDAR2013 and MSRA-TD500. The main motivation of Total-Text is to fill this gap and facilitate a new research direction for the scene text community. On top of the conventional horizontal and multi-oriented texts, it features curved-oriented text. Total-Text is highly diversified in orientations, more than half of its images have a combination of more than two orientations. Recently, a new breed of solutions that casted text detection as a segmentation problem has demonstrated their effectiveness against multi-oriented text. In order to evaluate its robustness against curved text, we fine-tuned DeconvNet and benchmark it on Total-Text. Total-Text with its annotation is available at https://github.com/cs-chan/Total-Text-Dataset



### Toward predictive machine learning for active vision
- **Arxiv ID**: http://arxiv.org/abs/1710.10460v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.10460v3)
- **Published**: 2017-10-28 13:08:19+00:00
- **Updated**: 2018-01-08 11:01:59+00:00
- **Authors**: Emmanuel Daucé
- **Comment**: submitted to ICLR 2018
- **Journal**: None
- **Summary**: We develop a comprehensive description of the active inference framework, as proposed by Friston (2010), under a machine-learning compliant perspective. Stemming from a biological inspiration and the auto-encoding principles, the sketch of a cognitive architecture is proposed that should provide ways to implement estimation-oriented control policies. Computer simulations illustrate the effectiveness of the approach through a foveated inspection of the input data. The pros and cons of the control policy are analyzed in detail, showing interesting promises in terms of processing compression. Though optimizing future posterior entropy over the actions set is shown enough to attain locally optimal action selection, offline calculation using class-specific saliency maps is shown better for it saves processing costs through saccades pathways pre-processing, with a negligible effect on the recognition/compression rates.



### SeeThrough: Finding Chairs in Heavily Occluded Indoor Scene Images
- **Arxiv ID**: http://arxiv.org/abs/1710.10473v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.10473v2)
- **Published**: 2017-10-28 14:30:39+00:00
- **Updated**: 2017-12-04 15:23:45+00:00
- **Authors**: Moos Hueting, Pradyumna Reddy, Vladimir Kim, Ersin Yumer, Nathan Carr, Niloy Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: Discovering 3D arrangements of objects from single indoor images is important given its many applications including interior design, content creation, etc. Although heavily researched in the recent years, existing approaches break down under medium or heavy occlusion as the core object detection module starts failing in absence of directly visible cues. Instead, we take into account holistic contextual 3D information, exploiting the fact that objects in indoor scenes co-occur mostly in typical near-regular configurations. First, we use a neural network trained on real indoor annotated images to extract 2D keypoints, and feed them to a 3D candidate object generation stage. Then, we solve a global selection problem among these 3D candidates using pairwise co-occurrence statistics discovered from a large 3D scene database. We iterate the process allowing for candidates with low keypoint response to be incrementally detected based on the location of the already discovered nearby objects. Focusing on chairs, we demonstrate significant performance improvement over combinations of state-of-the-art methods, especially for scenes with moderately to severely occluded objects.



### Learning to diagnose from scratch by exploiting dependencies among labels
- **Arxiv ID**: http://arxiv.org/abs/1710.10501v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.10501v2)
- **Published**: 2017-10-28 17:25:23+00:00
- **Updated**: 2018-02-01 22:16:56+00:00
- **Authors**: Li Yao, Eric Poblenz, Dmitry Dagunts, Ben Covington, Devon Bernard, Kevin Lyman
- **Comment**: include the link for the dataset split
- **Journal**: None
- **Summary**: The field of medical diagnostics contains a wealth of challenges which closely resemble classical machine learning problems; practical constraints, however, complicate the translation of these endpoints naively into classical architectures. Many tasks in radiology, for example, are largely problems of multi-label classification wherein medical images are interpreted to indicate multiple present or suspected pathologies. Clinical settings drive the necessity for high accuracy simultaneously across a multitude of pathological outcomes and greatly limit the utility of tools which consider only a subset. This issue is exacerbated by a general scarcity of training data and maximizes the need to extract clinically relevant features from available samples -- ideally without the use of pre-trained models which may carry forward undesirable biases from tangentially related tasks. We present and evaluate a partial solution to these constraints in using LSTMs to leverage interdependencies among target labels in predicting 14 pathologic patterns from chest x-rays and establish state of the art results on the largest publicly available chest x-ray dataset from the NIH without pre-training. Furthermore, we propose and discuss alternative evaluation metrics and their relevance in clinical practice.



### Exploiting Points and Lines in Regression Forests for RGB-D Camera Relocalization
- **Arxiv ID**: http://arxiv.org/abs/1710.10519v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1710.10519v3)
- **Published**: 2017-10-28 19:37:33+00:00
- **Updated**: 2018-07-28 22:29:09+00:00
- **Authors**: Lili Meng, Frederick Tung, James J. Little, Julien Valentin, Clarence de Silva
- **Comment**: published as a conference paper at 2018 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)
- **Journal**: None
- **Summary**: Camera relocalization plays a vital role in many robotics and computer vision tasks, such as global localization, recovery from tracking failure and loop closure detection. Recent random forests based methods exploit randomly sampled pixel comparison features to predict 3D world locations for 2D image locations to guide the camera pose optimization. However, these image features are only sampled randomly in the images, without considering the spatial structures or geometric information, leading to large errors or failure cases with the existence of poorly textured areas or in motion blur. Line segment features are more robust in these environments. In this work, we propose to jointly exploit points and lines within the framework of uncertainty driven regression forests. The proposed approach is thoroughly evaluated on three publicly available datasets against several strong state-of-the-art baselines in terms of several different error metrics. Experimental results prove the efficacy of our method, showing superior or on-par state-of-the-art performance.



### Object Recognition by Using Multi-level Feature Point Extraction
- **Arxiv ID**: http://arxiv.org/abs/1710.10522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.10522v1)
- **Published**: 2017-10-28 19:54:21+00:00
- **Updated**: 2017-10-28 19:54:21+00:00
- **Authors**: Yang Cheng, Timeo Dubois
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel approach for object recognition in real-time by employing multilevel feature analysis and demonstrate the practicality of adapting feature extraction into a Naive Bayesian classification framework that enables simple, efficient, and robust performance. We also show the proposed method scales well as the number of level-classes grows. To effectively understand the patches surrounding a keypoint, the trained classifier uses hundreds of simple binary features and models class posterior probabilities. In addition, the classification process is computationally cheap under the assumed independence between arbitrary sets of features. Even though for some particular scenarios, this assumption can be invalid. We demonstrate that the efficient classifier nevertheless performs remarkably well on image datasets with a large variation in the illumination environment and image capture perspectives. The experiment results show consistent accuracy can be achieved on many challenging dataset while offer interactive speed for large resolution images. The method demonstrates promising results that outperform the state-of-the-art methods on pattern recognition.



### Automated Tumor Segmentation and Brain Mapping for the Tumor Area
- **Arxiv ID**: http://arxiv.org/abs/1710.11121v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.11121v1)
- **Published**: 2017-10-28 20:49:25+00:00
- **Updated**: 2017-10-28 20:49:25+00:00
- **Authors**: Pranay Manocha, Snehal Bhasme, Tanvi Gupta, BK Panigrahi, Tapan K. Gandhi
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) is an important diagnostic tool for precise detection of various pathologies. Magnetic Resonance (MR) is more preferred than Computed Tomography (CT) due to the high resolution in MR images which help in better detection of neurological conditions. Graphical user interface (GUI) aided disease detection has become increasingly useful due to the increasing workload of doctors. In this proposed work, a novel two steps GUI technique for brain tumor segmentation as well as Brodmann area detec-tion of the segmented tumor is proposed. A data set of T2 weighted images of 15 patients is used for validating the proposed method. The patient data incor-porates variations in ethnicities, gender (male and female) and age (25-50), thus enhancing the authenticity of the proposed method. The tumors were segmented using Fuzzy C Means Clustering and Brodmann area detection was done using a known template, mapping each area to the segmented tumor image. The proposed method was found to be fairly accurate and robust in detecting tumor.



### Multi-Resolution Fully Convolutional Neural Networks for Monaural Audio Source Separation
- **Arxiv ID**: http://arxiv.org/abs/1710.11473v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS, 68T01, H.5.5; I.5; I.2.6; I.4.3; I.4; I.2
- **Links**: [PDF](http://arxiv.org/pdf/1710.11473v1)
- **Published**: 2017-10-28 22:12:08+00:00
- **Updated**: 2017-10-28 22:12:08+00:00
- **Authors**: Emad M. Grais, Hagen Wierstorf, Dominic Ward, Mark D. Plumbley
- **Comment**: arXiv admin note: text overlap with arXiv:1703.08019
- **Journal**: None
- **Summary**: In deep neural networks with convolutional layers, each layer typically has fixed-size/single-resolution receptive field (RF). Convolutional layers with a large RF capture global information from the input features, while layers with small RF size capture local details with high resolution from the input features. In this work, we introduce novel deep multi-resolution fully convolutional neural networks (MR-FCNN), where each layer has different RF sizes to extract multi-resolution features that capture the global and local details information from its input features. The proposed MR-FCNN is applied to separate a target audio source from a mixture of many audio sources. Experimental results show that using MR-FCNN improves the performance compared to feedforward deep neural networks (DNNs) and single resolution deep fully convolutional neural networks (FCNNs) on the audio source separation problem.



