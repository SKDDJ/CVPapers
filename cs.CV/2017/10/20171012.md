# Arxiv Papers in cs.CV on 2017-10-12
### A Finite Element Computational Framework for Active Contours on Graphs
- **Arxiv ID**: http://arxiv.org/abs/1710.04346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04346v1)
- **Published**: 2017-10-12 02:50:10+00:00
- **Updated**: 2017-10-12 02:50:10+00:00
- **Authors**: Nikolaos Kolotouros, Petros Maragos
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a new framework for the solution of active contour models on graphs. With the use of the Finite Element Method we generalize active contour models on graphs and reduce the problem from a partial differential equation to the solution of a sparse non-linear system. Additionally, we extend the proposed framework to solve models where the curve evolution is locally constrained around its current location. Based on the previous extension, we propose a fast algorithm for the solution of a wide range active contour models. Last, we present a supervised extension of Geodesic Active Contours for image segmentation and provide experimental evidence for the effectiveness of our framework.



### Fast initial guess estimation for digital image correlation
- **Arxiv ID**: http://arxiv.org/abs/1710.04359v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.ins-det, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1710.04359v2)
- **Published**: 2017-10-12 04:20:00+00:00
- **Updated**: 2019-09-22 16:56:25+00:00
- **Authors**: Peihan Tu
- **Comment**: The method does not have sufficient validations
- **Journal**: None
- **Summary**: Digital image correlation (DIC) is a widely used optical metrology for quantitative deformation measurement due to its non-contact, low-cost, highly precise feature. DIC relies on nonlinear optimization algorithm. Thus it is quite important to efficiently obtain a reliable initial guess. The most widely used method for obtaining initial guess is reliability-guided digital image correlation (RG-DIC) method, which is reliable but path-dependent. This path-dependent method limits the further improvement of computation speed of DIC using parallel computing technology, and error of calculation may be spread out along the calculation path. Therefore, a reliable and path-independent algorithm which is able to provide reliable initial guess is desirable to reach full potential of the ability of parallel computing. In this paper, an algorithm used for initial guess estimation is proposed. Numerical and real experiments show that the proposed algorithm, adaptive incremental dissimilarity approximations algorithm (A-IDA), has the following characteristics: 1) Compared with inverse compositional Gauss-Newton (IC-GN) sub-pixel registration algorithm, the computational time required by A-IDA algorithm is negligible, especially when subset size is relatively large; 2) the efficiency of A-IDA algorithm is less influenced by search range; 3) the efficiency is less influenced by subset size; 4) it is easy to select the threshold for the proposed algorithm.



### Fast, Accurate and Fully Parallelizable Digital Image Correlation
- **Arxiv ID**: http://arxiv.org/abs/1710.04374v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/1710.04374v2)
- **Published**: 2017-10-12 05:36:32+00:00
- **Updated**: 2019-09-22 16:56:21+00:00
- **Authors**: Peihan Tu
- **Comment**: The method does not have sufficient validations
- **Journal**: None
- **Summary**: Digital image correlation (DIC) is a widely used optical metrology for surface deformation measurements. DIC relies on nonlinear optimization method. Thus an initial guess is quite important due to its influence on the converge characteristics of the algorithm. In order to obtain a reliable, accurate initial guess, a reliability-guided digital image correlation (RG-DIC) method, which is able to intelligently obtain a reliable initial guess without using time-consuming integer-pixel registration, was proposed. However, the RG-DIC and its improved methods are path-dependent and cannot be fully parallelized. Besides, it is highly possible that RG-DIC fails in the full-field analysis of deformation without manual intervention if the deformation fields contain large areas of discontinuous deformation. Feature-based initial guess is highly robust while it is relatively time-consuming. Recently, path-independent algorithm, fast Fourier transform-based cross correlation (FFT-CC) algorithm, was proposed to estimate the initial guess. Complete parallelizability is the major advantage of the FFT-CC algorithm, while it is sensitive to small deformation. Wu et al proposed an efficient integer-pixel search scheme, but the parameters of this algorithm are set by the users empirically. In this technical note, a fully parallelizable DIC method is proposed. Different from RG-DIC method, the proposed method divides DIC algorithm into two parts: full-field initial guess estimation and sub-pixel registration. The proposed method has the following benefits: 1) providing a pre-knowledge of deformation fields; 2) saving computational time; 3) reducing error propagation; 4) integratability with well-established DIC algorithms; 5) fully parallelizability.



### Self-Taught Support Vector Machine
- **Arxiv ID**: http://arxiv.org/abs/1710.04450v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.04450v1)
- **Published**: 2017-10-12 11:12:30+00:00
- **Updated**: 2017-10-12 11:12:30+00:00
- **Authors**: Parvin Razzaghi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a new approach for classification of target task using limited labeled target data as well as enormous unlabeled source data is proposed which is called self-taught learning. The target and source data can be drawn from different distributions. In the previous approaches, covariate shift assumption is considered where the marginal distributions p(x) change over domains and the conditional distributions p(y|x) remain the same. In our approach, we propose a new objective function which simultaneously learns a common space T(.) where the conditional distributions over domains p(T(x)|y) remain the same and learns robust SVM classifiers for target task using both source and target data in the new representation. Hence, in the proposed objective function, the hidden label of the source data is also incorporated. We applied the proposed approach on Caltech-256, MSRC+LMO datasets and compared the performance of our algorithm to the available competing methods. Our method has a superior performance to the successful existing algorithms.



### VOIDD: automatic vessel of intervention dynamic detection in PCI procedures
- **Arxiv ID**: http://arxiv.org/abs/1710.04476v1
- **DOI**: 10.1109/MSP.2009.934154
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04476v1)
- **Published**: 2017-10-12 12:40:29+00:00
- **Updated**: 2017-10-12 12:40:29+00:00
- **Authors**: Ketan Bacchuwar, Jean Cousty, RÃ©gis Vaillant, Laurent Najman
- **Comment**: None
- **Journal**: CVII-Stent Workshop MICCAI 2017, Sep 2017, Quebec City, Canada. 26
  (6), pp.136 - 157, 2009
- **Summary**: In this article, we present the work towards improving the overall workflow of the Percutaneous Coronary Interventions (PCI) procedures by capacitating the imaging instruments to precisely monitor the steps of the procedure. In the long term, such capabilities can be used to optimize the image acquisition to reduce the amount of dose or contrast media employed during the procedure. We present the automatic VOIDD algorithm to detect the vessel of intervention which is going to be treated during the procedure by combining information from the vessel image with contrast agent injection and images acquired during guidewire tip navigation. Due to the robust guidewire tip segmentation method, this algorithm is also able to automatically detect the sequence corresponding to guidewire navigation. We present an evaluation methodology which characterizes the correctness of the guide wire tip detection and correct identification of the vessel navigated during the procedure. On a dataset of 2213 images from 8 sequences of 4 patients, VOIDD identifies vessel-of-intervention with accuracy in the range of 88% or above and absence of tip with accuracy in range of 98% or above depending on the test case.



### Multimodal Observation and Interpretation of Subjects Engaged in Problem Solving
- **Arxiv ID**: http://arxiv.org/abs/1710.04486v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.04486v1)
- **Published**: 2017-10-12 12:59:42+00:00
- **Updated**: 2017-10-12 12:59:42+00:00
- **Authors**: Thomas Guntz, Raffaella Balzarini, Dominique Vaufreydaz, James L. Crowley
- **Comment**: None
- **Journal**: 1st Workshop on "Behavior, Emotion and Representation: Building
  Blocks of Interaction'', Oct 2017, Bielefeld, Germany. 2017
- **Summary**: In this paper we present the first results of a pilot experiment in the capture and interpretation of multimodal signals of human experts engaged in solving challenging chess problems. Our goal is to investigate the extent to which observations of eye-gaze, posture, emotion and other physiological signals can be used to model the cognitive state of subjects, and to explore the integration of multiple sensor modalities to improve the reliability of detection of human displays of awareness and emotion. We observed chess players engaged in problems of increasing difficulty while recording their behavior. Such recordings can be used to estimate a participant's awareness of the current situation and to predict ability to respond effectively to challenging situations. Results show that a multimodal approach is more accurate than a unimodal one. By combining body posture, visual attention and emotion, the multimodal approach can reach up to 93% of accuracy when determining player's chess expertise while unimodal approach reaches 86%. Finally this experiment validates the use of our equipment as a general and reproducible tool for the study of participants engaged in screen-based interaction and/or problem solving.



### Hierarchical Convolutional-Deconvolutional Neural Networks for Automatic Liver and Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1710.04540v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04540v1)
- **Published**: 2017-10-12 14:32:38+00:00
- **Updated**: 2017-10-12 14:32:38+00:00
- **Authors**: Yading Yuan
- **Comment**: 2017 MICCAI-LiTS challenge
- **Journal**: None
- **Summary**: Automatic segmentation of liver and its tumors is an essential step for extracting quantitative imaging biomarkers for accurate tumor detection, diagnosis, prognosis and assessment of tumor response to treatment. MICCAI 2017 Liver Tumor Segmentation Challenge (LiTS) provides a common platform for comparing different automatic algorithms on contrast-enhanced abdominal CT images in tasks including 1) liver segmentation, 2) liver tumor segmentation, and 3) tumor burden estimation. We participate this challenge by developing a hierarchical framework based on deep fully convolutional-deconvolutional neural networks (CDNN). A simple CDNN model is firstly trained to provide a quick but coarse segmentation of the liver on the entire CT volume, then another CDNN is applied to the liver region for fine liver segmentation. At last, the segmented liver region, which is enhanced by histogram equalization, is employed as an additional input to the third CDNN for tumor segmentation. Jaccard distance is used as loss function when training CDNN models to eliminate the need of sample re-weighting. Our framework is trained using the 130 challenge training cases provided by LiTS. The evaluation on the 70 challenge testing cases resulted in a mean Dice Similarity Coefficient (DSC) of 0.963 for liver segmentation, a mean DSC of 0.657 for tumor segmentation, and a root mean square error (RMSE) of 0.017 for tumor burden estimation, which ranked our method in the first, fifth and third place, respectively



### Analysis of planar ornament patterns via motif asymmetry assumption and local connections
- **Arxiv ID**: http://arxiv.org/abs/1710.04623v1
- **DOI**: 10.1007/s10851-018-0835-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04623v1)
- **Published**: 2017-10-12 17:19:06+00:00
- **Updated**: 2017-10-12 17:19:06+00:00
- **Authors**: Venera Adanova, Sibel Tari
- **Comment**: None
- **Journal**: None
- **Summary**: Planar ornaments, a.k.a. wallpapers, are regular repetitive patterns which exhibit translational symmetry in two independent directions. There are exactly $17$ distinct planar symmetry groups. We present a fully automatic method for complete analysis of planar ornaments in $13$ of these groups, specifically, the groups called $p6m, \, p6, \, p4g, \,p4m, \,p4, \, p31m, \,p3m, \, p3, \, cmm, \, pgg, \, pg, \, p2$ and $p1$. Given the image of an ornament fragment, we present a method to simultaneously classify the input into one of the $13$ groups and extract the so called fundamental domain (FD), the minimum region that is sufficient to reconstruct the entire ornament. A nice feature of our method is that even when the given ornament image is a small portion such that it does not contain multiple translational units, the symmetry group as well as the fundamental domain can still be defined. This is because, in contrast to common approach, we do not attempt to first identify a global translational repetition lattice. Though the presented constructions work for quite a wide range of ornament patterns, a key assumption we make is that the perceivable motifs (shapes that repeat) alone do not provide clues for the underlying symmetries of the ornament. In this sense, our main target is the planar arrangements of asymmetric interlocking shapes, as in the symmetry art of Escher.



### Progressive Representation Adaptation for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1710.04647v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04647v1)
- **Published**: 2017-10-12 17:58:30+00:00
- **Updated**: 2017-10-12 17:58:30+00:00
- **Authors**: Dong Li, Jia-Bin Huang, Yali Li, Shengjin Wang, Ming-Hsuan Yang
- **Comment**: Project page: https://sites.google.com/site/lidonggg930/wsl
- **Journal**: None
- **Summary**: We address the problem of weakly supervised object localization where only image-level annotations are available for training object detectors. Numerous methods have been proposed to tackle this problem through mining object proposals. However, a substantial amount of noise in object proposals causes ambiguities for learning discriminative object models. Such approaches are sensitive to model initialization and often converge to undesirable local minimum solutions. In this paper, we propose to overcome these drawbacks by progressive representation adaptation with two main steps: 1) classification adaptation and 2) detection adaptation. In classification adaptation, we transfer a pre-trained network to a multi-label classification task for recognizing the presence of a certain object in an image. Through the classification adaptation step, the network learns discriminative representations that are specific to object categories of interest. In detection adaptation, we mine class-specific object proposals by exploiting two scoring strategies based on the adapted classification network. Class-specific proposal mining helps remove substantial noise from the background clutter and potential confusion from similar objects. We further refine these proposals using multiple instance learning and segmentation cues. Using these refined object bounding boxes, we fine-tune all the layer of the classification network and obtain a fully adapted detection network. We present detailed experimental validation on the PASCAL VOC and ILSVRC datasets. Experimental results demonstrate that our progressive representation adaptation algorithm performs favorably against the state-of-the-art methods.



### Hyperspectral band selection using genetic algorithm and support vector machines for early identification of charcoal rot disease in soybean
- **Arxiv ID**: http://arxiv.org/abs/1710.04681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04681v1)
- **Published**: 2017-10-12 18:27:41+00:00
- **Updated**: 2017-10-12 18:27:41+00:00
- **Authors**: Koushik Nagasubramanian, Sarah Jones, Soumik Sarkar, Asheesh K. Singh, Arti Singh, Baskar Ganapathysubramanian
- **Comment**: None
- **Journal**: None
- **Summary**: Charcoal rot is a fungal disease that thrives in warm dry conditions and affects the yield of soybeans and other important agronomic crops worldwide. There is a need for robust, automatic and consistent early detection and quantification of disease symptoms which are important in breeding programs for the development of improved cultivars and in crop production for the implementation of disease control measures for yield protection. Current methods of plant disease phenotyping are predominantly visual and hence are slow and prone to human error and variation. There has been increasing interest in hyperspectral imaging applications for early detection of disease symptoms. However, the high dimensionality of hyperspectral data makes it very important to have an efficient analysis pipeline in place for the identification of disease so that effective crop management decisions can be made. The focus of this work is to determine the minimal number of most effective hyperspectral bands that can distinguish between healthy and diseased specimens early in the growing season. Healthy and diseased hyperspectral data cubes were captured at 3, 6, 9, 12, and 15 days after inoculation. We utilized inoculated and control specimens from 4 different genotypes. Each hyperspectral image was captured at 240 different wavelengths in the range of 383 to 1032 nm. We used a combination of genetic algorithm as an optimizer and support vector machines as a classifier for identification of maximally effective band combinations. A binary classification between healthy and infected samples using six selected band combinations obtained a classification accuracy of 97% and a F1 score of 0.97 for the infected class. The results demonstrated that these carefully chosen bands are more informative than RGB images, and could be used in a multispectral camera for remote identification of charcoal rot infection in soybean.



### Hardware design for binarization and thinning of fingerprint images
- **Arxiv ID**: http://arxiv.org/abs/1710.05749v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/1710.05749v1)
- **Published**: 2017-10-12 19:50:47+00:00
- **Updated**: 2017-10-12 19:50:47+00:00
- **Authors**: Farshad Kheiri, Shadrokh Samavi, Nader Karimi
- **Comment**: None
- **Journal**: None
- **Summary**: Two critical steps in fingerprint recognition are binarization and thinning of the image. The need for real time processing motivates us to select local adaptive thresholding approach for the binarization step. We introduce a new hardware for this purpose based on pipeline architecture. We propose a formula for selecting an optimal block size for the thresholding purpose. To decrease minutiae false detection, the binarized image is dilated. We also present in this paper a new pipeline structure for implementing the thinning algorithm



### Can the early human visual system compete with Deep Neural Networks?
- **Arxiv ID**: http://arxiv.org/abs/1710.04744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04744v1)
- **Published**: 2017-10-12 22:50:00+00:00
- **Updated**: 2017-10-12 22:50:00+00:00
- **Authors**: Samuel Dodge, Lina Karam
- **Comment**: Accepted as an oral paper at the Mutual Benefits of Cognitive and
  Computer Vision Workshop (held in conjunction with ICCV2017)
- **Journal**: None
- **Summary**: We study and compare the human visual system and state-of-the-art deep neural networks on classification of distorted images. Different from previous works, we limit the display time to 100ms to test only the early mechanisms of the human visual system, without allowing time for any eye movements or other higher level processes. Our findings show that the human visual system still outperforms modern deep neural networks under blurry and noisy images. These findings motivate future research into developing more robust deep networks.



### Explaining Aviation Safety Incidents Using Deep Temporal Multiple Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/1710.04749v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.04749v2)
- **Published**: 2017-10-12 23:42:00+00:00
- **Updated**: 2018-02-12 05:16:08+00:00
- **Authors**: Vijay Manikandan Janakiraman
- **Comment**: None
- **Journal**: None
- **Summary**: Although aviation accidents are rare, safety incidents occur more frequently and require a careful analysis to detect and mitigate risks in a timely manner. Analyzing safety incidents using operational data and producing event-based explanations is invaluable to airline companies as well as to governing organizations such as the Federal Aviation Administration (FAA) in the United States. However, this task is challenging because of the complexity involved in mining multi-dimensional heterogeneous time series data, the lack of time-step-wise annotation of events in a flight, and the lack of scalable tools to perform analysis over a large number of events. In this work, we propose a precursor mining algorithm that identifies events in the multidimensional time series that are correlated with the safety incident. Precursors are valuable to systems health and safety monitoring and in explaining and forecasting safety incidents. Current methods suffer from poor scalability to high dimensional time series data and are inefficient in capturing temporal behavior. We propose an approach by combining multiple-instance learning (MIL) and deep recurrent neural networks (DRNN) to take advantage of MIL's ability to learn using weakly supervised data and DRNN's ability to model temporal behavior. We describe the algorithm, the data, the intuition behind taking a MIL approach, and a comparative analysis of the proposed algorithm with baseline models. We also discuss the application to a real-world aviation safety problem using data from a commercial airline company and discuss the model's abilities and shortcomings, with some final remarks about possible deployment directions.



