# Arxiv Papers in cs.CV on 2017-10-14
### An adaptive thresholding approach for automatic optic disk segmentation
- **Arxiv ID**: http://arxiv.org/abs/1710.05104v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05104v1)
- **Published**: 2017-10-14 00:04:52+00:00
- **Updated**: 2017-10-14 00:04:52+00:00
- **Authors**: Farnoosh Ghadiri, Robert Bergevin, Masoud Shafiee
- **Comment**: Accepted in VISAPP 2015
- **Journal**: None
- **Summary**: Optic disk segmentation is a prerequisite step in automatic retinal screening systems. In this paper, we propose an algorithm for optic disk segmentation based on a local adaptive thresholding method. Location of the optic disk is validated by intensity and average vessel width of retinal images. Then an adaptive thresholding is applied on the temporal and nasal part of the optic disc separately. Adaptive thresholding, makes our algorithm robust to illumination variations and various image acquisition conditions. Moreover, experimental results on the DRIVE and KHATAM databases show promising results compared to the recent literature. In the DRIVE database, the optic disk in all images is correctly located and the mean overlap reached to 43.21%. The optic disk is correctly detected in 98% of the images with the mean overlap of 36.32% in the KHATAM database.



### CM-GANs: Cross-modal Generative Adversarial Networks for Common Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1710.05106v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1710.05106v2)
- **Published**: 2017-10-14 00:15:56+00:00
- **Updated**: 2018-04-26 16:38:56+00:00
- **Authors**: Yuxin Peng, Jinwei Qi, Yuxin Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: It is known that the inconsistent distribution and representation of different modalities, such as image and text, cause the heterogeneity gap that makes it challenging to correlate such heterogeneous data. Generative adversarial networks (GANs) have shown its strong ability of modeling data distribution and learning discriminative representation, existing GANs-based works mainly focus on generative problem to generate new data. We have different goal, aim to correlate heterogeneous data, by utilizing the power of GANs to model cross-modal joint distribution. Thus, we propose Cross-modal GANs to learn discriminative common representation for bridging heterogeneity gap. The main contributions are: (1) Cross-modal GANs architecture is proposed to model joint distribution over data of different modalities. The inter-modality and intra-modality correlation can be explored simultaneously in generative and discriminative models. Both of them beat each other to promote cross-modal correlation learning. (2) Cross-modal convolutional autoencoders with weight-sharing constraint are proposed to form generative model. They can not only exploit cross-modal correlation for learning common representation, but also preserve reconstruction information for capturing semantic consistency within each modality. (3) Cross-modal adversarial mechanism is proposed, which utilizes two kinds of discriminative models to simultaneously conduct intra-modality and inter-modality discrimination. They can mutually boost to make common representation more discriminative by adversarial training process. To the best of our knowledge, our proposed CM-GANs approach is the first to utilize GANs to perform cross-modal common representation learning. Experiments are conducted to verify the performance of our proposed approach on cross-modal retrieval paradigm, compared with 10 methods on 3 cross-modal datasets.



### Video Classification With CNNs: Using The Codec As A Spatio-Temporal Activity Sensor
- **Arxiv ID**: http://arxiv.org/abs/1710.05112v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05112v2)
- **Published**: 2017-10-14 00:43:18+00:00
- **Updated**: 2017-12-19 22:11:49+00:00
- **Authors**: Aaron Chadha, Alhabib Abbas, Yiannis Andreopoulos
- **Comment**: Accepted in IEEE Transactions on Circuits and Systems for Video
  Technology. Extension of ICIP 2017 conference paper
- **Journal**: None
- **Summary**: We investigate video classification via a two-stream convolutional neural network (CNN) design that directly ingests information extracted from compressed video bitstreams. Our approach begins with the observation that all modern video codecs divide the input frames into macroblocks (MBs). We demonstrate that selective access to MB motion vector (MV) information within compressed video bitstreams can also provide for selective, motion-adaptive, MB pixel decoding (a.k.a., MB texture decoding). This in turn allows for the derivation of spatio-temporal video activity regions at extremely high speed in comparison to conventional full-frame decoding followed by optical flow estimation. In order to evaluate the accuracy of a video classification framework based on such activity data, we independently train two CNN architectures on MB texture and MV correspondences and then fuse their scores to derive the final classification of each test video. Evaluation on two standard datasets shows that the proposed approach is competitive to the best two-stream video classification approaches found in the literature. At the same time: (i) a CPU-based realization of our MV extraction is over 977 times faster than GPU-based optical flow methods; (ii) selective decoding is up to 12 times faster than full-frame decoding; (iii) our proposed spatial and temporal CNNs perform inference at 5 to 49 times lower cloud computing cost than the fastest methods from the literature.



### Hierarchical semantic segmentation using modular convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1710.05126v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05126v1)
- **Published**: 2017-10-14 02:44:31+00:00
- **Updated**: 2017-10-14 02:44:31+00:00
- **Authors**: Sagi Eppel
- **Comment**: None
- **Journal**: None
- **Summary**: Image recognition tasks that involve identifying parts of an object or the contents of a vessel can be viewed as a hierarchical problem, which can be solved by initial recognition of the main object, followed by recognition of its parts or contents. To achieve such modular recognition, it is necessary to use the output of one recognition method (which identifies the general object) as the input for a second method (which identifies the parts or contents). In recent years, convolutional neural networks have emerged as the dominant method for segmentation and classification of images. This work examines a method for serially connecting convolutional neural networks for semantic segmentation of materials inside transparent vessels. It applies one fully convolutional neural net to segment the image into vessel and background, and the vessel region is used as an input for a second net which recognizes the contents of the glass vessel. Transferring the segmentation map generated by the first nets to the second net was performed using the valve filter attention method that involves using different filters on different segments of the image. This modular semantic segmentation method outperforms a single step method in which both the vessel and its contents are identified using a single net. An advantage of the modular neural net is that it allows networks to be built from existing trained modules, as well the transfer and reuse of trained net modules without the need for any retraining of the assembled net.



### GHCLNet: A Generalized Hierarchically tuned Contact Lens detection Network
- **Arxiv ID**: http://arxiv.org/abs/1710.05152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05152v1)
- **Published**: 2017-10-14 08:27:13+00:00
- **Updated**: 2017-10-14 08:27:13+00:00
- **Authors**: Avantika Singh, Vishesh Mistry, Dhananjay Yadav, Aditya Nigam
- **Comment**: Accepted in ISBA 2018: International Conference on Identity, Security
  and Behavior Analysis
- **Journal**: None
- **Summary**: Iris serves as one of the best biometric modality owing to its complex, unique and stable structure. However, it can still be spoofed using fabricated eyeballs and contact lens. Accurate identification of contact lens is must for reliable performance of any biometric authentication system based on this modality. In this paper, we present a novel approach for detecting contact lens using a Generalized Hierarchically tuned Contact Lens detection Network (GHCLNet) . We have proposed hierarchical architecture for three class oculus classification namely: no lens, soft lens and cosmetic lens. Our network architecture is inspired by ResNet-50 model. This network works on raw input iris images without any pre-processing and segmentation requirement and this is one of its prodigious strength. We have performed extensive experimentation on two publicly available data-sets namely: 1)IIIT-D 2)ND and on IIT-K data-set (not publicly available) to ensure the generalizability of our network. The proposed architecture results are quite promising and outperforms the available state-of-the-art lens detection algorithms.



### BrainSegNet : A Segmentation Network for Human Brain Fiber Tractography Data into Anatomically Meaningful Clusters
- **Arxiv ID**: http://arxiv.org/abs/1710.05158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05158v1)
- **Published**: 2017-10-14 09:43:45+00:00
- **Updated**: 2017-10-14 09:43:45+00:00
- **Authors**: Tushar Gupta, Shreyas Malakarjun Patil, Mukkaram Tailor, Daksh Thapar, Aditya Nigam
- **Comment**: Deep Learning in Irregular Domains - British Machine Vision
  Conference (DLID-BMVC)
- **Journal**: None
- **Summary**: The segregation of brain fiber tractography data into distinct and anatomically meaningful clusters can help to comprehend the complex brain structure and early investigation and management of various neural disorders. We propose a novel stacked bidirectional long short-term memory(LSTM) based segmentation network, (BrainSegNet) for human brain fiber tractography data classification. We perform a two-level hierarchical classification a) White vs Grey matter (Macro) and b) White matter clusters (Micro). BrainSegNet is trained over three brain tractography data having over 250,000 fibers each. Our experimental evaluation shows that our model achieves state-of-the-art results. We have performed inter as well as intra class testing over three patient's brain tractography data and achieved a high classification accuracy for both macro and micro levels both under intra as well as inter brain testing scenario.



### Co-saliency Detection for RGBD Images Based on Multi-constraint Feature Matching and Cross Label Propagation
- **Arxiv ID**: http://arxiv.org/abs/1710.05172v1
- **DOI**: 10.1109/TIP.2017.2763819
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05172v1)
- **Published**: 2017-10-14 12:28:35+00:00
- **Updated**: 2017-10-14 12:28:35+00:00
- **Authors**: Runmin Cong, Jianjun Lei, Huazhu Fu, Qingming Huang, Xiaochun Cao, Chunping Hou
- **Comment**: 11 pages, 8 figures, Accepted by IEEE Transactions on Image
  Processing, Project URL: https://rmcong.github.io/proj_RGBD_cosal.html
- **Journal**: None
- **Summary**: Co-saliency detection aims at extracting the common salient regions from an image group containing two or more relevant images. It is a newly emerging topic in computer vision community. Different from the most existing co-saliency methods focusing on RGB images, this paper proposes a novel co-saliency detection model for RGBD images, which utilizes the depth information to enhance identification of co-saliency. First, the intra saliency map for each image is generated by the single image saliency model, while the inter saliency map is calculated based on the multi-constraint feature matching, which represents the constraint relationship among multiple images. Then, the optimization scheme, namely Cross Label Propagation (CLP), is used to refine the intra and inter saliency maps in a cross way. Finally, all the original and optimized saliency maps are integrated to generate the final co-saliency result. The proposed method introduces the depth information and multi-constraint feature matching to improve the performance of co-saliency detection. Moreover, the proposed method can effectively exploit any existing single image saliency model to work well in co-saliency scenarios. Experiments on two RGBD co-saliency datasets demonstrate the effectiveness of our proposed model.



### Saliency Detection for Stereoscopic Images Based on Depth Confidence Analysis and Multiple Cues Fusion
- **Arxiv ID**: http://arxiv.org/abs/1710.05174v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05174v1)
- **Published**: 2017-10-14 12:34:10+00:00
- **Updated**: 2017-10-14 12:34:10+00:00
- **Authors**: Runmin Cong, Jianjun Lei, Changqing Zhang, Qingming Huang, Xiaochun Cao, Chunping Hou
- **Comment**: 5 pages, 6 figures, Published on IEEE Signal Processing Letters 2016,
  Project URL: https://rmcong.github.io/proj_RGBD_sal.html
- **Journal**: None
- **Summary**: Stereoscopic perception is an important part of human visual system that allows the brain to perceive depth. However, depth information has not been well explored in existing saliency detection models. In this letter, a novel saliency detection method for stereoscopic images is proposed. Firstly, we propose a measure to evaluate the reliability of depth map, and use it to reduce the influence of poor depth map on saliency detection. Then, the input image is represented as a graph, and the depth information is introduced into graph construction. After that, a new definition of compactness using color and depth cues is put forward to compute the compactness saliency map. In order to compensate the detection errors of compactness saliency when the salient regions have similar appearances with background, foreground saliency map is calculated based on depth-refined foreground seeds selection mechanism and multiple cues contrast. Finally, these two saliency maps are integrated into a final saliency map through weighted-sum method according to their importance. Experiments on two publicly available stereo datasets demonstrate that the proposed method performs better than other 10 state-of-the-art approaches.



### Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization
- **Arxiv ID**: http://arxiv.org/abs/1710.05179v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.05179v2)
- **Published**: 2017-10-14 13:10:59+00:00
- **Updated**: 2017-11-09 13:50:43+00:00
- **Authors**: Hyeonwoo Noh, Tackgeun You, Jonghwan Mun, Bohyung Han
- **Comment**: NIPS 2017 camera ready
- **Journal**: None
- **Summary**: Overfitting is one of the most critical challenges in deep neural networks, and there are various types of regularization methods to improve generalization performance. Injecting noises to hidden units during training, e.g., dropout, is known as a successful regularizer, but it is still not clear enough why such training techniques work well in practice and how we can maximize their benefit in the presence of two conflicting objectives---optimizing to true data distribution and preventing overfitting by regularization. This paper addresses the above issues by 1) interpreting that the conventional training methods with regularization by noise injection optimize the lower bound of the true objective and 2) proposing a technique to achieve a tighter lower bound using multiple noise samples per training example in a stochastic gradient descent iteration. We demonstrate the effectiveness of our idea in several computer vision applications.



### Microaneurysm Detection in Fundus Images Using a Two-step Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.05191v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05191v2)
- **Published**: 2017-10-14 14:15:56+00:00
- **Updated**: 2018-07-08 12:01:27+00:00
- **Authors**: Noushin Eftekheri, Mojtaba Masoudi, Hamidreza Pourreza, Kamaledin Ghiasi Shirazi, Ehsan Saeedi
- **Comment**: None
- **Journal**: None
- **Summary**: Diabetic Retinopathy (DR) is a prominent cause of blindness in the world. The early treatment of DR can be conducted from detection of microaneurysms (MAs) which appears as reddish spots in retinal images. An automated microaneurysm detection can be a helpful system for ophthalmologists. In this paper, deep learning, in particular convolutional neural network (CNN), is used as a powerful tool to efficiently detect MAs from fundus images. In our method a new technique is used to utilise a two-stage training process which results in an accurate detection, while decreasing computational complexity in comparison with previous works. To validate our proposed method, an experiment is conducted using Keras library to implement our proposed CNN on two standard publicly available datasets. Our results show a promising sensitivity value of about 0.8 at the average number of false positive per image greater than 6 which is a competitive value with the state-of-the-art approaches.



### K-means clustering for efficient and robust registration of multi-view point sets
- **Arxiv ID**: http://arxiv.org/abs/1710.05193v4
- **DOI**: 10.1016/j.ins.2019.03.024
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05193v4)
- **Published**: 2017-10-14 14:29:14+00:00
- **Updated**: 2018-04-30 08:42:34+00:00
- **Authors**: Zutao Jiang, Jihua Zhu, Georgios D. Evangelidis, Changqing Zhang, Shanmin Pang, Yaochen Li
- **Comment**: None
- **Journal**: None
- **Summary**: Generally, there are three main factors that determine the practical usability of registration, i.e., accuracy, robustness, and efficiency. In real-time applications, efficiency and robustness are more important. To promote these two abilities, we cast the multi-view registration into a clustering task. All the centroids are uniformly sampled from the initially aligned point sets involved in the multi-view registration, which makes it rather efficient and effective for the clustering. Then, each point is assigned to a single cluster and each cluster centroid is updated accordingly. Subsequently, the shape comprised by all cluster centroids is used to sequentially estimate the rigid transformation for each point set. For accuracy and stability, clustering and transformation estimation are alternately and iteratively applied to all point sets. We tested our proposed approach on several benchmark datasets and compared it with state-of-the-art approaches. Experimental results validate its efficiency and robustness for the registration of multi-view point sets.



### An Adaptive Framework for Missing Depth Inference Using Joint Bilateral Filter
- **Arxiv ID**: http://arxiv.org/abs/1710.05221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05221v1)
- **Published**: 2017-10-14 18:45:04+00:00
- **Updated**: 2017-10-14 18:45:04+00:00
- **Authors**: Rajer Sindhu, Jayesh Ananya
- **Comment**: None
- **Journal**: None
- **Summary**: Depth imaging has largely focused on sensor and intrinsics properties. However, the accuracy of acquire pixel is largely dependent on the capture. We propose a new depth estimation and approximation algorithm which takes an arbitrary 3D point cloud as input, with potentially complex geometric structures, and generates automatically a bounding box which is used to clamp the 3D distribution into a valid range. We then infer the desired compact geometric network from complex 3D geometries by using a series of adaptive joint bilateral filters. Our approach leverages these input depth in the construction of a compact descriptive adaptive filter framework. The built system that allows a user to control the result of capture depth map to fit the target geometry. In addition, it is desirable to visualize structurally problematic areas of the depth data in a dynamic environment. To provide this feature, we investigate a fast update algorithm for the fragility of each pixel's corresponding 3D point using machine learning. We present a new for of feature vector analysis and demonstrate the effectiveness in the dataset. In our experiment, we demonstrate the practicality and benefits of our proposed method by computing accurate solutions captured depth map from different types of sensors and shows better results than existing methods.



