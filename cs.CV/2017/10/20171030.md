# Arxiv Papers in cs.CV on 2017-10-30
### Can you find a face in a HEVC bitstream?
- **Arxiv ID**: http://arxiv.org/abs/1710.10736v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.10736v2)
- **Published**: 2017-10-30 01:48:38+00:00
- **Updated**: 2018-02-23 22:27:51+00:00
- **Authors**: Saeed Ranjbar Alvar, Hyomin Choi, Ivan V. Bajic
- **Comment**: None
- **Journal**: None
- **Summary**: Finding faces in images is one of the most important tasks in computer vision, with applications in biometrics, surveillance, human-computer interaction, and other areas. In our earlier work, we demonstrated that it is possible to tell whether or not an image contains a face by only examining the HEVC syntax, without fully reconstructing the image. In the present work we move further in this direction by showing how to localize faces in HEVC-coded images, without full reconstruction. We also demonstrate the benefits that such approach can have in privacy-friendly face localization.



### Evolving Deep Convolutional Neural Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1710.10741v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.10741v3)
- **Published**: 2017-10-30 02:04:07+00:00
- **Updated**: 2019-03-10 23:23:51+00:00
- **Authors**: Yanan Sun, Bing Xue, Mengjie Zhang, Gary G. Yen
- **Comment**: None
- **Journal**: None
- **Summary**: Evolutionary computation methods have been successfully applied to neural networks since two decades ago, while those methods cannot scale well to the modern deep neural networks due to the complicated architectures and large quantities of connection weights. In this paper, we propose a new method using genetic algorithms for evolving the architectures and connection weight initialization values of a deep convolutional neural network to address image classification problems. In the proposed algorithm, an efficient variable-length gene encoding strategy is designed to represent the different building blocks and the unpredictable optimal depth in convolutional neural networks. In addition, a new representation scheme is developed for effectively initializing connection weights of deep convolutional neural networks, which is expected to avoid networks getting stuck into local minima which is typically a major issue in the backward gradient-based optimization. Furthermore, a novel fitness evaluation method is proposed to speed up the heuristic search with substantially less computational resource. The proposed algorithm is examined and compared with 22 existing algorithms on nine widely used image classification tasks, including the state-of-the-art methods. The experimental results demonstrate the remarkable superiority of the proposed algorithm over the state-of-the-art algorithms in terms of classification error rate and the number of parameters (weights).



### Cascade Region Proposal and Global Context for Deep Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1710.10749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.10749v1)
- **Published**: 2017-10-30 03:00:04+00:00
- **Updated**: 2017-10-30 03:00:04+00:00
- **Authors**: Qiaoyong Zhong, Chao Li, Yingying Zhang, Di Xie, Shicai Yang, Shiliang Pu
- **Comment**: Preprint to appear in Neurocomputing
- **Journal**: None
- **Summary**: Deep region-based object detector consists of a region proposal step and a deep object recognition step. In this paper, we make significant improvements on both of the two steps. For region proposal we propose a novel lightweight cascade structure which can effectively improve RPN proposal quality. For object recognition we re-implement global context modeling with a few modications and obtain a performance boost (4.2% mAP gain on the ILSVRC 2016 validation set). Besides, we apply the idea of pre-training extensively and show its importance in both steps. Together with common training and testing tricks, we improve Faster R-CNN baseline by a large margin. In particular, we obtain 87.9% mAP on the PASCAL VOC 2012 test set, 65.3% on the ILSVRC 2016 test set and 36.8% on the COCO test-std set.



### Predicting Head Movement in Panoramic Video: A Deep Reinforcement Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1710.10755v5
- **DOI**: 10.1109/TPAMI.2018.2858783
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1710.10755v5)
- **Published**: 2017-10-30 03:32:22+00:00
- **Updated**: 2019-11-28 09:33:09+00:00
- **Authors**: Yuhang Song, Mai Xu, Jianyi Wang, Minglang Qiao, Liangyu Huo, Zulin Wang
- **Comment**: 15 pages, 10 figures, published on TPAMI 2018
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence.
  2018 Jul 24
- **Summary**: Panoramic video provides immersive and interactive experience by enabling humans to control the field of view (FoV) through head movement (HM). Thus, HM plays a key role in modeling human attention on panoramic video. This paper establishes a database collecting subjects' HM in panoramic video sequences. From this database, we find that the HM data are highly consistent across subjects. Furthermore, we find that deep reinforcement learning (DRL) can be applied to predict HM positions, via maximizing the reward of imitating human HM scanpaths through the agent's actions. Based on our findings, we propose a DRL-based HM prediction (DHP) approach with offline and online versions, called offline-DHP and online-DHP. In offline-DHP, multiple DRL workflows are run to determine potential HM positions at each panoramic frame. Then, a heat map of the potential HM positions, named the HM map, is generated as the output of offline-DHP. In online-DHP, the next HM position of one subject is estimated given the currently observed HM position, which is achieved by developing a DRL algorithm upon the learned offline-DHP model. Finally, the experiments validate that our approach is effective in both offline and online prediction of HM positions for panoramic video, and that the learned offline-DHP model can improve the performance of online-DHP.



### Stochastic variance reduced multiplicative update for nonnegative matrix factorization
- **Arxiv ID**: http://arxiv.org/abs/1710.10781v2
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.10781v2)
- **Published**: 2017-10-30 06:14:17+00:00
- **Updated**: 2018-04-03 21:45:46+00:00
- **Authors**: Hiroyuki Kasai
- **Comment**: IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP2018)
- **Journal**: None
- **Summary**: Nonnegative matrix factorization (NMF), a dimensionality reduction and factor analysis method, is a special case in which factor matrices have low-rank nonnegative constraints. Considering the stochastic learning in NMF, we specifically address the multiplicative update (MU) rule, which is the most popular, but which has slow convergence property. This present paper introduces on the stochastic MU rule a variance-reduced technique of stochastic gradient. Numerical comparisons suggest that our proposed algorithms robustly outperform state-of-the-art algorithms across different synthetic and real-world datasets.



### DART: Distribution Aware Retinal Transform for Event-based Cameras
- **Arxiv ID**: http://arxiv.org/abs/1710.10800v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.10800v3)
- **Published**: 2017-10-30 08:08:57+00:00
- **Updated**: 2018-11-14 07:40:55+00:00
- **Authors**: Bharath Ramesh, Hong Yang, Garrick Orchard, Ngoc Anh Le Thi, Shihao Zhang, Cheng Xiang
- **Comment**: 12 pages, revision submitted to TPAMI in Nov 2018
- **Journal**: None
- **Summary**: We introduce a generic visual descriptor, termed as distribution aware retinal transform (DART), that encodes the structural context using log-polar grids for event cameras. The DART descriptor is applied to four different problems, namely object classification, tracking, detection and feature matching: (1) The DART features are directly employed as local descriptors in a bag-of-features classification framework and testing is carried out on four standard event-based object datasets (N-MNIST, MNIST-DVS, CIFAR10-DVS, NCaltech-101). (2) Extending the classification system, tracking is demonstrated using two key novelties: (i) For overcoming the low-sample problem for the one-shot learning of a binary classifier, statistical bootstrapping is leveraged with online learning; (ii) To achieve tracker robustness, the scale and rotation equivariance property of the DART descriptors is exploited for the one-shot learning. (3) To solve the long-term object tracking problem, an object detector is designed using the principle of cluster majority voting. The detection scheme is then combined with the tracker to result in a high intersection-over-union score with augmented ground truth annotations on the publicly available event camera dataset. (4) Finally, the event context encoded by DART greatly simplifies the feature correspondence problem, especially for spatio-temporal slices far apart in time, which has not been explicitly tackled in the event-based vision domain.



### PupilNet v2.0: Convolutional Neural Networks for CPU based real time Robust Pupil Detection
- **Arxiv ID**: http://arxiv.org/abs/1711.00112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00112v1)
- **Published**: 2017-10-30 10:55:00+00:00
- **Updated**: 2017-10-30 10:55:00+00:00
- **Authors**: Wolfgang Fuhl, Thiago Santini, Gjergji Kasneci, Wolfgang Rosenstiel, Enkelejda Kasneci
- **Comment**: Pupil detection, pupil center estimation, image processing, CNN.
  arXiv admin note: substantial text overlap with arXiv:1601.04902
- **Journal**: None
- **Summary**: Real-time, accurate, and robust pupil detection is an essential prerequisite for pervasive video-based eye-tracking. However, automated pupil detection in realworld scenarios has proven to be an intricate challenge due to fast illumination changes, pupil occlusion, non-centered and off-axis eye recording, as well as physiological eye characteristics. In this paper, we approach this challenge through: I) a convolutional neural network (CNN) running in real time on a single core, II) a novel computational intensive two stage CNN for accuracy improvement, and III) a fast propability distribution based refinement method as a practical alternative to II. We evaluate the proposed approaches against the state-of-the-art pupil detection algorithms, improving the detection rate up to ~9% percent points on average over all data sets (~7% on one CPU core 7ms). This evaluation was performed on over 135,000 images: 94,000 images from the literature, and 41,000 new hand-labeled and challenging images contributed by this work (v1.0).



### Open Set Logo Detection and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1710.10891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.10891v1)
- **Published**: 2017-10-30 12:01:32+00:00
- **Updated**: 2017-10-30 12:01:32+00:00
- **Authors**: Andras Tüzkö, Christian Herrmann, Daniel Manger, Jürgen Beyerer
- **Comment**: accepted at VISAPP 2018
- **Journal**: None
- **Summary**: Current logo retrieval research focuses on closed set scenarios. We argue that the logo domain is too large for this strategy and requires an open set approach. To foster research in this direction, a large-scale logo dataset, called Logos in the Wild, is collected and released to the public. A typical open set logo retrieval application is, for example, assessing the effectiveness of advertisement in sports event broadcasts. Given a query sample in shape of a logo image, the task is to find all further occurrences of this logo in a set of images or videos. Currently, common logo retrieval approaches are unsuitable for this task because of their closed world assumption. Thus, an open set logo retrieval method is proposed in this work which allows searching for previously unseen logos by a single query sample. A two stage concept with separate logo detection and comparison is proposed where both modules are based on task specific CNNs. If trained with the Logos in the Wild data, significant performance improvements are observed, especially compared with state-of-the-art closed set approaches.



### Learning to solve inverse problems using Wasserstein loss
- **Arxiv ID**: http://arxiv.org/abs/1710.10898v1
- **DOI**: None
- **Categories**: **cs.CV**, math.FA, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1710.10898v1)
- **Published**: 2017-10-30 12:30:16+00:00
- **Updated**: 2017-10-30 12:30:16+00:00
- **Authors**: Jonas Adler, Axel Ringh, Ozan Öktem, Johan Karlsson
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: We propose using the Wasserstein loss for training in inverse problems. In particular, we consider a learned primal-dual reconstruction scheme for ill-posed inverse problems using the Wasserstein distance as loss function in the learning. This is motivated by miss-alignments in training data, which when using standard mean squared error loss could severely degrade reconstruction quality. We prove that training with the Wasserstein loss gives a reconstruction operator that correctly compensates for miss-alignments in certain cases, whereas training with the mean squared error gives a smeared reconstruction. Moreover, we demonstrate these effects by training a reconstruction algorithm using both mean squared error and optimal transport loss for a problem in computerized tomography.



### Optimization Landscape and Expressivity of Deep CNNs
- **Arxiv ID**: http://arxiv.org/abs/1710.10928v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.10928v2)
- **Published**: 2017-10-30 13:24:28+00:00
- **Updated**: 2018-06-06 12:49:58+00:00
- **Authors**: Quynh Nguyen, Matthias Hein
- **Comment**: Accepted at ICML 2018
- **Journal**: None
- **Summary**: We analyze the loss landscape and expressiveness of practical deep convolutional neural networks (CNNs) with shared weights and max pooling layers. We show that such CNNs produce linearly independent features at a "wide" layer which has more neurons than the number of training samples. This condition holds e.g. for the VGG network. Furthermore, we provide for such wide CNNs necessary and sufficient conditions for global minima with zero training error. For the case where the wide layer is followed by a fully connected layer we show that almost every critical point of the empirical loss is a global minimum with zero training error. Our analysis suggests that both depth and width are very important in deep learning. While depth brings more representational power and allows the network to learn high level features, width smoothes the optimization landscape of the loss function in the sense that a sufficiently wide network has a well-behaved loss surface with almost no bad local minima.



### Denoising random forests
- **Arxiv ID**: http://arxiv.org/abs/1710.11004v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.11004v1)
- **Published**: 2017-10-30 15:16:51+00:00
- **Updated**: 2017-10-30 15:16:51+00:00
- **Authors**: Masaya Hibino, Akisato Kimura, Takayoshi Yamashita, Yuji Yamauchi, Hironobu Fujiyoshi
- **Comment**: 20 pages, 10 figures, submitted to Pattern Recognition
- **Journal**: None
- **Summary**: This paper proposes a novel type of random forests called a denoising random forests that are robust against noises contained in test samples. Such noise-corrupted samples cause serious damage to the estimation performances of random forests, since unexpected child nodes are often selected and the leaf nodes that the input sample reaches are sometimes far from those for a clean sample. Our main idea for tackling this problem originates from a binary indicator vector that encodes a traversal path of a sample in the forest. Our proposed method effectively employs this vector by introducing denoising autoencoders into random forests. A denoising autoencoder can be trained with indicator vectors produced from clean and noisy input samples, and non-leaf nodes where incorrect decisions are made can be identified by comparing the input and output of the trained denoising autoencoder. Multiple traversal paths with respect to the nodes with incorrect decisions caused by the noises can then be considered for the estimation.



### A Connection between Feed-Forward Neural Networks and Probabilistic Graphical Models
- **Arxiv ID**: http://arxiv.org/abs/1710.11052v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1710.11052v1)
- **Published**: 2017-10-30 16:31:24+00:00
- **Updated**: 2017-10-30 16:31:24+00:00
- **Authors**: Dmitrij Schlesinger
- **Comment**: None
- **Journal**: None
- **Summary**: Two of the most popular modelling paradigms in computer vision are feed-forward neural networks (FFNs) and probabilistic graphical models (GMs). Various connections between the two have been studied in recent works, such as e.g. expressing mean-field based inference in a GM as an FFN. This paper establishes a new connection between FFNs and GMs. Our key observation is that any FFN implements a certain approximation of a corresponding Bayesian network (BN). We characterize various benefits of having this connection. In particular, it results in a new learning algorithm for BNs. We validate the proposed methods for a classification problem on CIFAR-10 dataset and for binary image segmentation on Weizmann Horse dataset. We show that statistically learned BNs improve performance, having at the same time essentially better generalization capability, than their FFN counterparts.



### Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.11063v3
- **DOI**: 10.1109/WACV.2018.00097
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.11063v3)
- **Published**: 2017-10-30 16:55:43+00:00
- **Updated**: 2018-11-09 19:21:05+00:00
- **Authors**: Aditya Chattopadhyay, Anirban Sarkar, Prantik Howlader, Vineeth N Balasubramanian
- **Comment**: 17 Pages, 15 Figures, 11 Tables. Accepted in the proceedings of IEEE
  Winter Conf. on Applications of Computer Vision (WACV2018). Extended version
  is under review at IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision problems. However, these deep models are perceived as "black box" methods considering the lack of understanding of their internal functioning. There has been a significant recent interest in developing explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose a generalized method called Grad-CAM++ that can provide better visual explanations of CNN model predictions, in terms of better object localization as well as explaining occurrences of multiple object instances in a single image, when compared to state-of-the-art. We provide a mathematical derivation for the proposed method, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the corresponding class label. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ provides promising human-interpretable visual explanations for a given CNN architecture across multiple tasks including classification, image caption generation and 3D action recognition; as well as in new settings such as knowledge distillation.



### Continuous Authentication Using One-class Classifiers and their Fusion
- **Arxiv ID**: http://arxiv.org/abs/1710.11075v1
- **DOI**: None
- **Categories**: **cs.CV**, K.6.5
- **Links**: [PDF](http://arxiv.org/pdf/1710.11075v1)
- **Published**: 2017-10-30 17:16:21+00:00
- **Updated**: 2017-10-30 17:16:21+00:00
- **Authors**: Rajesh Kumar, Partha Pratim Kundu, Vir V. Phoha
- **Comment**: 2018 IEEE 4th International Conference on Identity, Security, and
  Behavior Analysis (ISBA) 978-1-5386-2248-3/18/$31.00 (c) 2018 IEEE
- **Journal**: None
- **Summary**: While developing continuous authentication systems (CAS), we generally assume that samples from both genuine and impostor classes are readily available. However, the assumption may not be true in certain circumstances. Therefore, we explore the possibility of implementing CAS using only genuine samples. Specifically, we investigate the usefulness of four one-class classifiers OCC (elliptic envelope, isolation forest, local outliers factor, and one-class support vector machines) and their fusion. The performance of these classifiers was evaluated on four distinct behavioral biometric datasets, and compared with eight multi-class classifiers (MCC). The results demonstrate that if we have sufficient training data from the genuine user the OCC, and their fusion can closely match the performance of the majority of MCC. Our findings encourage the research community to use OCC in order to build CAS as they do not require knowledge of impostor class during the enrollment process.



### An Integrated Approach to Crowd Video Analysis: From Tracking to Multi-level Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1710.11087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.11087v1)
- **Published**: 2017-10-30 17:34:00+00:00
- **Updated**: 2017-10-30 17:34:00+00:00
- **Authors**: Neha Bhargava, Subhasis Chaudhuri
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We present an integrated framework for simultaneous tracking, group detection and multi-level activity recognition in crowd videos. Instead of solving these problems independently and sequentially, we solve them together in a unified framework to utilize the strong correlation that exists among individual motion, groups, and activities. We explore the hierarchical structure hidden in the video that connects individuals over time to produce tracks, connects individuals to form groups and also connects groups together to form a crowd. We show that estimation of this hidden structure corresponds to track association and group detection. We estimate this hidden structure under a linear programming formulation. The obtained graphical representation is further explored to recognize the node values that corresponds to multi-level activity recognition. This problem is solved under a structured SVM framework. The results on publicly available dataset show very competitive performance at all levels of granularity with the state-of-the-art batch processing methods despite the proposed technique being an online (causal) one.



### High efficiency compression for object detection
- **Arxiv ID**: http://arxiv.org/abs/1710.11151v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.11151v2)
- **Published**: 2017-10-30 18:03:22+00:00
- **Updated**: 2018-02-16 01:50:22+00:00
- **Authors**: Hyomin Choi, Ivan V. Bajic
- **Comment**: The paper is published in IEEE ICASSP 18'
- **Journal**: None
- **Summary**: Image and video compression has traditionally been tailored to human vision. However, modern applications such as visual analytics and surveillance rely on computers seeing and analyzing the images before (or instead of) humans. For these applications, it is important to adjust compression to computer vision. In this paper we present a bit allocation and rate control strategy that is tailored to object detection. Using the initial convolutional layers of a state-of-the-art object detector, we create an importance map that can guide bit allocation to areas that are important for object detection. The proposed method enables bit rate savings of 7% or more compared to default HEVC, at the equivalent object detection rate.



### CrescendoNet: A Simple Deep Convolutional Neural Network with Ensemble Behavior
- **Arxiv ID**: http://arxiv.org/abs/1710.11176v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.11176v2)
- **Published**: 2017-10-30 18:35:01+00:00
- **Updated**: 2018-01-04 17:01:21+00:00
- **Authors**: Xiang Zhang, Nishant Vishwamitra, Hongxin Hu, Feng Luo
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new deep convolutional neural network, CrescendoNet, by stacking simple building blocks without residual connections. Each Crescendo block contains independent convolution paths with increased depths. The numbers of convolution layers and parameters are only increased linearly in Crescendo blocks. In experiments, CrescendoNet with only 15 layers outperforms almost all networks without residual connections on benchmark datasets, CIFAR10, CIFAR100, and SVHN. Given sufficient amount of data as in SVHN dataset, CrescendoNet with 15 layers and 4.1M parameters can match the performance of DenseNet-BC with 250 layers and 15.3M parameters. CrescendoNet provides a new way to construct high performance deep convolutional neural networks without residual connections. Moreover, through investigating the behavior and performance of subnetworks in CrescendoNet, we note that the high performance of CrescendoNet may come from its implicit ensemble behavior, which differs from the FractalNet that is also a deep convolutional neural network without residual connections. Furthermore, the independence between paths in CrescendoNet allows us to introduce a new path-wise training procedure, which can reduce the memory needed for training.



### Deep word embeddings for visual speech recognition
- **Arxiv ID**: http://arxiv.org/abs/1710.11201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.11201v1)
- **Published**: 2017-10-30 19:09:29+00:00
- **Updated**: 2017-10-30 19:09:29+00:00
- **Authors**: Themos Stafylakis, Georgios Tzimiropoulos
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a deep learning architecture for extracting word embeddings for visual speech recognition. The embeddings summarize the information of the mouth region that is relevant to the problem of word recognition, while suppressing other types of variability such as speaker, pose and illumination. The system is comprised of a spatiotemporal convolutional layer, a Residual Network and bidirectional LSTMs and is trained on the Lipreading in-the-wild database. We first show that the proposed architecture goes beyond state-of-the-art on closed-set word identification, by attaining 11.92% error rate on a vocabulary of 500 words. We then examine the capacity of the embeddings in modelling words unseen during training. We deploy Probabilistic Linear Discriminant Analysis (PLDA) to model the embeddings and perform low-shot learning experiments on words unseen during training. The experiments demonstrate that word-level visual speech recognition is feasible even in cases where the target words are not included in the training set.



### Deep Learning and Conditional Random Fields-based Depth Estimation and Topographical Reconstruction from Conventional Endoscopy
- **Arxiv ID**: http://arxiv.org/abs/1710.11216v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.11216v3)
- **Published**: 2017-10-30 19:56:52+00:00
- **Updated**: 2017-11-27 20:42:13+00:00
- **Authors**: Faisal Mahmood, Nicholas J. Durr
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: Colorectal cancer is the fourth leading cause of cancer deaths worldwide and the second leading cause in the United States. The risk of colorectal cancer can be mitigated by the identification and removal of premalignant lesions through optical colonoscopy. Unfortunately, conventional colonoscopy misses more than 20% of the polyps that should be removed, due in part to poor contrast of lesion topography. Imaging tissue topography during a colonoscopy is difficult because of the size constraints of the endoscope and the deforming mucosa. Most existing methods make geometric assumptions or incorporate a priori information, which limits accuracy and sensitivity. In this paper, we present a method that avoids these restrictions, using a joint deep convolutional neural network-conditional random field (CNN-CRF) framework. Estimated depth is used to reconstruct the topography of the surface of the colon from a single image. We train the unary and pairwise potential functions of a CRF in a CNN on synthetic data, generated by developing an endoscope camera model and rendering over 100,000 images of an anatomically-realistic colon. We validate our approach with real endoscopy images from a porcine colon, transferred to a synthetic-like domain, with ground truth from registered computed tomography measurements. The CNN-CRF approach estimates depths with a relative error of 0.152 for synthetic endoscopy images and 0.242 for real endoscopy images. We show that the estimated depth maps can be used for reconstructing the topography of the mucosa from conventional colonoscopy images. This approach can easily be integrated into existing endoscopy systems and provides a foundation for improving computer-aided detection algorithms for detection, segmentation and classification of lesions.



### Stochastic Variational Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1710.11252v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1710.11252v2)
- **Published**: 2017-10-30 21:48:54+00:00
- **Updated**: 2018-03-06 16:35:06+00:00
- **Authors**: Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H. Campbell, Sergey Levine
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging. Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images requires the predictive model to build an intricate understanding of the natural world. Many existing methods tackle this problem by making simplifying assumptions about the environment. One common assumption is that the outcome is deterministic and there is only one plausible future. This can lead to low-quality predictions in real-world settings with stochastic dynamics. In this paper, we develop a stochastic variational video prediction (SV2P) method that predicts a different possible future for each sample of its latent variables. To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world video. We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned. We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods. Our SV2P implementation will be open sourced upon publication.



### Log-DenseNet: How to Sparsify a DenseNet
- **Arxiv ID**: http://arxiv.org/abs/1711.00002v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.00002v1)
- **Published**: 2017-10-30 22:01:08+00:00
- **Updated**: 2017-10-30 22:01:08+00:00
- **Authors**: Hanzhang Hu, Debadeepta Dey, Allison Del Giorno, Martial Hebert, J. Andrew Bagnell
- **Comment**: None
- **Journal**: None
- **Summary**: Skip connections are increasingly utilized by deep neural networks to improve accuracy and cost-efficiency. In particular, the recent DenseNet is efficient in computation and parameters, and achieves state-of-the-art predictions by directly connecting each feature layer to all previous ones. However, DenseNet's extreme connectivity pattern may hinder its scalability to high depths, and in applications like fully convolutional networks, full DenseNet connections are prohibitively expensive. This work first experimentally shows that one key advantage of skip connections is to have short distances among feature layers during backpropagation. Specifically, using a fixed number of skip connections, the connection patterns with shorter backpropagation distance among layers have more accurate predictions. Following this insight, we propose a connection template, Log-DenseNet, which, in comparison to DenseNet, only slightly increases the backpropagation distances among layers from 1 to ($1 + \log_2 L$), but uses only $L\log_2 L$ total connections instead of $O(L^2)$. Hence, Log-DenseNets are easier than DenseNets to implement and to scale. We demonstrate the effectiveness of our design principle by showing better performance than DenseNets on tabula rasa semantic segmentation, and competitive results on visual recognition.



