# Arxiv Papers in cs.CV on 2017-10-18
### Fast PET Scan Tumor Segmentation using Superpixels, Principal Component Analysis and K-means Clustering
- **Arxiv ID**: http://arxiv.org/abs/1710.08798v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, physics.data-an, q-bio.QM, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1710.08798v1)
- **Published**: 2017-10-18 00:19:57+00:00
- **Updated**: 2017-10-18 00:19:57+00:00
- **Authors**: Yeman B. Hagos, Vu H. Minh, Saed Khawaldeh, Usama Pervaiz, Tajwar A. Aleef
- **Comment**: 7 pages, 8 figures
- **Journal**: None
- **Summary**: Positron Emission Tomography scan images are extensively used in radiotherapy planning, clinical diagnosis, assessment of growth and treatment of a tumor. These all rely on fidelity and speed of detection and delineation algorithm. Despite intensive research, segmentation remained a challenging problem due to the diverse image content, resolution, shape, and noise. This paper presents a fast positron emission tomography tumor segmentation method in which superpixels are extracted first from the input image. Principal component analysis is then applied on the superpixels and also on their average. Distance vector of each superpixel from the average is computed in principal components coordinate system. Finally, k-means clustering is applied on distance vector to recognize tumor and non-tumor superpixels. The proposed approach is implemented in MATLAB 2016 which resulted in an average Dice similarity of 84.2% on the dataset. Additionally, a very fast execution time was achieved as the number of superpixels and the size of distance vector on which clustering was done was very small compared to the number of raw pixels in dataset images.



### Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1710.06555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06555v1)
- **Published**: 2017-10-18 01:48:02+00:00
- **Updated**: 2017-10-18 01:48:02+00:00
- **Authors**: Dangwei Li, Xiaotang Chen, Zhang Zhang, Kaiqi Huang
- **Comment**: Accepted by CVPR 2017
- **Journal**: None
- **Summary**: Person Re-identification (ReID) is to identify the same person across different cameras. It is a challenging task due to the large variations in person pose, occlusion, background clutter, etc How to extract powerful features is a fundamental problem in ReID and is still an open problem today. In this paper, we design a Multi-Scale Context-Aware Network (MSCAN) to learn powerful features over full body and body parts, which can well capture the local context knowledge by stacking multi-scale convolutions in each layer. Moreover, instead of using predefined rigid parts, we propose to learn and localize deformable pedestrian parts using Spatial Transformer Networks (STN) with novel spatial constraints. The learned body parts can release some difficulties, eg pose variations and background clutters, in part-based representation. Finally, we integrate the representation learning processes of full body and body parts into a unified framework for person ReID through multi-class person identification tasks. Extensive evaluations on current challenging large-scale person ReID datasets, including the image-based Market1501, CUHK03 and sequence-based MARS datasets, show that the proposed method achieves the state-of-the-art results.



### Cell Segmentation in 3D Confocal Images using Supervoxel Merge-Forests with CNN-based Hypothesis Selection
- **Arxiv ID**: http://arxiv.org/abs/1710.06608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06608v1)
- **Published**: 2017-10-18 07:53:27+00:00
- **Updated**: 2017-10-18 07:53:27+00:00
- **Authors**: Johannes Stegmaier, Thiago V. Spina, Alexandre X. Falcão, Andreas Bartschat, Ralf Mikut, Elliot Meyerowitz, Alexandre Cunha
- **Comment**: 5 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: Automated segmentation approaches are crucial to quantitatively analyze large-scale 3D microscopy images. Particularly in deep tissue regions, automatic methods still fail to provide error-free segmentations. To improve the segmentation quality throughout imaged samples, we present a new supervoxel-based 3D segmentation approach that outperforms current methods and reduces the manual correction effort. The algorithm consists of gentle preprocessing and a conservative super-voxel generation method followed by supervoxel agglomeration based on local signal properties and a postprocessing step to fix under-segmentation errors using a Convolutional Neural Network. We validate the functionality of the algorithm on manually labeled 3D confocal images of the plant Arabidopis thaliana and compare the results to a state-of-the-art meristem segmentation algorithm.



### The Robust Reading Competition Annotation and Evaluation Platform
- **Arxiv ID**: http://arxiv.org/abs/1710.06617v2
- **DOI**: 10.1109/DAS.2018.22
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06617v2)
- **Published**: 2017-10-18 08:27:31+00:00
- **Updated**: 2018-05-21 17:10:01+00:00
- **Authors**: Dimosthenis Karatzas, Lluis Gómez, Anguelos Nicolaou, Marçal Rusiñol
- **Comment**: 6 pages, accepted to DAS 2018
- **Journal**: Proc. of the 13th IAPR Int. W. on Document Analysis Systems (DAS
  2018), IEEE CPS, pp. 61-66, 2018
- **Summary**: The ICDAR Robust Reading Competition (RRC), initiated in 2003 and re-established in 2011, has become a de-facto evaluation standard for robust reading systems and algorithms. Concurrent with its second incarnation in 2011, a continuous effort started to develop an on-line framework to facilitate the hosting and management of competitions. This paper outlines the Robust Reading Competition Annotation and Evaluation Platform, the backbone of the competitions. The RRC Annotation and Evaluation Platform is a modular framework, fully accessible through on-line interfaces. It comprises a collection of tools and services for managing all processes involved with defining and evaluating a research task, from dataset definition to annotation management, evaluation specification and results analysis. Although the framework has been designed with robust reading research in mind, many of the provided tools are generic by design. All aspects of the RRC Annotation and Evaluation Framework are available for research use.



### Image Restoration by Iterative Denoising and Backward Projections
- **Arxiv ID**: http://arxiv.org/abs/1710.06647v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1710.06647v4)
- **Published**: 2017-10-18 09:39:30+00:00
- **Updated**: 2018-10-10 20:35:55+00:00
- **Authors**: Tom Tirer, Raja Giryes
- **Comment**: To appear in IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Inverse problems appear in many applications, such as image deblurring and inpainting. The common approach to address them is to design a specific algorithm for each problem. The Plug-and-Play (P&P) framework, which has been recently introduced, allows solving general inverse problems by leveraging the impressive capabilities of existing denoising algorithms. While this fresh strategy has found many applications, a burdensome parameter tuning is often required in order to obtain high-quality results. In this work, we propose an alternative method for solving inverse problems using off-the-shelf denoisers, which requires less parameter tuning. First, we transform a typical cost function, composed of fidelity and prior terms, into a closely related, novel optimization problem. Then, we propose an efficient minimization scheme with a plug-and-play property, i.e., the prior term is handled solely by a denoising operation. Finally, we present an automatic tuning mechanism to set the method's parameters. We provide a theoretical analysis of the method, and empirically demonstrate its competitiveness with task-specific techniques and the P&P approach for image inpainting and deblurring.



### Simultaneous Recognition and Pose Estimation of Instruments in Minimally Invasive Surgery
- **Arxiv ID**: http://arxiv.org/abs/1710.06668v1
- **DOI**: 10.1007/978-3-319-66185-8_57
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06668v1)
- **Published**: 2017-10-18 10:37:01+00:00
- **Updated**: 2017-10-18 10:37:01+00:00
- **Authors**: Thomas Kurmann, Pablo Marquez Neila, Xiaofei Du, Pascal Fua, Danail Stoyanov, Sebastian Wolf, Raphael Sznitman
- **Comment**: 8 pages, 2 figures, MICCAI 2017
- **Journal**: None
- **Summary**: Detection of surgical instruments plays a key role in ensuring patient safety in minimally invasive surgery. In this paper, we present a novel method for 2D vision-based recognition and pose estimation of surgical instruments that generalizes to different surgical applications. At its core, we propose a novel scene model in order to simultaneously recognize multiple instruments as well as their parts. We use a Convolutional Neural Network architecture to embody our model and show that the cross-entropy loss is well suited to optimize its parameters which can be trained in an end-to-end fashion. An additional advantage of our approach is that instrument detection at test time is achieved while avoiding the need for scale-dependent sliding window evaluation. This allows our approach to be relatively parameter free at test time and shows good performance for both instrument detection and tracking. We show that our approach surpasses state-of-the-art results on in-vivo retinal microsurgery image data, as well as ex-vivo laparoscopic sequences.



### Dropout Sampling for Robust Object Detection in Open-Set Conditions
- **Arxiv ID**: http://arxiv.org/abs/1710.06677v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06677v2)
- **Published**: 2017-10-18 11:16:53+00:00
- **Updated**: 2018-04-18 06:10:02+00:00
- **Authors**: Dimity Miller, Lachlan Nicholson, Feras Dayoub, Niko Sünderhauf
- **Comment**: to appear in IEEE International Conference on Robotics and Automation
  2018 (ICRA 2018)
- **Journal**: None
- **Summary**: Dropout Variational Inference, or Dropout Sampling, has been recently proposed as an approximation technique for Bayesian Deep Learning and evaluated for image classification and regression tasks. This paper investigates the utility of Dropout Sampling for object detection for the first time. We demonstrate how label uncertainty can be extracted from a state-of-the-art object detection system via Dropout Sampling. We evaluate this approach on a large synthetic dataset of 30,000 images, and a real-world dataset captured by a mobile robot in a versatile campus environment. We show that this uncertainty can be utilized to increase object detection performance under the open-set conditions that are typically encountered in robotic vision. A Dropout Sampling network is shown to achieve a 12.3% increase in recall (for the same precision score as a standard network) and a 15.1% increase in precision (for the same recall score as the standard network).



### Enhancing the Performance of Convolutional Neural Networks on Quality Degraded Datasets
- **Arxiv ID**: http://arxiv.org/abs/1710.06805v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06805v1)
- **Published**: 2017-10-18 15:57:41+00:00
- **Updated**: 2017-10-18 15:57:41+00:00
- **Authors**: Jonghwa Yim, Kyung-Ah Sohn
- **Comment**: The International Conference on Digital Image Computing: Techniques
  and Applications (DICTA), 2017
- **Journal**: None
- **Summary**: Despite the appeal of deep neural networks that largely replace the traditional handmade filters, they still suffer from isolated cases that cannot be properly handled only by the training of convolutional filters. Abnormal factors, including real-world noise, blur, or other quality degradations, ruin the output of a neural network. These unexpected problems can produce critical complications, and it is surprising that there has only been minimal research into the effects of noise in the deep neural network model. Therefore, we present an exhaustive investigation into the effect of noise in image classification and suggest a generalized architecture of a dual-channel model to treat quality degraded input images. We compare the proposed dual-channel model with a simple single model and show it improves the overall performance of neural networks on various types of quality degraded input datasets.



### Identifying Mild Traumatic Brain Injury Patients From MR Images Using Bag of Visual Words
- **Arxiv ID**: http://arxiv.org/abs/1710.06824v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06824v3)
- **Published**: 2017-10-18 16:55:52+00:00
- **Updated**: 2018-02-14 22:16:08+00:00
- **Authors**: Shervin Minaee, Siyun Wang, Yao Wang, Sohae Chung, Xiuyuan Wang, Els Fieremans, Steven Flanagan, Joseph Rath, Yvonne W. Lui
- **Comment**: None
- **Journal**: None
- **Summary**: Mild traumatic brain injury (mTBI) is a growing public health problem with an estimated incidence of one million people annually in US. Neurocognitive tests are used to both assess the patient condition and to monitor the patient progress. This work aims to directly use MR images taken shortly after injury to detect whether a patient suffers from mTBI, by incorporating machine learning and computer vision techniques to learn features suitable discriminating between mTBI and normal patients. We focus on 3 regions in brain, and extract multiple patches from them, and use bag-of-visual-word technique to represent each subject as a histogram of representative patterns derived from patches from all training subjects. After extracting the features, we use greedy forward feature selection, to choose a subset of features which achieves highest accuracy. We show through experimental studies that BoW features perform better than the simple mean value features which were used previously.



### Using Deep Convolutional Networks for Gesture Recognition in American Sign Language
- **Arxiv ID**: http://arxiv.org/abs/1710.06836v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06836v3)
- **Published**: 2017-10-18 17:35:04+00:00
- **Updated**: 2017-11-20 16:49:10+00:00
- **Authors**: Vivek Bheda, Dianna Radpour
- **Comment**: 12 figures
- **Journal**: None
- **Summary**: In the realm of multimodal communication, sign language is, and continues to be, one of the most understudied areas. In line with recent advances in the field of deep learning, there are far reaching implications and applications that neural networks can have for sign language interpretation. In this paper, we present a method for using deep convolutional networks to classify images of both the the letters and digits in American Sign Language.



### VisDA: The Visual Domain Adaptation Challenge
- **Arxiv ID**: http://arxiv.org/abs/1710.06924v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06924v2)
- **Published**: 2017-10-18 20:20:49+00:00
- **Updated**: 2017-11-29 04:04:18+00:00
- **Authors**: Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, Kate Saenko
- **Comment**: None
- **Journal**: None
- **Summary**: We present the 2017 Visual Domain Adaptation (VisDA) dataset and challenge, a large-scale testbed for unsupervised domain adaptation across visual domains. Unsupervised domain adaptation aims to solve the real-world problem of domain shift, where machine learning models trained on one domain must be transferred and adapted to a novel visual domain without additional supervision. The VisDA2017 challenge is focused on the simulation-to-reality shift and has two associated tasks: image classification and image segmentation. The goal in both tracks is to first train a model on simulated, synthetic data in the source domain and then adapt it to perform well on real image data in the unlabeled test domain. Our dataset is the largest one to date for cross-domain object classification, with over 280K images across 12 categories in the combined training, validation and testing domains. The image segmentation dataset is also large-scale with over 30K images across 18 categories in the three domains. We compare VisDA to existing cross-domain adaptation datasets and provide a baseline performance analysis using various domain adaptation models that are currently popular in the field.



### Unsupervised Object Discovery and Segmentation of RGBD-images
- **Arxiv ID**: http://arxiv.org/abs/1710.06929v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.06929v1)
- **Published**: 2017-10-18 20:33:48+00:00
- **Updated**: 2017-10-18 20:33:48+00:00
- **Authors**: Johan Ekekrantz, Nils Bore, Rares Ambrus, John Folkesson, Patric Jensfelt
- **Comment**: 15 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper we introduce a system for unsupervised object discovery and segmentation of RGBD-images. The system models the sensor noise directly from data, allowing accurate segmentation without sensor specific hand tuning of measurement noise models making use of the recently introduced Statistical Inlier Estimation (SIE) method. Through a fully probabilistic formulation, the system is able to apply probabilistic inference, enabling reliable segmentation in previously challenging scenarios. In addition, we introduce new methods for filtering out false positives, significantly improving the signal to noise ratio. We show that the system significantly outperform state-of-the-art in on a challenging real-world dataset.



