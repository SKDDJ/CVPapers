# Arxiv Papers in cs.CV on 2017-10-11
### Convolutional Neural Networks for Histopathology Image Classification: Training vs. Using Pre-Trained Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.05726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05726v1)
- **Published**: 2017-10-11 00:02:53+00:00
- **Updated**: 2017-10-11 00:02:53+00:00
- **Authors**: Brady Kieffer, Morteza Babaie, Shivam Kalra, H. R. Tizhoosh
- **Comment**: To appear in proceedings of the 7th International Conference on Image
  Processing Theory, Tools and Applications (IPTA 2017), Nov 28-Dec 1,
  Montreal, Canada
- **Journal**: None
- **Summary**: We explore the problem of classification within a medical image data-set based on a feature vector extracted from the deepest layer of pre-trained Convolution Neural Networks. We have used feature vectors from several pre-trained structures, including networks with/without transfer learning to evaluate the performance of pre-trained deep features versus CNNs which have been trained by that specific dataset as well as the impact of transfer learning with a small number of samples. All experiments are done on Kimia Path24 dataset which consists of 27,055 histopathology training patches in 24 tissue texture classes along with 1,325 test patches for evaluation. The result shows that pre-trained networks are quite competitive against training from scratch. As well, fine-tuning does not seem to add any tangible improvement for VGG16 to justify additional training while we observed considerable improvement in retrieval and classification accuracy when we fine-tuned the Inception structure.



### Deep Hyperalignment
- **Arxiv ID**: http://arxiv.org/abs/1710.03923v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.03923v1)
- **Published**: 2017-10-11 06:21:45+00:00
- **Updated**: 2017-10-11 06:21:45+00:00
- **Authors**: Muhammad Yousefnezhad, Daoqiang Zhang
- **Comment**: 31st Conference on Neural Information Processing Systems (NIPS 2017),
  Long Beach, CA, USA
- **Journal**: None
- **Summary**: This paper proposes Deep Hyperalignment (DHA) as a regularized, deep extension, scalable Hyperalignment (HA) method, which is well-suited for applying functional alignment to fMRI datasets with nonlinearity, high-dimensionality (broad ROI), and a large number of subjects. Unlink previous methods, DHA is not limited by a restricted fixed kernel function. Further, it uses a parametric approach, rank-$m$ Singular Value Decomposition (SVD), and stochastic gradient descent for optimization. Therefore, DHA has a suitable time complexity for large datasets, and DHA does not require the training data when it computes the functional alignment for a new subject. Experimental studies on multi-subject fMRI analysis confirm that the DHA method achieves superior performance to other state-of-the-art HA algorithms.



### Detect to Track and Track to Detect
- **Arxiv ID**: http://arxiv.org/abs/1710.03958v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.03958v2)
- **Published**: 2017-10-11 08:33:48+00:00
- **Updated**: 2018-03-07 10:49:41+00:00
- **Authors**: Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman
- **Comment**: ICCV 2017. Code and models:
  https://github.com/feichtenhofer/Detect-Track Results:
  https://www.robots.ox.ac.uk/~vgg/research/detect-track/
- **Journal**: None
- **Summary**: Recent approaches for high accuracy detection and tracking of object categories in video consist of complex multistage solutions that become more cumbersome each year. In this paper we propose a ConvNet architecture that jointly performs detection and tracking, solving the task in a simple and effective way. Our contributions are threefold: (i) we set up a ConvNet architecture for simultaneous detection and tracking, using a multi-task objective for frame-based object detection and across-frame track regression; (ii) we introduce correlation features that represent object co-occurrences across time to aid the ConvNet during tracking; and (iii) we link the frame level detections based on our across-frame tracklets to produce high accuracy detections at the video level. Our ConvNet architecture for spatiotemporal object detection is evaluated on the large-scale ImageNet VID dataset where it achieves state-of-the-art results. Our approach provides better single model performance than the winning method of the last ImageNet challenge while being conceptually much simpler. Finally, we show that by increasing the temporal stride we can dramatically increase the tracker speed.



### Deep learning in remote sensing: a review
- **Arxiv ID**: http://arxiv.org/abs/1710.03959v1
- **DOI**: 10.1109/MGRS.2017.2762307
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1710.03959v1)
- **Published**: 2017-10-11 08:35:05+00:00
- **Updated**: 2017-10-11 08:35:05+00:00
- **Authors**: Xiao Xiang Zhu, Devis Tuia, Lichao Mou, Gui-Song Xia, Liangpei Zhang, Feng Xu, Friedrich Fraundorfer
- **Comment**: Accepted for publication IEEE Geoscience and Remote Sensing Magazine
- **Journal**: None
- **Summary**: Standing at the paradigm shift towards data-intensive science, machine learning techniques are becoming increasingly important. In particular, as a major breakthrough in the field, deep learning has proven as an extremely powerful tool in many fields. Shall we embrace deep learning as the key to all? Or, should we resist a 'black-box' solution? There are controversial opinions in the remote sensing community. In this article, we analyze the challenges of using deep learning for remote sensing data analysis, review the recent advances, and provide resources to make deep learning in remote sensing ridiculously simple to start with. More importantly, we advocate remote sensing scientists to bring their expertise into deep learning, and use it as an implicit general model to tackle unprecedented large-scale influential challenges, such as climate change and urbanization.



### Algebraic Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1710.04207v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.04207v1)
- **Published**: 2017-10-11 08:42:35+00:00
- **Updated**: 2017-10-11 08:42:35+00:00
- **Authors**: Enrico Celeghini
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: We propose an approach to image processing related to algebraic operators acting in the space of images. In view of the interest in the applications in optics and computer science, mathematical aspects of the paper have been simplified as much as possible. Underlying theory, related to rigged Hilbert spaces and Lie algebras, is discussed elsewhere



### A Review of Convolutional Neural Networks for Inverse Problems in Imaging
- **Arxiv ID**: http://arxiv.org/abs/1710.04011v1
- **DOI**: 10.1109/MSP.2017.2739299
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.04011v1)
- **Published**: 2017-10-11 11:26:47+00:00
- **Updated**: 2017-10-11 11:26:47+00:00
- **Authors**: Michael T. McCann, Kyong Hwan Jin, Michael Unser
- **Comment**: None
- **Journal**: IEEE Signal Processing Magazine, vol. 34, no. 6, pp. 85-95, Nov.
  2017
- **Summary**: In this survey paper, we review recent uses of convolution neural networks (CNNs) to solve inverse problems in imaging. It has recently become feasible to train deep CNNs on large databases of images, and they have shown outstanding performance on object classification and segmentation tasks. Motivated by these successes, researchers have begun to apply CNNs to the resolution of inverse problems such as denoising, deconvolution, super-resolution, and medical image reconstruction, and they have started to report improvements over state-of-the-art methods, including sparsity-based techniques such as compressed sensing. Here, we review the recent experimental work in these areas, with a focus on the critical design decisions: Where does the training data come from? What is the architecture of the CNN? and How is the learning problem formulated and solved? We also bring together a few key theoretical papers that offer perspective on why CNNs are appropriate for inverse problems and point to some next steps in the field.



### FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1710.04026v2
- **DOI**: 10.1109/TIP.2018.2839891
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04026v2)
- **Published**: 2017-10-11 12:04:37+00:00
- **Updated**: 2018-05-22 15:16:19+00:00
- **Authors**: Kai Zhang, Wangmeng Zuo, Lei Zhang
- **Comment**: IEEE Transactions on Image Processing, code:
  https://github.com/cszn/FFDNet
- **Journal**: None
- **Summary**: Due to the fast inference and good performance, discriminative learning methods have been widely studied in image denoising. However, these methods mostly learn a specific model for each noise level, and require multiple models for denoising images with different noise levels. They also lack flexibility to deal with spatially variant noise, limiting their applications in practical denoising. To address these issues, we present a fast and flexible denoising convolutional neural network, namely FFDNet, with a tunable noise level map as the input. The proposed FFDNet works on downsampled sub-images, achieving a good trade-off between inference speed and denoising performance. In contrast to the existing discriminative denoisers, FFDNet enjoys several desirable properties, including (i) the ability to handle a wide range of noise levels (i.e., [0, 75]) effectively with a single network, (ii) the ability to remove spatially variant noise by specifying a non-uniform noise level map, and (iii) faster speed than benchmark BM3D even on CPU without sacrificing denoising performance. Extensive experiments on synthetic and real noisy images are conducted to evaluate FFDNet in comparison with state-of-the-art denoisers. The results show that FFDNet is effective and efficient, making it highly attractive for practical denoising applications.



### Image retargeting via Beltrami representation
- **Arxiv ID**: http://arxiv.org/abs/1710.04034v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1710.04034v1)
- **Published**: 2017-10-11 12:20:20+00:00
- **Updated**: 2017-10-11 12:20:20+00:00
- **Authors**: Chun Pong Lau, Chun Pang Yung, Lok Ming Lui
- **Comment**: 13pages, 13 figures
- **Journal**: None
- **Summary**: Image retargeting aims to resize an image to one with a prescribed aspect ratio. Simple scaling inevitably introduces unnatural geometric distortions on the important content of the image. In this paper, we propose a simple and yet effective method to resize an image, which preserves the geometry of the important content, using the Beltrami representation. Our algorithm allows users to interactively label content regions as well as line structures. Image resizing can then be achieved by warping the image by an orientation-preserving bijective warping map with controlled distortion. The warping map is represented by its Beltrami representation, which captures the local geometric distortion of the map. By carefully prescribing the values of the Beltrami representation, images with different complexity can be effectively resized. Our method does not require solving any optimization problems and tuning parameters throughout the process. This results in a simple and efficient algorithm to solve the image retargeting problem. Extensive experiments have been carried out, which demonstrate the efficacy of our proposed method.



### Interactive Medical Image Segmentation using Deep Learning with Image-specific Fine-tuning
- **Arxiv ID**: http://arxiv.org/abs/1710.04043v1
- **DOI**: 10.1109/TMI.2018.2791721
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04043v1)
- **Published**: 2017-10-11 12:57:52+00:00
- **Updated**: 2017-10-11 12:57:52+00:00
- **Authors**: Guotai Wang, Wenqi Li, Maria A. Zuluaga, Rosalind Pratt, Premal A. Patel, Michael Aertsen, Tom Doel, Anna L. David, Jan Deprest, Sebastien Ourselin, Tom Vercauteren
- **Comment**: 11 pages, 11 figures
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have achieved state-of-the-art performance for automatic medical image segmentation. However, they have not demonstrated sufficiently accurate and robust results for clinical use. In addition, they are limited by the lack of image-specific adaptation and the lack of generalizability to previously unseen object classes. To address these problems, we propose a novel deep learning-based framework for interactive segmentation by incorporating CNNs into a bounding box and scribble-based segmentation pipeline. We propose image-specific fine-tuning to make a CNN model adaptive to a specific test image, which can be either unsupervised (without additional user interactions) or supervised (with additional scribbles). We also propose a weighted loss function considering network and interaction-based uncertainty for the fine-tuning. We applied this framework to two applications: 2D segmentation of multiple organs from fetal MR slices, where only two types of these organs were annotated for training; and 3D segmentation of brain tumor core (excluding edema) and whole brain tumor (including edema) from different MR sequences, where only tumor cores in one MR sequence were annotated for training. Experimental results show that 1) our model is more robust to segment previously unseen objects than state-of-the-art CNNs; 2) image-specific fine-tuning with the proposed weighted loss function significantly improves segmentation accuracy; and 3) our method leads to accurate results with fewer user interactions and less user time than traditional interactive segmentation methods.



### Local Radon Descriptors for Image Search
- **Arxiv ID**: http://arxiv.org/abs/1710.04097v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04097v1)
- **Published**: 2017-10-11 14:50:01+00:00
- **Updated**: 2017-10-11 14:50:01+00:00
- **Authors**: Morteza Babaie, H. R. Tizhoosh, Amin Khatami, M. E. Shiri
- **Comment**: To appear in proceedings of the 7th International Conference on Image
  Processing Theory, Tools and Applications (IPTA 2017), Nov 28-Dec 1,
  Montreal, Canada
- **Journal**: None
- **Summary**: Radon transform and its inverse operation are important techniques in medical imaging tasks. Recently, there has been renewed interest in Radon transform for applications such as content-based medical image retrieval. However, all studies so far have used Radon transform as a global or quasi-global image descriptor by extracting projections of the whole image or large sub-images. This paper attempts to show that the dense sampling to generate the histogram of local Radon projections has a much higher discrimination capability than the global one. In this paper, we introduce Local Radon Descriptor (LRD) and apply it to the IRMA dataset, which contains 14,410 x-ray images as well as to the INRIA Holidays dataset with 1,990 images. Our results show significant improvement in retrieval performance by using LRD versus its global version. We also demonstrate that LRD can deliver results comparable to well-established descriptors like LBP and HOG.



### Lung Cancer Screening Using Adaptive Memory-Augmented Recurrent Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.05719v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1710.05719v2)
- **Published**: 2017-10-11 14:54:04+00:00
- **Updated**: 2018-09-07 14:41:56+00:00
- **Authors**: Aryan Mobiny, Supratik Moulik, Hien Van Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate the effectiveness of deep learning techniques for lung nodule classification in computed tomography scans. Using less than 10,000 training examples, our deep networks perform two times better than a standard radiology software. Visualization of the networks' neurons reveals semantically meaningful features that are consistent with the clinical knowledge and radiologists' perception. Our paper also proposes a novel framework for rapidly adapting deep networks to the radiologists' feedback, or change in the data due to the shift in sensor's resolution or patient population. The classification accuracy of our approach remains above 80% while popular deep networks' accuracy is around chance. Finally, we provide in-depth analysis of our framework by asking a radiologist to examine important networks' features and perform blind re-labeling of networks' mistakes.



### Batch-based Activity Recognition from Egocentric Photo-Streams Revisited
- **Arxiv ID**: http://arxiv.org/abs/1710.04112v2
- **DOI**: 10.1007/s10044-018-0708-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04112v2)
- **Published**: 2017-10-11 15:23:07+00:00
- **Updated**: 2018-05-09 19:02:38+00:00
- **Authors**: Alejandro Cartas, Juan Marin, Petia Radeva, Mariella Dimiccoli
- **Comment**: None
- **Journal**: Cartas, A., Marin, J., Radeva, P. et al. Pattern Anal Applic
  (2018). https://doi.org/10.1007/s10044-018-0708-1
- **Summary**: Wearable cameras can gather large a\-mounts of image data that provide rich visual information about the daily activities of the wearer. Motivated by the large number of health applications that could be enabled by the automatic recognition of daily activities, such as lifestyle characterization for habit improvement, context-aware personal assistance and tele-rehabilitation services, we propose a system to classify 21 daily activities from photo-streams acquired by a wearable photo-camera. Our approach combines the advantages of a Late Fusion Ensemble strategy relying on convolutional neural networks at image level with the ability of recurrent neural networks to account for the temporal evolution of high level features in photo-streams without relying on event boundaries. The proposed batch-based approach achieved an overall accuracy of 89.85\%, outperforming state of the art end-to-end methodologies. These results were achieved on a dataset consists of 44,902 egocentric pictures from three persons captured during 26 days in average.



### On Data-Driven Saak Transform
- **Arxiv ID**: http://arxiv.org/abs/1710.04176v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04176v2)
- **Published**: 2017-10-11 17:08:28+00:00
- **Updated**: 2017-10-14 17:40:37+00:00
- **Authors**: C. -C. Jay Kuo, Yueru Chen
- **Comment**: 30 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: Being motivated by the multilayer RECOS (REctified-COrrelations on a Sphere) transform, we develop a data-driven Saak (Subspace approximation with augmented kernels) transform in this work. The Saak transform consists of three steps: 1) building the optimal linear subspace approximation with orthonormal bases using the second-order statistics of input vectors, 2) augmenting each transform kernel with its negative, 3) applying the rectified linear unit (ReLU) to the transform output. The Karhunen-Lo\'eve transform (KLT) is used in the first step. The integration of Steps 2 and 3 is powerful since they resolve the sign confusion problem, remove the rectification loss and allow a straightforward implementation of the inverse Saak transform at the same time. Multiple Saak transforms are cascaded to transform images of a larger size. All Saak transform kernels are derived from the second-order statistics of input random vectors in a one-pass feedforward manner. Neither data labels nor backpropagation is used in kernel determination. Multi-stage Saak transforms offer a family of joint spatial-spectral representations between two extremes; namely, the full spatial-domain representation and the full spectral-domain representation. We select Saak coefficients of higher discriminant power to form a feature vector for pattern recognition, and use the MNIST dataset classification problem as an illustrative example.



### Joint Image Filtering with Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.04200v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04200v5)
- **Published**: 2017-10-11 17:56:59+00:00
- **Updated**: 2019-01-02 19:30:38+00:00
- **Authors**: Yijun Li, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang
- **Comment**: Accepted by TPAMI
- **Journal**: None
- **Summary**: Joint image filters leverage the guidance image as a prior and transfer the structural details from the guidance image to the target image for suppressing noise or enhancing spatial resolution. Existing methods either rely on various explicit filter constructions or hand-designed objective functions, thereby making it difficult to understand, improve, and accelerate these filters in a coherent framework. In this paper, we propose a learning-based approach for constructing joint filters based on Convolutional Neural Networks. In contrast to existing methods that consider only the guidance image, the proposed algorithm can selectively transfer salient structures that are consistent with both guidance and target images. We show that the model trained on a certain type of data, e.g., RGB and depth images, generalizes well to other modalities, e.g., flash/non-Flash and RGB/NIR images. We validate the effectiveness of the proposed joint filter through extensive experimental evaluations with state-of-the-art methods.



### Generating Reflectance Curves from sRGB Triplets
- **Arxiv ID**: http://arxiv.org/abs/1710.05732v5
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM, I.2.10; I.3.7; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1710.05732v5)
- **Published**: 2017-10-11 19:02:54+00:00
- **Updated**: 2020-01-09 16:36:40+00:00
- **Authors**: Scott Allen Burns
- **Comment**: v3 minor editing to clarify some points, and some webpage link
  updates, v4 adds the LHTSS method, v5 indicates LHTSS should be preferred to
  ILLSS generally
- **Journal**: None
- **Summary**: The color sensation evoked by an object depends on both the spectral power distribution of the illumination and the reflectance properties of the object being illuminated. The color sensation can be characterized by three color-space values, such as XYZ, RGB, HSV, L*a*b*, etc. It is straightforward to compute the three values given the illuminant and reflectance curves. The converse process of computing a reflectance curve given the color-space values and the illuminant is complicated by the fact that an infinite number of different reflectance curves can give rise to a single set of color-space values (metamerism). This paper presents five algorithms for generating a reflectance curve from a specified sRGB triplet, written for a general audience. The algorithms are designed to generate reflectance curves that are similar to those found with naturally occurring colored objects. The computed reflectance curves are compared to a database of thousands of reflectance curves measured from paints and pigments available both commercially and in nature, and the similarity is quantified. One particularly useful application of these algorithms is in the field of computer graphics, where modeling color transformations sometimes requires wavelength-specific information, such as when modeling subtractive color mixture.



### Solutions of Quadratic First-Order ODEs applied to Computer Vision Problems
- **Arxiv ID**: http://arxiv.org/abs/1710.04265v3
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.04265v3)
- **Published**: 2017-10-11 19:29:42+00:00
- **Updated**: 2018-06-27 09:33:14+00:00
- **Authors**: David Casillas-Perez, Daniel Pizarro, Manuel Mazo, Adrien Bartoli
- **Comment**: The version 2: New change of variable. Maximal Curve Maximal Solution
  Convergence Cones The version 3: modifies the author's list and the abstract
  in metadata
- **Journal**: None
- **Summary**: This article is a study about the existence and the uniqueness of solutions of a specific quadratic first-order ODE that frequently appears in multiple reconstruction problems. It is called the \emph{planar-perspective equation} due to the duality with the geometric problem of reconstruction of planar-perspective curves from their modulus. Solutions of the \emph{planar-perspective equation} are related with planar curves parametrized with perspective parametrization due to this geometric interpretation. The article proves the existence of only two local solutions to the \emph{initial value problem} with \emph{regular initial conditions} and a maximum of two analytic solutions with \emph{critical initial conditions}. The article also gives theorems to extend the local definition domain where the existence of both solutions are guaranteed. It introduces the \emph{maximal depth function} as a function that upper-bound all possible solutions of the \emph{planar-perspective equation} and contains all its possible \emph{critical points}. Finally, the article describes the \emph{maximal-depth solution problem} that consists of finding the solution of the referred equation that has maximum the depth and proves its uniqueness. It is an important problem as it does not need initial conditions to obtain the unique solution and its the frequent solution that practical algorithms of the state-of-the-art give.



