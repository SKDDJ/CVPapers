# Arxiv Papers in cs.CV on 2017-10-09
### UG^2: a Video Benchmark for Assessing the Impact of Image Restoration and Enhancement on Automatic Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1710.02909v2
- **DOI**: 10.1109/WACV.2018.00177
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02909v2)
- **Published**: 2017-10-09 02:01:58+00:00
- **Updated**: 2018-02-07 02:12:50+00:00
- **Authors**: Rosaura G. Vidal, Sreya Banerjee, Klemen Grm, Vitomir Struc, Walter J. Scheirer
- **Comment**: Supplemental material: https://goo.gl/vVM1xe, Dataset:
  https://goo.gl/AjA6En, CVPR 2018 Prize Challenge: ug2challenge.org
- **Journal**: None
- **Summary**: Advances in image restoration and enhancement techniques have led to discussion about how such algorithmscan be applied as a pre-processing step to improve automatic visual recognition. In principle, techniques like deblurring and super-resolution should yield improvements by de-emphasizing noise and increasing signal in an input image. But the historically divergent goals of the computational photography and visual recognition communities have created a significant need for more work in this direction. To facilitate new research, we introduce a new benchmark dataset called UG^2, which contains three difficult real-world scenarios: uncontrolled videos taken by UAVs and manned gliders, as well as controlled videos taken on the ground. Over 160,000 annotated frames forhundreds of ImageNet classes are available, which are used for baseline experiments that assess the impact of known and unknown image artifacts and other conditions on common deep learning-based object classification approaches. Further, current image restoration and enhancement techniques are evaluated by determining whether or not theyimprove baseline classification performance. Results showthat there is plenty of room for algorithmic innovation, making this dataset a useful tool going forward.



### Face Sketch Matching via Coupled Deep Transform Learning
- **Arxiv ID**: http://arxiv.org/abs/1710.02914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02914v1)
- **Published**: 2017-10-09 02:42:01+00:00
- **Updated**: 2017-10-09 02:42:01+00:00
- **Authors**: Shruti Nagpal, Maneet Singh, Richa Singh, Mayank Vatsa, Afzel Noore, Angshul Majumdar
- **Comment**: International Conference on Computer Vision, 2017
- **Journal**: None
- **Summary**: Face sketch to digital image matching is an important challenge of face recognition that involves matching across different domains. Current research efforts have primarily focused on extracting domain invariant representations or learning a mapping from one domain to the other. In this research, we propose a novel transform learning based approach termed as DeepTransformer, which learns a transformation and mapping function between the features of two domains. The proposed formulation is independent of the input information and can be applied with any existing learned or hand-crafted feature. Since the mapping function is directional in nature, we propose two variants of DeepTransformer: (i) semi-coupled and (ii) symmetrically-coupled deep transform learning. This research also uses a novel IIIT-D Composite Sketch with Age (CSA) variations database which contains sketch images of 150 subjects along with age-separated digital photos. The performance of the proposed models is evaluated on a novel application of sketch-to-sketch matching, along with sketch-to-digital photo matching. Experimental results demonstrate the robustness of the proposed models in comparison to existing state-of-the-art sketch matching algorithms and a commercial face recognition system.



### Visual Servoing of Unmanned Surface Vehicle from Small Tethered Unmanned Aerial Vehicle
- **Arxiv ID**: http://arxiv.org/abs/1710.02932v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1710.02932v1)
- **Published**: 2017-10-09 04:14:05+00:00
- **Updated**: 2017-10-09 04:14:05+00:00
- **Authors**: Haresh Karnan, Aritra Biswas, Pranav Vaidik Dhulipala, Jan Dufek, Robin Murphy
- **Comment**: 6 pages, 13 images
- **Journal**: None
- **Summary**: This paper presents an algorithm and the implementation of a motor schema to aid the visual localization subsystem of the ongoing EMILY project at Texas A and M University. The EMILY project aims to team an Unmanned Surface Vehicle (USV) with an Unmanned Aerial Vehicle (UAV) to augment the search and rescue of marine casualties during an emergency response phase. The USV is designed to serve as a flotation device once it reaches the victims. A live video feed from the UAV is provided to the casuality responders giving them a visual estimate of the USVs orientation and position to help with its navigation. One of the challenges involved with casualty response using a USV UAV team is to simultaneously control the USV and track it. In this paper, we present an implemented solution to automate the UAV camera movements to keep the USV in view at all times. The motor schema proposed, uses the USVs coordinates from the visual localization subsystem to control the UAVs camera movements and track the USV with minimal camera movements such that the USV is always in the cameras field of view.



### Does Normalization Methods Play a Role for Hyperspectral Image Classification?
- **Arxiv ID**: http://arxiv.org/abs/1710.02939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02939v1)
- **Published**: 2017-10-09 05:08:32+00:00
- **Updated**: 2017-10-09 05:08:32+00:00
- **Authors**: Faxian Cao, Zhijing Yang, Jinchang Ren, Mengying Jiang, Wing-Kuen Ling
- **Comment**: 6 pages. 1 figure, 4 tables
- **Journal**: None
- **Summary**: For Hyperspectral image (HSI) datasets, each class have their salient feature and classifiers classify HSI datasets according to the class's saliency features, however, there will be different salient features when use different normalization method. In this letter, we report the effect on classifiers by different normalization methods and recommend the best normalization methods for classifier after analyzing the impact of different normalization methods on classifiers. Pavia University datasets, Indian Pines datasets and Kennedy Space Center datasets will apply to several typical classifiers in order to evaluate and analysis the impact of different normalization methods on typical classifiers.



### Algorithm guided outlining of 105 pancreatic cancer liver metastases in Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/1710.02984v1
- **DOI**: 10.1038/s41598-017-12925-z
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1710.02984v1)
- **Published**: 2017-10-09 08:22:28+00:00
- **Updated**: 2017-10-09 08:22:28+00:00
- **Authors**: Alexander Hann, Lucas Bettac, Mark M. Haenle, Tilmann Graeter, Andreas W. Berger, Jens Dreyhaupt, Dieter Schmalstieg, Wolfram G. Zoller, Jan Egger
- **Comment**: 7 pages, 3 Figures, 3 Tables, 46 References
- **Journal**: Sci Rep. 2017 Oct 6;7(1):12779
- **Summary**: Manual segmentation of hepatic metastases in ultrasound images acquired from patients suffering from pancreatic cancer is common practice. Semiautomatic measurements promising assistance in this process are often assessed using a small number of lesions performed by examiners who already know the algorithm. In this work, we present the application of an algorithm for the segmentation of liver metastases due to pancreatic cancer using a set of 105 different images of metastases. The algorithm and the two examiners had never assessed the images before. The examiners first performed a manual segmentation and, after five weeks, a semiautomatic segmentation using the algorithm. They were satisfied in up to 90% of the cases with the semiautomatic segmentation results. Using the algorithm was significantly faster and resulted in a median Dice similarity score of over 80%. Estimation of the inter-operator variability by using the intra class correlation coefficient was good with 0.8. In conclusion, the algorithm facilitates fast and accurate segmentation of liver metastases, comparable to the current gold standard of manual segmentation.



### Age Group and Gender Estimation in the Wild with Deep RoR Architecture
- **Arxiv ID**: http://arxiv.org/abs/1710.02985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02985v1)
- **Published**: 2017-10-09 08:27:13+00:00
- **Updated**: 2017-10-09 08:27:13+00:00
- **Authors**: Ke Zhang, Ce Gao, Liru Guo, Miao Sun, Xingfang Yuan, Tony X. Han, Zhenbing Zhao, Baogang Li
- **Comment**: accepted by IEEE ACCESS
- **Journal**: None
- **Summary**: Automatically predicting age group and gender from face images acquired in unconstrained conditions is an important and challenging task in many real-world applications. Nevertheless, the conventional methods with manually-designed features on in-the-wild benchmarks are unsatisfactory because of incompetency to tackle large variations in unconstrained images. This difficulty is alleviated to some degree through Convolutional Neural Networks (CNN) for its powerful feature representation. In this paper, we propose a new CNN based method for age group and gender estimation leveraging Residual Networks of Residual Networks (RoR), which exhibits better optimization ability for age group and gender classification than other CNN architectures.Moreover, two modest mechanisms based on observation of the characteristics of age group are presented to further improve the performance of age estimation.In order to further improve the performance and alleviate over-fitting problem, RoR model is pre-trained on ImageNet firstly, and then it is fune-tuned on the IMDB-WIKI-101 data set for further learning the features of face images, finally, it is used to fine-tune on Adience data set. Our experiments illustrate the effectiveness of RoR method for age and gender estimation in the wild, where it achieves better performance than other CNN methods. Finally, the RoR-152+IMDB-WIKI-101 with two mechanisms achieves new state-of-the-art results on Adience benchmark.



### Personalized Saliency and its Prediction
- **Arxiv ID**: http://arxiv.org/abs/1710.03011v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.03011v2)
- **Published**: 2017-10-09 09:43:48+00:00
- **Updated**: 2018-06-16 23:18:38+00:00
- **Authors**: Yanyu Xu, Shenghua Gao, Junru Wu, Nianyi Li, Jingyi Yu
- **Comment**: 15 pages, 10 figures, journal
- **Journal**: None
- **Summary**: Nearly all existing visual saliency models by far have focused on predicting a universal saliency map across all observers. Yet psychology studies suggest that visual attention of different observers can vary significantly under specific circumstances, especially a scene is composed of multiple salient objects. To study such heterogenous visual attention pattern across observers, we first construct a personalized saliency dataset and explore correlations between visual attention, personal preferences, and image contents. Specifically, we propose to decompose a personalized saliency map (referred to as PSM) into a universal saliency map (referred to as USM) predictable by existing saliency detection models and a new discrepancy map across users that characterizes personalized saliency. We then present two solutions towards predicting such discrepancy maps, i.e., a multi-task convolutional neural network (CNN) framework and an extended CNN with Person-specific Information Encoded Filters (CNN-PIEF). Extensive experimental results demonstrate the effectiveness of our models for PSM prediction as well their generalization capability for unseen observers.



### An automatic deep learning approach for coronary artery calcium segmentation
- **Arxiv ID**: http://arxiv.org/abs/1710.03023v1
- **DOI**: 10.1007/978-981-10-5122-7_94
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.03023v1)
- **Published**: 2017-10-09 10:48:13+00:00
- **Updated**: 2017-10-09 10:48:13+00:00
- **Authors**: G. Santini, D. Della Latta, N. Martini, G. Valvano, A. Gori, A. Ripoli, C. L. Susini, L. Landini, D. Chiappino
- **Comment**: None
- **Journal**: EMBEC & NBC 2017. IFMBE, volume 65, pp. 374-377
- **Summary**: Coronary artery calcium (CAC) is a significant marker of atherosclerosis and cardiovascular events. In this work we present a system for the automatic quantification of calcium score in ECG-triggered non-contrast enhanced cardiac computed tomography (CT) images. The proposed system uses a supervised deep learning algorithm, i.e. convolutional neural network (CNN) for the segmentation and classification of candidate lesions as coronary or not, previously extracted in the region of the heart using a cardiac atlas. We trained our network with 45 CT volumes; 18 volumes were used to validate the model and 56 to test it. Individual lesions were detected with a sensitivity of 91.24%, a specificity of 95.37% and a positive predicted value (PPV) of 90.5%; comparing calcium score obtained by the system and calcium score manually evaluated by an expert operator, a Pearson coefficient of 0.983 was obtained. A high agreement (Cohen's k = 0.879) between manual and automatic risk prediction was also observed. These results demonstrated that convolutional neural networks can be effectively applied for the automatic segmentation and classification of coronary calcifications.



### A Sequential Thinning Algorithm For Multi-Dimensional Binary Patterns
- **Arxiv ID**: http://arxiv.org/abs/1710.03025v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1710.03025v2)
- **Published**: 2017-10-09 10:52:55+00:00
- **Updated**: 2017-11-16 21:19:29+00:00
- **Authors**: Himanshu Jain, Archana Praveen Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Thinning is the removal of contour pixels/points of connected components in an image to produce their skeleton with retained connectivity and structural properties. The output requirements of a thinning procedure often vary with application. This paper proposes a sequential algorithm that is very easy to understand and modify based on application to perform the thinning of multi-dimensional binary patterns. The algorithm was tested on 2D and 3D patterns and showed very good results. Moreover, comparisons were also made with two of the state-of-the-art methods used for 2D patterns. The results obtained prove the validity of the procedure.



### A Bottom Up Procedure for Text Line Segmentation of Latin Script
- **Arxiv ID**: http://arxiv.org/abs/1710.03027v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1710.03027v1)
- **Published**: 2017-10-09 11:00:00+00:00
- **Updated**: 2017-10-09 11:00:00+00:00
- **Authors**: Himanshu Jain, Archana Praveen Kumar
- **Comment**: Accepted and presented at the IEEE conference "International
  Conference on Advances in Computing, Communications and Informatics (ICACCI)
  2017"
- **Journal**: None
- **Summary**: In this paper we present a bottom up procedure for segmentation of text lines written or printed in the Latin script. The proposed method uses a combination of image morphology, feature extraction and Gaussian mixture model to perform this task. The experimental results show the validity of the procedure.



### Deeper, Broader and Artier Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/1710.03077v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.03077v1)
- **Published**: 2017-10-09 13:19:27+00:00
- **Updated**: 2017-10-09 13:19:27+00:00
- **Authors**: Da Li, Yongxin Yang, Yi-Zhe Song, Timothy M. Hospedales
- **Comment**: 9 pages, 4 figures, ICCV 2017
- **Journal**: None
- **Summary**: The problem of domain generalization is to learn from multiple training domains, and extract a domain-agnostic model that can then be applied to an unseen domain. Domain generalization (DG) has a clear motivation in contexts where there are target domains with distinct characteristics, yet sparse data for training. For example recognition in sketch images, which are distinctly more abstract and rarer than photos. Nevertheless, DG methods have primarily been evaluated on photo-only benchmarks focusing on alleviating the dataset bias where both problems of domain distinctiveness and data sparsity can be minimal. We argue that these benchmarks are overly straightforward, and show that simple deep learning baselines perform surprisingly well on them. In this paper, we make two main contributions: Firstly, we build upon the favorable domain shift-robust properties of deep learning methods, and develop a low-rank parameterized CNN model for end-to-end DG learning. Secondly, we develop a DG benchmark dataset covering photo, sketch, cartoon and painting domains. This is both more practically relevant, and harder (bigger domain shift) than existing benchmarks. The results show that our method outperforms existing DG alternatives, and our dataset provides a more significant DG challenge to drive future research.



### Person Recognition in Personal Photo Collections
- **Arxiv ID**: http://arxiv.org/abs/1710.03224v2
- **DOI**: 10.1109/TPAMI.2018.2877588
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.03224v2)
- **Published**: 2017-10-09 13:33:39+00:00
- **Updated**: 2018-10-20 23:46:47+00:00
- **Authors**: Seong Joon Oh, Rodrigo Benenson, Mario Fritz, Bernt Schiele
- **Comment**: 18 pages, 20 figures; to appear in IEEE Transactions on Pattern
  Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: People nowadays share large parts of their personal lives through social media. Being able to automatically recognise people in personal photos may greatly enhance user convenience by easing photo album organisation. For human identification task, however, traditional focus of computer vision has been face recognition and pedestrian re-identification. Person recognition in social media photos sets new challenges for computer vision, including non-cooperative subjects (e.g. backward viewpoints, unusual poses) and great changes in appearance. To tackle this problem, we build a simple person recognition framework that leverages convnet features from multiple image regions (head, body, etc.). We propose new recognition scenarios that focus on the time and appearance gap between training and testing samples. We present an in-depth analysis of the importance of different features according to time and viewpoint generalisability. In the process, we verify that our simple approach achieves the state of the art result on the PIPA benchmark, arguably the largest social media based benchmark for person recognition to date with diverse poses, viewpoints, social groups, and events.   Compared the conference version of the paper, this paper additionally presents (1) analysis of a face recogniser (DeepID2+), (2) new method naeil2 that combines the conference version method naeil and DeepID2+ to achieve state of the art results even compared to post-conference works, (3) discussion of related work since the conference version, (4) additional analysis including the head viewpoint-wise breakdown of performance, and (5) results on the open-world setup.



### Handwritten digit string recognition by combination of residual network and RNN-CTC
- **Arxiv ID**: http://arxiv.org/abs/1710.03112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.03112v1)
- **Published**: 2017-10-09 14:16:00+00:00
- **Updated**: 2017-10-09 14:16:00+00:00
- **Authors**: Hongjian Zhan, Qingqing Wang, Yue Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Recurrent neural network (RNN) and connectionist temporal classification (CTC) have showed successes in many sequence labeling tasks with the strong ability of dealing with the problems where the alignment between the inputs and the target labels is unknown. Residual network is a new structure of convolutional neural network and works well in various computer vision tasks. In this paper, we take advantage of the architectures mentioned above to create a new network for handwritten digit string recognition. First we design a residual network to extract features from input images, then we employ a RNN to model the contextual information within feature sequences and predict recognition results. At the top of this network, a standard CTC is applied to calculate the loss and yield the final results. These three parts compose an end-to-end trainable network. The proposed new architecture achieves the highest performances on ORAND-CAR-A and ORAND-CAR-B with recognition rates 89.75% and 91.14%, respectively. In addition, the experiments on a generated captcha dataset which has much longer string length show the potential of the proposed network to handle long strings.



### Toward Multidiversified Ensemble Clustering of High-Dimensional Data: From Subspaces to Metrics and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1710.03113v5
- **DOI**: 10.1109/TCYB.2021.3049633
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.03113v5)
- **Published**: 2017-10-09 14:19:04+00:00
- **Updated**: 2021-09-05 03:05:32+00:00
- **Authors**: Dong Huang, Chang-Dong Wang, Jian-Huang Lai, Chee-Keong Kwoh
- **Comment**: Accepted by IEEE Transactions on Cybernetics. The MATLAB source code
  is available at https://github.com/huangdonghere/MDEC
- **Journal**: None
- **Summary**: The rapid emergence of high-dimensional data in various areas has brought new challenges to current ensemble clustering research. To deal with the curse of dimensionality, recently considerable efforts in ensemble clustering have been made by means of different subspace-based techniques. However, besides the emphasis on subspaces, rather limited attention has been paid to the potential diversity in similarity/dissimilarity metrics. It remains a surprisingly open problem in ensemble clustering how to create and aggregate a large population of diversified metrics, and furthermore, how to jointly investigate the multi-level diversity in the large populations of metrics, subspaces, and clusters in a unified framework. To tackle this problem, this paper proposes a novel multidiversified ensemble clustering approach. In particular, we create a large number of diversified metrics by randomizing a scaled exponential similarity kernel, which are then coupled with random subspaces to form a large set of metric-subspace pairs. Based on the similarity matrices derived from these metric-subspace pairs, an ensemble of diversified base clusterings can thereby be constructed. Further, an entropy-based criterion is utilized to explore the cluster-wise diversity in ensembles, based on which three specific ensemble clustering algorithms are presented by incorporating three types of consensus functions. Extensive experiments are conducted on 30 high-dimensional datasets, including 18 cancer gene expression datasets and 12 image/speech datasets, which demonstrate the superiority of our algorithms over the state-of-the-art. The source code is available at https://github.com/huangdonghere/MDEC.



### Island Loss for Learning Discriminative Features in Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1710.03144v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.03144v3)
- **Published**: 2017-10-09 15:26:22+00:00
- **Updated**: 2017-10-23 15:32:10+00:00
- **Authors**: Jie Cai, Zibo Meng, Ahmed Shehab Khan, Zhiyuan Li, James O'Reilly, Yan Tong
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Over the past few years, Convolutional Neural Networks (CNNs) have shown promise on facial expression recognition. However, the performance degrades dramatically under real-world settings due to variations introduced by subtle facial appearance changes, head pose variations, illumination changes, and occlusions.   In this paper, a novel island loss is proposed to enhance the discriminative power of the deeply learned features. Specifically, the IL is designed to reduce the intra-class variations while enlarging the inter-class differences simultaneously. Experimental results on four benchmark expression databases have demonstrated that the CNN with the proposed island loss (IL-CNN) outperforms the baseline CNN models with either traditional softmax loss or the center loss and achieves comparable or better performance compared with the state-of-the-art methods for facial expression recognition.



### Multitask training with unlabeled data for end-to-end sign language fingerspelling recognition
- **Arxiv ID**: http://arxiv.org/abs/1710.03255v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.03255v2)
- **Published**: 2017-10-09 18:21:57+00:00
- **Updated**: 2019-02-17 22:52:59+00:00
- **Authors**: Bowen Shi, Karen Livescu
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of automatic American Sign Language fingerspelling recognition from video. Prior work has largely relied on frame-level labels, hand-crafted features, or other constraints, and has been hampered by the scarcity of data for this task. We introduce a model for fingerspelling recognition that addresses these issues. The model consists of an auto-encoder-based feature extractor and an attention-based neural encoder-decoder, which are trained jointly. The model receives a sequence of image frames and outputs the fingerspelled word, without relying on any frame-level training labels or hand-crafted features. In addition, the auto-encoder subcomponent makes it possible to leverage unlabeled data to improve the feature learning. The model achieves 11.6% and 4.4% absolute letter accuracy improvement respectively in signer-independent and signer-adapted fingerspelling recognition over previous approaches that required frame-level training labels.



### Standard detectors aren't (currently) fooled by physical adversarial stop signs
- **Arxiv ID**: http://arxiv.org/abs/1710.03337v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1710.03337v2)
- **Published**: 2017-10-09 22:20:59+00:00
- **Updated**: 2017-10-26 21:53:58+00:00
- **Authors**: Jiajun Lu, Hussein Sibai, Evan Fabry, David Forsyth
- **Comment**: Follow up for previous adversarial stop sign paper
- **Journal**: None
- **Summary**: An adversarial example is an example that has been adjusted to produce the wrong label when presented to a system at test time. If adversarial examples existed that could fool a detector, they could be used to (for example) wreak havoc on roads populated with smart vehicles. Recently, we described our difficulties creating physical adversarial stop signs that fool a detector. More recently, Evtimov et al. produced a physical adversarial stop sign that fools a proxy model of a detector. In this paper, we show that these physical adversarial stop signs do not fool two standard detectors (YOLO and Faster RCNN) in standard configuration. Evtimov et al.'s construction relies on a crop of the image to the stop sign; this crop is then resized and presented to a classifier. We argue that the cropping and resizing procedure largely eliminates the effects of rescaling and of view angle. Whether an adversarial attack is robust under rescaling and change of view direction remains moot. We argue that attacking a classifier is very different from attacking a detector, and that the structure of detectors - which must search for their own bounding box, and which cannot estimate that box very accurately - likely makes it difficult to make adversarial patterns. Finally, an adversarial pattern on a physical object that could fool a detector would have to be adversarial in the face of a wide family of parametric distortions (scale; view angle; box shift inside the detector; illumination; and so on). Such a pattern would be of great theoretical and practical interest. There is currently no evidence that such patterns exist.



### Vehicle classification based on convolutional networks applied to FM-CW radar signals
- **Arxiv ID**: http://arxiv.org/abs/1710.05718v3
- **DOI**: 10.1007/978-3-319-75608-0_9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05718v3)
- **Published**: 2017-10-09 22:23:30+00:00
- **Updated**: 2017-10-31 10:14:03+00:00
- **Authors**: Samuele Capobianco, Luca Facheris, Fabrizio Cuccoli, Simone Marinai
- **Comment**: in Proceedings of 1st European Conference on Traffic Mining Applied
  to Police Activities (TRAP 2017)
- **Journal**: Springer, Cham, 2018, ISBN: 978-3-319-75608-0
- **Summary**: This paper investigates the processing of Frequency Modulated-Continuos Wave (FM-CW) radar signals for vehicle classification. In the last years deep learning has gained interest in several scientific fields and signal processing is not one exception. In this work we address the recognition of the vehicle category using a Convolutional Neural Network (CNN) applied to range Doppler signature. The developed system first transforms the 1-dimensional signal into a 3-dimensional signal that is subsequently used as input to the CNN. When using the trained model to predict the vehicle category we obtain good performance.



### Iterative PET Image Reconstruction Using Convolutional Neural Network Representation
- **Arxiv ID**: http://arxiv.org/abs/1710.03344v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.03344v1)
- **Published**: 2017-10-09 22:51:28+00:00
- **Updated**: 2017-10-09 22:51:28+00:00
- **Authors**: Kuang Gong, Jiahui Guan, Kyungsang Kim, Xuezhu Zhang, Georges El Fakhri, Jinyi Qi, Quanzheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: PET image reconstruction is challenging due to the ill-poseness of the inverse problem and limited number of detected photons. Recently deep neural networks have been widely and successfully used in computer vision tasks and attracted growing interests in medical imaging. In this work, we trained a deep residual convolutional neural network to improve PET image quality by using the existing inter-patient information. An innovative feature of the proposed method is that we embed the neural network in the iterative reconstruction framework for image representation, rather than using it as a post-processing tool. We formulate the objective function as a constraint optimization problem and solve it using the alternating direction method of multipliers (ADMM) algorithm. Both simulation data and hybrid real data are used to evaluate the proposed method. Quantification results show that our proposed iterative neural network method can outperform the neural network denoising and conventional penalized maximum likelihood methods.



