# Arxiv Papers in cs.CV on 2017-10-23
### Feedback-prop: Convolutional Neural Network Inference under Partial Evidence
- **Arxiv ID**: http://arxiv.org/abs/1710.08049v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.08049v2)
- **Published**: 2017-10-23 00:29:49+00:00
- **Updated**: 2018-03-29 19:12:46+00:00
- **Authors**: Tianlu Wang, Kota Yamaguchi, Vicente Ordonez
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: We propose an inference procedure for deep convolutional neural networks (CNNs) when partial evidence is available. Our method consists of a general feedback-based propagation approach (feedback-prop) that boosts the prediction accuracy for an arbitrary set of unknown target labels when the values for a non-overlapping arbitrary set of target labels are known. We show that existing models trained in a multi-label or multi-task setting can readily take advantage of feedback-prop without any retraining or fine-tuning. Our feedback-prop inference procedure is general, simple, reliable, and works on different challenging visual recognition tasks. We present two variants of feedback-prop based on layer-wise and residual iterative updates. We experiment using several multi-task models and show that feedback-prop is effective in all of them. Our results unveil a previously unreported but interesting dynamic property of deep CNNs. We also present an associated technical approach that takes advantage of this property for inference under partial evidence in general visual recognition tasks.



### VGGFace2: A dataset for recognising faces across pose and age
- **Arxiv ID**: http://arxiv.org/abs/1710.08092v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.08092v2)
- **Published**: 2017-10-23 05:26:32+00:00
- **Updated**: 2018-05-13 10:35:21+00:00
- **Authors**: Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, Andrew Zisserman
- **Comment**: This paper has been accepted by IEEE Conference on Automatic Face and
  Gesture Recognition (F&G), 2018. (Oral)
- **Journal**: None
- **Summary**: In this paper, we introduce a new large-scale face dataset named VGGFace2. The dataset contains 3.31 million images of 9131 subjects, with an average of 362.6 images for each subject. Images are downloaded from Google Image Search and have large variations in pose, age, illumination, ethnicity and profession (e.g. actors, athletes, politicians). The dataset was collected with three goals in mind: (i) to have both a large number of identities and also a large number of images for each identity; (ii) to cover a large range of pose, age and ethnicity; and (iii) to minimize the label noise. We describe how the dataset was collected, in particular the automated and manual filtering stages to ensure a high accuracy for the images of each identity. To assess face recognition performance using the new dataset, we train ResNet-50 (with and without Squeeze-and-Excitation blocks) Convolutional Neural Networks on VGGFace2, on MS- Celeb-1M, and on their union, and show that training on VGGFace2 leads to improved recognition performance over pose and age. Finally, using the models trained on these datasets, we demonstrate state-of-the-art performance on all the IARPA Janus face recognition benchmarks, e.g. IJB-A, IJB-B and IJB-C, exceeding the previous state-of-the-art by a large margin. Datasets and models are publicly available.



### Accelerating GMM-based patch priors for image restoration: Three ingredients for a 100$\times$ speed-up
- **Arxiv ID**: http://arxiv.org/abs/1710.08124v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.08124v1)
- **Published**: 2017-10-23 07:39:35+00:00
- **Updated**: 2017-10-23 07:39:35+00:00
- **Authors**: Shibin Parameswaran, Charles-Alban Deledalle, Lo√Øc Denis, Truong Q. Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Image restoration methods aim to recover the underlying clean image from corrupted observations. The Expected Patch Log-likelihood (EPLL) algorithm is a powerful image restoration method that uses a Gaussian mixture model (GMM) prior on the patches of natural images. Although it is very effective for restoring images, its high runtime complexity makes EPLL ill-suited for most practical applications. In this paper, we propose three approximations to the original EPLL algorithm. The resulting algorithm, which we call the fast-EPLL (FEPLL), attains a dramatic speed-up of two orders of magnitude over EPLL while incurring a negligible drop in the restored image quality (less than 0.5 dB). We demonstrate the efficacy and versatility of our algorithm on a number of inverse problems such as denoising, deblurring, super-resolution, inpainting and devignetting. To the best of our knowledge, FEPLL is the first algorithm that can competitively restore a 512x512 pixel image in under 0.5s for all the degradations mentioned above without specialized code optimizations such as CPU parallelization or GPU implementation.



### An iterative closest point method for measuring the level of similarity of 3d log scans in wood industry
- **Arxiv ID**: http://arxiv.org/abs/1710.08135v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.08135v1)
- **Published**: 2017-10-23 08:12:45+00:00
- **Updated**: 2017-10-23 08:12:45+00:00
- **Authors**: Cyrine Selma, Hind Haouzi, Philippe Thomas, Jonathan Gaudreault, Michael Morin
- **Comment**: 7th Workshop on Service Orientation in Holonic and Multi Agent
  Manufacturing SOHOMA'17, Oct 2017, Nantes, France
- **Journal**: None
- **Summary**: In the Canadian's lumber industry, simulators are used to predict the lumbers resulting from the sawing of a log at a given sawmill. Giving a log or several logs' 3D scans as input, simulators perform a real-time job to predict the lumbers. These simulators, however, tend to be slow at processing large volume of wood. We thus explore an alternative approximation techniques based on the Iterative Closest Point (ICP) algorithm to identify the already processed log to which an unseen log resembles the most. The main benefit of the ICP approach is that it can easily handle 3D scans with a variable number of points. We compare this ICP-based nearest neighbor predictor, to predictors built using machine learning algorithms such as the K-nearest-neighbor (kNN) and Random Forest (RF). The implemented ICP-based predictor enabled us to identify key points in using the 3D scans directly for distance calculation. The long-term goal of this ongoing research is to integrated ICP distance calculations and machine learning.



### Image Segmentation and Classification for Sickle Cell Disease using Deformable U-Net
- **Arxiv ID**: http://arxiv.org/abs/1710.08149v3
- **DOI**: None
- **Categories**: **q-bio.CB**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.08149v3)
- **Published**: 2017-10-23 08:53:07+00:00
- **Updated**: 2017-10-29 04:02:32+00:00
- **Authors**: Mo Zhang, Xiang Li, Mengjia Xu, Quanzheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Reliable cell segmentation and classification from biomedical images is a crucial step for both scientific research and clinical practice. A major challenge for more robust segmentation and classification methods is the large variations in the size, shape and viewpoint of the cells, combining with the low image quality caused by noise and artifacts. To address this issue, in this work we propose a learning-based, simultaneous cell segmentation and classification method based on the deep U-Net structure with deformable convolution layers. The U-Net architecture for deep learning has been shown to offer a precise localization for image semantic segmentation. Moreover, deformable convolution layer enables the free form deformation of the feature learning process, thus makes the whole network more robust to various cell morphologies and image settings. The proposed method is tested on microscopic red blood cell images from patients with sickle cell disease. The results show that U-Net with deformable convolution achieves the highest accuracy for segmentation and classification, comparing with original U-Net structure.



### Progressive Learning for Systematic Design of Large Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.08177v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.08177v1)
- **Published**: 2017-10-23 10:06:15+00:00
- **Updated**: 2017-10-23 10:06:15+00:00
- **Authors**: Saikat Chatterjee, Alireza M. Javid, Mostafa Sadeghi, Partha P. Mitra, Mikael Skoglund
- **Comment**: None
- **Journal**: None
- **Summary**: We develop an algorithm for systematic design of a large artificial neural network using a progression property. We find that some non-linear functions, such as the rectifier linear unit and its derivatives, hold the property. The systematic design addresses the choice of network size and regularization of parameters. The number of nodes and layers in network increases in progression with the objective of consistently reducing an appropriate cost. Each layer is optimized at a time, where appropriate parameters are learned using convex optimization. Regularization parameters for convex optimization do not need a significant manual effort for tuning. We also use random instances for some weight matrices, and that helps to reduce the number of parameters we learn. The developed network is expected to show good generalization power due to appropriate regularization and use of random weights in the layers. This expectation is verified by extensive experiments for classification and regression problems, using standard databases.



### Investigating the feature collection for semantic segmentation via single skip connection
- **Arxiv ID**: http://arxiv.org/abs/1710.08192v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1710.08192v1)
- **Published**: 2017-10-23 10:40:58+00:00
- **Updated**: 2017-10-23 10:40:58+00:00
- **Authors**: Jonghwa Yim, Kyung-Ah Sohn
- **Comment**: (In pressing) Journal of KIISE
- **Journal**: None
- **Summary**: Since the study of deep convolutional neural network became prevalent, one of the important discoveries is that a feature map from a convolutional network can be extracted before going into the fully connected layer and can be used as a saliency map for object detection. Furthermore, the model can use features from each different layer for accurate object detection: the features from different layers can have different properties. As the model goes deeper, it has many latent skip connections and feature maps to elaborate object detection. Although there are many intermediate layers that we can use for semantic segmentation through skip connection, still the characteristics of each skip connection and the best skip connection for this task are uncertain. Therefore, in this study, we exhaustively research skip connections of state-of-the-art deep convolutional networks and investigate the characteristics of the features from each intermediate layer. In addition, this study would suggest how to use a recent deep neural network model for semantic segmentation and it would therefore become a cornerstone for later studies with the state-of-the-art network models.



### Generic 3D Representation via Pose Estimation and Matching
- **Arxiv ID**: http://arxiv.org/abs/1710.08247v1
- **DOI**: 10.1007/978-3-319-46487-9_33
- **Categories**: **cs.CV**, cs.LG, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1710.08247v1)
- **Published**: 2017-10-23 13:01:05+00:00
- **Updated**: 2017-10-23 13:01:05+00:00
- **Authors**: Amir R. Zamir, Tilman Wekel, Pulkit Argrawal, Colin Weil, Jitendra Malik, Silvio Savarese
- **Comment**: Published in ECCV16. See the project website
  http://3drepresentation.stanford.edu/ and dataset website
  https://github.com/amir32002/3D_Street_View
- **Journal**: ECCV 2016 535-553
- **Summary**: Though a large body of computer vision research has investigated developing generic semantic representations, efforts towards developing a similar representation for 3D has been limited. In this paper, we learn a generic 3D representation through solving a set of foundational proxy 3D tasks: object-centric camera pose estimation and wide baseline feature matching. Our method is based upon the premise that by providing supervision over a set of carefully selected foundational tasks, generalization to novel tasks and abstraction capabilities can be achieved. We empirically show that the internal representation of a multi-task ConvNet trained to solve the above core problems generalizes to novel 3D tasks (e.g., scene layout estimation, object pose estimation, surface normal estimation) without the need for fine-tuning and shows traits of abstraction abilities (e.g., cross-modality pose estimation). In the context of the core supervised tasks, we demonstrate our representation achieves state-of-the-art wide baseline feature matching results without requiring apriori rectification (unlike SIFT and the majority of learned features). We also show 6DOF camera pose estimation given a pair local image patches. The accuracy of both supervised tasks come comparable to humans. Finally, we contribute a large-scale dataset composed of object-centric street view scenes along with point correspondences and camera pose information, and conclude with a discussion on the learned representation and open research questions.



### A Survey of Model Compression and Acceleration for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.09282v9
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.09282v9)
- **Published**: 2017-10-23 20:16:55+00:00
- **Updated**: 2020-06-14 19:10:03+00:00
- **Authors**: Yu Cheng, Duo Wang, Pan Zhou, Tao Zhang
- **Comment**: Published in IEEE Signal Processing Magazine, updated version
  including more recent works
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have recently achieved great success in many visual recognition tasks. However, existing deep neural network models are computationally expensive and memory intensive, hindering their deployment in devices with low memory resources or in applications with strict latency requirements. Therefore, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance. During the past five years, tremendous progress has been made in this area. In this paper, we review the recent techniques for compacting and accelerating DNN models. In general, these techniques are divided into four categories: parameter pruning and quantization, low-rank factorization, transferred/compact convolutional filters, and knowledge distillation. Methods of parameter pruning and quantization are described first, after that the other techniques are introduced. For each category, we also provide insightful analysis about the performance, related applications, advantages, and drawbacks. Then we go through some very recent successful methods, for example, dynamic capacity networks and stochastic depths networks. After that, we survey the evaluation matrices, the main datasets used for evaluating the model performance, and recent benchmark efforts. Finally, we conclude this paper, discuss remaining the challenges and possible directions for future work.



### ContextVP: Fully Context-Aware Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1710.08518v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.08518v3)
- **Published**: 2017-10-23 21:55:12+00:00
- **Updated**: 2018-09-09 09:55:04+00:00
- **Authors**: Wonmin Byeon, Qin Wang, Rupesh Kumar Srivastava, Petros Koumoutsakos
- **Comment**: 19 pages. ECCV 2018 oral presentation. Project webpage is at
  https://wonmin-byeon.github.io/publication/2018-eccv
- **Journal**: None
- **Summary**: Video prediction models based on convolutional networks, recurrent networks, and their combinations often result in blurry predictions. We identify an important contributing factor for imprecise predictions that has not been studied adequately in the literature: blind spots, i.e., lack of access to all relevant past information for accurately predicting the future. To address this issue, we introduce a fully context-aware architecture that captures the entire available past context for each pixel using Parallel Multi-Dimensional LSTM units and aggregates it using blending units. Our model outperforms a strong baseline network of 20 recurrent convolutional layers and yields state-of-the-art performance for next step prediction on three challenging real-world video datasets: Human 3.6M, Caltech Pedestrian, and UCF-101. Moreover, it does so with fewer parameters than several recently proposed models, and does not rely on deep convolutional networks, multi-scale architectures, separation of background and foreground modeling, motion flow learning, or adversarial training. These results highlight that full awareness of past context is of crucial importance for video prediction.



### Neural Stain-Style Transfer Learning using GAN for Histopathological Images
- **Arxiv ID**: http://arxiv.org/abs/1710.08543v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1710.08543v2)
- **Published**: 2017-10-23 23:02:25+00:00
- **Updated**: 2017-10-25 11:15:25+00:00
- **Authors**: Hyungjoo Cho, Sungbin Lim, Gunho Choi, Hyunseok Min
- **Comment**: 10 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: Performance of data-driven network for tumor classification varies with stain-style of histopathological images. This article proposes the stain-style transfer (SST) model based on conditional generative adversarial networks (GANs) which is to learn not only the certain color distribution but also the corresponding histopathological pattern. Our model considers feature-preserving loss in addition to well-known GAN loss. Consequently our model does not only transfers initial stain-styles to the desired one but also prevent the degradation of tumor classifier on transferred images. The model is examined using the CAMELYON16 dataset.



