# Arxiv Papers in cs.CV on 2017-10-06
### FPGA based Parallelized Architecture of Efficient Graph based Image Segmentation Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1710.02260v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1710.02260v1)
- **Published**: 2017-10-06 02:48:26+00:00
- **Updated**: 2017-10-06 02:48:26+00:00
- **Authors**: Roopal Nahar, Akanksha Baranwal, K. Madhava Krishna
- **Comment**: 6 pages, 10 figures, 4 tables
- **Journal**: None
- **Summary**: Efficient and real time segmentation of color images has a variety of importance in many fields of computer vision such as image compression, medical imaging, mapping and autonomous navigation. Being one of the most computationally expensive operation, it is usually done through software imple- mentation using high-performance processors. In robotic systems, however, with the constrained platform dimensions and the need for portability, low power consumption and simultaneously the need for real time image segmentation, we envision hardware parallelism as the way forward to achieve higher acceleration. Field-programmable gate arrays (FPGAs) are among the best suited for this task as they provide high computing power in a small physical area. They exceed the computing speed of software based implementations by breaking the paradigm of sequential execution and accomplishing more per clock cycle operations by enabling hardware level parallelization at an architectural level. In this paper, we propose three novel architectures of a well known Efficient Graph based Image Segmentation algorithm. These proposed implementations optimizes time and power consumption when compared to software implementations. The hybrid design proposed, has notable furtherance of acceleration capabilities delivering atleast 2X speed gain over other implemen- tations, which henceforth allows real time image segmentation that can be deployed on Mobile Robotic systems.



### Eigen-Distortions of Hierarchical Representations
- **Arxiv ID**: http://arxiv.org/abs/1710.02266v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02266v3)
- **Published**: 2017-10-06 03:22:20+00:00
- **Updated**: 2018-02-01 16:45:02+00:00
- **Authors**: Alexander Berardino, Johannes Ballé, Valero Laparra, Eero P. Simoncelli
- **Comment**: Selected for oral presentation at NIPS 2017
- **Journal**: Adv. Neural Information Processing Systems (NIPS), Dec 2017, vol
  30, pp 3530-3539
- **Summary**: We develop a method for comparing hierarchical image representations in terms of their ability to explain perceptual sensitivity in humans. Specifically, we utilize Fisher information to establish a model-derived prediction of sensitivity to local perturbations of an image. For a given image, we compute the eigenvectors of the Fisher information matrix with largest and smallest eigenvalues, corresponding to the model-predicted most- and least-noticeable image distortions, respectively. For human subjects, we then measure the amount of each distortion that can be reliably detected when added to the image. We use this method to test the ability of a variety of representations to mimic human perceptual sensitivity. We find that the early layers of VGG16, a deep neural network optimized for object recognition, provide a better match to human perception than later layers, and a better match than a 4-stage convolutional neural network (CNN) trained on a database of human ratings of distorted image quality. On the other hand, we find that simple models of early visual processing, incorporating one or more stages of local gain control, trained on the same database of distortion ratings, provide substantially better predictions of human sensitivity than either the CNN, or any combination of layers of VGG16.



### Efficient K-Shot Learning with Regularized Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.02277v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.02277v1)
- **Published**: 2017-10-06 05:07:28+00:00
- **Updated**: 2017-10-06 05:07:28+00:00
- **Authors**: Donghyun Yoo, Haoqi Fan, Vishnu Naresh Boddeti, Kris M. Kitani
- **Comment**: None
- **Journal**: None
- **Summary**: Feature representations from pre-trained deep neural networks have been known to exhibit excellent generalization and utility across a variety of related tasks. Fine-tuning is by far the simplest and most widely used approach that seeks to exploit and adapt these feature representations to novel tasks with limited data. Despite the effectiveness of fine-tuning, itis often sub-optimal and requires very careful optimization to prevent severe over-fitting to small datasets. The problem of sub-optimality and over-fitting, is due in part to the large number of parameters used in a typical deep convolutional neural network. To address these problems, we propose a simple yet effective regularization method for fine-tuning pre-trained deep networks for the task of k-shot learning. To prevent overfitting, our key strategy is to cluster the model parameters while ensuring intra-cluster similarity and inter-cluster diversity of the parameters, effectively regularizing the dimensionality of the parameter search space. In particular, we identify groups of neurons within each layer of a deep network that shares similar activation patterns. When the network is to be fine-tuned for a classification task using only k examples, we propagate a single gradient to all of the neuron parameters that belong to the same group. The grouping of neurons is non-trivial as neuron activations depend on the distribution of the input data. To efficiently search for optimal groupings conditioned on the input data, we propose a reinforcement learning search strategy using recurrent networks to learn the optimal group assignments for each network layer. Experimental results show that our method can be easily applied to several popular convolutional neural networks and improve upon other state-of-the-art fine-tuning based k-shot learning strategies by more than10%



### Deep Convolutional Neural Networks as Generic Feature Extractors
- **Arxiv ID**: http://arxiv.org/abs/1710.02286v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1710.02286v1)
- **Published**: 2017-10-06 06:42:11+00:00
- **Updated**: 2017-10-06 06:42:11+00:00
- **Authors**: Lars Hertel, Erhardt Barth, Thomas Käster, Thomas Martinetz
- **Comment**: 4 pages, accepted version for publication in Proceedings of the IEEE
  International Joint Conference on Neural Networks (IJCNN), July 2015,
  Killarney, Ireland
- **Journal**: None
- **Summary**: Recognizing objects in natural images is an intricate problem involving multiple conflicting objectives. Deep convolutional neural networks, trained on large datasets, achieve convincing results and are currently the state-of-the-art approach for this task. However, the long time needed to train such deep networks is a major drawback. We tackled this problem by reusing a previously trained network. For this purpose, we first trained a deep convolutional network on the ILSVRC2012 dataset. We then maintained the learned convolution kernels and only retrained the classification part on different datasets. Using this approach, we achieved an accuracy of 67.68 % on CIFAR-100, compared to the previous state-of-the-art result of 65.43 %. Furthermore, our findings indicate that convolutional networks are able to learn generic feature extractors that can be used for different tasks.



### Detecting the Moment of Completion: Temporal Models for Localising Action Completion
- **Arxiv ID**: http://arxiv.org/abs/1710.02310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02310v1)
- **Published**: 2017-10-06 08:30:05+00:00
- **Updated**: 2017-10-06 08:30:05+00:00
- **Authors**: Farnoosh Heidarivincheh, Majid Mirmehdi, Dima Damen
- **Comment**: None
- **Journal**: None
- **Summary**: Action completion detection is the problem of modelling the action's progression towards localising the moment of completion - when the action's goal is confidently considered achieved. In this work, we assess the ability of two temporal models, namely Hidden Markov Models (HMM) and Long-Short Term Memory (LSTM), to localise completion for six object interactions: switch, plug, open, pull, pick and drink. We use a supervised approach, where annotations of pre-completion and post-completion frames are available per action, and fine-tuned CNN features are used to train temporal models. Tested on the Action-Completion-2016 dataset, we detect completion within 10 frames of annotations for ~75% of completed action sequences using both temporal models. Results show that fine-tuned CNN features outperform hand-crafted features for localisation, and that observing incomplete instances is necessary when incomplete sequences are also present in the test set.



### A Multiscale Patch Based Convolutional Network for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1710.02316v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1710.02316v1)
- **Published**: 2017-10-06 09:04:28+00:00
- **Updated**: 2017-10-06 09:04:28+00:00
- **Authors**: Jean Stawiaski
- **Comment**: None
- **Journal**: None
- **Summary**: This article presents a multiscale patch based convolutional neural network for the automatic segmentation of brain tumors in multi-modality 3D MR images. We use multiscale deep supervision and inputs to train a convolutional network. We evaluate the effectiveness of the proposed approach on the BRATS 2017 segmentation challenge where we obtained dice scores of 0.755, 0.900, 0.782 and 95% Hausdorff distance of 3.63mm, 4.10mm, and 6.81mm for enhanced tumor core, whole tumor and tumor core respectively.



### Human Pose Regression by Combining Indirect Part Detection and Contextual Information
- **Arxiv ID**: http://arxiv.org/abs/1710.02322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02322v1)
- **Published**: 2017-10-06 09:27:44+00:00
- **Updated**: 2017-10-06 09:27:44+00:00
- **Authors**: Diogo C. Luvizon, Hedi Tabia, David Picard
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an end-to-end trainable regression approach for human pose estimation from still images. We use the proposed Soft-argmax function to convert feature maps directly to joint coordinates, resulting in a fully differentiable framework. Our method is able to learn heat maps representations indirectly, without additional steps of artificial ground truth generation. Consequently, contextual information can be included to the pose predictions in a seamless way. We evaluated our method on two very challenging datasets, the Leeds Sports Poses (LSP) and the MPII Human Pose datasets, reaching the best performance among all the existing regression methods and comparable results to the state-of-the-art detection based approaches.



### Projection Based Weight Normalization for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.02338v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.02338v1)
- **Published**: 2017-10-06 10:24:38+00:00
- **Updated**: 2017-10-06 10:24:38+00:00
- **Authors**: Lei Huang, Xianglong Liu, Bo Lang, Bo Li
- **Comment**: 12 pages, 3 figures
- **Journal**: None
- **Summary**: Optimizing deep neural networks (DNNs) often suffers from the ill-conditioned problem. We observe that the scaling-based weight space symmetry property in rectified nonlinear network will cause this negative effect. Therefore, we propose to constrain the incoming weights of each neuron to be unit-norm, which is formulated as an optimization problem over Oblique manifold. A simple yet efficient method referred to as projection based weight normalization (PBWN) is also developed to solve this problem. PBWN executes standard gradient updates, followed by projecting the updated weight back to Oblique manifold. This proposed method has the property of regularization and collaborates well with the commonly used batch normalization technique. We conduct comprehensive experiments on several widely-used image datasets including CIFAR-10, CIFAR-100, SVHN and ImageNet for supervised learning over the state-of-the-art convolutional neural networks, such as Inception, VGG and residual networks. The results show that our method is able to improve the performance of DNNs with different architectures consistently. We also apply our method to Ladder network for semi-supervised learning on permutation invariant MNIST dataset, and our method outperforms the state-of-the-art methods: we obtain test errors as 2.52%, 1.06%, and 0.91% with only 20, 50, and 100 labeled samples, respectively.



### End-to-end Driving via Conditional Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1710.02410v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1710.02410v2)
- **Published**: 2017-10-06 14:00:31+00:00
- **Updated**: 2018-03-02 16:43:34+00:00
- **Authors**: Felipe Codevilla, Matthias Müller, Antonio López, Vladlen Koltun, Alexey Dosovitskiy
- **Comment**: Published at the International Conference on Robotics and Automation
  (ICRA), 2018
- **Journal**: None
- **Summary**: Deep networks trained on demonstrations of human driving have learned to follow roads and avoid obstacles. However, driving policies trained via imitation learning cannot be controlled at test time. A vehicle trained end-to-end to imitate an expert cannot be guided to take a specific turn at an upcoming intersection. This limits the utility of such systems. We propose to condition imitation learning on high-level command input. At test time, the learned driving policy functions as a chauffeur that handles sensorimotor coordination but continues to respond to navigational commands. We evaluate different architectures for conditional imitation learning in vision-based driving. We conduct experiments in realistic three-dimensional simulations of urban driving and on a 1/5 scale robotic truck that is trained to drive in a residential area. Both systems drive based on visual input yet remain responsive to high-level navigational commands. The supplementary video can be viewed at https://youtu.be/cFtnflNe5fM



### Contrastive Learning for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1710.02534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02534v1)
- **Published**: 2017-10-06 18:00:48+00:00
- **Updated**: 2017-10-06 18:00:48+00:00
- **Authors**: Bo Dai, Dahua Lin
- **Comment**: accepted to 31st Conference on Neural Information Processing Systems
  (NIPS 2017), Long Beach, CA, USA
- **Journal**: None
- **Summary**: Image captioning, a popular topic in computer vision, has achieved substantial progress in recent years. However, the distinctiveness of natural descriptions is often overlooked in previous work. It is closely related to the quality of captions, as distinctive captions are more likely to describe images with their unique aspects. In this work, we propose a new learning method, Contrastive Learning (CL), for image captioning. Specifically, via two constraints formulated on top of a reference model, the proposed method can encourage distinctiveness, while maintaining the overall quality of the generated captions. We tested our method on two challenging datasets, where it improves the baseline model by significant margins. We also showed in our studies that the proposed method is generic and can be used for models with various structures.



### CAMREP- Concordia Action and Motion Repository
- **Arxiv ID**: http://arxiv.org/abs/1710.02566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02566v1)
- **Published**: 2017-10-06 19:42:13+00:00
- **Updated**: 2017-10-06 19:42:13+00:00
- **Authors**: Kaustubha Mendhurwar, Qing Gu, Vladimir de la Cruz, Sudhir Mudur, Tiberiu Popa
- **Comment**: None
- **Journal**: None
- **Summary**: Action recognition, motion classification, gait analysis and synthesis are fundamental problems in a number of fields such as computer graphics, bio-mechanics and human computer interaction that generate a large body of research. This type of data is complex because it is inherently multidimensional and has multiple modalities such as video, motion capture data, accelerometer data, etc. While some of this data, such as monocular video are easy to acquire, others are much more difficult and expensive such as motion capture data or multi-view video. This creates a large barrier of entry in the research community for data driven research. We have embarked on creating a new large repository of motion and action data (CAMREP) consisting of several motion and action databases. What makes this database unique is that we use a variety of modalities, enabling multi-modal analysis. Presently, the size of datasets varies with some having a large number of subjects while others having smaller numbers. We have also acquired long capture sequences in a number of cases, making some datasets rather large.



### Bag-Level Aggregation for Multiple Instance Active Learning in Instance Classification Problems
- **Arxiv ID**: http://arxiv.org/abs/1710.02584v1
- **DOI**: 10.1109/tnnls.2018.2869164
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02584v1)
- **Published**: 2017-10-06 20:58:15+00:00
- **Updated**: 2017-10-06 20:58:15+00:00
- **Authors**: Marc-André Carbonneau, Eric Granger, Ghyslain Gagnon
- **Comment**: None
- **Journal**: None
- **Summary**: A growing number of applications, e.g. video surveillance and medical image analysis, require training recognition systems from large amounts of weakly annotated data while some targeted interactions with a domain expert are allowed to improve the training process. In such cases, active learning (AL) can reduce labeling costs for training a classifier by querying the expert to provide the labels of most informative instances. This paper focuses on AL methods for instance classification problems in multiple instance learning (MIL), where data is arranged into sets, called bags, that are weakly labeled. Most AL methods focus on single instance learning problems. These methods are not suitable for MIL problems because they cannot account for the bag structure of data. In this paper, new methods for bag-level aggregation of instance informativeness are proposed for multiple instance active learning (MIAL). The \textit{aggregated informativeness} method identifies the most informative instances based on classifier uncertainty, and queries bags incorporating the most information. The other proposed method, called \textit{cluster-based aggregative sampling}, clusters data hierarchically in the instance space. The informativeness of instances is assessed by considering bag labels, inferred instance labels, and the proportion of labels that remain to be discovered in clusters. Both proposed methods significantly outperform reference methods in extensive experiments using benchmark data from several application domains. Results indicate that using an appropriate strategy to address MIAL problems yields a significant reduction in the number of queries needed to achieve the same level of performance as single instance AL methods.



