# Arxiv Papers in cs.CV on 2017-10-16
### A multi-branch convolutional neural network for detecting double JPEG compression
- **Arxiv ID**: http://arxiv.org/abs/1710.05477v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1710.05477v1)
- **Published**: 2017-10-16 02:54:57+00:00
- **Updated**: 2017-10-16 02:54:57+00:00
- **Authors**: Bin Li, Hu Luo, Haoxin Zhang, Shunquan Tan, Zhongzhou Ji
- **Comment**: This paper was accepted by the 3rd International Workshop on Digital
  Crime and Forensics (IWDCF2017)
- **Journal**: None
- **Summary**: Detection of double JPEG compression is important to forensics analysis. A few methods were proposed based on convolutional neural networks (CNNs). These methods only accept inputs from pre-processed data, such as histogram features and/or decompressed images. In this paper, we present a CNN solution by using raw DCT (discrete cosine transformation) coefficients from JPEG images as input. Considering the DCT sub-band nature in JPEG, a multiple-branch CNN structure has been designed to reveal whether a JPEG format image has been doubly compressed. Comparing with previous methods, the proposed method provides end-to-end detection capability. Extensive experiments have been carried out to demonstrate the effectiveness of the proposed network.



### The Feeling of Success: Does Touch Sensing Help Predict Grasp Outcomes?
- **Arxiv ID**: http://arxiv.org/abs/1710.05512v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.05512v1)
- **Published**: 2017-10-16 05:32:38+00:00
- **Updated**: 2017-10-16 05:32:38+00:00
- **Authors**: Roberto Calandra, Andrew Owens, Manu Upadhyaya, Wenzhen Yuan, Justin Lin, Edward H. Adelson, Sergey Levine
- **Comment**: 10 pages, accepted at the 1st Annual Conference on Robot Learning
  (CoRL)
- **Journal**: None
- **Summary**: A successful grasp requires careful balancing of the contact forces. Deducing whether a particular grasp will be successful from indirect measurements, such as vision, is therefore quite challenging, and direct sensing of contacts through touch sensing provides an appealing avenue toward more successful and consistent robotic grasping. However, in order to fully evaluate the value of touch sensing for grasp outcome prediction, we must understand how touch sensing can influence outcome prediction accuracy when combined with other modalities. Doing so using conventional model-based techniques is exceptionally difficult. In this work, we investigate the question of whether touch sensing aids in predicting grasp outcomes within a multimodal sensing framework that combines vision and touch. To that end, we collected more than 9,000 grasping trials using a two-finger gripper equipped with GelSight high-resolution tactile sensors on each finger, and evaluated visuo-tactile deep neural network models to directly predict grasp outcomes from either modality individually, and from both modalities together. Our experimental results indicate that incorporating tactile readings substantially improve grasping performance.



### Entanglement Entropy of Target Functions for Image Classification and Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1710.05520v1
- **DOI**: None
- **Categories**: **cs.LG**, cond-mat.str-el, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.05520v1)
- **Published**: 2017-10-16 05:54:38+00:00
- **Updated**: 2017-10-16 05:54:38+00:00
- **Authors**: Ya-Hui Zhang
- **Comment**: 9pages, 1 figures
- **Journal**: None
- **Summary**: The success of deep convolutional neural network (CNN) in computer vision especially image classification problems requests a new information theory for function of image, instead of image itself. In this article, after establishing a deep mathematical connection between image classification problem and quantum spin model, we propose to use entanglement entropy, a generalization of classical Boltzmann-Shannon entropy, as a powerful tool to characterize the information needed for representation of general function of image. We prove that there is a sub-volume-law bound for entanglement entropy of target functions of reasonable image classification problems. Therefore target functions of image classification only occupy a small subspace of the whole Hilbert space. As a result, a neural network with polynomial number of parameters is efficient for representation of such target functions of image. The concept of entanglement entropy can also be useful to characterize the expressive power of different neural networks. For example, we show that to maintain the same expressive power, number of channels $D$ in a convolutional neural network should scale with the number of convolution layers $n_c$ as $D\sim D_0^{\frac{1}{n_c}}$. Therefore, deeper CNN with large $n_c$ is more efficient than shallow ones.



### What is (missing or wrong) in the scene? A Hybrid Deep Boltzmann Machine For Contextualized Scene Modeling
- **Arxiv ID**: http://arxiv.org/abs/1710.05664v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1710.05664v2)
- **Published**: 2017-10-16 12:54:52+00:00
- **Updated**: 2018-08-20 13:53:11+00:00
- **Authors**: İlker Bozcan, Yağmur Oymak, İdil Zeynep Alemdar, Sinan Kalkan
- **Comment**: 6 pages, 7 figures, submitted to ICRA 2018
- **Journal**: None
- **Summary**: Scene models allow robots to reason about what is in the scene, what else should be in it, and what should not be in it. In this paper, we propose a hybrid Boltzmann Machine (BM) for scene modeling where relations between objects are integrated. To be able to do that, we extend BM to include tri-way edges between visible (object) nodes and make the network to share the relations across different objects. We evaluate our method against several baseline models (Deep Boltzmann Machines, and Restricted Boltzmann Machines) on a scene classification dataset, and show that it performs better in several scene reasoning tasks.



### Searching for Activation Functions
- **Arxiv ID**: http://arxiv.org/abs/1710.05941v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1710.05941v2)
- **Published**: 2017-10-16 18:05:45+00:00
- **Updated**: 2017-10-27 17:45:21+00:00
- **Authors**: Prajit Ramachandran, Barret Zoph, Quoc V. Le
- **Comment**: Updated version of "Swish: a Self-Gated Activation Function"
- **Journal**: None
- **Summary**: The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, $f(x) = x \cdot \text{sigmoid}(\beta x)$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\% for Mobile NASNet-A and 0.6\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.



### Isointense Infant Brain Segmentation with a Hyper-dense Connected Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1710.05956v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05956v4)
- **Published**: 2017-10-16 18:43:45+00:00
- **Updated**: 2019-03-02 02:44:54+00:00
- **Authors**: Jose Dolz, Ismail Ben Ayed, Jing Yuan, Christian Desrosiers
- **Comment**: Oral presentation at ISBI 2018. The last version of the paper is
  updated with the reference of the iSEG comparative study, published in 2019
  at IEEE TMI
- **Journal**: None
- **Summary**: Neonatal brain segmentation in magnetic resonance (MR) is a challenging problem due to poor image quality and low contrast between white and gray matter regions. Most existing approaches for this problem are based on multi-atlas label fusion strategies, which are time-consuming and sensitive to registration errors. As alternative to these methods, we propose a hyper-densely connected 3D convolutional neural network that employs MR-T1 and T2 images as input, which are processed independently in two separated paths. An important difference with previous densely connected networks is the use of direct connections between layers from the same and different paths. Adopting such dense connectivity helps the learning process by including deep supervision and improving gradient flow. We evaluated our approach on data from the MICCAI Grand Challenge on 6-month infant Brain MRI Segmentation (iSEG), obtaining very competitive results. Among 21 teams, our approach ranked first or second in most metrics, translating into a state-of-the-art performance.



### Gradient-free Policy Architecture Search and Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1710.05958v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.05958v1)
- **Published**: 2017-10-16 18:47:35+00:00
- **Updated**: 2017-10-16 18:47:35+00:00
- **Authors**: Sayna Ebrahimi, Anna Rohrbach, Trevor Darrell
- **Comment**: Accepted in Conference on Robot Learning, 2017
- **Journal**: None
- **Summary**: We develop a method for policy architecture search and adaptation via gradient-free optimization which can learn to perform autonomous driving tasks. By learning from both demonstration and environmental reward we develop a model that can learn with relatively few early catastrophic failures. We first learn an architecture of appropriate complexity to perceive aspects of world state relevant to the expert demonstration, and then mitigate the effect of domain-shift during deployment by adapting a policy demonstrated in a source domain to rewards obtained in a target environment. We show that our approach allows safer learning than baseline methods, offering a reduced cumulative crash metric over the agent's lifetime as it learns to drive in a realistic simulated environment.



### Pushing the envelope in deep visual recognition for mobile platforms
- **Arxiv ID**: http://arxiv.org/abs/1710.05982v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1710.05982v2)
- **Published**: 2017-10-16 20:11:23+00:00
- **Updated**: 2017-11-22 11:37:48+00:00
- **Authors**: Lorenzo Alvino
- **Comment**: None
- **Journal**: None
- **Summary**: Image classification is the task of assigning to an input image a label from a fixed set of categories. One of its most important applicative fields is that of robotics, in particular the needing of a robot to be aware of what's around and the consequent exploitation of that information as a benefit for its tasks. In this work we consider the problem of a robot that enters a new environment and wants to understand visual data coming from its camera, so to extract knowledge from them. As main novelty we want to overcome the needing of a physical robot, as it could be expensive and unhandy, so to hopefully enhance, speed up and ease the research in this field. That's why we propose to develop an application for a mobile platform that wraps several deep visual recognition tasks. First we deal with a simple Image classification, testing a model obtained from an AlexNet trained on the ILSVRC 2012 dataset. Several photo settings are considered to better understand which factors affect most the quality of classification. For the same purpose we are interested to integrate the classification task with an extra module dealing with segmentation of the object inside the image. In particular we propose a technique for extracting the object shape and moving out all the background, so to focus the classification only on the region occupied by the object. Another significant task that is included is that of object discovery. Its purpose is to simulate the situation in which the robot needs a certain object to complete one of its activities. It starts searching for what it needs by looking around and trying to understand the location of the object by scanning the surrounding environment. Finally we provide a tool for dealing with the creation of customized task-specific databases, meant to better suit to one's needing in a particular vision task.



### Volumetric Data Exploration with Machine Learning-Aided Visualization in Neutron Science
- **Arxiv ID**: http://arxiv.org/abs/1710.05994v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05994v3)
- **Published**: 2017-10-16 20:32:10+00:00
- **Updated**: 2018-09-24 13:37:30+00:00
- **Authors**: Yawei Hui, Yaohua Liu
- **Comment**: 14 pages, 7 figures; the Computer Vision Conference (CVC), Las Vegas,
  Nevada, 2019; accepted
- **Journal**: None
- **Summary**: Recent advancements in neutron and X-ray sources, instrumentation and data collection modes have significantly increased the experimental data size (which could easily contain 10$^{8}$ -- 10$^{10}$ data points), so that conventional volumetric visualization approaches become inefficient for both still imaging and interactive OpenGL rendition in a 3D setting. We introduce a new approach based on the unsupervised machine learning algorithm, Density-Based Spatial Clustering of Applications with Noise (DBSCAN), to efficiently analyze and visualize large volumetric datasets. Here we present two examples of analyzing and visualizing datasets from the diffuse scattering experiment of a single crystal sample and the tomographic reconstruction of a neutron scanning of a turbine blade. We found that by using the intensity as the weighting factor in the clustering process, DBSCAN becomes very effective in de-noising and feature/boundary detection, and thus enables better visualization of the hierarchical internal structures of the neutron scattering data.



