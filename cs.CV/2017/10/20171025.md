# Arxiv Papers in cs.CV on 2017-10-25
### Crop Planning using Stochastic Visual Optimization
- **Arxiv ID**: http://arxiv.org/abs/1710.09077v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1710.09077v1)
- **Published**: 2017-10-25 05:16:28+00:00
- **Updated**: 2017-10-25 05:16:28+00:00
- **Authors**: Gunjan Sehgal, Bindu Gupta, Kaushal Paneri, Karamjit Singh, Geetika Sharma, Gautam Shroff
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: As the world population increases and arable land decreases, it becomes vital to improve the productivity of the agricultural land available. Given the weather and soil properties, farmers need to take critical decisions such as which seed variety to plant and in what proportion, in order to maximize productivity. These decisions are irreversible and any unusual behavior of external factors, such as weather, can have catastrophic impact on the productivity of crop. A variety which is highly desirable to a farmer might be unavailable or in short supply, therefore, it is very critical to evaluate which variety or varieties are more likely to be chosen by farmers from a growing region in order to meet demand. In this paper, we present our visual analytics tool, ViSeed, showcased on the data given in Syngenta 2016 crop data challenge 1 . This tool helps to predict optimal soybean seed variety or mix of varieties in appropriate proportions which is more likely to be chosen by farmers from a growing region. It also allows to analyse solutions generated from our approach and helps in the decision making process by providing insightful visualizations



### Compressive Online Robust Principal Component Analysis with Optical Flow for Video Foreground-Background Separation
- **Arxiv ID**: http://arxiv.org/abs/1710.09160v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1710.09160v1)
- **Published**: 2017-10-25 10:39:47+00:00
- **Updated**: 2017-10-25 10:39:47+00:00
- **Authors**: Srivatsa Prativadibhayankaram, Huynh Van Luong, Thanh-Ha Le, Andre Kaup
- **Comment**: preprint accepted
- **Journal**: None
- **Summary**: In the context of online Robust Principle Component Analysis (RPCA) for the video foreground-background separation, we propose a compressive online RPCA with optical flow that separates recursively a sequence of frames into sparse (foreground) and low-rank (background) components. Our method considers a small set of measurements taken per data vector (frame), which is different from conventional batch RPCA, processing all the data directly. The proposed method also incorporates multiple prior information, namely previous foreground and background frames, to improve the separation and then updates the prior information for the next frame. Moreover, the foreground prior frames are improved by estimating motions between the previous foreground frames using optical flow and compensating the motions to achieve higher quality foreground prior. The proposed method is applied to online video foreground and background separation from compressive measurements. The visual and quantitative results show that our method outperforms the existing methods.



### Anatomical labeling of brain CT scan anomalies using multi-context nearest neighbor relation networks
- **Arxiv ID**: http://arxiv.org/abs/1710.09180v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.09180v2)
- **Published**: 2017-10-25 11:42:36+00:00
- **Updated**: 2018-01-22 14:48:26+00:00
- **Authors**: Srikrishna Varadarajan, Muktabh Mayank Srivastava, Monika Grewal, Pulkit Kumar
- **Comment**: Accepted as a one page abstract at IEEE International Symposium on
  Biomedical Imaging (ISBI), 2018
- **Journal**: None
- **Summary**: This work is an endeavor to develop a deep learning methodology for automated anatomical labeling of a given region of interest (ROI) in brain computed tomography (CT) scans. We combine both local and global context to obtain a representation of the ROI. We then use Relation Networks (RNs) to predict the corresponding anatomy of the ROI based on its relationship score for each class. Further, we propose a novel strategy employing nearest neighbors approach for training RNs. We train RNs to learn the relationship of the target ROI with the joint representation of its nearest neighbors in each class instead of all data-points in each class. The proposed strategy leads to better training of RNs along with increased performance as compared to training baseline RN network.



### Biometrics-as-a-Service: A Framework to Promote Innovative Biometric Recognition in the Cloud
- **Arxiv ID**: http://arxiv.org/abs/1710.09183v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.09183v1)
- **Published**: 2017-10-25 11:46:52+00:00
- **Updated**: 2017-10-25 11:46:52+00:00
- **Authors**: Veeru Talreja, Terry Ferrett, Matthew C. Valenti, Arun Ross
- **Comment**: None
- **Journal**: None
- **Summary**: Biometric recognition, or simply biometrics, is the use of biological attributes such as face, fingerprints or iris in order to recognize an individual in an automated manner. A key application of biometrics is authentication; i.e., using said biological attributes to provide access by verifying the claimed identity of an individual. This paper presents a framework for Biometrics-as-a-Service (BaaS) that performs biometric matching operations in the cloud, while relying on simple and ubiquitous consumer devices such as smartphones. Further, the framework promotes innovation by providing interfaces for a plurality of software developers to upload their matching algorithms to the cloud. When a biometric authentication request is submitted, the system uses a criteria to automatically select an appropriate matching algorithm. Every time a particular algorithm is selected, the corresponding developer is rendered a micropayment. This creates an innovative and competitive ecosystem that benefits both software developers and the consumers. As a case study, we have implemented the following: (a) an ocular recognition system using a mobile web interface providing user access to a biometric authentication service, and (b) a Linux-based virtual machine environment used by software developers for algorithm development and submission.



### Supervised Classification: Quite a Brief Overview
- **Arxiv ID**: http://arxiv.org/abs/1710.09230v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.09230v1)
- **Published**: 2017-10-25 13:42:40+00:00
- **Updated**: 2017-10-25 13:42:40+00:00
- **Authors**: Marco Loog
- **Comment**: None
- **Journal**: None
- **Summary**: The original problem of supervised classification considers the task of automatically assigning objects to their respective classes on the basis of numerical measurements derived from these objects. Classifiers are the tools that implement the actual functional mapping from these measurements---also called features or inputs---to the so-called class label---or output. The fields of pattern recognition and machine learning study ways of constructing such classifiers. The main idea behind supervised methods is that of learning from examples: given a number of example input-output relations, to what extent can the general mapping be learned that takes any new and unseen feature vector to its correct class? This chapter provides a basic introduction to the underlying ideas of how to come to a supervised classification problem. In addition, it provides an overview of some specific classification techniques, delves into the issues of object representation and classifier evaluation, and (very) briefly covers some variations on the basic supervised classification task that may also be of interest to the practitioner.



### Automated cardiovascular magnetic resonance image analysis with fully convolutional networks
- **Arxiv ID**: http://arxiv.org/abs/1710.09289v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09289v4)
- **Published**: 2017-10-25 15:06:15+00:00
- **Updated**: 2018-05-22 11:46:53+00:00
- **Authors**: Wenjia Bai, Matthew Sinclair, Giacomo Tarroni, Ozan Oktay, Martin Rajchl, Ghislain Vaillant, Aaron M. Lee, Nay Aung, Elena Lukaschuk, Mihir M. Sanghvi, Filip Zemrak, Kenneth Fung, Jose Miguel Paiva, Valentina Carapella, Young Jin Kim, Hideaki Suzuki, Bernhard Kainz, Paul M. Matthews, Steffen E. Petersen, Stefan K. Piechnik, Stefan Neubauer, Ben Glocker, Daniel Rueckert
- **Comment**: Accepted for publication by Journal of Cardiovascular Magnetic
  Resonance
- **Journal**: None
- **Summary**: Cardiovascular magnetic resonance (CMR) imaging is a standard imaging modality for assessing cardiovascular diseases (CVDs), the leading cause of death globally. CMR enables accurate quantification of the cardiac chamber volume, ejection fraction and myocardial mass, providing information for diagnosis and monitoring of CVDs. However, for years, clinicians have been relying on manual approaches for CMR image analysis, which is time consuming and prone to subjective errors. It is a major clinical challenge to automatically derive quantitative and clinically relevant information from CMR images. Deep neural networks have shown a great potential in image pattern recognition and segmentation for a variety of tasks. Here we demonstrate an automated analysis method for CMR images, which is based on a fully convolutional network (FCN). The network is trained and evaluated on a large-scale dataset from the UK Biobank, consisting of 4,875 subjects with 93,500 pixelwise annotated images. The performance of the method has been evaluated using a number of technical metrics, including the Dice metric, mean contour distance and Hausdorff distance, as well as clinically relevant measures, including left ventricle (LV) end-diastolic volume (LVEDV) and end-systolic volume (LVESV), LV mass (LVM); right ventricle (RV) end-diastolic volume (RVEDV) and end-systolic volume (RVESV). By combining FCN with a large-scale annotated dataset, the proposed automated method achieves a high performance on par with human experts in segmenting the LV and RV on short-axis CMR images and the left atrium (LA) and right atrium (RA) on long-axis CMR images.



### LOOP Descriptor: Local Optimal Oriented Pattern
- **Arxiv ID**: http://arxiv.org/abs/1710.09317v4
- **DOI**: 10.1109/LSP.2018.2817176
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09317v4)
- **Published**: 2017-10-25 16:06:13+00:00
- **Updated**: 2019-03-22 13:22:09+00:00
- **Authors**: Tapabrata Chakraborti, Brendan McCane, Steven Mills, Umapada Pal
- **Comment**: None
- **Journal**: IEEE Signal Processing Letters, 25(5): 635-639, 2018
- **Summary**: This letter introduces the LOOP binary descriptor (local optimal oriented pattern) that encodes rotation invariance into the main formulation itself. This makes any post processing stage for rotation invariance redundant and improves on both accuracy and time complexity. We consider fine-grained lepidoptera (moth/butterfly) species recognition as the representative problem since it involves repetition of localized patterns and textures that may be exploited for discrimination. We evaluate the performance of LOOP against its predecessors as well as few other popular descriptors. Besides experiments on standard benchmarks, we also introduce a new small image dataset on NZ Lepidoptera. Loop performs as well or better on all datasets evaluated compared to previous binary descriptors. The new dataset and demo code of the proposed method are to be made available through the lead author's academic webpage and GitHub.



### Real-Time Automatic Fetal Brain Extraction in Fetal MRI by Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1710.09338v1
- **DOI**: 10.1109/ISBI.2018.8363675
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09338v1)
- **Published**: 2017-10-25 16:54:44+00:00
- **Updated**: 2017-10-25 16:54:44+00:00
- **Authors**: Seyed Sadegh Mohseni Salehi, Seyed Raein Hashemi, Clemente Velasco-Annis, Abdelhakim Ouaalam, Judy A. Estroff, Deniz Erdogmus, Simon K. Warfield, Ali Gholipour
- **Comment**: This work has been submitted to ISBI 2018
- **Journal**: None
- **Summary**: Brain segmentation is a fundamental first step in neuroimage analysis. In the case of fetal MRI, it is particularly challenging and important due to the arbitrary orientation of the fetus, organs that surround the fetal head, and intermittent fetal motion. Several promising methods have been proposed but are limited in their performance in challenging cases and in real-time segmentation. We aimed to develop a fully automatic segmentation method that independently segments sections of the fetal brain in 2D fetal MRI slices in real-time. To this end, we developed and evaluated a deep fully convolutional neural network based on 2D U-net and autocontext, and compared it to two alternative fast methods based on 1) a voxelwise fully convolutional network and 2) a method based on SIFT features, random forest and conditional random field. We trained the networks with manual brain masks on 250 stacks of training images, and tested on 17 stacks of normal fetal brain images as well as 18 stacks of extremely challenging cases based on extreme motion, noise, and severely abnormal brain shape. Experimental results show that our U-net approach outperformed the other methods and achieved average Dice metrics of 96.52% and 78.83% in the normal and challenging test sets, respectively. With an unprecedented performance and a test run time of about 1 second, our network can be used to segment the fetal brain in real-time while fetal MRI slices are being acquired. This can enable real-time motion tracking, motion detection, and 3D reconstruction of fetal brain MRI.



### High Five: Improving Gesture Recognition by Embracing Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1710.09441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09441v1)
- **Published**: 2017-10-25 20:04:37+00:00
- **Updated**: 2017-10-25 20:04:37+00:00
- **Authors**: Diman Zad Tootaghaj, Adrian Sampson, Todd Mytkowicz, Kathryn S McKinley
- **Comment**: None
- **Journal**: None
- **Summary**: Sensors on mobile devices---accelerometers, gyroscopes, pressure meters, and GPS---invite new applications in gesture recognition, gaming, and fitness tracking. However, programming them remains challenging because human gestures captured by sensors are noisy. This paper illustrates that noisy gestures degrade training and classification accuracy for gesture recognition in state-of-the-art deterministic Hidden Markov Models (HMM). We introduce a new statistical quantization approach that mitigates these problems by (1) during training, producing gesture-specific codebooks, HMMs, and error models for gesture sequences; and (2) during classification, exploiting the error model to explore multiple feasible HMM state sequences. We implement classification in Uncertain<t>, a probabilistic programming system that encapsulates HMMs and error models and then automates sampling and inference in the runtime. Uncertain<T> developers directly express a choice of application-specific trade-off between recall and precision at gesture recognition time, rather than at training time. We demonstrate benefits in configurability, precision, recall, and recognition on two data sets with 25 gestures from 28 people and 4200 total gestures. Incorporating gesture error more accurately in modeling improves the average recognition rate of 20 gestures from 34\% in prior work to 62\%. Incorporating the error model during classification further improves the average gesture recognition rate to 71\%. As far as we are aware, no prior work shows how to generate an HMM error model during training and use it to improve classification rates.



### Complete 3D Scene Parsing from an RGBD Image
- **Arxiv ID**: http://arxiv.org/abs/1710.09490v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.09490v2)
- **Published**: 2017-10-25 23:04:14+00:00
- **Updated**: 2018-11-13 18:05:14+00:00
- **Authors**: Chuhang Zou, Ruiqi Guo, Zhizhong Li, Derek Hoiem
- **Comment**: Accepted to International Journal of Computer Vision (IJCV), 2018
  arXiv admin note: text overlap with arXiv:1504.02437
- **Journal**: None
- **Summary**: One major goal of vision is to infer physical models of objects, surfaces, and their layout from sensors. In this paper, we aim to interpret indoor scenes from one RGBD image. Our representation encodes the layout of orthogonal walls and the extent of objects, modeled with CAD-like 3D shapes. We parse both the visible and occluded portions of the scene and all observable objects, producing a complete 3D parse. Such a scene interpretation is useful for robotics and visual reasoning, but difficult to produce due to the well-known challenge of segmentation, the high degree of occlusion, and the diversity of objects in indoor scenes. We take a data-driven approach, generating sets of potential object regions, matching to regions in training images, and transferring and aligning associated 3D models while encouraging fit to observations and spatial consistency. We use support inference to aid interpretation and propose a retrieval scheme that uses convolutional neural networks (CNNs) to classify regions and retrieve objects with similar shapes. We demonstrate the performance of our method on our newly annotated NYUd v2 dataset with detailed 3D shapes.



