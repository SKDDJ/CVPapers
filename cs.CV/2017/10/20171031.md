# Arxiv Papers in cs.CV on 2017-10-31
### Tumor Classification and Segmentation of MR Brain Images
- **Arxiv ID**: http://arxiv.org/abs/1710.11309v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.11309v1)
- **Published**: 2017-10-31 03:12:47+00:00
- **Updated**: 2017-10-31 03:12:47+00:00
- **Authors**: Tanvi Gupta, Pranay Manocha, Tapan K. Gandhi, RK Gupta, BK Panigrahi
- **Comment**: None
- **Journal**: None
- **Summary**: The diagnosis and segmentation of tumors using any medical diagnostic tool can be challenging due to the varying nature of this pathology. Magnetic Reso- nance Imaging (MRI) is an established diagnostic tool for various diseases and disorders and plays a major role in clinical neuro-diagnosis. Supplementing this technique with automated classification and segmentation tools is gaining importance, to reduce errors and time needed to make a conclusive diagnosis. In this paper a simple three-step algorithm is proposed; (1) identification of patients that present with tumors, (2) automatic selection of abnormal slices of the patients, and (3) segmentation and detection of the tumor. Features were extracted by using discrete wavelet transform on the normalized images and classified by support vector machine (for step (1)) and random forest (for step (2)). The 400 subjects were divided in a 3:1 ratio between training and test with no overlap. This study is novel in terms of use of data, as it employed the entire T2 weighted slices as a single image for classification and a unique combination of contralateral approach with patch thresholding for segmentation, which does not require a training set or a template as is used by most segmentation studies. Using the proposed method, the tumors were segmented accurately with a classification accuracy of 95% with 100% specificity and 90% sensitivity.



### Generating Natural Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1710.11342v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.11342v2)
- **Published**: 2017-10-31 06:22:26+00:00
- **Updated**: 2018-02-23 23:28:31+00:00
- **Authors**: Zhengli Zhao, Dheeru Dua, Sameer Singh
- **Comment**: Published as a conference paper at the International Conference on
  Learning Representations (ICLR) 2018
- **Journal**: None
- **Summary**: Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.



### Spatio-temporal interaction model for crowd video analysis
- **Arxiv ID**: http://arxiv.org/abs/1710.11354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.11354v1)
- **Published**: 2017-10-31 07:24:40+00:00
- **Updated**: 2017-10-31 07:24:40+00:00
- **Authors**: Neha Bhargava, Subhasis Chaudhuri
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: We present an unsupervised approach to analyze crowd at various levels of granularity $-$ individual, group and collective. We also propose a motion model to represent the collective motion of the crowd. The model captures the spatio-temporal interaction pattern of the crowd from the trajectory data captured over a time period. Furthermore, we also propose an effective group detection algorithm that utilizes the eigenvectors of the interaction matrix of the model. We also show that the eigenvalues of the interaction matrix characterize various group activities such as being stationary, walking, splitting and approaching. The algorithm is also extended trivially to recognize individual activity. Finally, we discover the overall crowd behavior by classifying a crowd video in one of the eight categories. Since the crowd behavior is determined by its constituent groups, we demonstrate the usefulness of group level features during classification. Extensive experimentation on various datasets demonstrates a superlative performance of our algorithms over the state-of-the-art methods.



### Common Representation Learning Using Step-based Correlation Multi-Modal CNN
- **Arxiv ID**: http://arxiv.org/abs/1711.00003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00003v1)
- **Published**: 2017-10-31 07:43:34+00:00
- **Updated**: 2017-10-31 07:43:34+00:00
- **Authors**: Gaurav Bhatt, Piyush Jha, Balasubramanian Raman
- **Comment**: Accepted in Asian Conference of Pattern Recognition (ACPR-2017)
- **Journal**: None
- **Summary**: Deep learning techniques have been successfully used in learning a common representation for multi-view data, wherein the different modalities are projected onto a common subspace. In a broader perspective, the techniques used to investigate common representation learning falls under the categories of canonical correlation-based approaches and autoencoder based approaches. In this paper, we investigate the performance of deep autoencoder based methods on multi-view data. We propose a novel step-based correlation multi-modal CNN (CorrMCNN) which reconstructs one view of the data given the other while increasing the interaction between the representations at each hidden layer or every intermediate step. Finally, we evaluate the performance of the proposed model on two benchmark datasets - MNIST and XRMB. Through extensive experiments, we find that the proposed model achieves better performance than the current state-of-the-art techniques on joint common representation learning and transfer learning tasks.



### Image Patch Matching Using Convolutional Descriptors with Euclidean Distance
- **Arxiv ID**: http://arxiv.org/abs/1710.11359v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.11359v1)
- **Published**: 2017-10-31 07:48:11+00:00
- **Updated**: 2017-10-31 07:48:11+00:00
- **Authors**: Iaroslav Melekhov, Juho Kannala, Esa Rahtu
- **Comment**: The paper was published in ACCV 2016 Workshops proceedings (Workshop
  on Interpretation and Visualization of Deep Neural Nets)
- **Journal**: None
- **Summary**: In this work we propose a neural network based image descriptor suitable for image patch matching, which is an important task in many computer vision applications. Our approach is influenced by recent success of deep convolutional neural networks (CNNs) in object detection and classification tasks. We develop a model which maps the raw input patch to a low dimensional feature vector so that the distance between representations is small for similar patches and large otherwise. As a distance metric we utilize L2 norm, i.e. Euclidean distance, which is fast to evaluate and used in most popular hand-crafted descriptors, such as SIFT. According to the results, our approach outperforms state-of-the-art L2-based descriptors and can be considered as a direct replacement of SIFT. In addition, we conducted experiments with batch normalization and histogram equalization as a preprocessing method of the input data. The results confirm that these techniques further improve the performance of the proposed descriptor. Finally, we show promising preliminary results by appending our CNNs with recently proposed spatial transformer networks and provide a visualisation and interpretation of their impact.



### A Computer Vision System to Localize and Classify Wastes on the Streets
- **Arxiv ID**: http://arxiv.org/abs/1710.11374v1
- **DOI**: 10.1007/978-3-319-68345-4_18
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.11374v1)
- **Published**: 2017-10-31 08:57:23+00:00
- **Updated**: 2017-10-31 08:57:23+00:00
- **Authors**: Mohammad Saeed Rad, Andreas von Kaenel, Andre Droux, Francois Tieche, Nabil Ouerhani, Hazim Kemal Ekenel, Jean-Philippe Thiran
- **Comment**: None
- **Journal**: Liu M., Chen H., Vincze M. (eds) Computer Vision Systems. pp
  195-204. ICVS 2017. Lecture Notes in Computer Science, vol 10528. Springer,
  Cham
- **Summary**: Littering quantification is an important step for improving cleanliness of cities. When human interpretation is too cumbersome or in some cases impossible, an objective index of cleanliness could reduce the littering by awareness actions. In this paper, we present a fully automated computer vision application for littering quantification based on images taken from the streets and sidewalks. We have employed a deep learning based framework to localize and classify different types of wastes. Since there was no waste dataset available, we built our acquisition system mounted on a vehicle. Collected images containing different types of wastes. These images are then annotated for training and benchmarking the developed system. Our results on real case scenarios show accurate detection of littering on variant backgrounds.



### RGB-D-based Human Motion Recognition with Deep Learning: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1711.08362v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.08362v2)
- **Published**: 2017-10-31 09:21:23+00:00
- **Updated**: 2018-04-24 23:58:12+00:00
- **Authors**: Pichao Wang, Wanqing Li, Philip Ogunbona, Jun Wan, Sergio Escalera
- **Comment**: None
- **Journal**: None
- **Summary**: Human motion recognition is one of the most important branches of human-centered research activities. In recent years, motion recognition based on RGB-D data has attracted much attention. Along with the development in artificial intelligence, deep learning techniques have gained remarkable success in computer vision. In particular, convolutional neural networks (CNN) have achieved great success for image-based tasks, and recurrent neural networks (RNN) are renowned for sequence-based problems. Specifically, deep learning methods based on the CNN and RNN architectures have been adopted for motion recognition using RGB-D data. In this paper, a detailed overview of recent advances in RGB-D-based motion recognition is presented. The reviewed methods are broadly categorized into four groups, depending on the modality adopted for recognition: RGB-based, depth-based, skeleton-based and RGB+D-based. As a survey focused on the application of deep learning to RGB-D-based motion recognition, we explicitly discuss the advantages and limitations of existing techniques. Particularly, we highlighted the methods of encoding spatial-temporal-structural information inherent in video sequence, and discuss potential directions for future research.



### Updating the VESICLE-CNN Synapse Detector
- **Arxiv ID**: http://arxiv.org/abs/1710.11397v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.11397v1)
- **Published**: 2017-10-31 10:17:15+00:00
- **Updated**: 2017-10-31 10:17:15+00:00
- **Authors**: Andrew Warrington, Frank Wood
- **Comment**: Submitted as two side extended abstract to NIPS 2017 workshop:
  BigNeuro 2017: Analyzing brain data from nano to macroscale
- **Journal**: None
- **Summary**: We present an updated version of the VESICLE-CNN algorithm presented by Roncal et al. (2014). The original implementation makes use of a patch-based approach. This methodology is known to be slow due to repeated computations. We update this implementation to be fully convolutional through the use of dilated convolutions, recovering the expanded field of view achieved through the use of strided maxpools, but without a degradation of spatial resolution. This updated implementation performs as well as the original implementation, but with a $600\times$ speedup at test time. We release source code and data into the public domain.



### Physics-guided Neural Networks (PGNN): An Application in Lake Temperature Modeling
- **Arxiv ID**: http://arxiv.org/abs/1710.11431v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, physics.data-an, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.11431v3)
- **Published**: 2017-10-31 12:24:26+00:00
- **Updated**: 2021-09-28 17:27:33+00:00
- **Authors**: Arka Daw, Anuj Karpatne, William Watkins, Jordan Read, Vipin Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a framework for combining scientific knowledge of physics-based models with neural networks to advance scientific discovery. This framework, termed physics-guided neural networks (PGNN), leverages the output of physics-based model simulations along with observational features in a hybrid modeling setup to generate predictions using a neural network architecture. Further, this framework uses physics-based loss functions in the learning objective of neural networks to ensure that the model predictions not only show lower errors on the training set but are also scientifically consistent with the known physics on the unlabeled set. We illustrate the effectiveness of PGNN for the problem of lake temperature modeling, where physical relationships between the temperature, density, and depth of water are used to design a physics-based loss function. By using scientific knowledge to guide the construction and learning of neural networks, we are able to show that the proposed framework ensures better generalizability as well as scientific consistency of results. All the code and datasets used in this study have been made available on this link \url{https://github.com/arkadaw9/PGNN}.



### Deep Hashing with Triplet Quantization Loss
- **Arxiv ID**: http://arxiv.org/abs/1710.11445v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.11445v1)
- **Published**: 2017-10-31 13:08:42+00:00
- **Updated**: 2017-10-31 13:08:42+00:00
- **Authors**: Yuefu Zhou, Shanshan Huang, Ya Zhang, Yanfeng Wang
- **Comment**: 4 pages, to be presented at IEEE VCIP 2017
- **Journal**: None
- **Summary**: With the explosive growth of image databases, deep hashing, which learns compact binary descriptors for images, has become critical for fast image retrieval. Many existing deep hashing methods leverage quantization loss, defined as distance between the features before and after quantization, to reduce the error from binarizing features. While minimizing the quantization loss guarantees that quantization has minimal effect on retrieval accuracy, it unfortunately significantly reduces the expressiveness of features even before the quantization. In this paper, we show that the above definition of quantization loss is too restricted and in fact not necessary for maintaining high retrieval accuracy. We therefore propose a new form of quantization loss measured in triplets. The core idea of the triplet quantization loss is to learn discriminative real-valued descriptors which lead to minimal loss on retrieval accuracy after quantization. Extensive experiments on two widely used benchmark data sets of different scales, CIFAR-10 and In-shop, demonstrate that the proposed method outperforms the state-of-the-art deep hashing methods. Moreover, we show that the compact binary descriptors obtained with triplet quantization loss lead to very small performance drop after quantization.



### Clothing Retrieval with Visual Attention Model
- **Arxiv ID**: http://arxiv.org/abs/1710.11446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.11446v1)
- **Published**: 2017-10-31 13:14:59+00:00
- **Updated**: 2017-10-31 13:14:59+00:00
- **Authors**: Zhonghao Wang, Yujun Gu, Ya Zhang, Jun Zhou, Xiao Gu
- **Comment**: 4 pages, to be presented at IEEE VCIP 2017
- **Journal**: None
- **Summary**: Clothing retrieval is a challenging problem in computer vision. With the advance of Convolutional Neural Networks (CNNs), the accuracy of clothing retrieval has been significantly improved. FashionNet[1], a recent study, proposes to employ a set of artificial features in the form of landmarks for clothing retrieval, which are shown to be helpful for retrieval. However, the landmark detection module is trained with strong supervision which requires considerable efforts to obtain. In this paper, we propose a self-learning Visual Attention Model (VAM) to extract attention maps from clothing images. The VAM is further connected to a global network to form an end-to-end network structure through Impdrop connection which randomly Dropout on the feature maps with the probabilities given by the attention map. Extensive experiments on several widely used benchmark clothing retrieval data sets have demonstrated the promise of the proposed method. We also show that compared to the trivial Product connection, the Impdrop connection makes the network structure more robust when training sets of limited size are used.



### A multi-layer network based on Sparse Ternary Codes for universal vector compression
- **Arxiv ID**: http://arxiv.org/abs/1710.11510v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1710.11510v1)
- **Published**: 2017-10-31 14:33:29+00:00
- **Updated**: 2017-10-31 14:33:29+00:00
- **Authors**: Sohrab Ferdowsi, Slava Voloshynovskiy, Dimche Kostadinov
- **Comment**: Submitted to ICASSP 2018
- **Journal**: None
- **Summary**: We present the multi-layer extension of the Sparse Ternary Codes (STC) for fast similarity search where we focus on the reconstruction of the database vectors from the ternary codes. To consider the trade-offs between the compactness of the STC and the quality of the reconstructed vectors, we study the rate-distortion behavior of these codes under different setups. We show that a single-layer code cannot achieve satisfactory results at high rates. Therefore, we extend the concept of STC to multiple layers and design the ML-STC, a codebook-free system that successively refines the reconstruction of the residuals of previous layers. While the ML-STC keeps the sparse ternary structure of the single-layer STC and hence is suitable for fast similarity search in large-scale databases, we show its superior rate-distortion performance on both model-based synthetic data and public large-scale databases, as compared to several binary hashing methods.



### Deep Learning as a Mixed Convex-Combinatorial Optimization Problem
- **Arxiv ID**: http://arxiv.org/abs/1710.11573v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1710.11573v3)
- **Published**: 2017-10-31 16:42:44+00:00
- **Updated**: 2018-04-16 20:46:14+00:00
- **Authors**: Abram L. Friesen, Pedro Domingos
- **Comment**: 14 pages (9 body, 5 pages of references and appendices)
- **Journal**: In Proceedings of the International Conference on Learning
  Representations (ICLR) 2018
- **Summary**: As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning. However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn networks of them in a principled way. We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such. The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve. Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches. Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case. Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.



### Multiple Instance Hybrid Estimator for Hyperspectral Target Characterization and Sub-pixel Target Detection
- **Arxiv ID**: http://arxiv.org/abs/1710.11599v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.11599v2)
- **Published**: 2017-10-31 17:19:57+00:00
- **Updated**: 2019-03-19 03:38:53+00:00
- **Authors**: Changzhe Jiao, Chao Chen, Ronald G. McGarvey, Stephanie Bohlman, Licheng Jiao, Alina Zare
- **Comment**: None
- **Journal**: None
- **Summary**: The Multiple Instance Hybrid Estimator for discriminative target characterization from imprecisely labeled hyperspectral data is presented. In many hyperspectral target detection problems, acquiring accurately labeled training data is difficult. Furthermore, each pixel containing target is likely to be a mixture of both target and non-target signatures (i.e., sub-pixel targets), making extracting a pure prototype signature for the target class from the data extremely difficult. The proposed approach addresses these problems by introducing a data mixing model and optimizing the response of the hybrid sub-pixel detector within a multiple instance learning framework. The proposed approach iterates between estimating a set of discriminative target and non-target signatures and solving a sparse unmixing problem. After learning target signatures, a signature based detector can then be applied on test data. Both simulated and real hyperspectral target detection experiments show the proposed algorithm is effective at learning discriminative target signatures and achieves superior performance over state-of-the-art comparison algorithms.



### Whodunnit? Crime Drama as a Case for Natural Language Understanding
- **Arxiv ID**: http://arxiv.org/abs/1710.11601v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.11601v1)
- **Published**: 2017-10-31 17:27:44+00:00
- **Updated**: 2017-10-31 17:27:44+00:00
- **Authors**: Lea Frermann, Shay B. Cohen, Mirella Lapata
- **Comment**: To appear in Transactions of the Association for Computational
  Linguistics (TACL)
- **Journal**: None
- **Summary**: In this paper we argue that crime drama exemplified in television programs such as CSI:Crime Scene Investigation is an ideal testbed for approximating real-world natural language understanding and the complex inferences associated with it. We propose to treat crime drama as a new inference task, capitalizing on the fact that each episode poses the same basic question (i.e., who committed the crime) and naturally provides the answer when the perpetrator is revealed. We develop a new dataset based on CSI episodes, formalize perpetrator identification as a sequence labeling problem, and develop an LSTM-based model which learns from multi-modal data. Experimental results show that an incremental inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input.



### Medical Image Segmentation Based on Multi-Modal Convolutional Neural Network: Study on Image Fusion Schemes
- **Arxiv ID**: http://arxiv.org/abs/1711.00049v2
- **DOI**: 10.1109/ISBI.2018.8363717
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.00049v2)
- **Published**: 2017-10-31 18:37:28+00:00
- **Updated**: 2017-11-02 16:02:28+00:00
- **Authors**: Zhe Guo, Xiang Li, Heng Huang, Ning Guo, Quanzheng Li
- **Comment**: Zhe Guo and Xiang Li contribute equally to this work
- **Journal**: 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI
  2018), Washington, DC, 2018, pp. 903-907
- **Summary**: Image analysis using more than one modality (i.e. multi-modal) has been increasingly applied in the field of biomedical imaging. One of the challenges in performing the multimodal analysis is that there exist multiple schemes for fusing the information from different modalities, where such schemes are application-dependent and lack a unified framework to guide their designs. In this work we firstly propose a conceptual architecture for the image fusion schemes in supervised biomedical image analysis: fusing at the feature level, fusing at the classifier level, and fusing at the decision-making level. Further, motivated by the recent success in applying deep learning for natural image analysis, we implement the three image fusion schemes above based on the Convolutional Neural Network (CNN) with varied structures, and combined into a single framework. The proposed image segmentation framework is capable of analyzing the multi-modality images using different fusing schemes simultaneously. The framework is applied to detect the presence of soft tissue sarcoma from the combination of Magnetic Resonance Imaging (MRI), Computed Tomography (CT) and Positron Emission Tomography (PET) images. It is found from the results that while all the fusion schemes outperform the single-modality schemes, fusing at the feature level can generally achieve the best performance in terms of both accuracy and computational cost, but also suffers from the decreased robustness in the presence of large errors in any image modalities.



### Semantic Image Retrieval via Active Grounding of Visual Situations
- **Arxiv ID**: http://arxiv.org/abs/1711.00088v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00088v1)
- **Published**: 2017-10-31 20:15:49+00:00
- **Updated**: 2017-10-31 20:15:49+00:00
- **Authors**: Max H. Quinn, Erik Conser, Jordan M. Witte, Melanie Mitchell
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a novel architecture for semantic image retrieval---in particular, retrieval of instances of visual situations. Visual situations are concepts such as "a boxing match," "walking the dog," "a crowd waiting for a bus," or "a game of ping-pong," whose instantiations in images are linked more by their common spatial and semantic structure than by low-level visual similarity. Given a query situation description, our architecture---called Situate---learns models capturing the visual features of expected objects as well the expected spatial configuration of relationships among objects. Given a new image, Situate uses these models in an attempt to ground (i.e., to create a bounding box locating) each expected component of the situation in the image via an active search procedure. Situate uses the resulting grounding to compute a score indicating the degree to which the new image is judged to contain an instance of the situation. Such scores can be used to rank images in a collection as part of a retrieval system. In the preliminary study described here, we demonstrate the promise of this system by comparing Situate's performance with that of two baseline methods, as well as with a related semantic image-retrieval system based on "scene graphs."



### Data Fusion on Motion and Magnetic Sensors embedded on Mobile Devices for the Identification of Activities of Daily Living
- **Arxiv ID**: http://arxiv.org/abs/1711.07328v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1711.07328v1)
- **Published**: 2017-10-31 20:29:22+00:00
- **Updated**: 2017-10-31 20:29:22+00:00
- **Authors**: Ivan Miguel Pires, Nuno M. Garcia, Nuno Pombo, Francisco Flórez-Revuelta, Susanna Spinsante
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1711.00124,
  arXiv:1711.00104; text overlap with arXiv:1711.00096
- **Journal**: None
- **Summary**: Several types of sensors have been available in off-the-shelf mobile devices, including motion, magnetic, vision, acoustic, and location sensors. This paper focuses on the fusion of the data acquired from motion and magnetic sensors, i.e., accelerometer, gyroscope and magnetometer sensors, for the recognition of Activities of Daily Living (ADL) using pattern recognition techniques. The system developed in this study includes data acquisition, data processing, data fusion, and artificial intelligence methods. Artificial Neural Networks (ANN) are included in artificial intelligence methods, which are used in this study for the recognition of ADL. The purpose of this study is the creation of a new method using ANN for the identification of ADL, comparing three types of ANN, in order to achieve results with a reliable accuracy. The best accuracy was obtained with Deep Learning, which, after the application of the L2 regularization and normalization techniques on the sensors data, reports an accuracy of 89.51%.



### Countering Adversarial Images using Input Transformations
- **Arxiv ID**: http://arxiv.org/abs/1711.00117v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00117v3)
- **Published**: 2017-10-31 21:22:16+00:00
- **Updated**: 2018-01-25 19:04:48+00:00
- **Authors**: Chuan Guo, Mayank Rana, Moustapha Cisse, Laurens van der Maaten
- **Comment**: 12 pages, 6 figures, submitted to ICLR 2018
- **Journal**: None
- **Summary**: This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods



### Accelerated Sparse Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1711.00126v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.00126v1)
- **Published**: 2017-10-31 22:05:47+00:00
- **Updated**: 2017-10-31 22:05:47+00:00
- **Authors**: Abolfazl Hashemi, Haris Vikalo
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art algorithms for sparse subspace clustering perform spectral clustering on a similarity matrix typically obtained by representing each data point as a sparse combination of other points using either basis pursuit (BP) or orthogonal matching pursuit (OMP). BP-based methods are often prohibitive in practice while the performance of OMP-based schemes are unsatisfactory, especially in settings where data points are highly similar. In this paper, we propose a novel algorithm that exploits an accelerated variant of orthogonal least-squares to efficiently find the underlying subspaces. We show that under certain conditions the proposed algorithm returns a subspace-preserving solution. Simulation results illustrate that the proposed method compares favorably with BP-based method in terms of running time while being significantly more accurate than OMP-based schemes.



### Segmentation-by-Detection: A Cascade Network for Volumetric Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.00139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.00139v1)
- **Published**: 2017-10-31 23:04:28+00:00
- **Updated**: 2017-10-31 23:04:28+00:00
- **Authors**: Min Tang, Zichen Zhang, Dana Cobzas, Martin Jagersand, Jacob L. Jaremko
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an attention mechanism for 3D medical image segmentation. The method, named segmentation-by-detection, is a cascade of a detection module followed by a segmentation module. The detection module enables a region of interest to come to attention and produces a set of object region candidates which are further used as an attention model. Rather than dealing with the entire volume, the segmentation module distills the information from the potential region. This scheme is an efficient solution for volumetric data as it reduces the influence of the surrounding noise which is especially important for medical data with low signal-to-noise ratio. Experimental results on 3D ultrasound data of the femoral head shows superiority of the proposed method when compared with a standard fully convolutional network like the U-Net.



### A multitask deep learning model for real-time deployment in embedded systems
- **Arxiv ID**: http://arxiv.org/abs/1711.00146v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.00146v1)
- **Published**: 2017-10-31 23:59:23+00:00
- **Updated**: 2017-10-31 23:59:23+00:00
- **Authors**: Miquel Martí, Atsuto Maki
- **Comment**: 2 pages, 5 figures. Poster presentation at Swedish Symposium on Deep
  Learning SSDL2017, Stockholm, Sweden. June 20-21, 2017
- **Journal**: Swedish Symposium on Deep Learning 2017
- **Summary**: We propose an approach to Multitask Learning (MTL) to make deep learning models faster and lighter for applications in which multiple tasks need to be solved simultaneously, which is particularly useful in embedded, real-time systems. We develop a multitask model for both Object Detection and Semantic Segmentation and analyze the challenges that appear during its training. Our multitask network is 1.6x faster, lighter and uses less memory than deploying the single-task models in parallel. We conclude that MTL has the potential to give superior performance in exchange of a more complex training process that introduces challenges not present in single-task models.



