# Arxiv Papers in cs.CV on 2017-10-13
### Residual Connections Encourage Iterative Inference
- **Arxiv ID**: http://arxiv.org/abs/1710.04773v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04773v2)
- **Published**: 2017-10-13 01:39:32+00:00
- **Updated**: 2018-03-08 18:45:27+00:00
- **Authors**: Stanisław Jastrzębski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, Yoshua Bengio
- **Comment**: First two authors contributed equally. Published in ICLR 2018
- **Journal**: None
- **Summary**: Residual networks (Resnets) have become a prominent architecture in deep learning. However, a comprehensive understanding of Resnets is still a topic of ongoing research.   A recent view argues that Resnets perform iterative refinement of features. We attempt to further expose properties of this aspect. To this end, we study Resnets both analytically and empirically. We formalize the notion of iterative refinement in Resnets by showing that residual connections naturally encourage features of residual blocks to move along the negative gradient of loss as we go from one block to the next. In addition, our empirical analysis suggests that Resnets are able to perform both representation learning and iterative refinement. In general, a Resnet block tends to concentrate representation learning behavior in the first few layers while higher layers perform iterative refinement of features. Finally we observe that sharing residual layers naively leads to representation explosion and counterintuitively, overfitting, and we show that simple existing strategies can help alleviating this problem.



### Retinal Fluid Segmentation and Detection in Optical Coherence Tomography Images using Fully Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1710.04778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04778v1)
- **Published**: 2017-10-13 01:51:01+00:00
- **Updated**: 2017-10-13 01:51:01+00:00
- **Authors**: Donghuan Lu, Morgan Heisler, Sieun Lee, Gavin Ding, Marinko V. Sarunic, Mirza Faisal Beg
- **Comment**: 9 pages, 5 figures, MICCAI Retinal OCT Fluid Challenge 2017
- **Journal**: None
- **Summary**: As a non-invasive imaging modality, optical coherence tomography (OCT) can provide micrometer-resolution 3D images of retinal structures. Therefore it is commonly used in the diagnosis of retinal diseases associated with edema in and under the retinal layers. In this paper, a new framework is proposed for the task of fluid segmentation and detection in retinal OCT images. Based on the raw images and layers segmented by a graph-cut algorithm, a fully convolutional neural network was trained to recognize and label the fluid pixels. Random forest classification was performed on the segmented fluid regions to detect and reject the falsely labeled fluid regions. The leave-one-out cross validation experiments on the RETOUCH database show that our method performs well in both segmentation (mean Dice: 0.7317) and detection (mean AUC: 0.985) tasks.



### Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer's Disease using structural MR and FDG-PET images
- **Arxiv ID**: http://arxiv.org/abs/1710.04782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04782v1)
- **Published**: 2017-10-13 02:14:39+00:00
- **Updated**: 2017-10-13 02:14:39+00:00
- **Authors**: Donghuan Lu, Karteek Popuri, Weiguang Ding, Rakesh Balachandar, Mirza Faisal Beg
- **Comment**: 12 pages, 4 figures, Alzheimer's disease, deep learning, multimodal,
  early diagnosis, multiscale
- **Journal**: None
- **Summary**: Alzheimer's Disease (AD) is a progressive neurodegenerative disease. Amnestic mild cognitive impairment (MCI) is a common first symptom before the conversion to clinical impairment where the individual becomes unable to perform activities of daily living independently. Although there is currently no treatment available, the earlier a conclusive diagnosis is made, the earlier the potential for interventions to delay or perhaps even prevent progression to full-blown AD. Neuroimaging scans acquired from MRI and metabolism images obtained by FDG-PET provide in-vivo view into the structure and function (glucose metabolism) of the living brain. It is hypothesized that combining different image modalities could better characterize the change of human brain and result in a more accuracy early diagnosis of AD. In this paper, we proposed a novel framework to discriminate normal control(NC) subjects from subjects with AD pathology (AD and NC, MCI subjects convert to AD in future). Our novel approach utilizing a multimodal and multiscale deep neural network was found to deliver a 85.68\% accuracy in the prediction of subjects within 3 years to conversion. Cross validation experiments proved that it has better discrimination ability compared with results in existing published literature.



### Retinal Vasculature Segmentation Using Local Saliency Maps and Generative Adversarial Networks For Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1710.04783v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04783v3)
- **Published**: 2017-10-13 02:17:05+00:00
- **Updated**: 2018-05-21 05:24:11+00:00
- **Authors**: Dwarikanath Mahapatra, Behzad Bozorgtabar
- **Comment**: Accepted in MICCAI 2017 conference
- **Journal**: None
- **Summary**: We propose an image super resolution(ISR) method using generative adversarial networks (GANs) that takes a low resolution input fundus image and generates a high resolution super resolved (SR) image upto scaling factor of $16$. This facilitates more accurate automated image analysis, especially for small or blurred landmarks and pathologies. Local saliency maps, which define each pixel's importance, are used to define a novel saliency loss in the GAN cost function. Experimental results show the resulting SR images have perceptual quality very close to the original images and perform better than competing methods that do not weigh pixels according to their importance. When used for retinal vasculature segmentation, our SR images result in accuracy levels close to those obtained when using the original images.



### VGR-Net: A View Invariant Gait Recognition Network
- **Arxiv ID**: http://arxiv.org/abs/1710.04803v1
- **DOI**: 10.1109/ISBA.2018.8311475
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04803v1)
- **Published**: 2017-10-13 05:02:33+00:00
- **Updated**: 2017-10-13 05:02:33+00:00
- **Authors**: Daksh Thapar, Divyansh Aggarwal, Punjal Agarwal, Aditya Nigam
- **Comment**: Accepted in ISBA (IEEE International conference on Identity, Security
  and Behaviour Analysis)-2018
- **Journal**: None
- **Summary**: Biometric identification systems have become immensely popular and important because of their high reliability and efficiency. However person identification at a distance, still remains a challenging problem. Gait can be seen as an essential biometric feature for human recognition and identification. It can be easily acquired from a distance and does not require any user cooperation thus making it suitable for surveillance. But the task of recognizing an individual using gait can be adversely affected by varying view points making this task more and more challenging. Our proposed approach tackles this problem by identifying spatio-temporal features and performing extensive experimentation and training mechanism. In this paper, we propose a 3-D Convolution Deep Neural Network for person identification using gait under multiple view. It is a 2-stage network, in which we have a classification network that initially identifies the viewing point angle. After that another set of networks (one for each angle) has been trained to identify the person under a particular viewing angle. We have tested this network over CASIA-B publicly available database and have achieved state-of-the-art results. The proposed system is much more efficient in terms of time and space and performing better for almost all angles.



### WeText: Scene Text Detection under Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/1710.04826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04826v1)
- **Published**: 2017-10-13 07:08:43+00:00
- **Updated**: 2017-10-13 07:08:43+00:00
- **Authors**: Shangxuan Tian, Shijian Lu, Chongshou Li
- **Comment**: accepted by ICCV2017
- **Journal**: None
- **Summary**: The requiring of large amounts of annotated training data has become a common constraint on various deep learning systems. In this paper, we propose a weakly supervised scene text detection method (WeText) that trains robust and accurate scene text detection models by learning from unannotated or weakly annotated data. With a "light" supervised model trained on a small fully annotated dataset, we explore semi-supervised and weakly supervised learning on a large unannotated dataset and a large weakly annotated dataset, respectively. For the unsupervised learning, the light supervised model is applied to the unannotated dataset to search for more character training samples, which are further combined with the small annotated dataset to retrain a superior character detection model. For the weakly supervised learning, the character searching is guided by high-level annotations of words/text lines that are widely available and also much easier to prepare. In addition, we design an unified scene character detector by adapting regression based deep networks, which greatly relieves the error accumulation issue that widely exists in most traditional approaches. Extensive experiments across different unannotated and weakly annotated datasets show that the scene text detection performance can be clearly boosted under both scenarios, where the weakly supervised learning can achieve the state-of-the-art performance by using only 229 fully annotated scene text images.



### Filmy Cloud Removal on Satellite Imagery with Multispectral Conditional Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1710.04835v1
- **DOI**: 10.1109/CVPRW.2017.197
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04835v1)
- **Published**: 2017-10-13 08:26:13+00:00
- **Updated**: 2017-10-13 08:26:13+00:00
- **Authors**: Kenji Enomoto, Ken Sakurada, Weimin Wang, Hiroshi Fukui, Masashi Matsuoka, Ryosuke Nakamura, Nobuo Kawaguchi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a method for cloud removal from visible light RGB satellite images by extending the conditional Generative Adversarial Networks (cGANs) from RGB images to multispectral images. Satellite images have been widely utilized for various purposes, such as natural environment monitoring (pollution, forest or rivers), transportation improvement and prompt emergency response to disasters. However, the obscurity caused by clouds makes it unstable to monitor the situation on the ground with the visible light camera. Images captured by a longer wavelength are introduced to reduce the effects of clouds. Synthetic Aperture Radar (SAR) is such an example that improves visibility even the clouds exist. On the other hand, the spatial resolution decreases as the wavelength increases. Furthermore, the images captured by long wavelengths differs considerably from those captured by visible light in terms of their appearance. Therefore, we propose a network that can remove clouds and generate visible light images from the multispectral images taken as inputs. This is achieved by extending the input channels of cGANs to be compatible with multispectral images. The networks are trained to output images that are close to the ground truth using the images synthesized with clouds over the ground truth as inputs. In the available dataset, the proportion of images of the forest or the sea is very high, which will introduce bias in the training dataset if uniformly sampled from the original dataset. Thus, we utilize the t-Distributed Stochastic Neighbor Embedding (t-SNE) to improve the problem of bias in the training dataset. Finally, we confirm the feasibility of the proposed network on the dataset of four bands images, which include three visible light bands and one near-infrared (NIR) band.



### Recent Advances in Zero-shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1710.04837v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.04837v1)
- **Published**: 2017-10-13 08:29:29+00:00
- **Updated**: 2017-10-13 08:29:29+00:00
- **Authors**: Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue, Leonid Sigal, Shaogang Gong
- **Comment**: accepted by IEEE Signal Processing Magazine
- **Journal**: None
- **Summary**: With the recent renaissance of deep convolution neural networks, encouraging breakthroughs have been achieved on the supervised recognition tasks, where each class has sufficient training data and fully annotated training data. However, to scale the recognition to a large number of classes with few or now training samples for each class remains an unsolved problem. One approach to scaling up the recognition is to develop models capable of recognizing unseen categories without any training instances, or zero-shot recognition/ learning. This article provides a comprehensive review of existing zero-shot recognition techniques covering various aspects ranging from representations of models, and from datasets and evaluation settings. We also overview related recognition tasks including one-shot and open set recognition which can be used as natural extensions of zero-shot recognition when limited number of class samples become available or when zero-shot recognition is implemented in a real-world setting. Importantly, we highlight the limitations of existing approaches and point out future research directions in this existing new research area.



### Dynamic texture recognition using time-causal and time-recursive spatio-temporal receptive fields
- **Arxiv ID**: http://arxiv.org/abs/1710.04842v3
- **DOI**: 10.1007/s10851-018-0826-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04842v3)
- **Published**: 2017-10-13 08:47:13+00:00
- **Updated**: 2018-06-21 12:32:44+00:00
- **Authors**: Ylva Jansson, Tony Lindeberg
- **Comment**: 29 pages, 16 figures
- **Journal**: Journal of Mathematical Imaging and Vision, 60(9): 1369-1398, 2018
- **Summary**: This work presents a first evaluation of using spatio-temporal receptive fields from a recently proposed time-causal spatio-temporal scale-space framework as primitives for video analysis. We propose a new family of video descriptors based on regional statistics of spatio-temporal receptive field responses and evaluate this approach on the problem of dynamic texture recognition. Our approach generalises a previously used method, based on joint histograms of receptive field responses, from the spatial to the spatio-temporal domain and from object recognition to dynamic texture recognition. The time-recursive formulation enables computationally efficient time-causal recognition. The experimental evaluation demonstrates competitive performance compared to state-of-the-art. Especially, it is shown that binary versions of our dynamic texture descriptors achieve improved performance compared to a large range of similar methods using different primitives either handcrafted or learned from data. Further, our qualitative and quantitative investigation into parameter choices and the use of different sets of receptive fields highlights the robustness and flexibility of our approach. Together, these results support the descriptive power of this family of time-causal spatio-temporal receptive fields, validate our approach for dynamic texture recognition and point towards the possibility of designing a range of video analysis methods based on these new time-causal spatio-temporal primitives.



### TensorQuant - A Simulation Toolbox for Deep Neural Network Quantization
- **Arxiv ID**: http://arxiv.org/abs/1710.05758v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.05758v1)
- **Published**: 2017-10-13 10:15:27+00:00
- **Updated**: 2017-10-13 10:15:27+00:00
- **Authors**: Dominik Marek Loroch, Norbert Wehn, Franz-Josef Pfreundt, Janis Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research implies that training and inference of deep neural networks (DNN) can be computed with low precision numerical representations of the training/test data, weights and gradients without a general loss in accuracy. The benefit of such compact representations is twofold: they allow a significant reduction of the communication bottleneck in distributed DNN training and faster neural network implementations on hardware accelerators like FPGAs. Several quantization methods have been proposed to map the original 32-bit floating point problem to low-bit representations. While most related publications validate the proposed approach on a single DNN topology, it appears to be evident, that the optimal choice of the quantization method and number of coding bits is topology dependent. To this end, there is no general theory available, which would allow users to derive the optimal quantization during the design of a DNN topology. In this paper, we present a quantization tool box for the TensorFlow framework. TensorQuant allows a transparent quantization simulation of existing DNN topologies during training and inference. TensorQuant supports generic quantization methods and allows experimental evaluation of the impact of the quantization on single layers as well as on the full topology. In a first series of experiments with TensorQuant, we show an analysis of fix-point quantizations of popular CNN topologies.



### RADNET: Radiologist Level Accuracy using Deep Learning for HEMORRHAGE detection in CT Scans
- **Arxiv ID**: http://arxiv.org/abs/1710.04934v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.04934v2)
- **Published**: 2017-10-13 14:14:39+00:00
- **Updated**: 2018-01-03 12:05:54+00:00
- **Authors**: Monika Grewal, Muktabh Mayank Srivastava, Pulkit Kumar, Srikrishna Varadarajan
- **Comment**: Accepted at IEEE Symposium on Biomedical Imaging (ISBI) 2018 as
  conference paper
- **Journal**: None
- **Summary**: We describe a deep learning approach for automated brain hemorrhage detection from computed tomography (CT) scans. Our model emulates the procedure followed by radiologists to analyse a 3D CT scan in real-world. Similar to radiologists, the model sifts through 2D cross-sectional slices while paying close attention to potential hemorrhagic regions. Further, the model utilizes 3D context from neighboring slices to improve predictions at each slice and subsequently, aggregates the slice-level predictions to provide diagnosis at CT level. We refer to our proposed approach as Recurrent Attention DenseNet (RADnet) as it employs original DenseNet architecture along with adding the components of attention for slice level predictions and recurrent neural network layer for incorporating 3D context. The real-world performance of RADnet has been benchmarked against independent analysis performed by three senior radiologists for 77 brain CTs. RADnet demonstrates 81.82% hemorrhage prediction accuracy at CT level that is comparable to radiologists. Further, RADnet achieves higher recall than two of the three radiologists, which is remarkable.



### Object Classification in Images of Neoclassical Artifacts Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1710.04943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.04943v1)
- **Published**: 2017-10-13 14:35:27+00:00
- **Updated**: 2017-10-13 14:35:27+00:00
- **Authors**: Bernhard Bermeitinger, Maria Christoforaki, Simon Donig, Siegfried Handschuh
- **Comment**: Published in Digital Humanities 2017, Montreal, Canada
- **Journal**: None
- **Summary**: In this paper, we report on our efforts for using Deep Learning for classifying artifacts and their features in digital visuals as a part of the Neoclassica framework. It was conceived to provide scholars with new methods for analyzing and classifying artifacts and aesthetic forms from the era of Classicism. The framework accommodates both traditional knowledge representation as a formal ontology and data-driven knowledge discovery, where cultural patterns will be identified by means of algorithms in statistical analysis and machine learning. We created a Deep Learning approach trained on photographs to classify the objects inside these photographs. In a next step, we will apply a different Deep Learning approach. It is capable of locating multiple objects inside an image and classifying them with a high accuracy.



### Real time ridge orientation estimation for fingerprint images
- **Arxiv ID**: http://arxiv.org/abs/1710.05027v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/1710.05027v1)
- **Published**: 2017-10-13 15:23:18+00:00
- **Updated**: 2017-10-13 15:23:18+00:00
- **Authors**: Eman Alibeigi, Shadrokh Samavi, Shahram Shirani, Zahra Rahmani
- **Comment**: 8 pages, 15 figures, 1 table
- **Journal**: None
- **Summary**: Fingerprint verification is an important bio-metric technique for personal identification. Most of the automatic verification systems are based on matching of fingerprint minutiae. Extraction of minutiae is an essential process which requires estimation of orientation of the lines in an image. Most of the existing methods involve intense mathematical computations and hence are performed through software means. In this paper a hardware scheme to perform real time orientation estimation is presented which is based on pipelined architecture. Synthesized circuits proved the functionality and accuracy of the suggested method.



### Skin Lesion Analysis Toward Melanoma Detection: A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI), Hosted by the International Skin Imaging Collaboration (ISIC)
- **Arxiv ID**: http://arxiv.org/abs/1710.05006v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05006v3)
- **Published**: 2017-10-13 17:08:53+00:00
- **Updated**: 2018-01-08 16:37:20+00:00
- **Authors**: Noel C. F. Codella, David Gutman, M. Emre Celebi, Brian Helba, Michael A. Marchetti, Stephen W. Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, Allan Halpern
- **Comment**: None
- **Journal**: None
- **Summary**: This article describes the design, implementation, and results of the latest installment of the dermoscopic image analysis benchmark challenge. The goal is to support research and development of algorithms for automated diagnosis of melanoma, the most lethal skin cancer. The challenge was divided into 3 tasks: lesion segmentation, feature detection, and disease classification. Participation involved 593 registrations, 81 pre-submissions, 46 finalized submissions (including a 4-page manuscript), and approximately 50 attendees, making this the largest standardized and comparative study in this field to date. While the official challenge duration and ranking of participants has concluded, the dataset snapshots remain available for further research and development.



### Improving Shadow Suppression for Illumination Robust Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1710.05073v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.05073v1)
- **Published**: 2017-10-13 20:56:23+00:00
- **Updated**: 2017-10-13 20:56:23+00:00
- **Authors**: Wuming Zhang, Xi Zhao, Jean-Marie Morvan, Liming Chen
- **Comment**: None
- **Journal**: None
- **Summary**: 2D face analysis techniques, such as face landmarking, face recognition and face verification, are reasonably dependent on illumination conditions which are usually uncontrolled and unpredictable in the real world. An illumination robust preprocessing method thus remains a significant challenge in reliable face analysis. In this paper we propose a novel approach for improving lighting normalization through building the underlying reflectance model which characterizes interactions between skin surface, lighting source and camera sensor, and elaborates the formation of face color appearance. Specifically, the proposed illumination processing pipeline enables the generation of Chromaticity Intrinsic Image (CII) in a log chromaticity space which is robust to illumination variations. Moreover, as an advantage over most prevailing methods, a photo-realistic color face image is subsequently reconstructed which eliminates a wide variety of shadows whilst retaining the color information and identity details. Experimental results under different scenarios and using various face databases show the effectiveness of the proposed approach to deal with lighting variations, including both soft and hard shadows, in face recognition.



