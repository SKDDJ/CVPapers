# Arxiv Papers in cs.CV on 2017-10-20
### Superpixel Based Segmentation and Classification of Polyps in Wireless Capsule Endoscopy
- **Arxiv ID**: http://arxiv.org/abs/1710.07390v2
- **DOI**: 10.1109/SPMB.2017.8257027
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07390v2)
- **Published**: 2017-10-20 01:32:53+00:00
- **Updated**: 2018-05-28 15:59:16+00:00
- **Authors**: Omid Haji Maghsoudi
- **Comment**: This paper has been published in SPMB 2017
- **Journal**: None
- **Summary**: Wireless Capsule Endoscopy (WCE) is a relatively new technology to record the entire GI trace, in vivo. The large amounts of frames captured during an examination cause difficulties for physicians to review all these frames. The need for reducing the reviewing time using some intelligent methods has been a challenge. Polyps are considered as growing tissues on the surface of intestinal tract not inside of an organ. Most polyps are not cancerous, but if one becomes larger than a centimeter, it can turn into cancer by great chance. The WCE frames provide the early stage possibility for detection of polyps. Here, the application of simple linear iterative clustering (SLIC) superpixel for segmentation of polyps in WCE frames is evaluated. Different SLIC superpixel numbers are examined to find the highest sensitivity for detection of polyps. The SLIC superpixel segmentation is promising to improve the results of previous studies. Finally, the superpixels were classified using a support vector machine (SVM) by extracting some texture and color features. The classification results showed a sensitivity of 91%.



### Linear-Time Algorithm in Bayesian Image Denoising based on Gaussian Markov Random Field
- **Arxiv ID**: http://arxiv.org/abs/1710.07393v2
- **DOI**: 10.1587/transinf.2017EDP7346
- **Categories**: **stat.ML**, cond-mat.dis-nn, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.07393v2)
- **Published**: 2017-10-20 02:06:41+00:00
- **Updated**: 2020-03-03 06:30:43+00:00
- **Authors**: Muneki Yasuda, Junpei Watanabe, Shun Kataoka, kazuyuki Tanaka
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider Bayesian image denoising based on a Gaussian Markov random field (GMRF) model, for which we propose an new algorithm. Our method can solve Bayesian image denoising problems, including hyperparameter estimation, in $O(n)$-time, where $n$ is the number of pixels in a given image. From the perspective of the order of the computational time, this is a state-of-the-art algorithm for the present problem setting. Moreover, the results of our numerical experiments we show our method is in fact effective in practice.



### Fast and Efficient Calculations of Structural Invariants of Chirality
- **Arxiv ID**: http://arxiv.org/abs/1711.05866v2
- **DOI**: None
- **Categories**: **physics.comp-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.05866v2)
- **Published**: 2017-10-20 06:53:58+00:00
- **Updated**: 2017-12-21 07:21:47+00:00
- **Authors**: He Zhang, Hanlin Mo, You Hao, Shirui Li, Hua Li
- **Comment**: None
- **Journal**: None
- **Summary**: Chirality plays an important role in physics, chemistry, biology, and other fields. It describes an essential symmetry in structure. However, chirality invariants are usually complicated in expression or difficult to evaluate. In this paper, we present five general three-dimensional chirality invariants based on the generating functions. And the five chiral invariants have four characteristics:(1) They play an important role in the detection of symmetry, especially in the treatment of 'false zero' problem. (2) Three of the five chiral invariants decode an universal chirality index. (3) Three of them are proposed for the first time. (4) The five chiral invariants have low order no bigger than 4, brief expression, low time complexity O(n) and can act as descriptors of three-dimensional objects in shape analysis. The five chiral invariants give a geometric view to study the chiral invariants. And the experiments show that the five chirality invariants are effective and efficient, they can be used as a tool for symmetry detection or features in shape analysis.



### Light-weight place recognition and loop detection using road markings
- **Arxiv ID**: http://arxiv.org/abs/1710.07434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07434v1)
- **Published**: 2017-10-20 07:15:17+00:00
- **Updated**: 2017-10-20 07:15:17+00:00
- **Authors**: Oleksandr Bailo, Francois Rameau, In So Kweon
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an efficient algorithm for robust place recognition and loop detection using camera information only. Our pipeline purely relies on spatial localization and semantic information of road markings. The creation of the database of road markings sequences is performed online, which makes the method applicable for real-time loop closure for visual SLAM techniques. Furthermore, our algorithm is robust to various weather conditions, occlusions from vehicles, and shadows. We have performed an extensive number of experiments which highlight the effectiveness and scalability of the proposed method.



### Generalized Zero-Shot Learning for Action Recognition with Web-Scale Video Data
- **Arxiv ID**: http://arxiv.org/abs/1710.07455v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07455v1)
- **Published**: 2017-10-20 08:49:11+00:00
- **Updated**: 2017-10-20 08:49:11+00:00
- **Authors**: Kun Liu, Wu Liu, Huadong Ma, Wenbing Huang, Xiongxiong Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Action recognition in surveillance video makes our life safer by detecting the criminal events or predicting violent emergencies. However, efficient action recognition is not free of difficulty. First, there are so many action classes in daily life that we cannot pre-define all possible action classes beforehand. Moreover, it is very hard to collect real-word videos for certain particular actions such as steal and street fight due to legal restrictions and privacy protection. These challenges make existing data-driven recognition methods insufficient to attain desired performance. Zero-shot learning is potential to be applied to solve these issues since it can perform classification without positive example. Nevertheless, current zero-shot learning algorithms have been studied under the unreasonable setting where seen classes are absent during the testing phase. Motivated by this, we study the task of action recognition in surveillance video under a more realistic \emph{generalized zero-shot setting}, where testing data contains both seen and unseen classes. To our best knowledge, this is the first work to study video action recognition under the generalized zero-shot setting. We firstly perform extensive empirical studies on several existing zero-shot leaning approaches under this new setting on a web-scale video data. Our experimental results demonstrate that, under the generalize setting, typical zero-shot learning methods are no longer effective for the dataset we applied. Then, we propose a method for action recognition by deploying generalized zero-shot learning, which transfers the knowledge of web video to detect the anomalous actions in surveillance videos. To verify the effectiveness of our proposed method, we further construct a new surveillance video dataset consisting of nine action classes related to the public safety situation.



### Learning Wasserstein Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1710.07457v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/1710.07457v1)
- **Published**: 2017-10-20 09:09:34+00:00
- **Updated**: 2017-10-20 09:09:34+00:00
- **Authors**: Nicolas Courty, Rémi Flamary, Mélanie Ducoffe
- **Comment**: None
- **Journal**: None
- **Summary**: The Wasserstein distance received a lot of attention recently in the community of machine learning, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy computational cost. Our goal is to alleviate this problem by providing an approximation mechanism that allows to break its inherent complexity. It relies on the search of an embedding where the Euclidean distance mimics the Wasserstein distance. We show that such an embedding can be found with a siamese architecture associated with a decoder network that allows to move from the embedding space back to the original input space. Once this embedding has been found, computing optimization problems in the Wasserstein space (e.g. barycenters, principal directions or even archetypes) can be conducted extremely fast. Numerical experiments supporting this idea are conducted on image datasets, and show the wide potential benefits of our method.



### Anticipating Daily Intention using On-Wrist Motion Triggered Sensing
- **Arxiv ID**: http://arxiv.org/abs/1710.07477v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07477v1)
- **Published**: 2017-10-20 10:39:58+00:00
- **Updated**: 2017-10-20 10:39:58+00:00
- **Authors**: Tz-Ying Wu, Ting-An Chien, Cheng-Sheng Chan, Chan-Wei Hu, Min Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Anticipating human intention by observing one's actions has many applications. For instance, picking up a cellphone, then a charger (actions) implies that one wants to charge the cellphone (intention). By anticipating the intention, an intelligent system can guide the user to the closest power outlet. We propose an on-wrist motion triggered sensing system for anticipating daily intentions, where the on-wrist sensors help us to persistently observe one's actions. The core of the system is a novel Recurrent Neural Network (RNN) and Policy Network (PN), where the RNN encodes visual and motion observation to anticipate intention, and the PN parsimoniously triggers the process of visual observation to reduce computation requirement. We jointly trained the whole network using policy gradient and cross-entropy loss. To evaluate, we collect the first daily "intention" dataset consisting of 2379 videos with 34 intentions and 164 unique action sequences. Our method achieves 92.68%, 90.85%, 97.56% accuracy on three users while processing only 29% of the visual observation on average.



### HDR image reconstruction from a single exposure using deep CNNs
- **Arxiv ID**: http://arxiv.org/abs/1710.07480v1
- **DOI**: 10.1145/3130800.3130816
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1710.07480v1)
- **Published**: 2017-10-20 10:48:22+00:00
- **Updated**: 2017-10-20 10:48:22+00:00
- **Authors**: Gabriel Eilertsen, Joel Kronander, Gyorgy Denes, Rafał K. Mantiuk, Jonas Unger
- **Comment**: 15 pages, 19 figures, Siggraph Asia 2017. Project webpage located at
  http://hdrv.org/hdrcnn/ where paper with high quality images is available, as
  well as supplementary material (document, images, video and source code)
- **Journal**: ACM Trans. Graph. 36, 6, Article 178 (2017)
- **Summary**: Camera sensors can only capture a limited range of luminance simultaneously, and in order to create high dynamic range (HDR) images a set of different exposures are typically combined. In this paper we address the problem of predicting information that have been lost in saturated image areas, in order to enable HDR reconstruction from a single exposure. We show that this problem is well-suited for deep learning algorithms, and propose a deep convolutional neural network (CNN) that is specifically designed taking into account the challenges in predicting HDR values. To train the CNN we gather a large dataset of HDR images, which we augment by simulating sensor saturation for a range of cameras. To further boost robustness, we pre-train the CNN on a simulated HDR dataset created from a subset of the MIT Places database. We demonstrate that our approach can reconstruct high-resolution visually convincing HDR results in a wide range of situations, and that it generalizes well to reconstruction of images captured with arbitrary and low-end cameras that use unknown camera response functions and post-processing. Furthermore, we compare to existing methods for HDR expansion, and show high quality results also for image based lighting. Finally, we evaluate the results in a subjective experiment performed on an HDR display. This shows that the reconstructed HDR images are visually convincing, with large improvements as compared to existing methods.



### MR to X-Ray Projection Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1710.07498v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07498v2)
- **Published**: 2017-10-20 12:15:06+00:00
- **Updated**: 2018-04-03 15:35:35+00:00
- **Authors**: Bernhard Stimpel, Christopher Syben, Tobias Würfl, Katrin Mentl, Arnd Dörfler, Andreas Maier
- **Comment**: In Proceedings of the 5th International Conference on Image Formation
  in X-ray Computed Tomography
- **Journal**: None
- **Summary**: Hybrid imaging promises large potential in medical imaging applications. To fully utilize the possibilities of corresponding information from different modalities, the information must be transferable between the domains. In radiation therapy planning, existing methods make use of reconstructed 3D magnetic resonance imaging data to synthesize corresponding X-ray attenuation maps. In contrast, for fluoroscopic procedures only line integral data, i.e., 2D projection images, are present. The question arises which approaches could potentially be used for this MR to X-ray projection image-to-image translation. We examine three network architectures and two loss-functions regarding their suitability as generator networks for this task. All generators proved to yield suitable results for this task. A cascaded refinement network paired with a perceptual-loss function achieved the best qualitative results in our evaluation. The perceptual-loss showed to be able to preserve most of the high-frequency details in the projection images and, thus, is recommended for the underlying task and similar problems.



### Real-time Convolutional Neural Networks for Emotion and Gender Classification
- **Arxiv ID**: http://arxiv.org/abs/1710.07557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1710.07557v1)
- **Published**: 2017-10-20 14:53:57+00:00
- **Updated**: 2017-10-20 14:53:57+00:00
- **Authors**: Octavio Arriaga, Matias Valdenegro-Toro, Paul Plöger
- **Comment**: Submitted to ICRA 2018
- **Journal**: None
- **Summary**: In this paper we propose an implement a general convolutional neural network (CNN) building framework for designing real-time CNNs. We validate our models by creating a real-time vision system which accomplishes the tasks of face detection, gender classification and emotion classification simultaneously in one blended step using our proposed CNN architecture. After presenting the details of the training procedure setup we proceed to evaluate on standard benchmark sets. We report accuracies of 96% in the IMDB gender dataset and 66% in the FER-2013 emotion dataset. Along with this we also introduced the very recent real-time enabled guided back-propagation visualization technique. Guided back-propagation uncovers the dynamics of the weight changes and evaluates the learned features. We argue that the careful implementation of modern CNN architectures, the use of the current regularization methods and the visualization of previously hidden features are necessary in order to reduce the gap between slow performances and real-time architectures. Our system has been validated by its deployment on a Care-O-bot 3 robot used during RoboCup@Home competitions. All our code, demos and pre-trained architectures have been released under an open-source license in our public repository.



### Classification Driven Dynamic Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1710.07558v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1710.07558v3)
- **Published**: 2017-10-20 14:54:29+00:00
- **Updated**: 2018-03-28 19:11:33+00:00
- **Authors**: Vivek Sharma, Ali Diba, Davy Neven, Michael S. Brown, Luc Van Gool, Rainer Stiefelhagen
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks rely on image texture and structure to serve as discriminative features to classify the image content. Image enhancement techniques can be used as preprocessing steps to help improve the overall image quality and in turn improve the overall effectiveness of a CNN. Existing image enhancement methods, however, are designed to improve the perceptual quality of an image for a human observer. In this paper, we are interested in learning CNNs that can emulate image enhancement and restoration, but with the overall goal to improve image classification and not necessarily human perception. To this end, we present a unified CNN architecture that uses a range of enhancement filters that can enhance image-specific details via end-to-end dynamic filter learning. We demonstrate the effectiveness of this strategy on four challenging benchmark datasets for fine-grained, object, scene, and texture classification: CUB-200-2011, PASCAL-VOC2007, MIT-Indoor, and DTD. Experiments using our proposed enhancement show promising results on all the datasets. In addition, our approach is capable of improving the performance of all generic CNN architectures.



### SEGCloud: Semantic Segmentation of 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1710.07563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07563v1)
- **Published**: 2017-10-20 15:05:41+00:00
- **Updated**: 2017-10-20 15:05:41+00:00
- **Authors**: Lyne P. Tchapmi, Christopher B. Choy, Iro Armeni, JunYoung Gwak, Silvio Savarese
- **Comment**: Accepted as a spotlight at the International Conference of 3D Vision
  (3DV 2017)
- **Journal**: None
- **Summary**: 3D semantic scene labeling is fundamental to agents operating in the real world. In particular, labeling raw 3D point sets from sensors provides fine-grained semantics. Recent works leverage the capabilities of Neural Networks (NNs), but are limited to coarse voxel predictions and do not explicitly enforce global consistency. We present SEGCloud, an end-to-end framework to obtain 3D point-level segmentation that combines the advantages of NNs, trilinear interpolation(TI) and fully connected Conditional Random Fields (FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN are transferred back to the raw 3D points via trilinear interpolation. Then the FC-CRF enforces global consistency and provides fine-grained semantics on the points. We implement the latter as a differentiable Recurrent NN to allow joint optimization. We evaluate the framework on two indoor and two outdoor 3D datasets (NYU V2, S3DIS, KITTI, Semantic3D.net), and show performance comparable or superior to the state-of-the-art on all datasets.



### Employing Fusion of Learned and Handcrafted Features for Unconstrained Ear Recognition
- **Arxiv ID**: http://arxiv.org/abs/1710.07662v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07662v1)
- **Published**: 2017-10-20 18:43:21+00:00
- **Updated**: 2017-10-20 18:43:21+00:00
- **Authors**: Earnest E. Hansley, Mauricio Pamplona Segundo, Sudeep Sarkar
- **Comment**: 23 pages, 7 figures, 7 tables
- **Journal**: None
- **Summary**: We present an unconstrained ear recognition framework that outperforms state-of-the-art systems in different publicly available image databases. To this end, we developed CNN-based solutions for ear normalization and description, we used well-known handcrafted descriptors, and we fused learned and handcrafted features to improve recognition. We designed a two-stage landmark detector that successfully worked under untrained scenarios. We used the results generated to perform a geometric image normalization that boosted the performance of all evaluated descriptors. Our CNN descriptor outperformed other CNN-based works in the literature, specially in more difficult scenarios. The fusion of learned and handcrafted matchers appears to be complementary as it achieved the best performance in all experiments. The obtained results outperformed all other reported results for the UERC challenge, which contains the most difficult database nowadays.



### Generalized linear mixing model accounting for endmember variability
- **Arxiv ID**: http://arxiv.org/abs/1710.07723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.07723v1)
- **Published**: 2017-10-20 22:46:12+00:00
- **Updated**: 2017-10-20 22:46:12+00:00
- **Authors**: Tales Imbiriba, Ricardo Augusto Borsoi, José Carlos Moreira Bermudez
- **Comment**: None
- **Journal**: None
- **Summary**: Endmember variability is an important factor for accurately unveiling vital information relating the pure materials and their distribution in hyperspectral images. Recently, the extended linear mixing model (ELMM) has been proposed as a modification of the linear mixing model (LMM) to consider endmember variability effects resulting mainly from illumination changes. In this paper, we further generalize the ELMM leading to a new model (GLMM) to account for more complex spectral distortions where different wavelength intervals can be affected unevenly. We also extend the existing methodology to jointly estimate the variability and the abundances for the GLMM. Simulations with real and synthetic data show that the unmixing process can benefit from the extra flexibility introduced by the GLMM.



