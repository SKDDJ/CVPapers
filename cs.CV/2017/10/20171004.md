# Arxiv Papers in cs.CV on 2017-10-04
### Effective Image Differencing with ConvNets for Real-time Transient Hunting
- **Arxiv ID**: http://arxiv.org/abs/1710.01422v1
- **DOI**: 10.1093/mnras/sty613
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.01422v1)
- **Published**: 2017-10-04 00:02:02+00:00
- **Updated**: 2017-10-04 00:02:02+00:00
- **Authors**: Nima Sedaghat, Ashish Mahabal
- **Comment**: None
- **Journal**: None
- **Summary**: Large sky surveys are increasingly relying on image subtraction pipelines for real-time (and archival) transient detection. In this process one has to contend with varying PSF, small brightness variations in many sources, as well as artifacts resulting from saturated stars, and, in general, matching errors. Very often the differencing is done with a reference image that is deeper than individual images and the attendant difference in noise characteristics can also lead to artifacts. We present here a deep-learning approach to transient detection that encapsulates all the steps of a traditional image subtraction pipeline -- image registration, background subtraction, noise removal, psf matching, and subtraction -- into a single real-time convolutional network. Once trained the method works lighteningly fast, and given that it does multiple steps at one go, the advantages for multi-CCD, fast surveys like ZTF and LSST are obvious.



### Visual Tracking via Dynamic Graph Learning
- **Arxiv ID**: http://arxiv.org/abs/1710.01444v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01444v2)
- **Published**: 2017-10-04 03:00:49+00:00
- **Updated**: 2018-04-30 16:42:50+00:00
- **Authors**: Chenglong Li, Liang Lin, Wangmeng Zuo, Jin Tang, Ming-Hsuan Yang
- **Comment**: Submitted to TPAMI 2017
- **Journal**: None
- **Summary**: Existing visual tracking methods usually localize a target object with a bounding box, in which the performance of the foreground object trackers or detectors is often affected by the inclusion of background clutter. To handle this problem, we learn a patch-based graph representation for visual tracking. The tracked object is modeled by with a graph by taking a set of non-overlapping image patches as nodes, in which the weight of each node indicates how likely it belongs to the foreground and edges are weighted for indicating the appearance compatibility of two neighboring nodes. This graph is dynamically learned and applied in object tracking and model updating. During the tracking process, the proposed algorithm performs three main steps in each frame. First, the graph is initialized by assigning binary weights of some image patches to indicate the object and background patches according to the predicted bounding box. Second, the graph is optimized to refine the patch weights by using a novel alternating direction method of multipliers. Third, the object feature representation is updated by imposing the weights of patches on the extracted image features. The object location is predicted by maximizing the classification score in the structured support vector machine. Extensive experiments show that the proposed tracking algorithm performs well against the state-of-the-art methods on large-scale benchmark datasets.



### Content-Adaptive Sketch Portrait Generation by Decompositional Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1710.01453v1
- **DOI**: 10.1109/TIP.2016.2623485
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01453v1)
- **Published**: 2017-10-04 03:37:39+00:00
- **Updated**: 2017-10-04 03:37:39+00:00
- **Authors**: Dongyu Zhang, Liang Lin, Tianshui Chen, Xian Wu, Wenwei Tan, Ebroul Izquierdo
- **Comment**: Published in TIP 2017
- **Journal**: None
- **Summary**: Sketch portrait generation benefits a wide range of applications such as digital entertainment and law enforcement. Although plenty of efforts have been dedicated to this task, several issues still remain unsolved for generating vivid and detail-preserving personal sketch portraits. For example, quite a few artifacts may exist in synthesizing hairpins and glasses, and textural details may be lost in the regions of hair or mustache. Moreover, the generalization ability of current systems is somewhat limited since they usually require elaborately collecting a dictionary of examples or carefully tuning features/components. In this paper, we present a novel representation learning framework that generates an end-to-end photo-sketch mapping through structure and texture decomposition. In the training stage, we first decompose the input face photo into different components according to their representational contents (i.e., structural and textural parts) by using a pre-trained Convolutional Neural Network (CNN). Then, we utilize a Branched Fully Convolutional Neural Network (BFCN) for learning structural and textural representations, respectively. In addition, we design a Sorted Matching Mean Square Error (SM-MSE) metric to measure texture patterns in the loss function. In the stage of sketch rendering, our approach automatically generates structural and textural representations for the input photo and produces the final result via a probabilistic fusion scheme. Extensive experiments on several challenging benchmarks suggest that our approach outperforms example-based synthesis algorithms in terms of both perceptual and objective metrics. In addition, the proposed method also has better generalization ability across dataset without additional training.



### Learning to Segment Human by Watching YouTube
- **Arxiv ID**: http://arxiv.org/abs/1710.01457v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01457v2)
- **Published**: 2017-10-04 04:16:23+00:00
- **Updated**: 2018-02-28 02:07:00+00:00
- **Authors**: Xiaodan Liang, Yunchao Wei, Liang Lin, Yunpeng Chen, Xiaohui Shen, Jianchao Yang, Shuicheng Yan
- **Comment**: Very-weakly supervised learning framework. New state-of-the-art
  performance on the human segmentation task! (Published in T-PAMI 2017)
- **Journal**: 10.1109/TPAMI.2016.2598340
- **Summary**: An intuition on human segmentation is that when a human is moving in a video, the video-context (e.g., appearance and motion clues) may potentially infer reasonable mask information for the whole human body. Inspired by this, based on popular deep convolutional neural networks (CNN), we explore a very-weakly supervised learning framework for human segmentation task, where only an imperfect human detector is available along with massive weakly-labeled YouTube videos. In our solution, the video-context guided human mask inference and CNN based segmentation network learning iterate to mutually enhance each other until no further improvement gains. In the first step, each video is decomposed into supervoxels by the unsupervised video segmentation. The superpixels within the supervoxels are then classified as human or non-human by graph optimization with unary energies from the imperfect human detection results and the predicted confidence maps by the CNN trained in the previous iteration. In the second step, the video-context derived human masks are used as direct labels to train CNN. Extensive experiments on the challenging PASCAL VOC 2012 semantic segmentation benchmark demonstrate that the proposed framework has already achieved superior results than all previous weakly-supervised methods with object class or bounding box annotations. In addition, by augmenting with the annotated masks from PASCAL VOC 2012, our method reaches a new state-of-the-art performance on the human segmentation task.



### Secrets in Computing Optical Flow by Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.01462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01462v1)
- **Published**: 2017-10-04 05:20:41+00:00
- **Updated**: 2017-10-04 05:20:41+00:00
- **Authors**: Junxuan Li
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been widely used over many areas in compute vision. Especially in classification. Recently, FlowNet and several works on opti- cal estimation using CNNs shows the potential ability of CNNs in doing per-pixel regression. We proposed several CNNs network architectures that can estimate optical flow, and fully unveiled the intrinsic different between these structures.



### Image Labeling Based on Graphical Models Using Wasserstein Messages and Geometric Assignment
- **Arxiv ID**: http://arxiv.org/abs/1710.01493v2
- **DOI**: 10.1137/17M1150669
- **Categories**: **cs.LG**, cs.CV, cs.NA, math.OC, 62H35, 62M40, 65K10, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1710.01493v2)
- **Published**: 2017-10-04 08:00:50+00:00
- **Updated**: 2018-01-09 08:54:37+00:00
- **Authors**: Ruben Hühnerbein, Fabrizio Savarino, Freddie Åström, Christoph Schnörr
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel approach to Maximum A Posteriori inference based on discrete graphical models. By utilizing local Wasserstein distances for coupling assignment measures across edges of the underlying graph, a given discrete objective function is smoothly approximated and restricted to the assignment manifold. A corresponding multiplicative update scheme combines in a single process (i) geometric integration of the resulting Riemannian gradient flow and (ii) rounding to integral solutions that represent valid labelings. Throughout this process, local marginalization constraints known from the established LP relaxation are satisfied, whereas the smooth geometric setting results in rapidly converging iterations that can be carried out in parallel for every edge.



### Monitoring tool usage in surgery videos using boosted convolutional and recurrent neural networks
- **Arxiv ID**: http://arxiv.org/abs/1710.01559v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01559v2)
- **Published**: 2017-10-04 11:48:34+00:00
- **Updated**: 2018-05-06 08:21:51+00:00
- **Authors**: Hassan Al Hajj, Mathieu Lamard, Pierre-Henri Conze, Béatrice Cochener, Gwenolé Quellec
- **Comment**: Accepted for publication in Medical Image Analysis
- **Journal**: None
- **Summary**: This paper investigates the automatic monitoring of tool usage during a surgery, with potential applications in report generation, surgical training and real-time decision support. Two surgeries are considered: cataract surgery, the most common surgical procedure, and cholecystectomy, one of the most common digestive surgeries. Tool usage is monitored in videos recorded either through a microscope (cataract surgery) or an endoscope (cholecystectomy). Following state-of-the-art video analysis solutions, each frame of the video is analyzed by convolutional neural networks (CNNs) whose outputs are fed to recurrent neural networks (RNNs) in order to take temporal relationships between events into account. Novelty lies in the way those CNNs and RNNs are trained. Computational complexity prevents the end-to-end training of "CNN+RNN" systems. Therefore, CNNs are usually trained first, independently from the RNNs. This approach is clearly suboptimal for surgical tool analysis: many tools are very similar to one another, but they can generally be differentiated based on past events. CNNs should be trained to extract the most useful visual features in combination with the temporal context. A novel boosting strategy is proposed to achieve this goal: the CNN and RNN parts of the system are simultaneously enriched by progressively adding weak classifiers (either CNNs or RNNs) trained to improve the overall classification accuracy. Experiments were performed in a dataset of 50 cataract surgery videos and a dataset of 80 cholecystectomy videos. Very good classification performance are achieved in both datasets: tool usage could be labeled with an average area under the ROC curve of $A_z = 0.9961$ and $A_z = 0.9939$, respectively, in offline mode (using past, present and future information), and $A_z = 0.9957$ and $A_z = 0.9936$, respectively, in online mode (using past and present information only).



### GraphMatch: Efficient Large-Scale Graph Construction for Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1710.01602v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01602v1)
- **Published**: 2017-10-04 13:45:00+00:00
- **Updated**: 2017-10-04 13:45:00+00:00
- **Authors**: Qiaodong Cui, Victor Fragoso, Chris Sweeney, Pradeep Sen
- **Comment**: Published at IEEE 3DV 2017
- **Journal**: None
- **Summary**: We present GraphMatch, an approximate yet efficient method for building the matching graph for large-scale structure-from-motion (SfM) pipelines. Unlike modern SfM pipelines that use vocabulary (Voc.) trees to quickly build the matching graph and avoid a costly brute-force search of matching image pairs, GraphMatch does not require an expensive offline pre-processing phase to construct a Voc. tree. Instead, GraphMatch leverages two priors that can predict which image pairs are likely to match, thereby making the matching process for SfM much more efficient. The first is a score computed from the distance between the Fisher vectors of any two images. The second prior is based on the graph distance between vertices in the underlying matching graph. GraphMatch combines these two priors into an iterative "sample-and-propagate" scheme similar to the PatchMatch algorithm. Its sampling stage uses Fisher similarity priors to guide the search for matching image pairs, while its propagation stage explores neighbors of matched pairs to find new ones with a high image similarity score. Our experiments show that GraphMatch finds the most image pairs as compared to competing, approximate methods while at the same time being the most efficient.



### Blind Image Fusion for Hyperspectral Imaging with the Directional Total Variation
- **Arxiv ID**: http://arxiv.org/abs/1710.05705v4
- **DOI**: 10.1088/1361-6420/aaaf63
- **Categories**: **cs.CV**, cs.NA, math.NA, 49M37, 65K10, 90C30, 90C90
- **Links**: [PDF](http://arxiv.org/pdf/1710.05705v4)
- **Published**: 2017-10-04 15:18:13+00:00
- **Updated**: 2018-04-09 14:40:20+00:00
- **Authors**: Leon Bungert, David A. Coomes, Matthias J. Ehrhardt, Jennifer Rasch, Rafael Reisenhofer, Carola-Bibiane Schönlieb
- **Comment**: 24 pages, 18 figures, published in Inverse Problems, typo corrected,
  figure added
- **Journal**: Inverse Problems, 34(4), 044003, 2018
- **Summary**: Hyperspectral imaging is a cutting-edge type of remote sensing used for mapping vegetation properties, rock minerals and other materials. A major drawback of hyperspectral imaging devices is their intrinsic low spatial resolution. In this paper, we propose a method for increasing the spatial resolution of a hyperspectral image by fusing it with an image of higher spatial resolution that was obtained with a different imaging modality. This is accomplished by solving a variational problem in which the regularization functional is the directional total variation. To accommodate for possible mis-registrations between the two images, we consider a non-convex blind super-resolution problem where both a fused image and the corresponding convolution kernel are estimated. Using this approach, our model can realign the given images if needed. Our experimental results indicate that the non-convexity is negligible in practice and that reliable solutions can be computed using a variety of different optimization algorithms. Numerical results on real remote sensing data from plant sciences and urban monitoring show the potential of the proposed method and suggests that it is robust with respect to the regularization parameters, mis-registration and the shape of the kernel.



### Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy
- **Arxiv ID**: http://arxiv.org/abs/1710.01711v3
- **DOI**: 10.1016/j.ophtha.2018.01.034
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01711v3)
- **Published**: 2017-10-04 17:29:06+00:00
- **Updated**: 2018-07-03 18:02:16+00:00
- **Authors**: Jonathan Krause, Varun Gulshan, Ehsan Rahimy, Peter Karth, Kasumi Widner, Greg S. Corrado, Lily Peng, Dale R. Webster
- **Comment**: None
- **Journal**: Ophthalmology (2018)
- **Summary**: Diabetic retinopathy (DR) and diabetic macular edema are common complications of diabetes which can lead to vision loss. The grading of DR is a fairly complex process that requires the detection of fine features such as microaneurysms, intraretinal hemorrhages, and intraretinal microvascular abnormalities. Because of this, there can be a fair amount of grader variability. There are different methods of obtaining the reference standard and resolving disagreements between graders, and while it is usually accepted that adjudication until full consensus will yield the best reference standard, the difference between various methods of resolving disagreements has not been examined extensively. In this study, we examine the variability in different methods of grading, definitions of reference standards, and their effects on building deep learning models for the detection of diabetic eye disease. We find that a small set of adjudicated DR grades allows substantial improvements in algorithm performance. The resulting algorithm's performance was on par with that of individual U.S. board-certified ophthalmologists and retinal specialists.



### Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.01992v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01992v3)
- **Published**: 2017-10-04 17:58:55+00:00
- **Updated**: 2018-08-09 19:31:38+00:00
- **Authors**: Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang
- **Comment**: The code and datasets are available at
  http://vllab.ucmerced.edu/wlai24/LapSRN/
- **Journal**: None
- **Summary**: Convolutional neural networks have recently demonstrated high-quality reconstruction for single image super-resolution. However, existing methods often require a large number of network parameters and entail heavy computational loads at runtime for generating high-accuracy super-resolution results. In this paper, we propose the deep Laplacian Pyramid Super-Resolution Network for fast and accurate image super-resolution. The proposed network progressively reconstructs the sub-band residuals of high-resolution images at multiple pyramid levels. In contrast to existing methods that involve the bicubic interpolation for pre-processing (which results in large feature maps), the proposed method directly extracts features from the low-resolution input space and thereby entails low computational loads. We train the proposed network with deep supervision using the robust Charbonnier loss functions and achieve high-quality image reconstruction. Furthermore, we utilize the recursive layers to share parameters across as well as within pyramid levels, and thus drastically reduce the number of parameters. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of run-time and image quality.



### Semantic 3D Reconstruction with Finite Element Bases
- **Arxiv ID**: http://arxiv.org/abs/1710.01749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01749v1)
- **Published**: 2017-10-04 18:08:01+00:00
- **Updated**: 2017-10-04 18:08:01+00:00
- **Authors**: Audrey Richard, Christoph Vogel, Maros Blaha, Thomas Pock, Konrad Schindler
- **Comment**: None
- **Journal**: BMVC 2017, 28th British Machine Vision Conference
- **Summary**: We propose a novel framework for the discretisation of multi-label problems on arbitrary, continuous domains. Our work bridges the gap between general FEM discretisations, and labeling problems that arise in a variety of computer vision tasks, including for instance those derived from the generalised Potts model. Starting from the popular formulation of labeling as a convex relaxation by functional lifting, we show that FEM discretisation is valid for the most general case, where the regulariser is anisotropic and non-metric. While our findings are generic and applicable to different vision problems, we demonstrate their practical implementation in the context of semantic 3D reconstruction, where such regularisers have proved particularly beneficial. The proposed FEM approach leads to a smaller memory footprint as well as faster computation, and it constitutes a very simple way to enable variable, adaptive resolution within the same model.



### Accelerating CS in Parallel Imaging Reconstructions Using an Efficient and Effective Circulant Preconditioner
- **Arxiv ID**: http://arxiv.org/abs/1710.01758v1
- **DOI**: 10.1002/mrm.27371
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01758v1)
- **Published**: 2017-10-04 18:48:20+00:00
- **Updated**: 2017-10-04 18:48:20+00:00
- **Authors**: Kirsten Koolstra, Jeroen van Gemert, Peter Börnert, Andrew Webb, Rob Remis
- **Comment**: 27 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: Purpose: Design of a preconditioner for fast and efficient parallel imaging and compressed sensing reconstructions. Theory: Parallel imaging and compressed sensing reconstructions become time consuming when the problem size or the number of coils is large, due to the large linear system of equations that has to be solved in l_1 and l_2-norm based reconstruction algorithms. Such linear systems can be solved efficiently using effective preconditioning techniques. Methods: In this paper we construct such a preconditioner by approximating the system matrix of the linear system, which comprises the data fidelity and includes total variation and wavelet regularization, by a matrix with the assumption that is a block circulant matrix with circulant blocks. Due to its circulant structure, the preconditioner can be constructed quickly and its inverse can be evaluated fast using only two fast Fourier transformations. We test the performance of the preconditioner for the conjugate gradient method as the linear solver, integrated into the Split Bregman algorithm. Results: The designed circulant preconditioner reduces the number of iterations required in the conjugate gradient method by almost a factor of~5. The speed up results in a total acceleration factor of approximately 2.5 for the entire reconstruction algorithm when implemented in MATLAB, while the initialization time of the preconditioner is negligible. Conclusion: The proposed preconditioner reduces the reconstruction time for parallel imaging and compressed sensing in a Split Bregman implementation and can easily handle large systems since it is Fourier-based, allowing for efficient computations.   Key words: preconditioning; compressed sensing; Split Bregman; parallel imaging



### DeepLesion: Automated Deep Mining, Categorization and Detection of Significant Radiology Image Findings using Large-Scale Clinical Lesion Annotations
- **Arxiv ID**: http://arxiv.org/abs/1710.01766v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01766v2)
- **Published**: 2017-10-04 19:10:38+00:00
- **Updated**: 2017-10-10 17:49:41+00:00
- **Authors**: Ke Yan, Xiaosong Wang, Le Lu, Ronald M. Summers
- **Comment**: None
- **Journal**: None
- **Summary**: Extracting, harvesting and building large-scale annotated radiological image datasets is a greatly important yet challenging problem. It is also the bottleneck to designing more effective data-hungry computing paradigms (e.g., deep learning) for medical image analysis. Yet, vast amounts of clinical annotations (usually associated with disease image findings and marked using arrows, lines, lesion diameters, segmentation, etc.) have been collected over several decades and stored in hospitals' Picture Archiving and Communication Systems. In this paper, we mine and harvest one major type of clinical annotation data - lesion diameters annotated on bookmarked images - to learn an effective multi-class lesion detector via unsupervised and supervised deep Convolutional Neural Networks (CNN). Our dataset is composed of 33,688 bookmarked radiology images from 10,825 studies of 4,477 unique patients. For every bookmarked image, a bounding box is created to cover the target lesion based on its measured diameters. We categorize the collection of lesions using an unsupervised deep mining scheme to generate clustered pseudo lesion labels. Next, we adopt a regional-CNN method to detect lesions of multiple categories, regardless of missing annotations (normally only one lesion is annotated, despite the presence of multiple co-existing findings). Our integrated mining, categorization and detection framework is validated with promising empirical results, as a scalable, universal or multi-purpose CAD paradigm built upon abundant retrospective medical data. Furthermore, we demonstrate that detection accuracy can be significantly improved by incorporating pseudo lesion labels (e.g., Liver lesion/tumor, Lung nodule/tumor, Abdomen lesions, Chest lymph node and others). This dataset will be made publicly available (under the open science initiative).



### Privacy-Preserving Deep Inference for Rich User Data on The Cloud
- **Arxiv ID**: http://arxiv.org/abs/1710.01727v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1710.01727v3)
- **Published**: 2017-10-04 19:15:32+00:00
- **Updated**: 2017-10-11 20:26:15+00:00
- **Authors**: Seyed Ali Osia, Ali Shahin Shamsabadi, Ali Taheri, Kleomenis Katevas, Hamid R. Rabiee, Nicholas D. Lane, Hamed Haddadi
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1703.02952
- **Journal**: None
- **Summary**: Deep neural networks are increasingly being used in a variety of machine learning applications applied to rich user data on the cloud. However, this approach introduces a number of privacy and efficiency challenges, as the cloud operator can perform secondary inferences on the available data. Recently, advances in edge processing have paved the way for more efficient, and private, data processing at the source for simple tasks and lighter models, though they remain a challenge for larger, and more complicated models. In this paper, we present a hybrid approach for breaking down large, complex deep models for cooperative, privacy-preserving analytics. We do this by breaking down the popular deep architectures and fine-tune them in a particular way. We then evaluate the privacy benefits of this approach based on the information exposed to the cloud service. We also asses the local inference cost of different layers on a modern handset for mobile applications. Our evaluations show that by using certain kind of fine-tuning and embedding techniques and at a small processing costs, we can greatly reduce the level of information available to unintended tasks applied to the data feature on the cloud, and hence achieving the desired tradeoff between privacy and performance.



### Energy-Based Spherical Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1710.01820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01820v1)
- **Published**: 2017-10-04 22:22:50+00:00
- **Updated**: 2017-10-04 22:22:50+00:00
- **Authors**: Bailey Kong, Charless C. Fowlkes
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore an efficient variant of convolutional sparse coding with unit norm code vectors where reconstruction quality is evaluated using an inner product (cosine distance). To use these codes for discriminative classification, we describe a model we term Energy-Based Spherical Sparse Coding (EB-SSC) in which the hypothesized class label introduces a learned linear bias into the coding step. We evaluate and visualize performance of stacking this encoder to make a deep layered model for image classification.



