# Arxiv Papers in cs.CV on 2015-06-03
### Bilinear Random Projections for Locality-Sensitive Binary Codes
- **Arxiv ID**: http://arxiv.org/abs/1506.01092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.01092v1)
- **Published**: 2015-06-03 00:30:26+00:00
- **Updated**: 2015-06-03 00:30:26+00:00
- **Authors**: Saehoon Kim, Seungjin Choi
- **Comment**: 11 pages, 23 figures, CVPR-2015
- **Journal**: None
- **Summary**: Locality-sensitive hashing (LSH) is a popular data-independent indexing method for approximate similarity search, where random projections followed by quantization hash the points from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. Most of high-dimensional visual descriptors for images exhibit a natural matrix structure. When visual descriptors are represented by high-dimensional feature vectors and long binary codes are assigned, a random projection matrix requires expensive complexities in both space and time. In this paper we analyze a bilinear random projection method where feature matrices are transformed to binary codes by two smaller random projection matrices. We base our theoretical analysis on extending Raginsky and Lazebnik's result where random Fourier features are composed with random binary quantizers to form locality sensitive binary codes. To this end, we answer the following two questions: (1) whether a bilinear random projection also yields similarity-preserving binary codes; (2) whether a bilinear random projection yields performance gain or loss, compared to a large linear projection. Regarding the first question, we present upper and lower bounds on the expected Hamming distance between binary codes produced by bilinear random projections. In regards to the second question, we analyze the upper and lower bounds on covariance between two bits of binary codes, showing that the correlation between two bits is small. Numerical experiments on MNIST and Flickr45K datasets confirm the validity of our method.



### Hyperspectral Image Classification and Clutter Detection via Multiple Structural Embeddings and Dimension Reductions
- **Arxiv ID**: http://arxiv.org/abs/1506.01115v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6; I.4.8; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1506.01115v1)
- **Published**: 2015-06-03 04:04:43+00:00
- **Updated**: 2015-06-03 04:04:43+00:00
- **Authors**: Alexandros-Stavros Iliopoulos, Tiancheng Liu, Xiaobai Sun
- **Comment**: 13 pages, 6 figures (30 images), submitted to International
  Conference on Computer Vision (ICCV) 2015
- **Journal**: None
- **Summary**: We present a new and effective approach for Hyperspectral Image (HSI) classification and clutter detection, overcoming a few long-standing challenges presented by HSI data characteristics. Residing in a high-dimensional spectral attribute space, HSI data samples are known to be strongly correlated in their spectral signatures, exhibit nonlinear structure due to several physical laws, and contain uncertainty and noise from multiple sources. In the presented approach, we generate an adaptive, structurally enriched representation environment, and employ the locally linear embedding (LLE) in it. There are two structure layers external to LLE. One is feature space embedding: the HSI data attributes are embedded into a discriminatory feature space where spatio-spectral coherence and distinctive structures are distilled and exploited to mitigate various difficulties encountered in the native hyperspectral attribute space. The other structure layer encloses the ranges of algorithmic parameters for LLE and feature embedding, and supports a multiplexing and integrating scheme for contending with multi-source uncertainty. Experiments on two commonly used HSI datasets with a small number of learning samples have rendered remarkably high-accuracy classification results, as well as distinctive maps of detected clutter regions.



### Unsupervised domain adaption dictionary learning for visual recognition
- **Arxiv ID**: http://arxiv.org/abs/1506.01125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.01125v1)
- **Published**: 2015-06-03 05:21:37+00:00
- **Updated**: 2015-06-03 05:21:37+00:00
- **Authors**: Zhun Zhong, Zongmin Li, Runlin Li, Xiaoxia Sun
- **Comment**: 5 pages, 3 figures, ICIP 2015
- **Journal**: None
- **Summary**: Over the last years, dictionary learning method has been extensively applied to deal with various computer vision recognition applications, and produced state-of-the-art results. However, when the data instances of a target domain have a different distribution than that of a source domain, the dictionary learning method may fail to perform well. In this paper, we address the cross-domain visual recognition problem and propose a simple but effective unsupervised domain adaption approach, where labeled data are only from source domain. In order to bring the original data in source and target domain into the same distribution, the proposed method forcing nearest coupled data between source and target domain to have identical sparse representations while jointly learning dictionaries for each domain, where the learned dictionaries can reconstruct original data in source and target domain respectively. So that sparse representations of original data can be used to perform visual recognition tasks. We demonstrate the effectiveness of our approach on standard datasets. Our method performs on par or better than competitive state-of-the-art methods.



### What value do explicit high level concepts have in vision to language problems?
- **Arxiv ID**: http://arxiv.org/abs/1506.01144v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.01144v6)
- **Published**: 2015-06-03 07:06:11+00:00
- **Updated**: 2016-04-28 04:59:36+00:00
- **Authors**: Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, Anton van den Hengel
- **Comment**: Accepted to IEEE Conf. Computer Vision and Pattern Recognition 2016.
  Fixed title
- **Journal**: None
- **Summary**: Much of the recent progress in Vision-to-Language (V2L) problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. We propose here a method of incorporating high-level concepts into the very successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art performance in both image captioning and visual question answering. We also show that the same mechanism can be used to introduce external semantic information and that doing so further improves performance. In doing so we provide an analysis of the value of high level semantic information in V2L problems.



### Understanding deep features with computer-generated imagery
- **Arxiv ID**: http://arxiv.org/abs/1506.01151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.01151v1)
- **Published**: 2015-06-03 07:41:14+00:00
- **Updated**: 2015-06-03 07:41:14+00:00
- **Authors**: Mathieu Aubry, Bryan Russell
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an approach for analyzing the variation of features generated by convolutional neural networks (CNNs) with respect to scene factors that occur in natural images. Such factors may include object style, 3D viewpoint, color, and scene lighting configuration. Our approach analyzes CNN feature responses corresponding to different scene factors by controlling for them via rendering using a large database of 3D CAD models. The rendered images are presented to a trained CNN and responses for different layers are studied with respect to the input scene factors. We perform a decomposition of the responses based on knowledge of the input scene factors and analyze the resulting components. In particular, we quantify their relative importance in the CNN responses and visualize them using principal component analysis. We show qualitative and quantitative results of our study on three CNNs trained on large image datasets: AlexNet, Places, and Oxford VGG. We observe important differences across the networks and CNN layers for different scene factors and object categories. Finally, we demonstrate that our analysis based on computer-generated imagery translates to the network representation of natural images.



### Image Retrieval System Base on EMD Similarity Measure and S-Tree
- **Arxiv ID**: http://arxiv.org/abs/1506.01165v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, H.2.8; H.3.3
- **Links**: [PDF](http://arxiv.org/pdf/1506.01165v1)
- **Published**: 2015-06-03 08:50:13+00:00
- **Updated**: 2015-06-03 08:50:13+00:00
- **Authors**: Thanh Manh Le, Thanh The Van
- **Comment**: 14 pages, 3 figures, Appendix
- **Journal**: None
- **Summary**: The paper approaches the binary signature for each image based on the percentage of the pixels in each color images, at the same time the paper builds a similar measure between images based on EMD (Earth Mover's Distance). Besides, the paper proceeded to create the S-tree based on the similar measure EMD to store the image's binary signatures to quickly query image signature data. From there, the paper build an image retrieval algorithm and CBIR (Content-Based Image Retrieval) based on a similar measure EMD and S-tree. Based on this theory, the paper proceeded to build application and experimental assessment of the process of querying image on the database system which have over 10,000 images.



### Color Image Retrieval Using Fuzzy Measure Hamming and S-Tree
- **Arxiv ID**: http://arxiv.org/abs/1506.01166v1
- **DOI**: None
- **Categories**: **cs.CV**, H.2.8; H.3.3
- **Links**: [PDF](http://arxiv.org/pdf/1506.01166v1)
- **Published**: 2015-06-03 08:55:12+00:00
- **Updated**: 2015-06-03 08:55:12+00:00
- **Authors**: Thanh The Van, Thanh Manh Le
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: This chapter approaches the image retrieval system on the base of the colors of image. It creates fuzzy signature to describe the color of image on color space HSV and builds fuzzy Hamming distance (FHD) to evaluate the similarity between the images. In order to reduce the storage space and speed up the search of similar images, it aims to create S-tree to store fuzzy signature relies on FHD and builds image retrieval algorithm on S-tree. Then, it provides the content-based image retrieval (CBIR) and an image retrieval method on FHD and S-tree. Last but not least, based on this theory, it also presents an application and experimental assessment of the process of querying similar image on the database system over 10,000 images.



### Cyclical Learning Rates for Training Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1506.01186v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1506.01186v6)
- **Published**: 2015-06-03 09:54:31+00:00
- **Updated**: 2017-04-04 11:34:46+00:00
- **Authors**: Leslie N. Smith
- **Comment**: Presented at WACV 2017; see https://github.com/bckenstler/CLR for
  instructions to implement CLR in Keras
- **Journal**: None
- **Summary**: It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.



### Implementation of Training Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1506.01195v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1506.01195v2)
- **Published**: 2015-06-03 10:18:49+00:00
- **Updated**: 2015-06-04 02:10:39+00:00
- **Authors**: Tianyi Liu, Shuangsang Fang, Yuehui Zhao, Peng Wang, Jun Zhang
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Deep learning refers to the shining branch of machine learning that is based on learning levels of representations. Convolutional Neural Networks (CNN) is one kind of deep neural network. It can study concurrently. In this article, we gave a detailed analysis of the process of CNN algorithm both the forward process and back propagation. Then we applied the particular convolutional neural network to implement the typical face recognition problem by java. Then, a parallel strategy was proposed in section4. In addition, by measuring the actual time of forward and backward computing, we analysed the maximal speed up and parallel efficiency theoretically.



### One-to-many face recognition with bilinear CNNs
- **Arxiv ID**: http://arxiv.org/abs/1506.01342v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.01342v5)
- **Published**: 2015-06-03 18:34:41+00:00
- **Updated**: 2016-03-28 21:32:33+00:00
- **Authors**: Aruni RoyChowdhury, Tsung-Yu Lin, Subhransu Maji, Erik Learned-Miller
- **Comment**: Published version at WACV 2016
- **Journal**: None
- **Summary**: The recent explosive growth in convolutional neural network (CNN) research has produced a variety of new architectures for deep learning. One intriguing new architecture is the bilinear CNN (B-CNN), which has shown dramatic performance gains on certain fine-grained recognition problems [15]. We apply this new CNN to the challenging new face recognition benchmark, the IARPA Janus Benchmark A (IJB-A) [12]. It features faces from a large number of identities in challenging real-world conditions. Because the face images were not identified automatically using a computerized face detection system, it does not have the bias inherent in such a database. We demonstrate the performance of the B-CNN model beginning from an AlexNet-style network pre-trained on ImageNet. We then show results for fine-tuning using a moderate-sized and public external database, FaceScrub [17]. We also present results with additional fine-tuning on the limited training data provided by the protocol. In each case, the fine-tuned bilinear model shows substantial improvements over the standard CNN. Finally, we demonstrate how a standard CNN pre-trained on a large face database, the recently released VGG-Face model [20], can be converted into a B-CNN without any additional feature training. This B-CNN improves upon the CNN performance on the IJB-A benchmark, achieving 89.5% rank-1 recall.



### Recognition of Changes in SAR Images Based on Gauss-Log Ratio and MRFFCM
- **Arxiv ID**: http://arxiv.org/abs/1506.01398v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.01398v1)
- **Published**: 2015-06-03 20:38:37+00:00
- **Updated**: 2015-06-03 20:38:37+00:00
- **Authors**: Jismy Alphonse, Biju V. G.
- **Comment**: 7 pages, 7 figures, 2 tables in International Journal of advanced
  studies in Computer Science and Engineering (IJASCSE), ISSN : 2278 7917,
  Volume 4 Issue 5, 2015, www.ijascse.org
- **Journal**: None
- **Summary**: A modified version of MRFFCM (Markov Random Field Fuzzy C means) based SAR (Synthetic aperture Radar) image change detection method is proposed in this paper. It involves three steps: Difference Image (DI) generation by using Gauss-log ratio operator, speckle noise reduction by SRAD (Speckle Reducing Anisotropic Diffusion), and the detection of changed regions by using MRFFCM. The proposed method is compared with existing methods such as FCM and MRFFCM using simulated and real SAR images. The measures used for evaluation includes Overall Error (OE), Percentage Correct Classification (PCC), Kappa Coefficient (KC), Root Mean Square Error (RMSE), and Peak Signal to Noise Ratio (PSNR). The results show that the proposed method is better compared to FCM and MRFFCM based change detection method.



