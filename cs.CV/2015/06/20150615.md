# Arxiv Papers in cs.CV on 2015-06-15
### A Survey of Multithreading Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1506.04472v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.04472v6)
- **Published**: 2015-06-15 03:52:36+00:00
- **Updated**: 2017-10-04 02:55:03+00:00
- **Authors**: Elham Sagheb Hossein Pour
- **Comment**: 6 Pages, 1 Figure
- **Journal**: None
- **Summary**: Digital image analysis has made a big advance in many areas of enterprise applications including medicine, industry, and entertainment by assisting the extraction of semantic information from digital images. However, its large computational complexity has been a trouble to most real-time developments. While image analysis in general has been studied for a log period in computer science community, the use of multithreading strategy as the most efficient improving computational capacity technique has been limited so far. In this survey an attempt is made to explain the current knowledge and so far progresses in incorporating image analysis with multithreading approaches. The present work also provides insights and tendencies for the possible future enhancement of multithreading image analysis.



### Circle-based Eye Center Localization (CECL)
- **Arxiv ID**: http://arxiv.org/abs/1506.04500v2
- **DOI**: 10.1109/MVA.2015.7153202
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.04500v2)
- **Published**: 2015-06-15 07:54:13+00:00
- **Updated**: 2015-08-21 01:52:53+00:00
- **Authors**: Yustinus Eko Soelistio, Eric Postma, Alfons Maes
- **Comment**: Published and presented at The 14th IAPR International Conference on
  Machine Vision Applications, 2015. http://www.mva-org.jp/mva2015/
- **Journal**: None
- **Summary**: We propose an improved eye center localization method based on the Hough transform, called Circle-based Eye Center Localization (CECL) that is simple, robust, and achieves accuracy on a par with typically more complex state-of-the-art methods. The CECL method relies on color and shape cues that distinguish the iris from other facial structures. The accuracy of the CECL method is demonstrated through a comparison with 15 state-of-the-art eye center localization methods against five error thresholds, as reported in the literature. The CECL method achieved an accuracy of 80.8% to 99.4% and ranked first for 2 of the 5 thresholds. It is concluded that the CECL method offers an attractive alternative to existing methods for automatic eye center localization.



### Optimising Spatial and Tonal Data for PDE-based Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1506.04566v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC, 94A08, 65Nxx, 65Kxx
- **Links**: [PDF](http://arxiv.org/pdf/1506.04566v1)
- **Published**: 2015-06-15 12:15:26+00:00
- **Updated**: 2015-06-15 12:15:26+00:00
- **Authors**: Laurent Hoeltgen, Markus Mainberger, Sebastian Hoffmann, Joachim Weickert, Ching Hoo Tang, Simon Setzer, Daniel Johannsen, Frank Neumann, Benjamin Doerr
- **Comment**: None
- **Journal**: None
- **Summary**: Some recent methods for lossy signal and image compression store only a few selected pixels and fill in the missing structures by inpainting with a partial differential equation (PDE). Suitable operators include the Laplacian, the biharmonic operator, and edge-enhancing anisotropic diffusion (EED). The quality of such approaches depends substantially on the selection of the data that is kept. Optimising this data in the domain and codomain gives rise to challenging mathematical problems that shall be addressed in our work.   In the 1D case, we prove results that provide insights into the difficulty of this problem, and we give evidence that a splitting into spatial and tonal (i.e. function value) optimisation does hardly deteriorate the results. In the 2D setting, we present generic algorithms that achieve a high reconstruction quality even if the specified data is very sparse. To optimise the spatial data, we use a probabilistic sparsification, followed by a nonlocal pixel exchange that avoids getting trapped in bad local optima. After this spatial optimisation we perform a tonal optimisation that modifies the function values in order to reduce the global reconstruction error. For homogeneous diffusion inpainting, this comes down to a least squares problem for which we prove that it has a unique solution. We demonstrate that it can be found efficiently with a gradient descent approach that is accelerated with fast explicit diffusion (FED) cycles. Our framework allows to specify the desired density of the inpainting mask a priori. Moreover, is more generic than other data optimisation approaches for the sparse inpainting problem, since it can also be extended to nonlinear inpainting operators such as EED. This is exploited to achieve reconstructions with state-of-the-art quality.   We also give an extensive literature survey on PDE-based image compression methods.



### ParseNet: Looking Wider to See Better
- **Arxiv ID**: http://arxiv.org/abs/1506.04579v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.04579v2)
- **Published**: 2015-06-15 13:00:59+00:00
- **Updated**: 2015-11-19 22:19:28+00:00
- **Authors**: Wei Liu, Andrew Rabinovich, Alexander C. Berg
- **Comment**: ICLR 2016 submission
- **Journal**: None
- **Summary**: We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at https://github.com/weiliu89/caffe/tree/fcn .



### Flow Segmentation in Dense Crowds
- **Arxiv ID**: http://arxiv.org/abs/1506.04608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.04608v1)
- **Published**: 2015-06-15 14:14:20+00:00
- **Updated**: 2015-06-15 14:14:20+00:00
- **Authors**: Javairia Nazir, Mehreen Sirshar
- **Comment**: None
- **Journal**: None
- **Summary**: A framework is proposed in this paper that is used to segment flow of dense crowds. The flow field that is generated by the movement in the crowd is treated just like an aperiodic dynamic system. On this flow field a grid of particles is put over for particle advection by the use of a numerical integration scheme. Then flow maps are generated which associates the initial position of the particles with final position. The gradient of the flow maps gives the amount of divergence of the neighboring particles. For forward integration and analysis forward Finite time Lyapunov Exponent is calculated and backward Finite time Lyapunov Exponent is also calculated it gives the Lagrangian coherent structures of the flow in crowd. Lagrangian Coherent Structures basically divides the flow in crowd into regions and these regions have different dynamics. These regions are then used to get the boundary in the different flow segments by using water shed algorithm. The experiment is conducted on the crowd dataset of UCF (University of central Florida).



### Thin Structure Estimation with Curvature Regularization
- **Arxiv ID**: http://arxiv.org/abs/1506.04654v2
- **DOI**: 10.1109/ICCV.2015.53
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.04654v2)
- **Published**: 2015-06-15 16:28:01+00:00
- **Updated**: 2015-09-16 15:40:15+00:00
- **Authors**: Dmitrii Marin, Yuri Boykov, Yuchen Zhong
- **Comment**: D. Marin, Y. Zhong, M. Drangova, Y. Boykov. Thin Structure Estimation
  with Curvature Regularization. International Conference on Computer Vision
  (ICCV), Santiago, Chili, December 2015, to appear
- **Journal**: The IEEE International Conference on Computer Vision (ICCV), 2015,
  pp. 397-405
- **Summary**: Many applications in vision require estimation of thin structures such as boundary edges, surfaces, roads, blood vessels, neurons, etc. Unlike most previous approaches, we simultaneously detect and delineate thin structures with sub-pixel localization and real-valued orientation estimation. This is an ill-posed problem that requires regularization. We propose an objective function combining detection likelihoods with a prior minimizing curvature of the center-lines or surfaces. Unlike simple block-coordinate descent, we develop a novel algorithm that is able to perform joint optimization of location and detection variables more effectively. Our lower bound optimization algorithm applies to quadratic or absolute curvature. The proposed early vision framework is sufficiently general and it can be used in many higher-level applications. We illustrate the advantage of our approach on a range of 2D and 3D examples.



### Leveraging the Power of Gabor Phase for Face Identification: A Block Matching Approach
- **Arxiv ID**: http://arxiv.org/abs/1506.04655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.04655v1)
- **Published**: 2015-06-15 16:28:03+00:00
- **Updated**: 2015-06-15 16:28:03+00:00
- **Authors**: Yang Zhong, Haibo Li
- **Comment**: None
- **Journal**: None
- **Summary**: Different from face verification, face identification is much more demanding. To reach comparable performance, an identifier needs to be roughly N times better than a verifier. To expect a breakthrough in face identification, we need a fresh look at the fundamental building blocks of face recognition. In this paper we focus on the selection of a suitable signal representation and better matching strategy for face identification. We demonstrate how Gabor phase could be leveraged to improve the performance of face identification by using the Block Matching method. Compared to the existing approaches, the proposed method features much lower algorithmic complexity: face images are only filtered by a single-scale Gabor filter pair and the matching is performed between any pairs of face images at hand without involving any training process. Benchmark evaluations show that the proposed approach is totally comparable to and even better than state-of-the-art algorithms, which are typically based on more features extracted from a large set of Gabor faces and/or rely on heavy training processes.



### Multi-path Convolutional Neural Networks for Complex Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1506.04701v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.04701v3)
- **Published**: 2015-06-15 18:51:35+00:00
- **Updated**: 2015-06-22 16:22:23+00:00
- **Authors**: Mingming Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks demonstrate high performance on ImageNet Large-Scale Visual Recognition Challenges contest. Nevertheless, the published results only show the overall performance for all image classes. There is no further analysis why certain images get worse results and how they could be improved. In this paper, we provide deep performance analysis based on different types of images and point out the weaknesses of convolutional neural networks through experiment. We design a novel multiple paths convolutional neural network, which feeds different versions of images into separated paths to learn more comprehensive features. This model has better presentation for image than the traditional single path model. We acquire better classification results on complex validation set on both top 1 and top 5 scores than the best ILSVRC 2013 classification model.



### Slow and steady feature analysis: higher order temporal coherence in video
- **Arxiv ID**: http://arxiv.org/abs/1506.04714v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.04714v2)
- **Published**: 2015-06-15 19:26:38+00:00
- **Updated**: 2016-04-14 18:37:33+00:00
- **Authors**: Dinesh Jayaraman, Kristen Grauman
- **Comment**: in Computer Vision and Pattern Recognition (CVPR) 2016, Las Vegas,
  NV, June 2016
- **Journal**: None
- **Summary**: How can unlabeled video augment visual learning? Existing methods perform "slow" feature analysis, encouraging the representations of temporally close frames to exhibit only small differences. While this standard approach captures the fact that high-level visual signals change slowly over time, it fails to capture *how* the visual content changes. We propose to generalize slow feature analysis to "steady" feature analysis. The key idea is to impose a prior that higher order derivatives in the learned feature space must be small. To this end, we train a convolutional neural network with a regularizer on tuples of sequential frames from unlabeled video. It encourages feature changes over time to be smooth, i.e., similar to the most recent changes. Using five diverse datasets, including unlabeled YouTube and KITTI videos, we demonstrate our method's impact on object, scene, and action recognition tasks. We further show that our features learned from unlabeled video can even surpass a standard heavily supervised pretraining approach.



### Automatic Layer Separation using Light Field Imaging
- **Arxiv ID**: http://arxiv.org/abs/1506.04721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.04721v1)
- **Published**: 2015-06-15 19:36:25+00:00
- **Updated**: 2015-06-15 19:36:25+00:00
- **Authors**: Qiaosong Wang, Haiting Lin, Yi Ma, Sing Bing Kang, Jingyi Yu
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: We propose a novel approach that jointly removes reflection or translucent layer from a scene and estimates scene depth. The input data are captured via light field imaging. The problem is couched as minimizing the rank of the transmitted scene layer via Robust Principle Component Analysis (RPCA). We also impose regularization based on piecewise smoothness, gradient sparsity, and layer independence to simultaneously recover 3D geometry of the transmitted layer. Experimental results on synthetic and real data show that our technique is robust and reliable, and can handle a broad range of layer separation problems.



### Layered Interpretation of Street View Images
- **Arxiv ID**: http://arxiv.org/abs/1506.04723v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.04723v2)
- **Published**: 2015-06-15 19:38:59+00:00
- **Updated**: 2015-07-29 15:38:28+00:00
- **Authors**: Ming-Yu Liu, Shuoxin Lin, Srikumar Ramalingam, Oncel Tuzel
- **Comment**: The paper will be presented in the 2015 Robotics: Science and Systems
  Conference (RSS)
- **Journal**: None
- **Summary**: We propose a layered street view model to encode both depth and semantic information on street view images for autonomous driving. Recently, stixels, stix-mantics, and tiered scene labeling methods have been proposed to model street view images. We propose a 4-layer street view model, a compact representation over the recently proposed stix-mantics model. Our layers encode semantic classes like ground, pedestrians, vehicles, buildings, and sky in addition to the depths. The only input to our algorithm is a pair of stereo images. We use a deep neural network to extract the appearance features for semantic classes. We use a simple and an efficient inference algorithm to jointly estimate both semantic classes and layered depth values. Our method outperforms other competing approaches in Daimler urban scene segmentation dataset. Our algorithm is massively parallelizable, allowing a GPU implementation with a processing speed about 9 fps.



### Image-based Recommendations on Styles and Substitutes
- **Arxiv ID**: http://arxiv.org/abs/1506.04757v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1506.04757v1)
- **Published**: 2015-06-15 20:01:49+00:00
- **Updated**: 2015-06-15 20:01:49+00:00
- **Authors**: Julian McAuley, Christopher Targett, Qinfeng Shi, Anton van den Hengel
- **Comment**: 11 pages, 10 figures, SIGIR 2015
- **Journal**: None
- **Summary**: Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.



