# Arxiv Papers in cs.CV on 2015-06-22
### DeepOrgan: Multi-level Deep Convolutional Networks for Automated Pancreas Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1506.06448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.06448v1)
- **Published**: 2015-06-22 02:56:42+00:00
- **Updated**: 2015-06-22 02:56:42+00:00
- **Authors**: Holger R. Roth, Le Lu, Amal Farag, Hoo-Chang Shin, Jiamin Liu, Evrim Turkbey, Ronald M. Summers
- **Comment**: To be presented at MICCAI 2015 - 18th International Conference on
  Medical Computing and Computer Assisted Interventions, Munich, Germany
- **Journal**: None
- **Summary**: Automatic organ segmentation is an important yet challenging problem for medical image analysis. The pancreas is an abdominal organ with very high anatomical variability. This inhibits previous segmentation methods from achieving high accuracies, especially compared to other organs such as the liver, heart or kidneys. In this paper, we present a probabilistic bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans, using multi-level deep convolutional networks (ConvNets). We propose and evaluate several variations of deep ConvNets in the context of hierarchical, coarse-to-fine classification on image patches and regions, i.e. superpixels. We first present a dense labeling of local image patches via $P{-}\mathrm{ConvNet}$ and nearest neighbor fusion. Then we describe a regional ConvNet ($R_1{-}\mathrm{ConvNet}$) that samples a set of bounding boxes around each image superpixel at different scales of contexts in a "zoom-out" fashion. Our ConvNets learn to assign class probabilities for each superpixel region of being pancreas. Last, we study a stacked $R_2{-}\mathrm{ConvNet}$ leveraging the joint space of CT intensities and the $P{-}\mathrm{ConvNet}$ dense probability maps. Both 3D Gaussian smoothing and 2D conditional random fields are exploited as structured predictions for post-processing. We evaluate on CT images of 82 patients in 4-fold cross-validation. We achieve a Dice Similarity Coefficient of 83.6$\pm$6.3% in training and 71.8$\pm$10.7% in testing.



### Understanding Neural Networks Through Deep Visualization
- **Arxiv ID**: http://arxiv.org/abs/1506.06579v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1506.06579v1)
- **Published**: 2015-06-22 12:57:15+00:00
- **Updated**: 2015-06-22 12:57:15+00:00
- **Authors**: Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, Hod Lipson
- **Comment**: 12 pages. To appear at ICML Deep Learning Workshop 2015
- **Journal**: None
- **Summary**: Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.



### Modality-dependent Cross-media Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1506.06628v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1506.06628v2)
- **Published**: 2015-06-22 14:33:39+00:00
- **Updated**: 2015-06-23 01:34:01+00:00
- **Authors**: Yunchao Wei, Yao Zhao, Zhenfeng Zhu, Shikui Wei, Yanhui Xiao, Jiashi Feng, Shuicheng Yan
- **Comment**: in ACM Transactions on Intelligent Systems and Technology
- **Journal**: None
- **Summary**: In this paper, we investigate the cross-media retrieval between images and text, i.e., using image to search text (I2T) and using text to search images (T2I). Existing cross-media retrieval methods usually learn one couple of projections, by which the original features of images and text can be projected into a common latent space to measure the content similarity. However, using the same projections for the two different retrieval tasks (I2T and T2I) may lead to a tradeoff between their respective performances, rather than their best performances. Different from previous works, we propose a modality-dependent cross-media retrieval (MDCR) model, where two couples of projections are learned for different cross-media retrieval tasks instead of one couple of projections. Specifically, by jointly optimizing the correlation between images and text and the linear regression from one modal space (image or text) to the semantic space, two couples of mappings are learned to project images and text from their original feature spaces into two common latent subspaces (one for I2T and the other for T2I). Extensive experiments show the superiority of the proposed MDCR compared with other methods. In particular, based the 4,096 dimensional convolutional neural network (CNN) visual feature and 100 dimensional LDA textual feature, the mAP of the proposed method achieves 41.5\%, which is a new state-of-the-art performance on the Wikipedia dataset.



### Target Tracking In Real Time Surveillance Cameras and Videos
- **Arxiv ID**: http://arxiv.org/abs/1506.06659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.06659v1)
- **Published**: 2015-06-22 15:55:11+00:00
- **Updated**: 2015-06-22 15:55:11+00:00
- **Authors**: Nayyab Naseem, Mehreen Sirshar
- **Comment**: None
- **Journal**: None
- **Summary**: Security concerns has been kept on increasing, so it is important for everyone to keep their property safe from thefts and destruction. So the need for surveillance techniques are also increasing. The system has been developed to detect the motion in a video. A system has been developed for real time applications by using the techniques of background subtraction and frame differencing. In this system, motion is detected from the webcam or from the real time video. Background subtraction and frames differencing method has been used to detect the moving target. In background subtraction method, current frame is subtracted from the referenced frame and then the threshold is applied. If the difference is greater than the threshold then it is considered as the pixel from the moving object, otherwise it is considered as background pixel. Similarly, two frames difference method takes difference between two continuous frames. Then that resultant difference frame is thresholded and the amount of difference pixels is calculated.



### Adaptive Digital Scan Variable Pixels
- **Arxiv ID**: http://arxiv.org/abs/1506.06681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.06681v1)
- **Published**: 2015-06-22 17:09:04+00:00
- **Updated**: 2015-06-22 17:09:04+00:00
- **Authors**: Sherin Sugathan, Reshma Scaria, Alex Pappachen James
- **Comment**: 4th International Conference on Advances in Computing, Communications
  and Informatics, August, 2015
- **Journal**: None
- **Summary**: The square and rectangular shape of the pixels in the digital images for sensing and display purposes introduces several inaccuracies in the representation of digital images. The major disadvantage of square pixel shapes is the inability to accurately capture and display the details in the objects having variable orientations to edges, shapes and regions. This effect can be observed by the inaccurate representation of diagonal edges in low resolution square pixel images. This paper explores a less investigated idea of using variable shaped pixels for improving visual quality of image scans without increasing the square pixel resolution. The proposed adaptive filtering technique reports an improvement in image PSNR.



### Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books
- **Arxiv ID**: http://arxiv.org/abs/1506.06724v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1506.06724v1)
- **Published**: 2015-06-22 19:26:56+00:00
- **Updated**: 2015-06-22 19:26:56+00:00
- **Authors**: Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler
- **Comment**: None
- **Journal**: None
- **Summary**: Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.



### DeepStereo: Learning to Predict New Views from the World's Imagery
- **Arxiv ID**: http://arxiv.org/abs/1506.06825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.06825v1)
- **Published**: 2015-06-22 23:48:21+00:00
- **Updated**: 2015-06-22 23:48:21+00:00
- **Authors**: John Flynn, Ivan Neulander, James Philbin, Noah Snavely
- **Comment**: Video showing additional results available at
  http://youtu.be/cizgVZ8rjKA
- **Journal**: None
- **Summary**: Deep networks have recently enjoyed enormous success when applied to recognition and classification problems in computer vision, but their use in graphics problems has been limited. In this work, we present a novel deep architecture that performs new view synthesis directly from pixels, trained from a large number of posed image sets. In contrast to traditional approaches which consist of multiple complex stages of processing, each of which require careful tuning and can fail in unexpected ways, our system is trained end-to-end. The pixels from neighboring views of a scene are presented to the network which then directly produces the pixels of the unseen view. The benefits of our approach include generality (we only require posed image sets and can easily apply our method to different domains), and high quality results on traditionally difficult scenes. We believe this is due to the end-to-end nature of our system which is able to plausibly generate pixels according to color, depth, and texture priors learnt automatically from the training data. To verify our method we show that it can convincingly reproduce known test views from nearby imagery. Additionally we show images rendered from novel viewpoints. To our knowledge, our work is the first to apply deep learning to the problem of new view synthesis from sets of real-world, natural imagery.



