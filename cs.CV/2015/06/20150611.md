# Arxiv Papers in cs.CV on 2015-06-11
### P-CNN: Pose-based CNN Features for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1506.03607v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.03607v2)
- **Published**: 2015-06-11 10:02:03+00:00
- **Updated**: 2015-09-23 10:48:29+00:00
- **Authors**: Guilhem Chéron, Ivan Laptev, Cordelia Schmid
- **Comment**: ICCV, December 2015, Santiago, Chile
- **Journal**: None
- **Summary**: This work targets human action recognition in video. While recent methods typically represent actions by statistics of local video features, here we argue for the importance of a representation derived from human pose. To this end we propose a new Pose-based Convolutional Neural Network descriptor (P-CNN) for action recognition. The descriptor aggregates motion and appearance information along tracks of human body parts. We investigate different schemes of temporal aggregation and experiment with P-CNN features obtained both for automatically estimated and manually annotated human poses. We evaluate our method on the recent and challenging JHMDB and MPII Cooking datasets. For both datasets our method shows consistent improvement over the state of the art.



### Constrained Convolutional Neural Networks for Weakly Supervised Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1506.03648v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1506.03648v2)
- **Published**: 2015-06-11 12:30:17+00:00
- **Updated**: 2015-10-18 23:51:40+00:00
- **Authors**: Deepak Pathak, Philipp Krähenbühl, Trevor Darrell
- **Comment**: 12 pages, ICCV 2015
- **Journal**: None
- **Summary**: We present an approach to learn a dense pixel-wise labeling from image-level tags. Each image-level tag imposes constraints on the output labeling of a Convolutional Neural Network (CNN) classifier. We propose Constrained CNN (CCNN), a method which uses a novel loss function to optimize for any set of linear constraints on the output space (i.e. predicted label distribution) of a CNN. Our loss formulation is easy to optimize and can be incorporated directly into standard stochastic gradient descent optimization. The key idea is to phrase the training objective as a biconvex optimization for linear models, which we then relax to nonlinear deep networks. Extensive experiments demonstrate the generality of our new learning framework. The constrained loss yields state-of-the-art results on weakly supervised semantic image segmentation. We further demonstrate that adding slightly more supervision can greatly improve the performance of the learning algorithm.



### Pose-Invariant 3D Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1506.03799v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.03799v1)
- **Published**: 2015-06-11 19:45:13+00:00
- **Updated**: 2015-06-11 19:45:13+00:00
- **Authors**: Amin Jourabloo, Xiaoming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Face alignment aims to estimate the locations of a set of landmarks for a given image. This problem has received much attention as evidenced by the recent advancement in both the methodology and performance. However, most of the existing works neither explicitly handle face images with arbitrary poses, nor perform large-scale experiments on non-frontal and profile face images. In order to address these limitations, this paper proposes a novel face alignment algorithm that estimates both 2D and 3D landmarks and their 2D visibilities for a face image with an arbitrary pose. By integrating a 3D deformable model, a cascaded coupled-regressor approach is designed to estimate both the camera projection matrix and the 3D landmarks. Furthermore, the 3D model also allows us to automatically estimate the 2D landmark visibilities via surface normals. We gather a substantially larger collection of all-pose face images to evaluate our algorithm and demonstrate superior performances than the state-of-the-art methods.



### Techniques for effective and efficient fire detection from social media images
- **Arxiv ID**: http://arxiv.org/abs/1506.03844v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.03844v2)
- **Published**: 2015-06-11 21:23:38+00:00
- **Updated**: 2015-07-04 20:02:17+00:00
- **Authors**: Marcos Bedo, Gustavo Blanco, Willian Oliveira, Mirela Cazzolato, Alceu Costa, Jose Rodrigues, Agma Traina, Caetano Traina Jr
- **Comment**: 12 pages, Proceedings of the International Conference on Enterprise
  Information Systems. Specifically: Marcos Bedo, Gustavo Blanco, Willian
  Oliveira, Mirela Cazzolato, Alceu Costa, Jose Rodrigues, Agma Traina, Caetano
  Traina, 2015, Techniques for effective and efficient fire detection from
  social media images, ICEIS, 34-45
- **Journal**: Int Conf on Enterp Inf Systems 34-45 SCITEPRESS (2015)
- **Summary**: Social media could provide valuable information to support decision making in crisis management, such as in accidents, explosions and fires. However, much of the data from social media are images, which are uploaded in a rate that makes it impossible for human beings to analyze them. Despite the many works on image analysis, there are no fire detection studies on social media. To fill this gap, we propose the use and evaluation of a broad set of content-based image retrieval and classification techniques for fire detection. Our main contributions are: (i) the development of the Fast-Fire Detection method (FFDnR), which combines feature extractor and evaluation functions to support instance-based learning, (ii) the construction of an annotated set of images with ground-truth depicting fire occurrences -- the FlickrFire dataset, and (iii) the evaluation of 36 efficient image descriptors for fire detection. Using real data from Flickr, our results showed that FFDnR was able to achieve a precision for fire detection comparable to that of human annotators. Therefore, our work shall provide a solid basis for further developments on monitoring images from social media.



### Tree-Cut for Probabilistic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1506.03852v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1506.03852v1)
- **Published**: 2015-06-11 21:55:06+00:00
- **Updated**: 2015-06-11 21:55:06+00:00
- **Authors**: Shell X. Hu, Christopher K. I. Williams, Sinisa Todorovic
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new probabilistic generative model for image segmentation, i.e. the task of partitioning an image into homogeneous regions. Our model is grounded on a mid-level image representation, called a region tree, in which regions are recursively split into subregions until superpixels are reached. Given the region tree, image segmentation is formalized as sampling cuts in the tree from the model. Inference for the cuts is exact, and formulated using dynamic programming. Our tree-cut model can be tuned to sample segmentations at a particular scale of interest out of many possible multiscale image segmentations. This generalizes the common notion that there should be only one correct segmentation per image. Also, it allows moving beyond the standard single-scale evaluation, where the segmentation result for an image is averaged against the corresponding set of coarse and fine human annotations, to conduct a scale-specific evaluation. Our quantitative results are comparable to those of the leading gPb-owt-ucm method, with the notable advantage that we additionally produce a distribution over all possible tree-consistent segmentations of the image.



