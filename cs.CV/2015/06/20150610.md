# Arxiv Papers in cs.CV on 2015-06-10
### ICDAR 2015 Text Reading in the Wild Competition
- **Arxiv ID**: http://arxiv.org/abs/1506.03184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.03184v1)
- **Published**: 2015-06-10 06:46:55+00:00
- **Updated**: 2015-06-10 06:46:55+00:00
- **Authors**: Xinyu Zhou, Shuchang Zhou, Cong Yao, Zhimin Cao, Qi Yin
- **Comment**: 3 pages, 2 figures
- **Journal**: None
- **Summary**: Recently, text detection and recognition in natural scenes are becoming increasing popular in the computer vision community as well as the document analysis community. However, majority of the existing ideas, algorithms and systems are specifically designed for English. This technical report presents the final results of the ICDAR 2015 Text Reading in the Wild (TRW 2015) competition, which aims at establishing a benchmark for assessing detection and recognition algorithms devised for both Chinese and English scripts and providing a playground for researchers from the community. In this article, we describe in detail the dataset, tasks, evaluation protocols and participants of this competition, and report the performance of the participating methods. Moreover, promising directions for future research are discussed.



### Wide baseline stereo matching with convex bounded-distortion constraints
- **Arxiv ID**: http://arxiv.org/abs/1506.03301v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.03301v1)
- **Published**: 2015-06-10 13:48:34+00:00
- **Updated**: 2015-06-10 13:48:34+00:00
- **Authors**: Meirav Galun, Tal Amir, Tal Hassner, Ronen Basri, Yaron Lipman
- **Comment**: None
- **Journal**: None
- **Summary**: Finding correspondences in wide baseline setups is a challenging problem. Existing approaches have focused largely on developing better feature descriptors for correspondence and on accurate recovery of epipolar line constraints. This paper focuses on the challenging problem of finding correspondences once approximate epipolar constraints are given. We introduce a novel method that integrates a deformation model. Specifically, we formulate the problem as finding the largest number of corresponding points related by a bounded distortion map that obeys the given epipolar constraints. We show that, while the set of bounded distortion maps is not convex, the subset of maps that obey the epipolar line constraints is convex, allowing us to introduce an efficient algorithm for matching. We further utilize a robust cost function for matching and employ majorization-minimization for its optimization. Our experiments indicate that our method finds significantly more accurate maps than existing approaches.



### Optical Flow on Evolving Sphere-Like Surfaces
- **Arxiv ID**: http://arxiv.org/abs/1506.03358v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1506.03358v1)
- **Published**: 2015-06-10 15:24:19+00:00
- **Updated**: 2015-06-10 15:24:19+00:00
- **Authors**: Lukas F. Lang, Otmar Scherzer
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we consider optical flow on evolving Riemannian 2-manifolds which can be parametrised from the 2-sphere. Our main motivation is to estimate cell motion in time-lapse volumetric microscopy images depicting fluorescently labelled cells of a live zebrafish embryo. We exploit the fact that the recorded cells float on the surface of the embryo and allow for the extraction of an image sequence together with a sphere-like surface. We solve the resulting variational problem by means of a Galerkin method based on vector spherical harmonics and present numerical results computed from the aforementioned microscopy data.



### LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop
- **Arxiv ID**: http://arxiv.org/abs/1506.03365v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.03365v3)
- **Published**: 2015-06-10 15:38:47+00:00
- **Updated**: 2016-06-04 09:51:30+00:00
- **Authors**: Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, Jianxiong Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset.



### Convergence rates for pretraining and dropout: Guiding learning parameters using network structure
- **Arxiv ID**: http://arxiv.org/abs/1506.03412v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1506.03412v3)
- **Published**: 2015-06-10 17:59:57+00:00
- **Updated**: 2017-02-22 17:32:07+00:00
- **Authors**: Vamsi K. Ithapu, Sathya Ravi, Vikas Singh
- **Comment**: This manuscript is now superseded by arXiv:1511.05297 and the
  corresponding accepted paper in 54th Allerton Conference on Communication,
  Control and Computing (2017)
- **Journal**: None
- **Summary**: Unsupervised pretraining and dropout have been well studied, especially with respect to regularization and output consistency. However, our understanding about the explicit convergence rates of the parameter estimates, and their dependence on the learning (like denoising and dropout rate) and structural (like depth and layer lengths) aspects of the network is less mature. An interesting question in this context is to ask if the network structure could "guide" the choices of such learning parameters. In this work, we explore these gaps between network structure, the learning mechanisms and their interaction with parameter convergence rates. We present a way to address these issues based on the backpropagation convergence rates for general nonconvex objectives using first-order information. We then incorporate two learning mechanisms into this general framework -- denoising autoencoder and dropout, and subsequently derive the convergence rates of deep networks. Building upon these bounds, we provide insights into the choices of learning parameters and network sizes that achieve certain levels of convergence accuracy. The results derived here support existing empirical observations, and we also conduct a set of experiments to evaluate them.



### Image Tag Completion and Refinement by Subspace Clustering and Matrix Completion
- **Arxiv ID**: http://arxiv.org/abs/1506.03475v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.03475v2)
- **Published**: 2015-06-10 20:42:50+00:00
- **Updated**: 2016-08-08 02:14:37+00:00
- **Authors**: Yuqing Hou, Zhouchen Lin
- **Comment**: This paper has been withdrawn by the author due to a error in the
  model formulation
- **Journal**: None
- **Summary**: Tag-based image retrieval (TBIR) has drawn much attention in recent years due to the explosive amount of digital images and crowdsourcing tags. However, the TBIR applications still suffer from the deficient and inaccurate tags provided by users. Inspired by the subspace clustering methods, we formulate the tag completion problem in a subspace clustering model which assumes that images are sampled from subspaces, and complete the tags using the state-of-the-art Low Rank Representation (LRR) method. And we propose a matrix completion algorithm to further refine the tags. Our empirical results on multiple benchmark datasets for image annotation show that the proposed algorithm outperforms state-of-the-art approaches when handling missing and noisy tags.



### Generative Image Modeling Using Spatial LSTMs
- **Arxiv ID**: http://arxiv.org/abs/1506.03478v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1506.03478v2)
- **Published**: 2015-06-10 20:56:14+00:00
- **Updated**: 2015-09-18 08:06:06+00:00
- **Authors**: Lucas Theis, Matthias Bethge
- **Comment**: None
- **Journal**: None
- **Summary**: Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.



### BoWFire: Detection of Fire in Still Images by Integrating Pixel Color and Texture Analysis
- **Arxiv ID**: http://arxiv.org/abs/1506.03495v1
- **DOI**: 10.1109/SIBGRAPI.2015.19
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.03495v1)
- **Published**: 2015-06-10 22:16:36+00:00
- **Updated**: 2015-06-10 22:16:36+00:00
- **Authors**: Daniel Y. T. Chino, Letricia P. S. Avalhais, Jose F. Rodrigues Jr., Agma J. M. Traina
- **Comment**: 8 pages, Proceedings of the 28th SIBGRAPI Conference on Graphics,
  Patterns and Images, IEEE Press
- **Journal**: None
- **Summary**: Emergency events involving fire are potentially harmful, demanding a fast and precise decision making. The use of crowdsourcing image and videos on crisis management systems can aid in these situations by providing more information than verbal/textual descriptions. Due to the usual high volume of data, automatic solutions need to discard non-relevant content without losing relevant information. There are several methods for fire detection on video using color-based models. However, they are not adequate for still image processing, because they can suffer on high false-positive results. These methods also suffer from parameters with little physical meaning, which makes fine tuning a difficult task. In this context, we propose a novel fire detection method for still images that uses classification based on color features combined with texture classification on superpixel regions. Our method uses a reduced number of parameters if compared to previous works, easing the process of fine tuning the method. Results show the effectiveness of our method of reducing false-positives while its precision remains compatible with the state-of-the-art methods.



### Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1506.03500v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1506.03500v2)
- **Published**: 2015-06-10 22:57:20+00:00
- **Updated**: 2015-11-23 16:36:48+00:00
- **Authors**: Angeliki Lazaridou, Dat Tien Nguyen, Raffaella Bernardi, Marco Baroni
- **Comment**: A 6-page version to appear at the Multimodal Machine Learning NIPS
  2015 Workshop
- **Journal**: None
- **Summary**: We introduce language-driven image generation, the task of generating an image visualizing the semantic contents of a word embedding, e.g., given the word embedding of grasshopper, we generate a natural image of a grasshopper. We implement a simple method based on two mapping functions. The first takes as input a word embedding (as produced, e.g., by the word2vec toolkit) and maps it onto a high-level visual space (e.g., the space defined by one of the top layers of a Convolutional Neural Network). The second function maps this abstract visual representation to pixel space, in order to generate the target image. Several user studies suggest that the current system produces images that capture general visual properties of the concepts encoded in the word embedding, such as color or typical environment, and are sufficient to discriminate between general categories of objects.



