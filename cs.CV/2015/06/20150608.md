# Arxiv Papers in cs.CV on 2015-06-08
### EventNet: A Large Scale Structured Concept Library for Complex Event Detection in Video
- **Arxiv ID**: http://arxiv.org/abs/1506.02328v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.02328v2)
- **Published**: 2015-06-08 00:34:51+00:00
- **Updated**: 2015-08-17 17:13:23+00:00
- **Authors**: Guangnan Ye, Yitong Li, Hongliang Xu, Dong Liu, Shih-Fu Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Event-specific concepts are the semantic concepts designed for the events of interest, which can be used as a mid-level representation of complex events in videos. Existing methods only focus on defining event-specific concepts for a small number of predefined events, but cannot handle novel unseen events. This motivates us to build a large scale event-specific concept library that covers as many real-world events and their concepts as possible. Specifically, we choose WikiHow, an online forum containing a large number of how-to articles on human daily life events. We perform a coarse-to-fine event discovery process and discover 500 events from WikiHow articles. Then we use each event name as query to search YouTube and discover event-specific concepts from the tags of returned videos. After an automatic filter process, we end up with 95,321 videos and 4,490 concepts. We train a Convolutional Neural Network (CNN) model on the 95,321 videos over the 500 events, and use the model to extract deep learning feature from video content. With the learned deep learning feature, we train 4,490 binary SVM classifiers as the event-specific concept library. The concepts and events are further organized in a hierarchical structure defined by WikiHow, and the resultant concept library is called EventNet. Finally, the EventNet concept library is used to generate concept based representation of event videos. To the best of our knowledge, EventNet represents the first video event ontology that organizes events and their concepts into a semantic structure. It offers great potential for event retrieval and browsing. Extensive experiments over the zero-shot event retrieval task when no training samples are available show that the EventNet concept library consistently and significantly outperforms the state-of-the-art (such as the 20K ImageNet concepts trained with CNN) by a large margin up to 207%.



### Wavelets and continuous wavelet transform for autostereoscopic multiview images
- **Arxiv ID**: http://arxiv.org/abs/1506.02345v1
- **DOI**: 10.1364/AO.55.006275
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.02345v1)
- **Published**: 2015-06-08 03:47:17+00:00
- **Updated**: 2015-06-08 03:47:17+00:00
- **Authors**: Vladimir Saveljev
- **Comment**: 4 pages, 10 figures
- **Journal**: Applied Optics, Vol. 55, Issue 23, pp. 6275-6284 (2016)
- **Summary**: Recently, the reference functions for the synthesis and analysis of the autostereoscopic multiview and integral images in three-dimensional displays we introduced. In the current paper, we propose the wavelets to analyze such images. The wavelets are built on the reference functions as on the scaling functions of the wavelet analysis. The continuous wavelet transform was successfully applied to the testing wireframe binary objects. The restored locations correspond to the structure of the testing wireframe binary objects.



### A Tensor-Based Dictionary Learning Approach to Tomographic Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1506.04954v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA, 15A69, 65F22, 65K10
- **Links**: [PDF](http://arxiv.org/pdf/1506.04954v1)
- **Published**: 2015-06-08 08:55:28+00:00
- **Updated**: 2015-06-08 08:55:28+00:00
- **Authors**: Sara Soltani, Misha E. Kilmer, Per Christian Hansen
- **Comment**: 29 pages
- **Journal**: None
- **Summary**: We consider tomographic reconstruction using priors in the form of a dictionary learned from training images. The reconstruction has two stages: first we construct a tensor dictionary prior from our training data, and then we pose the reconstruction problem in terms of recovering the expansion coefficients in that dictionary. Our approach differs from past approaches in that a) we use a third-order tensor representation for our images and b) we recast the reconstruction problem using the tensor formulation. The dictionary learning problem is presented as a non-negative tensor factorization problem with sparsity constraints. The reconstruction problem is formulated in a convex optimization framework by looking for a solution with a sparse representation in the tensor dictionary. Numerical results show that our tensor formulation leads to very sparse representations of both the training images and the reconstructions due to the ability of representing repeated features compactly in the dictionary.



### Reflection Invariance: an important consideration of image orientation
- **Arxiv ID**: http://arxiv.org/abs/1506.02432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.02432v1)
- **Published**: 2015-06-08 10:44:36+00:00
- **Updated**: 2015-06-08 10:44:36+00:00
- **Authors**: Craig Henderson, Ebroul Izquierdo
- **Comment**: 7 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: In this position paper, we consider the state of computer vision research with respect to invariance to the horizontal orientation of an image -- what we term reflection invariance. We describe why we consider reflection invariance to be an important property and provide evidence where the absence of this invariance produces surprising inconsistencies in state-of-the-art systems. We demonstrate inconsistencies in methods of object detection and scene classification when they are presented with images and the horizontal mirror of those images. Finally, we examine where some of the invariance is exhibited in feature detection and descriptors, and make a case for future consideration of reflection invariance as a measure of quality in computer vision algorithms.



### SVM and ELM: Who Wins? Object Recognition with Deep Convolutional Features from ImageNet
- **Arxiv ID**: http://arxiv.org/abs/1506.02509v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1506.02509v1)
- **Published**: 2015-06-08 13:58:01+00:00
- **Updated**: 2015-06-08 13:58:01+00:00
- **Authors**: Lei Zhang, David Zhang
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: Deep learning with a convolutional neural network (CNN) has been proved to be very effective in feature extraction and representation of images. For image classification problems, this work aim at finding which classifier is more competitive based on high-level deep features of images. In this report, we have discussed the nearest neighbor, support vector machines and extreme learning machines for image classification under deep convolutional activation feature representation. Specifically, we adopt the benchmark object recognition dataset from multiple sources with domain bias for evaluating different classifiers. The deep features of the object dataset are obtained by a well-trained CNN with five convolutional layers and three fully-connected layers on the challenging ImageNet. Experiments demonstrate that the ELMs outperform SVMs in cross-domain recognition tasks. In particular, state-of-the-art results are obtained by kernel ELM which outperforms SVMs with about 4% of the average accuracy. The features and codes are available in http://www.escience.cn/people/lei/index.html



### Fast ConvNets Using Group-wise Brain Damage
- **Arxiv ID**: http://arxiv.org/abs/1506.02515v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.02515v2)
- **Published**: 2015-06-08 14:20:37+00:00
- **Updated**: 2015-12-07 18:11:36+00:00
- **Authors**: Vadim Lebedev, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: We revisit the idea of brain damage, i.e. the pruning of the coefficients of a neural network, and suggest how brain damage can be modified and used to speedup convolutional layers. The approach uses the fact that many efficient implementations reduce generalized convolutions to matrix multiplications. The suggested brain damage process prunes the convolutional kernel tensor in a group-wise fashion by adding group-sparsity regularization to the standard training process. After such group-wise pruning, convolutions can be reduced to multiplications of thinned dense matrices, which leads to speedup. In the comparison on AlexNet, the method achieves very competitive performance.



### Learning with Group Invariant Features: A Kernel Perspective
- **Arxiv ID**: http://arxiv.org/abs/1506.02544v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1506.02544v2)
- **Published**: 2015-06-08 15:19:30+00:00
- **Updated**: 2015-12-04 20:49:25+00:00
- **Authors**: Youssef Mroueh, Stephen Voinea, Tomaso Poggio
- **Comment**: NIPS 2015
- **Journal**: None
- **Summary**: We analyze in this paper a random feature map based on a theory of invariance I-theory introduced recently. More specifically, a group invariant signal signature is obtained through cumulative distributions of group transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of $N$ points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting.



### License Plate Recognition System Based on Color Coding Of License Plates
- **Arxiv ID**: http://arxiv.org/abs/1506.03128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.03128v1)
- **Published**: 2015-06-08 15:44:29+00:00
- **Updated**: 2015-06-08 15:44:29+00:00
- **Authors**: Jani Biju Babjan
- **Comment**: None
- **Journal**: None
- **Summary**: License Plate Recognition Systems are used to determine the license plate number of a vehicle. The current system mainly uses Optical Character Recognition to recognize the number plate. There are several problems to this system. Some of them include interchanging of several letters or numbers (letter O with digit 0), difficulty in localizing the license plate, high error rate, use of different fonts in license plates etc. So a new system to recognize the license plate number using color coding of license plates is proposed in this paper. Easier localization of license plate can be done by searching for the start or stop patters of license plates. An eight segment display system along with traditional numbering with the first and last segments left for start or stop patterns is proposed in this paper. Practical applications include several areas under Internet of Things (IoT).



### Learning to Select Pre-Trained Deep Representations with Bayesian Evidence Framework
- **Arxiv ID**: http://arxiv.org/abs/1506.02565v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1506.02565v4)
- **Published**: 2015-06-08 15:56:26+00:00
- **Updated**: 2016-04-25 01:35:31+00:00
- **Authors**: Yong-Deok Kim, Taewoong Jang, Bohyung Han, Seungjin Choi
- **Comment**: Appearing in CVPR-2016 (oral presentation)
- **Journal**: None
- **Summary**: We propose a Bayesian evidence framework to facilitate transfer learning from pre-trained deep convolutional neural networks (CNNs). Our framework is formulated on top of a least squares SVM (LS-SVM) classifier, which is simple and fast in both training and testing, and achieves competitive performance in practice. The regularization parameters in LS-SVM is estimated automatically without grid search and cross-validation by maximizing evidence, which is a useful measure to select the best performing CNN out of multiple candidates for transfer learning; the evidence is optimized efficiently by employing Aitken's delta-squared process, which accelerates convergence of fixed point update. The proposed Bayesian evidence framework also provides a good solution to identify the best ensemble of heterogeneous CNNs through a greedy algorithm. Our Bayesian evidence framework for transfer learning is tested on 12 visual recognition datasets and illustrates the state-of-the-art performance consistently in terms of prediction accuracy and modeling efficiency.



### Circulant temporal encoding for video retrieval and temporal alignment
- **Arxiv ID**: http://arxiv.org/abs/1506.02588v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.02588v2)
- **Published**: 2015-06-08 17:05:53+00:00
- **Updated**: 2015-11-30 17:00:13+00:00
- **Authors**: Matthijs Douze, Jérôme Revaud, Jakob Verbeek, Hervé Jégou, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of specific video event retrieval. Given a query video of a specific event, e.g., a concert of Madonna, the goal is to retrieve other videos of the same event that temporally overlap with the query. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to efficiently compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. The descriptors can be compressed in the frequency domain with a product quantizer adapted to complex numbers. In this case, video retrieval is performed without decompressing the descriptors. We also consider the temporal alignment of a set of videos. We exploit the matching confidence and an estimate of the temporal offset computed for all pairs of videos by our retrieval approach. Our robust algorithm aligns the videos on a global timeline by maximizing the set of temporally consistent matches. The global temporal alignment enables synchronous playback of the videos of a given scene.



### Path-SGD: Path-Normalized Optimization in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1506.02617v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1506.02617v1)
- **Published**: 2015-06-08 19:01:33+00:00
- **Updated**: 2015-06-08 19:01:33+00:00
- **Authors**: Behnam Neyshabur, Ruslan Salakhutdinov, Nathan Srebro
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.



### Learning both Weights and Connections for Efficient Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1506.02626v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1506.02626v3)
- **Published**: 2015-06-08 19:28:43+00:00
- **Updated**: 2015-10-30 23:29:27+00:00
- **Authors**: Song Han, Jeff Pool, John Tran, William J. Dally
- **Comment**: Published as a conference paper at NIPS 2015
- **Journal**: None
- **Summary**: Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.



### You Only Look Once: Unified, Real-Time Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1506.02640v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.02640v5)
- **Published**: 2015-06-08 19:52:52+00:00
- **Updated**: 2016-05-09 22:22:11+00:00
- **Authors**: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi
- **Comment**: None
- **Journal**: None
- **Summary**: We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.   Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.



