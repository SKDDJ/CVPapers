# Arxiv Papers in cs.CV on 2015-06-18
### A Spatial Layout and Scale Invariant Feature Representation for Indoor Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/1506.05532v2
- **DOI**: 10.1109/TIP.2016.2599292
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.05532v2)
- **Published**: 2015-06-18 02:11:37+00:00
- **Updated**: 2015-08-14 04:01:11+00:00
- **Authors**: Munawar Hayat, Salman H. Khan, Mohammed Bennamoun, Senjian An
- **Comment**: None
- **Journal**: None
- **Summary**: Unlike standard object classification, where the image to be classified contains one or multiple instances of the same object, indoor scene classification is quite different since the image consists of multiple distinct objects. Further, these objects can be of varying sizes and are present across numerous spatial locations in different layouts. For automatic indoor scene categorization, large scale spatial layout deformations and scale variations are therefore two major challenges and the design of rich feature descriptors which are robust to these challenges is still an open problem. This paper introduces a new learnable feature descriptor called "spatial layout and scale invariant convolutional activations" to deal with these challenges. For this purpose, a new Convolutional Neural Network architecture is designed which incorporates a novel 'Spatially Unstructured' layer to introduce robustness against spatial layout deformations. To achieve scale invariance, we present a pyramidal image representation. For feasible training of the proposed network for images of indoor scenes, the paper proposes a new methodology which efficiently adapts a trained network model (on a large scale data) for our task with only a limited amount of available training data. Compared with existing state of the art, the proposed approach achieves a relative performance improvement of 3.2%, 3.8%, 7.0%, 11.9% and 2.1% on MIT-67, Scene-15, Sports-8, Graz-02 and NYU datasets respectively.



### Point-wise Map Recovery and Refinement from Functional Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1506.05603v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1506.05603v1)
- **Published**: 2015-06-18 09:38:47+00:00
- **Updated**: 2015-06-18 09:38:47+00:00
- **Authors**: Emanuele Rodol√†, Michael Moeller, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: Since their introduction in the shape analysis community, functional maps have met with considerable success due to their ability to compactly represent dense correspondences between deformable shapes, with applications ranging from shape matching and image segmentation, to exploration of large shape collections. Despite the numerous advantages of such representation, however, the problem of converting a given functional map back to a point-to-point map has received a surprisingly limited interest. In this paper we analyze the general problem of point-wise map recovery from arbitrary functional maps. In doing so, we rule out many of the assumptions required by the currently established approach -- most notably, the limiting requirement of the input shapes being nearly-isometric. We devise an efficient recovery process based on a simple probabilistic model. Experiments confirm that this approach achieves remarkable accuracy improvements in very challenging cases.



### Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1506.05751v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.05751v1)
- **Published**: 2015-06-18 17:03:54+00:00
- **Updated**: 2015-06-18 17:03:54+00:00
- **Authors**: Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach (Goodfellow et al.). Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.



