# Arxiv Papers in cs.CV on 2015-06-06
### What's the Point: Semantic Segmentation with Point Supervision
- **Arxiv ID**: http://arxiv.org/abs/1506.02106v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.02106v5)
- **Published**: 2015-06-06 02:45:48+00:00
- **Updated**: 2016-07-23 17:41:43+00:00
- **Authors**: Amy Bearman, Olga Russakovsky, Vittorio Ferrari, Li Fei-Fei
- **Comment**: ECCV (2016) submission
- **Journal**: None
- **Summary**: The semantic image segmentation task presents a trade-off between test time accuracy and training-time annotation cost. Detailed per-pixel annotations enable training accurate models but are very time-consuming to obtain, image-level class labels are an order of magnitude cheaper but result in less accurate models. We take a natural step from image-level annotation towards stronger supervision: we ask annotators to point to an object if one exists. We incorporate this point supervision along with a novel objectness potential in the training loss function of a CNN model. Experimental results on the PASCAL VOC 2012 benchmark reveal that the combined effect of point-level supervision and objectness potential yields an improvement of 12.9% mIOU over image-level supervision. Further, we demonstrate that models trained with point-level supervision are more accurate than models trained with image-level, squiggle-level or full supervision given a fixed annotation budget.



### Deeply Learning the Messages in Message Passing Inference
- **Arxiv ID**: http://arxiv.org/abs/1506.02108v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1506.02108v3)
- **Published**: 2015-06-06 02:52:38+00:00
- **Updated**: 2015-09-08 04:29:45+00:00
- **Authors**: Guosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel
- **Comment**: 11 pages. Appearing in Proc. The Twenty-ninth Annual Conference on
  Neural Information Processing Systems (NIPS), 2015, Montreal, Canada
- **Journal**: None
- **Summary**: Deep structured output learning shows great promise in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be used to estimate the messages in message passing inference for structured prediction with Conditional Random Fields (CRFs). With such CNN message estimators, we obviate the need to learn or evaluate potential functions for message calculation. This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is necessary to undertake expensive inference for every stochastic gradient iteration. The network output dimension for message estimation is the same as the number of classes, in contrast to the network output for general CNN potential functions in CRFs, which is exponential in the order of the potentials. Hence CNN message learning has fewer network parameters and is more scalable for cases that a large number of classes are involved. We apply our method to semantic image segmentation on the PASCAL VOC 2012 dataset. We achieve an intersection-over-union score of 73.4 on its test set, which is the best reported result for methods using the VOC training images alone. This impressive performance demonstrates the effectiveness and usefulness of our CNN message learning method.



### Color Constancy by Learning to Predict Chromaticity from Luminance
- **Arxiv ID**: http://arxiv.org/abs/1506.02167v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.02167v2)
- **Published**: 2015-06-06 16:13:34+00:00
- **Updated**: 2015-12-07 18:49:15+00:00
- **Authors**: Ayan Chakrabarti
- **Comment**: Appears in Advances in Neural Information Processing Systems 28 (NIPS
  2015)
- **Journal**: None
- **Summary**: Color constancy is the recovery of true surface color from observed color, and requires estimating the chromaticity of scene illumination to correct for the bias it induces. In this paper, we show that the per-pixel color statistics of natural scenes---without any spatial or semantic context---can by themselves be a powerful cue for color constancy. Specifically, we describe an illuminant estimation method that is built around a "classifier" for identifying the true chromaticity of a pixel given its luminance (absolute brightness across color channels). During inference, each pixel's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants, and applying the classifier over these values yields a distribution over the corresponding illuminants. A global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels. We begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images. These histograms reflect a preference for hues corresponding to smooth reflectance functions, and for achromatic colors in brighter pixels. Despite its simplicity, the resulting estimation algorithm outperforms current state-of-the-art color constancy methods. Next, we propose a method to learn the luminance-to-chromaticity classifier "end-to-end". Using stochastic gradient descent, we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set. This leads to further improvements in accuracy, most significantly in the tail of the error distribution.



### Capturing Hands in Action using Discriminative Salient Points and Physics Simulation
- **Arxiv ID**: http://arxiv.org/abs/1506.02178v4
- **DOI**: 10.1007/s11263-016-0895-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.02178v4)
- **Published**: 2015-06-06 18:08:56+00:00
- **Updated**: 2016-03-07 18:09:59+00:00
- **Authors**: Dimitrios Tzionas, Luca Ballan, Abhilash Srikantha, Pablo Aponte, Marc Pollefeys, Juergen Gall
- **Comment**: Accepted for publication by the International Journal of Computer
  Vision (IJCV) on 16.02.2016 (submitted on 17.10.14). A combination into a
  single framework of an ECCV'12 multicamera-RGB and a monocular-RGBD GCPR'14
  hand tracking paper with several extensions, additional experiments and
  details
- **Journal**: None
- **Summary**: Hand motion capture is a popular research field, recently gaining more attention due to the ubiquity of RGB-D sensors. However, even most recent approaches focus on the case of a single isolated hand. In this work, we focus on hands that interact with other hands or objects and present a framework that successfully captures motion in such interaction scenarios for both rigid and articulated objects. Our framework combines a generative model with discriminatively trained salient points to achieve a low tracking error and with collision detection and physics simulation to achieve physically plausible estimates even in case of occlusions and missing visual data. Since all components are unified in a single objective function which is almost everywhere differentiable, it can be optimized with standard optimization techniques. Our approach works for monocular RGB-D sequences as well as setups with multiple synchronized RGB cameras. For a qualitative and quantitative evaluation, we captured 29 sequences with a large variety of interactions and up to 150 degrees of freedom.



### First-Take-All: Temporal Order-Preserving Hashing for 3D Action Videos
- **Arxiv ID**: http://arxiv.org/abs/1506.02184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.02184v1)
- **Published**: 2015-06-06 19:36:11+00:00
- **Updated**: 2015-06-06 19:36:11+00:00
- **Authors**: Jun Ye, Hao Hu, Kai Li, Guo-Jun Qi, Kien A. Hua
- **Comment**: 9 pages, 11 figures
- **Journal**: None
- **Summary**: With the prevalence of the commodity depth cameras, the new paradigm of user interfaces based on 3D motion capturing and recognition have dramatically changed the way of interactions between human and computers. Human action recognition, as one of the key components in these devices, plays an important role to guarantee the quality of user experience. Although the model-driven methods have achieved huge success, they cannot provide a scalable solution for efficiently storing, retrieving and recognizing actions in the large-scale applications. These models are also vulnerable to the temporal translation and warping, as well as the variations in motion scales and execution rates. To address these challenges, we propose to treat the 3D human action recognition as a video-level hashing problem and propose a novel First-Take-All (FTA) Hashing algorithm capable of hashing the entire video into hash codes of fixed length. We demonstrate that this FTA algorithm produces a compact representation of the video invariant to the above mentioned variations, through which action recognition can be solved by an efficient nearest neighbor search by the Hamming distance between the FTA hash codes. Experiments on the public 3D human action datasets shows that the FTA algorithm can reach a recognition accuracy higher than 80%, with about 15 bits per frame considering there are 65 frames per video over the datasets.



