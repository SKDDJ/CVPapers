# Arxiv Papers in cs.CV on 2015-06-04
### ShapeFit: Exact location recovery from corrupted pairwise directions
- **Arxiv ID**: http://arxiv.org/abs/1506.01437v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.CO, math.IT, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1506.01437v2)
- **Published**: 2015-06-04 00:16:20+00:00
- **Updated**: 2015-07-04 04:51:53+00:00
- **Authors**: Paul Hand, Choongbum Lee, Vladislav Voroninski
- **Comment**: None
- **Journal**: None
- **Summary**: Let $t_1,\ldots,t_n \in \mathbb{R}^d$ and consider the location recovery problem: given a subset of pairwise direction observations $\{(t_i - t_j) / \|t_i - t_j\|_2\}_{i<j \in [n] \times [n]}$, where a constant fraction of these observations are arbitrarily corrupted, find $\{t_i\}_{i=1}^n$ up to a global translation and scale. We propose a novel algorithm for the location recovery problem, which consists of a simple convex program over $dn$ real variables. We prove that this program recovers a set of $n$ i.i.d. Gaussian locations exactly and with high probability if the observations are given by an \erdosrenyi graph, $d$ is large enough, and provided that at most a constant fraction of observations involving any particular location are adversarially corrupted. We also prove that the program exactly recovers Gaussian locations for $d=3$ if the fraction of corrupted observations at each location is, up to poly-logarithmic factors, at most a constant. Both of these recovery theorems are based on a set of deterministic conditions that we prove are sufficient for exact recovery.



### Comparing the Performance of L*A*B* and HSV Color Spaces with Respect to Color Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1506.01472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.01472v1)
- **Published**: 2015-06-04 06:37:14+00:00
- **Updated**: 2015-06-04 06:37:14+00:00
- **Authors**: Dibya Jyoti Bora, Anil Kumar Gupta, Fayaz Ahmad Khan
- **Comment**: 11 pages, 19 figures in International Journal of Emerging Technology
  and Advanced Engineering,Volume 5, Issue 2, February 2015
- **Journal**: None
- **Summary**: Color image segmentation is a very emerging topic for image processing research. Since it has the ability to present the result in a way that is much more close to the human yes perceive, so todays more research is going on this area. Choosing a proper color space is a very important issue for color image segmentation process. Generally LAB and HSV are the two frequently chosen color spaces. In this paper a comparative analysis is performed between these two color spaces with respect to color image segmentation. For measuring their performance, we consider the parameters: mse and psnr . It is found that HSV color space is performing better than LAB.



### A Novel Approach Towards Clustering Based Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1506.01710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.01710v1)
- **Published**: 2015-06-04 06:44:47+00:00
- **Updated**: 2015-06-04 06:44:47+00:00
- **Authors**: Dibya Jyoti Bora, Anil Kumar Gupta
- **Comment**: 5 pages, 7 figures, 1 table in International Journal of Emerging
  Science and Engineering, Volume-2 Issue-11, September 2014. arXiv admin note:
  text overlap with arXiv:1506.01472
- **Journal**: None
- **Summary**: In computer vision, image segmentation is always selected as a major research topic by researchers. Due to its vital rule in image processing, there always arises the need of a better image segmentation method. Clustering is an unsupervised study with its application in almost every field of science and engineering. Many researchers used clustering in image segmentation process. But still there requires improvement of such approaches. In this paper, a novel approach for clustering based image segmentation is proposed. Here, we give importance on color space and choose lab for this task. The famous hard clustering algorithm K-means is used, but as its performance is dependent on choosing a proper distance measure, so, we go for cosine distance measure. Then the segmented image is filtered with sobel filter. The filtered image is analyzed with marker watershed algorithm to have the final segmented result of our original image. The MSE and PSNR values are evaluated to observe the performance.



### Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks
- **Arxiv ID**: http://arxiv.org/abs/1506.01497v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.01497v3)
- **Published**: 2015-06-04 07:58:34+00:00
- **Updated**: 2016-01-06 06:30:17+00:00
- **Authors**: Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun
- **Comment**: Extended tech report
- **Journal**: None
- **Summary**: State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.



### Multilayer Structured NMF for Spectral Unmixing of Hyperspectral Images
- **Arxiv ID**: http://arxiv.org/abs/1506.01596v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.01596v1)
- **Published**: 2015-06-04 13:53:33+00:00
- **Updated**: 2015-06-04 13:53:33+00:00
- **Authors**: Roozbeh Rajabi, Hassan Ghassemian
- **Comment**: 4 pages, conference
- **Journal**: None
- **Summary**: One of the challenges in hyperspectral data analysis is the presence of mixed pixels. Mixed pixels are the result of low spatial resolution of hyperspectral sensors. Spectral unmixing methods decompose a mixed pixel into a set of endmembers and abundance fractions. Due to nonnegativity constraint on abundance fraction values, NMF based methods are well suited to this problem. In this paper multilayer NMF has been used to improve the results of NMF methods for spectral unmixing of hyperspectral data under the linear mixing framework. Sparseness constraint on both spectral signatures and abundance fractions matrices are used in this paper. Evaluation of the proposed algorithm is done using synthetic and real datasets in terms of spectral angle and abundance angle distances. Results show that the proposed algorithm outperforms other previously proposed methods.



### The Long-Short Story of Movie Description
- **Arxiv ID**: http://arxiv.org/abs/1506.01698v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1506.01698v1)
- **Published**: 2015-06-04 19:45:36+00:00
- **Updated**: 2015-06-04 19:45:36+00:00
- **Authors**: Anna Rohrbach, Marcus Rohrbach, Bernt Schiele
- **Comment**: None
- **Journal**: None
- **Summary**: Generating descriptions for videos has many applications including assisting blind people and human-robot interaction. The recent advances in image captioning as well as the release of large-scale movie description datasets such as MPII Movie Description allow to study this task in more depth. Many of the proposed methods for image captioning rely on pre-trained object classifier CNNs and Long-Short Term Memory recurrent networks (LSTMs) for generating descriptions. While image description focuses on objects, we argue that it is important to distinguish verbs, objects, and places in the challenging setting of movie description. In this work we show how to learn robust visual classifiers from the weak annotations of the sentence descriptions. Based on these visual classifiers we learn how to generate a description using an LSTM. We explore different design choices to build and train the LSTM and achieve the best performance to date on the challenging MPII-MD dataset. We compare and analyze our approach and prior work along various dimensions to better understand the key challenges of the movie description task.



### Monocular SLAM Supported Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1506.01732v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1506.01732v1)
- **Published**: 2015-06-04 21:07:56+00:00
- **Updated**: 2015-06-04 21:07:56+00:00
- **Authors**: Sudeep Pillai, John Leonard
- **Comment**: Accepted to appear at Robotics: Science and Systems 2015, Rome, Italy
- **Journal**: None
- **Summary**: In this work, we develop a monocular SLAM-aware object recognition system that is able to achieve considerably stronger recognition performance, as compared to classical object recognition systems that function on a frame-by-frame basis. By incorporating several key ideas including multi-view object proposals and efficient feature encoding methods, our proposed system is able to detect and robustly recognize objects in its environment using a single RGB camera in near-constant time. Through experiments, we illustrate the utility of using such a system to effectively detect and recognize objects, incorporating multiple object viewpoint detections into a unified prediction hypothesis. The performance of the proposed recognition system is evaluated on the UW RGB-D Dataset, showing strong recognition performance and scalable run-time performance compared to current state-of-the-art recognition systems.



