# Arxiv Papers in cs.CV on 2015-06-05
### Beyond Temporal Pooling: Recurrence and Temporal Convolutions for Gesture Recognition in Video
- **Arxiv ID**: http://arxiv.org/abs/1506.01911v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1506.01911v3)
- **Published**: 2015-06-05 13:43:01+00:00
- **Updated**: 2016-02-10 16:50:29+00:00
- **Authors**: Lionel Pigou, AÃ¤ron van den Oord, Sander Dieleman, Mieke Van Herreweghe, Joni Dambre
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have demonstrated the power of recurrent neural networks for machine translation, image captioning and speech recognition. For the task of capturing temporal structure in video, however, there still remain numerous open research questions. Current research suggests using a simple temporal feature pooling strategy to take into account the temporal aspect of video. We demonstrate that this method is not sufficient for gesture recognition, where temporal information is more discriminative compared to general video classification tasks. We explore deep architectures for gesture recognition in video and propose a new end-to-end trainable neural network architecture incorporating temporal convolutions and bidirectional recurrence. Our main contributions are twofold; first, we show that recurrence is crucial for this task; second, we show that adding temporal convolutions leads to significant improvements. We evaluate the different approaches on the Montalbano gesture recognition dataset, where we achieve state-of-the-art results.



### Learning to track for spatio-temporal action localization
- **Arxiv ID**: http://arxiv.org/abs/1506.01929v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.01929v2)
- **Published**: 2015-06-05 14:48:46+00:00
- **Updated**: 2015-09-27 11:21:16+00:00
- **Authors**: Philippe Weinzaepfel, Zaid Harchaoui, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an effective approach for spatio-temporal action localization in realistic videos. The approach first detects proposals at the frame-level and scores them with a combination of static and motion CNN features. It then tracks high-scoring proposals throughout the video using a tracking-by-detection approach. Our tracker relies simultaneously on instance-level and class-level detectors. The tracks are scored using a spatio-temporal motion histogram, a descriptor at the track level, in combination with the CNN features. Finally, we perform temporal localization of the action using a sliding-window approach at the track level. We present experimental results for spatio-temporal localization on the UCF-Sports, J-HMDB and UCF-101 action localization datasets, where our approach outperforms the state of the art with a margin of 15%, 7% and 12% respectively in mAP.



### Spatial Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/1506.02025v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.02025v3)
- **Published**: 2015-06-05 19:54:26+00:00
- **Updated**: 2016-02-04 18:08:46+00:00
- **Authors**: Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.



### Sentence Directed Video Object Codetection
- **Arxiv ID**: http://arxiv.org/abs/1506.02059v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1506.02059v2)
- **Published**: 2015-06-05 20:34:12+00:00
- **Updated**: 2016-01-26 20:38:42+00:00
- **Authors**: Haonan Yu, Jeffrey Mark Siskind
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of video object codetection by leveraging the weak semantic constraint implied by sentences that describe the video content. Unlike most existing work that focuses on codetecting large objects which are usually salient both in size and appearance, we can codetect objects that are small or medium sized. Our method assumes no human pose or depth information such as is required by the most recent state-of-the-art method. We employ weak semantic constraint on the codetection process by pairing the video with sentences. Although the semantic information is usually simple and weak, it can greatly boost the performance of our codetection framework by reducing the search space of the hypothesized object detections. Our experiment demonstrates an average IoU score of 0.423 on a new challenging dataset which contains 15 object classes and 150 videos with 12,509 frames in total, and an average IoU score of 0.373 on a subset of an existing dataset, originally intended for activity recognition, which contains 5 object classes and 75 videos with 8,854 frames in total.



### Automatic tracking of protein vesicles
- **Arxiv ID**: http://arxiv.org/abs/1506.02083v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1506.02083v1)
- **Published**: 2015-06-05 22:59:47+00:00
- **Updated**: 2015-06-05 22:59:47+00:00
- **Authors**: Min Xu
- **Comment**: Author's master thesis (University of Southern California, May 2009).
  Adviser: Sergey Lototsky. ISBN: 9781109140439
- **Journal**: None
- **Summary**: With the advance of fluorescence imaging technologies, recently cell biologists are able to record the movement of protein vesicles within a living cell. Automatic tracking of the movements of these vesicles become key for qualitative analysis of dynamics of theses vesicles. In this thesis, we formulate such tracking problem as video object tracking problem, and design a dynamic programming method for tracking single object. Our experiments on simulation data show that the method can identify a track with high accuracy which is robust to the choose of tracking parameters and presence of high level noise. We then extend this method to the tracking multiple objects using the track elimination strategy. In multiple object tracking, the above approach often fails to correctly identify a track when two tracks cross. We solve this problem by incorporating the Kalman filter into the dynamic programming framework. Our experiments on simulated data show that the tracking accuracy is significantly improved.



