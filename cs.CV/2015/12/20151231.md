# Arxiv Papers in cs.CV on 2015-12-31
### Exploiting Local Structures with the Kronecker Layer in Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1512.09194v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.09194v2)
- **Published**: 2015-12-31 01:32:16+00:00
- **Updated**: 2016-02-04 01:19:38+00:00
- **Authors**: Shuchang Zhou, Jia-Nan Wu, Yuxin Wu, Xinyu Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose and study a technique to reduce the number of parameters and computation time in convolutional neural networks. We use Kronecker product to exploit the local structures within convolution and fully-connected layers, by replacing the large weight matrices by combinations of multiple Kronecker products of smaller matrices. Just as the Kronecker product is a generalization of the outer product from vectors to matrices, our method is a generalization of the low rank approximation method for convolution neural networks. We also introduce combinations of different shapes of Kronecker product to increase modeling capacity. Experiments on SVHN, scene text recognition and ImageNet dataset demonstrate that we can achieve $3.3 \times$ speedup or $3.6 \times$ parameter reduction with less than 1\% drop in accuracy, showing the effectiveness and efficiency of our method. Moreover, the computation efficiency of Kronecker layer makes using larger feature map possible, which in turn enables us to outperform the previous state-of-the-art on both SVHN(digit recognition) and CASIA-HWDB (handwritten Chinese character recognition) datasets.



### Denoising and Completion of 3D Data via Multidimensional Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1512.09227v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1512.09227v1)
- **Published**: 2015-12-31 06:37:54+00:00
- **Updated**: 2015-12-31 06:37:54+00:00
- **Authors**: Zemin Zhang, Shuchin Aeron
- **Comment**: 9 pages, submitted to Conference on Computer Vision and Pattern
  Recognition (CVPR) 2016
- **Journal**: None
- **Summary**: In this paper a new dictionary learning algorithm for multidimensional data is proposed. Unlike most conventional dictionary learning methods which are derived for dealing with vectors or matrices, our algorithm, named KTSVD, learns a multidimensional dictionary directly via a novel algebraic approach for tensor factorization as proposed in [3, 12, 13]. Using this approach one can define a tensor-SVD and we propose to extend K-SVD algorithm used for 1-D data to a K-TSVD algorithm for handling 2-D and 3-D data. Our algorithm, based on the idea of sparse coding (using group-sparsity over multidimensional coefficient vectors), alternates between estimating a compact representation and dictionary learning. We analyze our KTSVD algorithm and demonstrate its result on video completion and multispectral image denoising.



### Learning Local Image Descriptors with Deep Siamese and Triplet Convolutional Networks by Minimising Global Loss Functions
- **Arxiv ID**: http://arxiv.org/abs/1512.09272v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.09272v2)
- **Published**: 2015-12-31 12:36:28+00:00
- **Updated**: 2016-08-01 06:47:57+00:00
- **Authors**: Vijay Kumar B G, Gustavo Carneiro, Ian Reid
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition 2016 (CVPR
  2016)
- **Journal**: None
- **Summary**: Recent innovations in training deep convolutional neural network (ConvNet) models have motivated the design of new methods to automatically learn local image descriptors. The latest deep ConvNets proposed for this task consist of a siamese network that is trained by penalising misclassification of pairs of local image patches. Current results from machine learning show that replacing this siamese by a triplet network can improve the classification accuracy in several problems, but this has yet to be demonstrated for local image descriptor learning. Moreover, current siamese and triplet networks have been trained with stochastic gradient descent that computes the gradient from individual pairs or triplets of local image patches, which can make them prone to overfitting. In this paper, we first propose the use of triplet networks for the problem of local image descriptor learning. Furthermore, we also propose the use of a global loss that minimises the overall classification error in the training set, which can improve the generalisation capability of the model. Using the UBC benchmark dataset for comparing local image descriptors, we show that the triplet network produces a more accurate embedding than the siamese network in terms of the UBC dataset errors. Moreover, we also demonstrate that a combination of the triplet and global losses produces the best embedding in the field, using this triplet network. Finally, we also show that the use of the central-surround siamese network trained with the global loss produces the best result of the field on the UBC dataset. Pre-trained models are available online at https://github.com/vijaykbg/deep-patchmatch



### Autoencoding beyond pixels using a learned similarity metric
- **Arxiv ID**: http://arxiv.org/abs/1512.09300v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1512.09300v2)
- **Published**: 2015-12-31 14:53:39+00:00
- **Updated**: 2016-02-10 21:18:27+00:00
- **Authors**: Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, Ole Winther
- **Comment**: None
- **Journal**: None
- **Summary**: We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.



### Event Specific Multimodal Pattern Mining with Image-Caption Pairs
- **Arxiv ID**: http://arxiv.org/abs/1601.00022v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1601.00022v2)
- **Published**: 2015-12-31 22:14:22+00:00
- **Updated**: 2016-01-05 01:55:22+00:00
- **Authors**: Hongzhi Li, Joseph G. Ellis, Shih-Fu Chang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we describe a novel framework and algorithms for discovering image patch patterns from a large corpus of weakly supervised image-caption pairs generated from news events. Current pattern mining techniques attempt to find patterns that are representative and discriminative, we stipulate that our discovered patterns must also be recognizable by humans and preferably with meaningful names. We propose a new multimodal pattern mining approach that leverages the descriptive captions often accompanying news images to learn semantically meaningful image patch patterns. The mutltimodal patterns are then named using words mined from the associated image captions for each pattern. A novel evaluation framework is provided that demonstrates our patterns are 26.2% more semantically meaningful than those discovered by the state of the art vision only pipeline, and that we can provide tags for the discovered images patches with 54.5% accuracy with no direct supervision. Our methods also discover named patterns beyond those covered by the existing image datasets like ImageNet. To the best of our knowledge this is the first algorithm developed to automatically mine image patch patterns that have strong semantic meaning specific to high-level news events, and then evaluate these patterns based on that criteria.



### Write a Classifier: Predicting Visual Classifiers from Unstructured Text
- **Arxiv ID**: http://arxiv.org/abs/1601.00025v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1601.00025v2)
- **Published**: 2015-12-31 22:23:34+00:00
- **Updated**: 2016-12-28 02:13:59+00:00
- **Authors**: Mohamed Elhoseiny, Ahmed Elgammal, Babak Saleh
- **Comment**: (TPAMI) Transactions on Pattern Analysis and Machine Intelligence
  2017
- **Journal**: None
- **Summary**: People typically learn through exposure to visual concepts associated with linguistic descriptions. For instance, teaching visual object categories to children is often accompanied by descriptions in text or speech. In a machine learning context, these observations motivates us to ask whether this learning process could be computationally modeled to learn visual classifiers. More specifically, the main question of this work is how to utilize purely textual description of visual classes with no training images, to learn explicit visual classifiers for them. We propose and investigate two baseline formulations, based on regression and domain transfer, that predict a linear classifier. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the parameters of a linear classifier. We also propose a generic kernelized models where a kernel classifier is predicted in the form defined by the representer theorem. The kernelized models allow defining and utilizing any two RKHS (Reproducing Kernel Hilbert Space) kernel functions in the visual space and text space, respectively. We finally propose a kernel function between unstructured text descriptions that builds on distributional semantics, which shows an advantage in our setting and could be useful for other applications. We applied all the studied models to predict visual classifiers on two fine-grained and challenging categorization datasets (CU Birds and Flower Datasets), and the results indicate successful predictions of our final model over several baselines that we designed.



### Computational Pathology: Challenges and Promises for Tissue Analysis
- **Arxiv ID**: http://arxiv.org/abs/1601.00027v1
- **DOI**: 10.1016/j.compmedimag.2011.02.006
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1601.00027v1)
- **Published**: 2015-12-31 22:33:44+00:00
- **Updated**: 2015-12-31 22:33:44+00:00
- **Authors**: Thomas J. Fuchs, Joachim M. Buhmann
- **Comment**: None
- **Journal**: Computerized Medical Imaging and Graphics, vol. 35, 7-8, p.
  515-530, 2011
- **Summary**: The histological assessment of human tissue has emerged as the key challenge for detection and treatment of cancer. A plethora of different data sources ranging from tissue microarray data to gene expression, proteomics or metabolomics data provide a detailed overview of the health status of a patient. Medical doctors need to assess these information sources and they rely on data driven automatic analysis tools. Methods for classification, grouping and segmentation of heterogeneous data sources as well as regression of noisy dependencies and estimation of survival probabilities enter the processing workflow of a pathology diagnosis system at various stages. This paper reports on state-of-the-art of the design and effectiveness of computational pathology workflows and it discusses future research directions in this emergent field of medical informatics and diagnostic machine learning.



