# Arxiv Papers in cs.CV on 2015-12-21
### Quantized Convolutional Neural Networks for Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1512.06473v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.06473v3)
- **Published**: 2015-12-21 02:26:46+00:00
- **Updated**: 2016-05-16 00:37:35+00:00
- **Authors**: Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng
- **Comment**: Accepted by the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2016
- **Journal**: None
- **Summary**: Recently, convolutional neural networks (CNN) have demonstrated impressive performance in various computer vision tasks. However, high performance hardware is typically indispensable for the application of CNN models due to the high computation complexity, which prohibits their further extensions. In this paper, we propose an efficient framework, namely Quantized CNN, to simultaneously speed-up the computation and reduce the storage and memory overhead of CNN models. Both filter kernels in convolutional layers and weighting matrices in fully-connected layers are quantized, aiming at minimizing the estimation error of each layer's response. Extensive experiments on the ILSVRC-12 benchmark demonstrate 4~6x speed-up and 15~20x compression with merely one percentage loss of classification accuracy. With our quantized CNN model, even mobile devices can accurately classify images within one second.



### Remote Health Coaching System and Human Motion Data Analysis for Physical Therapy with Microsoft Kinect
- **Arxiv ID**: http://arxiv.org/abs/1512.06492v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1512.06492v1)
- **Published**: 2015-12-21 04:58:36+00:00
- **Updated**: 2015-12-21 04:58:36+00:00
- **Authors**: Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy
- **Comment**: 6 pages, Computer Vision for Accessible and Affordable HealthCare
  Workshop (ICCV2015)
- **Journal**: None
- **Summary**: This paper summarizes the recent progress we have made for the computer vision technologies in physical therapy with the accessible and affordable devices. We first introduce the remote health coaching system we build with Microsoft Kinect. Since the motion data captured by Kinect is noisy, we investigate the data accuracy of Kinect with respect to the high accuracy motion capture system. We also propose an outlier data removal algorithm based on the data distribution. In order to generate the kinematic parameter from the noisy data captured by Kinect, we propose a kinematic filtering algorithm based on Unscented Kalman Filter and the kinematic model of human skeleton. The proposed algorithm can obtain smooth kinematic parameter with reduced noise compared to the kinematic parameter generated from the raw motion data from Kinect.



### Harnessing the Deep Net Object Models for Enhancing Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1512.06498v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.06498v2)
- **Published**: 2015-12-21 05:28:23+00:00
- **Updated**: 2015-12-24 04:37:51+00:00
- **Authors**: O. V. Ramana Murthy, Roland Goecke
- **Comment**: 6 pages. arXiv admin note: text overlap with arXiv:1411.4006 by other
  authors
- **Journal**: None
- **Summary**: In this study, the influence of objects is investigated in the scenario of human action recognition with large number of classes. We hypothesize that the objects the humans are interacting will have good say in determining the action being performed. Especially, if the objects are non-moving, such as objects appearing in the background, features such as spatio-temporal interest points, dense trajectories may fail to detect them. Hence we propose to detect objects using pre-trained object detectors in every frame statically. Trained Deep network models are used as object detectors. Information from different layers in conjunction with different encoding techniques is extensively studied to obtain the richest feature vectors. This technique is observed to yield state-of-the-art performance on HMDB51 and UCF101 datasets.



### Spatial Phase-Sweep: Increasing temporal resolution of transient imaging using a light source array
- **Arxiv ID**: http://arxiv.org/abs/1512.06539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.06539v1)
- **Published**: 2015-12-21 09:25:18+00:00
- **Updated**: 2015-12-21 09:25:18+00:00
- **Authors**: Ryuichi Tadano, Adithya Kumar Pediredla, Kaushik Mitra, Ashok Veeraraghavan
- **Comment**: None
- **Journal**: None
- **Summary**: Transient imaging or light-in-flight techniques capture the propagation of an ultra-short pulse of light through a scene, which in effect captures the optical impulse response of the scene. Recently, it has been shown that we can capture transient images using commercially available Time-of-Flight (ToF) systems such as Photonic Mixer Devices (PMD). In this paper, we propose `spatial phase-sweep', a technique that exploits the speed of light to increase the temporal resolution beyond the 100 picosecond limit imposed by current electronics. Spatial phase-sweep uses a linear array of light sources with spatial separation of about 3 mm between them, thereby resulting in a time shift of about 10 picoseconds, which translates into 100 Gfps of transient imaging in theory. We demonstrate a prototype and transient imaging results using spatial phase-sweep.



### Analysis of Vessel Connectivities in Retinal Images by Cortically Inspired Spectral Clustering
- **Arxiv ID**: http://arxiv.org/abs/1512.06559v2
- **DOI**: 10.1007/s10851-016-0640-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.06559v2)
- **Published**: 2015-12-21 10:09:23+00:00
- **Updated**: 2016-05-23 11:37:55+00:00
- **Authors**: Marta Favali, Samaneh Abbasi-Sureshjani, Bart ter Haar Romeny, Alessandro Sarti
- **Comment**: submitted to and accepted by JMIV
- **Journal**: Journal of Mathematical Imaging and Vision, September 2016, Volume
  56, Issue 1, pp 158-172
- **Summary**: Retinal images provide early signs of diabetic retinopathy, glaucoma, and hypertension. These signs can be investigated based on microaneurysms or smaller vessels. The diagnostic biomarkers are the change of vessel widths and angles especially at junctions, which are investigated using the vessel segmentation or tracking. Vessel paths may also be interrupted; crossings and bifurcations may be disconnected. This paper addresses a novel contextual method based on the geometry of the primary visual cortex (V1) to study these difficulties. We have analyzed the specific problems at junctions with a connectivity kernel obtained as the fundamental solution of the Fokker-Planck equation, which is usually used to represent the geometrical structure of multi-orientation cortical connectivity. Using the spectral clustering on a large local affinity matrix constructed by both the connectivity kernel and the feature of intensity, the vessels are identified successfully in a hierarchical topology each representing an individual perceptual unit.



### Local and global gestalt laws: A neurally based spectral approach
- **Arxiv ID**: http://arxiv.org/abs/1512.06566v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.06566v2)
- **Published**: 2015-12-21 10:27:58+00:00
- **Updated**: 2016-08-29 15:20:45+00:00
- **Authors**: Marta Favali, Giovanna Citti, Alessandro Sarti
- **Comment**: submitted to Neural Computation
- **Journal**: None
- **Summary**: A mathematical model of figure-ground articulation is presented, taking into account both local and global gestalt laws. The model is compatible with the functional architecture of the primary visual cortex (V1). Particularly the local gestalt law of good continuity is described by means of suitable connectivity kernels, that are derived from Lie group theory and are neurally implemented in long range connectivity in V1. Different kernels are compatible with the geometric structure of cortical connectivity and they are derived as the fundamental solutions of the Fokker Planck, the Sub-Riemannian Laplacian and the isotropic Laplacian equations. The kernels are used to construct matrices of connectivity among the features present in a visual stimulus. Global gestalt constraints are then introduced in terms of spectral analysis of the connectivity matrix, showing that this processing can be cortically implemented in V1 by mean field neural equations. This analysis performs grouping of local features and individuates perceptual units with the highest saliency. Numerical simulations are performed and results are obtained applying the technique to a number of stimuli.



### Deep Learning for Surface Material Classification Using Haptic And Visual Information
- **Arxiv ID**: http://arxiv.org/abs/1512.06658v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1512.06658v2)
- **Published**: 2015-12-21 15:22:16+00:00
- **Updated**: 2016-05-01 07:00:56+00:00
- **Authors**: Haitian Zheng, Lu Fang, Mengqi Ji, Matti Strese, Yigitcan Ozer, Eckehard Steinbach
- **Comment**: 8 pages, under review as a paper at Transactions on Multimedia
- **Journal**: None
- **Summary**: When a user scratches a hand-held rigid tool across an object surface, an acceleration signal can be captured, which carries relevant information about the surface. More importantly, such a haptic signal is complementary to the visual appearance of the surface, which suggests the combination of both modalities for the recognition of the surface material. In this paper, we present a novel deep learning method dealing with the surface material classification problem based on a Fully Convolutional Network (FCN), which takes as input the aforementioned acceleration signal and a corresponding image of the surface texture. Compared to previous surface material classification solutions, which rely on a careful design of hand-crafted domain-specific features, our method automatically extracts discriminative features utilizing the advanced deep learning methodologies. Experiments performed on the TUM surface material database demonstrate that our method achieves state-of-the-art classification accuracy robustly and efficiently.



### Sparse Coding with Fast Image Alignment via Large Displacement Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1512.06709v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.06709v1)
- **Published**: 2015-12-21 17:10:35+00:00
- **Updated**: 2015-12-21 17:10:35+00:00
- **Authors**: Xiaoxia Sun, Nasser M. Nasrabadi, Trac D. Tran
- **Comment**: ICASSP 2016
- **Journal**: None
- **Summary**: Sparse representation-based classifiers have shown outstanding accuracy and robustness in image classification tasks even with the presence of intense noise and occlusion. However, it has been discovered that the performance degrades significantly either when test image is not aligned with the dictionary atoms or the dictionary atoms themselves are not aligned with each other, in which cases the sparse linear representation assumption fails. In this paper, having both training and test images misaligned, we introduce a novel sparse coding framework that is able to efficiently adapt the dictionary atoms to the test image via large displacement optical flow. In the proposed algorithm, every dictionary atom is automatically aligned with the input image and the sparse code is then recovered using the adapted dictionary atoms. A corresponding supervised dictionary learning algorithm is also developed for the proposed framework. Experimental results on digit datasets recognition verify the efficacy and robustness of the proposed algorithm.



### Multilinear Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1512.06730v1
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, cs.LG, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1512.06730v1)
- **Published**: 2015-12-21 17:53:35+00:00
- **Updated**: 2015-12-21 17:53:35+00:00
- **Authors**: Eric Kernfeld, Nathan Majumder, Shuchin Aeron, Misha Kilmer
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a new model and an algorithm for unsupervised clustering of 2-D data such as images. We assume that the data comes from a union of multilinear subspaces (UOMS) model, which is a specific structured case of the much studied union of subspaces (UOS) model. For segmentation under this model, we develop Multilinear Subspace Clustering (MSC) algorithm and evaluate its performance on the YaleB and Olivietti image data sets. We show that MSC is highly competitive with existing algorithms employing the UOS model in terms of clustering performance while enjoying improvement in computational complexity.



### Instance-Level Segmentation for Autonomous Driving with Deep Densely Connected MRFs
- **Arxiv ID**: http://arxiv.org/abs/1512.06735v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.06735v2)
- **Published**: 2015-12-21 17:58:35+00:00
- **Updated**: 2016-04-27 00:37:05+00:00
- **Authors**: Ziyu Zhang, Sanja Fidler, Raquel Urtasun
- **Comment**: None
- **Journal**: None
- **Summary**: Our aim is to provide a pixel-wise instance-level labeling of a monocular image in the context of autonomous driving. We build on recent work [Zhang et al., ICCV15] that trained a convolutional neural net to predict instance labeling in local image patches, extracted exhaustively in a stride from an image. A simple Markov random field model using several heuristics was then proposed in [Zhang et al., ICCV15] to derive a globally consistent instance labeling of the image. In this paper, we formulate the global labeling problem with a novel densely connected Markov random field and show how to encode various intuitive potentials in a way that is amenable to efficient mean field inference [Kr\"ahenb\"uhl et al., NIPS11]. Our potentials encode the compatibility between the global labeling and the patch-level predictions, contrast-sensitive smoothness as well as the fact that separate regions form different instances. Our experiments on the challenging KITTI benchmark [Geiger et al., CVPR12] demonstrate that our method achieves a significant performance boost over the baseline [Zhang et al., ICCV15].



### GraphConnect: A Regularization Framework for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1512.06757v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1512.06757v2)
- **Published**: 2015-12-21 18:42:45+00:00
- **Updated**: 2016-01-27 03:21:15+00:00
- **Authors**: Jiaji Huang, Qiang Qiu, Robert Calderbank, Guillermo Sapiro
- **Comment**: Theorems need more validation
- **Journal**: None
- **Summary**: Deep neural networks have proved very successful in domains where large training sets are available, but when the number of training samples is small, their performance suffers from overfitting. Prior methods of reducing overfitting such as weight decay, Dropout and DropConnect are data-independent. This paper proposes a new method, GraphConnect, that is data-dependent, and is motivated by the observation that data of interest lie close to a manifold. The new method encourages the relationships between the learned decisions to resemble a graph representing the manifold structure. Essentially GraphConnect is designed to learn attributes that are present in data samples in contrast to weight decay, Dropout and DropConnect which are simply designed to make it more difficult to fit to random error or noise. Empirical Rademacher complexity is used to connect the generalization error of the neural network to spectral properties of the graph learned from the input data. This framework is used to show that GraphConnect is superior to weight decay. Experimental results on several benchmark datasets validate the theoretical analysis, and show that when the number of training samples is small, GraphConnect is able to significantly improve performance over weight decay.



### Beyond Classification: Latent User Interests Profiling from Visual Contents Analysis
- **Arxiv ID**: http://arxiv.org/abs/1512.06785v1
- **DOI**: 10.1109/ICDMW.2015.160
- **Categories**: **cs.IR**, cs.CV, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1512.06785v1)
- **Published**: 2015-12-21 19:54:11+00:00
- **Updated**: 2015-12-21 19:54:11+00:00
- **Authors**: Longqi Yang, Cheng-Kang Hsieh, Deborah Estrin
- **Comment**: 2015 IEEE 15th International Conference on Data Mining Workshops
- **Journal**: None
- **Summary**: User preference profiling is an important task in modern online social networks (OSN). With the proliferation of image-centric social platforms, such as Pinterest, visual contents have become one of the most informative data streams for understanding user preferences. Traditional approaches usually treat visual content analysis as a general classification problem where one or more labels are assigned to each image. Although such an approach simplifies the process of image analysis, it misses the rich context and visual cues that play an important role in people's perception of images. In this paper, we explore the possibilities of learning a user's latent visual preferences directly from image contents. We propose a distance metric learning method based on Deep Convolutional Neural Networks (CNN) to directly extract similarity information from visual contents and use the derived distance metric to mine individual users' fine-grained visual preferences. Through our preliminary experiments using data from 5,790 Pinterest users, we show that even for the images within the same category, each user possesses distinct and individually-identifiable visual preferences that are consistent over their lifetime. Our results underscore the untapped potential of finer-grained visual preference profiling in understanding users' preferences.



### Car Segmentation and Pose Estimation using 3D Object Models
- **Arxiv ID**: http://arxiv.org/abs/1512.06790v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.06790v2)
- **Published**: 2015-12-21 20:01:53+00:00
- **Updated**: 2016-06-17 11:58:47+00:00
- **Authors**: Siddharth Mahendran, René Vidal
- **Comment**: None
- **Journal**: None
- **Summary**: Image segmentation and 3D pose estimation are two key cogs in any algorithm for scene understanding. However, state-of-the-art CRF-based models for image segmentation rely mostly on 2D object models to construct top-down high-order potentials. In this paper, we propose new top-down potentials for image segmentation and pose estimation based on the shape and volume of a 3D object model. We show that these complex top-down potentials can be easily decomposed into standard forms for efficient inference in both the segmentation and pose estimation tasks. Experiments on a car dataset show that knowledge of segmentation helps perform pose estimation better and vice versa.



