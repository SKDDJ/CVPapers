# Arxiv Papers in cs.CV on 2015-12-15
### Noise-Compensated, Bias-Corrected Diffusion Weighted Endorectal Magnetic Resonance Imaging via a Stochastically Fully-Connected Joint Conditional Random Field Model
- **Arxiv ID**: http://arxiv.org/abs/1512.04636v2
- **DOI**: None
- **Categories**: **stat.ME**, cs.CV, physics.med-ph, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1512.04636v2)
- **Published**: 2015-12-15 03:44:28+00:00
- **Updated**: 2016-07-05 16:47:37+00:00
- **Authors**: Ameneh Boroomand, Mohammad Javad Shafiee, Farzad Khalvati, Masoom A. Haider, Alexander Wong
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Diffusion weighted magnetic resonance imaging (DW-MR) is a powerful tool in imaging-based prostate cancer screening and detection. Endorectal coils are commonly used in DW-MR imaging to improve the signal-to-noise ratio (SNR) of the acquisition, at the expense of significant intensity inhomogeneities (bias field) that worsens as we move away from the endorectal coil. The presence of bias field can have a significant negative impact on the accuracy of different image analysis tasks, as well as prostate tumor localization, thus leading to increased inter- and intra-observer variability. Retrospective bias correction approaches are introduced as a more efficient way of bias correction compared to the prospective methods such that they correct for both of the scanner and anatomy-related bias fields in MR imaging. Previously proposed retrospective bias field correction methods suffer from undesired noise amplification that can reduce the quality of bias-corrected DW-MR image. Here, we propose a unified data reconstruction approach that enables joint compensation of bias field as well as data noise in DW-MR imaging. The proposed noise-compensated, bias-corrected (NCBC) data reconstruction method takes advantage of a novel stochastically fully connected joint conditional random field (SFC-JCRF) model to mitigate the effects of data noise and bias field in the reconstructed MR data. The proposed NCBC reconstruction method was tested on synthetic DW-MR data, physical DW-phantom as well as real DW-MR data all acquired using endorectal MR coil. Both qualitative and quantitative analysis illustrated that the proposed NCBC method can achieve improved image quality when compared to other tested bias correction methods. As such, the proposed NCBC method may have potential as a useful retrospective approach for improving the consistency of image interpretations.



### On Deep Representation Learning from Noisy Web Images
- **Arxiv ID**: http://arxiv.org/abs/1512.04785v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1512.04785v2)
- **Published**: 2015-12-15 13:57:39+00:00
- **Updated**: 2016-07-15 20:50:54+00:00
- **Authors**: Phong D. Vo, Alexandru Ginsca, Hervé Le Borgne, Adrian Popescu
- **Comment**: None
- **Journal**: None
- **Summary**: The keep-growing content of Web images may be the next important data source to scale up deep neural networks, which recently obtained a great success in the ImageNet classification challenge and related tasks. This prospect, however, has not been validated on convolutional networks (convnet) -- one of best performing deep models -- because of their supervised regime. While unsupervised alternatives are not so good as convnet in generalizing the learned model to new domains, we use convnet to leverage semi-supervised representation learning. Our approach is to use massive amounts of unlabeled and noisy Web images to train convnets as general feature detectors despite challenges coming from data such as high level of mislabeled data, outliers, and data biases. Extensive experiments are conducted at several data scales, different network architectures, and data reranking techniques. The learned representations are evaluated on nine public datasets of various topics. The best results obtained by our convnets, trained on 3.14 million Web images, outperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and is closing the gap with VGG-16. These prominent results suggest a budget solution to use deep learning in practice and motivate more research in semi-supervised representation learning.



### Context Driven Label Fusion for segmentation of Subcutaneous and Visceral Fat in CT Volumes
- **Arxiv ID**: http://arxiv.org/abs/1512.04958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.04958v1)
- **Published**: 2015-12-15 21:02:32+00:00
- **Updated**: 2015-12-15 21:02:32+00:00
- **Authors**: Sarfaraz Hussein, Aileen Green, Arjun Watane, Georgios Papadakis, Medhat Osman, Ulas Bagci
- **Comment**: ISBI 2016 submission, 5 pages, 4 figures
- **Journal**: None
- **Summary**: Quantification of adipose tissue (fat) from computed tomography (CT) scans is conducted mostly through manual or semi-automated image segmentation algorithms with limited efficacy. In this work, we propose a completely unsupervised and automatic method to identify adipose tissue, and then separate Subcutaneous Adipose Tissue (SAT) from Visceral Adipose Tissue (VAT) at the abdominal region. We offer a three-phase pipeline consisting of (1) Initial boundary estimation using gradient points, (2) boundary refinement using Geometric Median Absolute Deviation and Appearance based Local Outlier Scores (3) Context driven label fusion using Conditional Random Fields (CRF) to obtain the final boundary between SAT and VAT. We evaluate the proposed method on 151 abdominal CT scans and obtain state-of-the-art 94% and 91% dice similarity scores for SAT and VAT segmentation, as well as significant reduction in fat quantification error measure.



### Multiple penalized principal curves: analysis and computation
- **Arxiv ID**: http://arxiv.org/abs/1512.05010v2
- **DOI**: None
- **Categories**: **math.AP**, cs.CV, stat.ML, 49M25, 65D10, 62G99, 65D18, 65K10, 49Q20
- **Links**: [PDF](http://arxiv.org/pdf/1512.05010v2)
- **Published**: 2015-12-15 23:26:50+00:00
- **Updated**: 2016-08-30 13:19:05+00:00
- **Authors**: Slav Kirov, Dejan Slepčev
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of finding the one-dimensional structure in a given data set. In other words we consider ways to approximate a given measure (data) by curves. We consider an objective functional whose minimizers are a regularization of principal curves and introduce a new functional which allows for multiple curves. We prove the existence of minimizers and establish their basic properties. We develop an efficient algorithm for obtaining (near) minimizers of the functional. While both of the functionals used are nonconvex, we argue that enlarging the configuration space to allow for multiple curves leads to a simpler energy landscape with fewer undesirable (high-energy) local minima. Furthermore we note that the approach proposed is able to find the one-dimensional structure even for data with considerable amount of noise.



