# Arxiv Papers in cs.CV on 2015-12-22
### Transformed Residual Quantization for Approximate Nearest Neighbor Search
- **Arxiv ID**: http://arxiv.org/abs/1512.06925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.06925v1)
- **Published**: 2015-12-22 01:12:54+00:00
- **Updated**: 2015-12-22 01:12:54+00:00
- **Authors**: Jiangbo Yuan, Xiuwen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The success of product quantization (PQ) for fast nearest neighbor search depends on the exponentially reduced complexities of both storage and computation with respect to the codebook size. Recent efforts have been focused on employing sophisticated optimization strategies, or seeking more effective models. Residual quantization (RQ) is such an alternative that holds the same property as PQ in terms of the aforementioned complexities. In addition to being a direct replacement of PQ, hybrids of PQ and RQ can yield more gains for approximate nearest neighbor search. This motivated us to propose a novel approach to optimizing RQ and the related hybrid models. With an observation of the general randomness increase in a residual space, we propose a new strategy that jointly learns a local transformation per residual cluster with an ultimate goal to reduce overall quantization errors. We have shown that our approach can achieve significantly better accuracy on nearest neighbor search than both the original and the optimized PQ on several very large scale benchmarks.



### Multi-Instance Visual-Semantic Embedding
- **Arxiv ID**: http://arxiv.org/abs/1512.06963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.06963v1)
- **Published**: 2015-12-22 05:54:34+00:00
- **Updated**: 2015-12-22 05:54:34+00:00
- **Authors**: Zhou Ren, Hailin Jin, Zhe Lin, Chen Fang, Alan Yuille
- **Comment**: 9 pages, CVPR 2016 submission
- **Journal**: None
- **Summary**: Visual-semantic embedding models have been recently proposed and shown to be effective for image classification and zero-shot learning, by mapping images into a continuous semantic label space. Although several approaches have been proposed for single-label embedding tasks, handling images with multiple labels (which is a more general setting) still remains an open problem, mainly due to the complex underlying corresponding relationship between image and its labels. In this work, we present Multi-Instance visual-semantic Embedding model (MIE) for embedding images associated with either single or multiple labels. Our model discovers and maps semantically-meaningful image subregions to their corresponding labels. And we demonstrate the superiority of our method over the state-of-the-art on two tasks, including multi-label image annotation and zero-shot learning.



### Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels
- **Arxiv ID**: http://arxiv.org/abs/1512.06974v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.06974v2)
- **Published**: 2015-12-22 07:28:06+00:00
- **Updated**: 2016-04-12 19:58:29+00:00
- **Authors**: Ishan Misra, C. Lawrence Zitnick, Margaret Mitchell, Ross Girshick
- **Comment**: To appear in CVPR 2016
- **Journal**: None
- **Summary**: When human annotators are given a choice about what to label in an image, they apply their own subjective judgments on what to ignore and what to mention. We refer to these noisy "human-centric" annotations as exhibiting human reporting bias. Examples of such annotations include image tags and keywords found on photo sharing sites, or in datasets containing image captions. In this paper, we use these noisy annotations for learning visually correct image classifiers. Such annotations do not use consistent vocabulary, and miss a significant amount of the information present in an image; however, we demonstrate that the noise in these annotations exhibits structure and can be modeled. We propose an algorithm to decouple the human reporting bias from the correct visually grounded labels. Our results are highly interpretable for reporting "what's in the image" versus "what's worth saying." We demonstrate the algorithm's efficacy along a variety of metrics and datasets, including MS COCO and Yahoo Flickr 100M. We show significant improvements over traditional algorithms for both image classification and image captioning, doubling the performance of existing methods in some cases.



### Deep Learning with S-shaped Rectified Linear Activation Units
- **Arxiv ID**: http://arxiv.org/abs/1512.07030v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.07030v1)
- **Published**: 2015-12-22 10:54:26+00:00
- **Updated**: 2015-12-22 10:54:26+00:00
- **Authors**: Xiaojie Jin, Chunyan Xu, Jiashi Feng, Yunchao Wei, Junjun Xiong, Shuicheng Yan
- **Comment**: Accepted by AAAI-16
- **Journal**: None
- **Summary**: Rectified linear activation units are important components for state-of-the-art deep convolutional networks. In this paper, we propose a novel S-shaped rectified linear activation unit (SReLU) to learn both convex and non-convex functions, imitating the multiple function forms given by the two fundamental laws, namely the Webner-Fechner law and the Stevens law, in psychophysics and neural sciences. Specifically, SReLU consists of three piecewise linear functions, which are formulated by four learnable parameters. The SReLU is learned jointly with the training of the whole deep network through back propagation. During the training phase, to initialize SReLU in different layers, we propose a "freezing" method to degenerate SReLU into a predefined leaky rectified linear unit in the initial several training epochs and then adaptively learn the good initial values. SReLU can be universally used in the existing deep networks with negligible additional parameters and computation cost. Experiments with two popular CNN architectures, Network in Network and GoogLeNet on scale-various benchmarks including CIFAR10, CIFAR100, MNIST and ImageNet demonstrate that SReLU achieves remarkable improvement compared to other activation functions.



### Implementation of deep learning algorithm for automatic detection of brain tumors using intraoperative IR-thermal mapping data
- **Arxiv ID**: http://arxiv.org/abs/1512.07041v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM, stat.ML, 92C55, 68T45, 68T10, 62M45, I.5.1; I.4.8; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1512.07041v1)
- **Published**: 2015-12-22 11:52:26+00:00
- **Updated**: 2015-12-22 11:52:26+00:00
- **Authors**: A. V. Makarenko, M. G. Volovik
- **Comment**: 7 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: The efficiency of deep machine learning for automatic delineation of tumor areas has been demonstrated for intraoperative neuronavigation using active IR-mapping with the use of the cold test. The proposed approach employs a matrix IR-imager to remotely register the space-time distribution of surface temperature pattern, which is determined by the dynamics of local cerebral blood flow. The advantages of this technique are non-invasiveness, zero risks for the health of patients and medical staff, low implementation and operational costs, ease and speed of use. Traditional IR-diagnostic technique has a crucial limitation - it involves a diagnostician who determines the boundaries of tumor areas, which gives rise to considerable uncertainty, which can lead to diagnosis errors that are difficult to control. The current study demonstrates that implementing deep learning algorithms allows to eliminate the explained drawback.



### Cost-based Feature Transfer for Vehicle Occupant Classification
- **Arxiv ID**: http://arxiv.org/abs/1512.07080v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1512.07080v1)
- **Published**: 2015-12-22 13:35:10+00:00
- **Updated**: 2015-12-22 13:35:10+00:00
- **Authors**: Toby Perrett, Majid Mirmehdi, Eduardo Dias
- **Comment**: 9 pages, 4 figures, 5 tables
- **Journal**: None
- **Summary**: Knowledge of human presence and interaction in a vehicle is of growing interest to vehicle manufacturers for design and safety purposes. We present a framework to perform the tasks of occupant detection and occupant classification for automatic child locks and airbag suppression. It operates for all passenger seats, using a single overhead camera. A transfer learning technique is introduced to make full use of training data from all seats whilst still maintaining some control over the bias, necessary for a system designed to penalize certain misclassifications more than others. An evaluation is performed on a challenging dataset with both weighted and unweighted classifiers, demonstrating the effectiveness of the transfer process.



### Recent Advances in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1512.07108v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1512.07108v6)
- **Published**: 2015-12-22 14:54:34+00:00
- **Updated**: 2017-10-19 16:34:35+00:00
- **Authors**: Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Li Wang, Gang Wang, Jianfei Cai, Tsuhan Chen
- **Comment**: Pattern Recognition, Elsevier
- **Journal**: None
- **Summary**: In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.



### SR-Clustering: Semantic Regularized Clustering for Egocentric Photo Streams Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1512.07143v2
- **DOI**: 10.1016/j.cviu.2016.10.005
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1512.07143v2)
- **Published**: 2015-12-22 16:13:54+00:00
- **Updated**: 2016-10-17 09:40:11+00:00
- **Authors**: Mariella Dimiccoli, Marc Bolaños, Estefania Talavera, Maedeh Aghaei, Stavri G. Nikolov, Petia Radeva
- **Comment**: 23 pages, 10 figures, 2 tables. In Press in Computer Vision and Image
  Understanding Journal
- **Journal**: None
- **Summary**: While wearable cameras are becoming increasingly popular, locating relevant information in large unstructured collections of egocentric images is still a tedious and time consuming processes. This paper addresses the problem of organizing egocentric photo streams acquired by a wearable camera into semantically meaningful segments. First, contextual and semantic information is extracted for each image by employing a Convolutional Neural Networks approach. Later, by integrating language processing, a vocabulary of concepts is defined in a semantic space. Finally, by exploiting the temporal coherence in photo streams, images which share contextual and semantic attributes are grouped together. The resulting temporal segmentation is particularly suited for further analysis, ranging from activity and event recognition to semantic indexing and summarization. Experiments over egocentric sets of nearly 17,000 images, show that the proposed approach outperforms state-of-the-art methods.



### Do Less and Achieve More: Training CNNs for Action Recognition Utilizing Action Images from the Web
- **Arxiv ID**: http://arxiv.org/abs/1512.07155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.07155v1)
- **Published**: 2015-12-22 16:52:19+00:00
- **Updated**: 2015-12-22 16:52:19+00:00
- **Authors**: Shugao Ma, Sarah Adel Bargal, Jianming Zhang, Leonid Sigal, Stan Sclaroff
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, attempts have been made to collect millions of videos to train CNN models for action recognition in videos. However, curating such large-scale video datasets requires immense human labor, and training CNNs on millions of videos demands huge computational resources. In contrast, collecting action images from the Web is much easier and training on images requires much less computation. In addition, labeled web images tend to contain discriminative action poses, which highlight discriminative portions of a video's temporal progression. We explore the question of whether we can utilize web action images to train better CNN models for action recognition in videos. We collect 23.8K manually filtered images from the Web that depict the 101 actions in the UCF101 action video dataset. We show that by utilizing web action images along with videos in training, significant performance boosts of CNN models can be achieved. We then investigate the scalability of the process by leveraging crawled web images (unfiltered) for UCF101 and ActivityNet. We replace 16.2M video frames by 393K unfiltered images and get comparable performance.



