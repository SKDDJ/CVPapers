# Arxiv Papers in cs.CV on 2015-12-11
### Randomized Low-Rank Dynamic Mode Decomposition for Motion Detection
- **Arxiv ID**: http://arxiv.org/abs/1512.03526v1
- **DOI**: 10.1016/j.cviu.2016.02.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.03526v1)
- **Published**: 2015-12-11 05:20:40+00:00
- **Updated**: 2015-12-11 05:20:40+00:00
- **Authors**: N. Benjamin Erichson, Carl Donovan
- **Comment**: Preprint submitted to Journal of Computer Vision and Image
  Understanding
- **Journal**: None
- **Summary**: This paper introduces a fast algorithm for randomized computation of a low-rank Dynamic Mode Decomposition (DMD) of a matrix. Here we consider this matrix to represent the development of a spatial grid through time e.g. data from a static video source. DMD was originally introduced in the fluid mechanics community, but is also suitable for motion detection in video streams and its use for background subtraction has received little previous investigation. In this study we present a comprehensive evaluation of background subtraction, using the randomized DMD and compare the results with leading robust principal component analysis algorithms. The results are convincing and show the random DMD is an efficient and powerful approach for background modeling, allowing processing of high resolution videos in real-time. Supplementary materials include implementations of the algorithms in Python.



### Robust Dictionary based Data Representation
- **Arxiv ID**: http://arxiv.org/abs/1512.03617v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.03617v1)
- **Published**: 2015-12-11 12:17:35+00:00
- **Updated**: 2015-12-11 12:17:35+00:00
- **Authors**: Wei-Ya Ren
- **Comment**: 8 pages. 2015.12.10
- **Journal**: None
- **Summary**: The robustness to noise and outliers is an important issue in linear representation in real applications. We focus on the problem that samples are grossly corrupted, which is also the 'sample specific' corruptions problem. A reasonable assumption is that corrupted samples cannot be represented by the dictionary while clean samples can be well represented. This assumption is enforced in this paper by investigating the coefficients of corrupted samples. Concretely, we require the coefficients of corrupted samples be zero. In this way, the representation quality of clean data can be assured without the effect of corrupted data. At last, a robust dictionary based data representation approach and its sparse representation version are proposed, which have directive significance for future applications.



### Deep Feature Learning with Relative Distance Comparison for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1512.03622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.03622v1)
- **Published**: 2015-12-11 12:34:22+00:00
- **Updated**: 2015-12-11 12:34:22+00:00
- **Authors**: Shengyong Ding, Liang Lin, Guangrun Wang, Hongyang Chao
- **Comment**: 29 pages, 9 figures, The code has been released.
  http://vision.sysu.edu.cn/projects/deepreid/
- **Journal**: None
- **Summary**: Identifying the same individual across different scenes is an important yet difficult task in intelligent video surveillance. Its main difficulty lies in how to preserve similarity of the same person against large appearance and structure variation while discriminating different individuals. In this paper, we present a scalable distance driven feature learning framework based on the deep neural network for person re-identification, and demonstrate its effectiveness to handle the existing challenges. Specifically, given the training images with the class labels (person IDs), we first produce a large number of triplet units, each of which contains three images, i.e. one person with a matched reference and a mismatched reference. Treating the units as the input, we build the convolutional neural network to generate the layered representations, and follow with the $L2$ distance metric. By means of parameter optimization, our framework tends to maximize the relative distance between the matched pair and the mismatched pair for each triplet unit. Moreover, a nontrivial issue arising with the framework is that the triplet organization cubically enlarges the number of training triplets, as one image can be involved into several triplet units. To overcome this problem, we develop an effective triplet generation scheme and an optimized gradient descent algorithm, making the computational load mainly depends on the number of original images instead of the number of triplets. On several challenging databases, our approach achieves very promising results and outperforms other state-of-the-art approaches.



### A New Approach of Gray Images Binarization with Threshold Methods
- **Arxiv ID**: http://arxiv.org/abs/1512.03706v1
- **DOI**: 10.13140/RG.2.1.4391.8165
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.03706v1)
- **Published**: 2015-12-11 16:57:08+00:00
- **Updated**: 2015-12-11 16:57:08+00:00
- **Authors**: Andrei Hossu, Daniela Andone
- **Comment**: None
- **Journal**: None
- **Summary**: The paper presents some aspects of the (gray level) image binarization methods used in artificial vision systems. It is introduced a new approach of gray level image binarization for artificial vision systems dedicated to industrial automation temporal thresholding. In the first part of the paper are extracted some limitations of using the global optimum thresholding in gray level image binarization. In the second part of this paper are presented some aspects of the dynamic optimum thresholding method for gray level image binarization. Starting from classic methods of global and dynamic optimal thresholding of the gray level images in the next section are introduced the concepts of temporal histogram and temporal thresholding. In the final section are presented some practical aspects of the temporal thresholding method in artificial vision applications form the moving scene in robotic automation class; pointing out the influence of the acquisition frequency on the methods results.



### Improving Human Activity Recognition Through Ranking and Re-ranking
- **Arxiv ID**: http://arxiv.org/abs/1512.03740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.03740v1)
- **Published**: 2015-12-11 17:41:53+00:00
- **Updated**: 2015-12-11 17:41:53+00:00
- **Authors**: Zhenzhong Lan, Shoou-I Yu, Alexander G. Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: We propose two well-motivated ranking-based methods to enhance the performance of current state-of-the-art human activity recognition systems. First, as an improvement over the classic power normalization method, we propose a parameter-free ranking technique called rank normalization (RaN). RaN normalizes each dimension of the video features to address the sparse and bursty distribution problems of Fisher Vectors and VLAD. Second, inspired by curriculum learning, we introduce a training-free re-ranking technique called multi-class iterative re-ranking (MIR). MIR captures relationships among action classes by separating easy and typical videos from difficult ones and re-ranking the prediction scores of classifiers accordingly. We demonstrate that our methods significantly improve the performance of state-of-the-art motion features on six real-world datasets.



