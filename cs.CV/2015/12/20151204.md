# Arxiv Papers in cs.CV on 2015-12-04
### Predicting and visualizing psychological attributions with a deep neural network
- **Arxiv ID**: http://arxiv.org/abs/1512.01289v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1512.01289v2)
- **Published**: 2015-12-04 00:24:16+00:00
- **Updated**: 2016-12-24 01:06:35+00:00
- **Authors**: Edward Grant, Stephan Sahm, Mariam Zabihi, Marcel van Gerven
- **Comment**: None
- **Journal**: None
- **Summary**: Judgments about personality based on facial appearance are strong effectors in social decision making, and are known to have impact on areas from presidential elections to jury decisions. Recent work has shown that it is possible to predict perception of memorability, trustworthiness, intelligence and other attributes in human face images. The most successful of these approaches require face images expertly annotated with key facial landmarks. We demonstrate a Convolutional Neural Network (CNN) model that is able to perform the same task without the need for landmark features, thereby greatly increasing efficiency. The model has high accuracy, surpassing human-level performance in some cases. Furthermore, we use a deconvolutional approach to visualize important features for perception of 22 attributes and demonstrate a new method for separately visualizing positive and negative features.



### What can we learn about CNNs from a large scale controlled object dataset?
- **Arxiv ID**: http://arxiv.org/abs/1512.01320v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.01320v2)
- **Published**: 2015-12-04 05:48:09+00:00
- **Updated**: 2016-01-26 16:56:11+00:00
- **Authors**: Ali Borji, Saeed Izadi, Laurent Itti
- **Comment**: None
- **Journal**: None
- **Summary**: Tolerance to image variations (e.g. translation, scale, pose, illumination) is an important desired property of any object recognition system, be it human or machine. Moving towards increasingly bigger datasets has been trending in computer vision specially with the emergence of highly popular deep learning models. While being very useful for learning invariance to object inter- and intra-class shape variability, these large-scale wild datasets are not very useful for learning invariance to other parameters forcing researchers to resort to other tricks for training a model. In this work, we introduce a large-scale synthetic dataset, which is freely and publicly available, and use it to answer several fundamental questions regarding invariance and selectivity properties of convolutional neural networks. Our dataset contains two parts: a) objects shot on a turntable: 16 categories, 8 rotation angles, 11 cameras on a semicircular arch, 5 lighting conditions, 3 focus levels, variety of backgrounds (23.4 per instance) generating 1320 images per instance (over 20 million images in total), and b) scenes: in which a robot arm takes pictures of objects on a 1:160 scale scene. We study: 1) invariance and selectivity of different CNN layers, 2) knowledge transfer from one object category to another, 3) systematic or random sampling of images to build a train set, 4) domain adaptation from synthetic to natural scenes, and 5) order of knowledge delivery to CNNs. We also explore how our analyses can lead the field to develop more efficient CNNs.



### Toward a Taxonomy and Computational Models of Abnormalities in Images
- **Arxiv ID**: http://arxiv.org/abs/1512.01325v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1512.01325v1)
- **Published**: 2015-12-04 06:29:53+00:00
- **Updated**: 2015-12-04 06:29:53+00:00
- **Authors**: Babak Saleh, Ahmed Elgammal, Jacob Feldman, Ali Farhadi
- **Comment**: To appear in the Thirtieth AAAI Conference on Artificial Intelligence
  (AAAI 2016)
- **Journal**: None
- **Summary**: The human visual system can spot an abnormal image, and reason about what makes it strange. This task has not received enough attention in computer vision. In this paper we study various types of atypicalities in images in a more comprehensive way than has been done before. We propose a new dataset of abnormal images showing a wide range of atypicalities. We design human subject experiments to discover a coarse taxonomy of the reasons for abnormality. Our experiments reveal three major categories of abnormality: object-centric, scene-centric, and contextual. Based on this taxonomy, we propose a comprehensive computational model that can predict all different types of abnormality in images and outperform prior arts in abnormality recognition.



### Staple: Complementary Learners for Real-Time Tracking
- **Arxiv ID**: http://arxiv.org/abs/1512.01355v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.01355v2)
- **Published**: 2015-12-04 09:56:48+00:00
- **Updated**: 2016-04-13 17:58:11+00:00
- **Authors**: Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej Miksik, Philip Torr
- **Comment**: To appear in CVPR 2016
- **Journal**: None
- **Summary**: Correlation Filter-based trackers have recently achieved excellent performance, showing great robustness to challenging situations exhibiting motion blur and illumination changes. However, since the model that they learn depends strongly on the spatial layout of the tracked object, they are notoriously sensitive to deformation. Models based on colour statistics have complementary traits: they cope well with variation in shape, but suffer when illumination is not consistent throughout a sequence. Moreover, colour distributions alone can be insufficiently discriminative. In this paper, we show that a simple tracker combining complementary cues in a ridge regression framework can operate faster than 80 FPS and outperform not only all entries in the popular VOT14 competition, but also recent and far more sophisticated trackers according to multiple benchmarks.



### Sublabel-Accurate Relaxation of Nonconvex Energies
- **Arxiv ID**: http://arxiv.org/abs/1512.01383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.01383v1)
- **Published**: 2015-12-04 12:16:13+00:00
- **Updated**: 2015-12-04 12:16:13+00:00
- **Authors**: Thomas MÃ¶llenhoff, Emanuel Laude, Michael Moeller, Jan Lellmann, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel spatially continuous framework for convex relaxations based on functional lifting. Our method can be interpreted as a sublabel-accurate solution to multilabel problems. We show that previously proposed functional lifting methods optimize an energy which is linear between two labels and hence require (often infinitely) many labels for a faithful approximation. In contrast, the proposed formulation is based on a piecewise convex approximation and therefore needs far fewer labels. In comparison to recent MRF-based approaches, our method is formulated in a spatially continuous setting and shows less grid bias. Moreover, in a local sense, our formulation is the tightest possible convex relaxation. It is easy to implement and allows an efficient primal-dual optimization on GPUs. We show the effectiveness of our approach on several computer vision problems.



### Max-Pooling Dropout for Regularization of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1512.01400v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1512.01400v1)
- **Published**: 2015-12-04 13:18:37+00:00
- **Updated**: 2015-12-04 13:18:37+00:00
- **Authors**: Haibing Wu, Xiaodong Gu
- **Comment**: The journal version of this paper [arXiv:1512.00242] has been
  published in Neural Networks,
  http://www.sciencedirect.com/science/article/pii/S0893608015001446
- **Journal**: None
- **Summary**: Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.



### Model Validation for Vision Systems via Graphics Simulation
- **Arxiv ID**: http://arxiv.org/abs/1512.01401v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.01401v1)
- **Published**: 2015-12-04 13:20:23+00:00
- **Updated**: 2015-12-04 13:20:23+00:00
- **Authors**: V S R Veeravasarapu, Rudra Narayan Hota, Constantin Rothkopf, Ramesh Visvanathan
- **Comment**: arXiv admin note: text overlap with arXiv:1512.01030
- **Journal**: None
- **Summary**: Rapid advances in computation, combined with latest advances in computer graphics simulations have facilitated the development of vision systems and training them in virtual environments. One major stumbling block is in certification of the designs and tuned parameters of these systems to work in real world. In this paper, we begin to explore the fundamental question: Which type of information transfer is more analogous to real world? Inspired from the performance characterization methodology outlined in the 90's, we note that insights derived from simulations can be qualitative or quantitative depending on the degree of the fidelity of models used in simulations and the nature of the questions posed by the experimenter. We adapt the methodology in the context of current graphics simulation tools for modeling data generation processes and, for systematic performance characterization and trade-off analysis for vision system design leading to qualitative and quantitative insights. In concrete, we examine invariance assumptions used in vision algorithms for video surveillance settings as a case study and assess the degree to which those invariance assumptions deviate as a function of contextual variables on both graphics simulations and in real data. As computer graphics rendering quality improves, we believe teasing apart the degree to which model assumptions are valid via systematic graphics simulation can be a significant aid to assisting more principled ways of approaching vision system design and performance modeling.



### Computational Imaging for VLBI Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1512.01413v2
- **DOI**: None
- **Categories**: **astro-ph.IM**, astro-ph.GA, cs.CV, I.4, I.4.5, I.4.4, G.1.8, J.2
- **Links**: [PDF](http://arxiv.org/pdf/1512.01413v2)
- **Published**: 2015-12-04 14:11:46+00:00
- **Updated**: 2016-11-07 15:57:40+00:00
- **Authors**: Katherine L. Bouman, Michael D. Johnson, Daniel Zoran, Vincent L. Fish, Sheperd S. Doeleman, William T. Freeman
- **Comment**: Accepted for publication at CVPR 2016, Project Website:
  http://vlbiimaging.csail.mit.edu/, Video of Oral Presentation at CVPR June
  2016: https://www.youtube.com/watch?v=YgB6o_d4tL8
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2016, pp. 913-922
- **Summary**: Very long baseline interferometry (VLBI) is a technique for imaging celestial radio emissions by simultaneously observing a source from telescopes distributed across Earth. The challenges in reconstructing images from fine angular resolution VLBI data are immense. The data is extremely sparse and noisy, thus requiring statistical image models such as those designed in the computer vision community. In this paper we present a novel Bayesian approach for VLBI image reconstruction. While other methods often require careful tuning and parameter selection for different types of data, our method (CHIRP) produces good results under different settings such as low SNR or extended emission. The success of our method is demonstrated on realistic synthetic experiments as well as publicly available real data. We present this problem in a way that is accessible to members of the community, and provide a dataset website (vlbiimaging.csail.mit.edu) that facilitates controlled comparisons across algorithms.



### ASIST: Automatic Semantically Invariant Scene Transformation
- **Arxiv ID**: http://arxiv.org/abs/1512.01515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.01515v1)
- **Published**: 2015-12-04 19:14:57+00:00
- **Updated**: 2015-12-04 19:14:57+00:00
- **Authors**: Or Litany, Tal Remez, Daniel Freedman, Lior Shapira, Alex Bronstein, Ran Gal
- **Comment**: None
- **Journal**: None
- **Summary**: We present ASIST, a technique for transforming point clouds by replacing objects with their semantically equivalent counterparts. Transformations of this kind have applications in virtual reality, repair of fused scans, and robotics. ASIST is based on a unified formulation of semantic labeling and object replacement; both result from minimizing a single objective. We present numerical tools for the efficient solution of this optimization problem. The method is experimentally assessed on new datasets of both synthetic and real point clouds, and is additionally compared to two recent works on object replacement on data from the corresponding papers.



### Learning the Semantics of Manipulation Action
- **Arxiv ID**: http://arxiv.org/abs/1512.01525v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1512.01525v1)
- **Published**: 2015-12-04 20:00:08+00:00
- **Updated**: 2015-12-04 20:00:08+00:00
- **Authors**: Yezhou Yang, Yiannis Aloimonos, Cornelia Fermuller, Eren Erdal Aksoy
- **Comment**: None
- **Journal**: The 53rd Annual Meeting of the Association for Computational
  Linguistics (ACL) 1 (2015) 676-686
- **Summary**: In this paper we present a formal computational framework for modeling manipulation actions. The introduced formalism leads to semantics of manipulation action and has applications to both observing and understanding human manipulation actions as well as executing them with a robotic mechanism (e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. The goal of the introduced framework is to: (1) represent manipulation actions with both syntax and semantic parts, where the semantic part employs $\lambda$-calculus; (2) enable a probabilistic semantic parsing schema to learn the $\lambda$-calculus representation of manipulation action from an annotated action corpus of videos; (3) use (1) and (2) to develop a system that visually observes manipulation actions and understands their meaning while it can reason beyond observations using propositional logic and axiom schemata. The experiments conducted on a public available large manipulation action dataset validate the theoretical framework and our implementation.



### Motion trails from time-lapse video
- **Arxiv ID**: http://arxiv.org/abs/1512.01533v1
- **DOI**: None
- **Categories**: **cs.CV**, I.3.3; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1512.01533v1)
- **Published**: 2015-12-04 20:28:27+00:00
- **Updated**: 2015-12-04 20:28:27+00:00
- **Authors**: Camille Goudeseune
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: From an image sequence captured by a stationary camera, background subtraction can detect moving foreground objects in the scene. Distinguishing foreground from background is further improved by various heuristics. Then each object's motion can be emphasized by duplicating its positions as a motion trail. These trails clarify the objects' spatial relationships. Also, adding motion trails to a video before previewing it at high speed reduces the risk of overlooking transient events.



### Creation of a Deep Convolutional Auto-Encoder in Caffe
- **Arxiv ID**: http://arxiv.org/abs/1512.01596v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, 68Txx, F.1.1; I.2.6; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1512.01596v3)
- **Published**: 2015-12-04 23:58:47+00:00
- **Updated**: 2016-04-22 03:20:41+00:00
- **Authors**: Volodymyr Turchenko, Artur Luczak
- **Comment**: 9 pages, 7 figures, 5 tables, 34 references in the list; Added
  references, corrected Table 3, changed several paragraphs in the text
- **Journal**: None
- **Summary**: The development of a deep (stacked) convolutional auto-encoder in the Caffe deep learning framework is presented in this paper. We describe simple principles which we used to create this model in Caffe. The proposed model of convolutional auto-encoder does not have pooling/unpooling layers yet. The results of our experimental research show comparable accuracy of dimensionality reduction in comparison with a classic auto-encoder on the example of MNIST dataset.



