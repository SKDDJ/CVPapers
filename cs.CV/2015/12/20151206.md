# Arxiv Papers in cs.CV on 2015-12-06
### A Restricted Visual Turing Test for Deep Scene and Event Understanding
- **Arxiv ID**: http://arxiv.org/abs/1512.01715v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1512.01715v2)
- **Published**: 2015-12-06 00:40:02+00:00
- **Updated**: 2015-12-16 19:19:25+00:00
- **Authors**: Hang Qi, Tianfu Wu, Mun-Wai Lee, Song-Chun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a restricted visual Turing test (VTT) for story-line based deep understanding in long-term and multi-camera captured videos. Given a set of videos of a scene (such as a multi-room office, a garden, and a parking lot.) and a sequence of story-line based queries, the task is to provide answers either simply in binary form "true/false" (to a polar query) or in an accurate natural language description (to a non-polar query). Queries, polar or non-polar, consist of view-based queries which can be answered from a particular camera view and scene-centered queries which involves joint inference across different cameras. The story lines are collected to cover spatial, temporal and causal understanding of input videos. The data and queries distinguish our VTT from recently proposed visual question answering in images and video captioning. A vision system is proposed to perform joint video and query parsing which integrates different vision modules, a knowledge base and a query engine. The system provides unified interfaces for different modules so that individual modules can be reconfigured to test a new method. We provide a benchmark dataset and a toolkit for ontology guided story-line query generation which consists of about 93.5 hours videos captured in four different locations and 3,426 queries split into 127 story lines. We also provide a baseline implementation and result analyses.



### Vanishing point attracts gaze in free-viewing and visual search tasks
- **Arxiv ID**: http://arxiv.org/abs/1512.01722v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.01722v4)
- **Published**: 2015-12-06 01:59:51+00:00
- **Updated**: 2016-09-06 16:01:13+00:00
- **Authors**: Ali Borji, Mengyang Feng
- **Comment**: There is a major problem with the arguments and results unfortunately
- **Journal**: None
- **Summary**: To investigate whether the vanishing point (VP) plays a significant role in gaze guidance, we ran two experiments. In the first one, we recorded fixations of 10 observers (4 female; mean age 22; SD=0.84) freely viewing 532 images, out of which 319 had VP (shuffled presentation; each image for 4 secs). We found that the average number of fixations at a local region (80x80 pixels) centered at the VP is significantly higher than the average fixations at random locations (t-test; n=319; p=1.8e-35). To address the confounding factor of saliency, we learned a combined model of bottom-up saliency and VP. AUC score of our model (0.85; SD=0.01) is significantly higher than the original saliency model (e.g., 0.8 using AIM model by Bruce & Tsotsos (2009), t-test; p= 3.14e-16) and the VP-only model (0.64, t-test; p= 4.02e-22). In the second experiment, we asked 14 subjects (4 female, mean age 23.07, SD=1.26) to search for a target character (T or L) placed randomly on a 3x3 imaginary grid overlaid on top of an image. Subjects reported their answers by pressing one of two keys. Stimuli consisted of 270 color images (180 with a single VP, 90 without). The target happened with equal probability inside each cell (15 times L, 15 times T). We found that subjects were significantly faster (and more accurate) when target happened inside the cell containing the VP compared to cells without VP (median across 14 subjects 1.34 sec vs. 1.96; Wilcoxon rank-sum test; p = 0.0014). Response time at VP cells were also significantly lower than response time on images without VP (median 2.37; p= 4.77e-05). These findings support the hypothesis that vanishing point, similar to face and text (Cerf et al., 2009) as well as gaze direction (Borji et al., 2014) attracts attention in free-viewing and visual search.



### Image reconstruction from dense binary pixels
- **Arxiv ID**: http://arxiv.org/abs/1512.01774v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.01774v1)
- **Published**: 2015-12-06 10:59:36+00:00
- **Updated**: 2015-12-06 10:59:36+00:00
- **Authors**: Or Litany, Tal Remez, Alex Bronstein
- **Comment**: Signal Processing with Adaptive Sparse Structured Representations
  (SPARS 2015)
- **Journal**: None
- **Summary**: Recently, the dense binary pixel Gigavision camera had been introduced, emulating a digital version of the photographic film. While seems to be a promising solution for HDR imaging, its output is not directly usable and requires an image reconstruction process. In this work, we formulate this problem as the minimization of a convex objective combining a maximum-likelihood term with a sparse synthesis prior. We present MLNet - a novel feed-forward neural network, producing acceptable output quality at a fixed complexity and is two orders of magnitude faster than iterative algorithms. We present state of the art results in the abstract.



### The Next Best Underwater View
- **Arxiv ID**: http://arxiv.org/abs/1512.01789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.01789v1)
- **Published**: 2015-12-06 13:17:14+00:00
- **Updated**: 2015-12-06 13:17:14+00:00
- **Authors**: Mark Sheinin, Yoav Y. Schechner
- **Comment**: None
- **Journal**: None
- **Summary**: To image in high resolution large and occlusion-prone scenes, a camera must move above and around. Degradation of visibility due to geometric occlusions and distances is exacerbated by scattering, when the scene is in a participating medium. Moreover, underwater and in other media, artificial lighting is needed. Overall, data quality depends on the observed surface, medium and the time-varying poses of the camera and light source. This work proposes to optimize camera/light poses as they move, so that the surface is scanned efficiently and the descattered recovery has the highest quality. The work generalizes the next best view concept of robot vision to scattering media and cooperative movable lighting. It also extends descattering to platforms that move optimally. The optimization criterion is information gain, taken from information theory. We exploit the existence of a prior rough 3D model, since underwater such a model is routinely obtained using sonar. We demonstrate this principle in a scaled-down setup.



### PatchBatch: a Batch Augmented Loss for Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1512.01815v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.01815v2)
- **Published**: 2015-12-06 18:30:28+00:00
- **Updated**: 2016-04-10 14:17:18+00:00
- **Authors**: David Gadot, Lior Wolf
- **Comment**: CVPR 2016
- **Journal**: None
- **Summary**: We propose a new pipeline for optical flow computation, based on Deep Learning techniques. We suggest using a Siamese CNN to independently, and in parallel, compute the descriptors of both images. The learned descriptors are then compared efficiently using the L2 norm and do not require network processing of patch pairs. The success of the method is based on an innovative loss function that computes higher moments of the loss distributions for each training batch. Combined with an Approximate Nearest Neighbor patch matching method and a flow interpolation technique, state of the art performance is obtained on the most challenging and competitive optical flow benchmarks.



### Rank Pooling for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1512.01848v2
- **DOI**: 10.1109/TPAMI.2016.2558148
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.01848v2)
- **Published**: 2015-12-06 22:30:53+00:00
- **Updated**: 2016-05-16 00:41:05+00:00
- **Authors**: Basura Fernando, Efstratios Gavves, Jose Oramas, Amir Ghodrati, Tinne Tuytelaars
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: We propose a function-based temporal pooling method that captures the latent structure of the video sequence data - e.g. how frame-level features evolve over time in a video. We show how the parameters of a function that has been fit to the video data can serve as a robust new video representation. As a specific example, we learn a pooling function via ranking machines. By learning to rank the frame-level features of a video in chronological order, we obtain a new representation that captures the video-wide temporal dynamics of a video, suitable for action recognition. Other than ranking functions, we explore different parametric models that could also explain the temporal changes in videos. The proposed functional pooling methods, and rank pooling in particular, is easy to interpret and implement, fast to compute and effective in recognizing a wide variety of actions. We evaluate our method on various benchmarks for generic action, fine-grained action and gesture recognition. Results show that rank pooling brings an absolute improvement of 7-10 average pooling baseline. At the same time, rank pooling is compatible with and complementary to several appearance and local motion based methods and features, such as improved trajectories and deep learning features.



### Fixation prediction with a combined model of bottom-up saliency and vanishing point
- **Arxiv ID**: http://arxiv.org/abs/1512.01858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.01858v1)
- **Published**: 2015-12-06 23:29:53+00:00
- **Updated**: 2015-12-06 23:29:53+00:00
- **Authors**: Mengyang Feng, Ali Borji, Huchuan Lu
- **Comment**: arXiv admin note: text overlap with arXiv:1512.01722
- **Journal**: None
- **Summary**: By predicting where humans look in natural scenes, we can understand how they perceive complex natural scenes and prioritize information for further high-level visual processing. Several models have been proposed for this purpose, yet there is a gap between best existing saliency models and human performance. While many researchers have developed purely computational models for fixation prediction, less attempts have been made to discover cognitive factors that guide gaze. Here, we study the effect of a particular type of scene structural information, known as the vanishing point, and show that human gaze is attracted to the vanishing point regions. We record eye movements of 10 observers over 532 images, out of which 319 have vanishing points. We then construct a combined model of traditional saliency and a vanishing point channel and show that our model outperforms state of the art saliency models using three scores on our dataset.



