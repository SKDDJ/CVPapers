# Arxiv Papers in cs.CV on 2015-12-09
### Window-Object Relationship Guided Representation Learning for Generic Object Detections
- **Arxiv ID**: http://arxiv.org/abs/1512.02736v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1512.02736v1)
- **Published**: 2015-12-09 03:32:21+00:00
- **Updated**: 2015-12-09 03:32:21+00:00
- **Authors**: Xingyu Zeng, Wanli Ouyang, Xiaogang Wang
- **Comment**: 9 pages, including 1 reference page, 6 figures
- **Journal**: None
- **Summary**: In existing works that learn representation for object detection, the relationship between a candidate window and the ground truth bounding box of an object is simplified by thresholding their overlap. This paper shows information loss in this simplification and picks up the relative location/size information discarded by thresholding. We propose a representation learning pipeline to use the relationship as supervision for improving the learned representation in object detection. Such relationship is not limited to object of the target category, but also includes surrounding objects of other categories. We show that image regions with multiple contexts and multiple rotations are effective in capturing such relationship during the representation learning process and in handling the semantic and visual variation caused by different window-object configurations. Experimental results show that the representation learned by our approach can improve the object detection accuracy by 6.4% in mean average precision (mAP) on ILSVRC2014. On the challenging ILSVRC2014 test dataset, 48.6% mAP is achieved by our single model and it is the best among published results. On PASCAL VOC, it outperforms the state-of-the-art result of Fast RCNN by 3.3% in absolute mAP.



### Sensor Fusion of Camera, GPS and IMU using Fuzzy Adaptive Multiple Motion Models
- **Arxiv ID**: http://arxiv.org/abs/1512.02766v1
- **DOI**: 10.1007/s00500-017-2516-8
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1512.02766v1)
- **Published**: 2015-12-09 06:25:09+00:00
- **Updated**: 2015-12-09 06:25:09+00:00
- **Authors**: Erkan Bostanci, Betul Bostanci, Nadia Kanwal, Adrian F. Clark
- **Comment**: 14 pages, journal
- **Journal**: None
- **Summary**: A tracking system that will be used for Augmented Reality (AR) applications has two main requirements: accuracy and frame rate. The first requirement is related to the performance of the pose estimation algorithm and how accurately the tracking system can find the position and orientation of the user in the environment. Accuracy problems of current tracking devices, considering that they are low-cost devices, cause static errors during this motion estimation process. The second requirement is related to dynamic errors (the end-to-end system delay; occurring because of the delay in estimating the motion of the user and displaying images based on this estimate. This paper investigates combining the vision-based estimates with measurements from other sensors, GPS and IMU, in order to improve the tracking accuracy in outdoor environments. The idea of using Fuzzy Adaptive Multiple Models (FAMM) was investigated using a novel fuzzy rule-based approach to decide on the model that results in improved accuracy and faster convergence for the fusion filter. Results show that the developed tracking system is more accurate than a conventional GPS-IMU fusion approach due to additional estimates from a camera and fuzzy motion models. The paper also presents an application in cultural heritage context.



### Affinity CNN: Learning Pixel-Centric Pairwise Relations for Figure/Ground Embedding
- **Arxiv ID**: http://arxiv.org/abs/1512.02767v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1512.02767v2)
- **Published**: 2015-12-09 06:45:23+00:00
- **Updated**: 2016-04-11 22:03:38+00:00
- **Authors**: Michael Maire, Takuya Narihira, Stella X. Yu
- **Comment**: minor updates; extended version of CVPR 2016 conference paper
- **Journal**: None
- **Summary**: Spectral embedding provides a framework for solving perceptual organization problems, including image segmentation and figure/ground organization. From an affinity matrix describing pairwise relationships between pixels, it clusters pixels into regions, and, using a complex-valued extension, orders pixels according to layer. We train a convolutional neural network (CNN) to directly predict the pairwise relationships that define this affinity matrix. Spectral embedding then resolves these predictions into a globally-consistent segmentation and figure/ground organization of the scene. Experiments demonstrate significant benefit to this direct coupling compared to prior works which use explicit intermediate stages, such as edge detection, on the pathway from image to affinities. Our results suggest spectral embedding as a powerful alternative to the conditional random field (CRF)-based globalization schemes typically coupled to deep neural networks.



### Embedding Label Structures for Fine-Grained Feature Representation
- **Arxiv ID**: http://arxiv.org/abs/1512.02895v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.02895v2)
- **Published**: 2015-12-09 15:22:26+00:00
- **Updated**: 2016-03-11 02:59:36+00:00
- **Authors**: Xiaofan Zhang, Feng Zhou, Yuanqing Lin, Shaoting Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent algorithms in convolutional neural networks (CNN) considerably advance the fine-grained image classification, which aims to differentiate subtle differences among subordinate classes. However, previous studies have rarely focused on learning a fined-grained and structured feature representation that is able to locate similar images at different levels of relevance, e.g., discovering cars from the same make or the same model, both of which require high precision. In this paper, we propose two main contributions to tackle this problem. 1) A multi-task learning framework is designed to effectively learn fine-grained feature representations by jointly optimizing both classification and similarity constraints. 2) To model the multi-level relevance, label structures such as hierarchy or shared attributes are seamlessly embedded into the framework by generalizing the triplet loss. Extensive and thorough experiments have been conducted on three fine-grained datasets, i.e., the Stanford car, the car-333, and the food datasets, which contain either hierarchical labels or shared attributes. Our proposed method has achieved very competitive performance, i.e., among state-of-the-art classification accuracy. More importantly, it significantly outperforms previous fine-grained feature representations for image retrieval at different levels of relevance.



### MovieQA: Understanding Stories in Movies through Question-Answering
- **Arxiv ID**: http://arxiv.org/abs/1512.02902v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1512.02902v2)
- **Published**: 2015-12-09 15:34:31+00:00
- **Updated**: 2016-09-21 04:52:35+00:00
- **Authors**: Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, Sanja Fidler
- **Comment**: CVPR 2016, Spotlight presentation. Benchmark @
  http://movieqa.cs.toronto.edu/ Code @
  https://github.com/makarandtapaswi/MovieQA_CVPR2016/
- **Journal**: None
- **Summary**: We introduce the MovieQA dataset which aims to evaluate automatic story comprehension from both video and text. The dataset consists of 14,944 questions about 408 movies with high semantic diversity. The questions range from simpler "Who" did "What" to "Whom", to "Why" and "How" certain events occurred. Each question comes with a set of five possible answers; a correct one and four deceiving answers provided by human annotators. Our dataset is unique in that it contains multiple sources of information -- video clips, plots, subtitles, scripts, and DVS. We analyze our data through various statistics and methods. We further extend existing QA techniques to show that question-answering with such open-ended semantics is hard. We make this data set public along with an evaluation benchmark to encourage inspiring work in this challenging domain.



### Yet Another Statistical Analysis of Bob Ross Paintings
- **Arxiv ID**: http://arxiv.org/abs/1512.02914v1
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1512.02914v1)
- **Published**: 2015-12-09 15:57:46+00:00
- **Updated**: 2015-12-09 15:57:46+00:00
- **Authors**: Christopher Steven Marcum
- **Comment**: This version based off of arXiv compliant pdflatex source (which
  result in lossy data). Original R-code, Sweave source, and data files are
  available upon request
- **Journal**: None
- **Summary**: In this paper, we analyze a sample of clippings from paintings by the late artist Bob Ross. Previous work focused on the qualitative themes of his paintings (Hickey, 2014); here, we expand on that line of research by considering the colorspace and luminosity values as our data. Our results demonstrate the subtle aesthetics of the average Ross painting, the common variation shared by his paintings, and the structure of the relationships between each painting in our sample. We reveal, for the first time, renderings of the average paintings and introduce "eigenross" components to identify and evaluate shared variance. Additionally, all data and code are embedded in this document to encourage future research, and, in the spirit of Bob Ross, to teach others how to do so.



### Video captioning with recurrent networks based on frame- and video-level features and visual content classification
- **Arxiv ID**: http://arxiv.org/abs/1512.02949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.02949v1)
- **Published**: 2015-12-09 17:17:29+00:00
- **Updated**: 2015-12-09 17:17:29+00:00
- **Authors**: Rakshith Shetty, Jorma Laaksonen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we describe the system for generating textual descriptions of short video clips using recurrent neural networks (RNN), which we used while participating in the Large Scale Movie Description Challenge 2015 in ICCV 2015. Our work builds on static image captioning systems with RNN based language models and extends this framework to videos utilizing both static image features and video-specific features. In addition, we study the usefulness of visual content classifiers as a source of additional information for caption generation. With experimental results we show that utilizing keyframe based features, dense trajectory video features and content classifier outputs together gives better performance than any one of them individually.



### Get More With Less: Near Real-Time Image Clustering on Mobile Phones
- **Arxiv ID**: http://arxiv.org/abs/1512.02972v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1512.02972v1)
- **Published**: 2015-12-09 18:08:59+00:00
- **Updated**: 2015-12-09 18:08:59+00:00
- **Authors**: Jorge Ortiz, Chien-Chin Huang, Supriyo Chakraborty
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning algorithms, in conjunction with user data, hold the promise of revolutionizing the way we interact with our phones, and indeed their widespread adoption in the design of apps bear testimony to this promise. However, currently, the computationally expensive segments of the learning pipeline, such as feature extraction and model training, are offloaded to the cloud, resulting in an over-reliance on the network and under-utilization of computing resources available on mobile platforms. In this paper, we show that by combining the computing power distributed over a number of phones, judicious optimization choices, and contextual information it is possible to execute the end-to-end pipeline entirely on the phones at the edge of the network, efficiently. We also show that by harnessing the power of this combination, it is possible to execute a computationally expensive pipeline at near real-time.   To demonstrate our approach, we implement an end-to-end image-processing pipeline -- that includes feature extraction, vocabulary learning, vectorization, and image clustering -- on a set of mobile phones. Our results show a 75% improvement over the standard, full pipeline implementation running on the phones without modification -- reducing the time to one minute under certain conditions. We believe that this result is a promising indication that fully distributed, infrastructure-less computing is possible on networks of mobile phones; enabling a new class of mobile applications that are less reliant on the cloud.



### ShapeNet: An Information-Rich 3D Model Repository
- **Arxiv ID**: http://arxiv.org/abs/1512.03012v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.AI, cs.CG, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1512.03012v1)
- **Published**: 2015-12-09 19:42:48+00:00
- **Updated**: 2015-12-09 19:42:48+00:00
- **Authors**: Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu
- **Comment**: None
- **Journal**: None
- **Summary**: We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.



### Minimally Supervised Feature Selection for Classification (Master's Thesis, University Politehnica of Bucharest)
- **Arxiv ID**: http://arxiv.org/abs/1512.03019v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.03019v1)
- **Published**: 2015-12-09 19:49:29+00:00
- **Updated**: 2015-12-09 19:49:29+00:00
- **Authors**: Alexandra Maria Radu
- **Comment**: None
- **Journal**: None
- **Summary**: In the context of the highly increasing number of features that are available nowadays we design a robust and fast method for feature selection. The method tries to select the most representative features that are independent from each other, but are strong together. We propose an algorithm that requires very limited labeled data (as few as one labeled frame per class) and can accommodate as many unlabeled samples. We also present here the supervised approach from which we started. We compare our two formulations with established methods like AdaBoost, SVM, Lasso, Elastic Net and FoBa and show that our method is much faster and it has constant training time. Moreover, the unsupervised approach outperforms all the methods with which we compared and the difference might be quite prominent. The supervised approach is in most cases better than the other methods, especially when the number of training shots is very limited. All that the algorithm needs is to choose from a pool of positively correlated features. The methods are evaluated on the Youtube-Objects dataset of videos and on MNIST digits dataset, while at training time we also used features obtained on CIFAR10 dataset and others pre-trained on ImageNet dataset. Thereby, we also proved that transfer learning is useful, even though the datasets differ very much: from low-resolution centered images from 10 classes, to high-resolution images with objects from 1000 classes occurring in different regions of the images or to very difficult videos with very high intraclass variance. 7



