# Arxiv Papers in cs.CV on 2015-12-08
### Direct Intrinsics: Learning Albedo-Shading Decomposition by Convolutional Regression
- **Arxiv ID**: http://arxiv.org/abs/1512.02311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.02311v1)
- **Published**: 2015-12-08 03:38:52+00:00
- **Updated**: 2015-12-08 03:38:52+00:00
- **Authors**: Takuya Narihira, Michael Maire, Stella X. Yu
- **Comment**: International Conference on Computer Vision (ICCV), 2015
- **Journal**: None
- **Summary**: We introduce a new approach to intrinsic image decomposition, the task of decomposing a single image into albedo and shading components. Our strategy, which we term direct intrinsics, is to learn a convolutional neural network (CNN) that directly predicts output albedo and shading channels from an input RGB image patch. Direct intrinsics is a departure from classical techniques for intrinsic image decomposition, which typically rely on physically-motivated priors and graph-based inference algorithms.   The large-scale synthetic ground-truth of the MPI Sintel dataset plays a key role in training direct intrinsics. We demonstrate results on both the synthetic images of Sintel and the real images of the classic MIT intrinsic image dataset. On Sintel, direct intrinsics, using only RGB input, outperforms all prior work, including methods that rely on RGB+Depth input. Direct intrinsics also generalizes across modalities; it produces quite reasonable decompositions on the real images of the MIT dataset. Our results indicate that the marriage of CNNs with synthetic training data may be a powerful new technique for tackling classic problems in computer vision.



### SSD: Single Shot MultiBox Detector
- **Arxiv ID**: http://arxiv.org/abs/1512.02325v5
- **DOI**: 10.1007/978-3-319-46448-0_2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.02325v5)
- **Published**: 2015-12-08 04:46:38+00:00
- **Updated**: 2016-12-29 19:05:11+00:00
- **Authors**: Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg
- **Comment**: ECCV 2016
- **Journal**: None
- **Summary**: We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .



### Learning to Point and Count
- **Arxiv ID**: http://arxiv.org/abs/1512.02326v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.02326v1)
- **Published**: 2015-12-08 04:48:52+00:00
- **Updated**: 2015-12-08 04:48:52+00:00
- **Authors**: Jie Shao, Dequan Wang, Xiangyang Xue, Zheng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes the problem of point-and-count as a test case to break the what-and-where deadlock. Different from the traditional detection problem, the goal is to discover key salient points as a way to localize and count the number of objects simultaneously. We propose two alternatives, one that counts first and then point, and another that works the other way around. Fundamentally, they pivot around whether we solve "what" or "where" first. We evaluate their performance on dataset that contains multiple instances of the same class, demonstrating the potentials and their synergies. The experiences derive a few important insights that explains why this is a much harder problem than classification, including strong data bias and the inability to deal with object scales robustly in state-of-art convolutional neural networks.



### Computational Models for Multiview Dense Depth Maps of Dynamic Scene
- **Arxiv ID**: http://arxiv.org/abs/1512.02329v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.02329v2)
- **Published**: 2015-12-08 05:04:57+00:00
- **Updated**: 2015-12-15 06:18:35+00:00
- **Authors**: Qifei Wang
- **Comment**: 4 pages, IEEE COMSOC MMTC E-Letter 2015
- **Journal**: None
- **Summary**: This paper reviews the recent progresses of the depth map generation for dynamic scene and its corresponding computational models. This paper mainly covers the homogeneous ambiguity models in depth sensing, resolution models in depth processing, and consistency models in depth optimization. We also summarize the future work in the depth map generation.



### Is Hamming distance the only way for matching binary image feature descriptors?
- **Arxiv ID**: http://arxiv.org/abs/1512.02355v1
- **DOI**: 10.1049/el.2014.0773
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.02355v1)
- **Published**: 2015-12-08 06:48:39+00:00
- **Updated**: 2015-12-08 06:48:39+00:00
- **Authors**: Erkan Bostanci
- **Comment**: 2 pages, journal
- **Journal**: Electronics Letters (2014), vol. 50, iss. 11, pp. 806-808
- **Summary**: Brute force matching of binary image feature descriptors is conventionally performed using the Hamming distance. This paper assesses the use of alternative metrics in order to see whether they can produce feature correspondences that yield more accurate homography matrices. Two statistical tests, namely ANOVA (Analysis of Variance) and McNemar's test were employed for evaluation. Results show that Jackard-Needham and Dice metrics can display better performance for some descriptors. Yet, these performance differences were not found to be statistically significant.



### Towards the Application of Linear Programming Methods For Multi-Camera Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1512.02357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.02357v1)
- **Published**: 2015-12-08 07:25:29+00:00
- **Updated**: 2015-12-08 07:25:29+00:00
- **Authors**: Masoud Aghamohamadian-Sharbaf, Ahmadreza Heravi, Hamidreza Pourreza
- **Comment**: None
- **Journal**: None
- **Summary**: We presented a separation based optimization algorithm which, rather than optimization the entire variables altogether, This would allow us to employ: 1) a class of nonlinear functions with three variables and 2) a convex quadratic multivariable polynomial, for minimization of reprojection error. Neglecting the inversion required to minimize the nonlinear functions, in this paper we demonstrate how separation allows eradication of matrix inversion.



### Tracking Objects with Higher Order Interactions using Delayed Column Generation
- **Arxiv ID**: http://arxiv.org/abs/1512.02413v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.02413v3)
- **Published**: 2015-12-08 11:41:30+00:00
- **Updated**: 2016-08-09 05:44:51+00:00
- **Authors**: Shaofei Wang, Steffen Wolf, Charless Fowlkes, Julian Yarkony
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of multi-target tracking and data association in video. We formulate this in terms of selecting a subset of high-quality tracks subject to the constraint that no pair of selected tracks is associated with a common detection (of an object). This objective is equivalent to the classic NP-hard problem of finding a maximum-weight set packing (MWSP) where tracks correspond to sets and is made further difficult since the number of candidate tracks grows exponentially in the number of detections. We present a relaxation of this combinatorial problem that uses a column generation formulation where the pricing problem is solved via dynamic programming to efficiently explore the space of tracks. We employ row generation to tighten the bound in such a way as to preserve efficient inference in the pricing problem. We show the practical utility of this algorithm for tracking problems in natural and biological video datasets.



### Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views
- **Arxiv ID**: http://arxiv.org/abs/1512.02497v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1512.02497v2)
- **Published**: 2015-12-08 15:04:46+00:00
- **Updated**: 2016-04-18 13:14:22+00:00
- **Authors**: Francisco Massa, Bryan Russell, Mathieu Aubry
- **Comment**: To appear in CVPR 2016
- **Journal**: None
- **Summary**: This paper presents an end-to-end convolutional neural network (CNN) for 2D-3D exemplar detection. We demonstrate that the ability to adapt the features of natural images to better align with those of CAD rendered views is critical to the success of our technique. We show that the adaptation can be learned by compositing rendered views of textured object models on natural images. Our approach can be naturally incorporated into a CNN detection pipeline and extends the accuracy and speed benefits from recent advances in deep learning to 2D-3D exemplar detection. We applied our method to two tasks: instance detection, where we evaluated on the IKEA dataset, and object category detection, where we out-perform Aubry et al. for "chair" detection on a subset of the Pascal VOC dataset.



### Fine-grained Image Classification by Exploring Bipartite-Graph Labels
- **Arxiv ID**: http://arxiv.org/abs/1512.02665v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.02665v2)
- **Published**: 2015-12-08 21:18:35+00:00
- **Updated**: 2015-12-10 18:49:54+00:00
- **Authors**: Feng Zhou, Yuanqing Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Given a food image, can a fine-grained object recognition engine tell "which restaurant which dish" the food belongs to? Such ultra-fine grained image recognition is the key for many applications like search by images, but it is very challenging because it needs to discern subtle difference between classes while dealing with the scarcity of training data. Fortunately, the ultra-fine granularity naturally brings rich relationships among object classes. This paper proposes a novel approach to exploit the rich relationships through bipartite-graph labels (BGL). We show how to model BGL in an overall convolutional neural networks and the resulting system can be optimized through back-propagation. We also show that it is computationally efficient in inference thanks to the bipartite structure. To facilitate the study, we construct a new food benchmark dataset, which consists of 37,885 food images collected from 6 restaurants and totally 975 menus. Experimental results on this new food and three other datasets demonstrates BGL advances previous works in fine-grained object recognition. An online demo is available at http://www.f-zhou.com/fg_demo/.



