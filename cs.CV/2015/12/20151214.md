# Arxiv Papers in cs.CV on 2015-12-14
### Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1512.04143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.04143v1)
- **Published**: 2015-12-14 00:37:31+00:00
- **Updated**: 2015-12-14 00:37:31+00:00
- **Authors**: Sean Bell, C. Lawrence Zitnick, Kavita Bala, Ross Girshick
- **Comment**: None
- **Journal**: None
- **Summary**: It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9% to 76.4% mAP. On the new and more challenging MS COCO dataset, we improve state-of-art-the from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection Challenge, our ION model won the Best Student Entry and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.



### Learning Deep Features for Discriminative Localization
- **Arxiv ID**: http://arxiv.org/abs/1512.04150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.04150v1)
- **Published**: 2015-12-14 01:32:33+00:00
- **Updated**: 2015-12-14 01:32:33+00:00
- **Authors**: Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them



### Understanding Human-Centric Images: From Geometry to Fashion
- **Arxiv ID**: http://arxiv.org/abs/1604.08164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.08164v1)
- **Published**: 2015-12-14 03:15:14+00:00
- **Updated**: 2015-12-14 03:15:14+00:00
- **Authors**: Edgar Simo-Serra
- **Comment**: PhD Thesis, May 2015. BarcelonaTech. 169 pages
- **Journal**: None
- **Summary**: Understanding humans from photographs has always been a fundamental goal of computer vision. In this thesis we have developed a hierarchy of tools that cover a wide range of topics with the objective of understanding humans from monocular RGB image: from low level feature point descriptors to high level fashion-aware conditional random fields models. In order to build these high level models it is paramount to have a battery of robust and reliable low and mid level cues. Along these lines, we have proposed two low-level keypoint descriptors: one based on the theory of the heat diffusion on images, and the other that uses a convolutional neural network to learn discriminative image patch representations. We also introduce distinct low-level generative models for representing human pose: in particular we present a discrete model based on a directed acyclic graph and a continuous model that consists of poses clustered on a Riemannian manifold. As mid level cues we propose two 3D human pose estimation algorithms: one that estimates the 3D pose given a noisy 2D estimation, and an approach that simultaneously estimates both the 2D and 3D pose. Finally, we formulate higher level models built upon low and mid level cues for understanding humans from single images. Concretely, we focus on two different tasks in the context of fashion: semantic segmentation of clothing, and predicting the fashionability from images with metadata to ultimately provide fashion advice to the user. For all presented approaches we present extensive results and comparisons against the state-of-the-art and show significant improvements on the entire variety of tasks we tackle.



### Compressed Dynamic Mode Decomposition for Background Modeling
- **Arxiv ID**: http://arxiv.org/abs/1512.04205v2
- **DOI**: 10.1007/s11554-016-0655-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.04205v2)
- **Published**: 2015-12-14 07:27:07+00:00
- **Updated**: 2016-08-25 03:09:25+00:00
- **Authors**: N. Benjamin Erichson, Steven L. Brunton, J. Nathan Kutz
- **Comment**: Preprint submitted to Journal of Real-Time Image Processing
- **Journal**: None
- **Summary**: We introduce the method of compressed dynamic mode decomposition (cDMD) for background modeling. The dynamic mode decomposition (DMD) is a regression technique that integrates two of the leading data analysis methods in use today: Fourier transforms and singular value decomposition. Borrowing ideas from compressed sensing and matrix sketching, cDMD eases the computational workload of high resolution video processing. The key principal of cDMD is to obtain the decomposition on a (small) compressed matrix representation of the video feed. Hence, the cDMD algorithm scales with the intrinsic rank of the matrix, rather then the size of the actual video (data) matrix. Selection of the optimal modes characterizing the background is formulated as a sparsity-constrained sparse coding problem. Our results show, that the quality of the resulting background model is competitive, quantified by the F-measure, Recall and Precision. A GPU (graphics processing unit) accelerated implementation is also presented which further boosts the computational efficiency of the algorithm.



### Watch-Bot: Unsupervised Learning for Reminding Humans of Forgotten Actions
- **Arxiv ID**: http://arxiv.org/abs/1512.04208v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1512.04208v1)
- **Published**: 2015-12-14 07:50:22+00:00
- **Updated**: 2015-12-14 07:50:22+00:00
- **Authors**: Chenxia Wu, Jiemi Zhang, Bart Selman, Silvio Savarese, Ashutosh Saxena
- **Comment**: None
- **Journal**: None
- **Summary**: We present a robotic system that watches a human using a Kinect v2 RGB-D sensor, detects what he forgot to do while performing an activity, and if necessary reminds the person using a laser pointer to point out the related object. Our simple setup can be easily deployed on any assistive robot.   Our approach is based on a learning algorithm trained in a purely unsupervised setting, which does not require any human annotations. This makes our approach scalable and applicable to variant scenarios. Our model learns the action/object co-occurrence and action temporal relations in the activity, and uses the learned rich relationships to infer the forgotten action and the related object. We show that our approach not only improves the unsupervised action segmentation and action cluster assignment performance, but also effectively detects the forgotten actions on a challenging human activity RGB-D video dataset. In robotic experiments, we show that our robot is able to remind people of forgotten actions successfully.



### On the Relation between two Rotation Metrics
- **Arxiv ID**: http://arxiv.org/abs/1512.04219v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.04219v1)
- **Published**: 2015-12-14 08:50:44+00:00
- **Updated**: 2015-12-14 08:50:44+00:00
- **Authors**: Thomas Ruland
- **Comment**: None
- **Journal**: None
- **Summary**: In their work "Global Optimization through Rotation Space Search", Richard Hartley and Fredrik Kahl introduce a global optimization strategy for problems in geometric computer vision, based on rotation space search using a branch-and-bound algorithm. In its core, Lemma 2 of their publication is the important foundation for a class of global optimization algorithms, which is adopted over a wide range of problems in subsequent publications. This lemma relates a metric on rotations represented by rotation matrices with a metric on rotations in axis-angle representation. This work focuses on a proof for this relationship, which is based on Rodrigues' Rotation Theorem for the composition of rotations in axis-angle representation.



### Origami: A 803 GOp/s/W Convolutional Network Accelerator
- **Arxiv ID**: http://arxiv.org/abs/1512.04295v2
- **DOI**: 10.1109/TCSVT.2016.2592330
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, B.7.1; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1512.04295v2)
- **Published**: 2015-12-14 13:06:43+00:00
- **Updated**: 2016-01-19 22:56:41+00:00
- **Authors**: Lukas Cavigelli, Luca Benini
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: An ever increasing number of computer vision and image/video processing challenges are being approached using deep convolutional neural networks, obtaining state-of-the-art results in object recognition and detection, semantic segmentation, action recognition, optical flow and superresolution. Hardware acceleration of these algorithms is essential to adopt these improvements in embedded and mobile computer vision systems. We present a new architecture, design and implementation as well as the first reported silicon measurements of such an accelerator, outperforming previous work in terms of power-, area- and I/O-efficiency. The manufactured device provides up to 196 GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power efficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it the first architecture scalable to TOp/s performance.



### We Are Humor Beings: Understanding and Predicting Visual Humor
- **Arxiv ID**: http://arxiv.org/abs/1512.04407v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1512.04407v4)
- **Published**: 2015-12-14 16:59:35+00:00
- **Updated**: 2016-05-05 21:36:13+00:00
- **Authors**: Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh
- **Comment**: 17 pages, 16 figures, 3 tables
- **Journal**: None
- **Summary**: Humor is an integral part of human lives. Despite being tremendously impactful, it is perhaps surprising that we do not have a detailed understanding of humor yet. As interactions between humans and AI systems increase, it is imperative that these systems are taught to understand subtleties of human expressions such as humor. In this work, we are interested in the question - what content in a scene causes it to be funny? As a first step towards understanding visual humor, we analyze the humor manifested in abstract scenes and design computational models for them. We collect two datasets of abstract scenes that facilitate the study of humor at both the scene-level and the object-level. We analyze the funny scenes and explore the different types of humor depicted in them via human studies. We model two tasks that we believe demonstrate an understanding of some aspects of visual humor. The tasks involve predicting the funniness of a scene and altering the funniness of a scene. We show that our models perform well quantitatively, and qualitatively through human studies. Our datasets are publicly available.



### Instance-aware Semantic Segmentation via Multi-task Network Cascades
- **Arxiv ID**: http://arxiv.org/abs/1512.04412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.04412v1)
- **Published**: 2015-12-14 17:17:23+00:00
- **Updated**: 2015-12-14 17:17:23+00:00
- **Authors**: Jifeng Dai, Kaiming He, Jian Sun
- **Comment**: Tech report. 1st-place winner of MS COCO 2015 segmentation
  competition
- **Journal**: None
- **Summary**: Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multi-task Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems.   The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place.



### Sparse Representation of a Blur Kernel for Blind Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1512.04418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.04418v1)
- **Published**: 2015-12-14 17:32:41+00:00
- **Updated**: 2015-12-14 17:32:41+00:00
- **Authors**: Chia-Chen Lee, Wen-Liang Hwang
- **Comment**: 11 pages, 37 figures
- **Journal**: None
- **Summary**: Blind image restoration is a non-convex problem which involves restoration of images from an unknown blur kernel. The factors affecting the performance of this restoration are how much prior information about an image and a blur kernel are provided and what algorithm is used to perform the restoration task. Prior information on images is often employed to restore the sharpness of the edges of an image. By contrast, no consensus is still present regarding what prior information to use in restoring from a blur kernel due to complex image blurring processes. In this paper, we propose modelling of a blur kernel as a sparse linear combinations of basic 2-D patterns. Our approach has a competitive edge over the existing blur kernel modelling methods because our method has the flexibility to customize the dictionary design, which makes it well-adaptive to a variety of applications. As a demonstration, we construct a dictionary formed by basic patterns derived from the Kronecker product of Gaussian sequences. We also compare our results with those derived by other state-of-the-art methods, in terms of peak signal to noise ratio (PSNR).



### On non-iterative training of a neural classifier
- **Arxiv ID**: http://arxiv.org/abs/1512.04509v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, 62M45
- **Links**: [PDF](http://arxiv.org/pdf/1512.04509v2)
- **Published**: 2015-12-14 20:44:12+00:00
- **Updated**: 2015-12-20 04:32:01+00:00
- **Authors**: K. Eswaran, K. Damodhar Rao
- **Comment**: 18 pages, 5 figures
- **Journal**: None
- **Summary**: Recently an algorithm, was discovered, which separates points in n-dimension by planes in such a manner that no two points are left un-separated by at least one plane{[}1-3{]}. By using this new algorithm we show that there are two ways of classification by a neural network, for a large dimension feature space, both of which are non-iterative and deterministic. To demonstrate the power of both these methods we apply them exhaustively to the classical pattern recognition problem: The Fisher-Anderson's, IRIS flower data set and present the results.   It is expected these methods will now be widely used for the training of neural networks for Deep Learning not only because of their non-iterative and deterministic nature but also because of their efficiency and speed and will supersede other classification methods which are iterative in nature and rely on error minimization.



### Semantic-enriched Visual Vocabulary Construction in a Weakly Supervised Context
- **Arxiv ID**: http://arxiv.org/abs/1512.04605v1
- **DOI**: 10.3233/IDA-140702
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.04605v1)
- **Published**: 2015-12-14 23:45:34+00:00
- **Updated**: 2015-12-14 23:45:34+00:00
- **Authors**: Marian-Andrei Rizoiu, Julien Velcin, Stéphane Lallich
- **Comment**: None
- **Journal**: M.-A. Rizoiu, J. Velcin, and S. Lallich, "Semantic-enriched Visual
  Vocabulary Construction in a Weakly Supervised Context," Intelligent Data
  Analysis, vol. 19, iss. 1, pp. 161-185, 2015
- **Summary**: One of the prevalent learning tasks involving images is content-based image classification. This is a difficult task especially because the low-level features used to digitally describe images usually capture little information about the semantics of the images. In this paper, we tackle this difficulty by enriching the semantic content of the image representation by using external knowledge. The underlying hypothesis of our work is that creating a more semantically rich representation for images would yield higher machine learning performances, without the need to modify the learning algorithms themselves. The external semantic information is presented under the form of non-positional image labels, therefore positioning our work in a weakly supervised context. Two approaches are proposed: the first one leverages the labels into the visual vocabulary construction algorithm, the result being dedicated visual vocabularies. The second approach adds a filtering phase as a pre-processing of the vocabulary construction. Known positive and known negative sets are constructed and features that are unlikely to be associated with the objects denoted by the labels are filtered. We apply our proposition to the task of content-based image classification and we show that semantically enriching the image representation yields higher classification performances than the baseline representation.



