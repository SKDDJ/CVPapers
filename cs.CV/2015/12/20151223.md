# Arxiv Papers in cs.CV on 2015-12-23
### Mid-level Representation for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1512.07314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.07314v1)
- **Published**: 2015-12-23 00:45:41+00:00
- **Updated**: 2015-12-23 00:45:41+00:00
- **Authors**: Moin Nabi
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Recognition is one of the fundamental challenges in AI, where the goal is to understand the semantics of visual data. Employing mid-level representation, in particular, shifted the paradigm in visual recognition. The mid-level image/video representation involves discovering and training a set of mid-level visual patterns (e.g., parts and attributes) and represent a given image/video utilizing them. The mid-level patterns can be extracted from images and videos using the motion and appearance information of visual phenomenas. This thesis targets employing mid-level representations for different high-level visual recognition tasks, namely (i)image understanding and (ii)video understanding.   In the case of image understanding, we focus on object detection/recognition task. We investigate on discovering and learning a set of mid-level patches to be used for representing the images of an object category. We specifically employ the discriminative patches in a subcategory-aware webly-supervised fashion. We, additionally, study the outcomes provided by employing the subcategory-based models for undoing dataset bias.



### Plug-and-Play Priors for Bright Field Electron Tomography and Sparse Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1512.07331v1
- **DOI**: 10.1109/TCI.2016.2599778
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1512.07331v1)
- **Published**: 2015-12-23 02:06:29+00:00
- **Updated**: 2015-12-23 02:06:29+00:00
- **Authors**: Suhas Sreehari, S. V. Venkatakrishnan, Brendt Wohlberg, Lawrence F. Drummy, Jeffrey P. Simmons, Charles A. Bouman
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: Many material and biological samples in scientific imaging are characterized by non-local repeating structures. These are studied using scanning electron microscopy and electron tomography. Sparse sampling of individual pixels in a 2D image acquisition geometry, or sparse sampling of projection images with large tilt increments in a tomography experiment, can enable high speed data acquisition and minimize sample damage caused by the electron beam.   In this paper, we present an algorithm for electron tomographic reconstruction and sparse image interpolation that exploits the non-local redundancy in images. We adapt a framework, termed plug-and-play (P&P) priors, to solve these imaging problems in a regularized inversion setting. The power of the P&P approach is that it allows a wide array of modern denoising algorithms to be used as a "prior model" for tomography and image interpolation. We also present sufficient mathematical conditions that ensure convergence of the P&P approach, and we use these insights to design a new non-local means denoising algorithm. Finally, we demonstrate that the algorithm produces higher quality reconstructions on both simulated and real electron microscope data, along with improved convergence properties compared to other methods.



### A Deep Generative Deconvolutional Image Model
- **Arxiv ID**: http://arxiv.org/abs/1512.07344v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1512.07344v1)
- **Published**: 2015-12-23 03:10:29+00:00
- **Updated**: 2015-12-23 03:10:29+00:00
- **Authors**: Yunchen Pu, Xin Yuan, Andrew Stevens, Chunyuan Li, Lawrence Carin
- **Comment**: 10 pages, 7 figures. Appearing in Proceedings of the 19th
  International Conference on Artificial Intelligence and Statistics (AISTATS)
  2016, Cadiz, Spain. JMLR: W&CP volume 41
- **Journal**: None
- **Summary**: A deep generative model is developed for representation and analysis of images, based on a hierarchical convolutional dictionary-learning framework. Stochastic {\em unpooling} is employed to link consecutive layers in the model, yielding top-down image generation. A Bayesian support vector machine is linked to the top-layer features, yielding max-margin discrimination. Deep deconvolutional inference is employed when testing, to infer the latent features, and the top-layer features are connected with the max-margin classifier for discrimination tasks. The model is efficiently trained using a Monte Carlo expectation-maximization (MCEM) algorithm, with implementation on graphical processor units (GPUs) for efficient large-scale learning, and fast testing. Excellent results are obtained on several benchmark datasets, including ImageNet, demonstrating that the proposed model achieves results that are highly competitive with similarly sized convolutional neural networks.



### Convolutional Architecture Exploration for Action Recognition and Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1512.07502v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.07502v1)
- **Published**: 2015-12-23 14:54:43+00:00
- **Updated**: 2015-12-23 14:54:43+00:00
- **Authors**: J. T. Turner, David Aha, Leslie Smith, Kalyan Moy Gupta
- **Comment**: 12 pages. 11 tables. 0 Images. Written Summer 2014
- **Journal**: None
- **Summary**: Convolutional Architecture for Fast Feature Encoding (CAFFE) [11] is a software package for the training, classifying, and feature extraction of images. The UCF Sports Action dataset is a widely used machine learning dataset that has 200 videos taken in 720x480 resolution of 9 different sporting activities: diving, golf, swinging, kicking, lifting, horseback riding, running, skateboarding, swinging (various gymnastics), and walking. In this report we report on a caffe feature extraction pipeline of images taken from the videos of the UCF Sports Action dataset. A similar test was performed on overfeat, and results were inferior to caffe. This study is intended to explore the architecture and hyper parameters needed for effective static analysis of action in videos and classification over a variety of image datasets.



### Recovering 6D Object Pose and Predicting Next-Best-View in the Crowd
- **Arxiv ID**: http://arxiv.org/abs/1512.07506v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.07506v2)
- **Published**: 2015-12-23 15:06:05+00:00
- **Updated**: 2016-04-19 17:31:56+00:00
- **Authors**: Andreas Doumanoglou, Rigas Kouskouridas, Sotiris Malassiotis, Tae-Kyun Kim
- **Comment**: CVPR 2016 accepted paper, project page:
  http://www.iis.ee.ic.ac.uk/rkouskou/6D_NBV.html
- **Journal**: None
- **Summary**: Object detection and 6D pose estimation in the crowd (scenes with multiple object instances, severe foreground occlusions and background distractors), has become an important problem in many rapidly evolving technological areas such as robotics and augmented reality. Single shot-based 6D pose estimators with manually designed features are still unable to tackle the above challenges, motivating the research towards unsupervised feature learning and next-best-view estimation. In this work, we present a complete framework for both single shot-based 6D object pose estimation and next-best-view prediction based on Hough Forests, the state of the art object pose estimator that performs classification and regression jointly. Rather than using manually designed features we a) propose an unsupervised feature learnt from depth-invariant patches using a Sparse Autoencoder and b) offer an extensive evaluation of various state of the art features. Furthermore, taking advantage of the clustering performed in the leaf nodes of Hough Forests, we learn to estimate the reduction of uncertainty in other views, formulating the problem of selecting the next-best-view. To further improve pose estimation, we propose an improved joint registration and hypotheses verification module as a final refinement step to reject false detections. We provide two additional challenging datasets inspired from realistic scenarios to extensively evaluate the state of the art and our framework. One is related to domestic environments and the other depicts a bin-picking scenario mostly found in industrial settings. We show that our framework significantly outperforms state of the art both on public and on our datasets.



### A Latent-Variable Lattice Model
- **Arxiv ID**: http://arxiv.org/abs/1512.07587v7
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1512.07587v7)
- **Published**: 2015-12-23 19:01:03+00:00
- **Updated**: 2016-06-08 03:25:09+00:00
- **Authors**: Rajasekaran Masatran
- **Comment**: 6 pages, with 4 figures, 8 algorithms, and 1 table
- **Journal**: None
- **Summary**: Markov random field (MRF) learning is intractable, and its approximation algorithms are computationally expensive. We target a small subset of MRF that is used frequently in computer vision. We characterize this subset with three concepts: Lattice, Homogeneity, and Inertia; and design a non-markov model as an alternative. Our goal is robust learning from small datasets. Our learning algorithm uses vector quantization and, at time complexity O(U log U) for a dataset of U pixels, is much faster than that of general-purpose MRF.



