# Arxiv Papers in cs.CV on 2015-12-13
### Action Recognition with Image Based CNN Features
- **Arxiv ID**: http://arxiv.org/abs/1512.03980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.03980v1)
- **Published**: 2015-12-13 00:17:24+00:00
- **Updated**: 2015-12-13 00:17:24+00:00
- **Authors**: Mahdyar Ravanbakhsh, Hossein Mousavi, Mohammad Rastegari, Vittorio Murino, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: Most of human actions consist of complex temporal compositions of more simple actions. Action recognition tasks usually relies on complex handcrafted structures as features to represent the human action model. Convolutional Neural Nets (CNN) have shown to be a powerful tool that eliminate the need for designing handcrafted features. Usually, the output of the last layer in CNN (a layer before the classification layer -known as fc7) is used as a generic feature for images. In this paper, we show that fc7 features, per se, can not get a good performance for the task of action recognition, when the network is trained only on images. We present a feature structure on top of fc7 features, which can capture the temporal variation in a video. To represent the temporal components, which is needed to capture motion information, we introduced a hierarchical structure. The hierarchical model enables to capture sub-actions from a complex action. At the higher levels of the hierarchy, it represents a coarse capture of action sequence and lower levels represent fine action elements. Furthermore, we introduce a method for extracting key-frames using binary coding of each frame in a video, which helps to improve the performance of our hierarchical model. We experimented our method on several action datasets and show that our method achieves superior results compared to other state-of-the-arts methods.



### Deep Tracking: Visual Tracking Using Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1512.03993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.03993v1)
- **Published**: 2015-12-13 02:58:56+00:00
- **Updated**: 2015-12-13 02:58:56+00:00
- **Authors**: Meera Hahn, Si Chen, Afshin Dehghan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study a discriminatively trained deep convolutional network for the task of visual tracking. Our tracker utilizes both motion and appearance features that are extracted from a pre-trained dual stream deep convolution network. We show that the features extracted from our dual-stream network can provide rich information about the target and this leads to competitive performance against state of the art tracking methods on a visual tracking benchmark.



### Cross-dimensional Weighting for Aggregated Deep Convolutional Features
- **Arxiv ID**: http://arxiv.org/abs/1512.04065v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.04065v2)
- **Published**: 2015-12-13 15:16:02+00:00
- **Updated**: 2016-07-30 02:14:18+00:00
- **Authors**: Yannis Kalantidis, Clayton Mellina, Simon Osindero
- **Comment**: Accepted for publications at the 4th Workshop on Web-scale Vision and
  Social Media (VSM), ECCV 2016
- **Journal**: None
- **Summary**: We propose a simple and straightforward way of creating powerful image representations via cross-dimensional weighting and aggregation of deep convolutional neural network layer outputs. We first present a generalized framework that encompasses a broad family of approaches and includes cross-dimensional pooling and weighting steps. We then propose specific non-parametric schemes for both spatial- and channel-wise weighting that boost the effect of highly active spatial responses and at the same time regulate burstiness effects. We experiment on different public datasets for image search and show that our approach outperforms the current state-of-the-art for approaches based on pre-trained networks. We also provide an easy-to-use, open source implementation that reproduces our results.



### Learning the Correction for Multi-Path Deviations in Time-of-Flight Cameras
- **Arxiv ID**: http://arxiv.org/abs/1512.04077v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.04077v2)
- **Published**: 2015-12-13 16:31:14+00:00
- **Updated**: 2016-01-12 11:17:58+00:00
- **Authors**: Mojmir Mutny, Rahul Nair, Jens-Malte Gottfried
- **Comment**: None
- **Journal**: None
- **Summary**: The Multipath effect in Time-of-Flight(ToF) cameras still remains to be a challenging problem that hinders further processing of 3D data information. Based on the evidence from previous literature, we explored the possibility of using machine learning techniques to correct this effect. Firstly, we created two new datasets of of ToF images rendered via ToF simulator of LuxRender. These two datasets contain corners in multiple orientations and with different material properties. We chose scenes with corners as multipath effects are most pronounced in corners. Secondly, we used this dataset to construct a learning model to predict real valued corrections to the ToF data using Random Forests. We found out that in our smaller dataset we were able to predict real valued correction and improve the quality of depth images significantly by removing multipath bias. With our algorithm, we improved relative per-pixel error from average value of 19% to 3%. Additionally, variance of the error was lowered by an order of magnitude.



### Deep Learning-Based Image Kernel for Inductive Transfer
- **Arxiv ID**: http://arxiv.org/abs/1512.04086v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.04086v3)
- **Published**: 2015-12-13 17:12:45+00:00
- **Updated**: 2016-02-16 09:51:27+00:00
- **Authors**: Neeraj Kumar, Animesh Karmakar, Ranti Dev Sharma, Abhinav Mittal, Amit Sethi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method to classify images from target classes with a small number of training examples based on transfer learning from non-target classes. Without using any more information than class labels for samples from non-target classes, we train a Siamese net to estimate the probability of two images to belong to the same class. With some post-processing, output of the Siamese net can be used to form a gram matrix of a Mercer kernel. Coupled with a support vector machine (SVM), such a kernel gave reasonable classification accuracy on target classes without any fine-tuning. When the Siamese net was only partially fine-tuned using a small number of samples from the target classes, the resulting classifier outperformed the state-of-the-art and other alternatives. We share class separation capabilities and insights into the learning process of such a kernel on MNIST, Dogs vs. Cats, and CIFAR-10 datasets.



### Deep Relative Attributes
- **Arxiv ID**: http://arxiv.org/abs/1512.04103v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.04103v2)
- **Published**: 2015-12-13 19:10:16+00:00
- **Updated**: 2016-09-13 08:21:43+00:00
- **Authors**: Yaser Souri, Erfan Noury, Ehsan Adeli
- **Comment**: ACCV 2016
- **Journal**: None
- **Summary**: Visual attributes are great means of describing images or scenes, in a way both humans and computers understand. In order to establish a correspondence between images and to be able to compare the strength of each property between images, relative attributes were introduced. However, since their introduction, hand-crafted and engineered features were used to learn increasingly complex models for the problem of relative attributes. This limits the applicability of those methods for more realistic cases. We introduce a deep neural network architecture for the task of relative attribute prediction. A convolutional neural network (ConvNet) is adopted to learn the features by including an additional layer (ranking layer) that learns to rank the images based on these features. We adopt an appropriate ranking loss to train the whole network in an end-to-end fashion. Our proposed method outperforms the baseline and state-of-the-art methods in relative attribute prediction on various coarse and fine-grained datasets. Our qualitative results along with the visualization of the saliency maps show that the network is able to learn effective features for each specific attribute. Source code of the proposed method is available at https://github.com/yassersouri/ghiaseddin.



### Unsupervised Temporal Segmentation of Repetitive Human Actions Based on Kinematic Modeling and Frequency Analysis
- **Arxiv ID**: http://arxiv.org/abs/1512.04115v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.04115v1)
- **Published**: 2015-12-13 20:08:43+00:00
- **Updated**: 2015-12-13 20:08:43+00:00
- **Authors**: Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy
- **Comment**: 9 pages, International Conference on 3D Vision 2015
- **Journal**: None
- **Summary**: In this paper, we propose a method for temporal segmentation of human repetitive actions based on frequency analysis of kinematic parameters, zero-velocity crossing detection, and adaptive k-means clustering. Since the human motion data may be captured with different modalities which have different temporal sampling rate and accuracy (e.g., optical motion capture systems vs. Microsoft Kinect), we first apply a generic full-body kinematic model with an unscented Kalman filter to convert the motion data into a unified representation that is robust to noise. Furthermore, we extract the most representative kinematic parameters via the primary frequency analysis. The sequences are segmented based on zero-velocity crossing of the selected parameters followed by an adaptive k-means clustering to identify the repetition segments. Experimental results demonstrate that for the motion data captured by both the motion capture system and the Microsoft Kinect, our proposed algorithm obtains robust segmentation of repetitive action sequences.



### Articulated Pose Estimation Using Hierarchical Exemplar-Based Models
- **Arxiv ID**: http://arxiv.org/abs/1512.04118v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.04118v1)
- **Published**: 2015-12-13 20:37:10+00:00
- **Updated**: 2015-12-13 20:37:10+00:00
- **Authors**: Jiongxin Liu, Yinxiao Li, Peter Allen, Peter Belhumeur
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Exemplar-based models have achieved great success on localizing the parts of semi-rigid objects. However, their efficacy on highly articulated objects such as humans is yet to be explored. Inspired by hierarchical object representation and recent application of Deep Convolutional Neural Networks (DCNNs) on human pose estimation, we propose a novel formulation that incorporates both hierarchical exemplar-based models and DCNNs in the spatial terms. Specifically, we obtain more expressive spatial models by assuming independence between exemplars at different levels in the hierarchy; we also obtain stronger spatial constraints by inferring the spatial relations between parts at the same level. As our method strikes a good balance between expressiveness and strength of spatial models, it is both effective and generalizable, achieving state-of-the-art results on different benchmarks: Leeds Sports Dataset and CUB-200-2011.



### A Person Re-Identification System For Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1512.04133v1
- **DOI**: 10.1109/SITIS.2015.96
- **Categories**: **cs.CV**, cs.CR, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1512.04133v1)
- **Published**: 2015-12-13 22:33:17+00:00
- **Updated**: 2015-12-13 22:33:17+00:00
- **Authors**: George Cushen
- **Comment**: Appearing in Proceedings of the 11th IEEE/ACM International
  Conference on Signal Image Technology & Internet Systems (SITIS 2015)
- **Journal**: None
- **Summary**: Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.



### Evaluation of Pose Tracking Accuracy in the First and Second Generations of Microsoft Kinect
- **Arxiv ID**: http://arxiv.org/abs/1512.04134v1
- **DOI**: 10.1109/ICHI.2015.54
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1512.04134v1)
- **Published**: 2015-12-13 22:58:55+00:00
- **Updated**: 2015-12-13 22:58:55+00:00
- **Authors**: Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy
- **Comment**: 10 pages, IEEE International Conference on Healthcare Informatics
  2015 (ICHI 2015)
- **Journal**: None
- **Summary**: Microsoft Kinect camera and its skeletal tracking capabilities have been embraced by many researchers and commercial developers in various applications of real-time human movement analysis. In this paper, we evaluate the accuracy of the human kinematic motion data in the first and second generation of the Kinect system, and compare the results with an optical motion capture system. We collected motion data in 12 exercises for 10 different subjects and from three different viewpoints. We report on the accuracy of the joint localization and bone length estimation of Kinect skeletons in comparison to the motion capture. We also analyze the distribution of the joint localization offsets by fitting a mixture of Gaussian and uniform distribution models to determine the outliers in the Kinect motion data. Our analysis shows that overall Kinect 2 has more robust and more accurate tracking of human pose as compared to Kinect 1.



