# Arxiv Papers in cs.CV on 2015-12-10
### Deep Learning Algorithms with Applications to Video Analytics for A Smart City: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1512.03131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.03131v1)
- **Published**: 2015-12-10 03:23:54+00:00
- **Updated**: 2015-12-10 03:23:54+00:00
- **Authors**: Li Wang, Dennis Sng
- **Comment**: 8 pages, 18 figures
- **Journal**: None
- **Summary**: Deep learning has recently achieved very promising results in a wide range of areas such as computer vision, speech recognition and natural language processing. It aims to learn hierarchical representations of data by using deep architecture models. In a smart city, a lot of data (e.g. videos captured from many distributed sensors) need to be automatically processed and analyzed. In this paper, we review the deep learning algorithms applied to video analytics of smart city in terms of different research topics: object detection, object tracking, face recognition, image classification and scene labeling.



### Evaluation of Object Detection Proposals Under Condition Variations
- **Arxiv ID**: http://arxiv.org/abs/1512.03424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.03424v1)
- **Published**: 2015-12-10 06:31:59+00:00
- **Updated**: 2015-12-10 06:31:59+00:00
- **Authors**: Fahimeh Rezazadegan, Sareh Shirazi, Michael Milford, Ben Upcroft
- **Comment**: 2 pages, 6 figures, CVPR Workshop, 2015
- **Journal**: None
- **Summary**: Object detection is a fundamental task in many computer vision applications, therefore the importance of evaluating the quality of object detection is well acknowledged in this domain. This process gives insight into the capabilities of methods in handling environmental changes. In this paper, a new method for object detection is introduced that combines the Selective Search and EdgeBoxes. We tested these three methods under environmental variations. Our experiments demonstrate the outperformance of the combination method under illumination and view point variations.



### Enhanced image feature coverage: Key-point selection using genetic algorithms
- **Arxiv ID**: http://arxiv.org/abs/1512.03155v1
- **DOI**: 10.1080/13682199.2016.1254939
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.03155v1)
- **Published**: 2015-12-10 06:51:28+00:00
- **Updated**: 2015-12-10 06:51:28+00:00
- **Authors**: Erkan Bostanci
- **Comment**: 14 pages, journal
- **Journal**: None
- **Summary**: Coverage of image features play an important role in many vision algorithms since their distribution affect the estimated homography. This paper presents a Genetic Algorithm (GA) in order to select the optimal set of features yielding maximum coverage of the image which is measured by a robust method based on spatial statistics. It is shown with statistical tests on two datasets that the metric yields better coverage and this is also confirmed by an accuracy test on the computed homography for the original set and the newly selected set of features. Results have demonstrated that the new set has similar performance in terms of the accuracy of the computed homography with the original one with an extra benefit of using fewer number of features ultimately reducing the time required for descriptor calculation and matching.



### 3D Reconstruction of Crime Scenes and Design Considerations for an Interactive Investigation Tool
- **Arxiv ID**: http://arxiv.org/abs/1512.03156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.03156v1)
- **Published**: 2015-12-10 07:20:43+00:00
- **Updated**: 2015-12-10 07:20:43+00:00
- **Authors**: Erkan Bostanci
- **Comment**: 9 pages, journal
- **Journal**: International Journal of Information Security Science, Vol 4, No 2
  (2015)
- **Summary**: Crime Scene Investigation (CSI) is a carefully planned systematic process with the purpose of acquiring physical evidences to shed light upon the physical reality of the crime and eventually detect the identity of the criminal. Capturing images and videos of the crime scene is an important part of this process in order to conduct a deeper analysis on the digital evidence for possible hints. This work brings this idea further to use the acquired footage for generating a 3D model of the crime scene. Results show that realistic reconstructions can be obtained using sophisticated computer vision techniques. The paper also discusses a number of important design considerations describing key features that should be present in a powerful interactive CSI analysis tool.



### VRFP: On-the-fly Video Retrieval using Web Images and Fast Fisher Vector Products
- **Arxiv ID**: http://arxiv.org/abs/1512.03384v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.03384v3)
- **Published**: 2015-12-10 19:50:50+00:00
- **Updated**: 2017-04-10 17:28:16+00:00
- **Authors**: Xintong Han, Bharat Singh, Vlad I. Morariu, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: VRFP is a real-time video retrieval framework based on short text input queries, which obtains weakly labeled training images from the web after the query is known. The retrieved web images representing the query and each database video are treated as unordered collections of images, and each collection is represented using a single Fisher Vector built on CNN features. Our experiments show that a Fisher Vector is robust to noise present in web images and compares favorably in terms of accuracy to other standard representations. While a Fisher Vector can be constructed efficiently for a new query, matching against the test set is slow due to its high dimensionality. To perform matching in real-time, we present a lossless algorithm that accelerates the inner product computation between high dimensional Fisher Vectors. We prove that the expected number of multiplications required decreases quadratically with the sparsity of Fisher Vectors. We are not only able to construct and apply query models in real-time, but with the help of a simple re-ranking scheme, we also outperform state-of-the-art automatic retrieval methods by a significant margin on TRECVID MED13 (3.5%), MED14 (1.3%) and CCV datasets (5.2%). We also provide a direct comparison on standard datasets between two different paradigms for automatic video retrieval - zero-shot learning and on-the-fly retrieval.



### Deep Residual Learning for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1512.03385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.03385v1)
- **Published**: 2015-12-10 19:51:55+00:00
- **Updated**: 2015-12-10 19:51:55+00:00
- **Authors**: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.   The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.



### Neural Self Talk: Image Understanding via Continuous Questioning and Answering
- **Arxiv ID**: http://arxiv.org/abs/1512.03460v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.RO, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1512.03460v1)
- **Published**: 2015-12-10 21:58:46+00:00
- **Updated**: 2015-12-10 21:58:46+00:00
- **Authors**: Yezhou Yang, Yi Li, Cornelia Fermuller, Yiannis Aloimonos
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we consider the problem of continuously discovering image contents by actively asking image based questions and subsequently answering the questions being asked. The key components include a Visual Question Generation (VQG) module and a Visual Question Answering module, in which Recurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are used. Given a dataset that contains images, questions and their answers, both modules are trained at the same time, with the difference being VQG uses the images as input and the corresponding questions as output, while VQA uses images and questions as input and the corresponding answers as output. We evaluate the self talk process subjectively using Amazon Mechanical Turk, which show effectiveness of the proposed method.



