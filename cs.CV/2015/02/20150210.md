# Arxiv Papers in cs.CV on 2015-02-10
### Multi-view Face Detection Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1502.02766v3
- **DOI**: None
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1502.02766v3)
- **Published**: 2015-02-10 03:15:21+00:00
- **Updated**: 2015-04-20 20:18:57+00:00
- **Authors**: Sachin Sudhakar Farfade, Mohammad Saberian, Li-Jia Li
- **Comment**: in International Conference on Multimedia Retrieval 2015 (ICMR)
- **Journal**: None
- **Summary**: In this paper we consider the problem of multi-view face detection. While there has been significant research on this problem, current state-of-the-art approaches for this task require annotation of facial landmarks, e.g. TSM [25], or annotation of face poses [28, 22]. They also require training dozens of models to fully capture faces in all orientations, e.g. 22 models in HeadHunter method [22]. In this paper we propose Deep Dense Face Detector (DDFD), a method that does not require pose/landmark annotation and is able to detect faces in a wide range of orientations using a single model based on deep convolutional neural networks. The proposed method has minimal complexity; unlike other recent deep learning object detection methods [9], it does not require additional components such as segmentation, bounding-box regression, or SVM classifiers. Furthermore, we analyzed scores of the proposed face detector for faces in different orientations and found that 1) the proposed method is able to detect faces from different angles and can handle occlusion to some extent, 2) there seems to be a correlation between dis- tribution of positive examples in the training set and scores of the proposed face detector. The latter suggests that the proposed methods performance can be further improved by using better sampling strategies and more sophisticated data augmentation techniques. Evaluations on popular face detection benchmark datasets show that our single-model face detector algorithm has similar or better performance compared to the previous methods, which are more complex and require annotations of either different poses or facial landmarks.



### A HMAX with LLC for visual recognition
- **Arxiv ID**: http://arxiv.org/abs/1502.02772v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.02772v2)
- **Published**: 2015-02-10 04:01:43+00:00
- **Updated**: 2015-02-11 04:40:20+00:00
- **Authors**: Kean Hong Lau, Yong Haur Tay, Fook Loong Lo
- **Comment**: 10 pages, 3 figures, 2 tables, 23 references
- **Journal**: None
- **Summary**: Today's high performance deep artificial neural networks (ANNs) rely heavily on parameter optimization, which is sequential in nature and even with a powerful GPU, would have taken weeks to train them up for solving challenging tasks [22]. HMAX [17] has demonstrated that a simple high performing network could be obtained without heavy optimization. In this paper, we had improved on the existing best HMAX neural network [12] in terms of structural simplicity and performance. Our design replaces the L1 minimization sparse coding (SC) with a locality-constrained linear coding (LLC) [20] which has a lower computational demand. We also put the simple orientation filter bank back into the front layer of the network replacing PCA. Our system's performance has improved over the existing architecture and reached 79.0% on the challenging Caltech-101 [7] dataset, which is state-of-the-art for ANNs (without transfer learning). From our empirical data, the main contributors to our system's performance include an introduction of partial signal whitening, a spot detector, and a spatial pyramid matching (SPM) [14] layer.



### Talk to the Hand: Generating a 3D Print from Photographs
- **Arxiv ID**: http://arxiv.org/abs/1502.02871v1
- **DOI**: None
- **Categories**: **math.HO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1502.02871v1)
- **Published**: 2015-02-10 12:00:35+00:00
- **Updated**: 2015-02-10 12:00:35+00:00
- **Authors**: Edward Aboufadel, Sylvanna V. Krawczyk, Melissa Sherman-Bennett
- **Comment**: 12 pages, 13 figures, final report from the 2013 REU program at Grand
  Valley State University
- **Journal**: None
- **Summary**: This manuscript presents a linear algebra-based technique that only requires two unique photographs from a digital camera to mathematically construct a 3D surface representation which can then be 3D printed. Basic computer vision theory and manufacturing principles are also briefly discussed.



### Real Time Implementation of Spatial Filtering On FPGA
- **Arxiv ID**: http://arxiv.org/abs/1502.02905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.02905v1)
- **Published**: 2015-02-10 13:37:49+00:00
- **Updated**: 2015-02-10 13:37:49+00:00
- **Authors**: Chaitannya Supe
- **Comment**: 8 pages, 7 figures, 1 table
- **Journal**: None
- **Summary**: Field Programmable Gate Array (FPGA) technology has gained vital importance mainly because of its parallel processing hardware which makes it ideal for image and video processing. In this paper, a step by step approach to apply a linear spatial filter on real time video frame sent by Omnivision OV7670 camera using Zynq Evaluation and Development board based on Xilinx XC7Z020 has been discussed. Face detection application was chosen to explain above procedure. This procedure is applicable to most of the complex image processing algorithms which needs to be implemented using FPGA.



### Video Primal Sketch: A Unified Middle-Level Representation for Video
- **Arxiv ID**: http://arxiv.org/abs/1502.02965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.02965v1)
- **Published**: 2015-02-10 16:13:01+00:00
- **Updated**: 2015-02-10 16:13:01+00:00
- **Authors**: Zhi Han, Zongben Xu, Song-Chun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a middle-level video representation named Video Primal Sketch (VPS), which integrates two regimes of models: i) sparse coding model using static or moving primitives to explicitly represent moving corners, lines, feature points, etc., ii) FRAME /MRF model reproducing feature statistics extracted from input video to implicitly represent textured motion, such as water and fire. The feature statistics include histograms of spatio-temporal filters and velocity distributions. This paper makes three contributions to the literature: i) Learning a dictionary of video primitives using parametric generative models; ii) Proposing the Spatio-Temporal FRAME (ST-FRAME) and Motion-Appearance FRAME (MA-FRAME) models for modeling and synthesizing textured motion; and iii) Developing a parsimonious hybrid model for generic video representation. Given an input video, VPS selects the proper models automatically for different motion patterns and is compatible with high-level action representations. In the experiments, we synthesize a number of textured motion; reconstruct real videos using the VPS; report a series of human perception experiments to verify the quality of reconstructed videos; demonstrate how the VPS changes over the scale transition in videos; and present the close connection between VPS and high-level action models.



### Show, Attend and Tell: Neural Image Caption Generation with Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/1502.03044v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1502.03044v3)
- **Published**: 2015-02-10 19:18:29+00:00
- **Updated**: 2016-04-19 16:43:09+00:00
- **Authors**: Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.



### Fast Fusion of Multi-Band Images Based on Solving a Sylvester Equation
- **Arxiv ID**: http://arxiv.org/abs/1502.03121v1
- **DOI**: 10.1109/TIP.2015.2458572
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.03121v1)
- **Published**: 2015-02-10 21:18:54+00:00
- **Updated**: 2015-02-10 21:18:54+00:00
- **Authors**: Qi Wei, Nicolas Dobigeon, Jean-Yves Tourneret
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a fast multi-band image fusion algorithm, which combines a high-spatial low-spectral resolution image and a low-spatial high-spectral resolution image. The well admitted forward model is explored to form the likelihoods of the observations. Maximizing the likelihoods leads to solving a Sylvester equation. By exploiting the properties of the circulant and downsampling matrices associated with the fusion problem, a closed-form solution for the corresponding Sylvester equation is obtained explicitly, getting rid of any iterative update step. Coupled with the alternating direction method of multipliers and the block coordinate descent method, the proposed algorithm can be easily generalized to incorporate prior information for the fusion problem, allowing a Bayesian estimator. Simulation results show that the proposed algorithm achieves the same performance as existing algorithms with the advantage of significantly decreasing the computational complexity of these algorithms.



### Kernel Task-Driven Dictionary Learning for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1502.03126v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1502.03126v1)
- **Published**: 2015-02-10 21:27:27+00:00
- **Updated**: 2015-02-10 21:27:27+00:00
- **Authors**: Soheil Bahrampour, Nasser M. Nasrabadi, Asok Ray, Kenneth W. Jenkins
- **Comment**: 5 pages, IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP), 2015
- **Journal**: None
- **Summary**: Dictionary learning algorithms have been successfully used in both reconstructive and discriminative tasks, where the input signal is represented by a linear combination of a few dictionary atoms. While these methods are usually developed under $\ell_1$ sparsity constrain (prior) in the input domain, recent studies have demonstrated the advantages of sparse representation using structured sparsity priors in the kernel domain. In this paper, we propose a supervised dictionary learning algorithm in the kernel domain for hyperspectral image classification. In the proposed formulation, the dictionary and classifier are obtained jointly for optimal classification performance. The supervised formulation is task-driven and provides learned features from the hyperspectral data that are well suited for the classification task. Moreover, the proposed algorithm uses a joint ($\ell_{12}$) sparsity prior to enforce collaboration among the neighboring pixels. The simulation results illustrate the efficiency of the proposed dictionary learning algorithm.



