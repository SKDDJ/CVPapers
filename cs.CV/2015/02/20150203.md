# Arxiv Papers in cs.CV on 2015-02-03
### Recovery of Piecewise Smooth Images from Few Fourier Samples
- **Arxiv ID**: http://arxiv.org/abs/1502.00705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00705v1)
- **Published**: 2015-02-03 01:48:27+00:00
- **Updated**: 2015-02-03 01:48:27+00:00
- **Authors**: Greg Ongie, Mathews Jacob
- **Comment**: 5 pages. Submitted to SampTA 2015
- **Journal**: None
- **Summary**: We introduce a Prony-like method to recover a continuous domain 2-D piecewise smooth image from few of its Fourier samples. Assuming the discontinuity set of the image is localized to the zero level-set of a trigonometric polynomial, we show the Fourier transform coefficients of partial derivatives of the signal satisfy an annihilation relation. We present necessary and sufficient conditions for unique recovery of piecewise constant images using the above annihilation relation. We pose the recovery of the Fourier coefficients of the signal from the measurements as a convex matrix completion algorithm, which relies on the lifting of the Fourier data to a structured low-rank matrix; this approach jointly estimates the signal and the annihilating filter. Finally, we demonstrate our algorithm on the recovery of MRI phantoms from few low-resolution Fourier samples.



### Deep Boosting: Layered Feature Mining for General Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1502.00712v1
- **DOI**: 10.1109/ICME.2014.6890323
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00712v1)
- **Published**: 2015-02-03 02:44:10+00:00
- **Updated**: 2015-02-03 02:44:10+00:00
- **Authors**: Zhanglin Peng, Liang Lin, Ruimao Zhang, Jing Xu
- **Comment**: 6 pages, 4 figures, ICME 2014
- **Journal**: Multimedia and Expo (ICME), 2014 IEEE International Conference on
  , vol., no., pp.1,6, 14-18 July 2014
- **Summary**: Constructing effective representations is a critical but challenging problem in multimedia understanding. The traditional handcraft features often rely on domain knowledge, limiting the performances of exiting methods. This paper discusses a novel computational architecture for general image feature mining, which assembles the primitive filters (i.e. Gabor wavelets) into compositional features in a layer-wise manner. In each layer, we produce a number of base classifiers (i.e. regression stumps) associated with the generated features, and discover informative compositions by using the boosting algorithm. The output compositional features of each layer are treated as the base components to build up the next layer. Our framework is able to generate expressive image representations while inducing very discriminate functions for image classification. The experiments are conducted on several public datasets, and we demonstrate superior performances over state-of-the-art approaches.



### Beyond Pixels: A Comprehensive Survey from Bottom-up to Semantic Image Segmentation and Cosegmentation
- **Arxiv ID**: http://arxiv.org/abs/1502.00717v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00717v1)
- **Published**: 2015-02-03 03:00:52+00:00
- **Updated**: 2015-02-03 03:00:52+00:00
- **Authors**: Hongyuan Zhu, Fanman Meng, Jianfei Cai, Shijian Lu
- **Comment**: submitted to Elsevier Journal of Visual Communications and Image
  Representation
- **Journal**: None
- **Summary**: Image segmentation refers to the process to divide an image into nonoverlapping meaningful regions according to human perception, which has become a classic topic since the early ages of computer vision. A lot of research has been conducted and has resulted in many applications. However, while many segmentation algorithms exist, yet there are only a few sparse and outdated summarizations available, an overview of the recent achievements and issues is lacking. We aim to provide a comprehensive review of the recent progress in this field. Covering 180 publications, we give an overview of broad areas of segmentation topics including not only the classic bottom-up approaches, but also the recent development in superpixel, interactive methods, object proposals, semantic image parsing and image cosegmentation. In addition, we also review the existing influential datasets and evaluation metrics. Finally, we suggest some design flavors and research directions for future research in image segmentation.



### Learning Contour-Fragment-based Shape Model with And-Or Tree Representation
- **Arxiv ID**: http://arxiv.org/abs/1502.00723v1
- **DOI**: 10.1109/CVPR.2012.6247668
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00723v1)
- **Published**: 2015-02-03 03:42:10+00:00
- **Updated**: 2015-02-03 03:42:10+00:00
- **Authors**: Liang Lin, Xiaolong Wang, Wei Yang, Jianhuang Lai
- **Comment**: 8 pages, 7 figures, CVPR 2012
- **Journal**: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE
  Conference on , vol., no., pp.135,142, 16-21 June 2012
- **Summary**: This paper proposes a simple yet effective method to learn the hierarchical object shape model consisting of local contour fragments, which represents a category of shapes in the form of an And-Or tree. This model extends the traditional hierarchical tree structures by introducing the "switch" variables (i.e. the or-nodes) that explicitly specify production rules to capture shape variations. We thus define the model with three layers: the leaf-nodes for detecting local contour fragments, the or-nodes specifying selection of leaf-nodes, and the root-node encoding the holistic distortion. In the training stage, for optimization of the And-Or tree learning, we extend the concave-convex procedure (CCCP) by embedding the structural clustering during the iterative learning steps. The inference of shape detection is consistent with the model optimization, which integrates the local testings via the leaf-nodes and or-nodes with the global verification via the root-node. The advantages of our approach are validated on the challenging shape databases (i.e., ETHZ and INRIA Horse) and summarized as follows. (1) The proposed method is able to accurately localize shape contours against unreliable edge detection and edge tracing. (2) The And-Or tree model enables us to well capture the intraclass variance.



### Clothing Co-Parsing by Joint Image Segmentation and Labeling
- **Arxiv ID**: http://arxiv.org/abs/1502.00739v1
- **DOI**: 10.1109/CVPR.2014.407
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00739v1)
- **Published**: 2015-02-03 04:59:41+00:00
- **Updated**: 2015-02-03 04:59:41+00:00
- **Authors**: Wei Yang, Ping Luo, Liang Lin
- **Comment**: 8 pages, 5 figures, CVPR 2014
- **Journal**: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE
  Conference on , vol., no., pp.3182,3189, 23-28 June 2014
- **Summary**: This paper aims at developing an integrated system of clothing co-parsing, in order to jointly parse a set of clothing images (unsegmented but annotated with tags) into semantic configurations. We propose a data-driven framework consisting of two phases of inference. The first phase, referred as "image co-segmentation", iterates to extract consistent regions on images and jointly refines the regions over all images by employing the exemplar-SVM (E-SVM) technique [23]. In the second phase (i.e. "region co-labeling"), we construct a multi-image graphical model by taking the segmented regions as vertices, and incorporate several contexts of clothing configuration (e.g., item location and mutual interactions). The joint label assignment can be solved using the efficient Graph Cuts algorithm. In addition to evaluate our framework on the Fashionista dataset [30], we construct a dataset called CCP consisting of 2098 high-resolution street fashion photos to demonstrate the performance of our system. We achieve 90.29% / 88.23% segmentation accuracy and 65.52% / 63.89% recognition rate on the Fashionista and the CCP datasets, respectively, which are superior compared with state-of-the-art methods.



### Dynamical And-Or Graph Learning for Object Shape Modeling and Detection
- **Arxiv ID**: http://arxiv.org/abs/1502.00741v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00741v1)
- **Published**: 2015-02-03 05:14:40+00:00
- **Updated**: 2015-02-03 05:14:40+00:00
- **Authors**: Xiaolong Wang, Liang Lin
- **Comment**: 9 pages, 4 figures, NIPS 2012
- **Journal**: Advances in Neural Information Processing Systems (pp. 242-250),
  2014
- **Summary**: This paper studies a novel discriminative part-based model to represent and recognize object shapes with an "And-Or graph". We define this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global verification. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the configuration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches.



### Deep Joint Task Learning for Generic Object Extraction
- **Arxiv ID**: http://arxiv.org/abs/1502.00743v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00743v1)
- **Published**: 2015-02-03 05:35:09+00:00
- **Updated**: 2015-02-03 05:35:09+00:00
- **Authors**: Xiaolong Wang, Liliang Zhang, Liang Lin, Zhujin Liang, Wangmeng Zuo
- **Comment**: 9 pages, 4 figures, NIPS 2014
- **Journal**: Advances in Neural Information Processing Systems (pp. 523-531),
  2014
- **Summary**: This paper investigates how to extract objects-of-interest without relying on hand-craft features and sliding windows approaches, that aims to jointly solve two sub-tasks: (i) rapidly localizing salient objects from images, and (ii) accurately segmenting the objects based on the localizations. We present a general joint task learning framework, in which each task (either object localization or object segmentation) is tackled via a multi-layer convolutional neural network, and the two networks work collaboratively to boost performance. In particular, we propose to incorporate latent variables bridging the two networks in a joint optimization manner. The first network directly predicts the positions and scales of salient objects from raw images, and the latent variables adjust the object localizations to feed the second network that produces pixelwise object masks. An EM-type method is presented for the optimization, iterating with two steps: (i) by using the two networks, it estimates the latent variables by employing an MCMC-based sampling method; (ii) it optimizes the parameters of the two networks unitedly via back propagation, with the fixed latent variables. Extensive experiments suggest that our framework significantly outperforms other state-of-the-art approaches in both accuracy and efficiency (e.g. 1000 times faster than competing approaches).



### Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection
- **Arxiv ID**: http://arxiv.org/abs/1502.00744v1
- **DOI**: 10.1109/CVPR.2013.428
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00744v1)
- **Published**: 2015-02-03 05:45:56+00:00
- **Updated**: 2015-02-03 05:45:56+00:00
- **Authors**: Xiaolong Wang, Liang Lin, Lichao Huang, Shuicheng Yan
- **Comment**: 8 pages, 6 figures, CVPR 2013
- **Journal**: Computer Vision and Pattern Recognition (CVPR), 2013 IEEE
  Conference on , vol., no., pp.3334,3341, 23-28 June 2013
- **Summary**: This paper proposes a reconfigurable model to recognize and detect multiclass (or multiview) objects with large variation in appearance. Compared with well acknowledged hierarchical models, we study two advanced capabilities in hierarchy for object modeling: (i) "switch" variables(i.e. or-nodes) for specifying alternative compositions, and (ii) making local classifiers (i.e. leaf-nodes) shared among different classes. These capabilities enable us to account well for structural variabilities while preserving the model compact. Our model, in the form of an And-Or Graph, comprises four layers: a batch of leaf-nodes with collaborative edges in bottom for localizing object parts; the or-nodes over bottom to activate their children leaf-nodes; the and-nodes to classify objects as a whole; one root-node on the top for switching multiclass classification, which is also an or-node. For model training, we present an EM-type algorithm, namely dynamical structural optimization (DSO), to iteratively determine the structural configuration, (e.g., leaf-node generation associated with their parent or-nodes and shared across other classes), along with optimizing multi-layer parameters. The proposed method is valid on challenging databases, e.g., PASCAL VOC 2007 and UIUC-People, and it achieves state-of-the-arts performance.



### Data-Driven Scene Understanding with Adaptively Retrieved Exemplars
- **Arxiv ID**: http://arxiv.org/abs/1502.00749v1
- **DOI**: 10.1109/MMUL.2015.22
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00749v1)
- **Published**: 2015-02-03 06:04:14+00:00
- **Updated**: 2015-02-03 06:04:14+00:00
- **Authors**: Xionghao Liu, Wei Yang, Liang Lin, Qing Wang, Zhaoquan Cai, Jianhuang Lai
- **Comment**: 8 pages, 5 figures
- **Journal**: MultiMedia, IEEE , vol.PP, no.99, pp.1,1, 2015
- **Summary**: This article investigates a data-driven approach for semantically scene understanding, without pixelwise annotation and classifier training. Our framework parses a target image with two steps: (i) retrieving its exemplars (i.e. references) from an image database, where all images are unsegmented but annotated with tags; (ii) recovering its pixel labels by propagating semantics from the references. We present a novel framework making the two steps mutually conditional and bootstrapped under the probabilistic Expectation-Maximization (EM) formulation. In the first step, the references are selected by jointly matching their appearances with the target as well as the semantics (i.e. the assigned labels of the target and the references). We process the second step via a combinatorial graphical representation, in which the vertices are superpixels extracted from the target and its selected references. Then we derive the potentials of assigning labels to one vertex of the target, which depend upon the graph edges that connect the vertex to its spatial neighbors of the target and to its similar vertices of the references. Besides, the proposed framework can be naturally applied to perform image annotation on new test images. In the experiments, we validate our approach on two public databases, and demonstrate superior performances over the state-of-the-art methods in both semantic segmentation and image annotation tasks.



### Recognizing Focal Liver Lesions in Contrast-Enhanced Ultrasound with Discriminatively Trained Spatio-Temporal Model
- **Arxiv ID**: http://arxiv.org/abs/1502.00750v1
- **DOI**: 10.1109/ISBI.2014.6868087
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00750v1)
- **Published**: 2015-02-03 06:14:30+00:00
- **Updated**: 2015-02-03 06:14:30+00:00
- **Authors**: Xiaodan Liang, Qingxing Cao, Rui Huang, Liang Lin
- **Comment**: 5 pages, 1 figures
- **Journal**: Biomedical Imaging (ISBI), 2014 IEEE 11th International Symposium
  on , vol., no., pp.1184-1187, April 2014
- **Summary**: The aim of this study is to provide an automatic computational framework to assist clinicians in diagnosing Focal Liver Lesions (FLLs) in Contrast-Enhancement Ultrasound (CEUS). We represent FLLs in a CEUS video clip as an ensemble of Region-of-Interests (ROIs), whose locations are modeled as latent variables in a discriminative model. Different types of FLLs are characterized by both spatial and temporal enhancement patterns of the ROIs. The model is learned by iteratively inferring the optimal ROI locations and optimizing the model parameters. To efficiently search the optimal spatial and temporal locations of the ROIs, we propose a data-driven inference algorithm by combining effective spatial and temporal pruning. The experiments show that our method achieves promising results on the largest dataset in the literature (to the best of our knowledge), which we have made publicly available.



### Design of a Mobile Face Recognition System for Visually Impaired Persons
- **Arxiv ID**: http://arxiv.org/abs/1502.00756v2
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1502.00756v2)
- **Published**: 2015-02-03 06:38:22+00:00
- **Updated**: 2015-06-03 09:58:48+00:00
- **Authors**: Shonal Chaudhry, Rohitash Chandra
- **Comment**: Added author names in sections 1 and 2. Certain details in sections 3
  and 4 are now clearer. Removed external camera from implementation, results
  unaffected
- **Journal**: None
- **Summary**: It is estimated that 285 million people globally are visually impaired. A majority of these people live in developing countries and are among the elderly population. One of the most difficult tasks faced by the visually impaired is identification of people. While naturally, voice recognition is a common method of identification, it is an intuitive and difficult process. The rise of computation capability of mobile devices gives motivation to develop applications that can assist visually impaired persons. With the availability of mobile devices, these people can be assisted by an additional method of identification through intelligent software based on computer vision techniques. In this paper, we present the design and implementation of a face detection and recognition system for the visually impaired through the use of mobile computing. This mobile system is assisted by a server-based support system. The system was tested on a custom video database. Experiment results show high face detection accuracy and promising face recognition accuracy in suitable conditions. The challenges of the system lie in better recognition techniques for difficult situations in terms of lighting and weather.



### Landmark-Guided Elastic Shape Analysis of Human Character Motions
- **Arxiv ID**: http://arxiv.org/abs/1502.07666v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, 65D18, 58D10, 49Q10
- **Links**: [PDF](http://arxiv.org/pdf/1502.07666v1)
- **Published**: 2015-02-03 08:45:48+00:00
- **Updated**: 2015-02-03 08:45:48+00:00
- **Authors**: Martin Bauer, Markus Eslitzbichler, Markus Grasmair
- **Comment**: None
- **Journal**: None
- **Summary**: Motions of virtual characters in movies or video games are typically generated by recording actors using motion capturing methods. Animations generated this way often need postprocessing, such as improving the periodicity of cyclic animations or generating entirely new motions by interpolation of existing ones. Furthermore, search and classification of recorded motions becomes more and more important as the amount of recorded motion data grows.   In this paper, we will apply methods from shape analysis to the processing of animations. More precisely, we will use the by now classical elastic metric model used in shape matching, and extend it by incorporating additional inexact feature point information, which leads to an improved temporal alignment of different animations.



### Task-Driven Dictionary Learning for Hyperspectral Image Classification with Structured Sparsity Constraints
- **Arxiv ID**: http://arxiv.org/abs/1502.00836v1
- **DOI**: 10.1109/TGRS.2015.2399978
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00836v1)
- **Published**: 2015-02-03 12:24:40+00:00
- **Updated**: 2015-02-03 12:24:40+00:00
- **Authors**: Xiaoxia Sun, Nasser M. Nasrabadi, Trac D. Tran
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse representation models a signal as a linear combination of a small number of dictionary atoms. As a generative model, it requires the dictionary to be highly redundant in order to ensure both a stable high sparsity level and a low reconstruction error for the signal. However, in practice, this requirement is usually impaired by the lack of labelled training samples. Fortunately, previous research has shown that the requirement for a redundant dictionary can be less rigorous if simultaneous sparse approximation is employed, which can be carried out by enforcing various structured sparsity constraints on the sparse codes of the neighboring pixels. In addition, numerous works have shown that applying a variety of dictionary learning methods for the sparse representation model can also improve the classification performance. In this paper, we highlight the task-driven dictionary learning algorithm, which is a general framework for the supervised dictionary learning method. We propose to enforce structured sparsity priors on the task-driven dictionary learning method in order to improve the performance of the hyperspectral classification. Our approach is able to benefit from both the advantages of the simultaneous sparse representation and those of the supervised dictionary learning. We enforce two different structured sparsity priors, the joint and Laplacian sparsity, on the task-driven dictionary learning method and provide the details of the corresponding optimization algorithms. Experiments on numerous popular hyperspectral images demonstrate that the classification performance of our approach is superior to sparse representation classifier with structured priors or the task-driven dictionary learning method.



### Face frontalization for Alignment and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1502.00852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00852v1)
- **Published**: 2015-02-03 13:05:34+00:00
- **Updated**: 2015-02-03 13:05:34+00:00
- **Authors**: Christos Sagonas, Yannis Panagakis, Stefanos Zafeiriou, Maja Pantic
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Recently, it was shown that excellent results can be achieved in both face landmark localization and pose-invariant face recognition. These breakthroughs are attributed to the efforts of the community to manually annotate facial images in many different poses and to collect 3D faces data. In this paper, we propose a novel method for joint face landmark localization and frontal face reconstruction (pose correction) using a small set of frontal images only. By observing that the frontal facial image is the one with the minimum rank from all different poses we formulate an appropriate model which is able to jointly recover the facial landmarks as well as the frontalized version of the face. To this end, a suitable optimization problem, involving the minimization of the nuclear norm and the matrix $\ell_1$ norm, is solved. The proposed method is assessed in frontal face reconstruction (pose correction), face landmark localization, and pose-invariant face recognition and verification by conducting experiments on $6$ facial images databases. The experimental results demonstrate the effectiveness of the proposed method.



### DeepID3: Face Recognition with Very Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1502.00873v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00873v1)
- **Published**: 2015-02-03 14:28:55+00:00
- **Updated**: 2015-02-03 14:28:55+00:00
- **Authors**: Yi Sun, Ding Liang, Xiaogang Wang, Xiaoou Tang
- **Comment**: None
- **Journal**: None
- **Summary**: The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net and GoogLeNet to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53% LFW face verification accuracy and 96.0% LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end.



### Classification of Hyperspectral Imagery on Embedded Grassmannians
- **Arxiv ID**: http://arxiv.org/abs/1502.00946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00946v1)
- **Published**: 2015-02-03 18:17:13+00:00
- **Updated**: 2015-02-03 18:17:13+00:00
- **Authors**: Sofya Chepushtanova, Michael Kirby
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an approach for capturing the signal variability in hyperspectral imagery using the framework of the Grassmann manifold. Labeled points from each class are sampled and used to form abstract points on the Grassmannian. The resulting points on the Grassmannian have representations as orthonormal matrices and as such do not reside in Euclidean space in the usual sense. There are a variety of metrics which allow us to determine a distance matrices that can be used to realize the Grassmannian as an embedding in Euclidean space. We illustrate that we can achieve an approximately isometric embedding of the Grassmann manifold using the chordal metric while this is not the case with geodesic distances. However, non-isometric embeddings generated by using a pseudometric on the Grassmannian lead to the best classification results. We observe that as the dimension of the Grassmannian grows, the accuracy of the classification grows to 100% on two illustrative examples. We also observe a decrease in classification rates if the dimension of the points on the Grassmannian is too large for the dimension of the Euclidean space. We use sparse support vector machines to perform additional model reduction. The resulting classifier selects a subset of dimensions of the embedding without loss in classification performance.



### ORB-SLAM: a Versatile and Accurate Monocular SLAM System
- **Arxiv ID**: http://arxiv.org/abs/1502.00956v2
- **DOI**: 10.1109/TRO.2015.2463671
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1502.00956v2)
- **Published**: 2015-02-03 18:52:23+00:00
- **Updated**: 2015-09-18 09:50:11+00:00
- **Authors**: Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardos
- **Comment**: 17 pages. 13 figures. IEEE Transactions on Robotics, 2015. Project
  webpage (videos, code): http://webdiis.unizar.es/~raulmur/orbslam/
- **Journal**: None
- **Summary**: This paper presents ORB-SLAM, a feature-based monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.



### DFDL: Discriminative Feature-oriented Dictionary Learning for Histopathological Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1502.01032v1
- **DOI**: 10.1109/ISBI.2015.7164037
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.01032v1)
- **Published**: 2015-02-03 21:05:23+00:00
- **Updated**: 2015-02-03 21:05:23+00:00
- **Authors**: Tiep H. Vu, Hojjat S. Mousavi, Vishal Monga, UK Arvind Rao, Ganesh Rao
- **Comment**: Accepted to IEEE International Symposium on Biomedical Imaging
  (ISBI), 2015
- **Journal**: None
- **Summary**: In histopathological image analysis, feature extraction for classification is a challenging task due to the diversity of histology features suitable for each problem as well as presence of rich geometrical structure. In this paper, we propose an automatic feature discovery framework for extracting discriminative class-specific features and present a low-complexity method for classification and disease grading in histopathology. Essentially, our Discriminative Feature-oriented Dictionary Learning (DFDL) method learns class-specific features which are suitable for representing samples from the same class while are poorly capable of representing samples from other classes. Experiments on three challenging real-world image databases: 1) histopathological images of intraductal breast lesions, 2) mammalian lung images provided by the Animal Diagnostics Lab (ADL) at Pennsylvania State University, and 3) brain tumor images from The Cancer Genome Atlas (TCGA) database, show the significance of DFDL model in a variety problems over state-of-the-art methods



