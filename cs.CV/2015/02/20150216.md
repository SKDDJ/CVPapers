# Arxiv Papers in cs.CV on 2015-02-16
### Towards Building Deep Networks with Bayesian Factor Graphs
- **Arxiv ID**: http://arxiv.org/abs/1502.04492v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1502.04492v1)
- **Published**: 2015-02-16 11:01:25+00:00
- **Updated**: 2015-02-16 11:01:25+00:00
- **Authors**: Amedeo Buonanno, Francesco A. N. Palmieri
- **Comment**: Submitted for journal publication
- **Journal**: None
- **Summary**: We propose a Multi-Layer Network based on the Bayesian framework of the Factor Graphs in Reduced Normal Form (FGrn) applied to a two-dimensional lattice. The Latent Variable Model (LVM) is the basic building block of a quadtree hierarchy built on top of a bottom layer of random variables that represent pixels of an image, a feature map, or more generally a collection of spatially distributed discrete variables. The multi-layer architecture implements a hierarchical data representation that, via belief propagation, can be used for learning and inference. Typical uses are pattern completion, correction and classification. The FGrn paradigm provides great flexibility and modularity and appears as a promising candidate for building deep networks: the system can be easily extended by introducing new and different (in cardinality and in type) variables. Prior knowledge, or supervised information, can be introduced at different scales. The FGrn paradigm provides a handy way for building all kinds of architectures by interconnecting only three types of units: Single Input Single Output (SISO) blocks, Sources and Replicators. The network is designed like a circuit diagram and the belief messages flow bidirectionally in the whole system. The learning algorithms operate only locally within each block. The framework is demonstrated in this paper in a three-layer structure applied to images extracted from a standard data set.



### Color Image Enhancement Using the lrgb Coordinates in the Context of Support Fuzzification
- **Arxiv ID**: http://arxiv.org/abs/1502.04499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.04499v1)
- **Published**: 2015-02-16 11:32:00+00:00
- **Updated**: 2015-02-16 11:32:00+00:00
- **Authors**: Vasile Patrascu
- **Comment**: None
- **Journal**: Romanian Academy Journal, Fuzzy systems and A. I., Reports and
  Letters, F.S.A.I., Vol.10, Nos. 1-2, pp. 29-40, 2004
- **Summary**: Image enhancement is an important stage in the image-processing domain. The most known image enhancement method is the histogram equalization. This method is an automated one, and realizes a simultaneous modification for brightness and contrast in the case of monochrome images and for brightness, contrast, saturation and hue in the case of color images. Simple and efficient methods can be obtained if affine transforms within logarithmic models are used. A very important thing in the affine transform determination for color images is the coordinate system that is used for color space representation. Thus, the using of the RGB coordinates leads to a simultaneous modification of luminosity and saturation. In this paper using the lrgb perceptual coordinates one can define affine transforms, which allow a separated modification of luminosity l and saturation s (saturation being calculated with the component rgb in the chromatic plane). Better results can be obtained if partitions are defined on the image support and then the pixels are separately processed in each window belonging to the defined partition. Classical partitions frequently lead to the appearance of some discontinuities at the boundaries between these windows. In order to avoid all these drawbacks the classical partitions may be replaced by fuzzy partitions. Their elements will be fuzzy windows and in each of them there will be defined an affine transform induced by parameters using the fuzzy mean, fuzzy variance and fuzzy saturation computed for the pixels that belong to the analyzed window. The final image is obtained by summing up in a weight way the images of every fuzzy window.



### Bi-Level Image Thresholding obtained by means of Kaniadakis Entropy
- **Arxiv ID**: http://arxiv.org/abs/1502.04500v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.04500v3)
- **Published**: 2015-02-16 11:32:34+00:00
- **Updated**: 2015-02-22 12:24:51+00:00
- **Authors**: Amelia Carolina Sparavigna
- **Comment**: Kaniadakis Entropy, Image Processing, Image Segmentation, Image
  Thresholding, Texture Transitions, added reference and revised typos This
  paper has been withdrawn by the author due to a crucial sign error in Figures
  3 and 4
- **Journal**: None
- **Summary**: In this paper we are proposing the use of Kaniadakis entropy in the bi-level thresholding of images, in the framework of a maximum entropy principle. We discuss the role of its entropic index in determining the threshold and in driving an "image transition", that is, an abrupt transition in the appearance of the corresponding bi-level image. Some examples are proposed to illustrate the method and for comparing it to the approach which is using the Tsallis entropy.



### Clustering by Descending to the Nearest Neighbor in the Delaunay Graph Space
- **Arxiv ID**: http://arxiv.org/abs/1502.04502v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1502.04502v1)
- **Published**: 2015-02-16 11:50:42+00:00
- **Updated**: 2015-02-16 11:50:42+00:00
- **Authors**: Teng Qiu, Yongjie Li
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: In our previous works, we proposed a physically-inspired rule to organize the data points into an in-tree (IT) structure, in which some undesired edges are allowed to occur. By removing those undesired or redundant edges, this IT structure is divided into several separate parts, each representing one cluster. In this work, we seek to prevent the undesired edges from arising at the source. Before using the physically-inspired rule, data points are at first organized into a proximity graph which restricts each point to select the optimal directed neighbor just among its neighbors. Consequently, separated in-trees or clusters automatically arise, without redundant edges requiring to be removed.



### Image Specificity
- **Arxiv ID**: http://arxiv.org/abs/1502.04569v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.04569v2)
- **Published**: 2015-02-16 15:16:25+00:00
- **Updated**: 2015-04-16 13:13:46+00:00
- **Authors**: Mainak Jas, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: For some images, descriptions written by multiple people are consistent with each other. But for other images, descriptions across people vary considerably. In other words, some images are specific $-$ they elicit consistent descriptions from different people $-$ while other images are ambiguous. Applications involving images and text can benefit from an understanding of which images are specific and which ones are ambiguous. For instance, consider text-based image retrieval. If a query description is moderately similar to the caption (or reference description) of an ambiguous image, that query may be considered a decent match to the image. But if the image is very specific, a moderate similarity between the query and the reference description may not be sufficient to retrieve the image.   In this paper, we introduce the notion of image specificity. We present two mechanisms to measure specificity given multiple descriptions of an image: an automated measure and a measure that relies on human judgement. We analyze image specificity with respect to image content and properties to better understand what makes an image specific. We then train models to automatically predict the specificity of an image from image features alone without requiring textual descriptions of the image. Finally, we show that modeling image specificity leads to improvements in a text-based image retrieval application.



### DRAW: A Recurrent Neural Network For Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1502.04623v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1502.04623v2)
- **Published**: 2015-02-16 16:48:56+00:00
- **Updated**: 2015-05-20 15:29:42+00:00
- **Authors**: Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.



### Inferring 3D Object Pose in RGB-D Images
- **Arxiv ID**: http://arxiv.org/abs/1502.04652v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.04652v1)
- **Published**: 2015-02-16 18:15:54+00:00
- **Updated**: 2015-02-16 18:15:54+00:00
- **Authors**: Saurabh Gupta, Pablo Arbel√°ez, Ross Girshick, Jitendra Malik
- **Comment**: 13 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: The goal of this work is to replace objects in an RGB-D scene with corresponding 3D models from a library. We approach this problem by first detecting and segmenting object instances in the scene using the approach from Gupta et al. [13]. We use a convolutional neural network (CNN) to predict the pose of the object. This CNN is trained using pixel normals in images containing rendered synthetic objects. When tested on real data, it outperforms alternative algorithms trained on real data. We then use this coarse pose estimate along with the inferred pixel support to align a small number of prototypical models to the data, and place the model that fits the best into the scene. We observe a 48% relative improvement in performance at the task of 3D detection over the current state-of-the-art [33], while being an order of magnitude faster at the same time.



### HEp-2 Cell Classification via Fusing Texture and Shape Information
- **Arxiv ID**: http://arxiv.org/abs/1502.04658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.04658v1)
- **Published**: 2015-02-16 18:36:31+00:00
- **Updated**: 2015-02-16 18:36:31+00:00
- **Authors**: Xianbiao Qi, Guoying Zhao, Chun-Guang Li, Jun Guo, Matti Pietik√§inen
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Indirect Immunofluorescence (IIF) HEp-2 cell image is an effective evidence for diagnosis of autoimmune diseases. Recently computer-aided diagnosis of autoimmune diseases by IIF HEp-2 cell classification has attracted great attention. However the HEp-2 cell classification task is quite challenging due to large intra-class variation and small between-class variation. In this paper we propose an effective and efficient approach for the automatic classification of IIF HEp-2 cell image by fusing multi-resolution texture information and richer shape information. To be specific, we propose to: a) capture the multi-resolution texture information by a novel Pairwise Rotation Invariant Co-occurrence of Local Gabor Binary Pattern (PRICoLGBP) descriptor, b) depict the richer shape information by using an Improved Fisher Vector (IFV) model with RootSIFT features which are sampled from large image patches in multiple scales, and c) combine them properly. We evaluate systematically the proposed approach on the IEEE International Conference on Pattern Recognition (ICPR) 2012, IEEE International Conference on Image Processing (ICIP) 2013 and ICPR 2014 contest data sets. The experimental results for the proposed methods significantly outperform the winners of ICPR 2012 and ICIP 2013 contest, and achieve comparable performance with the winner of the newly released ICPR 2014 contest.



### Unsupervised Learning of Video Representations using LSTMs
- **Arxiv ID**: http://arxiv.org/abs/1502.04681v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1502.04681v3)
- **Published**: 2015-02-16 20:00:07+00:00
- **Updated**: 2016-01-04 00:42:07+00:00
- **Authors**: Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov
- **Comment**: Added link to code on github
- **Journal**: None
- **Summary**: We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations ("percepts") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We try to visualize and interpret the learned features. We stress test the model by running it on longer time scales and on out-of-domain data. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only a few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.



### ICR: Iterative Convex Refinement for Sparse Signal Recovery Using Spike and Slab Priors
- **Arxiv ID**: http://arxiv.org/abs/1502.04726v1
- **DOI**: 10.1109/LSP.2015.2438255
- **Categories**: **stat.ML**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1502.04726v1)
- **Published**: 2015-02-16 21:17:52+00:00
- **Updated**: 2015-02-16 21:17:52+00:00
- **Authors**: Hojjat S. Mousavi, Vishal Monga, Trac D. Tran
- **Comment**: Submitted to IEEE Signal Processing Letters, Feb 2015
- **Journal**: None
- **Summary**: In this letter, we address sparse signal recovery using spike and slab priors. In particular, we focus on a Bayesian framework where sparsity is enforced on reconstruction coefficients via probabilistic priors. The optimization resulting from spike and slab prior maximization is known to be a hard non-convex problem, and existing solutions involve simplifying assumptions and/or relaxations. We propose an approach called Iterative Convex Refinement (ICR) that aims to solve the aforementioned optimization problem directly allowing for greater generality in the sparse structure. Essentially, ICR solves a sequence of convex optimization problems such that sequence of solutions converges to a sub-optimal solution of the original hard optimization problem. We propose two versions of our algorithm: a.) an unconstrained version, and b.) with a non-negativity constraint on sparse coefficients, which may be required in some real-world problems. Experimental validation is performed on both synthetic data and for a real-world image recovery problem, which illustrates merits of ICR over state of the art alternatives.



