# Arxiv Papers in cs.CV on 2015-02-05
### Fast unsupervised Bayesian image segmentation with adaptive spatial regularisation
- **Arxiv ID**: http://arxiv.org/abs/1502.01400v3
- **DOI**: None
- **Categories**: **stat.CO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1502.01400v3)
- **Published**: 2015-02-05 00:35:08+00:00
- **Updated**: 2016-02-02 08:53:06+00:00
- **Authors**: Marcelo Pereyra, Steve McLaughlin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new Bayesian estimation technique for hidden Potts-Markov random fields with unknown regularisation parameters, with application to fast unsupervised K-class image segmentation. The technique is derived by first removing the regularisation parameter from the Bayesian model by marginalisation, followed by a small-variance-asymptotic (SVA) analysis in which the spatial regularisation and the integer-constrained terms of the Potts model are decoupled. The evaluation of this SVA Bayesian estimator is then relaxed into a problem that can be computed efficiently by iteratively solving a convex total-variation denoising problem and a least-squares clustering (K-means) problem, both of which can be solved straightforwardly, even in high-dimensions, and with parallel computing techniques. This leads to a fast fully unsupervised Bayesian image segmentation methodology in which the strength of the spatial regularisation is adapted automatically to the observed image during the inference procedure, and that can be easily applied in large 2D and 3D scenarios or in applications requiring low computing times. Experimental results on real images, as well as extensive comparisons with state-of-the-art algorithms, confirm that the proposed methodology offer extremely fast convergence and produces accurate segmentation results, with the important additional advantage of self-adjusting regularisation parameters.



### Collaborative Feature Learning from Social Media
- **Arxiv ID**: http://arxiv.org/abs/1502.01423v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.01423v3)
- **Published**: 2015-02-05 03:32:19+00:00
- **Updated**: 2015-04-09 18:36:54+00:00
- **Authors**: Chen Fang, Hailin Jin, Jianchao Yang, Zhe Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Image feature representation plays an essential role in image recognition and related tasks. The current state-of-the-art feature learning paradigm is supervised learning from labeled data. However, this paradigm requires large-scale category labels, which limits its applicability to domains where labels are hard to obtain. In this paper, we propose a new data-driven feature learning paradigm which does not rely on category labels. Instead, we learn from user behavior data collected on social media. Concretely, we use the image relationship discovered in the latent space from the user behavior data to guide the image feature learning. We collect a large-scale image and user behavior dataset from Behance.net. The dataset consists of 1.9 million images and over 300 million view records from 1.9 million users. We validate our feature learning paradigm on this dataset and find that the learned feature significantly outperforms the state-of-the-art image features in learning better image similarities. We also show that the learned feature performs competitively on various recognition benchmarks.



### Fast Constraint Propagation for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1502.01475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.01475v1)
- **Published**: 2015-02-05 09:41:28+00:00
- **Updated**: 2015-02-05 09:41:28+00:00
- **Authors**: Peng Han
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel selective constraint propagation method for constrained image segmentation. In the literature, many pairwise constraint propagation methods have been developed to exploit pairwise constraints for cluster analysis. However, since most of these methods have a polynomial time complexity, they are not much suitable for segmentation of images even with a moderate size, which is actually equivalent to cluster analysis with a large data size. Considering the local homogeneousness of a natural image, we choose to perform pairwise constraint propagation only over a selected subset of pixels, but not over the whole image. Such a selective constraint propagation problem is then solved by an efficient graph-based learning algorithm. To further speed up our selective constraint propagation, we also discard those less important propagated constraints during graph-based learning. Finally, the selectively propagated constraints are exploited based on $L_1$-minimization for normalized cuts over the whole image. The experimental results demonstrate the promising performance of the proposed method for segmentation with selectively propagated constraints.



### Ring artifacts correction in compressed sensing tomographic reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1502.01480v1
- **DOI**: 10.1107/S1600577515010176
- **Categories**: **physics.comp-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1502.01480v1)
- **Published**: 2015-02-05 09:51:59+00:00
- **Updated**: 2015-02-05 09:51:59+00:00
- **Authors**: Pierre Paleo, Alessandro Mirone
- **Comment**: IUCR template, preprint mode, 35 figures
- **Journal**: None
- **Summary**: We present a novel approach to handle ring artifacts correction in compressed sensing tomographic reconstruction. The correction is part of the reconstruction process, which differs from classical sinogram pre-processing and image post-processing techniques. The principle of compressed sensing tomographic reconstruction is presented. Then, we show that the ring artifacts correction can be integrated in the reconstruction problem formalism. We provide numerical results for both simulated and real data. This technique is included in the PyHST2 code which is used at the European Synchrotron Radiation Facility for tomographic reconstruction.



### Object Proposal with Kernelized Partial Ranking
- **Arxiv ID**: http://arxiv.org/abs/1502.01526v3
- **DOI**: 10.1016/j.patcog.2017.03.022
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.01526v3)
- **Published**: 2015-02-05 12:58:29+00:00
- **Updated**: 2017-05-18 15:57:36+00:00
- **Authors**: Jing Wang, Jie Shen, Ping Li
- **Comment**: None
- **Journal**: Pattern Recognition, 2017
- **Summary**: Object proposals are an ensemble of bounding boxes with high potential to contain objects. In order to determine a small set of proposals with a high recall, a common scheme is extracting multiple features followed by a ranking algorithm which however, incurs two major challenges: {\bf 1)} The ranking model often imposes pairwise constraints between each proposal, rendering the problem away from an efficient training/testing phase; {\bf 2)} Linear kernels are utilized due to the computational and memory bottleneck of training a kernelized model.   In this paper, we remedy these two issues by suggesting a {\em kernelized partial ranking model}. In particular, we demonstrate that {\bf i)} our partial ranking model reduces the number of constraints from $O(n^2)$ to $O(nk)$ where $n$ is the number of all potential proposals for an image but we are only interested in top-$k$ of them that has the largest overlap with the ground truth; {\bf ii)} we permit non-linear kernels in our model which is often superior to the linear classifier in terms of accuracy. For the sake of mitigating the computational and memory issues, we introduce a consistent weighted sampling~(CWS) paradigm that approximates the non-linear kernel as well as facilitates an efficient learning. In fact, as we will show, training a linear CWS model amounts to learning a kernelized model. Extensive experiments demonstrate that equipped with the non-linear kernel and the partial ranking algorithm, recall at top-$k$ proposals can be substantially improved.



### Semantic Embedding Space for Zero-Shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1502.01540v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.01540v1)
- **Published**: 2015-02-05 13:34:48+00:00
- **Updated**: 2015-02-05 13:34:48+00:00
- **Authors**: Xun Xu, Timothy Hospedales, Shaogang Gong
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: The number of categories for action recognition is growing rapidly. It is thus becoming increasingly hard to collect sufficient training data to learn conventional models for each category. This issue may be ameliorated by the increasingly popular 'zero-shot learning' (ZSL) paradigm. In this framework a mapping is constructed between visual features and a human interpretable semantic description of each category, allowing categories to be recognised in the absence of any training data. Existing ZSL studies focus primarily on image data, and attribute-based semantic representations. In this paper, we address zero-shot recognition in contemporary video action recognition tasks, using semantic word vector space as the common space to embed videos and category labels. This is more challenging because the mapping between the semantic space and space-time features of videos containing complex actions is more complex and harder to learn. We demonstrate that a simple self-training and data augmentation strategy can significantly improve the efficacy of this mapping. Experiments on human action datasets including HMDB51 and UCF101 demonstrate that our approach achieves the state-of-the-art zero-shot action recognition performance.



### Performance Analysis of Cone Detection Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1502.01643v1
- **DOI**: 10.1364/JOSAA.32.000497
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1502.01643v1)
- **Published**: 2015-02-05 17:11:40+00:00
- **Updated**: 2015-02-05 17:11:40+00:00
- **Authors**: Letizia Mariotti, Nicholas Devaney
- **Comment**: 13 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: Many algorithms have been proposed to help clinicians evaluate cone density and spacing, as these may be related to the onset of retinal diseases. However, there has been no rigorous comparison of the performance of these algorithms. In addition, the performance of such algorithms is typically determined by comparison with human observers. Here we propose a technique to simulate realistic images of the cone mosaic. We use the simulated images to test the performance of two popular cone detection algorithms and we introduce an algorithm which is used by astronomers to detect stars in astronomical images. We use Free Response Operating Characteristic (FROC) curves to evaluate and compare the performance of the three algorithms. This allows us to optimize the performance of each algorithm. We observe that performance is significantly enhanced by up-sampling the images. We investigate the effect of noise and image quality on cone mosaic parameters estimated using the different algorithms, finding that the estimated regularity is the most sensitive parameter.   This paper was published in JOSA A and is made available as an electronic reprint with the permission of OSA. The paper can be found at the following URL on the OSA website: http://www.opticsinfobase.org/abstract.cfm?msid=224577. Systematic or multiple reproduction or distribution to multiple locations via electronic or other means is prohibited and is subject to penalties under law.



### Learning Articulated Motions From Visual Demonstration
- **Arxiv ID**: http://arxiv.org/abs/1502.01659v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1502.01659v1)
- **Published**: 2015-02-05 17:59:07+00:00
- **Updated**: 2015-02-05 17:59:07+00:00
- **Authors**: Sudeep Pillai, Matthew R. Walter, Seth Teller
- **Comment**: Published in Robotics: Science and Systems X, Berkeley, CA. ISBN:
  978-0-9923747-0-9
- **Journal**: None
- **Summary**: Many functional elements of human homes and workplaces consist of rigid components which are connected through one or more sliding or rotating linkages. Examples include doors and drawers of cabinets and appliances; laptops; and swivel office chairs. A robotic mobile manipulator would benefit from the ability to acquire kinematic models of such objects from observation. This paper describes a method by which a robot can acquire an object model by capturing depth imagery of the object as a human moves it through its range of motion. We envision that in future, a machine newly introduced to an environment could be shown by its human user the articulated objects particular to that environment, inferring from these "visual demonstrations" enough information to actuate each object independently of the user.   Our method employs sparse (markerless) feature tracking, motion segmentation, component pose estimation, and articulation learning; it does not require prior object models. Using the method, a robot can observe an object being exercised, infer a kinematic model incorporating rigid, prismatic and revolute joints, then use the model to predict the object's motion from a novel vantage point. We evaluate the method's performance, and compare it to that of a previously published technique, for a variety of household objects.



### A Framework for Symmetric Part Detection in Cluttered Scenes
- **Arxiv ID**: http://arxiv.org/abs/1502.01761v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.01761v1)
- **Published**: 2015-02-05 23:51:16+00:00
- **Updated**: 2015-02-05 23:51:16+00:00
- **Authors**: Tom Lee, Sanja Fidler, Alex Levinshtein, Cristian Sminchisescu, Sven Dickinson
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: The role of symmetry in computer vision has waxed and waned in importance during the evolution of the field from its earliest days. At first figuring prominently in support of bottom-up indexing, it fell out of favor as shape gave way to appearance and recognition gave way to detection. With a strong prior in the form of a target object, the role of the weaker priors offered by perceptual grouping was greatly diminished. However, as the field returns to the problem of recognition from a large database, the bottom-up recovery of the parts that make up the objects in a cluttered scene is critical for their recognition. The medial axis community has long exploited the ubiquitous regularity of symmetry as a basis for the decomposition of a closed contour into medial parts. However, today's recognition systems are faced with cluttered scenes, and the assumption that a closed contour exists, i.e. that figure-ground segmentation has been solved, renders much of the medial axis community's work inapplicable. In this article, we review a computational framework, previously reported in Lee et al. (2013), Levinshtein et al. (2009, 2013), that bridges the representation power of the medial axis and the need to recover and group an object's parts in a cluttered scene. Our framework is rooted in the idea that a maximally inscribed disc, the building block of a medial axis, can be modeled as a compact superpixel in the image. We evaluate the method on images of cluttered scenes.



