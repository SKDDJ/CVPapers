# Arxiv Papers in cs.CV on 2015-02-01
### Pose and Shape Estimation with Discriminatively Learned Parts
- **Arxiv ID**: http://arxiv.org/abs/1502.00192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00192v1)
- **Published**: 2015-02-01 04:09:23+00:00
- **Updated**: 2015-02-01 04:09:23+00:00
- **Authors**: Menglong Zhu, Xiaowei Zhou, Kostas Daniilidis
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new approach for estimating the 3D pose and the 3D shape of an object from a single image. Given a training set of view exemplars, we learn and select appearance-based discriminative parts which are mapped onto the 3D model from the training set through a facil- ity location optimization. The training set of 3D models is summarized into a sparse set of shapes from which we can generalize by linear combination. Given a test picture, we detect hypotheses for each part. The main challenge is to select from these hypotheses and compute the 3D pose and shape coefficients at the same time. To achieve this, we optimize a function that minimizes simultaneously the geometric reprojection error as well as the appearance matching of the parts. We apply the alternating direction method of multipliers (ADMM) to minimize the resulting convex function. We evaluate our approach on the Fine Grained 3D Car dataset with superior performance in shape and pose errors. Our main and novel contribution is the simultaneous solution for part localization, 3D pose and shape by maximizing both geometric and appearance compatibility.



### Driver distraction detection and recognition using RGB-D sensor
- **Arxiv ID**: http://arxiv.org/abs/1502.00250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00250v1)
- **Published**: 2015-02-01 13:24:49+00:00
- **Updated**: 2015-02-01 13:24:49+00:00
- **Authors**: Céline Craye, Fakhri Karray
- **Comment**: None
- **Journal**: None
- **Summary**: Driver inattention assessment has become a very active field in intelligent transportation systems. Based on active sensor Kinect and computer vision tools, we have built an efficient module for detecting driver distraction and recognizing the type of distraction. Based on color and depth map data from the Kinect, our system is composed of four sub-modules. We call them eye behavior (detecting gaze and blinking), arm position (is the right arm up, down, right of forward), head orientation, and facial expressions. Each module produces relevant information for assessing driver inattention. They are merged together later on using two different classification strategies: AdaBoost classifier and Hidden Markov Model. Evaluation is done using a driving simulator and 8 drivers of different gender, age and nationality for a total of more than 8 hours of recording. Qualitative and quantitative results show strong and accurate detection and recognition capacity (85% accuracy for the type of distraction and 90% for distraction detection). Moreover, each module is obtained independently and could be used for other types of inference, such as fatigue detection, and could be implemented for real cars systems.



### Freehand Sketch Recognition Using Deep Features
- **Arxiv ID**: http://arxiv.org/abs/1502.00254v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00254v2)
- **Published**: 2015-02-01 13:35:44+00:00
- **Updated**: 2015-02-04 14:41:15+00:00
- **Authors**: Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu
- **Comment**: Submitted to ICIP-2015, 5 pages. Removed an erroneous claim regarding
  a work cited in the paper
- **Journal**: None
- **Summary**: Freehand sketches often contain sparse visual detail. In spite of the sparsity, they are easily and consistently recognized by humans across cultures, languages and age groups. Therefore, analyzing such sparse sketches can aid our understanding of the neuro-cognitive processes involved in visual representation and recognition. In the recent past, Convolutional Neural Networks (CNNs) have emerged as a powerful framework for feature representation and recognition for a variety of image domains. However, the domain of sketch images has not been explored. This paper introduces a freehand sketch recognition framework based on "deep" features extracted from CNNs. We use two popular CNNs for our experiments -- Imagenet CNN and a modified version of LeNet CNN. We evaluate our recognition framework on a publicly available benchmark database containing thousands of freehand sketches depicting everyday objects. Our results are an improvement over the existing state-of-the-art accuracies by 3% - 11%. The effectiveness and relative compactness of our deep features also make them an ideal candidate for related problems such as sketch-based image retrieval. In addition, we provide a preliminary glimpse of how such features can help identify crucial attributes (e.g. object-parts) of the sketched objects.



### Human Re-identification by Matching Compositional Template with Cluster Sampling
- **Arxiv ID**: http://arxiv.org/abs/1502.00256v1
- **DOI**: 10.1109/ICCV.2013.391
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00256v1)
- **Published**: 2015-02-01 13:47:49+00:00
- **Updated**: 2015-02-01 13:47:49+00:00
- **Authors**: Yuanlu Xu, Liang Lin, Wei-Shi Zheng, Xiaobai Liu
- **Comment**: This manuscript has 8 pages with 7 figures, and a preliminary version
  was published in ICCV 2013
- **Journal**: None
- **Summary**: This paper aims at a newly raising task in visual surveillance: re-identifying people at a distance by matching body information, given several reference examples. Most of existing works solve this task by matching a reference template with the target individual, but often suffer from large human appearance variability (e.g. different poses/views, illumination) and high false positives in matching caused by conjunctions, occlusions or surrounding clutters. Addressing these problems, we construct a simple yet expressive template from a few reference images of a certain individual, which represents the body as an articulated assembly of compositional and alternative parts, and propose an effective matching algorithm with cluster sampling. This algorithm is designed within a candidacy graph whose vertices are matching candidates (i.e. a pair of source and target body parts), and iterates in two steps for convergence. (i) It generates possible partial matches based on compatible and competitive relations among body parts. (ii) It confirms the partial matches to generate a new matching solution, which is accepted by the Markov Chain Monte Carlo (MCMC) mechanism. In the experiments, we demonstrate the superior performance of our approach on three public databases compared to existing methods.



### Learning Latent Spatio-Temporal Compositional Model for Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1502.00258v1
- **DOI**: 10.1145/2502081.2502089
- **Categories**: **cs.CV**, 68U01, I.5; I.4
- **Links**: [PDF](http://arxiv.org/pdf/1502.00258v1)
- **Published**: 2015-02-01 13:49:31+00:00
- **Updated**: 2015-02-01 13:49:31+00:00
- **Authors**: Xiaodan Liang, Liang Lin, Liangliang Cao
- **Comment**: This manuscript has 10 pages with 7 figures, and a preliminary
  version was published in ACM MM'13
- **Journal**: None
- **Summary**: Action recognition is an important problem in multimedia understanding. This paper addresses this problem by building an expressive compositional action model. We model one action instance in the video with an ensemble of spatio-temporal compositions: a number of discrete temporal anchor frames, each of which is further decomposed to a layout of deformable parts. In this way, our model can identify a Spatio-Temporal And-Or Graph (STAOG) to represent the latent structure of actions e.g. triple jumping, swinging and high jumping. The STAOG model comprises four layers: (i) a batch of leaf-nodes in bottom for detecting various action parts within video patches; (ii) the or-nodes over bottom, i.e. switch variables to activate their children leaf-nodes for structural variability; (iii) the and-nodes within an anchor frame for verifying spatial composition; and (iv) the root-node at top for aggregating scores over temporal anchor frames. Moreover, the contextual interactions are defined between leaf-nodes in both spatial and temporal domains. For model training, we develop a novel weakly supervised learning algorithm which iteratively determines the structural configuration (e.g. the production of leaf-nodes associated with the or-nodes) along with the optimization of multi-layer parameters. By fully exploiting spatio-temporal compositions and interactions, our approach handles well large intra-class action variance (e.g. different views, individual appearances, spatio-temporal structures). The experimental results on the challenging databases demonstrate superior performance of our approach over other competing methods.



### Dynamic texture and scene classification by transferring deep image features
- **Arxiv ID**: http://arxiv.org/abs/1502.00303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00303v1)
- **Published**: 2015-02-01 20:22:00+00:00
- **Updated**: 2015-02-01 20:22:00+00:00
- **Authors**: Xianbiao Qi, Chun-Guang Li, Guoying Zhao, Xiaopeng Hong, Matti Pietikäinen
- **Comment**: None
- **Journal**: None
- **Summary**: Dynamic texture and scene classification are two fundamental problems in understanding natural video content. Extracting robust and effective features is a crucial step towards solving these problems. However the existing approaches suffer from the sensitivity to either varying illumination, or viewpoint changing, or even camera motion, and/or the lack of spatial information. Inspired by the success of deep structures in image classification, we attempt to leverage a deep structure to extract feature for dynamic texture and scene classification. To tackle with the challenges in training a deep structure, we propose to transfer some prior knowledge from image domain to video domain. To be specific, we propose to apply a well-trained Convolutional Neural Network (ConvNet) as a mid-level feature extractor to extract features from each frame, and then form a representation of a video by concatenating the first and the second order statistics over the mid-level features. We term this two-level feature extraction scheme as a Transferred ConvNet Feature (TCoF). Moreover we explore two different implementations of the TCoF scheme, i.e., the \textit{spatial} TCoF and the \textit{temporal} TCoF, in which the mean-removed frames and the difference between two adjacent frames are used as the inputs of the ConvNet, respectively. We evaluate systematically the proposed spatial TCoF and the temporal TCoF schemes on three benchmark data sets, including DynTex, YUPENN, and Maryland, and demonstrate that the proposed approach yields superior performance.



### Efficient refinement of GPS-based localization in urban areas using visual information and sensor parameter
- **Arxiv ID**: http://arxiv.org/abs/1502.00319v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1502.00319v3)
- **Published**: 2015-02-01 21:56:39+00:00
- **Updated**: 2015-10-28 18:08:37+00:00
- **Authors**: Mahdi Salarian, Rashid Ansari
- **Comment**: this paper has been withdrawn bt the authors due to couple of errors
  in exprimental part
- **Journal**: None
- **Summary**: An efficient method is proposed for refining GPS-acquired location coordinates in urban areas using camera images, Google Street View (GSV) and sensor parameters. The main goal is to compensate for GPS location imprecision in dense area of cities due to proximity to walls and buildings. Avail-able methods for better localization often use visual information by using query images acquired with camera-equipped mobile devices and applying image retrieval techniques to find the closest match in a GPS-referenced image data set. The search areas required for reliable search are about 1-2 sq. Km and the accuracy is typically 25-100 meters. Here we describe a method based on image retrieval where a reliable search can be confined to areas of 0.01 sq. Km and the accuracy in our experiments is less than 10 meters. To test our procedure we created a database by acquiring all Google Street View images close to what is seen by a pedestrian in a large region of downtown Chicago and saved all coordinates and orientation data to be used for confining our search region. Prior knowledge from approximate position of query image is leveraged to address complexity and accuracy issues of our search in a large scale geo-tagged data set. One key aspect that differentiates our work is that it utilizes the sensor information of GPS SOS and the camera orientation in improving localization. Finally we demonstrate retrieval-based technique are less accurate in sparse open areas compared with purely GPS measurement. The effectiveness of our approach is discussed in detail and experimental results show improved performance when compared with regular approaches.



### Modified Fast Fractal Image Compression Algorithm in spatial domain
- **Arxiv ID**: http://arxiv.org/abs/1502.00324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00324v1)
- **Published**: 2015-02-01 22:49:52+00:00
- **Updated**: 2015-02-01 22:49:52+00:00
- **Authors**: M. Salarian, H. Miar Naimi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper a new fractal image compression algorithm is proposed in which the time of encoding process is considerably reduced. The algorithm exploits a domain pool reduction approach, along with using innovative predefined values for contrast scaling factor, S, instead of searching it across [0,1]. Only the domain blocks with entropy greater than a threshold are considered as domain pool. As a novel point, it is assumed that in each step of the encoding process, the domain block with small enough distance shall be found only for the range blocks with low activity (equivalently low entropy). This novel point is used to find reasonable estimations of S, and use them in the encoding process as predefined values, mentioned above, the remaining range blocks are split into four new smaller range blocks and the algorithm must be iterated for them, considered as the other step of encoding process. The algorithm has been examined for some of the well-known images and the results have been compared with the state-of-the-art algorithms. The experiments show that our proposed algorithm has considerably lower encoding time than the other where the encoded images are approximately the same in quality.



